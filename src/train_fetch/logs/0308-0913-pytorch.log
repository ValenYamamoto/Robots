epoch: 0, step: 0
	action: tensor([[ 0.0375,  0.0354,  0.0127, -0.0123, -0.0221, -0.0084, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[0.0665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -13.318095719751307
epoch: 0, step: 1
	action: tensor([[ 0.0211,  0.0023,  0.0144, -0.0132, -0.0255, -0.0087,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[0.0785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27327070202428727, distance: 0.9755353869865645 entropy -13.317364830657171
epoch: 0, step: 2
	action: tensor([[-0.0113,  0.0326,  0.0169, -0.0137, -0.0232, -0.0084,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26267098889352314, distance: 0.9826239705912777 entropy -13.293971461626478
epoch: 0, step: 3
	action: tensor([[-0.0102,  0.0239,  0.0169, -0.0137, -0.0233, -0.0084,  0.0463]],
       dtype=torch.float64)
	q_value: tensor([[0.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25621363628445093, distance: 0.9869173847512046 entropy -13.294453844594718
epoch: 0, step: 4
	action: tensor([[ 0.0380,  0.0194,  0.0169, -0.0137, -0.0232, -0.0084,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[0.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985289744717773, distance: 0.9584325657352749 entropy -13.294340459737947
epoch: 0, step: 5
	action: tensor([[ 0.0679,  0.0155,  0.0169, -0.0137, -0.0232, -0.0084,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[0.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32310406948857817, distance: 0.9414941911379638 entropy -13.294069566133304
epoch: 0, step: 6
	action: tensor([[ 0.0298,  0.0264,  0.0169, -0.0137, -0.0231, -0.0084, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[0.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29437360644926946, distance: 0.961267151351477 entropy -13.293509352393714
epoch: 0, step: 7
	action: tensor([[ 0.1367,  0.0305,  0.0169, -0.0137, -0.0232, -0.0084, -0.0049]],
       dtype=torch.float64)
	q_value: tensor([[0.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39513911540608826, distance: 0.889988599322894 entropy -13.29432568715844
epoch: 0, step: 8
	action: tensor([[-0.0372,  0.0389,  0.0168, -0.0136, -0.0231, -0.0085, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[0.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23762192964431594, distance: 0.9991757633795945 entropy -13.292694092910356
epoch: 0, step: 9
	action: tensor([[ 0.1044, -0.0030,  0.0169, -0.0137, -0.0233, -0.0084, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[0.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3419362442588456, distance: 0.9283049775869754 entropy -13.295022764946486
epoch: 0, step: 10
	action: tensor([[-0.0231, -0.0031,  0.0168, -0.0137, -0.0231, -0.0085,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22074410453082682, distance: 1.010175291385251 entropy -13.293236828275003
epoch: 0, step: 11
	action: tensor([[ 0.0367,  0.0246,  0.0169, -0.0137, -0.0233, -0.0083, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[0.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29936449178870495, distance: 0.9578616044469833 entropy -13.294747200201652
epoch: 0, step: 12
	action: tensor([[ 0.0052, -0.0065,  0.0169, -0.0137, -0.0232, -0.0084, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[0.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24619600927145657, distance: 0.9935412648870326 entropy -13.29430024195405
epoch: 0, step: 13
	action: tensor([[ 0.0643, -0.0055,  0.0169, -0.0138, -0.0232, -0.0084,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[0.0520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30347379156150933, distance: 0.9550484948610585 entropy -13.294689514678366
epoch: 0, step: 14
	action: tensor([[-0.0063,  0.0244,  0.0169, -0.0137, -0.0231, -0.0084,  0.0039]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25706978457591234, distance: 0.9863492169533162 entropy -13.293488785561351
epoch: 0, step: 15
	action: tensor([[-0.0025, -0.0375,  0.0169, -0.0137, -0.0233, -0.0084, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[0.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21443603126508304, distance: 1.014255732653338 entropy -13.294658299586311
epoch: 0, step: 16
	action: tensor([[ 0.0054,  0.0092,  0.0168, -0.0138, -0.0232, -0.0083, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[0.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25799078245391793, distance: 0.9857376472315139 entropy -13.294577248637594
epoch: 0, step: 17
	action: tensor([[-0.0072,  0.0113,  0.0169, -0.0137, -0.0232, -0.0084,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2474951019275835, distance: 0.9926847698632906 entropy -13.294839260761446
epoch: 0, step: 18
	action: tensor([[-0.0130,  0.0081,  0.0169, -0.0137, -0.0232, -0.0084,  0.0497]],
       dtype=torch.float64)
	q_value: tensor([[0.0529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23833232472223453, distance: 0.9987101315613999 entropy -13.294515221645753
epoch: 0, step: 19
	action: tensor([[ 0.0443,  0.0132,  0.0169, -0.0137, -0.0232, -0.0083, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965048645888748, distance: 0.9598143586496403 entropy -13.294245286760898
epoch: 0, step: 20
	action: tensor([[-0.0136, -0.0014,  0.0169, -0.0137, -0.0232, -0.0084,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2293396910660136, distance: 1.004588470183719 entropy -13.29410978846748
epoch: 0, step: 21
	action: tensor([[-0.0046, -0.0133,  0.0169, -0.0137, -0.0232, -0.0083, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[0.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22893984690043523, distance: 1.0048490433223543 entropy -13.294715296813363
epoch: 0, step: 22
	action: tensor([[ 0.0367,  0.0003,  0.0169, -0.0138, -0.0232, -0.0083, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[0.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2792128397932945, distance: 0.9715389444587467 entropy -13.294643198641754
epoch: 0, step: 23
	action: tensor([[ 0.0376, -0.0019,  0.0169, -0.0137, -0.0232, -0.0084,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[0.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2783394489208112, distance: 0.9721273819358685 entropy -13.29421273255918
epoch: 0, step: 24
	action: tensor([[ 0.0395, -0.0004,  0.0169, -0.0137, -0.0232, -0.0084,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[0.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2809940090795625, distance: 0.9703377953466988 entropy -13.294180580689252
epoch: 0, step: 25
	action: tensor([[ 0.0108,  0.0010,  0.0169, -0.0137, -0.0232, -0.0084, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25360064052224096, distance: 0.9886494345267312 entropy -13.293889422199609
epoch: 0, step: 26
	action: tensor([[ 0.0493, -0.0283,  0.0169, -0.0137, -0.0232, -0.0084, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2681974957266975, distance: 0.9789345103265435 entropy -13.294510881441926
epoch: 0, step: 27
	action: tensor([[ 0.0631, -0.0143,  0.0168, -0.0138, -0.0231, -0.0084,  0.0506]],
       dtype=torch.float64)
	q_value: tensor([[0.0519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921503894880746, distance: 0.9627802925525804 entropy -13.294077870860905
epoch: 0, step: 28
	action: tensor([[ 0.0331,  0.0115,  0.0169, -0.0137, -0.0231, -0.0084,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28141487683325084, distance: 0.9700537617833925 entropy -13.293317653108087
epoch: 0, step: 29
	action: tensor([[-0.0293,  0.0120,  0.0169, -0.0137, -0.0232, -0.0084,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[0.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21988649996537912, distance: 1.0107310091663446 entropy -13.294147024146032
epoch: 0, step: 30
	action: tensor([[ 0.0541,  0.0407,  0.0169, -0.0137, -0.0233, -0.0083, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32195754598615034, distance: 0.9422912032528629 entropy -13.294848899280694
epoch: 0, step: 31
	action: tensor([[ 0.0567, -0.0222,  0.0169, -0.0137, -0.0232, -0.0084,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[0.0540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27759530493030193, distance: 0.9726284598728323 entropy -13.293945221422971
epoch: 0, step: 32
	action: tensor([[ 0.0189, -0.0139,  0.0168, -0.0137, -0.0231, -0.0084,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[0.0524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2465721479714834, distance: 0.9932933516661617 entropy -13.293494737984018
epoch: 0, step: 33
	action: tensor([[ 0.0093,  0.0251,  0.0169, -0.0138, -0.0232, -0.0084,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2655876049859489, distance: 0.9806785862221742 entropy -13.2941817448328
epoch: 0, step: 34
	action: tensor([[ 0.0070,  0.0033,  0.0169, -0.0137, -0.0232, -0.0084,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[0.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24689109482939642, distance: 0.9930830851302741 entropy -13.294508134677107
epoch: 0, step: 35
	action: tensor([[-0.0161, -0.0054,  0.0169, -0.0137, -0.0232, -0.0084,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[0.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21673365301533953, distance: 1.0127713961797642 entropy -13.294418103806027
epoch: 0, step: 36
	action: tensor([[ 0.0059,  0.0129,  0.0169, -0.0137, -0.0232, -0.0083, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25129523614482474, distance: 0.990175078572243 entropy -13.29453888726099
epoch: 0, step: 37
	action: tensor([[ 0.0454,  0.0069,  0.0169, -0.0137, -0.0232, -0.0084,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[0.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28535899511964735, distance: 0.9673879181051542 entropy -13.294659252857297
epoch: 0, step: 38
	action: tensor([[ 0.0013, -0.0028,  0.0169, -0.0137, -0.0232, -0.0084,  0.0407]],
       dtype=torch.float64)
	q_value: tensor([[0.0533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2341392811263393, distance: 1.0014553495527556 entropy -13.29376015356752
epoch: 0, step: 39
	action: tensor([[-0.0282,  0.0035,  0.0169, -0.0137, -0.0232, -0.0083,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[0.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20782049427381566, distance: 1.0185174982161762 entropy -13.29421844330675
epoch: 0, step: 40
	action: tensor([[ 0.0691,  0.0250,  0.0169, -0.0137, -0.0233, -0.0083, -0.0253]],
       dtype=torch.float64)
	q_value: tensor([[0.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3177774837822108, distance: 0.945191304737203 entropy -13.294731648707415
epoch: 0, step: 41
	action: tensor([[ 0.0192, -0.0203,  0.0168, -0.0137, -0.0232, -0.0085, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[0.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2370149109324683, distance: 0.999573464812969 entropy -13.293987067488558
epoch: 0, step: 42
	action: tensor([[-0.0643,  0.0132,  0.0168, -0.0138, -0.0232, -0.0084,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[0.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1782317566639151, distance: 1.037364472808953 entropy -13.294349285378258
epoch: 0, step: 43
	action: tensor([[-0.0147,  0.0461,  0.0169, -0.0137, -0.0233, -0.0083,  0.0480]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25266788244355454, distance: 0.9892669878588266 entropy -13.295046554446333
epoch: 0, step: 44
	action: tensor([[ 7.8248e-02,  9.1647e-05,  1.6945e-02, -1.3662e-02, -2.3261e-02,
         -8.3702e-03, -3.2432e-02]], dtype=torch.float64)
	q_value: tensor([[0.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308443386007781, distance: 0.9516353426987972 entropy -13.294227116347585
epoch: 0, step: 45
	action: tensor([[ 0.0607,  0.0002,  0.0168, -0.0137, -0.0231, -0.0085, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[0.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29300936426225666, distance: 0.9621959487763301 entropy -13.293986007484637
epoch: 0, step: 46
	action: tensor([[-0.0069, -0.0162,  0.0168, -0.0137, -0.0231, -0.0084,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[0.0529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.215218836617055, distance: 1.013750259680937 entropy -13.293868274977234
epoch: 0, step: 47
	action: tensor([[-0.0306, -0.0215,  0.0169, -0.0138, -0.0232, -0.0083, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[0.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18702005286900403, distance: 1.0318025812398508 entropy -13.294509622346615
epoch: 0, step: 48
	action: tensor([[ 0.0793, -0.0169,  0.0169, -0.0138, -0.0232, -0.0083,  0.0586]],
       dtype=torch.float64)
	q_value: tensor([[0.0512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984134871358137, distance: 0.9585114587075484 entropy -13.294754608447509
epoch: 0, step: 49
	action: tensor([[ 0.0240,  0.0092,  0.0168, -0.0137, -0.0231, -0.0084,  0.0561]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26359689877693826, distance: 0.982006805769603 entropy -13.293021415782471
epoch: 0, step: 50
	action: tensor([[ 0.0542,  0.0187,  0.0169, -0.0137, -0.0232, -0.0084,  0.0488]],
       dtype=torch.float64)
	q_value: tensor([[0.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991675855278877, distance: 0.9579961934700361 entropy -13.293833602813164
epoch: 0, step: 51
	action: tensor([[ 0.0133, -0.0048,  0.0169, -0.0137, -0.0232, -0.0084,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[0.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24038218529259203, distance: 0.9973653221051508 entropy -13.29354429297638
epoch: 0, step: 52
	action: tensor([[-0.0098, -0.0162,  0.0169, -0.0137, -0.0232, -0.0084,  0.0420]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20841907052275022, distance: 1.0181326261079182 entropy -13.29438635388289
epoch: 0, step: 53
	action: tensor([[ 0.0713,  0.0178,  0.0169, -0.0137, -0.0232, -0.0083,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[0.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31552971869222357, distance: 0.9467471174338311 entropy -13.294249018496183
epoch: 0, step: 54
	action: tensor([[ 0.0698, -0.0316,  0.0169, -0.0137, -0.0231, -0.0084, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[0.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27631378767017434, distance: 0.9734907797563012 entropy -13.293515853983584
epoch: 0, step: 55
	action: tensor([[ 0.0653,  0.0280,  0.0168, -0.0138, -0.0231, -0.0084,  0.0002]],
       dtype=torch.float64)
	q_value: tensor([[0.0520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31669640560044765, distance: 0.9459399030217835 entropy -13.293619222763763
epoch: 0, step: 56
	action: tensor([[ 0.0921,  0.0165,  0.0169, -0.0137, -0.0232, -0.0084,  0.0247]],
       dtype=torch.float64)
	q_value: tensor([[0.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3355007128165247, distance: 0.9328331109152148 entropy -13.293801816079096
epoch: 0, step: 57
	action: tensor([[ 0.0429,  0.0142,  0.0168, -0.0137, -0.0231, -0.0085,  0.0254]],
       dtype=torch.float64)
	q_value: tensor([[0.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2867884350328933, distance: 0.9664199390271629 entropy -13.293190840204147
epoch: 0, step: 58
	action: tensor([[-0.0037,  0.0076,  0.0169, -0.0137, -0.0232, -0.0084,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[0.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2354181503828372, distance: 1.0006188622325682 entropy -13.29384616735939
epoch: 0, step: 59
	action: tensor([[ 0.0312,  0.0171,  0.0169, -0.0137, -0.0232, -0.0084, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[0.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.275988087863659, distance: 0.9737098181232378 entropy -13.294595260600017
epoch: 0, step: 60
	action: tensor([[ 0.0658,  0.0160,  0.0169, -0.0137, -0.0232, -0.0084,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30863897223971504, distance: 0.9515007622978183 entropy -13.294415715781813
epoch: 0, step: 61
	action: tensor([[ 0.0748,  0.0049,  0.0169, -0.0137, -0.0232, -0.0084,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3092327715101344, distance: 0.951092059893846 entropy -13.29357671219332
epoch: 0, step: 62
	action: tensor([[ 0.0253, -0.0277,  0.0168, -0.0137, -0.0231, -0.0084,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[0.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23707446851154967, distance: 0.9995344513785477 entropy -13.293522510714775
epoch: 0, step: 63
	action: tensor([[ 0.0510,  0.0271,  0.0168, -0.0138, -0.0232, -0.0083, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.0519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30163238109870427, distance: 0.9563100952676412 entropy -13.293888154431672
epoch: 0, step: 64
	action: tensor([[-0.0783, -0.0212,  0.0169, -0.0137, -0.0232, -0.0084,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[0.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1365486168735327, distance: 1.0633485373475844 entropy -13.294102974697186
epoch: 0, step: 65
	action: tensor([[ 0.0666,  0.0344,  0.0169, -0.0137, -0.0233, -0.0082, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[0.0510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32231589374711134, distance: 0.9420421682792272 entropy -13.294656428982405
epoch: 0, step: 66
	action: tensor([[-0.0419,  0.0045,  0.0168, -0.0137, -0.0232, -0.0085,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[0.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19393556627069874, distance: 1.0274047583403763 entropy -13.293830593279242
epoch: 0, step: 67
	action: tensor([[ 0.0056, -0.0044,  0.0169, -0.0137, -0.0233, -0.0083,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[0.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2341418742978546, distance: 1.0014536541089103 entropy -13.294692879369235
epoch: 0, step: 68
	action: tensor([[ 0.0315,  0.0130,  0.0169, -0.0137, -0.0232, -0.0083,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27152649812002916, distance: 0.9767053638320408 entropy -13.294368774002844
epoch: 0, step: 69
	action: tensor([[-0.0268,  0.0071,  0.0169, -0.0137, -0.0232, -0.0084, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20882593944068006, distance: 1.017870934758148 entropy -13.29411443574216
epoch: 0, step: 70
	action: tensor([[ 0.0667, -0.0030,  0.0169, -0.0137, -0.0233, -0.0083, -0.0099]],
       dtype=torch.float64)
	q_value: tensor([[0.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29479874050044486, distance: 0.9609775299772758 entropy -13.295148890221087
epoch: 0, step: 71
	action: tensor([[-0.0044, -0.0122,  0.0168, -0.0137, -0.0231, -0.0084,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[0.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2193581132318011, distance: 1.0110732455557654 entropy -13.293854286662691
epoch: 0, step: 72
	action: tensor([[-0.0218,  0.0245,  0.0169, -0.0137, -0.0232, -0.0083, -0.0540]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2279532809703091, distance: 1.005491686392117 entropy -13.294286564458114
epoch: 0, step: 73
	action: tensor([[ 0.0178,  0.0371,  0.0169, -0.0137, -0.0233, -0.0084, -0.0411]],
       dtype=torch.float64)
	q_value: tensor([[0.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27774633581713437, distance: 0.9725267823654191 entropy -13.2952853669614
epoch: 0, step: 74
	action: tensor([[-0.0037,  0.0066,  0.0169, -0.0137, -0.0232, -0.0084, -0.0266]],
       dtype=torch.float64)
	q_value: tensor([[0.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23544552385455142, distance: 1.0006009500513562 entropy -13.29471667551584
epoch: 0, step: 75
	action: tensor([[-0.0390, -0.0032,  0.0169, -0.0137, -0.0232, -0.0084,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[0.0524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19331250790115495, distance: 1.0278017548177751 entropy -13.294936097863925
epoch: 0, step: 76
	action: tensor([[ 0.0014,  0.0081,  0.0169, -0.0137, -0.0233, -0.0083, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[0.0520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24173135061333273, distance: 0.9964792128682356 entropy -13.294794694773126
epoch: 0, step: 77
	action: tensor([[ 0.1194,  0.0096,  0.0169, -0.0137, -0.0232, -0.0084,  0.0362]],
       dtype=torch.float64)
	q_value: tensor([[0.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35754076219668307, distance: 0.9172326009403601 entropy -13.294754545269141
epoch: 0, step: 78
	action: tensor([[ 0.0373,  0.0088,  0.0168, -0.0137, -0.0231, -0.0085, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[0.0539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2794151432393175, distance: 0.9714025938962482 entropy -13.292710693555373
epoch: 0, step: 79
	action: tensor([[-0.0796,  0.0014,  0.0169, -0.0137, -0.0232, -0.0084,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15666440472958465, distance: 1.0508891518595198 entropy -13.29427781620856
epoch: 0, step: 80
	action: tensor([[-0.0057, -0.0002,  0.0169, -0.0137, -0.0233, -0.0083, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[0.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23015882406767374, distance: 1.0040544408986187 entropy -13.29492027691107
epoch: 0, step: 81
	action: tensor([[ 0.0574,  0.0121,  0.0169, -0.0137, -0.0232, -0.0084, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30071953322728573, distance: 0.9569348955380087 entropy -13.29486857939615
epoch: 0, step: 82
	action: tensor([[ 0.0693, -0.0025,  0.0169, -0.0137, -0.0232, -0.0084, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30136843553744863, distance: 0.9564907951975776 entropy -13.294044390803796
epoch: 0, step: 83
	action: tensor([[ 0.0801,  0.0077,  0.0168, -0.0137, -0.0231, -0.0084,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31936496783208046, distance: 0.9440909672538851 entropy -13.293905686010087
epoch: 0, step: 84
	action: tensor([[ 0.1063,  0.0013,  0.0168, -0.0137, -0.0231, -0.0084,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[0.0533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33805377129079517, distance: 0.9310393772860819 entropy -13.29342038351
epoch: 0, step: 85
	action: tensor([[-0.0082, -0.0130,  0.0168, -0.0137, -0.0231, -0.0085,  0.0002]],
       dtype=torch.float64)
	q_value: tensor([[0.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21802968858271632, distance: 1.0119331556656834 entropy -13.293093193606376
epoch: 0, step: 86
	action: tensor([[ 0.0663,  0.0661,  0.0169, -0.0138, -0.0232, -0.0083,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[0.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34779175525135586, distance: 0.924165678097185 entropy -13.294597560064679
epoch: 0, step: 87
	action: tensor([[ 0.0620,  0.0020,  0.0169, -0.0136, -0.0232, -0.0084, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[0.0554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959927423260478, distance: 0.9601636523776507 entropy -13.29346049079317
epoch: 0, step: 88
	action: tensor([[-0.0132,  0.0459,  0.0169, -0.0137, -0.0231, -0.0084,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25505746127540674, distance: 0.9876841413036371 entropy -13.293810833042702
epoch: 0, step: 89
	action: tensor([[-0.0455,  0.0071,  0.0169, -0.0137, -0.0233, -0.0084,  0.0311]],
       dtype=torch.float64)
	q_value: tensor([[0.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1929276015189748, distance: 1.028046930477314 entropy -13.294496606314706
epoch: 0, step: 90
	action: tensor([[ 0.1050, -0.0021,  0.0169, -0.0137, -0.0233, -0.0083,  0.0464]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33486676748488176, distance: 0.9332779753650456 entropy -13.294675224895018
epoch: 0, step: 91
	action: tensor([[ 0.1132,  0.0055,  0.0168, -0.0137, -0.0231, -0.0085,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[0.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34817081615253254, distance: 0.9238970783111239 entropy -13.292819105951084
epoch: 0, step: 92
	action: tensor([[ 0.0477, -0.0241,  0.0168, -0.0137, -0.0231, -0.0085,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[0.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26425721102885213, distance: 0.9815664378875315 entropy -13.292863019850174
epoch: 0, step: 93
	action: tensor([[ 0.0275, -0.0084,  0.0168, -0.0137, -0.0231, -0.0084, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2556555779109845, distance: 0.9872875543782506 entropy -13.293621045120164
epoch: 0, step: 94
	action: tensor([[ 0.0103, -0.0250,  0.0168, -0.0137, -0.0232, -0.0084, -0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.227029976028839, distance: 1.0060927497865255 entropy -13.294512301971258
epoch: 0, step: 95
	action: tensor([[ 0.0385,  0.0144,  0.0168, -0.0138, -0.0232, -0.0083,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[0.0514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28500588482345857, distance: 0.9676268859548616 entropy -13.294722984549066
epoch: 0, step: 96
	action: tensor([[-0.0051, -0.0064,  0.0169, -0.0137, -0.0232, -0.0084, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[0.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22625250821336518, distance: 1.0065985961440032 entropy -13.293986116218367
epoch: 0, step: 97
	action: tensor([[ 0.0085,  0.0406,  0.0169, -0.0138, -0.0232, -0.0083,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[0.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27431785624529526, distance: 0.9748323024592584 entropy -13.29463580784336
epoch: 0, step: 98
	action: tensor([[ 0.0073, -0.0126,  0.0169, -0.0137, -0.0232, -0.0084,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[0.0543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2325089750479793, distance: 1.002520693907378 entropy -13.294257409778918
epoch: 0, step: 99
	action: tensor([[ 0.0263,  0.0302,  0.0169, -0.0138, -0.0232, -0.0083, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.0520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2825690490010755, distance: 0.9692744115366674 entropy -13.29444823489167
epoch: 0, step: 100
	action: tensor([[ 0.0926,  0.0239,  0.0169, -0.0137, -0.0232, -0.0084,  0.0445]],
       dtype=torch.float64)
	q_value: tensor([[0.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3418551238756673, distance: 0.928362192510495 entropy -13.294413395361127
epoch: 0, step: 101
	action: tensor([[ 0.0823,  0.0116,  0.0168, -0.0137, -0.0231, -0.0085,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[0.0543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3219045356328135, distance: 0.9423280373869638 entropy -13.293051771704071
epoch: 0, step: 102
	action: tensor([[ 0.0375,  0.0051,  0.0168, -0.0137, -0.0231, -0.0084,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2741124643930507, distance: 0.9749702474593976 entropy -13.293380106028481
epoch: 0, step: 103
	action: tensor([[ 0.0375,  0.0132,  0.0169, -0.0137, -0.0232, -0.0084,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27968880068937296, distance: 0.9712181209657381 entropy -13.294117584751403
epoch: 0, step: 104
	action: tensor([[-0.0122,  0.0311,  0.0169, -0.0137, -0.0232, -0.0084,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[0.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2433138216760805, distance: 0.9954388671354266 entropy -13.293974066393984
epoch: 0, step: 105
	action: tensor([[ 0.0188,  0.0015,  0.0169, -0.0137, -0.0233, -0.0084,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[0.0533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2520014197094431, distance: 0.9897079983816045 entropy -13.294703670745566
epoch: 0, step: 106
	action: tensor([[ 0.0072, -0.0007,  0.0169, -0.0137, -0.0232, -0.0084, -0.0058]],
       dtype=torch.float64)
	q_value: tensor([[0.0529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23776526234091566, distance: 0.9990818327603307 entropy -13.294168226288408
epoch: 0, step: 107
	action: tensor([[-0.0262, -0.0029,  0.0169, -0.0137, -0.0232, -0.0084, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[0.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20283281406189846, distance: 1.0217188360676808 entropy -13.294592791043822
epoch: 0, step: 108
	action: tensor([[-0.0021,  0.0059,  0.0169, -0.0137, -0.0233, -0.0083, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[0.0519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23352145050461826, distance: 1.0018592121819498 entropy -13.29485144291491
epoch: 0, step: 109
	action: tensor([[ 0.0417,  0.0176,  0.0169, -0.0137, -0.0232, -0.0084,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28504189955117476, distance: 0.9676025156440161 entropy -13.294693437100193
epoch: 0, step: 110
	action: tensor([[ 0.0177, -0.0070,  0.0169, -0.0137, -0.0232, -0.0084, -0.0255]],
       dtype=torch.float64)
	q_value: tensor([[0.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2423898069175504, distance: 0.9960464635224222 entropy -13.293833553975485
epoch: 0, step: 111
	action: tensor([[-0.0249,  0.0140,  0.0168, -0.0137, -0.0232, -0.0084, -0.0280]],
       dtype=torch.float64)
	q_value: tensor([[0.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2162346061001107, distance: 1.0130939811876172 entropy -13.294620963730196
epoch: 0, step: 112
	action: tensor([[-0.0203,  0.0283,  0.0169, -0.0137, -0.0233, -0.0084, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[0.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2321250626436676, distance: 1.0027714017323257 entropy -13.29514619135131
epoch: 0, step: 113
	action: tensor([[ 0.0501, -0.0017,  0.0169, -0.0137, -0.0233, -0.0084, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2789794721323373, distance: 0.9716962082356689 entropy -13.294925380371938
epoch: 0, step: 114
	action: tensor([[ 0.0222,  0.0071,  0.0169, -0.0137, -0.0231, -0.0084,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[0.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25870906218472534, distance: 0.9852604248559488 entropy -13.293994509885252
epoch: 0, step: 115
	action: tensor([[ 0.0184,  0.0173,  0.0169, -0.0137, -0.0232, -0.0084,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26155266061729876, distance: 0.9833688753681432 entropy -13.294187329199287
epoch: 0, step: 116
	action: tensor([[-0.0052,  0.0101,  0.0169, -0.0137, -0.0232, -0.0084, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[0.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23183010072496046, distance: 1.0029639793015976 entropy -13.294185123494989
epoch: 0, step: 117
	action: tensor([[ 0.0315, -0.0209,  0.0169, -0.0137, -0.0232, -0.0084,  0.0244]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24547469044916304, distance: 0.994016513533888 entropy -13.294809210517288
epoch: 0, step: 118
	action: tensor([[-0.0638,  0.0099,  0.0169, -0.0138, -0.0232, -0.0084,  0.0398]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.171958380479726, distance: 1.0413165631487746 entropy -13.293901136312627
epoch: 0, step: 119
	action: tensor([[ 0.0191, -0.0157,  0.0169, -0.0137, -0.0233, -0.0083,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23638269703119053, distance: 0.9999875052672923 entropy -13.294602394337318
epoch: 0, step: 120
	action: tensor([[ 0.0182,  0.0095,  0.0169, -0.0137, -0.0232, -0.0083, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[0.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25358457827386305, distance: 0.9886600721613584 entropy -13.2940603544787
epoch: 0, step: 121
	action: tensor([[ 0.0690,  0.0441,  0.0169, -0.0137, -0.0232, -0.0084,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[0.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33217369155894283, distance: 0.9351654539413807 entropy -13.294554254856811
epoch: 0, step: 122
	action: tensor([[ 0.0250,  0.0007,  0.0169, -0.0137, -0.0232, -0.0084,  0.0497]],
       dtype=torch.float64)
	q_value: tensor([[0.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2577809581818694, distance: 0.9858770100989473 entropy -13.293614703648453
epoch: 0, step: 123
	action: tensor([[ 0.0457,  0.0091,  0.0169, -0.0137, -0.0232, -0.0084,  0.0025]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2824700378075282, distance: 0.9693412930291186 entropy -13.293882001299949
epoch: 0, step: 124
	action: tensor([[ 0.0433,  0.0613,  0.0169, -0.0137, -0.0232, -0.0084,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31794785051432184, distance: 0.9450732793097139 entropy -13.294026733986417
epoch: 0, step: 125
	action: tensor([[ 0.0612,  0.0193,  0.0169, -0.0137, -0.0232, -0.0084, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[0.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059245206361404, distance: 0.9533668441661053 entropy -13.293910501695139
epoch: 0, step: 126
	action: tensor([[ 0.0105,  0.0053,  0.0169, -0.0137, -0.0232, -0.0084,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[0.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2465512380962317, distance: 0.9933071350005003 entropy -13.293862220745387
epoch: 0, step: 127
	action: tensor([[ 0.0520,  0.0177,  0.0169, -0.0137, -0.0232, -0.0084, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[0.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954361891432963, distance: 0.9605431062530186 entropy -13.294439319583677
LOSS epoch 0 actor 9.967743011604913 critic 35.31720676677715 entropy 0.01
epoch: 1, step: 0
	action: tensor([[-6.1091, -6.2800,  6.1249,  6.0725,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.1748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 1
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.8600]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 2
	action: tensor([[-6.1644, -6.2800,  5.2649,  6.1061,  5.5999, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 3
	action: tensor([[-3.6341, -6.2800,  6.1800,  4.7393,  6.1360, -6.2800,  5.4231]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 4
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1011,  4.7943, -6.2800,  5.0443]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 5
	action: tensor([[-6.2800, -6.0803,  6.1800,  6.1800,  3.4158, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 6
	action: tensor([[-5.8600, -6.2800,  5.7473,  6.1800,  6.1800, -6.2800,  6.0429]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 7
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.8518, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 8
	action: tensor([[-6.2800, -5.6556,  4.8295,  6.1800,  5.6186, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 9
	action: tensor([[-6.2800, -6.1070,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 10
	action: tensor([[-6.0665, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.5506]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 11
	action: tensor([[-4.7186, -3.2999,  6.1800,  5.4041,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 12
	action: tensor([[-6.1112, -6.2800,  6.1800,  6.1800,  4.6051, -6.2800,  5.6374]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 13
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.8422, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 14
	action: tensor([[-4.3271, -5.6681,  5.6166,  6.1800,  5.6204, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 15
	action: tensor([[-6.2800, -5.4802,  6.1800,  4.6283,  4.6141, -6.2800,  5.8122]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 16
	action: tensor([[-6.0675, -6.2800,  5.2683,  6.1800,  6.0459, -6.2800,  5.5610]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 17
	action: tensor([[-4.1849, -5.9732,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 18
	action: tensor([[-6.0914, -6.2800,  6.0803,  5.8077,  5.2429, -6.2800,  5.6307]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 19
	action: tensor([[-6.2800, -6.2800,  5.5492,  6.1800,  5.4320, -6.2800,  4.6801]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 20
	action: tensor([[-5.9744, -5.9154,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 21
	action: tensor([[-5.9055, -5.9580,  6.1800,  5.3595,  5.6606, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 22
	action: tensor([[-6.2800, -6.2800,  5.4998,  5.6943,  6.1800, -6.2800,  5.7547]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 23
	action: tensor([[-5.4389, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.6534]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 24
	action: tensor([[-5.5427, -6.2053,  6.1800,  6.1800,  4.6897, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7662294034339749, distance: 0.5532884901378374 entropy -1.8704687425011064
epoch: 1, step: 25
	action: tensor([[-4.7298, -6.2800,  6.1800,  5.6111,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.1466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 26
	action: tensor([[-5.7187, -6.1017,  5.9517,  4.7032,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 27
	action: tensor([[-5.6590, -5.9730,  6.1506,  6.1800,  6.1800, -6.2800,  5.9890]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 28
	action: tensor([[-5.7085, -6.2800,  6.1800,  6.1800,  4.8214, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 29
	action: tensor([[-5.9463, -5.4873,  6.1800,  5.9555,  5.8394, -6.2800,  5.9454]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 30
	action: tensor([[-4.0394, -6.2800,  5.0895,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 31
	action: tensor([[-6.2800, -6.2800,  4.0811,  6.1257,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3991120040720767, distance: 0.8870609419732363 entropy -1.8704687425011064
epoch: 1, step: 32
	action: tensor([[-6.2800, -4.9173,  6.1800,  5.7306,  5.5929, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.1304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 33
	action: tensor([[-5.2128, -3.6889,  4.8254,  6.1800,  5.5470, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 34
	action: tensor([[-5.7940, -6.2800,  6.1800,  5.6736,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 35
	action: tensor([[-6.2800, -6.2800,  5.5809,  5.7926,  4.6086, -6.2800,  5.7045]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29086907276619, distance: 0.9636512888145634 entropy -1.8704687425011064
epoch: 1, step: 36
	action: tensor([[-5.5998, -6.2800,  6.1800,  5.2271,  3.5627, -6.2800,  5.0686]],
       dtype=torch.float64)
	q_value: tensor([[0.1822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 37
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 38
	action: tensor([[-5.5005, -4.1014,  5.9979,  6.1800,  5.2919, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 39
	action: tensor([[-5.1768, -5.2960,  6.1800,  6.1800,  4.5926, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 40
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.6139,  6.1800, -6.2800,  5.3822]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 41
	action: tensor([[-6.2800, -5.8438,  5.1993,  6.1800,  5.6875, -6.2800,  6.0983]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 42
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.9631,  5.2361, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 43
	action: tensor([[-3.7435, -6.2800,  5.5317,  6.1800,  6.1800, -6.2800,  5.6896]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 44
	action: tensor([[-4.1674, -5.1237,  4.2402,  5.9671,  5.8595, -6.2800,  6.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 45
	action: tensor([[-5.2320, -6.2129,  6.1800,  4.9357,  6.1800, -6.2800,  4.7233]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 46
	action: tensor([[-5.1270, -6.0030,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 47
	action: tensor([[-6.2730, -5.7820,  6.1800,  4.8879,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 48
	action: tensor([[-6.2800, -6.1257,  5.7098,  6.1800,  6.1467, -6.2800,  4.6982]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 49
	action: tensor([[-6.2800, -5.5830,  6.1800,  5.5534,  4.4177, -6.2800,  5.5601]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 50
	action: tensor([[-6.2800, -6.2800,  4.5396,  5.3871,  5.4263, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 51
	action: tensor([[-5.1206, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6684084994535513, distance: 0.6589589914758872 entropy -1.8704687425011064
epoch: 1, step: 52
	action: tensor([[-6.2800, -5.3469,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.1435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 53
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.9334,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 54
	action: tensor([[-6.2800, -6.2260,  5.7642,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2345137346726882, distance: 1.001210498011035 entropy -1.8704687425011064
epoch: 1, step: 55
	action: tensor([[-4.3407, -6.2800,  6.1800,  5.4411,  5.3872, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.1633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 56
	action: tensor([[-6.2800, -5.1922,  6.1197,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 57
	action: tensor([[-5.7835, -6.2800,  5.4908,  4.6584,  5.6171, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 58
	action: tensor([[-6.2800, -6.2800,  5.1858,  4.1217,  6.1800, -6.2800,  5.0862]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 59
	action: tensor([[-6.2800, -5.3711,  6.1800,  6.1800,  5.8874, -6.2800,  5.6678]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 60
	action: tensor([[-6.2800, -4.0726,  6.1800,  4.3645,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 61
	action: tensor([[-5.4744, -6.2800,  5.9506,  5.3993,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 62
	action: tensor([[-6.2800, -5.6943,  6.1800,  6.1017,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 63
	action: tensor([[-6.2800, -5.7257,  4.6413,  4.5145,  5.6582, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 64
	action: tensor([[-5.9046, -5.8989,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 65
	action: tensor([[-5.6529, -6.2800,  6.1800,  5.4244,  4.9836, -6.2800,  5.7218]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 66
	action: tensor([[-6.2800, -5.9253,  5.9489,  6.1800,  3.2602, -6.2800,  6.0030]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 67
	action: tensor([[-6.2800, -5.4868,  6.1800,  5.8696,  5.4275, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 68
	action: tensor([[-6.2800, -6.2800,  5.2063,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 69
	action: tensor([[-6.2800, -6.1469,  4.7472,  4.9393,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 70
	action: tensor([[-6.1049, -6.2800,  6.1379,  6.1800,  5.2716, -6.2800,  5.1774]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 71
	action: tensor([[-6.2800, -5.9326,  5.7379,  6.0947,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 72
	action: tensor([[-5.6668, -6.1145,  6.1800,  6.0945,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 73
	action: tensor([[-5.9113, -6.2800,  6.1800,  5.5534,  6.1173, -6.2800,  6.0560]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 74
	action: tensor([[-5.3899, -6.2800,  6.1800,  4.1545,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 75
	action: tensor([[-5.8418, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.9064]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 76
	action: tensor([[-5.4064, -4.1038,  6.1800,  6.1800,  5.5816, -6.2800,  5.0809]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 77
	action: tensor([[-5.7053, -5.0820,  5.6634,  6.1800,  5.8364, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 78
	action: tensor([[-6.1405, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 79
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.7511,  5.4017, -6.2800,  5.3566]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 80
	action: tensor([[-4.3452, -6.2800,  3.6934,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 81
	action: tensor([[-6.2800, -5.6955,  4.9471,  4.7777,  5.7484, -6.2800,  5.1149]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 82
	action: tensor([[-6.2800, -4.9916,  6.0233,  6.1800,  5.9016, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 83
	action: tensor([[-6.2800, -5.1962,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 84
	action: tensor([[-6.2800, -6.2800,  6.1126,  5.8771,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 85
	action: tensor([[-6.2800, -5.8093,  6.1800,  5.9788,  4.8814, -6.2800,  6.0258]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 86
	action: tensor([[-6.0848, -6.2800,  5.6319,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 87
	action: tensor([[-6.1670, -6.2800,  5.3611,  5.1328,  6.1437, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 88
	action: tensor([[-6.2219, -5.6132,  5.9893,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 89
	action: tensor([[-5.5820, -5.0986,  6.0842,  6.1800,  5.0188, -6.2800,  5.8561]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 90
	action: tensor([[-6.2790, -6.2098,  5.6189,  5.6275,  5.0765, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 91
	action: tensor([[-6.2800, -6.1947,  5.0920,  6.1800,  6.1800, -6.2800,  4.1872]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 92
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.9872,  4.4177, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 93
	action: tensor([[-6.0031, -6.2800,  5.4655,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 94
	action: tensor([[-6.2800, -6.2800,  5.1649,  4.6956,  6.1160, -6.2800,  4.7483]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 95
	action: tensor([[-5.4367, -3.7859,  6.1800,  4.5739,  6.1800, -6.2800,  5.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 96
	action: tensor([[-6.2800, -5.0394,  5.1986,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 97
	action: tensor([[-6.2800, -5.6393,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 98
	action: tensor([[-6.2800, -4.6934,  6.1576,  5.7563,  6.1800, -6.2800,  4.9093]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 99
	action: tensor([[-6.1034, -5.4711,  6.1800,  6.1800,  5.1293, -6.2800,  5.0390]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 100
	action: tensor([[-5.9392, -5.1719,  5.5109,  5.9369,  5.9760, -6.2800,  5.7763]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 101
	action: tensor([[-6.2800, -6.1668,  6.1800,  6.1800,  6.1100, -6.2800,  5.4222]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 102
	action: tensor([[-6.2800, -4.8293,  5.3655,  6.1800,  5.7789, -6.2800,  4.4442]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 103
	action: tensor([[-6.2800, -5.6410,  4.6514,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 104
	action: tensor([[-6.2800, -5.6696,  5.8319,  3.5245,  5.2346, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 105
	action: tensor([[-6.2800, -5.1004,  6.1800,  6.0277,  6.1800, -6.2800,  5.4288]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 106
	action: tensor([[-4.9427, -5.4734,  6.1800,  6.1800,  6.1800, -6.2800,  5.8270]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 107
	action: tensor([[-5.7826, -6.2800,  4.9384,  6.1302,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7396260954113967, distance: 0.5839227839187661 entropy -1.8704687425011064
epoch: 1, step: 108
	action: tensor([[-6.2800, -5.7995,  5.3138,  5.5688,  6.1800, -6.2800,  5.1209]],
       dtype=torch.float64)
	q_value: tensor([[0.1129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 109
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.0498,  6.1800, -6.2800,  5.3685]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 110
	action: tensor([[-6.2800, -5.5312,  6.1800,  6.1800,  4.6268, -6.2800,  5.7111]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 111
	action: tensor([[-5.6561, -6.2800,  6.1800,  5.5396,  4.5703, -6.2800,  5.3900]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 112
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.0434,  4.8874, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 113
	action: tensor([[-6.2328, -6.2800,  5.4772,  6.0460,  6.1251, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 114
	action: tensor([[-6.2800, -6.2538,  6.1800,  5.0344,  5.6244, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 115
	action: tensor([[-5.4751, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1592]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 116
	action: tensor([[-5.8366, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 117
	action: tensor([[-6.0361, -6.2800,  6.1800,  4.8112,  5.9085, -6.2800,  4.8308]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 118
	action: tensor([[-5.8271, -6.2786,  6.1800,  6.1800,  5.9219, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 119
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1687, -6.2800,  4.1081]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1610486675915106, distance: 1.0481539548449503 entropy -1.8704687425011064
epoch: 1, step: 120
	action: tensor([[-6.2800, -5.1018,  5.9941,  6.1800,  6.1433, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 121
	action: tensor([[-5.9697, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.0889]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 122
	action: tensor([[-5.7071, -5.0949,  5.9385,  3.6680,  4.5376, -6.2800,  5.4564]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 123
	action: tensor([[-4.5881, -4.8210,  5.9092,  6.1800,  6.1232, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 124
	action: tensor([[-6.2800, -5.3629,  5.7101,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 125
	action: tensor([[-6.2800, -6.2800,  5.2474,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 126
	action: tensor([[-6.0697, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 1, step: 127
	action: tensor([[-5.1438, -6.2800,  6.1800,  6.1800,  5.8224, -6.2800,  6.0972]],
       dtype=torch.float64)
	q_value: tensor([[0.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 1 actor 1381.2529805977629 critic 2508.273405963845 entropy 100
epoch: 2, step: 0
	action: tensor([[-6.2800, -5.8005,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 1
	action: tensor([[-6.1877, -6.2800,  5.8342,  6.1800,  6.1800, -6.2800,  6.1536]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 2
	action: tensor([[-5.4766, -6.2800,  6.1342,  5.2265,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 3
	action: tensor([[-5.8456, -5.9774,  4.7134,  5.8927,  6.1800, -6.2800,  5.2367]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 4
	action: tensor([[-6.2800, -5.6029,  5.8220,  6.1800,  4.2759, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 5
	action: tensor([[-6.2800, -6.2800,  5.6067,  6.1800,  5.2976, -6.2800,  5.6911]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 6
	action: tensor([[-6.2800, -6.0203,  6.1800,  6.1800,  5.4023, -6.2800,  6.1166]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 7
	action: tensor([[-5.5733, -5.4930,  6.1800,  5.1976,  4.9420, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 8
	action: tensor([[-6.2800, -5.4410,  5.2513,  6.1800,  6.1800, -6.2800,  5.8386]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 9
	action: tensor([[-4.8634, -6.2800,  6.1800,  6.1800,  5.5792, -6.2800,  6.0607]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5308758949139065, distance: 0.7837915331708327 entropy -1.8704687425011064
epoch: 2, step: 10
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1113, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.0078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 11
	action: tensor([[-6.2800, -5.4977,  5.5812,  6.1800,  5.6912, -6.2800,  5.2555]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 12
	action: tensor([[-6.2800, -6.0455,  5.2308,  6.1800,  5.3948, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 13
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.0469,  6.1800, -6.2800,  5.8257]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 14
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.9280,  6.0735, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 15
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.9603,  4.3102, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 16
	action: tensor([[-6.2800, -6.2800,  5.4484,  4.5389,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 17
	action: tensor([[-6.1181, -6.2800,  6.1800,  4.8045,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 18
	action: tensor([[-5.1196, -6.2800,  6.1800,  6.1800,  5.8191, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 19
	action: tensor([[-6.2444, -6.2800,  6.1800,  6.1800,  6.1329, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30059264262225904, distance: 0.9570217137359124 entropy -1.8704687425011064
epoch: 2, step: 20
	action: tensor([[-6.2800, -6.2800,  5.7452,  5.6721,  6.0238, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 21
	action: tensor([[-6.2800, -4.4842,  5.1777,  5.6649,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 22
	action: tensor([[-4.8170, -6.2800,  6.1589,  6.1800,  6.1800, -6.2800,  4.1214]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5229972180346486, distance: 0.7903457990860222 entropy -1.8704687425011064
epoch: 2, step: 23
	action: tensor([[-6.2800, -6.2800,  5.6772,  6.1800,  4.9683, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 24
	action: tensor([[-6.2800, -6.2769,  6.0311,  5.7713,  5.1735, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 25
	action: tensor([[-6.2800, -5.7896,  4.2870,  6.1800,  5.5920, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 26
	action: tensor([[-6.0338, -6.1795,  6.1800,  5.8798,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 27
	action: tensor([[-5.3548, -6.2800,  6.1800,  6.1800,  5.2431, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 28
	action: tensor([[-4.4926, -4.6294,  5.9015,  4.2330,  5.1389, -6.2800,  5.1660]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 29
	action: tensor([[-6.2800, -5.2651,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 30
	action: tensor([[-6.2800, -5.6541,  6.1800,  6.1800,  5.6268, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 31
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.9502,  6.1092, -6.2800,  5.7913]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 32
	action: tensor([[-5.8084, -5.6385,  6.1800,  6.1800,  6.1800, -6.2800,  4.7601]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 33
	action: tensor([[-5.9584, -5.6770,  6.1800,  5.3336,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 34
	action: tensor([[-5.1447, -6.1180,  6.1800,  6.1800,  6.0006, -6.2800,  5.9830]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 35
	action: tensor([[-6.2800, -5.2458,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 36
	action: tensor([[-6.2800, -5.9801,  4.8488,  6.1800,  5.5533, -6.2800,  4.9601]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 37
	action: tensor([[-6.2800, -6.2545,  6.1800,  4.7903,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 38
	action: tensor([[-6.2800, -6.2653,  5.9583,  6.1800,  4.7711, -6.2800,  5.0373]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 39
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.6325,  6.1204, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 40
	action: tensor([[-6.2800, -5.9240,  5.6476,  6.1800,  6.1800, -6.2800,  4.8475]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 41
	action: tensor([[-6.2800, -4.7975,  6.1800,  6.1715,  3.6475, -6.2800,  5.6918]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 42
	action: tensor([[-6.2800, -6.2800,  5.5545,  6.1800,  5.5892, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 43
	action: tensor([[-5.1114, -6.2757,  6.1800,  4.5770,  5.5781, -6.2800,  5.2917]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 44
	action: tensor([[-4.9811, -6.2800,  5.9360,  6.1800,  3.7182, -6.2800,  5.1206]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 45
	action: tensor([[-6.2800, -6.2800,  5.8908,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2839625413658232, distance: 0.968332625435333 entropy -1.8704687425011064
epoch: 2, step: 46
	action: tensor([[-4.8555, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5230768188835961, distance: 0.7902798510204988 entropy -1.8704687425011064
epoch: 2, step: 47
	action: tensor([[-6.2800, -4.7390,  5.3811,  5.3295,  6.1800, -6.2800,  5.8406]],
       dtype=torch.float64)
	q_value: tensor([[0.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 48
	action: tensor([[-5.2302, -6.2800,  6.1800,  6.1160,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 49
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.6871,  6.1800, -6.2800,  5.2421]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 50
	action: tensor([[-6.2800, -6.2800,  6.0341,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 51
	action: tensor([[-5.4444, -5.6328,  5.6144,  5.9978,  5.3885, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 52
	action: tensor([[-6.2800, -4.8980,  5.8670,  6.1800,  4.8806, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 53
	action: tensor([[-5.3719, -4.6400,  6.1800,  6.1800,  5.6675, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 54
	action: tensor([[-6.2800, -6.2800,  5.0934,  6.1800,  6.1800, -6.2800,  5.9497]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 55
	action: tensor([[-3.8133, -6.2800,  6.1800,  6.1800,  4.8337, -6.2800,  5.5494]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 56
	action: tensor([[-6.0996, -5.4891,  6.1800,  4.3920,  5.4178, -6.2800,  5.3291]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 57
	action: tensor([[-6.1717, -5.5779,  5.5037,  5.9284,  5.2868, -6.2800,  5.5393]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 58
	action: tensor([[-6.2800, -5.4894,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 59
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.8951,  5.7582, -6.2800,  5.8514]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 60
	action: tensor([[-6.2800, -6.2800,  5.4149,  5.7828,  5.3161, -6.2800,  5.2680]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 61
	action: tensor([[-5.0149, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5973188491374971, distance: 0.7261683960728313 entropy -1.8704687425011064
epoch: 2, step: 62
	action: tensor([[-5.8840, -6.2800,  4.2989,  6.1800,  5.6082, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.689220058463558, distance: 0.6379449201913525 entropy -1.8704687425011064
epoch: 2, step: 63
	action: tensor([[-5.6429, -6.2800,  5.1520,  6.1800,  6.1800, -6.2800,  5.5524]],
       dtype=torch.float64)
	q_value: tensor([[-0.0546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.739553078294475, distance: 0.5840046534316222 entropy -1.8704687425011064
epoch: 2, step: 64
	action: tensor([[-6.2800, -4.6842,  6.1800,  5.4112,  5.5905, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 65
	action: tensor([[-6.2800, -4.8139,  6.1800,  6.1800,  5.1981, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 66
	action: tensor([[-5.6668, -4.3241,  6.1800,  6.1800,  5.1781, -6.2800,  4.3143]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 67
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.4129]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 68
	action: tensor([[-5.2638, -4.2005,  6.1800,  6.1626,  4.6533, -6.2800,  5.9269]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 69
	action: tensor([[-6.2800, -5.6553,  6.1800,  6.1800,  5.6557, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 70
	action: tensor([[-6.0177, -5.6820,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 71
	action: tensor([[-4.3574, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 72
	action: tensor([[-6.2800, -5.6043,  5.5572,  6.1800,  5.7023, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 73
	action: tensor([[-6.2800, -6.2800,  5.7515,  6.1800,  6.1800, -6.2800,  5.5878]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 74
	action: tensor([[-6.2800, -4.5560,  5.8455,  6.1800,  6.1800, -6.2800,  5.1103]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 75
	action: tensor([[-6.2800, -6.2800,  6.1373,  6.1800,  5.1388, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 76
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0218,  5.0213, -6.2800,  3.3103]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 77
	action: tensor([[-6.2800, -6.2800,  4.6486,  5.6850,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6022290814225753, distance: 0.721727423306952 entropy -1.8704687425011064
epoch: 2, step: 78
	action: tensor([[-6.2800, -6.2800,  4.5000,  5.2376,  6.1800, -6.2800,  6.0876]],
       dtype=torch.float64)
	q_value: tensor([[-0.0206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 79
	action: tensor([[-6.2800, -5.9314,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 80
	action: tensor([[-5.3960, -6.1868,  6.1800,  5.7526,  5.5939, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 81
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.9807,  5.6420, -6.2800,  5.2156]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 82
	action: tensor([[-6.2800, -4.7821,  5.8853,  5.9175,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 83
	action: tensor([[-5.1474, -5.8253,  5.8842,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9042954272050571, distance: 0.35401611987565235 entropy -1.8704687425011064
epoch: 2, step: 84
	action: tensor([[-4.5982, -5.5734,  5.7720,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 85
	action: tensor([[-5.4725, -4.1195,  6.0562,  4.6253,  5.0147, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 86
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1171,  6.1800, -6.2800,  4.4248]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 87
	action: tensor([[-6.2800, -6.0105,  4.8084,  4.9031,  5.9051, -6.2800,  5.8444]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 88
	action: tensor([[-6.2800, -5.1059,  5.0706,  6.0882,  5.3716, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 89
	action: tensor([[-6.0089, -6.2800,  5.0908,  3.8867,  6.0959, -6.2800,  5.2414]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 90
	action: tensor([[-6.2800, -5.6032,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 91
	action: tensor([[-5.8675, -6.0338,  5.8121,  6.1800,  5.2469, -6.2800,  6.0620]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 92
	action: tensor([[-5.1892, -4.4337,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 93
	action: tensor([[-6.2800, -6.0189,  6.1800,  6.1180,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 94
	action: tensor([[-5.8371, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 95
	action: tensor([[-5.6665, -6.2800,  6.1800,  6.1800,  4.7456, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 96
	action: tensor([[-6.2800, -6.2800,  4.7301,  6.1800,  5.6539, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 97
	action: tensor([[-6.2800, -5.8567,  6.1800,  6.1800,  4.6198, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 98
	action: tensor([[-6.2123, -5.6764,  4.8842,  5.0508,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 99
	action: tensor([[-5.7037, -6.2800,  5.9531,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 100
	action: tensor([[-5.0844, -6.2800,  6.1800,  4.1718,  6.1800, -6.2800,  5.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 101
	action: tensor([[-5.5406, -5.3888,  6.1800,  5.5298,  5.5282, -6.2800,  4.7544]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 102
	action: tensor([[-4.7885, -6.2800,  5.8529,  6.1800,  5.6554, -6.2800,  5.9542]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 103
	action: tensor([[-6.2800, -5.2388,  4.7068,  5.8250,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 104
	action: tensor([[-6.2800, -4.5133,  6.1800,  6.1800,  6.1800, -6.2800,  5.4895]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 105
	action: tensor([[-4.4950, -6.2800,  5.5601,  6.1800,  5.7939, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 106
	action: tensor([[-6.2728, -5.0985,  4.8028,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 107
	action: tensor([[-6.2800, -6.2800,  5.4464,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 108
	action: tensor([[-6.2800, -5.2547,  5.8177,  6.1800,  5.1911, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 109
	action: tensor([[-6.0965, -6.2800,  6.1800,  5.0001,  5.5357, -6.2800,  5.5543]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 110
	action: tensor([[-5.2715, -6.2800,  5.3836,  6.1800,  6.1800, -6.2800,  5.8001]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 111
	action: tensor([[-5.7741, -6.2800,  5.9433,  6.1800,  5.2110, -6.2800,  5.7420]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 112
	action: tensor([[-5.9521, -6.2800,  6.1800,  6.0841,  5.7539, -6.2800,  6.0900]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 113
	action: tensor([[-6.0044, -5.4099,  5.1662,  4.5979,  4.7396, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 114
	action: tensor([[-5.4507, -5.8225,  5.4993,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 115
	action: tensor([[-6.2800, -4.6193,  6.0665,  6.1800,  6.1782, -6.2800,  4.9276]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 116
	action: tensor([[-6.2800, -5.9612,  6.1800,  6.1800,  5.0417, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 117
	action: tensor([[-5.8158, -6.2800,  6.0723,  6.1800,  6.1800, -6.2800,  5.0888]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 118
	action: tensor([[-5.0350, -6.0737,  5.2661,  5.1926,  5.6574, -6.2800,  6.1683]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23573334403189994, distance: 1.0004125918237308 entropy -1.8704687425011064
epoch: 2, step: 119
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0105,  5.5709, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.0408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 120
	action: tensor([[-6.2800, -6.2800,  6.1772,  5.6439,  6.1800, -6.2800,  5.5709]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 121
	action: tensor([[-4.8986, -4.6188,  6.1800,  5.4899,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 122
	action: tensor([[-6.2800, -4.5972,  6.0343,  6.1800,  6.1800, -6.2800,  5.4992]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 123
	action: tensor([[-6.2800, -5.7568,  5.1541,  6.1800,  6.1800, -6.2800,  6.1046]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 124
	action: tensor([[-6.2800, -5.5139,  5.7777,  6.1800,  6.1800, -6.2800,  5.0894]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 125
	action: tensor([[-5.1543, -5.2208,  6.1800,  6.1800,  5.5782, -6.2800,  3.3465]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 126
	action: tensor([[-6.0011, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.9707]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 2, step: 127
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.3611]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 2 actor 1330.8506980552509 critic 2464.1651678186176 entropy 100
epoch: 3, step: 0
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.8429, -6.2800,  4.7747]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 1
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.7167,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 2
	action: tensor([[-6.2743, -6.0766,  6.1800,  6.1800,  6.1800, -6.2800,  5.5219]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 3
	action: tensor([[-5.1222, -5.1003,  6.1800,  6.1800,  5.2728, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 4
	action: tensor([[-6.2800, -5.6091,  4.4340,  6.1800,  5.8832, -6.2800,  5.1805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 5
	action: tensor([[-4.3429, -6.2800,  5.9377,  6.1800,  6.1800, -6.2800,  5.4393]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 6
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1145,  4.7145, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 7
	action: tensor([[-6.2800, -6.2800,  4.6254,  4.4396,  6.0040, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 8
	action: tensor([[-5.4244, -6.2800,  6.1800,  6.1800,  5.2412, -6.2800,  5.9301]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7274998548187416, distance: 0.5973653973042183 entropy -1.8704687425011064
epoch: 3, step: 9
	action: tensor([[-5.9476, -6.2800,  5.7145,  6.1800,  4.9438, -6.2800,  5.2531]],
       dtype=torch.float64)
	q_value: tensor([[-0.1677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 10
	action: tensor([[-6.2800, -6.2800,  3.6640,  6.1800,  6.1800, -6.2800,  4.9856]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26750272856467516, distance: 0.9793990961925114 entropy -1.8704687425011064
epoch: 3, step: 11
	action: tensor([[-5.7421, -6.0512,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 12
	action: tensor([[-5.4528, -6.2800,  6.1800,  6.0871,  6.0062, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 13
	action: tensor([[-6.2800, -6.2800,  5.3994,  6.1800,  6.1800, -6.2800,  5.6241]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 14
	action: tensor([[-5.9890, -6.2800,  5.6576,  6.1800,  6.1696, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 15
	action: tensor([[-6.0486, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 16
	action: tensor([[-4.3248, -5.4668,  6.1800,  6.1800,  5.2946, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 17
	action: tensor([[-5.4853, -6.2374,  5.6260,  5.7131,  6.1328, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 18
	action: tensor([[-4.7700, -6.2800,  6.1800,  4.9013,  6.1800, -6.2800,  4.5770]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 19
	action: tensor([[-5.4922, -6.2800,  6.1615,  5.3710,  6.0130, -6.2800,  5.9416]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 20
	action: tensor([[-6.0733, -3.1211,  6.1800,  6.1800,  4.7144, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 21
	action: tensor([[-5.1285, -6.2800,  6.1800,  6.1800,  6.1085, -6.2800,  6.1433]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 22
	action: tensor([[-6.2800, -4.4440,  5.2515,  6.1800,  4.5998, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 23
	action: tensor([[-6.2800, -4.8130,  6.1800,  6.1800,  5.8866, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 24
	action: tensor([[-6.1440, -5.3996,  6.1800,  5.9442,  5.4322, -6.2800,  5.4504]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 25
	action: tensor([[-6.0660, -5.9570,  5.7011,  6.1800,  5.8994, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 26
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.6820,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 27
	action: tensor([[-6.2800, -6.2800,  6.0711,  4.6446,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12310449410279434, distance: 1.2127374130820532 entropy -1.8704687425011064
epoch: 3, step: 28
	action: tensor([[-6.2800, -5.6556,  3.7887,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 29
	action: tensor([[-6.2648, -5.8961,  6.1289,  5.4180,  5.8324, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 30
	action: tensor([[-5.3524, -5.1204,  6.1800,  4.7463,  6.1800, -6.2800,  5.9127]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 31
	action: tensor([[-5.3398, -6.2800,  5.8425,  5.1219,  6.0941, -6.2800,  5.4577]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 32
	action: tensor([[-6.2532, -6.2800,  5.7749,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 33
	action: tensor([[-5.1427, -6.1258,  6.1800,  5.6140,  5.7592, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 34
	action: tensor([[-4.7703, -5.6242,  6.1800,  6.1800,  6.1800, -6.2800,  4.8339]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 35
	action: tensor([[-6.2800, -5.3047,  5.9213,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 36
	action: tensor([[-3.4670, -6.1562,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 37
	action: tensor([[-6.2800, -5.6796,  5.5159,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 38
	action: tensor([[-6.2800, -6.2800,  5.5590,  6.1800,  5.2139, -6.2800,  5.4019]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 39
	action: tensor([[-6.2275, -5.2851,  5.8481,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 40
	action: tensor([[-5.7081, -6.2800,  6.1033,  5.4161,  6.1777, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 41
	action: tensor([[-6.2800, -5.6880,  6.1800,  6.1800,  6.1689, -6.2800,  4.9455]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 42
	action: tensor([[-6.2800, -5.3511,  5.5345,  5.7027,  5.3184, -6.2800,  5.5901]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 43
	action: tensor([[-6.0999, -6.2800,  6.0988,  4.6289,  6.1800, -6.2800,  6.1565]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 44
	action: tensor([[-5.2716, -6.2800,  6.1800,  5.2919,  4.9415, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 45
	action: tensor([[-6.2800, -6.2800,  5.9255,  6.1800,  6.1800, -6.2800,  5.5543]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 46
	action: tensor([[-3.3685, -6.2800,  6.1800,  6.1800,  5.8504, -6.2800,  5.0571]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 47
	action: tensor([[-6.2800, -6.2800,  4.3416,  5.2573,  5.6803, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 48
	action: tensor([[-6.2800, -5.6853,  6.0087,  5.5734,  5.6960, -6.2800,  5.5230]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 49
	action: tensor([[-5.8320, -4.0565,  5.9926,  5.9636,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 50
	action: tensor([[-6.2800, -5.1513,  6.0014,  6.1800,  6.1800, -6.2800,  3.9914]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 51
	action: tensor([[-6.2800, -5.6071,  3.9467,  6.1800,  5.7682, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 52
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.8789]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 53
	action: tensor([[-5.9974, -5.9254,  6.1800,  6.1800,  6.1239, -6.2800,  5.9036]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 54
	action: tensor([[-6.2800, -6.2216,  6.1800,  6.0790,  5.1992, -6.2800,  6.1277]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 55
	action: tensor([[-6.2800, -6.2800,  5.3541,  5.3636,  5.8387, -6.2800,  5.3880]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 56
	action: tensor([[-5.7255, -6.2800,  6.1800,  6.1800,  5.6624, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 57
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.8068, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 58
	action: tensor([[-5.6398, -6.2800,  6.1590,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 59
	action: tensor([[-4.3298, -6.2800,  5.1599,  5.7656,  6.1800, -6.2800,  4.9664]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 60
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.3730]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 61
	action: tensor([[-6.2800, -6.2265,  4.1518,  5.6278,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 62
	action: tensor([[-6.2800, -5.2091,  4.9251,  5.6611,  5.0234, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 63
	action: tensor([[-6.2578, -5.0353,  6.1800,  5.8689,  5.0491, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 64
	action: tensor([[-4.0088, -6.2021,  6.1439,  6.1800,  5.8115, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 65
	action: tensor([[-5.6552, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6789466720389237, distance: 0.6484033967650754 entropy -1.8704687425011064
epoch: 3, step: 66
	action: tensor([[-6.2800, -5.9446,  6.1800,  6.1800,  5.0339, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 67
	action: tensor([[-6.2800, -5.8908,  5.5031,  4.9996,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 68
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.5416, -6.2800,  6.0275]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 69
	action: tensor([[-6.2800, -6.2800,  5.5319,  5.6841,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 70
	action: tensor([[-5.0071, -6.1559,  5.5781,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 71
	action: tensor([[-6.2800, -4.7427,  6.1800,  5.4425,  6.0401, -6.2800,  5.3802]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 72
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 73
	action: tensor([[-6.2800, -5.6107,  6.1800,  6.1800,  4.7644, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 74
	action: tensor([[-4.3655, -6.2800,  6.1800,  5.4016,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 75
	action: tensor([[-5.7212, -5.6848,  6.1800,  4.5818,  5.3419, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 76
	action: tensor([[-5.7174, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 77
	action: tensor([[-5.4938, -6.2800,  6.1800,  5.0207,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 78
	action: tensor([[-5.8777, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.2074]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5774654290560175, distance: 0.7438541960629743 entropy -1.8704687425011064
epoch: 3, step: 79
	action: tensor([[-6.2800, -5.3584,  6.1800,  6.1800,  6.1800, -6.2800,  6.1005]],
       dtype=torch.float64)
	q_value: tensor([[-0.1302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 80
	action: tensor([[-5.7711, -5.4607,  6.0852,  5.7347,  4.7982, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 81
	action: tensor([[-4.6192, -5.3438,  5.0789,  6.1800,  4.3741, -6.2800,  6.0368]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 82
	action: tensor([[-6.2800, -6.2800,  4.2564,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 83
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.6032, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 84
	action: tensor([[-6.2800, -6.1322,  5.7761,  6.1800,  6.1800, -6.2800,  4.2814]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 85
	action: tensor([[-4.6541, -4.1889,  5.5100,  6.1800,  6.1800, -6.2800,  5.2921]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 86
	action: tensor([[-6.2800, -6.2800,  6.0890,  5.9132,  6.0535, -6.2800,  5.7963]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 87
	action: tensor([[-6.2800, -6.0924,  6.1800,  6.1800,  5.6435, -6.2800,  5.0425]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 88
	action: tensor([[-4.0015, -6.2800,  6.1800,  6.1800,  5.5992, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 89
	action: tensor([[-6.2581, -6.2800,  6.1800,  6.1800,  5.6205, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 90
	action: tensor([[-5.8031, -6.2800,  6.1800,  6.1264,  6.1744, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 91
	action: tensor([[-6.2800, -6.2800,  5.3226,  5.0664,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 92
	action: tensor([[-5.3897, -5.8583,  6.1800,  5.8855,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 93
	action: tensor([[-5.7053, -6.2800,  5.8000,  4.9197,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 94
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 95
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.4240,  5.7804, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 96
	action: tensor([[-6.0535, -5.1562,  5.6757,  6.1800,  5.0929, -6.2800,  6.1563]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 97
	action: tensor([[-5.7229, -5.0555,  4.4307,  6.1800,  5.5542, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 98
	action: tensor([[-6.2800, -5.0063,  6.1800,  6.1800,  5.4377, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 99
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.1014, -6.2800,  5.8899]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 100
	action: tensor([[-5.3399, -5.0567,  5.7002,  5.0887,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 101
	action: tensor([[-4.9914, -6.2800,  5.6834,  4.4391,  5.7492, -6.2800,  4.8990]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 102
	action: tensor([[-6.2800, -4.1377,  6.1800,  6.1800,  6.1800, -6.2800,  6.0755]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 103
	action: tensor([[-6.2772, -5.1396,  5.6013,  5.5665,  5.6204, -6.2800,  5.4217]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 104
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.1592, -6.2800,  5.5225]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 105
	action: tensor([[-5.8095, -4.6074,  5.8711,  6.1800,  6.1800, -6.2800,  4.0735]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 106
	action: tensor([[-5.2463, -3.1343,  6.1800,  6.1800,  5.5152, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 107
	action: tensor([[-5.7934, -5.6997,  6.1800,  5.7858,  4.3906, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 108
	action: tensor([[-6.2731, -6.2800,  6.1800,  3.1059,  5.8113, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 109
	action: tensor([[-6.1140, -6.2800,  6.1800,  6.1800,  5.6356, -6.2800,  5.5708]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 110
	action: tensor([[-6.1977, -6.2800,  5.0182,  6.1800,  5.6744, -6.2800,  6.0943]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 111
	action: tensor([[-3.6952, -6.2800,  6.1800,  5.6647,  6.1800, -6.2800,  5.5882]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 112
	action: tensor([[-6.2800, -5.1893,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 113
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0028,  6.0087, -6.2800,  5.2936]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12179157572400845, distance: 1.0723967586970578 entropy -1.8704687425011064
epoch: 3, step: 114
	action: tensor([[-5.4928, -6.0963,  5.5240,  6.1800,  4.5181, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8388862545269012, distance: 0.45932807408066767 entropy -1.8704687425011064
epoch: 3, step: 115
	action: tensor([[-6.2800, -6.1094,  6.1800,  5.5065,  5.9521, -6.2800,  5.0188]],
       dtype=torch.float64)
	q_value: tensor([[-0.2711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14560106412808294, distance: 1.0577597633714801 entropy -1.8704687425011064
epoch: 3, step: 116
	action: tensor([[-6.2800, -6.2800,  4.1976,  4.9513,  5.4364, -6.2800,  5.1126]],
       dtype=torch.float64)
	q_value: tensor([[-0.1915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 117
	action: tensor([[-6.2800, -6.2800,  5.5437,  5.9354,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 118
	action: tensor([[-5.2924, -6.2800,  4.1359,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7618860452194481, distance: 0.55840476730574 entropy -1.8704687425011064
epoch: 3, step: 119
	action: tensor([[-5.3725, -5.6327,  5.1243,  4.5605,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.3410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 120
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.7574, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 121
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.0907,  6.1800, -6.2800,  5.3641]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 122
	action: tensor([[-6.2800, -6.2800,  6.0524,  6.1800,  5.2788, -6.2800,  5.2749]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 123
	action: tensor([[-6.2413, -6.2800,  5.8811,  6.1800,  5.1618, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 124
	action: tensor([[-6.2800, -4.7068,  6.1800,  5.5238,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 125
	action: tensor([[-6.2800, -6.2800,  4.3391,  4.6020,  4.4119, -6.2800,  5.3294]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 126
	action: tensor([[-6.2800, -6.2800,  6.1077,  6.1800,  4.1341, -6.2800,  5.9556]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 3, step: 127
	action: tensor([[-6.2800, -6.2800,  5.6955,  6.1800,  6.1800, -6.2800,  5.5840]],
       dtype=torch.float64)
	q_value: tensor([[-0.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 3 actor 1336.6803373937666 critic 2443.813207087185 entropy 100
epoch: 4, step: 0
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 1
	action: tensor([[-5.5476, -4.8504,  6.1800,  5.9133,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 2
	action: tensor([[-5.6697, -5.4175,  6.1800,  6.0120,  5.5433, -6.2800,  4.2643]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 3
	action: tensor([[-6.2800, -5.3206,  5.4397,  6.1800,  6.1800, -6.2800,  5.6791]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 4
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 5
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.9767,  5.1737, -6.2800,  5.5236]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 6
	action: tensor([[-6.2800, -6.2800,  6.1232,  6.1800,  5.9134, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 7
	action: tensor([[-4.9760, -6.2800,  6.0781,  6.1800,  5.4965, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5796537685414374, distance: 0.74192545610989 entropy -1.8704687425011064
epoch: 4, step: 8
	action: tensor([[-5.3074, -6.0006,  5.0385,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 9
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.6884, -6.2800,  5.9847]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 10
	action: tensor([[-6.0787, -6.2800,  5.0215,  6.1800,  5.2342, -6.2800,  6.0679]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 11
	action: tensor([[-5.5265, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 12
	action: tensor([[-6.2800, -6.2800,  5.7620,  6.1800,  5.7698, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27014651202625195, distance: 0.9776300385793414 entropy -1.8704687425011064
epoch: 4, step: 13
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.6062,  6.1800, -6.2800,  5.6876]],
       dtype=torch.float64)
	q_value: tensor([[-0.3480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 14
	action: tensor([[-6.2800, -5.8132,  5.1640,  6.1800,  5.5782, -6.2800,  4.6669]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 15
	action: tensor([[-5.6822, -6.2800,  5.1082,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7439965596827699, distance: 0.5790013736796105 entropy -1.8704687425011064
epoch: 4, step: 16
	action: tensor([[-5.0865, -6.2800,  5.7662,  6.1800,  5.7226, -6.2800,  5.4798]],
       dtype=torch.float64)
	q_value: tensor([[-0.5161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5907526006089885, distance: 0.7320650230640147 entropy -1.8704687425011064
epoch: 4, step: 17
	action: tensor([[-6.2800, -5.9972,  5.1677,  5.5554,  6.1800, -6.2800,  5.8612]],
       dtype=torch.float64)
	q_value: tensor([[-0.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 18
	action: tensor([[-5.7445, -6.2800,  5.9466,  5.5695,  6.1800, -6.2800,  5.2438]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 19
	action: tensor([[-5.9429, -5.7381,  6.1800,  6.1800,  6.1460, -6.2800,  5.7520]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 20
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 21
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0696,  6.1800, -6.2800,  4.4895]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 22
	action: tensor([[-6.2800, -6.0154,  6.1800,  5.1191,  6.1800, -6.2800,  5.3957]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 23
	action: tensor([[-5.0147, -5.9289,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 24
	action: tensor([[-6.2800, -3.8564,  5.1735,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 25
	action: tensor([[-5.8518, -6.2616,  6.1800,  5.4608,  5.6407, -6.2800,  5.1590]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 26
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1720, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 27
	action: tensor([[-5.2653, -5.5859,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 28
	action: tensor([[-6.2800, -3.1466,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 29
	action: tensor([[-6.2800, -5.7106,  6.1800,  6.1800,  6.1800, -6.2800,  5.9014]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 30
	action: tensor([[-5.8481, -6.2800,  6.1800,  6.1800,  5.8078, -6.2800,  6.1772]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 31
	action: tensor([[-5.7374, -6.2800,  5.5569,  6.1800,  5.0354, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 32
	action: tensor([[-6.2800, -5.6917,  4.8191,  5.6963,  6.1800, -6.2800,  5.1947]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 33
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.3952,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 34
	action: tensor([[-5.8387, -6.2800,  6.1800,  6.0347,  6.1800, -6.2800,  5.0641]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 35
	action: tensor([[-5.2362, -6.2800,  6.1800,  5.7788,  6.0077, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 36
	action: tensor([[-5.3777, -6.2800,  5.6370,  4.9708,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 37
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.0898, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 38
	action: tensor([[-6.2800, -6.2800,  5.6632,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 39
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.5460, -6.2800,  5.7501]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 40
	action: tensor([[-5.5673, -5.9769,  5.2342,  5.6819,  5.8182, -6.2800,  5.5653]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 41
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1640,  5.7311, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 42
	action: tensor([[-5.7093, -6.2800,  5.8518,  4.9240,  5.4378, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 43
	action: tensor([[-6.2800, -6.2800,  5.9738,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 44
	action: tensor([[-5.9181, -5.7840,  6.1800,  6.1800,  6.1800, -6.2800,  5.4487]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 45
	action: tensor([[-6.2800, -6.2800,  4.5915,  5.3753,  5.0120, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 46
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 47
	action: tensor([[-6.2800, -6.1816,  5.7623,  6.1800,  6.1800, -6.2800,  5.0169]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 48
	action: tensor([[-6.2800, -6.1217,  6.1800,  5.5563,  5.1893, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 49
	action: tensor([[-5.9388, -6.2800,  6.1788,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 50
	action: tensor([[-5.1189, -5.7840,  4.7800,  5.6878,  6.1800, -6.2800,  5.9201]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 51
	action: tensor([[-6.2800, -6.2800,  5.6388,  6.1800,  5.4045, -6.2800,  5.7230]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 52
	action: tensor([[-6.2800, -5.2939,  6.1800,  5.5294,  6.1800, -6.2800,  5.8990]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 53
	action: tensor([[-6.2800, -6.2040,  6.1800,  6.1800,  5.0974, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 54
	action: tensor([[-5.7741, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 55
	action: tensor([[-5.2056, -5.6261,  5.3403,  5.3417,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6680930521510433, distance: 0.659272355091778 entropy -1.8704687425011064
epoch: 4, step: 56
	action: tensor([[-4.9322, -5.8389,  6.1800,  6.1800,  6.1800, -6.2800,  6.1242]],
       dtype=torch.float64)
	q_value: tensor([[-0.6199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 57
	action: tensor([[-5.1638, -6.2800,  6.1800,  6.1800,  4.7967, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7074062345939072, distance: 0.618997973534334 entropy -1.8704687425011064
epoch: 4, step: 58
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.7977,  6.1800, -6.2800,  4.8577]],
       dtype=torch.float64)
	q_value: tensor([[-0.5599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 59
	action: tensor([[-6.2800, -6.2800,  3.8313,  4.7962,  6.1151, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 60
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.0974]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 61
	action: tensor([[-5.2540, -5.8156,  6.0011,  4.8357,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 62
	action: tensor([[-6.2800, -5.7857,  6.1800,  6.0670,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 63
	action: tensor([[-6.2800, -5.5772,  6.0510,  6.1800,  5.1435, -6.2800,  5.8648]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 64
	action: tensor([[-6.2800, -4.6238,  6.1800,  6.1800,  5.7217, -6.2800,  4.1336]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 65
	action: tensor([[-6.2800, -5.9110,  3.9015,  6.1800,  6.1573, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 66
	action: tensor([[-5.3248, -5.1357,  5.6852,  5.7410,  5.8709, -6.2800,  4.7065]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 67
	action: tensor([[-5.7360, -6.2800,  6.1800,  6.1800,  5.1498, -6.2800,  5.5796]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 68
	action: tensor([[-6.2800, -5.4396,  4.3649,  5.1398,  3.8489, -6.2800,  5.5501]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9755732836235922, distance: 0.17885012093529998 entropy -1.8704687425011064
epoch: 4, step: 69
	action: tensor([[-6.2800, -6.2784,  5.8374,  6.1800,  6.1800, -6.2800,  5.3181]],
       dtype=torch.float64)
	q_value: tensor([[-1.2462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 70
	action: tensor([[-4.2543, -6.0176,  5.8821,  6.0074,  6.1800, -6.2800,  3.4853]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 71
	action: tensor([[-5.8801, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.2926]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 72
	action: tensor([[-5.9207, -6.2800,  6.1800,  6.1800,  5.9036, -6.2800,  5.9315]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 73
	action: tensor([[-6.2800, -6.2800,  4.7975,  6.1800,  6.1800, -6.2800,  4.6677]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28750349994960644, distance: 0.9659353518960507 entropy -1.8704687425011064
epoch: 4, step: 74
	action: tensor([[-6.2800, -6.2800,  4.4372,  5.3975,  5.9412, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8048213299517185, distance: 0.5055601924101204 entropy -1.8704687425011064
epoch: 4, step: 75
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0139,  6.0513, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 76
	action: tensor([[-6.2800, -6.2800,  5.5714,  5.4490,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995375335502043, distance: 0.9577433116308228 entropy -1.8704687425011064
epoch: 4, step: 77
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0174,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 78
	action: tensor([[-5.6085, -6.2800,  4.9888,  6.1213,  5.2028, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 79
	action: tensor([[-6.2800, -5.7356,  4.6749,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8598232421929235, distance: 0.4284446248734928 entropy -1.8704687425011064
epoch: 4, step: 80
	action: tensor([[-5.6294, -6.2800,  6.1800,  6.1800,  5.7926, -6.2800,  4.5152]],
       dtype=torch.float64)
	q_value: tensor([[-0.6183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06472818430262983, distance: 1.1066890600443968 entropy -1.8704687425011064
epoch: 4, step: 81
	action: tensor([[-6.2800, -5.2152,  6.1800,  6.1800,  5.4971, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.6323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 82
	action: tensor([[-5.5729, -4.8093,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 83
	action: tensor([[-4.8631, -6.2800,  5.9714,  6.1800,  6.1800, -6.2800,  5.4860]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.502043445895032, distance: 0.8075183866982578 entropy -1.8704687425011064
epoch: 4, step: 84
	action: tensor([[-6.2800, -5.9046,  5.5952,  5.8116,  5.5118, -6.2800,  5.3527]],
       dtype=torch.float64)
	q_value: tensor([[-0.5260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 85
	action: tensor([[-4.9581, -6.1760,  6.1800,  6.1800,  6.1800, -6.2800,  5.6251]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6529024413150741, distance: 0.6741902581497599 entropy -1.8704687425011064
epoch: 4, step: 86
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.3094, -6.2800,  6.1541]],
       dtype=torch.float64)
	q_value: tensor([[-0.4571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 87
	action: tensor([[-5.1956, -6.2800,  6.1800,  5.1320,  6.1800, -6.2800,  5.2609]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 88
	action: tensor([[-6.2800, -4.9382,  6.1800,  4.9054,  6.1800, -6.2800,  5.2515]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 89
	action: tensor([[-6.2800, -5.4209,  5.2471,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 90
	action: tensor([[-5.3473, -6.2800,  6.1800,  4.7920,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 91
	action: tensor([[-5.9323, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.5507]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 92
	action: tensor([[-6.2800, -5.7020,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 93
	action: tensor([[-6.2800, -5.7613,  3.7462,  6.1800,  4.5903, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 94
	action: tensor([[-3.3921, -6.2800,  6.1800,  6.1107,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 95
	action: tensor([[-6.1600, -5.0645,  5.8162,  5.2984,  5.5282, -6.2800,  5.8105]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 96
	action: tensor([[-6.2800, -5.9081,  6.1800,  6.1800,  6.1800, -6.2800,  5.3967]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 97
	action: tensor([[-6.2800, -3.5082,  5.0376,  6.1800,  5.9314, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 98
	action: tensor([[-6.0498, -6.0690,  4.8669,  6.1800,  6.1800, -6.2800,  5.9203]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 99
	action: tensor([[-6.2800, -6.2132,  5.2043,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 100
	action: tensor([[-5.1164, -6.2800,  5.7822,  6.1800,  5.2029, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6469499205821867, distance: 0.6799466681917573 entropy -1.8704687425011064
epoch: 4, step: 101
	action: tensor([[-6.2800, -6.2800,  6.0977,  6.1800,  6.1800, -6.2800,  5.1285]],
       dtype=torch.float64)
	q_value: tensor([[-0.5615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 102
	action: tensor([[-4.8760, -5.5413,  5.4121,  6.1800,  5.9013, -6.2800,  3.5481]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 103
	action: tensor([[-5.9744, -5.7026,  6.1800,  5.0614,  6.1626, -6.2800,  6.1156]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 104
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0808,  6.1800, -6.2800,  5.9927]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 105
	action: tensor([[-6.2800, -5.8131,  5.9783,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 106
	action: tensor([[-6.2800, -4.9817,  6.1800,  5.1718,  3.8380, -6.2800,  5.1298]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 107
	action: tensor([[-6.2504, -4.9806,  5.4786,  5.9685,  5.9589, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 108
	action: tensor([[-6.2553, -6.0862,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 109
	action: tensor([[-5.9453, -4.1941,  5.3331,  5.3677,  6.1800, -6.2800,  5.9339]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 110
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.9082]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 111
	action: tensor([[-5.0951, -3.6852,  6.1154,  5.6232,  6.0123, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 112
	action: tensor([[-6.2800, -5.5172,  6.1800,  6.1800,  4.5026, -6.2800,  5.0758]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 113
	action: tensor([[-6.2800, -6.2800,  5.1638,  4.6309,  6.0476, -6.2800,  5.8254]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 114
	action: tensor([[-6.2800, -5.3575,  6.1800,  5.6327,  3.5381, -6.2800,  5.4687]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 115
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.6355,  6.1800, -6.2800,  6.0564]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 116
	action: tensor([[-5.9344, -6.2800,  6.1800,  6.1800,  5.8601, -6.2800,  5.9185]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 117
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.7332, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 118
	action: tensor([[-5.7851, -6.2800,  6.1800,  6.1800,  5.9143, -6.2800,  5.4029]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 119
	action: tensor([[-5.7651, -6.2800,  5.0783,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 120
	action: tensor([[-5.2723, -6.2800,  5.1311,  6.1800,  5.5684, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 121
	action: tensor([[-6.2800, -6.1951,  4.6512,  5.1454,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 122
	action: tensor([[-6.2800, -4.3919,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 123
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.7795, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 124
	action: tensor([[-4.2717, -6.2800,  5.4514,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 125
	action: tensor([[-5.7397, -6.1703,  4.6025,  6.1800,  4.8037, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8042923610820998, distance: 0.5062448078341083 entropy -1.8704687425011064
epoch: 4, step: 126
	action: tensor([[-6.2800, -5.9075,  4.9015,  5.2729,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 4, step: 127
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.9938,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 4 actor 1249.0845065646502 critic 2378.5636368527435 entropy 100
epoch: 5, step: 0
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.1419]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 1
	action: tensor([[-4.4762, -6.2800,  6.1800,  3.8732,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 2
	action: tensor([[-5.6853, -5.4941,  6.1800,  6.1800,  6.1800, -6.2800,  4.3447]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 3
	action: tensor([[-6.2800, -6.2800,  5.7537,  6.1800,  5.8494, -6.2800,  5.3984]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 4
	action: tensor([[-5.8752, -6.2800,  5.9553,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 5
	action: tensor([[-6.2442, -6.2800,  4.7959,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 6
	action: tensor([[-6.2800, -5.9606,  6.1800,  6.1800,  6.1800, -6.2800,  4.3702]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 7
	action: tensor([[-5.3047, -6.2800,  6.1800,  5.7216,  6.1800, -6.2800,  5.5741]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 8
	action: tensor([[-6.2298, -6.2800,  6.1800,  6.1800,  5.8566, -6.2800,  4.6935]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 9
	action: tensor([[-6.0174, -6.2800,  6.1800,  5.2410,  5.2676, -6.2800,  5.5570]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 10
	action: tensor([[-6.2800, -6.2800,  4.7456,  5.6813,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5840513501765177, distance: 0.7380343113534104 entropy -1.8704687425011064
epoch: 5, step: 11
	action: tensor([[-5.5022, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.3043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 12
	action: tensor([[-6.2800, -5.8046,  4.7431,  6.1800,  5.5972, -6.2800,  6.0839]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 13
	action: tensor([[-6.2800, -5.3577,  4.9494,  6.1800,  3.7673, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 14
	action: tensor([[-5.8624, -5.7056,  5.4102,  6.1800,  6.1800, -6.2800,  5.9053]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 15
	action: tensor([[-6.2800, -5.9452,  4.5466,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 16
	action: tensor([[-4.8446, -5.4519,  6.1800,  6.1800,  5.1540, -6.2800,  5.4461]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 17
	action: tensor([[-5.9221, -6.2800,  6.1800,  4.7529,  6.1800, -6.2800,  6.1436]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 18
	action: tensor([[-4.9735, -5.7558,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8374758400122447, distance: 0.4613342074608128 entropy -1.8704687425011064
epoch: 5, step: 19
	action: tensor([[-6.1500, -6.2800,  6.1800,  6.1800,  5.8981, -6.2800,  6.0302]],
       dtype=torch.float64)
	q_value: tensor([[-0.9291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 20
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.8789, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2747075008664166, distance: 0.9745705562062503 entropy -1.8704687425011064
epoch: 5, step: 21
	action: tensor([[-6.2800, -6.1578,  5.6446,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.6058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 22
	action: tensor([[-5.3717, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 23
	action: tensor([[-6.2800, -5.1549,  4.4669,  6.1800,  6.1800, -6.2800,  6.1265]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 24
	action: tensor([[-6.2800, -6.0148,  4.2568,  6.1800,  6.1063, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 25
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1349]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 26
	action: tensor([[-6.2800, -6.2800,  5.9865,  5.5517,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 27
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.2893, -6.2800,  5.5923]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 28
	action: tensor([[-4.7538, -6.2800,  6.1800,  5.7906,  6.1800, -6.2800,  5.7749]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 29
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0184,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 30
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.1572]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 31
	action: tensor([[-6.0792, -6.2464,  5.8830,  4.9934,  5.3229, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 32
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 33
	action: tensor([[-5.4319, -6.2800,  6.1800,  5.7807,  6.1392, -6.2800,  5.8916]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 34
	action: tensor([[-5.9251, -6.2800,  5.6189,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 35
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.2030,  5.1936, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 36
	action: tensor([[-6.2800, -5.1428,  6.1800,  6.1800,  5.9272, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 37
	action: tensor([[-5.9285, -6.1742,  6.1800,  6.0957,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 38
	action: tensor([[-4.4052, -6.2800,  6.1800,  6.0650,  5.3538, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 39
	action: tensor([[-6.2800, -6.0005,  6.1800,  6.1800,  5.1214, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 40
	action: tensor([[-6.2286, -6.2800,  6.1800,  5.5816,  6.1800, -6.2800,  4.9190]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 41
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.7698,  6.1800, -6.2800,  5.3168]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 42
	action: tensor([[-5.7655, -5.0141,  5.7949,  6.1800,  6.1800, -6.2800,  4.8086]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 43
	action: tensor([[-6.2800, -6.2800,  5.7380,  6.1800,  6.0499, -6.2800,  5.3138]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 44
	action: tensor([[-4.0518, -6.0367,  6.1800,  6.1800,  5.6904, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 45
	action: tensor([[-5.9015, -6.2800,  4.8405,  6.1800,  6.1800, -6.2800,  5.0954]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6545863168523016, distance: 0.672552919211209 entropy -1.8704687425011064
epoch: 5, step: 46
	action: tensor([[-6.2800, -5.2340,  4.6075,  5.7272,  5.9406, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.5705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 47
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.5847,  6.1800, -6.2800,  5.2440]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 48
	action: tensor([[-4.5169, -6.2800,  6.1800,  6.1655,  6.1800, -6.2800,  2.8941]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 49
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.3715]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 50
	action: tensor([[-3.8930, -6.2800,  5.5667,  6.1800,  4.4495, -6.2800,  4.6545]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 51
	action: tensor([[-6.2800, -5.1889,  5.4668,  5.3751,  6.1800, -6.2800,  5.5371]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 52
	action: tensor([[-6.2800, -5.7584,  6.1800,  3.4809,  6.1527, -6.2800,  5.6865]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 53
	action: tensor([[-5.8217, -5.5946,  5.4607,  4.8191,  6.1645, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 54
	action: tensor([[-5.2085, -6.2800,  5.9920,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 55
	action: tensor([[-6.2800, -5.2458,  6.1800,  5.0677,  6.1800, -6.2800,  5.1442]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 56
	action: tensor([[-6.2800, -5.9133,  6.1800,  5.0498,  3.5785, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 57
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.9067,  4.6263, -6.2800,  6.1758]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 58
	action: tensor([[-4.8348, -5.3198,  4.3221,  6.1800,  5.8386, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 59
	action: tensor([[-6.2800, -6.0458,  6.1800,  5.2208,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 60
	action: tensor([[-6.2800, -5.8003,  6.1100,  4.4149,  4.8740, -6.2800,  5.9224]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 61
	action: tensor([[-6.2800, -5.3944,  6.0099,  5.3382,  5.4473, -6.2800,  4.9183]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 62
	action: tensor([[-5.3986, -6.2800,  5.3327,  6.1800,  5.7942, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 63
	action: tensor([[-5.0976, -5.8010,  6.1800,  6.1800,  4.3225, -6.2800,  5.0176]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 64
	action: tensor([[-6.2800, -5.5106,  6.1522,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 65
	action: tensor([[-5.1712, -6.2800,  6.1800,  5.9679,  6.1800, -6.2800,  4.4953]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 66
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0889,  4.1778, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 67
	action: tensor([[-5.1619, -6.2800,  5.1142,  5.5431,  5.7785, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 68
	action: tensor([[-3.2534, -5.0695,  5.6985,  5.0942,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 69
	action: tensor([[-6.2800, -6.2522,  6.1800,  6.1800,  5.8311, -6.2800,  4.0886]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 70
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.0804,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 71
	action: tensor([[-6.2800, -5.4346,  6.1800,  5.9506,  6.1800, -6.2800,  5.3663]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 72
	action: tensor([[-6.2800, -4.5551,  6.1800,  6.1800,  6.1800, -6.2800,  5.0947]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 73
	action: tensor([[-5.5385, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 74
	action: tensor([[-6.2800, -5.3682,  5.7141,  5.3104,  6.1800, -6.2800,  5.8024]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 75
	action: tensor([[-6.2800, -5.2837,  6.1800,  5.8777,  4.9940, -6.2800,  5.2664]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 76
	action: tensor([[-6.1664, -6.2800,  5.8183,  6.1800,  5.9098, -6.2800,  5.6793]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 77
	action: tensor([[-5.0990, -5.9601,  6.1800,  4.2770,  6.1800, -6.2800,  6.1445]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 78
	action: tensor([[-5.8024, -5.9963,  6.1800,  5.3760,  6.1800, -6.2800,  4.9778]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 79
	action: tensor([[-6.2541, -6.2800,  6.1731,  6.1800,  6.1800, -6.2800,  3.9830]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 80
	action: tensor([[-6.2800, -5.5226,  6.0699,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 81
	action: tensor([[-6.2800, -4.5580,  5.4437,  6.1800,  5.9006, -6.2800,  5.7414]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 82
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.6127, -6.2800,  5.4339]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 83
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.1214,  6.0048, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 84
	action: tensor([[-6.2800, -5.9595,  5.7984,  6.1800,  2.8097, -6.2800,  5.5785]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 85
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.7606,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 86
	action: tensor([[-6.2800, -5.3403,  6.1800,  6.1800,  6.1800, -6.2800,  6.0268]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 87
	action: tensor([[-6.2800, -6.1870,  6.1800,  4.4612,  5.6803, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 88
	action: tensor([[-6.2800, -6.1787,  6.1800,  6.1800,  6.1800, -6.2800,  6.0527]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 89
	action: tensor([[-6.2800, -6.2800,  5.8489,  6.1166,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 90
	action: tensor([[-6.2364, -5.3994,  6.1800,  6.1800,  6.1800, -6.2800,  4.6419]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 91
	action: tensor([[-6.0014, -6.2800,  6.1800,  5.9343,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 92
	action: tensor([[-4.8619, -6.2800,  6.1800,  5.6999,  5.3861, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26906840951289357, distance: 0.9783518248575902 entropy -1.8704687425011064
epoch: 5, step: 93
	action: tensor([[-6.2800, -5.2151,  5.0985,  6.1800,  6.1800, -6.2800,  4.8696]],
       dtype=torch.float64)
	q_value: tensor([[-1.1904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 94
	action: tensor([[-6.1486, -6.2800,  4.9754,  6.1800,  5.0507, -6.2800,  5.7037]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 95
	action: tensor([[-4.1017, -4.5596,  5.3868,  5.4169,  6.1800, -6.2800,  5.9325]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 96
	action: tensor([[-5.5496, -6.2800,  6.1800,  4.7188,  5.1187, -6.2800,  6.0598]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 97
	action: tensor([[-6.2800, -6.2800,  5.9726,  5.2623,  5.2513, -6.2800,  5.1235]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 98
	action: tensor([[-4.8862, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5417226156081474, distance: 0.7746774353733231 entropy -1.8704687425011064
epoch: 5, step: 99
	action: tensor([[-5.1830, -5.3906,  5.6832,  6.1031,  4.8342, -6.2800,  5.9239]],
       dtype=torch.float64)
	q_value: tensor([[-0.9200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 100
	action: tensor([[-6.2800, -6.2800,  6.0075,  6.1593,  6.1800, -6.2800,  4.8972]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 101
	action: tensor([[-6.2800, -5.9980,  5.9976,  5.2521,  5.4542, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 102
	action: tensor([[-5.2436, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 103
	action: tensor([[-6.2498, -6.2800,  4.2801,  6.1800,  6.1800, -6.2800,  5.6014]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37353029985440367, distance: 0.905746633759759 entropy -1.8704687425011064
epoch: 5, step: 104
	action: tensor([[-5.9336, -6.2800,  5.7536,  6.1800,  4.3107, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 105
	action: tensor([[-6.2800, -6.0199,  6.1800,  4.2454,  5.3488, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 106
	action: tensor([[-3.6030, -5.8776,  6.0180,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 107
	action: tensor([[-5.9993, -6.2800,  5.7223,  6.1800,  5.7288, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 108
	action: tensor([[-6.2800, -5.5439,  5.1215,  6.1800,  6.1800, -6.2800,  6.0408]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 109
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0213,  6.1800, -6.2800,  5.2920]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 110
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 111
	action: tensor([[-6.2800, -5.0121,  5.5680,  5.6923,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 112
	action: tensor([[-5.4444, -5.8638,  5.5707,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 113
	action: tensor([[-6.1989, -5.4881,  6.1800,  6.1800,  6.1800, -6.2800,  5.4928]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 114
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.3861,  6.1800, -6.2800,  5.7327]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 115
	action: tensor([[-5.3013, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.5283]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 116
	action: tensor([[-5.6450, -6.2800,  6.0115,  5.4958,  6.1800, -6.2800,  5.4797]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 117
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.0103,  5.9108, -6.2800,  5.8393]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 118
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.8192,  5.7528, -6.2800,  5.0936]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 119
	action: tensor([[-5.9533, -6.2800,  6.1800,  5.9448,  5.4961, -6.2800,  6.0872]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3947756708055996, distance: 0.8902559442479163 entropy -1.8704687425011064
epoch: 5, step: 120
	action: tensor([[-6.1501, -6.2310,  6.1800,  5.9389,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 121
	action: tensor([[-4.7945, -6.0863,  6.1800,  6.1800,  6.1800, -6.2800,  5.9749]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 122
	action: tensor([[-6.0726, -6.2800,  5.6914,  6.1800,  5.9378, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 123
	action: tensor([[-4.3755, -6.2800,  5.3991,  6.1800,  6.0676, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 124
	action: tensor([[-4.6743, -6.2800,  4.3966,  5.6057,  5.0202, -6.2800,  5.9782]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 125
	action: tensor([[-6.2800, -5.7690,  6.1800,  5.4065,  6.1800, -6.2800,  5.5894]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 126
	action: tensor([[-6.2080, -6.2800,  6.1800,  5.0841,  3.9014, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 5, step: 127
	action: tensor([[-6.2800, -6.2800,  5.9723,  6.1800,  6.1354, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 5 actor 1284.4579791322633 critic 2320.6989632283917 entropy 100
epoch: 6, step: 0
	action: tensor([[-5.2853, -6.2800,  6.1800,  5.4023,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 1
	action: tensor([[-5.3741, -6.2800,  5.6248,  6.1800,  6.1800, -6.2800,  5.3023]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7363069426566102, distance: 0.5876328164879963 entropy -1.8704687425011064
epoch: 6, step: 2
	action: tensor([[-5.6302, -5.4449,  6.1800,  6.1800,  4.3815, -6.2800,  5.6125]],
       dtype=torch.float64)
	q_value: tensor([[-2.3487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 3
	action: tensor([[-6.2800, -4.1391,  6.1800,  5.0959,  6.1800, -6.2800,  6.0694]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 4
	action: tensor([[-6.0935, -5.1459,  4.8341,  6.1800,  5.5052, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 5
	action: tensor([[-5.8548, -5.9983,  6.1800,  6.1800,  5.6111, -6.2800,  5.7876]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 6
	action: tensor([[-6.1896, -6.2800,  5.6217,  6.1800,  6.0286, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3685667260775556, distance: 0.9093277090358127 entropy -1.8704687425011064
epoch: 6, step: 7
	action: tensor([[-6.2800, -6.0849,  5.8242,  4.4572,  5.4992, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.6075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 8
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0655,  6.1800, -6.2800,  5.5623]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 9
	action: tensor([[-6.2800, -6.0966,  6.1800,  5.7864,  6.1800, -6.2800,  5.9567]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 10
	action: tensor([[-5.5854, -5.1404,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 11
	action: tensor([[-5.6404, -6.2800,  6.1800,  5.4742,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 12
	action: tensor([[-6.2800, -6.2800,  5.2707,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 13
	action: tensor([[-6.2800, -5.7961,  6.1800,  6.1800,  5.9239, -6.2800,  5.7550]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 14
	action: tensor([[-6.0272, -6.2800,  6.1800,  4.7641,  4.5934, -6.2800,  5.9619]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 15
	action: tensor([[-5.9769, -6.2800,  6.1800,  6.1800,  4.7723, -6.2800,  5.6744]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 16
	action: tensor([[-6.2800, -6.2800,  6.1578,  5.9610,  4.8057, -6.2800,  6.0027]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 17
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.3650,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 18
	action: tensor([[-4.7302, -6.2800,  6.1136,  5.1425,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 19
	action: tensor([[-6.2800, -6.2800,  5.8100,  6.1800,  4.6703, -6.2800,  4.1148]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 20
	action: tensor([[-5.8380, -5.7799,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 21
	action: tensor([[-5.3284, -5.5357,  5.9126,  5.5503,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 22
	action: tensor([[-3.9146, -5.2421,  6.1800,  5.8155,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 23
	action: tensor([[-6.1102, -6.2800,  5.9542,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 24
	action: tensor([[-5.7058, -5.1026,  5.2933,  5.5181,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 25
	action: tensor([[-5.3571, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 26
	action: tensor([[-6.2740, -5.9401,  6.1800,  6.1800,  6.1800, -6.2800,  4.3076]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 27
	action: tensor([[-6.2800, -6.2800,  4.8962,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4014540811236701, distance: 0.885330508454508 entropy -1.8704687425011064
epoch: 6, step: 28
	action: tensor([[-6.2800, -5.3767,  6.1800,  6.1800,  5.9060, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.3057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 29
	action: tensor([[-6.2800, -6.2800,  5.9468,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 30
	action: tensor([[-6.2800, -4.5467,  5.3803,  5.2863,  5.8861, -6.2800,  5.8856]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 31
	action: tensor([[-5.0354, -3.9531,  4.6205,  6.1800,  6.0571, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 32
	action: tensor([[-6.2800, -6.2422,  5.5921,  6.0011,  4.8065, -6.2800,  5.9930]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 33
	action: tensor([[-5.5702, -6.2800,  5.4407,  6.1023,  5.8721, -6.2800,  5.3953]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 34
	action: tensor([[-5.4450, -6.2800,  5.8719,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 35
	action: tensor([[-5.9727, -6.2800,  6.1800,  5.1413,  5.4058, -6.2800,  5.8241]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 36
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.9632]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 37
	action: tensor([[-6.2800, -6.1020,  6.1800,  6.1800,  6.1800, -6.2800,  5.4927]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 38
	action: tensor([[-6.2800, -6.2800,  5.7880,  4.2411,  6.1800, -6.2800,  3.6231]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 39
	action: tensor([[-6.2800, -5.0045,  6.1800,  6.1800,  6.1800, -6.2800,  4.1094]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 40
	action: tensor([[-4.7732, -5.6041,  5.5212,  4.2620,  5.7840, -6.2800,  5.5139]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 41
	action: tensor([[-6.2800, -5.3153,  5.5134,  6.1800,  6.0536, -6.2800,  5.1880]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 42
	action: tensor([[-6.2480, -5.5148,  5.4028,  5.6491,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 43
	action: tensor([[-6.0574, -5.7705,  6.1800,  6.1332,  6.0470, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 44
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 45
	action: tensor([[-6.2800, -4.6510,  6.1800,  5.3844,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 46
	action: tensor([[-6.2791, -5.3473,  6.1800,  5.9864,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 47
	action: tensor([[-5.8266, -6.2800,  4.9687,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7188822943396675, distance: 0.6067374376099731 entropy -1.8704687425011064
epoch: 6, step: 48
	action: tensor([[-6.0318, -5.4424,  6.0426,  6.1800,  4.0314, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.2197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 49
	action: tensor([[-5.2129, -6.0593,  5.7475,  5.9724,  5.5083, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 50
	action: tensor([[-6.2800, -5.5278,  6.1800,  5.9489,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 51
	action: tensor([[-5.2854, -6.2800,  6.1800,  5.3600,  6.1800, -6.2800,  4.6441]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 52
	action: tensor([[-4.7388, -5.1562,  5.2900,  6.1030,  6.1800, -6.2800,  5.4131]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 53
	action: tensor([[-5.9698, -6.1386,  6.1800,  6.1800,  5.9508, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 54
	action: tensor([[-6.2800, -6.2800,  5.6158,  6.1800,  5.8825, -6.2800,  5.4041]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 55
	action: tensor([[-5.5331, -3.6924,  6.1800,  6.1800,  5.3367, -6.2800,  5.4108]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 56
	action: tensor([[-6.2800, -6.2800,  5.2962,  5.0746,  6.0010, -6.2800,  5.0979]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 57
	action: tensor([[-6.1811, -6.2800,  5.3029,  6.1800,  4.1244, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 58
	action: tensor([[-6.2106, -4.9381,  5.6925,  5.7302,  6.1800, -6.2800,  5.2201]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 59
	action: tensor([[-6.2800, -6.2800,  5.2984,  6.1800,  6.1800, -6.2800,  5.6716]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 60
	action: tensor([[-6.2800, -5.0488,  6.1800,  4.5906,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 61
	action: tensor([[-6.2800, -5.5736,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 62
	action: tensor([[-6.1556, -6.2800,  4.5682,  5.2726,  6.1800, -6.2800,  5.1380]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 63
	action: tensor([[-6.2800, -6.0673,  5.7281,  6.1800,  6.1800, -6.2800,  4.7499]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 64
	action: tensor([[-6.2800, -6.1609,  6.1800,  5.8511,  5.6118, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 65
	action: tensor([[-6.2800, -6.1380,  6.1800,  6.1800,  4.9570, -6.2800,  5.3986]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 66
	action: tensor([[-4.5589, -5.1346,  6.1800,  4.9036,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 67
	action: tensor([[-6.1897, -6.0423,  6.1800,  6.1800,  5.3834, -6.2800,  5.8520]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 68
	action: tensor([[-5.3636, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.2106]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 69
	action: tensor([[-4.6007, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 70
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.5603,  6.1800, -6.2800,  5.9797]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 71
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 72
	action: tensor([[-6.2800, -4.3161,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 73
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 74
	action: tensor([[-4.4894, -5.8333,  6.1800,  6.1800,  6.1800, -6.2800,  5.1329]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 75
	action: tensor([[-4.6108, -6.2800,  5.4463,  5.2450,  6.0524, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 76
	action: tensor([[-6.2800, -5.4405,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 77
	action: tensor([[-6.1250, -6.0616,  5.3584,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 78
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.0610,  5.0992, -6.2800,  5.7543]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 79
	action: tensor([[-6.2800, -6.2800,  5.9156,  6.1800,  5.4233, -6.2800,  6.1222]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 80
	action: tensor([[-5.1998, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.0637]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 81
	action: tensor([[-5.6349, -6.2800,  5.4535,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 82
	action: tensor([[-6.2800, -6.2800,  5.5891,  4.8019,  5.0820, -6.2800,  5.4889]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 83
	action: tensor([[-4.4081, -5.3545,  6.1800,  6.1800,  5.8529, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 84
	action: tensor([[-6.0067, -5.4112,  5.0109,  6.1800,  6.1800, -6.2800,  5.8159]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 85
	action: tensor([[-4.7013, -6.2800,  4.5256,  5.4647,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 86
	action: tensor([[-5.9486, -5.8842,  5.9081,  6.1800,  5.4837, -6.2800,  5.4009]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 87
	action: tensor([[-5.8950, -5.4208,  6.1800,  6.1800,  4.7934, -6.2800,  5.9606]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 88
	action: tensor([[-6.2800, -5.5148,  6.1800,  6.0946,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 89
	action: tensor([[-6.2745, -5.4702,  6.1800,  6.1521,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 90
	action: tensor([[-6.2800, -6.2800,  4.1248,  4.9070,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 91
	action: tensor([[-4.5191, -6.2800,  6.1800,  6.1800,  6.1589, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 92
	action: tensor([[-6.2800, -5.1681,  5.6176,  4.6313,  6.1459, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 93
	action: tensor([[-3.9593, -5.4373,  6.1800,  6.1800,  3.3631, -6.2800,  5.0093]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 94
	action: tensor([[-5.6140, -6.2800,  6.1800,  4.8815,  5.7018, -6.2800,  5.9508]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 95
	action: tensor([[-6.2800, -5.0479,  5.2347,  6.1800,  6.1800, -6.2800,  6.0779]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 96
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0650,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 97
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 98
	action: tensor([[-6.2800, -6.2800,  5.3520,  5.5643,  6.1800, -6.2800,  5.9548]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 99
	action: tensor([[-4.7957, -6.1393,  4.4239,  5.5414,  6.1800, -6.2800,  6.0733]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 100
	action: tensor([[-5.0285, -6.2800,  5.9465,  5.3767,  5.8940, -6.2800,  5.9187]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 101
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 102
	action: tensor([[-5.4317, -5.9591,  4.3961,  6.1800,  6.1800, -6.2800,  5.1831]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9428172738164128, distance: 0.27364601087151474 entropy -1.8704687425011064
epoch: 6, step: 103
	action: tensor([[-4.0690, -5.8303,  6.1800,  6.1094,  5.2929, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.5352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 104
	action: tensor([[-6.2800, -5.5231,  5.2617,  5.6290,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 105
	action: tensor([[-6.2800, -6.2800,  5.3616,  6.1800,  3.9602, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 106
	action: tensor([[-6.2800, -6.2800,  6.0053,  6.1800,  4.4080, -6.2800,  5.3297]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 107
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 108
	action: tensor([[-5.5119, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6981406340505407, distance: 0.6287225265646861 entropy -1.8704687425011064
epoch: 6, step: 109
	action: tensor([[-6.2800, -5.5070,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.3772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 110
	action: tensor([[-6.2800, -6.2800,  5.7576,  5.9953,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 111
	action: tensor([[-4.5506, -5.5550,  6.1800,  4.3873,  6.1532, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 112
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.7151]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 113
	action: tensor([[-6.2800, -6.2800,  6.1435,  6.1800,  4.2957, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 114
	action: tensor([[-6.2800, -5.9025,  5.6197,  5.3331,  4.1868, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5065545309180342, distance: 0.8038523320748305 entropy -1.8704687425011064
epoch: 6, step: 115
	action: tensor([[-6.2800, -6.1617,  6.1800,  6.1800,  6.1800, -6.2800,  5.6944]],
       dtype=torch.float64)
	q_value: tensor([[-3.3234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 116
	action: tensor([[-6.2800, -6.2800,  4.9158,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 117
	action: tensor([[-5.2038, -6.2800,  6.1800,  5.7126,  4.6451, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 118
	action: tensor([[-5.4608, -5.6477,  5.8167,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 119
	action: tensor([[-6.2800, -6.2127,  6.1800,  6.1800,  6.1800, -6.2800,  5.5469]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 120
	action: tensor([[-5.1192, -5.5362,  6.1800,  5.0094,  5.7008, -6.2800,  5.5988]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 121
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.3125,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 122
	action: tensor([[-5.9126, -4.7402,  5.9306,  6.0997,  6.1800, -6.2800,  4.6193]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 123
	action: tensor([[-6.2800, -5.4172,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 124
	action: tensor([[-4.4867, -6.2800,  6.1800,  6.1800,  5.1110, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 125
	action: tensor([[-6.2800, -5.9550,  6.1472,  5.4864,  5.8220, -6.2800,  5.2438]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 126
	action: tensor([[-6.2800, -5.5798,  6.1800,  5.2837,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 6, step: 127
	action: tensor([[-4.8816, -5.2194,  6.1800,  6.1604,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 6 actor 1218.1655113833633 critic 2163.2001919215863 entropy 100
epoch: 7, step: 0
	action: tensor([[-5.7466, -4.7348,  4.6746,  4.8352,  6.1800, -6.2800,  4.7811]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 1
	action: tensor([[-5.2447, -5.7384,  5.7807,  5.8940,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 2
	action: tensor([[-6.2138, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.9632]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 3
	action: tensor([[-6.2800, -6.0682,  6.1800,  6.1800,  6.1800, -6.2800,  5.8358]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 4
	action: tensor([[-6.2800, -5.9579,  6.1800,  6.0883,  6.1800, -6.2800,  5.2869]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 5
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0604,  6.0207, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 6
	action: tensor([[-6.2800, -5.6183,  4.5647,  5.5068,  5.8549, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 7
	action: tensor([[-4.4434, -4.7766,  5.3294,  3.8592,  6.1726, -6.2800,  5.7191]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 8
	action: tensor([[-5.3351, -5.9331,  6.1800,  6.1800,  5.4613, -6.2800,  5.0098]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 9
	action: tensor([[-6.2800, -5.0882,  6.1800,  6.1800,  6.1800, -6.2800,  5.6072]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 10
	action: tensor([[-6.2800, -4.9468,  6.1800,  6.1800,  5.2828, -6.2800,  5.7984]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 11
	action: tensor([[-5.8064, -6.2800,  5.5442,  5.8975,  6.1800, -6.2800,  4.5372]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 12
	action: tensor([[-5.0716, -5.8459,  6.1800,  5.7505,  6.1800, -6.2800,  5.5506]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 13
	action: tensor([[-6.2601, -6.2800,  4.2532,  5.8990,  6.1800, -6.2800,  5.2558]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5143107251755761, distance: 0.797509655756128 entropy -1.8704687425011064
epoch: 7, step: 14
	action: tensor([[-6.2800, -6.2800,  4.3915,  5.5872,  4.7125, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6986787173841729, distance: 0.6281619078656039 entropy -1.8704687425011064
epoch: 7, step: 15
	action: tensor([[-6.2800, -5.9714,  5.9562,  5.4430,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.9258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 16
	action: tensor([[-6.2800, -6.1092,  6.1800,  6.1800,  6.1800, -6.2800,  5.6531]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 17
	action: tensor([[-4.9352, -6.2800,  5.9998,  5.6756,  6.1800, -6.2800,  4.9791]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25752438393007027, distance: 0.9860473969964971 entropy -1.8704687425011064
epoch: 7, step: 18
	action: tensor([[-6.1343, -6.2118,  6.1800,  6.1800,  6.1800, -6.2800,  5.5208]],
       dtype=torch.float64)
	q_value: tensor([[-4.9423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 19
	action: tensor([[-5.3147, -5.8358,  5.9182,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 20
	action: tensor([[-4.3450, -6.1264,  5.5748,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 21
	action: tensor([[-6.2800, -6.2800,  5.4285,  6.1800,  6.1800, -6.2800,  5.7089]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3087288149185744, distance: 0.9514389363115497 entropy -1.8704687425011064
epoch: 7, step: 22
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.3115, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-3.7771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 23
	action: tensor([[-6.0841, -6.2800,  6.1800,  5.5040,  6.1800, -6.2800,  5.7014]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 24
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 25
	action: tensor([[-6.1069, -4.2816,  6.1800,  6.0217,  6.1800, -6.2800,  4.8732]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 26
	action: tensor([[-5.6959, -6.2800,  5.8864,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 27
	action: tensor([[-6.2800, -6.0848,  6.1800,  6.1800,  4.6507, -6.2800,  5.9481]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 28
	action: tensor([[-5.3421, -6.2800,  5.3471,  6.1215,  5.6125, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 29
	action: tensor([[-6.2800, -6.2800,  6.1510,  5.8352,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 30
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.4232, -6.2800,  5.9862]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 31
	action: tensor([[-5.9926, -6.2800,  6.1800,  6.1800,  4.9675, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 32
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.1946, -6.2800,  4.9935]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 33
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.0909, -6.2800,  5.2081]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 34
	action: tensor([[-5.7102, -6.2800,  4.8226,  6.1800,  4.3048, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7364877908989851, distance: 0.5874312742280255 entropy -1.8704687425011064
epoch: 7, step: 35
	action: tensor([[-4.3672, -5.7489,  6.1249,  5.1938,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 36
	action: tensor([[-6.0280, -5.0545,  6.1800,  4.5830,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 37
	action: tensor([[-6.0191, -6.2800,  5.1757,  5.6489,  6.1800, -6.2800,  5.5888]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 38
	action: tensor([[-6.2800, -5.0312,  5.4616,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 39
	action: tensor([[-6.2800, -6.2800,  4.1106,  5.9258,  3.9328, -6.2800,  5.5044]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5756714253859742, distance: 0.745431657130864 entropy -1.8704687425011064
epoch: 7, step: 40
	action: tensor([[-5.6970, -6.2800,  3.7880,  5.9869,  4.3062, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-8.8771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.845297594264125, distance: 0.4500960762376787 entropy -1.8704687425011064
epoch: 7, step: 41
	action: tensor([[-6.2800, -6.2800,  6.1024,  6.1800,  5.4859, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-8.1500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 42
	action: tensor([[-6.2800, -6.2800,  5.6761,  6.1800,  4.7476, -6.2800,  5.4467]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 43
	action: tensor([[-6.2800, -4.8411,  6.1800,  6.1082,  6.1800, -6.2800,  5.9834]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 44
	action: tensor([[-5.3589, -5.1074,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 45
	action: tensor([[-5.8196, -6.2800,  6.1800,  6.1800,  5.3194, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 46
	action: tensor([[-4.6576, -6.0903,  6.1800,  5.5367,  5.2590, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 47
	action: tensor([[-6.2800, -5.7450,  6.1800,  6.1800,  6.1800, -6.2800,  4.5994]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 48
	action: tensor([[-6.2800, -3.0525,  6.1800,  5.2286,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 49
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.6718,  6.1643, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 50
	action: tensor([[-6.2800, -4.4298,  5.7728,  6.1800,  6.1800, -6.2800,  6.1122]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 51
	action: tensor([[-5.0319, -6.0798,  6.1800,  6.1800,  6.1800, -6.2800,  5.8877]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 52
	action: tensor([[-6.2800, -6.2800,  5.1885,  5.8176,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41950371393279684, distance: 0.8718793987513302 entropy -1.8704687425011064
epoch: 7, step: 53
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.7846,  6.1800, -6.2800,  5.9801]],
       dtype=torch.float64)
	q_value: tensor([[-3.8755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 54
	action: tensor([[-5.0903, -5.4883,  5.9286,  6.1800,  6.1800, -6.2800,  5.4588]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 55
	action: tensor([[-6.2800, -6.2800,  5.0066,  6.1800,  5.5707, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 56
	action: tensor([[-6.2800, -5.3862,  6.1800,  6.1800,  5.4718, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 57
	action: tensor([[-6.2800, -6.2086,  5.9140,  5.3234,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 58
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.1974,  5.4567, -6.2800,  5.8397]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 59
	action: tensor([[-5.9249, -6.2800,  5.7034,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 60
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1583]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 61
	action: tensor([[-6.2800, -4.8583,  6.1800,  6.1800,  4.4035, -6.2800,  5.8343]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 62
	action: tensor([[-6.2800, -4.6054,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 63
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.4410]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 64
	action: tensor([[-5.8568, -6.2800,  6.1800,  6.1396,  5.3899, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 65
	action: tensor([[-6.2800, -5.8803,  6.1800,  6.1800,  4.9912, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 66
	action: tensor([[-4.5822, -5.6604,  5.0000,  6.0122,  4.5477, -6.2800,  6.0368]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 67
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.0866]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 68
	action: tensor([[-4.7875, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1229]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 69
	action: tensor([[-5.8588, -5.1774,  5.9078,  5.8522,  6.1800, -6.2800,  5.2600]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 70
	action: tensor([[-6.2764, -6.2800,  4.4512,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33538831936812186, distance: 0.9329119973031862 entropy -1.8704687425011064
epoch: 7, step: 71
	action: tensor([[-6.0817, -4.5167,  6.1800,  6.1800,  5.9472, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-5.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 72
	action: tensor([[-5.4744, -5.7095,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 73
	action: tensor([[-6.2800, -6.2800,  5.7391,  6.1800,  6.1800, -6.2800,  5.9733]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 74
	action: tensor([[-4.5260, -5.9856,  4.5462,  6.1800,  5.0270, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 75
	action: tensor([[-6.0819, -5.8301,  5.4060,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 76
	action: tensor([[-5.4421, -6.2800,  6.1800,  6.0613,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 77
	action: tensor([[-4.5381, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 78
	action: tensor([[-5.5632, -6.2800,  4.9746,  5.7331,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.658530903904855, distance: 0.6687016514296992 entropy -1.8704687425011064
epoch: 7, step: 79
	action: tensor([[-6.2800, -6.2800,  5.8357,  6.1800,  5.5177, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.3912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 80
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.0993]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 81
	action: tensor([[-6.2800, -4.8661,  5.1904,  4.7308,  5.5176, -6.2800,  6.0752]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 82
	action: tensor([[-4.6444, -5.1434,  6.1800,  6.1351,  5.6524, -6.2800,  5.4775]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 83
	action: tensor([[-6.2800, -5.9388,  6.1800,  6.0843,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 84
	action: tensor([[-6.2800, -6.0520,  6.1800,  5.5879,  5.8107, -6.2800,  5.0104]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 85
	action: tensor([[-4.7091, -6.2800,  6.0065,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 86
	action: tensor([[-5.6873, -6.0393,  6.1800,  6.0935,  5.4381, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 87
	action: tensor([[-5.8309, -6.2800,  5.3201,  5.9536,  6.1800, -6.2800,  4.4683]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 88
	action: tensor([[-6.2800, -6.2800,  4.4972,  5.2025,  6.1800, -6.2800,  6.1277]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 89
	action: tensor([[-5.8302, -5.9899,  5.7948,  5.4183,  4.9620, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 90
	action: tensor([[-6.2800, -4.8663,  5.7842,  4.2549,  6.1800, -6.2800,  5.4524]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 91
	action: tensor([[-6.2800, -5.8123,  6.1800,  6.1800,  4.4336, -6.2800,  5.5380]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 92
	action: tensor([[-5.9623, -5.8232,  6.0144,  6.1800,  6.1800, -6.2800,  5.3746]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 93
	action: tensor([[-5.6026, -2.4159,  5.4777,  6.1800,  6.1800, -6.2800,  5.9254]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 94
	action: tensor([[-5.7563, -5.7499,  6.1800,  4.4356,  4.7638, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 95
	action: tensor([[-6.2800, -5.2625,  6.1800,  6.1110,  5.7908, -6.2800,  5.8485]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 96
	action: tensor([[-6.0721, -6.2800,  6.1800,  5.5322,  5.1342, -6.2800,  5.9868]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 97
	action: tensor([[-6.2800, -6.2800,  5.7817,  5.9764,  5.5543, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 98
	action: tensor([[-5.0470, -6.2800,  6.1800,  5.0446,  5.3968, -6.2800,  5.4802]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 99
	action: tensor([[-4.6733, -6.2756,  5.5518,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 100
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.4757,  5.7018, -6.2800,  6.1192]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 101
	action: tensor([[-6.1875, -3.7938,  6.1800,  6.1800,  5.7941, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 102
	action: tensor([[-3.7514, -6.2800,  4.7354,  4.6867,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 103
	action: tensor([[-3.9264, -4.5638,  6.1800,  6.1800,  5.9217, -6.2800,  6.1695]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 104
	action: tensor([[-6.2800, -6.2800,  6.0603,  6.1800,  5.2595, -6.2800,  5.7547]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 105
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.4424, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 106
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.0065]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 107
	action: tensor([[-6.0761, -5.8527,  5.4558,  6.1800,  5.3046, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 108
	action: tensor([[-4.5812, -5.9587,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 109
	action: tensor([[-6.2800, -6.1915,  6.1800,  6.0016,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 110
	action: tensor([[-4.7398, -6.2800,  6.0132,  6.1800,  6.1800, -6.2800,  5.6611]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 111
	action: tensor([[-6.2800, -5.9062,  6.1657,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 112
	action: tensor([[-5.4098, -5.8001,  6.1800,  6.1800,  5.4404, -6.2800,  6.0699]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 113
	action: tensor([[-3.2083, -6.1526,  6.1800,  5.7488,  6.1800, -6.2800,  4.5509]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 114
	action: tensor([[-6.2800, -6.2800,  5.4588,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 115
	action: tensor([[-5.0504, -6.2800,  4.7696,  5.0671,  5.5655, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 116
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.8299,  6.1800, -6.2800,  5.7208]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 117
	action: tensor([[-5.5598, -5.9883,  4.3109,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 118
	action: tensor([[-6.2800, -6.2800,  5.2815,  6.1800,  5.0557, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 119
	action: tensor([[-6.0474, -5.8615,  5.8910,  5.0114,  5.8166, -6.2800,  5.0430]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 120
	action: tensor([[-5.0824, -6.2800,  6.1800,  5.5170,  5.4130, -6.2800,  5.8995]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34130099134221004, distance: 0.9287529328126546 entropy -1.8704687425011064
epoch: 7, step: 121
	action: tensor([[-4.7765, -6.2800,  5.7635,  4.8221,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 122
	action: tensor([[-6.2800, -6.1370,  6.1800,  6.1800,  5.9940, -6.2800,  4.6711]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 123
	action: tensor([[-6.1907, -4.8041,  5.4860,  6.1800,  4.8351, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 124
	action: tensor([[-6.2800, -6.2755,  4.2442,  4.8551,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 125
	action: tensor([[-4.6573, -6.0558,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 126
	action: tensor([[-5.2344, -6.2800,  6.1726,  5.7699,  5.2683, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 7, step: 127
	action: tensor([[-5.9819, -6.2748,  6.1800,  6.1800,  6.1800, -6.2800,  5.6387]],
       dtype=torch.float64)
	q_value: tensor([[-6.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 7 actor 1062.3566150474308 critic 1880.9281155719598 entropy 100
epoch: 8, step: 0
	action: tensor([[-5.4643, -4.2488,  5.5994,  5.3122,  5.4312, -6.2800,  5.9858]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 1
	action: tensor([[-5.4320, -6.1417,  6.1800,  6.1800,  4.9495, -6.2800,  5.9265]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 2
	action: tensor([[-6.2800, -6.1480,  6.1800,  5.3157,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036362211204798545, distance: 1.123346158448013 entropy -1.8704687425011064
epoch: 8, step: 3
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-5.6075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 4
	action: tensor([[-6.2800, -6.1023,  4.9603,  6.1800,  5.1265, -6.2800,  5.5814]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 5
	action: tensor([[-6.2800, -5.5763,  6.1800,  6.1800,  5.3284, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 6
	action: tensor([[-5.3448, -6.2800,  6.1800,  5.0958,  5.4596, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 7
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.1878,  4.2559, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 8
	action: tensor([[-6.2800, -5.6400,  6.1800,  5.3451,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 9
	action: tensor([[-6.2800, -3.6276,  6.0660,  4.9050,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 10
	action: tensor([[-6.2800, -5.9357,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 11
	action: tensor([[-5.6776, -5.1017,  5.2827,  6.1800,  5.9491, -6.2800,  5.2048]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 12
	action: tensor([[-6.1302, -5.5819,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 13
	action: tensor([[-6.2800, -4.7004,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 14
	action: tensor([[-5.2421, -6.2800,  6.1800,  6.1800,  5.7051, -6.2800,  5.0971]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 15
	action: tensor([[-6.2800, -6.2800,  4.7696,  6.1800,  4.6176, -6.2800,  4.2504]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4001616298655928, distance: 0.8862858482715864 entropy -1.8704687425011064
epoch: 8, step: 16
	action: tensor([[-6.2800, -6.2800,  4.5928,  6.1270,  5.4040, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-14.8416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3952843099576552, distance: 0.8898817737249899 entropy -1.8704687425011064
epoch: 8, step: 17
	action: tensor([[-5.2952, -6.2800,  5.8477,  5.3120,  5.3486, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-9.4736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 18
	action: tensor([[-4.8877, -6.1275,  5.9195,  6.1402,  5.1030, -6.2800,  5.4420]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 19
	action: tensor([[-4.1986, -6.2800,  5.0285,  5.2363,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 20
	action: tensor([[-5.6762, -5.8732,  6.1800,  6.1800,  6.1800, -6.2800,  4.7138]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 21
	action: tensor([[-6.1708, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.2043]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 22
	action: tensor([[-6.2800, -5.8995,  6.1800,  5.7725,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 23
	action: tensor([[-5.1256, -6.2800,  5.8694,  4.4928,  6.0743, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 24
	action: tensor([[-6.2800, -4.3823,  6.0775,  5.5286,  5.2520, -6.2800,  5.8473]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 25
	action: tensor([[-6.2603, -5.8086,  5.8871,  6.1800,  5.1864, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 26
	action: tensor([[-6.2800, -6.2800,  5.0609,  6.0761,  6.1800, -6.2800,  5.8281]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 27
	action: tensor([[-6.2800, -6.2800,  4.4021,  5.4322,  5.4473, -6.2800,  5.7723]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.736158969353435, distance: 0.5877976706075777 entropy -1.8704687425011064
epoch: 8, step: 28
	action: tensor([[-5.7319, -5.3099,  6.1800,  6.1800,  5.2844, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 29
	action: tensor([[-4.9896, -6.2403,  5.8362,  6.1800,  5.2749, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 30
	action: tensor([[-4.8183, -6.2800,  5.1210,  5.4495,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05618988327867702, distance: 1.1117291863499985 entropy -1.8704687425011064
epoch: 8, step: 31
	action: tensor([[-6.1369, -6.1382,  6.1800,  5.6534,  5.6367, -6.2800,  6.0008]],
       dtype=torch.float64)
	q_value: tensor([[-8.4273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 32
	action: tensor([[-5.5783, -6.2800,  6.1800,  5.7532,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 33
	action: tensor([[-4.8360, -4.7334,  6.1800,  6.1800,  5.9610, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 34
	action: tensor([[-6.1834, -6.2800,  4.9941,  5.5528,  4.9612, -6.2800,  5.7636]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5850601064221832, distance: 0.737138829835595 entropy -1.8704687425011064
epoch: 8, step: 35
	action: tensor([[-6.2800, -5.1689,  5.6420,  4.2063,  6.1800, -6.2800,  5.1756]],
       dtype=torch.float64)
	q_value: tensor([[-10.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 36
	action: tensor([[-4.5785, -6.1924,  6.1800,  5.4800,  5.2957, -6.2800,  5.5663]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 37
	action: tensor([[-5.7148, -5.5268,  6.0937,  6.1800,  5.8729, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 38
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.7359, -6.2800,  6.0196]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 39
	action: tensor([[-5.1209, -5.8057,  4.8909,  5.0412,  6.1001, -6.2800,  6.1313]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 40
	action: tensor([[-6.2800, -5.4541,  6.1800,  6.1800,  6.0836, -6.2800,  5.3348]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 41
	action: tensor([[-5.8381, -6.2800,  6.1800,  5.8804,  6.1800, -6.2800,  5.2652]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 42
	action: tensor([[-6.1239, -4.9363,  6.1800,  6.1800,  5.8531, -6.2800,  5.9116]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 43
	action: tensor([[-5.7499, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.8801]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 44
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.4627,  6.1800, -6.2800,  6.0833]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 45
	action: tensor([[-4.8603, -5.9244,  6.1800,  6.1800,  6.1800, -6.2800,  5.0403]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7336988402406768, distance: 0.5905317088236762 entropy -1.8704687425011064
epoch: 8, step: 46
	action: tensor([[-6.2800, -6.2800,  3.4511,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.7853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33060285852182925, distance: 0.9362646364403614 entropy -1.8704687425011064
epoch: 8, step: 47
	action: tensor([[-4.6506, -5.7663,  4.9526,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-12.2469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 48
	action: tensor([[-5.3428, -5.8700,  5.6522,  5.9858,  6.1264, -6.2800,  5.7616]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 49
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 50
	action: tensor([[-6.2800, -6.0794,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 51
	action: tensor([[-5.6751, -6.2800,  4.3104,  5.8073,  5.4352, -6.2800,  5.4205]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 52
	action: tensor([[-6.2800, -5.6458,  6.0296,  6.1800,  5.1066, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 53
	action: tensor([[-4.9178, -5.9941,  5.4914,  6.0465,  5.6601, -6.2800,  6.1516]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7003135393052973, distance: 0.6264555405814587 entropy -1.8704687425011064
epoch: 8, step: 54
	action: tensor([[-4.8164, -5.3456,  6.1800,  5.4311,  5.7639, -6.2800,  5.0438]],
       dtype=torch.float64)
	q_value: tensor([[-7.6678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 55
	action: tensor([[-6.2800, -6.2800,  5.2270,  6.1800,  6.1206, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 56
	action: tensor([[-6.2800, -5.9047,  6.1800,  5.5664,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 57
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.9346,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 58
	action: tensor([[-6.2800, -6.2800,  5.8424,  6.1800,  6.1800, -6.2800,  5.7263]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 59
	action: tensor([[-6.1569, -6.2800,  6.1800,  5.9159,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 60
	action: tensor([[-5.5723, -4.1961,  6.1800,  4.6790,  6.0411, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 61
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.1783,  6.0628, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 62
	action: tensor([[-5.9651, -5.5641,  6.1800,  5.1955,  6.1800, -6.2800,  6.0492]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 63
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.2580,  5.9063, -6.2800,  5.9271]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 64
	action: tensor([[-6.2800, -5.9697,  5.2820,  5.9434,  6.1800, -6.2800,  4.4307]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 65
	action: tensor([[-5.9348, -6.2800,  5.9941,  6.0738,  5.7634, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 66
	action: tensor([[-6.2800, -6.0114,  6.1800,  4.0441,  5.2034, -6.2800,  5.5673]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 67
	action: tensor([[-5.8816, -6.2800,  5.9475,  6.1800,  6.1800, -6.2800,  4.8233]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 68
	action: tensor([[-6.2800, -5.2307,  6.0180,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 69
	action: tensor([[-5.2578, -5.8893,  5.9763,  5.6078,  4.2983, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 70
	action: tensor([[-6.2148, -6.2800,  6.1800,  4.8057,  6.0462, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 71
	action: tensor([[-5.6849, -6.2800,  5.7914,  6.1800,  5.6492, -6.2800,  5.3744]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 72
	action: tensor([[-5.4815, -6.2800,  6.1800,  4.9013,  5.3155, -6.2800,  5.4070]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 73
	action: tensor([[-4.7475, -5.0912,  5.6049,  6.1800,  6.1800, -6.2800,  5.7967]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 74
	action: tensor([[-5.3884, -5.9498,  6.1800,  6.1800,  6.1800, -6.2800,  6.0872]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 75
	action: tensor([[-5.6770, -6.0557,  6.1800,  6.1800,  5.8102, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 76
	action: tensor([[-5.4552, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.2610]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 77
	action: tensor([[-6.2800, -4.1869,  5.9794,  4.7272,  6.0529, -6.2800,  5.4835]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 78
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.7240,  5.6647, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 79
	action: tensor([[-6.2800, -6.2800,  4.6661,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35437045883481855, distance: 0.9194929210404841 entropy -1.8704687425011064
epoch: 8, step: 80
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.1005,  6.0723, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-8.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 81
	action: tensor([[-6.0864, -6.2800,  6.1800,  6.1800,  5.7766, -6.2800,  5.7675]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 82
	action: tensor([[-5.8571, -6.0459,  6.1800,  6.1800,  6.1800, -6.2800,  6.1050]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 83
	action: tensor([[-5.8393, -6.0804,  6.1800,  6.1800,  5.5815, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 84
	action: tensor([[-6.2800, -5.0125,  6.1800,  6.1800,  4.5822, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 85
	action: tensor([[-6.1187, -5.7356,  5.6645,  6.1800,  6.1800, -6.2800,  5.7007]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 86
	action: tensor([[-6.2800, -4.7505,  5.1839,  5.1729,  6.1800, -6.2800,  6.0101]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 87
	action: tensor([[-6.2800, -5.0402,  6.0442,  6.1800,  6.1173, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 88
	action: tensor([[-5.7048, -6.2800,  6.1800,  5.4554,  6.1800, -6.2800,  4.9035]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 89
	action: tensor([[-4.9414, -5.3452,  4.4854,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 90
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.8207]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 91
	action: tensor([[-6.2800, -5.5581,  5.7981,  6.1800,  6.1800, -6.2800,  4.9222]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 92
	action: tensor([[-5.8954, -6.2800,  6.1800,  5.7940,  4.8907, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 93
	action: tensor([[-6.1035, -6.2800,  5.8951,  6.1800,  5.7777, -6.2800,  5.8098]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 94
	action: tensor([[-6.2229, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 95
	action: tensor([[-5.0367, -5.5724,  3.4944,  6.1800,  6.1800, -6.2800,  5.5091]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 96
	action: tensor([[-4.7186, -6.2800,  6.1800,  3.6229,  5.9355, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 97
	action: tensor([[-6.2800, -5.8367,  5.6743,  4.7444,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 98
	action: tensor([[-6.2800, -5.0652,  3.6594,  4.3549,  6.1800, -6.2800,  6.1467]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 99
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.5724, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 100
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 101
	action: tensor([[-5.5919, -6.2800,  6.1800,  6.1800,  4.4176, -6.2800,  5.9858]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 102
	action: tensor([[-6.1611, -5.6829,  6.1800,  4.7300,  4.7375, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 103
	action: tensor([[-5.8969, -6.2800,  6.1800,  6.1800,  5.9146, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 104
	action: tensor([[-6.2800, -6.1120,  6.0176,  6.1800,  5.4197, -6.2800,  4.2896]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 105
	action: tensor([[-6.1851, -5.8355,  5.1042,  6.1520,  5.6194, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 106
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.0415]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 107
	action: tensor([[-6.2800, -5.5748,  6.1800,  5.8491,  6.1136, -6.2800,  4.7423]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 108
	action: tensor([[-6.2800, -6.2800,  5.3081,  5.6480,  6.1800, -6.2800,  5.4529]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 109
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.8324,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 110
	action: tensor([[-5.1336, -6.2800,  5.3656,  5.2331,  5.6673, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 111
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.7602, -6.2800,  4.0084]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 112
	action: tensor([[-6.2800, -5.7760,  5.3631,  6.0412,  4.0803, -6.2800,  4.7429]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 113
	action: tensor([[-5.0499, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.5201]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 114
	action: tensor([[-5.4537, -4.3109,  6.1800,  6.1800,  6.1800, -6.2800,  5.8616]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 115
	action: tensor([[-5.5431, -5.5733,  6.1800,  6.1800,  5.6700, -6.2800,  5.6083]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 116
	action: tensor([[-5.7866, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.0263]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 117
	action: tensor([[-5.4418, -4.9748,  5.8509,  6.1800,  6.1800, -6.2800,  5.5913]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 118
	action: tensor([[-5.0394, -6.2800,  4.0119,  5.3341,  5.4968, -6.2800,  5.5879]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 119
	action: tensor([[-5.9592, -6.2800,  5.4021,  5.6823,  6.1800, -6.2800,  6.0695]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 120
	action: tensor([[-6.2800, -5.3540,  5.2052,  6.1800,  5.6749, -6.2800,  5.5183]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 121
	action: tensor([[-6.0671, -6.0547,  6.1800,  6.1116,  5.4244, -6.2800,  4.9698]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 122
	action: tensor([[-6.0599, -6.2800,  4.9581,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 123
	action: tensor([[-5.1440, -4.2679,  5.7332,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 124
	action: tensor([[-6.2800, -6.1649,  6.1800,  5.7294,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 125
	action: tensor([[-5.3761, -5.3724,  6.1800,  6.1800,  6.1800, -6.2800,  6.0076]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 126
	action: tensor([[-5.2745, -6.2800,  6.1800,  5.9710,  5.9921, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 8, step: 127
	action: tensor([[-5.8526, -5.8053,  6.1800,  6.1800,  6.1800, -6.2800,  4.7611]],
       dtype=torch.float64)
	q_value: tensor([[-11.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 8 actor 883.6809520688007 critic 1478.778405941747 entropy 100
epoch: 9, step: 0
	action: tensor([[-6.2800, -5.4922,  5.5773,  6.1800,  6.1800, -6.2800,  5.1225]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 1
	action: tensor([[-5.4769, -4.9499,  6.1800,  5.9225,  6.1800, -6.2800,  5.4860]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 2
	action: tensor([[-6.1515, -6.2800,  6.1800,  5.9830,  5.0813, -6.2800,  5.6891]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 3
	action: tensor([[-6.2800, -5.5489,  6.1800,  6.1800,  5.7151, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 4
	action: tensor([[-6.2800, -5.8315,  4.6216,  5.2111,  4.7151, -6.2800,  5.8889]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 5
	action: tensor([[-6.2800, -5.9938,  6.1800,  4.8104,  4.8054, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 6
	action: tensor([[-6.2800, -5.7570,  6.1800,  5.7419,  4.9591, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 7
	action: tensor([[-5.7977, -4.8364,  4.9596,  6.1800,  5.3544, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 8
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.2501, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 9
	action: tensor([[-6.2800, -5.9103,  6.1800,  6.1800,  4.8811, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 10
	action: tensor([[-6.2800, -5.6726,  6.1800,  5.0000,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 11
	action: tensor([[-6.2323, -6.2800,  5.7296,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 12
	action: tensor([[-5.9778, -5.5020,  5.6811,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 13
	action: tensor([[-5.3348, -5.5527,  6.1800,  6.1357,  6.1800, -6.2800,  5.9421]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 14
	action: tensor([[-6.2800, -6.2800,  6.0683,  4.0383,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 15
	action: tensor([[-4.9149, -5.8202,  6.1800,  4.8761,  5.5209, -6.2800,  5.1483]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 16
	action: tensor([[-5.5575, -6.2800,  6.1800,  6.1800,  6.0805, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 17
	action: tensor([[-6.2800, -6.2800,  5.8710,  4.5687,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 18
	action: tensor([[-6.0499, -5.6236,  6.1800,  6.1800,  5.2882, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 19
	action: tensor([[-6.2800, -6.0893,  6.1800,  5.5579,  4.7828, -6.2800,  4.7986]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 20
	action: tensor([[-4.6855, -5.2704,  6.1800,  6.1800,  5.9490, -6.2800,  4.7768]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 21
	action: tensor([[-6.2800, -6.2800,  5.1482,  6.0351,  6.1800, -6.2800,  5.9046]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3729015667638893, distance: 0.9062010292949322 entropy -1.8704687425011064
epoch: 9, step: 22
	action: tensor([[-6.2800, -4.1202,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.8929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 23
	action: tensor([[-6.2800, -6.2800,  4.5576,  5.3504,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 24
	action: tensor([[-6.2800, -5.9881,  4.2199,  6.1800,  5.5080, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 25
	action: tensor([[-6.2800, -5.1144,  6.1800,  5.9334,  4.3433, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 26
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.4575,  5.9594, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 27
	action: tensor([[-6.1784, -4.9375,  5.8662,  5.6703,  5.1762, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 28
	action: tensor([[-6.2145, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.4578]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 29
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.0217, -6.2800,  4.8872]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 30
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.8429, -6.2800,  5.8651]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 31
	action: tensor([[-6.2800, -6.2258,  5.7832,  6.1800,  5.7828, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 32
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.0372,  6.1556, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 33
	action: tensor([[-6.2800, -5.6392,  6.1800,  6.1800,  6.1800, -6.2800,  4.8346]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 34
	action: tensor([[-5.7062, -6.2800,  6.1800,  6.1800,  5.7500, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 35
	action: tensor([[-5.6415, -4.2843,  5.0073,  4.8496,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 36
	action: tensor([[-5.2967, -6.2800,  6.1800,  6.1800,  5.6544, -6.2800,  5.2427]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 37
	action: tensor([[-6.2800, -6.0676,  5.9898,  4.8751,  6.1800, -6.2800,  4.6424]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 38
	action: tensor([[-5.6159, -5.6002,  6.1800,  5.2468,  5.5858, -6.2800,  6.1323]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 39
	action: tensor([[-6.2800, -6.2800,  4.6101,  6.1637,  5.6079, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35245272488968726, distance: 0.9208575079959093 entropy -1.8704687425011064
epoch: 9, step: 40
	action: tensor([[-6.2800, -4.7115,  6.1800,  6.1800,  3.9105, -6.2800,  5.6612]],
       dtype=torch.float64)
	q_value: tensor([[-15.1371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 41
	action: tensor([[-6.2800, -6.2800,  4.6113,  4.2868,  5.8819, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 42
	action: tensor([[-6.2800, -5.7821,  5.8705,  6.1800,  5.4614, -6.2800,  4.3518]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 43
	action: tensor([[-6.2800, -6.2800,  4.9530,  5.5666,  5.7868, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 44
	action: tensor([[-6.2800, -4.4776,  6.1800,  6.1800,  5.3485, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 45
	action: tensor([[-6.0569, -5.7520,  6.1800,  6.1800,  4.9231, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 46
	action: tensor([[-5.1302, -6.2800,  5.9703,  6.1800,  4.9972, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 47
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.0582,  4.5725, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 48
	action: tensor([[-6.2800, -6.2800,  6.0009,  4.9243,  5.3536, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 49
	action: tensor([[-5.0347, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  3.1006]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6176283779973653, distance: 0.7076190561465392 entropy -1.8704687425011064
epoch: 9, step: 50
	action: tensor([[-6.2800, -6.2800,  5.8928,  6.1800,  5.8937, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-9.5534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 51
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.3313, -6.2800,  6.0127]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 52
	action: tensor([[-6.0936, -6.2800,  5.2197,  6.1800,  6.1010, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 53
	action: tensor([[-6.2800, -6.2800,  5.5232,  6.1669,  4.2358, -6.2800,  5.5844]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 54
	action: tensor([[-6.2800, -4.9391,  5.0682,  5.5386,  6.1800, -6.2800,  5.6636]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 55
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.2931,  4.2586, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 56
	action: tensor([[-6.2800, -5.0393,  5.6182,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 57
	action: tensor([[-6.2800, -5.3299,  4.3962,  6.1800,  4.5910, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 58
	action: tensor([[-6.2800, -6.2800,  5.2154,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 59
	action: tensor([[-5.2549, -6.1586,  6.1800,  5.1490,  4.5081, -6.2800,  6.0711]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 60
	action: tensor([[-6.2800, -6.0446,  4.1216,  5.1067,  6.1800, -6.2800,  6.0306]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 61
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.4279]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 62
	action: tensor([[-4.6617, -5.8129,  6.1800,  5.1583,  6.1800, -6.2800,  5.2704]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 63
	action: tensor([[-6.2800, -5.2036,  6.1800,  5.3051,  5.3716, -6.2800,  5.3657]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 64
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 65
	action: tensor([[-6.2800, -6.0887,  6.1800,  6.1800,  5.8264, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 66
	action: tensor([[-6.2800, -5.2278,  6.1800,  6.1800,  6.1222, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 67
	action: tensor([[-5.7387, -6.2800,  6.1800,  5.8119,  4.9943, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 68
	action: tensor([[-5.1334, -6.0058,  5.7612,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 69
	action: tensor([[-5.6885, -5.4565,  6.1800,  6.0329,  6.1015, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 70
	action: tensor([[-5.9605, -6.2800,  5.7365,  6.1800,  6.1800, -6.2800,  5.4907]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 71
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 72
	action: tensor([[-6.2800, -6.2800,  6.1200,  6.1800,  5.6139, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 73
	action: tensor([[-6.0329, -5.7009,  6.1800,  6.1800,  6.1800, -6.2800,  4.9204]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 74
	action: tensor([[-6.2800, -5.2665,  6.1800,  6.1800,  4.9325, -6.2800,  5.1167]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 75
	action: tensor([[-4.4873, -5.4636,  6.1800,  5.6454,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 76
	action: tensor([[-5.7278, -4.8326,  6.1800,  5.9062,  4.2959, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 77
	action: tensor([[-5.0389, -5.9487,  4.9928,  6.1128,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 78
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.1342, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 79
	action: tensor([[-5.4284, -6.2800,  6.1800,  5.6969,  5.8885, -6.2800,  5.5352]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 80
	action: tensor([[-6.2800, -5.9692,  6.1800,  6.1800,  6.1800, -6.2800,  5.5754]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 81
	action: tensor([[-6.2800, -5.0370,  5.5529,  5.0480,  4.4225, -6.2800,  5.6865]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 82
	action: tensor([[-5.5779, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 83
	action: tensor([[-6.1885, -5.7065,  4.5930,  6.1800,  5.6495, -6.2800,  5.4756]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 84
	action: tensor([[-5.9814, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 85
	action: tensor([[-5.3870, -6.2800,  6.0385,  6.0596,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 86
	action: tensor([[-6.2111, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 87
	action: tensor([[-5.2967, -5.3801,  6.1800,  6.1800,  5.0351, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 88
	action: tensor([[-5.6185, -5.3555,  5.7220,  6.1800,  5.9771, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 89
	action: tensor([[-5.6488, -6.2800,  4.9001,  6.1800,  5.8533, -6.2800,  6.1408]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 90
	action: tensor([[-5.7290, -5.1196,  5.5670,  4.3697,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 91
	action: tensor([[-6.1927, -5.2838,  6.1800,  5.1511,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 92
	action: tensor([[-6.2656, -5.8781,  6.1800,  6.0011,  6.1800, -6.2800,  6.1175]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 93
	action: tensor([[-5.8225, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.8282]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 94
	action: tensor([[-4.4302, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.4267]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 95
	action: tensor([[-5.9308, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.5172]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 96
	action: tensor([[-5.9865, -6.2800,  6.1800,  5.5090,  6.0757, -6.2800,  5.3958]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 97
	action: tensor([[-5.6150, -6.2800,  6.1800,  6.1800,  3.8997, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 98
	action: tensor([[-6.2800, -4.4388,  6.1800,  6.1800,  6.1398, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 99
	action: tensor([[-6.2800, -5.1221,  5.8102,  5.4039,  6.1800, -6.2800,  3.9591]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 100
	action: tensor([[-4.5302, -6.1911,  5.6815,  6.1800,  6.1800, -6.2800,  5.8276]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 101
	action: tensor([[-6.2800, -5.8613,  5.5954,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 102
	action: tensor([[-6.2687, -5.1101,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 103
	action: tensor([[-6.2800, -6.2800,  5.9496,  6.1800,  4.7032, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 104
	action: tensor([[-6.2800, -6.2800,  4.6514,  6.1800,  4.0795, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 105
	action: tensor([[-6.1273, -6.2800,  6.1800,  5.5822,  6.0519, -6.2800,  5.9943]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 106
	action: tensor([[-5.8009, -6.0160,  6.1800,  5.4605,  4.8332, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 107
	action: tensor([[-5.8106, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.8707]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 108
	action: tensor([[-6.1802, -6.2800,  6.1800,  5.3797,  5.8352, -6.2800,  6.0687]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 109
	action: tensor([[-6.2800, -5.8185,  5.2803,  5.3700,  4.4587, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 110
	action: tensor([[-6.2800, -6.0379,  5.9138,  6.1800,  5.2545, -6.2800,  5.9681]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 111
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 112
	action: tensor([[-6.1042, -5.6474,  6.1800,  6.1800,  6.0741, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 113
	action: tensor([[-5.0408, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.1772]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 114
	action: tensor([[-6.2800, -5.6667,  6.1800,  6.1800,  5.4979, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 115
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 116
	action: tensor([[-5.3765, -6.2800,  6.1800,  6.1800,  5.7975, -6.2800,  6.1015]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 117
	action: tensor([[-6.2800, -6.2404,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 118
	action: tensor([[-6.2800, -6.2800,  5.5549,  5.9115,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 119
	action: tensor([[-4.7004, -4.9525,  6.1800,  5.7829,  5.6145, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 120
	action: tensor([[-6.2800, -5.5652,  6.1800,  6.1800,  6.1800, -6.2800,  5.7367]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 121
	action: tensor([[-6.0615, -6.2800,  5.7926,  6.1426,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 122
	action: tensor([[-4.3972, -4.5486,  6.1800,  5.8938,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 123
	action: tensor([[-6.0295, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.7587]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 124
	action: tensor([[-6.2800, -5.5669,  5.3749,  6.1800,  6.0805, -6.2800,  5.9201]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 125
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1464,  5.7373, -6.2800,  5.8488]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 126
	action: tensor([[-5.8281, -6.2800,  6.0399,  6.1800,  6.1525, -6.2800,  5.1732]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 9, step: 127
	action: tensor([[-5.1407, -4.6464,  5.3547,  6.1800,  5.5960, -6.2800,  5.3564]],
       dtype=torch.float64)
	q_value: tensor([[-19.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 9 actor 559.0449935226684 critic 939.702553371942 entropy 50
epoch: 10, step: 0
	action: tensor([[-6.2800, -5.4720,  5.2147,  5.2454,  5.8229, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 1
	action: tensor([[-5.5619, -5.5181,  6.1477,  5.8035,  6.1800, -6.2800,  6.1092]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 2
	action: tensor([[-5.9741, -5.7155,  4.8948,  6.1800,  6.1800, -6.2800,  5.7171]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 3
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.5077,  5.8168, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 4
	action: tensor([[-6.0214, -5.5461,  5.8986,  5.9042,  6.1524, -6.2800,  4.9502]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 5
	action: tensor([[-6.0622, -5.3415,  5.2196,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 6
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  3.1586]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 7
	action: tensor([[-6.2800, -5.8295,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 8
	action: tensor([[-6.2800, -6.2800,  6.0514,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 9
	action: tensor([[-6.2800, -5.6822,  6.1800,  6.1800,  6.1800, -6.2800,  5.9943]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 10
	action: tensor([[-6.2800, -6.2800,  4.8288,  6.1735,  5.8847, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 11
	action: tensor([[-5.8401, -6.2496,  4.3791,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559821319076405, distance: 0.5652850599276671 entropy -1.8704687425011064
epoch: 10, step: 12
	action: tensor([[-5.5893, -6.0594,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-23.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 13
	action: tensor([[-5.0285, -6.1794,  6.1800,  6.1800,  6.1800, -6.2800,  5.2358]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 14
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.3400]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2621685439661563, distance: 0.9829587128444951 entropy -1.8704687425011064
epoch: 10, step: 15
	action: tensor([[-5.5586, -6.2800,  6.1800,  6.1248,  5.5011, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-15.2540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 16
	action: tensor([[-6.2800, -6.1177,  5.5410,  6.1800,  5.6526, -6.2800,  5.2774]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 17
	action: tensor([[-4.8292, -4.6049,  6.0935,  6.1800,  6.1800, -6.2800,  5.8641]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 18
	action: tensor([[-6.2480, -4.9361,  6.1800,  6.1800,  4.5123, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 19
	action: tensor([[-6.2800, -5.8501,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 20
	action: tensor([[-5.8931, -6.2800,  5.2958,  6.1800,  6.1800, -6.2800,  5.4328]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 21
	action: tensor([[-6.2800, -5.8726,  6.1800,  5.2615,  6.1800, -6.2800,  5.4783]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 22
	action: tensor([[-6.0155, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.9746]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 23
	action: tensor([[-5.3168, -5.2404,  6.1800,  6.1800,  5.5903, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 24
	action: tensor([[-6.2042, -6.2800,  6.1800,  5.3293,  4.7543, -6.2800,  6.1263]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 25
	action: tensor([[-6.2737, -4.8298,  5.2898,  6.1800,  6.1800, -6.2800,  6.0801]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 26
	action: tensor([[-5.7166, -6.2800,  6.1800,  6.1800,  5.8291, -6.2800,  5.5177]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 27
	action: tensor([[-6.2800, -5.5542,  5.8051,  5.6160,  6.1800, -6.2800,  5.1871]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 28
	action: tensor([[-6.2800, -6.2800,  5.6685,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 29
	action: tensor([[-6.2800, -6.2800,  5.8728,  6.1800,  6.1800, -6.2800,  5.0300]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 30
	action: tensor([[-4.1406, -5.4070,  4.9372,  6.1800,  6.1800, -6.2800,  5.8988]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 31
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1715, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 32
	action: tensor([[-5.5302, -6.2800,  6.1800,  6.1800,  4.9905, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 33
	action: tensor([[-5.2191, -6.2800,  5.4527,  5.7991,  4.2349, -6.2800,  5.5975]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 34
	action: tensor([[-6.2800, -6.2800,  5.4645,  6.1800,  6.0532, -6.2800,  6.0789]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 35
	action: tensor([[-4.8885, -6.1159,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 36
	action: tensor([[-5.7382, -4.6397,  6.1800,  6.1800,  5.2992, -6.2800,  5.3687]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 37
	action: tensor([[-5.0291, -6.2800,  5.5041,  4.7510,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 38
	action: tensor([[-4.4944, -6.2800,  6.1800,  4.4592,  5.3663, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 39
	action: tensor([[-6.2800, -5.8843,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 40
	action: tensor([[-6.1493, -6.2800,  6.1800,  5.9750,  6.1800, -6.2800,  5.7651]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 41
	action: tensor([[-6.2800, -4.6379,  6.1800,  6.1800,  6.0760, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 42
	action: tensor([[-5.8238, -5.7851,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 43
	action: tensor([[-5.5962, -6.2800,  6.1800,  6.1415,  4.1761, -6.2800,  5.8899]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 44
	action: tensor([[-5.9329, -5.9189,  5.7854,  6.1273,  6.0496, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 45
	action: tensor([[-6.2800, -6.2800,  6.0546,  5.9823,  6.1800, -6.2800,  6.1411]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 46
	action: tensor([[-6.2800, -6.2800,  6.1045,  6.1800,  6.1038, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 47
	action: tensor([[-5.5839, -4.6124,  6.1800,  6.1800,  5.4865, -6.2800,  5.4426]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 48
	action: tensor([[-5.2407, -5.4886,  6.1800,  5.7608,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 49
	action: tensor([[-6.0535, -5.2724,  6.1800,  5.4287,  6.1800, -6.2800,  5.5810]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 50
	action: tensor([[-5.4400, -4.8488,  6.1800,  4.7870,  5.5975, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 51
	action: tensor([[-5.3934, -3.6894,  6.1800,  6.1800,  5.7307, -6.2800,  5.1500]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 52
	action: tensor([[-6.2800, -6.0950,  6.1800,  5.0619,  4.2244, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 53
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.5054,  5.6776, -6.2800,  5.6630]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 54
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.6264,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 55
	action: tensor([[-6.1264, -6.2800,  5.0645,  6.1800,  6.1800, -6.2800,  6.0450]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4398341722124778, distance: 0.8564756082696925 entropy -1.8704687425011064
epoch: 10, step: 56
	action: tensor([[-6.2800, -6.2800,  5.1232,  4.1387,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-18.5391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 57
	action: tensor([[-5.5246, -5.4238,  6.1800,  5.0240,  4.7193, -6.2800,  5.2669]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 58
	action: tensor([[-6.2800, -5.4775,  6.1489,  5.7696,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 59
	action: tensor([[-5.3622, -6.2800,  6.1800,  5.0618,  5.3394, -6.2800,  6.1678]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11241787533468861, distance: 1.0781047683561253 entropy -1.8704687425011064
epoch: 10, step: 60
	action: tensor([[-4.8426, -4.4115,  6.1800,  6.1800,  6.1800, -6.2800,  5.5263]],
       dtype=torch.float64)
	q_value: tensor([[-20.6349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 61
	action: tensor([[-6.2800, -5.4923,  4.9510,  6.1597,  5.4144, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 62
	action: tensor([[-4.4951, -6.2800,  5.7673,  5.4244,  6.1800, -6.2800,  5.7391]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 63
	action: tensor([[-6.2800, -4.8138,  6.1800,  4.8738,  5.5457, -6.2800,  5.7289]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 64
	action: tensor([[-5.5545, -5.7144,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.983703640385952, distance: 0.14608377838131703 entropy -1.8704687425011064
epoch: 10, step: 65
	action: tensor([[-5.5045, -5.8012,  5.9028,  6.0265,  5.7300, -6.2800,  5.5959]],
       dtype=torch.float64)
	q_value: tensor([[-12.5397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 66
	action: tensor([[-6.2800, -6.2800,  3.9464,  5.0109,  6.1800, -6.2800,  4.8884]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 67
	action: tensor([[-6.2800, -6.2800,  5.6691,  4.3076,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 68
	action: tensor([[-6.2800, -6.2800,  6.1679,  5.7123,  5.5619, -6.2800,  5.7658]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 69
	action: tensor([[-6.2800, -5.4471,  6.1800,  6.1800,  5.2646, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 70
	action: tensor([[-6.2800, -6.2800,  4.4020,  5.3413,  5.8610, -6.2800,  6.0176]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 71
	action: tensor([[-5.4201, -4.9930,  4.5242,  6.1800,  4.7748, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 72
	action: tensor([[-6.2800, -5.6875,  5.3996,  6.1800,  6.1800, -6.2800,  5.1639]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 73
	action: tensor([[-5.3491, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.9585]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 74
	action: tensor([[-5.3906, -4.9995,  6.1800,  4.5824,  4.6813, -6.2800,  5.6624]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 75
	action: tensor([[-6.2800, -6.2800,  5.7095,  6.0211,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 76
	action: tensor([[-6.0624, -6.1300,  5.4395,  6.1389,  5.7283, -6.2800,  5.1406]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 77
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.9845,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 78
	action: tensor([[-6.2418, -6.2800,  5.9720,  5.1419,  3.9287, -6.2800,  5.2853]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 79
	action: tensor([[-6.2800, -6.2269,  6.1800,  5.2025,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 80
	action: tensor([[-5.3602, -5.4331,  5.9319,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 81
	action: tensor([[-4.9121, -6.2800,  4.3722,  5.6953,  5.9498, -6.2800,  5.9290]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4792148152507335, distance: 0.825821154584318 entropy -1.8704687425011064
epoch: 10, step: 82
	action: tensor([[-6.2800, -6.2800,  5.9033,  6.1800,  6.1800, -6.2800,  5.7081]],
       dtype=torch.float64)
	q_value: tensor([[-27.7198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 83
	action: tensor([[-6.2800, -5.7318,  5.4658,  5.6758,  6.1800, -6.2800,  5.6344]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 84
	action: tensor([[-5.5297, -6.2800,  4.1374,  6.1800,  6.1800, -6.2800,  5.0391]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 85
	action: tensor([[-6.2800, -5.7142,  6.1800,  6.1800,  5.7422, -6.2800,  6.0273]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 86
	action: tensor([[-5.2200, -6.1138,  5.6634,  5.6525,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 87
	action: tensor([[-6.2800, -5.5728,  5.2563,  4.7230,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 88
	action: tensor([[-4.2099, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.0993]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 89
	action: tensor([[-5.5286, -6.2089,  6.1800,  6.1800,  4.7583, -6.2800,  5.2852]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 90
	action: tensor([[-6.0645, -6.0374,  5.5255,  6.1800,  5.6290, -6.2800,  5.3992]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 91
	action: tensor([[-5.9635, -6.2800,  4.9335,  6.1800,  5.1247, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6173823722153724, distance: 0.707846649381916 entropy -1.8704687425011064
epoch: 10, step: 92
	action: tensor([[-6.2577, -5.2498,  6.0085,  5.6073,  6.1800, -6.2800,  5.4844]],
       dtype=torch.float64)
	q_value: tensor([[-23.7332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 93
	action: tensor([[-6.2800, -6.2800,  5.1661,  5.8643,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 94
	action: tensor([[-6.2800, -6.2800,  4.9956,  6.1800,  6.1800, -6.2800,  5.0088]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 95
	action: tensor([[-6.2800, -5.4805,  6.1795,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 96
	action: tensor([[-3.4410, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.5656]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 97
	action: tensor([[-6.2800, -5.6772,  6.1800,  5.8898,  5.0676, -6.2800,  6.0243]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 98
	action: tensor([[-6.2800, -6.2800,  4.9107,  6.1800,  5.9685, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 99
	action: tensor([[-6.2800, -5.3539,  5.8188,  6.1800,  5.9721, -6.2800,  4.6225]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 100
	action: tensor([[-5.9253, -5.0377,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 101
	action: tensor([[-5.6512, -5.1444,  4.8793,  3.5958,  5.3588, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 102
	action: tensor([[-5.5610, -5.5920,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 103
	action: tensor([[-6.2800, -4.9414,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 104
	action: tensor([[-6.2800, -6.2800,  5.6371,  6.0397,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 105
	action: tensor([[-6.2277, -5.2600,  6.1800,  5.7925,  6.1800, -6.2800,  5.9539]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9548057189100371, distance: 0.24327553397437587 entropy -1.8704687425011064
epoch: 10, step: 106
	action: tensor([[-4.6517, -6.2800,  6.1800,  4.8582,  5.7280, -6.2800,  4.2465]],
       dtype=torch.float64)
	q_value: tensor([[-17.1232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 107
	action: tensor([[-6.2800, -6.2800,  4.8582,  6.1800,  6.1800, -6.2800,  4.8449]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 108
	action: tensor([[-6.2800, -6.2800,  5.6761,  6.1800,  6.1800, -6.2800,  5.5638]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 109
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.5278,  5.5315, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 110
	action: tensor([[-5.2344, -6.2800,  6.1800,  6.1800,  6.1665, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 111
	action: tensor([[-5.9367, -4.5259,  5.9003,  6.1800,  4.9793, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 112
	action: tensor([[-5.6044, -6.2800,  6.1800,  5.9727,  6.1800, -6.2800,  5.1928]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 113
	action: tensor([[-6.0368, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 114
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 115
	action: tensor([[-6.2800, -3.9482,  6.1483,  6.1800,  5.8345, -6.2800,  5.6622]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 116
	action: tensor([[-6.2800, -5.7522,  5.4483,  6.1800,  5.8904, -6.2800,  4.7180]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 117
	action: tensor([[-6.2800, -6.2319,  6.1800,  6.1800,  5.7448, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 118
	action: tensor([[-6.1779, -6.0668,  6.1800,  6.1800,  4.9171, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 119
	action: tensor([[-6.2800, -5.9568,  6.1800,  4.8769,  5.7579, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 120
	action: tensor([[-5.6135, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 121
	action: tensor([[-5.1000, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.9985]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 122
	action: tensor([[-5.5263, -5.1363,  6.1800,  6.1800,  6.0008, -6.2800,  5.4799]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 123
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.8706, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 124
	action: tensor([[-5.8589, -6.2800,  6.1800,  6.1800,  4.3341, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 125
	action: tensor([[-6.2800, -6.2800,  6.1575,  5.5975,  5.4123, -6.2800,  4.9070]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 126
	action: tensor([[-6.1417, -6.2800,  6.1800,  5.8744,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 10, step: 127
	action: tensor([[-6.2800, -6.2800,  5.9555,  5.6455,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-31.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 10 actor 301.707533079721 critic 366.7436871705593 entropy 50
epoch: 11, step: 0
	action: tensor([[-3.4913, -6.2800,  4.2547,  6.1800,  5.7959, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 1
	action: tensor([[-6.2800, -6.2800,  5.2895,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 2
	action: tensor([[-6.2800, -6.1182,  4.9963,  4.7072,  6.1800, -6.2800,  4.8342]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 3
	action: tensor([[-4.9772, -5.4735,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 4
	action: tensor([[-5.2250, -6.2800,  6.1800,  6.1800,  6.1238, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 5
	action: tensor([[-5.2366, -5.6653,  6.1800,  5.6607,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 6
	action: tensor([[-4.6952, -5.5730,  6.1523,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 7
	action: tensor([[-3.1182, -5.2693,  6.1354,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 8
	action: tensor([[-4.7450, -5.5962,  6.0611,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 9
	action: tensor([[-6.2800, -5.6421,  5.3830,  6.1800,  6.0130, -6.2800,  4.6210]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 10
	action: tensor([[-6.2800, -3.2105,  6.1800,  6.1800,  6.1800, -6.2800,  5.4905]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 11
	action: tensor([[-6.2800, -5.0097,  6.1800,  6.1800,  4.2759, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 12
	action: tensor([[-6.0337, -5.5311,  6.1800,  5.5757,  5.8467, -6.2800,  5.9381]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7464957453827477, distance: 0.5761682459481391 entropy -1.8704687425011064
epoch: 11, step: 13
	action: tensor([[-6.2800, -6.2800,  5.5378,  6.1800,  5.1723, -6.2800,  5.9297]],
       dtype=torch.float64)
	q_value: tensor([[-27.6448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2851757005460518, distance: 0.9675119703120808 entropy -1.8704687425011064
epoch: 11, step: 14
	action: tensor([[-5.6539, -6.2800,  6.1800,  6.1800,  6.1203, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-30.2421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6805874795004391, distance: 0.6467443765205165 entropy -1.8704687425011064
epoch: 11, step: 15
	action: tensor([[-4.1320, -6.2800,  6.1800,  5.5378,  5.3861, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-16.3094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 16
	action: tensor([[-5.7765, -6.2800,  4.8159,  6.1800,  5.7159, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 17
	action: tensor([[-4.8400, -6.2800,  5.8355,  5.0753,  6.1800, -6.2800,  5.3381]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 18
	action: tensor([[-4.7531, -6.2800,  6.1800,  5.6876,  6.1800, -6.2800,  4.2165]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 19
	action: tensor([[-6.2800, -6.2800,  5.0303,  6.1800,  5.2403, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32655013424191015, distance: 0.9390945683121329 entropy -1.8704687425011064
epoch: 11, step: 20
	action: tensor([[-5.3364, -6.1017,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-33.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 21
	action: tensor([[-6.2800, -5.5124,  6.1800,  6.1800,  6.1800, -6.2800,  5.9787]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 22
	action: tensor([[-6.2800, -6.2800,  6.1036,  5.3579,  5.0078, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 23
	action: tensor([[-5.7160, -5.4731,  5.6521,  6.1800,  5.2084, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 24
	action: tensor([[-5.8378, -4.3451,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 25
	action: tensor([[-6.2800, -6.0920,  6.1800,  5.2371,  6.1800, -6.2800,  4.1495]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 26
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.6425]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 27
	action: tensor([[-6.1804, -6.2800,  6.1800,  4.3732,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 28
	action: tensor([[-5.6778, -4.4656,  5.8658,  6.1800,  5.6554, -6.2800,  5.0878]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 29
	action: tensor([[-5.4924, -6.2800,  6.1800,  4.9903,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 30
	action: tensor([[-4.5297, -6.2800,  6.1800,  4.7281,  4.9214, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 31
	action: tensor([[-5.9787, -6.2800,  6.1598,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 32
	action: tensor([[-3.8757, -6.2800,  4.9004,  6.1800,  4.4269, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 33
	action: tensor([[-5.4195, -6.2800,  6.1800,  6.1800,  5.4117, -6.2800,  6.0100]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 34
	action: tensor([[-6.0860, -6.2800,  6.1800,  5.9532,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 35
	action: tensor([[-6.2800, -6.2641,  5.4659,  6.1800,  5.9371, -6.2800,  5.9558]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21568597813902335, distance: 1.0134484970019042 entropy -1.8704687425011064
epoch: 11, step: 36
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.9553,  6.1800, -6.2800,  4.3503]],
       dtype=torch.float64)
	q_value: tensor([[-25.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 37
	action: tensor([[-6.2800, -4.5548,  5.4577,  5.3098,  6.0827, -6.2800,  5.5247]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 38
	action: tensor([[-4.2239, -6.2800,  6.1800,  5.4217,  4.1244, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 39
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0814,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 40
	action: tensor([[-5.4917, -6.2800,  6.1800,  5.4598,  4.2482, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 41
	action: tensor([[-6.2800, -5.4223,  6.1800,  5.4997,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 42
	action: tensor([[-6.0718, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.5322]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 43
	action: tensor([[-4.6828, -6.2800,  6.1800,  6.1800,  6.0320, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 44
	action: tensor([[-4.1734, -4.5399,  6.1800,  6.1800,  6.1800, -6.2800,  5.4900]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 45
	action: tensor([[-5.6710, -4.7844,  6.0795,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 46
	action: tensor([[-5.1918, -5.2818,  5.7756,  6.1800,  6.1800, -6.2800,  4.2302]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 47
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.8960, -6.2800,  4.4101]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 48
	action: tensor([[-6.2800, -5.7960,  6.1800,  5.9534,  6.1800, -6.2800,  5.5204]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 49
	action: tensor([[-5.7739, -5.5996,  6.1800,  6.1800,  5.8623, -6.2800,  5.4970]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 50
	action: tensor([[-5.0957, -6.2800,  6.0400,  6.1775,  6.0205, -6.2800,  5.4783]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 51
	action: tensor([[-6.0415, -4.5546,  6.0306,  4.5126,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 52
	action: tensor([[-6.2800, -6.0290,  6.1800,  6.0702,  5.3747, -6.2800,  5.4430]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 53
	action: tensor([[-5.9717, -4.8483,  6.0493,  6.0361,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 54
	action: tensor([[-6.2800, -4.3343,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 55
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.8525,  6.1800, -6.2800,  4.0689]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 56
	action: tensor([[-3.4606, -6.2800,  6.0362,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 57
	action: tensor([[-6.2800, -6.2800,  5.8703,  5.4511,  6.1536, -6.2800,  5.3931]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 58
	action: tensor([[-5.9795, -6.2800,  6.1800,  6.1800,  6.1027, -6.2800,  4.7440]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 59
	action: tensor([[-4.5440, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 60
	action: tensor([[-6.2800, -6.2800,  5.6200,  6.1800,  6.1475, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 61
	action: tensor([[-6.2525, -5.7574,  6.1800,  6.1800,  5.3652, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 62
	action: tensor([[-6.2800, -4.9907,  5.5119,  5.9944,  6.1800, -6.2800,  5.3313]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 63
	action: tensor([[-4.9139, -6.2800,  5.5153,  6.1791,  6.1800, -6.2800,  5.7094]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 64
	action: tensor([[-4.9995, -6.2800,  4.4988,  6.1800,  6.0575, -6.2800,  5.5200]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6327404452321446, distance: 0.6934948574710621 entropy -1.8704687425011064
epoch: 11, step: 65
	action: tensor([[-6.2667, -6.2800,  5.9430,  6.1800,  5.7750, -6.2800,  5.8330]],
       dtype=torch.float64)
	q_value: tensor([[-39.4641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 66
	action: tensor([[-5.0461, -6.2800,  4.9726,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6596625488216159, distance: 0.6675926772405665 entropy -1.8704687425011064
epoch: 11, step: 67
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1495,  6.0238, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-28.4672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 68
	action: tensor([[-6.2800, -6.2800,  5.4283,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 69
	action: tensor([[-6.2800, -6.2800,  5.6697,  5.9759,  6.1800, -6.2800,  5.9262]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18109719423978154, distance: 1.035554291587063 entropy -1.8704687425011064
epoch: 11, step: 70
	action: tensor([[-5.3914, -4.9523,  6.1800,  6.1800,  5.8491, -6.2800,  5.7962]],
       dtype=torch.float64)
	q_value: tensor([[-23.1520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 71
	action: tensor([[-6.2800, -5.4243,  6.1800,  5.9205,  5.9311, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 72
	action: tensor([[-6.2800, -6.0358,  5.4689,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 73
	action: tensor([[-6.2800, -6.2800,  6.0881,  5.0115,  5.3367, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 74
	action: tensor([[-6.2800, -6.2800,  5.6075,  6.0666,  5.6071, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 75
	action: tensor([[-5.4253, -6.2548,  6.1800,  6.1800,  4.6163, -6.2800,  5.7125]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7518274221172391, distance: 0.5700770914642309 entropy -1.8704687425011064
epoch: 11, step: 76
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0735,  5.9994, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-32.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 77
	action: tensor([[-6.1514, -5.4109,  4.3959,  4.6064,  5.7864, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 78
	action: tensor([[-4.3696, -6.2800,  6.0928,  5.3336,  5.2757, -6.2800,  5.0530]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 79
	action: tensor([[-6.2800, -6.2800,  5.9218,  5.7516,  5.7667, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 80
	action: tensor([[-6.2800, -5.8800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 81
	action: tensor([[-5.9873, -6.2800,  5.3472,  6.1800,  5.3635, -6.2800,  6.1469]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 82
	action: tensor([[-6.2800, -5.8092,  6.1800,  6.1800,  5.8087, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 83
	action: tensor([[-5.6686, -6.2800,  3.7900,  6.1800,  6.1800, -6.2800,  5.1902]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 84
	action: tensor([[-6.2800, -5.7730,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 85
	action: tensor([[-5.8962, -6.2800,  6.1800,  5.3229,  6.0665, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 86
	action: tensor([[-6.2800, -5.2546,  4.9065,  6.1800,  5.9033, -6.2800,  5.8693]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 87
	action: tensor([[-5.2908, -6.2800,  6.1800,  5.7555,  6.1800, -6.2800,  4.1174]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 88
	action: tensor([[-6.1718, -5.4644,  5.7266,  5.3421,  6.1800, -6.2800,  4.5999]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 89
	action: tensor([[-5.0734, -6.2800,  5.9568,  6.1800,  4.8158, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 90
	action: tensor([[-6.2800, -5.4108,  5.4604,  4.7243,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 91
	action: tensor([[-6.2800, -4.6807,  6.1800,  5.0984,  5.2771, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 92
	action: tensor([[-6.2800, -5.4359,  6.1800,  6.1800,  5.6926, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 93
	action: tensor([[-6.2800, -5.9131,  6.1800,  6.1800,  4.6116, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 94
	action: tensor([[-6.2800, -5.1354,  6.1800,  5.6862,  6.1800, -6.2800,  5.7972]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 95
	action: tensor([[-4.6910, -5.6765,  6.1800,  4.7222,  5.3483, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 96
	action: tensor([[-5.1274, -5.4463,  3.8529,  5.1973,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 97
	action: tensor([[-6.2800, -5.8809,  6.1800,  5.5832,  5.6889, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 98
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 99
	action: tensor([[-6.2302, -6.2800,  6.1800,  5.2525,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 100
	action: tensor([[-5.5662, -6.0854,  4.6728,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 101
	action: tensor([[-6.2800, -6.2800,  5.2923,  6.1800,  6.1800, -6.2800,  6.1667]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 102
	action: tensor([[-5.3453, -6.2800,  6.1800,  5.9462,  5.4688, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 103
	action: tensor([[-4.3608, -6.2800,  6.0649,  6.1800,  6.1800, -6.2800,  5.5743]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 104
	action: tensor([[-4.8322, -6.2800,  6.0841,  6.1800,  5.6230, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49655209148545276, distance: 0.8119587452535544 entropy -1.8704687425011064
epoch: 11, step: 105
	action: tensor([[-6.2800, -5.8783,  5.9690,  5.3488,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-23.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 106
	action: tensor([[-6.2630, -5.3585,  6.1800,  6.1755,  5.8347, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 107
	action: tensor([[-5.9159, -6.2800,  6.1800,  5.8855,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 108
	action: tensor([[-5.5144, -5.7365,  5.5190,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 109
	action: tensor([[-4.9303, -4.6188,  6.1800,  4.8008,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 110
	action: tensor([[-6.2800, -5.6079,  6.1800,  4.7789,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 111
	action: tensor([[-6.0458, -6.2800,  6.1800,  4.2593,  5.5427, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 112
	action: tensor([[-5.9081, -6.2800,  6.0332,  5.2939,  6.1800, -6.2800,  5.5387]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 113
	action: tensor([[-5.1662, -6.2800,  4.3619,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7317569498042558, distance: 0.5926809013412437 entropy -1.8704687425011064
epoch: 11, step: 114
	action: tensor([[-5.5140, -6.0493,  4.5279,  6.1543,  4.4307, -6.2800,  4.1087]],
       dtype=torch.float64)
	q_value: tensor([[-35.0873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 115
	action: tensor([[-5.4277, -5.8032,  6.1800,  5.8306,  5.6994, -6.2800,  4.6489]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 116
	action: tensor([[-5.4543, -6.2800,  6.1800,  5.9674,  4.7911, -6.2800,  4.9684]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 117
	action: tensor([[-6.2800, -6.2800,  4.3537,  6.1800,  4.2338, -6.2800,  5.0929]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38661023336997113, distance: 0.8962413071805663 entropy -1.8704687425011064
epoch: 11, step: 118
	action: tensor([[-6.2800, -6.2800,  5.4813,  5.9149,  6.1155, -6.2800,  5.6508]],
       dtype=torch.float64)
	q_value: tensor([[-59.0688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 119
	action: tensor([[-6.2800, -4.7148,  6.1800,  5.7216,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 120
	action: tensor([[-6.1620, -5.6468,  6.1800,  4.4572,  6.0918, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 121
	action: tensor([[-5.9051, -6.2800,  4.4555,  6.1800,  6.1800, -6.2800,  5.2343]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 122
	action: tensor([[-6.2800, -5.5758,  5.5268,  5.8580,  5.0274, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 123
	action: tensor([[-6.2629, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.7440]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 124
	action: tensor([[-5.1797, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.5440]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 125
	action: tensor([[-4.5791, -5.5091,  5.5532,  5.7079,  5.7806, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 126
	action: tensor([[-5.7284, -6.2800,  5.8008,  5.9694,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 11, step: 127
	action: tensor([[-6.2736, -6.2800,  6.1800,  5.5779,  6.1800, -6.2800,  5.2544]],
       dtype=torch.float64)
	q_value: tensor([[-48.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 11 actor 128.90713141098325 critic 43.168395257832735 entropy 5
epoch: 12, step: 0
	action: tensor([[-6.2800, -6.2800,  5.2196,  6.1800,  6.1800, -6.2800,  4.9154]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 1
	action: tensor([[-5.9733, -4.7896,  5.5907,  5.9004,  4.0342, -6.2800,  5.9169]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 2
	action: tensor([[-6.2800, -5.4840,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 3
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.9247,  4.6498, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 4
	action: tensor([[-5.9153, -3.8669,  6.1800,  6.1800,  6.1800, -6.2800,  5.2301]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 5
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.7378,  6.0741, -6.2800,  5.5007]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 6
	action: tensor([[-6.1679, -5.9203,  5.2408,  5.7456,  6.1800, -6.2800,  4.1268]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 7
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  4.6465, -6.2800,  5.6165]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 8
	action: tensor([[-6.2800, -6.2800,  5.6435,  5.7254,  5.7472, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 9
	action: tensor([[-4.9430, -6.2800,  6.1800,  5.5571,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 10
	action: tensor([[-4.2145, -4.8352,  4.4792,  5.6608,  5.6596, -6.2800,  5.5675]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 11
	action: tensor([[-3.7516, -4.4596,  6.1800,  5.6765,  5.6936, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 12
	action: tensor([[-6.2800, -3.6534,  6.1800,  4.2145,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 13
	action: tensor([[-5.9927, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 14
	action: tensor([[-5.6062, -6.2800,  6.1800,  5.9886,  6.0971, -6.2800,  5.5000]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 15
	action: tensor([[-5.7581, -5.4856,  6.1800,  6.1800,  6.1800, -6.2800,  5.1598]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 16
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.9805, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 17
	action: tensor([[-6.2800, -6.2800,  6.1338,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 18
	action: tensor([[-6.1803, -5.3100,  5.7721,  5.8406,  6.1800, -6.2800,  5.1999]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 19
	action: tensor([[-4.9688, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 20
	action: tensor([[-6.2800, -5.2892,  6.1800,  4.5867,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 21
	action: tensor([[-5.9077, -5.7995,  4.7290,  5.7395,  6.1800, -6.2800,  5.7412]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 22
	action: tensor([[-5.7522, -5.7326,  5.2365,  5.3742,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 23
	action: tensor([[-6.2800, -6.2800,  4.6738,  5.2031,  5.5687, -6.2800,  4.7474]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 24
	action: tensor([[-6.2800, -5.9175,  6.1800,  6.0356,  4.6307, -6.2800,  5.9148]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 25
	action: tensor([[-6.0480, -5.2027,  6.1800,  6.1800,  5.1684, -6.2800,  5.9554]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 26
	action: tensor([[-3.8445, -4.9948,  5.8745,  6.1800,  5.6258, -6.2800,  5.7137]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 27
	action: tensor([[-6.2800, -6.2559,  6.1800,  5.2209,  5.4683, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 28
	action: tensor([[-5.1338, -6.2800,  6.1800,  4.8348,  6.1800, -6.2800,  5.4885]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 29
	action: tensor([[-5.9587, -6.2489,  5.9756,  5.4014,  6.1800, -6.2800,  5.0576]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 30
	action: tensor([[-4.4240, -6.2800,  6.1800,  6.1800,  6.1128, -6.2800,  5.9482]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 31
	action: tensor([[-6.2800, -5.0833,  5.4861,  6.0669,  5.8887, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 32
	action: tensor([[-6.2800, -6.2800,  5.9897,  5.6716,  6.1800, -6.2800,  5.9594]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 33
	action: tensor([[-6.2800, -5.7315,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 34
	action: tensor([[-4.9475, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5604963209816716, distance: 0.7586438659923148 entropy -1.8704687425011064
epoch: 12, step: 35
	action: tensor([[-4.4665, -4.8940,  5.9342,  6.1800,  5.8088, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-25.6881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 36
	action: tensor([[-5.7573, -6.2800,  5.4467,  5.7912,  6.1800, -6.2800,  5.6750]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 37
	action: tensor([[-6.2800, -6.2800,  5.6387,  6.0236,  4.4123, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 38
	action: tensor([[-6.2363, -5.4288,  5.9919,  5.8126,  6.1800, -6.2800,  3.6532]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 39
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.0998,  6.1800, -6.2800,  5.7663]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 40
	action: tensor([[-5.1470, -6.2800,  5.1103,  4.8460,  5.9766, -6.2800,  4.9166]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 41
	action: tensor([[-5.1926, -4.5902,  6.1800,  6.1800,  6.1800, -6.2800,  3.9293]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 42
	action: tensor([[-5.2745, -5.1738,  6.1800,  5.5123,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 43
	action: tensor([[-6.2800, -5.1152,  5.9919,  3.4428,  6.1800, -6.2800,  5.7319]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 44
	action: tensor([[-6.1721, -6.2800,  5.9100,  6.1800,  4.6232, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 45
	action: tensor([[-6.2800, -5.9532,  6.1589,  6.1800,  6.1800, -6.2800,  5.3359]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 46
	action: tensor([[-6.2268, -4.0213,  6.1800,  6.0957,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 47
	action: tensor([[-5.0515, -6.2800,  6.1800,  5.2950,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 48
	action: tensor([[-6.2800, -5.5941,  5.6235,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 49
	action: tensor([[-4.6060, -6.2800,  4.9289,  6.1800,  5.2836, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 50
	action: tensor([[-6.2211, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.6158]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 51
	action: tensor([[-4.8324, -6.2800,  6.1800,  5.9740,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4100272101165999, distance: 0.8789672308946133 entropy -1.8704687425011064
epoch: 12, step: 52
	action: tensor([[-5.9909, -6.2800,  5.7139,  5.1991,  6.0501, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-27.4185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 53
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 54
	action: tensor([[-5.0068, -6.2800,  5.5898,  6.1800,  5.6052, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 55
	action: tensor([[-5.2842, -5.1852,  6.1800,  6.1619,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 56
	action: tensor([[-5.4707, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.2351]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 57
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  5.6170]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 58
	action: tensor([[-6.2800, -4.5155,  5.9325,  5.5136,  5.3007, -6.2800,  5.9607]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 59
	action: tensor([[-6.2800, -5.5630,  6.1800,  6.1800,  4.7873, -6.2800,  4.5024]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 60
	action: tensor([[-6.2800, -5.9639,  6.1800,  6.1800,  6.1541, -6.2800,  5.7865]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 61
	action: tensor([[-6.2800, -6.2800,  4.3082,  6.1800,  4.5441, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38524490932747524, distance: 0.8972382097227031 entropy -1.8704687425011064
epoch: 12, step: 62
	action: tensor([[-6.2027, -5.3292,  6.1800,  6.1800,  6.1800, -6.2800,  6.0865]],
       dtype=torch.float64)
	q_value: tensor([[-63.7766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 63
	action: tensor([[-6.2800, -6.2800,  5.9089,  6.1800,  6.1800, -6.2800,  5.3229]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 64
	action: tensor([[-4.3484, -6.2800,  6.1800,  6.0024,  5.7782, -6.2800,  5.8104]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 65
	action: tensor([[-5.0913, -5.6870,  3.9345,  5.1452,  6.1800, -6.2800,  5.8572]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 66
	action: tensor([[-6.2038, -6.2800,  5.8095,  4.5335,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 67
	action: tensor([[-4.3427, -6.2800,  6.1800,  6.1800,  5.9438, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 68
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.8679,  6.1800, -6.2800,  5.0209]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 69
	action: tensor([[-5.7161, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 70
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.9578,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 71
	action: tensor([[-4.8694, -6.2800,  6.1800,  6.1800,  4.9467, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 72
	action: tensor([[-6.2800, -5.7325,  5.4104,  6.1800,  5.4976, -6.2800,  5.5689]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 73
	action: tensor([[-6.1819, -6.2800,  6.1800,  6.1800,  5.6588, -6.2800,  5.8238]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 74
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 75
	action: tensor([[-3.6432, -5.9554,  6.1800,  4.7070,  6.1800, -6.2800,  5.4939]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 76
	action: tensor([[-6.2800, -5.3663,  4.3193,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 77
	action: tensor([[-6.2800, -4.6839,  5.6123,  6.1800,  5.8572, -6.2800,  5.7985]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 78
	action: tensor([[-6.2800, -5.8397,  6.1494,  6.1258,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 79
	action: tensor([[-5.4991, -6.2800,  6.1800,  6.1800,  5.0923, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 80
	action: tensor([[-4.8409, -6.2800,  6.1800,  5.6019,  6.1800, -6.2800,  5.7201]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 81
	action: tensor([[-4.6272, -5.2437,  5.0154,  5.8522,  5.2726, -6.2800,  5.6270]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 82
	action: tensor([[-5.0468, -5.8181,  6.1800,  5.9707,  6.0408, -6.2800,  5.4380]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 83
	action: tensor([[-5.3266, -6.0706,  6.1800,  4.5466,  5.3518, -6.2800,  5.6562]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 84
	action: tensor([[-5.5055, -5.4649,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 85
	action: tensor([[-5.1060, -6.2800,  5.5336,  5.3282,  3.8519, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 86
	action: tensor([[-5.6900, -6.2800,  5.5836,  6.1800,  6.0627, -6.2800,  5.7445]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 87
	action: tensor([[-5.2913, -5.1736,  6.1800,  5.9213,  6.1800, -6.2800,  5.3157]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 88
	action: tensor([[-6.2800, -5.7238,  6.1561,  4.6249,  5.9547, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19265734753970876, distance: 1.0282190405132863 entropy -1.8704687425011064
epoch: 12, step: 89
	action: tensor([[-6.2800, -6.2800,  5.0252,  6.1800,  6.1800, -6.2800,  6.0167]],
       dtype=torch.float64)
	q_value: tensor([[-44.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40832990455903295, distance: 0.8802306828110196 entropy -1.8704687425011064
epoch: 12, step: 90
	action: tensor([[-5.4419, -6.2800,  5.5902,  6.1800,  4.8056, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-38.1124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 91
	action: tensor([[-6.2800, -6.2800,  5.5311,  5.9371,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 92
	action: tensor([[-4.6939, -6.2800,  5.4989,  6.1800,  6.1800, -6.2800,  4.8592]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 93
	action: tensor([[-6.0572, -5.8032,  4.7299,  4.5832,  6.0337, -6.2800,  5.9582]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 94
	action: tensor([[-6.2800, -6.2800,  5.8511,  5.6763,  6.1800, -6.2800,  5.7218]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 95
	action: tensor([[-6.2800, -5.8551,  6.1800,  6.1800,  5.6334, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 96
	action: tensor([[-5.7633, -5.1362,  6.1800,  5.3286,  5.7130, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 97
	action: tensor([[-5.9288, -6.2800,  5.9596,  5.9814,  6.1800, -6.2800,  5.8929]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 98
	action: tensor([[-6.2800, -6.2800,  5.4136,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 99
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  5.2772, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 100
	action: tensor([[-6.0295, -6.2800,  4.7121,  5.7073,  4.8750, -6.2800,  5.6850]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7111327060566915, distance: 0.6150435661598882 entropy -1.8704687425011064
epoch: 12, step: 101
	action: tensor([[-6.2800, -6.2800,  4.5383,  6.0003,  6.1358, -6.2800,  5.8623]],
       dtype=torch.float64)
	q_value: tensor([[-62.2739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5362215292511454, distance: 0.7793131156544545 entropy -1.8704687425011064
epoch: 12, step: 102
	action: tensor([[-6.2800, -4.9082,  6.1800,  4.8309,  6.1800, -6.2800,  5.3907]],
       dtype=torch.float64)
	q_value: tensor([[-48.9897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 103
	action: tensor([[-6.2800, -4.6849,  6.1800,  5.3954,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 104
	action: tensor([[-6.2800, -6.2800,  5.5570,  6.1800,  6.1800, -6.2800,  3.8398]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 105
	action: tensor([[-3.8879, -6.2800,  6.1800,  4.7032,  5.5086, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 106
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.3963,  6.0573, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 107
	action: tensor([[-6.2800, -4.8127,  6.1800,  6.1800,  5.9619, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 108
	action: tensor([[-6.0455, -6.2800,  6.1800,  6.1800,  5.4879, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 109
	action: tensor([[-6.2800, -6.2800,  5.5799,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 110
	action: tensor([[-5.5329, -6.1143,  6.1800,  6.1800,  6.1800, -6.2800,  5.9718]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 111
	action: tensor([[-5.6345, -5.2152,  5.0637,  6.1800,  5.4533, -6.2800,  6.1074]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 112
	action: tensor([[-6.2800, -4.6467,  6.1800,  6.1800,  6.0053, -6.2800,  6.0124]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 113
	action: tensor([[-6.2800, -5.4021,  5.9928,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 114
	action: tensor([[-6.2800, -5.6200,  6.1800,  6.1800,  5.7525, -6.2800,  4.1729]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 115
	action: tensor([[-5.6026, -6.2800,  5.8314,  6.1800,  5.7755, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 116
	action: tensor([[-5.7708, -6.2800,  6.1117,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 117
	action: tensor([[-6.2800, -6.2800,  5.4567,  6.1800,  6.1800, -6.2800,  6.0780]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 118
	action: tensor([[-5.2287, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 119
	action: tensor([[-6.2800, -5.9169,  6.1800,  5.5364,  5.8626, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 120
	action: tensor([[-5.1587, -5.8295,  6.1218,  6.1800,  5.8410, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 121
	action: tensor([[-5.1603, -6.2800,  6.1726,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 122
	action: tensor([[-6.2800, -6.1332,  5.9727,  5.0792,  5.6457, -6.2800,  4.4970]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 123
	action: tensor([[-6.2800, -6.2800,  6.1800,  4.7173,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 124
	action: tensor([[-3.8573, -6.1806,  5.4201,  5.4607,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 125
	action: tensor([[-6.2800, -5.4426,  5.4153,  4.2075,  6.1800, -6.2800,  5.0346]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 126
	action: tensor([[-6.2800, -6.0136,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 12, step: 127
	action: tensor([[-6.2800, -3.6561,  5.4753,  6.1800,  6.1391, -6.2800,  5.5398]],
       dtype=torch.float64)
	q_value: tensor([[-64.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
LOSS epoch 12 actor 296.74399820662313 critic 207.71742139133386 entropy 50
epoch: 13, step: 0
	action: tensor([[-6.0201, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 1
	action: tensor([[-6.2800, -6.2800,  5.2576,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 2
	action: tensor([[-6.2800, -5.6534,  6.1800,  5.9369,  6.1800, -6.2800,  5.9408]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 3
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.8058]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 4
	action: tensor([[-5.1977, -5.8060,  6.1800,  6.0447,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9163580956405661, distance: 0.3309547734446904 entropy -1.8704687425011064
epoch: 13, step: 5
	action: tensor([[-6.2800, -5.0897,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-28.3439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 6
	action: tensor([[-6.0611, -6.2800,  6.1800,  5.5200,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 7
	action: tensor([[-6.2800, -6.2800,  5.0997,  5.9827,  6.1800, -6.2800,  4.2225]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 8
	action: tensor([[-6.2800, -5.8560,  4.2752,  6.1800,  6.1800, -6.2800,  5.4663]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 9
	action: tensor([[-5.8286, -6.2800,  5.5446,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 10
	action: tensor([[-5.6869, -6.2800,  6.1800,  5.4110,  6.1800, -6.2800,  5.5284]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 11
	action: tensor([[-4.2626, -6.2800,  6.1800,  6.1800,  6.1628, -6.2800,  5.9835]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 12
	action: tensor([[-5.9645, -4.8303,  5.7444,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 13
	action: tensor([[-4.7578, -6.2800,  6.1800,  5.5668,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 14
	action: tensor([[-6.2800, -5.5102,  5.9768,  4.8894,  5.6843, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 15
	action: tensor([[-6.2800, -4.3530,  6.1800,  6.1800,  5.6958, -6.2800,  4.9168]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 16
	action: tensor([[-6.1349, -4.4886,  6.1800,  5.9014,  6.0693, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 17
	action: tensor([[-6.0164, -5.0437,  6.1800,  5.6973,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 18
	action: tensor([[-5.1622, -5.4644,  6.1800,  5.3354,  6.1800, -6.2800,  4.7156]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8780046560387418, distance: 0.3996946371287079 entropy -1.8704687425011064
epoch: 13, step: 19
	action: tensor([[-6.2800, -6.1230,  6.1800,  6.1800,  4.1799, -6.2800,  5.7987]],
       dtype=torch.float64)
	q_value: tensor([[-56.5844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33280886933669573, distance: 0.9347206244895794 entropy -1.8704687425011064
epoch: 13, step: 20
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.4024,  5.5343, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.2611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029642397536278065, distance: 1.1611809488127387 entropy -1.8704687425011064
epoch: 13, step: 21
	action: tensor([[-6.2800, -6.2800,  5.9747,  6.1800,  6.1800, -6.2800,  5.9249]],
       dtype=torch.float64)
	q_value: tensor([[-34.0951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28760655867409046, distance: 0.9658654907351014 entropy -1.8704687425011064
epoch: 13, step: 22
	action: tensor([[-4.1314, -6.0209,  6.1800,  4.0074,  5.2040, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-25.4510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 23
	action: tensor([[-6.2800, -5.9553,  6.1800,  6.1599,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 24
	action: tensor([[-5.5349, -5.4078,  5.0787,  5.9204,  5.7684, -6.2800,  5.2890]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 25
	action: tensor([[-6.2800, -6.2800,  5.4111,  5.7944,  6.1800, -6.2800,  5.4788]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 26
	action: tensor([[-6.2800, -6.1852,  5.1695,  5.2367,  6.1800, -6.2800,  5.9752]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 27
	action: tensor([[-6.2800, -4.6877,  4.7348,  6.1800,  5.6530, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 28
	action: tensor([[-3.6946, -5.5395,  6.1800,  6.1800,  5.9150, -6.2800,  5.1159]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 29
	action: tensor([[-6.0707, -6.2800,  5.6219,  4.4137,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 30
	action: tensor([[-5.6046, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  4.7597]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 31
	action: tensor([[-6.2800, -6.2800,  4.8755,  6.1800,  5.6170, -6.2800,  4.3686]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32103817280546776, distance: 0.9429298238802617 entropy -1.8704687425011064
epoch: 13, step: 32
	action: tensor([[-6.1056, -6.2800,  5.6334,  4.2472,  5.8916, -6.2800,  5.6680]],
       dtype=torch.float64)
	q_value: tensor([[-67.5523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 33
	action: tensor([[-3.8355, -6.2800,  4.7592,  6.1800,  6.1800, -6.2800,  5.6655]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 34
	action: tensor([[-6.2800, -4.3462,  5.6549,  6.1800,  6.1800, -6.2800,  5.8278]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 35
	action: tensor([[-5.9169, -6.1628,  6.1800,  4.8366,  5.2595, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 36
	action: tensor([[-6.2800, -5.4584,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 37
	action: tensor([[-6.2465, -5.2311,  6.1800,  4.9280,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 38
	action: tensor([[-4.8049, -6.1482,  6.1800,  4.7608,  6.1800, -6.2800,  5.9200]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 39
	action: tensor([[-6.2800, -5.6750,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 40
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.6747,  4.9742, -6.2800,  5.3523]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 41
	action: tensor([[-6.2800, -3.9305,  6.1800,  6.0649,  6.1800, -6.2800,  5.3407]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 42
	action: tensor([[-5.8077, -6.2800,  6.1800,  5.3407,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 43
	action: tensor([[-4.5410, -5.9816,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 44
	action: tensor([[-6.2800, -5.0885,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 45
	action: tensor([[-4.7945, -5.4562,  5.8924,  5.0351,  6.1204, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 46
	action: tensor([[-6.2800, -5.4631,  6.1800,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 47
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.3656,  6.1800, -6.2800,  5.1102]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 48
	action: tensor([[-5.6192, -4.5195,  5.1244,  6.1800,  5.2932, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 49
	action: tensor([[-6.2800, -6.2800,  6.1800,  5.7026,  4.2465, -6.2800,  5.8111]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 50
	action: tensor([[-6.2800, -5.2451,  6.0181,  6.1800,  6.1800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
epoch: 13, step: 51
	action: tensor([[-6.2800, -6.2800,  6.1800,  6.1800,  6.1800, -6.2800,  6.1686]],
       dtype=torch.float64)
	q_value: tensor([[-63.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011064
