epoch: 0, step: 0
	action: tensor([[-0.0009,  0.0098,  0.0083, -0.0059, -0.0152, -0.0392,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-0.0111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2831207452373047, distance: 0.9689016593667666 entropy -8.420535610629802
epoch: 0, step: 1
	action: tensor([[-0.0113,  0.0158,  0.0134, -0.0088, -0.0048,  0.0030, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2851299552814899, distance: 0.9675429278375971 entropy -11.118859461872935
epoch: 0, step: 2
	action: tensor([[-0.0119,  0.0162,  0.0198, -0.0080,  0.0018, -0.0021, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2840513828211897, distance: 0.9682725512399768 entropy -11.143607696424855
epoch: 0, step: 3
	action: tensor([[-0.0119,  0.0162,  0.0221, -0.0080, -0.0107,  0.0021,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28494680566472264, distance: 0.9676668620891579 entropy -11.149470915051975
epoch: 0, step: 4
	action: tensor([[-0.0118,  0.0159,  0.0074, -0.0082, -0.0069,  0.0175,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2884195718952355, distance: 0.9653141903303778 entropy -11.140789584256648
epoch: 0, step: 5
	action: tensor([[-0.0120,  0.0162,  0.0084, -0.0078, -0.0154,  0.0145, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[0.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28786986020668315, distance: 0.9656869816751766 entropy -11.144776541846179
epoch: 0, step: 6
	action: tensor([[-0.0120,  0.0164,  0.0063, -0.0078, -0.0069, -0.0077,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28287158095274456, distance: 0.9690700243415615 entropy -11.14536469205366
epoch: 0, step: 7
	action: tensor([[-0.0117,  0.0161,  0.0212, -0.0083, -0.0049,  0.0067, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2860836282988597, distance: 0.9668973367022532 entropy -11.136246234000485
epoch: 0, step: 8
	action: tensor([[-0.0120,  0.0161,  0.0118, -0.0079, -0.0063, -0.0129,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28145474400087245, distance: 0.9700268520749317 entropy -11.149380293758762
epoch: 0, step: 9
	action: tensor([[-0.0115,  0.0157,  0.0123, -0.0085, -0.0113,  0.0008, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845205282692169, distance: 0.9679552553544065 entropy -11.12718471143007
epoch: 0, step: 10
	action: tensor([[-0.0119,  0.0163,  0.0180, -0.0080, -0.0093,  0.0002,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2846048927671597, distance: 0.9678981863050855 entropy -11.146374222590824
epoch: 0, step: 11
	action: tensor([[-0.0117,  0.0158,  0.0028, -0.0083, -0.0024, -0.0062, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[0.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2829263625533993, distance: 0.9690330098964882 entropy -11.135416597361466
epoch: 0, step: 12
	action: tensor([[-0.0118,  0.0164,  0.0114, -0.0081, -0.0056,  0.0028,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[0.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28541723176897604, distance: 0.9673485007065172 entropy -11.139837695027442
epoch: 0, step: 13
	action: tensor([[-0.0117,  0.0158,  0.0104, -0.0083, -0.0106,  0.0001,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843471082904222, distance: 0.9680725561407716 entropy -11.133746677036225
epoch: 0, step: 14
	action: tensor([[-0.0118,  0.0162,  0.0110, -0.0081, -0.0059, -0.0117,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[0.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2818498436475896, distance: 0.9697601256841043 entropy -11.14152746557967
epoch: 0, step: 15
	action: tensor([[-0.0116,  0.0160,  0.0041, -0.0084, -0.0106,  0.0070, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28617559537372117, distance: 0.9668350565898864 entropy -11.133684297829726
epoch: 0, step: 16
	action: tensor([[-0.0120,  0.0165,  0.0163, -0.0079, -0.0098,  0.0254,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2905992408073099, distance: 0.9638346112266756 entropy -11.143090877591856
epoch: 0, step: 17
	action: tensor([[-0.0119,  0.0158, -0.0017, -0.0080, -0.0036, -0.0064,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[0.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828400752959266, distance: 0.9690913112209291 entropy -11.139567809524817
epoch: 0, step: 18
	action: tensor([[-0.0118,  0.0164,  0.0096, -0.0081, -0.0073, -0.0080,  0.0509]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28290228619346314, distance: 0.9690492778154881 entropy -11.138219865366112
epoch: 0, step: 19
	action: tensor([[-0.0115,  0.0156, -0.0016, -0.0087, -0.0041,  0.0044,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.285252696459112, distance: 0.9674598620711309 entropy -11.121896332755409
epoch: 0, step: 20
	action: tensor([[-0.0118,  0.0163,  0.0174, -0.0081, -0.0038, -0.0050,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28342125522565254, distance: 0.9686985602086493 entropy -11.132786230716377
epoch: 0, step: 21
	action: tensor([[-0.0117,  0.0158,  0.0026, -0.0083, -0.0011,  0.0045,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28545243566742806, distance: 0.9673246722157295 entropy -11.137916166176892
epoch: 0, step: 22
	action: tensor([[-0.0118,  0.0161,  0.0054, -0.0082, -0.0048,  0.0073, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2862293598819605, distance: 0.9667986454011807 entropy -11.13483449327549
epoch: 0, step: 23
	action: tensor([[-0.0120,  0.0164,  0.0158, -0.0079, -0.0035, -0.0118,  0.0343]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28194006858146736, distance: 0.9696992057700883 entropy -11.144477926054643
epoch: 0, step: 24
	action: tensor([[-0.0116,  0.0157,  0.0011, -0.0085, -0.0008, -0.0203, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[0.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27971934217670313, distance: 0.971197530726133 entropy -11.128863618472508
epoch: 0, step: 25
	action: tensor([[-1.1651e-02,  1.6430e-02,  1.1053e-02, -8.2617e-03,  3.6166e-05,
          1.0123e-02, -2.7408e-02]], dtype=torch.float64)
	q_value: tensor([[0.0252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2872379256873341, distance: 0.9661153553546983 entropy -11.136463201770676
epoch: 0, step: 26
	action: tensor([[-0.0121,  0.0164,  0.0128, -0.0077, -0.0033, -0.0169,  0.0273]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807065667813974, distance: 0.9705317354938583 entropy -11.151299521676563
epoch: 0, step: 27
	action: tensor([[-0.0115,  0.0158,  0.0169, -0.0085, -0.0084, -0.0036,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2835350781182816, distance: 0.9686216220829746 entropy -11.130262897697333
epoch: 0, step: 28
	action: tensor([[-0.0116,  0.0157,  0.0115, -0.0084,  0.0092, -0.0064, -0.0002]],
       dtype=torch.float64)
	q_value: tensor([[0.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2827481670844111, distance: 0.9691534065728141 entropy -11.132200892599107
epoch: 0, step: 29
	action: tensor([[-0.0117,  0.0161,  0.0127, -0.0082, -0.0104, -0.0018,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28414798499832117, distance: 0.9682072250491759 entropy -11.141709771068554
epoch: 0, step: 30
	action: tensor([[-0.0117,  0.0160, -0.0010, -0.0082, -0.0084, -0.0148,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2811794582206123, distance: 0.9702126504120903 entropy -11.13636460769573
epoch: 0, step: 31
	action: tensor([[-0.0117,  0.0163,  0.0081, -0.0083, -0.0123, -0.0015, -0.0002]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28437184016125083, distance: 0.9680558284418103 entropy -11.133398226596563
epoch: 0, step: 32
	action: tensor([[-0.0119,  0.0163,  0.0162, -0.0081, -0.0016,  0.0143, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2880157127686851, distance: 0.9655880846301975 entropy -11.141396995314723
epoch: 0, step: 33
	action: tensor([[-0.0120,  0.0162,  0.0065, -0.0078, -0.0079,  0.0097, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2867462613265811, distance: 0.9664485118309306 entropy -11.149836305133752
epoch: 0, step: 34
	action: tensor([[-0.0120,  0.0164,  0.0124, -0.0078, -0.0071,  0.0156,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28828295608699916, distance: 0.9654068508673755 entropy -11.144639914812517
epoch: 0, step: 35
	action: tensor([[-0.0119,  0.0159,  0.0098, -0.0080, -0.0111, -0.0064,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828969124651495, distance: 0.969052908699875 entropy -11.138711649179493
epoch: 0, step: 36
	action: tensor([[-0.0117,  0.0160,  0.0080, -0.0084, -0.0003, -0.0151,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28096358510362895, distance: 0.9703583245368721 entropy -11.131060792296859
epoch: 0, step: 37
	action: tensor([[-0.0116,  0.0162,  0.0149, -0.0083, -0.0097,  0.0102,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2869801060317372, distance: 0.9662900707447744 entropy -11.134563472368317
epoch: 0, step: 38
	action: tensor([[-0.0118,  0.0158,  0.0227, -0.0082, -0.0075,  0.0137,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[0.0238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2874249424287363, distance: 0.9659886008559158 entropy -11.13670308302046
epoch: 0, step: 39
	action: tensor([[-0.0120,  0.0160,  0.0181, -0.0079, -0.0107, -0.0006,  0.0232]],
       dtype=torch.float64)
	q_value: tensor([[0.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2841661033525581, distance: 0.968194972214713 entropy -11.14847528038184
epoch: 0, step: 40
	action: tensor([[-0.0117,  0.0158,  0.0082, -0.0083, -0.0113,  0.0066, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28590057477600994, distance: 0.9670212886289364 entropy -11.138058901734297
epoch: 0, step: 41
	action: tensor([[-0.0121,  0.0165,  0.0081, -0.0077, -0.0050, -0.0069, -0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2831427056499026, distance: 0.968886818901641 entropy -11.15149849243016
epoch: 0, step: 42
	action: tensor([[-0.0119,  0.0164,  0.0162, -0.0080, -0.0143, -0.0083,  0.0312]],
       dtype=torch.float64)
	q_value: tensor([[0.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2827461866993658, distance: 0.9691547445237958 entropy -11.144827587980531
epoch: 0, step: 43
	action: tensor([[-0.0116,  0.0158,  0.0055, -0.0084, -0.0067,  0.0019,  0.0494]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2848348605674125, distance: 0.9677426056299918 entropy -11.131361146020051
epoch: 0, step: 44
	action: tensor([[-0.0116,  0.0157,  0.0112, -0.0085, -0.0100,  0.0070, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28584375884388935, distance: 0.967059757447951 entropy -11.124142808495083
epoch: 0, step: 45
	action: tensor([[-0.0120,  0.0163,  0.0100, -0.0079, -0.0113, -0.0009, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[0.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843907241859306, distance: 0.96804305580947 entropy -11.146438722466945
epoch: 0, step: 46
	action: tensor([[-0.0119,  0.0164,  0.0175, -0.0080, -0.0157,  0.0059,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2859979740768992, distance: 0.9669553381422076 entropy -11.14345530045999
epoch: 0, step: 47
	action: tensor([[-0.0118,  0.0159,  0.0106, -0.0081, -0.0049,  0.0133,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[0.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28743060931170306, distance: 0.9659847597481099 entropy -11.140695394873173
epoch: 0, step: 48
	action: tensor([[-0.0118,  0.0159,  0.0105, -0.0081, -0.0055, -0.0158, -0.0253]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28072477412496377, distance: 0.9705194519696664 entropy -11.135606449412267
epoch: 0, step: 49
	action: tensor([[-0.0119,  0.0165,  0.0179, -0.0080,  0.0018, -0.0178,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[0.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2805562223237079, distance: 0.9706331589442032 entropy -11.147842769491119
epoch: 0, step: 50
	action: tensor([[-0.0115,  0.0157,  0.0081, -0.0085, -0.0044, -0.0058,  0.0699]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2830128413449078, distance: 0.9689745756395378 entropy -11.131691681966165
epoch: 0, step: 51
	action: tensor([[-1.1494e-02,  1.5423e-02, -4.7172e-05, -8.8324e-03, -3.4720e-03,
         -9.9733e-03, -1.1556e-02]], dtype=torch.float64)
	q_value: tensor([[0.0238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28179829250993715, distance: 0.9697949313164417 entropy -11.114525211508617
epoch: 0, step: 52
	action: tensor([[-0.0118,  0.0166,  0.0026, -0.0081, -0.0042, -0.0272, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[0.0251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.278746343216669, distance: 0.9718532857157535 entropy -11.141080454679173
epoch: 0, step: 53
	action: tensor([[-0.0116,  0.0165,  0.0078, -0.0083, -0.0147, -0.0058,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[0.0253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2836280771320635, distance: 0.9685587552375768 entropy -11.138929091287604
epoch: 0, step: 54
	action: tensor([[-0.0118,  0.0162,  0.0105, -0.0082, -0.0068, -0.0130, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28165214363973257, distance: 0.9698935994359363 entropy -11.136739500527963
epoch: 0, step: 55
	action: tensor([[-0.0119,  0.0165,  0.0088, -0.0080, -0.0034,  0.0047,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[0.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28591607970186883, distance: 0.9670107903185873 entropy -11.14780145628192
epoch: 0, step: 56
	action: tensor([[-0.0118,  0.0162,  0.0119, -0.0080, -0.0104,  0.0122,  0.0202]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2873966707379576, distance: 0.966007763651364 entropy -11.139608415621222
epoch: 0, step: 57
	action: tensor([[-0.0119,  0.0160,  0.0136, -0.0081, -0.0087,  0.0064, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28587694686859544, distance: 0.9670372867516551 entropy -11.137966736441726
epoch: 0, step: 58
	action: tensor([[-0.0119,  0.0162,  0.0005, -0.0080, -0.0080, -0.0213, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27974491266592394, distance: 0.97118029146243 entropy -11.145283567811004
epoch: 0, step: 59
	action: tensor([[-0.0117,  0.0165,  0.0079, -0.0082, -0.0057, -0.0064, -0.0204]],
       dtype=torch.float64)
	q_value: tensor([[0.0251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2833904115066247, distance: 0.968719407843976 entropy -11.139264262413139
epoch: 0, step: 60
	action: tensor([[-0.0119,  0.0164,  0.0084, -0.0080, -0.0073,  0.0040,  0.0433]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2856596545808262, distance: 0.9671843998867204 entropy -11.146415805612994
epoch: 0, step: 61
	action: tensor([[-0.0116,  0.0157,  0.0098, -0.0084,  0.0021, -0.0153,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2806778074068469, distance: 0.9705511375941778 entropy -11.127060095061717
epoch: 0, step: 62
	action: tensor([[-0.0116,  0.0159,  0.0060, -0.0085, -0.0031, -0.0283,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2779986272435788, distance: 0.9723569101917874 entropy -11.128622200752572
epoch: 0, step: 63
	action: tensor([[-1.1555e-02,  1.6282e-02,  1.4299e-02, -8.3772e-03, -3.5162e-05,
         -2.3808e-02, -1.9542e-02]], dtype=torch.float64)
	q_value: tensor([[0.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27926239427155275, distance: 0.9715055469902085 entropy -11.13707086383806
epoch: 0, step: 64
	action: tensor([[-0.0117,  0.0163,  0.0120, -0.0082, -0.0044,  0.0075, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28648567535927016, distance: 0.9666250408193323 entropy -11.145699727939064
epoch: 0, step: 65
	action: tensor([[-0.0119,  0.0162,  0.0079, -0.0079, -0.0045,  0.0198,  0.0039]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28919832061991524, distance: 0.9647858291279328 entropy -11.143890609267862
epoch: 0, step: 66
	action: tensor([[-0.0120,  0.0162,  0.0180, -0.0078, -0.0112,  0.0098,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28679638585587364, distance: 0.966414552227753 entropy -11.142959465143424
epoch: 0, step: 67
	action: tensor([[-0.0119,  0.0161,  0.0066, -0.0080, -0.0186,  0.0046, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[0.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2855506577190394, distance: 0.9672581854736522 entropy -11.144892331528071
epoch: 0, step: 68
	action: tensor([[-0.0120,  0.0165,  0.0123, -0.0079, -0.0011, -0.0104,  0.0131]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822962930332311, distance: 0.9694586454736004 entropy -11.144899694967476
epoch: 0, step: 69
	action: tensor([[-0.0117,  0.0160,  0.0132, -0.0083, -0.0030,  0.0034, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28530975630253885, distance: 0.9674212440800113 entropy -11.136976159164856
epoch: 0, step: 70
	action: tensor([[-0.0119,  0.0163,  0.0083, -0.0079,  0.0045, -0.0055,  0.0176]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28334714985883425, distance: 0.9687486481486136 entropy -11.145499440791507
epoch: 0, step: 71
	action: tensor([[-0.0117,  0.0160,  0.0009, -0.0083,  0.0025,  0.0037,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2853913294864837, distance: 0.9673660328302884 entropy -11.134638585646599
epoch: 0, step: 72
	action: tensor([[-0.0118,  0.0162,  0.0037, -0.0081, -0.0091, -0.0342,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2769249833470828, distance: 0.973079607741567 entropy -11.137658068849106
epoch: 0, step: 73
	action: tensor([[-0.0114,  0.0160,  0.0115, -0.0087, -0.0133, -0.0176,  0.0281]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2805916201697365, distance: 0.9706092802570779 entropy -11.123369505831821
epoch: 0, step: 74
	action: tensor([[-0.0115,  0.0160,  0.0009, -0.0085, -0.0029,  0.0109,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28706738988225333, distance: 0.966230925060261 entropy -11.130453595914572
epoch: 0, step: 75
	action: tensor([[-0.0119,  0.0163,  0.0067, -0.0079, -0.0112,  0.0084, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28657726372340286, distance: 0.9665629997051078 entropy -11.140321147034387
epoch: 0, step: 76
	action: tensor([[-0.0120,  0.0163,  0.0124, -0.0079,  0.0016, -0.0078,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828105612120556, distance: 0.9691112520646572 entropy -11.14252256952909
epoch: 0, step: 77
	action: tensor([[-0.0117,  0.0160,  0.0058, -0.0083, -0.0044, -0.0263,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2785260606817225, distance: 0.9720016842436543 entropy -11.138365610577086
epoch: 0, step: 78
	action: tensor([[-0.0115,  0.0161,  0.0056, -0.0085, -0.0067,  0.0156,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883031217414088, distance: 0.9653931739440893 entropy -11.131016171679805
epoch: 0, step: 79
	action: tensor([[-0.0119,  0.0160,  0.0024, -0.0080, -0.0090, -0.0100,  0.0376]],
       dtype=torch.float64)
	q_value: tensor([[0.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28222033666387003, distance: 0.9695099442268736 entropy -11.135273739018412
epoch: 0, step: 80
	action: tensor([[-0.0116,  0.0159,  0.0046, -0.0085,  0.0010,  0.0003, -0.0049]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845617957904538, distance: 0.9679273400251474 entropy -11.12488516698643
epoch: 0, step: 81
	action: tensor([[-0.0119,  0.0163,  0.0101, -0.0080, -0.0015, -0.0008,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28452132282543174, distance: 0.967954717886102 entropy -11.1420494638609
epoch: 0, step: 82
	action: tensor([[-0.0118,  0.0162,  0.0052, -0.0081, -0.0157,  0.0051,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2858000062128623, distance: 0.9670893803499719 entropy -11.13983011561346
epoch: 0, step: 83
	action: tensor([[-0.0119,  0.0164,  0.0106, -0.0080, -0.0113,  0.0090,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286750509455414, distance: 0.9664456337500462 entropy -11.140343157275467
epoch: 0, step: 84
	action: tensor([[-0.0119,  0.0161,  0.0136, -0.0080, -0.0087, -0.0265,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27838432288011095, distance: 0.9720971572806157 entropy -11.139396319271105
epoch: 0, step: 85
	action: tensor([[-0.0116,  0.0162,  0.0187, -0.0084, -0.0082, -0.0022,  0.0258]],
       dtype=torch.float64)
	q_value: tensor([[0.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28417974551129976, distance: 0.9681857463785964 entropy -11.139126211077711
epoch: 0, step: 86
	action: tensor([[-0.0117,  0.0158,  0.0125, -0.0083, -0.0001, -0.0137,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28111029813265653, distance: 0.9702593229641657 entropy -11.136346842645747
epoch: 0, step: 87
	action: tensor([[-0.0116,  0.0161,  0.0088, -0.0083, -0.0048,  0.0002,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[0.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2846468653748233, distance: 0.9678697923404675 entropy -11.136346879626657
epoch: 0, step: 88
	action: tensor([[-0.0117,  0.0158,  0.0085, -0.0084,  0.0004, -0.0138,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2811558455172891, distance: 0.970228585651839 entropy -11.128861670915118
epoch: 0, step: 89
	action: tensor([[-0.0115,  0.0156,  0.0113, -0.0087, -0.0103,  0.0128,  0.0292]],
       dtype=torch.float64)
	q_value: tensor([[0.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28720906723929407, distance: 0.9661349132889551 entropy -11.122416339386604
epoch: 0, step: 90
	action: tensor([[-0.0118,  0.0159,  0.0048, -0.0081, -0.0059, -0.0030,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[0.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28369261943992585, distance: 0.9685151225773093 entropy -11.135110927135262
epoch: 0, step: 91
	action: tensor([[-0.0118,  0.0163, -0.0014, -0.0081,  0.0026,  0.0010,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[0.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28499781547933567, distance: 0.9676323462042045 entropy -11.138532618153082
epoch: 0, step: 92
	action: tensor([[-0.0118,  0.0164,  0.0060, -0.0081, -0.0068, -0.0315,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2776045372183178, distance: 0.9726222447867282 entropy -11.136181854239807
epoch: 0, step: 93
	action: tensor([[-0.0115,  0.0163,  0.0133, -0.0084, -0.0050, -0.0160, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[0.0251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2811187230444355, distance: 0.9702536375487765 entropy -11.133918972204965
epoch: 0, step: 94
	action: tensor([[-0.0118,  0.0163,  0.0055, -0.0081, -0.0036, -0.0187,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28044012487435976, distance: 0.9707114718657411 entropy -11.145609479317873
epoch: 0, step: 95
	action: tensor([[-0.0116,  0.0161,  0.0091, -0.0084, -0.0075,  0.0026, -0.0088]],
       dtype=torch.float64)
	q_value: tensor([[0.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28526302436797624, distance: 0.9674528722767441 entropy -11.133798441216856
epoch: 0, step: 96
	action: tensor([[-0.0119,  0.0163,  0.0088, -0.0080, -0.0086, -0.0170,  0.0147]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.280762921693534, distance: 0.9704937153344311 entropy -11.145548980060607
epoch: 0, step: 97
	action: tensor([[-0.0116,  0.0161, -0.0015, -0.0084, -0.0020, -0.0213,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2798307688658319, distance: 0.9711224061828284 entropy -11.134954787691017
epoch: 0, step: 98
	action: tensor([[-0.0116,  0.0162, -0.0009, -0.0084, -0.0109, -0.0134, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[0.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.281771983068721, distance: 0.9698126941042552 entropy -11.130982843170598
epoch: 0, step: 99
	action: tensor([[-0.0118,  0.0165,  0.0139, -0.0082, -0.0061, -0.0117, -0.0341]],
       dtype=torch.float64)
	q_value: tensor([[0.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28210630417866167, distance: 0.9695869534007343 entropy -11.138777068652734
epoch: 0, step: 100
	action: tensor([[-0.0119,  0.0165,  0.0081, -0.0079, -0.0046,  0.0076,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[0.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28655742839370124, distance: 0.9665764363125656 entropy -11.150110037274784
epoch: 0, step: 101
	action: tensor([[-0.0119,  0.0162,  0.0025, -0.0080, -0.0029, -0.0173,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2806302730786928, distance: 0.970583205097863 entropy -11.139162362360576
epoch: 0, step: 102
	action: tensor([[-0.0116,  0.0162,  0.0098, -0.0083, -0.0034,  0.0167,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28859202101650083, distance: 0.9651972129204961 entropy -11.132097226014258
epoch: 0, step: 103
	action: tensor([[-0.0119,  0.0159,  0.0068, -0.0080,  0.0002, -0.0358, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[0.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.276161140589134, distance: 0.973593443513168 entropy -11.139070732551891
epoch: 0, step: 104
	action: tensor([[-0.0115,  0.0163,  0.0181, -0.0084, -0.0073, -0.0005,  0.0131]],
       dtype=torch.float64)
	q_value: tensor([[0.0252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28472860729735394, distance: 0.9678144925214153 entropy -11.137405320566003
epoch: 0, step: 105
	action: tensor([[-0.0118,  0.0160,  0.0127, -0.0082, -0.0073, -0.0093,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28230527888425694, distance: 0.9694525765079903 entropy -11.140037143212279
epoch: 0, step: 106
	action: tensor([[-0.0117,  0.0160,  0.0162, -0.0083, -0.0064, -0.0087,  0.0346]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2824604342314956, distance: 0.9693477799440567 entropy -11.137136973181759
epoch: 0, step: 107
	action: tensor([[-0.0116,  0.0157,  0.0100, -0.0085, -0.0063,  0.0257, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[0.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29031173318557113, distance: 0.9640299040297858 entropy -11.130946357462795
epoch: 0, step: 108
	action: tensor([[-0.0123,  0.0166,  0.0079, -0.0074, -0.0004,  0.0032, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28550662489133405, distance: 0.967287991965369 entropy -11.157845430995582
epoch: 0, step: 109
	action: tensor([[-0.0120,  0.0163,  0.0066, -0.0079, -0.0047, -0.0083, -0.0224]],
       dtype=torch.float64)
	q_value: tensor([[0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2827669394372815, distance: 0.9691407238525045 entropy -11.145329378004055
epoch: 0, step: 110
	action: tensor([[-0.0119,  0.0165, -0.0060, -0.0080, -0.0073, -0.0029, -0.0160]],
       dtype=torch.float64)
	q_value: tensor([[0.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28426556487936994, distance: 0.9681277069623216 entropy -11.145808898231092
epoch: 0, step: 111
	action: tensor([[-0.0119,  0.0167,  0.0116, -0.0079, -0.0003, -0.0050,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[0.0251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.283738715786374, distance: 0.9684839587724104 entropy -11.141967868918533
epoch: 0, step: 112
	action: tensor([[-0.0116,  0.0158,  0.0064, -0.0084,  0.0001,  0.0117,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28708070764266, distance: 0.9662219002998069 entropy -11.13163157913155
epoch: 0, step: 113
	action: tensor([[-0.0119,  0.0162,  0.0074, -0.0080,  0.0040, -0.0029, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28391498593240594, distance: 0.9683647806756588 entropy -11.139579816943106
epoch: 0, step: 114
	action: tensor([[-0.0118,  0.0163,  0.0022, -0.0081,  0.0033,  0.0104,  0.0358]],
       dtype=torch.float64)
	q_value: tensor([[0.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28710334972149754, distance: 0.9662065567354076 entropy -11.14216808014697
epoch: 0, step: 115
	action: tensor([[-0.0117,  0.0158,  0.0118, -0.0082, -0.0062, -0.0180, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[0.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2802215230510301, distance: 0.9708589114263373 entropy -11.129570417086551
epoch: 0, step: 116
	action: tensor([[-0.0117,  0.0163,  0.0102, -0.0082, -0.0002, -0.0104,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[0.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.282358803227004, distance: 0.9694164258453594 entropy -11.142369505613162
epoch: 0, step: 117
	action: tensor([[-0.0117,  0.0162,  0.0120, -0.0082, -0.0026, -0.0104,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[0.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822189953818912, distance: 0.9695108500658625 entropy -11.137685740340359
epoch: 0, step: 118
	action: tensor([[-0.0116,  0.0160,  0.0103, -0.0084, -0.0122, -0.0131,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2815147436669245, distance: 0.969986351840795 entropy -11.133568172316775
epoch: 0, step: 119
	action: tensor([[-0.0117,  0.0161, -0.0020, -0.0083, -0.0124,  0.0253, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904245044056035, distance: 0.9639533076193506 entropy -11.137032963074674
epoch: 0, step: 120
	action: tensor([[-0.0122,  0.0165,  0.0161, -0.0076, -0.0062, -0.0081,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828311751887379, distance: 0.9690973245169102 entropy -11.145359807897185
epoch: 0, step: 121
	action: tensor([[-0.0116,  0.0158,  0.0106, -0.0084, -0.0142, -0.0005, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2842439556543527, distance: 0.9681423215529679 entropy -11.133351194303401
epoch: 0, step: 122
	action: tensor([[-0.0119,  0.0164,  0.0124, -0.0080, -0.0021, -0.0207,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[0.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27985177075778844, distance: 0.9711082459301638 entropy -11.145823789654292
epoch: 0, step: 123
	action: tensor([[-0.0116,  0.0160,  0.0110, -0.0084, -0.0045,  0.0011,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[0.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28480177059620015, distance: 0.967764993609209 entropy -11.132555825738056
epoch: 0, step: 124
	action: tensor([[-0.0117,  0.0160,  0.0199, -0.0082, -0.0018, -0.0257, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27838277205334894, distance: 0.9720982018487457 entropy -11.137071521573873
epoch: 0, step: 125
	action: tensor([[-0.0116,  0.0162,  0.0037, -0.0083, -0.0074,  0.0069,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[0.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2863288004044241, distance: 0.9667312972183766 entropy -11.143221721845125
epoch: 0, step: 126
	action: tensor([[-0.0119,  0.0163,  0.0092, -0.0080, -0.0070, -0.0156, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[0.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28110736003363856, distance: 0.9702613056849113 entropy -11.139385681467113
epoch: 0, step: 127
	action: tensor([[-0.0117,  0.0163,  0.0121, -0.0082, -0.0061, -0.0031,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[0.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2840093007336425, distance: 0.9683010074242295 entropy -11.13938840229316
LOSS epoch 0 actor 0.10423819453925381 critic 18.455092906222383
epoch: 1, step: 0
	action: tensor([[-0.0098,  0.0139,  0.0170, -0.0092, -0.0073, -0.0227,  0.0380]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2791382897891199, distance: 0.9715891856143006 entropy -11.122431421967585
epoch: 1, step: 1
	action: tensor([[-0.0096,  0.0139,  0.0064, -0.0095, -0.0010, -0.0042, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2836298018110064, distance: 0.9685575893251213 entropy -11.120951881888246
epoch: 1, step: 2
	action: tensor([[-0.0101,  0.0147,  0.0075, -0.0087, -0.0058, -0.0015, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[0.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2846788955077859, distance: 0.967848123780098 entropy -11.138301698596464
epoch: 1, step: 3
	action: tensor([[-0.0101,  0.0146,  0.0094, -0.0088, -0.0060,  0.0076,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286651557478638, distance: 0.9665126708741949 entropy -11.138714431791014
epoch: 1, step: 4
	action: tensor([[-0.0101,  0.0144,  0.0058, -0.0088, -0.0133,  0.0152,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882500387283007, distance: 0.9654291759447604 entropy -11.135762458759869
epoch: 1, step: 5
	action: tensor([[-0.0102,  0.0144,  0.0085, -0.0087, -0.0007,  0.0180, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[0.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889488874648303, distance: 0.9649550946712998 entropy -11.134806266028779
epoch: 1, step: 6
	action: tensor([[-0.0102,  0.0144,  0.0141, -0.0086, -0.0116, -0.0022, -0.0169]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2842210928433717, distance: 0.9681577837193878 entropy -11.139266329388288
epoch: 1, step: 7
	action: tensor([[-0.0102,  0.0145,  0.0122, -0.0087, -0.0021, -0.0135,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28167478731510076, distance: 0.9698783128803448 entropy -11.14177535200823
epoch: 1, step: 8
	action: tensor([[-0.0098,  0.0140,  0.0200, -0.0093, -0.0088, -0.0030,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2838532045189053, distance: 0.9684065534075256 entropy -11.122402077545942
epoch: 1, step: 9
	action: tensor([[-0.0098,  0.0139,  0.0206, -0.0092, -0.0079, -0.0008,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28422904314025177, distance: 0.9681524069451465 entropy -11.12892628772014
epoch: 1, step: 10
	action: tensor([[-0.0098,  0.0139,  0.0105, -0.0092, -0.0082,  0.0125,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28735507272384675, distance: 0.9660359584523972 entropy -11.127624144658554
epoch: 1, step: 11
	action: tensor([[-0.0101,  0.0143,  0.0042, -0.0088, -0.0136,  0.0012,  0.0503]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2850171640333954, distance: 0.967619253647407 entropy -11.13333795594819
epoch: 1, step: 12
	action: tensor([[-0.0099,  0.0140,  0.0079, -0.0093, -0.0049, -0.0025,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28390942255633544, distance: 0.9683685423567991 entropy -11.11507138189245
epoch: 1, step: 13
	action: tensor([[-0.0100,  0.0145,  0.0093, -0.0089, -0.0063,  0.0117, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28759643926035183, distance: 0.9658723506788071 entropy -11.133348188589633
epoch: 1, step: 14
	action: tensor([[-0.0102,  0.0145,  0.0129, -0.0086, -0.0117, -0.0096,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2825630112757541, distance: 0.9692784901175657 entropy -11.1396047968805
epoch: 1, step: 15
	action: tensor([[-0.0098,  0.0141, -0.0013, -0.0092, -0.0090,  0.0072,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28633362208131974, distance: 0.9667280315171435 entropy -11.126483819378993
epoch: 1, step: 16
	action: tensor([[-0.0099,  0.0142,  0.0072, -0.0091, -0.0035, -0.0093,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[0.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2824837443412259, distance: 0.9693320346195412 entropy -11.121422606409448
epoch: 1, step: 17
	action: tensor([[-0.0098,  0.0142,  0.0008, -0.0092,  0.0034, -0.0147,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2813639359847506, distance: 0.9700881449652761 entropy -11.123549039979368
epoch: 1, step: 18
	action: tensor([[-0.0098,  0.0144,  0.0127, -0.0092, -0.0098,  0.0002, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[0.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849078459462169, distance: 0.967693223425171 entropy -11.123593122979742
epoch: 1, step: 19
	action: tensor([[-0.0100,  0.0144,  0.0051, -0.0088, -0.0039, -0.0341,  0.0341]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27699610606140834, distance: 0.9730317497992382 entropy -11.137278642092673
epoch: 1, step: 20
	action: tensor([[-0.0096,  0.0141,  0.0178, -0.0095, -0.0130, -0.0036,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28391454243404934, distance: 0.9683650805479602 entropy -11.11680192006979
epoch: 1, step: 21
	action: tensor([[-0.0100,  0.0142,  0.0138, -0.0090, -0.0134, -0.0167,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2808106782491443, distance: 0.9704614949387756 entropy -11.135011925934874
epoch: 1, step: 22
	action: tensor([[-0.0097,  0.0140, -0.0015, -0.0094, -0.0064,  0.0021,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2850958981937921, distance: 0.9675659748949854 entropy -11.118383097245683
epoch: 1, step: 23
	action: tensor([[-0.0100,  0.0144,  0.0033, -0.0089, -0.0066, -0.0082,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28298653222619397, distance: 0.968992353247532 entropy -11.127494887308329
epoch: 1, step: 24
	action: tensor([[-0.0099,  0.0144,  0.0163, -0.0091, -0.0104, -0.0102,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28242764285075217, distance: 0.9693699291679418 entropy -11.127980994449562
epoch: 1, step: 25
	action: tensor([[-0.0099,  0.0142,  0.0130, -0.0091, -0.0068, -0.0095,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2824966961273019, distance: 0.9693232859412321 entropy -11.132834834622795
epoch: 1, step: 26
	action: tensor([[-0.0099,  0.0144,  0.0085, -0.0090, -0.0051,  0.0099,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871471878482905, distance: 0.9661768488270415 entropy -11.134173420217689
epoch: 1, step: 27
	action: tensor([[-0.0099,  0.0140,  0.0085, -0.0091, -0.0074,  0.0289,  0.0001]],
       dtype=torch.float64)
	q_value: tensor([[0.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29121352525629396, distance: 0.9634172189191812 entropy -11.123338264855992
epoch: 1, step: 28
	action: tensor([[-0.0103,  0.0144,  0.0069, -0.0085, -0.0023, -0.0018,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[0.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28429410878484895, distance: 0.9681084020218821 entropy -11.139547952659123
epoch: 1, step: 29
	action: tensor([[-0.0099,  0.0141,  0.0123, -0.0091, -0.0138, -0.0028,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28392857196113397, distance: 0.96835559441015 entropy -11.12239579945388
epoch: 1, step: 30
	action: tensor([[-0.0100,  0.0143,  0.0032, -0.0090, -0.0025,  0.0185,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2890519663097214, distance: 0.9648851488826827 entropy -11.132866991946557
epoch: 1, step: 31
	action: tensor([[-0.0101,  0.0142, -0.0044, -0.0088, -0.0047,  0.0114,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[0.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28728208943135114, distance: 0.9660854239556143 entropy -11.129298279667566
epoch: 1, step: 32
	action: tensor([[-0.0100,  0.0143,  0.0082, -0.0089, -0.0157, -0.0040,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[0.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2838458781818335, distance: 0.9684115068994159 entropy -11.122541549735152
epoch: 1, step: 33
	action: tensor([[-0.0099,  0.0143,  0.0101, -0.0090, -0.0037, -0.0098, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.282503370406594, distance: 0.9693187775652115 entropy -11.127884273484879
epoch: 1, step: 34
	action: tensor([[-0.0099,  0.0144, -0.0027, -0.0090, -0.0035,  0.0088,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28694554033937036, distance: 0.9663134923076357 entropy -11.135051156034862
epoch: 1, step: 35
	action: tensor([[-0.0100,  0.0144, -0.0012, -0.0089, -0.0059, -0.0021, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28435948046897475, distance: 0.9680641881048 entropy -11.127536403195109
epoch: 1, step: 36
	action: tensor([[-0.0100,  0.0147,  0.0086, -0.0088, -0.0127, -0.0018, -0.0236]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28460447203744244, distance: 0.9678984709194659 entropy -11.133810822587057
epoch: 1, step: 37
	action: tensor([[-0.0102,  0.0147,  0.0062, -0.0087, -0.0048, -0.0199,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28034488173576844, distance: 0.9707757129048021 entropy -11.142668321784678
epoch: 1, step: 38
	action: tensor([[-0.0098,  0.0143,  0.0049, -0.0092, -0.0063,  0.0143,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881608343360651, distance: 0.9654896731905259 entropy -11.124485834420357
epoch: 1, step: 39
	action: tensor([[-0.0101,  0.0143,  0.0190, -0.0088, -0.0072, -0.0220, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2795604008590602, distance: 0.9713046798622413 entropy -11.132472497905097
epoch: 1, step: 40
	action: tensor([[-0.0098,  0.0144,  0.0091, -0.0091, -0.0012, -0.0176,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[0.1965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28079897189288583, distance: 0.9704693930828542 entropy -11.136940328420982
epoch: 1, step: 41
	action: tensor([[-0.0098,  0.0143,  0.0066, -0.0091, -0.0114,  0.0004, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.284968908587087, distance: 0.9676519062572129 entropy -11.129898482184112
epoch: 1, step: 42
	action: tensor([[-0.0101,  0.0147,  0.0136, -0.0087, -0.0054, -0.0187, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28061590861181096, distance: 0.9705928954143903 entropy -11.13841576955703
epoch: 1, step: 43
	action: tensor([[-0.0100,  0.0146,  0.0056, -0.0089, -0.0028,  0.0168, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[0.1967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888839769915915, distance: 0.9649991381031556 entropy -11.139673296102519
epoch: 1, step: 44
	action: tensor([[-0.0103,  0.0146,  0.0120, -0.0084, -0.0086,  0.0257, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29084697342724275, distance: 0.9636663043002275 entropy -11.143277132262712
epoch: 1, step: 45
	action: tensor([[-0.0103,  0.0144,  0.0100, -0.0085, -0.0107, -0.0030,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2839932373275592, distance: 0.9683118693844963 entropy -11.139763425809248
epoch: 1, step: 46
	action: tensor([[-0.0099,  0.0141,  0.0074, -0.0091, -0.0143, -0.0025, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28409059541605175, distance: 0.9682460346728269 entropy -11.126729631885565
epoch: 1, step: 47
	action: tensor([[-0.0101,  0.0146,  0.0015, -0.0088,  0.0035,  0.0020,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2853823955900605, distance: 0.9673720797209058 entropy -11.137272219493804
epoch: 1, step: 48
	action: tensor([[-0.0100,  0.0144,  0.0103, -0.0089, -0.0069, -0.0201,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2801730763014373, distance: 0.9708915840948381 entropy -11.129379389493346
epoch: 1, step: 49
	action: tensor([[-0.0097,  0.0142,  0.0074, -0.0093, -0.0098,  0.0154,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883039525828808, distance: 0.9653926104394961 entropy -11.12556388532659
epoch: 1, step: 50
	action: tensor([[-0.0100,  0.0141,  0.0173, -0.0089, -0.0095, -0.0035, -0.0232]],
       dtype=torch.float64)
	q_value: tensor([[0.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2837291805623442, distance: 0.9684904052193885 entropy -11.126982903039195
epoch: 1, step: 51
	action: tensor([[-0.0102,  0.0145,  0.0078, -0.0087, -0.0085, -0.0085,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828737173619843, distance: 0.9690685808541464 entropy -11.144082619396091
epoch: 1, step: 52
	action: tensor([[-0.0099,  0.0142,  0.0046, -0.0092, -0.0113,  0.0171,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2886717894448384, distance: 0.9651430988078178 entropy -11.12398132978178
epoch: 1, step: 53
	action: tensor([[-0.0101,  0.0143,  0.0182, -0.0087, -0.0019,  0.0132,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[0.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877599534563049, distance: 0.9657614985487875 entropy -11.132582876637183
epoch: 1, step: 54
	action: tensor([[-0.0101,  0.0142,  0.0035, -0.0088, -0.0111,  0.0286,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29120527954390596, distance: 0.9634228228905292 entropy -11.136616091525033
epoch: 1, step: 55
	action: tensor([[-0.0103,  0.0144,  0.0041, -0.0085,  0.0002, -0.0148, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[0.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.281332230124682, distance: 0.9701095446286627 entropy -11.1365452561526
epoch: 1, step: 56
	action: tensor([[-0.0099,  0.0146,  0.0143, -0.0090, -0.0060,  0.0132, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[0.1965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2880709801196252, distance: 0.965550607306505 entropy -11.13159457606497
epoch: 1, step: 57
	action: tensor([[-0.0102,  0.0144, -0.0011, -0.0087, -0.0081,  0.0043,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2857950395246095, distance: 0.9670927430097003 entropy -11.138395791758041
epoch: 1, step: 58
	action: tensor([[-0.0100,  0.0145,  0.0069, -0.0088, -0.0019, -0.0252,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27908058899850063, distance: 0.9716280698701636 entropy -11.130386980138663
epoch: 1, step: 59
	action: tensor([[-0.0097,  0.0144,  0.0063, -0.0092, -0.0062,  0.0058, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[0.1965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2862671725547642, distance: 0.9667730365270079 entropy -11.125760315970783
epoch: 1, step: 60
	action: tensor([[-0.0101,  0.0146,  0.0099, -0.0087, -0.0082, -0.0026,  0.0282]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28426735129773406, distance: 0.9681264987752422 entropy -11.1380975296547
epoch: 1, step: 61
	action: tensor([[-0.0099,  0.0141,  0.0018, -0.0091, -0.0153,  0.0057, -0.0177]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28593676317273975, distance: 0.966996785463009 entropy -11.124911054565105
epoch: 1, step: 62
	action: tensor([[-0.0102,  0.0148,  0.0171, -0.0086, -0.0064,  0.0213,  0.0367]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2898938917533901, distance: 0.9643136570389869 entropy -11.140463676893715
epoch: 1, step: 63
	action: tensor([[-0.0100,  0.0138,  0.0134, -0.0089, -0.0136, -0.0037,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[0.1952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28346039690578206, distance: 0.9686721032370643 entropy -11.128577942875404
epoch: 1, step: 64
	action: tensor([[-0.0099,  0.0142,  0.0125, -0.0091,  0.0019, -0.0109,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.282130478500342, distance: 0.9695706283484526 entropy -11.129563777370336
epoch: 1, step: 65
	action: tensor([[-0.0098,  0.0143,  0.0109, -0.0091, -0.0068,  0.0084,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[0.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28675699299495905, distance: 0.9664412411757736 entropy -11.131462876279803
epoch: 1, step: 66
	action: tensor([[-0.0101,  0.0144,  0.0105, -0.0088, -0.0092, -0.0160, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28108175567556815, distance: 0.9702785841318669 entropy -11.134228416767135
epoch: 1, step: 67
	action: tensor([[-0.0100,  0.0146,  0.0121, -0.0089, -0.0076, -0.0016,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[0.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28457099480503, distance: 0.9679211172610575 entropy -11.139577973768072
epoch: 1, step: 68
	action: tensor([[-0.0100,  0.0143,  0.0043, -0.0090, -0.0073,  0.0016, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2851747226760488, distance: 0.9675126320835192 entropy -11.13272193705959
epoch: 1, step: 69
	action: tensor([[-0.0101,  0.0146,  0.0094, -0.0088, -0.0030, -0.0217, -0.0226]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2799058901873538, distance: 0.9710717556566223 entropy -11.136381098093638
epoch: 1, step: 70
	action: tensor([[-0.0100,  0.0146,  0.0017, -0.0089, -0.0131,  0.0194,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[0.1967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894844232716941, distance: 0.9645916430606438 entropy -11.139824968975006
epoch: 1, step: 71
	action: tensor([[-0.0102,  0.0144,  0.0139, -0.0087, -0.0069, -0.0154, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[0.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2811927575468045, distance: 0.9702036751300507 entropy -11.130457546164635
epoch: 1, step: 72
	action: tensor([[-0.0099,  0.0144,  0.0160, -0.0090, -0.0024,  0.0140,  0.0025]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2880975676939176, distance: 0.9655325774977217 entropy -11.135509591142362
epoch: 1, step: 73
	action: tensor([[-0.0101,  0.0143,  0.0072, -0.0087, -0.0064,  0.0196,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28920098385405024, distance: 0.9647840216948367 entropy -11.13689063186961
epoch: 1, step: 74
	action: tensor([[-0.0102,  0.0144,  0.0115, -0.0087, -0.0026, -0.0027,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2841082905186135, distance: 0.9682340685506818 entropy -11.133892642573134
epoch: 1, step: 75
	action: tensor([[-0.0099,  0.0141,  0.0009, -0.0091, -0.0058,  0.0048,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28579555953160063, distance: 0.967092390943342 entropy -11.129323959126252
epoch: 1, step: 76
	action: tensor([[-0.0101,  0.0146,  0.0113, -0.0088, -0.0043, -0.0052,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2837065366807551, distance: 0.9685057138206459 entropy -11.132017715583954
epoch: 1, step: 77
	action: tensor([[-0.0099,  0.0141, -0.0032, -0.0091, -0.0064, -0.0004, -0.0001]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845940849334797, distance: 0.9679054975398157 entropy -11.127740110769947
epoch: 1, step: 78
	action: tensor([[-0.0100,  0.0147,  0.0088, -0.0088, -0.0103,  0.0066,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28653632109537586, distance: 0.9665907343563831 entropy -11.130760593046956
epoch: 1, step: 79
	action: tensor([[-0.0100,  0.0142,  0.0030, -0.0089, -0.0009,  0.0058, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2860547943726859, distance: 0.9669168622139472 entropy -11.131426093618272
epoch: 1, step: 80
	action: tensor([[-0.0102,  0.0147,  0.0098, -0.0086, -0.0096, -0.0207, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28022671071775274, distance: 0.97085541277939 entropy -11.137105406003544
epoch: 1, step: 81
	action: tensor([[-0.0100,  0.0147,  0.0026, -0.0089, -0.0126,  0.0040,  0.0212]],
       dtype=torch.float64)
	q_value: tensor([[0.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28598593147195095, distance: 0.9669634926092936 entropy -11.13964798039792
epoch: 1, step: 82
	action: tensor([[-0.0100,  0.0143,  0.0025, -0.0090, -0.0082, -0.0035, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2839802763618986, distance: 0.9683206334079296 entropy -11.125580226295854
epoch: 1, step: 83
	action: tensor([[-0.0101,  0.0147,  0.0088, -0.0088, -0.0034, -0.0180,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2808280103704044, distance: 0.9704498010376424 entropy -11.137055543198107
epoch: 1, step: 84
	action: tensor([[-0.0098,  0.0144,  0.0074, -0.0091, -0.0065, -0.0008, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[0.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2847208944725512, distance: 0.9678197105152956 entropy -11.132398095086188
epoch: 1, step: 85
	action: tensor([[-0.0101,  0.0146,  0.0164, -0.0088, -0.0068, -0.0164, -0.0217]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.281088398107912, distance: 0.9702741016856444 entropy -11.136348085214305
epoch: 1, step: 86
	action: tensor([[-0.0100,  0.0145,  0.0153, -0.0089, -0.0128, -0.0016,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[0.1965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28447921408961774, distance: 0.9679832014392121 entropy -11.141804214754854
epoch: 1, step: 87
	action: tensor([[-0.0100,  0.0143,  0.0115, -0.0089, -0.0198,  0.0097,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28692301028626477, distance: 0.9663287582680637 entropy -11.13629110267803
epoch: 1, step: 88
	action: tensor([[-0.0101,  0.0143, -0.0034, -0.0089, -0.0053,  0.0029,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[0.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28540808514045, distance: 0.9673546916961906 entropy -11.131751978689405
epoch: 1, step: 89
	action: tensor([[-0.0100,  0.0144,  0.0115, -0.0089, -0.0103,  0.0118,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28756135806095107, distance: 0.965896131822857 entropy -11.124162115465738
epoch: 1, step: 90
	action: tensor([[-0.0101,  0.0142,  0.0118, -0.0088, -0.0097, -0.0247, -0.0241]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27897907903915076, distance: 0.971696473115164 entropy -11.132156633807938
epoch: 1, step: 91
	action: tensor([[-0.0099,  0.0147,  0.0078, -0.0089, -0.0080, -0.0062,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[0.1967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.283659516773089, distance: 0.9685375012822524 entropy -11.14044567639065
epoch: 1, step: 92
	action: tensor([[-0.0099,  0.0143,  0.0033, -0.0090, -0.0018,  0.0089,  0.0134]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2868778610068071, distance: 0.9663593498852386 entropy -11.12825513756492
epoch: 1, step: 93
	action: tensor([[-0.0100,  0.0143,  0.0053, -0.0089, -0.0033,  0.0182, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889667335650282, distance: 0.9649429852792633 entropy -11.129692330447751
epoch: 1, step: 94
	action: tensor([[-0.0102,  0.0145,  0.0193, -0.0086, -0.0107, -0.0032,  0.0335]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28403137296918535, distance: 0.968286082137733 entropy -11.136378140802721
epoch: 1, step: 95
	action: tensor([[-0.0098,  0.0138,  0.0099, -0.0092, -0.0081, -0.0361,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[0.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2761645898945905, distance: 0.9735911237805824 entropy -11.125946543035417
epoch: 1, step: 96
	action: tensor([[-0.0096,  0.0143,  0.0052, -0.0094, -0.0133, -0.0141,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[0.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2817538330116278, distance: 0.9698249479039885 entropy -11.124241887241492
epoch: 1, step: 97
	action: tensor([[-0.0099,  0.0145,  0.0129, -0.0090, -0.0097,  0.0078,  0.0327]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28670938470583607, distance: 0.9664734951521974 entropy -11.131878754726499
epoch: 1, step: 98
	action: tensor([[-0.0099,  0.0139,  0.0028, -0.0091, -0.0070, -0.0034, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[0.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2837089899632331, distance: 0.9685040552684926 entropy -11.12621662623739
epoch: 1, step: 99
	action: tensor([[-0.0102,  0.0148,  0.0085, -0.0086,  0.0024,  0.0001,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28505210060523656, distance: 0.9675956127214708 entropy -11.141696449614733
epoch: 1, step: 100
	action: tensor([[-0.0100,  0.0143,  0.0065, -0.0089, -0.0043,  0.0021, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28526009848392175, distance: 0.9674548524821428 entropy -11.130435076931198
epoch: 1, step: 101
	action: tensor([[-0.0101,  0.0145,  0.0051, -0.0088, -0.0075, -0.0153,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2813735545092442, distance: 0.970081652911598 entropy -11.136684510738377
epoch: 1, step: 102
	action: tensor([[-0.0098,  0.0143,  0.0086, -0.0092, -0.0005, -0.0096,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2825610825106313, distance: 0.9692797930259179 entropy -11.123239501364457
epoch: 1, step: 103
	action: tensor([[-0.0098,  0.0141,  0.0128, -0.0092, -0.0155,  0.0114, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2872549903056636, distance: 0.966103790143056 entropy -11.124780340184634
epoch: 1, step: 104
	action: tensor([[-0.0102,  0.0145,  0.0079, -0.0086, -0.0092, -0.0180,  0.0417]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28070372380242803, distance: 0.9705336534861958 entropy -11.140580436542509
epoch: 1, step: 105
	action: tensor([[-0.0097,  0.0140,  0.0022, -0.0094, -0.0041,  0.0031,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28529571457540503, distance: 0.9674307476366428 entropy -11.115752029552182
epoch: 1, step: 106
	action: tensor([[-0.0100,  0.0145,  0.0081, -0.0088, -0.0054,  0.0135,  0.0297]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28803759037148746, distance: 0.9655732493906803 entropy -11.132687039062064
epoch: 1, step: 107
	action: tensor([[-0.0100,  0.0141,  0.0036, -0.0090, -0.0135,  0.0217,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[0.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28950348937492276, distance: 0.9645787009606948 entropy -11.125763597866756
epoch: 1, step: 108
	action: tensor([[-0.0100,  0.0141,  0.0161, -0.0089, -0.0170,  0.0062,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.1950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2858884806029848, distance: 0.9670294774559066 entropy -11.122960088645428
epoch: 1, step: 109
	action: tensor([[-0.0100,  0.0142,  0.0076, -0.0089, -0.0053,  0.0018, -0.0126]],
       dtype=torch.float64)
	q_value: tensor([[0.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28505818663169524, distance: 0.9675914943619108 entropy -11.1328981589581
epoch: 1, step: 110
	action: tensor([[-0.0101,  0.0146,  0.0062, -0.0087, -0.0107,  0.0126, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28780020264530937, distance: 0.965734210230438 entropy -11.139286143500371
epoch: 1, step: 111
	action: tensor([[-0.0102,  0.0146,  0.0071, -0.0086, -0.0090, -0.0112,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2823158409390858, distance: 0.9694454429394699 entropy -11.137940248254774
epoch: 1, step: 112
	action: tensor([[-0.0098,  0.0141,  0.0170, -0.0093, -0.0034, -0.0101,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28222968621358113, distance: 0.9695036299556564 entropy -11.121403032137422
epoch: 1, step: 113
	action: tensor([[-9.8011e-03,  1.4056e-02,  1.5321e-02, -9.1924e-03, -8.8038e-03,
         -7.9716e-05, -2.2285e-02]], dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845785012044836, distance: 0.9679160394535106 entropy -11.127377355619128
epoch: 1, step: 114
	action: tensor([[-0.0102,  0.0146,  0.0002, -0.0087, -0.0115, -0.0011,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[0.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2846709797864513, distance: 0.9678534788540636 entropy -11.14333959621058
epoch: 1, step: 115
	action: tensor([[-0.0100,  0.0146, -0.0020, -0.0089, -0.0111, -0.0032, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28430262190638467, distance: 0.9681026443153329 entropy -11.130916473209421
epoch: 1, step: 116
	action: tensor([[-0.0100,  0.0147,  0.0074, -0.0088, -0.0015,  0.0227,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2902863495123068, distance: 0.9640471442762693 entropy -11.133458097014858
epoch: 1, step: 117
	action: tensor([[-0.0101,  0.0142,  0.0139, -0.0087, -0.0066,  0.0036, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[0.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28539223099528954, distance: 0.967365422643687 entropy -11.132654516577004
epoch: 1, step: 118
	action: tensor([[-0.0102,  0.0146,  0.0090, -0.0086, -0.0075, -0.0171,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2809137823526422, distance: 0.9703919290082316 entropy -11.144761657556007
epoch: 1, step: 119
	action: tensor([[-0.0099,  0.0144,  0.0125, -0.0091, -0.0071,  0.0080,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[0.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2867278275148162, distance: 0.9664610005245575 entropy -11.132269176629405
epoch: 1, step: 120
	action: tensor([[-0.0100,  0.0143,  0.0158, -0.0089, -0.0013,  0.0036,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28549352578044707, distance: 0.9672968587757792 entropy -11.132803237789057
epoch: 1, step: 121
	action: tensor([[-0.0100,  0.0143,  0.0106, -0.0089, -0.0008,  0.0319,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[0.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29220375504268903, distance: 0.9627439993400512 entropy -11.135887409593023
epoch: 1, step: 122
	action: tensor([[-0.0103,  0.0143,  0.0079, -0.0085, -0.0042, -0.0093,  0.0405]],
       dtype=torch.float64)
	q_value: tensor([[0.1956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2825136056003237, distance: 0.9693118638039149 entropy -11.138675303068082
epoch: 1, step: 123
	action: tensor([[-0.0098,  0.0139,  0.0072, -0.0093, -0.0067, -0.0120,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2817536473299156, distance: 0.9698250732640347 entropy -11.118386261843403
epoch: 1, step: 124
	action: tensor([[-0.0099,  0.0144,  0.0118, -0.0091, -0.0086,  0.0043,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[0.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28581591842637644, distance: 0.9670786070241839 entropy -11.129344232305124
epoch: 1, step: 125
	action: tensor([[-0.0100,  0.0142,  0.0044, -0.0089, -0.0017,  0.0086,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28666932566539105, distance: 0.9665006337801529 entropy -11.13222107867075
epoch: 1, step: 126
	action: tensor([[-0.0100,  0.0144,  0.0086, -0.0088, -0.0029, -0.0071, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28310047313521625, distance: 0.9689153586992313 entropy -11.131733825000795
epoch: 1, step: 127
	action: tensor([[-0.0100,  0.0146,  0.0166, -0.0089, -0.0058,  0.0108,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[0.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28743081994750797, distance: 0.9659846169753532 entropy -11.135978195065471
LOSS epoch 1 actor 0.07462878162475597 critic 18.35941372702834
epoch: 2, step: 0
	action: tensor([[-0.0084,  0.0138,  0.0045, -0.0087, -0.0079, -0.0138,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[0.4054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28298617839843077, distance: 0.9689925923339439 entropy -11.134259974386458
epoch: 2, step: 1
	action: tensor([[-0.0082,  0.0141,  0.0110, -0.0090, -0.0047,  0.0014, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[0.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28676637521736603, distance: 0.9664348847191396 entropy -11.125927596206179
epoch: 2, step: 2
	action: tensor([[-0.0084,  0.0141,  0.0040, -0.0087, -0.0028, -0.0195, -0.0276]],
       dtype=torch.float64)
	q_value: tensor([[0.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2818880401396082, distance: 0.9697343358634659 entropy -11.134765912806207
epoch: 2, step: 3
	action: tensor([[-0.0084,  0.0144,  0.0116, -0.0087, -0.0027, -0.0171,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[0.4072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28265496525220435, distance: 0.9692163718626133 entropy -11.138290478425365
epoch: 2, step: 4
	action: tensor([[-0.0082,  0.0140,  0.0052, -0.0090, -0.0095,  0.0025,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[0.4069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2869953177001211, distance: 0.9662797632045212 entropy -11.128879702564634
epoch: 2, step: 5
	action: tensor([[-0.0083,  0.0139,  0.0010, -0.0089, -0.0101,  0.0185, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290513695214509, distance: 0.9638927231729736 entropy -11.124958795461884
epoch: 2, step: 6
	action: tensor([[-0.0087,  0.0144,  0.0058, -0.0083, -0.0022, -0.0038,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[0.4054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2856030647950787, distance: 0.9672227091285157 entropy -11.139363485458686
epoch: 2, step: 7
	action: tensor([[-0.0083,  0.0141,  0.0133, -0.0088, -0.0123, -0.0096,  0.0420]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2841480009047508, distance: 0.9682072142922598 entropy -11.130863234994147
epoch: 2, step: 8
	action: tensor([[-0.0081,  0.0136,  0.0141, -0.0093, -0.0012, -0.0145,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2826040022520935, distance: 0.9692507997142163 entropy -11.11687526614889
epoch: 2, step: 9
	action: tensor([[-0.0081,  0.0138,  0.0077, -0.0091, -0.0031, -0.0081,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[0.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28434718189842456, distance: 0.9680725063555408 entropy -11.125384905603436
epoch: 2, step: 10
	action: tensor([[-0.0081,  0.0136,  0.0159, -0.0092, -0.0039,  0.0094,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[0.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882054290998831, distance: 0.965459430081988 entropy -11.116118619423053
epoch: 2, step: 11
	action: tensor([[-0.0084,  0.0139,  0.0133, -0.0088, -0.0031, -0.0204,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2814178197062783, distance: 0.9700517754160983 entropy -11.131922064199683
epoch: 2, step: 12
	action: tensor([[-0.0081,  0.0140,  0.0261, -0.0091, -0.0084,  0.0025,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[0.4070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2868710613697617, distance: 0.9663639570042805 entropy -11.127174310286462
epoch: 2, step: 13
	action: tensor([[-0.0081,  0.0133,  0.0026, -0.0092, -0.0029, -0.0055, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[0.4055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845935470942983, distance: 0.9679058613734042 entropy -11.12376710861085
epoch: 2, step: 14
	action: tensor([[-0.0085,  0.0145,  0.0153, -0.0086, -0.0079, -0.0020,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[0.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28611146872055293, distance: 0.966878483589272 entropy -11.137200778451946
epoch: 2, step: 15
	action: tensor([[-0.0083,  0.0138,  0.0113, -0.0089,  0.0019,  0.0066,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28773305876817934, distance: 0.9657797322954295 entropy -11.127773800773882
epoch: 2, step: 16
	action: tensor([[-0.0084,  0.0141,  0.0152, -0.0087, -0.0097, -0.0008,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[0.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286101297699496, distance: 0.9668853712936949 entropy -11.133079876208713
epoch: 2, step: 17
	action: tensor([[-0.0082,  0.0137,  0.0054, -0.0090, -0.0116,  0.0014, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864113813200071, distance: 0.9666753639922161 entropy -11.125885718062497
epoch: 2, step: 18
	action: tensor([[-0.0085,  0.0143,  0.0034, -0.0086, -0.0011,  0.0077,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[0.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882964994954683, distance: 0.9653976653619604 entropy -11.134871451654586
epoch: 2, step: 19
	action: tensor([[-0.0084,  0.0141,  0.0125, -0.0088, -0.0082, -0.0006, -0.0050]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28616012804614166, distance: 0.9668455313463048 entropy -11.126712662066819
epoch: 2, step: 20
	action: tensor([[-0.0084,  0.0142,  0.0116, -0.0087, -0.0064, -0.0031,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2856576278228494, distance: 0.9671857719548947 entropy -11.135191892603283
epoch: 2, step: 21
	action: tensor([[-0.0083,  0.0139,  0.0104, -0.0089, -0.0041, -0.0108, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[0.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2837624954448713, distance: 0.9684678819513819 entropy -11.12738430455259
epoch: 2, step: 22
	action: tensor([[-0.0084,  0.0142,  0.0174, -0.0087, -0.0125,  0.0165, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[0.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29025427256866076, distance: 0.9640689300602197 entropy -11.136894717334718
epoch: 2, step: 23
	action: tensor([[-0.0086,  0.0140,  0.0054, -0.0085, -0.0056, -0.0149, -0.0343]],
       dtype=torch.float64)
	q_value: tensor([[0.4054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28284541319543377, distance: 0.9690877046880764 entropy -11.138001287806315
epoch: 2, step: 24
	action: tensor([[-8.4527e-03,  1.4464e-02, -2.3542e-05, -8.5779e-03, -8.8629e-03,
          6.7095e-05,  1.7329e-02]], dtype=torch.float64)
	q_value: tensor([[0.4070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28667985822525544, distance: 0.966493498405405 entropy -11.141324949446735
epoch: 2, step: 25
	action: tensor([[-0.0083,  0.0141,  0.0114, -0.0089, -0.0059,  0.0088, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883536553249998, distance: 0.9653588997719257 entropy -11.124147317350088
epoch: 2, step: 26
	action: tensor([[-0.0084,  0.0140,  0.0133, -0.0086, -0.0030, -0.0326, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2787235792721092, distance: 0.9718686222340157 entropy -11.135136593401201
epoch: 2, step: 27
	action: tensor([[-0.0081,  0.0142,  0.0088, -0.0089, -0.0095, -0.0112,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[0.4077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2839970004520692, distance: 0.9683093247975546 entropy -11.135893822082197
epoch: 2, step: 28
	action: tensor([[-0.0082,  0.0139,  0.0025, -0.0091, -0.0196, -0.0330,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2788905409844227, distance: 0.9717561312139028 entropy -11.122015393096012
epoch: 2, step: 29
	action: tensor([[-0.0080,  0.0141,  0.0041, -0.0093, -0.0104,  0.0078,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[0.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882823875279803, distance: 0.9654072364775469 entropy -11.116606404975068
epoch: 2, step: 30
	action: tensor([[-0.0084,  0.0142,  0.0076, -0.0087,  0.0037,  0.0076, -0.0045]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288196047699248, distance: 0.9654657924035321 entropy -11.13018941974603
epoch: 2, step: 31
	action: tensor([[-0.0084,  0.0141,  0.0099, -0.0086, -0.0032,  0.0082,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28829088849310025, distance: 0.9654014709059167 entropy -11.13283639052111
epoch: 2, step: 32
	action: tensor([[-0.0084,  0.0139, -0.0014, -0.0088, -0.0101, -0.0178, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28222273651442986, distance: 0.9695083234781634 entropy -11.130024273783494
epoch: 2, step: 33
	action: tensor([[-0.0083,  0.0143,  0.0034, -0.0089, -0.0049, -0.0053, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[0.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2854050841918121, distance: 0.9673567229105356 entropy -11.129818327965392
epoch: 2, step: 34
	action: tensor([[-0.0084,  0.0144,  0.0129, -0.0087, -0.0105,  0.0065,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28803616960456735, distance: 0.9655742128220558 entropy -11.133916901192597
epoch: 2, step: 35
	action: tensor([[-0.0084,  0.0141,  0.0086, -0.0087, -0.0132,  0.0102, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28867670194818895, distance: 0.9651397661149609 entropy -11.13390930773538
epoch: 2, step: 36
	action: tensor([[-0.0087,  0.0144, -0.0008, -0.0083, -0.0105,  0.0090, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[0.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2885500685330805, distance: 0.9652256718528639 entropy -11.141961094160264
epoch: 2, step: 37
	action: tensor([[-0.0085,  0.0143,  0.0164, -0.0085, -0.0026, -0.0061,  0.0158]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849812077075652, distance: 0.9676435840187135 entropy -11.13333140919086
epoch: 2, step: 38
	action: tensor([[-0.0082,  0.0138,  0.0098, -0.0090, -0.0085,  0.0012, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[0.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864786819837325, distance: 0.9666297779038707 entropy -11.127621241559199
epoch: 2, step: 39
	action: tensor([[-0.0084,  0.0141,  0.0064, -0.0087, -0.0066,  0.0105,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28877653807576587, distance: 0.9650720337643759 entropy -11.134656291080281
epoch: 2, step: 40
	action: tensor([[-0.0083,  0.0138,  0.0011, -0.0089, -0.0112, -0.0096, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[0.4053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.283991717133397, distance: 0.9683128973226013 entropy -11.122026627282244
epoch: 2, step: 41
	action: tensor([[-0.0084,  0.0144,  0.0051, -0.0087, -0.0036, -0.0002,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[0.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28654732959167917, distance: 0.9665832772482734 entropy -11.134781121706647
epoch: 2, step: 42
	action: tensor([[-0.0083,  0.0140,  0.0021, -0.0089, -0.0033,  0.0122,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891829019937493, distance: 0.9647962930813754 entropy -11.128295599022177
epoch: 2, step: 43
	action: tensor([[-0.0085,  0.0142,  0.0096, -0.0086, -0.0074, -0.0157, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[0.4055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28276711473876937, distance: 0.9691406054169286 entropy -11.131714484506217
epoch: 2, step: 44
	action: tensor([[-0.0083,  0.0142,  0.0058, -0.0088, -0.0116, -0.0326,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.4068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2791035705782502, distance: 0.97161258289229 entropy -11.13344892235507
epoch: 2, step: 45
	action: tensor([[-0.0079,  0.0139,  0.0113, -0.0093, -0.0083, -0.0044,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[0.4070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28538791543051145, distance: 0.9673683436319716 entropy -11.117626099565303
epoch: 2, step: 46
	action: tensor([[-0.0082,  0.0139,  0.0150, -0.0090, -0.0011,  0.0051,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[0.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28741735743446684, distance: 0.9659937420676562 entropy -11.127844425109819
epoch: 2, step: 47
	action: tensor([[-0.0083,  0.0139,  0.0138, -0.0088, -0.0195,  0.0122, -0.0346]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28900289700243864, distance: 0.9649184462734156 entropy -11.131296276564893
epoch: 2, step: 48
	action: tensor([[-0.0088,  0.0144,  0.0179, -0.0082, -0.0053,  0.0088, -0.0614]],
       dtype=torch.float64)
	q_value: tensor([[0.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28845461725331467, distance: 0.9652904191608356 entropy -11.146503228380283
epoch: 2, step: 49
	action: tensor([[-0.0088,  0.0144,  0.0044, -0.0081, -0.0093, -0.0039,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28552156903113757, distance: 0.9672778761524922 entropy -11.152789345428399
epoch: 2, step: 50
	action: tensor([[-0.0083,  0.0140,  0.0028, -0.0089, -0.0146,  0.0116, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889867926136763, distance: 0.9649293741194818 entropy -11.125802362327866
epoch: 2, step: 51
	action: tensor([[-8.5559e-03,  1.4357e-02,  1.4855e-05, -8.4749e-03, -1.1602e-02,
         -8.4462e-03,  8.2591e-03]], dtype=torch.float64)
	q_value: tensor([[0.4055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28462258481943814, distance: 0.9678862179459143 entropy -11.13485078349242
epoch: 2, step: 52
	action: tensor([[-0.0083,  0.0142,  0.0202, -0.0089, -0.0126,  0.0011,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[0.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28660558599179464, distance: 0.9665438136552371 entropy -11.126864233939935
epoch: 2, step: 53
	action: tensor([[-0.0083,  0.0137,  0.0089, -0.0090, -0.0083,  0.0058,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2873814010797189, distance: 0.9660181133991009 entropy -11.129142223641546
epoch: 2, step: 54
	action: tensor([[-0.0084,  0.0141,  0.0002, -0.0087, -0.0058, -0.0115, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2837526900606836, distance: 0.9684745111540519 entropy -11.130576016656052
epoch: 2, step: 55
	action: tensor([[-0.0084,  0.0144,  0.0015, -0.0087,  0.0022, -0.0022, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[0.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28618790138891204, distance: 0.9668267226508285 entropy -11.133029760540813
epoch: 2, step: 56
	action: tensor([[-0.0085,  0.0145,  0.0090, -0.0085, -0.0072,  0.0022,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[0.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28710103062758807, distance: 0.96620812829695 entropy -11.136748133100548
epoch: 2, step: 57
	action: tensor([[-0.0083,  0.0140,  0.0162, -0.0089, -0.0073, -0.0132,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[0.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2831660655694551, distance: 0.9688710324245344 entropy -11.126724472401317
epoch: 2, step: 58
	action: tensor([[-0.0082,  0.0140,  0.0047, -0.0089, -0.0035, -0.0094,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[0.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2842362164668337, distance: 0.9681475556091642 entropy -11.13291106093456
epoch: 2, step: 59
	action: tensor([[-0.0082,  0.0141,  0.0073, -0.0089, -0.0066,  0.0048,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[0.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2875453019037656, distance: 0.9659070159129574 entropy -11.127182347584435
epoch: 2, step: 60
	action: tensor([[-0.0083,  0.0139,  0.0080, -0.0089, -0.0051, -0.0293,  0.0387]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2794729057572237, distance: 0.9713636590062703 entropy -11.127529815023575
epoch: 2, step: 61
	action: tensor([[-0.0079,  0.0137, -0.0102, -0.0094, -0.0046, -0.0262,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[0.4069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28047629674880015, distance: 0.9706870729963948 entropy -11.113393780832952
epoch: 2, step: 62
	action: tensor([[-0.0081,  0.0143,  0.0113, -0.0091, -0.0062, -0.0040,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[0.4069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2856824088110753, distance: 0.9671689956666407 entropy -11.11967936807518
epoch: 2, step: 63
	action: tensor([[-0.0083,  0.0139,  0.0120, -0.0089, -0.0053, -0.0051, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[0.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28507290025663123, distance: 0.9675815377113676 entropy -11.128672882848578
epoch: 2, step: 64
	action: tensor([[-0.0084,  0.0143,  0.0018, -0.0086,  0.0050,  0.0124,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[0.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28939420493077106, distance: 0.9646528810533409 entropy -11.137832544918377
epoch: 2, step: 65
	action: tensor([[-0.0084,  0.0142,  0.0138, -0.0086,  0.0006, -0.0328,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27873459075311235, distance: 0.9718612036123827 entropy -11.129241658330526
epoch: 2, step: 66
	action: tensor([[-0.0080,  0.0139,  0.0134, -0.0092,  0.0025, -0.0010,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.4075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28613886198107963, distance: 0.9668599329282639 entropy -11.125230441936775
epoch: 2, step: 67
	action: tensor([[-0.0083,  0.0139,  0.0168, -0.0089, -0.0136,  0.0043,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[0.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2872481557537002, distance: 0.9661084221437275 entropy -11.12978718742166
epoch: 2, step: 68
	action: tensor([[-0.0084,  0.0139,  0.0083, -0.0088, -0.0044,  0.0121,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28899554161335683, distance: 0.9649234373853477 entropy -11.134183064267267
epoch: 2, step: 69
	action: tensor([[-0.0084,  0.0139,  0.0076, -0.0087, -0.0044,  0.0100, -0.0116]],
       dtype=torch.float64)
	q_value: tensor([[0.4054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28852957640362564, distance: 0.9652395726111035 entropy -11.130834575863455
epoch: 2, step: 70
	action: tensor([[-0.0085,  0.0142,  0.0100, -0.0085, -0.0062,  0.0172,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[0.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29038590660254326, distance: 0.963979524684486 entropy -11.137792825436948
epoch: 2, step: 71
	action: tensor([[-0.0085,  0.0139,  0.0154, -0.0086, -0.0035,  0.0148,  0.0292]],
       dtype=torch.float64)
	q_value: tensor([[0.4052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28956494575360725, distance: 0.9645369830951086 entropy -11.131697231700727
epoch: 2, step: 72
	action: tensor([[-0.0083,  0.0136,  0.0018, -0.0089, -0.0146,  0.0090,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[0.4052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28806740129701724, distance: 0.9655530341846792 entropy -11.126648863336275
epoch: 2, step: 73
	action: tensor([[-0.0084,  0.0140,  0.0091, -0.0088, -0.0056, -0.0093,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[0.4052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28409837204848265, distance: 0.9682407758272843 entropy -11.12544868869197
epoch: 2, step: 74
	action: tensor([[-0.0082,  0.0139, -0.0009, -0.0091, -0.0103, -0.0119,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2836132566369872, distance: 0.9685687740878159 entropy -11.121427617243942
epoch: 2, step: 75
	action: tensor([[-0.0082,  0.0141,  0.0033, -0.0090, -0.0098,  0.0041, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[0.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28736097244566916, distance: 0.966031959718243 entropy -11.122922009298259
epoch: 2, step: 76
	action: tensor([[-0.0084,  0.0143,  0.0171, -0.0087, -0.0008,  0.0028, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28708264126107863, distance: 0.9662205899791649 entropy -11.131743776346655
epoch: 2, step: 77
	action: tensor([[-0.0084,  0.0140,  0.0141, -0.0087, -0.0037,  0.0045, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2873151799926392, distance: 0.9660629966533624 entropy -11.135001287841744
epoch: 2, step: 78
	action: tensor([[-0.0086,  0.0142,  0.0103, -0.0084, -0.0020, -0.0083,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28442486538906586, distance: 0.9680199632155516 entropy -11.142997385527652
epoch: 2, step: 79
	action: tensor([[-0.0082,  0.0137,  0.0038, -0.0091, -0.0037, -0.0096,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2839645394843181, distance: 0.9683312743574987 entropy -11.121862787528297
epoch: 2, step: 80
	action: tensor([[-0.0082,  0.0141,  0.0047, -0.0090,  0.0020,  0.0016, -0.0234]],
       dtype=torch.float64)
	q_value: tensor([[0.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2867630920512253, distance: 0.9664371090692888 entropy -11.125763245575811
epoch: 2, step: 81
	action: tensor([[-0.0085,  0.0143,  0.0097, -0.0085, -0.0084, -0.0086, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[0.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845152704869338, distance: 0.9679588119125216 entropy -11.139158945341602
epoch: 2, step: 82
	action: tensor([[-0.0084,  0.0143,  0.0008, -0.0087,  0.0001, -0.0047,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[0.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2854431018710988, distance: 0.9673309900470685 entropy -11.134965187112341
epoch: 2, step: 83
	action: tensor([[-0.0083,  0.0141, -0.0010, -0.0089, -0.0104,  0.0081, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[0.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28832052719020307, distance: 0.9653813689153578 entropy -11.123943588990418
epoch: 2, step: 84
	action: tensor([[-0.0086,  0.0145,  0.0063, -0.0085, -0.0049,  0.0041,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[0.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28751499773423017, distance: 0.9659275580602622 entropy -11.135453591008726
epoch: 2, step: 85
	action: tensor([[-8.2697e-03,  1.3822e-02,  1.7755e-02, -8.9579e-03, -1.2870e-05,
         -9.5408e-03,  3.1305e-03]], dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2838656568601168, distance: 0.9683981340573791 entropy -11.121568021407388
epoch: 2, step: 86
	action: tensor([[-0.0082,  0.0139,  0.0070, -0.0089, -0.0004, -0.0089,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[0.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28422121476536644, distance: 0.9681577012639451 entropy -11.13343478830843
epoch: 2, step: 87
	action: tensor([[-0.0082,  0.0141,  0.0083, -0.0089, -0.0046, -0.0095, -0.0474]],
       dtype=torch.float64)
	q_value: tensor([[0.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28421096012062197, distance: 0.9681646364217964 entropy -11.12986163410348
epoch: 2, step: 88
	action: tensor([[-0.0086,  0.0146,  0.0060, -0.0084, -0.0073,  0.0171, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[0.4071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2906301471637085, distance: 0.963813615379072 entropy -11.145218332870158
epoch: 2, step: 89
	action: tensor([[-0.0086,  0.0141,  0.0035, -0.0085, -0.0132, -0.0156,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[0.4053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28280924519530637, distance: 0.9691121412063439 entropy -11.134825470249456
epoch: 2, step: 90
	action: tensor([[-0.0082,  0.0142,  0.0088, -0.0089, -0.0087,  0.0016,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[0.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286792718842774, distance: 0.9664170366867469 entropy -11.127671125299326
epoch: 2, step: 91
	action: tensor([[-0.0083,  0.0138, -0.0041, -0.0090, -0.0095, -0.0113,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28362434867737496, distance: 0.9685612757319025 entropy -11.123001728863267
epoch: 2, step: 92
	action: tensor([[-0.0082,  0.0142,  0.0026, -0.0090, -0.0040, -0.0067,  0.0050]],
       dtype=torch.float64)
	q_value: tensor([[0.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849462084136454, distance: 0.9676672662129068 entropy -11.12214347273582
epoch: 2, step: 93
	action: tensor([[-0.0083,  0.0141,  0.0078, -0.0089, -0.0132,  0.0058, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877402817419247, distance: 0.9657748353818346 entropy -11.12824570230859
epoch: 2, step: 94
	action: tensor([[-0.0085,  0.0142,  0.0041, -0.0086, -0.0083,  0.0082, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883067746666964, distance: 0.965390696405006 entropy -11.13580692330214
epoch: 2, step: 95
	action: tensor([[-0.0086,  0.0144,  0.0184, -0.0085, -0.0090, -0.0110, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28386840697583804, distance: 0.9683962746231016 entropy -11.136461819488677
epoch: 2, step: 96
	action: tensor([[-0.0083,  0.0140,  0.0123, -0.0088, -0.0130, -0.0189, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[0.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2819496221691704, distance: 0.9696927549594967 entropy -11.135961763974484
epoch: 2, step: 97
	action: tensor([[-0.0083,  0.0143,  0.0127, -0.0088, -0.0120,  0.0081,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[0.4069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28836202806827216, distance: 0.9653532208795461 entropy -11.135527483034354
epoch: 2, step: 98
	action: tensor([[-8.4176e-03,  1.4029e-02,  1.1673e-02, -8.7008e-03,  3.2285e-06,
          1.6549e-02, -9.5925e-03]], dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901431260976415, distance: 0.9641444138993871 entropy -11.132769555240364
epoch: 2, step: 99
	action: tensor([[-0.0086,  0.0141,  0.0113, -0.0085, -0.0062,  0.0057,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2875788280856727, distance: 0.965884289165992 entropy -11.138377682126078
epoch: 2, step: 100
	action: tensor([[-0.0084,  0.0139,  0.0165, -0.0088, -0.0007, -0.0074,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2844316659219791, distance: 0.9680153633720532 entropy -11.131466780057151
epoch: 2, step: 101
	action: tensor([[-0.0082,  0.0138,  0.0113, -0.0090, -0.0014, -0.0110,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[0.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2836093907667855, distance: 0.9685713874500433 entropy -11.12941175635787
epoch: 2, step: 102
	action: tensor([[-0.0081,  0.0138,  0.0153, -0.0091,  0.0058,  0.0117,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[0.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28888474243216933, distance: 0.9649986187436751 entropy -11.12015721977133
epoch: 2, step: 103
	action: tensor([[-0.0082,  0.0135,  0.0059, -0.0090, -0.0011, -0.0142, -0.0435]],
       dtype=torch.float64)
	q_value: tensor([[0.4055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2826748019674986, distance: 0.9692029709180106 entropy -11.121721096241062
epoch: 2, step: 104
	action: tensor([[-0.0085,  0.0146,  0.0117, -0.0085, -0.0056,  0.0119,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.4073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894561644244822, distance: 0.9646108248898185 entropy -11.142995240549888
epoch: 2, step: 105
	action: tensor([[-0.0084,  0.0139,  0.0065, -0.0087, -0.0069, -0.0195,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[0.4055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2817180233462383, distance: 0.9698491239285918 entropy -11.129372269862708
epoch: 2, step: 106
	action: tensor([[-0.0081,  0.0140,  0.0035, -0.0091,  0.0006,  0.0370,  0.0516]],
       dtype=torch.float64)
	q_value: tensor([[0.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29506137052077164, distance: 0.960798570391555 entropy -11.126027821241864
epoch: 2, step: 107
	action: tensor([[-0.0085,  0.0135,  0.0101, -0.0088, -0.0004, -0.0179, -0.0266]],
       dtype=torch.float64)
	q_value: tensor([[0.4040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28161830843428637, distance: 0.9699164408507814 entropy -11.12121483442054
epoch: 2, step: 108
	action: tensor([[-0.0084,  0.0143,  0.0040, -0.0087,  0.0019, -0.0178,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[0.4072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2824644207408483, distance: 0.9693450871870698 entropy -11.139932543789723
epoch: 2, step: 109
	action: tensor([[-0.0081,  0.0139,  0.0161, -0.0091, -0.0040,  0.0106,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[0.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887618537456844, distance: 0.9650819964300553 entropy -11.120550329058677
epoch: 2, step: 110
	action: tensor([[-0.0084,  0.0138,  0.0058, -0.0088, -0.0027, -0.0064,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2846499890118497, distance: 0.9678676792042596 entropy -11.130463482858099
epoch: 2, step: 111
	action: tensor([[-0.0082,  0.0138,  0.0003, -0.0091, -0.0120, -0.0010, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[0.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2860052500563437, distance: 0.966950411289879 entropy -11.118422935651692
epoch: 2, step: 112
	action: tensor([[-0.0085,  0.0144,  0.0056, -0.0086, -0.0021, -0.0022, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[0.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28604936556699834, distance: 0.9669205384020783 entropy -11.135132139209391
epoch: 2, step: 113
	action: tensor([[-0.0085,  0.0144,  0.0105, -0.0085, -0.0068,  0.0019, -0.0119]],
       dtype=torch.float64)
	q_value: tensor([[0.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28698425164297525, distance: 0.966287261658497 entropy -11.138697570925558
epoch: 2, step: 114
	action: tensor([[-0.0085,  0.0142,  0.0122, -0.0086, -0.0098, -0.0144, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28308818060912444, distance: 0.9689236655580207 entropy -11.136535091356068
epoch: 2, step: 115
	action: tensor([[-0.0083,  0.0141,  0.0135, -0.0089, -0.0047, -0.0153,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[0.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828716321316246, distance: 0.9690699897620371 entropy -11.132818499851622
epoch: 2, step: 116
	action: tensor([[-0.0081,  0.0139,  0.0239, -0.0091, -0.0094, -0.0007,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[0.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2860446551869066, distance: 0.9669237280874168 entropy -11.124774923789703
epoch: 2, step: 117
	action: tensor([[-0.0083,  0.0139,  0.0088, -0.0088, -0.0095, -0.0037,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[0.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28534197545486495, distance: 0.967399437539346 entropy -11.135995459871532
epoch: 2, step: 118
	action: tensor([[-0.0083,  0.0142,  0.0075, -0.0088, -0.0142, -0.0107,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2839914849280367, distance: 0.9683130543371445 entropy -11.132081804620796
epoch: 2, step: 119
	action: tensor([[-0.0082,  0.0140, -0.0009, -0.0090, -0.0075, -0.0074,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[0.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28470156902217936, distance: 0.9678327847301776 entropy -11.126756955980753
epoch: 2, step: 120
	action: tensor([[-0.0082,  0.0139,  0.0232, -0.0092, -0.0061, -0.0261, -0.0287]],
       dtype=torch.float64)
	q_value: tensor([[0.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28002596341258246, distance: 0.9709907908375124 entropy -11.114392218259969
epoch: 2, step: 121
	action: tensor([[-0.0083,  0.0141,  0.0174, -0.0088, -0.0028,  0.0099, -0.0169]],
       dtype=torch.float64)
	q_value: tensor([[0.4076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28872985482859437, distance: 0.9651037059176688 entropy -11.142708266921515
epoch: 2, step: 122
	action: tensor([[-0.0086,  0.0141, -0.0019, -0.0085, -0.0028, -0.0062, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849215689238356, distance: 0.9676839381211141 entropy -11.14187760002834
epoch: 2, step: 123
	action: tensor([[-0.0084,  0.0144,  0.0097, -0.0087, -0.0035, -0.0036, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[0.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2857128124464605, distance: 0.9671484125487342 entropy -11.132635055717298
epoch: 2, step: 124
	action: tensor([[-0.0086,  0.0144,  0.0122, -0.0085, -0.0028,  0.0004, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[0.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2865427769408072, distance: 0.9665863612013949 entropy -11.143023313771518
epoch: 2, step: 125
	action: tensor([[-0.0084,  0.0141,  0.0094, -0.0088, -0.0052,  0.0039,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[0.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28723639175739735, distance: 0.9661163949389593 entropy -11.1331393220666
epoch: 2, step: 126
	action: tensor([[-0.0084,  0.0141,  0.0094, -0.0087, -0.0076,  0.0132, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[0.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893829013934708, distance: 0.9646605533426302 entropy -11.131160277245758
epoch: 2, step: 127
	action: tensor([[-0.0085,  0.0141,  0.0073, -0.0086, -0.0061,  0.0204,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.4055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2910257861627029, distance: 0.9635448025472907 entropy -11.135697719346386
LOSS epoch 2 actor 0.08269522342535367 critic 18.234125920764804
epoch: 3, step: 0
	action: tensor([[-0.0070,  0.0144,  0.0071, -0.0095, -0.0069, -0.0007, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[0.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28745763973030514, distance: 0.9659664378692958 entropy -11.124947820332034
epoch: 3, step: 1
	action: tensor([[-0.0069,  0.0146, -0.0014, -0.0096,  0.0032, -0.0016, -0.0269]],
       dtype=torch.float64)
	q_value: tensor([[0.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2875136314317961, distance: 0.9659284842190843 entropy -11.129181152102982
epoch: 3, step: 2
	action: tensor([[-0.0070,  0.0150,  0.0090, -0.0094,  0.0070,  0.0110,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[0.7310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2906444277931548, distance: 0.9638039138571409 entropy -11.13123025393715
epoch: 3, step: 3
	action: tensor([[-0.0069,  0.0143,  0.0082, -0.0097, -0.0049, -0.0145,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.284253326105101, distance: 0.9681359842263709 entropy -11.12120466438014
epoch: 3, step: 4
	action: tensor([[-0.0067,  0.0144,  0.0162, -0.0100, -0.0072,  0.0099, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[0.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2900093400287609, distance: 0.9642352653221667 entropy -11.116894836174565
epoch: 3, step: 5
	action: tensor([[-0.0071,  0.0146,  0.0099, -0.0093, -0.0086, -0.0026,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[0.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28717395641546006, distance: 0.966158708046618 entropy -11.136390552069452
epoch: 3, step: 6
	action: tensor([[-0.0068,  0.0145,  0.0056, -0.0098, -0.0124,  0.0050,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[0.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288892710292716, distance: 0.9649932124499144 entropy -11.125598553355289
epoch: 3, step: 7
	action: tensor([[-0.0069,  0.0145,  0.0075, -0.0097, -0.0088,  0.0048,  0.0401]],
       dtype=torch.float64)
	q_value: tensor([[0.7297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888143455955139, distance: 0.9650463825683463 entropy -11.124557420684567
epoch: 3, step: 8
	action: tensor([[-0.0068,  0.0140, -0.0086, -0.0100, -0.0076, -0.0055,  0.0158]],
       dtype=torch.float64)
	q_value: tensor([[0.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286223996804445, distance: 0.9668022775247552 entropy -11.111817703047505
epoch: 3, step: 9
	action: tensor([[-0.0068,  0.0147,  0.0023, -0.0098, -0.0114, -0.0103,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[0.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2856001206754355, distance: 0.9672247021496942 entropy -11.114900191511769
epoch: 3, step: 10
	action: tensor([[-0.0068,  0.0146,  0.0159, -0.0098, -0.0013, -0.0045,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[0.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28677195908741493, distance: 0.9664311016267688 entropy -11.123393423453177
epoch: 3, step: 11
	action: tensor([[-0.0068,  0.0143,  0.0178, -0.0098, -0.0061,  0.0345,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[0.7312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29572959354086203, distance: 0.9603430839740947 entropy -11.127585255921842
epoch: 3, step: 12
	action: tensor([[-0.0072,  0.0142,  0.0054, -0.0093, -0.0066,  0.0201,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[0.7276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29207994891450917, distance: 0.9628281961664842 entropy -11.132069927157607
epoch: 3, step: 13
	action: tensor([[-0.0069,  0.0142, -0.0034, -0.0097, -0.0121, -0.0066,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28604037335253707, distance: 0.966926627569208 entropy -11.119486323067736
epoch: 3, step: 14
	action: tensor([[-0.0068,  0.0147,  0.0181, -0.0098, -0.0162,  0.0002, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[0.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2879035111200111, distance: 0.9656641651764716 entropy -11.118607781556658
epoch: 3, step: 15
	action: tensor([[-0.0071,  0.0146,  0.0115, -0.0095, -0.0018,  0.0136,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[0.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29093647753465035, distance: 0.9636054889254974 entropy -11.136128757515905
epoch: 3, step: 16
	action: tensor([[-0.0069,  0.0143,  0.0107, -0.0096, -0.0057,  0.0028, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[0.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28822174461696337, distance: 0.9654483650495108 entropy -11.124563944056462
epoch: 3, step: 17
	action: tensor([[-0.0069,  0.0146,  0.0019, -0.0096, -0.0084,  0.0190,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[0.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29221714745721394, distance: 0.962734891120134 entropy -11.12888437166924
epoch: 3, step: 18
	action: tensor([[-0.0071,  0.0146,  0.0139, -0.0094, -0.0020,  0.0137,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[0.7286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2909328741335012, distance: 0.9636079374033873 entropy -11.126852061022234
epoch: 3, step: 19
	action: tensor([[-0.0069,  0.0142,  0.0109, -0.0097, -0.0055,  0.0041, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[0.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288444459850999, distance: 0.9652973089590027 entropy -11.126126886050644
epoch: 3, step: 20
	action: tensor([[-0.0069,  0.0146,  0.0080, -0.0096, -0.0098,  0.0075,  0.0258]],
       dtype=torch.float64)
	q_value: tensor([[0.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28950307279211396, distance: 0.9645789837395963 entropy -11.128885462534992
epoch: 3, step: 21
	action: tensor([[-0.0068,  0.0143,  0.0083, -0.0098, -0.0098, -0.0200, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[0.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28298076734148625, distance: 0.968996248654119 entropy -11.11786347026702
epoch: 3, step: 22
	action: tensor([[-0.0067,  0.0146,  0.0140, -0.0099, -0.0078,  0.0063,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[0.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28932790070254255, distance: 0.9646978842557592 entropy -11.126119656863898
epoch: 3, step: 23
	action: tensor([[-0.0069,  0.0144,  0.0099, -0.0097, -0.0123,  0.0121,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[0.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29044020461359044, distance: 0.9639426432494349 entropy -11.126162168702619
epoch: 3, step: 24
	action: tensor([[-0.0070,  0.0145,  0.0154, -0.0096, -0.0174,  0.0057,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[0.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28894797765538915, distance: 0.9649557120143847 entropy -11.126136217431798
epoch: 3, step: 25
	action: tensor([[-0.0068,  0.0140,  0.0056, -0.0100, -0.0152,  0.0233,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[0.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2927082882576192, distance: 0.9624008052920218 entropy -11.114632483945519
epoch: 3, step: 26
	action: tensor([[-7.0620e-03,  1.4485e-02,  1.9224e-02, -9.4758e-03,  1.1920e-05,
         -1.0130e-02,  5.1908e-03]], dtype=torch.float64)
	q_value: tensor([[0.7281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2851997200472036, distance: 0.9674957150238046 entropy -11.124844169683785
epoch: 3, step: 27
	action: tensor([[-0.0067,  0.0143,  0.0070, -0.0099, -0.0065,  0.0071, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[0.7318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28934385837279086, distance: 0.9646870533696446 entropy -11.125516124900164
epoch: 3, step: 28
	action: tensor([[-0.0070,  0.0146,  0.0061, -0.0095, -0.0068, -0.0032,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28709915106937434, distance: 0.9662094020000305 entropy -11.13076750487805
epoch: 3, step: 29
	action: tensor([[-0.0069,  0.0146,  0.0117, -0.0097, -0.0009,  0.0031,  0.0399]],
       dtype=torch.float64)
	q_value: tensor([[0.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28851051077739454, distance: 0.9652525055265265 entropy -11.126164144889643
epoch: 3, step: 30
	action: tensor([[-0.0067,  0.0140,  0.0161, -0.0101, -0.0101,  0.0045, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[0.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28838779036951034, distance: 0.9653357471453284 entropy -11.112071948588452
epoch: 3, step: 31
	action: tensor([[-0.0070,  0.0145,  0.0161, -0.0095, -0.0127,  0.0073,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[0.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893500471662256, distance: 0.9646828528417445 entropy -11.133419111784848
epoch: 3, step: 32
	action: tensor([[-0.0068,  0.0141,  0.0097, -0.0099, -0.0050, -0.0026, -0.0345]],
       dtype=torch.float64)
	q_value: tensor([[0.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2867760047882816, distance: 0.9664283606403901 entropy -11.119635393053528
epoch: 3, step: 33
	action: tensor([[-0.0071,  0.0148,  0.0002, -0.0094, -0.0126, -0.0190, -0.0045]],
       dtype=torch.float64)
	q_value: tensor([[0.7310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2836902321007635, distance: 0.9685167365297952 entropy -11.138500815691371
epoch: 3, step: 34
	action: tensor([[-0.0068,  0.0148,  0.0074, -0.0098, -0.0039,  0.0076, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[0.7318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2897642522098055, distance: 0.964401677319143 entropy -11.125117978512378
epoch: 3, step: 35
	action: tensor([[-0.0070,  0.0146,  0.0042, -0.0095, -0.0046, -0.0142,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28457005982833405, distance: 0.9679217497370866 entropy -11.127337209188358
epoch: 3, step: 36
	action: tensor([[-0.0067,  0.0145, -0.0007, -0.0100, -0.0007,  0.0052, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[0.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28907034078382066, distance: 0.964872680058738 entropy -11.117508804112166
epoch: 3, step: 37
	action: tensor([[-0.0071,  0.0149,  0.0108, -0.0094, -0.0053,  0.0182, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[0.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29222769350940647, distance: 0.9627277186588391 entropy -11.130160893967679
epoch: 3, step: 38
	action: tensor([[-0.0072,  0.0147,  0.0141, -0.0092, -0.0079,  0.0021,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[0.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881968606652122, distance: 0.9654652410641849 entropy -11.136203558148342
epoch: 3, step: 39
	action: tensor([[-0.0068,  0.0142,  0.0149, -0.0099, -0.0133,  0.0068, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[0.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2890923364785355, distance: 0.9648577536811468 entropy -11.120723028818569
epoch: 3, step: 40
	action: tensor([[-0.0071,  0.0146,  0.0104, -0.0094, -0.0119,  0.0109, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2902679367898894, distance: 0.9640596497545868 entropy -11.13629902752469
epoch: 3, step: 41
	action: tensor([[-0.0071,  0.0146,  0.0132, -0.0094, -0.0122,  0.0059, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[0.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28910121903763186, distance: 0.9648517258710473 entropy -11.133339142016377
epoch: 3, step: 42
	action: tensor([[-0.0071,  0.0146,  0.0077, -0.0095, -0.0137, -0.0140,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845804281892351, distance: 0.9679147359137735 entropy -11.134015422382932
epoch: 3, step: 43
	action: tensor([[-0.0068,  0.0146,  0.0063, -0.0098, -0.0062, -0.0009,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[0.7315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28770310470477, distance: 0.965800039796398 entropy -11.12497210986886
epoch: 3, step: 44
	action: tensor([[-0.0068,  0.0145,  0.0028, -0.0098, -0.0056, -0.0085,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[0.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28584087889402443, distance: 0.9670617073579634 entropy -11.121278915034944
epoch: 3, step: 45
	action: tensor([[-0.0067,  0.0144,  0.0096, -0.0100, -0.0190, -0.0285,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[0.7310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2812572839557864, distance: 0.9701601271797328 entropy -11.113825179705822
epoch: 3, step: 46
	action: tensor([[-0.0066,  0.0145,  0.0086, -0.0101, -0.0134,  0.0187,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[0.7326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921737087647289, distance: 0.9627644335872173 entropy -11.117241669852971
epoch: 3, step: 47
	action: tensor([[-0.0070,  0.0144,  0.0083, -0.0095, -0.0028, -0.0065,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[0.7286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28613558703417064, distance: 0.9668621507344649 entropy -11.12816126771603
epoch: 3, step: 48
	action: tensor([[-0.0067,  0.0145,  0.0089, -0.0099, -0.0126,  0.0175, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[0.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2918166705250538, distance: 0.9630072194136254 entropy -11.12173824786725
epoch: 3, step: 49
	action: tensor([[-0.0071,  0.0147,  0.0096, -0.0093,  0.0023,  0.0021,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[0.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28826858827941604, distance: 0.9654165954069983 entropy -11.132053526258401
epoch: 3, step: 50
	action: tensor([[-0.0068,  0.0142,  0.0098, -0.0098, -0.0012, -0.0166,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[0.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28372846592084, distance: 0.9684908883629485 entropy -11.120923138690001
epoch: 3, step: 51
	action: tensor([[-0.0067,  0.0145,  0.0035, -0.0099, -0.0072, -0.0040,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286910933199108, distance: 0.9663369413867101 entropy -11.12420074732808
epoch: 3, step: 52
	action: tensor([[-0.0068,  0.0145,  0.0027, -0.0099, -0.0070, -0.0042,  0.0178]],
       dtype=torch.float64)
	q_value: tensor([[0.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28680531808291476, distance: 0.9664085004768553 entropy -11.120009868755021
epoch: 3, step: 53
	action: tensor([[-0.0068,  0.0145,  0.0007, -0.0099, -0.0075, -0.0032,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[0.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28707864358570545, distance: 0.9662232990104399 entropy -11.117426574044211
epoch: 3, step: 54
	action: tensor([[-0.0069,  0.0147,  0.0060, -0.0097, -0.0155, -0.0016,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[0.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2875717785754156, distance: 0.9658890679363514 entropy -11.123173090390777
epoch: 3, step: 55
	action: tensor([[-0.0069,  0.0146,  0.0135, -0.0097, -0.0113,  0.0065,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[0.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892489988542948, distance: 0.9647514352067307 entropy -11.12549184648866
epoch: 3, step: 56
	action: tensor([[-0.0069,  0.0143,  0.0076, -0.0097, -0.0161,  0.0050,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[0.7298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887221854910069, distance: 0.9651089090646204 entropy -11.125921594818577
epoch: 3, step: 57
	action: tensor([[-0.0069,  0.0146,  0.0099, -0.0096, -0.0098, -0.0020,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[0.7297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2873292963656253, distance: 0.9660534290487661 entropy -11.126913522957508
epoch: 3, step: 58
	action: tensor([[-0.0068,  0.0144,  0.0063, -0.0098, -0.0102, -0.0044, -0.0118]],
       dtype=torch.float64)
	q_value: tensor([[0.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2867561475553778, distance: 0.9664418139591224 entropy -11.12283003112511
epoch: 3, step: 59
	action: tensor([[-0.0069,  0.0147,  0.0173, -0.0096, -0.0091,  0.0337, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[0.7308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29575361165349145, distance: 0.9603267082860064 entropy -11.13054317965952
epoch: 3, step: 60
	action: tensor([[-0.0072,  0.0144,  0.0036, -0.0092, -0.0102, -0.0039,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[0.7277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28666537758762, distance: 0.9665033084263761 entropy -11.136082757829133
epoch: 3, step: 61
	action: tensor([[-0.0068,  0.0144,  0.0068, -0.0100, -0.0096,  0.0015, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28803587210819215, distance: 0.9655744145561791 entropy -11.114408129803326
epoch: 3, step: 62
	action: tensor([[-0.0070,  0.0147,  0.0105, -0.0095, -0.0070,  0.0205,  0.0723]],
       dtype=torch.float64)
	q_value: tensor([[0.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925789538296326, distance: 0.9624887929397801 entropy -11.131622640042549
epoch: 3, step: 63
	action: tensor([[-0.0068,  0.0137,  0.0142, -0.0102, -0.0013, -0.0197, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[0.7283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28236895084067326, distance: 0.9694095719341055 entropy -11.103376297584628
epoch: 3, step: 64
	action: tensor([[-0.0068,  0.0146,  0.0090, -0.0098,  0.0001,  0.0102,  0.0521]],
       dtype=torch.float64)
	q_value: tensor([[0.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29024014004747734, distance: 0.9640785283263494 entropy -11.13136778686746
epoch: 3, step: 65
	action: tensor([[-0.0068,  0.0138,  0.0136, -0.0101, -0.0122, -0.0096,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[0.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849436774116907, distance: 0.967668978787264 entropy -11.109168925924342
epoch: 3, step: 66
	action: tensor([[-0.0067,  0.0143,  0.0050, -0.0100, -0.0014,  0.0177, -0.0126]],
       dtype=torch.float64)
	q_value: tensor([[0.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917995392406425, distance: 0.9630188671396518 entropy -11.118017582721974
epoch: 3, step: 67
	action: tensor([[-0.0071,  0.0147,  0.0125, -0.0093, -0.0109, -0.0219, -0.0174]],
       dtype=torch.float64)
	q_value: tensor([[0.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28276886079873376, distance: 0.9691394257591942 entropy -11.132220221749382
epoch: 3, step: 68
	action: tensor([[-0.0068,  0.0147,  0.0188, -0.0098, -0.0153, -0.0033, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[0.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2870847828208667, distance: 0.9662191387445368 entropy -11.131731731798295
epoch: 3, step: 69
	action: tensor([[-0.0070,  0.0146,  0.0011, -0.0096, -0.0058,  0.0007,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[0.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2879576726787588, distance: 0.9656274406091002 entropy -11.132944962525233
epoch: 3, step: 70
	action: tensor([[-0.0069,  0.0147,  0.0004, -0.0097, -0.0024,  0.0013, -0.0409]],
       dtype=torch.float64)
	q_value: tensor([[0.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288253677085508, distance: 0.9654267083782831 entropy -11.123506370515619
epoch: 3, step: 71
	action: tensor([[-0.0072,  0.0150,  0.0131, -0.0092, -0.0123, -0.0160,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28432262226070215, distance: 0.9680891172773918 entropy -11.138266245516528
epoch: 3, step: 72
	action: tensor([[-0.0067,  0.0144,  0.0127, -0.0100, -0.0038,  0.0011,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[0.7318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28795687574752316, distance: 0.9656279809831936 entropy -11.120065229655523
epoch: 3, step: 73
	action: tensor([[-0.0069,  0.0144,  0.0019, -0.0097, -0.0011,  0.0277,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[0.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29417324640503406, distance: 0.9614036158181438 entropy -11.127699137544127
epoch: 3, step: 74
	action: tensor([[-0.0070,  0.0143,  0.0044, -0.0096,  0.0015, -0.0095,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[0.7279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28533033399104646, distance: 0.9674073167622055 entropy -11.120957777155175
epoch: 3, step: 75
	action: tensor([[-0.0067,  0.0144,  0.0091, -0.0099, -0.0013,  0.0130,  0.0452]],
       dtype=torch.float64)
	q_value: tensor([[0.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29076973673186834, distance: 0.9637187812491235 entropy -11.119898450841223
epoch: 3, step: 76
	action: tensor([[-0.0068,  0.0139,  0.0063, -0.0100, -0.0097,  0.0091, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[0.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28938260192114773, distance: 0.9646607566089934 entropy -11.112344907290671
epoch: 3, step: 77
	action: tensor([[-0.0070,  0.0147,  0.0099, -0.0094, -0.0044,  0.0163, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[0.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2916553155772236, distance: 0.9631169206189981 entropy -11.130299331301899
epoch: 3, step: 78
	action: tensor([[-0.0070,  0.0145,  0.0131, -0.0094, -0.0053,  0.0167,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[0.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2915815105930636, distance: 0.9631670946226549 entropy -11.129010998406041
epoch: 3, step: 79
	action: tensor([[-0.0069,  0.0142,  0.0081, -0.0097,  0.0028, -0.0012,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[0.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2872075230925599, distance: 0.9661359597762169 entropy -11.123306667252013
epoch: 3, step: 80
	action: tensor([[-0.0067,  0.0142,  0.0227, -0.0100, -0.0102,  0.0144, -0.0274]],
       dtype=torch.float64)
	q_value: tensor([[0.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2908929404305841, distance: 0.9636350715701826 entropy -11.115625921725623
epoch: 3, step: 81
	action: tensor([[-0.0072,  0.0146,  0.0061, -0.0092, -0.0038,  0.0057, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[0.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889889750627861, distance: 0.9649278931970126 entropy -11.141405981297364
epoch: 3, step: 82
	action: tensor([[-0.0070,  0.0146,  0.0096, -0.0095, -0.0082,  0.0098,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[0.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2900551546149599, distance: 0.9642041545239227 entropy -11.129011468995975
epoch: 3, step: 83
	action: tensor([[-0.0069,  0.0144, -0.0039, -0.0097, -0.0067,  0.0084, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[0.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28964814828225915, distance: 0.9644805006163067 entropy -11.123402928027856
epoch: 3, step: 84
	action: tensor([[-0.0071,  0.0149,  0.0056, -0.0094, -0.0143, -0.0015,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2876302955897414, distance: 0.9658493993049059 entropy -11.127771810891284
epoch: 3, step: 85
	action: tensor([[-0.0068,  0.0144,  0.0095, -0.0098, -0.0067, -0.0005, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[0.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2875946385555821, distance: 0.965873571370242 entropy -11.121039072526615
epoch: 3, step: 86
	action: tensor([[-0.0070,  0.0147,  0.0125, -0.0094, -0.0077,  0.0068,  0.0147]],
       dtype=torch.float64)
	q_value: tensor([[0.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289393322653813, distance: 0.9646534799020635 entropy -11.135489071100574
epoch: 3, step: 87
	action: tensor([[-0.0069,  0.0142,  0.0118, -0.0098, -0.0058,  0.0046,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[0.7298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2885817570947614, distance: 0.9652041756431651 entropy -11.123380025638978
epoch: 3, step: 88
	action: tensor([[-0.0069,  0.0143,  0.0035, -0.0098, -0.0091,  0.0216,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[0.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926140011049929, distance: 0.9624649506836385 entropy -11.125016607681953
epoch: 3, step: 89
	action: tensor([[-0.0070,  0.0145,  0.0099, -0.0095, -0.0084,  0.0278, -0.0061]],
       dtype=torch.float64)
	q_value: tensor([[0.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2941609458702813, distance: 0.9614119930346532 entropy -11.124730429079921
epoch: 3, step: 90
	action: tensor([[-0.0072,  0.0145,  0.0158, -0.0093, -0.0066,  0.0047,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.7280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2886856617776894, distance: 0.9651336876447327 entropy -11.132860301625342
epoch: 3, step: 91
	action: tensor([[-0.0068,  0.0142,  0.0089, -0.0098, -0.0040, -0.0046, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[0.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286442139527888, distance: 0.9666545301942622 entropy -11.12462346694136
epoch: 3, step: 92
	action: tensor([[-0.0069,  0.0146,  0.0128, -0.0097, -0.0044,  0.0117, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2905136136933639, distance: 0.9638927785494028 entropy -11.128761947107341
epoch: 3, step: 93
	action: tensor([[-0.0070,  0.0146,  0.0031, -0.0094, -0.0105, -0.0231, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.7297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28255495633585703, distance: 0.9692839313327455 entropy -11.132045202060947
epoch: 3, step: 94
	action: tensor([[-0.0069,  0.0149,  0.0089, -0.0096, -0.0027,  0.0105,  0.0460]],
       dtype=torch.float64)
	q_value: tensor([[0.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2905184348779597, distance: 0.9638895035655551 entropy -11.132972437631151
epoch: 3, step: 95
	action: tensor([[-0.0068,  0.0139,  0.0024, -0.0100, -0.0074, -0.0027,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[0.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2866632517863188, distance: 0.9665047485586755 entropy -11.11109223666743
epoch: 3, step: 96
	action: tensor([[-0.0068,  0.0146,  0.0042, -0.0097, -0.0081,  0.0226,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[0.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29306015301608546, distance: 0.9621613870661772 entropy -11.123991422523385
epoch: 3, step: 97
	action: tensor([[-0.0071,  0.0145, -0.0008, -0.0095, -0.0031,  0.0046,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[0.7283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887291471367902, distance: 0.965104186041724 entropy -11.126021342125947
epoch: 3, step: 98
	action: tensor([[-0.0069,  0.0146,  0.0016, -0.0097, -0.0085,  0.0062,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28926458236484964, distance: 0.9647408588610359 entropy -11.119678154217713
epoch: 3, step: 99
	action: tensor([[-0.0069,  0.0146,  0.0131, -0.0096, -0.0024,  0.0015,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[0.7297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28813823286446316, distance: 0.9655050006094663 entropy -11.12559502678808
epoch: 3, step: 100
	action: tensor([[-0.0068,  0.0142,  0.0023, -0.0098, -0.0114, -0.0177,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28362145845667897, distance: 0.9685632295622818 entropy -11.123526643154776
epoch: 3, step: 101
	action: tensor([[-6.6999e-03,  1.4610e-02,  7.1227e-03, -9.9513e-03,  4.9495e-05,
          7.2887e-05,  2.6633e-02]], dtype=torch.float64)
	q_value: tensor([[0.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28791228588654305, distance: 0.9656582154885897 entropy -11.118724166577609
epoch: 3, step: 102
	action: tensor([[-0.0067,  0.0142,  0.0142, -0.0099,  0.0009, -0.0072, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[0.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.285869862853786, distance: 0.9670420831865563 entropy -11.115109449641961
epoch: 3, step: 103
	action: tensor([[-0.0069,  0.0145,  0.0082, -0.0097, -0.0053, -0.0263,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[0.7315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28176157825967163, distance: 0.9698197188088843 entropy -11.132485642004696
epoch: 3, step: 104
	action: tensor([[-0.0066,  0.0145,  0.0079, -0.0101, -0.0146,  0.0141,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[0.7328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2910837058075948, distance: 0.9635054433494733 entropy -11.118047800609446
epoch: 3, step: 105
	action: tensor([[-0.0069,  0.0142,  0.0085, -0.0098, -0.0052,  0.0205, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[0.7288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29223968464966055, distance: 0.9627195633157357 entropy -11.119410557013266
epoch: 3, step: 106
	action: tensor([[-0.0071,  0.0146,  0.0097, -0.0094, -0.0006, -0.0026,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[0.7287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28711961358413784, distance: 0.9661955352638232 entropy -11.12990876652466
epoch: 3, step: 107
	action: tensor([[-0.0068,  0.0145,  0.0109, -0.0098, -0.0024, -0.0070,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[0.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28611686315353724, distance: 0.9668748305182333 entropy -11.123882940776001
epoch: 3, step: 108
	action: tensor([[-0.0067,  0.0141,  0.0139, -0.0101, -0.0065, -0.0021,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2870081375340269, distance: 0.9662710763038171 entropy -11.112993435036502
epoch: 3, step: 109
	action: tensor([[-0.0068,  0.0144,  0.0059, -0.0097, -0.0116, -0.0331,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[0.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28024238614780195, distance: 0.9708448409373969 entropy -11.1287509429936
epoch: 3, step: 110
	action: tensor([[-0.0066,  0.0146,  0.0120, -0.0101, -0.0200,  0.0004, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28805256322036055, distance: 0.9655630961451188 entropy -11.11867874904733
epoch: 3, step: 111
	action: tensor([[-0.0070,  0.0146,  0.0016, -0.0095, -0.0098,  0.0061,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[0.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28921915130587494, distance: 0.9647716920637899 entropy -11.132633581349905
epoch: 3, step: 112
	action: tensor([[-0.0068,  0.0143,  0.0129, -0.0098, -0.0103, -0.0185,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[0.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28330431134611156, distance: 0.9687776015889918 entropy -11.116867676273573
epoch: 3, step: 113
	action: tensor([[-0.0066,  0.0143,  0.0076, -0.0101, -0.0106,  0.0047, -0.0175]],
       dtype=torch.float64)
	q_value: tensor([[0.7319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28877257865819617, distance: 0.9650747200634456 entropy -11.117406717460891
epoch: 3, step: 114
	action: tensor([[-0.0071,  0.0147,  0.0031, -0.0094, -0.0144, -0.0064,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[0.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864200525168441, distance: 0.9666694906797756 entropy -11.133720892591446
epoch: 3, step: 115
	action: tensor([[-0.0068,  0.0145,  0.0081, -0.0099,  0.0030, -0.0024, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[0.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871561996110771, distance: 0.9661707416868709 entropy -11.11872937862729
epoch: 3, step: 116
	action: tensor([[-0.0068,  0.0145, -0.0006, -0.0097, -0.0086, -0.0002,  0.0495]],
       dtype=torch.float64)
	q_value: tensor([[0.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28781526738534624, distance: 0.9657239963748523 entropy -11.125266544961635
epoch: 3, step: 117
	action: tensor([[-0.0067,  0.0141, -0.0060, -0.0101, -0.0146,  0.0146,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2907851962677649, distance: 0.9637082778011925 entropy -11.10489953486022
epoch: 3, step: 118
	action: tensor([[-0.0069,  0.0144, -0.0002, -0.0097,  0.0012, -0.0201,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[0.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2830927696571647, distance: 0.9689205644483417 entropy -11.114808384786183
epoch: 3, step: 119
	action: tensor([[-0.0067,  0.0146,  0.0060, -0.0100, -0.0106,  0.0057, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[0.7322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28924493857603795, distance: 0.9647541908508838 entropy -11.119292764803129
epoch: 3, step: 120
	action: tensor([[-0.0071,  0.0148,  0.0087, -0.0094, -0.0044,  0.0069,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894909866889138, distance: 0.9645871878226249 entropy -11.135164580830056
epoch: 3, step: 121
	action: tensor([[-0.0069,  0.0144,  0.0043, -0.0097, -0.0097,  0.0218,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29270577856527136, distance: 0.9624025127401326 entropy -11.122588794271268
epoch: 3, step: 122
	action: tensor([[-0.0070,  0.0144,  0.0104, -0.0096, -0.0124,  0.0069, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[0.7283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28916023555964687, distance: 0.9648116756019482 entropy -11.1206056768535
epoch: 3, step: 123
	action: tensor([[-0.0071,  0.0147,  0.0126, -0.0094, -0.0047, -0.0061,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28638978574953833, distance: 0.9666899912910939 entropy -11.134694600296239
epoch: 3, step: 124
	action: tensor([[-0.0067,  0.0143,  0.0117, -0.0099, -0.0182,  0.0054, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[0.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28888669354854213, distance: 0.9649972948894361 entropy -11.120630011094054
epoch: 3, step: 125
	action: tensor([[-0.0070,  0.0145,  0.0040, -0.0096, -0.0029,  0.0121,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.7297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29055404641039273, distance: 0.9638653126529587 entropy -11.130097086757877
epoch: 3, step: 126
	action: tensor([[-0.0069,  0.0144,  0.0138, -0.0097, -0.0059,  0.0087,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[0.7293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2896318508862952, distance: 0.9644915644506761 entropy -11.122114307793394
epoch: 3, step: 127
	action: tensor([[-0.0068,  0.0141,  0.0072, -0.0099, -0.0057, -0.0271, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[0.7297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2812487898581246, distance: 0.970165859836452 entropy -11.11863112283539
LOSS epoch 3 actor 0.1610440436225682 critic 18.12221032162832
epoch: 4, step: 0
	action: tensor([[-0.0051,  0.0157,  0.0021, -0.0110, -0.0233, -0.0167,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[1.2546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2862348840123601, distance: 0.966794904190895 entropy -11.119636539656183
epoch: 4, step: 1
	action: tensor([[-0.0050,  0.0154,  0.0008, -0.0111, -0.0013,  0.0169,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[1.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2936170894694369, distance: 0.961782310731561 entropy -11.103979712717438
epoch: 4, step: 2
	action: tensor([[-0.0053,  0.0156,  0.0143, -0.0106, -0.0068,  0.0024, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[1.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29019202307962055, distance: 0.9641112068081817 entropy -11.116302412999971
epoch: 4, step: 3
	action: tensor([[-0.0053,  0.0155,  0.0017, -0.0107, -0.0041,  0.0018, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[1.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29013251675268104, distance: 0.9641516188042316 entropy -11.12292809840294
epoch: 4, step: 4
	action: tensor([[-0.0053,  0.0157, -0.0044, -0.0106, -0.0065,  0.0114, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[1.2510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925495881518506, distance: 0.9625087696160495 entropy -11.119948142717503
epoch: 4, step: 5
	action: tensor([[-0.0054,  0.0159,  0.0114, -0.0105, -0.0065, -0.0237,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[1.2493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28440598921718363, distance: 0.9680327308410784 entropy -11.12079219497119
epoch: 4, step: 6
	action: tensor([[-0.0050,  0.0153,  0.0086, -0.0111, -0.0089, -0.0205,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[1.2554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849584923235595, distance: 0.9676589543986658 entropy -11.112043358908029
epoch: 4, step: 7
	action: tensor([[-0.0050,  0.0156,  0.0122, -0.0110, -0.0161, -0.0037, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[1.2544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889811564956406, distance: 0.9649331985527094 entropy -11.1165086636654
epoch: 4, step: 8
	action: tensor([[-0.0054,  0.0157,  0.0023, -0.0106, -0.0017,  0.0043, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[1.2518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2908191622254762, distance: 0.963685200401817 entropy -11.129574962303986
epoch: 4, step: 9
	action: tensor([[-0.0053,  0.0157,  0.0164, -0.0106, -0.0198,  0.0252,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[1.2508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955634024187437, distance: 0.9604563863923903 entropy -11.122522743606305
epoch: 4, step: 10
	action: tensor([[-0.0054,  0.0149,  0.0076, -0.0108, -0.0085,  0.0061,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[1.2484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904809893789523, distance: 0.9639149396367629 entropy -11.11379596126003
epoch: 4, step: 11
	action: tensor([[-0.0052,  0.0153,  0.0176, -0.0109, -0.0026, -0.0030,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[1.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28876367125246727, distance: 0.9650807633380141 entropy -11.112407034477345
epoch: 4, step: 12
	action: tensor([[-0.0050,  0.0150,  0.0155, -0.0111, -0.0042, -0.0102,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[1.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2869148741111798, distance: 0.9663342711352951 entropy -11.10996403821484
epoch: 4, step: 13
	action: tensor([[-0.0050,  0.0152,  0.0020, -0.0111, -0.0130, -0.0100,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[1.2538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28729241320611176, distance: 0.9660784270187573 entropy -11.112649149629112
epoch: 4, step: 14
	action: tensor([[-0.0052,  0.0156, -0.0034, -0.0109, -0.0059, -0.0270,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[1.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28378062740088583, distance: 0.9684556232187245 entropy -11.115051350934394
epoch: 4, step: 15
	action: tensor([[-0.0050,  0.0156,  0.0189, -0.0112, -0.0090, -0.0064,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[1.2550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882756491438647, distance: 0.9654118065972654 entropy -11.105944100012579
epoch: 4, step: 16
	action: tensor([[-0.0051,  0.0151,  0.0149, -0.0111, -0.0085,  0.0128,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[1.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29231713347485666, distance: 0.9626668876164586 entropy -11.112372753631456
epoch: 4, step: 17
	action: tensor([[-0.0053,  0.0152,  0.0038, -0.0107, -0.0083,  0.0271,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[1.2502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295686170269995, distance: 0.960372689502528 entropy -11.119142807502802
epoch: 4, step: 18
	action: tensor([[-0.0055,  0.0154,  0.0077, -0.0105, -0.0005,  0.0016,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[1.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2899018544188613, distance: 0.9643082504324976 entropy -11.120151792268539
epoch: 4, step: 19
	action: tensor([[-0.0052,  0.0155,  0.0101, -0.0108, -0.0100, -0.0029, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[1.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28903223215782414, distance: 0.9648985401975586 entropy -11.11755937465691
epoch: 4, step: 20
	action: tensor([[-0.0054,  0.0158,  0.0127, -0.0105, -0.0103, -0.0104, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[1.2518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28747985009279686, distance: 0.9659513828823043 entropy -11.131116495747252
epoch: 4, step: 21
	action: tensor([[-0.0052,  0.0156,  0.0083, -0.0107, -0.0018,  0.0030, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[1.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29044309509691635, distance: 0.9639406798751053 entropy -11.12531138079115
epoch: 4, step: 22
	action: tensor([[-0.0053,  0.0156,  0.0135, -0.0106, -0.0074,  0.0092,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[1.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29188479440682336, distance: 0.9629608999332046 entropy -11.122940552372144
epoch: 4, step: 23
	action: tensor([[-0.0053,  0.0154,  0.0008, -0.0107, -0.0090, -0.0118, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[1.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286940592717711, distance: 0.9663168447481165 entropy -11.120414555545779
epoch: 4, step: 24
	action: tensor([[-0.0052,  0.0158,  0.0089, -0.0108, -0.0081, -0.0251,  0.0399]],
       dtype=torch.float64)
	q_value: tensor([[1.2526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2841488738667587, distance: 0.9682066239409213 entropy -11.121417114931495
epoch: 4, step: 25
	action: tensor([[-0.0049,  0.0151,  0.0085, -0.0114, -0.0009, -0.0026,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[1.2557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888753323213209, distance: 0.9650050035834274 entropy -11.099214594124721
epoch: 4, step: 26
	action: tensor([[-0.0051,  0.0154,  0.0029, -0.0109, -0.0036,  0.0138, -0.0393]],
       dtype=torch.float64)
	q_value: tensor([[1.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2928769900561363, distance: 0.9622860234936657 entropy -11.112925166967369
epoch: 4, step: 27
	action: tensor([[-0.0057,  0.0159,  0.0068, -0.0101, -0.0006, -0.0147,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[1.2492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864737983586052, distance: 0.9666330858985133 entropy -11.13504864635127
epoch: 4, step: 28
	action: tensor([[-0.0050,  0.0154,  0.0122, -0.0111, -0.0066, -0.0022,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[1.2541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891495228894516, distance: 0.9648189456442684 entropy -11.110661792185313
epoch: 4, step: 29
	action: tensor([[-0.0051,  0.0154,  0.0154, -0.0109,  0.0008,  0.0347,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[1.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29777750253830015, distance: 0.9589458031726963 entropy -11.116317277642542
epoch: 4, step: 30
	action: tensor([[-0.0055,  0.0152,  0.0062, -0.0103,  0.0061,  0.0003,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[1.2475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894568278841999, distance: 0.964610374544222 entropy -11.126458381663785
epoch: 4, step: 31
	action: tensor([[-5.1974e-03,  1.5515e-02, -8.2978e-05, -1.0769e-02, -9.8993e-03,
         -1.6226e-03,  3.2591e-02]], dtype=torch.float64)
	q_value: tensor([[1.2520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28939694374169667, distance: 0.9646510220730075 entropy -11.117444420317627
epoch: 4, step: 32
	action: tensor([[-0.0051,  0.0154,  0.0110, -0.0110, -0.0161, -0.0117, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[1.2515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28688621218312327, distance: 0.9663536914850142 entropy -11.104958478481413
epoch: 4, step: 33
	action: tensor([[-0.0053,  0.0159,  0.0139, -0.0106, -0.0078, -0.0057,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[1.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28857838914201017, distance: 0.9652064603457601 entropy -11.131121216899112
epoch: 4, step: 34
	action: tensor([[-0.0052,  0.0155,  0.0159, -0.0108, -0.0092, -0.0089,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[1.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28759642401015484, distance: 0.9658723610168685 entropy -11.120799218208523
epoch: 4, step: 35
	action: tensor([[-0.0051,  0.0154,  0.0137, -0.0109, -0.0166, -0.0230, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[1.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2844156708341472, distance: 0.9680261823277684 entropy -11.120185984698981
epoch: 4, step: 36
	action: tensor([[-0.0053,  0.0159,  0.0075, -0.0106, -0.0112, -0.0136, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[1.2545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2868909130698234, distance: 0.9663505063504815 entropy -11.13308253980038
epoch: 4, step: 37
	action: tensor([[-0.0052,  0.0157,  0.0044, -0.0107, -0.0056,  0.0176,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[1.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29394318072201964, distance: 0.9615602887958469 entropy -11.124658329801163
epoch: 4, step: 38
	action: tensor([[-0.0053,  0.0154,  0.0179, -0.0106, -0.0027, -0.0194, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[1.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.284969684040012, distance: 0.9676513815466699 entropy -11.115673913398672
epoch: 4, step: 39
	action: tensor([[-0.0051,  0.0154,  0.0196, -0.0110, -0.0122, -0.0005,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[1.2551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2895105121938457, distance: 0.9645739338167385 entropy -11.121055456769273
epoch: 4, step: 40
	action: tensor([[-0.0052,  0.0153,  0.0071, -0.0108,  0.0009,  0.0057,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[1.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2908486463655212, distance: 0.9636651676237002 entropy -11.122967551309312
epoch: 4, step: 41
	action: tensor([[-0.0051,  0.0152,  0.0153, -0.0110, -0.0077,  0.0039,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[1.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29030998101798, distance: 0.9640310940881475 entropy -11.108224017332898
epoch: 4, step: 42
	action: tensor([[-0.0053,  0.0155,  0.0073, -0.0107,  0.0029, -0.0194,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[1.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28516519949145014, distance: 0.9675190768547115 entropy -11.123417860240234
epoch: 4, step: 43
	action: tensor([[-0.0049,  0.0153,  0.0144, -0.0112, -0.0120, -0.0296, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[1.2550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828432289933426, distance: 0.9690891804380878 entropy -11.10801774959543
epoch: 4, step: 44
	action: tensor([[-0.0051,  0.0157,  0.0098, -0.0108, -0.0091, -0.0111, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[1.2557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2873368623019503, distance: 0.9660483010716426 entropy -11.127118029610234
epoch: 4, step: 45
	action: tensor([[-0.0053,  0.0157,  0.0097, -0.0107,  0.0017,  0.0075,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[1.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2916025772821146, distance: 0.9631527733613935 entropy -11.127433269274622
epoch: 4, step: 46
	action: tensor([[-0.0053,  0.0154,  0.0013, -0.0107, -0.0063,  0.0163, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[1.2512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934106540024364, distance: 0.9619228375433719 entropy -11.120731052855655
epoch: 4, step: 47
	action: tensor([[-0.0055,  0.0158,  0.0005, -0.0103,  0.0011, -0.0050,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[1.2488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887233302813793, distance: 0.9651081324005593 entropy -11.125300347027666
epoch: 4, step: 48
	action: tensor([[-0.0051,  0.0154,  0.0130, -0.0110,  0.0011,  0.0019, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[1.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2900874368224279, distance: 0.964182232403767 entropy -11.11052554505935
epoch: 4, step: 49
	action: tensor([[-0.0054,  0.0157,  0.0088, -0.0105, -0.0138,  0.0019, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[1.2518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2902477265165976, distance: 0.9640733758994924 entropy -11.129995374631909
epoch: 4, step: 50
	action: tensor([[-0.0055,  0.0158,  0.0139, -0.0104,  0.0021, -0.0075, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[1.2509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28800938711295077, distance: 0.965592374025587 entropy -11.13113118862061
epoch: 4, step: 51
	action: tensor([[-0.0053,  0.0156,  0.0107, -0.0106, -0.0121, -0.0223, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[1.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28465978690142724, distance: 0.9678610509153294 entropy -11.128525494886144
epoch: 4, step: 52
	action: tensor([[-0.0051,  0.0157,  0.0067, -0.0109, -0.0032,  0.0166,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[1.2546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937262039951628, distance: 0.96170802490718 entropy -11.120363328148425
epoch: 4, step: 53
	action: tensor([[-0.0054,  0.0156,  0.0126, -0.0105, -0.0103, -0.0040,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[1.2493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28873295238303176, distance: 0.9651016044204094 entropy -11.121465620953726
epoch: 4, step: 54
	action: tensor([[-0.0051,  0.0151,  0.0047, -0.0111,  0.0059,  0.0169,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[1.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29335465014383155, distance: 0.9619609575090042 entropy -11.105590797682146
epoch: 4, step: 55
	action: tensor([[-0.0054,  0.0155,  0.0118, -0.0105,  0.0013,  0.0023,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[1.2495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29013895875329043, distance: 0.9641472439881188 entropy -11.121106869913069
epoch: 4, step: 56
	action: tensor([[-0.0051,  0.0151,  0.0051, -0.0110, -0.0068,  0.0167, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[1.2522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29330083856998823, distance: 0.9619975838362478 entropy -11.112721945766568
epoch: 4, step: 57
	action: tensor([[-0.0055,  0.0157,  0.0065, -0.0104, -0.0093,  0.0255,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[1.2489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29562400130558586, distance: 0.9604150740586448 entropy -11.126255775558365
epoch: 4, step: 58
	action: tensor([[-0.0054,  0.0153,  0.0117, -0.0105, -0.0076,  0.0012, -0.0111]],
       dtype=torch.float64)
	q_value: tensor([[1.2479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2896955020973191, distance: 0.9644483527502039 entropy -11.119491237372172
epoch: 4, step: 59
	action: tensor([[-0.0053,  0.0156,  0.0130, -0.0106, -0.0122,  0.0075, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[1.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914623389089971, distance: 0.9632481042374496 entropy -11.12557206770619
epoch: 4, step: 60
	action: tensor([[-0.0054,  0.0156,  0.0124, -0.0105, -0.0215, -0.0053, -0.0284]],
       dtype=torch.float64)
	q_value: tensor([[1.2505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2884997451527255, distance: 0.9652598081691053 entropy -11.126035816427704
epoch: 4, step: 61
	action: tensor([[-0.0054,  0.0158,  0.0045, -0.0105, -0.0085, -0.0114, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[1.2517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2872318800870407, distance: 0.9661194526089583 entropy -11.132697152807365
epoch: 4, step: 62
	action: tensor([[-0.0053,  0.0159,  0.0075, -0.0106, -0.0103,  0.0182, -0.0266]],
       dtype=torch.float64)
	q_value: tensor([[1.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2941967525798729, distance: 0.961387606854524 entropy -11.126805061820617
epoch: 4, step: 63
	action: tensor([[-0.0056,  0.0158,  0.0103, -0.0102, -0.0044,  0.0126,  0.0468]],
       dtype=torch.float64)
	q_value: tensor([[1.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29269264903059555, distance: 0.9624114452594015 entropy -11.131651531704605
epoch: 4, step: 64
	action: tensor([[-0.0052,  0.0149,  0.0036, -0.0110, -0.0030,  0.0038,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[1.2506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29008737238760784, distance: 0.9641822761604987 entropy -11.104644871631539
epoch: 4, step: 65
	action: tensor([[-0.0051,  0.0151,  0.0112, -0.0110, -0.0042, -0.0047,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[1.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28828782518745777, distance: 0.9654035485219982 entropy -11.105215998767264
epoch: 4, step: 66
	action: tensor([[-0.0051,  0.0154,  0.0058, -0.0109, -0.0083, -0.0134, -0.0319]],
       dtype=torch.float64)
	q_value: tensor([[1.2526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28660792853276185, distance: 0.9665422267558346 entropy -11.118283337296266
epoch: 4, step: 67
	action: tensor([[-0.0053,  0.0159,  0.0038, -0.0106, -0.0077, -0.0144,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[1.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2867336560331859, distance: 0.9664570517884228 entropy -11.130918829699544
epoch: 4, step: 68
	action: tensor([[-0.0050,  0.0153,  0.0128, -0.0112,  0.0005,  0.0090,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[1.2538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2916147877822236, distance: 0.9631444724928795 entropy -11.10256401332816
epoch: 4, step: 69
	action: tensor([[-0.0053,  0.0153,  0.0111, -0.0107, -0.0096,  0.0142,  0.0204]],
       dtype=torch.float64)
	q_value: tensor([[1.2510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2928269586794334, distance: 0.9623200654087875 entropy -11.119320626801008
epoch: 4, step: 70
	action: tensor([[-0.0053,  0.0152,  0.0063, -0.0107, -0.0126,  0.0047,  0.0002]],
       dtype=torch.float64)
	q_value: tensor([[1.2498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29049892418269996, distance: 0.9639027569226559 entropy -11.1170868103195
epoch: 4, step: 71
	action: tensor([[-0.0053,  0.0156,  0.0041, -0.0106, -0.0068, -0.0008,  0.0282]],
       dtype=torch.float64)
	q_value: tensor([[1.2504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28955797813870476, distance: 0.9645417129474289 entropy -11.12240551880055
epoch: 4, step: 72
	action: tensor([[-0.0051,  0.0153,  0.0187, -0.0110, -0.0056,  0.0124,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[1.2518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29237899947683854, distance: 0.9626248082783375 entropy -11.107162623524971
epoch: 4, step: 73
	action: tensor([[-0.0053,  0.0154,  0.0016, -0.0106, -0.0149, -0.0064,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[1.2505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881480807438179, distance: 0.9654983221993554 entropy -11.124653355616037
epoch: 4, step: 74
	action: tensor([[-0.0052,  0.0155,  0.0097, -0.0110, -0.0025, -0.0091,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[1.2518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28750829968591696, distance: 0.9659320983763564 entropy -11.110988880878955
epoch: 4, step: 75
	action: tensor([[-0.0051,  0.0154,  0.0111, -0.0110, -0.0100, -0.0055,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[1.2534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28834254528819536, distance: 0.9653664352079797 entropy -11.11498864032574
epoch: 4, step: 76
	action: tensor([[-0.0052,  0.0155,  0.0204, -0.0108, -0.0061, -0.0108,  0.0297]],
       dtype=torch.float64)
	q_value: tensor([[1.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28710294207132225, distance: 0.9662068329845775 entropy -11.119806122094975
epoch: 4, step: 77
	action: tensor([[-0.0050,  0.0151,  0.0087, -0.0112, -0.0124, -0.0117, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[1.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28673690679456865, distance: 0.9664548494380187 entropy -11.110023818962036
epoch: 4, step: 78
	action: tensor([[-0.0053,  0.0158,  0.0096, -0.0107, -0.0025, -0.0102, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[1.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28752927072364154, distance: 0.9659178829493397 entropy -11.12578505267977
epoch: 4, step: 79
	action: tensor([[-0.0052,  0.0156,  0.0069, -0.0108, -0.0090, -0.0139, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[1.2533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2866020236247089, distance: 0.9665462268922227 entropy -11.122576386629122
epoch: 4, step: 80
	action: tensor([[-0.0052,  0.0158,  0.0103, -0.0107, -0.0062,  0.0053,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[1.2533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29114235746283323, distance: 0.9634655850767494 entropy -11.123601749916432
epoch: 4, step: 81
	action: tensor([[-0.0052,  0.0153,  0.0003, -0.0108, -0.0047,  0.0067,  0.0411]],
       dtype=torch.float64)
	q_value: tensor([[1.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2911217637301313, distance: 0.9634795802765119 entropy -11.114561261594622
epoch: 4, step: 82
	action: tensor([[-0.0052,  0.0152,  0.0025, -0.0110, -0.0027, -0.0249,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[1.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2837703656220305, distance: 0.9684625610664145 entropy -11.102798796391918
epoch: 4, step: 83
	action: tensor([[-0.0050,  0.0157,  0.0067, -0.0110, -0.0052,  0.0009,  0.0407]],
       dtype=torch.float64)
	q_value: tensor([[1.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29011724093933977, distance: 0.9641619926565879 entropy -11.114019086445394
epoch: 4, step: 84
	action: tensor([[-0.0051,  0.0151,  0.0058, -0.0111, -0.0091, -0.0017,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[1.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889669191895028, distance: 0.9649428593238019 entropy -11.103850339385428
epoch: 4, step: 85
	action: tensor([[-0.0052,  0.0156,  0.0079, -0.0108, -0.0117, -0.0086,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[1.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.287795395099427, distance: 0.9657374697116314 entropy -11.120110819619388
epoch: 4, step: 86
	action: tensor([[-0.0051,  0.0154,  0.0096, -0.0109,  0.0044, -0.0014,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[1.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893021323858662, distance: 0.9647153736262222 entropy -11.114938081094548
epoch: 4, step: 87
	action: tensor([[-0.0051,  0.0152,  0.0102, -0.0109,  0.0041,  0.0156,  0.0273]],
       dtype=torch.float64)
	q_value: tensor([[1.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29318464800287847, distance: 0.9620766630661258 entropy -11.11404613224319
epoch: 4, step: 88
	action: tensor([[-0.0052,  0.0151,  0.0117, -0.0108, -0.0053, -0.0177, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[1.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2853071154017217, distance: 0.9674230314705741 entropy -11.113289629075828
epoch: 4, step: 89
	action: tensor([[-0.0051,  0.0156,  0.0102, -0.0109, -0.0024, -0.0110, -0.0266]],
       dtype=torch.float64)
	q_value: tensor([[1.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28727068254491106, distance: 0.9660931549123267 entropy -11.120062970414647
epoch: 4, step: 90
	action: tensor([[-0.0053,  0.0158,  0.0156, -0.0106, -0.0067, -0.0142,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[1.2533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28658515052678557, distance: 0.966557657070919 entropy -11.129067747962376
epoch: 4, step: 91
	action: tensor([[-0.0050,  0.0151, -0.0007, -0.0112, -0.0054,  0.0143,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[1.2545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2927858568784617, distance: 0.9623480306385023 entropy -11.107046437433057
epoch: 4, step: 92
	action: tensor([[-0.0054,  0.0156,  0.0062, -0.0106, -0.0075,  0.0194, -0.0371]],
       dtype=torch.float64)
	q_value: tensor([[1.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942362150584752, distance: 0.961360730194265 entropy -11.116309998639741
epoch: 4, step: 93
	action: tensor([[-0.0057,  0.0159,  0.0043, -0.0101, -0.0129, -0.0118,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[1.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871951800225683, distance: 0.9661443247860442 entropy -11.134890459885744
epoch: 4, step: 94
	action: tensor([[-0.0051,  0.0156,  0.0137, -0.0109,  0.0015, -0.0018,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[1.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28927727249630186, distance: 0.9647322461321208 entropy -11.114501244652498
epoch: 4, step: 95
	action: tensor([[-0.0051,  0.0151,  0.0026, -0.0110, -0.0082, -0.0009,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[1.2529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28928777664229544, distance: 0.9647251169624679 entropy -11.114214881115798
epoch: 4, step: 96
	action: tensor([[-0.0052,  0.0154,  0.0033, -0.0109, -0.0044,  0.0014,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[1.2515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28994075255348306, distance: 0.9642818383751837 entropy -11.111413227652694
epoch: 4, step: 97
	action: tensor([[-0.0052,  0.0155, -0.0010, -0.0108, -0.0015, -0.0005,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[1.2512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28965149089333897, distance: 0.9644782313978959 entropy -11.117628814933097
epoch: 4, step: 98
	action: tensor([[-0.0052,  0.0154,  0.0010, -0.0109, -0.0092,  0.0171,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[1.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29360978469588817, distance: 0.9617872836603473 entropy -11.110052708436587
epoch: 4, step: 99
	action: tensor([[-0.0054,  0.0156,  0.0107, -0.0105, -0.0033, -0.0065,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[1.2486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881634100117352, distance: 0.9654879264543034 entropy -11.121358624287737
epoch: 4, step: 100
	action: tensor([[-0.0051,  0.0154,  0.0129, -0.0109, -0.0076, -0.0063,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[1.2529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881541613946972, distance: 0.965494198539223 entropy -11.117105502493441
epoch: 4, step: 101
	action: tensor([[-0.0052,  0.0154,  0.0210, -0.0109, -0.0090, -0.0124,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[1.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28663031799952443, distance: 0.966527059405564 entropy -11.118982341393977
epoch: 4, step: 102
	action: tensor([[-0.0050,  0.0149,  0.0087, -0.0113, -0.0032, -0.0020,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[1.2546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28883038783603177, distance: 0.9650354982138958 entropy -11.105374392405963
epoch: 4, step: 103
	action: tensor([[-0.0052,  0.0155,  0.0043, -0.0108, -0.0052,  0.0186,  0.0349]],
       dtype=torch.float64)
	q_value: tensor([[1.2520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29402757536101354, distance: 0.9615028196543103 entropy -11.120676953370943
epoch: 4, step: 104
	action: tensor([[-0.0053,  0.0151,  0.0153, -0.0108, -0.0069, -0.0016,  0.0293]],
       dtype=torch.float64)
	q_value: tensor([[1.2492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889130159990134, distance: 0.9649794346354272 entropy -11.109872873887117
epoch: 4, step: 105
	action: tensor([[-0.0051,  0.0151,  0.0092, -0.0111,  0.0050,  0.0043,  0.0483]],
       dtype=torch.float64)
	q_value: tensor([[1.2525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290358593437552, distance: 0.9639980763724392 entropy -11.111392726735678
epoch: 4, step: 106
	action: tensor([[-0.0051,  0.0149,  0.0095, -0.0111, -0.0051, -0.0038,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[1.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28836553903454787, distance: 0.9653508395234938 entropy -11.102545317114467
epoch: 4, step: 107
	action: tensor([[-0.0052,  0.0155,  0.0125, -0.0108, -0.0026, -0.0141,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[1.2522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2863645032672497, distance: 0.9667071155597948 entropy -11.119187063786399
epoch: 4, step: 108
	action: tensor([[-0.0050,  0.0152,  0.0100, -0.0111, -0.0036, -0.0088,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[1.2544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2874955500494534, distance: 0.9659407407422985 entropy -11.111038829720885
epoch: 4, step: 109
	action: tensor([[-0.0051,  0.0155,  0.0098, -0.0109, -0.0028,  0.0250,  0.0155]],
       dtype=torch.float64)
	q_value: tensor([[1.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955467086015402, distance: 0.9604677668263502 entropy -11.119688495656968
epoch: 4, step: 110
	action: tensor([[-0.0054,  0.0153,  0.0096, -0.0106, -0.0088, -0.0003,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[1.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28937375114141906, distance: 0.9646667640423235 entropy -11.118673609926674
epoch: 4, step: 111
	action: tensor([[-0.0051,  0.0151,  0.0027, -0.0111, -0.0100, -0.0086,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[1.2519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28745772204677444, distance: 0.965966382072649 entropy -11.108425744069203
epoch: 4, step: 112
	action: tensor([[-0.0052,  0.0156,  0.0138, -0.0108, -0.0116,  0.0173,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[1.2522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937559759413708, distance: 0.9616877549909747 entropy -11.118113203773527
epoch: 4, step: 113
	action: tensor([[-0.0054,  0.0153,  0.0099, -0.0106,  0.0002,  0.0021, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[1.2494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28993028236331275, distance: 0.9642889477657336 entropy -11.121359880323215
epoch: 4, step: 114
	action: tensor([[-0.0053,  0.0155,  0.0071, -0.0107, -0.0121, -0.0165, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[1.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28599612383582873, distance: 0.9669565910093255 entropy -11.124012565763385
epoch: 4, step: 115
	action: tensor([[-0.0053,  0.0159,  0.0099, -0.0106, -0.0163, -0.0024, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[1.2534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28945335673169137, distance: 0.9646127307033996 entropy -11.12880947375412
epoch: 4, step: 116
	action: tensor([[-0.0054,  0.0157,  0.0085, -0.0106, -0.0080,  0.0193,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[1.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942439439151481, distance: 0.9613554662236594 entropy -11.127067002566083
epoch: 4, step: 117
	action: tensor([[-0.0053,  0.0152,  0.0120, -0.0107, -0.0131,  0.0242,  0.0367]],
       dtype=torch.float64)
	q_value: tensor([[1.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949537447859366, distance: 0.9608719120281667 entropy -11.116126379506065
epoch: 4, step: 118
	action: tensor([[-0.0053,  0.0151,  0.0041, -0.0108, -0.0106, -0.0019, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[1.2484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28884630798274247, distance: 0.9650246965764017 entropy -11.112430144129013
epoch: 4, step: 119
	action: tensor([[-0.0054,  0.0158,  0.0174, -0.0106, -0.0074,  0.0011,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[1.2512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29010680476590345, distance: 0.9641690798302558 entropy -11.125134646630443
epoch: 4, step: 120
	action: tensor([[-0.0051,  0.0151,  0.0179, -0.0110, -0.0024, -0.0062,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[1.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2878367823825806, distance: 0.965709409073915 entropy -11.114780898448204
epoch: 4, step: 121
	action: tensor([[-0.0050,  0.0151,  0.0039, -0.0111, -0.0067, -0.0081,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[1.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28757335751248414, distance: 0.9658879975977177 entropy -11.112436092667455
epoch: 4, step: 122
	action: tensor([[-0.0051,  0.0154,  0.0174, -0.0110, -0.0080,  0.0117,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[1.2526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2922813093422192, distance: 0.9626912532408546 entropy -11.111123824593562
epoch: 4, step: 123
	action: tensor([[-0.0052,  0.0151,  0.0060, -0.0108, -0.0127,  0.0247,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[1.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950498729908363, distance: 0.9608064056589244 entropy -11.116357094233624
epoch: 4, step: 124
	action: tensor([[-0.0054,  0.0154,  0.0135, -0.0106,  0.0015,  0.0142,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[1.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29278118535269604, distance: 0.9623512090436858 entropy -11.116528154274464
epoch: 4, step: 125
	action: tensor([[-0.0052,  0.0151,  0.0047, -0.0108, -0.0051, -0.0030, -0.0121]],
       dtype=torch.float64)
	q_value: tensor([[1.2506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887062076910266, distance: 0.9651197488730581 entropy -11.114088763021199
epoch: 4, step: 126
	action: tensor([[-0.0053,  0.0158,  0.0037, -0.0106, -0.0116,  0.0136, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[1.2517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29302263517152716, distance: 0.9621869180520076 entropy -11.123673418976747
epoch: 4, step: 127
	action: tensor([[-0.0055,  0.0158,  0.0144, -0.0104, -0.0080, -0.0060,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[1.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883575703036101, distance: 0.9653562444047831 entropy -11.124745984166223
LOSS epoch 4 actor 0.5349440286161113 critic 17.61900328905612
epoch: 5, step: 0
	action: tensor([[-0.0043,  0.0157,  0.0030, -0.0118, -0.0192, -0.0126,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[2.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28754605015043155, distance: 0.9659065086969224 entropy -11.10859440015285
epoch: 5, step: 1
	action: tensor([[-0.0044,  0.0161,  0.0048, -0.0117, -0.0050,  0.0216,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[2.0639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29561616632707577, distance: 0.9604204155314909 entropy -11.110696425049719
epoch: 5, step: 2
	action: tensor([[-0.0046,  0.0158, -0.0011, -0.0114, -0.0079,  0.0242, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[2.0586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959280186574422, distance: 0.960207788205146 entropy -11.110809584983576
epoch: 5, step: 3
	action: tensor([[-0.0048,  0.0164, -0.0006, -0.0109, -0.0026, -0.0047,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[2.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28966302630530893, distance: 0.9644704002426526 entropy -11.12287956073517
epoch: 5, step: 4
	action: tensor([[-0.0043,  0.0159,  0.0105, -0.0118, -0.0119,  0.0168,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[2.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943205120578434, distance: 0.9613033156268098 entropy -11.10167781199327
epoch: 5, step: 5
	action: tensor([[-0.0046,  0.0158,  0.0036, -0.0114, -0.0130,  0.0048, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[2.0596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29145898593809816, distance: 0.9632503833957909 entropy -11.1138647458729
epoch: 5, step: 6
	action: tensor([[-0.0046,  0.0163,  0.0087, -0.0113,  0.0013,  0.0039,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[2.0595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2915720080621499, distance: 0.9631735544303367 entropy -11.118396605431027
epoch: 5, step: 7
	action: tensor([[-0.0044,  0.0158,  0.0116, -0.0117, -0.0091,  0.0178,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[2.0639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2944463736173206, distance: 0.9612175851132895 entropy -11.106243387974514
epoch: 5, step: 8
	action: tensor([[-0.0045,  0.0157,  0.0090, -0.0115, -0.0160,  0.0188, -0.0526]],
       dtype=torch.float64)
	q_value: tensor([[2.0600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2945543826329857, distance: 0.9611440087507472 entropy -11.111847638825028
epoch: 5, step: 9
	action: tensor([[-0.0050,  0.0164,  0.0192, -0.0108, -0.0041, -0.0027,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[2.0547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28991380091928853, distance: 0.9643001387669126 entropy -11.13481972041173
epoch: 5, step: 10
	action: tensor([[-0.0044,  0.0157,  0.0074, -0.0117, -0.0060, -0.0078, -0.0273]],
       dtype=torch.float64)
	q_value: tensor([[2.0659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2885379104538016, distance: 0.9652339192639503 entropy -11.113196376947831
epoch: 5, step: 11
	action: tensor([[-0.0045,  0.0163,  0.0220, -0.0114, -0.0058, -0.0266, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[2.0629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2844603232741545, distance: 0.9679959794546781 entropy -11.123842016285463
epoch: 5, step: 12
	action: tensor([[-0.0043,  0.0160,  0.0126, -0.0117, -0.0068, -0.0072, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[2.0702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28886858706934215, distance: 0.9650095802679177 entropy -11.119561494456121
epoch: 5, step: 13
	action: tensor([[-0.0045,  0.0161,  0.0105, -0.0115, -0.0074, -0.0071,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[2.0645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288926958857019, distance: 0.9649699740226664 entropy -11.119497385890972
epoch: 5, step: 14
	action: tensor([[-0.0044,  0.0160,  0.0081, -0.0117, -0.0073, -0.0058,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[2.0650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28915813848004224, distance: 0.964813098767545 entropy -11.112862554074365
epoch: 5, step: 15
	action: tensor([[-0.0044,  0.0160,  0.0196, -0.0117, -0.0067,  0.0020,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[2.0643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2909461356050681, distance: 0.963598926324924 entropy -11.1110534762333
epoch: 5, step: 16
	action: tensor([[-0.0044,  0.0158,  0.0085, -0.0116, -0.0024,  0.0417,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[2.0643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001285433128972, distance: 0.9573391821293247 entropy -11.115451042991435
epoch: 5, step: 17
	action: tensor([[-0.0048,  0.0158,  0.0103, -0.0111, -0.0055,  0.0061,  0.0292]],
       dtype=torch.float64)
	q_value: tensor([[2.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29164307862732197, distance: 0.963125239706348 entropy -11.119213103558902
epoch: 5, step: 18
	action: tensor([[-0.0044,  0.0157,  0.0112, -0.0117, -0.0123,  0.0118,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[2.0631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2929331124128314, distance: 0.9622478357736315 entropy -11.104182899732944
epoch: 5, step: 19
	action: tensor([[-0.0045,  0.0158,  0.0053, -0.0116, -0.0080,  0.0173, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[2.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943064344116979, distance: 0.9613129041302704 entropy -11.109153831497409
epoch: 5, step: 20
	action: tensor([[-0.0047,  0.0162,  0.0092, -0.0111, -0.0028, -0.0079, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[2.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887721438473504, distance: 0.9650750150639467 entropy -11.120893054813811
epoch: 5, step: 21
	action: tensor([[-0.0044,  0.0161,  0.0105, -0.0116,  0.0007, -0.0105, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[2.0647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881497825970011, distance: 0.9654971680707153 entropy -11.116964021206977
epoch: 5, step: 22
	action: tensor([[-0.0043,  0.0161,  0.0131, -0.0116, -0.0076, -0.0011, -0.0367]],
       dtype=torch.float64)
	q_value: tensor([[2.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29032405813028417, distance: 0.9640215329833624 entropy -11.114368497353299
epoch: 5, step: 23
	action: tensor([[-0.0047,  0.0163,  0.0144, -0.0112, -0.0014,  0.0012,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[2.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29089267063550783, distance: 0.9636352548880395 entropy -11.129312963433327
epoch: 5, step: 24
	action: tensor([[-0.0044,  0.0158,  0.0146, -0.0116,  0.0003, -0.0154,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[2.0644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28674986373094913, distance: 0.9664460712249108 entropy -11.112359933380967
epoch: 5, step: 25
	action: tensor([[-0.0042,  0.0157,  0.0062, -0.0119, -0.0101, -0.0063,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[2.0688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889570201148599, distance: 0.9649495763165848 entropy -11.107743797127707
epoch: 5, step: 26
	action: tensor([[-0.0044,  0.0160,  0.0103, -0.0117, -0.0103,  0.0056,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[2.0641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29176067609268186, distance: 0.9630452900476874 entropy -11.109613641943524
epoch: 5, step: 27
	action: tensor([[-0.0045,  0.0159,  0.0079, -0.0116, -0.0071, -0.0041,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[2.0621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28943542218807594, distance: 0.9646249042743338 entropy -11.112633754471034
epoch: 5, step: 28
	action: tensor([[-0.0043,  0.0156,  0.0141, -0.0119,  0.0050,  0.0263, -0.0061]],
       dtype=torch.float64)
	q_value: tensor([[2.0656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29639404978753103, distance: 0.9598899508207346 entropy -11.099425779679066
epoch: 5, step: 29
	action: tensor([[-0.0047,  0.0159,  0.0054, -0.0112, -0.0132,  0.0250, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[2.0584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2961378767300663, distance: 0.9600646761521027 entropy -11.12083674055425
epoch: 5, step: 30
	action: tensor([[-0.0048,  0.0163,  0.0076, -0.0110,  0.0021,  0.0277,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[2.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2970611204138779, distance: 0.9594348194870004 entropy -11.123726333677213
epoch: 5, step: 31
	action: tensor([[-0.0046,  0.0157,  0.0027, -0.0114,  0.0008, -0.0046,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[2.0586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891209647806805, distance: 0.9648383260403929 entropy -11.110766336393736
epoch: 5, step: 32
	action: tensor([[-0.0043,  0.0160,  0.0105, -0.0117, -0.0065, -0.0084, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[2.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28856902893082714, distance: 0.9652128099607067 entropy -11.108371322124498
epoch: 5, step: 33
	action: tensor([[-0.0045,  0.0162,  0.0066, -0.0115, -0.0034,  0.0100,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[2.0643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29293883032088985, distance: 0.9622439450131712 entropy -11.119819018369435
epoch: 5, step: 34
	action: tensor([[-0.0044,  0.0158,  0.0052, -0.0116, -0.0064,  0.0048,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[2.0618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914712721409283, distance: 0.9632420319096695 entropy -11.106208469292906
epoch: 5, step: 35
	action: tensor([[-0.0044,  0.0160,  0.0057, -0.0116, -0.0070,  0.0085, -0.0001]],
       dtype=torch.float64)
	q_value: tensor([[2.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924669548199087, distance: 0.9625649806118602 entropy -11.109365674557123
epoch: 5, step: 36
	action: tensor([[-0.0045,  0.0161,  0.0040, -0.0114, -0.0099, -0.0155,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[2.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28702152745643494, distance: 0.9662620030197002 entropy -11.115390788050723
epoch: 5, step: 37
	action: tensor([[-0.0043,  0.0160,  0.0054, -0.0118,  0.0028, -0.0007,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[2.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904288923992697, distance: 0.9639503270855145 entropy -11.105214401259664
epoch: 5, step: 38
	action: tensor([[-0.0043,  0.0157, -0.0018, -0.0118, -0.0077,  0.0171, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[2.0651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294296729446357, distance: 0.9613195142772206 entropy -11.102895731733483
epoch: 5, step: 39
	action: tensor([[-0.0046,  0.0162,  0.0026, -0.0112,  0.0012, -0.0081, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[2.0571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887633184932539, distance: 0.9650810026685115 entropy -11.115917793908011
epoch: 5, step: 40
	action: tensor([[-0.0044,  0.0162,  0.0203, -0.0115, -0.0092, -0.0012,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[2.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29032666837901644, distance: 0.9640197601049613 entropy -11.11561218283825
epoch: 5, step: 41
	action: tensor([[-0.0043,  0.0156,  0.0004, -0.0118, -0.0100, -0.0011, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[2.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2900272813473086, distance: 0.9642230822313766 entropy -11.109123280345196
epoch: 5, step: 42
	action: tensor([[-0.0046,  0.0163,  0.0046, -0.0114, -0.0102, -0.0157,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[2.0606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871906347993344, distance: 0.9661474051065674 entropy -11.119272468264816
epoch: 5, step: 43
	action: tensor([[-0.0042,  0.0158, -0.0036, -0.0120, -0.0044,  0.0135,  0.0420]],
       dtype=torch.float64)
	q_value: tensor([[2.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2935653019747442, distance: 0.9618175659611939 entropy -11.097742913591322
epoch: 5, step: 44
	action: tensor([[-0.0044,  0.0157,  0.0144, -0.0117, -0.0102,  0.0149,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[2.0607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29360865843997985, distance: 0.9617880503882518 entropy -11.09682071923257
epoch: 5, step: 45
	action: tensor([[-0.0045,  0.0158,  0.0116, -0.0115, -0.0011,  0.0158,  0.0202]],
       dtype=torch.float64)
	q_value: tensor([[2.0604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29397444687348895, distance: 0.9615389982837695 entropy -11.113848230143688
epoch: 5, step: 46
	action: tensor([[-0.0045,  0.0156,  0.0025, -0.0116, -0.0218,  0.0261,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[2.0614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29614752954527557, distance: 0.960058092931913 entropy -11.110180660852164
epoch: 5, step: 47
	action: tensor([[-0.0046,  0.0159,  0.0215, -0.0114, -0.0024,  0.0006,  0.0437]],
       dtype=torch.float64)
	q_value: tensor([[2.0557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2903319714353154, distance: 0.9640161582642384 entropy -11.10770332176881
epoch: 5, step: 48
	action: tensor([[-0.0043,  0.0153,  0.0188, -0.0120, -0.0031,  0.0210,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[2.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2948162925765313, distance: 0.9609655707986363 entropy -11.101689679122597
epoch: 5, step: 49
	action: tensor([[-0.0045,  0.0155, -0.0114, -0.0115, -0.0094, -0.0032,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[2.0611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894815417260098, distance: 0.9645935990431437 entropy -11.11208498253001
epoch: 5, step: 50
	action: tensor([[-0.0044,  0.0163,  0.0071, -0.0116, -0.0046, -0.0157,  0.0386]],
       dtype=torch.float64)
	q_value: tensor([[2.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28711441646592906, distance: 0.9661990571893477 entropy -11.106358483845336
epoch: 5, step: 51
	action: tensor([[-0.0042,  0.0156,  0.0059, -0.0120, -0.0007,  0.0045, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[2.0687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29132256384498467, distance: 0.963343110779358 entropy -11.094540044834018
epoch: 5, step: 52
	action: tensor([[-0.0046,  0.0162,  0.0050, -0.0113, -0.0093, -0.0107,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[2.0611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882164128756104, distance: 0.9654519810010334 entropy -11.120080583081394
epoch: 5, step: 53
	action: tensor([[-0.0043,  0.0159,  0.0106, -0.0119, -0.0145, -0.0083,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[2.0658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28853293331476326, distance: 0.9652372954768852 entropy -11.101748942018423
epoch: 5, step: 54
	action: tensor([[-0.0043,  0.0158,  0.0050, -0.0118,  0.0009, -0.0095,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[2.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28819958640551646, distance: 0.9654633925123214 entropy -11.104026994765576
epoch: 5, step: 55
	action: tensor([[-0.0042,  0.0158, -0.0021, -0.0119, -0.0089,  0.0005,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[2.0667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29057240196421097, distance: 0.9638528434888383 entropy -11.100225838288376
epoch: 5, step: 56
	action: tensor([[-0.0044,  0.0161,  0.0107, -0.0116,  0.0017,  0.0281, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[2.0616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297135202580608, distance: 0.9593842611204666 entropy -11.107978897071641
epoch: 5, step: 57
	action: tensor([[-0.0048,  0.0161,  0.0103, -0.0110, -0.0033,  0.0142, -0.0293]],
       dtype=torch.float64)
	q_value: tensor([[2.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937887644201691, distance: 0.9616654308059316 entropy -11.123343733232673
epoch: 5, step: 58
	action: tensor([[-0.0048,  0.0163,  0.0131, -0.0110, -0.0026,  0.0049,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[2.0584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917153521352881, distance: 0.9630761047200098 entropy -11.126486718542166
epoch: 5, step: 59
	action: tensor([[-0.0043,  0.0154,  0.0097, -0.0119, -0.0020, -0.0010,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[2.0652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28978998592431815, distance: 0.9643842057526082 entropy -11.100594704052797
epoch: 5, step: 60
	action: tensor([[-0.0044,  0.0160,  0.0065, -0.0116, -0.0150,  0.0177,  0.0362]],
       dtype=torch.float64)
	q_value: tensor([[2.0636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294542969949636, distance: 0.9611517834023935 entropy -11.11455535993129
epoch: 5, step: 61
	action: tensor([[-0.0045,  0.0156,  0.0193, -0.0117, -0.0112, -0.0129,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[2.0598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.287021563404643, distance: 0.9662619786603477 entropy -11.103416851710454
epoch: 5, step: 62
	action: tensor([[-0.0042,  0.0152, -0.0013, -0.0122, -0.0039,  0.0005,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[2.0700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2900843114713094, distance: 0.964184354781126 entropy -11.092547517046707
epoch: 5, step: 63
	action: tensor([[-0.0044,  0.0161,  0.0188, -0.0116, -0.0103, -0.0066, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[2.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28898668521260473, distance: 0.9649294469974845 entropy -11.110109093117297
epoch: 5, step: 64
	action: tensor([[-0.0045,  0.0160,  0.0064, -0.0115, -0.0032,  0.0125,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[2.0648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934136550595142, distance: 0.961920794780895 entropy -11.121100048817738
epoch: 5, step: 65
	action: tensor([[-0.0045,  0.0159, -0.0114, -0.0115, -0.0062, -0.0236, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[2.0607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2852748172678983, distance: 0.9674448909337097 entropy -11.110092411406084
epoch: 5, step: 66
	action: tensor([[-0.0043,  0.0165,  0.0137, -0.0117, -0.0082,  0.0084, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[2.0650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2928068200715833, distance: 0.9623337676056478 entropy -11.107824238020713
epoch: 5, step: 67
	action: tensor([[-0.0045,  0.0160,  0.0049, -0.0114, -0.0059, -0.0165,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[2.0618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28668508173501284, distance: 0.9664899596735186 entropy -11.116151390343445
epoch: 5, step: 68
	action: tensor([[-0.0043,  0.0160, -0.0037, -0.0118, -0.0083,  0.0282,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[2.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2970270300411243, distance: 0.9594580840232463 entropy -11.105838232153035
epoch: 5, step: 69
	action: tensor([[-0.0047,  0.0160,  0.0024, -0.0113, -0.0107,  0.0019,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[2.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290896757095922, distance: 0.963632478254071 entropy -11.111796485415699
epoch: 5, step: 70
	action: tensor([[-0.0044,  0.0158,  0.0119, -0.0117, -0.0100, -0.0125,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[2.0626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28741954895400146, distance: 0.9659922566288753 entropy -11.10334728188104
epoch: 5, step: 71
	action: tensor([[-0.0043,  0.0158,  0.0040, -0.0119, -0.0135, -0.0016,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[2.0668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2900784423180165, distance: 0.964188340419263 entropy -11.106629108885404
epoch: 5, step: 72
	action: tensor([[-0.0045,  0.0161,  0.0072, -0.0116, -0.0044,  0.0169, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[2.0621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2944674092375281, distance: 0.9612032559694701 entropy -11.112432042135334
epoch: 5, step: 73
	action: tensor([[-0.0046,  0.0160,  0.0146, -0.0113, -0.0101,  0.0029,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[2.0590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2910864512294119, distance: 0.963503577662757 entropy -11.117563236368104
epoch: 5, step: 74
	action: tensor([[-0.0045,  0.0159,  0.0063, -0.0115, -0.0117,  0.0162, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[2.0629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2941420012344905, distance: 0.9614248950395333 entropy -11.116068433017952
epoch: 5, step: 75
	action: tensor([[-0.0047,  0.0163,  0.0114, -0.0111, -0.0007, -0.0115,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[2.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28792387794304763, distance: 0.9656503554947061 entropy -11.121907431630758
epoch: 5, step: 76
	action: tensor([[-0.0042,  0.0157,  0.0190, -0.0119, -0.0069,  0.0032,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[2.0680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2910070458880839, distance: 0.9635575371253128 entropy -11.101503381125466
epoch: 5, step: 77
	action: tensor([[-0.0045,  0.0158,  0.0133, -0.0115, -0.0044, -0.0141,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[2.0636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28705845353716375, distance: 0.9662369807139108 entropy -11.118169816144293
epoch: 5, step: 78
	action: tensor([[-0.0043,  0.0159,  0.0055, -0.0118, -0.0062, -0.0093,  0.0014]],
       dtype=torch.float64)
	q_value: tensor([[2.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883639208727604, distance: 0.9653519370623551 entropy -11.110574005826422
epoch: 5, step: 79
	action: tensor([[-0.0044,  0.0161,  0.0127, -0.0116, -0.0015,  0.0199, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[2.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2952166324229404, distance: 0.960692757188651 entropy -11.112515908413197
epoch: 5, step: 80
	action: tensor([[-0.0046,  0.0159,  0.0045, -0.0112, -0.0094, -0.0087,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[2.0593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288422627800366, distance: 0.9653121175416688 entropy -11.120679822725865
epoch: 5, step: 81
	action: tensor([[-0.0043,  0.0160,  0.0016, -0.0117, -0.0033,  0.0194,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[2.0644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950590676272037, distance: 0.9608001397586944 entropy -11.107446329824265
epoch: 5, step: 82
	action: tensor([[-0.0046,  0.0160,  0.0111, -0.0114,  0.0019,  0.0120, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[2.0584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29323296127849474, distance: 0.9620437818693832 entropy -11.109838701668094
epoch: 5, step: 83
	action: tensor([[-0.0046,  0.0161,  0.0062, -0.0112,  0.0015, -0.0025,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[2.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28997379588843264, distance: 0.9642594011922375 entropy -11.120345451455583
epoch: 5, step: 84
	action: tensor([[-4.3666e-03,  1.5991e-02,  1.0990e-02, -1.1630e-02, -9.6312e-05,
          2.2298e-02, -2.3785e-02]], dtype=torch.float64)
	q_value: tensor([[2.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29574286242453285, distance: 0.9603340372069943 entropy -11.11023165028019
epoch: 5, step: 85
	action: tensor([[-0.0048,  0.0161,  0.0090, -0.0110, -0.0074,  0.0126,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[2.0576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934316451423391, distance: 0.9619085491827148 entropy -11.126476660974813
epoch: 5, step: 86
	action: tensor([[-0.0045,  0.0160,  0.0040, -0.0114, -0.0114, -0.0119,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[2.0601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877884426779661, distance: 0.9657421833834318 entropy -11.11437949207818
epoch: 5, step: 87
	action: tensor([[-0.0043,  0.0161,  0.0055, -0.0117, -0.0093, -0.0101, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[2.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288322861927061, distance: 0.9653797853981997 entropy -11.109205705305637
epoch: 5, step: 88
	action: tensor([[-0.0044,  0.0162,  0.0077, -0.0116, -0.0050, -0.0152,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[2.0644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871712077173625, distance: 0.9661605708265111 entropy -11.112738530847377
epoch: 5, step: 89
	action: tensor([[-0.0042,  0.0158,  0.0195, -0.0119, -0.0029, -0.0026, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[2.0677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28973318293942407, distance: 0.9644227709693032 entropy -11.101354520471807
epoch: 5, step: 90
	action: tensor([[-0.0045,  0.0161,  0.0156, -0.0113, -0.0014,  0.0037, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[2.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29140230127623123, distance: 0.9632889135790683 entropy -11.12481751170462
epoch: 5, step: 91
	action: tensor([[-0.0045,  0.0159,  0.0032, -0.0115, -0.0155,  0.0124,  0.0491]],
       dtype=torch.float64)
	q_value: tensor([[2.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29328648424561155, distance: 0.9620073537326915 entropy -11.119084869031218
epoch: 5, step: 92
	action: tensor([[-0.0044,  0.0156,  0.0044, -0.0118, -0.0107, -0.0196, -0.0045]],
       dtype=torch.float64)
	q_value: tensor([[2.0610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2856682203453391, distance: 0.9671786010411849 entropy -11.095401723904862
epoch: 5, step: 93
	action: tensor([[-0.0043,  0.0162,  0.0081, -0.0117, -0.0104, -0.0016,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[2.0659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29033639798831834, distance: 0.9640131517350566 entropy -11.113614007452085
epoch: 5, step: 94
	action: tensor([[-0.0044,  0.0160,  0.0070, -0.0116,  0.0015, -0.0057, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[2.0635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891325104855198, distance: 0.9648304908228593 entropy -11.111320615655156
epoch: 5, step: 95
	action: tensor([[-0.0044,  0.0161,  0.0018, -0.0116, -0.0021,  0.0246,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[2.0645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29635746204214475, distance: 0.9599149077959812 entropy -11.11385007109323
epoch: 5, step: 96
	action: tensor([[-0.0046,  0.0160,  0.0119, -0.0113,  0.0023, -0.0161,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[2.0574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2866447875444088, distance: 0.9665172571403863 entropy -11.111072031714206
epoch: 5, step: 97
	action: tensor([[-0.0042,  0.0159,  0.0068, -0.0118, -0.0076, -0.0046,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[2.0683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894490168219058, distance: 0.964615676552118 entropy -11.110138856844959
epoch: 5, step: 98
	action: tensor([[-0.0043,  0.0159,  0.0129, -0.0117, -0.0014,  0.0039,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[2.0644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29128527409701754, distance: 0.9633684554195516 entropy -11.107492653024886
epoch: 5, step: 99
	action: tensor([[-0.0044,  0.0158,  0.0143, -0.0116, -0.0015, -0.0037, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[2.0637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28942782857235805, distance: 0.9646300586061803 entropy -11.112563757433344
epoch: 5, step: 100
	action: tensor([[-0.0045,  0.0161,  0.0052, -0.0115, -0.0114,  0.0374,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[2.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29918898742654665, distance: 0.9579815657976024 entropy -11.12134240344811
epoch: 5, step: 101
	action: tensor([[-0.0048,  0.0159,  0.0047, -0.0111, -0.0023, -0.0057,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[2.0539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2890076166518608, distance: 0.9649152436694066 entropy -11.115982847538158
epoch: 5, step: 102
	action: tensor([[-0.0043,  0.0159,  0.0221, -0.0117, -0.0079,  0.0059,  0.0340]],
       dtype=torch.float64)
	q_value: tensor([[2.0645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917488116208278, distance: 0.9630533565130052 entropy -11.108632345623684
epoch: 5, step: 103
	action: tensor([[-0.0043,  0.0154,  0.0062, -0.0118, -0.0061, -0.0085, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[2.0653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881283212283656, distance: 0.9655117222100266 entropy -11.105845491606564
epoch: 5, step: 104
	action: tensor([[-0.0044,  0.0161,  0.0062, -0.0116, -0.0156,  0.0053,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[2.0641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2918149329101486, distance: 0.963008400841232 entropy -11.114505636805998
epoch: 5, step: 105
	action: tensor([[-0.0044,  0.0156,  0.0087, -0.0118, -0.0095,  0.0015, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[2.0627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290526575459642, distance: 0.9638839737222651 entropy -11.100226327839929
epoch: 5, step: 106
	action: tensor([[-0.0045,  0.0161,  0.0143, -0.0115, -0.0119, -0.0068,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[2.0620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28891205797974306, distance: 0.9649800846744543 entropy -11.116198020545543
epoch: 5, step: 107
	action: tensor([[-0.0043,  0.0157,  0.0238, -0.0118, -0.0096, -0.0004, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[2.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901346115321537, distance: 0.9641501962242731 entropy -11.10477235515929
epoch: 5, step: 108
	action: tensor([[-0.0045,  0.0160,  0.0110, -0.0114, -0.0011,  0.0004,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[2.0639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29052993091850843, distance: 0.9638816943720188 entropy -11.12372589245931
epoch: 5, step: 109
	action: tensor([[-0.0043,  0.0156,  0.0084, -0.0118, -0.0051, -0.0020,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[2.0652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289793530062328, distance: 0.9643817994819867 entropy -11.10284885266964
epoch: 5, step: 110
	action: tensor([[-0.0044,  0.0160,  0.0126, -0.0116, -0.0158,  0.0107, -0.0050]],
       dtype=torch.float64)
	q_value: tensor([[2.0636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29292170532923134, distance: 0.962255597696463 entropy -11.112912177121936
epoch: 5, step: 111
	action: tensor([[-4.5991e-03,  1.6034e-02,  1.0739e-02, -1.1337e-02, -4.3957e-05,
         -4.6040e-03,  2.2740e-02]], dtype=torch.float64)
	q_value: tensor([[2.0599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28936507677811196, distance: 0.9646726516973739 entropy -11.120421991881766
epoch: 5, step: 112
	action: tensor([[-0.0043,  0.0157,  0.0099, -0.0118, -0.0107,  0.0094,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[2.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924952757312508, distance: 0.9625457157941505 entropy -11.10411465982632
epoch: 5, step: 113
	action: tensor([[-0.0045,  0.0160,  0.0023, -0.0115, -0.0062,  0.0063, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[2.0607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29195041567379554, distance: 0.9629162799491661 entropy -11.113854603947882
epoch: 5, step: 114
	action: tensor([[-0.0045,  0.0162,  0.0094, -0.0114, -0.0050, -0.0209, -0.0219]],
       dtype=torch.float64)
	q_value: tensor([[2.0604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28579719120380453, distance: 0.9670912862325723 entropy -11.11311258622027
epoch: 5, step: 115
	action: tensor([[-0.0044,  0.0162,  0.0040, -0.0116, -0.0109, -0.0108,  0.0227]],
       dtype=torch.float64)
	q_value: tensor([[2.0668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883074227752188, distance: 0.9653902568349371 entropy -11.119552101901446
epoch: 5, step: 116
	action: tensor([[-0.0043,  0.0159,  0.0046, -0.0119, -0.0055,  0.0016,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[2.0656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2908338807308495, distance: 0.9636752000756379 entropy -11.101964603364635
epoch: 5, step: 117
	action: tensor([[-0.0044,  0.0161, -0.0041, -0.0115, -0.0107,  0.0018, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[2.0622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2910852382447323, distance: 0.9635044019625468 entropy -11.111446752766573
epoch: 5, step: 118
	action: tensor([[-0.0047,  0.0166,  0.0066, -0.0112, -0.0181,  0.0106, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[2.0588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2932777614796023, distance: 0.9620132906073698 entropy -11.121018199075307
epoch: 5, step: 119
	action: tensor([[-0.0046,  0.0162,  0.0066, -0.0113, -0.0110,  0.0034,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[2.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29131272638558114, distance: 0.9633497970482955 entropy -11.119733784484412
epoch: 5, step: 120
	action: tensor([[-0.0044,  0.0158,  0.0086, -0.0117, -0.0025, -0.0062, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[2.0625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28890487587113733, distance: 0.9649849578932652 entropy -11.107073077896374
epoch: 5, step: 121
	action: tensor([[-0.0044,  0.0162,  0.0129, -0.0115, -0.0023,  0.0168, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[2.0641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29453199825110143, distance: 0.961159257583075 entropy -11.11658814703702
epoch: 5, step: 122
	action: tensor([[-0.0046,  0.0160,  0.0087, -0.0113, -0.0058,  0.0122,  0.0372]],
       dtype=torch.float64)
	q_value: tensor([[2.0599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2932781791983109, distance: 0.9620130063011836 entropy -11.119062571485392
epoch: 5, step: 123
	action: tensor([[-0.0044,  0.0155,  0.0084, -0.0117, -0.0107, -0.0034, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[2.0622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289323062335177, distance: 0.9647011681575902 entropy -11.102466473355452
epoch: 5, step: 124
	action: tensor([[-0.0044,  0.0161,  0.0102, -0.0115, -0.0017, -0.0165, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[2.0627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286776396049632, distance: 0.9664280955580878 entropy -11.114556737911245
epoch: 5, step: 125
	action: tensor([[-0.0044,  0.0162,  0.0066, -0.0116, -0.0077, -0.0130, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[2.0664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877549329044272, distance: 0.9657649023500647 entropy -11.11808213213152
epoch: 5, step: 126
	action: tensor([[-0.0045,  0.0164,  0.0096, -0.0114, -0.0062, -0.0003,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[2.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29071556303006063, distance: 0.9637555867970946 entropy -11.121466999079727
epoch: 5, step: 127
	action: tensor([[-0.0044,  0.0159,  0.0015, -0.0117, -0.0103, -0.0136, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[2.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28740178172788544, distance: 0.9660042994065308 entropy -11.108979443644577
LOSS epoch 5 actor 1.6384905060172628 critic 16.77837528120536
epoch: 6, step: 0
	action: tensor([[-0.0049,  0.0169, -0.0019, -0.0121, -0.0017, -0.0048,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[3.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894258846045752, distance: 0.9646313781121709 entropy -11.11178219821539
epoch: 6, step: 1
	action: tensor([[-0.0047,  0.0164,  0.0087, -0.0125, -0.0155,  0.0105, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[3.2854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29254641202354126, distance: 0.9625109302253789 entropy -11.092975793317805
epoch: 6, step: 2
	action: tensor([[-0.0051,  0.0168,  0.0152, -0.0118, -0.0024,  0.0041,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[3.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29118525870922285, distance: 0.9634364293638173 entropy -11.117778221033754
epoch: 6, step: 3
	action: tensor([[-0.0048,  0.0163,  0.0102, -0.0123, -0.0098, -0.0211,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[3.2851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28514211120054256, distance: 0.9675347015707414 entropy -11.106154378030935
epoch: 6, step: 4
	action: tensor([[-0.0046,  0.0165,  0.0108, -0.0125, -0.0033,  0.0179,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[3.2884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29438044074627534, distance: 0.9612624961960139 entropy -11.103819824343308
epoch: 6, step: 5
	action: tensor([[-0.0049,  0.0164,  0.0163, -0.0121, -0.0064,  0.0052,  0.0530]],
       dtype=torch.float64)
	q_value: tensor([[3.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2911390454200322, distance: 0.9634678359063491 entropy -11.105387907194142
epoch: 6, step: 6
	action: tensor([[-0.0047,  0.0157,  0.0220, -0.0127, -0.0011, -0.0139,  0.0025]],
       dtype=torch.float64)
	q_value: tensor([[3.2927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28613269332692115, distance: 0.9668641103595202 entropy -11.088804269338565
epoch: 6, step: 7
	action: tensor([[-0.0046,  0.0163,  0.0116, -0.0125, -0.0100,  0.0248, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[3.2918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2957973760149082, distance: 0.9602968687733027 entropy -11.10730818713128
epoch: 6, step: 8
	action: tensor([[-0.0050,  0.0165,  0.0058, -0.0119, -0.0034,  0.0027,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[3.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290702691971616, distance: 0.9637643311721035 entropy -11.113041704389058
epoch: 6, step: 9
	action: tensor([[-0.0047,  0.0161, -0.0008, -0.0125, -0.0134,  0.0165,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[3.2873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29369497708194936, distance: 0.961729284963309 entropy -11.09043287680529
epoch: 6, step: 10
	action: tensor([[-0.0049,  0.0165,  0.0115, -0.0121, -0.0099, -0.0118,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[3.2749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28734619102946213, distance: 0.9660419782871607 entropy -11.101083193707229
epoch: 6, step: 11
	action: tensor([[-0.0046,  0.0162, -0.0039, -0.0126, -0.0076,  0.0238,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[3.2907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954890472420284, distance: 0.9605070744317208 entropy -11.093286728836699
epoch: 6, step: 12
	action: tensor([[-0.0050,  0.0165,  0.0113, -0.0120, -0.0059, -0.0237, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[3.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28465831074132564, distance: 0.9678620495430481 entropy -11.099750636883112
epoch: 6, step: 13
	action: tensor([[-0.0046,  0.0166,  0.0183, -0.0124, -0.0223,  0.0002, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[3.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29030064585924087, distance: 0.9640374344293232 entropy -11.105973575798673
epoch: 6, step: 14
	action: tensor([[-0.0049,  0.0166,  0.0034, -0.0121, -0.0001,  0.0145, -0.0217]],
       dtype=torch.float64)
	q_value: tensor([[3.2797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2935167950376052, distance: 0.9618505867226935 entropy -11.114168276353618
epoch: 6, step: 15
	action: tensor([[-0.0051,  0.0168,  0.0158, -0.0118, -0.0054, -0.0002,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[3.2738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29015131188551924, distance: 0.9641388548179028 entropy -11.113859896008531
epoch: 6, step: 16
	action: tensor([[-0.0047,  0.0160, -0.0010, -0.0125, -0.0016, -0.0044,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[3.2908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28884493297093783, distance: 0.9650256295109579 entropy -11.095680703193144
epoch: 6, step: 17
	action: tensor([[-0.0047,  0.0161, -0.0010, -0.0126, -0.0030,  0.0038, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[3.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29081238096731477, distance: 0.9636898078179796 entropy -11.084466071713596
epoch: 6, step: 18
	action: tensor([[-0.0049,  0.0168,  0.0069, -0.0121, -0.0040,  0.0002, -0.0405]],
       dtype=torch.float64)
	q_value: tensor([[3.2771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2903982200756404, distance: 0.9639711609924828 entropy -11.1068806957613
epoch: 6, step: 19
	action: tensor([[-0.0050,  0.0169,  0.0090, -0.0118, -0.0104, -0.0085,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[3.2748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28847672503640476, distance: 0.9652754232115341 entropy -11.119117911000036
epoch: 6, step: 20
	action: tensor([[-0.0046,  0.0162,  0.0120, -0.0126, -0.0156,  0.0008,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[3.2901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29010147062107017, distance: 0.964172702211685 entropy -11.090595750302636
epoch: 6, step: 21
	action: tensor([[-0.0048,  0.0165,  0.0139, -0.0123, -0.0028, -0.0071, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[3.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2884438237885574, distance: 0.9652977404005356 entropy -11.104132157441594
epoch: 6, step: 22
	action: tensor([[-0.0048,  0.0167,  0.0117, -0.0122,  0.0022, -0.0146, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[3.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28685292028908815, distance: 0.9663762484526182 entropy -11.112515089483972
epoch: 6, step: 23
	action: tensor([[-0.0047,  0.0166,  0.0121, -0.0123, -0.0008,  0.0027, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[3.2870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29094201558384225, distance: 0.9636017258599537 entropy -11.109252975690001
epoch: 6, step: 24
	action: tensor([[-0.0048,  0.0165,  0.0129, -0.0121, -0.0073, -0.0111,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[3.2826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28760173721691595, distance: 0.9658687592037246 entropy -11.10877891477554
epoch: 6, step: 25
	action: tensor([[-0.0047,  0.0165,  0.0107, -0.0124, -0.0052,  0.0208,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[3.2870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29495464928295523, distance: 0.9608712956813298 entropy -11.105304729686395
epoch: 6, step: 26
	action: tensor([[-0.0050,  0.0163,  0.0013, -0.0121,  0.0039,  0.0094,  0.0500]],
       dtype=torch.float64)
	q_value: tensor([[3.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29216767484418416, distance: 0.9627685371588048 entropy -11.106621320959858
epoch: 6, step: 27
	action: tensor([[-0.0047,  0.0159,  0.0108, -0.0125, -0.0081, -0.0039,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[3.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887613693706427, distance: 0.9650823250552529 entropy -11.086286495193216
epoch: 6, step: 28
	action: tensor([[-0.0047,  0.0163,  0.0093, -0.0124, -0.0117,  0.0104,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[3.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924235844791496, distance: 0.9625944818003687 entropy -11.098844268478912
epoch: 6, step: 29
	action: tensor([[-0.0048,  0.0162,  0.0138, -0.0124, -0.0138,  0.0007, -0.0200]],
       dtype=torch.float64)
	q_value: tensor([[3.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2899777291407609, distance: 0.9642567303888465 entropy -11.095199175443284
epoch: 6, step: 30
	action: tensor([[-0.0050,  0.0167,  0.0130, -0.0120, -0.0021, -0.0143,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[3.2780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28686039770577454, distance: 0.9663711821647285 entropy -11.11595171892982
epoch: 6, step: 31
	action: tensor([[-0.0045,  0.0160,  0.0031, -0.0127, -0.0174, -0.0151,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[3.2958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28647010395694983, distance: 0.966635588347787 entropy -11.090155977218856
epoch: 6, step: 32
	action: tensor([[-0.0046,  0.0163,  0.0048, -0.0127, -0.0069,  0.0132,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[3.2882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293098257149894, distance: 0.9621354564156178 entropy -11.088476060689995
epoch: 6, step: 33
	action: tensor([[-0.0049,  0.0165,  0.0169, -0.0121, -0.0123,  0.0069,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[3.2771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2916767883203728, distance: 0.9631023225571751 entropy -11.106691272649813
epoch: 6, step: 34
	action: tensor([[-0.0048,  0.0161,  0.0074, -0.0124, -0.0154, -0.0062,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[3.2860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28841020397504236, distance: 0.9653205444656074 entropy -11.099769915518294
epoch: 6, step: 35
	action: tensor([[-0.0048,  0.0166,  0.0054, -0.0123, -0.0078, -0.0036, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[3.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28942084682657643, distance: 0.9646347975936965 entropy -11.105545306984952
epoch: 6, step: 36
	action: tensor([[-0.0050,  0.0170,  0.0007, -0.0118, -0.0070,  0.0018,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[3.2744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29096468805658326, distance: 0.9635863199206726 entropy -11.119076000211589
epoch: 6, step: 37
	action: tensor([[-0.0049,  0.0166,  0.0030, -0.0122,  0.0135,  0.0232,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[3.2795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2957293133332365, distance: 0.9603432750196264 entropy -11.103888301520454
epoch: 6, step: 38
	action: tensor([[-0.0050,  0.0165, -0.0028, -0.0120, -0.0088, -0.0041, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[3.2781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892490934818409, distance: 0.9647513709844757 entropy -11.104512313250897
epoch: 6, step: 39
	action: tensor([[-0.0048,  0.0168,  0.0014, -0.0122, -0.0074,  0.0131,  0.0333]],
       dtype=torch.float64)
	q_value: tensor([[3.2785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2933647783161142, distance: 0.9619540637105665 entropy -11.104630313911006
epoch: 6, step: 40
	action: tensor([[-0.0048,  0.0162,  0.0027, -0.0124, -0.0019, -0.0150,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[3.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28642471961788385, distance: 0.9666663294701748 entropy -11.09263333055853
epoch: 6, step: 41
	action: tensor([[-0.0046,  0.0165,  0.0126, -0.0125, -0.0047, -0.0100, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[3.2877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28788680229486385, distance: 0.9656754944129518 entropy -11.09691877509176
epoch: 6, step: 42
	action: tensor([[-0.0048,  0.0166,  0.0092, -0.0122, -0.0031, -0.0105, -0.0058]],
       dtype=torch.float64)
	q_value: tensor([[3.2840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2878310498112211, distance: 0.9657132958142175 entropy -11.112515628620999
epoch: 6, step: 43
	action: tensor([[-0.0047,  0.0166,  0.0070, -0.0123, -0.0188, -0.0100, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[3.2851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2880685515769661, distance: 0.9655522541552597 entropy -11.10668062928971
epoch: 6, step: 44
	action: tensor([[-0.0048,  0.0168,  0.0010, -0.0122, -0.0064,  0.0076,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[3.2801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29214552171336783, distance: 0.9627836029941129 entropy -11.108695826790784
epoch: 6, step: 45
	action: tensor([[-0.0049,  0.0166,  0.0055, -0.0122,  0.0021, -0.0049,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[3.2788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28905010520925556, distance: 0.9648864118069329 entropy -11.102687884117518
epoch: 6, step: 46
	action: tensor([[-0.0047,  0.0163,  0.0052, -0.0125, -0.0036,  0.0020,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[3.2886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904682391215295, distance: 0.9639236005099856 entropy -11.095773720610357
epoch: 6, step: 47
	action: tensor([[-0.0048,  0.0163,  0.0160, -0.0124, -0.0164,  0.0301,  0.0228]],
       dtype=torch.float64)
	q_value: tensor([[3.2843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969350969534703, distance: 0.9595208197643467 entropy -11.098388359006
epoch: 6, step: 48
	action: tensor([[-0.0050,  0.0161,  0.0169, -0.0120, -0.0083, -0.0056,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[3.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28833212391000096, distance: 0.9653735035045459 entropy -11.105916186800641
epoch: 6, step: 49
	action: tensor([[-0.0047,  0.0163,  0.0040, -0.0124, -0.0067,  0.0351, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[3.2873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29819708720253557, distance: 0.9586592707163929 entropy -11.103021795216053
epoch: 6, step: 50
	action: tensor([[-0.0051,  0.0166, -0.0016, -0.0117, -0.0147, -0.0097,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[3.2687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2879999365535686, distance: 0.9655987823669495 entropy -11.111240087655329
epoch: 6, step: 51
	action: tensor([[-0.0047,  0.0165,  0.0026, -0.0125, -0.0069, -0.0118,  0.0300]],
       dtype=torch.float64)
	q_value: tensor([[3.2837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2874559714977858, distance: 0.9659675686481596 entropy -11.094160545903573
epoch: 6, step: 52
	action: tensor([[-0.0046,  0.0163,  0.0059, -0.0126, -0.0030, -0.0098, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[3.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877928010790769, distance: 0.9657392284340204 entropy -11.089815575545057
epoch: 6, step: 53
	action: tensor([[-0.0048,  0.0168,  0.0162, -0.0122, -0.0016,  0.0151,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[3.2816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938553051487909, distance: 0.961620124654868 entropy -11.111814344104323
epoch: 6, step: 54
	action: tensor([[-0.0049,  0.0163,  0.0021, -0.0121, -0.0027,  0.0055,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[3.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29127004826206926, distance: 0.9633788037372739 entropy -11.106785112211366
epoch: 6, step: 55
	action: tensor([[-0.0049,  0.0166,  0.0193, -0.0122, -0.0066,  0.0023,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[3.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29063863398665235, distance: 0.9638078498817524 entropy -11.104207524806403
epoch: 6, step: 56
	action: tensor([[-0.0048,  0.0162,  0.0153, -0.0123, -0.0100,  0.0019,  0.0293]],
       dtype=torch.float64)
	q_value: tensor([[3.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29032179940327796, distance: 0.9640230671059176 entropy -11.106645235173934
epoch: 6, step: 57
	action: tensor([[-0.0047,  0.0161,  0.0038, -0.0125, -0.0147, -0.0244,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[3.2875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843921976085898, distance: 0.9680420592200211 entropy -11.097089973798433
epoch: 6, step: 58
	action: tensor([[-0.0046,  0.0167,  0.0095, -0.0125,  0.0050,  0.0162,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[3.2864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29410835353333975, distance: 0.9614478099544959 entropy -11.101602338918253
epoch: 6, step: 59
	action: tensor([[-4.8222e-03,  1.6107e-02,  1.0105e-02, -1.2246e-02, -8.8068e-07,
         -3.3383e-03,  2.0415e-02]], dtype=torch.float64)
	q_value: tensor([[3.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28905050334162674, distance: 0.9648861416384079 entropy -11.10014404498143
epoch: 6, step: 60
	action: tensor([[-0.0047,  0.0163,  0.0041, -0.0125, -0.0175, -0.0159, -0.0275]],
       dtype=torch.float64)
	q_value: tensor([[3.2881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864901111096456, distance: 0.9666220361746436 entropy -11.097529374420029
epoch: 6, step: 61
	action: tensor([[-0.0049,  0.0169,  0.0098, -0.0121, -0.0105, -0.0123, -0.0009]],
       dtype=torch.float64)
	q_value: tensor([[3.2782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2876425826563187, distance: 0.9658410697065436 entropy -11.114319541455762
epoch: 6, step: 62
	action: tensor([[-0.0047,  0.0165,  0.0115, -0.0124, -0.0165, -0.0095, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[3.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28799770647641165, distance: 0.9656002945564925 entropy -11.104231425316215
epoch: 6, step: 63
	action: tensor([[-0.0048,  0.0167,  0.0021, -0.0123, -0.0090, -0.0065,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[3.2823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888567376609218, distance: 0.9650176200934724 entropy -11.108286288502859
epoch: 6, step: 64
	action: tensor([[-0.0047,  0.0166, -0.0114, -0.0124, -0.0089, -0.0148, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[3.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2870694525116829, distance: 0.9662295273280245 entropy -11.098597635591329
epoch: 6, step: 65
	action: tensor([[-0.0048,  0.0171,  0.0047, -0.0122, -0.0059, -0.0108,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[3.2761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28818259965965587, distance: 0.9654749125839098 entropy -11.10670169606305
epoch: 6, step: 66
	action: tensor([[-0.0046,  0.0163,  0.0060, -0.0126, -0.0100, -0.0100, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[3.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28778463852786396, distance: 0.9657447625491882 entropy -11.092584183604306
epoch: 6, step: 67
	action: tensor([[-0.0048,  0.0167,  0.0023, -0.0123, -0.0138, -0.0059,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[3.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288975756573421, distance: 0.9649368626991479 entropy -11.10621618313907
epoch: 6, step: 68
	action: tensor([[-0.0048,  0.0167,  0.0080, -0.0123, -0.0003, -0.0068, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[3.2812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2886930605533342, distance: 0.9651286681858188 entropy -11.101528073716917
epoch: 6, step: 69
	action: tensor([[-0.0048,  0.0166,  0.0129, -0.0123, -0.0101, -0.0158, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[3.2844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28658718270694616, distance: 0.9665562804380279 entropy -11.107714290940043
epoch: 6, step: 70
	action: tensor([[-0.0047,  0.0165,  0.0060, -0.0124, -0.0193, -0.0136, -0.0090]],
       dtype=torch.float64)
	q_value: tensor([[3.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871711313646649, distance: 0.9661606225703273 entropy -11.106310888378784
epoch: 6, step: 71
	action: tensor([[-0.0048,  0.0168,  0.0065, -0.0123, -0.0101, -0.0066,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[3.2809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28887901131570115, distance: 0.9650025073596721 entropy -11.10768513336836
epoch: 6, step: 72
	action: tensor([[-0.0047,  0.0163,  0.0064, -0.0125,  0.0029, -0.0147, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[3.2869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2866509933300875, distance: 0.9665130530553157 entropy -11.09504464286285
epoch: 6, step: 73
	action: tensor([[-0.0048,  0.0168,  0.0083, -0.0122, -0.0101,  0.0113, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[3.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930711863914739, distance: 0.9621538786980787 entropy -11.111950446198739
epoch: 6, step: 74
	action: tensor([[-0.0050,  0.0167,  0.0171, -0.0119, -0.0111,  0.0153,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[3.2757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937159808368561, distance: 0.9617149851383688 entropy -11.113135243564372
epoch: 6, step: 75
	action: tensor([[-0.0049,  0.0163,  0.0084, -0.0121, -0.0086, -0.0006,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[3.2805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28976930018406744, distance: 0.9643982500884588 entropy -11.108609335948634
epoch: 6, step: 76
	action: tensor([[-0.0048,  0.0164,  0.0093, -0.0124, -0.0122, -0.0012,  0.0014]],
       dtype=torch.float64)
	q_value: tensor([[3.2842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28976956177171576, distance: 0.9643980724879316 entropy -11.100424766571537
epoch: 6, step: 77
	action: tensor([[-0.0048,  0.0166,  0.0245, -0.0122, -0.0068,  0.0091,  0.0307]],
       dtype=torch.float64)
	q_value: tensor([[3.2812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29221167513673807, distance: 0.9627386128715548 entropy -11.105565236220771
epoch: 6, step: 78
	action: tensor([[-0.0047,  0.0158,  0.0045, -0.0124, -0.0006,  0.0185,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[3.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2939536649375122, distance: 0.9615531496801087 entropy -11.100825216902953
epoch: 6, step: 79
	action: tensor([[-0.0049,  0.0164,  0.0057, -0.0121, -0.0037, -0.0022,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[3.2783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2895473787378404, distance: 0.9645489081342417 entropy -11.101150195345536
epoch: 6, step: 80
	action: tensor([[-0.0047,  0.0163,  0.0066, -0.0124, -0.0068,  0.0243,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[3.2858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2956939128983196, distance: 0.9603674107269949 entropy -11.096074901284785
epoch: 6, step: 81
	action: tensor([[-0.0050,  0.0165,  0.0043, -0.0119, -0.0042,  0.0141,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[3.2739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2933586541051265, distance: 0.961958232195735 entropy -11.109562565071442
epoch: 6, step: 82
	action: tensor([[-0.0049,  0.0164,  0.0110, -0.0121, -0.0159,  0.0065, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[3.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29153409135971153, distance: 0.9631993297221271 entropy -11.10348260811992
epoch: 6, step: 83
	action: tensor([[-0.0049,  0.0166,  0.0020, -0.0121, -0.0166,  0.0105,  0.0244]],
       dtype=torch.float64)
	q_value: tensor([[3.2780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925623224334969, distance: 0.9625001068796151 entropy -11.110137830346432
epoch: 6, step: 84
	action: tensor([[-4.8574e-03,  1.6395e-02, -3.6792e-05, -1.2286e-02, -5.6588e-03,
          3.2687e-03,  1.7890e-02]], dtype=torch.float64)
	q_value: tensor([[3.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29081796214680833, distance: 0.9636860157774595 entropy -11.09637960982104
epoch: 6, step: 85
	action: tensor([[-0.0048,  0.0164,  0.0097, -0.0124, -0.0038, -0.0010,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[3.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28980165595647656, distance: 0.9643762824336574 entropy -11.096978168059595
epoch: 6, step: 86
	action: tensor([[-0.0047,  0.0163,  0.0069, -0.0124,  0.0027, -0.0040,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[3.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2890918023725888, distance: 0.9648581161305855 entropy -11.098125536370237
epoch: 6, step: 87
	action: tensor([[-0.0047,  0.0163,  0.0068, -0.0125, -0.0074, -0.0181,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[3.2883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2859366899785799, distance: 0.9669968350234064 entropy -11.095985282366057
epoch: 6, step: 88
	action: tensor([[-0.0046,  0.0166,  0.0155, -0.0125, -0.0049,  0.0074, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[3.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29194356743417016, distance: 0.9629209365901876 entropy -11.101292710421736
epoch: 6, step: 89
	action: tensor([[-0.0049,  0.0165,  0.0247, -0.0121, -0.0105, -0.0076,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[3.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882286219844372, distance: 0.9654437008446324 entropy -11.109684396940215
epoch: 6, step: 90
	action: tensor([[-0.0047,  0.0160,  0.0034, -0.0125, -0.0112,  0.0128,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[3.2927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2927787966656553, distance: 0.9623528342506747 entropy -11.101043447919642
epoch: 6, step: 91
	action: tensor([[-0.0049,  0.0165,  0.0103, -0.0121, -0.0007,  0.0006, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[3.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2902399464672101, distance: 0.964078659797976 entropy -11.101989510431315
epoch: 6, step: 92
	action: tensor([[-0.0048,  0.0165,  0.0063, -0.0122, -0.0084,  0.0017,  0.0287]],
       dtype=torch.float64)
	q_value: tensor([[3.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2905852268343755, distance: 0.9638441312936288 entropy -11.106810747130583
epoch: 6, step: 93
	action: tensor([[-0.0047,  0.0163,  0.0066, -0.0125, -0.0082, -0.0164,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[3.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2862275568530934, distance: 0.9667998664970534 entropy -11.093976658897802
epoch: 6, step: 94
	action: tensor([[-0.0046,  0.0165,  0.0024, -0.0125, -0.0120, -0.0239,  0.0227]],
       dtype=torch.float64)
	q_value: tensor([[3.2876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2848630289362738, distance: 0.967723547095395 entropy -11.098980725069136
epoch: 6, step: 95
	action: tensor([[-0.0046,  0.0164,  0.0098, -0.0127, -0.0062,  0.0047,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[3.2904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2912365832476693, distance: 0.9634015480173035 entropy -11.092594341166405
epoch: 6, step: 96
	action: tensor([[-0.0048,  0.0165, -0.0010, -0.0122, -0.0079,  0.0060,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[3.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29155513726024396, distance: 0.9631850230719494 entropy -11.104075272160506
epoch: 6, step: 97
	action: tensor([[-4.8399e-03,  1.6559e-02,  2.2755e-03, -1.2242e-02,  1.4650e-03,
         -8.0805e-05,  1.6839e-02]], dtype=torch.float64)
	q_value: tensor([[3.2790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901899583852966, distance: 0.9641126090138159 entropy -11.098480009410943
epoch: 6, step: 98
	action: tensor([[-0.0047,  0.0164,  0.0258, -0.0124, -0.0065, -0.0080,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[3.2849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2880905610068646, distance: 0.9655373289690308 entropy -11.096207241316835
epoch: 6, step: 99
	action: tensor([[-0.0047,  0.0162,  0.0155, -0.0124, -0.0098, -0.0048,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[3.2906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888352466851708, distance: 0.9650322015531355 entropy -11.107322092238974
epoch: 6, step: 100
	action: tensor([[-0.0047,  0.0162, -0.0105, -0.0125, -0.0145,  0.0146, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[3.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293356389563179, distance: 0.9619597735667826 entropy -11.096963790053833
epoch: 6, step: 101
	action: tensor([[-0.0051,  0.0170,  0.0181, -0.0118, -0.0078, -0.0033, -0.0111]],
       dtype=torch.float64)
	q_value: tensor([[3.2666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28961950940707837, distance: 0.9644999426422609 entropy -11.108482722126572
epoch: 6, step: 102
	action: tensor([[-0.0048,  0.0165,  0.0103, -0.0122, -0.0081, -0.0032,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[3.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893408593170158, distance: 0.9646890889161324 entropy -11.111384361740225
epoch: 6, step: 103
	action: tensor([[-0.0047,  0.0162,  0.0122, -0.0125, -0.0129,  0.0061,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[3.2879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2913112311340629, distance: 0.9633508133283534 entropy -11.094802060452606
epoch: 6, step: 104
	action: tensor([[-0.0048,  0.0162,  0.0232, -0.0123, -0.0169, -0.0030,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[3.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891087957530998, distance: 0.9648465841921826 entropy -11.101381729845341
epoch: 6, step: 105
	action: tensor([[-0.0047,  0.0161,  0.0054, -0.0125, -0.0059, -0.0035,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[3.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2890983010831878, distance: 0.9648537060337427 entropy -11.10057116493729
epoch: 6, step: 106
	action: tensor([[-0.0048,  0.0165,  0.0173, -0.0123, -0.0048,  0.0104,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[3.2829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29254430596532943, distance: 0.9625123629006628 entropy -11.103283528397851
epoch: 6, step: 107
	action: tensor([[-0.0048,  0.0161,  0.0008, -0.0123, -0.0006,  0.0064,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[3.2862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29136221418584285, distance: 0.9633161609887702 entropy -11.100618667035251
epoch: 6, step: 108
	action: tensor([[-0.0048,  0.0164,  0.0002, -0.0123, -0.0093,  0.0037,  0.0300]],
       dtype=torch.float64)
	q_value: tensor([[3.2820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29098436384743254, distance: 0.963572950026318 entropy -11.096011876705878
epoch: 6, step: 109
	action: tensor([[-0.0048,  0.0163,  0.0123, -0.0124, -0.0049,  0.0183,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[3.2830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29419513678255527, distance: 0.9613887073075419 entropy -11.093178191727345
epoch: 6, step: 110
	action: tensor([[-0.0050,  0.0164,  0.0213, -0.0120, -0.0105, -0.0006,  0.0225]],
       dtype=torch.float64)
	q_value: tensor([[3.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2897720097338582, distance: 0.964396410483687 entropy -11.108927123965824
epoch: 6, step: 111
	action: tensor([[-0.0047,  0.0161,  0.0125, -0.0125, -0.0017, -0.0134, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[3.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28672790693714534, distance: 0.9664609467171971 entropy -11.10031911545419
epoch: 6, step: 112
	action: tensor([[-0.0047,  0.0166,  0.0186, -0.0123, -0.0094,  0.0135,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[3.2866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2933624829824112, distance: 0.9619556260469193 entropy -11.107924948641259
epoch: 6, step: 113
	action: tensor([[-0.0048,  0.0160,  0.0111, -0.0123, -0.0086, -0.0107,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[3.2854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2872401342635549, distance: 0.9661138585431183 entropy -11.10167484367474
epoch: 6, step: 114
	action: tensor([[-0.0046,  0.0159,  0.0010, -0.0128, -0.0027, -0.0037,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[3.2947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888687843910942, distance: 0.9650094463845096 entropy -11.084605868174474
epoch: 6, step: 115
	action: tensor([[-0.0048,  0.0165,  0.0087, -0.0123,  0.0052, -0.0147,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[3.2827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28675499344119415, distance: 0.9664425958682479 entropy -11.100714115604275
epoch: 6, step: 116
	action: tensor([[-0.0046,  0.0164,  0.0067, -0.0125, -0.0115,  0.0014, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[3.2910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29051808742546803, distance: 0.9638897395870285 entropy -11.099338777015037
epoch: 6, step: 117
	action: tensor([[-0.0049,  0.0166,  0.0013, -0.0121, -0.0144,  0.0159,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[3.2790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29389746563409536, distance: 0.9615914173824103 entropy -11.108658534154008
epoch: 6, step: 118
	action: tensor([[-0.0049,  0.0163,  0.0135, -0.0122, -0.0101,  0.0303,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[3.2776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29695029663951633, distance: 0.9595104476820692 entropy -11.097661198070663
epoch: 6, step: 119
	action: tensor([[-0.0050,  0.0162,  0.0058, -0.0120,  0.0006, -0.0004, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[3.2766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289731217809547, distance: 0.9644241051261615 entropy -11.10521039299852
epoch: 6, step: 120
	action: tensor([[-0.0048,  0.0166,  0.0085, -0.0122,  0.0008, -0.0002,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[3.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290185531468775, distance: 0.9641156154797241 entropy -11.10832407491207
epoch: 6, step: 121
	action: tensor([[-0.0047,  0.0162,  0.0117, -0.0125, -0.0160,  0.0033, -0.0241]],
       dtype=torch.float64)
	q_value: tensor([[3.2885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29066357679586574, distance: 0.9637909048606392 entropy -11.096188959269085
epoch: 6, step: 122
	action: tensor([[-0.0050,  0.0168,  0.0093, -0.0119, -0.0093, -0.0137,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[3.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871669959516735, distance: 0.966163425113526 entropy -11.11662833848302
epoch: 6, step: 123
	action: tensor([[-0.0047,  0.0165,  0.0029, -0.0124, -0.0018,  0.0146, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[3.2867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29358784760102963, distance: 0.9618022177963347 entropy -11.103106980183188
epoch: 6, step: 124
	action: tensor([[-0.0050,  0.0168,  0.0014, -0.0118, -0.0081, -0.0101,  0.0346]],
       dtype=torch.float64)
	q_value: tensor([[3.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288032564812303, distance: 0.9655766572510412 entropy -11.11125513987534
epoch: 6, step: 125
	action: tensor([[-0.0046,  0.0162,  0.0030, -0.0126, -0.0113,  0.0015,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[3.2890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29031136490675336, distance: 0.9640301541619664 entropy -11.08856195805607
epoch: 6, step: 126
	action: tensor([[-0.0047,  0.0161,  0.0165, -0.0126, -0.0155,  0.0041,  0.0001]],
       dtype=torch.float64)
	q_value: tensor([[3.2869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29068732519500107, distance: 0.9637747709910667 entropy -11.087043080577176
epoch: 6, step: 127
	action: tensor([[-0.0049,  0.0164,  0.0100, -0.0122, -0.0082,  0.0219,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[3.2812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29512400506588243, distance: 0.9607558854572299 entropy -11.109328153881167
LOSS epoch 6 actor 4.54903245597144 critic 15.203610060773057
epoch: 7, step: 0
	action: tensor([[-5.7965e-03,  1.7361e-02, -2.5546e-05, -1.1795e-02, -1.0005e-02,
          8.3694e-03,  6.3802e-02]], dtype=torch.float64)
	q_value: tensor([[5.0215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29191225152299494, distance: 0.9629422303824043 entropy -11.099803644187576
epoch: 7, step: 1
	action: tensor([[-0.0056,  0.0169, -0.0021, -0.0124, -0.0157,  0.0238, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[5.0412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29507416982848134, distance: 0.9607898479213595 entropy -11.07521848048643
epoch: 7, step: 2
	action: tensor([[-0.0060,  0.0179,  0.0138, -0.0115, -0.0096,  0.0008, -0.0118]],
       dtype=torch.float64)
	q_value: tensor([[4.9988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29052154837670463, distance: 0.9638873885901987 entropy -11.10542642151786
epoch: 7, step: 3
	action: tensor([[-0.0057,  0.0176, -0.0014, -0.0119,  0.0024, -0.0171,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[5.0226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2863681460524723, distance: 0.9667046482561747 entropy -11.105773636789241
epoch: 7, step: 4
	action: tensor([[-0.0055,  0.0176,  0.0048, -0.0123, -0.0121,  0.0153, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[5.0349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293787461387528, distance: 0.9616663179915249 entropy -11.090520866165251
epoch: 7, step: 5
	action: tensor([[-0.0059,  0.0179,  0.0095, -0.0115, -0.0064, -0.0063,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[5.0040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889086845300446, distance: 0.9649823736374099 entropy -11.111204681878773
epoch: 7, step: 6
	action: tensor([[-0.0056,  0.0174,  0.0102, -0.0122, -0.0039,  0.0108,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[5.0349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29261212706557793, distance: 0.9624662255859275 entropy -11.093656938046555
epoch: 7, step: 7
	action: tensor([[-0.0057,  0.0175,  0.0138, -0.0119, -0.0059,  0.0089,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[5.0257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29214353343724486, distance: 0.9627849551634952 entropy -11.098726882086437
epoch: 7, step: 8
	action: tensor([[-0.0057,  0.0174,  0.0124, -0.0120, -0.0080, -0.0075, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[5.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288277852634085, distance: 0.9654103121443088 entropy -11.09945247358561
epoch: 7, step: 9
	action: tensor([[-0.0057,  0.0178,  0.0116, -0.0119, -0.0082, -0.0079,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[5.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28846511643382866, distance: 0.9652832974817475 entropy -11.106848271707815
epoch: 7, step: 10
	action: tensor([[-0.0055,  0.0174,  0.0093, -0.0122, -0.0114, -0.0184,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[5.0380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2858505600521698, distance: 0.9670551525799191 entropy -11.09235803098265
epoch: 7, step: 11
	action: tensor([[-0.0055,  0.0177,  0.0102, -0.0122, -0.0039,  0.0118, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[5.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930350818080789, distance: 0.9621784481602043 entropy -11.098032649930868
epoch: 7, step: 12
	action: tensor([[-0.0058,  0.0177,  0.0178, -0.0117, -0.0093, -0.0123, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[5.0165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2874023906191323, distance: 0.966003886697326 entropy -11.106531498782376
epoch: 7, step: 13
	action: tensor([[-0.0057,  0.0177,  0.0145, -0.0120, -0.0046,  0.0183,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[5.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2944629534295695, distance: 0.961206291215759 entropy -11.107991525037136
epoch: 7, step: 14
	action: tensor([[-0.0057,  0.0171,  0.0059, -0.0120, -0.0090,  0.0210,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[5.0352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946306075756283, distance: 0.961092080491288 entropy -11.094666309809316
epoch: 7, step: 15
	action: tensor([[-0.0058,  0.0176, -0.0013, -0.0117, -0.0144, -0.0118, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[5.0146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28757668810613046, distance: 0.9658857398324996 entropy -11.102642921710101
epoch: 7, step: 16
	action: tensor([[-0.0057,  0.0180,  0.0199, -0.0119, -0.0059,  0.0007,  0.0409]],
       dtype=torch.float64)
	q_value: tensor([[5.0125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29055981933248265, distance: 0.9638613910500188 entropy -11.103834644966094
epoch: 7, step: 17
	action: tensor([[-0.0055,  0.0169,  0.0027, -0.0123, -0.0053,  0.0106,  0.0617]],
       dtype=torch.float64)
	q_value: tensor([[5.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921793790077051, distance: 0.9627605773309099 entropy -11.088697859841718
epoch: 7, step: 18
	action: tensor([[-0.0056,  0.0170,  0.0137, -0.0123, -0.0092, -0.0271,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[5.0418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2834035327684997, distance: 0.9687105390823638 entropy -11.077540971005817
epoch: 7, step: 19
	action: tensor([[-0.0054,  0.0176,  0.0043, -0.0123, -0.0106,  0.0127,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[5.0379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29325417844599755, distance: 0.9620293414695484 entropy -11.095955428675165
epoch: 7, step: 20
	action: tensor([[-0.0058,  0.0176,  0.0215, -0.0118, -0.0008,  0.0080, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[5.0187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29199135065635207, distance: 0.9628884446592919 entropy -11.099126621495625
epoch: 7, step: 21
	action: tensor([[-0.0057,  0.0174,  0.0062, -0.0118, -0.0090,  0.0201,  0.0263]],
       dtype=torch.float64)
	q_value: tensor([[5.0297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29468597634121707, distance: 0.9610543587478942 entropy -11.107760366314789
epoch: 7, step: 22
	action: tensor([[-0.0057,  0.0174,  0.0164, -0.0119, -0.0083,  0.0129,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[5.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29291316631421593, distance: 0.9622614080078898 entropy -11.093226410482119
epoch: 7, step: 23
	action: tensor([[-0.0057,  0.0173,  0.0115, -0.0120, -0.0105, -0.0075, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[5.0306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881750053045504, distance: 0.9654800628793168 entropy -11.09909265812924
epoch: 7, step: 24
	action: tensor([[-0.0057,  0.0177,  0.0031, -0.0120, -0.0057,  0.0051,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[5.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29150895478815075, distance: 0.963216416863392 entropy -11.103605177641212
epoch: 7, step: 25
	action: tensor([[-0.0057,  0.0176,  0.0039, -0.0120, -0.0108,  0.0042, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[5.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2911979314222416, distance: 0.9634278168115931 entropy -11.096018853829344
epoch: 7, step: 26
	action: tensor([[-0.0060,  0.0181,  0.0136, -0.0115, -0.0010,  0.0055, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[4.9998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917824057477879, distance: 0.9630305162263152 entropy -11.114435804775157
epoch: 7, step: 27
	action: tensor([[-0.0058,  0.0177,  0.0119, -0.0117, -0.0006, -0.0084,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[5.0201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28828760339527904, distance: 0.9654036989472313 entropy -11.108357974895137
epoch: 7, step: 28
	action: tensor([[-0.0055,  0.0174, -0.0054, -0.0122, -0.0073,  0.0083,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[5.0389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2920743320325978, distance: 0.9628320158644451 entropy -11.094802292785184
epoch: 7, step: 29
	action: tensor([[-0.0057,  0.0178, -0.0032, -0.0119, -0.0074,  0.0041,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[5.0166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29130769683736135, distance: 0.9633532154853414 entropy -11.093239591593294
epoch: 7, step: 30
	action: tensor([[-0.0057,  0.0178,  0.0038, -0.0119, -0.0068, -0.0020,  0.0334]],
       dtype=torch.float64)
	q_value: tensor([[5.0159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28991550552268863, distance: 0.9642989813368119 entropy -11.096597907196484
epoch: 7, step: 31
	action: tensor([[-0.0055,  0.0173,  0.0073, -0.0123, -0.0159, -0.0066, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[5.0371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2885115704712835, distance: 0.9652517867017386 entropy -11.084703508411403
epoch: 7, step: 32
	action: tensor([[-0.0057,  0.0177,  0.0162, -0.0120, -0.0110, -0.0146,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[5.0216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2868482320434267, distance: 0.9663794249374815 entropy -11.100294405387087
epoch: 7, step: 33
	action: tensor([[-0.0055,  0.0173,  0.0072, -0.0123, -0.0125,  0.0056,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[5.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2913238170776493, distance: 0.9633422589859144 entropy -11.091838247656288
epoch: 7, step: 34
	action: tensor([[-0.0057,  0.0175,  0.0070, -0.0120, -0.0065,  0.0042,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[5.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29109516823022885, distance: 0.9634976538887602 entropy -11.097260676936186
epoch: 7, step: 35
	action: tensor([[-0.0056,  0.0174,  0.0118, -0.0120, -0.0099,  0.0088,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[5.0289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2920759767078509, distance: 0.962830897422933 entropy -11.094881798112848
epoch: 7, step: 36
	action: tensor([[-0.0057,  0.0176,  0.0105, -0.0119, -0.0051,  0.0150, -0.0001]],
       dtype=torch.float64)
	q_value: tensor([[5.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2936173413530071, distance: 0.9617821392543209 entropy -11.102648842748598
epoch: 7, step: 37
	action: tensor([[-0.0058,  0.0176,  0.0039, -0.0118, -0.0114, -0.0039,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[5.0206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28932016971402197, distance: 0.96470313143515 entropy -11.102265045322696
epoch: 7, step: 38
	action: tensor([[-0.0056,  0.0175,  0.0118, -0.0122, -0.0067, -0.0070, -0.0342]],
       dtype=torch.float64)
	q_value: tensor([[5.0307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28848255953766655, distance: 0.9652714655671265 entropy -11.089079663371
epoch: 7, step: 39
	action: tensor([[-0.0058,  0.0179,  0.0214, -0.0117, -0.0081, -0.0036,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[5.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28944206693190466, distance: 0.9646203939873277 entropy -11.112748090248704
epoch: 7, step: 40
	action: tensor([[-0.0056,  0.0172,  0.0113, -0.0122, -0.0057,  0.0066, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[5.0409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914582551959759, distance: 0.9632508801118836 entropy -11.097340531824878
epoch: 7, step: 41
	action: tensor([[-0.0058,  0.0177,  0.0085, -0.0118,  0.0034,  0.0037, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[5.0206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29111180337430953, distance: 0.9634863491158172 entropy -11.10533352012146
epoch: 7, step: 42
	action: tensor([[-0.0058,  0.0178,  0.0075, -0.0117, -0.0144,  0.0086, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[5.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2923394184964855, distance: 0.9626517302470214 entropy -11.107397260890508
epoch: 7, step: 43
	action: tensor([[-0.0058,  0.0177,  0.0052, -0.0118, -0.0007, -0.0065,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[5.0158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888075558405898, distance: 0.965050989250686 entropy -11.10230174500237
epoch: 7, step: 44
	action: tensor([[-0.0056,  0.0176,  0.0148, -0.0121, -0.0122, -0.0067, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[5.0301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28867470824016284, distance: 0.9651411186684025 entropy -11.095517345582492
epoch: 7, step: 45
	action: tensor([[-0.0057,  0.0177,  0.0152, -0.0119, -0.0021, -0.0297,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[5.0210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2833680162000738, distance: 0.9687345448160118 entropy -11.108181275512864
epoch: 7, step: 46
	action: tensor([[-0.0053,  0.0173,  0.0106, -0.0125,  0.0008,  0.0236,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[5.0514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29561564713094046, distance: 0.9604207694908327 entropy -11.089921405116367
epoch: 7, step: 47
	action: tensor([[-0.0057,  0.0170,  0.0102, -0.0120, -0.0012,  0.0090, -0.0272]],
       dtype=torch.float64)
	q_value: tensor([[5.0372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2918153127590716, distance: 0.9630081425769943 entropy -11.091198651651682
epoch: 7, step: 48
	action: tensor([[-0.0059,  0.0178,  0.0018, -0.0116, -0.0108,  0.0166,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[5.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2941704726325536, distance: 0.9614055048881307 entropy -11.111822607297375
epoch: 7, step: 49
	action: tensor([[-0.0058,  0.0176,  0.0092, -0.0118, -0.0055,  0.0230,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[5.0171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954448684387496, distance: 0.9605371899236804 entropy -11.09749377103233
epoch: 7, step: 50
	action: tensor([[-0.0058,  0.0172,  0.0161, -0.0119, -0.0122, -0.0010,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[5.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28957625114539165, distance: 0.9645293085639877 entropy -11.095838449907657
epoch: 7, step: 51
	action: tensor([[-0.0056,  0.0175,  0.0011, -0.0120, -0.0043,  0.0021,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[5.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29067332166831805, distance: 0.9637842845522858 entropy -11.099851198012848
epoch: 7, step: 52
	action: tensor([[-0.0057,  0.0177,  0.0045, -0.0119, -0.0025, -0.0082,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[5.0214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28844693954996015, distance: 0.9652956269758795 entropy -11.096431138386198
epoch: 7, step: 53
	action: tensor([[-0.0055,  0.0173,  0.0062, -0.0123, -0.0014,  0.0049,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[5.0417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29115660941699695, distance: 0.963455899537967 entropy -11.083772702411222
epoch: 7, step: 54
	action: tensor([[-0.0056,  0.0174,  0.0149, -0.0121, -0.0083,  0.0124,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[5.0311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29287984430849934, distance: 0.9622840813917333 entropy -11.094031554244973
epoch: 7, step: 55
	action: tensor([[-0.0057,  0.0174,  0.0108, -0.0119, -0.0203, -0.0173,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[5.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2860831438745294, distance: 0.9668976647438376 entropy -11.101128359056977
epoch: 7, step: 56
	action: tensor([[-0.0055,  0.0176,  0.0159, -0.0122, -0.0042,  0.0033,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[5.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2909449284433563, distance: 0.9635997465864674 entropy -11.094767592080867
epoch: 7, step: 57
	action: tensor([[-0.0056,  0.0173, -0.0051, -0.0121, -0.0005,  0.0061, -0.0213]],
       dtype=torch.float64)
	q_value: tensor([[5.0367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29150290266939893, distance: 0.9632205308798844 entropy -11.095699359729164
epoch: 7, step: 58
	action: tensor([[-0.0059,  0.0180,  0.0077, -0.0116, -0.0079, -0.0050,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[5.0071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893321789593182, distance: 0.9646949805034694 entropy -11.105318362834717
epoch: 7, step: 59
	action: tensor([[-0.0056,  0.0176,  0.0125, -0.0121,  0.0005, -0.0005,  0.0341]],
       dtype=torch.float64)
	q_value: tensor([[5.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29009645758175784, distance: 0.964176106520137 entropy -11.09671391915815
epoch: 7, step: 60
	action: tensor([[-0.0055,  0.0171,  0.0110, -0.0123, -0.0011, -0.0115,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[5.0450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.287197318308969, distance: 0.9661428756552567 entropy -11.087700181510197
epoch: 7, step: 61
	action: tensor([[-0.0055,  0.0174,  0.0100, -0.0122, -0.0065, -0.0009, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[5.0387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2899299719864987, distance: 0.9642891585146863 entropy -11.094378294592753
epoch: 7, step: 62
	action: tensor([[-0.0057,  0.0178,  0.0043, -0.0118, -0.0046,  0.0054,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[5.0189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29160142732443084, distance: 0.963153555115002 entropy -11.107033213327792
epoch: 7, step: 63
	action: tensor([[-0.0057,  0.0175,  0.0075, -0.0120, -0.0044, -0.0093,  0.0614]],
       dtype=torch.float64)
	q_value: tensor([[5.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2879462770599124, distance: 0.9656351675923229 entropy -11.094541668244505
epoch: 7, step: 64
	action: tensor([[-0.0054,  0.0169,  0.0088, -0.0126, -0.0062, -0.0027,  0.0360]],
       dtype=torch.float64)
	q_value: tensor([[5.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889985044548711, distance: 0.9649214269069583 entropy -11.074224445063168
epoch: 7, step: 65
	action: tensor([[-0.0055,  0.0172,  0.0099, -0.0123, -0.0059,  0.0152,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[5.0395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934806203260001, distance: 0.9618752116691488 entropy -11.086608152956575
epoch: 7, step: 66
	action: tensor([[-0.0058,  0.0176,  0.0126, -0.0117, -0.0074, -0.0108, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[5.0191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877098666180363, distance: 0.9657954555621805 entropy -11.102657922565623
epoch: 7, step: 67
	action: tensor([[-0.0057,  0.0178,  0.0158, -0.0119, -0.0033,  0.0275,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[5.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967337332220996, distance: 0.9596582173639866 entropy -11.107613970818305
epoch: 7, step: 68
	action: tensor([[-0.0058,  0.0173,  0.0129, -0.0117, -0.0124,  0.0071,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[5.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.291497252376137, distance: 0.963224371733746 entropy -11.104190876184386
epoch: 7, step: 69
	action: tensor([[-0.0057,  0.0174,  0.0111, -0.0120, -0.0090, -0.0142,  0.0228]],
       dtype=torch.float64)
	q_value: tensor([[5.0289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2867268190240845, distance: 0.9664616837606732 entropy -11.095407148818285
epoch: 7, step: 70
	action: tensor([[-0.0055,  0.0174,  0.0037, -0.0123, -0.0027,  0.0009, -0.0217]],
       dtype=torch.float64)
	q_value: tensor([[5.0391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29039412637991946, distance: 0.9639739415653598 entropy -11.090352660572083
epoch: 7, step: 71
	action: tensor([[-0.0058,  0.0179,  0.0160, -0.0117, -0.0153,  0.0207,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[5.0136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29515253626501947, distance: 0.9607364410478046 entropy -11.107340175286364
epoch: 7, step: 72
	action: tensor([[-0.0058,  0.0172,  0.0026, -0.0119,  0.0003,  0.0126,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[5.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2928175345258601, distance: 0.9623264775747913 entropy -11.097372768767539
epoch: 7, step: 73
	action: tensor([[-0.0058,  0.0177,  0.0032, -0.0118, -0.0044, -0.0008,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[5.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901020617456901, distance: 0.9641723007836519 entropy -11.098490354731167
epoch: 7, step: 74
	action: tensor([[-0.0056,  0.0176,  0.0042, -0.0120, -0.0094,  0.0055,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[5.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2915410863732706, distance: 0.9631945746528144 entropy -11.094550157858054
epoch: 7, step: 75
	action: tensor([[-0.0057,  0.0177,  0.0081, -0.0119, -0.0116,  0.0091,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[5.0213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2923422349744378, distance: 0.9626498145757643 entropy -11.096683682012799
epoch: 7, step: 76
	action: tensor([[-0.0057,  0.0175,  0.0198, -0.0119, -0.0061, -0.0081,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[5.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28812889706517686, distance: 0.9655113317061476 entropy -11.097933936860462
epoch: 7, step: 77
	action: tensor([[-0.0054,  0.0171,  0.0282, -0.0124, -0.0031, -0.0095,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[5.0498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28744600155845856, distance: 0.965974326549411 entropy -11.088339357091387
epoch: 7, step: 78
	action: tensor([[-0.0054,  0.0170,  0.0092, -0.0123, -0.0138,  0.0243,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[5.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29544163208561347, distance: 0.960539396020718 entropy -11.09474789635745
epoch: 7, step: 79
	action: tensor([[-0.0058,  0.0174,  0.0063, -0.0118, -0.0094, -0.0075,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[5.0209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28825210954983427, distance: 0.9654277714958039 entropy -11.09669512125804
epoch: 7, step: 80
	action: tensor([[-0.0056,  0.0177,  0.0027, -0.0121, -0.0116,  0.0123,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[5.0253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2931050084044441, distance: 0.9621308619744858 entropy -11.098458828162789
epoch: 7, step: 81
	action: tensor([[-0.0058,  0.0177,  0.0057, -0.0118, -0.0029, -0.0055, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[5.0144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28901200001018346, distance: 0.9649122692523228 entropy -11.10127720926587
epoch: 7, step: 82
	action: tensor([[-0.0056,  0.0177,  0.0007, -0.0120, -0.0076,  0.0048,  0.0482]],
       dtype=torch.float64)
	q_value: tensor([[5.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914172014636768, distance: 0.9632787856455054 entropy -11.099596718112306
epoch: 7, step: 83
	action: tensor([[-0.0056,  0.0172,  0.0103, -0.0123, -0.0142,  0.0189,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[5.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2941854963049402, distance: 0.9613952730136932 entropy -11.079693070614976
epoch: 7, step: 84
	action: tensor([[-0.0058,  0.0174,  0.0145, -0.0118, -0.0094, -0.0088, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[5.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2879337704294379, distance: 0.9656436478437392 entropy -11.100285927025164
epoch: 7, step: 85
	action: tensor([[-0.0056,  0.0176, -0.0017, -0.0120, -0.0041,  0.0220,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[5.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2953605180130192, distance: 0.9605946866988825 entropy -11.10397282846364
epoch: 7, step: 86
	action: tensor([[-0.0058,  0.0177,  0.0037, -0.0117, -0.0114,  0.0096, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[5.0128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29246379404801237, distance: 0.9625671306492063 entropy -11.097944495247644
epoch: 7, step: 87
	action: tensor([[-0.0058,  0.0178,  0.0065, -0.0117, -0.0046, -0.0111, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[5.0101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28780832965617653, distance: 0.965728700151075 entropy -11.106347243318288
epoch: 7, step: 88
	action: tensor([[-0.0056,  0.0177,  0.0040, -0.0121, -0.0094, -0.0048,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[5.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892235612886891, distance: 0.964768699134824 entropy -11.099134350443945
epoch: 7, step: 89
	action: tensor([[-0.0056,  0.0176,  0.0057, -0.0121, -0.0156, -0.0358,  0.0026]],
       dtype=torch.float64)
	q_value: tensor([[5.0293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2821525637117732, distance: 0.9695557138435326 entropy -11.091634057168553
epoch: 7, step: 90
	action: tensor([[-0.0054,  0.0178,  0.0097, -0.0123, -0.0006,  0.0112,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[5.0337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930161745582426, distance: 0.9621913144469224 entropy -11.094402133405907
epoch: 7, step: 91
	action: tensor([[-0.0056,  0.0173,  0.0078, -0.0120, -0.0098, -0.0028,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[5.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893444426101789, distance: 0.9646866568302164 entropy -11.092223968110286
epoch: 7, step: 92
	action: tensor([[-0.0056,  0.0176,  0.0108, -0.0121, -0.0058,  0.0273,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[5.0269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965544938442669, distance: 0.9597805021880339 entropy -11.095998277299683
epoch: 7, step: 93
	action: tensor([[-0.0059,  0.0175,  0.0103, -0.0116,  0.0003,  0.0116, -0.0370]],
       dtype=torch.float64)
	q_value: tensor([[5.0175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2927731750203013, distance: 0.9623566590763185 entropy -11.1032646728774
epoch: 7, step: 94
	action: tensor([[-0.0060,  0.0179,  0.0125, -0.0115, -0.0008,  0.0241,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[5.0085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29593728125504437, distance: 0.9602014720557717 entropy -11.115127085715065
epoch: 7, step: 95
	action: tensor([[-0.0059,  0.0175,  0.0194, -0.0117, -0.0146, -0.0061, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[5.0209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2885909214859703, distance: 0.9651979588099329 entropy -11.10361404345052
epoch: 7, step: 96
	action: tensor([[-0.0058,  0.0177, -0.0006, -0.0118, -0.0136,  0.0200, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[5.0191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29490575912532535, distance: 0.9609046100877726 entropy -11.110853177537951
epoch: 7, step: 97
	action: tensor([[-0.0059,  0.0178,  0.0089, -0.0116, -0.0114,  0.0024, -0.0248]],
       dtype=torch.float64)
	q_value: tensor([[5.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2908308911078554, distance: 0.9636772313504142 entropy -11.102846263177607
epoch: 7, step: 98
	action: tensor([[-0.0058,  0.0178, -0.0018, -0.0117, -0.0092,  0.0067,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[5.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29191106783425314, distance: 0.9629430352426799 entropy -11.110000995193861
epoch: 7, step: 99
	action: tensor([[-0.0057,  0.0178,  0.0067, -0.0118, -0.0078,  0.0238, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[5.0159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29583007125004823, distance: 0.9602745758313992 entropy -11.096540274593282
epoch: 7, step: 100
	action: tensor([[-0.0059,  0.0178, -0.0004, -0.0115, -0.0036,  0.0197, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[5.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2948758206424379, distance: 0.9609250099997205 entropy -11.108040646619518
epoch: 7, step: 101
	action: tensor([[-0.0059,  0.0177, -0.0029, -0.0116, -0.0114,  0.0164,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[5.0111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29407390031673886, distance: 0.9614712728794581 entropy -11.102431658102898
epoch: 7, step: 102
	action: tensor([[-0.0058,  0.0176,  0.0145, -0.0119, -0.0011,  0.0029,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[5.0169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290752798113059, distance: 0.9637302894774481 entropy -11.093626123348107
epoch: 7, step: 103
	action: tensor([[-0.0056,  0.0174,  0.0192, -0.0120, -0.0130, -0.0094, -0.0238]],
       dtype=torch.float64)
	q_value: tensor([[5.0327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2878416008144956, distance: 0.9657061421171326 entropy -11.10000677164881
epoch: 7, step: 104
	action: tensor([[-0.0057,  0.0178,  0.0079, -0.0118, -0.0055, -0.0064,  0.0613]],
       dtype=torch.float64)
	q_value: tensor([[5.0209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888314578522714, distance: 0.9650347722239029 entropy -11.110377801880277
epoch: 7, step: 105
	action: tensor([[-0.0055,  0.0169,  0.0076, -0.0125, -0.0078, -0.0037,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[5.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288762047537909, distance: 0.9650818649513089 entropy -11.075044552201108
epoch: 7, step: 106
	action: tensor([[-0.0056,  0.0175,  0.0081, -0.0121,  0.0010,  0.0011,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[5.0298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904310787190262, distance: 0.963948842029792 entropy -11.093893830518494
epoch: 7, step: 107
	action: tensor([[-0.0056,  0.0175,  0.0037, -0.0120,  0.0008,  0.0077, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[5.0303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29198478181943777, distance: 0.962892911442639 entropy -11.098081348451046
epoch: 7, step: 108
	action: tensor([[-0.0058,  0.0178,  0.0185, -0.0117, -0.0105, -0.0062,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[5.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888694247139262, distance: 0.9650090119233954 entropy -11.103703958547095
epoch: 7, step: 109
	action: tensor([[-0.0056,  0.0174,  0.0162, -0.0121, -0.0071, -0.0030, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[5.0365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289346372281829, distance: 0.9646853471030656 entropy -11.097003722045779
epoch: 7, step: 110
	action: tensor([[-0.0057,  0.0176,  0.0142, -0.0119, -0.0107, -0.0024,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[5.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28961559731698394, distance: 0.9645025984059867 entropy -11.106009895795326
epoch: 7, step: 111
	action: tensor([[-0.0056,  0.0173,  0.0119, -0.0122, -0.0067, -0.0155, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[5.0358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28645083597930754, distance: 0.9666486396489817 entropy -11.093253844047302
epoch: 7, step: 112
	action: tensor([[-0.0057,  0.0179,  0.0070, -0.0119, -0.0079,  0.0141,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[5.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937268376537624, distance: 0.9617075934918419 entropy -11.108440740845534
epoch: 7, step: 113
	action: tensor([[-0.0058,  0.0176,  0.0085, -0.0118, -0.0142,  0.0161,  0.0473]],
       dtype=torch.float64)
	q_value: tensor([[5.0210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29383039953261003, distance: 0.9616370826032452 entropy -11.0984022652538
epoch: 7, step: 114
	action: tensor([[-0.0057,  0.0171,  0.0171, -0.0122, -0.0113, -0.0087,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[5.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28763768426646297, distance: 0.9658443904117472 entropy -11.084892346477066
epoch: 7, step: 115
	action: tensor([[-0.0055,  0.0172,  0.0162, -0.0123, -0.0029, -0.0210, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[5.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849811939158591, distance: 0.967643593350955 entropy -11.090930730538172
epoch: 7, step: 116
	action: tensor([[-0.0055,  0.0177,  0.0096, -0.0121, -0.0006, -0.0117,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[5.0336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28758843696129066, distance: 0.9658777753974117 entropy -11.104227521089667
epoch: 7, step: 117
	action: tensor([[-0.0055,  0.0175,  0.0064, -0.0122, -0.0009, -0.0038, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[5.0349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28937751685713364, distance: 0.9646642080815699 entropy -11.097368553401497
epoch: 7, step: 118
	action: tensor([[-0.0057,  0.0178,  0.0080, -0.0118, -0.0006,  0.0004, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[5.0190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904828059109984, distance: 0.9639137057138714 entropy -11.106387956032696
epoch: 7, step: 119
	action: tensor([[-0.0057,  0.0176,  0.0119, -0.0119, -0.0026, -0.0140, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[5.0267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28695697272321585, distance: 0.9663057458368789 entropy -11.100859252310814
epoch: 7, step: 120
	action: tensor([[-0.0055,  0.0176,  0.0161, -0.0121, -0.0107,  0.0133,  0.0673]],
       dtype=torch.float64)
	q_value: tensor([[5.0331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293290169040754, distance: 0.9620048457824645 entropy -11.100310845443019
epoch: 7, step: 121
	action: tensor([[-0.0056,  0.0166,  0.0217, -0.0124, -0.0095,  0.0104,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[5.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2916850076897479, distance: 0.9630967346302447 entropy -11.080327851903691
epoch: 7, step: 122
	action: tensor([[-0.0057,  0.0174,  0.0090, -0.0118, -0.0012,  0.0054,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[5.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29130829649592127, distance: 0.9633528079155671 entropy -11.105278817818672
epoch: 7, step: 123
	action: tensor([[-0.0057,  0.0176,  0.0238, -0.0119, -0.0011, -0.0246, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[5.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843043987127115, distance: 0.9681014425979425 entropy -11.098189898889867
epoch: 7, step: 124
	action: tensor([[-0.0054,  0.0175,  0.0009, -0.0122, -0.0122,  0.0253, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[5.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2960519878045551, distance: 0.9601232504135713 entropy -11.101708607919887
epoch: 7, step: 125
	action: tensor([[-0.0059,  0.0178,  0.0133, -0.0115, -0.0013, -0.0224, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[5.0044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2851045261454904, distance: 0.9675601362533668 entropy -11.104566321840244
epoch: 7, step: 126
	action: tensor([[-0.0056,  0.0178,  0.0102, -0.0121, -0.0162, -0.0078,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[5.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2886264211495062, distance: 0.965173876582647 entropy -11.10493180419047
epoch: 7, step: 127
	action: tensor([[-0.0056,  0.0175,  0.0081, -0.0122,  0.0012, -0.0003,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[5.0324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29004358283058407, distance: 0.9642120125407655 entropy -11.092129750448526
LOSS epoch 7 actor 11.277146505579053 critic 13.27609473385473
epoch: 8, step: 0
	action: tensor([[-0.0053,  0.0186,  0.0005, -0.0116, -0.0145,  0.0078,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[7.2176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29329000858345344, distance: 0.9620049549932644 entropy -11.095065211305966
epoch: 8, step: 1
	action: tensor([[-0.0054,  0.0185,  0.0079, -0.0116, -0.0044, -0.0042,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.2120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2903998194675401, distance: 0.9639700746306819 entropy -11.087547887103367
epoch: 8, step: 2
	action: tensor([[-5.2882e-03,  1.8712e-02,  2.4949e-02, -1.1626e-02, -4.5866e-03,
          2.0674e-05,  2.3473e-02]], dtype=torch.float64)
	q_value: tensor([[7.2123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914430141159926, distance: 0.9632612400552938 entropy -11.094833396469843
epoch: 8, step: 3
	action: tensor([[-0.0052,  0.0182,  0.0017, -0.0118, -0.0129, -0.0203, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.2452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2866383849093289, distance: 0.9665215945611427 entropy -11.093908456251214
epoch: 8, step: 4
	action: tensor([[-0.0052,  0.0190,  0.0027, -0.0117, -0.0029,  0.0124,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[7.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29465940017182934, distance: 0.9610724647985184 entropy -11.094787558004628
epoch: 8, step: 5
	action: tensor([[-0.0054,  0.0186,  0.0127, -0.0115, -0.0025,  0.0068, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.2124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2929850904966984, distance: 0.9622124666177467 entropy -11.091120908322699
epoch: 8, step: 6
	action: tensor([[-0.0054,  0.0186,  0.0195, -0.0114, -0.0095, -0.0033, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[7.2100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29065021086428433, distance: 0.9637999851095796 entropy -11.101119907484106
epoch: 8, step: 7
	action: tensor([[-0.0055,  0.0188,  0.0006, -0.0114, -0.0103,  0.0087,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[7.1991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29357215314015406, distance: 0.9618129019864248 entropy -11.108838098308803
epoch: 8, step: 8
	action: tensor([[-0.0055,  0.0188,  0.0097, -0.0114, -0.0089,  0.0058,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[7.1937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2929092032427423, distance: 0.9622641046394852 entropy -11.097283247201954
epoch: 8, step: 9
	action: tensor([[-0.0053,  0.0182,  0.0097, -0.0118, -0.0014, -0.0214,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[7.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286235795805714, distance: 0.966794286678614 entropy -11.086565568934713
epoch: 8, step: 10
	action: tensor([[-0.0051,  0.0186,  0.0101, -0.0119, -0.0063, -0.0025, -0.0118]],
       dtype=torch.float64)
	q_value: tensor([[7.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2909954078141215, distance: 0.9635654454605087 entropy -11.091252681236117
epoch: 8, step: 11
	action: tensor([[-0.0054,  0.0189,  0.0045, -0.0114, -0.0031,  0.0091,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.1988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937902719451856, distance: 0.9616644043882006 entropy -11.101790252289652
epoch: 8, step: 12
	action: tensor([[-0.0054,  0.0187,  0.0097, -0.0115, -0.0135, -0.0047, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[7.2069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29044166717042563, distance: 0.9639416498013582 entropy -11.094686128216082
epoch: 8, step: 13
	action: tensor([[-0.0054,  0.0189,  0.0059, -0.0115, -0.0012,  0.0112,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942415239382862, distance: 0.9613571144248827 entropy -11.102433457606375
epoch: 8, step: 14
	action: tensor([[-0.0054,  0.0185,  0.0100, -0.0116, -0.0041, -0.0034,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[7.2211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29057110041004175, distance: 0.9638537276566624 entropy -11.089986527530694
epoch: 8, step: 15
	action: tensor([[-0.0053,  0.0186,  0.0169, -0.0116, -0.0068,  0.0014, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[7.2164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29175762516707815, distance: 0.963047364329833 entropy -11.095834584903697
epoch: 8, step: 16
	action: tensor([[-0.0054,  0.0187,  0.0109, -0.0114, -0.0097,  0.0075,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[7.2067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2932671911372612, distance: 0.962020484927317 entropy -11.103198650144634
epoch: 8, step: 17
	action: tensor([[-0.0054,  0.0186,  0.0154, -0.0115, -0.0003, -0.0014, -0.0111]],
       dtype=torch.float64)
	q_value: tensor([[7.2068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29110497582456596, distance: 0.9634909889413693 entropy -11.099340987098563
epoch: 8, step: 18
	action: tensor([[-0.0054,  0.0187,  0.0137, -0.0115, -0.0131,  0.0127,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29447077082835527, distance: 0.9612009660852467 entropy -11.102493116601675
epoch: 8, step: 19
	action: tensor([[-0.0055,  0.0186,  0.0054, -0.0114, -0.0051, -0.0072,  0.0375]],
       dtype=torch.float64)
	q_value: tensor([[7.2060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28977264112158896, distance: 0.9643959818127497 entropy -11.10104556513871
epoch: 8, step: 20
	action: tensor([[-0.0052,  0.0183,  0.0165, -0.0119, -0.0007, -0.0115, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2885204946689697, distance: 0.965245733107319 entropy -11.082168129839422
epoch: 8, step: 21
	action: tensor([[-0.0052,  0.0187,  0.0122, -0.0117, -0.0101,  0.0047,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.2193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29263916929797407, distance: 0.9624478286886308 entropy -11.099348209290739
epoch: 8, step: 22
	action: tensor([[-0.0054,  0.0185,  0.0114, -0.0116, -0.0055, -0.0271,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.2163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28513508391979236, distance: 0.9675394571462067 entropy -11.096174488624698
epoch: 8, step: 23
	action: tensor([[-0.0051,  0.0185,  0.0014, -0.0120, -0.0070,  0.0008,  0.0350]],
       dtype=torch.float64)
	q_value: tensor([[7.2328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917313538877341, distance: 0.9630652256246464 entropy -11.089875977543532
epoch: 8, step: 24
	action: tensor([[-0.0052,  0.0184,  0.0089, -0.0118, -0.0090,  0.0078,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[7.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930768738068581, distance: 0.9621500083085341 entropy -11.083684321463755
epoch: 8, step: 25
	action: tensor([[-0.0054,  0.0187,  0.0063, -0.0115,  0.0008, -0.0009, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.2082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2913126560333089, distance: 0.9633498448647642 entropy -11.095775309569015
epoch: 8, step: 26
	action: tensor([[-0.0053,  0.0187,  0.0100, -0.0115, -0.0031, -0.0032, -0.0325]],
       dtype=torch.float64)
	q_value: tensor([[7.2101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29084336999520966, distance: 0.9636687526445824 entropy -11.09736500839664
epoch: 8, step: 27
	action: tensor([[-0.0055,  0.0189,  0.0164, -0.0113,  0.0013, -0.0048,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[7.1862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2905015069961574, distance: 0.9639010024621182 entropy -11.108817878722176
epoch: 8, step: 28
	action: tensor([[-0.0052,  0.0183,  0.0127, -0.0118, -0.0062,  0.0072,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29294320163152054, distance: 0.9622409705366566 entropy -11.091053825952729
epoch: 8, step: 29
	action: tensor([[-0.0054,  0.0186,  0.0125, -0.0115, -0.0066, -0.0002, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[7.2105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29142769401425417, distance: 0.9632716536005164 entropy -11.09842876026652
epoch: 8, step: 30
	action: tensor([[-0.0054,  0.0187, -0.0026, -0.0115, -0.0090, -0.0304,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[7.2082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28480921523899116, distance: 0.9677599567654811 entropy -11.099276115096767
epoch: 8, step: 31
	action: tensor([[-0.0051,  0.0186,  0.0121, -0.0120,  0.0082, -0.0190,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[7.2262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2870628242679648, distance: 0.9662340189225646 entropy -11.080581555081823
epoch: 8, step: 32
	action: tensor([[-0.0051,  0.0186,  0.0065, -0.0118, -0.0131, -0.0057, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[7.2318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29026384670414695, distance: 0.9640624276204546 entropy -11.093713280509963
epoch: 8, step: 33
	action: tensor([[-0.0053,  0.0189,  0.0176, -0.0116, -0.0072, -0.0069,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[7.1994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2899933434113543, distance: 0.9642461277296732 entropy -11.097539448949602
epoch: 8, step: 34
	action: tensor([[-0.0052,  0.0185, -0.0018, -0.0117, -0.0048,  0.0096, -0.0002]],
       dtype=torch.float64)
	q_value: tensor([[7.2257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937072259987371, distance: 0.9617209456533257 entropy -11.095810726351461
epoch: 8, step: 35
	action: tensor([[-0.0055,  0.0189,  0.0065, -0.0114, -0.0145, -0.0110,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[7.1909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891901300808877, distance: 0.9647913877059806 entropy -11.096442210136235
epoch: 8, step: 36
	action: tensor([[-0.0053,  0.0187,  0.0023, -0.0117,  0.0085,  0.0069,  0.0131]],
       dtype=torch.float64)
	q_value: tensor([[7.2102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29321113629188944, distance: 0.9620586357246222 entropy -11.092394145694247
epoch: 8, step: 37
	action: tensor([[-0.0053,  0.0186,  0.0123, -0.0116, -0.0090, -0.0014,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[7.2200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2910785434007649, distance: 0.963508951519667 entropy -11.09192772058669
epoch: 8, step: 38
	action: tensor([[-0.0052,  0.0183,  0.0051, -0.0118, -0.0134,  0.0162,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[7.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949957974324773, distance: 0.9608432558874307 entropy -11.087337348707194
epoch: 8, step: 39
	action: tensor([[-0.0055,  0.0188,  0.0049, -0.0113, -0.0115, -0.0068,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2900287847241707, distance: 0.9642220613531083 entropy -11.097497276462013
epoch: 8, step: 40
	action: tensor([[-0.0053,  0.0188,  0.0220, -0.0116, -0.0204, -0.0036,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[7.2056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2906947089027312, distance: 0.9637697546914861 entropy -11.093895538528594
epoch: 8, step: 41
	action: tensor([[-0.0053,  0.0186, -0.0046, -0.0116, -0.0102,  0.0072,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[7.2175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293128280572312, distance: 0.9621150243638887 entropy -11.098623259569967
epoch: 8, step: 42
	action: tensor([[-0.0054,  0.0189,  0.0028, -0.0115, -0.0067, -0.0161,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[7.1942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2879848605343115, distance: 0.9656090051952367 entropy -11.091729584381671
epoch: 8, step: 43
	action: tensor([[-0.0052,  0.0186,  0.0124, -0.0119, -0.0148, -0.0186,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[7.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2872555192866949, distance: 0.9661034316342857 entropy -11.086502439604127
epoch: 8, step: 44
	action: tensor([[-0.0051,  0.0183,  0.0121, -0.0120, -0.0017, -0.0040,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[7.2414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2903655733432555, distance: 0.9639933355046186 entropy -11.083552125690542
epoch: 8, step: 45
	action: tensor([[-0.0052,  0.0185,  0.0077, -0.0118, -0.0021,  0.0073, -0.0198]],
       dtype=torch.float64)
	q_value: tensor([[7.2296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29309734681759225, distance: 0.9621360759237118 entropy -11.091091036700373
epoch: 8, step: 46
	action: tensor([[-0.0055,  0.0189,  0.0128, -0.0112, -0.0022,  0.0123,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[7.1875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2945234065550968, distance: 0.9611651104091351 entropy -11.10478316121809
epoch: 8, step: 47
	action: tensor([[-0.0054,  0.0185,  0.0287, -0.0114, -0.0021,  0.0101, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[7.2139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2936474532136827, distance: 0.9617616394883164 entropy -11.099729853596404
epoch: 8, step: 48
	action: tensor([[-0.0054,  0.0183,  0.0088, -0.0115, -0.0026, -0.0066,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[7.2270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2897234678953434, distance: 0.9644293666439628 entropy -11.104542238886248
epoch: 8, step: 49
	action: tensor([[-0.0052,  0.0184,  0.0168, -0.0118, -0.0130,  0.0338,  0.0335]],
       dtype=torch.float64)
	q_value: tensor([[7.2307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991562108877853, distance: 0.9580039676664637 entropy -11.088863056371025
epoch: 8, step: 50
	action: tensor([[-0.0055,  0.0181,  0.0077, -0.0114, -0.0053,  0.0141,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[7.2236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29426831455999325, distance: 0.9613388676732851 entropy -11.094803851857579
epoch: 8, step: 51
	action: tensor([[-0.0055,  0.0186,  0.0116, -0.0114, -0.0165, -0.0091, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[7.2037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893739950690164, distance: 0.964666598477889 entropy -11.098893385514668
epoch: 8, step: 52
	action: tensor([[-0.0054,  0.0189, -0.0009, -0.0115, -0.0111, -0.0185,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[7.1927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28759072988946577, distance: 0.965876221035957 entropy -11.10291005094543
epoch: 8, step: 53
	action: tensor([[-0.0052,  0.0187,  0.0110, -0.0119, -0.0103,  0.0124,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.2161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29441188078683533, distance: 0.9612410806413219 entropy -11.085393879015756
epoch: 8, step: 54
	action: tensor([[-0.0054,  0.0184,  0.0155, -0.0116, -0.0090, -0.0026,  0.0014]],
       dtype=torch.float64)
	q_value: tensor([[7.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29061042028166584, distance: 0.9638270166430557 entropy -11.092732374412197
epoch: 8, step: 55
	action: tensor([[-0.0053,  0.0186,  0.0162, -0.0116, -0.0067,  0.0038,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[7.2152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29230594444017743, distance: 0.9626744978548324 entropy -11.09911287367305
epoch: 8, step: 56
	action: tensor([[-0.0053,  0.0183,  0.0125, -0.0117, -0.0138,  0.0068, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[7.2318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2928000611741922, distance: 0.9623383662781427 entropy -11.091918545015165
epoch: 8, step: 57
	action: tensor([[-0.0055,  0.0189,  0.0047, -0.0113, -0.0085, -0.0162,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[7.1893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2879554198778467, distance: 0.9656289681620103 entropy -11.105422875728152
epoch: 8, step: 58
	action: tensor([[-0.0051,  0.0185,  0.0107, -0.0119, -0.0081,  0.0043, -0.0130]],
       dtype=torch.float64)
	q_value: tensor([[7.2325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29238171646916866, distance: 0.9626229602221539 entropy -11.083522845534045
epoch: 8, step: 59
	action: tensor([[-0.0054,  0.0188,  0.0089, -0.0114, -0.0145,  0.0023,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[7.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921346788407684, distance: 0.9627909768967238 entropy -11.104436734870278
epoch: 8, step: 60
	action: tensor([[-0.0054,  0.0187,  0.0095, -0.0115, -0.0108,  0.0042,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[7.2042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925058147784305, distance: 0.9625385466883188 entropy -11.096331844716886
epoch: 8, step: 61
	action: tensor([[-0.0054,  0.0187,  0.0142, -0.0115, -0.0075,  0.0075,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[7.2048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2932502500362135, distance: 0.9620320151607898 entropy -11.097386414890893
epoch: 8, step: 62
	action: tensor([[-0.0054,  0.0185,  0.0144, -0.0115, -0.0024,  0.0015, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[7.2173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29169853462618145, distance: 0.9630875382905869 entropy -11.096299116176665
epoch: 8, step: 63
	action: tensor([[-0.0055,  0.0189,  0.0101, -0.0112, -0.0125, -0.0019,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[7.1910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29119266850824066, distance: 0.9634313935705775 entropy -11.108210876920921
epoch: 8, step: 64
	action: tensor([[-0.0053,  0.0187,  0.0040, -0.0116, -0.0050, -0.0236,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[7.2089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28618762594730507, distance: 0.9668269091875102 entropy -11.095729121223542
epoch: 8, step: 65
	action: tensor([[-0.0051,  0.0185,  0.0122, -0.0120,  0.0015, -0.0204, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[7.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28669920368786994, distance: 0.9664803925154152 entropy -11.082778352126637
epoch: 8, step: 66
	action: tensor([[-0.0053,  0.0190,  0.0175, -0.0115, -0.0084,  0.0186, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[7.1969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29608366591272317, distance: 0.9601016470921367 entropy -11.106157479061784
epoch: 8, step: 67
	action: tensor([[-0.0056,  0.0186,  0.0077, -0.0112,  0.0097,  0.0071,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[7.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930948731343812, distance: 0.9621377593363888 entropy -11.108222579789613
epoch: 8, step: 68
	action: tensor([[-0.0053,  0.0183,  0.0132, -0.0117, -0.0035, -0.0278, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28482130148122453, distance: 0.9677517794854273 entropy -11.090062587544168
epoch: 8, step: 69
	action: tensor([[-0.0051,  0.0187,  0.0046, -0.0118,  0.0029, -0.0113,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.2212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2890694416608687, distance: 0.9648732902027143 entropy -11.096677180772312
epoch: 8, step: 70
	action: tensor([[-0.0052,  0.0188,  0.0147, -0.0117, -0.0119,  0.0165,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.2123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954396758087744, distance: 0.9605407295366867 entropy -11.09508203482705
epoch: 8, step: 71
	action: tensor([[-0.0054,  0.0184,  0.0153, -0.0114, -0.0103, -0.0273,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[7.2154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849697567242323, distance: 0.9676513323648475 entropy -11.098162241377963
epoch: 8, step: 72
	action: tensor([[-0.0051,  0.0185,  0.0115, -0.0119, -0.0156,  0.0067,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[7.2312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29302922709420665, distance: 0.9621824322814104 entropy -11.091924534309744
epoch: 8, step: 73
	action: tensor([[-0.0054,  0.0187,  0.0163, -0.0115, -0.0155, -0.0119,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[7.2020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887132853385024, distance: 0.9651149472043099 entropy -11.100678037554013
epoch: 8, step: 74
	action: tensor([[-0.0053,  0.0186,  0.0086, -0.0117, -0.0075,  0.0108,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.2155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29399666659641344, distance: 0.9615238676003761 entropy -11.098042534457951
epoch: 8, step: 75
	action: tensor([[-0.0054,  0.0187,  0.0208, -0.0114, -0.0102, -0.0181,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[7.2071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28721304954625804, distance: 0.9661322144252418 entropy -11.096552579804884
epoch: 8, step: 76
	action: tensor([[-0.0051,  0.0183,  0.0139, -0.0120, -0.0039,  0.0072, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[7.2455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.292979841452086, distance: 0.9622160384567007 entropy -11.08806712573411
epoch: 8, step: 77
	action: tensor([[-0.0055,  0.0188,  0.0110, -0.0113, -0.0061,  0.0026,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[7.2000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921294431827912, distance: 0.9627945374857136 entropy -11.105603608395368
epoch: 8, step: 78
	action: tensor([[-0.0052,  0.0182,  0.0014, -0.0119, -0.0144,  0.0052,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[7.2414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29236758266761764, distance: 0.9626325737767123 entropy -11.083069332213782
epoch: 8, step: 79
	action: tensor([[-0.0054,  0.0189,  0.0135, -0.0115, -0.0067,  0.0094,  0.0428]],
       dtype=torch.float64)
	q_value: tensor([[7.1937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29375452142952063, distance: 0.9616887452899846 entropy -11.095294735808235
epoch: 8, step: 80
	action: tensor([[-0.0053,  0.0180,  0.0241, -0.0118, -0.0043, -0.0012,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[7.2447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29059911023428087, distance: 0.9638346999288635 entropy -11.085347481565197
epoch: 8, step: 81
	action: tensor([[-0.0052,  0.0183,  0.0143, -0.0117, -0.0212, -0.0009, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[7.2367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29104064760483417, distance: 0.9635347036324874 entropy -11.096336840527782
epoch: 8, step: 82
	action: tensor([[-0.0056,  0.0190,  0.0030, -0.0112, -0.0032, -0.0102, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[7.1733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893323952751159, distance: 0.9646948336846787 entropy -11.113421876707164
epoch: 8, step: 83
	action: tensor([[-0.0053,  0.0189,  0.0120, -0.0116, -0.0088,  0.0167, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[7.2052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955372935702162, distance: 0.9604741851396766 entropy -11.095382889258401
epoch: 8, step: 84
	action: tensor([[-0.0055,  0.0188,  0.0165, -0.0112, -0.0120, -0.0068,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[7.1914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28995239860284616, distance: 0.9642739304999057 entropy -11.105484134594507
epoch: 8, step: 85
	action: tensor([[-0.0052,  0.0184,  0.0085, -0.0118, -0.0103, -0.0139,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.2308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28816470547407047, distance: 0.9654870479143192 entropy -11.0921487401712
epoch: 8, step: 86
	action: tensor([[-0.0052,  0.0187,  0.0012, -0.0117,  0.0027,  0.0124, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.2150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2944882540598449, distance: 0.9611890565835477 entropy -11.09414036455864
epoch: 8, step: 87
	action: tensor([[-0.0055,  0.0189,  0.0105, -0.0112, -0.0129, -0.0105, -0.0281]],
       dtype=torch.float64)
	q_value: tensor([[7.1900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892662253076539, distance: 0.964739743808283 entropy -11.10214916010564
epoch: 8, step: 88
	action: tensor([[-0.0054,  0.0190,  0.0151, -0.0114, -0.0117,  0.0144,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[7.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950127908716852, distance: 0.960831675722524 entropy -11.106695097526657
epoch: 8, step: 89
	action: tensor([[-0.0054,  0.0183,  0.0123, -0.0116, -0.0095, -0.0114,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[7.2237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2885587447080814, distance: 0.9652197863421583 entropy -11.093441405127404
epoch: 8, step: 90
	action: tensor([[-0.0052,  0.0186,  0.0147, -0.0117, -0.0078, -0.0008, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.2185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2912875721918319, distance: 0.9633668934978323 entropy -11.095104377013058
epoch: 8, step: 91
	action: tensor([[-0.0054,  0.0188, -0.0011, -0.0114, -0.0012,  0.0167,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[7.2027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29547505375701266, distance: 0.9605166135130926 entropy -11.104669063729862
epoch: 8, step: 92
	action: tensor([[-0.0055,  0.0187,  0.0138, -0.0114, -0.0039,  0.0040,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[7.2026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924173996077778, distance: 0.9625986887735752 entropy -11.092520480165376
epoch: 8, step: 93
	action: tensor([[-0.0053,  0.0183,  0.0172, -0.0117, -0.0025, -0.0004, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.2330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2910650455036906, distance: 0.9635181241033374 entropy -11.091721984546556
epoch: 8, step: 94
	action: tensor([[-0.0053,  0.0186,  0.0025, -0.0115,  0.0012,  0.0102,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[7.2162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29390875116078485, distance: 0.9615837328688736 entropy -11.100414740037575
epoch: 8, step: 95
	action: tensor([[-0.0054,  0.0188,  0.0113, -0.0114, -0.0105, -0.0009, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.2025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.291360329749636, distance: 0.9633174418311709 entropy -11.097593556555255
epoch: 8, step: 96
	action: tensor([[-0.0055,  0.0189,  0.0050, -0.0113,  0.0055, -0.0012,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[7.1900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2913823803204243, distance: 0.9633024540555916 entropy -11.10674341944466
epoch: 8, step: 97
	action: tensor([[-0.0053,  0.0186,  0.0152, -0.0117, -0.0036,  0.0096, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.2217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29367798062009176, distance: 0.9617408563780965 entropy -11.092730747212002
epoch: 8, step: 98
	action: tensor([[-0.0054,  0.0186,  0.0091, -0.0114, -0.0063,  0.0012,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[7.2067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29176442943628966, distance: 0.9630427381951211 entropy -11.10400640304964
epoch: 8, step: 99
	action: tensor([[-0.0054,  0.0187,  0.0110, -0.0116, -0.0128, -0.0235,  0.0455]],
       dtype=torch.float64)
	q_value: tensor([[7.2107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28611778256779186, distance: 0.9668742078961667 entropy -11.096932990101099
epoch: 8, step: 100
	action: tensor([[-0.0051,  0.0182,  0.0196, -0.0121, -0.0139,  0.0117,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[7.2508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938404781866999, distance: 0.9616302201993049 entropy -11.078571124041876
epoch: 8, step: 101
	action: tensor([[-0.0054,  0.0183,  0.0113, -0.0115, -0.0003, -0.0147,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.2225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28779950358832795, distance: 0.9657346841866781 entropy -11.09803131135547
epoch: 8, step: 102
	action: tensor([[-0.0052,  0.0187,  0.0248, -0.0117, -0.0004,  0.0264, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[7.2179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977844325216581, distance: 0.9589410714139756 entropy -11.096183878648946
epoch: 8, step: 103
	action: tensor([[-0.0055,  0.0184,  0.0028, -0.0112, -0.0037,  0.0170,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[7.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2952133990298693, distance: 0.9606949609110177 entropy -11.106870845526974
epoch: 8, step: 104
	action: tensor([[-0.0055,  0.0187,  0.0192, -0.0113, -0.0050, -0.0128,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[7.1983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2884869739732927, distance: 0.9652684711673314 entropy -11.096474363312835
epoch: 8, step: 105
	action: tensor([[-0.0052,  0.0185,  0.0035, -0.0117, -0.0008, -0.0145, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[7.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881833286931885, distance: 0.9654744181707597 entropy -11.098505516059008
epoch: 8, step: 106
	action: tensor([[-0.0053,  0.0190,  0.0148, -0.0116, -0.0027,  0.0258,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.1998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29779836806289384, distance: 0.9589315562239383 entropy -11.098411146230234
epoch: 8, step: 107
	action: tensor([[-0.0055,  0.0184,  0.0156, -0.0113, -0.0067,  0.0102,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[7.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2935490189980535, distance: 0.9618286506112019 entropy -11.100433891108155
epoch: 8, step: 108
	action: tensor([[-0.0054,  0.0186,  0.0074, -0.0114, -0.0123, -0.0099,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[7.2087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891924888222873, distance: 0.964789786929618 entropy -11.100942568071995
epoch: 8, step: 109
	action: tensor([[-0.0052,  0.0186,  0.0020, -0.0118, -0.0099, -0.0092, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[7.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28949154780144537, distance: 0.9645868069393035 entropy -11.09029699937955
epoch: 8, step: 110
	action: tensor([[-0.0053,  0.0189, -0.0002, -0.0116, -0.0132, -0.0131,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[7.2004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28879015407082165, distance: 0.9650627958246594 entropy -11.096188429879321
epoch: 8, step: 111
	action: tensor([[-5.2369e-03,  1.8728e-02, -9.7173e-05, -1.1774e-02, -2.2639e-03,
          7.3931e-04,  4.6979e-02]], dtype=torch.float64)
	q_value: tensor([[7.2115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917708951288617, distance: 0.9630383422337001 entropy -11.088924254060457
epoch: 8, step: 112
	action: tensor([[-0.0052,  0.0182,  0.0046, -0.0119, -0.0135,  0.0176, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[7.2381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2952118193574138, distance: 0.9606960375365633 entropy -11.077058253991842
epoch: 8, step: 113
	action: tensor([[-0.0057,  0.0190,  0.0073, -0.0110, -0.0018,  0.0195, -0.0538]],
       dtype=torch.float64)
	q_value: tensor([[7.1693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29625298006914447, distance: 0.9599861727318374 entropy -11.107936574109262
epoch: 8, step: 114
	action: tensor([[-0.0058,  0.0191,  0.0083, -0.0108, -0.0012, -0.0087, -0.0174]],
       dtype=torch.float64)
	q_value: tensor([[7.1602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28965040425539823, distance: 0.9644789690908909 entropy -11.11757658665393
epoch: 8, step: 115
	action: tensor([[-0.0054,  0.0189, -0.0003, -0.0115, -0.0072, -0.0233, -0.0306]],
       dtype=torch.float64)
	q_value: tensor([[7.2008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864779839617084, distance: 0.9666302507199535 entropy -11.102212641548217
epoch: 8, step: 116
	action: tensor([[-0.0053,  0.0191,  0.0055, -0.0115, -0.0072, -0.0053, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[7.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29072213134417557, distance: 0.9637511243672502 entropy -11.103558730228118
epoch: 8, step: 117
	action: tensor([[-0.0053,  0.0188,  0.0136, -0.0116, -0.0116, -0.0118,  0.0408]],
       dtype=torch.float64)
	q_value: tensor([[7.2069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888283659887617, distance: 0.9650368700054961 entropy -11.096486004002893
epoch: 8, step: 118
	action: tensor([[-0.0051,  0.0182,  0.0020, -0.0120, -0.0028, -0.0019,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[7.2475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2908152404020793, distance: 0.9636878650239696 entropy -11.082450996249475
epoch: 8, step: 119
	action: tensor([[-0.0053,  0.0188,  0.0120, -0.0115, -0.0090,  0.0117, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[7.2019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29434470389976153, distance: 0.9612868379641041 entropy -11.096507144040276
epoch: 8, step: 120
	action: tensor([[-0.0055,  0.0188,  0.0075, -0.0113, -0.0003, -0.0131,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.1943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28854147237578387, distance: 0.9652315030479153 entropy -11.105772742541296
epoch: 8, step: 121
	action: tensor([[-0.0052,  0.0187,  0.0024, -0.0117, -0.0096, -0.0185,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[7.2168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28746282126869027, distance: 0.9659629256561595 entropy -11.094054871541184
epoch: 8, step: 122
	action: tensor([[-0.0052,  0.0188,  0.0044, -0.0118, -0.0119, -0.0053,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[7.2092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29048570834155774, distance: 0.9639117341651334 entropy -11.093021233013362
epoch: 8, step: 123
	action: tensor([[-0.0052,  0.0185,  0.0068, -0.0118, -0.0061,  0.0098,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[7.2238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2936447282847511, distance: 0.9617634946027357 entropy -11.086381784806557
epoch: 8, step: 124
	action: tensor([[-0.0053,  0.0182,  0.0106, -0.0117, -0.0026, -0.0111,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[7.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2885670289746005, distance: 0.9652141666503161 entropy -11.085056588531549
epoch: 8, step: 125
	action: tensor([[-0.0052,  0.0186,  0.0092, -0.0117, -0.0168, -0.0101,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.2199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28925465497519676, distance: 0.9647475964772704 entropy -11.095385497521931
epoch: 8, step: 126
	action: tensor([[-0.0053,  0.0186,  0.0089, -0.0117, -0.0160,  0.0053,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[7.2140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29270454531964873, distance: 0.9624033517673244 entropy -11.093261070828325
epoch: 8, step: 127
	action: tensor([[-0.0054,  0.0187,  0.0035, -0.0115,  0.0022, -0.0036,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[7.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2906743195657471, distance: 0.9637836066148648 entropy -11.09800779601589
LOSS epoch 8 actor 24.01900990544875 critic 11.02402067354129
epoch: 9, step: 0
	action: tensor([[-0.0038,  0.0203, -0.0042, -0.0111, -0.0102,  0.0101, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[8.9063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29686054673264883, distance: 0.9595716903116683 entropy -11.088984737771625
epoch: 9, step: 1
	action: tensor([[-0.0041,  0.0208,  0.0144, -0.0106, -0.0012,  0.0037,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[8.8494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29562677066518295, distance: 0.9604131860490294 entropy -11.101334488648599
epoch: 9, step: 2
	action: tensor([[-0.0039,  0.0203,  0.0027, -0.0108, -0.0048,  0.0145, -0.0259]],
       dtype=torch.float64)
	q_value: tensor([[8.9001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979380089252749, distance: 0.9588362042044932 entropy -11.099455825702886
epoch: 9, step: 3
	action: tensor([[-0.0042,  0.0208,  0.0069, -0.0104, -0.0051, -0.0104,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[8.8437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924084328583876, distance: 0.9626047879582188 entropy -11.108884359424964
epoch: 9, step: 4
	action: tensor([[-0.0038,  0.0200,  0.0082, -0.0113, -0.0012,  0.0021,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[8.9330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2947348697744483, distance: 0.9610210473008548 entropy -11.080928839970195
epoch: 9, step: 5
	action: tensor([[-0.0038,  0.0202,  0.0092, -0.0110,  0.0016, -0.0217,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[8.9144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893874704979503, distance: 0.9646574520649175 entropy -11.090095312599008
epoch: 9, step: 6
	action: tensor([[-0.0037,  0.0202,  0.0202, -0.0113, -0.0010, -0.0095,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[8.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29221004199100586, distance: 0.9627397235790183 entropy -11.089487348525308
epoch: 9, step: 7
	action: tensor([[-0.0038,  0.0203,  0.0149, -0.0110, -0.0034,  0.0004,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[8.9112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946008573516704, distance: 0.9611123481719281 entropy -11.100690370622932
epoch: 9, step: 8
	action: tensor([[-0.0038,  0.0202,  0.0054, -0.0110, -0.0046, -0.0206,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[8.9170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28973155125664507, distance: 0.9644238787439349 entropy -11.095161085494397
epoch: 9, step: 9
	action: tensor([[-0.0037,  0.0204,  0.0056, -0.0112, -0.0022,  0.0035, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[8.9031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29544763543941477, distance: 0.9605353037620914 entropy -11.08948804104987
epoch: 9, step: 10
	action: tensor([[-0.0041,  0.0207,  0.0113, -0.0106, -0.0060,  0.0088,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[8.8557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29682086437898814, distance: 0.9595987671068709 entropy -11.105566425242262
epoch: 9, step: 11
	action: tensor([[-0.0040,  0.0203,  0.0124, -0.0108, -0.0080,  0.0067,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[8.8956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29605962241020745, distance: 0.9601180439337301 entropy -11.097679633305626
epoch: 9, step: 12
	action: tensor([[-0.0039,  0.0203,  0.0056, -0.0109,  0.0009,  0.0017,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[8.8975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29492725457156765, distance: 0.9608899629458607 entropy -11.096928589703609
epoch: 9, step: 13
	action: tensor([[-0.0039,  0.0203,  0.0066, -0.0110, -0.0146, -0.0178,  0.0147]],
       dtype=torch.float64)
	q_value: tensor([[8.9044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29050009170357294, distance: 0.963901963846303 entropy -11.093245822090443
epoch: 9, step: 14
	action: tensor([[-0.0038,  0.0205,  0.0064, -0.0111, -0.0014,  0.0235, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[8.8927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30012444030337215, distance: 0.957341988334656 entropy -11.091224774143052
epoch: 9, step: 15
	action: tensor([[-0.0041,  0.0205,  0.0082, -0.0105, -0.0049,  0.0013,  0.0333]],
       dtype=torch.float64)
	q_value: tensor([[8.8754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949046823061777, distance: 0.9609053438337232 entropy -11.102608726171772
epoch: 9, step: 16
	action: tensor([[-0.0038,  0.0201,  0.0169, -0.0111, -0.0147, -0.0014,  0.0335]],
       dtype=torch.float64)
	q_value: tensor([[8.9224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2939207397806265, distance: 0.9615755695395787 entropy -11.0876185094177
epoch: 9, step: 17
	action: tensor([[-0.0038,  0.0200,  0.0004, -0.0111,  0.0083, -0.0267,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[8.9266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28820637476131616, distance: 0.9654587887466303 entropy -11.08973924584039
epoch: 9, step: 18
	action: tensor([[-0.0036,  0.0201,  0.0148, -0.0114,  0.0059, -0.0297,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[8.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2873763323703773, distance: 0.9660215489369443 entropy -11.080435904608068
epoch: 9, step: 19
	action: tensor([[-0.0036,  0.0198,  0.0101, -0.0116, -0.0056,  0.0008,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[8.9637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29434787443309884, distance: 0.9612846784143871 entropy -11.080555255378384
epoch: 9, step: 20
	action: tensor([[-0.0039,  0.0204,  0.0223, -0.0109,  0.0025,  0.0054,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[8.8898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2958262674799572, distance: 0.9602771694232503 entropy -11.097928399703392
epoch: 9, step: 21
	action: tensor([[-0.0039,  0.0200, -0.0055, -0.0109, -0.0143, -0.0132, -0.0203]],
       dtype=torch.float64)
	q_value: tensor([[8.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914788993551405, distance: 0.9632368473117846 entropy -11.100067914989367
epoch: 9, step: 22
	action: tensor([[-0.0039,  0.0209,  0.0072, -0.0108, -0.0042, -0.0013,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[8.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946780101762809, distance: 0.9610597860438648 entropy -11.102217005400613
epoch: 9, step: 23
	action: tensor([[-0.0039,  0.0204,  0.0015, -0.0109, -0.0025,  0.0162, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[8.8921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29842290458502074, distance: 0.9585050255997969 entropy -11.095692095857741
epoch: 9, step: 24
	action: tensor([[-0.0042,  0.0208,  0.0135, -0.0104,  0.0046, -0.0135,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[8.8389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29161971872510484, distance: 0.9631411203487615 entropy -11.108428841349523
epoch: 9, step: 25
	action: tensor([[-0.0037,  0.0202,  0.0204, -0.0112, -0.0048, -0.0018,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[8.9236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293970084348827, distance: 0.9615419689489919 entropy -11.09192101051438
epoch: 9, step: 26
	action: tensor([[-0.0039,  0.0202,  0.0130, -0.0110, -0.0051, -0.0008,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[8.9110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29425380153969105, distance: 0.961348752349759 entropy -11.10042226377256
epoch: 9, step: 27
	action: tensor([[-0.0039,  0.0203,  0.0158, -0.0110, -0.0076,  0.0212,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[8.9051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29939205434102134, distance: 0.9578427634305555 entropy -11.096717333570364
epoch: 9, step: 28
	action: tensor([[-0.0040,  0.0201,  0.0071, -0.0107, -0.0135, -0.0001,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[8.9042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942970552230967, distance: 0.9613192923882297 entropy -11.100240389033177
epoch: 9, step: 29
	action: tensor([[-0.0038,  0.0199, -0.0017, -0.0113, -0.0009,  0.0089,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[8.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29620394551444384, distance: 0.9600196163367364 entropy -11.078386108974751
epoch: 9, step: 30
	action: tensor([[-0.0040,  0.0204,  0.0030, -0.0109, -0.0076,  0.0038,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[8.8890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29546855866516786, distance: 0.9605210410562747 entropy -11.091047684851164
epoch: 9, step: 31
	action: tensor([[-0.0040,  0.0206,  0.0017, -0.0108, -0.0110, -0.0257,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[8.8749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28896342943557485, distance: 0.9649452272930937 entropy -11.097282416574332
epoch: 9, step: 32
	action: tensor([[-0.0037,  0.0206,  0.0147, -0.0112, -0.0101,  0.0013,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[8.8852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950076944345652, distance: 0.9608351486999801 entropy -11.09104419430676
epoch: 9, step: 33
	action: tensor([[-0.0039,  0.0204,  0.0188, -0.0109, -0.0083,  0.0060, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[8.8952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295890624209032, distance: 0.9602332869927701 entropy -11.09890161314261
epoch: 9, step: 34
	action: tensor([[-0.0040,  0.0204,  0.0093, -0.0107, -0.0121,  0.0060, -0.0350]],
       dtype=torch.float64)
	q_value: tensor([[8.8830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959639434428024, distance: 0.9601832909238721 entropy -11.107250808915134
epoch: 9, step: 35
	action: tensor([[-0.0042,  0.0208,  0.0044, -0.0104, -0.0031, -0.0102,  0.0204]],
       dtype=torch.float64)
	q_value: tensor([[8.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924827779409396, distance: 0.9625542172507371 entropy -11.111633711051839
epoch: 9, step: 36
	action: tensor([[-0.0038,  0.0203, -0.0042, -0.0111, -0.0088, -0.0122,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[8.9100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2918307417594621, distance: 0.9629976521398212 entropy -11.08875155445729
epoch: 9, step: 37
	action: tensor([[-0.0038,  0.0205,  0.0173, -0.0111,  0.0058,  0.0079,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[8.8850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2964530540119965, distance: 0.9598497019075778 entropy -11.089253299902717
epoch: 9, step: 38
	action: tensor([[-0.0038,  0.0199,  0.0065, -0.0110, -0.0038, -0.0074,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[8.9355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925638573285193, distance: 0.9624990627329212 entropy -11.09484989573497
epoch: 9, step: 39
	action: tensor([[-0.0038,  0.0201,  0.0061, -0.0112, -0.0169, -0.0073,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[8.9230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29270737441667993, distance: 0.9624014270163805 entropy -11.084284922029704
epoch: 9, step: 40
	action: tensor([[-0.0038,  0.0204,  0.0098, -0.0111, -0.0032,  0.0186,  0.0344]],
       dtype=torch.float64)
	q_value: tensor([[8.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988454465024454, distance: 0.9582163406113859 entropy -11.08875751266354
epoch: 9, step: 41
	action: tensor([[-0.0039,  0.0199,  0.0150, -0.0109, -0.0068, -0.0008, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[8.9254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2939605523007853, distance: 0.9615484597740289 entropy -11.090096755493247
epoch: 9, step: 42
	action: tensor([[-0.0039,  0.0205,  0.0133, -0.0108, -0.0184, -0.0004, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[8.8831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2945168378356888, distance: 0.9611695851211789 entropy -11.104045041325861
epoch: 9, step: 43
	action: tensor([[-0.0040,  0.0205, -0.0014, -0.0108, -0.0161, -0.0091, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[8.8742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926778777171246, distance: 0.9624214946438067 entropy -11.103186780891608
epoch: 9, step: 44
	action: tensor([[-3.9173e-03,  2.0725e-02,  2.1116e-02, -1.0866e-02,  7.7555e-05,
          4.4677e-03, -1.7367e-03]], dtype=torch.float64)
	q_value: tensor([[8.8588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2957828571375747, distance: 0.9603067681688053 entropy -11.09798402264967
epoch: 9, step: 45
	action: tensor([[-0.0039,  0.0202,  0.0059, -0.0108, -0.0033,  0.0074,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[8.9091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2961817848694097, distance: 0.9600347304357286 entropy -11.10340336708676
epoch: 9, step: 46
	action: tensor([[-0.0039,  0.0204,  0.0203, -0.0109, -0.0018, -0.0416,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[8.8951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2847733221648858, distance: 0.9677842408060305 entropy -11.09409833633942
epoch: 9, step: 47
	action: tensor([[-0.0035,  0.0197,  0.0009, -0.0117, -0.0095, -0.0070,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[8.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.292592752782722, distance: 0.9624794057420555 entropy -11.077921693445274
epoch: 9, step: 48
	action: tensor([[-0.0039,  0.0205,  0.0109, -0.0110, -0.0047, -0.0023,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[8.8826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29409516512212885, distance: 0.9614567914390576 entropy -11.092423168728873
epoch: 9, step: 49
	action: tensor([[-0.0038,  0.0199, -0.0007, -0.0113, -0.0002, -0.0351,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[8.9433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2862250263009908, distance: 0.9668015803032463 entropy -11.082679379359153
epoch: 9, step: 50
	action: tensor([[-0.0036,  0.0204,  0.0116, -0.0114,  0.0022,  0.0094, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[8.9058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29682939197889624, distance: 0.9595929484621235 entropy -11.084570661321566
epoch: 9, step: 51
	action: tensor([[-0.0040,  0.0204, -0.0012, -0.0107,  0.0058, -0.0172, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[8.8911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29070874879669595, distance: 0.9637602162795417 entropy -11.10149951738247
epoch: 9, step: 52
	action: tensor([[-0.0039,  0.0207,  0.0113, -0.0109, -0.0099, -0.0254, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[8.8690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2890505402047132, distance: 0.9648861166234934 entropy -11.09956289721642
epoch: 9, step: 53
	action: tensor([[-0.0038,  0.0206,  0.0022, -0.0110, -0.0057, -0.0196,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[8.8753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2903694342272093, distance: 0.9639907131181312 entropy -11.102080373137403
epoch: 9, step: 54
	action: tensor([[-0.0037,  0.0204,  0.0046, -0.0112, -0.0160, -0.0056, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[8.9034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29337292115533065, distance: 0.9619485212046172 entropy -11.088367363515072
epoch: 9, step: 55
	action: tensor([[-0.0039,  0.0207,  0.0139, -0.0109,  0.0058,  0.0033,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[8.8677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955133745266497, distance: 0.9604904907784728 entropy -11.098198015418024
epoch: 9, step: 56
	action: tensor([[-0.0038,  0.0201,  0.0173, -0.0110, -0.0060, -0.0119,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[8.9235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29157022150448775, distance: 0.9631747689249842 entropy -11.093835140212695
epoch: 9, step: 57
	action: tensor([[-0.0037,  0.0200, -0.0019, -0.0113, -0.0041, -0.0005, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[8.9370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942986230414001, distance: 0.9613182245346589 entropy -11.087477726039507
epoch: 9, step: 58
	action: tensor([[-0.0040,  0.0208,  0.0017, -0.0107, -0.0109, -0.0217,  0.0228]],
       dtype=torch.float64)
	q_value: tensor([[8.8503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2900372643925897, distance: 0.9642163031564375 entropy -11.101747135106525
epoch: 9, step: 59
	action: tensor([[-0.0037,  0.0204,  0.0028, -0.0112, -0.0014, -0.0137,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[8.9015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29149227320263293, distance: 0.9632277563732974 entropy -11.085841162306346
epoch: 9, step: 60
	action: tensor([[-0.0038,  0.0204,  0.0117, -0.0111, -0.0054, -0.0056, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[8.9022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2932788614055887, distance: 0.9620125419795698 entropy -11.090734174139838
epoch: 9, step: 61
	action: tensor([[-0.0039,  0.0205,  0.0115, -0.0109, -0.0150,  0.0183, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[8.8882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988482715031272, distance: 0.9582144102491508 entropy -11.099442172909578
epoch: 9, step: 62
	action: tensor([[-0.0041,  0.0205, -0.0025, -0.0106, -0.0070,  0.0109, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[8.8683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29715130580543847, distance: 0.9593732709065579 entropy -11.106092863393565
epoch: 9, step: 63
	action: tensor([[-0.0041,  0.0207,  0.0126, -0.0106, -0.0019, -0.0114, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[8.8561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29209848322162013, distance: 0.9628155920099407 entropy -11.100982952868607
epoch: 9, step: 64
	action: tensor([[-0.0038,  0.0205,  0.0037, -0.0110, -0.0030, -0.0037,  0.0350]],
       dtype=torch.float64)
	q_value: tensor([[8.8908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938680611378569, distance: 0.9616114391324236 entropy -11.099617755835325
epoch: 9, step: 65
	action: tensor([[-0.0038,  0.0202,  0.0058, -0.0112, -0.0118, -0.0056,  0.0353]],
       dtype=torch.float64)
	q_value: tensor([[8.9195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29310356363557755, distance: 0.9621318451870007 entropy -11.084060193333155
epoch: 9, step: 66
	action: tensor([[-0.0038,  0.0202,  0.0107, -0.0112, -0.0032,  0.0089, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[8.9137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2964448479939108, distance: 0.9598552996300119 entropy -11.084820601018198
epoch: 9, step: 67
	action: tensor([[-0.0040,  0.0204,  0.0096, -0.0107, -0.0045, -0.0166, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[8.8879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29077840539808386, distance: 0.9637128916372438 entropy -11.101814819738506
epoch: 9, step: 68
	action: tensor([[-0.0038,  0.0206,  0.0022, -0.0109, -0.0061, -0.0079, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[8.8755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29302723255377805, distance: 0.9621837895584584 entropy -11.100801737432038
epoch: 9, step: 69
	action: tensor([[-0.0039,  0.0207,  0.0074, -0.0109, -0.0059,  0.0006, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[8.8707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294938169139658, distance: 0.9608825256001514 entropy -11.099445315781082
epoch: 9, step: 70
	action: tensor([[-0.0040,  0.0206,  0.0094, -0.0107,  0.0017,  0.0005,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[8.8698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29483713115690235, distance: 0.9609513721536512 entropy -11.10387554860622
epoch: 9, step: 71
	action: tensor([[-0.0039,  0.0203,  0.0184, -0.0109, -0.0123,  0.0042,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[8.9035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954281307693013, distance: 0.960548599292355 entropy -11.096555935673974
epoch: 9, step: 72
	action: tensor([[-0.0038,  0.0198,  0.0049, -0.0112, -0.0101, -0.0068, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[8.9411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926121386342281, distance: 0.9624662177158215 entropy -11.087487493846513
epoch: 9, step: 73
	action: tensor([[-3.9177e-03,  2.0649e-02,  2.2049e-02, -1.0843e-02, -9.9499e-05,
          1.0528e-02,  1.2639e-02]], dtype=torch.float64)
	q_value: tensor([[8.8668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971729063407744, distance: 0.9593585286614409 entropy -11.101170249150487
epoch: 9, step: 74
	action: tensor([[-0.0039,  0.0200,  0.0144, -0.0109, -0.0040,  0.0252,  0.0556]],
       dtype=torch.float64)
	q_value: tensor([[8.9246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30011557103362463, distance: 0.9573480543397602 entropy -11.100138559997045
epoch: 9, step: 75
	action: tensor([[-0.0040,  0.0196,  0.0019, -0.0111, -0.0029, -0.0071, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[8.9508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2922721284557963, distance: 0.9626974974806276 entropy -11.085261985714268
epoch: 9, step: 76
	action: tensor([[-0.0040,  0.0208,  0.0176, -0.0107, -0.0047,  0.0078,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[8.8462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966532405640818, distance: 0.9597131348498588 entropy -11.105925185311028
epoch: 9, step: 77
	action: tensor([[-0.0040,  0.0203, -0.0040, -0.0108, -0.0063,  0.0122,  0.0039]],
       dtype=torch.float64)
	q_value: tensor([[8.9023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973292105686267, distance: 0.9592518451446734 entropy -11.100601321195848
epoch: 9, step: 78
	action: tensor([[-0.0040,  0.0206,  0.0113, -0.0107, -0.0003,  0.0286, -0.0618]],
       dtype=torch.float64)
	q_value: tensor([[8.8662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301391241197296, distance: 0.9564751835484263 entropy -11.096924073292525
epoch: 9, step: 79
	action: tensor([[-0.0045,  0.0207,  0.0116, -0.0099, -0.0089,  0.0068,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[8.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962228094857988, distance: 0.9600067504619003 entropy -11.123398972828372
epoch: 9, step: 80
	action: tensor([[-0.0039,  0.0202,  0.0177, -0.0109, -0.0067,  0.0066, -0.0327]],
       dtype=torch.float64)
	q_value: tensor([[8.9070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29591236608375515, distance: 0.9602184615737969 entropy -11.093214558849997
epoch: 9, step: 81
	action: tensor([[-0.0041,  0.0206,  0.0130, -0.0105, -0.0043, -0.0102,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[8.8579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29225162485069833, distance: 0.9627114425484634 entropy -11.113720655957403
epoch: 9, step: 82
	action: tensor([[-0.0037,  0.0201,  0.0077, -0.0112, -0.0072,  0.0239,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[8.9295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299877626788866, distance: 0.9575107784213012 entropy -11.087407656302258
epoch: 9, step: 83
	action: tensor([[-0.0040,  0.0202,  0.0128, -0.0108, -0.0069, -0.0245,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[8.9017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2887175020510432, distance: 0.965112086460359 entropy -11.093982857510131
epoch: 9, step: 84
	action: tensor([[-0.0037,  0.0204,  0.0125, -0.0112, -0.0025, -0.0018, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[8.9032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29420594815641, distance: 0.9613813441020163 entropy -11.093731076585538
epoch: 9, step: 85
	action: tensor([[-0.0040,  0.0207,  0.0040, -0.0106, -0.0089,  0.0190,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[8.8594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991541092660013, distance: 0.9580054040496703 entropy -11.108527178245211
epoch: 9, step: 86
	action: tensor([[-0.0040,  0.0203,  0.0025, -0.0108, -0.0050, -0.0103, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[8.8923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29213961222509865, distance: 0.9627876218614844 entropy -11.0932280493657
epoch: 9, step: 87
	action: tensor([[-0.0039,  0.0207,  0.0006, -0.0109,  0.0008, -0.0002, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[8.8668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29484227907974614, distance: 0.9609478645153683 entropy -11.099153551755881
epoch: 9, step: 88
	action: tensor([[-0.0040,  0.0207,  0.0005, -0.0107, -0.0020,  0.0274, -0.0440]],
       dtype=torch.float64)
	q_value: tensor([[8.8592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011833243290176, distance: 0.9566175039301424 entropy -11.103416370981169
epoch: 9, step: 89
	action: tensor([[-0.0044,  0.0209,  0.0002, -0.0101, -0.0016, -0.0007,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[8.8171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29472509965234506, distance: 0.9610277038474895 entropy -11.115063602472665
epoch: 9, step: 90
	action: tensor([[-0.0038,  0.0202,  0.0069, -0.0112, -0.0053,  0.0089, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[8.9167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29640591484625556, distance: 0.9598818573710935 entropy -11.082755466680345
epoch: 9, step: 91
	action: tensor([[-0.0040,  0.0205,  0.0078, -0.0107, -0.0040, -0.0069,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[8.8763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2931003759602293, distance: 0.9621340145008841 entropy -11.100483197099553
epoch: 9, step: 92
	action: tensor([[-0.0038,  0.0198,  0.0160, -0.0114, -0.0097,  0.0013, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[8.9511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942837637141672, distance: 0.9613283452934857 entropy -11.075958556960979
epoch: 9, step: 93
	action: tensor([[-0.0039,  0.0204,  0.0050, -0.0108, -0.0096,  0.0147, -0.0337]],
       dtype=torch.float64)
	q_value: tensor([[8.8864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980016960594035, distance: 0.9587927130914117 entropy -11.103420041034656
epoch: 9, step: 94
	action: tensor([[-0.0042,  0.0208,  0.0124, -0.0103, -0.0092,  0.0089,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[8.8311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29685108311147923, distance: 0.9595781477735307 entropy -11.111535271986467
epoch: 9, step: 95
	action: tensor([[-0.0039,  0.0200,  0.0172, -0.0111, -0.0053,  0.0032,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[8.9284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949019772057565, distance: 0.960907187090154 entropy -11.086814232796417
epoch: 9, step: 96
	action: tensor([[-0.0038,  0.0200,  0.0136, -0.0111, -0.0135,  0.0045, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[8.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2953199196122662, distance: 0.9606223590386608 entropy -11.092145181366808
epoch: 9, step: 97
	action: tensor([[-0.0040,  0.0206,  0.0167, -0.0107, -0.0004,  0.0063, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[8.8713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29611113747087636, distance: 0.9600829120917567 entropy -11.104494557935535
epoch: 9, step: 98
	action: tensor([[-0.0040,  0.0203,  0.0112, -0.0108, -0.0105, -0.0126,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[8.8956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2916299467007535, distance: 0.9631341671488036 entropy -11.104099007065292
epoch: 9, step: 99
	action: tensor([[-0.0038,  0.0203,  0.0133, -0.0111, -0.0018,  0.0086,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[8.9070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965414541237018, distance: 0.9597893978391147 entropy -11.091061998936226
epoch: 9, step: 100
	action: tensor([[-0.0040,  0.0203,  0.0029, -0.0108, -0.0072, -0.0011,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[8.8961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943231223015259, distance: 0.9613015377387814 entropy -11.100101301129373
epoch: 9, step: 101
	action: tensor([[-0.0039,  0.0204,  0.0109, -0.0110,  0.0080, -0.0144,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[8.8895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29118473642827447, distance: 0.9634367843115704 entropy -11.092280152208279
epoch: 9, step: 102
	action: tensor([[-0.0037,  0.0202,  0.0076, -0.0112, -0.0024,  0.0102, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[8.9235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29686188447255535, distance: 0.9595707775069979 entropy -11.09079907569163
epoch: 9, step: 103
	action: tensor([[-0.0040,  0.0206,  0.0058, -0.0106, -0.0035,  0.0052, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[8.8732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959256023401543, distance: 0.9602094358809159 entropy -11.102564121297663
epoch: 9, step: 104
	action: tensor([[-0.0040,  0.0206,  0.0101, -0.0107,  0.0023, -0.0061,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[8.8754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2932399897634018, distance: 0.9620389983080108 entropy -11.101719197588606
epoch: 9, step: 105
	action: tensor([[-0.0038,  0.0203,  0.0106, -0.0111,  0.0010,  0.0023, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[8.9122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29504228286208345, distance: 0.9608115780990846 entropy -11.094139523598221
epoch: 9, step: 106
	action: tensor([[-0.0040,  0.0206,  0.0140, -0.0107, -0.0052,  0.0174,  0.0014]],
       dtype=torch.float64)
	q_value: tensor([[8.8713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29876478952739394, distance: 0.9582714530031957 entropy -11.105167696182729
epoch: 9, step: 107
	action: tensor([[-0.0040,  0.0203,  0.0163, -0.0107, -0.0030,  0.0049,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[8.8913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955502853706532, distance: 0.9604653284987527 entropy -11.103341476998084
epoch: 9, step: 108
	action: tensor([[-0.0039,  0.0201,  0.0070, -0.0110, -0.0026,  0.0069, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[8.9189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29597685714362665, distance: 0.9601744848574074 entropy -11.095902058003533
epoch: 9, step: 109
	action: tensor([[-0.0041,  0.0208,  0.0095, -0.0105, -0.0069, -0.0018, -0.0241]],
       dtype=torch.float64)
	q_value: tensor([[8.8468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29440348475542544, distance: 0.9612467996904932 entropy -11.108621223613408
epoch: 9, step: 110
	action: tensor([[-0.0040,  0.0207,  0.0099, -0.0107, -0.0075, -0.0035,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[8.8590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29392790589246676, distance: 0.9615706899347338 entropy -11.107294055484351
epoch: 9, step: 111
	action: tensor([[-0.0039,  0.0204,  0.0084, -0.0110,  0.0041, -0.0114, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[8.8977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29192677236126074, distance: 0.9629323567473189 entropy -11.094684457575427
epoch: 9, step: 112
	action: tensor([[-0.0038,  0.0205,  0.0024, -0.0110, -0.0172, -0.0153,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[8.8937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29131634035537857, distance: 0.9633473407310742 entropy -11.097712477122556
epoch: 9, step: 113
	action: tensor([[-0.0038,  0.0204,  0.0047, -0.0112,  0.0069,  0.0190,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[8.8972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299033689479528, distance: 0.9580877030634186 entropy -11.087465011595175
epoch: 9, step: 114
	action: tensor([[-0.0040,  0.0201,  0.0057, -0.0108, -0.0076,  0.0076,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[8.9141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29609538485482545, distance: 0.9600936550752599 entropy -11.09163200824989
epoch: 9, step: 115
	action: tensor([[-0.0039,  0.0201,  0.0060, -0.0111, -0.0113, -0.0152, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[8.9179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29088150351825426, distance: 0.9636428425867342 entropy -11.085062986963733
epoch: 9, step: 116
	action: tensor([[-0.0039,  0.0207,  0.0055, -0.0108, -0.0118, -0.0112,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[8.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.292333286076926, distance: 0.9626559012943178 entropy -11.104726161042345
epoch: 9, step: 117
	action: tensor([[-0.0039,  0.0206,  0.0004, -0.0110, -0.0056,  0.0167,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[8.8803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29856803094644047, distance: 0.9584058835795475 entropy -11.095803901265015
epoch: 9, step: 118
	action: tensor([[-0.0040,  0.0202,  0.0095, -0.0109, -0.0152,  0.0316, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[8.9035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015971999883973, distance: 0.9563341826009701 entropy -11.08899561570261
epoch: 9, step: 119
	action: tensor([[-0.0042,  0.0205,  0.0181, -0.0104, -0.0086, -0.0227,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[8.8639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892557365623747, distance: 0.9647468624175719 entropy -11.105522928756574
epoch: 9, step: 120
	action: tensor([[-0.0037,  0.0202,  0.0102, -0.0112, -0.0030, -0.0041,  0.0178]],
       dtype=torch.float64)
	q_value: tensor([[8.9167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29357126687725776, distance: 0.961813505316846 entropy -11.094237161101264
epoch: 9, step: 121
	action: tensor([[-0.0038,  0.0203, -0.0016, -0.0111, -0.0027,  0.0052,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[8.9108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29572375201388534, distance: 0.9603470667187535 entropy -11.093316060050862
epoch: 9, step: 122
	action: tensor([[-0.0039,  0.0202,  0.0193, -0.0110, -0.0029,  0.0260, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[8.9063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30047189636167315, distance: 0.9571043206780541 entropy -11.086319336451455
epoch: 9, step: 123
	action: tensor([[-0.0042,  0.0204,  0.0040, -0.0104, -0.0045,  0.0177, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[8.8805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986167019765733, distance: 0.9583726320220872 entropy -11.110134613987082
epoch: 9, step: 124
	action: tensor([[-0.0041,  0.0207,  0.0054, -0.0105, -0.0060,  0.0054, -0.0175]],
       dtype=torch.float64)
	q_value: tensor([[8.8575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29600487561743105, distance: 0.9601553783183715 entropy -11.10450426669078
epoch: 9, step: 125
	action: tensor([[-0.0041,  0.0207,  0.0166, -0.0106, -0.0010,  0.0200, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[8.8598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994054476861029, distance: 0.9578336079677497 entropy -11.105368492715646
epoch: 9, step: 126
	action: tensor([[-0.0041,  0.0204,  0.0151, -0.0105, -0.0166,  0.0080,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[8.8800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29636205747580247, distance: 0.9599117732267644 entropy -11.107903645513803
epoch: 9, step: 127
	action: tensor([[-0.0040,  0.0204,  0.0167, -0.0108, -0.0103, -0.0084,  0.0002]],
       dtype=torch.float64)
	q_value: tensor([[8.8872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925425155728717, distance: 0.962513580838266 entropy -11.10002892228123
LOSS epoch 9 actor 37.03566816174551 critic 9.696001681242738
epoch: 10, step: 0
	action: tensor([[-0.0023,  0.0215,  0.0127, -0.0096, -0.0095,  0.0025, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[8.5207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982369303056419, distance: 0.9586320575903102 entropy -11.10358238802136
epoch: 10, step: 1
	action: tensor([[-0.0025,  0.0217,  0.0070, -0.0093, -0.0109,  0.0042,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[8.4945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987409288626476, distance: 0.9582877562342894 entropy -11.108668752271043
epoch: 10, step: 2
	action: tensor([[-0.0023,  0.0213,  0.0067, -0.0097,  0.0004,  0.0142,  0.0382]],
       dtype=torch.float64)
	q_value: tensor([[8.5392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30065070371811453, distance: 0.9569819894735695 entropy -11.090718462914406
epoch: 10, step: 3
	action: tensor([[-0.0024,  0.0211,  0.0073, -0.0097, -0.0025, -0.0121,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[8.5492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2944705979305887, distance: 0.9612010838617209 entropy -11.089010967036128
epoch: 10, step: 4
	action: tensor([[-0.0023,  0.0215,  0.0161, -0.0097, -0.0143,  0.0123, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[8.5257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30042532198314964, distance: 0.957136182011957 entropy -11.094817115453923
epoch: 10, step: 5
	action: tensor([[-0.0025,  0.0215,  0.0087, -0.0093, -0.0111, -0.0272,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[8.5083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914755570208375, distance: 0.9632391192665345 entropy -11.106820551108859
epoch: 10, step: 6
	action: tensor([[-0.0022,  0.0214,  0.0099, -0.0099, -0.0058, -0.0247, -0.0234]],
       dtype=torch.float64)
	q_value: tensor([[8.5356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2919456024257281, distance: 0.9629195528463963 entropy -11.09050688291587
epoch: 10, step: 7
	action: tensor([[-0.0023,  0.0218,  0.0085, -0.0095, -0.0044, -0.0045, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[8.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969052036257839, distance: 0.959541218283229 entropy -11.106649329602154
epoch: 10, step: 8
	action: tensor([[-0.0024,  0.0218,  0.0131, -0.0094, -0.0066,  0.0064,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[8.4957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29925860853480646, distance: 0.9579339799337689 entropy -11.106267664308268
epoch: 10, step: 9
	action: tensor([[-0.0023,  0.0212,  0.0043, -0.0097, -0.0119, -0.0012,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[8.5428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971393527389903, distance: 0.9593814287105029 entropy -11.093926325095552
epoch: 10, step: 10
	action: tensor([[-0.0024,  0.0216,  0.0021, -0.0096, -0.0034, -0.0045, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[8.5162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966295446080276, distance: 0.9597293012204475 entropy -11.09449897026897
epoch: 10, step: 11
	action: tensor([[-0.0024,  0.0219,  0.0146, -0.0094, -0.0102,  0.0098,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[8.4886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000647766491291, distance: 0.9573827936630614 entropy -11.10443137719733
epoch: 10, step: 12
	action: tensor([[-0.0024,  0.0213,  0.0039, -0.0095, -0.0032,  0.0164, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[8.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012744340161232, distance: 0.9565551413925089 entropy -11.100245543857111
epoch: 10, step: 13
	action: tensor([[-0.0025,  0.0217,  0.0023, -0.0092, -0.0056, -0.0073, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[8.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29609306972172267, distance: 0.9600952339417194 entropy -11.104493018053716
epoch: 10, step: 14
	action: tensor([[-0.0024,  0.0218,  0.0098, -0.0095, -0.0017, -0.0121, -0.0045]],
       dtype=torch.float64)
	q_value: tensor([[8.4966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29501937365062925, distance: 0.9608271898558256 entropy -11.102870342683772
epoch: 10, step: 15
	action: tensor([[-0.0023,  0.0216,  0.0067, -0.0096, -0.0112,  0.0011,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[8.5149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29800612102957447, distance: 0.958789691263711 entropy -11.101403007239062
epoch: 10, step: 16
	action: tensor([[-0.0024,  0.0215,  0.0041, -0.0096, -0.0016,  0.0033,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[8.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983959499436143, distance: 0.9585234383382316 entropy -11.09717691886903
epoch: 10, step: 17
	action: tensor([[-0.0024,  0.0216,  0.0215, -0.0095, -0.0136, -0.0016, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[8.5127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29721253090489697, distance: 0.959331484528046 entropy -11.098613464687704
epoch: 10, step: 18
	action: tensor([[-0.0024,  0.0214,  0.0129, -0.0095, -0.0045,  0.0012, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[8.5248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29782246801227463, distance: 0.9589151005513752 entropy -11.105040322419757
epoch: 10, step: 19
	action: tensor([[-0.0024,  0.0216,  0.0082, -0.0094, -0.0037,  0.0228,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[8.5108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029434128936208, distance: 0.9554120425292724 entropy -11.106225444653772
epoch: 10, step: 20
	action: tensor([[-0.0025,  0.0214, -0.0017, -0.0093, -0.0047,  0.0042, -0.0274]],
       dtype=torch.float64)
	q_value: tensor([[8.5176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985267992502447, distance: 0.9584340517564094 entropy -11.101232122081198
epoch: 10, step: 21
	action: tensor([[-0.0026,  0.0220,  0.0154, -0.0092, -0.0028, -0.0058,  0.0247]],
       dtype=torch.float64)
	q_value: tensor([[8.4681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29650965267497786, distance: 0.9598110923288227 entropy -11.10945536841428
epoch: 10, step: 22
	action: tensor([[-0.0023,  0.0212, -0.0023, -0.0098, -0.0102,  0.0019,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[8.5507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29789846159807754, distance: 0.9588632095655517 entropy -11.093069885156012
epoch: 10, step: 23
	action: tensor([[-0.0023,  0.0211,  0.0020, -0.0099, -0.0036, -0.0083, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[8.5473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2953368334602722, distance: 0.9606108304609977 entropy -11.08010792613828
epoch: 10, step: 24
	action: tensor([[-0.0024,  0.0219,  0.0114, -0.0094, -0.0087, -0.0045,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[8.4851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968481192902559, distance: 0.9595801701154661 entropy -11.104897365056772
epoch: 10, step: 25
	action: tensor([[-0.0023,  0.0216,  0.0042, -0.0096, -0.0059,  0.0039,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[8.5162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985906335799905, distance: 0.9583904418314894 entropy -11.100696649348013
epoch: 10, step: 26
	action: tensor([[-0.0024,  0.0216,  0.0083, -0.0095, -0.0067, -0.0008,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[8.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29750697320904096, distance: 0.9591305010321195 entropy -11.100630901746518
epoch: 10, step: 27
	action: tensor([[-0.0024,  0.0216,  0.0143, -0.0095, -0.0107,  0.0092,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[8.5130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29976336064452624, distance: 0.9575889123341369 entropy -11.100112386811423
epoch: 10, step: 28
	action: tensor([[-0.0024,  0.0214,  0.0004, -0.0095, -0.0118, -0.0182, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[8.5237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29351694531906847, distance: 0.9618504844213789 entropy -11.10218177884641
epoch: 10, step: 29
	action: tensor([[-0.0023,  0.0219,  0.0070, -0.0095, -0.0080, -0.0238, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[8.4854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29253319237223163, distance: 0.9625199230410597 entropy -11.103146276046306
epoch: 10, step: 30
	action: tensor([[-0.0023,  0.0219,  0.0108, -0.0095, -0.0163, -0.0090,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[8.4866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29590102881788183, distance: 0.960226192293418 entropy -11.106132571192287
epoch: 10, step: 31
	action: tensor([[-0.0023,  0.0215,  0.0202, -0.0097, -0.0072, -0.0055,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[8.5295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29624212090635027, distance: 0.9599935792327678 entropy -11.093383844080112
epoch: 10, step: 32
	action: tensor([[-0.0023,  0.0213,  0.0240, -0.0097,  0.0054,  0.0057,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[8.5444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987488633359472, distance: 0.9582823348925658 entropy -11.098956356549449
epoch: 10, step: 33
	action: tensor([[-0.0024,  0.0212,  0.0139, -0.0095, -0.0032, -0.0202,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[8.5450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29268298257268177, distance: 0.962418021663516 entropy -11.10581395648415
epoch: 10, step: 34
	action: tensor([[-0.0021,  0.0209,  0.0004, -0.0102, -0.0086,  0.0136,  0.0172]],
       dtype=torch.float64)
	q_value: tensor([[8.5799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30023232864579064, distance: 0.9572681966301139 entropy -11.081509298485637
epoch: 10, step: 35
	action: tensor([[-0.0025,  0.0215,  0.0119, -0.0095, -0.0050, -0.0138,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[8.5123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943389102572751, distance: 0.961290784183218 entropy -11.096449328470502
epoch: 10, step: 36
	action: tensor([[-0.0022,  0.0213,  0.0108, -0.0098, -0.0045, -0.0068,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[8.5429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29593122886085776, distance: 0.9602055991776625 entropy -11.093120325917182
epoch: 10, step: 37
	action: tensor([[-0.0023,  0.0214,  0.0077, -0.0098, -0.0034, -0.0194,  0.0536]],
       dtype=torch.float64)
	q_value: tensor([[8.5388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29305980996408687, distance: 0.962161620516978 entropy -11.094346717280414
epoch: 10, step: 38
	action: tensor([[-0.0022,  0.0209,  0.0101, -0.0101, -0.0007,  0.0226,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[8.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023791643889152, distance: 0.9557986544455863 entropy -11.078789664353675
epoch: 10, step: 39
	action: tensor([[-0.0025,  0.0212,  0.0106, -0.0094, -0.0012, -0.0016, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[8.5356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29696733928608465, distance: 0.9594988178524408 entropy -11.097260893447418
epoch: 10, step: 40
	action: tensor([[-0.0025,  0.0217,  0.0030, -0.0094, -0.0206,  0.0110,  0.0325]],
       dtype=torch.float64)
	q_value: tensor([[8.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30024699388018394, distance: 0.9572581657037015 entropy -11.108889852552958
epoch: 10, step: 41
	action: tensor([[-0.0024,  0.0214,  0.0080, -0.0096, -0.0133, -0.0090,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[8.5221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954017820882877, distance: 0.9605665598107423 entropy -11.090133900937639
epoch: 10, step: 42
	action: tensor([[-0.0023,  0.0214,  0.0034, -0.0098, -0.0095,  0.0270,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[8.5341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30370226637586906, distance: 0.9548918443131363 entropy -11.090145017200948
epoch: 10, step: 43
	action: tensor([[-0.0025,  0.0212,  0.0082, -0.0095,  0.0046, -0.0052,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[8.5331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2960429284750723, distance: 0.960129428458494 entropy -11.092843785093985
epoch: 10, step: 44
	action: tensor([[-0.0023,  0.0214, -0.0011, -0.0097,  0.0010, -0.0155, -0.0224]],
       dtype=torch.float64)
	q_value: tensor([[8.5368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29406025972218686, distance: 0.961480562078751 entropy -11.095347678308642
epoch: 10, step: 45
	action: tensor([[-0.0024,  0.0219,  0.0113, -0.0095, -0.0038, -0.0122,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[8.4830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950988158692699, distance: 0.9607730519183285 entropy -11.105406236813028
epoch: 10, step: 46
	action: tensor([[-0.0022,  0.0213,  0.0080, -0.0098, -0.0097,  0.0024, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[8.5433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980430647589093, distance: 0.9587644618892507 entropy -11.091831940998924
epoch: 10, step: 47
	action: tensor([[-0.0024,  0.0217,  0.0164, -0.0094, -0.0005, -0.0300,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[8.5032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29075178050436723, distance: 0.9637309808442697 entropy -11.102799146505259
epoch: 10, step: 48
	action: tensor([[-0.0021,  0.0212, -0.0107, -0.0100, -0.0110, -0.0169,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[8.5575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29382489014623525, distance: 0.9616408338408399 entropy -11.09039289354793
epoch: 10, step: 49
	action: tensor([[-0.0023,  0.0214,  0.0072, -0.0100, -0.0034,  0.0032,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[8.5277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29821659691836655, distance: 0.9586459455366341 entropy -11.077606402688177
epoch: 10, step: 50
	action: tensor([[-2.4097e-03,  2.1608e-02,  1.0055e-03, -9.4767e-03, -1.4034e-05,
         -1.3517e-02,  9.5781e-03]], dtype=torch.float64)
	q_value: tensor([[8.5119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29463139870693944, distance: 0.9610915415181916 entropy -11.100970103879474
epoch: 10, step: 51
	action: tensor([[-0.0023,  0.0216, -0.0029, -0.0097, -0.0050,  0.0047, -0.0351]],
       dtype=torch.float64)
	q_value: tensor([[8.5188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29884297101595336, distance: 0.95821803214252 entropy -11.09507910679889
epoch: 10, step: 52
	action: tensor([[-0.0026,  0.0221,  0.0106, -0.0091,  0.0023,  0.0078, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[8.4588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29978627917831047, distance: 0.9575732414079213 entropy -11.111595384658987
epoch: 10, step: 53
	action: tensor([[-0.0024,  0.0215,  0.0105, -0.0094, -0.0051,  0.0042,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[8.5184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29856063918145836, distance: 0.9584109334580088 entropy -11.104257722120343
epoch: 10, step: 54
	action: tensor([[-0.0023,  0.0213,  0.0081, -0.0097, -0.0074,  0.0193,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[8.5407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018524821622872, distance: 0.9561593856339408 entropy -11.0941765396929
epoch: 10, step: 55
	action: tensor([[-0.0025,  0.0214,  0.0211, -0.0093, -0.0012, -0.0034,  0.0228]],
       dtype=torch.float64)
	q_value: tensor([[8.5151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29664007386104296, distance: 0.9597221177580347 entropy -11.102227733566314
epoch: 10, step: 56
	action: tensor([[-0.0022,  0.0211,  0.0104, -0.0098, -0.0094, -0.0039, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[8.5578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29644589960516854, distance: 0.9598545822768569 entropy -11.096911497679022
epoch: 10, step: 57
	action: tensor([[-0.0024,  0.0217,  0.0115, -0.0095, -0.0064,  0.0128,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[8.5065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30069473845557537, distance: 0.9569518606708005 entropy -11.102905968355655
epoch: 10, step: 58
	action: tensor([[-0.0025,  0.0214,  0.0169, -0.0094, -0.0067,  0.0010,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[8.5220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977099902294751, distance: 0.9589918990251242 entropy -11.10050559275623
epoch: 10, step: 59
	action: tensor([[-2.3323e-03,  2.1336e-02,  1.3454e-04, -9.6128e-03, -1.2003e-02,
         -2.0725e-02, -8.9241e-05]], dtype=torch.float64)
	q_value: tensor([[8.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29289235676589365, distance: 0.9622755675686316 entropy -11.099894848666063
epoch: 10, step: 60
	action: tensor([[-0.0023,  0.0218,  0.0207, -0.0097, -0.0080,  0.0247,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[8.4969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30356623301583785, distance: 0.9549851167711689 entropy -11.097560270899779
epoch: 10, step: 61
	action: tensor([[-0.0025,  0.0211,  0.0142, -0.0093, -0.0040, -0.0072,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[8.5368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2956046346917368, distance: 0.9604282771348854 entropy -11.105213078207537
epoch: 10, step: 62
	action: tensor([[-0.0023,  0.0214,  0.0077, -0.0097, -0.0085,  0.0028, -0.0280]],
       dtype=torch.float64)
	q_value: tensor([[8.5320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29824000274430085, distance: 0.9586299590605183 entropy -11.09763893895502
epoch: 10, step: 63
	action: tensor([[-0.0026,  0.0218,  0.0068, -0.0092, -0.0049,  0.0065,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[8.4782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993245788960156, distance: 0.9578888871656889 entropy -11.111690055934263
epoch: 10, step: 64
	action: tensor([[-0.0024,  0.0213,  0.0074, -0.0097, -0.0099, -0.0037,  0.0001]],
       dtype=torch.float64)
	q_value: tensor([[8.5387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965452140159993, distance: 0.9597868328623954 entropy -11.091110731585166
epoch: 10, step: 65
	action: tensor([[-0.0024,  0.0217,  0.0014, -0.0095, -0.0084,  0.0057, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[8.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29911358239212993, distance: 0.9580331022950606 entropy -11.101274291430906
epoch: 10, step: 66
	action: tensor([[-0.0025,  0.0217,  0.0211, -0.0094,  0.0018, -0.0135, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[8.4950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2945245722844452, distance: 0.9611643162943119 entropy -11.10305604284225
epoch: 10, step: 67
	action: tensor([[-0.0023,  0.0215,  0.0171, -0.0096, -0.0147, -0.0018, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[8.5237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971968461990573, distance: 0.9593421895767604 entropy -11.107916400874837
epoch: 10, step: 68
	action: tensor([[-0.0025,  0.0217,  0.0115, -0.0093, -0.0060,  0.0060, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[8.4930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29912848954359006, distance: 0.9580229140390605 entropy -11.110486054073553
epoch: 10, step: 69
	action: tensor([[-0.0026,  0.0218,  0.0053, -0.0092, -0.0073,  0.0069, -0.0217]],
       dtype=torch.float64)
	q_value: tensor([[8.4848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993558513409561, distance: 0.957867510747428 entropy -11.112969117824415
epoch: 10, step: 70
	action: tensor([[-0.0026,  0.0219,  0.0102, -0.0092, -0.0054, -0.0163,  0.0307]],
       dtype=torch.float64)
	q_value: tensor([[8.4796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2940860891643844, distance: 0.9614629722390272 entropy -11.108933395728357
epoch: 10, step: 71
	action: tensor([[-0.0022,  0.0212,  0.0043, -0.0099, -0.0045,  0.0003, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[8.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974915208864827, distance: 0.9591410496865631 entropy -11.08820765581171
epoch: 10, step: 72
	action: tensor([[-2.5151e-03,  2.1869e-02,  3.9645e-03, -9.2688e-03, -5.4706e-06,
         -4.2172e-03,  2.2818e-02]], dtype=torch.float64)
	q_value: tensor([[8.4810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969140622030163, distance: 0.9595351734391994 entropy -11.109181481478746
epoch: 10, step: 73
	action: tensor([[-0.0023,  0.0214,  0.0114, -0.0098, -0.0059, -0.0140, -0.0203]],
       dtype=torch.float64)
	q_value: tensor([[8.5353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942850932312172, distance: 0.9613274397574061 entropy -11.090618399772875
epoch: 10, step: 74
	action: tensor([[-0.0024,  0.0218,  0.0088, -0.0095, -0.0032, -0.0082,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[8.4941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29595806430596217, distance: 0.9601872999778983 entropy -11.107530375451386
epoch: 10, step: 75
	action: tensor([[-0.0023,  0.0215,  0.0013, -0.0097, -0.0103, -0.0005,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[8.5271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975686022672134, distance: 0.9590884282971109 entropy -11.097385608915046
epoch: 10, step: 76
	action: tensor([[-0.0024,  0.0215,  0.0097, -0.0097, -0.0069, -0.0020, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[8.5182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29713274438717185, distance: 0.9593859387902262 entropy -11.09226054448431
epoch: 10, step: 77
	action: tensor([[-0.0024,  0.0216,  0.0180, -0.0095, -0.0040,  0.0003,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[8.5121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976969587271405, distance: 0.9590007963802228 entropy -11.101289661804618
epoch: 10, step: 78
	action: tensor([[-0.0023,  0.0213,  0.0088, -0.0096, -0.0067,  0.0155,  0.0353]],
       dtype=torch.float64)
	q_value: tensor([[8.5396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30103909236857573, distance: 0.9567162191613395 entropy -11.098913781750719
epoch: 10, step: 79
	action: tensor([[-0.0024,  0.0212,  0.0014, -0.0096, -0.0007,  0.0082,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[8.5415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29920803325759004, distance: 0.9579685482517282 entropy -11.09188402933655
epoch: 10, step: 80
	action: tensor([[-0.0024,  0.0212,  0.0122, -0.0097, -0.0103, -0.0173,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[8.5403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29334838560062626, distance: 0.9619652214814661 entropy -11.08849547906399
epoch: 10, step: 81
	action: tensor([[-0.0022,  0.0213,  0.0108, -0.0099, -0.0082, -0.0021,  0.0630]],
       dtype=torch.float64)
	q_value: tensor([[8.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969550534728913, distance: 0.9595072016533605 entropy -11.089002536181278
epoch: 10, step: 82
	action: tensor([[-2.2914e-03,  2.0788e-02,  9.3190e-03, -1.0051e-02, -7.9695e-04,
         -1.2974e-05, -1.6334e-02]], dtype=torch.float64)
	q_value: tensor([[8.5778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969239182690022, distance: 0.9595284478921356 entropy -11.078425662543028
epoch: 10, step: 83
	action: tensor([[-0.0025,  0.0218,  0.0197, -0.0094, -0.0066,  0.0156,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[8.4944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30137322715342074, distance: 0.9564875151107171 entropy -11.107537515013135
epoch: 10, step: 84
	action: tensor([[-0.0024,  0.0213, -0.0110, -0.0094, -0.0122,  0.0046,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[8.5336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985761451366509, distance: 0.9584003401266743 entropy -11.102958830985994
epoch: 10, step: 85
	action: tensor([[-0.0025,  0.0217,  0.0179, -0.0095, -0.0025, -0.0058, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[8.4966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962765518181656, distance: 0.9599700954043509 entropy -11.092073634309838
epoch: 10, step: 86
	action: tensor([[-0.0024,  0.0216,  0.0028, -0.0094, -0.0100, -0.0003, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[8.5052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29769403405628714, distance: 0.9590027932096488 entropy -11.109512178784351
epoch: 10, step: 87
	action: tensor([[-0.0026,  0.0220,  0.0084, -0.0092, -0.0087,  0.0229,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[8.4692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032040075467227, distance: 0.9552334354056872 entropy -11.10960231990977
epoch: 10, step: 88
	action: tensor([[-0.0025,  0.0213,  0.0189, -0.0094,  0.0007,  0.0006,  0.0524]],
       dtype=torch.float64)
	q_value: tensor([[8.5221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29749864093501877, distance: 0.9591361891415779 entropy -11.100373819125023
epoch: 10, step: 89
	action: tensor([[-0.0023,  0.0207,  0.0099, -0.0100,  0.0005, -0.0030, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[8.5870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29624580456067673, distance: 0.9599910667997038 entropy -11.0861486054084
epoch: 10, step: 90
	action: tensor([[-0.0024,  0.0216,  0.0133, -0.0095, -0.0008, -0.0049, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[8.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29655987652320304, distance: 0.9597768301195296 entropy -11.104177428530864
epoch: 10, step: 91
	action: tensor([[-0.0024,  0.0217,  0.0028, -0.0094, -0.0065, -0.0028,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[8.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29714852613704557, distance: 0.95937516799831 entropy -11.108479247987534
epoch: 10, step: 92
	action: tensor([[-0.0023,  0.0213,  0.0045, -0.0098, -0.0029,  0.0041,  0.0370]],
       dtype=torch.float64)
	q_value: tensor([[8.5408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29833051170087344, distance: 0.9585681377858282 entropy -11.087452608581485
epoch: 10, step: 93
	action: tensor([[-0.0023,  0.0212,  0.0109, -0.0098, -0.0130,  0.0137,  0.0158]],
       dtype=torch.float64)
	q_value: tensor([[8.5472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30042894048574387, distance: 0.95713370664775 entropy -11.087746002805702
epoch: 10, step: 94
	action: tensor([[-0.0024,  0.0214,  0.0009, -0.0095, -0.0025, -0.0061,  0.0376]],
       dtype=torch.float64)
	q_value: tensor([[8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29613209445371547, distance: 0.9600686196430844 entropy -11.099911212557931
epoch: 10, step: 95
	action: tensor([[-0.0023,  0.0212,  0.0066, -0.0099, -0.0159, -0.0028,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[8.5449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967436260514382, distance: 0.9596514675959374 entropy -11.084820530202933
epoch: 10, step: 96
	action: tensor([[-0.0023,  0.0214,  0.0225, -0.0098, -0.0101,  0.0048, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[8.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29849342862579953, distance: 0.958456848894197 entropy -11.089427548281835
epoch: 10, step: 97
	action: tensor([[-0.0024,  0.0214,  0.0040, -0.0095, -0.0042,  0.0145,  0.0176]],
       dtype=torch.float64)
	q_value: tensor([[8.5255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008749181842133, distance: 0.95682857085451 entropy -11.10709064762369
epoch: 10, step: 98
	action: tensor([[-0.0025,  0.0214,  0.0006, -0.0095, -0.0150, -0.0103, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[8.5212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2952314212809536, distance: 0.9606826777628543 entropy -11.097264873690829
epoch: 10, step: 99
	action: tensor([[-0.0024,  0.0219,  0.0157, -0.0094, -0.0068, -0.0017, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[8.4796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29749121437889603, distance: 0.9591412589253019 entropy -11.105105659254418
epoch: 10, step: 100
	action: tensor([[-0.0024,  0.0216,  0.0064, -0.0095, -0.0067, -0.0090,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[8.5107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2956587511508776, distance: 0.9603913830988912 entropy -11.106242087925994
epoch: 10, step: 101
	action: tensor([[-0.0023,  0.0216,  0.0011, -0.0097, -0.0055,  0.0018, -0.0019]],
       dtype=torch.float64)
	q_value: tensor([[8.5196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29813972975937697, distance: 0.9586984449063137 entropy -11.095795345724303
epoch: 10, step: 102
	action: tensor([[-0.0024,  0.0218,  0.0149, -0.0094, -0.0141, -0.0040,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[8.4968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29682773078089797, distance: 0.9595940819515821 entropy -11.101031132061705
epoch: 10, step: 103
	action: tensor([[-0.0023,  0.0214,  0.0084, -0.0097, -0.0058,  0.0026,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[8.5309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981666299110969, distance: 0.9586800727446283 entropy -11.096404187476915
epoch: 10, step: 104
	action: tensor([[-0.0024,  0.0214,  0.0091, -0.0096, -0.0067,  0.0078, -0.0130]],
       dtype=torch.float64)
	q_value: tensor([[8.5289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993248147428843, distance: 0.9578887259533023 entropy -11.0948609040548
epoch: 10, step: 105
	action: tensor([[-0.0025,  0.0217,  0.0040, -0.0093,  0.0104,  0.0265,  0.0549]],
       dtype=torch.float64)
	q_value: tensor([[8.4937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30399673763090984, distance: 0.954689906318903 entropy -11.10751863658449
epoch: 10, step: 106
	action: tensor([[-0.0025,  0.0208,  0.0102, -0.0097, -0.0110,  0.0164,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[8.5685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006925536045628, distance: 0.9569533555799079 entropy -11.084537042672508
epoch: 10, step: 107
	action: tensor([[-0.0025,  0.0215,  0.0091, -0.0094, -0.0120, -0.0059,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[8.5137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29619702328042397, distance: 0.9600243374942828 entropy -11.101193765798767
epoch: 10, step: 108
	action: tensor([[-0.0024,  0.0217,  0.0144, -0.0096, -0.0057,  0.0037,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[8.5070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985465223132693, distance: 0.958420577693492 entropy -11.100939991556743
epoch: 10, step: 109
	action: tensor([[-0.0023,  0.0213,  0.0085, -0.0096, -0.0150, -0.0084,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[8.5400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29554951979319644, distance: 0.9604658504028634 entropy -11.096047500862552
epoch: 10, step: 110
	action: tensor([[-0.0023,  0.0215,  0.0065, -0.0097, -0.0065, -0.0102,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[8.5266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29525617862910547, distance: 0.9606658040222312 entropy -11.092775902721765
epoch: 10, step: 111
	action: tensor([[-0.0023,  0.0216,  0.0120, -0.0096, -0.0113,  0.0299,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[8.5156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045840864565378, distance: 0.9542869956034074 entropy -11.098742702678871
epoch: 10, step: 112
	action: tensor([[-0.0026,  0.0212,  0.0014, -0.0093, -0.0018,  0.0031, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[8.5240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981280574433355, distance: 0.9587064167100953 entropy -11.10287381943191
epoch: 10, step: 113
	action: tensor([[-0.0025,  0.0219,  0.0110, -0.0092, -0.0134, -0.0039,  0.0515]],
       dtype=torch.float64)
	q_value: tensor([[8.4794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969342971279505, distance: 0.9595213655525213 entropy -11.108221593558152
epoch: 10, step: 114
	action: tensor([[-0.0023,  0.0209,  0.0056, -0.0100, -0.0108, -0.0086, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[8.5665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295171938517639, distance: 0.9607232179177169 entropy -11.082418842851245
epoch: 10, step: 115
	action: tensor([[-0.0024,  0.0218,  0.0125, -0.0095, -0.0076, -0.0165, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[8.4954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29405115689529104, distance: 0.96148676102351 entropy -11.10380742255206
epoch: 10, step: 116
	action: tensor([[-0.0024,  0.0217,  0.0077, -0.0095, -0.0015,  0.0002,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[8.5009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29783539681481375, distance: 0.9589062725267731 entropy -11.107007004346995
epoch: 10, step: 117
	action: tensor([[-0.0023,  0.0215,  0.0054, -0.0096, -0.0035, -0.0154,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[8.5271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29405975361006165, distance: 0.9614809067376897 entropy -11.095885510581324
epoch: 10, step: 118
	action: tensor([[-0.0022,  0.0213,  0.0018, -0.0099, -0.0100, -0.0131,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[8.5427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29454116837028654, distance: 0.961153010684868 entropy -11.087132938868795
epoch: 10, step: 119
	action: tensor([[-0.0023,  0.0215,  0.0129, -0.0098, -0.0114, -0.0222,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[8.5221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925361500139463, distance: 0.9625179110794947 entropy -11.090515508373768
epoch: 10, step: 120
	action: tensor([[-0.0022,  0.0216,  0.0137, -0.0098, -0.0072, -0.0086,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2957124911750746, distance: 0.9603547442957865 entropy -11.097619855003385
epoch: 10, step: 121
	action: tensor([[-0.0023,  0.0214,  0.0106, -0.0097, -0.0088, -0.0115,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[8.5352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949280839907904, distance: 0.9608893977695397 entropy -11.09569753948407
epoch: 10, step: 122
	action: tensor([[-2.2976e-03,  2.1606e-02,  1.9835e-02, -9.6421e-03, -6.9899e-03,
          6.7099e-05,  1.9042e-02]], dtype=torch.float64)
	q_value: tensor([[8.5158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976721110124563, distance: 0.9590177611133966 entropy -11.099474227578526
epoch: 10, step: 123
	action: tensor([[-0.0023,  0.0212,  0.0190, -0.0097, -0.0026,  0.0093, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[8.5460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995550357645669, distance: 0.95773134615497 entropy -11.097433066162596
epoch: 10, step: 124
	action: tensor([[-0.0024,  0.0214,  0.0127, -0.0094, -0.0009,  0.0161,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[8.5222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30127541105476496, distance: 0.9565544726094355 entropy -11.106310991637383
epoch: 10, step: 125
	action: tensor([[-2.3892e-03,  2.1103e-02,  1.8910e-02, -9.5903e-03, -2.8646e-04,
          6.9720e-06, -3.6048e-03]], dtype=torch.float64)
	q_value: tensor([[8.5481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29722331068268193, distance: 0.9593241270975738 entropy -11.094081906710025
epoch: 10, step: 126
	action: tensor([[-0.0024,  0.0215,  0.0131, -0.0095, -0.0087,  0.0233, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[8.5229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029926382336898, distance: 0.9553783068800378 entropy -11.105879332381543
epoch: 10, step: 127
	action: tensor([[-0.0026,  0.0215,  0.0090, -0.0092, -0.0077, -0.0141,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[8.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29434486678711114, distance: 0.9612867270165387 entropy -11.109016209038256
LOSS epoch 10 actor 33.87183468750265 critic 10.339675651027449
epoch: 11, step: 0
	action: tensor([[-0.0008,  0.0220,  0.0130, -0.0085, -0.0080, -0.0132, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29707499302411433, distance: 0.9594253521411519 entropy -11.099103401330666
epoch: 11, step: 1
	action: tensor([[-8.8495e-04,  2.2216e-02,  2.3555e-02, -8.3038e-03, -3.5363e-05,
         -2.5497e-03, -3.6655e-03]], dtype=torch.float64)
	q_value: tensor([[7.0016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29958097134748585, distance: 0.9577136148898827 entropy -11.106896077242634
epoch: 11, step: 2
	action: tensor([[-0.0009,  0.0218,  0.0106, -0.0084, -0.0135, -0.0059, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[7.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29860439770362757, distance: 0.9583810382863689 entropy -11.108107080681929
epoch: 11, step: 3
	action: tensor([[-0.0009,  0.0222,  0.0053, -0.0083, -0.0030, -0.0011, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[7.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29996090584803436, distance: 0.9574538291141268 entropy -11.105600315804296
epoch: 11, step: 4
	action: tensor([[-0.0010,  0.0222,  0.0149, -0.0083, -0.0042, -0.0052, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[7.0003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989541477836696, distance: 0.9581420607106769 entropy -11.103853279447462
epoch: 11, step: 5
	action: tensor([[-0.0009,  0.0221,  0.0092, -0.0083, -0.0043,  0.0128, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[7.0068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30311714044849614, distance: 0.9552929763413182 entropy -11.108534797629298
epoch: 11, step: 6
	action: tensor([[-0.0011,  0.0222,  0.0101, -0.0080, -0.0082, -0.0027,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[6.9933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299532933677335, distance: 0.9577464563319691 entropy -11.11176952230639
epoch: 11, step: 7
	action: tensor([[-0.0008,  0.0219,  0.0093, -0.0085, -0.0010, -0.0011, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.0238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996745815964009, distance: 0.9576496140547549 entropy -11.095852033029562
epoch: 11, step: 8
	action: tensor([[-0.0009,  0.0221,  0.0016, -0.0083, -0.0019, -0.0112, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[7.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29763468603980137, distance: 0.9590433123807796 entropy -11.104813311205648
epoch: 11, step: 9
	action: tensor([[-0.0009,  0.0222,  0.0098, -0.0083, -0.0130, -0.0066,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[7.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987225889930549, distance: 0.9583002870936383 entropy -11.102684195668827
epoch: 11, step: 10
	action: tensor([[-0.0009,  0.0221,  0.0157, -0.0084, -0.0033,  0.0029,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[7.0112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007589591152193, distance: 0.956907918851585 entropy -11.100917336356686
epoch: 11, step: 11
	action: tensor([[-0.0009,  0.0219,  0.0084, -0.0083, -0.0127, -0.0122,  0.0451]],
       dtype=torch.float64)
	q_value: tensor([[7.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972421986479459, distance: 0.9593112355184892 entropy -11.105255706365991
epoch: 11, step: 12
	action: tensor([[-0.0008,  0.0216,  0.0030, -0.0088, -0.0138, -0.0020, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[7.0358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29927855440569184, distance: 0.9579203465422788 entropy -11.08430993425854
epoch: 11, step: 13
	action: tensor([[-0.0010,  0.0223,  0.0070, -0.0083, -0.0127, -0.0061, -0.0002]],
       dtype=torch.float64)
	q_value: tensor([[6.9942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988987845744856, distance: 0.9581798933088737 entropy -11.102760112038634
epoch: 11, step: 14
	action: tensor([[-0.0009,  0.0222, -0.0103, -0.0083, -0.0088,  0.0051,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[7.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014390254521443, distance: 0.9564424719377327 entropy -11.102031697471881
epoch: 11, step: 15
	action: tensor([[-0.0010,  0.0221,  0.0102, -0.0084, -0.0115, -0.0069, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[7.0009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984206574748829, distance: 0.9585065606161547 entropy -11.090615485785367
epoch: 11, step: 16
	action: tensor([[-0.0009,  0.0221,  0.0003, -0.0083, -0.0040,  0.0128,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[7.0079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30314102926868036, distance: 0.9552766027016185 entropy -11.10321446884402
epoch: 11, step: 17
	action: tensor([[-0.0010,  0.0221, -0.0084, -0.0082, -0.0162,  0.0019,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[7.0050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300529640840989, distance: 0.95706481645302 entropy -11.099545782561389
epoch: 11, step: 18
	action: tensor([[-0.0010,  0.0221,  0.0091, -0.0085, -0.0110, -0.0136, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[7.0033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29689743056679163, distance: 0.9595465223687657 entropy -11.088596089989556
epoch: 11, step: 19
	action: tensor([[-0.0010,  0.0223,  0.0107, -0.0082, -0.0128,  0.0018,  0.0375]],
       dtype=torch.float64)
	q_value: tensor([[6.9898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30070528296483356, distance: 0.9569446459204279 entropy -11.11097372568615
epoch: 11, step: 20
	action: tensor([[-0.0009,  0.0217, -0.0005, -0.0086,  0.0003,  0.0251,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[7.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30552778413924897, distance: 0.9536392795124392 entropy -11.089697679426928
epoch: 11, step: 21
	action: tensor([[-0.0011,  0.0219,  0.0098, -0.0082, -0.0086, -0.0047,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[7.0088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987637408835475, distance: 0.9582721695139127 entropy -11.097141385119373
epoch: 11, step: 22
	action: tensor([[-0.0008,  0.0218,  0.0125, -0.0086, -0.0177, -0.0169, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[7.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29606932576194533, distance: 0.9601114266153522 entropy -11.09352197952993
epoch: 11, step: 23
	action: tensor([[-0.0009,  0.0223,  0.0102, -0.0082, -0.0116,  0.0010, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[6.9917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30055865055289266, distance: 0.9570449696774472 entropy -11.109042890765048
epoch: 11, step: 24
	action: tensor([[-0.0010,  0.0222,  0.0178, -0.0081,  0.0007, -0.0082,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[6.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982424572567097, distance: 0.9586282825819523 entropy -11.109823757330842
epoch: 11, step: 25
	action: tensor([[-0.0008,  0.0219,  0.0116, -0.0084, -0.0087, -0.0066,  0.0327]],
       dtype=torch.float64)
	q_value: tensor([[7.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984862731904868, distance: 0.9584617370583097 entropy -11.103884999762002
epoch: 11, step: 26
	action: tensor([[-0.0008,  0.0217,  0.0079, -0.0086, -0.0093,  0.0111, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[7.0326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30236618722502284, distance: 0.9558075443019893 entropy -11.09203357277632
epoch: 11, step: 27
	action: tensor([[-0.0011,  0.0223,  0.0184, -0.0080, -0.0046,  0.0004,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[6.9892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002401747101302, distance: 0.9572628299854246 entropy -11.11143632222686
epoch: 11, step: 28
	action: tensor([[-0.0008,  0.0217,  0.0078, -0.0085,  0.0070,  0.0020,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[7.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30031712720850434, distance: 0.9572101935028658 entropy -11.098231938286833
epoch: 11, step: 29
	action: tensor([[-0.0008,  0.0214,  0.0185, -0.0088, -0.0161, -0.0141,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[7.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2961852661010952, distance: 0.9600323561667884 entropy -11.083586848058358
epoch: 11, step: 30
	action: tensor([[-0.0008,  0.0220,  0.0040, -0.0085,  0.0047, -0.0118,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[7.0187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973901514948556, distance: 0.9592102474545905 entropy -11.100161855159675
epoch: 11, step: 31
	action: tensor([[-0.0008,  0.0219,  0.0007, -0.0086, -0.0049, -0.0067, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.0213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985554085394091, distance: 0.9584145068924804 entropy -11.095215897777342
epoch: 11, step: 32
	action: tensor([[-0.0010,  0.0223,  0.0107, -0.0082, -0.0017,  0.0065,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[6.9917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30180535705040856, distance: 0.9561916555744054 entropy -11.105732145893697
epoch: 11, step: 33
	action: tensor([[-0.0009,  0.0220,  0.0095, -0.0083, -0.0052,  0.0122,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[7.0144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30281809840364027, distance: 0.9554979190524276 entropy -11.103302773899681
epoch: 11, step: 34
	action: tensor([[-0.0010,  0.0220,  0.0088, -0.0082, -0.0073, -0.0118, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[7.0078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29733364124565087, distance: 0.9592488208679606 entropy -11.103161199702459
epoch: 11, step: 35
	action: tensor([[-0.0009,  0.0222,  0.0017, -0.0083,  0.0002, -0.0039,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.0046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993343940181368, distance: 0.9578821780468527 entropy -11.104039583855737
epoch: 11, step: 36
	action: tensor([[-0.0009,  0.0221,  0.0139, -0.0084, -0.0031,  0.0041, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[7.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010418362753092, distance: 0.9567143412718728 entropy -11.098588080851838
epoch: 11, step: 37
	action: tensor([[-0.0010,  0.0222,  0.0183, -0.0081,  0.0034, -0.0007,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[6.9959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29997675777029054, distance: 0.9574429885984242 entropy -11.111737110819377
epoch: 11, step: 38
	action: tensor([[-0.0008,  0.0218,  0.0075, -0.0084,  0.0008,  0.0116,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[7.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30259630705146534, distance: 0.9556498911010988 entropy -11.101446887735092
epoch: 11, step: 39
	action: tensor([[-0.0009,  0.0218,  0.0130, -0.0083, -0.0160,  0.0125, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.0197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027187286289569, distance: 0.9555660104857499 entropy -11.098571789495955
epoch: 11, step: 40
	action: tensor([[-0.0011,  0.0222,  0.0107, -0.0081, -0.0104,  0.0052, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[6.9956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013427472124174, distance: 0.9565083798744143 entropy -11.109474612000492
epoch: 11, step: 41
	action: tensor([[-0.0010,  0.0223,  0.0007, -0.0081, -0.0022, -0.0068,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[6.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987147164326748, distance: 0.9583056660318311 entropy -11.1099604072577
epoch: 11, step: 42
	action: tensor([[-0.0009,  0.0221,  0.0038, -0.0085, -0.0082, -0.0003,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[7.0116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000305020313928, distance: 0.9574062340661766 entropy -11.096263264767556
epoch: 11, step: 43
	action: tensor([[-0.0009,  0.0221,  0.0064, -0.0083, -0.0039, -0.0008,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[7.0050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29996823145816576, distance: 0.9574488194283256 entropy -11.100893079536641
epoch: 11, step: 44
	action: tensor([[-0.0009,  0.0219,  0.0043, -0.0085, -0.0135,  0.0108,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[7.0222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30238699884264464, distance: 0.9557932875034207 entropy -11.094937412593655
epoch: 11, step: 45
	action: tensor([[-0.0010,  0.0219,  0.0104, -0.0084, -0.0081, -0.0009,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[7.0123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29970187405386595, distance: 0.9576309535397148 entropy -11.095029106998078
epoch: 11, step: 46
	action: tensor([[-0.0009,  0.0219,  0.0050, -0.0085, -0.0023,  0.0105,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[7.0229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30235427449506824, distance: 0.9558157049074159 entropy -11.096531662607445
epoch: 11, step: 47
	action: tensor([[-0.0010,  0.0219,  0.0138, -0.0084,  0.0041, -0.0140, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[7.0171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966013899875545, distance: 0.9597485091240726 entropy -11.095589314213303
epoch: 11, step: 48
	action: tensor([[-0.0009,  0.0221,  0.0194, -0.0083, -0.0115, -0.0091,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[7.0087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980255601196029, distance: 0.9587764161279054 entropy -11.108352298305968
epoch: 11, step: 49
	action: tensor([[-0.0008,  0.0219,  0.0149, -0.0085, -0.0151,  0.0032,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[7.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007015445700576, distance: 0.9569472038062498 entropy -11.101523164288837
epoch: 11, step: 50
	action: tensor([[-0.0009,  0.0218,  0.0045, -0.0084, -0.0051,  0.0211, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[7.0238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047740107116117, distance: 0.9541566745828187 entropy -11.098286458869826
epoch: 11, step: 51
	action: tensor([[-0.0011,  0.0222,  0.0070, -0.0079, -0.0089, -0.0140,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[6.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969869992302181, distance: 0.9594854018149725 entropy -11.108997212314705
epoch: 11, step: 52
	action: tensor([[-0.0008,  0.0221, -0.0020, -0.0085, -0.0163, -0.0074,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[7.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298516385630452, distance: 0.9584411658776323 entropy -11.096742495616965
epoch: 11, step: 53
	action: tensor([[-0.0009,  0.0222,  0.0037, -0.0084, -0.0128,  0.0123, -0.0111]],
       dtype=torch.float64)
	q_value: tensor([[7.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30297057910062675, distance: 0.9553934248340292 entropy -11.09617852035434
epoch: 11, step: 54
	action: tensor([[-0.0011,  0.0223,  0.0083, -0.0080, -0.0095, -0.0080,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[6.9869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984062769982134, distance: 0.9585163839602622 entropy -11.107796000082446
epoch: 11, step: 55
	action: tensor([[-8.0983e-04,  2.1631e-02,  5.1383e-03, -8.7528e-03, -9.4147e-05,
         -2.7561e-03, -2.7293e-02]], dtype=torch.float64)
	q_value: tensor([[7.0361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29908875158818227, distance: 0.9580500726064044 entropy -11.08520069796141
epoch: 11, step: 56
	action: tensor([[-0.0011,  0.0224,  0.0076, -0.0080, -0.0094, -0.0003,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[6.9829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30030518093788283, distance: 0.9572183650929162 entropy -11.111428697167872
epoch: 11, step: 57
	action: tensor([[-0.0009,  0.0221,  0.0013, -0.0083, -0.0074, -0.0006,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[7.0056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30004478520397393, distance: 0.9573964658775114 entropy -11.103339444042557
epoch: 11, step: 58
	action: tensor([[-0.0009,  0.0221,  0.0174, -0.0083, -0.0028, -0.0049,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[7.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989336490223651, distance: 0.9581560687686116 entropy -11.099156660494264
epoch: 11, step: 59
	action: tensor([[-0.0008,  0.0218,  0.0063, -0.0085, -0.0124,  0.0017,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[7.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003343963019417, distance: 0.957198380826787 entropy -11.100253391714729
epoch: 11, step: 60
	action: tensor([[-0.0009,  0.0219,  0.0095, -0.0085, -0.0054, -0.0138,  0.0392]],
       dtype=torch.float64)
	q_value: tensor([[7.0159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29676438552465934, distance: 0.9596373034826056 entropy -11.094429873750482
epoch: 11, step: 61
	action: tensor([[-0.0008,  0.0216,  0.0107, -0.0088, -0.0006, -0.0179, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[7.0396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29556779792489785, distance: 0.9604533898852828 entropy -11.087124734502158
epoch: 11, step: 62
	action: tensor([[-0.0008,  0.0222,  0.0096, -0.0084, -0.0081, -0.0380, -0.0226]],
       dtype=torch.float64)
	q_value: tensor([[7.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29166646748997005, distance: 0.9631093391135715 entropy -11.106018783055548
epoch: 11, step: 63
	action: tensor([[-0.0008,  0.0223,  0.0198, -0.0084, -0.0017, -0.0143, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[6.9963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29701024693875244, distance: 0.9594695372282941 entropy -11.105713968809651
epoch: 11, step: 64
	action: tensor([[-0.0009,  0.0221,  0.0082, -0.0083, -0.0089,  0.0093, -0.0264]],
       dtype=torch.float64)
	q_value: tensor([[7.0106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30230180530896544, distance: 0.9558516471681897 entropy -11.109171464699058
epoch: 11, step: 65
	action: tensor([[-0.0011,  0.0224,  0.0178, -0.0079, -0.0100,  0.0083,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[6.9816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021366117343742, distance: 0.9559647986756598 entropy -11.113410213217966
epoch: 11, step: 66
	action: tensor([[-0.0009,  0.0215,  0.0063, -0.0086, -0.0115, -0.0133,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[7.0417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296564154959966, distance: 0.9597739113560264 entropy -11.093130084778148
epoch: 11, step: 67
	action: tensor([[-0.0008,  0.0215, -0.0025, -0.0089, -0.0089,  0.0051, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[7.0404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30079196645841344, distance: 0.9568853333917913 entropy -11.080194639661485
epoch: 11, step: 68
	action: tensor([[-0.0010,  0.0223,  0.0184, -0.0082, -0.0071,  0.0016, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[6.9914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005577979817784, distance: 0.9570455529634141 entropy -11.102819060260646
epoch: 11, step: 69
	action: tensor([[-0.0009,  0.0220,  0.0035, -0.0083, -0.0135,  0.0233,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[7.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30540572400487476, distance: 0.9537230814426899 entropy -11.107198792656508
epoch: 11, step: 70
	action: tensor([[-0.0011,  0.0221,  0.0025, -0.0081,  0.0024,  0.0121, -0.0061]],
       dtype=torch.float64)
	q_value: tensor([[6.9989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028450693771274, distance: 0.9554794368182882 entropy -11.102086049679555
epoch: 11, step: 71
	action: tensor([[-0.0011,  0.0222,  0.0112, -0.0081, -0.0082,  0.0062, -0.0177]],
       dtype=torch.float64)
	q_value: tensor([[6.9955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301562513462837, distance: 0.9563579307146688 entropy -11.105150480578146
epoch: 11, step: 72
	action: tensor([[-0.0011,  0.0223,  0.0107, -0.0080,  0.0067,  0.0115, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[6.9914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029019670364195, distance: 0.9554404457344258 entropy -11.110884948525584
epoch: 11, step: 73
	action: tensor([[-0.0010,  0.0221,  0.0147, -0.0081, -0.0095,  0.0179,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.0045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041708007593692, distance: 0.9545705198793373 entropy -11.109613594394233
epoch: 11, step: 74
	action: tensor([[-0.0010,  0.0219,  0.0139, -0.0081, -0.0013,  0.0131, -0.0226]],
       dtype=torch.float64)
	q_value: tensor([[7.0099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030117692839196, distance: 0.9553651954576925 entropy -11.107621885853598
epoch: 11, step: 75
	action: tensor([[-0.0011,  0.0221,  0.0094, -0.0080, -0.0145,  0.0059, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[6.9946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014581133061375, distance: 0.9564294046754348 entropy -11.114894184185514
epoch: 11, step: 76
	action: tensor([[-0.0010,  0.0221,  0.0123, -0.0082, -0.0108, -0.0006,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[7.0016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999771661022599, distance: 0.9574427093543831 entropy -11.1056317850426
epoch: 11, step: 77
	action: tensor([[-0.0009,  0.0220,  0.0152, -0.0084, -0.0015, -0.0150,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[7.0179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296489955084247, distance: 0.9598245294952376 entropy -11.100013066068543
epoch: 11, step: 78
	action: tensor([[-0.0007,  0.0217, -0.0004, -0.0087,  0.0014, -0.0073,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[7.0372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982380974461927, distance: 0.9586312604132705 entropy -11.094149951700834
epoch: 11, step: 79
	action: tensor([[-0.0008,  0.0220,  0.0113, -0.0085, -0.0021,  0.0016, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[7.0130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004091314638305, distance: 0.9571472576288093 entropy -11.095200546873015
epoch: 11, step: 80
	action: tensor([[-0.0011,  0.0223, -0.0008, -0.0080, -0.0032, -0.0039, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[6.9901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29939980780182107, distance: 0.957837463307347 entropy -11.11390444683308
epoch: 11, step: 81
	action: tensor([[-0.0010,  0.0223,  0.0163, -0.0082, -0.0034,  0.0002, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[6.9917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002925678829075, distance: 0.9572269927067869 entropy -11.104976143190848
epoch: 11, step: 82
	action: tensor([[-0.0009,  0.0220,  0.0167, -0.0083, -0.0143,  0.0043,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30104147192009656, distance: 0.956714590631441 entropy -11.108209070112355
epoch: 11, step: 83
	action: tensor([[-0.0009,  0.0218,  0.0090, -0.0084, -0.0133, -0.0165,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[7.0253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29614544022649225, distance: 0.9600595178511981 entropy -11.099193915596567
epoch: 11, step: 84
	action: tensor([[-0.0008,  0.0222,  0.0060, -0.0084,  0.0032, -0.0038, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[7.0052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993281596157409, distance: 0.9578864395730435 entropy -11.100711188988935
epoch: 11, step: 85
	action: tensor([[-0.0009,  0.0221,  0.0074, -0.0083, -0.0190,  0.0041, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[7.0072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011028417756524, distance: 0.9566725890514329 entropy -11.102457389645346
epoch: 11, step: 86
	action: tensor([[-0.0010,  0.0222,  0.0153, -0.0082, -0.0054,  0.0003,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[6.9986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30021006313730314, distance: 0.957283425894402 entropy -11.104895414556227
epoch: 11, step: 87
	action: tensor([[-0.0008,  0.0218,  0.0142, -0.0085, -0.0057, -0.0041, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989125257938495, distance: 0.958170503348958 entropy -11.097149389319906
epoch: 11, step: 88
	action: tensor([[-0.0009,  0.0221,  0.0088, -0.0083, -0.0088, -0.0207,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[7.0036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29544509353731974, distance: 0.9605370364824836 entropy -11.107537441898339
epoch: 11, step: 89
	action: tensor([[-0.0008,  0.0219,  0.0106, -0.0086, -0.0087, -0.0062,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[7.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29861153558259357, distance: 0.9583761617051382 entropy -11.094471347812496
epoch: 11, step: 90
	action: tensor([[-0.0008,  0.0219,  0.0052, -0.0085, -0.0017, -0.0146,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.0219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296657487468586, distance: 0.9597102374055092 entropy -11.097057068109404
epoch: 11, step: 91
	action: tensor([[-0.0008,  0.0222,  0.0069, -0.0084, -0.0142,  0.0135,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.0068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032859982345276, distance: 0.9551772334819416 entropy -11.100513032745795
epoch: 11, step: 92
	action: tensor([[-0.0010,  0.0220,  0.0069, -0.0082, -0.0041,  0.0126, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[7.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029330028911006, distance: 0.9554191766736313 entropy -11.101478852789787
epoch: 11, step: 93
	action: tensor([[-0.0010,  0.0221,  0.0060, -0.0081, -0.0149, -0.0032,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[7.0028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29935292240290123, distance: 0.9578695128562568 entropy -11.105646605690348
epoch: 11, step: 94
	action: tensor([[-0.0009,  0.0217,  0.0189, -0.0087, -0.0124, -0.0057,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[7.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29835347905932186, distance: 0.9585524495176725 entropy -11.086120034715096
epoch: 11, step: 95
	action: tensor([[-0.0008,  0.0216, -0.0031, -0.0086, -0.0096,  0.0037, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[7.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30066306141754295, distance: 0.9569735343653065 entropy -11.093184507996785
epoch: 11, step: 96
	action: tensor([[-0.0011,  0.0224,  0.0044, -0.0080, -0.0104, -0.0002,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[6.9794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30032433675865355, distance: 0.9572052619310503 entropy -11.108052749328921
epoch: 11, step: 97
	action: tensor([[-0.0009,  0.0218,  0.0155, -0.0085, -0.0023,  0.0137, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[7.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030377251639774, distance: 0.9553474063658542 entropy -11.09233719528081
epoch: 11, step: 98
	action: tensor([[-0.0011,  0.0221,  0.0159, -0.0080, -0.0100, -0.0013,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[7.0021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997276686700595, distance: 0.9576133168008354 entropy -11.112729658611363
epoch: 11, step: 99
	action: tensor([[-0.0009,  0.0220,  0.0183, -0.0084, -0.0156, -0.0116,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[7.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29732718456226737, distance: 0.9592532280461452 entropy -11.101431452650896
epoch: 11, step: 100
	action: tensor([[-0.0008,  0.0221,  0.0062, -0.0084, -0.0104,  0.0002,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[7.0133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30020213099969373, distance: 0.9572888512956174 entropy -11.103397203158753
epoch: 11, step: 101
	action: tensor([[-0.0009,  0.0221,  0.0107, -0.0083, -0.0002, -0.0095,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[7.0056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29789619596165895, distance: 0.9588647566591961 entropy -11.100193333964048
epoch: 11, step: 102
	action: tensor([[-0.0008,  0.0216,  0.0088, -0.0087, -0.0143, -0.0089,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[7.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29775816243070163, distance: 0.9589590083800112 entropy -11.090750261312635
epoch: 11, step: 103
	action: tensor([[-0.0008,  0.0220,  0.0160, -0.0085, -0.0032, -0.0033,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[7.0148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992549179302415, distance: 0.9579365025126456 entropy -11.097737340079876
epoch: 11, step: 104
	action: tensor([[-0.0008,  0.0218, -0.0035, -0.0085, -0.0066, -0.0252,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.0294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29429245201718124, distance: 0.9613224276616971 entropy -11.09713616051128
epoch: 11, step: 105
	action: tensor([[-0.0008,  0.0222,  0.0002, -0.0086, -0.0041, -0.0022,  0.0212]],
       dtype=torch.float64)
	q_value: tensor([[7.0056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997536952811225, distance: 0.9575955211092073 entropy -11.094378664194881
epoch: 11, step: 106
	action: tensor([[-0.0009,  0.0220,  0.0089, -0.0085, -0.0051,  0.0131, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[7.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030064348148026, distance: 0.9553688514422232 entropy -11.093792013538549
epoch: 11, step: 107
	action: tensor([[-0.0010,  0.0221, -0.0083, -0.0081, -0.0132, -0.0234,  0.0372]],
       dtype=torch.float64)
	q_value: tensor([[6.9992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29504018020263, distance: 0.9608130109921448 entropy -11.108763057993126
epoch: 11, step: 108
	action: tensor([[-0.0008,  0.0220,  0.0036, -0.0088, -0.0142, -0.0054,  0.0193]],
       dtype=torch.float64)
	q_value: tensor([[7.0177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987306433684751, distance: 0.9582947838988122 entropy -11.081985873230925
epoch: 11, step: 109
	action: tensor([[-0.0009,  0.0220,  0.0045, -0.0085, -0.0085, -0.0184, -0.0307]],
       dtype=torch.float64)
	q_value: tensor([[7.0137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959000121643297, distance: 0.9602268855319092 entropy -11.094876489002854
epoch: 11, step: 110
	action: tensor([[-0.0009,  0.0225,  0.0034, -0.0082, -0.0025, -0.0137,  0.0369]],
       dtype=torch.float64)
	q_value: tensor([[6.9809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973351087526922, distance: 0.9592478191798078 entropy -11.110186486979504
epoch: 11, step: 111
	action: tensor([[-0.0008,  0.0217,  0.0170, -0.0088, -0.0098,  0.0140,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[7.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30298305515156787, distance: 0.9553848745562241 entropy -11.085339601427604
epoch: 11, step: 112
	action: tensor([[-0.0010,  0.0219,  0.0107, -0.0082, -0.0094,  0.0130,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[7.0123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029563103578574, distance: 0.9554032036129794 entropy -11.106082631632239
epoch: 11, step: 113
	action: tensor([[-0.0010,  0.0219,  0.0105, -0.0083, -0.0087,  0.0016, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[7.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30029962918089237, distance: 0.9572221626294019 entropy -11.101932221586534
epoch: 11, step: 114
	action: tensor([[-1.0836e-03,  2.2347e-02,  3.7190e-03, -8.0132e-03, -2.4163e-03,
         -7.0687e-06, -8.8547e-03]], dtype=torch.float64)
	q_value: tensor([[6.9848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30030065466327016, distance: 0.9572214611757083 entropy -11.112508261152623
epoch: 11, step: 115
	action: tensor([[-0.0010,  0.0222,  0.0195, -0.0082, -0.0082,  0.0305, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[6.9978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.307292035930241, distance: 0.952427186597223 entropy -11.105759522871194
epoch: 11, step: 116
	action: tensor([[-0.0011,  0.0219,  0.0060, -0.0079, -0.0052,  0.0198, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[7.0063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30448100408630563, distance: 0.9543577205597679 entropy -11.112526000109211
epoch: 11, step: 117
	action: tensor([[-0.0011,  0.0221,  0.0032, -0.0080, -0.0158, -0.0107,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[6.9954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29774147099225046, distance: 0.9589704049597201 entropy -11.108442515409555
epoch: 11, step: 118
	action: tensor([[-0.0009,  0.0222,  0.0014, -0.0084, -0.0066,  0.0083, -0.0187]],
       dtype=torch.float64)
	q_value: tensor([[7.0008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30215503551482337, distance: 0.9559521797289511 entropy -11.100133728016473
epoch: 11, step: 119
	action: tensor([[-0.0011,  0.0223,  0.0005, -0.0080, -0.0033, -0.0028,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[6.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29966725811382844, distance: 0.9576546212355168 entropy -11.109363754267534
epoch: 11, step: 120
	action: tensor([[-0.0009,  0.0220,  0.0081, -0.0085, -0.0093,  0.0231, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[7.0128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30533643240842656, distance: 0.9537706511888826 entropy -11.09349934445035
epoch: 11, step: 121
	action: tensor([[-0.0011,  0.0221,  0.0143, -0.0080, -0.0137,  0.0027,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[6.9941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30068741893953754, distance: 0.9569568687885343 entropy -11.110587162947924
epoch: 11, step: 122
	action: tensor([[-0.0009,  0.0220,  0.0020, -0.0084,  0.0029,  0.0247,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[7.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30574582294489183, distance: 0.9534895638809425 entropy -11.100571622561068
epoch: 11, step: 123
	action: tensor([[-0.0011,  0.0217,  0.0127, -0.0083,  0.0015, -0.0209,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[7.0197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29481497767965237, distance: 0.9609664667141455 entropy -11.094326572717275
epoch: 11, step: 124
	action: tensor([[-0.0007,  0.0215,  0.0055, -0.0089, -0.0084,  0.0075, -0.0202]],
       dtype=torch.float64)
	q_value: tensor([[7.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013820779402916, distance: 0.956481456300575 entropy -11.084792371044944
epoch: 11, step: 125
	action: tensor([[-0.0011,  0.0224,  0.0040, -0.0080, -0.0044, -0.0075, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[6.9829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986093465675439, distance: 0.9583776572374365 entropy -11.11084737959182
epoch: 11, step: 126
	action: tensor([[-0.0009,  0.0222,  0.0093, -0.0083, -0.0081,  0.0045,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[7.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30122051182016296, distance: 0.956592050417456 entropy -11.101469315489094
epoch: 11, step: 127
	action: tensor([[-0.0009,  0.0220,  0.0079, -0.0083, -0.0010, -0.0142,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[7.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296776138227116, distance: 0.9596292845636372 entropy -11.10149790917362
LOSS epoch 11 actor 22.579856054070667 critic 12.127847091229745
epoch: 12, step: 0
	action: tensor([[-0.0005,  0.0218,  0.0071, -0.0081, -0.0061, -0.0215, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[5.9509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29562171837988827, distance: 0.960416630439385 entropy -11.093046216845199
epoch: 12, step: 1
	action: tensor([[-0.0005,  0.0222,  0.0172, -0.0078, -0.0037, -0.0276, -0.0238]],
       dtype=torch.float64)
	q_value: tensor([[5.9336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2944224302210201, distance: 0.9612338947299215 entropy -11.102804800714836
epoch: 12, step: 2
	action: tensor([[-0.0005,  0.0221,  0.0128, -0.0078, -0.0002,  0.0220,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[5.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30593393443700256, distance: 0.9533603788490626 entropy -11.105739213795946
epoch: 12, step: 3
	action: tensor([[-0.0007,  0.0217,  0.0091, -0.0076, -0.0027,  0.0043,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[5.9424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013671548748762, distance: 0.956491671869534 entropy -11.103089005349714
epoch: 12, step: 4
	action: tensor([[-0.0006,  0.0218,  0.0119, -0.0079,  0.0009,  0.0042, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[5.9450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014058097764364, distance: 0.9564652104720235 entropy -11.094807857821214
epoch: 12, step: 5
	action: tensor([[-0.0007,  0.0221,  0.0077, -0.0076, -0.0040, -0.0037, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[5.9312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29981584271868655, distance: 0.9575530264698536 entropy -11.106992501388076
epoch: 12, step: 6
	action: tensor([[-0.0007,  0.0222,  0.0098, -0.0077, -0.0034,  0.0153, -0.0293]],
       dtype=torch.float64)
	q_value: tensor([[5.9307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042812794311276, distance: 0.9544947370078993 entropy -11.1037586051901
epoch: 12, step: 7
	action: tensor([[-0.0009,  0.0222,  0.0073, -0.0073, -0.0127, -0.0007, -0.0394]],
       dtype=torch.float64)
	q_value: tensor([[5.9219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30058144138883014, distance: 0.9570293772102662 entropy -11.11311614986927
epoch: 12, step: 8
	action: tensor([[-0.0008,  0.0224,  0.0120, -0.0074, -0.0163, -0.0058,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[5.9139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995512822264953, distance: 0.9577339122925238 entropy -11.112358363260187
epoch: 12, step: 9
	action: tensor([[-0.0005,  0.0217,  0.0029, -0.0081, -0.0056, -0.0062, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[5.9554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29896919158992863, distance: 0.9581317802270155 entropy -11.089554572664284
epoch: 12, step: 10
	action: tensor([[-0.0006,  0.0222,  0.0167, -0.0077, -0.0046, -0.0080, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[5.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988779002401407, distance: 0.9581941642870115 entropy -11.10249985290785
epoch: 12, step: 11
	action: tensor([[-0.0006,  0.0220,  0.0109, -0.0078, -0.0045, -0.0023, -0.0116]],
       dtype=torch.float64)
	q_value: tensor([[5.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30008698645227094, distance: 0.9573676040774045 entropy -11.104848057778318
epoch: 12, step: 12
	action: tensor([[-0.0007,  0.0221,  0.0069, -0.0077, -0.0012, -0.0071, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[5.9348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29903513222339806, distance: 0.9580867170845984 entropy -11.105156675896678
epoch: 12, step: 13
	action: tensor([[-0.0006,  0.0222, -0.0023, -0.0077, -0.0093, -0.0166,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[5.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29706633723206943, distance: 0.9594312592867812 entropy -11.103533893813594
epoch: 12, step: 14
	action: tensor([[-0.0005,  0.0220,  0.0048, -0.0080, -0.0184, -0.0190,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[5.9402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963724103581098, distance: 0.959904711434351 entropy -11.0907599829894
epoch: 12, step: 15
	action: tensor([[-0.0005,  0.0221,  0.0103, -0.0079, -0.0059, -0.0153, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[5.9385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29717337082583295, distance: 0.9593582116504861 entropy -11.094109187168188
epoch: 12, step: 16
	action: tensor([[-0.0005,  0.0220,  0.0023, -0.0078, -0.0101,  0.0156,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[5.9398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30427718244301494, distance: 0.9544975474453062 entropy -11.101252304730874
epoch: 12, step: 17
	action: tensor([[-0.0007,  0.0219,  0.0160, -0.0077, -0.0041,  0.0025,  0.0362]],
       dtype=torch.float64)
	q_value: tensor([[5.9371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30098941364873355, distance: 0.95675021791006 entropy -11.09413579228026
epoch: 12, step: 18
	action: tensor([[-0.0005,  0.0214,  0.0079, -0.0081,  0.0020, -0.0001, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[5.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30010387983527564, distance: 0.9573560503022277 entropy -11.09003469156379
epoch: 12, step: 19
	action: tensor([[-0.0006,  0.0220,  0.0139, -0.0077, -0.0012, -0.0095,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[5.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29839201094683954, distance: 0.958526129040653 entropy -11.101001087581437
epoch: 12, step: 20
	action: tensor([[-0.0005,  0.0218,  0.0069, -0.0080, -0.0059, -0.0021,  0.0313]],
       dtype=torch.float64)
	q_value: tensor([[5.9509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30003121986309556, distance: 0.9574057431471256 entropy -11.097300327283833
epoch: 12, step: 21
	action: tensor([[-0.0005,  0.0217,  0.0054, -0.0081, -0.0039, -0.0235,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[5.9507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950071205727074, distance: 0.9608355397585256 entropy -11.088496892249522
epoch: 12, step: 22
	action: tensor([[-0.0005,  0.0220,  0.0125, -0.0080, -0.0053, -0.0076,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[5.9436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29889859614455594, distance: 0.9581800220704231 entropy -11.095057748741864
epoch: 12, step: 23
	action: tensor([[-0.0005,  0.0219,  0.0147, -0.0079, -0.0058,  0.0147, -0.0273]],
       dtype=torch.float64)
	q_value: tensor([[5.9471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039464923829438, distance: 0.9547243657589453 entropy -11.098111587691534
epoch: 12, step: 24
	action: tensor([[-0.0008,  0.0221,  0.0114, -0.0074, -0.0058, -0.0075,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[5.9259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988857594412523, distance: 0.958188793851714 entropy -11.113700996089323
epoch: 12, step: 25
	action: tensor([[-0.0005,  0.0215,  0.0048, -0.0082, -0.0095, -0.0231, -0.0171]],
       dtype=torch.float64)
	q_value: tensor([[5.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950134245277719, distance: 0.9608312439154156 entropy -11.087260586768862
epoch: 12, step: 26
	action: tensor([[-0.0006,  0.0222,  0.0153, -0.0078, -0.0012, -0.0247,  0.0332]],
       dtype=torch.float64)
	q_value: tensor([[5.9303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29508705589841533, distance: 0.960781066244463 entropy -11.102718588920945
epoch: 12, step: 27
	action: tensor([[-0.0004,  0.0215,  0.0232, -0.0083, -0.0072,  0.0123, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[5.9680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030632119108214, distance: 0.955329938476289 entropy -11.086089209975608
epoch: 12, step: 28
	action: tensor([[-0.0008,  0.0219,  0.0068, -0.0075, -0.0067, -0.0065,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[5.9350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990864647392978, distance: 0.9580516355103796 entropy -11.111879778212707
epoch: 12, step: 29
	action: tensor([[-0.0006,  0.0220,  0.0191, -0.0079,  0.0046, -0.0214,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[5.9404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955401225284925, distance: 0.9604722566173445 entropy -11.096113136315449
epoch: 12, step: 30
	action: tensor([[-0.0004,  0.0217,  0.0134, -0.0081, -0.0104,  0.0041, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[5.9595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013793607603691, distance: 0.9564833163513984 entropy -11.096694484657458
epoch: 12, step: 31
	action: tensor([[-0.0007,  0.0221,  0.0141, -0.0076, -0.0111, -0.0061,  0.0452]],
       dtype=torch.float64)
	q_value: tensor([[5.9320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29922760342084, distance: 0.9579551721482236 entropy -11.105923225901043
epoch: 12, step: 32
	action: tensor([[-0.0005,  0.0214,  0.0114, -0.0082, -0.0084, -0.0100,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[5.9636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977805502045522, distance: 0.9589437222439712 entropy -11.083936800014598
epoch: 12, step: 33
	action: tensor([[-0.0005,  0.0219,  0.0160, -0.0080, -0.0034,  0.0031, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[5.9462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012143990209796, distance: 0.9565962344572106 entropy -11.095089567683571
epoch: 12, step: 34
	action: tensor([[-0.0006,  0.0219,  0.0190, -0.0077, -0.0068, -0.0168,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[5.9412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966148854653069, distance: 0.9597393021633288 entropy -11.105056078858027
epoch: 12, step: 35
	action: tensor([[-0.0005,  0.0218,  0.0065, -0.0080, -0.0088, -0.0012,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[5.9550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002214075051516, distance: 0.9572756665520914 entropy -11.096986714522982
epoch: 12, step: 36
	action: tensor([[-0.0006,  0.0220,  0.0149, -0.0078, -0.0088,  0.0044, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[5.9393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016036461365712, distance: 0.9563297691836202 entropy -11.098290078942751
epoch: 12, step: 37
	action: tensor([[-0.0007,  0.0220,  0.0126, -0.0077, -0.0046,  0.0137,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[5.9363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037738917939301, distance: 0.9548427300598443 entropy -11.104731435913235
epoch: 12, step: 38
	action: tensor([[-0.0007,  0.0218, -0.0039, -0.0077, -0.0031,  0.0141,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[5.9426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036950433655107, distance: 0.9548967970621277 entropy -11.099349920157872
epoch: 12, step: 39
	action: tensor([[-0.0007,  0.0220,  0.0164, -0.0077, -0.0060, -0.0087, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[5.9324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29847998916982355, distance: 0.9584660299036543 entropy -11.09550105227491
epoch: 12, step: 40
	action: tensor([[-0.0006,  0.0220,  0.0014, -0.0078, -0.0022,  0.0083, -0.0312]],
       dtype=torch.float64)
	q_value: tensor([[5.9408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025870908263121, distance: 0.9556562055611133 entropy -11.103219011510957
epoch: 12, step: 41
	action: tensor([[-0.0009,  0.0224, -0.0012, -0.0073, -0.0124, -0.0137, -0.0259]],
       dtype=torch.float64)
	q_value: tensor([[5.9139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29782345300562885, distance: 0.9589144279826233 entropy -11.109694516804895
epoch: 12, step: 42
	action: tensor([[-0.0007,  0.0224,  0.0002, -0.0076,  0.0015,  0.0042,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[5.9175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30194473231729013, distance: 0.9560962121904674 entropy -11.104739196005424
epoch: 12, step: 43
	action: tensor([[-0.0006,  0.0216,  0.0090, -0.0081, -0.0179,  0.0091, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[5.9538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30223744099304006, distance: 0.955895735943985 entropy -11.084470921392832
epoch: 12, step: 44
	action: tensor([[-0.0008,  0.0222,  0.0064, -0.0075, -0.0101, -0.0076,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[5.9260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989883801112192, distance: 0.958118667209974 entropy -11.107626580039414
epoch: 12, step: 45
	action: tensor([[-0.0005,  0.0219,  0.0065, -0.0080, -0.0127,  0.0026,  0.0223]],
       dtype=torch.float64)
	q_value: tensor([[5.9436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011218645790604, distance: 0.9566595694546234 entropy -11.09285271337735
epoch: 12, step: 46
	action: tensor([[-0.0006,  0.0218,  0.0025, -0.0079, -0.0144,  0.0155, -0.0315]],
       dtype=torch.float64)
	q_value: tensor([[5.9455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30400664744375727, distance: 0.9546831097760655 entropy -11.093136206595824
epoch: 12, step: 47
	action: tensor([[-0.0009,  0.0224, -0.0055, -0.0072, -0.0015, -0.0076,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[5.9095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991688967117939, distance: 0.9579952973144411 entropy -11.11142236175784
epoch: 12, step: 48
	action: tensor([[-0.0006,  0.0220,  0.0096, -0.0080, -0.0175,  0.0151,  0.0345]],
       dtype=torch.float64)
	q_value: tensor([[5.9369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039682466447361, distance: 0.9547094462981228 entropy -11.090299271791064
epoch: 12, step: 49
	action: tensor([[-6.7402e-04,  2.1620e-02,  1.1242e-02, -7.9070e-03, -4.1894e-03,
         -3.2262e-03, -7.1979e-05]], dtype=torch.float64)
	q_value: tensor([[5.9483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994516932125144, distance: 0.9578019946399923 entropy -11.0906977761734
epoch: 12, step: 50
	action: tensor([[-0.0006,  0.0219,  0.0159, -0.0078, -0.0032,  0.0116,  0.0014]],
       dtype=torch.float64)
	q_value: tensor([[5.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032790166190472, distance: 0.9551820192788659 entropy -11.100929716831152
epoch: 12, step: 51
	action: tensor([[-0.0007,  0.0218,  0.0214, -0.0077, -0.0015,  0.0017, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[5.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300817923797823, distance: 0.9568675715621852 entropy -11.103882969070492
epoch: 12, step: 52
	action: tensor([[-0.0007,  0.0219,  0.0157, -0.0077, -0.0041, -0.0010,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[5.9401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002824589503611, distance: 0.957233907388343 entropy -11.10785596409211
epoch: 12, step: 53
	action: tensor([[-0.0006,  0.0219,  0.0226, -0.0078, -0.0112,  0.0161, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[5.9443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042429096443885, distance: 0.954521057454275 entropy -11.100639797478276
epoch: 12, step: 54
	action: tensor([[-0.0008,  0.0219,  0.0114, -0.0075, -0.0049,  0.0092,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[5.9369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30256851300095955, distance: 0.955668933957461 entropy -11.11186063520761
epoch: 12, step: 55
	action: tensor([[-5.9486e-04,  2.1470e-02,  1.2564e-02, -8.0387e-03, -8.9189e-05,
         -2.3724e-03,  6.6305e-03]], dtype=torch.float64)
	q_value: tensor([[5.9571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299564584349594, distance: 0.9577248181543236 entropy -11.089961759413498
epoch: 12, step: 56
	action: tensor([[-0.0006,  0.0219, -0.0035, -0.0079, -0.0053,  0.0210,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[5.9440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053751585508504, distance: 0.9537440653892303 entropy -11.099023033125869
epoch: 12, step: 57
	action: tensor([[-0.0008,  0.0220,  0.0134, -0.0076, -0.0097, -0.0071,  0.0515]],
       dtype=torch.float64)
	q_value: tensor([[5.9320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988005309512517, distance: 0.9582470315094543 entropy -11.096024915697157
epoch: 12, step: 58
	action: tensor([[-0.0005,  0.0213,  0.0054, -0.0083, -0.0026, -0.0277, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[5.9691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29364250916220846, distance: 0.9617650053647139 entropy -11.081719107381508
epoch: 12, step: 59
	action: tensor([[-0.0005,  0.0221,  0.0145, -0.0080, -0.0181, -0.0050, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[5.9403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995885265853572, distance: 0.9577084495721303 entropy -11.097334331163129
epoch: 12, step: 60
	action: tensor([[-0.0007,  0.0221,  0.0007, -0.0077, -0.0120,  0.0108,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[5.9341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3031856049064766, distance: 0.9552460493568212 entropy -11.106018742799304
epoch: 12, step: 61
	action: tensor([[-0.0008,  0.0222,  0.0095, -0.0076,  0.0028, -0.0064,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[5.9267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29917998648121313, distance: 0.9579877177500282 entropy -11.099962953150422
epoch: 12, step: 62
	action: tensor([[-0.0005,  0.0219,  0.0120, -0.0079, -0.0061, -0.0070,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[5.9443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989511022076845, distance: 0.9581441419520621 entropy -11.097639955621906
epoch: 12, step: 63
	action: tensor([[-0.0005,  0.0218,  0.0050, -0.0080, -0.0102, -0.0194,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[5.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2960991397523426, distance: 0.9600910943177284 entropy -11.095755706586308
epoch: 12, step: 64
	action: tensor([[-0.0005,  0.0217,  0.0082, -0.0082, -0.0087,  0.0303, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[5.9552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30735485820689956, distance: 0.9523839974006425 entropy -11.084783626799876
epoch: 12, step: 65
	action: tensor([[-0.0009,  0.0220,  0.0119, -0.0074, -0.0008,  0.0045, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[5.9268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30157992177199855, distance: 0.956346012197558 entropy -11.106556082778354
epoch: 12, step: 66
	action: tensor([[-0.0007,  0.0221,  0.0015, -0.0076,  0.0051,  0.0079,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[5.9308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025346466379196, distance: 0.9556921366940562 entropy -11.10683088017747
epoch: 12, step: 67
	action: tensor([[-0.0007,  0.0219, -0.0049, -0.0078, -0.0021, -0.0027, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[5.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29995556699072656, distance: 0.9574574801242411 entropy -11.096838601654076
epoch: 12, step: 68
	action: tensor([[-0.0007,  0.0224,  0.0031, -0.0076, -0.0133, -0.0100,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[5.9206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29864176159129896, distance: 0.9583555110958307 entropy -11.101665212777245
epoch: 12, step: 69
	action: tensor([[-0.0006,  0.0221,  0.0127, -0.0079, -0.0039, -0.0125, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[5.9357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29776965444768566, distance: 0.9589511617823661 entropy -11.09586396187838
epoch: 12, step: 70
	action: tensor([[-0.0005,  0.0220,  0.0052, -0.0078, -0.0043, -0.0091, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[5.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986087220702701, distance: 0.9583780838927519 entropy -11.101603423590545
epoch: 12, step: 71
	action: tensor([[-0.0006,  0.0221,  0.0069, -0.0078, -0.0024,  0.0004,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[5.9338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30082334969452285, distance: 0.9568638587419855 entropy -11.101923436938424
epoch: 12, step: 72
	action: tensor([[-0.0006,  0.0219,  0.0144, -0.0078, -0.0077,  0.0116,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[5.9404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30320928862270646, distance: 0.9552298155011758 entropy -11.097002693149514
epoch: 12, step: 73
	action: tensor([[-0.0006,  0.0216,  0.0064, -0.0079, -0.0084, -0.0119,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[5.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975686958412723, distance: 0.9590883644148583 entropy -11.094828498951557
epoch: 12, step: 74
	action: tensor([[-0.0005,  0.0219,  0.0055, -0.0080, -0.0018,  0.0030,  0.0345]],
       dtype=torch.float64)
	q_value: tensor([[5.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30123114198259004, distance: 0.9565847743255574 entropy -11.093086570541757
epoch: 12, step: 75
	action: tensor([[-0.0006,  0.0216,  0.0078, -0.0081, -0.0115, -0.0095, -0.0472]],
       dtype=torch.float64)
	q_value: tensor([[5.9543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980757356451935, distance: 0.9587421499441621 entropy -11.087798827655844
epoch: 12, step: 76
	action: tensor([[-0.0008,  0.0224,  0.0069, -0.0074, -0.0030,  0.0268,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[5.9138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071506374615337, distance: 0.9525243883626028 entropy -11.114996157898348
epoch: 12, step: 77
	action: tensor([[-0.0008,  0.0216, -0.0004, -0.0077, -0.0067, -0.0286, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[5.9438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938623180327541, distance: 0.9616153496086604 entropy -11.097284982410217
epoch: 12, step: 78
	action: tensor([[-0.0005,  0.0222,  0.0117, -0.0080, -0.0009, -0.0016,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[5.9357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30038438509250365, distance: 0.9571641858886412 entropy -11.09539872817949
epoch: 12, step: 79
	action: tensor([[-0.0005,  0.0217,  0.0158, -0.0080, -0.0144, -0.0113,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[5.9539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977628506034412, distance: 0.9589558073652437 entropy -11.093696694652408
epoch: 12, step: 80
	action: tensor([[-0.0005,  0.0220,  0.0109, -0.0079,  0.0045,  0.0217, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[5.9424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30569642186933665, distance: 0.9535234870272756 entropy -11.099950608504406
epoch: 12, step: 81
	action: tensor([[-0.0008,  0.0219,  0.0055, -0.0075, -0.0101, -0.0095, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[5.9350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29835317088858015, distance: 0.9585526600209516 entropy -11.105103170175761
epoch: 12, step: 82
	action: tensor([[-0.0006,  0.0221,  0.0079, -0.0078, -0.0021,  0.0120,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[5.9323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303499490293471, distance: 0.9550308761689164 entropy -11.100221051626809
epoch: 12, step: 83
	action: tensor([[-0.0007,  0.0219,  0.0011, -0.0077, -0.0044, -0.0102, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[5.9405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982004934156839, distance: 0.9586569442785032 entropy -11.0977755288119
epoch: 12, step: 84
	action: tensor([[-0.0006,  0.0222,  0.0097, -0.0078, -0.0110,  0.0137,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[5.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30384023363898005, distance: 0.9547972365507704 entropy -11.098501156583039
epoch: 12, step: 85
	action: tensor([[-0.0007,  0.0216,  0.0033, -0.0080, -0.0070, -0.0076,  0.0204]],
       dtype=torch.float64)
	q_value: tensor([[5.9513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29845959544687894, distance: 0.9584799614724666 entropy -11.090002677226835
epoch: 12, step: 86
	action: tensor([[-0.0005,  0.0219,  0.0190, -0.0080, -0.0104, -0.0198,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[5.9428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959126289866095, distance: 0.9602182823033583 entropy -11.09120901923337
epoch: 12, step: 87
	action: tensor([[-0.0004,  0.0217,  0.0106, -0.0081, -0.0075, -0.0038, -0.0261]],
       dtype=torch.float64)
	q_value: tensor([[5.9588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29958331119770754, distance: 0.9577120151987022 entropy -11.093742721589361
epoch: 12, step: 88
	action: tensor([[-0.0007,  0.0222,  0.0087, -0.0075, -0.0137,  0.0019, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[5.9245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012326453438473, distance: 0.9565837453062717 entropy -11.10863925565511
epoch: 12, step: 89
	action: tensor([[-0.0008,  0.0223,  0.0061, -0.0075, -0.0092,  0.0129,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[5.9219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303735034717033, distance: 0.9548693750529602 entropy -11.110429028927541
epoch: 12, step: 90
	action: tensor([[-0.0007,  0.0220,  0.0032, -0.0076, -0.0020, -0.0141, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[5.9347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973876659217536, distance: 0.9592119441181703 entropy -11.10133276486169
epoch: 12, step: 91
	action: tensor([[-0.0006,  0.0222,  0.0006, -0.0078, -0.0043, -0.0276, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[5.9331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2945788549509484, distance: 0.9611273372843667 entropy -11.101384471658651
epoch: 12, step: 92
	action: tensor([[-0.0005,  0.0222,  0.0039, -0.0079,  0.0002, -0.0102,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[5.9350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985037974788878, distance: 0.9584497654716129 entropy -11.0976502522331
epoch: 12, step: 93
	action: tensor([[-0.0005,  0.0219,  0.0169, -0.0079, -0.0093, -0.0034,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[5.9438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997547003402953, distance: 0.9575948338935082 entropy -11.095032626800867
epoch: 12, step: 94
	action: tensor([[-0.0005,  0.0218,  0.0098, -0.0080, -0.0072, -0.0056,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[5.9514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299141511596981, distance: 0.9580140140600354 entropy -11.095822535496366
epoch: 12, step: 95
	action: tensor([[-0.0005,  0.0217,  0.0061, -0.0081, -0.0160, -0.0100, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[5.9533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29810997289864405, distance: 0.9587187677219446 entropy -11.088686962976395
epoch: 12, step: 96
	action: tensor([[-0.0006,  0.0222,  0.0135, -0.0077, -0.0109,  0.0027,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[5.9276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013919114763649, distance: 0.9564747247040151 entropy -11.104740438336924
epoch: 12, step: 97
	action: tensor([[-0.0006,  0.0216, -0.0001, -0.0081, -0.0017,  0.0311,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[5.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074339443215768, distance: 0.9523296243200927 entropy -11.089884163081205
epoch: 12, step: 98
	action: tensor([[-0.0009,  0.0220,  0.0195, -0.0074, -0.0029,  0.0190, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[5.9268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049294351595887, distance: 0.9540500131762588 entropy -11.102714826022787
epoch: 12, step: 99
	action: tensor([[-0.0008,  0.0218,  0.0148, -0.0075, -0.0037,  0.0111, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[5.9377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30302931670162525, distance: 0.9553531692161271 entropy -11.108319111432975
epoch: 12, step: 100
	action: tensor([[-0.0007,  0.0219,  0.0036, -0.0076,  0.0015,  0.0069, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[5.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30216248672381485, distance: 0.9559470761465986 entropy -11.10493871959898
epoch: 12, step: 101
	action: tensor([[-0.0008,  0.0223,  0.0032, -0.0074, -0.0057,  0.0117,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[5.9201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034860777627457, distance: 0.9550400716534311 entropy -11.107903406066852
epoch: 12, step: 102
	action: tensor([[-0.0007,  0.0219,  0.0106, -0.0077, -0.0065, -0.0084,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[5.9371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29857510006834953, distance: 0.9584010540982875 entropy -11.09758978736828
epoch: 12, step: 103
	action: tensor([[-0.0005,  0.0218,  0.0025, -0.0080, -0.0005, -0.0189,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[5.9497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29619220130804935, distance: 0.9600276262008232 entropy -11.09438547040247
epoch: 12, step: 104
	action: tensor([[-0.0004,  0.0217,  0.0199, -0.0083, -0.0082, -0.0017, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[5.9554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998718401172813, distance: 0.9575147354359437 entropy -11.082490682192681
epoch: 12, step: 105
	action: tensor([[-0.0006,  0.0219,  0.0115, -0.0078, -0.0095,  0.0059, -0.0111]],
       dtype=torch.float64)
	q_value: tensor([[5.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30193303220743983, distance: 0.9561042247249717 entropy -11.103781818743775
epoch: 12, step: 106
	action: tensor([[-0.0007,  0.0221, -0.0012, -0.0076, -0.0103, -0.0257,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[5.9302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949741113510804, distance: 0.9608580336467307 entropy -11.105764229263382
epoch: 12, step: 107
	action: tensor([[-0.0005,  0.0219,  0.0143, -0.0082, -0.0088,  0.0031, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[5.9472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30119615208244455, distance: 0.9566087238664962 entropy -11.085852237799111
epoch: 12, step: 108
	action: tensor([[-0.0006,  0.0220,  0.0046, -0.0077, -0.0137, -0.0232,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[5.9383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29539257420988385, distance: 0.9605728362608953 entropy -11.104996498270912
epoch: 12, step: 109
	action: tensor([[-0.0005,  0.0220,  0.0079, -0.0081, -0.0053,  0.0014,  0.0330]],
       dtype=torch.float64)
	q_value: tensor([[5.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30089698285015753, distance: 0.9568134717900435 entropy -11.090117814036054
epoch: 12, step: 110
	action: tensor([[-0.0006,  0.0216,  0.0097, -0.0081, -0.0008,  0.0181, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[5.9533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30444058912715977, distance: 0.9543854478887281 entropy -11.08794983022048
epoch: 12, step: 111
	action: tensor([[-0.0008,  0.0221,  0.0068, -0.0074, -0.0057,  0.0140,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[5.9258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038707485156452, distance: 0.9547763104353844 entropy -11.109059079486409
epoch: 12, step: 112
	action: tensor([[-0.0007,  0.0220,  0.0103, -0.0076, -0.0019, -0.0008,  0.0172]],
       dtype=torch.float64)
	q_value: tensor([[5.9329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003821743399756, distance: 0.9571656981844126 entropy -11.101171498962486
epoch: 12, step: 113
	action: tensor([[-0.0006,  0.0218,  0.0050, -0.0079, -0.0104, -0.0114,  0.0405]],
       dtype=torch.float64)
	q_value: tensor([[5.9476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29787103893984845, distance: 0.9588819350061739 entropy -11.094398186176212
epoch: 12, step: 114
	action: tensor([[-0.0005,  0.0216,  0.0140, -0.0082, -0.0083, -0.0034,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[5.9547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994537793901302, distance: 0.9578005685095244 entropy -11.082902625613448
epoch: 12, step: 115
	action: tensor([[-0.0005,  0.0217,  0.0127, -0.0080, -0.0047, -0.0006, -0.0204]],
       dtype=torch.float64)
	q_value: tensor([[5.9538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002482251837223, distance: 0.9572573234951832 entropy -11.093881395126244
epoch: 12, step: 116
	action: tensor([[-0.0007,  0.0221,  0.0151, -0.0076, -0.0093,  0.0166, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[5.9289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045348812919588, distance: 0.9543207559885106 entropy -11.107773534115196
epoch: 12, step: 117
	action: tensor([[-0.0008,  0.0220,  0.0155, -0.0075, -0.0081,  0.0147, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[5.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30395606508184536, distance: 0.9547178006599907 entropy -11.109817245165534
epoch: 12, step: 118
	action: tensor([[-0.0007,  0.0219,  0.0086, -0.0076,  0.0013, -0.0083,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[5.9360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986075595909854, distance: 0.9583788780958187 entropy -11.105574941779775
epoch: 12, step: 119
	action: tensor([[-0.0005,  0.0215,  0.0066, -0.0082,  0.0022,  0.0045, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[5.9614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30121947465062393, distance: 0.9565927603322322 entropy -11.08568096619268
epoch: 12, step: 120
	action: tensor([[-0.0007,  0.0220,  0.0215, -0.0077, -0.0149, -0.0084,  0.0588]],
       dtype=torch.float64)
	q_value: tensor([[5.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29860547297214757, distance: 0.9583803036671666 entropy -11.101666449383178
epoch: 12, step: 121
	action: tensor([[-4.8232e-04,  2.1110e-02,  1.0457e-02, -8.4014e-03, -1.1545e-02,
         -6.0041e-05,  1.0247e-02]], dtype=torch.float64)
	q_value: tensor([[5.9778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997869322805341, distance: 0.9575727948347248 entropy -11.081699395280646
epoch: 12, step: 122
	action: tensor([[-0.0006,  0.0219,  0.0141, -0.0078, -0.0103, -0.0019,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[5.9419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30008986398597426, distance: 0.9573656360754348 entropy -11.098477106391345
epoch: 12, step: 123
	action: tensor([[-0.0006,  0.0219,  0.0033, -0.0078, -0.0060,  0.0131, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[5.9454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035877041196271, distance: 0.9549703955273311 entropy -11.09958665266785
epoch: 12, step: 124
	action: tensor([[-0.0009,  0.0223, -0.0050, -0.0073, -0.0098,  0.0059,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[5.9156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30217722295523364, distance: 0.955936982728403 entropy -11.111480858077998
epoch: 12, step: 125
	action: tensor([[-0.0007,  0.0222,  0.0204, -0.0077, -0.0050, -0.0072,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[5.9269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989130948651427, distance: 0.9581701144763436 entropy -11.095926778936619
epoch: 12, step: 126
	action: tensor([[-0.0005,  0.0217,  0.0232, -0.0080, -0.0068, -0.0190, -0.0402]],
       dtype=torch.float64)
	q_value: tensor([[5.9564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959356585691778, distance: 0.960202578565475 entropy -11.096218918017556
epoch: 12, step: 127
	action: tensor([[-0.0007,  0.0221,  0.0016, -0.0076, -0.0135, -0.0093, -0.0368]],
       dtype=torch.float64)
	q_value: tensor([[5.9324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29870674688798127, distance: 0.9583111112039435 entropy -11.115246094442465
LOSS epoch 12 actor 15.97555277809524 critic 13.320754434582717
epoch: 13, step: 0
	action: tensor([[-0.0012,  0.0224,  0.0091, -0.0067,  0.0079, -0.0327,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[5.7436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2932326909824139, distance: 0.9620439658314419 entropy -11.106041763163754
epoch: 13, step: 1
	action: tensor([[-0.0008,  0.0219,  0.0091, -0.0074, -0.0075,  0.0058,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[5.7780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018105507733774, distance: 0.9561880991135747 entropy -11.090991076463427
epoch: 13, step: 2
	action: tensor([[-0.0011,  0.0219,  0.0107, -0.0070, -0.0031,  0.0147,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[5.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303803606739673, distance: 0.9548223534879736 entropy -11.096355942300702
epoch: 13, step: 3
	action: tensor([[-0.0011,  0.0216,  0.0132, -0.0071, -0.0122,  0.0100,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[5.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3024137894948361, distance: 0.9557749345116211 entropy -11.090773914554646
epoch: 13, step: 4
	action: tensor([[-0.0011,  0.0219,  0.0142, -0.0069, -0.0070,  0.0017, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[5.7615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007702269389283, distance: 0.9569002088399133 entropy -11.097866832402355
epoch: 13, step: 5
	action: tensor([[-0.0011,  0.0220,  0.0045, -0.0069,  0.0042, -0.0088, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[5.7620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984490186935683, distance: 0.9584871866927321 entropy -11.101652920619115
epoch: 13, step: 6
	action: tensor([[-0.0011,  0.0220,  0.0219, -0.0071, -0.0117, -0.0089,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[5.7614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29839809768304126, distance: 0.9585219712285737 entropy -11.09552352200897
epoch: 13, step: 7
	action: tensor([[-0.0010,  0.0216,  0.0052, -0.0073, -0.0087,  0.0211,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[5.7830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050639573921152, distance: 0.9539576864726805 entropy -11.091831020508044
epoch: 13, step: 8
	action: tensor([[-0.0012,  0.0217,  0.0148, -0.0070, -0.0115, -0.0061,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[5.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987262112001168, distance: 0.9582978122053167 entropy -11.091861570651757
epoch: 13, step: 9
	action: tensor([[-0.0010,  0.0217,  0.0057, -0.0073, -0.0123, -0.0098, -0.0045]],
       dtype=torch.float64)
	q_value: tensor([[5.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980584353047139, distance: 0.9587539649392173 entropy -11.090237557830033
epoch: 13, step: 10
	action: tensor([[-0.0011,  0.0221, -0.0016, -0.0070, -0.0064, -0.0004,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[5.7599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005131454035478, distance: 0.9570761014985726 entropy -11.096864303860146
epoch: 13, step: 11
	action: tensor([[-0.0011,  0.0215,  0.0038, -0.0075, -0.0035,  0.0060,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[5.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30129794958736733, distance: 0.9565390448512251 entropy -11.074831050967243
epoch: 13, step: 12
	action: tensor([[-0.0011,  0.0219,  0.0032, -0.0071, -0.0053, -0.0024,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[5.7629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998020842982628, distance: 0.9575624342465517 entropy -11.090721911197225
epoch: 13, step: 13
	action: tensor([[-0.0010,  0.0219, -0.0065, -0.0072, -0.0136,  0.0046,  0.0299]],
       dtype=torch.float64)
	q_value: tensor([[5.7668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014385792661176, distance: 0.9564427773879466 entropy -11.09004408342196
epoch: 13, step: 14
	action: tensor([[-0.0011,  0.0219, -0.0006, -0.0072,  0.0046, -0.0068,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[5.7621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29871645891871135, distance: 0.9583044754783443 entropy -11.083821993936203
epoch: 13, step: 15
	action: tensor([[-0.0010,  0.0219,  0.0075, -0.0073, -0.0084,  0.0060,  0.0455]],
       dtype=torch.float64)
	q_value: tensor([[5.7663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301731052267449, distance: 0.9562425351707359 entropy -11.087268825648755
epoch: 13, step: 16
	action: tensor([[-0.0011,  0.0214,  0.0064, -0.0074, -0.0066, -0.0104,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[5.7796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975637478955204, distance: 0.9590917423315409 entropy -11.082311792463551
epoch: 13, step: 17
	action: tensor([[-0.0010,  0.0219, -0.0009, -0.0072, -0.0047,  0.0218, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[5.7668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30547829346217636, distance: 0.9536732588508008 entropy -11.092566777730706
epoch: 13, step: 18
	action: tensor([[-1.3796e-03,  2.2254e-02,  8.1591e-03, -6.5520e-03, -9.7754e-05,
          4.8983e-03,  1.1143e-02]], dtype=torch.float64)
	q_value: tensor([[5.7406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017228574960128, distance: 0.9562481463081407 entropy -11.10467297038889
epoch: 13, step: 19
	action: tensor([[-0.0011,  0.0218,  0.0045, -0.0071, -0.0055,  0.0027,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[5.7683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30093391418666615, distance: 0.9567881987858332 entropy -11.093263776213151
epoch: 13, step: 20
	action: tensor([[-0.0011,  0.0220,  0.0054, -0.0070, -0.0107,  0.0068,  0.0625]],
       dtype=torch.float64)
	q_value: tensor([[5.7601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30197490252919046, distance: 0.9560755505478004 entropy -11.09383476568703
epoch: 13, step: 21
	action: tensor([[-0.0011,  0.0212,  0.0019, -0.0075, -0.0077, -0.0054, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[5.7840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984932516667327, distance: 0.9584569697823154 entropy -11.07474824667845
epoch: 13, step: 22
	action: tensor([[-0.0012,  0.0224,  0.0034, -0.0067, -0.0078, -0.0049,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[5.7430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29967622815083916, distance: 0.9576484882758615 entropy -11.105535818784517
epoch: 13, step: 23
	action: tensor([[-0.0010,  0.0218,  0.0101, -0.0073, -0.0030,  0.0223, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[5.7697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30543601205844906, distance: 0.953702287483042 entropy -11.085704307024267
epoch: 13, step: 24
	action: tensor([[-0.0013,  0.0220,  0.0033, -0.0067, -0.0027, -0.0019,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[5.7504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30005263165720497, distance: 0.9573910996858701 entropy -11.104626997642265
epoch: 13, step: 25
	action: tensor([[-0.0010,  0.0218,  0.0083, -0.0073,  0.0041, -0.0162,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[5.7694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2964914451126798, distance: 0.9598235130445081 entropy -11.085813313688393
epoch: 13, step: 26
	action: tensor([[-0.0009,  0.0219,  0.0029, -0.0073, -0.0075,  0.0122,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[5.7707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303244171382295, distance: 0.9552059048286438 entropy -11.091961412768478
epoch: 13, step: 27
	action: tensor([[-0.0012,  0.0221,  0.0100, -0.0069, -0.0020,  0.0034,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[5.7542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012510068548694, distance: 0.9565711771473187 entropy -11.09602668542792
epoch: 13, step: 28
	action: tensor([[-0.0010,  0.0217,  0.0042, -0.0072, -0.0048,  0.0082, -0.0105]],
       dtype=torch.float64)
	q_value: tensor([[5.7719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30212309923539216, distance: 0.9559740536466595 entropy -11.089625401334448
epoch: 13, step: 29
	action: tensor([[-0.0012,  0.0222,  0.0034, -0.0068, -0.0085,  0.0288, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[5.7502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071706695158035, distance: 0.9525106183000214 entropy -11.100045410294669
epoch: 13, step: 30
	action: tensor([[-0.0013,  0.0220,  0.0157, -0.0067, -0.0110, -0.0179, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[5.7503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962457242716362, distance: 0.9599911215608407 entropy -11.100296233480464
epoch: 13, step: 31
	action: tensor([[-0.0010,  0.0220,  0.0044, -0.0071, -0.0006, -0.0130,  0.0497]],
       dtype=torch.float64)
	q_value: tensor([[5.7683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29754212246559064, distance: 0.9591065056723974 entropy -11.09648197907398
epoch: 13, step: 32
	action: tensor([[-0.0009,  0.0214,  0.0152, -0.0076, -0.0058, -0.0101,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[5.7851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29756285419880335, distance: 0.9590923524487341 entropy -11.075084605065223
epoch: 13, step: 33
	action: tensor([[-0.0010,  0.0218,  0.0169, -0.0072, -0.0138,  0.0034,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[5.7725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011696868506394, distance: 0.9566268381284616 entropy -11.09554193638873
epoch: 13, step: 34
	action: tensor([[-0.0011,  0.0218,  0.0129, -0.0071, -0.0099,  0.0137,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[5.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034743846265471, distance: 0.9550480882676056 entropy -11.09692028800326
epoch: 13, step: 35
	action: tensor([[-0.0011,  0.0216,  0.0085, -0.0071,  0.0006, -0.0216, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[5.7722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29515351382198607, distance: 0.9607357748221494 entropy -11.092003924084713
epoch: 13, step: 36
	action: tensor([[-0.0010,  0.0220,  0.0024, -0.0071, -0.0024, -0.0018,  0.0378]],
       dtype=torch.float64)
	q_value: tensor([[5.7667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30020056210216006, distance: 0.957289924382106 entropy -11.096688848912347
epoch: 13, step: 37
	action: tensor([[-0.0010,  0.0216,  0.0072, -0.0074, -0.0091,  0.0039,  0.0384]],
       dtype=torch.float64)
	q_value: tensor([[5.7765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009724750957091, distance: 0.9567618099134132 entropy -11.081900189785047
epoch: 13, step: 38
	action: tensor([[-0.0010,  0.0216,  0.0192, -0.0074, -0.0105, -0.0138,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[5.7766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29681780138657765, distance: 0.9596008570725646 entropy -11.08410696547828
epoch: 13, step: 39
	action: tensor([[-0.0010,  0.0218,  0.0122, -0.0072, -0.0085,  0.0249, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[5.7765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061073693373214, distance: 0.9532412574130562 entropy -11.094700476976493
epoch: 13, step: 40
	action: tensor([[-0.0013,  0.0220,  0.0034, -0.0067, -0.0196, -0.0054,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[5.7509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992264657190914, distance: 0.9579559497664897 entropy -11.104732979403265
epoch: 13, step: 41
	action: tensor([[-0.0011,  0.0221,  0.0176, -0.0070, -0.0110, -0.0143, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[5.7573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297231597181985, distance: 0.9593184713449963 entropy -11.09356244483069
epoch: 13, step: 42
	action: tensor([[-0.0011,  0.0220,  0.0177, -0.0070, -0.0002, -0.0070, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[5.7678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29883183806219504, distance: 0.9582256393932896 entropy -11.100264342758209
epoch: 13, step: 43
	action: tensor([[-0.0010,  0.0218,  0.0120, -0.0071, -0.0084, -0.0167, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[5.7721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29656488617183163, distance: 0.9597734125200206 entropy -11.098829735716526
epoch: 13, step: 44
	action: tensor([[-0.0011,  0.0222,  0.0115, -0.0070, -0.0068, -0.0020,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[5.7592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001870932290902, distance: 0.9572991367033403 entropy -11.100822171988
epoch: 13, step: 45
	action: tensor([[-0.0011,  0.0219,  0.0226, -0.0070, -0.0002, -0.0045,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[5.7669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29932969864470604, distance: 0.9578853875714498 entropy -11.096605151615876
epoch: 13, step: 46
	action: tensor([[-0.0010,  0.0217,  0.0162, -0.0071, -0.0108, -0.0151,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[5.7766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29679538571973263, distance: 0.959616151772672 entropy -11.09930477258193
epoch: 13, step: 47
	action: tensor([[-0.0009,  0.0216,  0.0095, -0.0074, -0.0068, -0.0489,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[5.7819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891292286872743, distance: 0.9648327179435283 entropy -11.086039791084024
epoch: 13, step: 48
	action: tensor([[-0.0008,  0.0220,  0.0063, -0.0074, -0.0015,  0.0319,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[5.7759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30794461770054893, distance: 0.9519784527122079 entropy -11.088245704908873
epoch: 13, step: 49
	action: tensor([[-0.0013,  0.0218,  0.0116, -0.0068, -0.0008,  0.0038,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[5.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30112563020444705, distance: 0.9566569921623667 entropy -11.099205453603743
epoch: 13, step: 50
	action: tensor([[-0.0010,  0.0216,  0.0065, -0.0072, -0.0047, -0.0107, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[5.7754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977426774011557, distance: 0.9589695812524346 entropy -11.09011802374779
epoch: 13, step: 51
	action: tensor([[-0.0010,  0.0220,  0.0082, -0.0071, -0.0071, -0.0029,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[5.7637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29985169868551487, distance: 0.9575285083279027 entropy -11.09567329657742
epoch: 13, step: 52
	action: tensor([[-0.0010,  0.0218,  0.0043, -0.0072,  0.0005,  0.0174,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[5.7708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043743317671227, distance: 0.9544309030672595 entropy -11.09087192543684
epoch: 13, step: 53
	action: tensor([[-0.0012,  0.0219,  0.0214, -0.0069,  0.0019,  0.0118,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[5.7579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030891750094895, distance: 0.9553121437803553 entropy -11.097118151057561
epoch: 13, step: 54
	action: tensor([[-0.0010,  0.0212,  0.0119, -0.0074, -0.0155, -0.0231,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[5.7868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2945624631252395, distance: 0.9611385040465102 entropy -11.08814339116263
epoch: 13, step: 55
	action: tensor([[-0.0009,  0.0217, -0.0009, -0.0075, -0.0137,  0.0103,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[5.7794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302614277265438, distance: 0.9556375787604644 entropy -11.083549629957892
epoch: 13, step: 56
	action: tensor([[-0.0012,  0.0218,  0.0016, -0.0072, -0.0081, -0.0098, -0.0189]],
       dtype=torch.float64)
	q_value: tensor([[5.7627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29803082515320745, distance: 0.9587728205557388 entropy -11.08543015357489
epoch: 13, step: 57
	action: tensor([[-0.0011,  0.0223,  0.0031, -0.0069, -0.0027, -0.0106, -0.0409]],
       dtype=torch.float64)
	q_value: tensor([[5.7517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29832281372016956, distance: 0.9585733959728469 entropy -11.100393733340342
epoch: 13, step: 58
	action: tensor([[-0.0012,  0.0224,  0.0005, -0.0067, -0.0038,  0.0269,  0.0238]],
       dtype=torch.float64)
	q_value: tensor([[5.7457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3069869410088104, distance: 0.9526369060779287 entropy -11.10630144133374
epoch: 13, step: 59
	action: tensor([[-0.0013,  0.0217,  0.0195, -0.0070, -0.0016, -0.0048,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[5.7598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29896832136232043, distance: 0.9581323749173335 entropy -11.090484267779894
epoch: 13, step: 60
	action: tensor([[-0.0010,  0.0217,  0.0119, -0.0071, -0.0072, -0.0219, -0.0119]],
       dtype=torch.float64)
	q_value: tensor([[5.7747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2953173846328846, distance: 0.9606240868835024 entropy -11.09762447313598
epoch: 13, step: 61
	action: tensor([[-0.0010,  0.0221,  0.0057, -0.0071, -0.0066, -0.0002,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[5.7657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30055399427801677, distance: 0.957048155260509 entropy -11.098381917544783
epoch: 13, step: 62
	action: tensor([[-0.0010,  0.0215,  0.0073, -0.0074, -0.0086,  0.0133,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[5.7807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30301147104233683, distance: 0.9553653998582726 entropy -11.080497881138774
epoch: 13, step: 63
	action: tensor([[-0.0012,  0.0220,  0.0096, -0.0069, -0.0149,  0.0054, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[5.7568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30166330829465393, distance: 0.9562889199465391 entropy -11.09673466077648
epoch: 13, step: 64
	action: tensor([[-1.3074e-03,  2.2280e-02,  1.1440e-02, -6.6536e-03, -1.5397e-05,
         -5.1803e-03, -5.3032e-03]], dtype=torch.float64)
	q_value: tensor([[5.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299416095711174, distance: 0.9578263290965461 entropy -11.106456456779966
epoch: 13, step: 65
	action: tensor([[-0.0011,  0.0219,  0.0075, -0.0070, -0.0091, -0.0035, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[5.7665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996788313735471, distance: 0.95764670840296 entropy -11.098156916227579
epoch: 13, step: 66
	action: tensor([[-0.0012,  0.0223,  0.0134, -0.0067, -0.0153, -0.0004, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[5.7480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005828358163607, distance: 0.9570284231972792 entropy -11.106314069091217
epoch: 13, step: 67
	action: tensor([[-0.0011,  0.0220,  0.0142, -0.0069, -0.0082,  0.0131,  0.0384]],
       dtype=torch.float64)
	q_value: tensor([[5.7605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30350184378076606, distance: 0.955029262634611 entropy -11.101361019906088
epoch: 13, step: 68
	action: tensor([[-0.0011,  0.0214,  0.0116, -0.0073, -0.0081,  0.0182, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[5.7805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041110728641343, distance: 0.9546114877379958 entropy -11.087312936876286
epoch: 13, step: 69
	action: tensor([[-0.0013,  0.0221,  0.0048, -0.0066, -0.0030, -0.0165, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[5.7475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296753703836907, distance: 0.9596445915855513 entropy -11.106322886276292
epoch: 13, step: 70
	action: tensor([[-0.0010,  0.0221,  0.0072, -0.0071, -0.0060,  0.0078, -0.0321]],
       dtype=torch.float64)
	q_value: tensor([[5.7615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30240851177160655, distance: 0.9557785500547306 entropy -11.095212436010474
epoch: 13, step: 71
	action: tensor([[-0.0013,  0.0223, -0.0005, -0.0066, -0.0012,  0.0115,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[5.7437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033039537253919, distance: 0.9551649251412537 entropy -11.10703247899002
epoch: 13, step: 72
	action: tensor([[-0.0012,  0.0219,  0.0151, -0.0070,  0.0002,  0.0010, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[5.7611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30050573376839185, distance: 0.9570811719870629 entropy -11.091189758844221
epoch: 13, step: 73
	action: tensor([[-0.0011,  0.0219,  0.0008, -0.0070, -0.0041,  0.0053,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[5.7653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30166555747434887, distance: 0.9562873799534536 entropy -11.101032569102733
epoch: 13, step: 74
	action: tensor([[-0.0012,  0.0221,  0.0193, -0.0069, -0.0027,  0.0020,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[5.7563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30093263473154874, distance: 0.9567890743589885 entropy -11.095865542293788
epoch: 13, step: 75
	action: tensor([[-0.0011,  0.0218,  0.0027, -0.0070,  0.0027,  0.0004,  0.0002]],
       dtype=torch.float64)
	q_value: tensor([[5.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30047072338418646, distance: 0.9571051231199745 entropy -11.098657981766527
epoch: 13, step: 76
	action: tensor([[-0.0011,  0.0220,  0.0169, -0.0070,  0.0007, -0.0039,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[5.7599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29951728573448544, distance: 0.9577571539641893 entropy -11.095981057969643
epoch: 13, step: 77
	action: tensor([[-0.0010,  0.0214,  0.0096, -0.0075, -0.0166, -0.0120, -0.0274]],
       dtype=torch.float64)
	q_value: tensor([[5.7864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29728763342907016, distance: 0.9592802242650829 entropy -11.085058681509507
epoch: 13, step: 78
	action: tensor([[-0.0011,  0.0223, -0.0036, -0.0068, -0.0074,  0.0181,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[5.7519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30485286794634947, distance: 0.9541025595959202 entropy -11.104759712758256
epoch: 13, step: 79
	action: tensor([[-0.0013,  0.0221,  0.0091, -0.0068, -0.0069,  0.0007,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[5.7478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30064445310650423, distance: 0.9569862660985696 entropy -11.095407862510088
epoch: 13, step: 80
	action: tensor([[-0.0010,  0.0216,  0.0135, -0.0073, -0.0007, -0.0014,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[5.7759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998020841003145, distance: 0.9575624343819048 entropy -11.08713733098495
epoch: 13, step: 81
	action: tensor([[-0.0010,  0.0216,  0.0082, -0.0073, -0.0167,  0.0146,  0.0386]],
       dtype=torch.float64)
	q_value: tensor([[5.7796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034587737902392, distance: 0.9550587906850807 entropy -11.08892976597854
epoch: 13, step: 82
	action: tensor([[-0.0011,  0.0215, -0.0014, -0.0072, -0.0040, -0.0075,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[5.7724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983334492444629, distance: 0.9585661312580525 entropy -11.086407656703177
epoch: 13, step: 83
	action: tensor([[-0.0010,  0.0218,  0.0124, -0.0073, -0.0045,  0.0032,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[5.7686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009987608811133, distance: 0.9567438210137271 entropy -11.08493085064509
epoch: 13, step: 84
	action: tensor([[-0.0010,  0.0216,  0.0105, -0.0073, -0.0106,  0.0204,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[5.7775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048170845390038, distance: 0.9541271159809035 entropy -11.088932638332576
epoch: 13, step: 85
	action: tensor([[-0.0012,  0.0217,  0.0175, -0.0070, -0.0103, -0.0082,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[5.7662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29822668925762896, distance: 0.9586390523735465 entropy -11.094179071616965
epoch: 13, step: 86
	action: tensor([[-0.0010,  0.0217,  0.0096, -0.0072, -0.0147, -0.0009,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[5.7767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000975898061329, distance: 0.957360352214864 entropy -11.09292358543625
epoch: 13, step: 87
	action: tensor([[-0.0011,  0.0219,  0.0004, -0.0071, -0.0021, -0.0133,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[5.7655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974256288861693, distance: 0.9591860300977151 entropy -11.094687660161496
epoch: 13, step: 88
	action: tensor([[-0.0010,  0.0220,  0.0106, -0.0072, -0.0041, -0.0053,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[5.7646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29922335883376017, distance: 0.9579580733169978 entropy -11.088837437765585
epoch: 13, step: 89
	action: tensor([[-0.0010,  0.0219,  0.0174, -0.0071, -0.0132,  0.0187,  0.0505]],
       dtype=torch.float64)
	q_value: tensor([[5.7675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046919697823769, distance: 0.9542129710908489 entropy -11.093987540564692
epoch: 13, step: 90
	action: tensor([[-0.0011,  0.0212,  0.0031, -0.0073, -0.0087,  0.0048, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[5.7847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30082574722799915, distance: 0.9568622181586657 entropy -11.08604448805841
epoch: 13, step: 91
	action: tensor([[-0.0012,  0.0221,  0.0102, -0.0069, -0.0105,  0.0066,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[5.7524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30208256261425237, distance: 0.9560018174222431 entropy -11.099224736667846
epoch: 13, step: 92
	action: tensor([[-0.0011,  0.0219,  0.0048, -0.0070, -0.0095,  0.0043, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[5.7633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301413591661443, distance: 0.9564598832569443 entropy -11.09730842733039
epoch: 13, step: 93
	action: tensor([[-0.0011,  0.0221,  0.0066, -0.0069, -0.0047,  0.0090,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[5.7578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30258188037209066, distance: 0.9556597754646415 entropy -11.097178971021881
epoch: 13, step: 94
	action: tensor([[-0.0011,  0.0219,  0.0087, -0.0070, -0.0049,  0.0101, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[5.7632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30269600072788383, distance: 0.9555815837082606 entropy -11.094968316641127
epoch: 13, step: 95
	action: tensor([[-0.0012,  0.0221,  0.0132, -0.0068, -0.0053,  0.0285,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[5.7515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071557423194825, distance: 0.9525208792948927 entropy -11.10252219646786
epoch: 13, step: 96
	action: tensor([[-0.0012,  0.0217,  0.0132, -0.0069, -0.0083,  0.0132,  0.0372]],
       dtype=torch.float64)
	q_value: tensor([[5.7634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30316411052344017, distance: 0.955260782308872 entropy -11.097164291592192
epoch: 13, step: 97
	action: tensor([[-0.0011,  0.0214,  0.0139, -0.0073, -0.0072, -0.0088, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[5.7776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29793730655728123, distance: 0.9588366838314357 entropy -11.088118495837833
epoch: 13, step: 98
	action: tensor([[-0.0011,  0.0220,  0.0181, -0.0070, -0.0015,  0.0176, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[5.7612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30463982340360474, distance: 0.9542487522237761 entropy -11.099992529357834
epoch: 13, step: 99
	action: tensor([[-0.0013,  0.0220,  0.0179, -0.0066, -0.0048,  0.0055,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[5.7539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30167682662648165, distance: 0.9562796640291148 entropy -11.108624375844075
epoch: 13, step: 100
	action: tensor([[-0.0011,  0.0217,  0.0132, -0.0070, -0.0138,  0.0310,  0.0492]],
       dtype=torch.float64)
	q_value: tensor([[5.7712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3073976042899984, distance: 0.9523546091098966 entropy -11.097932867232782
epoch: 13, step: 101
	action: tensor([[-0.0013,  0.0213,  0.0115, -0.0072,  0.0036,  0.0041,  0.0155]],
       dtype=torch.float64)
	q_value: tensor([[5.7750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006336885777692, distance: 0.9569936310694819 entropy -11.087331010357914
epoch: 13, step: 102
	action: tensor([[-0.0011,  0.0217,  0.0102, -0.0071, -0.0068, -0.0037, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[5.7697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993889446712652, distance: 0.9578448891353709 entropy -11.092786597966791
epoch: 13, step: 103
	action: tensor([[-0.0011,  0.0220,  0.0013, -0.0070, -0.0008, -0.0083,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[5.7643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985994554435394, distance: 0.9583844148116817 entropy -11.09738894802482
epoch: 13, step: 104
	action: tensor([[-0.0010,  0.0216,  0.0048, -0.0075, -0.0071, -0.0054,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[5.7767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29888476781204765, distance: 0.9581894714642917 entropy -11.078867910545922
epoch: 13, step: 105
	action: tensor([[-0.0010,  0.0216,  0.0047, -0.0074, -0.0107,  0.0346,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[5.7783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30800063456715654, distance: 0.9519399240550281 entropy -11.081507030702271
epoch: 13, step: 106
	action: tensor([[-0.0013,  0.0215,  0.0153, -0.0070, -0.0042, -0.0115,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[5.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972463179381707, distance: 0.9593084239615216 entropy -11.090972716803916
epoch: 13, step: 107
	action: tensor([[-0.0009,  0.0216,  0.0051, -0.0074, -0.0006,  0.0133, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[5.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30325294439244166, distance: 0.9551998912029732 entropy -11.087700880467992
epoch: 13, step: 108
	action: tensor([[-0.0013,  0.0222,  0.0171, -0.0066, -0.0032, -0.0120, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[5.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977474964632085, distance: 0.9589662909045106 entropy -11.105767306764395
epoch: 13, step: 109
	action: tensor([[-0.0011,  0.0220, -0.0032, -0.0070,  0.0010,  0.0154,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[5.7664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30407147346617047, distance: 0.9546386483242373 entropy -11.101893989405259
epoch: 13, step: 110
	action: tensor([[-0.0012,  0.0217,  0.0031, -0.0072, -0.0119, -0.0126, -0.0336]],
       dtype=torch.float64)
	q_value: tensor([[5.7633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29733456117713497, distance: 0.9592481929430079 entropy -11.085270275244557
epoch: 13, step: 111
	action: tensor([[-0.0012,  0.0224,  0.0100, -0.0068,  0.0004, -0.0147,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[5.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29742276645504573, distance: 0.9591879840553759 entropy -11.104238460883536
epoch: 13, step: 112
	action: tensor([[-0.0010,  0.0219,  0.0129, -0.0072,  0.0025, -0.0012, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[5.7696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001590621582829, distance: 0.9573183088641563 entropy -11.093861135731661
epoch: 13, step: 113
	action: tensor([[-0.0011,  0.0219,  0.0106, -0.0070, -0.0082,  0.0051,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[5.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016165044171689, distance: 0.9563209655772955 entropy -11.1006706129568
epoch: 13, step: 114
	action: tensor([[-0.0010,  0.0216,  0.0127, -0.0073,  0.0020, -0.0336,  0.0232]],
       dtype=torch.float64)
	q_value: tensor([[5.7762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2923214342501733, distance: 0.9626639624218024 entropy -11.08758625341257
epoch: 13, step: 115
	action: tensor([[-0.0008,  0.0216,  0.0165, -0.0076, -0.0085, -0.0038,  0.0349]],
       dtype=torch.float64)
	q_value: tensor([[5.7873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993219868602648, distance: 0.9578906589417993 entropy -11.084649430605127
epoch: 13, step: 116
	action: tensor([[-0.0010,  0.0215,  0.0097, -0.0074, -0.0036,  0.0191, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[5.7839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30448926137335286, distance: 0.9543520554168458 entropy -11.087024685208773
epoch: 13, step: 117
	action: tensor([[-1.2323e-03,  2.1937e-02,  1.4749e-02, -6.7807e-03, -1.3581e-02,
         -9.7207e-07,  2.7195e-02]], dtype=torch.float64)
	q_value: tensor([[5.7563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30037536182287816, distance: 0.957170358365708 entropy -11.101737304892934
epoch: 13, step: 118
	action: tensor([[-0.0010,  0.0217,  0.0130, -0.0073, -0.0131, -0.0011,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[5.7751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29992450514399294, distance: 0.9574787216813494 entropy -11.088988234873693
epoch: 13, step: 119
	action: tensor([[-0.0010,  0.0216, -0.0007, -0.0073,  0.0004, -0.0118,  0.0223]],
       dtype=torch.float64)
	q_value: tensor([[5.7766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975156597853311, distance: 0.9591245710187901 entropy -11.086582722215324
epoch: 13, step: 120
	action: tensor([[-0.0010,  0.0218,  0.0188, -0.0073, -0.0103,  0.0034,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[5.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30104589569606977, distance: 0.9567115630571674 entropy -11.085522494618829
epoch: 13, step: 121
	action: tensor([[-0.0010,  0.0216,  0.0043, -0.0072, -0.0079,  0.0051,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[5.7786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30136071940623677, distance: 0.956496077229249 entropy -11.092595370111027
epoch: 13, step: 122
	action: tensor([[-0.0011,  0.0220,  0.0163, -0.0069, -0.0024,  0.0170,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[5.7579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044253424174087, distance: 0.9543959079284796 entropy -11.096573806793145
epoch: 13, step: 123
	action: tensor([[-0.0011,  0.0217, -0.0016, -0.0070,  0.0015,  0.0047,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[5.7693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013162268554147, distance: 0.9565265337709691 entropy -11.098336341386249
epoch: 13, step: 124
	action: tensor([[-0.0011,  0.0220,  0.0088, -0.0070, -0.0063, -0.0214,  0.0437]],
       dtype=torch.float64)
	q_value: tensor([[5.7585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955318250521952, distance: 0.960477913058947 entropy -11.09287093550125
epoch: 13, step: 125
	action: tensor([[-0.0009,  0.0215,  0.0210, -0.0076,  0.0042, -0.0143, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[5.7887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29660425391378786, distance: 0.9597465552875911 entropy -11.077693031528884
epoch: 13, step: 126
	action: tensor([[-0.0010,  0.0219,  0.0072, -0.0071, -0.0102,  0.0004, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[5.7689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005945668253457, distance: 0.95702039726034 entropy -11.10285419929716
epoch: 13, step: 127
	action: tensor([[-0.0012,  0.0222,  0.0124, -0.0068, -0.0188, -0.0234, -0.0001]],
       dtype=torch.float64)
	q_value: tensor([[5.7506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295333972767993, distance: 0.9606127803352975 entropy -11.102112900516994
LOSS epoch 13 actor 15.00569061002331 critic 13.540072061433722
epoch: 14, step: 0
	action: tensor([[-0.0016,  0.0228,  0.0111, -0.0070, -0.0025, -0.0105, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[6.3024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980655397493225, distance: 0.9587491130882836 entropy -11.088756546294302
epoch: 14, step: 1
	action: tensor([[-0.0017,  0.0228,  0.0142, -0.0069, -0.0014,  0.0031, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[6.2995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011944095107649, distance: 0.9566099165890581 entropy -11.092211776844296
epoch: 14, step: 2
	action: tensor([[-0.0018,  0.0228,  0.0096, -0.0067, -0.0051,  0.0280,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[6.2927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3069277822208366, distance: 0.9526775659481455 entropy -11.097307590498074
epoch: 14, step: 3
	action: tensor([[-0.0019,  0.0226,  0.0086, -0.0066, -0.0035, -0.0130, -0.0207]],
       dtype=torch.float64)
	q_value: tensor([[6.2889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972626801865208, distance: 0.9592972560840389 entropy -11.094013119564535
epoch: 14, step: 4
	action: tensor([[-0.0017,  0.0229,  0.0115, -0.0068, -0.0010, -0.0037,  0.0313]],
       dtype=torch.float64)
	q_value: tensor([[6.2912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997221628138659, distance: 0.9576170813868484 entropy -11.096319884009615
epoch: 14, step: 5
	action: tensor([[-0.0016,  0.0223,  0.0077, -0.0072, -0.0109,  0.0225, -0.0537]],
       dtype=torch.float64)
	q_value: tensor([[6.3189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30520240519677566, distance: 0.9538626561979306 entropy -11.080743705328985
epoch: 14, step: 6
	action: tensor([[-0.0022,  0.0232,  0.0067, -0.0061, -0.0010, -0.0063, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[6.2580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29918668603548426, distance: 0.9579831387526245 entropy -11.109248239969054
epoch: 14, step: 7
	action: tensor([[-0.0017,  0.0227,  0.0112, -0.0069, -0.0079,  0.0081, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[6.2988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302267672647506, distance: 0.955875027880587 entropy -11.0909651146404
epoch: 14, step: 8
	action: tensor([[-0.0018,  0.0228,  0.0123, -0.0067, -0.0115, -0.0214, -0.0316]],
       dtype=torch.float64)
	q_value: tensor([[6.2899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955568377498148, distance: 0.9604608616450458 entropy -11.095586137462352
epoch: 14, step: 9
	action: tensor([[-0.0017,  0.0230,  0.0094, -0.0067, -0.0092, -0.0010,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[6.2884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004704271750368, distance: 0.9571053257585732 entropy -11.098638484217107
epoch: 14, step: 10
	action: tensor([[-0.0017,  0.0226,  0.0164, -0.0069, -0.0013,  0.0011, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[6.3030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30058477459411204, distance: 0.9570270967594999 entropy -11.088758103791816
epoch: 14, step: 11
	action: tensor([[-0.0019,  0.0228,  0.0057, -0.0066, -0.0068, -0.0049,  0.0434]],
       dtype=torch.float64)
	q_value: tensor([[6.2884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993721799072069, distance: 0.9578563490940651 entropy -11.103479092796599
epoch: 14, step: 12
	action: tensor([[-0.0017,  0.0222, -0.0023, -0.0073, -0.0074, -0.0051, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[6.3187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29882037810669637, distance: 0.9582334700243081 entropy -11.074896602576045
epoch: 14, step: 13
	action: tensor([[-0.0018,  0.0231,  0.0064, -0.0067, -0.0170, -0.0066, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[6.2809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29918540270453986, distance: 0.957984015882681 entropy -11.092226468929924
epoch: 14, step: 14
	action: tensor([[-0.0018,  0.0231,  0.0131, -0.0066, -0.0162,  0.0018,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[6.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30104337698519124, distance: 0.9567132868310431 entropy -11.099199123506594
epoch: 14, step: 15
	action: tensor([[-0.0017,  0.0225,  0.0023, -0.0070,  0.0002, -0.0060,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[6.3073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989138318349456, distance: 0.9581696108707131 entropy -11.087432386368475
epoch: 14, step: 16
	action: tensor([[-0.0017,  0.0227,  0.0204, -0.0069,  0.0060, -0.0209, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[6.2971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29548644442647665, distance: 0.960508848726664 entropy -11.088657873637699
epoch: 14, step: 17
	action: tensor([[-0.0016,  0.0226,  0.0074, -0.0070, -0.0071,  0.0338, -0.0180]],
       dtype=torch.float64)
	q_value: tensor([[6.3127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30821271276759554, distance: 0.951794041568325 entropy -11.092737541066976
epoch: 14, step: 18
	action: tensor([[-0.0021,  0.0228,  0.0098, -0.0063, -0.0006,  0.0043, -0.0323]],
       dtype=torch.float64)
	q_value: tensor([[6.2759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30137235763393944, distance: 0.9564881103385995 entropy -11.102121328561056
epoch: 14, step: 19
	action: tensor([[-0.0019,  0.0230,  0.0079, -0.0065, -0.0135,  0.0069,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[6.2797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021272470485217, distance: 0.9559712127390306 entropy -11.10163912332413
epoch: 14, step: 20
	action: tensor([[-0.0017,  0.0223, -0.0049, -0.0072, -0.0037, -0.0172, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[6.3113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29615012119256356, distance: 0.9600563254207174 entropy -11.078031060422353
epoch: 14, step: 21
	action: tensor([[-0.0017,  0.0231,  0.0068, -0.0068,  0.0009,  0.0328,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[6.2830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3083578440676268, distance: 0.9516941970389244 entropy -11.092033363897784
epoch: 14, step: 22
	action: tensor([[-0.0019,  0.0222,  0.0089, -0.0069, -0.0081, -0.0096,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[6.3030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29765680481661183, distance: 0.9590282112421219 entropy -11.084867045403517
epoch: 14, step: 23
	action: tensor([[-0.0017,  0.0227,  0.0042, -0.0070, -0.0066, -0.0019,  0.0225]],
       dtype=torch.float64)
	q_value: tensor([[6.3010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29998475640456523, distance: 0.9574375185956309 entropy -11.087522322424267
epoch: 14, step: 24
	action: tensor([[-0.0017,  0.0226,  0.0090, -0.0071, -0.0008, -0.0009,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[6.3028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000259656444686, distance: 0.9574093364571359 entropy -11.08258271449118
epoch: 14, step: 25
	action: tensor([[-0.0017,  0.0227,  0.0031, -0.0069, -0.0156,  0.0024, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[6.2975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30098464275364867, distance: 0.9567534829157786 entropy -11.09089497913421
epoch: 14, step: 26
	action: tensor([[-0.0018,  0.0229,  0.0135, -0.0067, -0.0016, -0.0015, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[6.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001777206915709, distance: 0.9573055471967366 entropy -11.092877266169245
epoch: 14, step: 27
	action: tensor([[-0.0018,  0.0228,  0.0056, -0.0068, -0.0007, -0.0038, -0.0178]],
       dtype=torch.float64)
	q_value: tensor([[6.2944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29959726050392066, distance: 0.9577024783866203 entropy -11.09604980907792
epoch: 14, step: 28
	action: tensor([[-0.0018,  0.0230,  0.0134, -0.0067, -0.0080,  0.0113,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[6.2862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3031902798469398, distance: 0.9552428449698783 entropy -11.09561425785221
epoch: 14, step: 29
	action: tensor([[-0.0018,  0.0227,  0.0007, -0.0067, -0.0118,  0.0099, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[6.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026245794725758, distance: 0.9556305201038816 entropy -11.093678683060933
epoch: 14, step: 30
	action: tensor([[-0.0019,  0.0229,  0.0012, -0.0066, -0.0060, -0.0100, -0.0251]],
       dtype=torch.float64)
	q_value: tensor([[6.2809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29832321414397434, distance: 0.9585731224598493 entropy -11.094759953157562
epoch: 14, step: 31
	action: tensor([[-0.0018,  0.0231,  0.0068, -0.0067, -0.0072, -0.0079, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[6.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989228929043257, distance: 0.9581634190003204 entropy -11.095622310567878
epoch: 14, step: 32
	action: tensor([[-0.0018,  0.0230,  0.0062, -0.0066, -0.0134, -0.0106, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[6.2830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29826934842119046, distance: 0.9586099152141544 entropy -11.09792917459583
epoch: 14, step: 33
	action: tensor([[-0.0017,  0.0228,  0.0126, -0.0069, -0.0018,  0.0268, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[6.2959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30673891942350573, distance: 0.9528073598496027 entropy -11.091131083116498
epoch: 14, step: 34
	action: tensor([[-0.0020,  0.0227,  0.0089, -0.0065, -0.0078, -0.0003,  0.0346]],
       dtype=torch.float64)
	q_value: tensor([[6.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002418187379984, distance: 0.9572617054798176 entropy -11.099660877910589
epoch: 14, step: 35
	action: tensor([[-0.0017,  0.0223,  0.0049, -0.0072, -0.0107,  0.0002,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[6.3144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30009646070617546, distance: 0.9573611244333057 entropy -11.079869809800316
epoch: 14, step: 36
	action: tensor([[-0.0017,  0.0226, -0.0018, -0.0070,  0.0009, -0.0079,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[6.2989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29854928009402726, distance: 0.9584186936651785 entropy -11.08503913991592
epoch: 14, step: 37
	action: tensor([[-0.0017,  0.0229,  0.0132, -0.0069,  0.0018,  0.0044,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[6.2913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015137983396312, distance: 0.9563912825056075 entropy -11.087845158797222
epoch: 14, step: 38
	action: tensor([[-0.0017,  0.0224,  0.0030, -0.0070, -0.0010,  0.0088,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[6.3096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022017559232969, distance: 0.9559201789077447 entropy -11.0863093036259
epoch: 14, step: 39
	action: tensor([[-0.0018,  0.0227,  0.0151, -0.0069, -0.0084,  0.0033,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[6.2942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010137282721498, distance: 0.9567335778018793 entropy -11.086815027120915
epoch: 14, step: 40
	action: tensor([[-0.0017,  0.0225,  0.0173, -0.0069, -0.0054, -0.0010,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[6.3047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999885252784036, distance: 0.9574349411759919 entropy -11.089521846657775
epoch: 14, step: 41
	action: tensor([[-0.0016,  0.0223,  0.0139, -0.0071, -0.0043, -0.0105,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[6.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29766633350812666, distance: 0.9590217056512395 entropy -11.086061208157137
epoch: 14, step: 42
	action: tensor([[-0.0016,  0.0224,  0.0050, -0.0072, -0.0121,  0.0120,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[6.3160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30291413549303514, distance: 0.9554321066732034 entropy -11.082161975370406
epoch: 14, step: 43
	action: tensor([[-0.0019,  0.0228,  0.0020, -0.0067, -0.0044,  0.0021, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[6.2873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30094511261210677, distance: 0.9567805353013403 entropy -11.091640748780673
epoch: 14, step: 44
	action: tensor([[-0.0018,  0.0229,  0.0116, -0.0067, -0.0032, -0.0243,  0.0434]],
       dtype=torch.float64)
	q_value: tensor([[6.2842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949387516867964, distance: 0.9608821286424261 entropy -11.093148288461053
epoch: 14, step: 45
	action: tensor([[-0.0015,  0.0222,  0.0130, -0.0075, -0.0085, -0.0165,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[6.3288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296152873073217, distance: 0.9600544486263255 entropy -11.07278493120873
epoch: 14, step: 46
	action: tensor([[-0.0016,  0.0226, -0.0041, -0.0071, -0.0045, -0.0142,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[6.3121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29718131851380014, distance: 0.9593527873388893 entropy -11.08567739611317
epoch: 14, step: 47
	action: tensor([[-0.0017,  0.0229,  0.0092, -0.0070, -0.0184,  0.0048, -0.0176]],
       dtype=torch.float64)
	q_value: tensor([[6.2912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30161818756030623, distance: 0.9563198131831951 entropy -11.086028535868156
epoch: 14, step: 48
	action: tensor([[-0.0019,  0.0229,  0.0108, -0.0066, -0.0007,  0.0253,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[6.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064140136420056, distance: 0.9530306064487097 entropy -11.097501930238762
epoch: 14, step: 49
	action: tensor([[-0.0018,  0.0223,  0.0028, -0.0068, -0.0038, -0.0015, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[6.3032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997289166737944, distance: 0.957612463486007 entropy -11.08978863993453
epoch: 14, step: 50
	action: tensor([[-0.0018,  0.0230,  0.0144, -0.0066, -0.0136,  0.0012, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[6.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008711488843172, distance: 0.9568311501990833 entropy -11.09730923718086
epoch: 14, step: 51
	action: tensor([[-0.0018,  0.0227,  0.0054, -0.0068, -0.0046,  0.0134,  0.0597]],
       dtype=torch.float64)
	q_value: tensor([[6.2993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034517358748209, distance: 0.9550636156730044 entropy -11.09425944823889
epoch: 14, step: 52
	action: tensor([[-0.0018,  0.0219,  0.0084, -0.0073, -0.0065, -0.0225,  0.0341]],
       dtype=torch.float64)
	q_value: tensor([[6.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29441739978780646, distance: 0.9612373212945536 entropy -11.073064358517296
epoch: 14, step: 53
	action: tensor([[-0.0015,  0.0224,  0.0102, -0.0074, -0.0137, -0.0060, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[6.3175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987967564094972, distance: 0.958249610617628 entropy -11.076348600657028
epoch: 14, step: 54
	action: tensor([[-0.0018,  0.0228,  0.0119, -0.0068, -0.0042, -0.0187,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[6.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296182279354408, distance: 0.9600343931875264 entropy -11.094286603104768
epoch: 14, step: 55
	action: tensor([[-0.0016,  0.0226,  0.0051, -0.0071,  0.0040,  0.0180,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[6.3113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30451553919008034, distance: 0.9543340265612906 entropy -11.084890680314825
epoch: 14, step: 56
	action: tensor([[-0.0019,  0.0226,  0.0085, -0.0068, -0.0049,  0.0087, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[6.2921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022696303959972, distance: 0.9558736868475483 entropy -11.090268034315248
epoch: 14, step: 57
	action: tensor([[-0.0018,  0.0227,  0.0096, -0.0067, -0.0057,  0.0038, -0.0178]],
       dtype=torch.float64)
	q_value: tensor([[6.2921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012719873955245, distance: 0.9565568161025652 entropy -11.094466306189926
epoch: 14, step: 58
	action: tensor([[-0.0019,  0.0228,  0.0151, -0.0067, -0.0080,  0.0089,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[6.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30251121871319064, distance: 0.955708187452218 entropy -11.098027424950276
epoch: 14, step: 59
	action: tensor([[-0.0018,  0.0226,  0.0113, -0.0068,  0.0020,  0.0042, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[6.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30123723895999044, distance: 0.9565806010653972 entropy -11.093493156274702
epoch: 14, step: 60
	action: tensor([[-0.0018,  0.0227,  0.0015, -0.0068, -0.0160, -0.0083,  0.0331]],
       dtype=torch.float64)
	q_value: tensor([[6.2947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985641791303246, distance: 0.9584085150522511 entropy -11.094065620709154
epoch: 14, step: 61
	action: tensor([[-0.0017,  0.0225,  0.0142, -0.0072,  0.0043,  0.0119,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[6.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30292523686770045, distance: 0.9554244988213214 entropy -11.077605479246783
epoch: 14, step: 62
	action: tensor([[-0.0017,  0.0222, -0.0080, -0.0070, -0.0042,  0.0044,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[6.3114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010885898663149, distance: 0.9566823432343888 entropy -11.085823621582367
epoch: 14, step: 63
	action: tensor([[-0.0018,  0.0227,  0.0129, -0.0070, -0.0152, -0.0169, -0.0236]],
       dtype=torch.float64)
	q_value: tensor([[6.2943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963884813575578, distance: 0.9598937491608397 entropy -11.080324507813218
epoch: 14, step: 64
	action: tensor([[-0.0017,  0.0229,  0.0011, -0.0068, -0.0112, -0.0025, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[6.2931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30007883421799675, distance: 0.9573731795290091 entropy -11.097681978374007
epoch: 14, step: 65
	action: tensor([[-0.0018,  0.0229,  0.0059, -0.0067, -0.0095,  0.0038, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[6.2868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30143849918329324, distance: 0.9564428322110692 entropy -11.09287426882339
epoch: 14, step: 66
	action: tensor([[-0.0018,  0.0229,  0.0068, -0.0067, -0.0079,  0.0006,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[6.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006334267559899, distance: 0.956993810204327 entropy -11.093209737928905
epoch: 14, step: 67
	action: tensor([[-0.0017,  0.0226,  0.0122, -0.0070, -0.0059, -0.0143,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[6.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969849780965027, distance: 0.9594867810546872 entropy -11.085741947064705
epoch: 14, step: 68
	action: tensor([[-0.0016,  0.0225,  0.0107, -0.0072, -0.0064,  0.0172, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[6.3144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042162432835881, distance: 0.9545393492973452 entropy -11.084063710641185
epoch: 14, step: 69
	action: tensor([[-0.0019,  0.0227,  0.0220, -0.0067, -0.0049,  0.0103, -0.0049]],
       dtype=torch.float64)
	q_value: tensor([[6.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30272556517273685, distance: 0.9555613260168373 entropy -11.09462596471378
epoch: 14, step: 70
	action: tensor([[-0.0018,  0.0225,  0.0019, -0.0068, -0.0093,  0.0055,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.3011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015203241038529, distance: 0.9563868148449894 entropy -11.096972507945518
epoch: 14, step: 71
	action: tensor([[-0.0018,  0.0227,  0.0066, -0.0069, -0.0042, -0.0128,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[6.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973824660698333, distance: 0.959215493551193 entropy -11.085219189899627
epoch: 14, step: 72
	action: tensor([[-0.0017,  0.0228,  0.0047, -0.0070,  0.0013, -0.0043,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[6.2987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994982500581511, distance: 0.9577701674411524 entropy -11.089007578533606
epoch: 14, step: 73
	action: tensor([[-0.0017,  0.0227,  0.0097, -0.0070,  0.0003, -0.0172,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[6.3013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963778024331054, distance: 0.9599010334318216 entropy -11.087961921379408
epoch: 14, step: 74
	action: tensor([[-0.0016,  0.0225,  0.0053, -0.0072, -0.0067, -0.0042, -0.0298]],
       dtype=torch.float64)
	q_value: tensor([[6.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299293735522584, distance: 0.9579099698229624 entropy -11.081215629872291
epoch: 14, step: 75
	action: tensor([[-0.0019,  0.0231,  0.0057, -0.0066, -0.0027, -0.0118,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[6.2783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29799754484572294, distance: 0.9587955479611835 entropy -11.099100006337324
epoch: 14, step: 76
	action: tensor([[-0.0017,  0.0227,  0.0096, -0.0070, -0.0013, -0.0271, -0.0176]],
       dtype=torch.float64)
	q_value: tensor([[6.3011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942399035402351, distance: 0.9613582180462725 entropy -11.088913256133475
epoch: 14, step: 77
	action: tensor([[-0.0016,  0.0229,  0.0104, -0.0069, -0.0141,  0.0035,  0.0402]],
       dtype=torch.float64)
	q_value: tensor([[6.3001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30142842449979557, distance: 0.9564497291156832 entropy -11.093576966434464
epoch: 14, step: 78
	action: tensor([[-0.0017,  0.0222,  0.0135, -0.0072,  0.0029,  0.0016, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[6.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002776890102644, distance: 0.957237170091039 entropy -11.078861134300306
epoch: 14, step: 79
	action: tensor([[-0.0017,  0.0226,  0.0089, -0.0069,  0.0040,  0.0015, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[6.3025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007030225320054, distance: 0.95694619255541 entropy -11.093461467903376
epoch: 14, step: 80
	action: tensor([[-0.0018,  0.0227,  0.0076, -0.0068, -0.0023, -0.0124,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[6.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29757890936735176, distance: 0.9590813916976882 entropy -11.094368617806225
epoch: 14, step: 81
	action: tensor([[-0.0016,  0.0224,  0.0111, -0.0072, -0.0050,  0.0118,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[6.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028654071820408, distance: 0.955465499818594 entropy -11.080125661799759
epoch: 14, step: 82
	action: tensor([[-0.0018,  0.0225,  0.0211, -0.0069, -0.0148, -0.0213,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[6.3011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29529690684199106, distance: 0.9606380444552094 entropy -11.09066180850346
epoch: 14, step: 83
	action: tensor([[-0.0015,  0.0224,  0.0028, -0.0072, -0.0051,  0.0095,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[6.3240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023946548043742, distance: 0.9557880428069379 entropy -11.083296411842772
epoch: 14, step: 84
	action: tensor([[-0.0018,  0.0227,  0.0028, -0.0068, -0.0040,  0.0166, -0.0150]],
       dtype=torch.float64)
	q_value: tensor([[6.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30418631954580755, distance: 0.9545598751272546 entropy -11.088484163371763
epoch: 14, step: 85
	action: tensor([[-0.0020,  0.0229, -0.0024, -0.0065, -0.0053,  0.0222,  0.0331]],
       dtype=torch.float64)
	q_value: tensor([[6.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055932632641213, distance: 0.9535943209554241 entropy -11.096927286820527
epoch: 14, step: 86
	action: tensor([[-0.0019,  0.0224,  0.0093, -0.0070, -0.0013,  0.0163, -0.0045]],
       dtype=torch.float64)
	q_value: tensor([[6.2980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30376734382056025, distance: 0.9548472201744742 entropy -11.081362033110604
epoch: 14, step: 87
	action: tensor([[-0.0019,  0.0227,  0.0071, -0.0067, -0.0065,  0.0055,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[6.2912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301603643174591, distance: 0.9563297712115731 entropy -11.095479736300517
epoch: 14, step: 88
	action: tensor([[-0.0018,  0.0227,  0.0124, -0.0069, -0.0033, -0.0032, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[6.2954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29960519661683327, distance: 0.9576970526107728 entropy -11.089199827712248
epoch: 14, step: 89
	action: tensor([[-0.0018,  0.0229,  0.0086, -0.0067, -0.0069,  0.0134,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[6.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036154551190723, distance: 0.9549513682592252 entropy -11.097854409653479
epoch: 14, step: 90
	action: tensor([[-0.0018,  0.0227,  0.0046, -0.0067, -0.0010, -0.0088, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[6.2930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983663810864351, distance: 0.9585436364428709 entropy -11.093607509861616
epoch: 14, step: 91
	action: tensor([[-0.0017,  0.0229,  0.0039, -0.0069, -0.0083, -0.0115, -0.0099]],
       dtype=torch.float64)
	q_value: tensor([[6.2920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979554132375436, distance: 0.9588243192225755 entropy -11.09182545719636
epoch: 14, step: 92
	action: tensor([[-0.0017,  0.0229,  0.0030, -0.0068, -0.0108,  0.0193, -0.0121]],
       dtype=torch.float64)
	q_value: tensor([[6.2921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305006356176174, distance: 0.9539972210346636 entropy -11.092599861263752
epoch: 14, step: 93
	action: tensor([[-0.0020,  0.0229,  0.0058, -0.0065, -0.0017,  0.0093, -0.0045]],
       dtype=torch.float64)
	q_value: tensor([[6.2774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026452702143304, distance: 0.9556163434851008 entropy -11.096077204034023
epoch: 14, step: 94
	action: tensor([[-0.0018,  0.0228,  0.0086, -0.0067, -0.0042, -0.0134,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[6.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29734503268632495, distance: 0.9592410452926922 entropy -11.092489140119236
epoch: 14, step: 95
	action: tensor([[-0.0016,  0.0225,  0.0110, -0.0072,  0.0003, -0.0096, -0.0009]],
       dtype=torch.float64)
	q_value: tensor([[6.3109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980320437776025, distance: 0.958771988336535 entropy -11.081783159476004
epoch: 14, step: 96
	action: tensor([[-0.0017,  0.0227,  0.0169, -0.0069, -0.0059,  0.0063,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[6.3011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018577170617186, distance: 0.9561558008560255 entropy -11.091318352010175
epoch: 14, step: 97
	action: tensor([[-0.0017,  0.0222,  0.0346, -0.0071, -0.0143,  0.0057,  0.0368]],
       dtype=torch.float64)
	q_value: tensor([[6.3193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012005856180685, distance: 0.9566056892770366 entropy -11.08386221680551
epoch: 14, step: 98
	action: tensor([[-0.0016,  0.0219,  0.0192, -0.0072, -0.0041,  0.0156,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[6.3332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033945371010677, distance: 0.9551028285668156 entropy -11.087850877987416
epoch: 14, step: 99
	action: tensor([[-0.0018,  0.0225,  0.0176, -0.0068, -0.0124, -0.0108,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[6.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976928410069313, distance: 0.9590036077671452 entropy -11.09410319716701
epoch: 14, step: 100
	action: tensor([[-0.0017,  0.0226,  0.0063, -0.0070, -0.0174, -0.0023,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[6.3081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29985769214181357, distance: 0.9575244099693548 entropy -11.090943863363568
epoch: 14, step: 101
	action: tensor([[-0.0018,  0.0228,  0.0142, -0.0069, -0.0048, -0.0115,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[6.2941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29776888629987797, distance: 0.9589516862655615 entropy -11.088893071716338
epoch: 14, step: 102
	action: tensor([[-0.0016,  0.0226,  0.0066, -0.0070, -0.0035,  0.0019, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[6.3106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007865680793025, distance: 0.9568890272994532 entropy -11.088647635041104
epoch: 14, step: 103
	action: tensor([[-0.0018,  0.0228,  0.0084, -0.0068, -0.0114,  0.0048, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[6.2935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015596215267474, distance: 0.9563599106507438 entropy -11.093098096897894
epoch: 14, step: 104
	action: tensor([[-1.9044e-03,  2.2942e-02,  7.0733e-03, -6.5798e-03, -1.2699e-03,
          4.2058e-05,  5.0357e-03]], dtype=torch.float64)
	q_value: tensor([[6.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005580022571701, distance: 0.9570454132085717 entropy -11.099540407884367
epoch: 14, step: 105
	action: tensor([[-0.0017,  0.0227, -0.0003, -0.0069, -0.0078,  0.0071, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[6.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30202563921009784, distance: 0.9560408032430081 entropy -11.08876694037099
epoch: 14, step: 106
	action: tensor([[-0.0020,  0.0232,  0.0109, -0.0063, -0.0110,  0.0039,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[6.2652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30157322080101534, distance: 0.9563506000034545 entropy -11.103579632154409
epoch: 14, step: 107
	action: tensor([[-0.0017,  0.0224,  0.0167, -0.0070, -0.0070, -0.0222,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[6.3089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950380882550876, distance: 0.9608144365832703 entropy -11.085346627749603
epoch: 14, step: 108
	action: tensor([[-0.0016,  0.0226,  0.0204, -0.0071, -0.0079, -0.0072,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[6.3130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29863380834260633, distance: 0.9583609448511432 entropy -11.08705018830167
epoch: 14, step: 109
	action: tensor([[-0.0016,  0.0223,  0.0004, -0.0072, -0.0081,  0.0053,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[6.3195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013797973804677, distance: 0.956483017462501 entropy -11.08463101202324
epoch: 14, step: 110
	action: tensor([[-0.0018,  0.0226,  0.0051, -0.0070, -0.0016,  0.0054, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[6.2976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30151902165712086, distance: 0.9563877065261248 entropy -11.084343101453813
epoch: 14, step: 111
	action: tensor([[-0.0018,  0.0228,  0.0093, -0.0067, -0.0042,  0.0063, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[6.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019064317061798, distance: 0.956122441178784 entropy -11.09433702722494
epoch: 14, step: 112
	action: tensor([[-0.0019,  0.0228,  0.0013, -0.0067,  0.0006, -0.0020, -0.0317]],
       dtype=torch.float64)
	q_value: tensor([[6.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30004781752280163, distance: 0.9573943920773511 entropy -11.097428942976634
epoch: 14, step: 113
	action: tensor([[-0.0019,  0.0231,  0.0140, -0.0065, -0.0136, -0.0236, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29533657724233964, distance: 0.9606110051016702 entropy -11.098802696194792
epoch: 14, step: 114
	action: tensor([[-0.0016,  0.0227,  0.0130, -0.0070, -0.0075,  0.0122,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[6.3060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303273372508309, distance: 0.9551858882180124 entropy -11.090889829772438
epoch: 14, step: 115
	action: tensor([[-0.0018,  0.0225,  0.0169, -0.0069, -0.0129, -0.0060,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[6.3033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987975414169606, distance: 0.9582490742301302 entropy -11.09073879858108
epoch: 14, step: 116
	action: tensor([[-0.0016,  0.0223,  0.0061, -0.0073,  0.0011, -0.0029, -0.0338]],
       dtype=torch.float64)
	q_value: tensor([[6.3204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993899315189329, distance: 0.9578442145475803 entropy -11.079812507990543
epoch: 14, step: 117
	action: tensor([[-0.0019,  0.0230,  0.0096, -0.0066, -0.0041,  0.0096, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[6.2801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30287432719591756, distance: 0.9554593870873741 entropy -11.101101024412518
epoch: 14, step: 118
	action: tensor([[-0.0019,  0.0229,  0.0175, -0.0066, -0.0092,  0.0114, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[6.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030899318215159, distance: 0.955311625068427 entropy -11.098994467106204
epoch: 14, step: 119
	action: tensor([[-0.0018,  0.0226,  0.0161, -0.0067, -0.0039,  0.0048, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[6.2965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014169989941633, distance: 0.9564575507029532 entropy -11.095943816953797
epoch: 14, step: 120
	action: tensor([[-0.0018,  0.0227,  0.0046, -0.0067, -0.0028,  0.0011, -0.0338]],
       dtype=torch.float64)
	q_value: tensor([[6.2930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30069119524677224, distance: 0.956954284988297 entropy -11.097766342825157
epoch: 14, step: 121
	action: tensor([[-0.0019,  0.0231,  0.0068, -0.0065, -0.0108,  0.0143,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[6.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039374100727459, distance: 0.9547305945003157 entropy -11.100665471848881
epoch: 14, step: 122
	action: tensor([[-0.0018,  0.0225,  0.0079, -0.0069, -0.0025, -0.0074,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[6.2981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984941441814336, distance: 0.9584563600681467 entropy -11.08603720411676
epoch: 14, step: 123
	action: tensor([[-0.0017,  0.0227,  0.0024, -0.0070, -0.0174,  0.0005,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[6.3020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30054343121539884, distance: 0.9570553819237124 entropy -11.08936615561493
epoch: 14, step: 124
	action: tensor([[-0.0018,  0.0228,  0.0116, -0.0068, -0.0074,  0.0065,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[6.2925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019040254982539, distance: 0.9561240889718007 entropy -11.088756405976742
epoch: 14, step: 125
	action: tensor([[-0.0018,  0.0226,  0.0013, -0.0068,  0.0006,  0.0164,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[6.2995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041429257279705, distance: 0.9545896398133391 entropy -11.092521504458821
epoch: 14, step: 126
	action: tensor([[-0.0019,  0.0227,  0.0072, -0.0067, -0.0061,  0.0110,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[6.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302845965243949, distance: 0.9554788229069727 entropy -11.090651087209318
epoch: 14, step: 127
	action: tensor([[-0.0018,  0.0227,  0.0118, -0.0068, -0.0104, -0.0067, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[6.2941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29878894181695903, distance: 0.9582549502316857 entropy -11.089713986138678
LOSS epoch 14 actor 18.05189995890664 critic 12.948345012877796
epoch: 15, step: 0
	action: tensor([[-0.0022,  0.0228,  0.0044, -0.0073, -0.0091,  0.0090,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.1594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30188895763309864, distance: 0.9561344075179833 entropy -11.092108563896272
epoch: 15, step: 1
	action: tensor([[-0.0022,  0.0226,  0.0087, -0.0074, -0.0084,  0.0022,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.1618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001560778861645, distance: 0.9573203499674537 entropy -11.083617207278662
epoch: 15, step: 2
	action: tensor([[-0.0021,  0.0225,  0.0114, -0.0076, -0.0100,  0.0007, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[7.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29976537331536834, distance: 0.95758753614746 entropy -11.082494187830404
epoch: 15, step: 3
	action: tensor([[-0.0022,  0.0228,  0.0118, -0.0073,  0.0035, -0.0015,  0.0134]],
       dtype=torch.float64)
	q_value: tensor([[7.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994243836603526, distance: 0.9578206635086222 entropy -11.091764988901895
epoch: 15, step: 4
	action: tensor([[-0.0020,  0.0223,  0.0084, -0.0077, -0.0058,  0.0095, -0.0477]],
       dtype=torch.float64)
	q_value: tensor([[7.1857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016533876006645, distance: 0.956295712526794 entropy -11.079500756245753
epoch: 15, step: 5
	action: tensor([[-0.0024,  0.0229,  0.0044, -0.0070, -0.0134, -0.0116,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[7.1391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29723147379913795, distance: 0.9593185555572621 entropy -11.09881203032977
epoch: 15, step: 6
	action: tensor([[-0.0020,  0.0223,  0.0014, -0.0079, -0.0072, -0.0026, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[7.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988622834536663, distance: 0.9582048356310928 entropy -11.070072161176332
epoch: 15, step: 7
	action: tensor([[-0.0022,  0.0229,  0.0178, -0.0073, -0.0007, -0.0128,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[7.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968496577375823, distance: 0.9595791203676127 entropy -11.088197888939828
epoch: 15, step: 8
	action: tensor([[-0.0019,  0.0221,  0.0180, -0.0080, -0.0080,  0.0325, -0.0245]],
       dtype=torch.float64)
	q_value: tensor([[7.2043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068756570632186, distance: 0.952713390161955 entropy -11.074540067185707
epoch: 15, step: 9
	action: tensor([[-0.0024,  0.0225,  0.0022, -0.0070,  0.0004, -0.0218, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[7.1525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29456719619591776, distance: 0.9611352797042658 entropy -11.09828532221612
epoch: 15, step: 10
	action: tensor([[-0.0019,  0.0227,  0.0159, -0.0077, -0.0145, -0.0115, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[7.1758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29709437932685145, distance: 0.9594121218261183 entropy -11.080493073849345
epoch: 15, step: 11
	action: tensor([[-0.0021,  0.0227,  0.0020, -0.0075, -0.0092, -0.0111, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[7.1694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973342643668343, distance: 0.9592483955392215 entropy -11.088734521360255
epoch: 15, step: 12
	action: tensor([[-0.0020,  0.0227,  0.0164, -0.0076, -0.0051,  0.0151, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[7.1676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032467373925154, distance: 0.9552041459123416 entropy -11.082849529799075
epoch: 15, step: 13
	action: tensor([[-0.0022,  0.0226, -0.0004, -0.0072, -0.0090, -0.0015, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993288901308413, distance: 0.9578859402303764 entropy -11.093428564878637
epoch: 15, step: 14
	action: tensor([[-0.0021,  0.0227,  0.0068, -0.0075, -0.0033, -0.0048,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[7.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298618888180628, distance: 0.9583711384024268 entropy -11.082233388492543
epoch: 15, step: 15
	action: tensor([[-0.0020,  0.0225,  0.0155, -0.0077, -0.0016,  0.0138,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[7.1806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027364602807189, distance: 0.9555538605312414 entropy -11.079765029797494
epoch: 15, step: 16
	action: tensor([[-0.0020,  0.0221,  0.0080, -0.0077, -0.0086,  0.0004,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.1900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299353015162683, distance: 0.9578694494493201 entropy -11.080600513028168
epoch: 15, step: 17
	action: tensor([[-0.0020,  0.0225,  0.0197, -0.0076, -0.0133,  0.0044, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[7.1752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300531780407549, distance: 0.957063352698755 entropy -11.079897844607142
epoch: 15, step: 18
	action: tensor([[-0.0021,  0.0226,  0.0116, -0.0074,  0.0040,  0.0047,  0.0523]],
       dtype=torch.float64)
	q_value: tensor([[7.1692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30070696795985297, distance: 0.9569434930102927 entropy -11.09061939744336
epoch: 15, step: 19
	action: tensor([[-0.0020,  0.0218,  0.0068, -0.0081, -0.0018,  0.0038,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.2064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29981462354284116, distance: 0.9575538601255428 entropy -11.06768383430018
epoch: 15, step: 20
	action: tensor([[-0.0021,  0.0225, -0.0012, -0.0076, -0.0095, -0.0092, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.1746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975336014377412, distance: 0.9591123227816668 entropy -11.081902092121277
epoch: 15, step: 21
	action: tensor([[-0.0021,  0.0228,  0.0026, -0.0076, -0.0137,  0.0018, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[7.1625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002165619044692, distance: 0.9572789808629627 entropy -11.08092953572139
epoch: 15, step: 22
	action: tensor([[-0.0021,  0.0227,  0.0094, -0.0074, -0.0171,  0.0029,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004030678465853, distance: 0.9571514055975027 entropy -11.084619010476471
epoch: 15, step: 23
	action: tensor([[-0.0021,  0.0225,  0.0012, -0.0075, -0.0024, -0.0143,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[7.1727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29640153637502265, distance: 0.9598848440424212 entropy -11.08348741576175
epoch: 15, step: 24
	action: tensor([[-0.0020,  0.0226,  0.0046, -0.0077, -0.0045, -0.0061, -0.0206]],
       dtype=torch.float64)
	q_value: tensor([[7.1780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982973381510725, distance: 0.9585907971236973 entropy -11.077515631038027
epoch: 15, step: 25
	action: tensor([[-0.0021,  0.0229,  0.0060, -0.0074, -0.0075, -0.0026,  0.0375]],
       dtype=torch.float64)
	q_value: tensor([[7.1563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992788106075944, distance: 0.9579201714220229 entropy -11.0881597952304
epoch: 15, step: 26
	action: tensor([[-0.0020,  0.0222,  0.0028, -0.0079, -0.0073,  0.0245, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[7.1926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048822038357448, distance: 0.9540824273525639 entropy -11.069337183787919
epoch: 15, step: 27
	action: tensor([[-0.0023,  0.0228,  0.0069, -0.0071, -0.0100,  0.0146, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[7.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303071741707141, distance: 0.9553240923257731 entropy -11.090665923628555
epoch: 15, step: 28
	action: tensor([[-0.0022,  0.0226,  0.0114, -0.0073, -0.0050,  0.0090, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[7.1577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017215201776129, distance: 0.9562490619958748 entropy -11.08677983876341
epoch: 15, step: 29
	action: tensor([[-2.1143e-03,  2.2513e-02,  9.1303e-03, -7.4426e-03, -2.6990e-03,
         -6.7379e-05, -1.7156e-02]], dtype=torch.float64)
	q_value: tensor([[7.1700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995846315327577, distance: 0.9577111125207979 entropy -11.087130396836276
epoch: 15, step: 30
	action: tensor([[-0.0021,  0.0227,  0.0119, -0.0074, -0.0088,  0.0083,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[7.1603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016533051933358, distance: 0.9562957689499021 entropy -11.08883293680446
epoch: 15, step: 31
	action: tensor([[-0.0021,  0.0224,  0.0062, -0.0076,  0.0018, -0.0147,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[7.1787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29608101797098285, distance: 0.9601034529110333 entropy -11.080806653452026
epoch: 15, step: 32
	action: tensor([[-0.0019,  0.0225,  0.0087, -0.0077, -0.0058,  0.0277, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[7.1796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060795148177017, distance: 0.953260389919782 entropy -11.07957270635741
epoch: 15, step: 33
	action: tensor([[-0.0025,  0.0229,  0.0136, -0.0069, -0.0130,  0.0036, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[7.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30057436615294764, distance: 0.9570342177963378 entropy -11.098609329219356
epoch: 15, step: 34
	action: tensor([[-0.0021,  0.0225,  0.0150, -0.0075, -0.0131,  0.0244,  0.0431]],
       dtype=torch.float64)
	q_value: tensor([[7.1733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051819710278656, distance: 0.9538766827629834 entropy -11.086541640946534
epoch: 15, step: 35
	action: tensor([[-0.0021,  0.0219, -0.0050, -0.0077, -0.0127, -0.0069, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.1938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29752927568973, distance: 0.9591152758566069 entropy -11.076567110015626
epoch: 15, step: 36
	action: tensor([[-0.0022,  0.0230,  0.0165, -0.0074, -0.0069,  0.0085, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30187056858441097, distance: 0.9561470002756687 entropy -11.085398782754552
epoch: 15, step: 37
	action: tensor([[-0.0021,  0.0225,  0.0183, -0.0074, -0.0091, -0.0036, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[7.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986956584389401, distance: 0.9583186873084273 entropy -11.089242081178119
epoch: 15, step: 38
	action: tensor([[-0.0020,  0.0225,  0.0133, -0.0076, -0.0099, -0.0070,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.1809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979558302760219, distance: 0.9588240344353255 entropy -11.087065278764346
epoch: 15, step: 39
	action: tensor([[-0.0020,  0.0225,  0.0139, -0.0076, -0.0074,  0.0074,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[7.1778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30132573791126593, distance: 0.9565200232374512 entropy -11.083248642950254
epoch: 15, step: 40
	action: tensor([[-0.0020,  0.0223,  0.0109, -0.0077, -0.0057, -0.0087,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[7.1838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973868892726701, distance: 0.9592124742617754 entropy -11.079799293280322
epoch: 15, step: 41
	action: tensor([[-0.0019,  0.0222,  0.0219, -0.0079, -0.0025,  0.0073,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[7.1965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30100755803687684, distance: 0.956737800530193 entropy -11.073597367483805
epoch: 15, step: 42
	action: tensor([[-0.0020,  0.0222,  0.0079, -0.0076, -0.0016, -0.0165,  0.0147]],
       dtype=torch.float64)
	q_value: tensor([[7.1880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955963661936585, distance: 0.9604339140800233 entropy -11.084775361384896
epoch: 15, step: 43
	action: tensor([[-0.0019,  0.0224,  0.0014, -0.0078, -0.0101,  0.0060, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[7.1852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30097857040214016, distance: 0.9567576385689969 entropy -11.076938895865066
epoch: 15, step: 44
	action: tensor([[-0.0022,  0.0227,  0.0167, -0.0074, -0.0066, -0.0031,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[7.1571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29896357905267956, distance: 0.9581356156786662 entropy -11.0860787279497
epoch: 15, step: 45
	action: tensor([[-0.0019,  0.0219,  0.0022, -0.0080, -0.0100,  0.0209,  0.0254]],
       dtype=torch.float64)
	q_value: tensor([[7.2086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038894581507564, distance: 0.9547634797459883 entropy -11.071451025181469
epoch: 15, step: 46
	action: tensor([[-0.0022,  0.0224,  0.0091, -0.0076, -0.0066, -0.0146, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[7.1684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29603420189884133, distance: 0.9601353795434548 entropy -11.077372242645223
epoch: 15, step: 47
	action: tensor([[-0.0020,  0.0226,  0.0082, -0.0076, -0.0068, -0.0205, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.1759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29508206110863333, distance: 0.9607844701335473 entropy -11.084139854452017
epoch: 15, step: 48
	action: tensor([[-0.0020,  0.0227,  0.0147, -0.0076, -0.0049,  0.0033,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[7.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005774300405739, distance: 0.9570321216131503 entropy -11.085347951182568
epoch: 15, step: 49
	action: tensor([[-0.0020,  0.0223,  0.0068, -0.0077,  0.0012, -0.0163,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[7.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29567987608703994, distance: 0.9603769807343997 entropy -11.08096357581634
epoch: 15, step: 50
	action: tensor([[-0.0019,  0.0225,  0.0012, -0.0077, -0.0042,  0.0016,  0.0254]],
       dtype=torch.float64)
	q_value: tensor([[7.1811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000439210006822, distance: 0.9573970569045546 entropy -11.080860538202899
epoch: 15, step: 51
	action: tensor([[-0.0020,  0.0224,  0.0074, -0.0078, -0.0051,  0.0001, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[7.1807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994784312349512, distance: 0.9577837161177671 entropy -11.074393975975424
epoch: 15, step: 52
	action: tensor([[-0.0022,  0.0228,  0.0183, -0.0073, -0.0033, -0.0021, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[7.1559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29928935090719966, distance: 0.9579129668421366 entropy -11.091271001562498
epoch: 15, step: 53
	action: tensor([[-0.0020,  0.0225,  0.0055, -0.0075, -0.0091, -0.0241, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[7.1793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2941806120185051, distance: 0.961398599469683 entropy -11.086760821143729
epoch: 15, step: 54
	action: tensor([[-0.0020,  0.0228,  0.0077, -0.0075, -0.0062,  0.0033,  0.0330]],
       dtype=torch.float64)
	q_value: tensor([[7.1643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30067221144868217, distance: 0.9569672738878473 entropy -11.087398436612238
epoch: 15, step: 55
	action: tensor([[-0.0020,  0.0222,  0.0021, -0.0079, -0.0141,  0.0042, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.1923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002372292215897, distance: 0.957264844679343 entropy -11.07278385135127
epoch: 15, step: 56
	action: tensor([[-0.0022,  0.0228,  0.0122, -0.0074, -0.0023, -0.0071, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[7.1536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981198197108579, distance: 0.958712042767595 entropy -11.085545726272215
epoch: 15, step: 57
	action: tensor([[-0.0020,  0.0226,  0.0081, -0.0075, -0.0045,  0.0067, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[7.1751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30123645582146374, distance: 0.9565811371092124 entropy -11.086737759038586
epoch: 15, step: 58
	action: tensor([[-0.0022,  0.0228,  0.0168, -0.0072, -0.0070,  0.0062, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[7.1509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30118727788788036, distance: 0.9566147978921414 entropy -11.09198344207991
epoch: 15, step: 59
	action: tensor([[-0.0021,  0.0225,  0.0054, -0.0074, -0.0148, -0.0062,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[7.1707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982096403369573, distance: 0.958650696918618 entropy -11.090495769464786
epoch: 15, step: 60
	action: tensor([[-0.0020,  0.0225,  0.0128, -0.0077, -0.0047,  0.0146,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[7.1771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029516501809232, distance: 0.9554063973443028 entropy -11.078749623382665
epoch: 15, step: 61
	action: tensor([[-0.0021,  0.0223,  0.0145, -0.0076, -0.0022, -0.0028, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[7.1807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986796582977148, distance: 0.9583296191863381 entropy -11.08072838131225
epoch: 15, step: 62
	action: tensor([[-0.0020,  0.0225,  0.0053, -0.0075,  0.0012, -0.0077,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.1751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29788193924213546, distance: 0.9588744918268611 entropy -11.085965394099762
epoch: 15, step: 63
	action: tensor([[-0.0020,  0.0225,  0.0099, -0.0077, -0.0129,  0.0102,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[7.1776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30193792496671246, distance: 0.956100874046438 entropy -11.08097428047488
epoch: 15, step: 64
	action: tensor([[-0.0021,  0.0222,  0.0017, -0.0077, -0.0079,  0.0168,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[7.1849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30318468125914, distance: 0.9552466824595407 entropy -11.077129852025854
epoch: 15, step: 65
	action: tensor([[-0.0022,  0.0225,  0.0059, -0.0075, -0.0017, -0.0051,  0.0473]],
       dtype=torch.float64)
	q_value: tensor([[7.1621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983674891346244, distance: 0.9585428795571321 entropy -11.080527125820241
epoch: 15, step: 66
	action: tensor([[-1.9645e-03,  2.2028e-02,  1.9841e-02, -8.0724e-03, -1.4719e-02,
          8.3619e-05,  3.4191e-02]], dtype=torch.float64)
	q_value: tensor([[7.2001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990534519072673, distance: 0.9580741972271392 entropy -11.065784251354417
epoch: 15, step: 67
	action: tensor([[-0.0019,  0.0221,  0.0077, -0.0079, -0.0114, -0.0064,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[7.2011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29782689062545353, distance: 0.9589120807187504 entropy -11.076457230954457
epoch: 15, step: 68
	action: tensor([[-0.0020,  0.0221,  0.0125, -0.0080, -0.0091,  0.0055,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[7.1993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004032458769632, distance: 0.9571512838116361 entropy -11.068053580964795
epoch: 15, step: 69
	action: tensor([[-0.0020,  0.0224, -0.0034, -0.0076, -0.0200,  0.0042,  0.0282]],
       dtype=torch.float64)
	q_value: tensor([[7.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004891825739495, distance: 0.9570924949840017 entropy -11.082485378992178
epoch: 15, step: 70
	action: tensor([[-0.0021,  0.0225, -0.0033, -0.0077, -0.0054, -0.0018, -0.0169]],
       dtype=torch.float64)
	q_value: tensor([[7.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991228865429755, distance: 0.9580267434087582 entropy -11.072838656245422
epoch: 15, step: 71
	action: tensor([[-0.0022,  0.0229,  0.0131, -0.0074, -0.0158, -0.0061,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[7.1519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298452395824251, distance: 0.9584848797040252 entropy -11.086282117641662
epoch: 15, step: 72
	action: tensor([[-0.0020,  0.0223,  0.0093, -0.0078, -0.0041,  0.0020, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[7.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29992506016697085, distance: 0.9574783421345715 entropy -11.07726502278629
epoch: 15, step: 73
	action: tensor([[-0.0021,  0.0227,  0.0083, -0.0074, -0.0064, -0.0125, -0.0430]],
       dtype=torch.float64)
	q_value: tensor([[7.1641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29687264835309046, distance: 0.9595634327588501 entropy -11.088857167771522
epoch: 15, step: 74
	action: tensor([[-0.0021,  0.0229,  0.0137, -0.0072, -0.0013,  0.0033,  0.0297]],
       dtype=torch.float64)
	q_value: tensor([[7.1510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30068108847006825, distance: 0.9569612001653002 entropy -11.094886880897985
epoch: 15, step: 75
	action: tensor([[-0.0020,  0.0221, -0.0015, -0.0078, -0.0004, -0.0194, -0.0150]],
       dtype=torch.float64)
	q_value: tensor([[7.1975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949011881345328, distance: 0.9609077247629125 entropy -11.075653620534506
epoch: 15, step: 76
	action: tensor([[-0.0020,  0.0229,  0.0052, -0.0076, -0.0054,  0.0069,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[7.1620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30151529736036353, distance: 0.956390256249737 entropy -11.084315800600539
epoch: 15, step: 77
	action: tensor([[-0.0021,  0.0225,  0.0064, -0.0076, -0.0125, -0.0015, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[7.1726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992362162123028, distance: 0.9579492852914352 entropy -11.081598232088714
epoch: 15, step: 78
	action: tensor([[-0.0021,  0.0227,  0.0096, -0.0075,  0.0022,  0.0028,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[7.1625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30040115476055207, distance: 0.9571527142880142 entropy -11.084936199279138
epoch: 15, step: 79
	action: tensor([[-0.0020,  0.0222,  0.0141, -0.0078, -0.0106,  0.0108,  0.0325]],
       dtype=torch.float64)
	q_value: tensor([[7.1890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30183237191485535, distance: 0.956173156694404 entropy -11.077623891813582
epoch: 15, step: 80
	action: tensor([[-0.0020,  0.0222,  0.0149, -0.0078, -0.0116,  0.0161,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[7.1887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30293139123319435, distance: 0.9554202811642946 entropy -11.076525588088447
epoch: 15, step: 81
	action: tensor([[-0.0021,  0.0222,  0.0113, -0.0076, -0.0076, -0.0061,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[7.1838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979199906824591, distance: 0.9588485082694613 entropy -11.081646112064975
epoch: 15, step: 82
	action: tensor([[-0.0020,  0.0225,  0.0142, -0.0076, -0.0041,  0.0174, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[7.1788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036515173036195, distance: 0.9549266419219037 entropy -11.083231777272179
epoch: 15, step: 83
	action: tensor([[-0.0022,  0.0224,  0.0052, -0.0074, -0.0028, -0.0005, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[7.1685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994409712726148, distance: 0.9578093242245687 entropy -11.087643683690292
epoch: 15, step: 84
	action: tensor([[-0.0021,  0.0226, -0.0012, -0.0075, -0.0033,  0.0130, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[7.1679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026979956434712, distance: 0.9555802167965823 entropy -11.08448816985563
epoch: 15, step: 85
	action: tensor([[-0.0022,  0.0228,  0.0103, -0.0073, -0.0110,  0.0013, -0.0160]],
       dtype=torch.float64)
	q_value: tensor([[7.1518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000058151040844, distance: 0.9574231170944946 entropy -11.086754554265255
epoch: 15, step: 86
	action: tensor([[-0.0021,  0.0227,  0.0093, -0.0074, -0.0050,  0.0108, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[7.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30222148872194854, distance: 0.9559066627417653 entropy -11.08905614246623
epoch: 15, step: 87
	action: tensor([[-0.0023,  0.0228,  0.0065, -0.0072, -0.0064, -0.0102,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.1511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29740352550318405, distance: 0.959201118243754 entropy -11.094362845215437
epoch: 15, step: 88
	action: tensor([[-0.0020,  0.0225, -0.0022, -0.0078, -0.0061,  0.0111, -0.0118]],
       dtype=torch.float64)
	q_value: tensor([[7.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021362056225506, distance: 0.9559650768307814 entropy -11.076283787386341
epoch: 15, step: 89
	action: tensor([[-0.0022,  0.0228,  0.0056, -0.0073, -0.0142, -0.0146,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[7.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965231672403781, distance: 0.9598018729471163 entropy -11.086238570679066
epoch: 15, step: 90
	action: tensor([[-0.0020,  0.0224,  0.0074, -0.0078, -0.0037,  0.0024,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[7.1857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001151191057204, distance: 0.9573483634280983 entropy -11.074927972054867
epoch: 15, step: 91
	action: tensor([[-0.0020,  0.0225,  0.0041, -0.0076,  0.0032,  0.0093,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[7.1760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017380545716728, distance: 0.9562377405153588 entropy -11.082060080246029
epoch: 15, step: 92
	action: tensor([[-0.0021,  0.0224,  0.0088, -0.0076, -0.0036, -0.0038,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[7.1754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985634701308615, distance: 0.9584089994236865 entropy -11.079878748936
epoch: 15, step: 93
	action: tensor([[-0.0020,  0.0221,  0.0076, -0.0080, -0.0017, -0.0214, -0.0199]],
       dtype=torch.float64)
	q_value: tensor([[7.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29428104928946053, distance: 0.9613301940896223 entropy -11.069523735063635
epoch: 15, step: 94
	action: tensor([[-0.0020,  0.0228,  0.0036, -0.0075, -0.0073, -0.0162,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[7.1681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29626738926913887, distance: 0.9599763448368912 entropy -11.087413276670654
epoch: 15, step: 95
	action: tensor([[-0.0020,  0.0227,  0.0083, -0.0077, -0.0030,  0.0024, -0.0330]],
       dtype=torch.float64)
	q_value: tensor([[7.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30032068023876124, distance: 0.9572077631153078 entropy -11.080280800083827
epoch: 15, step: 96
	action: tensor([[-0.0022,  0.0228,  0.0103, -0.0072, -0.0039,  0.0075, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.1531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30153455344667557, distance: 0.9563770730975637 entropy -11.093533244182765
epoch: 15, step: 97
	action: tensor([[-0.0022,  0.0226,  0.0188, -0.0073, -0.0089, -0.0006,  0.0468]],
       dtype=torch.float64)
	q_value: tensor([[7.1626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994711634006705, distance: 0.9577886845552722 entropy -11.089983565996466
epoch: 15, step: 98
	action: tensor([[-0.0020,  0.0219, -0.0012, -0.0080, -0.0101, -0.0070, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[7.2090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29751245181728636, distance: 0.9591267609875097 entropy -11.070386885058754
epoch: 15, step: 99
	action: tensor([[-0.0021,  0.0228,  0.0149, -0.0075, -0.0029,  0.0066,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.1565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013331195200527, distance: 0.9565149703282196 entropy -11.083273328435194
epoch: 15, step: 100
	action: tensor([[-0.0020,  0.0223,  0.0007, -0.0076,  0.0006,  0.0126,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023810033366642, distance: 0.9557973946889589 entropy -11.08293306086013
epoch: 15, step: 101
	action: tensor([[-0.0021,  0.0224,  0.0198, -0.0076, -0.0105,  0.0198,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.1710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039797224286407, distance: 0.9547015759068626 entropy -11.076577687617311
epoch: 15, step: 102
	action: tensor([[-0.0021,  0.0223,  0.0023, -0.0075, -0.0099,  0.0297,  0.0191]],
       dtype=torch.float64)
	q_value: tensor([[7.1788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30619044017368635, distance: 0.9531841960499936 entropy -11.088085053150868
epoch: 15, step: 103
	action: tensor([[-0.0022,  0.0224,  0.0066, -0.0074, -0.0077,  0.0004, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[7.1609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29950738213824335, distance: 0.9577639244429118 entropy -11.080454601763671
epoch: 15, step: 104
	action: tensor([[-0.0021,  0.0226,  0.0074, -0.0075, -0.0124,  0.0038,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[7.1650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30054604430096554, distance: 0.9570535942001764 entropy -11.084141735631656
epoch: 15, step: 105
	action: tensor([[-0.0021,  0.0225,  0.0010, -0.0075, -0.0005, -0.0070, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[7.1725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980491833795619, distance: 0.9587602833360329 entropy -11.082644994043758
epoch: 15, step: 106
	action: tensor([[-0.0021,  0.0227,  0.0097, -0.0075, -0.0048,  0.0055, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.1663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30101276970032975, distance: 0.9567342338215822 entropy -11.083395816516953
epoch: 15, step: 107
	action: tensor([[-0.0021,  0.0225,  0.0131, -0.0075,  0.0019, -0.0095,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[7.1716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297381150571569, distance: 0.959216391511794 entropy -11.08569468342455
epoch: 15, step: 108
	action: tensor([[-0.0019,  0.0224,  0.0053, -0.0077, -0.0027, -0.0256,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[7.1862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937612797984195, distance: 0.961684143870894 entropy -11.082777498745747
epoch: 15, step: 109
	action: tensor([[-0.0019,  0.0224,  0.0042, -0.0079, -0.0022,  0.0181, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.1933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037469073889586, distance: 0.9548612338281993 entropy -11.072991627517238
epoch: 15, step: 110
	action: tensor([[-0.0022,  0.0227,  0.0082, -0.0073,  0.0043,  0.0009,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998677546466889, distance: 0.9575175291334485 entropy -11.087465920188263
epoch: 15, step: 111
	action: tensor([[-0.0020,  0.0224, -0.0016, -0.0076, -0.0035, -0.0128,  0.0392]],
       dtype=torch.float64)
	q_value: tensor([[7.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966974980399819, distance: 0.9596829398226157 entropy -11.082601122701444
epoch: 15, step: 112
	action: tensor([[-0.0020,  0.0223,  0.0064, -0.0080, -0.0012, -0.0078, -0.0171]],
       dtype=torch.float64)
	q_value: tensor([[7.1890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975560667441659, distance: 0.959096986158774 entropy -11.066080937351158
epoch: 15, step: 113
	action: tensor([[-0.0021,  0.0228,  0.0091, -0.0074, -0.0007,  0.0145, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.1615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3031953951796188, distance: 0.9552393387085794 entropy -11.087444186207266
epoch: 15, step: 114
	action: tensor([[-0.0022,  0.0226,  0.0135, -0.0073, -0.0109, -0.0065,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[7.1587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981664081001101, distance: 0.9586802242376786 entropy -11.089460034594266
epoch: 15, step: 115
	action: tensor([[-0.0020,  0.0225,  0.0165, -0.0076,  0.0028,  0.0028,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[7.1788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002230491601857, distance: 0.9572745436845477 entropy -11.082617866784448
epoch: 15, step: 116
	action: tensor([[-0.0020,  0.0224,  0.0060, -0.0076,  0.0069,  0.0010, -0.0093]],
       dtype=torch.float64)
	q_value: tensor([[7.1809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29978287469944276, distance: 0.9575755692928264 entropy -11.085164317785956
epoch: 15, step: 117
	action: tensor([[-0.0021,  0.0226,  0.0042, -0.0075, -0.0039, -0.0066,  0.0360]],
       dtype=torch.float64)
	q_value: tensor([[7.1673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29818769187822813, distance: 0.9586656876779357 entropy -11.086483378464354
epoch: 15, step: 118
	action: tensor([[-0.0020,  0.0222,  0.0073, -0.0080, -0.0075,  0.0108, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[7.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30177077206489145, distance: 0.9562153376989933 entropy -11.069724917900661
epoch: 15, step: 119
	action: tensor([[-0.0022,  0.0227,  0.0094, -0.0073, -0.0172,  0.0186,  0.0510]],
       dtype=torch.float64)
	q_value: tensor([[7.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30391956722776603, distance: 0.9547428311878446 entropy -11.089920239996914
epoch: 15, step: 120
	action: tensor([[-0.0021,  0.0219,  0.0175, -0.0079, -0.0062,  0.0178,  0.0533]],
       dtype=torch.float64)
	q_value: tensor([[7.1952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029824029777106, distance: 0.9553853215158696 entropy -11.070494924371403
epoch: 15, step: 121
	action: tensor([[-0.0021,  0.0218,  0.0141, -0.0079, -0.0070, -0.0109,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[7.2036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29631610520606644, distance: 0.959943117048574 entropy -11.071686112807331
epoch: 15, step: 122
	action: tensor([[-0.0019,  0.0224,  0.0003, -0.0077, -0.0193,  0.0107,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[7.1849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30200831563677555, distance: 0.9560526675328153 entropy -11.080064554006693
epoch: 15, step: 123
	action: tensor([[-0.0021,  0.0223,  0.0102, -0.0077,  0.0008, -0.0037,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.1764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29847758959913706, distance: 0.9584676691334603 entropy -11.07289399796928
epoch: 15, step: 124
	action: tensor([[-0.0020,  0.0225,  0.0050, -0.0076, -0.0057,  0.0383, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[7.1761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3084695111670136, distance: 0.9516173674100339 entropy -11.083025622165191
epoch: 15, step: 125
	action: tensor([[-0.0023,  0.0226,  0.0155, -0.0071, -0.0063, -0.0264, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[7.1486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934572378064364, distance: 0.9618911283449806 entropy -11.089406462629546
epoch: 15, step: 126
	action: tensor([[-0.0019,  0.0226,  0.0055, -0.0077, -0.0056, -0.0177, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[7.1813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295779266624722, distance: 0.9603092162698162 entropy -11.08476910148576
epoch: 15, step: 127
	action: tensor([[-0.0020,  0.0228,  0.0003, -0.0076, -0.0058,  0.0045,  0.0147]],
       dtype=torch.float64)
	q_value: tensor([[7.1669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30091499211437556, distance: 0.9568011476996213 entropy -11.084478529528424
LOSS epoch 15 actor 23.690117720599048 critic 11.945539445833504
epoch: 16, step: 0
	action: tensor([[-0.0024,  0.0225,  0.0053, -0.0077, -0.0014,  0.0212,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[7.8156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304140318805283, distance: 0.9545914279241098 entropy -11.073031839512781
epoch: 16, step: 1
	action: tensor([[-0.0024,  0.0224,  0.0085, -0.0075, -0.0073, -0.0296,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[7.8092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924191201725159, distance: 0.9625975184406933 entropy -11.078595771062906
epoch: 16, step: 2
	action: tensor([[-0.0021,  0.0223,  0.0027, -0.0080, -0.0183, -0.0108, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.8447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968333818399903, distance: 0.9595902260445776 entropy -11.067500326396912
epoch: 16, step: 3
	action: tensor([[-0.0024,  0.0229,  0.0059, -0.0074,  0.0009, -0.0096, -0.0019]],
       dtype=torch.float64)
	q_value: tensor([[7.7962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29741117907773695, distance: 0.9591958938101502 entropy -11.081098199925862
epoch: 16, step: 4
	action: tensor([[-0.0023,  0.0226,  0.0105, -0.0077, -0.0080, -0.0079,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[7.8181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975770259692556, distance: 0.959082677486854 entropy -11.07677751789187
epoch: 16, step: 5
	action: tensor([[-0.0023,  0.0225,  0.0202, -0.0077,  0.0011,  0.0075, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[7.8262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010091252215974, distance: 0.956736727996634 entropy -11.076526178176943
epoch: 16, step: 6
	action: tensor([[-0.0023,  0.0223,  0.0103, -0.0076, -0.0093, -0.0086, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[7.8279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971707268760033, distance: 0.9593600161442897 entropy -11.082798827625362
epoch: 16, step: 7
	action: tensor([[-0.0023,  0.0225,  0.0134, -0.0076, -0.0012,  0.0004,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[7.8201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29944001613990734, distance: 0.9578099771564871 entropy -11.07866371586755
epoch: 16, step: 8
	action: tensor([[-0.0023,  0.0224,  0.0058, -0.0076, -0.0042, -0.0264, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.8272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2932500949583101, distance: 0.962032120707272 entropy -11.078537638529872
epoch: 16, step: 9
	action: tensor([[-0.0022,  0.0227,  0.0064, -0.0076, -0.0087, -0.0103, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[7.8168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972455558392544, distance: 0.9593089441208112 entropy -11.079303566944812
epoch: 16, step: 10
	action: tensor([[-0.0023,  0.0227,  0.0105, -0.0075, -0.0065,  0.0373,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[7.8078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30813785012621586, distance: 0.9518455399766617 entropy -11.079979367024261
epoch: 16, step: 11
	action: tensor([[-0.0025,  0.0221,  0.0101, -0.0074, -0.0056, -0.0086, -0.0248]],
       dtype=torch.float64)
	q_value: tensor([[7.8145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29693745652150916, distance: 0.9595192096309517 entropy -11.077392533468336
epoch: 16, step: 12
	action: tensor([[-0.0024,  0.0227,  0.0191, -0.0074, -0.0132, -0.0042,  0.0425]],
       dtype=torch.float64)
	q_value: tensor([[7.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984798737140274, distance: 0.9584661087755697 entropy -11.084970059336374
epoch: 16, step: 13
	action: tensor([[-0.0022,  0.0219,  0.0139, -0.0080,  0.0066, -0.0128,  0.0134]],
       dtype=torch.float64)
	q_value: tensor([[7.8589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2957857213354038, distance: 0.9603048152829765 entropy -11.066555687832311
epoch: 16, step: 14
	action: tensor([[-0.0022,  0.0223,  0.0066, -0.0079, -0.0124,  0.0135,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[7.8383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022523787427628, distance: 0.9558855039477979 entropy -11.073987891187652
epoch: 16, step: 15
	action: tensor([[-0.0024,  0.0222,  0.0133, -0.0078, -0.0082, -0.0030, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[7.8291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982478165114225, distance: 0.9586246220990668 entropy -11.069321234587045
epoch: 16, step: 16
	action: tensor([[-0.0024,  0.0226,  0.0100, -0.0075, -0.0057,  0.0055, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[7.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006739690214758, distance: 0.9569660713468292 entropy -11.08311264197784
epoch: 16, step: 17
	action: tensor([[-0.0024,  0.0225,  0.0069, -0.0075, -0.0013,  0.0116, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.8156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30197823115628797, distance: 0.9560732709572882 entropy -11.079909136573628
epoch: 16, step: 18
	action: tensor([[-0.0024,  0.0226,  0.0135, -0.0074, -0.0094, -0.0178,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[7.8044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29522694798105853, distance: 0.9606857265756366 entropy -11.082440057963165
epoch: 16, step: 19
	action: tensor([[-0.0022,  0.0224,  0.0105, -0.0078, -0.0083,  0.0018,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.8357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996631357838464, distance: 0.9576574397261512 entropy -11.072535647163425
epoch: 16, step: 20
	action: tensor([[-0.0023,  0.0224,  0.0181, -0.0076, -0.0033,  0.0049,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[7.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30033019666718064, distance: 0.9572012535402232 entropy -11.077363085166974
epoch: 16, step: 21
	action: tensor([[-0.0023,  0.0222, -0.0055, -0.0077, -0.0097, -0.0236,  0.0485]],
       dtype=torch.float64)
	q_value: tensor([[7.8343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29387544649825414, distance: 0.9616064104223351 entropy -11.07622241661897
epoch: 16, step: 22
	action: tensor([[-0.0022,  0.0223,  0.0095, -0.0082, -0.0081,  0.0157, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[7.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30260539372546424, distance: 0.9556436653616948 entropy -11.055831094792852
epoch: 16, step: 23
	action: tensor([[-0.0025,  0.0226,  0.0109, -0.0073, -0.0006, -0.0117, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.7988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29661563053027884, distance: 0.9597387938583399 entropy -11.085589616285583
epoch: 16, step: 24
	action: tensor([[-0.0023,  0.0225,  0.0115, -0.0077, -0.0018, -0.0081,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.8230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29748449358271356, distance: 0.9591458468946099 entropy -11.077792563115514
epoch: 16, step: 25
	action: tensor([[-0.0023,  0.0225, -0.0030, -0.0077, -0.0074,  0.0266, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.8235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30544810465225414, distance: 0.9536939853073091 entropy -11.077428527436505
epoch: 16, step: 26
	action: tensor([[-0.0026,  0.0227,  0.0171, -0.0073, -0.0049,  0.0225, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.7849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045624664965796, distance: 0.9543018295221304 entropy -11.079792808823724
epoch: 16, step: 27
	action: tensor([[-0.0025,  0.0225,  0.0073, -0.0073, -0.0066,  0.0136,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[7.8028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023453041062819, distance: 0.955821849867964 entropy -11.087228274416224
epoch: 16, step: 28
	action: tensor([[-0.0024,  0.0224,  0.0205, -0.0075, -0.0035,  0.0028, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[7.8120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299804021861209, distance: 0.9575611093791584 entropy -11.076406655295859
epoch: 16, step: 29
	action: tensor([[-0.0023,  0.0223,  0.0022, -0.0076, -0.0038, -0.0092,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[7.8297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29711034270184056, distance: 0.9594012273745903 entropy -11.081878110021318
epoch: 16, step: 30
	action: tensor([[-0.0023,  0.0225,  0.0151, -0.0077, -0.0055,  0.0047,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[7.8217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30035109387170955, distance: 0.9571869589546667 entropy -11.07276131085047
epoch: 16, step: 31
	action: tensor([[-0.0023,  0.0224,  0.0076, -0.0076, -0.0102,  0.0016, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[7.8211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29963956301984174, distance: 0.9576735565722148 entropy -11.079428489329896
epoch: 16, step: 32
	action: tensor([[-0.0024,  0.0227,  0.0029, -0.0074, -0.0151, -0.0037,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[7.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29861817039551186, distance: 0.9583716287951329 entropy -11.081069283869278
epoch: 16, step: 33
	action: tensor([[-0.0023,  0.0225,  0.0013, -0.0078,  0.0040,  0.0043,  0.0479]],
       dtype=torch.float64)
	q_value: tensor([[7.8210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002193840084709, distance: 0.9572770505918107 entropy -11.069851627934877
epoch: 16, step: 34
	action: tensor([[-0.0023,  0.0220,  0.0178, -0.0080, -0.0037, -0.0260,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[7.8432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926995037005258, distance: 0.9624067817784064 entropy -11.061266634755713
epoch: 16, step: 35
	action: tensor([[-0.0021,  0.0224,  0.0064, -0.0079, -0.0082, -0.0195,  0.0483]],
       dtype=torch.float64)
	q_value: tensor([[7.8434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29482968108948804, distance: 0.9609564483806562 entropy -11.074109980493764
epoch: 16, step: 36
	action: tensor([[-0.0022,  0.0221,  0.0208, -0.0082, -0.0171,  0.0051,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[7.8553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999978838488684, distance: 0.9574285411006833 entropy -11.059602804793125
epoch: 16, step: 37
	action: tensor([[-0.0023,  0.0221,  0.0004, -0.0078, -0.0044, -0.0029,  0.0341]],
       dtype=torch.float64)
	q_value: tensor([[7.8446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983448070098855, distance: 0.9585583731475178 entropy -11.073253770109037
epoch: 16, step: 38
	action: tensor([[-0.0023,  0.0223,  0.0072, -0.0079, -0.0062, -0.0041, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[7.8309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29811818058738637, distance: 0.9587131622225522 entropy -11.06548515014488
epoch: 16, step: 39
	action: tensor([[-0.0024,  0.0228,  0.0206, -0.0074, -0.0115, -0.0064, -0.0209]],
       dtype=torch.float64)
	q_value: tensor([[7.7968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979920149782135, distance: 0.9587993243026112 entropy -11.084153786906597
epoch: 16, step: 40
	action: tensor([[-0.0024,  0.0226,  0.0116, -0.0075, -0.0143, -0.0108,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.8190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968994891537603, distance: 0.9595451176580603 entropy -11.085942943775947
epoch: 16, step: 41
	action: tensor([[-0.0023,  0.0225,  0.0063, -0.0077, -0.0064,  0.0369,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[7.8279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30780976775280877, distance: 0.952071196728744 entropy -11.073511558223041
epoch: 16, step: 42
	action: tensor([[-0.0025,  0.0223,  0.0026, -0.0074, -0.0055, -0.0069,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[7.8056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974543085437632, distance: 0.9591664525207461 entropy -11.07749248703239
epoch: 16, step: 43
	action: tensor([[-0.0023,  0.0224,  0.0068, -0.0078, -0.0111, -0.0201, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.8266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29465950035414423, distance: 0.9610723965460545 entropy -11.069977904920078
epoch: 16, step: 44
	action: tensor([[-0.0022,  0.0226,  0.0181, -0.0077, -0.0054, -0.0020, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[7.8221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29891010282993147, distance: 0.958172159069989 entropy -11.07627018930746
epoch: 16, step: 45
	action: tensor([[-0.0023,  0.0224,  0.0080, -0.0076, -0.0087,  0.0077,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.8252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010478436597863, distance: 0.9567102298933021 entropy -11.082308864361943
epoch: 16, step: 46
	action: tensor([[-0.0024,  0.0225,  0.0115, -0.0075, -0.0093, -0.0010,  0.0330]],
       dtype=torch.float64)
	q_value: tensor([[7.8125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990360389009179, distance: 0.9580860974558486 entropy -11.079272593170709
epoch: 16, step: 47
	action: tensor([[-0.0022,  0.0221,  0.0141, -0.0079, -0.0027,  0.0041,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[7.8426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29990794026501666, distance: 0.9574900493348978 entropy -11.068190930895096
epoch: 16, step: 48
	action: tensor([[-0.0023,  0.0221,  0.0038, -0.0079,  0.0034, -0.0043,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[7.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29799740429220656, distance: 0.9587956439452345 entropy -11.070050421899822
epoch: 16, step: 49
	action: tensor([[-0.0023,  0.0224,  0.0078, -0.0078, -0.0036, -0.0039, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.8252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983485539839834, distance: 0.9585558137007792 entropy -11.072666810991217
epoch: 16, step: 50
	action: tensor([[-0.0024,  0.0227,  0.0112, -0.0075, -0.0026,  0.0056,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[7.8058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30072799306186215, distance: 0.9569291070627214 entropy -11.081441877009835
epoch: 16, step: 51
	action: tensor([[-0.0023,  0.0224,  0.0154, -0.0076, -0.0116, -0.0101,  0.0406]],
       dtype=torch.float64)
	q_value: tensor([[7.8200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968703630381414, distance: 0.9595649921512351 entropy -11.077365734497144
epoch: 16, step: 52
	action: tensor([[-0.0022,  0.0221,  0.0031, -0.0080, -0.0015,  0.0052, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[7.8511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30018585891347416, distance: 0.9572999809352353 entropy -11.065323463036721
epoch: 16, step: 53
	action: tensor([[-0.0025,  0.0228,  0.0170, -0.0073,  0.0090, -0.0111,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.7948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967964434086111, distance: 0.95961543009388 entropy -11.083861550594756
epoch: 16, step: 54
	action: tensor([[-0.0022,  0.0221,  0.0192, -0.0079,  0.0029, -0.0053,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[7.8490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29779560046834785, distance: 0.9589334459455006 entropy -11.073727593989991
epoch: 16, step: 55
	action: tensor([[-0.0022,  0.0223,  0.0109, -0.0077, -0.0149,  0.0136,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[7.8340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023064604369262, distance: 0.9558484583831355 entropy -11.080726479539548
epoch: 16, step: 56
	action: tensor([[-0.0024,  0.0223,  0.0152, -0.0076, -0.0072,  0.0068, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[7.8215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006792725429459, distance: 0.956962442638971 entropy -11.07632727448049
epoch: 16, step: 57
	action: tensor([[-0.0025,  0.0226,  0.0015, -0.0074, -0.0106, -0.0025, -0.0373]],
       dtype=torch.float64)
	q_value: tensor([[7.8063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29881974200110817, distance: 0.9582339046757923 entropy -11.086659907342192
epoch: 16, step: 58
	action: tensor([[-0.0025,  0.0229,  0.0070, -0.0072,  0.0005,  0.0146,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029957428821207, distance: 0.9553761791284335 entropy -11.087104334050204
epoch: 16, step: 59
	action: tensor([[-0.0024,  0.0223,  0.0032, -0.0076,  0.0033, -0.0108, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[7.8203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966520787417515, distance: 0.9597139274999104 entropy -11.075834694503806
epoch: 16, step: 60
	action: tensor([[-0.0023,  0.0228,  0.0042, -0.0075,  0.0020, -0.0037,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[7.8036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987059703058832, distance: 0.9583116418001271 entropy -11.082246507220711
epoch: 16, step: 61
	action: tensor([[-0.0023,  0.0226,  0.0057, -0.0076, -0.0058,  0.0027,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[7.8147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999771555222406, distance: 0.9574427165896917 entropy -11.076130658699286
epoch: 16, step: 62
	action: tensor([[-0.0023,  0.0224,  0.0019, -0.0077, -0.0080, -0.0091,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[7.8210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29718927555133845, distance: 0.9593473566154689 entropy -11.07475018528356
epoch: 16, step: 63
	action: tensor([[-0.0022,  0.0223,  0.0182, -0.0079, -0.0069,  0.0013, -0.0162]],
       dtype=torch.float64)
	q_value: tensor([[7.8330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29936750412911084, distance: 0.9578595453104695 entropy -11.066260660802286
epoch: 16, step: 64
	action: tensor([[-0.0024,  0.0225,  0.0136, -0.0074, -0.0081,  0.0005,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[7.8129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29941965883448785, distance: 0.9578238933728289 entropy -11.084752681405737
epoch: 16, step: 65
	action: tensor([[-0.0023,  0.0225,  0.0073, -0.0076, -0.0097, -0.0058, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[7.8194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29798367001025683, distance: 0.9588050230453826 entropy -11.079077144793095
epoch: 16, step: 66
	action: tensor([[-0.0024,  0.0228,  0.0060, -0.0074, -0.0163,  0.0194,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[7.7994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039249997806661, distance: 0.9547391055400517 entropy -11.084923740132238
epoch: 16, step: 67
	action: tensor([[-0.0024,  0.0222,  0.0214, -0.0077, -0.0106, -0.0090,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[7.8240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29678834154394196, distance: 0.9596209581176525 entropy -11.070710545484662
epoch: 16, step: 68
	action: tensor([[-0.0022,  0.0224,  0.0169, -0.0077, -0.0100,  0.0068,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[7.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007948450628264, distance: 0.9568833636652962 entropy -11.07866400337251
epoch: 16, step: 69
	action: tensor([[-0.0023,  0.0224,  0.0049, -0.0076, -0.0114,  0.0100,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[7.8243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014942516321394, distance: 0.9564046644234013 entropy -11.080572214520428
epoch: 16, step: 70
	action: tensor([[-0.0024,  0.0222,  0.0211, -0.0078, -0.0004,  0.0081,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[7.8272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30079716123207034, distance: 0.9568817787902157 entropy -11.06686486388824
epoch: 16, step: 71
	action: tensor([[-0.0023,  0.0222,  0.0146, -0.0076, -0.0002, -0.0068,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[7.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975028670406278, distance: 0.9591333041529067 entropy -11.080128308577782
epoch: 16, step: 72
	action: tensor([[-0.0022,  0.0222,  0.0172, -0.0079,  0.0007,  0.0200, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[7.8397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30377670495515263, distance: 0.9548408009959788 entropy -11.071917145988163
epoch: 16, step: 73
	action: tensor([[-0.0025,  0.0224, -0.0096, -0.0073, -0.0078,  0.0124,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[7.8068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30207544175845324, distance: 0.9560066944559411 entropy -11.086595442781002
epoch: 16, step: 74
	action: tensor([[-0.0024,  0.0225,  0.0067, -0.0077, -0.0090,  0.0267, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[7.8035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053162778522164, distance: 0.953784487156155 entropy -11.069477642581878
epoch: 16, step: 75
	action: tensor([[-0.0025,  0.0225,  0.0111, -0.0073,  0.0033,  0.0042,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[7.7980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30021146887263206, distance: 0.9572824644002903 entropy -11.083294625709758
epoch: 16, step: 76
	action: tensor([[-0.0023,  0.0220,  0.0011, -0.0080,  0.0027, -0.0093,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[7.8445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967471988802206, distance: 0.9596490298827035 entropy -11.066197073651242
epoch: 16, step: 77
	action: tensor([[-0.0022,  0.0224,  0.0093, -0.0078, -0.0016, -0.0062,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297795441318072, distance: 0.9589335546136561 entropy -11.070724699810437
epoch: 16, step: 78
	action: tensor([[-0.0022,  0.0224,  0.0148, -0.0078, -0.0157,  0.0058, -0.0336]],
       dtype=torch.float64)
	q_value: tensor([[7.8313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30054493698084106, distance: 0.9570543517656153 entropy -11.073887317409946
epoch: 16, step: 79
	action: tensor([[-0.0025,  0.0228,  0.0147, -0.0072, -0.0094,  0.0090,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[7.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30149152485101305, distance: 0.9564065311966162 entropy -11.089527338962915
epoch: 16, step: 80
	action: tensor([[-0.0024,  0.0224,  0.0169, -0.0075, -0.0056,  0.0005, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[7.8181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29930073976254945, distance: 0.95790518219054 entropy -11.079585524804946
epoch: 16, step: 81
	action: tensor([[-0.0023,  0.0224,  0.0154, -0.0076, -0.0024, -0.0077, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[7.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29746090362630073, distance: 0.9591619504671114 entropy -11.08014854662318
epoch: 16, step: 82
	action: tensor([[-0.0023,  0.0224,  0.0096, -0.0076, -0.0048, -0.0062,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[7.8260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297880383271359, distance: 0.9588755543118935 entropy -11.07898312600586
epoch: 16, step: 83
	action: tensor([[-0.0022,  0.0223,  0.0122, -0.0079, -0.0076,  0.0170, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[7.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030164863418455, distance: 0.9553619626049744 entropy -11.070242582359016
epoch: 16, step: 84
	action: tensor([[-0.0024,  0.0225,  0.0074, -0.0074, -0.0034, -0.0005,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[7.8065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29915277339870205, distance: 0.9580063170659974 entropy -11.082401256283315
epoch: 16, step: 85
	action: tensor([[-0.0023,  0.0225,  0.0073, -0.0076, -0.0074, -0.0276,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[7.8173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930267840561158, distance: 0.9621840947591194 entropy -11.075907320953794
epoch: 16, step: 86
	action: tensor([[-0.0022,  0.0226,  0.0074, -0.0078, -0.0102,  0.0389,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.8277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3083801061338729, distance: 0.9516788807011572 entropy -11.07317707525001
epoch: 16, step: 87
	action: tensor([[-0.0026,  0.0223,  0.0124, -0.0073, -0.0015, -0.0143,  0.0340]],
       dtype=torch.float64)
	q_value: tensor([[7.8011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29574992704212155, distance: 0.9603292204934798 entropy -11.081560696606955
epoch: 16, step: 88
	action: tensor([[-0.0021,  0.0221,  0.0115, -0.0081,  0.0011,  0.0181,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[7.8528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032001420772025, distance: 0.9552360849764714 entropy -11.066185561071768
epoch: 16, step: 89
	action: tensor([[-0.0024,  0.0223,  0.0049, -0.0075, -0.0112,  0.0024, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[7.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997429638199498, distance: 0.9576028587843316 entropy -11.080613719565287
epoch: 16, step: 90
	action: tensor([[-0.0024,  0.0228,  0.0010, -0.0074, -0.0151,  0.0307,  0.0225]],
       dtype=torch.float64)
	q_value: tensor([[7.7969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064515755177968, distance: 0.9530047999137983 entropy -11.081598525414764
epoch: 16, step: 91
	action: tensor([[-0.0025,  0.0224,  0.0103, -0.0075, -0.0035,  0.0035, -0.0568]],
       dtype=torch.float64)
	q_value: tensor([[7.8028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29989291244006155, distance: 0.9575003257802893 entropy -11.073423183037377
epoch: 16, step: 92
	action: tensor([[-0.0026,  0.0229,  0.0092, -0.0070, -0.0097, -0.0018,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[7.7755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29916760348617466, distance: 0.9579961811960813 entropy -11.09449513657315
epoch: 16, step: 93
	action: tensor([[-0.0023,  0.0222,  0.0244, -0.0079, -0.0016, -0.0173,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[7.8351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949075712045034, distance: 0.9609033753334278 entropy -11.068276567835213
epoch: 16, step: 94
	action: tensor([[-0.0021,  0.0221,  0.0069, -0.0079, -0.0080, -0.0042, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[7.8552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29814170232048653, distance: 0.9586970977061109 entropy -11.074999858737387
epoch: 16, step: 95
	action: tensor([[-2.3567e-03,  2.2670e-02,  1.2793e-02, -7.5220e-03, -6.6300e-04,
          7.2393e-05, -9.7615e-03]], dtype=torch.float64)
	q_value: tensor([[7.8077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29942133750175237, distance: 0.9578227458466397 entropy -11.079489253118284
epoch: 16, step: 96
	action: tensor([[-0.0024,  0.0225,  0.0018, -0.0075, -0.0088, -0.0077, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.8149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297632676041224, distance: 0.9590446846540653 entropy -11.08125099539275
epoch: 16, step: 97
	action: tensor([[-0.0024,  0.0228, -0.0016, -0.0075, -0.0057,  0.0072, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[7.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012079894933475, distance: 0.9566006215796192 entropy -11.080648689748983
epoch: 16, step: 98
	action: tensor([[-0.0025,  0.0228,  0.0163, -0.0073, -0.0154,  0.0070, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[7.7891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30106859768799865, distance: 0.9566960259608029 entropy -11.08299281248632
epoch: 16, step: 99
	action: tensor([[-0.0024,  0.0226,  0.0062, -0.0074, -0.0054, -0.0006, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[7.8100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991959893230641, distance: 0.957976780124701 entropy -11.085208435132344
epoch: 16, step: 100
	action: tensor([[-0.0024,  0.0226, -0.0003, -0.0075, -0.0112,  0.0067,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.8124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009220194455501, distance: 0.9567963387169123 entropy -11.078958600381343
epoch: 16, step: 101
	action: tensor([[-0.0024,  0.0226,  0.0131, -0.0076, -0.0143, -0.0118,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[7.8075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965975682653226, distance: 0.9597511163848634 entropy -11.075142502035211
epoch: 16, step: 102
	action: tensor([[-0.0022,  0.0222,  0.0107, -0.0079, -0.0094,  0.0029,  0.0014]],
       dtype=torch.float64)
	q_value: tensor([[7.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997534931808393, distance: 0.9575956592965214 entropy -11.068716081658792
epoch: 16, step: 103
	action: tensor([[-0.0024,  0.0225,  0.0137, -0.0075, -0.0061,  0.0132,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[7.8136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023395231616691, distance: 0.9558258099514838 entropy -11.078992370670319
epoch: 16, step: 104
	action: tensor([[-0.0024,  0.0222,  0.0070, -0.0077, -0.0131, -0.0157, -0.0118]],
       dtype=torch.float64)
	q_value: tensor([[7.8274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29551839930062107, distance: 0.9604870654073947 entropy -11.074875464925768
epoch: 16, step: 105
	action: tensor([[-0.0023,  0.0227,  0.0148, -0.0075, -0.0036, -0.0002,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[7.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994287717676276, distance: 0.9578176638135821 entropy -11.08039258863423
epoch: 16, step: 106
	action: tensor([[-0.0023,  0.0224,  0.0021, -0.0076, -0.0097, -0.0087,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[7.8248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29731911220534546, distance: 0.9592587380160956 entropy -11.078361656251316
epoch: 16, step: 107
	action: tensor([[-0.0023,  0.0226,  0.0104, -0.0077,  0.0127,  0.0143, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[7.8129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30274909797225946, distance: 0.9555452009291175 entropy -11.07416881967372
epoch: 16, step: 108
	action: tensor([[-0.0025,  0.0226,  0.0164, -0.0072, -0.0053, -0.0248,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.7969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29361984960824916, distance: 0.9617804316832158 entropy -11.088006723742263
epoch: 16, step: 109
	action: tensor([[-0.0021,  0.0224,  0.0081, -0.0078,  0.0074, -0.0012,  0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29901474271915085, distance: 0.9581006512858904 entropy -11.074791816730606
epoch: 16, step: 110
	action: tensor([[-0.0022,  0.0223, -0.0002, -0.0078, -0.0070, -0.0310,  0.0039]],
       dtype=torch.float64)
	q_value: tensor([[7.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2922328193625111, distance: 0.962724232502518 entropy -11.074036666941984
epoch: 16, step: 111
	action: tensor([[-0.0022,  0.0227,  0.0008, -0.0078, -0.0183,  0.0126, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[7.8231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30235728690914665, distance: 0.9558136413128945 entropy -11.071626718283841
epoch: 16, step: 112
	action: tensor([[-0.0025,  0.0228,  0.0194, -0.0072, -0.0008,  0.0139, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.7855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027233794506876, distance: 0.9555628236981795 entropy -11.084195627283862
epoch: 16, step: 113
	action: tensor([[-0.0024,  0.0223,  0.0027, -0.0075, -0.0054,  0.0135,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[7.8229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30223006049798307, distance: 0.9559007913636313 entropy -11.083808179936325
epoch: 16, step: 114
	action: tensor([[-0.0024,  0.0224,  0.0097, -0.0076, -0.0080, -0.0075,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.8156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973978731930045, distance: 0.959204976568936 entropy -11.07342889020545
epoch: 16, step: 115
	action: tensor([[-0.0023,  0.0225,  0.0031, -0.0077, -0.0171, -0.0047,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[7.8251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982672907306968, distance: 0.9586113206829474 entropy -11.076195756545934
epoch: 16, step: 116
	action: tensor([[-0.0023,  0.0224,  0.0032, -0.0078, -0.0102, -0.0073,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[7.8281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29750250469693873, distance: 0.9591335515103965 entropy -11.068156972131245
epoch: 16, step: 117
	action: tensor([[-0.0022,  0.0222, -0.0041, -0.0080,  0.0005, -0.0039,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[7.8410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298124878398785, distance: 0.9587085878803429 entropy -11.062596486887726
epoch: 16, step: 118
	action: tensor([[-0.0023,  0.0226,  0.0087, -0.0077, -0.0041, -0.0137,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[7.8095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29617261688190677, distance: 0.9600409831566439 entropy -11.071951141816953
epoch: 16, step: 119
	action: tensor([[-0.0022,  0.0224,  0.0003, -0.0078,  0.0024, -0.0070,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[7.8309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977376561628571, distance: 0.9589730096297732 entropy -11.07400463473186
epoch: 16, step: 120
	action: tensor([[-0.0022,  0.0223,  0.0138, -0.0079,  0.0004,  0.0111, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[7.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30164000789866463, distance: 0.9563048733719574 entropy -11.066464070191765
epoch: 16, step: 121
	action: tensor([[-0.0024,  0.0225, -0.0016, -0.0074, -0.0046, -0.0106,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.8086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969418710943572, distance: 0.9595161971862277 entropy -11.085689641249038
epoch: 16, step: 122
	action: tensor([[-0.0023,  0.0227,  0.0110, -0.0076,  0.0008, -0.0032,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[7.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29866517734057674, distance: 0.9583395129949721 entropy -11.075167695379296
epoch: 16, step: 123
	action: tensor([[-0.0023,  0.0224,  0.0013, -0.0077, -0.0023,  0.0096,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.8309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30145302024109344, distance: 0.9564328913298373 entropy -11.075844963468324
epoch: 16, step: 124
	action: tensor([[-0.0024,  0.0225,  0.0088, -0.0076, -0.0051,  0.0132, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[7.8101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022997157356545, distance: 0.9558530785325038 entropy -11.075806421197953
epoch: 16, step: 125
	action: tensor([[-0.0026,  0.0227,  0.0042, -0.0072, -0.0047,  0.0153, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[7.7910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30291748391982753, distance: 0.9554298119786263 entropy -11.086552667108718
epoch: 16, step: 126
	action: tensor([[-0.0025,  0.0226,  0.0026, -0.0074, -0.0058, -0.0103,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[7.8007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2970051768651927, distance: 0.95947299714527 entropy -11.079554036800568
epoch: 16, step: 127
	action: tensor([[-0.0023,  0.0224,  0.0101, -0.0078,  0.0005, -0.0074,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[7.8262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29749644555194366, distance: 0.9591376878359819 entropy -11.068654468310255
LOSS epoch 16 actor 28.341743459198007 critic 11.233293864593833
epoch: 17, step: 0
	action: tensor([[-0.0019,  0.0228,  0.0068, -0.0074, -0.0212, -0.0051, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[7.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29891624612496703, distance: 0.9581679610725663 entropy -11.070123248481185
epoch: 17, step: 1
	action: tensor([[-0.0021,  0.0233,  0.0093, -0.0071, -0.0025,  0.0123,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[7.7277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3031681567377925, distance: 0.9552580089187834 entropy -11.078118417870837
epoch: 17, step: 2
	action: tensor([[-0.0020,  0.0226,  0.0116, -0.0075, -0.0029, -0.0023, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[7.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991870667943969, distance: 0.9579828785116583 entropy -11.066532267849372
epoch: 17, step: 3
	action: tensor([[-0.0021,  0.0231,  0.0014, -0.0071, -0.0056, -0.0017, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[7.7355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998842288001372, distance: 0.9575062638449243 entropy -11.079010604232304
epoch: 17, step: 4
	action: tensor([[-0.0021,  0.0233, -0.0009, -0.0070, -0.0187, -0.0170,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[7.7225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29662392200513943, distance: 0.9597331371549225 entropy -11.079581611278982
epoch: 17, step: 5
	action: tensor([[-0.0020,  0.0230,  0.0057, -0.0075, -0.0001,  0.0053,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.7498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013038685484689, distance: 0.9565349932462628 entropy -11.063023560433692
epoch: 17, step: 6
	action: tensor([[-0.0021,  0.0229,  0.0054, -0.0072, -0.0164,  0.0051,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.7432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30122767909872394, distance: 0.9565871445928066 entropy -11.073480480390744
epoch: 17, step: 7
	action: tensor([[-0.0021,  0.0231,  0.0062, -0.0071, -0.0087, -0.0207,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[7.7357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29548659982438485, distance: 0.9605087427946539 entropy -11.074903106640814
epoch: 17, step: 8
	action: tensor([[-0.0019,  0.0230,  0.0122, -0.0074, -0.0227,  0.0057,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.7535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30146978286241, distance: 0.9564214157820092 entropy -11.070723663435329
epoch: 17, step: 9
	action: tensor([[-0.0021,  0.0230,  0.0112, -0.0071, -0.0060,  0.0183, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[7.7415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043552644834263, distance: 0.9544439835794691 entropy -11.076589587175642
epoch: 17, step: 10
	action: tensor([[-0.0023,  0.0232,  0.0011, -0.0068, -0.0101,  0.0070,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[7.7156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018182274426042, distance: 0.956182842402849 entropy -11.087376431594338
epoch: 17, step: 11
	action: tensor([[-0.0021,  0.0230,  0.0127, -0.0073, -0.0055,  0.0147,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[7.7371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30340951210660694, distance: 0.955092562535484 entropy -11.069094300961657
epoch: 17, step: 12
	action: tensor([[-0.0021,  0.0227,  0.0060, -0.0073, -0.0076, -0.0147,  0.0486]],
       dtype=torch.float64)
	q_value: tensor([[7.7553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29648548514157, distance: 0.9598275787436761 entropy -11.071779886037403
epoch: 17, step: 13
	action: tensor([[-0.0019,  0.0226,  0.0041, -0.0078, -0.0120, -0.0020,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[7.7755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992707967650863, distance: 0.9579256490638844 entropy -11.057936831606792
epoch: 17, step: 14
	action: tensor([[-0.0020,  0.0227,  0.0054, -0.0076, -0.0002, -0.0029,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[7.7616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991246233294139, distance: 0.9580255564040361 entropy -11.061655803505134
epoch: 17, step: 15
	action: tensor([[-0.0020,  0.0227,  0.0053, -0.0075, -0.0042, -0.0076,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981549076014913, distance: 0.9586880788456342 entropy -11.064702516026697
epoch: 17, step: 16
	action: tensor([[-0.0020,  0.0230,  0.0110, -0.0073, -0.0019, -0.0147,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[7.7478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29672722727665657, distance: 0.9596626562727338 entropy -11.071843094439586
epoch: 17, step: 17
	action: tensor([[-0.0019,  0.0224,  0.0139, -0.0079, -0.0146, -0.0017, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[7.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991534190321308, distance: 0.958005875799326 entropy -11.05711895268166
epoch: 17, step: 18
	action: tensor([[-0.0020,  0.0231,  0.0065, -0.0072, -0.0112,  0.0121,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[7.7411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029614995061345, distance: 0.9553996473523112 entropy -11.07695587235349
epoch: 17, step: 19
	action: tensor([[-0.0021,  0.0230,  0.0218, -0.0071,  0.0011, -0.0117, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[7.7364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297319223168483, distance: 0.9592586622759087 entropy -11.075203037417946
epoch: 17, step: 20
	action: tensor([[-0.0020,  0.0229,  0.0171, -0.0072, -0.0022,  0.0078,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[7.7537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30189313765558745, distance: 0.9561315450297357 entropy -11.080290277238765
epoch: 17, step: 21
	action: tensor([[-0.0020,  0.0225,  0.0240, -0.0076, -0.0106, -0.0188, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[7.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295229817724162, distance: 0.9606837706807303 entropy -11.066219703580106
epoch: 17, step: 22
	action: tensor([[-0.0021,  0.0231,  0.0133, -0.0070, -0.0048, -0.0123,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[7.7368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974375873747698, distance: 0.9591778669305914 entropy -11.090462166822222
epoch: 17, step: 23
	action: tensor([[-0.0019,  0.0226,  0.0112, -0.0076, -0.0064,  0.0215,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[7.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30478641253302696, distance: 0.9541481641605709 entropy -11.063961999990012
epoch: 17, step: 24
	action: tensor([[-0.0022,  0.0229,  0.0105, -0.0070,  0.0010,  0.0286,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[7.7335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067206580541658, distance: 0.9528199088394533 entropy -11.079636048687025
epoch: 17, step: 25
	action: tensor([[-0.0022,  0.0228,  0.0031, -0.0070, -0.0075, -0.0002,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[7.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999410495255873, distance: 0.9574674079108728 entropy -11.078063702738831
epoch: 17, step: 26
	action: tensor([[-0.0021,  0.0231,  0.0064, -0.0072, -0.0071,  0.0015,  0.0585]],
       dtype=torch.float64)
	q_value: tensor([[7.7351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30049329510232325, distance: 0.9570896815351393 entropy -11.072423667651469
epoch: 17, step: 27
	action: tensor([[-0.0020,  0.0224,  0.0054, -0.0078, -0.0054,  0.0018,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[7.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29985803330613103, distance: 0.9575241766787809 entropy -11.056735786762033
epoch: 17, step: 28
	action: tensor([[-0.0020,  0.0228,  0.0146, -0.0074, -0.0121, -0.0047,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[7.7508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988714233749461, distance: 0.9581985901067808 entropy -11.068897362056898
epoch: 17, step: 29
	action: tensor([[-0.0020,  0.0230,  0.0068, -0.0072, -0.0118, -0.0022, -0.0324]],
       dtype=torch.float64)
	q_value: tensor([[7.7486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996740521859742, distance: 0.9576499760219074 entropy -11.07511437709806
epoch: 17, step: 30
	action: tensor([[-0.0022,  0.0233,  0.0205, -0.0069, -0.0079,  0.0033,  0.0292]],
       dtype=torch.float64)
	q_value: tensor([[7.7183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30107783268161004, distance: 0.9566897055187372 entropy -11.084096490373227
epoch: 17, step: 31
	action: tensor([[-0.0020,  0.0226,  0.0083, -0.0075, -0.0055, -0.0097,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[7.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975962629858748, distance: 0.9590695443630739 entropy -11.069392917107843
epoch: 17, step: 32
	action: tensor([[-0.0020,  0.0230,  0.0061, -0.0073, -0.0128,  0.0321, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[7.7511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30749457755159715, distance: 0.9522879358164653 entropy -11.07229163205626
epoch: 17, step: 33
	action: tensor([[-0.0022,  0.0230,  0.0022, -0.0069, -0.0049, -0.0138,  0.0026]],
       dtype=torch.float64)
	q_value: tensor([[7.7206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29696568412642066, distance: 0.9594999473325455 entropy -11.080461065454509
epoch: 17, step: 34
	action: tensor([[-0.0020,  0.0231,  0.0135, -0.0073, -0.0065,  0.0080, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30205581610628107, distance: 0.9560201358248838 entropy -11.072513278581335
epoch: 17, step: 35
	action: tensor([[-0.0021,  0.0231,  0.0053, -0.0070, -0.0116, -0.0009, -0.0025]],
       dtype=torch.float64)
	q_value: tensor([[7.7330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29996342297645295, distance: 0.9574521077556846 entropy -11.08045667857623
epoch: 17, step: 36
	action: tensor([[-0.0021,  0.0232,  0.0047, -0.0071, -0.0023,  0.0156,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[7.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30382335339883215, distance: 0.9548088122764331 entropy -11.075422951574822
epoch: 17, step: 37
	action: tensor([[-0.0021,  0.0227,  0.0172, -0.0074, -0.0061,  0.0068, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[7.7491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013150938118616, distance: 0.9565273093620339 entropy -11.066309349426556
epoch: 17, step: 38
	action: tensor([[-0.0021,  0.0230,  0.0202, -0.0070, -0.0095,  0.0237,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.7365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305586610091231, distance: 0.9535988891806099 entropy -11.083452160425207
epoch: 17, step: 39
	action: tensor([[-0.0021,  0.0227,  0.0188, -0.0071, -0.0029, -0.0247,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[7.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29408839276609233, distance: 0.961461403471563 entropy -11.0797082610226
epoch: 17, step: 40
	action: tensor([[-0.0018,  0.0226,  0.0236, -0.0077, -0.0154, -0.0249,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[7.7833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29409007666808984, distance: 0.9614602567219367 entropy -11.066495313046135
epoch: 17, step: 41
	action: tensor([[-0.0018,  0.0226,  0.0192, -0.0077, -0.0088,  0.0124, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[7.7871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30267880138338943, distance: 0.9555933685793659 entropy -11.064501769825183
epoch: 17, step: 42
	action: tensor([[-0.0021,  0.0229,  0.0046, -0.0070, -0.0056, -0.0181, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[7.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29597823587597116, distance: 0.9601735446722159 entropy -11.083323551866103
epoch: 17, step: 43
	action: tensor([[-0.0020,  0.0231,  0.0028, -0.0073, -0.0110,  0.0264,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[7.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062774476532827, distance: 0.9531244269436331 entropy -11.073566057126515
epoch: 17, step: 44
	action: tensor([[-0.0022,  0.0228,  0.0024, -0.0072, -0.0084,  0.0150,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[7.7389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30326432599849473, distance: 0.9551920894094996 entropy -11.070249312244192
epoch: 17, step: 45
	action: tensor([[-0.0021,  0.0229,  0.0042, -0.0072, -0.0039,  0.0005, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.7342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30012390126883715, distance: 0.9573423570004046 entropy -11.070478574936313
epoch: 17, step: 46
	action: tensor([[-0.0021,  0.0231,  0.0099, -0.0071,  0.0030,  0.0164,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.7330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3040533330264792, distance: 0.9546510903001845 entropy -11.076845390992824
epoch: 17, step: 47
	action: tensor([[-0.0021,  0.0228,  0.0137, -0.0071, -0.0007,  0.0158, -0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.7418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30364113554503125, distance: 0.9549337603268128 entropy -11.076507209263342
epoch: 17, step: 48
	action: tensor([[-0.0021,  0.0229,  0.0090, -0.0070, -0.0063,  0.0168,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[7.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30392917128210717, distance: 0.9547362447121809 entropy -11.079996506139972
epoch: 17, step: 49
	action: tensor([[-0.0021,  0.0228,  0.0155, -0.0072, -0.0073,  0.0052, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[7.7423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011383390508503, distance: 0.9566482938450634 entropy -11.074418832776699
epoch: 17, step: 50
	action: tensor([[-0.0021,  0.0230,  0.0065, -0.0071, -0.0123, -0.0135,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[7.7356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29708950643664855, distance: 0.9594154473805662 entropy -11.080558841139469
epoch: 17, step: 51
	action: tensor([[-0.0019,  0.0228,  0.0163, -0.0076, -0.0111, -0.0028,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[7.7618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992570958916916, distance: 0.9579350138469036 entropy -11.062859722837075
epoch: 17, step: 52
	action: tensor([[-0.0020,  0.0229, -0.0068, -0.0073, -0.0043,  0.0194, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30456126245446247, distance: 0.9543026556345003 entropy -11.07459252659083
epoch: 17, step: 53
	action: tensor([[-0.0023,  0.0233,  0.0167, -0.0069, -0.0014, -0.0201,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[7.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29562699829122463, distance: 0.9604130308649165 entropy -11.079557740421233
epoch: 17, step: 54
	action: tensor([[-0.0018,  0.0228,  0.0146, -0.0075, -0.0101, -0.0050, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[7.7702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988471866473126, distance: 0.9582151515466691 entropy -11.069632211471042
epoch: 17, step: 55
	action: tensor([[-0.0021,  0.0231,  0.0208, -0.0071, -0.0011,  0.0009,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[7.7400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003581662437713, distance: 0.9571821211000264 entropy -11.078993398244515
epoch: 17, step: 56
	action: tensor([[-0.0019,  0.0224,  0.0088, -0.0076, -0.0119,  0.0030, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[7.7823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003382144571699, distance: 0.9571957690526988 entropy -11.06687832098729
epoch: 17, step: 57
	action: tensor([[-0.0022,  0.0233,  0.0015, -0.0069, -0.0039, -0.0017,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[7.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999661127439087, distance: 0.9574502683332328 entropy -11.082856127562719
epoch: 17, step: 58
	action: tensor([[-0.0020,  0.0230,  0.0182, -0.0073, -0.0127, -0.0174,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[7.7408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29606319976983486, distance: 0.9601156043152618 entropy -11.0697807598345
epoch: 17, step: 59
	action: tensor([[-0.0019,  0.0229,  0.0021, -0.0074, -0.0056,  0.0005, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[7.7633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002444449571674, distance: 0.9572599091582692 entropy -11.071969119422745
epoch: 17, step: 60
	action: tensor([[-0.0021,  0.0232,  0.0177, -0.0071, -0.0146,  0.0014,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[7.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30054167151626976, distance: 0.9570565858072257 entropy -11.076870734374028
epoch: 17, step: 61
	action: tensor([[-0.0020,  0.0228,  0.0167, -0.0073, -0.0055,  0.0125, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028985980661626, distance: 0.9554427544818349 entropy -11.075426988989856
epoch: 17, step: 62
	action: tensor([[-0.0021,  0.0229,  0.0076, -0.0071,  0.0040, -0.0199,  0.0257]],
       dtype=torch.float64)
	q_value: tensor([[7.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954273466966787, distance: 0.96054913375852 entropy -11.079861197622805
epoch: 17, step: 63
	action: tensor([[-0.0018,  0.0227,  0.0084, -0.0076,  0.0011,  0.0168,  0.0001]],
       dtype=torch.float64)
	q_value: tensor([[7.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30387012432543725, distance: 0.9547767384894339 entropy -11.065313475152946
epoch: 17, step: 64
	action: tensor([[-0.0021,  0.0229,  0.0133, -0.0071, -0.0094, -0.0227,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[7.7345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2948491233681061, distance: 0.9609432010053138 entropy -11.07838633381686
epoch: 17, step: 65
	action: tensor([[-0.0019,  0.0228,  0.0102, -0.0076, -0.0008,  0.0052,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.7717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012098680213484, distance: 0.9565993357876855 entropy -11.066708897403604
epoch: 17, step: 66
	action: tensor([[-0.0020,  0.0229,  0.0133, -0.0072, -0.0127,  0.0154,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[7.7464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30359475503427635, distance: 0.9549655611556273 entropy -11.075188106751819
epoch: 17, step: 67
	action: tensor([[-0.0021,  0.0228,  0.0087, -0.0072, -0.0196,  0.0150, -0.0534]],
       dtype=torch.float64)
	q_value: tensor([[7.7481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033601380179691, distance: 0.9551264102480744 entropy -11.07453505265323
epoch: 17, step: 68
	action: tensor([[-0.0024,  0.0234, -0.0031, -0.0066, -0.0032,  0.0052, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[7.6961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30157587419896803, distance: 0.956348783362677 entropy -11.091918142023333
epoch: 17, step: 69
	action: tensor([[-0.0021,  0.0232,  0.0144, -0.0071, -0.0175, -0.0009,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[7.7245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999908354908716, distance: 0.9574333612876307 entropy -11.075186537715211
epoch: 17, step: 70
	action: tensor([[-0.0020,  0.0227,  0.0153, -0.0074, -0.0047,  0.0244,  0.0178]],
       dtype=torch.float64)
	q_value: tensor([[7.7639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30554193904475613, distance: 0.9536295608063736 entropy -11.068688990278435
epoch: 17, step: 71
	action: tensor([[-0.0021,  0.0226,  0.0037, -0.0072, -0.0064, -0.0117, -0.0232]],
       dtype=torch.float64)
	q_value: tensor([[7.7495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971716671207092, distance: 0.9593593744283622 entropy -11.075806228145396
epoch: 17, step: 72
	action: tensor([[-0.0021,  0.0234,  0.0124, -0.0071, -0.0076,  0.0084,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[7.7227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023291464369182, distance: 0.9558329182117256 entropy -11.079589890323385
epoch: 17, step: 73
	action: tensor([[-0.0020,  0.0228,  0.0005, -0.0073, -0.0020,  0.0327,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[7.7554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074766527806698, distance: 0.9523002602200713 entropy -11.072101130282482
epoch: 17, step: 74
	action: tensor([[-0.0022,  0.0228,  0.0120, -0.0071, -0.0038, -0.0163, -0.0377]],
       dtype=torch.float64)
	q_value: tensor([[7.7298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29609084623551385, distance: 0.960096750304703 entropy -11.070675742644273
epoch: 17, step: 75
	action: tensor([[-2.0799e-03,  2.3259e-02,  6.1390e-04, -7.0190e-03, -4.4029e-05,
          9.4088e-04,  3.3525e-02]], dtype=torch.float64)
	q_value: tensor([[7.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006239455702461, distance: 0.957000297078068 entropy -11.084958804782369
epoch: 17, step: 76
	action: tensor([[-0.0020,  0.0228,  0.0071, -0.0075, -0.0003,  0.0051,  0.0228]],
       dtype=torch.float64)
	q_value: tensor([[7.7531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301012454573935, distance: 0.9567344494851614 entropy -11.062619161497093
epoch: 17, step: 77
	action: tensor([[-0.0020,  0.0228,  0.0174, -0.0074, -0.0001,  0.0016, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[7.7535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002555210972885, distance: 0.95725233309348 entropy -11.069533837571878
epoch: 17, step: 78
	action: tensor([[-0.0021,  0.0229, -0.0011, -0.0071,  0.0015,  0.0049, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.7458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012590739854293, distance: 0.9565656552739962 entropy -11.080762217060961
epoch: 17, step: 79
	action: tensor([[-0.0022,  0.0232,  0.0103, -0.0071, -0.0103, -0.0090,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[7.7216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981816059918003, distance: 0.9586698442863205 entropy -11.076278863486465
epoch: 17, step: 80
	action: tensor([[-0.0019,  0.0227,  0.0023, -0.0075, -0.0057, -0.0239, -0.0427]],
       dtype=torch.float64)
	q_value: tensor([[7.7654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294590909188185, distance: 0.9611191253768319 entropy -11.066315350988353
epoch: 17, step: 81
	action: tensor([[-0.0021,  0.0235,  0.0112, -0.0070, -0.0105, -0.0190, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[7.7151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29621912025416297, distance: 0.9600092666578502 entropy -11.082379049816163
epoch: 17, step: 82
	action: tensor([[-0.0020,  0.0231,  0.0019, -0.0072, -0.0043,  0.0109,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[7.7484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30279599513360655, distance: 0.9555130653586377 entropy -11.07674886444916
epoch: 17, step: 83
	action: tensor([[-0.0021,  0.0229,  0.0086, -0.0073,  0.0035, -0.0109,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[7.7371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29746912747880316, distance: 0.95915633652368 entropy -11.069997653775383
epoch: 17, step: 84
	action: tensor([[-0.0019,  0.0228,  0.0138, -0.0075,  0.0010,  0.0270, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[7.7641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30627336266003224, distance: 0.9531272331815922 entropy -11.067827741444138
epoch: 17, step: 85
	action: tensor([[-0.0022,  0.0229,  0.0049, -0.0069,  0.0052, -0.0145,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[7.7293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29670738878763536, distance: 0.9596761916487282 entropy -11.083930552174534
epoch: 17, step: 86
	action: tensor([[-0.0019,  0.0230, -0.0024, -0.0074, -0.0116, -0.0151, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[7.7503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296826470739478, distance: 0.9595949417179587 entropy -11.070591537080533
epoch: 17, step: 87
	action: tensor([[-0.0020,  0.0233,  0.0011, -0.0072, -0.0124, -0.0068,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[7.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988716409020604, distance: 0.9581984414648645 entropy -11.074430650021778
epoch: 17, step: 88
	action: tensor([[-0.0020,  0.0231,  0.0100, -0.0073, -0.0110,  0.0038,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[7.7426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30101665297497193, distance: 0.9567315762144124 entropy -11.070472245620678
epoch: 17, step: 89
	action: tensor([[-0.0021,  0.0230,  0.0196, -0.0072, -0.0065, -0.0188,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[7.7394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2957470859187441, distance: 0.9603311575973236 entropy -11.075173316521983
epoch: 17, step: 90
	action: tensor([[-0.0018,  0.0226,  0.0187, -0.0076, -0.0034, -0.0272,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[7.7819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2935275785528463, distance: 0.9618432460173744 entropy -11.067088513925142
epoch: 17, step: 91
	action: tensor([[-0.0018,  0.0227,  0.0073, -0.0077, -0.0041,  0.0039,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[7.7806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008495608428663, distance: 0.9568459228343736 entropy -11.066055834384725
epoch: 17, step: 92
	action: tensor([[-0.0020,  0.0228,  0.0167, -0.0073, -0.0025,  0.0210, -0.0306]],
       dtype=torch.float64)
	q_value: tensor([[7.7504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30488550413764537, distance: 0.9540801624387585 entropy -11.070791845067186
epoch: 17, step: 93
	action: tensor([[-0.0023,  0.0231,  0.0114, -0.0068, -0.0101,  0.0003,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[7.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002019249510042, distance: 0.9572889922278118 entropy -11.088313487064896
epoch: 17, step: 94
	action: tensor([[-0.0021,  0.0230,  0.0148, -0.0072, -0.0044, -0.0274,  0.0244]],
       dtype=torch.float64)
	q_value: tensor([[7.7427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937961129643405, distance: 0.9616604274450927 entropy -11.074906303196498
epoch: 17, step: 95
	action: tensor([[-0.0018,  0.0227,  0.0199, -0.0077, -0.0094,  0.0010,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001281931309674, distance: 0.9573394216324771 entropy -11.065537433640255
epoch: 17, step: 96
	action: tensor([[-0.0020,  0.0228,  0.0099, -0.0073, -0.0020, -0.0012,  0.0510]],
       dtype=torch.float64)
	q_value: tensor([[7.7570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29972425096774646, distance: 0.9576156536297433 entropy -11.074971084042733
epoch: 17, step: 97
	action: tensor([[-0.0020,  0.0225,  0.0098, -0.0077, -0.0090, -0.0034, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[7.7732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988270306422244, distance: 0.9582289243293644 entropy -11.059600850902362
epoch: 17, step: 98
	action: tensor([[-0.0020,  0.0231,  0.0180, -0.0072, -0.0029, -0.0187,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.7405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29582097891264325, distance: 0.9602807754090182 entropy -11.07698983244923
epoch: 17, step: 99
	action: tensor([[-0.0019,  0.0228,  0.0072, -0.0074, -0.0047,  0.0022, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.7691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005671861915, distance: 0.9570391300063145 entropy -11.072826477036614
epoch: 17, step: 100
	action: tensor([[-0.0021,  0.0231,  0.0038, -0.0071, -0.0068,  0.0106,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[7.7315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026487264751082, distance: 0.9556139753478832 entropy -11.077386698932356
epoch: 17, step: 101
	action: tensor([[-0.0021,  0.0228, -0.0048, -0.0074, -0.0077, -0.0100, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[7.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297719507723626, distance: 0.9589854008330767 entropy -11.067668188751465
epoch: 17, step: 102
	action: tensor([[-0.0021,  0.0233,  0.0123, -0.0072, -0.0133,  0.0117,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[7.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030449010184936, distance: 0.9553424882720823 entropy -11.074861537585473
epoch: 17, step: 103
	action: tensor([[-0.0021,  0.0227,  0.0052, -0.0073,  0.0049, -0.0123, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[7.7524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29706567733774447, distance: 0.9594317096301976 entropy -11.070082074831499
epoch: 17, step: 104
	action: tensor([[-0.0021,  0.0232,  0.0159, -0.0071, -0.0099,  0.0073,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.7315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019873858821954, distance: 0.9560670013695532 entropy -11.080374185849179
epoch: 17, step: 105
	action: tensor([[-0.0020,  0.0227,  0.0048, -0.0073, -0.0104,  0.0185,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.7599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041389246781032, distance: 0.9545923841665999 entropy -11.07232274878775
epoch: 17, step: 106
	action: tensor([[-0.0022,  0.0230,  0.0009, -0.0071, -0.0036,  0.0116,  0.0636]],
       dtype=torch.float64)
	q_value: tensor([[7.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30270756784655595, distance: 0.9555736579173353 entropy -11.073363488349449
epoch: 17, step: 107
	action: tensor([[-0.0021,  0.0223,  0.0128, -0.0077, -0.0107, -0.0081,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29747137373952703, distance: 0.9591548031270613 entropy -11.0551587668771
epoch: 17, step: 108
	action: tensor([[-0.0020,  0.0230,  0.0047, -0.0073, -0.0047,  0.0116, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30283291275101787, distance: 0.9554877673594083 entropy -11.075182975511122
epoch: 17, step: 109
	action: tensor([[-0.0022,  0.0232,  0.0133, -0.0070,  0.0030,  0.0046,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[7.7226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012397869653647, distance: 0.9565788570008374 entropy -11.078430327473852
epoch: 17, step: 110
	action: tensor([[-0.0020,  0.0224,  0.0040, -0.0076, -0.0111,  0.0068, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.7741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011982927349164, distance: 0.9566072586710743 entropy -11.0648400575809
epoch: 17, step: 111
	action: tensor([[-0.0022,  0.0232,  0.0143, -0.0070,  0.0023,  0.0136, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[7.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30340356022963355, distance: 0.9550966428246831 entropy -11.078723344351074
epoch: 17, step: 112
	action: tensor([[-0.0021,  0.0229, -0.0040, -0.0070, -0.0094, -0.0043, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[7.7348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29917420735483957, distance: 0.9579916676377062 entropy -11.080682479099462
epoch: 17, step: 113
	action: tensor([[-2.1569e-03,  2.3420e-02, -1.8505e-05, -6.9768e-03, -3.8719e-03,
          8.7739e-03,  3.3516e-02]], dtype=torch.float64)
	q_value: tensor([[7.7139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3024537868919889, distance: 0.9557475335559686 entropy -11.079428150113817
epoch: 17, step: 114
	action: tensor([[-0.0021,  0.0228,  0.0054, -0.0075, -0.0130, -0.0177, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.7479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2958706166036701, distance: 0.960246929640379 entropy -11.063441109850771
epoch: 17, step: 115
	action: tensor([[-0.0020,  0.0233,  0.0123, -0.0072, -0.0025, -0.0011, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[7.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001464146075077, distance: 0.957326959170649 entropy -11.077444699186927
epoch: 17, step: 116
	action: tensor([[-0.0021,  0.0230,  0.0021, -0.0071, -0.0158, -0.0187,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[7.7398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29601787593863294, distance: 0.960146512926044 entropy -11.080142788691054
epoch: 17, step: 117
	action: tensor([[-0.0020,  0.0231,  0.0120, -0.0073, -0.0014, -0.0059,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988703299210568, distance: 0.9581993372918204 entropy -11.070895975614992
epoch: 17, step: 118
	action: tensor([[-0.0019,  0.0228,  0.0053, -0.0074, -0.0046,  0.0047,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[7.7603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30109109013569324, distance: 0.956680632026307 entropy -11.071363738154147
epoch: 17, step: 119
	action: tensor([[-0.0020,  0.0227,  0.0085, -0.0075, -0.0070,  0.0098, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.7563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021124218538972, distance: 0.9559813667276749 entropy -11.065900240337756
epoch: 17, step: 120
	action: tensor([[-0.0021,  0.0230,  0.0094, -0.0071, -0.0038,  0.0242,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[7.7330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30572708343849386, distance: 0.9535024322249878 entropy -11.078670914357577
epoch: 17, step: 121
	action: tensor([[-0.0021,  0.0226,  0.0099, -0.0072, -0.0153,  0.0164,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[7.7474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30351930015977957, distance: 0.9550172945790685 entropy -11.072411353703925
epoch: 17, step: 122
	action: tensor([[-0.0021,  0.0227,  0.0090, -0.0073,  0.0024, -0.0119,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[7.7529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29702873643452443, distance: 0.959456919530438 entropy -11.069030837617817
epoch: 17, step: 123
	action: tensor([[-0.0019,  0.0228,  0.0098, -0.0075, -0.0113,  0.0229, -0.0438]],
       dtype=torch.float64)
	q_value: tensor([[7.7622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30523296370113584, distance: 0.9538416796308943 entropy -11.066713916784561
epoch: 17, step: 124
	action: tensor([[-0.0024,  0.0233,  0.0052, -0.0066,  0.0024,  0.0182,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[7.7020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30446765798193987, distance: 0.9543668769571596 entropy -11.090601828116771
epoch: 17, step: 125
	action: tensor([[-0.0021,  0.0227,  0.0053, -0.0073, -0.0021, -0.0048, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29877598521638693, distance: 0.9582638032504304 entropy -11.07101997344204
epoch: 17, step: 126
	action: tensor([[-0.0021,  0.0232,  0.0178, -0.0072, -0.0044,  0.0062, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[7.7315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017025114791658, distance: 0.9562620775239783 entropy -11.076986745006282
epoch: 17, step: 127
	action: tensor([[-0.0022,  0.0231,  0.0094, -0.0069, -0.0067, -0.0430, -0.0187]],
       dtype=torch.float64)
	q_value: tensor([[7.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904382910576283, distance: 0.9639439430390795 entropy -11.087334884896777
LOSS epoch 17 actor 27.783137938588684 critic 11.387813445112304
epoch: 18, step: 0
	action: tensor([[-0.0020,  0.0230,  0.0029, -0.0075, -0.0186,  0.0162,  0.0661]],
       dtype=torch.float64)
	q_value: tensor([[7.1373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30377072231994084, distance: 0.9548449034527384 entropy -11.071076970917266
epoch: 18, step: 1
	action: tensor([[-0.0023,  0.0222,  0.0083, -0.0078, -0.0083,  0.0073,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006916761194498, distance: 0.956953955968238 entropy -11.05057728849468
epoch: 18, step: 2
	action: tensor([[-0.0022,  0.0229,  0.0075, -0.0073, -0.0181,  0.0028,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[7.1192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30045302302530696, distance: 0.9571172319748619 entropy -11.070884952631188
epoch: 18, step: 3
	action: tensor([[-0.0021,  0.0226,  0.0208, -0.0076, -0.0104,  0.0027,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[7.1342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300161572946535, distance: 0.9573165915984254 entropy -11.059761246640965
epoch: 18, step: 4
	action: tensor([[-0.0021,  0.0226,  0.0063, -0.0074, -0.0063, -0.0080,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979058338352165, distance: 0.9588581754037285 entropy -11.071519374879399
epoch: 18, step: 5
	action: tensor([[-0.0021,  0.0230,  0.0221, -0.0074,  0.0006, -0.0126,  0.0349]],
       dtype=torch.float64)
	q_value: tensor([[7.1221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29688178460298653, distance: 0.9595571985825158 entropy -11.068752970471797
epoch: 18, step: 6
	action: tensor([[-0.0019,  0.0223,  0.0059, -0.0078, -0.0010,  0.0053,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006991602917558, distance: 0.9569488351744384 entropy -11.061082247968672
epoch: 18, step: 7
	action: tensor([[-0.0022,  0.0228,  0.0060, -0.0074, -0.0025, -0.0084,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[7.1199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2978565056454464, distance: 0.9588918588535854 entropy -11.067468040187183
epoch: 18, step: 8
	action: tensor([[-0.0020,  0.0226,  0.0088, -0.0077, -0.0053, -0.0088,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[7.1426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29759470964436063, distance: 0.9590706048370122 entropy -11.05966454332802
epoch: 18, step: 9
	action: tensor([[-0.0021,  0.0228,  0.0194, -0.0075, -0.0058, -0.0043,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.1323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987691434740807, distance: 0.9582684780603864 entropy -11.065333845176726
epoch: 18, step: 10
	action: tensor([[-0.0021,  0.0226,  0.0086, -0.0075, -0.0103, -0.0214,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[7.1414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2948463892054184, distance: 0.9609450639913679 entropy -11.067985779319569
epoch: 18, step: 11
	action: tensor([[-0.0020,  0.0228,  0.0075, -0.0076, -0.0090,  0.0099,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[7.1375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021434693524072, distance: 0.9559601017266168 entropy -11.063877192796081
epoch: 18, step: 12
	action: tensor([[-0.0021,  0.0225,  0.0127, -0.0076,  0.0029,  0.0063, -0.0169]],
       dtype=torch.float64)
	q_value: tensor([[7.1371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30093515798252535, distance: 0.9567873476147113 entropy -11.060333920451134
epoch: 18, step: 13
	action: tensor([[-0.0022,  0.0229,  0.0109, -0.0072, -0.0130, -0.0048,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[7.1168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29873655858959713, distance: 0.9582907422724052 entropy -11.07627992408376
epoch: 18, step: 14
	action: tensor([[-0.0021,  0.0229,  0.0118, -0.0074, -0.0054,  0.0168, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[7.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30373114676762114, distance: 0.9548720410485422 entropy -11.068453504547438
epoch: 18, step: 15
	action: tensor([[-0.0023,  0.0228,  0.0147, -0.0072, -0.0101,  0.0021,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[7.1162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30022287835444794, distance: 0.9572746605131845 entropy -11.074583209235639
epoch: 18, step: 16
	action: tensor([[-0.0021,  0.0227,  0.0085, -0.0075, -0.0127, -0.0160,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[7.1334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29606309102119277, distance: 0.9601156784776469 entropy -11.06705798147459
epoch: 18, step: 17
	action: tensor([[-0.0020,  0.0228,  0.0208, -0.0076, -0.0054,  0.0063,  0.0727]],
       dtype=torch.float64)
	q_value: tensor([[7.1374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30120945258674703, distance: 0.9565996201394432 entropy -11.062557982132784
epoch: 18, step: 18
	action: tensor([[-0.0021,  0.0218,  0.0161, -0.0080, -0.0094, -0.0054,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.1722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975458390781772, distance: 0.9591039684156167 entropy -11.05259952126175
epoch: 18, step: 19
	action: tensor([[-0.0021,  0.0228,  0.0088, -0.0074, -0.0087,  0.0033, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[7.1318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005802672200819, distance: 0.9570301805300382 entropy -11.071625614760544
epoch: 18, step: 20
	action: tensor([[-0.0022,  0.0230,  0.0068, -0.0072,  0.0032,  0.0125,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[7.1164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027973222642304, distance: 0.9555121559467259 entropy -11.073493882434587
epoch: 18, step: 21
	action: tensor([[-0.0022,  0.0226,  0.0012, -0.0075, -0.0040, -0.0158, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[7.1291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2960347229707673, distance: 0.9601350241997113 entropy -11.06520873013017
epoch: 18, step: 22
	action: tensor([[-2.1239e-03,  2.3143e-02,  8.1857e-03, -7.3589e-03,  9.0216e-04,
          2.1686e-05, -1.7480e-02]], dtype=torch.float64)
	q_value: tensor([[7.1158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001095047597121, distance: 0.9573522032554258 entropy -11.070154464285565
epoch: 18, step: 23
	action: tensor([[-0.0022,  0.0230,  0.0107, -0.0072, -0.0108,  0.0004,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[7.1139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000195184386465, distance: 0.9574137456212848 entropy -11.074050428885384
epoch: 18, step: 24
	action: tensor([[-0.0021,  0.0226,  0.0118, -0.0076, -0.0192, -0.0134,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[7.1358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966083927891613, distance: 0.9597437316443946 entropy -11.063026523374347
epoch: 18, step: 25
	action: tensor([[-0.0020,  0.0227,  0.0029, -0.0077, -0.0115, -0.0103,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[7.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29737295687824894, distance: 0.9592219845171855 entropy -11.060334422391577
epoch: 18, step: 26
	action: tensor([[-0.0021,  0.0226,  0.0089, -0.0078, -0.0109, -0.0059,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[7.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29818351804381127, distance: 0.9586685383730902 entropy -11.055118031418113
epoch: 18, step: 27
	action: tensor([[-0.0021,  0.0228,  0.0221, -0.0075, -0.0065,  0.0032, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[7.1337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004845553896227, distance: 0.9570956605076492 entropy -11.066110980247382
epoch: 18, step: 28
	action: tensor([[-0.0022,  0.0227,  0.0067, -0.0073, -0.0085,  0.0094,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[7.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019221742347751, distance: 0.9561116604823182 entropy -11.076828231163711
epoch: 18, step: 29
	action: tensor([[-0.0022,  0.0228,  0.0136, -0.0073, -0.0148, -0.0084,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[7.1199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29784399225608504, distance: 0.9589004033561136 entropy -11.069219513275339
epoch: 18, step: 30
	action: tensor([[-0.0020,  0.0221,  0.0167, -0.0080,  0.0046,  0.0035, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[7.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999228778568366, distance: 0.9574798344841345 entropy -11.050775143360871
epoch: 18, step: 31
	action: tensor([[-0.0022,  0.0227,  0.0069, -0.0073,  0.0007,  0.0179, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[7.1275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039323411029241, distance: 0.9547340708340756 entropy -11.074505999873477
epoch: 18, step: 32
	action: tensor([[-0.0023,  0.0229,  0.0131, -0.0071, -0.0069,  0.0109,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[7.1040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023496105473853, distance: 0.9558188998434907 entropy -11.077224942435036
epoch: 18, step: 33
	action: tensor([[-2.1352e-03,  2.2460e-02, -8.6650e-05, -7.5523e-03, -4.3392e-03,
          7.0460e-04, -1.4542e-02]], dtype=torch.float64)
	q_value: tensor([[7.1391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997097578419009, distance: 0.9576255631349556 entropy -11.063930480191464
epoch: 18, step: 34
	action: tensor([[-0.0023,  0.0231,  0.0117, -0.0072, -0.0007,  0.0206, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304798851964472, distance: 0.95413962785318 entropy -11.072938456837637
epoch: 18, step: 35
	action: tensor([[-0.0023,  0.0228,  0.0067, -0.0072, -0.0077,  0.0026, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[7.1148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30034419575801585, distance: 0.957191677584351 entropy -11.075271006575665
epoch: 18, step: 36
	action: tensor([[-0.0022,  0.0229,  0.0233, -0.0073, -0.0113, -0.0040, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[7.1177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989060857347061, distance: 0.9581749041268692 entropy -11.071358103964986
epoch: 18, step: 37
	action: tensor([[-0.0021,  0.0228,  0.0014, -0.0074, -0.0020, -0.0007,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[7.1342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29967019212045287, distance: 0.9576526152119672 entropy -11.073820892713835
epoch: 18, step: 38
	action: tensor([[-0.0021,  0.0228,  0.0119, -0.0074, -0.0073, -0.0241,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[7.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29429328082088846, distance: 0.96132186315886 entropy -11.065979353488832
epoch: 18, step: 39
	action: tensor([[-0.0020,  0.0228,  0.0035, -0.0075, -0.0028, -0.0039, -0.0449]],
       dtype=torch.float64)
	q_value: tensor([[7.1403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990702327062571, distance: 0.9580627289156188 entropy -11.068305009547412
epoch: 18, step: 40
	action: tensor([[-0.0023,  0.0233,  0.0058, -0.0070, -0.0075,  0.0181, -0.0068]],
       dtype=torch.float64)
	q_value: tensor([[7.0941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043531261354686, distance: 0.9544454505148676 entropy -11.079865866010868
epoch: 18, step: 41
	action: tensor([[-0.0023,  0.0229,  0.0034, -0.0071, -0.0111, -0.0014, -0.0149]],
       dtype=torch.float64)
	q_value: tensor([[7.1087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29955638513353355, distance: 0.9577304236459612 entropy -11.07388174690704
epoch: 18, step: 42
	action: tensor([[-0.0022,  0.0231,  0.0102, -0.0072, -0.0177, -0.0135,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.1104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969518081937018, distance: 0.9595094162096102 entropy -11.073389641974684
epoch: 18, step: 43
	action: tensor([[-0.0021,  0.0229,  0.0010, -0.0074, -0.0047,  0.0015,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[7.1288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003041038276717, distance: 0.9572191018636128 entropy -11.068416910428265
epoch: 18, step: 44
	action: tensor([[-0.0022,  0.0229,  0.0108, -0.0074, -0.0057,  0.0018,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[7.1195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30024556201816344, distance: 0.9572591450927846 entropy -11.067089149582975
epoch: 18, step: 45
	action: tensor([[-0.0021,  0.0227,  0.0074, -0.0075, -0.0073,  0.0096, -0.0429]],
       dtype=torch.float64)
	q_value: tensor([[7.1332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30193166620286294, distance: 0.9561051601954698 entropy -11.067121368302894
epoch: 18, step: 46
	action: tensor([[-0.0024,  0.0232,  0.0171, -0.0069, -0.0024,  0.0048,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[7.0942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011023740922364, distance: 0.9566729091413233 entropy -11.0824539796926
epoch: 18, step: 47
	action: tensor([[-0.0021,  0.0226,  0.0079, -0.0074, -0.0126, -0.0028, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[7.1363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29904027210631134, distance: 0.9580832044530813 entropy -11.069790559912404
epoch: 18, step: 48
	action: tensor([[-0.0022,  0.0230,  0.0176, -0.0073, -0.0145, -0.0251,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.1174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294194339903034, distance: 0.9613892500289062 entropy -11.070825603900118
epoch: 18, step: 49
	action: tensor([[-0.0020,  0.0227,  0.0212, -0.0077, -0.0107,  0.0080,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[7.1534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015394192599109, distance: 0.9563737418230189 entropy -11.064079717934234
epoch: 18, step: 50
	action: tensor([[-0.0021,  0.0226,  0.0083, -0.0074, -0.0078,  0.0163, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[7.1373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033943484820544, distance: 0.9551029578728221 entropy -11.071294822143901
epoch: 18, step: 51
	action: tensor([[-0.0023,  0.0229,  0.0078, -0.0071, -0.0036,  0.0111, -0.0180]],
       dtype=torch.float64)
	q_value: tensor([[7.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30243228089561414, distance: 0.9557622667336284 entropy -11.076572027720418
epoch: 18, step: 52
	action: tensor([[-0.0023,  0.0230,  0.0228, -0.0071, -0.0119, -0.0078, -0.0226]],
       dtype=torch.float64)
	q_value: tensor([[7.1055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980321102722169, distance: 0.9587719429262178 entropy -11.075439219707045
epoch: 18, step: 53
	action: tensor([[-0.0022,  0.0229,  0.0173, -0.0072, -0.0035,  0.0003,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[7.1265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999128921075519, distance: 0.9574866631028507 entropy -11.077439786727913
epoch: 18, step: 54
	action: tensor([[-0.0020,  0.0224, -0.0009, -0.0077, -0.0044, -0.0004,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[7.1522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29939482125217465, distance: 0.9578408720243431 entropy -11.06173187947437
epoch: 18, step: 55
	action: tensor([[-0.0021,  0.0227, -0.0071, -0.0076, -0.0092,  0.0138, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[7.1271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028680731272124, distance: 0.9554636728965783 entropy -11.061543720714013
epoch: 18, step: 56
	action: tensor([[-0.0023,  0.0231,  0.0045, -0.0072, -0.0087,  0.0137, -0.0376]],
       dtype=torch.float64)
	q_value: tensor([[7.0995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3031147566659701, distance: 0.9552946101946449 entropy -11.069448597054556
epoch: 18, step: 57
	action: tensor([[-0.0024,  0.0232,  0.0141, -0.0069, -0.0098,  0.0092,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[7.0933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30211050594690714, distance: 0.9559826789520095 entropy -11.080780097641748
epoch: 18, step: 58
	action: tensor([[-0.0022,  0.0227,  0.0053, -0.0074, -0.0046, -0.0077,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29797314478854486, distance: 0.9588122106257777 entropy -11.069018859998021
epoch: 18, step: 59
	action: tensor([[-0.0021,  0.0229,  0.0151, -0.0074, -0.0101,  0.0144,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[7.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30317552802043923, distance: 0.9552529564118736 entropy -11.06701492217169
epoch: 18, step: 60
	action: tensor([[-0.0022,  0.0226,  0.0202, -0.0074, -0.0073, -0.0107,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[7.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29705613170216405, distance: 0.9594382240043556 entropy -11.067056064253348
epoch: 18, step: 61
	action: tensor([[-0.0020,  0.0225,  0.0006, -0.0077, -0.0112,  0.0025,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[7.1497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002458932644435, distance: 0.9572589185214683 entropy -11.064438937484356
epoch: 18, step: 62
	action: tensor([[-0.0022,  0.0228,  0.0224, -0.0075, -0.0048,  0.0176, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[7.1230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037954132600619, distance: 0.9548279720853575 entropy -11.061673897260897
epoch: 18, step: 63
	action: tensor([[-0.0024,  0.0228,  0.0081, -0.0070, -0.0019, -0.0046, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.1121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29873558306661385, distance: 0.9582914088081629 entropy -11.082081770029832
epoch: 18, step: 64
	action: tensor([[-0.0022,  0.0230,  0.0083, -0.0073, -0.0040, -0.0062, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.1174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985173945763724, distance: 0.9584404766130279 entropy -11.072312609127179
epoch: 18, step: 65
	action: tensor([[-0.0022,  0.0229,  0.0089, -0.0073, -0.0099, -0.0266,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.1227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938905491210988, distance: 0.9615961269273597 entropy -11.072119541079925
epoch: 18, step: 66
	action: tensor([[-0.0020,  0.0229,  0.0201, -0.0076, -0.0061,  0.0114, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[7.1414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30249406792459466, distance: 0.9557199374966355 entropy -11.065300128375887
epoch: 18, step: 67
	action: tensor([[-0.0022,  0.0227,  0.0031, -0.0072, -0.0054,  0.0227,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[7.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30495725695315967, distance: 0.9540309189651917 entropy -11.07676529520291
epoch: 18, step: 68
	action: tensor([[-0.0022,  0.0226,  0.0191, -0.0074, -0.0090, -0.0096,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[7.1196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972936280172551, distance: 0.9592761326178912 entropy -11.065026307190221
epoch: 18, step: 69
	action: tensor([[-0.0020,  0.0225,  0.0188, -0.0077,  0.0013, -0.0099,  0.0212]],
       dtype=torch.float64)
	q_value: tensor([[7.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29723781916936176, distance: 0.9593142246539054 entropy -11.06311219175711
epoch: 18, step: 70
	action: tensor([[-0.0020,  0.0225,  0.0103, -0.0077, -0.0121,  0.0078,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[7.1485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30141211117604183, distance: 0.9564608967494248 entropy -11.065072487699096
epoch: 18, step: 71
	action: tensor([[-0.0022,  0.0228,  0.0128, -0.0074, -0.0065,  0.0073, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[7.1267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014238046676454, distance: 0.9564528917335581 entropy -11.068473851059123
epoch: 18, step: 72
	action: tensor([[-0.0022,  0.0229,  0.0075, -0.0072,  0.0036, -0.0204, -0.0150]],
       dtype=torch.float64)
	q_value: tensor([[7.1179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29514313064199516, distance: 0.960742851154324 entropy -11.075186172021604
epoch: 18, step: 73
	action: tensor([[-0.0021,  0.0230,  0.0088, -0.0074,  0.0015,  0.0002,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[7.1276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000756216638102, distance: 0.9573753766405105 entropy -11.072265546668845
epoch: 18, step: 74
	action: tensor([[-0.0021,  0.0226,  0.0134, -0.0076, -0.0221,  0.0159, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.1370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032694640427702, distance: 0.955188567393563 entropy -11.064540927763145
epoch: 18, step: 75
	action: tensor([[-0.0023,  0.0229,  0.0046, -0.0072, -0.0088, -0.0127,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[7.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969947456157228, distance: 0.9594801155938961 entropy -11.074599670407935
epoch: 18, step: 76
	action: tensor([[-0.0021,  0.0229,  0.0108, -0.0075,  0.0007,  0.0099,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[7.1289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30217772818759614, distance: 0.9559366366732128 entropy -11.066864912150878
epoch: 18, step: 77
	action: tensor([[-0.0022,  0.0227,  0.0123, -0.0074, -0.0116,  0.0139,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[7.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029162431814115, distance: 0.9554306622638629 entropy -11.069749188709547
epoch: 18, step: 78
	action: tensor([[-0.0022,  0.0225,  0.0123, -0.0075, -0.0067, -0.0093, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[7.1326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973618705604192, distance: 0.9592295519727413 entropy -11.06360001869887
epoch: 18, step: 79
	action: tensor([[-0.0022,  0.0231,  0.0130, -0.0072, -0.0072,  0.0247,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[7.1150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057313010276629, distance: 0.9534995360385297 entropy -11.075982005317924
epoch: 18, step: 80
	action: tensor([[-0.0022,  0.0225,  0.0100, -0.0074, -0.0115,  0.0101, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[7.1271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30182272495309903, distance: 0.9561797626538042 entropy -11.068873165044163
epoch: 18, step: 81
	action: tensor([[-0.0023,  0.0230,  0.0054, -0.0071,  0.0043, -0.0030,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[7.1081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992281609684486, distance: 0.9579547910646332 entropy -11.077138991268606
epoch: 18, step: 82
	action: tensor([[-0.0020,  0.0225,  0.0042, -0.0077, -0.0006,  0.0012, -0.0009]],
       dtype=torch.float64)
	q_value: tensor([[7.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29980660255180014, distance: 0.9575593447508974 entropy -11.058708621442346
epoch: 18, step: 83
	action: tensor([[-0.0022,  0.0229,  0.0117, -0.0073,  0.0004,  0.0214,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[7.1184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048725389954844, distance: 0.9540890600573044 entropy -11.070116618402045
epoch: 18, step: 84
	action: tensor([[-0.0022,  0.0226,  0.0108, -0.0074, -0.0008,  0.0069,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[7.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30114814980076854, distance: 0.9566415790182 entropy -11.0677112873767
epoch: 18, step: 85
	action: tensor([[-0.0022,  0.0228,  0.0151, -0.0074, -0.0068, -0.0121,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[7.1223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29695009450212506, distance: 0.9595105856189204 entropy -11.06967062065451
epoch: 18, step: 86
	action: tensor([[-0.0020,  0.0227,  0.0157, -0.0075, -0.0123,  0.0060, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[7.1413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011394728697845, distance: 0.9566475178213885 entropy -11.067875108111853
epoch: 18, step: 87
	action: tensor([[-0.0022,  0.0229,  0.0028, -0.0072, -0.0028, -0.0148,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[7.1178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296518474682518, distance: 0.9598050741316488 entropy -11.074806545981554
epoch: 18, step: 88
	action: tensor([[-0.0021,  0.0229,  0.0115, -0.0075, -0.0153,  0.0105,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[7.1322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302224855391489, distance: 0.9559043566907227 entropy -11.064694811043855
epoch: 18, step: 89
	action: tensor([[-0.0022,  0.0225,  0.0199, -0.0076, -0.0094,  0.0077,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[7.1351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30125118446711197, distance: 0.9565710555737891 entropy -11.061567557721476
epoch: 18, step: 90
	action: tensor([[-0.0021,  0.0222,  0.0070, -0.0078, -0.0006,  0.0088,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[7.1537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301240769262363, distance: 0.956578184635087 entropy -11.05869390534993
epoch: 18, step: 91
	action: tensor([[-0.0021,  0.0227,  0.0026, -0.0075, -0.0040,  0.0057,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[7.1255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30097636055544796, distance: 0.9567591508874627 entropy -11.064473269638322
epoch: 18, step: 92
	action: tensor([[-0.0021,  0.0226,  0.0041, -0.0077, -0.0098,  0.0010,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[7.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997622908753962, distance: 0.9575896438001934 entropy -11.058634989388223
epoch: 18, step: 93
	action: tensor([[-0.0021,  0.0227,  0.0163, -0.0076, -0.0009,  0.0008,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[7.1326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997718048188287, distance: 0.957583138520251 entropy -11.061783451935929
epoch: 18, step: 94
	action: tensor([[-0.0021,  0.0226,  0.0190, -0.0075, -0.0039, -0.0058,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[7.1390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982395976420349, distance: 0.9586302357527735 entropy -11.06649117001581
epoch: 18, step: 95
	action: tensor([[-0.0021,  0.0226,  0.0053, -0.0075, -0.0047, -0.0015, -0.0019]],
       dtype=torch.float64)
	q_value: tensor([[7.1422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29941581303051534, distance: 0.957826522334607 entropy -11.069690978340725
epoch: 18, step: 96
	action: tensor([[-0.0022,  0.0230,  0.0141, -0.0073, -0.0011, -0.0142,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.1169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965918727536898, distance: 0.9597550019717879 entropy -11.069764633285939
epoch: 18, step: 97
	action: tensor([[-0.0020,  0.0226,  0.0069, -0.0076,  0.0029,  0.0011,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[7.1458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30000495044331477, distance: 0.9574237084179452 entropy -11.065830512860604
epoch: 18, step: 98
	action: tensor([[-0.0022,  0.0229,  0.0089, -0.0074,  0.0008, -0.0032, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[7.1208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991047786019019, distance: 0.9580391191729501 entropy -11.069205430411671
epoch: 18, step: 99
	action: tensor([[-0.0022,  0.0229,  0.0104, -0.0073, -0.0166,  0.0435,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[7.1200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098780230292385, distance: 0.9506477445353515 entropy -11.071821808871121
epoch: 18, step: 100
	action: tensor([[-0.0024,  0.0224,  0.0115, -0.0072, -0.0110,  0.0067,  0.0263]],
       dtype=torch.float64)
	q_value: tensor([[7.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008926042411125, distance: 0.9568164681335432 entropy -11.067520318368809
epoch: 18, step: 101
	action: tensor([[-0.0021,  0.0226,  0.0058, -0.0075, -0.0151, -0.0121, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[7.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968828509919258, distance: 0.9595564709228216 entropy -11.063805535206225
epoch: 18, step: 102
	action: tensor([[-0.0022,  0.0231,  0.0222, -0.0073, -0.0017,  0.0228, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[7.1178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053604622434062, distance: 0.9537541546064846 entropy -11.070657791904907
epoch: 18, step: 103
	action: tensor([[-0.0023,  0.0226, -0.0019, -0.0071, -0.0051, -0.0104,  0.0368]],
       dtype=torch.float64)
	q_value: tensor([[7.1219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29731359350240594, distance: 0.9592625049134987 entropy -11.079260062659221
epoch: 18, step: 104
	action: tensor([[-0.0021,  0.0227,  0.0069, -0.0078, -0.0119, -0.0048,  0.0474]],
       dtype=torch.float64)
	q_value: tensor([[7.1376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985129260580699, distance: 0.9584435292917102 entropy -11.05551221071984
epoch: 18, step: 105
	action: tensor([[-0.0021,  0.0224,  0.0123, -0.0078, -0.0020,  0.0126, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[7.1489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30232873426070206, distance: 0.9558332005594368 entropy -11.054731102111361
epoch: 18, step: 106
	action: tensor([[-0.0023,  0.0229,  0.0041, -0.0072, -0.0018,  0.0037,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[7.1101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007067283908593, distance: 0.9569436569286783 entropy -11.076113481735705
epoch: 18, step: 107
	action: tensor([[-0.0021,  0.0225,  0.0097, -0.0077, -0.0030, -0.0228,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[7.1357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29424872151167347, distance: 0.9613522122832443 entropy -11.057198585024043
epoch: 18, step: 108
	action: tensor([[-0.0020,  0.0228,  0.0071, -0.0076, -0.0114,  0.0275,  0.0357]],
       dtype=torch.float64)
	q_value: tensor([[7.1380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30618955345878773, distance: 0.9531848051525534 entropy -11.066030039900069
epoch: 18, step: 109
	action: tensor([[-0.0023,  0.0224,  0.0057, -0.0074, -0.0052, -0.0076, -0.0303]],
       dtype=torch.float64)
	q_value: tensor([[7.1261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976810023735378, distance: 0.9590116905868528 entropy -11.064095649026013
epoch: 18, step: 110
	action: tensor([[-0.0023,  0.0232,  0.0072, -0.0071, -0.0101,  0.0121, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[7.1042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028951350532353, distance: 0.9554451276706505 entropy -11.076464376432579
epoch: 18, step: 111
	action: tensor([[-0.0023,  0.0230,  0.0183, -0.0071, -0.0114,  0.0015, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.1087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300202710223445, distance: 0.9572884551208226 entropy -11.075315542285026
epoch: 18, step: 112
	action: tensor([[-0.0022,  0.0227,  0.0043, -0.0074, -0.0061,  0.0070, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[7.1320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013944989334725, distance: 0.9564729534393698 entropy -11.07263201143139
epoch: 18, step: 113
	action: tensor([[-0.0022,  0.0230,  0.0047, -0.0073,  0.0002, -0.0184, -0.0338]],
       dtype=torch.float64)
	q_value: tensor([[7.1109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29571308364928495, distance: 0.9603543403517073 entropy -11.070350894751082
epoch: 18, step: 114
	action: tensor([[-0.0022,  0.0232,  0.0118, -0.0072, -0.0185, -0.0011,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[7.1100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998899470726889, distance: 0.9575023535680872 entropy -11.075543840136458
epoch: 18, step: 115
	action: tensor([[-0.0022,  0.0229,  0.0178, -0.0074, -0.0116, -0.0004, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[7.1265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29970544719132364, distance: 0.9576285104720637 entropy -11.068741404522006
epoch: 18, step: 116
	action: tensor([[-0.0022,  0.0228,  0.0136, -0.0073, -0.0105, -0.0125, -0.0309]],
       dtype=torch.float64)
	q_value: tensor([[7.1274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29697582761387387, distance: 0.959493025401024 entropy -11.074767300577687
epoch: 18, step: 117
	action: tensor([[-0.0022,  0.0231,  0.0168, -0.0072, -0.0077,  0.0076,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[7.1148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017768878326107, distance: 0.9562111499598764 entropy -11.077554351893927
epoch: 18, step: 118
	action: tensor([[-0.0022,  0.0227,  0.0072, -0.0074, -0.0113, -0.0023,  0.0147]],
       dtype=torch.float64)
	q_value: tensor([[7.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29917907768721697, distance: 0.9579883388889706 entropy -11.071545468456222
epoch: 18, step: 119
	action: tensor([[-0.0021,  0.0228,  0.0011, -0.0075, -0.0172,  0.0138,  0.0466]],
       dtype=torch.float64)
	q_value: tensor([[7.1300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302916225916258, distance: 0.9554306740957673 entropy -11.066046761103754
epoch: 18, step: 120
	action: tensor([[-0.0022,  0.0225,  0.0083, -0.0076, -0.0066,  0.0099, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[7.1286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30168032353368424, distance: 0.9562772697039952 entropy -11.055874665828009
epoch: 18, step: 121
	action: tensor([[-0.0023,  0.0230,  0.0133, -0.0071, -0.0081, -0.0085, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[7.1071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979559127471888, distance: 0.9588239781174022 entropy -11.074821926866663
epoch: 18, step: 122
	action: tensor([[-0.0021,  0.0228,  0.0115, -0.0074, -0.0112, -0.0060, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[7.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298464955695232, distance: 0.9584762997443499 entropy -11.07071222742015
epoch: 18, step: 123
	action: tensor([[-0.0021,  0.0229,  0.0094, -0.0073, -0.0002, -0.0112, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.1274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972930010443783, distance: 0.9592765605633364 entropy -11.071196775215467
epoch: 18, step: 124
	action: tensor([[-0.0021,  0.0229,  0.0096, -0.0075, -0.0022, -0.0009,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[7.1287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996892462407529, distance: 0.9576395875269772 entropy -11.069276149100958
epoch: 18, step: 125
	action: tensor([[-0.0021,  0.0229,  0.0118, -0.0074, -0.0025, -0.0083, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[7.1237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29792646890132846, distance: 0.9588440845251974 entropy -11.069720257113163
epoch: 18, step: 126
	action: tensor([[-0.0021,  0.0228,  0.0048, -0.0074, -0.0144,  0.0011,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[7.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30012285520618387, distance: 0.9573430724411188 entropy -11.070932459900776
epoch: 18, step: 127
	action: tensor([[-0.0022,  0.0228,  0.0082, -0.0074,  0.0126, -0.0097,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[7.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29752678516282494, distance: 0.9591169760700107 entropy -11.065084799751753
LOSS epoch 18 actor 23.371382045150924 critic 12.01258938277454
epoch: 19, step: 0
	action: tensor([[-0.0019,  0.0215,  0.0025, -0.0084, -0.0089,  0.0091,  0.0212]],
       dtype=torch.float64)
	q_value: tensor([[6.6073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30078600445204784, distance: 0.9568894129675498 entropy -11.051279937260878
epoch: 19, step: 1
	action: tensor([[-0.0021,  0.0219,  0.0099, -0.0080, -0.0066, -0.0094,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[6.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967715427616515, distance: 0.9596324200772844 entropy -11.057496628291025
epoch: 19, step: 2
	action: tensor([[-0.0019,  0.0217,  0.0045, -0.0082, -0.0071,  0.0296,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[6.5991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056670077100625, distance: 0.9535436848165102 entropy -11.05317295810686
epoch: 19, step: 3
	action: tensor([[-0.0022,  0.0220,  0.0157, -0.0076, -0.0011, -0.0001,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[6.5584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989734255109576, distance: 0.9581288868731362 entropy -11.066660201336655
epoch: 19, step: 4
	action: tensor([[-0.0020,  0.0217,  0.0101, -0.0080,  0.0024, -0.0078,  0.0212]],
       dtype=torch.float64)
	q_value: tensor([[6.5924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29712027299278276, distance: 0.9593944502319273 entropy -11.060648853050822
epoch: 19, step: 5
	action: tensor([[-0.0019,  0.0217,  0.0190, -0.0081, -0.0077, -0.0048, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[6.5967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29775885471626784, distance: 0.9589585356983735 entropy -11.057946126941639
epoch: 19, step: 6
	action: tensor([[-0.0020,  0.0219,  0.0176, -0.0079, -0.0016,  0.0204,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[6.5893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303804463941058, distance: 0.9548217656687148 entropy -11.06726963481327
epoch: 19, step: 7
	action: tensor([[-0.0020,  0.0215,  0.0032, -0.0079, -0.0114,  0.0099,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[6.5897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009755077718669, distance: 0.9567597344930806 entropy -11.062168383289395
epoch: 19, step: 8
	action: tensor([[-0.0021,  0.0218,  0.0156, -0.0080, -0.0097,  0.0117, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[6.5805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30157085699498976, distance: 0.9563522183731 entropy -11.057509869547511
epoch: 19, step: 9
	action: tensor([[-0.0022,  0.0220,  0.0159, -0.0077, -0.0122,  0.0169, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[6.5727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029382273294149, distance: 0.9554155962875807 entropy -11.068930423944707
epoch: 19, step: 10
	action: tensor([[-0.0022,  0.0220,  0.0124, -0.0076, -0.0076, -0.0085,  0.0484]],
       dtype=torch.float64)
	q_value: tensor([[6.5703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29705202370095873, distance: 0.9594410274768661 entropy -11.069780393694769
epoch: 19, step: 11
	action: tensor([[-1.8903e-03,  2.1442e-02,  1.5977e-02, -8.4169e-03, -2.8270e-05,
          1.0877e-02, -2.5368e-02]], dtype=torch.float64)
	q_value: tensor([[6.6118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011163930958243, distance: 0.9566633142681229 entropy -11.048610141999783
epoch: 19, step: 12
	action: tensor([[-0.0022,  0.0221,  0.0184, -0.0076, -0.0010, -0.0120,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[6.5667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962588307071825, distance: 0.959982182275656 entropy -11.072978370963074
epoch: 19, step: 13
	action: tensor([[-0.0019,  0.0218,  0.0061, -0.0080, -0.0009, -0.0300,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[6.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29219755215591714, distance: 0.9627482179121171 entropy -11.063090832917286
epoch: 19, step: 14
	action: tensor([[-0.0018,  0.0220,  0.0089, -0.0081, -0.0048, -0.0405, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[6.5952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2899998613510145, distance: 0.9642417017764489 entropy -11.05859864587473
epoch: 19, step: 15
	action: tensor([[-0.0018,  0.0220,  0.0004, -0.0081, -0.0016, -0.0035,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[6.6027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29848643468961955, distance: 0.9584616267320664 entropy -11.060513860350595
epoch: 19, step: 16
	action: tensor([[-2.0218e-03,  2.1998e-02,  7.8079e-03, -7.9980e-03,  2.5978e-05,
         -2.9638e-03, -4.9209e-03]], dtype=torch.float64)
	q_value: tensor([[6.5794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983790235469873, distance: 0.958535000593352 entropy -11.057876083684013
epoch: 19, step: 17
	action: tensor([[-0.0020,  0.0220,  0.0090, -0.0078,  0.0037, -0.0120, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[6.5812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29633883063553645, distance: 0.9599276162705291 entropy -11.064662523346604
epoch: 19, step: 18
	action: tensor([[-0.0020,  0.0220,  0.0055, -0.0079, -0.0002,  0.0009,  0.0388]],
       dtype=torch.float64)
	q_value: tensor([[6.5843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29936824609175494, distance: 0.9578590381286065 entropy -11.06401511465316
epoch: 19, step: 19
	action: tensor([[-0.0020,  0.0216,  0.0060, -0.0083, -0.0053,  0.0272, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[6.5967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30500200708609115, distance: 0.9540002059636619 entropy -11.051846594303914
epoch: 19, step: 20
	action: tensor([[-0.0023,  0.0221,  0.0132, -0.0074, -0.0048, -0.0141,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[6.5530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2958834246174251, distance: 0.9602381962228409 entropy -11.071165744887557
epoch: 19, step: 21
	action: tensor([[-0.0019,  0.0216,  0.0124, -0.0083, -0.0086, -0.0176, -0.0202]],
       dtype=torch.float64)
	q_value: tensor([[6.6053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29480636827380235, distance: 0.960972332781069 entropy -11.053135009449347
epoch: 19, step: 22
	action: tensor([[-0.0020,  0.0222,  0.0101, -0.0078, -0.0063,  0.0037, -0.0111]],
       dtype=torch.float64)
	q_value: tensor([[6.5836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001013679883904, distance: 0.9573577682212062 entropy -11.068476226680227
epoch: 19, step: 23
	action: tensor([[-0.0021,  0.0220,  0.0102, -0.0077, -0.0121,  0.0244,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[6.5760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30469530494966157, distance: 0.9542106825629147 entropy -11.067201428200761
epoch: 19, step: 24
	action: tensor([[-0.0021,  0.0217,  0.0153, -0.0079, -0.0135,  0.0211, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[6.5796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30357763981942876, distance: 0.9549772959468629 entropy -11.058897738639363
epoch: 19, step: 25
	action: tensor([[-0.0022,  0.0219,  0.0051, -0.0077, -0.0070,  0.0241,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[6.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30455439199296763, distance: 0.9543073695532848 entropy -11.068696537394471
epoch: 19, step: 26
	action: tensor([[-0.0022,  0.0219,  0.0013, -0.0077, -0.0103,  0.0086,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.5664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009438524099203, distance: 0.9567813977059962 entropy -11.063045146527388
epoch: 19, step: 27
	action: tensor([[-0.0021,  0.0219, -0.0032, -0.0079,  0.0001,  0.0014, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[6.5772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299351420075081, distance: 0.9578705397878547 entropy -11.058751852893119
epoch: 19, step: 28
	action: tensor([[-0.0021,  0.0222,  0.0062, -0.0078, -0.0046,  0.0030, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[6.5692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998772526885416, distance: 0.9575110342373261 entropy -11.062709324653229
epoch: 19, step: 29
	action: tensor([[-0.0021,  0.0220,  0.0086, -0.0078,  0.0003,  0.0082,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[6.5779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30100550648384017, distance: 0.9567392045488791 entropy -11.063777839677927
epoch: 19, step: 30
	action: tensor([[-2.0626e-03,  2.1893e-02,  9.7091e-07, -7.8870e-03, -1.4286e-02,
         -2.8436e-03,  1.7756e-02]], dtype=torch.float64)
	q_value: tensor([[6.5782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29839972093414446, distance: 0.9585208623926783 entropy -11.061938591545712
epoch: 19, step: 31
	action: tensor([[-0.0020,  0.0220,  0.0098, -0.0080,  0.0090, -0.0015, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[6.5791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987313765885058, distance: 0.9582942829207587 entropy -11.056782623766976
epoch: 19, step: 32
	action: tensor([[-0.0022,  0.0222,  0.0076, -0.0076, -0.0023, -0.0011, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[6.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29896672597992124, distance: 0.9581334651581199 entropy -11.07314375984144
epoch: 19, step: 33
	action: tensor([[-0.0021,  0.0220,  0.0078, -0.0078, -0.0042,  0.0015,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[6.5797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29947129952828566, distance: 0.9577885914959369 entropy -11.0646024407926
epoch: 19, step: 34
	action: tensor([[-0.0020,  0.0219,  0.0103, -0.0079, -0.0068, -0.0440,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[6.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28899354943364364, distance: 0.9649247892049654 entropy -11.060808306455968
epoch: 19, step: 35
	action: tensor([[-0.0018,  0.0219,  0.0086, -0.0082, -0.0082,  0.0256, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[6.6104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050440102482679, distance: 0.9539713773690245 entropy -11.056673482042546
epoch: 19, step: 36
	action: tensor([[-0.0023,  0.0220,  0.0147, -0.0075, -0.0096, -0.0071,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.5607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29740411904168373, distance: 0.9592007130873567 entropy -11.071812210189753
epoch: 19, step: 37
	action: tensor([[-0.0020,  0.0219,  0.0085, -0.0079, -0.0025,  0.0143,  0.0527]],
       dtype=torch.float64)
	q_value: tensor([[6.5898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30233418969379267, distance: 0.9558294634883252 entropy -11.062236904560178
epoch: 19, step: 38
	action: tensor([[-0.0021,  0.0214,  0.0052, -0.0083, -0.0152, -0.0101,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[6.5987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29617031859499665, distance: 0.9600425506203975 entropy -11.049520975641105
epoch: 19, step: 39
	action: tensor([[-0.0019,  0.0218,  0.0088, -0.0081, -0.0070, -0.0197,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[6.5937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294474020119772, distance: 0.9611987526931465 entropy -11.05500254593718
epoch: 19, step: 40
	action: tensor([[-0.0019,  0.0218,  0.0041, -0.0082, -0.0101,  0.0020,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[6.6028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994586205066192, distance: 0.9577972590689767 entropy -11.054747579076503
epoch: 19, step: 41
	action: tensor([[-0.0020,  0.0218,  0.0151, -0.0081, -0.0117,  0.0220,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[6.5879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039349045199362, distance: 0.9547323128269471 entropy -11.055917759531848
epoch: 19, step: 42
	action: tensor([[-0.0021,  0.0217,  0.0134, -0.0079,  0.0005, -0.0093,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[6.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29661401161159817, distance: 0.9597398983313957 entropy -11.062773036520088
epoch: 19, step: 43
	action: tensor([[-0.0019,  0.0219,  0.0075, -0.0080, -0.0048, -0.0072,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[6.5921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29743088637552184, distance: 0.9591824412109139 entropy -11.063410174854955
epoch: 19, step: 44
	action: tensor([[-0.0020,  0.0220,  0.0180, -0.0079, -0.0019, -0.0133,  0.0548]],
       dtype=torch.float64)
	q_value: tensor([[6.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959227385279162, distance: 0.9602113886977962 entropy -11.061654882873045
epoch: 19, step: 45
	action: tensor([[-0.0018,  0.0213,  0.0030, -0.0085,  0.0003,  0.0046,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[6.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29955463840194874, distance: 0.9577316178155577 entropy -11.047900701375317
epoch: 19, step: 46
	action: tensor([[-0.0020,  0.0218,  0.0149, -0.0080, -0.0076,  0.0049,  0.0546]],
       dtype=torch.float64)
	q_value: tensor([[6.5831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000167970000902, distance: 0.9574156067732867 entropy -11.057687702033006
epoch: 19, step: 47
	action: tensor([[-0.0020,  0.0213, -0.0097, -0.0084, -0.0140,  0.0186,  0.0134]],
       dtype=torch.float64)
	q_value: tensor([[6.6062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026862709597059, distance: 0.9555882504955306 entropy -11.048922200904078
epoch: 19, step: 48
	action: tensor([[-0.0022,  0.0222,  0.0010, -0.0078, -0.0091, -0.0043,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[6.5567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29814164508528285, distance: 0.9586971367960687 entropy -11.058316812547114
epoch: 19, step: 49
	action: tensor([[-0.0020,  0.0221,  0.0097, -0.0079, -0.0141,  0.0061, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[6.5779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004843255388562, distance: 0.9570958177516062 entropy -11.059033207406328
epoch: 19, step: 50
	action: tensor([[-0.0022,  0.0221,  0.0023, -0.0076, -0.0082, -0.0050,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[6.5702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29807222780643805, distance: 0.9587445455791608 entropy -11.069593402988817
epoch: 19, step: 51
	action: tensor([[-0.0020,  0.0220,  0.0122, -0.0080,  0.0057,  0.0082,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[6.5845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30095689071051757, distance: 0.9567724750595961 entropy -11.058437514746268
epoch: 19, step: 52
	action: tensor([[-2.0121e-03,  2.1746e-02,  8.8945e-03, -7.9294e-03, -8.3994e-05,
         -8.6376e-03, -1.2811e-02]], dtype=torch.float64)
	q_value: tensor([[6.5864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969208440805551, distance: 0.9595305456509141 entropy -11.062563915828838
epoch: 19, step: 53
	action: tensor([[-0.0020,  0.0221,  0.0196, -0.0078,  0.0007,  0.0151, -0.0364]],
       dtype=torch.float64)
	q_value: tensor([[6.5802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026949880043237, distance: 0.9555822776234203 entropy -11.066717052119287
epoch: 19, step: 54
	action: tensor([[-0.0023,  0.0220,  0.0147, -0.0075,  0.0005,  0.0304,  0.0385]],
       dtype=torch.float64)
	q_value: tensor([[6.5678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30611100040645123, distance: 0.9532387633028162 entropy -11.077526784943903
epoch: 19, step: 55
	action: tensor([[-0.0021,  0.0213,  0.0156, -0.0080, -0.0036, -0.0348,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[6.5891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904397428787445, distance: 0.96394295688462 entropy -11.059170415917738
epoch: 19, step: 56
	action: tensor([[-0.0018,  0.0219,  0.0011, -0.0082, -0.0045,  0.0007,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[6.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993135972805925, distance: 0.9578963935843843 entropy -11.060775408337012
epoch: 19, step: 57
	action: tensor([[-0.0021,  0.0221,  0.0088, -0.0079, -0.0144,  0.0009,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[6.5729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299340512870655, distance: 0.9578779954863534 entropy -11.061213251109107
epoch: 19, step: 58
	action: tensor([[-0.0021,  0.0220,  0.0179, -0.0079, -0.0101, -0.0130,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[6.5831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2960270239883258, distance: 0.96014027448884 entropy -11.062396436161935
epoch: 19, step: 59
	action: tensor([[-0.0019,  0.0216,  0.0068, -0.0082, -0.0240,  0.0074, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[6.6102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004963987877828, distance: 0.957087558246969 entropy -11.055452618415375
epoch: 19, step: 60
	action: tensor([[-0.0022,  0.0221,  0.0072, -0.0077, -0.0139, -0.0143, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[6.5715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959520096785925, distance: 0.9601914286835286 entropy -11.065964117142869
epoch: 19, step: 61
	action: tensor([[-0.0020,  0.0222,  0.0092, -0.0078, -0.0037,  0.0076,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[6.5800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30102794225563956, distance: 0.9567238500969414 entropy -11.064359104825513
epoch: 19, step: 62
	action: tensor([[-0.0020,  0.0216,  0.0087, -0.0081, -0.0114,  0.0124,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[6.5911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30158212090927305, distance: 0.9563445065579839 entropy -11.05451890109438
epoch: 19, step: 63
	action: tensor([[-0.0021,  0.0220,  0.0036, -0.0078, -0.0011, -0.0366, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[6.5741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2907236581071512, distance: 0.9637500871007633 entropy -11.062719683903268
epoch: 19, step: 64
	action: tensor([[-0.0018,  0.0222,  0.0030, -0.0081, -0.0142, -0.0019, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[6.5917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29893478642963267, distance: 0.9581552915139931 entropy -11.060101683740857
epoch: 19, step: 65
	action: tensor([[-0.0021,  0.0221,  0.0146, -0.0078, -0.0008,  0.0013,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[6.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994596702391449, distance: 0.9577965414587527 entropy -11.062861322226864
epoch: 19, step: 66
	action: tensor([[-0.0020,  0.0218,  0.0046, -0.0079, -0.0045,  0.0058, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[6.5898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30033108478784054, distance: 0.9572006460318851 entropy -11.063453215414258
epoch: 19, step: 67
	action: tensor([[-0.0022,  0.0222,  0.0005, -0.0076, -0.0123, -0.0139,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[6.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2961030143195237, distance: 0.9600884519409578 entropy -11.067408122942158
epoch: 19, step: 68
	action: tensor([[-0.0020,  0.0221,  0.0081, -0.0080, -0.0076,  0.0136,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[6.5843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022413456007933, distance: 0.9558930613929681 entropy -11.056809202109623
epoch: 19, step: 69
	action: tensor([[-0.0021,  0.0215, -0.0056, -0.0082,  0.0009, -0.0011,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[6.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984075607509461, distance: 0.958515507029248 entropy -11.051221524795528
epoch: 19, step: 70
	action: tensor([[-0.0021,  0.0221,  0.0161, -0.0079, -0.0082, -0.0023,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[6.5700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29860209331643495, distance: 0.958382612632693 entropy -11.060581030130487
epoch: 19, step: 71
	action: tensor([[-1.9752e-03,  2.1817e-02, -3.5487e-03, -7.9725e-03, -6.7356e-03,
         -1.1884e-02, -2.2458e-06]], dtype=torch.float64)
	q_value: tensor([[6.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296386481778647, distance: 0.9598951131108909 entropy -11.061436032193786
epoch: 19, step: 72
	action: tensor([[-0.0020,  0.0222,  0.0179, -0.0079, -0.0159, -0.0189, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[6.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29487370377514655, distance: 0.9609264524046585 entropy -11.060814376892912
epoch: 19, step: 73
	action: tensor([[-0.0020,  0.0221,  0.0045, -0.0078, -0.0094, -0.0064, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[6.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977578802352133, distance: 0.9589592010585615 entropy -11.06813060791686
epoch: 19, step: 74
	action: tensor([[-0.0020,  0.0221,  0.0065, -0.0079, -0.0053,  0.0096, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[6.5803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013754434194763, distance: 0.9564859979683916 entropy -11.062737328226518
epoch: 19, step: 75
	action: tensor([[-0.0022,  0.0221,  0.0208, -0.0077, -0.0084,  0.0005, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[6.5686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992092451290028, distance: 0.9579677199494252 entropy -11.067931181339796
epoch: 19, step: 76
	action: tensor([[-0.0021,  0.0219,  0.0145, -0.0077, -0.0072,  0.0096,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[6.5840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012352309746741, distance: 0.9565819754935494 entropy -11.070609753319518
epoch: 19, step: 77
	action: tensor([[-0.0020,  0.0217,  0.0156, -0.0080, -0.0167, -0.0001,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[6.5906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29873346432179837, distance: 0.9582928564599841 entropy -11.058309130141339
epoch: 19, step: 78
	action: tensor([[-0.0020,  0.0218,  0.0092, -0.0080, -0.0042,  0.0134, -0.0187]],
       dtype=torch.float64)
	q_value: tensor([[6.5935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30200012105790963, distance: 0.9560582796526765 entropy -11.058280291784474
epoch: 19, step: 79
	action: tensor([[-0.0022,  0.0221,  0.0124, -0.0076, -0.0055, -0.0007,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[6.5629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989471056420454, distance: 0.9581468730600559 entropy -11.070037524221496
epoch: 19, step: 80
	action: tensor([[-1.9393e-03,  2.1596e-02,  6.3831e-06, -8.1880e-03, -5.0008e-03,
          3.6597e-02,  2.3355e-02]], dtype=torch.float64)
	q_value: tensor([[6.6007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072058065501393, distance: 0.9524864645718386 entropy -11.054837381922692
epoch: 19, step: 81
	action: tensor([[-0.0022,  0.0218,  0.0097, -0.0077, -0.0125, -0.0199,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[6.5615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29430061911940264, distance: 0.9613168649882693 entropy -11.060122125311226
epoch: 19, step: 82
	action: tensor([[-0.0019,  0.0218,  0.0149, -0.0082,  0.0011,  0.0027,  0.0386]],
       dtype=torch.float64)
	q_value: tensor([[6.6036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995455066039836, distance: 0.957737860831511 entropy -11.054132266545386
epoch: 19, step: 83
	action: tensor([[-0.0019,  0.0214,  0.0012, -0.0083, -0.0066,  0.0108,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[6.6046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30112984983747726, distance: 0.9566541041271843 entropy -11.054158160429782
epoch: 19, step: 84
	action: tensor([[-0.0021,  0.0221,  0.0189, -0.0078, -0.0002,  0.0381, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[6.5671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080548752022453, distance: 0.9519026156119406 entropy -11.061896688564707
epoch: 19, step: 85
	action: tensor([[-0.0024,  0.0219,  0.0163, -0.0073, -0.0057, -0.0016,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[6.5582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29853302636029266, distance: 0.9584297976469487 entropy -11.078391188811663
epoch: 19, step: 86
	action: tensor([[-0.0020,  0.0218,  0.0018, -0.0079, -0.0063, -0.0128,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[6.5920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2961261507484343, distance: 0.9600726732111142 entropy -11.063089222218878
epoch: 19, step: 87
	action: tensor([[-0.0020,  0.0221,  0.0064, -0.0079, -0.0156, -0.0271, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[6.5830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29311891539982715, distance: 0.9621213977574351 entropy -11.060914772338412
epoch: 19, step: 88
	action: tensor([[-0.0019,  0.0222,  0.0187, -0.0079, -0.0043,  0.0109, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[6.5897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30178371864169895, distance: 0.9562064725783384 entropy -11.062994300199103
epoch: 19, step: 89
	action: tensor([[-0.0021,  0.0218,  0.0116, -0.0078, -0.0069, -0.0110, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[6.5809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29645195737888674, distance: 0.9598504499760325 entropy -11.067245655016624
epoch: 19, step: 90
	action: tensor([[-0.0020,  0.0221,  0.0066, -0.0078, -0.0100,  0.0158,  0.0155]],
       dtype=torch.float64)
	q_value: tensor([[6.5824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028088301017411, distance: 0.9555042702024829 entropy -11.065291172078146
epoch: 19, step: 91
	action: tensor([[-0.0021,  0.0219,  0.0007, -0.0078, -0.0070,  0.0006,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[6.5778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29910971347782656, distance: 0.9580357464773163 entropy -11.061215028943504
epoch: 19, step: 92
	action: tensor([[-0.0020,  0.0219,  0.0162, -0.0081, -0.0071, -0.0029,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[6.5810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29824969817265445, distance: 0.9586233368818357 entropy -11.055408290597173
epoch: 19, step: 93
	action: tensor([[-0.0020,  0.0217,  0.0151, -0.0080, -0.0123, -0.0095, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[6.5973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29674194360353723, distance: 0.9596526155148839 entropy -11.06059329438771
epoch: 19, step: 94
	action: tensor([[-0.0020,  0.0220,  0.0107, -0.0078, -0.0084,  0.0105,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[6.5875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015447782142331, distance: 0.9563700729165914 entropy -11.066191577860069
epoch: 19, step: 95
	action: tensor([[-0.0020,  0.0218,  0.0028, -0.0080, -0.0122, -0.0114, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[6.5870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963531690843738, distance: 0.9599178360355738 entropy -11.0598103443329
epoch: 19, step: 96
	action: tensor([[-0.0020,  0.0222,  0.0239, -0.0078, -0.0024,  0.0054, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[6.5773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004596504186179, distance: 0.9571126981782953 entropy -11.0642758225496
epoch: 19, step: 97
	action: tensor([[-0.0020,  0.0217,  0.0180, -0.0079,  0.0122,  0.0088,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[6.5930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30094946587800375, distance: 0.9567775561886592 entropy -11.067532436664036
epoch: 19, step: 98
	action: tensor([[-0.0020,  0.0217,  0.0060, -0.0079, -0.0108, -0.0068, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[6.5886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29734054664570897, distance: 0.9592441073840611 entropy -11.065787279087484
epoch: 19, step: 99
	action: tensor([[-0.0021,  0.0222, -0.0038, -0.0078, -0.0052, -0.0122,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[6.5737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965549346947497, distance: 0.9597802014399586 entropy -11.065254130986517
epoch: 19, step: 100
	action: tensor([[-0.0020,  0.0219,  0.0105, -0.0082, -0.0069,  0.0026,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[6.5893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995240319012117, distance: 0.9577525419981343 entropy -11.049461236745362
epoch: 19, step: 101
	action: tensor([[-0.0020,  0.0219,  0.0068, -0.0079, -0.0076,  0.0081, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[6.5818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30092748108662637, distance: 0.9567926011593456 entropy -11.062192930643297
epoch: 19, step: 102
	action: tensor([[-0.0022,  0.0222,  0.0152, -0.0076,  0.0026,  0.0022, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[6.5639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29969677198309563, distance: 0.9576344419768651 entropy -11.068362351040859
epoch: 19, step: 103
	action: tensor([[-0.0022,  0.0220,  0.0120, -0.0077, -0.0084, -0.0036, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[6.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982647478195132, distance: 0.958613057570232 entropy -11.07176200852363
epoch: 19, step: 104
	action: tensor([[-0.0022,  0.0222,  0.0117, -0.0076, -0.0023,  0.0119,  0.0578]],
       dtype=torch.float64)
	q_value: tensor([[6.5728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019357355398914, distance: 0.9561023734197249 entropy -11.071215458079172
epoch: 19, step: 105
	action: tensor([[-0.0020,  0.0212,  0.0125, -0.0084, -0.0065, -0.0095,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[6.6060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2961363240515328, distance: 0.9600657350746828 entropy -11.04858336451925
epoch: 19, step: 106
	action: tensor([[-0.0019,  0.0217,  0.0078, -0.0082, -0.0134,  0.0102, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[6.6008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012438331653242, distance: 0.9565760874420794 entropy -11.05615139423425
epoch: 19, step: 107
	action: tensor([[-0.0023,  0.0222,  0.0100, -0.0075, -0.0053,  0.0035,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[6.5604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30000359994199166, distance: 0.9574246319968778 entropy -11.073141028260327
epoch: 19, step: 108
	action: tensor([[-0.0021,  0.0219,  0.0101, -0.0079, -0.0009, -0.0075,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[6.5803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29731506918628847, distance: 0.9592614976583792 entropy -11.062723920832394
epoch: 19, step: 109
	action: tensor([[-0.0019,  0.0217,  0.0083, -0.0082, -0.0032, -0.0040,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[6.5988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979700167372755, distance: 0.958814346733812 entropy -11.056366690343225
epoch: 19, step: 110
	action: tensor([[-0.0020,  0.0219,  0.0178, -0.0079,  0.0007, -0.0200, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[6.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943366603325819, distance: 0.9612923166682736 entropy -11.061550269114488
epoch: 19, step: 111
	action: tensor([[-0.0019,  0.0218,  0.0210, -0.0080, -0.0106, -0.0011, -0.0231]],
       dtype=torch.float64)
	q_value: tensor([[6.6014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987834927200367, distance: 0.9582586735143513 entropy -11.063996208930366
epoch: 19, step: 112
	action: tensor([[-0.0021,  0.0221,  0.0153, -0.0077, -0.0040, -0.0169,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[6.5781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29519637395974174, distance: 0.9607065642816583 entropy -11.07167213353226
epoch: 19, step: 113
	action: tensor([[-0.0018,  0.0216, -0.0012, -0.0082, -0.0016,  0.0271,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[6.6090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051394290554009, distance: 0.9539058840594021 entropy -11.055488034341726
epoch: 19, step: 114
	action: tensor([[-0.0022,  0.0220,  0.0090, -0.0077, -0.0003, -0.0029,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[6.5619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29830583870253446, distance: 0.9585849908359894 entropy -11.063180669735534
epoch: 19, step: 115
	action: tensor([[-0.0019,  0.0216,  0.0038, -0.0083, -0.0110,  0.0002,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[6.6006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29880696855234457, distance: 0.9582426327423642 entropy -11.052887566343893
epoch: 19, step: 116
	action: tensor([[-0.0021,  0.0221,  0.0187, -0.0078, -0.0060, -0.0013, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[6.5759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29879423577104347, distance: 0.9582513329418387 entropy -11.06298874658445
epoch: 19, step: 117
	action: tensor([[-0.0020,  0.0219,  0.0160, -0.0079,  0.0006,  0.0171,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[6.5877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030063901469139, distance: 0.955368882055353 entropy -11.065470331841942
epoch: 19, step: 118
	action: tensor([[-0.0020,  0.0216,  0.0176, -0.0080, -0.0158, -0.0065,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[6.5878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29719129562458735, distance: 0.9593459778990829 entropy -11.060072791371445
epoch: 19, step: 119
	action: tensor([[-0.0019,  0.0218,  0.0058, -0.0080, -0.0063, -0.0086,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[6.5959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2970233551185676, distance: 0.959460591893177 entropy -11.059832102005885
epoch: 19, step: 120
	action: tensor([[-0.0020,  0.0220,  0.0130, -0.0080, -0.0122, -0.0306,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[6.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921178946180526, distance: 0.9628023912165897 entropy -11.06046772259391
epoch: 19, step: 121
	action: tensor([[-0.0019,  0.0220,  0.0158, -0.0081, -0.0061,  0.0003,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.6018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29918519161560253, distance: 0.9579841601575102 entropy -11.061000520869268
epoch: 19, step: 122
	action: tensor([[-0.0020,  0.0218,  0.0056, -0.0079, -0.0065,  0.0037, -0.0352]],
       dtype=torch.float64)
	q_value: tensor([[6.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998853534223117, distance: 0.9575054948055358 entropy -11.063653758967819
epoch: 19, step: 123
	action: tensor([[-0.0023,  0.0223,  0.0154, -0.0075,  0.0009,  0.0065,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[6.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007626742819097, distance: 0.956905376754554 entropy -11.072146612940797
epoch: 19, step: 124
	action: tensor([[-0.0020,  0.0218, -0.0011, -0.0079, -0.0029,  0.0103, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[6.5845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013734622592954, distance: 0.9564873541693808 entropy -11.063827966213603
epoch: 19, step: 125
	action: tensor([[-0.0022,  0.0222,  0.0053, -0.0076, -0.0127, -0.0009,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[6.5609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990052680836526, distance: 0.9581071261893735 entropy -11.066718714577847
epoch: 19, step: 126
	action: tensor([[-0.0021,  0.0220,  0.0178, -0.0079, -0.0029, -0.0111,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[6.5795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2964430607930605, distance: 0.9598565187619544 entropy -11.060168084643566
epoch: 19, step: 127
	action: tensor([[-0.0019,  0.0217,  0.0088, -0.0082, -0.0153,  0.0030, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[6.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996094069593893, distance: 0.9576941740637755 entropy -11.057978746734682
LOSS epoch 19 actor 19.815583359003256 critic 12.494720719831
epoch: 20, step: 0
	action: tensor([[-0.0015,  0.0212, -0.0033, -0.0081,  0.0019, -0.0031, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[6.4631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983838855890125, distance: 0.9585316793945239 entropy -11.064695880541134
epoch: 20, step: 1
	action: tensor([[-0.0015,  0.0214,  0.0082, -0.0081, -0.0126,  0.0030,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[6.4534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29983090204208684, distance: 0.9575427290516065 entropy -11.063345400474917
epoch: 20, step: 2
	action: tensor([[-0.0013,  0.0209,  0.0098, -0.0084, -0.0076,  0.0093,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[6.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008810384709185, distance: 0.9568243827069596 entropy -11.053679362741862
epoch: 20, step: 3
	action: tensor([[-0.0013,  0.0208,  0.0091, -0.0084, -0.0059, -0.0071, -0.0116]],
       dtype=torch.float64)
	q_value: tensor([[6.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29705848137769675, distance: 0.9594366204834306 entropy -11.054136338664533
epoch: 20, step: 4
	action: tensor([[-0.0014,  0.0212,  0.0074, -0.0082, -0.0158, -0.0037,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[6.4691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298186365295127, distance: 0.9586665937243851 entropy -11.06106132175202
epoch: 20, step: 5
	action: tensor([[-0.0013,  0.0210,  0.0032, -0.0083, -0.0086, -0.0026,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[6.4761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29834607202010144, distance: 0.9585575090573685 entropy -11.057146111673124
epoch: 20, step: 6
	action: tensor([[-0.0013,  0.0210,  0.0285, -0.0084, -0.0137,  0.0129,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[6.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017864099711355, distance: 0.9562046296901276 entropy -11.054119034407362
epoch: 20, step: 7
	action: tensor([[-0.0013,  0.0206,  0.0014, -0.0084, -0.0079,  0.0061,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[6.4927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29996165016511944, distance: 0.9574533201072452 entropy -11.059812981041036
epoch: 20, step: 8
	action: tensor([[-0.0013,  0.0208,  0.0156, -0.0086, -0.0142, -0.0135, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[6.4790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955067580305566, distance: 0.960495001202396 entropy -11.048517414904733
epoch: 20, step: 9
	action: tensor([[-0.0013,  0.0211,  0.0094, -0.0083, -0.0041,  0.0051, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[6.4762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30019030136609504, distance: 0.9572969424380766 entropy -11.062240320426882
epoch: 20, step: 10
	action: tensor([[-0.0014,  0.0211,  0.0051, -0.0082, -0.0187,  0.0048, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[6.4667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300034359240972, distance: 0.9574035961500993 entropy -11.060325250233614
epoch: 20, step: 11
	action: tensor([[-0.0015,  0.0213,  0.0229, -0.0080, -0.0096,  0.0048,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[6.4551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30010370925613494, distance: 0.9573561669659424 entropy -11.063581657434383
epoch: 20, step: 12
	action: tensor([[-0.0012,  0.0206,  0.0023, -0.0085, -0.0021,  0.0443,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.4920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30888244351075, distance: 0.9513332062091558 entropy -11.055128019018772
epoch: 20, step: 13
	action: tensor([[-0.0016,  0.0210,  0.0133, -0.0079, -0.0051,  0.0117, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[6.4447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014669644983272, distance: 0.9564233452196923 entropy -11.062082397823332
epoch: 20, step: 14
	action: tensor([[-0.0014,  0.0210, -0.0050, -0.0082, -0.0054, -0.0059,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[6.4675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975717656036012, distance: 0.959086268710602 entropy -11.063394431545094
epoch: 20, step: 15
	action: tensor([[-0.0014,  0.0212,  0.0017, -0.0083, -0.0081,  0.0025,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[6.4652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29959821939592013, distance: 0.9577018228112585 entropy -11.055717322981923
epoch: 20, step: 16
	action: tensor([[-0.0013,  0.0208,  0.0075, -0.0086, -0.0058, -0.0071,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[6.4809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2970481301343406, distance: 0.9594436846026492 entropy -11.049066865831751
epoch: 20, step: 17
	action: tensor([[-0.0013,  0.0210,  0.0082, -0.0084,  0.0127, -0.0116,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[6.4764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962041623919689, distance: 0.9600194684198128 entropy -11.055632908358112
epoch: 20, step: 18
	action: tensor([[-0.0012,  0.0209,  0.0067, -0.0086, -0.0093,  0.0060,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[6.4833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001981445122044, distance: 0.9572915779505298 entropy -11.054512843817852
epoch: 20, step: 19
	action: tensor([[-0.0013,  0.0206,  0.0148, -0.0087,  0.0061,  0.0087, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[6.4856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30048499075974944, distance: 0.9570953626651019 entropy -11.045793040250825
epoch: 20, step: 20
	action: tensor([[-0.0015,  0.0210,  0.0095, -0.0081,  0.0004, -0.0182, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[6.4649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946840964147307, distance: 0.9610556395322538 entropy -11.066972293376637
epoch: 20, step: 21
	action: tensor([[-0.0013,  0.0211,  0.0091, -0.0083, -0.0079, -0.0093,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[6.4789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969011237658473, distance: 0.9595440022521299 entropy -11.060846759029323
epoch: 20, step: 22
	action: tensor([[-0.0012,  0.0209,  0.0039, -0.0085, -0.0120, -0.0150,  0.0479]],
       dtype=torch.float64)
	q_value: tensor([[6.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954894801656226, distance: 0.9605067793147768 entropy -11.054290602464699
epoch: 20, step: 23
	action: tensor([[-0.0012,  0.0207,  0.0059, -0.0089, -0.0173,  0.0024, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[6.4960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29910420334329735, distance: 0.9580395123273882 entropy -11.040924014889958
epoch: 20, step: 24
	action: tensor([[-0.0014,  0.0212,  0.0091, -0.0082,  0.0175,  0.0124, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[6.4636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30193600569790713, distance: 0.9561021884090044 entropy -11.059486073429952
epoch: 20, step: 25
	action: tensor([[-0.0014,  0.0209,  0.0033, -0.0083, -0.0071, -0.0105,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[6.4695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296415723400552, distance: 0.9598751666623965 entropy -11.060596104904564
epoch: 20, step: 26
	action: tensor([[-0.0013,  0.0211,  0.0148, -0.0084, -0.0045,  0.0139, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[6.4746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30216084915270347, distance: 0.9559481977761812 entropy -11.05426652839243
epoch: 20, step: 27
	action: tensor([[-0.0015,  0.0211,  0.0113, -0.0080, -0.0033,  0.0035, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[6.4585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997458616893407, distance: 0.957600877361268 entropy -11.06740754330813
epoch: 20, step: 28
	action: tensor([[-0.0013,  0.0210,  0.0182, -0.0083, -0.0059, -0.0053,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[6.4742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975742996118558, distance: 0.9590845387582776 entropy -11.059818082336205
epoch: 20, step: 29
	action: tensor([[-0.0012,  0.0208,  0.0142, -0.0084,  0.0026,  0.0119,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[6.4855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015800193968412, distance: 0.9563459453587492 entropy -11.057628713778486
epoch: 20, step: 30
	action: tensor([[-0.0013,  0.0208,  0.0181, -0.0083, -0.0085,  0.0173, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[6.4770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30268987971543615, distance: 0.9555857778000331 entropy -11.058813227527251
epoch: 20, step: 31
	action: tensor([[-0.0015,  0.0210,  0.0129, -0.0080, -0.0113,  0.0047, -0.0343]],
       dtype=torch.float64)
	q_value: tensor([[6.4643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29991067712145225, distance: 0.9574881777843438 entropy -11.067772393214081
epoch: 20, step: 32
	action: tensor([[-0.0016,  0.0213,  0.0038, -0.0079, -0.0165,  0.0005,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[6.4554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991625917960432, distance: 0.9579996065310457 entropy -11.068484528351865
epoch: 20, step: 33
	action: tensor([[-0.0013,  0.0209,  0.0151, -0.0085, -0.0012,  0.0084,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[6.4787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30063996883285227, distance: 0.9569893341956472 entropy -11.04947260259
epoch: 20, step: 34
	action: tensor([[-0.0013,  0.0208,  0.0094, -0.0084, -0.0104,  0.0079,  0.0001]],
       dtype=torch.float64)
	q_value: tensor([[6.4802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005091218552267, distance: 0.9570788541138494 entropy -11.05592995865066
epoch: 20, step: 35
	action: tensor([[-0.0014,  0.0211,  0.0085, -0.0082, -0.0092,  0.0025, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[6.4665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29948477616619085, distance: 0.9577793785742476 entropy -11.059370379815599
epoch: 20, step: 36
	action: tensor([[-0.0014,  0.0211,  0.0029, -0.0082, -0.0111, -0.0046, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[6.4705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979370978844963, distance: 0.9588368263280448 entropy -11.059950016162711
epoch: 20, step: 37
	action: tensor([[-0.0014,  0.0212,  0.0182, -0.0082, -0.0103,  0.0262,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[6.4680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050556175973449, distance: 0.95396341058754 entropy -11.05893042046037
epoch: 20, step: 38
	action: tensor([[-1.3828e-03,  2.0449e-02, -8.0882e-05, -8.4370e-03,  3.8181e-03,
         -7.8939e-03, -6.1238e-03]], dtype=torch.float64)
	q_value: tensor([[6.4855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29661498947772236, distance: 0.9597392312030507 entropy -11.054831728012758
epoch: 20, step: 39
	action: tensor([[-0.0013,  0.0212,  0.0064, -0.0083, -0.0078,  0.0044,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[6.4671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000376514758767, distance: 0.9574013446101237 entropy -11.058459510457151
epoch: 20, step: 40
	action: tensor([[-0.0013,  0.0209,  0.0129, -0.0084, -0.0049, -0.0111,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[6.4750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29624540417095835, distance: 0.9599913398854636 entropy -11.055793366075047
epoch: 20, step: 41
	action: tensor([[-0.0012,  0.0208,  0.0053, -0.0086, -0.0014,  0.0031, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.4893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29951237287035815, distance: 0.9577605125927594 entropy -11.052735147167718
epoch: 20, step: 42
	action: tensor([[-0.0015,  0.0212,  0.0061, -0.0081, -0.0046, -0.0013,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[6.4583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987404266200886, distance: 0.9582880993976285 entropy -11.062554393585929
epoch: 20, step: 43
	action: tensor([[-0.0013,  0.0210,  0.0036, -0.0084, -0.0042, -0.0075,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[6.4744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971666333660623, distance: 0.959362809955102 entropy -11.05484355530267
epoch: 20, step: 44
	action: tensor([[-0.0012,  0.0209,  0.0134, -0.0085, -0.0138,  0.0130,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[6.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301758515623315, distance: 0.9562237301763642 entropy -11.051898090055285
epoch: 20, step: 45
	action: tensor([[-0.0014,  0.0209,  0.0051, -0.0083, -0.0122,  0.0069,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[6.4747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30036883803572034, distance: 0.9571748210166742 entropy -11.059053646338695
epoch: 20, step: 46
	action: tensor([[-0.0014,  0.0211,  0.0025, -0.0083, -0.0008, -0.0040, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[6.4661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298025439642321, distance: 0.9587764984035305 entropy -11.056734717262229
epoch: 20, step: 47
	action: tensor([[-0.0014,  0.0211,  0.0241, -0.0083,  0.0046, -0.0108,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[6.4688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29632818410284634, distance: 0.9599348781904237 entropy -11.058737365024156
epoch: 20, step: 48
	action: tensor([[-0.0011,  0.0203,  0.0049, -0.0089, -0.0069,  0.0069,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[6.5139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999896711099963, distance: 0.9574341575747995 entropy -11.048941189105523
epoch: 20, step: 49
	action: tensor([[-0.0014,  0.0210,  0.0172, -0.0083, -0.0094, -0.0200, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[6.4683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29424771533931315, distance: 0.9613528975711845 entropy -11.057747015927836
epoch: 20, step: 50
	action: tensor([[-0.0012,  0.0210,  0.0145, -0.0084, -0.0181,  0.0026, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[6.4905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29948513126973264, distance: 0.9577791358165736 entropy -11.05933729356389
epoch: 20, step: 51
	action: tensor([[-0.0014,  0.0211,  0.0060, -0.0082,  0.0016,  0.0020,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[6.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994030906481966, distance: 0.9578352192051021 entropy -11.062344463061931
epoch: 20, step: 52
	action: tensor([[-0.0013,  0.0208,  0.0069, -0.0086, -0.0151, -0.0165,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[6.4826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949480659689533, distance: 0.9608757817067196 entropy -11.05155372024844
epoch: 20, step: 53
	action: tensor([[-0.0012,  0.0208,  0.0075, -0.0086, -0.0004, -0.0004, -0.0327]],
       dtype=torch.float64)
	q_value: tensor([[6.4925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2987004975045088, distance: 0.9583153810584241 entropy -11.048906564220289
epoch: 20, step: 54
	action: tensor([[-0.0015,  0.0213,  0.0139, -0.0080, -0.0004, -0.0071, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[6.4588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29738121079636226, distance: 0.9592163504023006 entropy -11.06653718381372
epoch: 20, step: 55
	action: tensor([[-0.0014,  0.0211,  0.0178, -0.0082, -0.0055,  0.0015,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[6.4733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29924157978079335, distance: 0.9579456192654843 entropy -11.06416025898719
epoch: 20, step: 56
	action: tensor([[-0.0012,  0.0207,  0.0143, -0.0085,  0.0013, -0.0203,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[6.4904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29394321641177845, distance: 0.9615602644933727 entropy -11.055503159846555
epoch: 20, step: 57
	action: tensor([[-0.0011,  0.0207,  0.0111, -0.0087, -0.0100, -0.0058, -0.0150]],
       dtype=torch.float64)
	q_value: tensor([[6.5006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29737008579737745, distance: 0.9592239443058849 entropy -11.051164254350784
epoch: 20, step: 58
	action: tensor([[-0.0014,  0.0211,  0.0121, -0.0082, -0.0060, -0.0180, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[6.4714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2948401800650664, distance: 0.9609492947217697 entropy -11.062660973981101
epoch: 20, step: 59
	action: tensor([[-0.0012,  0.0210,  0.0039, -0.0084, -0.0091,  0.0204,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[6.4830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30369505584858913, distance: 0.9548967885026225 entropy -11.057787488059708
epoch: 20, step: 60
	action: tensor([[-0.0015,  0.0211,  0.0137, -0.0081, -0.0081, -0.0059,  0.0417]],
       dtype=torch.float64)
	q_value: tensor([[6.4596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974642982346246, distance: 0.9591596331704009 entropy -11.060231497222986
epoch: 20, step: 61
	action: tensor([[-0.0012,  0.0206,  0.0128, -0.0088, -0.0080,  0.0369,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[6.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070511512273246, distance: 0.9525927723923799 entropy -11.045866956831548
epoch: 20, step: 62
	action: tensor([[-0.0016,  0.0209, -0.0106, -0.0080, -0.0083, -0.0023,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[6.4578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29828796383569, distance: 0.9585972001936381 entropy -11.065068513680256
epoch: 20, step: 63
	action: tensor([[-0.0014,  0.0211, -0.0104, -0.0085, -0.0072, -0.0073,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[6.4664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29729022022209983, distance: 0.9592784586339375 entropy -11.046992421182443
epoch: 20, step: 64
	action: tensor([[-0.0014,  0.0212,  0.0143, -0.0084, -0.0049,  0.0037, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[6.4652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998324395421429, distance: 0.9575416777178558 entropy -11.052589975487605
epoch: 20, step: 65
	action: tensor([[-0.0014,  0.0210,  0.0067, -0.0083, -0.0065, -0.0056, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[6.4735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29760922912764, distance: 0.9590606922681828 entropy -11.060247751637306
epoch: 20, step: 66
	action: tensor([[-0.0014,  0.0211,  0.0086, -0.0083, -0.0094,  0.0149, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[6.4691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302406593246487, distance: 0.9557798643510825 entropy -11.059087449663517
epoch: 20, step: 67
	action: tensor([[-0.0015,  0.0211,  0.0026, -0.0081,  0.0022, -0.0102,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[6.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29656455138304694, distance: 0.9597736409144594 entropy -11.062968202668529
epoch: 20, step: 68
	action: tensor([[-0.0012,  0.0207,  0.0133, -0.0088, -0.0073,  0.0210,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[6.4923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30342991413508413, distance: 0.9550785758613354 entropy -11.04504238628483
epoch: 20, step: 69
	action: tensor([[-0.0014,  0.0207, -0.0008, -0.0084, -0.0158,  0.0200, -0.0202]],
       dtype=torch.float64)
	q_value: tensor([[6.4769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303175261909547, distance: 0.9552531388130395 entropy -11.056492791342626
epoch: 20, step: 70
	action: tensor([[-0.0016,  0.0213,  0.0122, -0.0079, -0.0016, -0.0009,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[6.4449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988148518466698, distance: 0.958237246115856 entropy -11.065264873143848
epoch: 20, step: 71
	action: tensor([[-0.0012,  0.0207,  0.0198, -0.0085, -0.0069,  0.0064,  0.0626]],
       dtype=torch.float64)
	q_value: tensor([[6.4856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30012395864710073, distance: 0.9573423177572847 entropy -11.052519600836364
epoch: 20, step: 72
	action: tensor([[-0.0013,  0.0202,  0.0047, -0.0089, -0.0099, -0.0043,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[6.5059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29713974034207125, distance: 0.9593811641778034 entropy -11.043146764266236
epoch: 20, step: 73
	action: tensor([[-0.0013,  0.0209,  0.0035, -0.0086, -0.0122,  0.0016, -0.0361]],
       dtype=torch.float64)
	q_value: tensor([[6.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991987063909899, distance: 0.9579749230502078 entropy -11.050596603580194
epoch: 20, step: 74
	action: tensor([[-0.0016,  0.0214, -0.0049, -0.0079, -0.0129, -0.0111,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[6.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967505736216902, distance: 0.9596467273172126 entropy -11.066947294546038
epoch: 20, step: 75
	action: tensor([[-0.0013,  0.0212,  0.0116, -0.0084,  0.0006, -0.0226,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[6.4710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937604224791762, distance: 0.9616847275758429 entropy -11.053447884478683
epoch: 20, step: 76
	action: tensor([[-0.0012,  0.0209,  0.0101, -0.0085, -0.0039, -0.0296, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[6.4911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29215475188724316, distance: 0.9627773257935707 entropy -11.056488378355422
epoch: 20, step: 77
	action: tensor([[-0.0012,  0.0211,  0.0059, -0.0084,  0.0004, -0.0140,  0.0394]],
       dtype=torch.float64)
	q_value: tensor([[6.4862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29585926654955386, distance: 0.9602546688506388 entropy -11.059447558226411
epoch: 20, step: 78
	action: tensor([[-0.0012,  0.0206,  0.0169, -0.0088, -0.0135, -0.0034,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[6.4983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977411004508189, distance: 0.9589706579564479 entropy -11.044752236367655
epoch: 20, step: 79
	action: tensor([[-0.0012,  0.0207, -0.0024, -0.0086,  0.0121, -0.0051,  0.0537]],
       dtype=torch.float64)
	q_value: tensor([[6.4915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975471315521083, distance: 0.9591030860680447 entropy -11.051893515823858
epoch: 20, step: 80
	action: tensor([[-0.0012,  0.0205,  0.0152, -0.0089, -0.0033,  0.0062,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[6.4905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29980122333377335, distance: 0.9575630229565295 entropy -11.038231768390734
epoch: 20, step: 81
	action: tensor([[-0.0013,  0.0208,  0.0041, -0.0084, -0.0043,  0.0164, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[6.4780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025686433466167, distance: 0.9556688446531326 entropy -11.057285223196478
epoch: 20, step: 82
	action: tensor([[-0.0015,  0.0211,  0.0117, -0.0081, -0.0130,  0.0088,  0.0172]],
       dtype=torch.float64)
	q_value: tensor([[6.4558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30094656288317323, distance: 0.9567795428243031 entropy -11.060789643161417
epoch: 20, step: 83
	action: tensor([[-0.0013,  0.0208,  0.0197, -0.0084, -0.0128,  0.0131,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[6.4796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30171725053257714, distance: 0.9562519854984152 entropy -11.055745604926944
epoch: 20, step: 84
	action: tensor([[-0.0013,  0.0206,  0.0011, -0.0084, -0.0029, -0.0106, -0.0105]],
       dtype=torch.float64)
	q_value: tensor([[6.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29623584902296674, distance: 0.9599978569499644 entropy -11.056369391870144
epoch: 20, step: 85
	action: tensor([[-0.0014,  0.0213,  0.0117, -0.0083, -0.0034,  0.0002, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[6.4642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29913880542722904, distance: 0.9580158636103278 entropy -11.059063792689818
epoch: 20, step: 86
	action: tensor([[-0.0013,  0.0210,  0.0151, -0.0083, -0.0057,  0.0198,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[6.4741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034407061796667, distance: 0.9550711772587376 entropy -11.0589378340528
epoch: 20, step: 87
	action: tensor([[-0.0014,  0.0207,  0.0107, -0.0083, -0.0065,  0.0069,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[6.4775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30019347287743925, distance: 0.9572947732186742 entropy -11.057868183079645
epoch: 20, step: 88
	action: tensor([[-1.3302e-03,  2.0875e-02, -3.1569e-03, -8.3532e-03,  2.2438e-05,
          7.4292e-03,  3.2495e-02]], dtype=torch.float64)
	q_value: tensor([[6.4765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005257140012826, distance: 0.9570675029391584 entropy -11.056967438430503
epoch: 20, step: 89
	action: tensor([[-0.0013,  0.0208,  0.0156, -0.0086, -0.0097,  0.0231, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[6.4749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039459214418615, distance: 0.9547247573173852 entropy -11.048078767148555
epoch: 20, step: 90
	action: tensor([[-0.0015,  0.0209,  0.0006, -0.0081, -0.0122,  0.0072, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[6.4639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004747289139508, distance: 0.9571023829068953 entropy -11.063474769984584
epoch: 20, step: 91
	action: tensor([[-0.0016,  0.0214,  0.0080, -0.0080, -0.0031, -0.0026,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[6.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984957069580887, distance: 0.9584552924690339 entropy -11.063826015630074
epoch: 20, step: 92
	action: tensor([[-0.0013,  0.0209,  0.0068, -0.0085, -0.0041,  0.0044,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[6.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997976064472422, distance: 0.9575654961059004 entropy -11.05291344982477
epoch: 20, step: 93
	action: tensor([[-0.0013,  0.0209,  0.0202, -0.0084, -0.0059, -0.0113,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[6.4758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2961278217599097, distance: 0.960071533593974 entropy -11.055588868568831
epoch: 20, step: 94
	action: tensor([[-0.0012,  0.0207, -0.0011, -0.0086, -0.0143, -0.0134,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[6.4946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29579300422860744, distance: 0.9602998495958932 entropy -11.054834924882202
epoch: 20, step: 95
	action: tensor([[-0.0013,  0.0210, -0.0096, -0.0085, -0.0042,  0.0028,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[6.4793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29962056629578093, distance: 0.9576865445538099 entropy -11.051132771991808
epoch: 20, step: 96
	action: tensor([[-0.0014,  0.0211,  0.0098, -0.0084, -0.0010, -0.0076,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[6.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29707764237475265, distance: 0.9594235440844325 entropy -11.051072605785718
epoch: 20, step: 97
	action: tensor([[-0.0012,  0.0208,  0.0128, -0.0085, -0.0025, -0.0036,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[6.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29794232088502004, distance: 0.9588332596856931 entropy -11.054308878055911
epoch: 20, step: 98
	action: tensor([[-0.0013,  0.0209,  0.0111, -0.0084, -0.0035, -0.0045,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[6.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2978111662623437, distance: 0.958922817527902 entropy -11.058372698072107
epoch: 20, step: 99
	action: tensor([[-0.0013,  0.0209,  0.0083, -0.0084, -0.0099,  0.0145, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[6.4819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30219100236364427, distance: 0.955927544579091 entropy -11.056874889991166
epoch: 20, step: 100
	action: tensor([[-0.0015,  0.0211,  0.0114, -0.0081, -0.0084,  0.0138, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[6.4635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020588726232015, distance: 0.9560180424662829 entropy -11.06186251351231
epoch: 20, step: 101
	action: tensor([[-0.0015,  0.0210,  0.0047, -0.0081, -0.0172,  0.0128,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[6.4643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017747372796996, distance: 0.9562126225415291 entropy -11.063691556083915
epoch: 20, step: 102
	action: tensor([[-0.0014,  0.0209,  0.0099, -0.0084, -0.0077,  0.0133, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[6.4738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017779957269112, distance: 0.9562103913332392 entropy -11.053203294742845
epoch: 20, step: 103
	action: tensor([[-0.0014,  0.0210, -0.0047, -0.0082, -0.0096, -0.0046, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[6.4647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29791688685351914, distance: 0.958850627756144 entropy -11.060083407164598
epoch: 20, step: 104
	action: tensor([[-0.0014,  0.0214,  0.0106, -0.0082, -0.0030,  0.0077, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[6.4562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30089159257081644, distance: 0.9568171604323625 entropy -11.05865827538806
epoch: 20, step: 105
	action: tensor([[-0.0014,  0.0210,  0.0047, -0.0082, -0.0078,  0.0033,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[6.4675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996503832494166, distance: 0.9576661587473121 entropy -11.062771151710402
epoch: 20, step: 106
	action: tensor([[-0.0013,  0.0209,  0.0096, -0.0085, -0.0069,  0.0126,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[6.4786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016341332881505, distance: 0.9563088955860826 entropy -11.052409221419312
epoch: 20, step: 107
	action: tensor([[-0.0014,  0.0209,  0.0112, -0.0084, -0.0191,  0.0108, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[6.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30117836668401665, distance: 0.9566208972103402 entropy -11.055559108374057
epoch: 20, step: 108
	action: tensor([[-0.0014,  0.0211,  0.0125, -0.0081, -0.0066, -0.0040,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[6.4642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979643984825787, distance: 0.9588181833594248 entropy -11.061371198186558
epoch: 20, step: 109
	action: tensor([[-0.0012,  0.0207,  0.0125, -0.0086, -0.0080,  0.0087, -0.0231]],
       dtype=torch.float64)
	q_value: tensor([[6.4925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006651371302972, distance: 0.9569721141602987 entropy -11.05096969831283
epoch: 20, step: 110
	action: tensor([[-0.0015,  0.0211,  0.0066, -0.0080,  0.0004,  0.0193, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[6.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30344484873252353, distance: 0.9550683372717103 entropy -11.06676476512861
epoch: 20, step: 111
	action: tensor([[-0.0015,  0.0210,  0.0134, -0.0081, -0.0033,  0.0004,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[6.4580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989444816437332, distance: 0.9581486662010907 entropy -11.060950111687683
epoch: 20, step: 112
	action: tensor([[-0.0013,  0.0208,  0.0014, -0.0084,  0.0084,  0.0214,  0.0025]],
       dtype=torch.float64)
	q_value: tensor([[6.4841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037887346516891, distance: 0.9548325518503555 entropy -11.056164047131032
epoch: 20, step: 113
	action: tensor([[-0.0015,  0.0210,  0.0046, -0.0082, -0.0063,  0.0115,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[6.4555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30150401086628253, distance: 0.9563979831543641 entropy -11.058518287390232
epoch: 20, step: 114
	action: tensor([[-0.0014,  0.0208,  0.0053, -0.0085, -0.0131,  0.0205,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[6.4727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033712745538474, distance: 0.9551187758571739 entropy -11.05184972239954
epoch: 20, step: 115
	action: tensor([[-0.0014,  0.0209,  0.0103, -0.0082, -0.0029,  0.0180,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[6.4659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029296141806226, distance: 0.9554214990006483 entropy -11.057161420215028
epoch: 20, step: 116
	action: tensor([[-0.0014,  0.0208,  0.0023, -0.0084, -0.0051, -0.0101,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[6.4714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296390463304005, distance: 0.959892397236308 entropy -11.05552217207004
epoch: 20, step: 117
	action: tensor([[-0.0012,  0.0207,  0.0027, -0.0088, -0.0106, -0.0047,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[6.4911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29757059975472444, distance: 0.9590870646276298 entropy -11.044759613685097
epoch: 20, step: 118
	action: tensor([[-0.0013,  0.0208,  0.0062, -0.0086, -0.0111, -0.0262,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[6.4854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2927626858120379, distance: 0.9623637956282959 entropy -11.047961334755408
epoch: 20, step: 119
	action: tensor([[-0.0011,  0.0208,  0.0025, -0.0088, -0.0124, -0.0011,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[6.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985078969920082, distance: 0.9584469648982704 entropy -11.045402119129765
epoch: 20, step: 120
	action: tensor([[-0.0013,  0.0210,  0.0082, -0.0084,  0.0010,  0.0068,  0.0202]],
       dtype=torch.float64)
	q_value: tensor([[6.4762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30039751591417974, distance: 0.957155203519648 entropy -11.053161324080516
epoch: 20, step: 121
	action: tensor([[-0.0013,  0.0208,  0.0224, -0.0085, -0.0116, -0.0203,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[6.4809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938978145257609, distance: 0.9615911798168613 entropy -11.053368767488148
epoch: 20, step: 122
	action: tensor([[-0.0011,  0.0207,  0.0065, -0.0086, -0.0014,  0.0055,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[6.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30000234909780155, distance: 0.9574254874216366 entropy -11.054547993746032
epoch: 20, step: 123
	action: tensor([[-0.0013,  0.0208,  0.0073, -0.0085,  0.0002, -0.0121, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[6.4810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295902508374294, distance: 0.9602251834085704 entropy -11.051874891433728
epoch: 20, step: 124
	action: tensor([[-0.0014,  0.0212, -0.0017, -0.0082, -0.0099, -0.0058, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[6.4693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2978164921413873, distance: 0.9589191809586652 entropy -11.063056965215711
epoch: 20, step: 125
	action: tensor([[-0.0014,  0.0213,  0.0043, -0.0082, -0.0108, -0.0011,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[6.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29885539286668117, distance: 0.9582095441051854 entropy -11.057668353212462
epoch: 20, step: 126
	action: tensor([[-0.0013,  0.0211,  0.0121, -0.0084, -0.0033, -0.0009, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[6.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986745327342092, distance: 0.9583331211312095 entropy -11.055080992838953
epoch: 20, step: 127
	action: tensor([[-0.0014,  0.0211,  0.0007, -0.0082, -0.0022,  0.0067,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[6.4722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30050498032421624, distance: 0.9570816874359201 entropy -11.062392272817158
LOSS epoch 20 actor 19.13490022705041 critic 12.62204461104964
epoch: 21, step: 0
	action: tensor([[ 0.0005,  0.0201,  0.0074, -0.0083, -0.0064,  0.0148,  0.0325]],
       dtype=torch.float64)
	q_value: tensor([[6.7693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30355506740553073, distance: 0.9549927721645673 entropy -11.054947712672913
epoch: 21, step: 1
	action: tensor([[ 0.0006,  0.0197,  0.0066, -0.0085, -0.0087, -0.0144, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[6.7887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29651118416849886, distance: 0.9598100475772672 entropy -11.049283345746721
epoch: 21, step: 2
	action: tensor([[ 0.0006,  0.0201,  0.0174, -0.0084, -0.0079, -0.0046,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[6.7793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29915060710150443, distance: 0.9580077976488672 entropy -11.055818728267523
epoch: 21, step: 3
	action: tensor([[ 0.0007,  0.0197,  0.0011, -0.0086,  0.0018,  0.0254,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[6.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058351149243038, distance: 0.9534282450505707 entropy -11.051152398157907
epoch: 21, step: 4
	action: tensor([[ 0.0004,  0.0200,  0.0114, -0.0082,  0.0011,  0.0106, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[6.7633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30249999064165034, distance: 0.9557158798459906 entropy -11.055081549909492
epoch: 21, step: 5
	action: tensor([[ 0.0004,  0.0202,  0.0064, -0.0080, -0.0083, -0.0163,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.7630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29650616651499595, distance: 0.9598134705070646 entropy -11.063975202116833
epoch: 21, step: 6
	action: tensor([[ 0.0007,  0.0199,  0.0009, -0.0086, -0.0144,  0.0019, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[6.7906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005882913359126, distance: 0.9570246907342206 entropy -11.049917204836621
epoch: 21, step: 7
	action: tensor([[ 0.0004,  0.0203,  0.0100, -0.0081, -0.0127,  0.0142,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[6.7634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30356578341751184, distance: 0.9549854250270726 entropy -11.060211445825031
epoch: 21, step: 8
	action: tensor([[ 0.0005,  0.0198,  0.0123, -0.0084,  0.0035,  0.0027, -0.0234]],
       dtype=torch.float64)
	q_value: tensor([[6.7852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30054469128283223, distance: 0.9570545198581349 entropy -11.053289824984377
epoch: 21, step: 9
	action: tensor([[ 0.0005,  0.0201,  0.0142, -0.0081, -0.0127, -0.0035, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[6.7709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299361452075234, distance: 0.9578636822903344 entropy -11.063873007098028
epoch: 21, step: 10
	action: tensor([[ 0.0005,  0.0201,  0.0043, -0.0082, -0.0143, -0.0116,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[6.7773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976053064316365, distance: 0.9590633703346154 entropy -11.062412495110163
epoch: 21, step: 11
	action: tensor([[ 0.0007,  0.0200,  0.0152, -0.0085, -0.0142, -0.0283,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[6.7852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29365032999355223, distance: 0.961759680990854 entropy -11.050466896320268
epoch: 21, step: 12
	action: tensor([[ 0.0008,  0.0199, -0.0006, -0.0085, -0.0017, -0.0225,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[6.8025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295080431930691, distance: 0.9607855803960866 entropy -11.053985475211173
epoch: 21, step: 13
	action: tensor([[ 0.0008,  0.0199,  0.0086, -0.0087, -0.0105, -0.0152,  0.0254]],
       dtype=torch.float64)
	q_value: tensor([[6.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29663262679496605, distance: 0.9597271984392356 entropy -11.045679325692664
epoch: 21, step: 14
	action: tensor([[ 0.0007,  0.0198,  0.0083, -0.0086, -0.0129,  0.0061, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[6.8000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30140692924379064, distance: 0.9564644441243909 entropy -11.04823023482234
epoch: 21, step: 15
	action: tensor([[ 0.0005,  0.0202,  0.0181, -0.0081, -0.0103,  0.0112,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[6.7691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302785535415815, distance: 0.9555202328156914 entropy -11.061204864292492
epoch: 21, step: 16
	action: tensor([[ 0.0006,  0.0197,  0.0148, -0.0084, -0.0107,  0.0137, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[6.7909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030759944261262, distance: 0.9553211775845111 entropy -11.056240711433778
epoch: 21, step: 17
	action: tensor([[ 0.0005,  0.0200,  0.0119, -0.0082,  0.0029,  0.0149,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[6.7766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035390950385033, distance: 0.9550037230717363 entropy -11.060785898248623
epoch: 21, step: 18
	action: tensor([[ 0.0006,  0.0197,  0.0022, -0.0085, -0.0076,  0.0120,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[6.7837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30263045867610894, distance: 0.9556264918875105 entropy -11.053171186678593
epoch: 21, step: 19
	action: tensor([[ 0.0005,  0.0200,  0.0152, -0.0083, -0.0129, -0.0135,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.7749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29690517875644007, distance: 0.9595412352533165 entropy -11.053202597119935
epoch: 21, step: 20
	action: tensor([[ 0.0007,  0.0200,  0.0160, -0.0084, -0.0126,  0.0196,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[6.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046482476871918, distance: 0.9542429718478502 entropy -11.055335692679055
epoch: 21, step: 21
	action: tensor([[ 0.0005,  0.0199,  0.0136, -0.0082, -0.0049,  0.0223,  0.0378]],
       dtype=torch.float64)
	q_value: tensor([[6.7752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30518257605876287, distance: 0.9538762674564091 entropy -11.05968614633144
epoch: 21, step: 22
	action: tensor([[ 0.0005,  0.0195,  0.0100, -0.0085, -0.0139,  0.0011,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[6.7936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29987139560997156, distance: 0.9575150393961726 entropy -11.050329134106544
epoch: 21, step: 23
	action: tensor([[ 0.0006,  0.0198,  0.0048, -0.0085, -0.0062, -0.0150,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[6.7890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965508221761781, distance: 0.9597830069953096 entropy -11.050008094245014
epoch: 21, step: 24
	action: tensor([[ 0.0007,  0.0201,  0.0081, -0.0085, -0.0159,  0.0229,  0.0468]],
       dtype=torch.float64)
	q_value: tensor([[6.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30543741481179143, distance: 0.9537013244258074 entropy -11.053196418146424
epoch: 21, step: 25
	action: tensor([[ 4.6865e-04,  1.9545e-02,  1.0964e-02, -8.5600e-03,  1.1272e-03,
         -8.1255e-03,  4.4654e-05]], dtype=torch.float64)
	q_value: tensor([[6.7912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976614768432413, distance: 0.9590250214818855 entropy -11.046600962659818
epoch: 21, step: 26
	action: tensor([[ 0.0007,  0.0200,  0.0123, -0.0084, -0.0074,  0.0124, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[6.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30300982485400585, distance: 0.9553665280765855 entropy -11.056816840697946
epoch: 21, step: 27
	action: tensor([[ 0.0005,  0.0200,  0.0011, -0.0082,  0.0002,  0.0167,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[6.7753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30397549047398764, distance: 0.9547044782990523 entropy -11.060328964953374
epoch: 21, step: 28
	action: tensor([[ 0.0005,  0.0200,  0.0208, -0.0083, -0.0159, -0.0508, -0.0241]],
       dtype=torch.float64)
	q_value: tensor([[6.7723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28839211271803744, distance: 0.9653328154053834 entropy -11.05369345465715
epoch: 21, step: 29
	action: tensor([[ 0.0008,  0.0201,  0.0015, -0.0084, -0.0103,  0.0211,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[6.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051833948742506, distance: 0.9538757054030809 entropy -11.062218343483023
epoch: 21, step: 30
	action: tensor([[ 0.0005,  0.0199,  0.0008, -0.0084, -0.0070, -0.0025,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[6.7735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993536983434888, distance: 0.9578689824536746 entropy -11.0499708263186
epoch: 21, step: 31
	action: tensor([[ 0.0006,  0.0201, -0.0023, -0.0084, -0.0005,  0.0223, -0.0273]],
       dtype=torch.float64)
	q_value: tensor([[6.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053258246255186, distance: 0.953777933385517 entropy -11.052805275925405
epoch: 21, step: 32
	action: tensor([[ 0.0002,  0.0204,  0.0154, -0.0078, -0.0121, -0.0040,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[6.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993432785622895, distance: 0.9578761049833189 entropy -11.06443183506604
epoch: 21, step: 33
	action: tensor([[ 0.0007,  0.0195,  0.0117, -0.0088, -0.0085, -0.0093,  0.0358]],
       dtype=torch.float64)
	q_value: tensor([[6.8103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29755678619142256, distance: 0.9590964950022278 entropy -11.045109567716475
epoch: 21, step: 34
	action: tensor([[ 0.0007,  0.0197,  0.0165, -0.0087, -0.0121,  0.0119, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[6.8042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30257372837858354, distance: 0.9556653607149382 entropy -11.046673212817536
epoch: 21, step: 35
	action: tensor([[ 0.0005,  0.0200, -0.0063, -0.0082, -0.0048,  0.0113,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[6.7774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30270340664224793, distance: 0.9555765091825336 entropy -11.061810210236052
epoch: 21, step: 36
	action: tensor([[ 0.0005,  0.0200,  0.0065, -0.0084, -0.0007,  0.0003,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[6.7705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000896058217336, distance: 0.9573658126392084 entropy -11.049843842530835
epoch: 21, step: 37
	action: tensor([[ 0.0006,  0.0199,  0.0033, -0.0085, -0.0064, -0.0109,  0.0405]],
       dtype=torch.float64)
	q_value: tensor([[6.7839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297547590621562, distance: 0.9591027726697932 entropy -11.051363594731459
epoch: 21, step: 38
	action: tensor([[ 0.0007,  0.0197,  0.0138, -0.0088,  0.0016, -0.0107,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[6.8003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29730135786707845, distance: 0.959270856530231 entropy -11.043027810394506
epoch: 21, step: 39
	action: tensor([[ 0.0008,  0.0196, -0.0004, -0.0087, -0.0116, -0.0147,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[6.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29663062084141334, distance: 0.9597285669749994 entropy -11.049701211430945
epoch: 21, step: 40
	action: tensor([[ 0.0007,  0.0198,  0.0023, -0.0087, -0.0090,  0.0077,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[6.7957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30172011844957236, distance: 0.9562500217851923 entropy -11.043433050182443
epoch: 21, step: 41
	action: tensor([[ 0.0006,  0.0199,  0.0053, -0.0084, -0.0054,  0.0162, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[6.7792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30377831639306974, distance: 0.9548396959858672 entropy -11.051844952056674
epoch: 21, step: 42
	action: tensor([[ 0.0004,  0.0202,  0.0069, -0.0080, -0.0040, -0.0177, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[6.7582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29620743146009765, distance: 0.9600172388153424 entropy -11.061576187761592
epoch: 21, step: 43
	action: tensor([[ 0.0006,  0.0203,  0.0066, -0.0082, -0.0174,  0.0025,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[6.7730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009349532260166, distance: 0.956787487736493 entropy -11.06060795457886
epoch: 21, step: 44
	action: tensor([[ 0.0006,  0.0199,  0.0065, -0.0084, -0.0096,  0.0120,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[6.7835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027990980936216, distance: 0.9555109390640999 entropy -11.050998770428802
epoch: 21, step: 45
	action: tensor([[ 0.0005,  0.0199, -0.0108, -0.0083, -0.0127, -0.0120, -0.0224]],
       dtype=torch.float64)
	q_value: tensor([[6.7785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974834262639682, distance: 0.9591465755005579 entropy -11.054419089866881
epoch: 21, step: 46
	action: tensor([[ 0.0004,  0.0205,  0.0147, -0.0081, -0.0038, -0.0266,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[6.7553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943074528642773, distance: 0.961312210449608 entropy -11.058883007097162
epoch: 21, step: 47
	action: tensor([[ 0.0009,  0.0195, -0.0079, -0.0089, -0.0121, -0.0099, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[6.8215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976372500021176, distance: 0.9590415619005752 entropy -11.042606800877431
epoch: 21, step: 48
	action: tensor([[ 0.0005,  0.0204,  0.0092, -0.0082, -0.0087,  0.0186, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[6.7606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047107415023531, distance: 0.9542000902245197 entropy -11.055400946344303
epoch: 21, step: 49
	action: tensor([[ 0.0003,  0.0202, -0.0002, -0.0079, -0.0048,  0.0279, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[6.7577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066525548271585, distance: 0.9528667070873645 entropy -11.064368379327664
epoch: 21, step: 50
	action: tensor([[ 0.0004,  0.0201,  0.0170, -0.0080, -0.0021,  0.0076,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[6.7575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301857700536152, distance: 0.9561558121724982 entropy -11.059347620483363
epoch: 21, step: 51
	action: tensor([[ 0.0006,  0.0197,  0.0239, -0.0085, -0.0016,  0.0313,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[6.7934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072446919665279, distance: 0.9524597334351875 entropy -11.054850036998939
epoch: 21, step: 52
	action: tensor([[ 0.0005,  0.0195,  0.0022, -0.0083, -0.0145,  0.0079,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[6.7876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30146705787709116, distance: 0.9564232812931288 entropy -11.057903474140483
epoch: 21, step: 53
	action: tensor([[ 0.0005,  0.0198,  0.0052, -0.0086, -0.0162, -0.0266, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[6.7823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2939321566907195, distance: 0.9615677954362862 entropy -11.04644056963213
epoch: 21, step: 54
	action: tensor([[ 0.0007,  0.0203,  0.0051, -0.0084, -0.0162,  0.0018, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[6.7811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300808816479192, distance: 0.9568738034649904 entropy -11.055982660001193
epoch: 21, step: 55
	action: tensor([[ 0.0005,  0.0202,  0.0140, -0.0082, -0.0061,  0.0304,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[6.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30731599468769366, distance: 0.9524107156091209 entropy -11.057710360496603
epoch: 21, step: 56
	action: tensor([[ 0.0004,  0.0197, -0.0004, -0.0082, -0.0099,  0.0021, -0.0176]],
       dtype=torch.float64)
	q_value: tensor([[6.7771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003906477134459, distance: 0.9571599018434006 entropy -11.05888747191422
epoch: 21, step: 57
	action: tensor([[ 0.0004,  0.0204,  0.0002, -0.0081, -0.0155, -0.0033,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[6.7568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996324195381892, distance: 0.9576784405617115 entropy -11.06035232254624
epoch: 21, step: 58
	action: tensor([[ 0.0006,  0.0198,  0.0112, -0.0086, -0.0067,  0.0169,  0.0344]],
       dtype=torch.float64)
	q_value: tensor([[6.7919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037767084901044, distance: 0.9548407985719604 entropy -11.044453190705593
epoch: 21, step: 59
	action: tensor([[ 0.0006,  0.0196,  0.0143, -0.0085,  0.0005,  0.0050,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[6.7922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30087542777446064, distance: 0.9568282221396593 entropy -11.049877493897112
epoch: 21, step: 60
	action: tensor([[ 0.0006,  0.0199, -0.0045, -0.0084, -0.0086,  0.0152, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[6.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035330244168133, distance: 0.9550078851529391 entropy -11.056684508112529
epoch: 21, step: 61
	action: tensor([[ 0.0004,  0.0203,  0.0120, -0.0080, -0.0057, -0.0125, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[6.7547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973975058796642, distance: 0.9592052273002707 entropy -11.059194619625833
epoch: 21, step: 62
	action: tensor([[ 0.0006,  0.0201,  0.0108, -0.0082, -0.0112, -0.0082,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[6.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983672291602404, distance: 0.9585430571405307 entropy -11.06139618324977
epoch: 21, step: 63
	action: tensor([[ 0.0007,  0.0196,  0.0100, -0.0088, -0.0197, -0.0151,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[6.8092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29629922375720186, distance: 0.9599546315464709 entropy -11.042601009283745
epoch: 21, step: 64
	action: tensor([[ 0.0007,  0.0199,  0.0165, -0.0086,  0.0039, -0.0013, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[6.7933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997545678831147, distance: 0.957594924461989 entropy -11.050230603999902
epoch: 21, step: 65
	action: tensor([[ 0.0006,  0.0200,  0.0074, -0.0083, -0.0008, -0.0091, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[6.7816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980371194125201, distance: 0.9587685220918613 entropy -11.06043727961948
epoch: 21, step: 66
	action: tensor([[ 0.0006,  0.0200,  0.0156, -0.0084,  0.0012, -0.0085,  0.0541]],
       dtype=torch.float64)
	q_value: tensor([[6.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29814155913362106, distance: 0.9586971954985183 entropy -11.056563989141123
epoch: 21, step: 67
	action: tensor([[ 0.0008,  0.0193, -0.0005, -0.0090, -0.0117,  0.0230, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[6.8160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048186702998231, distance: 0.9541260277649634 entropy -11.04128724349683
epoch: 21, step: 68
	action: tensor([[ 0.0003,  0.0203,  0.0053, -0.0079, -0.0052, -0.0008,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[6.7494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001428058794543, distance: 0.9573294273499028 entropy -11.063263077134602
epoch: 21, step: 69
	action: tensor([[ 0.0006,  0.0200,  0.0074, -0.0084, -0.0016, -0.0039, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[6.7824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29922615118293727, distance: 0.9579561647515967 entropy -11.05382589390254
epoch: 21, step: 70
	action: tensor([[ 0.0005,  0.0202,  0.0239, -0.0082, -0.0039,  0.0165,  0.0513]],
       dtype=torch.float64)
	q_value: tensor([[6.7738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30404032967624683, distance: 0.9546600088029097 entropy -11.060310806098148
epoch: 21, step: 71
	action: tensor([[ 0.0006,  0.0192,  0.0014, -0.0087, -0.0002, -0.0136,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[6.8132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963141874171563, distance: 0.9599444251409033 entropy -11.047995816899732
epoch: 21, step: 72
	action: tensor([[ 0.0007,  0.0199,  0.0066, -0.0087, -0.0122, -0.0034,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[6.7890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992670956641601, distance: 0.9579281788391595 entropy -11.046421466292475
epoch: 21, step: 73
	action: tensor([[ 0.0006,  0.0200,  0.0047, -0.0084, -0.0105, -0.0013,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[6.7835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29980021690384184, distance: 0.9575637111323544 entropy -11.05183853815986
epoch: 21, step: 74
	action: tensor([[ 0.0006,  0.0198,  0.0123, -0.0086, -0.0032,  0.0128,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[6.7885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30288873571332053, distance: 0.955449513097017 entropy -11.047287800120316
epoch: 21, step: 75
	action: tensor([[ 0.0006,  0.0199,  0.0043, -0.0084, -0.0098, -0.0084,  0.0485]],
       dtype=torch.float64)
	q_value: tensor([[6.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298087043334479, distance: 0.9587344274571723 entropy -11.055421230344917
epoch: 21, step: 76
	action: tensor([[ 0.0007,  0.0196,  0.0141, -0.0088,  0.0055, -0.0098,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[6.8037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973673452049994, distance: 0.959225815019908 entropy -11.040723755617895
epoch: 21, step: 77
	action: tensor([[ 0.0008,  0.0197,  0.0159, -0.0086,  0.0030,  0.0065,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[6.8002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30146328569677683, distance: 0.9564258637026228 entropy -11.052478192974672
epoch: 21, step: 78
	action: tensor([[ 0.0006,  0.0199,  0.0025, -0.0083, -0.0010, -0.0252,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[6.7807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29432057666391687, distance: 0.9613032716223892 entropy -11.058746726471744
epoch: 21, step: 79
	action: tensor([[ 0.0008,  0.0198,  0.0024, -0.0088, -0.0123,  0.0126,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[6.8011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028930457399198, distance: 0.9554465594658021 entropy -11.044887062776871
epoch: 21, step: 80
	action: tensor([[ 0.0004,  0.0202,  0.0144, -0.0082, -0.0081,  0.0007,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[6.7639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003261599379746, distance: 0.9572040148118005 entropy -11.057059376081268
epoch: 21, step: 81
	action: tensor([[ 0.0006,  0.0199,  0.0069, -0.0084, -0.0100,  0.0119,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[6.7881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30275375821123707, distance: 0.955542007619207 entropy -11.054514931530601
epoch: 21, step: 82
	action: tensor([[ 0.0006,  0.0198,  0.0020, -0.0084, -0.0016,  0.0093, -0.0105]],
       dtype=torch.float64)
	q_value: tensor([[6.7840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021081634182766, distance: 0.9559842833714309 entropy -11.051647114373077
epoch: 21, step: 83
	action: tensor([[ 0.0004,  0.0202,  0.0157, -0.0081, -0.0056, -0.0112, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[6.7638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976052953369298, distance: 0.9590633779090794 entropy -11.059830778865571
epoch: 21, step: 84
	action: tensor([[ 0.0007,  0.0200,  0.0267, -0.0084, -0.0094, -0.0092, -0.0116]],
       dtype=torch.float64)
	q_value: tensor([[6.7909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29790413296919993, distance: 0.9588593368492839 entropy -11.058329487782382
epoch: 21, step: 85
	action: tensor([[ 0.0006,  0.0199,  0.0062, -0.0083, -0.0023, -0.0226, -0.0093]],
       dtype=torch.float64)
	q_value: tensor([[6.7949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949048473114859, distance: 0.9609052313989306 entropy -11.062943115004918
epoch: 21, step: 86
	action: tensor([[ 0.0007,  0.0202,  0.0138, -0.0084, -0.0043,  0.0159,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[6.7804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30402680154407, distance: 0.9546692871451575 entropy -11.057306434785616
epoch: 21, step: 87
	action: tensor([[ 0.0005,  0.0199,  0.0017, -0.0082, -0.0039, -0.0126, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[6.7755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971957657411253, distance: 0.9593429270012258 entropy -11.058696045917511
epoch: 21, step: 88
	action: tensor([[ 0.0005,  0.0204,  0.0211, -0.0081,  0.0001,  0.0065,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[6.7635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301860288700446, distance: 0.9561540398327989 entropy -11.060973832041498
epoch: 21, step: 89
	action: tensor([[ 0.0006,  0.0197, -0.0008, -0.0084, -0.0120, -0.0007,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[6.7929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299825233189265, distance: 0.9575466053707916 entropy -11.058459234807923
epoch: 21, step: 90
	action: tensor([[ 0.0006,  0.0199,  0.0091, -0.0086, -0.0082, -0.0036,  0.0332]],
       dtype=torch.float64)
	q_value: tensor([[6.7838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990715645396622, distance: 0.9580618187099089 entropy -11.045386992922149
epoch: 21, step: 91
	action: tensor([[ 0.0007,  0.0197, -0.0008, -0.0087, -0.0154,  0.0005, -0.0203]],
       dtype=torch.float64)
	q_value: tensor([[6.7995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000280411285341, distance: 0.9574079170549885 entropy -11.046864741033717
epoch: 21, step: 92
	action: tensor([[ 4.0370e-04,  2.0391e-02,  1.3836e-02, -8.0341e-03, -1.6996e-05,
          8.0984e-03,  7.6515e-03]], dtype=torch.float64)
	q_value: tensor([[6.7588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30223834808217476, distance: 0.9558951146130561 entropy -11.061631389946486
epoch: 21, step: 93
	action: tensor([[ 0.0006,  0.0198,  0.0046, -0.0084, -0.0102,  0.0014,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[6.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003266222887826, distance: 0.9572036985472088 entropy -11.055997519386894
epoch: 21, step: 94
	action: tensor([[ 0.0006,  0.0200,  0.0099, -0.0084, -0.0117, -0.0071,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[6.7763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985078132075284, distance: 0.9584470221355348 entropy -11.05338407278781
epoch: 21, step: 95
	action: tensor([[ 0.0007,  0.0197,  0.0056, -0.0086, -0.0058, -0.0220,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[6.7993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949324781038718, distance: 0.9608864035620305 entropy -11.048172354274438
epoch: 21, step: 96
	action: tensor([[ 0.0008,  0.0198,  0.0089, -0.0087, -0.0027, -0.0056, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[6.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29869903965876576, distance: 0.9583163771202119 entropy -11.045705567306527
epoch: 21, step: 97
	action: tensor([[ 0.0006,  0.0200, -0.0040, -0.0083,  0.0021, -0.0008,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[6.7812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000497842562302, distance: 0.9573930470276999 entropy -11.057859764848619
epoch: 21, step: 98
	action: tensor([[ 0.0006,  0.0200,  0.0152, -0.0085,  0.0063,  0.0032,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[6.7772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30077915693088964, distance: 0.9568940984461737 entropy -11.05008620734352
epoch: 21, step: 99
	action: tensor([[ 0.0006,  0.0198,  0.0060, -0.0084, -0.0040,  0.0079, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[6.7874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018582382748486, distance: 0.9561554439366245 entropy -11.058149036017129
epoch: 21, step: 100
	action: tensor([[ 0.0005,  0.0201,  0.0038, -0.0082, -0.0167, -0.0015, -0.0295]],
       dtype=torch.float64)
	q_value: tensor([[6.7715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29988045501822724, distance: 0.9575088444288052 entropy -11.058613921193897
epoch: 21, step: 101
	action: tensor([[ 0.0004,  0.0204,  0.0173, -0.0080, -0.0073,  0.0217, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[6.7598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053724521952159, distance: 0.9537459233476252 entropy -11.064444629445827
epoch: 21, step: 102
	action: tensor([[ 0.0005,  0.0199,  0.0131, -0.0082, -0.0037, -0.0142,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[6.7739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296709659234352, distance: 0.9596746425812546 entropy -11.061317118011223
epoch: 21, step: 103
	action: tensor([[ 0.0007,  0.0199,  0.0119, -0.0084, -0.0156,  0.0073,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[6.7921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30180199591137113, distance: 0.9561939571455113 entropy -11.056258711333758
epoch: 21, step: 104
	action: tensor([[ 0.0006,  0.0198,  0.0159, -0.0084, -0.0022, -0.0018, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[6.7881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995392679499813, distance: 0.9577421259064665 entropy -11.053737451869214
epoch: 21, step: 105
	action: tensor([[ 0.0005,  0.0200,  0.0094, -0.0083, -0.0171, -0.0153,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[6.7811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966715071058087, distance: 0.9597006724665382 entropy -11.061712402105943
epoch: 21, step: 106
	action: tensor([[ 0.0007,  0.0201,  0.0149, -0.0084,  0.0007, -0.0019,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[6.7855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29975355300754936, distance: 0.9575956183896418 entropy -11.054483084829997
epoch: 21, step: 107
	action: tensor([[ 0.0007,  0.0196,  0.0035, -0.0086, -0.0022, -0.0391, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[6.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2909903556322524, distance: 0.9635688785124626 entropy -11.05050844154747
epoch: 21, step: 108
	action: tensor([[ 0.0008,  0.0202,  0.0110, -0.0085, -0.0019,  0.0124,  0.0449]],
       dtype=torch.float64)
	q_value: tensor([[6.7896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303235766988303, distance: 0.9552116657438952 entropy -11.055283204873621
epoch: 21, step: 109
	action: tensor([[ 0.0006,  0.0195,  0.0023, -0.0087, -0.0043,  0.0079,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[6.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30141837270727256, distance: 0.9564566103000807 entropy -11.04550908898339
epoch: 21, step: 110
	action: tensor([[ 0.0006,  0.0197,  0.0114, -0.0087, -0.0048,  0.0074,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[6.7891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30138702726678457, distance: 0.9564780682201618 entropy -11.043172909251751
epoch: 21, step: 111
	action: tensor([[ 0.0006,  0.0199,  0.0131, -0.0084, -0.0041,  0.0121, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[6.7803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30282472730987087, distance: 0.9554933765355103 entropy -11.05518267205037
epoch: 21, step: 112
	action: tensor([[ 0.0004,  0.0201,  0.0108, -0.0081, -0.0073, -0.0020, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[6.7668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29970017121237, distance: 0.9576321178243771 entropy -11.062990680831364
epoch: 21, step: 113
	action: tensor([[ 0.0005,  0.0201,  0.0172, -0.0083, -0.0165,  0.0114, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[6.7757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30277849762287223, distance: 0.9555250553896071 entropy -11.058831991841116
epoch: 21, step: 114
	action: tensor([[ 0.0005,  0.0200,  0.0224, -0.0082,  0.0050,  0.0240,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[6.7761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057563455347989, distance: 0.953482337984426 entropy -11.060780184504244
epoch: 21, step: 115
	action: tensor([[ 0.0005,  0.0196,  0.0006, -0.0083, -0.0111, -0.0074,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[6.7874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981570915731563, distance: 0.9586865872421085 entropy -11.060161485739956
epoch: 21, step: 116
	action: tensor([[ 0.0006,  0.0201,  0.0141, -0.0084, -0.0127, -0.0120,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[6.7763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29744563264575863, distance: 0.9591723749858052 entropy -11.054053621122222
epoch: 21, step: 117
	action: tensor([[ 0.0007,  0.0198,  0.0040, -0.0085,  0.0028,  0.0161,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[6.7998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037942909182777, distance: 0.9548287417175305 entropy -11.051847979880874
epoch: 21, step: 118
	action: tensor([[ 0.0005,  0.0200,  0.0150, -0.0083, -0.0075,  0.0452, -0.0442]],
       dtype=torch.float64)
	q_value: tensor([[6.7691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3105908175630979, distance: 0.9501566780293399 entropy -11.055187214627427
epoch: 21, step: 119
	action: tensor([[-1.9482e-05,  2.0224e-02,  9.3265e-03, -7.4807e-03, -1.9395e-03,
          1.8374e-02,  1.0961e-02]], dtype=torch.float64)
	q_value: tensor([[6.7403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043231515855904, distance: 0.9544660132210779 entropy -11.073617080358718
epoch: 21, step: 120
	action: tensor([[ 0.0005,  0.0199,  0.0119, -0.0083, -0.0037, -0.0207,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[6.7747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29517274789050674, distance: 0.9607226663055068 entropy -11.055338923976235
epoch: 21, step: 121
	action: tensor([[ 0.0008,  0.0199,  0.0143, -0.0086,  0.0025,  0.0101,  0.0472]],
       dtype=torch.float64)
	q_value: tensor([[6.7940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30247567775126194, distance: 0.9557325364858712 entropy -11.053172213732237
epoch: 21, step: 122
	action: tensor([[ 0.0006,  0.0194, -0.0080, -0.0088, -0.0045, -0.0085,  0.0459]],
       dtype=torch.float64)
	q_value: tensor([[6.8059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297684427658759, distance: 0.9590093519824672 entropy -11.04611427090633
epoch: 21, step: 123
	action: tensor([[ 0.0007,  0.0198,  0.0088, -0.0088, -0.0057,  0.0103,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[6.7907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30219576634159984, distance: 0.9559242814902388 entropy -11.039225097522275
epoch: 21, step: 124
	action: tensor([[ 0.0006,  0.0198,  0.0114, -0.0085, -0.0129, -0.0190, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[6.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955916248091863, distance: 0.9604371464446544 entropy -11.050967173372285
epoch: 21, step: 125
	action: tensor([[ 0.0006,  0.0202,  0.0028, -0.0083, -0.0061,  0.0300,  0.0456]],
       dtype=torch.float64)
	q_value: tensor([[6.7830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072072288156812, distance: 0.9524854868721222 entropy -11.06003933189711
epoch: 21, step: 126
	action: tensor([[ 0.0004,  0.0196,  0.0099, -0.0085, -0.0021, -0.0040,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[6.7803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29860982501065036, distance: 0.9583773303659138 entropy -11.045892771059025
epoch: 21, step: 127
	action: tensor([[ 0.0007,  0.0195,  0.0016, -0.0088, -0.0039,  0.0014, -0.0253]],
       dtype=torch.float64)
	q_value: tensor([[6.8055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30006026367533334, distance: 0.9573858801176017 entropy -11.043873410481964
LOSS epoch 21 actor 21.080960502384496 critic 12.42354947153666
epoch: 22, step: 0
	action: tensor([[ 0.0021,  0.0190,  0.0004, -0.0077, -0.0094, -0.0106,  0.0257]],
       dtype=torch.float64)
	q_value: tensor([[7.2182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988836345689152, distance: 0.9581902458456982 entropy -11.060336847763732
epoch: 22, step: 1
	action: tensor([[ 0.0023,  0.0185,  0.0164, -0.0083, -0.0074, -0.0068,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[7.2546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29924337820699043, distance: 0.9579443900290223 entropy -11.045341311368531
epoch: 22, step: 2
	action: tensor([[ 0.0024,  0.0184,  0.0144, -0.0083, -0.0133, -0.0051,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[7.2612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29968100336259385, distance: 0.957645223370187 entropy -11.05101601038626
epoch: 22, step: 3
	action: tensor([[ 0.0024,  0.0184,  0.0146, -0.0083, -0.0003,  0.0071,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[7.2611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3024382993882144, distance: 0.9557581436496756 entropy -11.048764108479036
epoch: 22, step: 4
	action: tensor([[ 0.0023,  0.0182,  0.0073, -0.0083, -0.0100,  0.0306,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[7.2603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30767851200161556, distance: 0.9521614601352324 entropy -11.048010302599604
epoch: 22, step: 5
	action: tensor([[ 0.0021,  0.0185,  0.0036, -0.0079, -0.0087,  0.0104, -0.0292]],
       dtype=torch.float64)
	q_value: tensor([[7.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032273987677123, distance: 0.9552174018290375 entropy -11.054077393941967
epoch: 22, step: 6
	action: tensor([[ 0.0020,  0.0191,  0.0162, -0.0076, -0.0126,  0.0045,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[7.2111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30221751698485155, distance: 0.9559093832357972 entropy -11.062099173483663
epoch: 22, step: 7
	action: tensor([[ 0.0022,  0.0186,  0.0011, -0.0080, -0.0048, -0.0021,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[7.2476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004710923465036, distance: 0.9571048707104026 entropy -11.055543234497877
epoch: 22, step: 8
	action: tensor([[ 0.0023,  0.0186,  0.0150, -0.0082, -0.0030,  0.0032,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[7.2433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016083635082295, distance: 0.9563265393768333 entropy -11.047749190416457
epoch: 22, step: 9
	action: tensor([[ 0.0024,  0.0183,  0.0087, -0.0083, -0.0122, -0.0238, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[7.2593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29534498141880616, distance: 0.9606052767154556 entropy -11.049597979523286
epoch: 22, step: 10
	action: tensor([[ 2.2597e-03,  1.8920e-02,  2.7635e-02, -7.9363e-03, -9.0811e-03,
         -1.0277e-02, -1.2953e-05]], dtype=torch.float64)
	q_value: tensor([[7.2367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29874750398420635, distance: 0.9582832636911521 entropy -11.059444259168462
epoch: 22, step: 11
	action: tensor([[ 0.0024,  0.0184, -0.0021, -0.0082, -0.0031,  0.0089,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[7.2687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029218662318811, distance: 0.9554268087486892 entropy -11.058096694368256
epoch: 22, step: 12
	action: tensor([[ 0.0022,  0.0182,  0.0229, -0.0085,  0.0044, -0.0076, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[7.2582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985360250309108, distance: 0.9584277490698572 entropy -11.03775424292665
epoch: 22, step: 13
	action: tensor([[ 0.0024,  0.0184,  0.0015, -0.0081, -0.0100, -0.0029,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.2605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002855707482335, distance: 0.9572317788710675 entropy -11.058548248063088
epoch: 22, step: 14
	action: tensor([[ 0.0023,  0.0187,  0.0056, -0.0081, -0.0072,  0.0037, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.2409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019010194445807, distance: 0.956126147540637 entropy -11.051788504325279
epoch: 22, step: 15
	action: tensor([[ 0.0021,  0.0188,  0.0144, -0.0078,  0.0052, -0.0144, -0.0105]],
       dtype=torch.float64)
	q_value: tensor([[7.2306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977255400459483, distance: 0.9589812821643064 entropy -11.058570933897423
epoch: 22, step: 16
	action: tensor([[ 0.0023,  0.0186,  0.0039, -0.0081, -0.0068, -0.0137, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[7.2484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979607604542448, distance: 0.9588206677086455 entropy -11.057761812496576
epoch: 22, step: 17
	action: tensor([[ 0.0022,  0.0190,  0.0055, -0.0078, -0.0191,  0.0077, -0.0209]],
       dtype=torch.float64)
	q_value: tensor([[7.2249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030679296895735, distance: 0.9553267050102204 entropy -11.060679078827798
epoch: 22, step: 18
	action: tensor([[ 0.0020,  0.0190,  0.0178, -0.0077, -0.0045, -0.0028,  0.0461]],
       dtype=torch.float64)
	q_value: tensor([[7.2236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300462322504943, distance: 0.9571108701992543 entropy -11.061077399874609
epoch: 22, step: 19
	action: tensor([[ 0.0024,  0.0180,  0.0198, -0.0086,  0.0021, -0.0075,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[7.2777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986446682412437, distance: 0.9583535252298874 entropy -11.042840337963318
epoch: 22, step: 20
	action: tensor([[ 0.0024,  0.0184,  0.0114, -0.0082, -0.0020,  0.0247, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[7.2620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066205107895015, distance: 0.9528887258775649 entropy -11.055635082365376
epoch: 22, step: 21
	action: tensor([[ 0.0021,  0.0187, -0.0038, -0.0078, -0.0028,  0.0255,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[7.2277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30686660402830346, distance: 0.9527196119311875 entropy -11.058864335572578
epoch: 22, step: 22
	action: tensor([[ 0.0021,  0.0186,  0.0199, -0.0081, -0.0224, -0.0340,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[7.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29299025662343814, distance: 0.9622089511893117 entropy -11.048543124892612
epoch: 22, step: 23
	action: tensor([[ 0.0025,  0.0185,  0.0112, -0.0083, -0.0042,  0.0106,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[7.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034067843261754, distance: 0.9550944325583289 entropy -11.050253288116668
epoch: 22, step: 24
	action: tensor([[ 0.0023,  0.0183,  0.0092, -0.0083, -0.0022,  0.0266,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[7.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068610165406491, distance: 0.9527234519556133 entropy -11.047979241242853
epoch: 22, step: 25
	action: tensor([[ 0.0022,  0.0184,  0.0158, -0.0081, -0.0127,  0.0198, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.2429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30524339722010385, distance: 0.9538345175448281 entropy -11.05217259577709
epoch: 22, step: 26
	action: tensor([[ 0.0021,  0.0187,  0.0124, -0.0078, -0.0034, -0.0003, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.2349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30087282465889187, distance: 0.9568300034617744 entropy -11.060478111282729
epoch: 22, step: 27
	action: tensor([[ 2.1733e-03,  1.8780e-02,  9.6991e-05, -7.8859e-03, -1.4059e-02,
          2.5843e-02, -2.4073e-03]], dtype=torch.float64)
	q_value: tensor([[7.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3069878835580476, distance: 0.9526362582492314 entropy -11.059243043205585
epoch: 22, step: 28
	action: tensor([[ 0.0020,  0.0188,  0.0086, -0.0077, -0.0140, -0.0041,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[7.2197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30009978869163345, distance: 0.9573588483427383 entropy -11.057176643701533
epoch: 22, step: 29
	action: tensor([[ 0.0023,  0.0185, -0.0003, -0.0082, -0.0101,  0.0051, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[7.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020932794797003, distance: 0.9559944774550583 entropy -11.048850681785181
epoch: 22, step: 30
	action: tensor([[ 0.0021,  0.0190,  0.0113, -0.0078, -0.0130, -0.0023,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[7.2204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006716279793945, distance: 0.9569676731004243 entropy -11.056726062533867
epoch: 22, step: 31
	action: tensor([[ 0.0023,  0.0186,  0.0135, -0.0080, -0.0042,  0.0142, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[7.2495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042638449301852, distance: 0.9545066966078394 entropy -11.05460503737748
epoch: 22, step: 32
	action: tensor([[ 0.0020,  0.0189,  0.0058, -0.0076, -0.0020,  0.0074, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028462065244265, distance: 0.955478657564455 entropy -11.063871889710585
epoch: 22, step: 33
	action: tensor([[ 0.0022,  0.0188,  0.0090, -0.0079, -0.0117,  0.0016,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[7.2292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014289763336493, distance: 0.9564493513437677 entropy -11.056570925168176
epoch: 22, step: 34
	action: tensor([[ 0.0023,  0.0184,  0.0165, -0.0082, -0.0113,  0.0069, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[7.2552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30239761944535715, distance: 0.955786011879686 entropy -11.049550958798525
epoch: 22, step: 35
	action: tensor([[ 0.0021,  0.0187, -0.0022, -0.0079, -0.0070,  0.0115,  0.0397]],
       dtype=torch.float64)
	q_value: tensor([[7.2363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036918194166387, distance: 0.9548990076846854 entropy -11.059929726832099
epoch: 22, step: 36
	action: tensor([[ 0.0022,  0.0184, -0.0080, -0.0083, -0.0026,  0.0070,  0.0597]],
       dtype=torch.float64)
	q_value: tensor([[7.2486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30224836163957547, distance: 0.9558882555767974 entropy -11.04284657496307
epoch: 22, step: 37
	action: tensor([[ 0.0022,  0.0183,  0.0155, -0.0085, -0.0087, -0.0080,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[7.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29855263495573736, distance: 0.9584164017251099 entropy -11.033716889622507
epoch: 22, step: 38
	action: tensor([[ 0.0024,  0.0184,  0.0124, -0.0082, -0.0053,  0.0044,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[7.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30187652641829876, distance: 0.9561429203895493 entropy -11.050899019868208
epoch: 22, step: 39
	action: tensor([[ 0.0023,  0.0185,  0.0059, -0.0081, -0.0033, -0.0068,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[7.2502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993470699300128, distance: 0.9578735133679463 entropy -11.053969286539086
epoch: 22, step: 40
	action: tensor([[ 0.0024,  0.0182,  0.0132, -0.0085, -0.0143,  0.0067,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[7.2674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021130419385568, distance: 0.95598094202495 entropy -11.04174868606464
epoch: 22, step: 41
	action: tensor([[ 0.0023,  0.0185, -0.0033, -0.0081, -0.0124,  0.0110, -0.0271]],
       dtype=torch.float64)
	q_value: tensor([[7.2509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30345514289742515, distance: 0.9550612799215039 entropy -11.053399691329528
epoch: 22, step: 42
	action: tensor([[ 0.0020,  0.0191,  0.0119, -0.0076,  0.0118,  0.0044,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[7.2093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30228207386113604, distance: 0.9558651631867977 entropy -11.061222886369277
epoch: 22, step: 43
	action: tensor([[ 0.0024,  0.0182,  0.0035, -0.0084, -0.0102,  0.0004,  0.0431]],
       dtype=torch.float64)
	q_value: tensor([[7.2630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007295353092464, distance: 0.9569280518065326 entropy -11.048553604355972
epoch: 22, step: 44
	action: tensor([[ 0.0023,  0.0183,  0.0143, -0.0084, -0.0190,  0.0104, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.2605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029540209185748, distance: 0.9554047726221115 entropy -11.041979582483748
epoch: 22, step: 45
	action: tensor([[ 0.0021,  0.0188,  0.0055, -0.0079, -0.0065, -0.0152, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[7.2349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29760508519600803, distance: 0.9590635213743176 entropy -11.058525804141182
epoch: 22, step: 46
	action: tensor([[ 0.0022,  0.0189,  0.0110, -0.0079,  0.0011, -0.0248,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[7.2325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954935922712538, distance: 0.9605039761550025 entropy -11.058425714864772
epoch: 22, step: 47
	action: tensor([[ 0.0025,  0.0185,  0.0146, -0.0083, -0.0086,  0.0382,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[7.2655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098174698004942, distance: 0.950689449863962 entropy -11.050630612506767
epoch: 22, step: 48
	action: tensor([[ 0.0021,  0.0184,  0.0163, -0.0079, -0.0136,  0.0041,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[7.2368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30158929141986635, distance: 0.9563395972506612 entropy -11.055436115930418
epoch: 22, step: 49
	action: tensor([[ 0.0023,  0.0182,  0.0033, -0.0084, -0.0026, -0.0156,  0.0503]],
       dtype=torch.float64)
	q_value: tensor([[7.2709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969941570708824, distance: 0.9594805172245737 entropy -11.045646984571537
epoch: 22, step: 50
	action: tensor([[ 0.0024,  0.0182,  0.0068, -0.0086,  0.0079,  0.0065, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.2730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30206952074590443, distance: 0.9560107497048661 entropy -11.037702086257232
epoch: 22, step: 51
	action: tensor([[ 0.0022,  0.0187,  0.0023, -0.0080, -0.0096, -0.0173, -0.0448]],
       dtype=torch.float64)
	q_value: tensor([[7.2319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971742258907174, distance: 0.9593576280686084 entropy -11.05644500993833
epoch: 22, step: 52
	action: tensor([[ 0.0021,  0.0191,  0.0136, -0.0077, -0.0057,  0.0228,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[7.2194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066416170543447, distance: 0.952874222942895 entropy -11.065272589009387
epoch: 22, step: 53
	action: tensor([[ 0.0022,  0.0183, -0.0006, -0.0081, -0.0131,  0.0052, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[7.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30189478888549925, distance: 0.956130414261636 entropy -11.051938064806569
epoch: 22, step: 54
	action: tensor([[ 0.0021,  0.0189,  0.0021, -0.0078, -0.0023,  0.0121,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[7.2238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30398793912634003, distance: 0.9546959406418805 entropy -11.057111604058294
epoch: 22, step: 55
	action: tensor([[ 0.0022,  0.0185,  0.0091, -0.0082, -0.0134, -0.0013, -0.0209]],
       dtype=torch.float64)
	q_value: tensor([[7.2433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30045164031358074, distance: 0.9571181778845788 entropy -11.046813961975307
epoch: 22, step: 56
	action: tensor([[ 0.0021,  0.0189,  0.0140, -0.0078, -0.0050,  0.0022, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.2309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30163719632989017, distance: 0.9563067983921798 entropy -11.061029550745465
epoch: 22, step: 57
	action: tensor([[ 0.0023,  0.0186,  0.0115, -0.0080, -0.0122, -0.0019,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[7.2476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005121852251448, distance: 0.9570767583826022 entropy -11.056772141189665
epoch: 22, step: 58
	action: tensor([[ 0.0024,  0.0183, -0.0069, -0.0083, -0.0036,  0.0099,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[7.2633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030780792164107, distance: 0.9553197487014599 entropy -11.047434337636002
epoch: 22, step: 59
	action: tensor([[ 0.0021,  0.0189,  0.0070, -0.0079, -0.0180, -0.0036, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[7.2231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30027477801597957, distance: 0.9572391612445219 entropy -11.052701065311254
epoch: 22, step: 60
	action: tensor([[ 0.0022,  0.0188,  0.0253, -0.0080, -0.0021, -0.0006,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[7.2385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300853817067236, distance: 0.9568430103302079 entropy -11.054694457193184
epoch: 22, step: 61
	action: tensor([[ 0.0024,  0.0181,  0.0069, -0.0084, -0.0066, -0.0107,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981620480741508, distance: 0.9586832020548001 entropy -11.050632148842752
epoch: 22, step: 62
	action: tensor([[ 0.0023,  0.0187,  0.0020, -0.0081, -0.0055, -0.0228,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.2433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959050524919531, distance: 0.9602234486146807 entropy -11.053106485958537
epoch: 22, step: 63
	action: tensor([[ 0.0024,  0.0188,  0.0099, -0.0082, -0.0129,  0.0105,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30353066785725813, distance: 0.955009500829728 entropy -11.050885215387607
epoch: 22, step: 64
	action: tensor([[ 0.0022,  0.0187,  0.0163, -0.0079, -0.0069,  0.0185, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[7.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305302230978715, distance: 0.9537941301216406 entropy -11.055941218393375
epoch: 22, step: 65
	action: tensor([[ 0.0020,  0.0188,  0.0200, -0.0076, -0.0067,  0.0039, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[7.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018874549874788, distance: 0.95613543653079 entropy -11.06477219068351
epoch: 22, step: 66
	action: tensor([[ 0.0022,  0.0186,  0.0067, -0.0079,  0.0003, -0.0259, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.2471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294982702413202, distance: 0.9608521793821305 entropy -11.059867563731771
epoch: 22, step: 67
	action: tensor([[ 0.0024,  0.0188,  0.0094, -0.0081, -0.0071,  0.0067,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30271899420318094, distance: 0.9555658285120354 entropy -11.055653110371852
epoch: 22, step: 68
	action: tensor([[ 0.0022,  0.0187,  0.0047, -0.0080, -0.0139,  0.0010,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[7.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012558485103519, distance: 0.956567863084506 entropy -11.05514540297786
epoch: 22, step: 69
	action: tensor([[ 0.0022,  0.0187,  0.0123, -0.0080,  0.0019, -0.0092, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988855811474538, distance: 0.958188915685717 entropy -11.052685914318818
epoch: 22, step: 70
	action: tensor([[ 0.0023,  0.0187,  0.0020, -0.0080,  0.0017,  0.0220,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[7.2451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061646468876261, distance: 0.953201913825564 entropy -11.05878503890283
epoch: 22, step: 71
	action: tensor([[ 0.0022,  0.0185,  0.0121, -0.0081, -0.0075,  0.0094,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[7.2346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029456097451335, distance: 0.9554105369843257 entropy -11.049706510971825
epoch: 22, step: 72
	action: tensor([[ 0.0023,  0.0184,  0.0124, -0.0082, -0.0036, -0.0062, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[7.2512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29933822391607223, distance: 0.9578795601104519 entropy -11.05011718232946
epoch: 22, step: 73
	action: tensor([[ 0.0022,  0.0187,  0.0046, -0.0080, -0.0087,  0.0021,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[7.2437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015581180508522, distance: 0.9563609399893555 entropy -11.058494174595802
epoch: 22, step: 74
	action: tensor([[ 0.0022,  0.0187,  0.0169, -0.0080, -0.0005, -0.0091,  0.0191]],
       dtype=torch.float64)
	q_value: tensor([[7.2405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988294804441065, distance: 0.9582272503678917 entropy -11.05354145528847
epoch: 22, step: 75
	action: tensor([[ 0.0024,  0.0183, -0.0004, -0.0083, -0.0010,  0.0027,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[7.2683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014566441862705, distance: 0.9564304104194908 entropy -11.050921490755693
epoch: 22, step: 76
	action: tensor([[ 0.0023,  0.0186,  0.0086, -0.0082, -0.0031, -0.0129, -0.0389]],
       dtype=torch.float64)
	q_value: tensor([[7.2406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29791702351472105, distance: 0.9588505344355078 entropy -11.047418637575477
epoch: 22, step: 77
	action: tensor([[ 0.0021,  0.0190,  0.0138, -0.0077, -0.0057,  0.0040,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[7.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021509039551724, distance: 0.9559550095606537 entropy -11.064825201071695
epoch: 22, step: 78
	action: tensor([[ 0.0023,  0.0184,  0.0005, -0.0081, -0.0118, -0.0192,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.2554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29657069053743157, distance: 0.9597694527466443 entropy -11.052962848831124
epoch: 22, step: 79
	action: tensor([[ 0.0024,  0.0186,  0.0134, -0.0083, -0.0061,  0.0149, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30446495337569923, distance: 0.954368732502866 entropy -11.047618737562798
epoch: 22, step: 80
	action: tensor([[ 0.0022,  0.0186,  0.0151, -0.0079, -0.0068, -0.0037, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[7.2367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000834087352543, distance: 0.9573700509428898 entropy -11.05726623122465
epoch: 22, step: 81
	action: tensor([[ 0.0022,  0.0187,  0.0062, -0.0079, -0.0003, -0.0060, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.2401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299692410635239, distance: 0.95763742394971 entropy -11.059160755564744
epoch: 22, step: 82
	action: tensor([[ 0.0023,  0.0187,  0.0071, -0.0081, -0.0104, -0.0352,  0.0429]],
       dtype=torch.float64)
	q_value: tensor([[7.2438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29306619751649143, distance: 0.9621572737053651 entropy -11.054597788454592
epoch: 22, step: 83
	action: tensor([[ 0.0025,  0.0183,  0.0099, -0.0087, -0.0132, -0.0078,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[7.2790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989333552480782, distance: 0.9581562695210696 entropy -11.037880828904864
epoch: 22, step: 84
	action: tensor([[ 0.0023,  0.0186,  0.0123, -0.0081,  0.0019,  0.0012, -0.0025]],
       dtype=torch.float64)
	q_value: tensor([[7.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012618505105483, distance: 0.9565637547618682 entropy -11.051510440105911
epoch: 22, step: 85
	action: tensor([[ 0.0023,  0.0186,  0.0040, -0.0080, -0.0100,  0.0045,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[7.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020277750787439, distance: 0.956039340453504 entropy -11.056373467155979
epoch: 22, step: 86
	action: tensor([[ 0.0022,  0.0187,  0.0077, -0.0080, -0.0114, -0.0054, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29978556458626016, distance: 0.9575737300259183 entropy -11.05343435600293
epoch: 22, step: 87
	action: tensor([[ 0.0022,  0.0188, -0.0023, -0.0079, -0.0081,  0.0110, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036808607683933, distance: 0.9549065218583331 entropy -11.0568295987369
epoch: 22, step: 88
	action: tensor([[ 0.0021,  0.0189,  0.0135, -0.0078, -0.0011,  0.0008, -0.0118]],
       dtype=torch.float64)
	q_value: tensor([[7.2184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30131408295815987, distance: 0.9565280013111609 entropy -11.05580833932803
epoch: 22, step: 89
	action: tensor([[ 0.0022,  0.0187,  0.0072, -0.0080, -0.0137, -0.0211, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[7.2430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29624524446840395, distance: 0.9599914488105501 entropy -11.058612899750967
epoch: 22, step: 90
	action: tensor([[ 0.0023,  0.0188,  0.0100, -0.0080, -0.0105,  0.0169, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[7.2438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050901539126293, distance: 0.9539397059636217 entropy -11.057200649955698
epoch: 22, step: 91
	action: tensor([[ 0.0021,  0.0187,  0.0016, -0.0078, -0.0010, -0.0164, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[7.2332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972861347352783, distance: 0.9592812472060602 entropy -11.058550371033556
epoch: 22, step: 92
	action: tensor([[ 0.0023,  0.0188,  0.0018, -0.0081, -0.0072,  0.0236,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[7.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30659717712981716, distance: 0.952904759084876 entropy -11.052898097549786
epoch: 22, step: 93
	action: tensor([[ 0.0021,  0.0188,  0.0179, -0.0078, -0.0083,  0.0003,  0.0387]],
       dtype=torch.float64)
	q_value: tensor([[7.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30103308596541756, distance: 0.9567203298425753 entropy -11.055380227608424
epoch: 22, step: 94
	action: tensor([[ 0.0024,  0.0181,  0.0127, -0.0084, -0.0022,  0.0006,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.2712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30070260085558376, distance: 0.9569464810748791 entropy -11.045500268881145
epoch: 22, step: 95
	action: tensor([[ 0.0023,  0.0186,  0.0080, -0.0081, -0.0010, -0.0255, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[7.2451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2951124219405349, distance: 0.9607637794061029 entropy -11.054767303822304
epoch: 22, step: 96
	action: tensor([[ 0.0023,  0.0189,  0.0159, -0.0080, -0.0043, -0.0012,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[7.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30092548744031633, distance: 0.9567939654703628 entropy -11.057816642886804
epoch: 22, step: 97
	action: tensor([[ 2.3680e-03,  1.8358e-02,  1.3913e-02, -8.2115e-03,  2.4807e-05,
         -1.1852e-02,  3.2201e-02]], dtype=torch.float64)
	q_value: tensor([[7.2618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29801544536333546, distance: 0.9587833236121952 entropy -11.052171138598291
epoch: 22, step: 98
	action: tensor([[ 0.0025,  0.0182,  0.0023, -0.0085, -0.0006, -0.0090,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[7.2689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986716559765832, distance: 0.9583350866161491 entropy -11.045875254515902
epoch: 22, step: 99
	action: tensor([[ 0.0023,  0.0187, -0.0041, -0.0081, -0.0057,  0.0102, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[7.2415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30347105422930665, distance: 0.9550503715186773 entropy -11.053139300704183
epoch: 22, step: 100
	action: tensor([[ 0.0019,  0.0192,  0.0036, -0.0075, -0.0027,  0.0050,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[7.1988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025118191736629, distance: 0.9557077760727604 entropy -11.063538645327965
epoch: 22, step: 101
	action: tensor([[ 0.0022,  0.0186,  0.0006, -0.0081, -0.0085,  0.0058,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[7.2414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30223656873653737, distance: 0.9558963334151449 entropy -11.049987512170407
epoch: 22, step: 102
	action: tensor([[ 0.0022,  0.0186,  0.0215, -0.0081, -0.0058,  0.0052,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[7.2385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30206223336218607, distance: 0.9560157407456599 entropy -11.048579352955418
epoch: 22, step: 103
	action: tensor([[ 0.0023,  0.0184,  0.0045, -0.0081, -0.0040, -0.0184,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.2597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966043225415568, distance: 0.9597465084680967 entropy -11.055519510170878
epoch: 22, step: 104
	action: tensor([[ 0.0024,  0.0186,  0.0147, -0.0082, -0.0036,  0.0012,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[7.2515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30129087502688834, distance: 0.9565438874563024 entropy -11.051032837868416
epoch: 22, step: 105
	action: tensor([[ 0.0023,  0.0185,  0.0088, -0.0081, -0.0071,  0.0018,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[7.2547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30129385873719494, distance: 0.9565418450807267 entropy -11.054143821625818
epoch: 22, step: 106
	action: tensor([[ 0.0023,  0.0185,  0.0053, -0.0081, -0.0022,  0.0100,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[7.2469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30321718787419893, distance: 0.9552244009471297 entropy -11.05102410540764
epoch: 22, step: 107
	action: tensor([[ 0.0022,  0.0187,  0.0200, -0.0080, -0.0165, -0.0091, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[7.2321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988776092015103, distance: 0.958194363162135 entropy -11.054499021036042
epoch: 22, step: 108
	action: tensor([[ 0.0022,  0.0187,  0.0040, -0.0079, -0.0027, -0.0117,  0.0225]],
       dtype=torch.float64)
	q_value: tensor([[7.2458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983930212275061, distance: 0.958525438925389 entropy -11.060408469285646
epoch: 22, step: 109
	action: tensor([[ 0.0024,  0.0184,  0.0163, -0.0083, -0.0161, -0.0199,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[7.2578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962720342442563, distance: 0.9599731766779022 entropy -11.046427951794174
epoch: 22, step: 110
	action: tensor([[ 0.0025,  0.0183,  0.0014, -0.0084, -0.0076,  0.0174,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[7.2768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30479926022887427, distance: 0.9541393476887244 entropy -11.046300634283195
epoch: 22, step: 111
	action: tensor([[ 0.0021,  0.0187,  0.0144, -0.0080, -0.0044,  0.0208, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[7.2289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305797559484986, distance: 0.9534540357042667 entropy -11.052110415034436
epoch: 22, step: 112
	action: tensor([[ 0.0021,  0.0187,  0.0066, -0.0078, -0.0077, -0.0044,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[7.2326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999618517273186, distance: 0.9574531822673606 entropy -11.061037365315201
epoch: 22, step: 113
	action: tensor([[ 0.0023,  0.0187,  0.0154, -0.0081, -0.0115,  0.0063,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[7.2422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30241934393870873, distance: 0.9557711293842287 entropy -11.052512144299074
epoch: 22, step: 114
	action: tensor([[ 0.0023,  0.0184,  0.0082, -0.0082, -0.0073,  0.0041,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[7.2589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30173028349213793, distance: 0.9562430615692337 entropy -11.051680466180098
epoch: 22, step: 115
	action: tensor([[ 0.0023,  0.0185,  0.0124, -0.0081, -0.0120, -0.0022, -0.0162]],
       dtype=torch.float64)
	q_value: tensor([[7.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003963595571082, distance: 0.9571559945493798 entropy -11.052093803513937
epoch: 22, step: 116
	action: tensor([[ 0.0022,  0.0188,  0.0168, -0.0079, -0.0016, -0.0224,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[7.2336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29587549057667484, distance: 0.9602436062272236 entropy -11.059830707635667
epoch: 22, step: 117
	action: tensor([[ 0.0025,  0.0184,  0.0136, -0.0083, -0.0098, -0.0014,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[7.2682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30058350370573184, distance: 0.9570279662530569 entropy -11.053041255072419
epoch: 22, step: 118
	action: tensor([[ 0.0024,  0.0183,  0.0061, -0.0084, -0.0079,  0.0081,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[7.2674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025558435478857, distance: 0.955677614199142 entropy -11.046885164854787
epoch: 22, step: 119
	action: tensor([[ 0.0022,  0.0186, -0.0032, -0.0081, -0.0095,  0.0019, -0.0273]],
       dtype=torch.float64)
	q_value: tensor([[7.2394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30143683801632015, distance: 0.9564439694125167 entropy -11.051480893123061
epoch: 22, step: 120
	action: tensor([[ 0.0020,  0.0191,  0.0037, -0.0077, -0.0161,  0.0039,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[7.2103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30222505988040593, distance: 0.9559042166227779 entropy -11.060124121893832
epoch: 22, step: 121
	action: tensor([[ 0.0022,  0.0187,  0.0172, -0.0080, -0.0155,  0.0021,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.2424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30142020465721286, distance: 0.9564553562005521 entropy -11.051918451710227
epoch: 22, step: 122
	action: tensor([[ 0.0023,  0.0185,  0.0062, -0.0081, -0.0106, -0.0109,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[7.2540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984498487954024, distance: 0.9584866196332966 entropy -11.055368047929507
epoch: 22, step: 123
	action: tensor([[ 0.0023,  0.0187,  0.0152, -0.0081,  0.0138, -0.0114,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.2447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29832823007871323, distance: 0.9585696962749487 entropy -11.051887755036057
epoch: 22, step: 124
	action: tensor([[ 0.0024,  0.0184,  0.0149, -0.0082, -0.0065,  0.0007, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[7.2602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010247913947609, distance: 0.9567260064781016 entropy -11.05500372892728
epoch: 22, step: 125
	action: tensor([[ 0.0022,  0.0187,  0.0117, -0.0079, -0.0095, -0.0134, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[7.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979896412368268, distance: 0.9588009453238547 entropy -11.05936722948346
epoch: 22, step: 126
	action: tensor([[ 2.2939e-03,  1.8717e-02,  9.4566e-03, -8.0353e-03, -8.8772e-03,
         -5.8469e-05,  5.1557e-02]], dtype=torch.float64)
	q_value: tensor([[7.2451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010702588476507, distance: 0.9566948890639797 entropy -11.056242862609233
epoch: 22, step: 127
	action: tensor([[ 0.0023,  0.0181,  0.0064, -0.0085, -0.0020,  0.0108, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[7.2745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029400714120192, distance: 0.955414332506869 entropy -11.03926575273385
LOSS epoch 22 actor 24.183430712915943 critic 12.014111614874375
epoch: 23, step: 0
	action: tensor([[ 0.0031,  0.0188,  0.0127, -0.0077, -0.0189, -0.0026, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[7.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014868140124901, distance: 0.9564097562606596 entropy -11.054564485363226
epoch: 23, step: 1
	action: tensor([[ 0.0031,  0.0188,  0.0075, -0.0078, -0.0117,  0.0199, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[7.5047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30665703946340617, distance: 0.952863625466805 entropy -11.055761460188748
epoch: 23, step: 2
	action: tensor([[ 0.0029,  0.0190,  0.0036, -0.0075, -0.0019, -0.0043, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[7.4788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30120617800748095, distance: 0.9566018614817339 entropy -11.058761729578313
epoch: 23, step: 3
	action: tensor([[ 0.0031,  0.0189,  0.0033, -0.0078, -0.0138, -0.0050, -0.0180]],
       dtype=torch.float64)
	q_value: tensor([[7.4911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011212477879629, distance: 0.9566599916018803 entropy -11.054227747108916
epoch: 23, step: 4
	action: tensor([[ 0.0030,  0.0190,  0.0109, -0.0077, -0.0040, -0.0102,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[7.4923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29985896139862067, distance: 0.9575235420422831 entropy -11.055718243932992
epoch: 23, step: 5
	action: tensor([[ 0.0033,  0.0186,  0.0008, -0.0080,  0.0032, -0.0247,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[7.5170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29637380193428686, distance: 0.9599037622239953 entropy -11.050815212272264
epoch: 23, step: 6
	action: tensor([[ 0.0033,  0.0187,  0.0077, -0.0081, -0.0153,  0.0262,  0.0449]],
       dtype=torch.float64)
	q_value: tensor([[7.5112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30807981157259867, distance: 0.9518854630884417 entropy -11.048580509103758
epoch: 23, step: 7
	action: tensor([[ 0.0030,  0.0183, -0.0042, -0.0081, -0.0079, -0.0084, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.5161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29961068431358984, distance: 0.9576933007544779 entropy -11.041186945250798
epoch: 23, step: 8
	action: tensor([[ 0.0031,  0.0189,  0.0013, -0.0078, -0.0144, -0.0160,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[7.4930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986159963735969, distance: 0.9583731140911737 entropy -11.050394309280533
epoch: 23, step: 9
	action: tensor([[ 0.0032,  0.0188,  0.0029, -0.0080, -0.0125, -0.0182,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[7.5076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29801798952697434, distance: 0.9587815861780425 entropy -11.0492080582211
epoch: 23, step: 10
	action: tensor([[ 0.0033,  0.0186,  0.0096, -0.0082,  0.0004,  0.0023, -0.0271]],
       dtype=torch.float64)
	q_value: tensor([[7.5181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3024453659943851, distance: 0.9557533025126873 entropy -11.042476352691894
epoch: 23, step: 11
	action: tensor([[ 0.0030,  0.0189,  0.0095, -0.0076, -0.0027, -0.0205,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[7.4879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29744571067858405, distance: 0.9591723217180895 entropy -11.058184237511371
epoch: 23, step: 12
	action: tensor([[ 0.0034,  0.0181,  0.0077, -0.0086, -0.0150,  0.0182,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[7.5460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055941322350567, distance: 0.9535937242979001 entropy -11.03355088710317
epoch: 23, step: 13
	action: tensor([[ 0.0031,  0.0184,  0.0173, -0.0081, -0.0102,  0.0072,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[7.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30325777240878016, distance: 0.9551965817317432 entropy -11.043911552839985
epoch: 23, step: 14
	action: tensor([[ 0.0032,  0.0184,  0.0127, -0.0081, -0.0038,  0.0125, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.5252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046504668924397, distance: 0.9542414491201852 entropy -11.048661040676635
epoch: 23, step: 15
	action: tensor([[ 0.0031,  0.0187, -0.0012, -0.0077, -0.0016, -0.0084,  0.0158]],
       dtype=torch.float64)
	q_value: tensor([[7.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001531587004129, distance: 0.9573223465504938 entropy -11.055870241141776
epoch: 23, step: 16
	action: tensor([[ 0.0032,  0.0187,  0.0045, -0.0081, -0.0037,  0.0037, -0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.5051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30284310904419043, distance: 0.9554807801757054 entropy -11.044682389369829
epoch: 23, step: 17
	action: tensor([[ 0.0031,  0.0188,  0.0065, -0.0078, -0.0163,  0.0210,  0.0470]],
       dtype=torch.float64)
	q_value: tensor([[7.4943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30688251533632716, distance: 0.9527086767329335 entropy -11.052161913853123
epoch: 23, step: 18
	action: tensor([[ 0.0030,  0.0182,  0.0206, -0.0082, -0.0065, -0.0182,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[7.5225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971022777458546, distance: 0.9594067314436829 entropy -11.04020047417472
epoch: 23, step: 19
	action: tensor([[ 0.0033,  0.0185,  0.0033, -0.0081, -0.0140, -0.0197,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[7.5262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975351123014446, distance: 0.9591112913524212 entropy -11.051905357654192
epoch: 23, step: 20
	action: tensor([[ 0.0032,  0.0188,  0.0052, -0.0080, -0.0110, -0.0007,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[7.5060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30199002030408184, distance: 0.9560651971856756 entropy -11.049290420241347
epoch: 23, step: 21
	action: tensor([[ 0.0032,  0.0184,  0.0061, -0.0082,  0.0044,  0.0065, -0.0188]],
       dtype=torch.float64)
	q_value: tensor([[7.5217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032650789046337, distance: 0.9551915733097727 entropy -11.043018043886274
epoch: 23, step: 22
	action: tensor([[ 0.0030,  0.0189,  0.0177, -0.0077,  0.0045, -0.0157, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.4908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984141792290197, distance: 0.9585109859366072 entropy -11.056418123481734
epoch: 23, step: 23
	action: tensor([[ 0.0033,  0.0187,  0.0068, -0.0079, -0.0082,  0.0129,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[7.5132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305055557879915, distance: 0.9539634515751668 entropy -11.056009114539231
epoch: 23, step: 24
	action: tensor([[ 0.0031,  0.0188,  0.0220, -0.0078, -0.0129,  0.0258,  0.0370]],
       dtype=torch.float64)
	q_value: tensor([[7.4955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079739889345341, distance: 0.9519582512375334 entropy -11.05192337142636
epoch: 23, step: 25
	action: tensor([[ 0.0031,  0.0181,  0.0101, -0.0081, -0.0062, -0.0060,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.5317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000179233323985, distance: 0.957414836491543 entropy -11.046958889679784
epoch: 23, step: 26
	action: tensor([[ 0.0032,  0.0186,  0.0040, -0.0080,  0.0041,  0.0062, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[7.5140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30343988579795267, distance: 0.9550717396822854 entropy -11.050079669735283
epoch: 23, step: 27
	action: tensor([[ 0.0030,  0.0189,  0.0101, -0.0077, -0.0054, -0.0053,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[7.4899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009220741556138, distance: 0.9567963012773196 entropy -11.055638212648601
epoch: 23, step: 28
	action: tensor([[ 0.0033,  0.0183,  0.0072, -0.0082, -0.0003,  0.0100, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[7.5281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3040423271818171, distance: 0.9546586387953252 entropy -11.043934217885704
epoch: 23, step: 29
	action: tensor([[ 0.0030,  0.0189,  0.0061, -0.0077, -0.0034, -0.0214, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[7.4889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29725191718346466, distance: 0.9593046022712091 entropy -11.055215176857185
epoch: 23, step: 30
	action: tensor([[ 0.0032,  0.0188,  0.0088, -0.0079,  0.0037,  0.0208, -0.0373]],
       dtype=torch.float64)
	q_value: tensor([[7.5076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070680976583111, distance: 0.952581124240859 entropy -11.053321282049458
epoch: 23, step: 31
	action: tensor([[ 0.0028,  0.0190,  0.0125, -0.0074, -0.0069, -0.0112,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.4774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995364320012319, distance: 0.9577440647052533 entropy -11.063256611001963
epoch: 23, step: 32
	action: tensor([[ 0.0032,  0.0186,  0.0199, -0.0080, -0.0103, -0.0007,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[7.5164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017949841313683, distance: 0.9561987585090179 entropy -11.052001266638044
epoch: 23, step: 33
	action: tensor([[ 0.0032,  0.0184,  0.0114, -0.0080,  0.0023,  0.0084, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[7.5253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30381423741238534, distance: 0.9548150635603144 entropy -11.05137895753961
epoch: 23, step: 34
	action: tensor([[ 0.0029,  0.0190, -0.0040, -0.0074, -0.0046, -0.0133, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[7.4779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29926440977903523, distance: 0.9579300146902839 entropy -11.063437881436645
epoch: 23, step: 35
	action: tensor([[ 0.0031,  0.0190,  0.0164, -0.0078, -0.0053, -0.0004,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[7.4893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021181616862859, distance: 0.9559774354468596 entropy -11.051512321209362
epoch: 23, step: 36
	action: tensor([[ 0.0033,  0.0184,  0.0091, -0.0081,  0.0053,  0.0115,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.5232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045152830892097, distance: 0.9543342022702865 entropy -11.048220118210102
epoch: 23, step: 37
	action: tensor([[ 0.0031,  0.0186,  0.0164, -0.0079, -0.0089,  0.0251, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[7.5038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30771120127584173, distance: 0.9521389808138201 entropy -11.050151421191257
epoch: 23, step: 38
	action: tensor([[ 0.0029,  0.0188,  0.0150, -0.0075, -0.0058,  0.0070, -0.0341]],
       dtype=torch.float64)
	q_value: tensor([[7.4898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036290439345595, distance: 0.9549420510498305 entropy -11.059331004969525
epoch: 23, step: 39
	action: tensor([[ 0.0029,  0.0189,  0.0105, -0.0075, -0.0103, -0.0216, -0.0202]],
       dtype=torch.float64)
	q_value: tensor([[7.4863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29720352072598744, distance: 0.9593376341258866 entropy -11.061865018067001
epoch: 23, step: 40
	action: tensor([[ 0.0032,  0.0189, -0.0051, -0.0078, -0.0145,  0.0223,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072913545168756, distance: 0.9524276550460417 entropy -11.056172925401299
epoch: 23, step: 41
	action: tensor([[ 0.0030,  0.0188,  0.0039, -0.0078,  0.0036,  0.0033,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[7.4859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30274948567819937, distance: 0.9555449352639168 entropy -11.047096341891091
epoch: 23, step: 42
	action: tensor([[ 0.0032,  0.0186,  0.0016, -0.0081, -0.0153, -0.0072, -0.0397]],
       dtype=torch.float64)
	q_value: tensor([[7.5054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003290531824685, distance: 0.9572020357267256 entropy -11.046995760146016
epoch: 23, step: 43
	action: tensor([[ 0.0029,  0.0192,  0.0121, -0.0075, -0.0027, -0.0112,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997279584046265, distance: 0.9576131186966864 entropy -11.0614142455522
epoch: 23, step: 44
	action: tensor([[ 0.0033,  0.0186, -0.0070, -0.0080, -0.0159,  0.0247, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[7.5147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3076282691108688, distance: 0.9521960094574266 entropy -11.050766933800077
epoch: 23, step: 45
	action: tensor([[ 0.0029,  0.0190,  0.0021, -0.0075, -0.0145,  0.0012,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[7.4735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30248019768600276, distance: 0.9557294399231029 entropy -11.053639795954036
epoch: 23, step: 46
	action: tensor([[ 0.0031,  0.0187,  0.0144, -0.0080,  0.0009,  0.0415,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[7.5038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117447351184208, distance: 0.9493611695948577 entropy -11.046688415753094
epoch: 23, step: 47
	action: tensor([[ 0.0029,  0.0185,  0.0113, -0.0076, -0.0041,  0.0005,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[7.4987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30183571976528134, distance: 0.9561708641729386 entropy -11.05581684151778
epoch: 23, step: 48
	action: tensor([[ 0.0032,  0.0183,  0.0063, -0.0082,  0.0033, -0.0062, -0.0402]],
       dtype=torch.float64)
	q_value: tensor([[7.5275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300244015725772, distance: 0.9572602027507534 entropy -11.04399033977854
epoch: 23, step: 49
	action: tensor([[ 0.0030,  0.0191,  0.0103, -0.0075, -0.0133, -0.0187, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[7.4787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29807138544290956, distance: 0.9587451208600002 entropy -11.061237350178462
epoch: 23, step: 50
	action: tensor([[ 0.0032,  0.0188,  0.0171, -0.0079, -0.0136,  0.0251, -0.0337]],
       dtype=torch.float64)
	q_value: tensor([[7.5092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079337937131472, distance: 0.9519858973205098 entropy -11.052692464716506
epoch: 23, step: 51
	action: tensor([[ 0.0028,  0.0189,  0.0158, -0.0074, -0.0083, -0.0231,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[7.4850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29664991253656536, distance: 0.959715405385622 entropy -11.063790343544744
epoch: 23, step: 52
	action: tensor([[ 0.0034,  0.0182,  0.0045, -0.0084, -0.0144,  0.0108,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[7.5454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30412762997692266, distance: 0.9546001312531668 entropy -11.04131562260215
epoch: 23, step: 53
	action: tensor([[ 0.0031,  0.0185,  0.0007, -0.0080, -0.0181,  0.0285,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[7.5120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3082491510876899, distance: 0.9517689744463219 entropy -11.045234884392727
epoch: 23, step: 54
	action: tensor([[ 0.0030,  0.0186,  0.0178, -0.0079, -0.0093,  0.0142, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[7.4965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049699796830021, distance: 0.9540221871761678 entropy -11.044541557680777
epoch: 23, step: 55
	action: tensor([[ 0.0031,  0.0186,  0.0026, -0.0078, -0.0016,  0.0053,  0.0391]],
       dtype=torch.float64)
	q_value: tensor([[7.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3031473339087082, distance: 0.955272281390577 entropy -11.054440634999603
epoch: 23, step: 56
	action: tensor([[ 0.0032,  0.0183,  0.0212, -0.0083, -0.0076, -0.0160,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.5165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29779364868138125, distance: 0.9589347786290485 entropy -11.039810709185776
epoch: 23, step: 57
	action: tensor([[ 0.0033,  0.0184,  0.0023, -0.0082, -0.0058,  0.0012, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022055364325912, distance: 0.955917589427156 entropy -11.050024500124193
epoch: 23, step: 58
	action: tensor([[ 0.0030,  0.0190,  0.0135, -0.0077, -0.0114, -0.0031,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[7.4859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014971677246403, distance: 0.9564026680422785 entropy -11.054620320484068
epoch: 23, step: 59
	action: tensor([[ 0.0032,  0.0186,  0.0046, -0.0080, -0.0103, -0.0246,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[7.5164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963853907313051, distance: 0.9598958573338049 entropy -11.049182979359879
epoch: 23, step: 60
	action: tensor([[ 0.0033,  0.0185,  0.0088, -0.0083,  0.0040,  0.0040,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.5249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30279847123960757, distance: 0.9555113686144141 entropy -11.040981548515258
epoch: 23, step: 61
	action: tensor([[ 0.0032,  0.0185,  0.0029, -0.0081, -0.0093, -0.0119, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[7.5103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992144303141566, distance: 0.9579641759178059 entropy -11.048172605006041
epoch: 23, step: 62
	action: tensor([[ 0.0031,  0.0190,  0.0226, -0.0078, -0.0044,  0.0083, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[7.4914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041390450455539, distance: 0.9545923016056831 entropy -11.054610915108896
epoch: 23, step: 63
	action: tensor([[ 0.0031,  0.0185,  0.0144, -0.0078,  0.0151, -0.0060,  0.0227]],
       dtype=torch.float64)
	q_value: tensor([[7.5135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30043909407165426, distance: 0.9571267606953344 entropy -11.055144166259286
epoch: 23, step: 64
	action: tensor([[ 0.0033,  0.0182,  0.0139, -0.0083, -0.0103,  0.0038,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[7.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30253475952714426, distance: 0.9556920593516132 entropy -11.046653301992317
epoch: 23, step: 65
	action: tensor([[ 0.0032,  0.0186,  0.0021, -0.0079, -0.0069, -0.0111,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[7.5127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29945386764324167, distance: 0.9578005081788278 entropy -11.052116294297141
epoch: 23, step: 66
	action: tensor([[ 0.0032,  0.0186,  0.0075, -0.0082, -0.0189,  0.0070, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[7.5121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30352040797816404, distance: 0.9550165350561288 entropy -11.043797548087252
epoch: 23, step: 67
	action: tensor([[ 0.0030,  0.0189,  0.0168, -0.0077, -0.0027, -0.0268,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[7.4922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29587827117177223, distance: 0.9602417102192987 entropy -11.054582959664534
epoch: 23, step: 68
	action: tensor([[ 0.0034,  0.0182,  0.0010, -0.0084, -0.0006,  0.0048, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30283662646995313, distance: 0.9554852224761197 entropy -11.043076666929773
epoch: 23, step: 69
	action: tensor([[ 0.0031,  0.0189,  0.0168, -0.0077, -0.0102,  0.0083, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[7.4911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304037106267546, distance: 0.9546622196032957 entropy -11.0532467906807
epoch: 23, step: 70
	action: tensor([[ 0.0029,  0.0189,  0.0006, -0.0075, -0.0024,  0.0272,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[7.4905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30839930864461274, distance: 0.9516656691445364 entropy -11.061361010843438
epoch: 23, step: 71
	action: tensor([[ 0.0030,  0.0185,  0.0145, -0.0080, -0.0178,  0.0042, -0.0331]],
       dtype=torch.float64)
	q_value: tensor([[7.4985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026207857350245, distance: 0.9556331194258192 entropy -11.044551242785706
epoch: 23, step: 72
	action: tensor([[ 0.0029,  0.0190,  0.0091, -0.0075, -0.0107,  0.0193, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[7.4859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30661635027522915, distance: 0.9528915847025363 entropy -11.061981270896108
epoch: 23, step: 73
	action: tensor([[ 0.0030,  0.0188,  0.0191, -0.0076,  0.0031,  0.0007,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[7.4922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021516912669572, distance: 0.9559544703073529 entropy -11.05651071205017
epoch: 23, step: 74
	action: tensor([[ 0.0032,  0.0185,  0.0135, -0.0080,  0.0034,  0.0166, -0.0234]],
       dtype=torch.float64)
	q_value: tensor([[7.5203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057739935359689, distance: 0.9534702189220138 entropy -11.053312803119391
epoch: 23, step: 75
	action: tensor([[ 0.0030,  0.0189,  0.0056, -0.0076, -0.0034,  0.0191,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065021483571767, distance: 0.9529700532143548 entropy -11.059560725952151
epoch: 23, step: 76
	action: tensor([[ 0.0030,  0.0187,  0.0050, -0.0078, -0.0125, -0.0165,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[7.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29823464206746564, distance: 0.958633620494357 entropy -11.051889919320415
epoch: 23, step: 77
	action: tensor([[ 0.0033,  0.0186,  0.0171, -0.0081, -0.0061,  0.0079,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[7.5154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037939432219744, distance: 0.9548289801458987 entropy -11.047776273940014
epoch: 23, step: 78
	action: tensor([[ 0.0032,  0.0182,  0.0219, -0.0082, -0.0186,  0.0182, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[7.5302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305723104739993, distance: 0.9535051643589743 entropy -11.044032988015687
epoch: 23, step: 79
	action: tensor([[ 0.0030,  0.0186,  0.0118, -0.0077, -0.0033,  0.0061,  0.0613]],
       dtype=torch.float64)
	q_value: tensor([[7.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033051070259932, distance: 0.955164134557796 entropy -11.057820477277604
epoch: 23, step: 80
	action: tensor([[ 0.0032,  0.0179,  0.0074, -0.0085, -0.0117,  0.0142,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[7.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044602104897436, distance: 0.9543719864397909 entropy -11.035126337606854
epoch: 23, step: 81
	action: tensor([[ 0.0031,  0.0188,  0.0098, -0.0078, -0.0076, -0.0117, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[7.4953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29934148375862224, distance: 0.9578773318312119 entropy -11.051864096371132
epoch: 23, step: 82
	action: tensor([[ 0.0032,  0.0187,  0.0113, -0.0079, -0.0046,  0.0139, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[7.5082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30529064279253904, distance: 0.9538020851622023 entropy -11.051644689393454
epoch: 23, step: 83
	action: tensor([[ 0.0031,  0.0187,  0.0025, -0.0078,  0.0037, -0.0153,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.5037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29848028188484765, distance: 0.958465829939696 entropy -11.053602702182618
epoch: 23, step: 84
	action: tensor([[ 0.0033,  0.0187,  0.0066, -0.0080, -0.0098, -0.0030, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[7.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014019042069769, distance: 0.9564678840814997 entropy -11.04976951914702
epoch: 23, step: 85
	action: tensor([[ 0.0031,  0.0188,  0.0096, -0.0078, -0.0086,  0.0027,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[7.5039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30271318432123706, distance: 0.9555698094845864 entropy -11.052541521535854
epoch: 23, step: 86
	action: tensor([[ 0.0032,  0.0187,  0.0067, -0.0079, -0.0071, -0.0079,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[7.5071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001952059743018, distance: 0.9572935878299054 entropy -11.049910828228565
epoch: 23, step: 87
	action: tensor([[ 0.0033,  0.0184,  0.0087, -0.0083, -0.0110, -0.0091,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[7.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299636221379163, distance: 0.9576758412508798 entropy -11.04194192980758
epoch: 23, step: 88
	action: tensor([[ 0.0032,  0.0187,  0.0062, -0.0079, -0.0041,  0.0247, -0.0349]],
       dtype=torch.float64)
	q_value: tensor([[7.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078245253458971, distance: 0.9520610475287501 entropy -11.050932496957586
epoch: 23, step: 89
	action: tensor([[ 0.0028,  0.0190,  0.0133, -0.0074, -0.0065, -0.0124, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.4731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29926654062605396, distance: 0.9579285582180451 entropy -11.062446800386876
epoch: 23, step: 90
	action: tensor([[ 0.0032,  0.0187,  0.0124, -0.0079, -0.0100,  0.0255,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[7.5134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079284659650271, distance: 0.9519895616601838 entropy -11.054089419390577
epoch: 23, step: 91
	action: tensor([[ 0.0030,  0.0186,  0.0162, -0.0078, -0.0037,  0.0288, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.5036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308606449707831, distance: 0.9515231419581692 entropy -11.05291317405216
epoch: 23, step: 92
	action: tensor([[ 0.0029,  0.0187,  0.0140, -0.0076, -0.0043,  0.0158,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[7.4901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30564808528121745, distance: 0.9535566780343151 entropy -11.059010163893857
epoch: 23, step: 93
	action: tensor([[ 0.0031,  0.0184,  0.0114, -0.0080, -0.0228, -0.0176, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[7.5127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2978022702043196, distance: 0.9589288918241574 entropy -11.048675407616676
epoch: 23, step: 94
	action: tensor([[ 0.0032,  0.0188,  0.0107, -0.0079, -0.0089,  0.0038,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[7.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30294976283798625, distance: 0.9554076907827678 entropy -11.052867168818333
epoch: 23, step: 95
	action: tensor([[ 0.0032,  0.0186,  0.0221, -0.0079, -0.0095,  0.0007,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[7.5136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30204381966065985, distance: 0.9560283519505819 entropy -11.049953861071119
epoch: 23, step: 96
	action: tensor([[ 0.0032,  0.0185,  0.0132, -0.0080, -0.0060, -0.0125,  0.0325]],
       dtype=torch.float64)
	q_value: tensor([[7.5209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29895881147431025, distance: 0.9581388736968899 entropy -11.052305243491867
epoch: 23, step: 97
	action: tensor([[ 0.0033,  0.0183,  0.0055, -0.0083, -0.0036,  0.0092,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.5355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038252659753302, distance: 0.9548075007227078 entropy -11.043083300258022
epoch: 23, step: 98
	action: tensor([[ 0.0031,  0.0188,  0.0046, -0.0078, -0.0067,  0.0082,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.4953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039430886599427, distance: 0.9547267000718073 entropy -11.051217026637586
epoch: 23, step: 99
	action: tensor([[ 0.0031,  0.0187,  0.0097, -0.0079,  0.0024, -0.0210,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[7.5027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29705326755464057, distance: 0.9594401786197511 entropy -11.047657900490497
epoch: 23, step: 100
	action: tensor([[ 0.0034,  0.0183,  0.0122, -0.0084, -0.0073, -0.0048, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[7.5318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006184419935437, distance: 0.9570040625159495 entropy -11.04168647102702
epoch: 23, step: 101
	action: tensor([[ 0.0031,  0.0188, -0.0014, -0.0078, -0.0054, -0.0027,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[7.5023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30154805804233853, distance: 0.9563678274375479 entropy -11.054687060658342
epoch: 23, step: 102
	action: tensor([[ 0.0031,  0.0188,  0.0101, -0.0080, -0.0059, -0.0181,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[7.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29787367813133436, distance: 0.958880132861869 entropy -11.04736226252307
epoch: 23, step: 103
	action: tensor([[ 0.0033,  0.0185,  0.0226, -0.0082, -0.0105, -0.0027,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[7.5237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012053292878205, distance: 0.9566024424016955 entropy -11.045459204554527
epoch: 23, step: 104
	action: tensor([[ 0.0033,  0.0182,  0.0057, -0.0083, -0.0013,  0.0008, -0.0178]],
       dtype=torch.float64)
	q_value: tensor([[7.5391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018295536385306, distance: 0.9561750865729474 entropy -11.04494691219377
epoch: 23, step: 105
	action: tensor([[ 0.0030,  0.0190,  0.0003, -0.0077, -0.0081, -0.0120, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[7.4876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995216453151478, distance: 0.9577541735722543 entropy -11.055756213066127
epoch: 23, step: 106
	action: tensor([[ 0.0031,  0.0189,  0.0103, -0.0079, -0.0127, -0.0136, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[7.4954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29910269727923144, distance: 0.9580405416302568 entropy -11.051425394192757
epoch: 23, step: 107
	action: tensor([[ 0.0032,  0.0188,  0.0089, -0.0079, -0.0130,  0.0039, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[7.5073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303028953927021, distance: 0.9553534178476916 entropy -11.052822592631104
epoch: 23, step: 108
	action: tensor([[ 0.0030,  0.0190,  0.0079, -0.0076, -0.0031, -0.0287, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[7.4867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2956112976461952, distance: 0.9604237347249553 entropy -11.058263032101467
epoch: 23, step: 109
	action: tensor([[ 0.0033,  0.0187,  0.0061, -0.0080, -0.0147, -0.0030, -0.0336]],
       dtype=torch.float64)
	q_value: tensor([[7.5166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014801710406566, distance: 0.9564143040545268 entropy -11.051533937274629
epoch: 23, step: 110
	action: tensor([[ 0.0030,  0.0191,  0.0109, -0.0075,  0.0046,  0.0291,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[7.4854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30907067428210233, distance: 0.9512036462117603 entropy -11.060423495844146
epoch: 23, step: 111
	action: tensor([[ 0.0030,  0.0185,  0.0024, -0.0078, -0.0171, -0.0243, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[7.5049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963570041385972, distance: 0.9599152201338135 entropy -11.051714433330146
epoch: 23, step: 112
	action: tensor([[ 0.0031,  0.0191,  0.0134, -0.0077, -0.0020, -0.0131,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[7.4878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993033700863018, distance: 0.9579033842700477 entropy -11.057159970965396
epoch: 23, step: 113
	action: tensor([[ 0.0033,  0.0184,  0.0134, -0.0082, -0.0014, -0.0217, -0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.5305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29678479453585294, distance: 0.9596233782844457 entropy -11.046815713909899
epoch: 23, step: 114
	action: tensor([[ 0.0033,  0.0186,  0.0079, -0.0080, -0.0105,  0.0081, -0.0176]],
       dtype=torch.float64)
	q_value: tensor([[7.5185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039452018891413, distance: 0.9547252507952912 entropy -11.053526816836765
epoch: 23, step: 115
	action: tensor([[ 0.0030,  0.0189,  0.0040, -0.0076, -0.0129,  0.0082,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[7.4876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30406749456056337, distance: 0.9546413773484809 entropy -11.056608715675008
epoch: 23, step: 116
	action: tensor([[ 0.0031,  0.0186,  0.0137, -0.0079, -0.0100, -0.0109, -0.0245]],
       dtype=torch.float64)
	q_value: tensor([[7.5070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299405368168809, distance: 0.957833662324677 entropy -11.047811130230851
epoch: 23, step: 117
	action: tensor([[ 3.0863e-03,  1.8853e-02,  8.8356e-05, -7.7308e-03, -8.0775e-03,
         -6.8704e-03,  2.3835e-02]], dtype=torch.float64)
	q_value: tensor([[7.5028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006322817552812, distance: 0.9569945935976033 entropy -11.058406390274675
epoch: 23, step: 118
	action: tensor([[ 0.0032,  0.0185,  0.0137, -0.0082, -0.0095,  0.0232,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[7.5149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30716716540223954, distance: 0.9525130270470318 entropy -11.0430723950156
epoch: 23, step: 119
	action: tensor([[ 0.0031,  0.0183,  0.0135, -0.0081,  0.0011, -0.0039,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[7.5169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006336289366006, distance: 0.9569936718751486 entropy -11.045817593829284
epoch: 23, step: 120
	action: tensor([[ 0.0033,  0.0184,  0.0204, -0.0082, -0.0032,  0.0248, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[7.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30761595161453414, distance: 0.9522044793430166 entropy -11.047365349386641
epoch: 23, step: 121
	action: tensor([[ 0.0029,  0.0187,  0.0076, -0.0075, -0.0022,  0.0172,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[7.4918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30596735690730026, distance: 0.9533374242312453 entropy -11.06118727157099
epoch: 23, step: 122
	action: tensor([[ 0.0031,  0.0181,  0.0120, -0.0083, -0.0040,  0.0041,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[7.5232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023010782812723, distance: 0.9558521451846977 entropy -11.038822884604356
epoch: 23, step: 123
	action: tensor([[ 0.0032,  0.0185,  0.0125, -0.0081,  0.0034,  0.0180, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.5145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060578559749132, distance: 0.9532752665203861 entropy -11.047799614970314
epoch: 23, step: 124
	action: tensor([[ 0.0031,  0.0187,  0.0044, -0.0078, -0.0017, -0.0071, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[7.4968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30036519046174837, distance: 0.9571773161609752 entropy -11.054592772877033
epoch: 23, step: 125
	action: tensor([[ 0.0031,  0.0190,  0.0180, -0.0077, -0.0046,  0.0103,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[7.4906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30458479881600575, distance: 0.9542865068343577 entropy -11.05516577771204
epoch: 23, step: 126
	action: tensor([[ 0.0032,  0.0182,  0.0003, -0.0082, -0.0019, -0.0011, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[7.5295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013561461033737, distance: 0.9564992078427226 entropy -11.04494958823217
epoch: 23, step: 127
	action: tensor([[ 0.0031,  0.0190, -0.0005, -0.0078, -0.0035, -0.0028,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[7.4872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016081489585146, distance: 0.956326686271183 entropy -11.052931113781515
LOSS epoch 23 actor 26.023491711241554 critic 11.849696731559673
epoch: 24, step: 0
	action: tensor([[ 3.4359e-03,  1.9470e-02,  7.2901e-03, -8.0237e-03, -1.4182e-02,
          1.5856e-03, -1.9372e-05]], dtype=torch.float64)
	q_value: tensor([[7.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30318454150068164, distance: 0.9552467782552224 entropy -11.04520367750782
epoch: 24, step: 1
	action: tensor([[ 0.0034,  0.0194,  0.0078, -0.0080, -0.0131, -0.0061,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[7.3699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301416858885177, distance: 0.9564576466173191 entropy -11.047027919209173
epoch: 24, step: 2
	action: tensor([[ 0.0035,  0.0193,  0.0051, -0.0081, -0.0018, -0.0165,  0.0409]],
       dtype=torch.float64)
	q_value: tensor([[7.3775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989451078570097, distance: 0.958148238270967 entropy -11.044700500732938
epoch: 24, step: 3
	action: tensor([[ 0.0036,  0.0189,  0.0144, -0.0086, -0.0167, -0.0095, -0.0088]],
       dtype=torch.float64)
	q_value: tensor([[7.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001116556506599, distance: 0.9573507321954644 entropy -11.032666330209484
epoch: 24, step: 4
	action: tensor([[ 0.0035,  0.0194,  0.0136, -0.0080, -0.0110,  0.0131,  0.0408]],
       dtype=torch.float64)
	q_value: tensor([[7.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058175820181914, distance: 0.9534402856065773 entropy -11.050899167102594
epoch: 24, step: 5
	action: tensor([[ 0.0034,  0.0188, -0.0088, -0.0084, -0.0067, -0.0204,  0.0451]],
       dtype=torch.float64)
	q_value: tensor([[7.3988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29767924384180566, distance: 0.9590128912175059 entropy -11.03814753508128
epoch: 24, step: 6
	action: tensor([[ 0.0036,  0.0191,  0.0123, -0.0086, -0.0066,  0.0070, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[7.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3040236448982687, distance: 0.9546714521346905 entropy -11.028703863515288
epoch: 24, step: 7
	action: tensor([[ 0.0034,  0.0195,  0.0043, -0.0078, -0.0060,  0.0051, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304055968990267, distance: 0.9546492823828495 entropy -11.05180494881836
epoch: 24, step: 8
	action: tensor([[ 0.0034,  0.0194,  0.0031, -0.0079, -0.0142, -0.0042, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.3673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019118815875508, distance: 0.9561187090400801 entropy -11.048027322982449
epoch: 24, step: 9
	action: tensor([[ 0.0034,  0.0195,  0.0034, -0.0079, -0.0113,  0.0202,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[7.3667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30751846492494006, distance: 0.9522715115020239 entropy -11.048858841659584
epoch: 24, step: 10
	action: tensor([[ 0.0034,  0.0192,  0.0082, -0.0080, -0.0096, -0.0031,  0.0529]],
       dtype=torch.float64)
	q_value: tensor([[7.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30182348679328364, distance: 0.9561792409694185 entropy -11.043173732077898
epoch: 24, step: 11
	action: tensor([[ 0.0035,  0.0187,  0.0067, -0.0086, -0.0013, -0.0063,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[7.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30063349376246573, distance: 0.9569937643594248 entropy -11.031022269886126
epoch: 24, step: 12
	action: tensor([[ 0.0035,  0.0193,  0.0193, -0.0081, -0.0046,  0.0101,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[7.3769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30505143187528894, distance: 0.9539662834934709 entropy -11.046247642695993
epoch: 24, step: 13
	action: tensor([[ 0.0035,  0.0188,  0.0146, -0.0083, -0.0040,  0.0012,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[7.3985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30258244924869226, distance: 0.9556593857037643 entropy -11.04337545065086
epoch: 24, step: 14
	action: tensor([[ 0.0035,  0.0192,  0.0210, -0.0081, -0.0045,  0.0088, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[7.3822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30469548888132514, distance: 0.9542105563523691 entropy -11.047898813580113
epoch: 24, step: 15
	action: tensor([[ 0.0034,  0.0193,  0.0092, -0.0078, -0.0093,  0.0033, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[7.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30350337771990243, distance: 0.9550282109752808 entropy -11.055286559614581
epoch: 24, step: 16
	action: tensor([[ 0.0033,  0.0196, -0.0004, -0.0077,  0.0011, -0.0027, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[7.3589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30232381804646713, distance: 0.9558365682432869 entropy -11.05531675322348
epoch: 24, step: 17
	action: tensor([[ 0.0035,  0.0194,  0.0127, -0.0080, -0.0045, -0.0050,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[7.3666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30161949844244906, distance: 0.9563189156632914 entropy -11.046127602626873
epoch: 24, step: 18
	action: tensor([[ 3.5772e-03,  1.8919e-02,  1.0340e-03, -8.4153e-03,  4.7550e-05,
         -1.2277e-02, -1.1098e-02]], dtype=torch.float64)
	q_value: tensor([[7.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996193964413367, distance: 0.9576873443726928 entropy -11.038929030763208
epoch: 24, step: 19
	action: tensor([[ 0.0035,  0.0196,  0.0062, -0.0080, -0.0082,  0.0036,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[7.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303788351941651, distance: 0.9548328142879017 entropy -11.04849932293486
epoch: 24, step: 20
	action: tensor([[ 0.0035,  0.0193,  0.0060, -0.0080, -0.0106,  0.0081, -0.0380]],
       dtype=torch.float64)
	q_value: tensor([[7.3768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045710184451086, distance: 0.9542959618741936 entropy -11.045194510900943
epoch: 24, step: 21
	action: tensor([[ 0.0032,  0.0197,  0.0050, -0.0075, -0.0025, -0.0124,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[7.3437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300179191291735, distance: 0.957304541359629 entropy -11.057338317203554
epoch: 24, step: 22
	action: tensor([[ 0.0036,  0.0191,  0.0056, -0.0083, -0.0008, -0.0076, -0.0274]],
       dtype=torch.float64)
	q_value: tensor([[7.3864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008729587236185, distance: 0.9568299117208411 entropy -11.041301531583414
epoch: 24, step: 23
	action: tensor([[ 0.0034,  0.0196,  0.0162, -0.0078, -0.0101, -0.0055,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.3598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30167696799102306, distance: 0.9562795672372223 entropy -11.05343285197793
epoch: 24, step: 24
	action: tensor([[ 0.0035,  0.0192, -0.0006, -0.0081, -0.0146, -0.0129,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[7.3833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998609197493092, distance: 0.9575222029062407 entropy -11.047547266691618
epoch: 24, step: 25
	action: tensor([[ 0.0035,  0.0191,  0.0090, -0.0084, -0.0075, -0.0001, -0.0379]],
       dtype=torch.float64)
	q_value: tensor([[7.3899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30244905361700347, distance: 0.9557507762144002 entropy -11.034748334237872
epoch: 24, step: 26
	action: tensor([[ 0.0033,  0.0197, -0.0039, -0.0076, -0.0050,  0.0054,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[7.3506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30428411644388065, distance: 0.9544927908790817 entropy -11.057278934436434
epoch: 24, step: 27
	action: tensor([[ 0.0034,  0.0193,  0.0096, -0.0082, -0.0127, -0.0092,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[7.3690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30045670134125224, distance: 0.9571147156433504 entropy -11.038984034440386
epoch: 24, step: 28
	action: tensor([[ 0.0035,  0.0193,  0.0109, -0.0081,  0.0039, -0.0153,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991797890306297, distance: 0.9579878527028098 entropy -11.047483833155619
epoch: 24, step: 29
	action: tensor([[ 0.0036,  0.0192,  0.0028, -0.0082, -0.0090, -0.0305,  0.0425]],
       dtype=torch.float64)
	q_value: tensor([[7.3869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2958139050886952, distance: 0.9602855986428005 entropy -11.04646238906858
epoch: 24, step: 30
	action: tensor([[ 0.0037,  0.0190,  0.0094, -0.0087, -0.0081,  0.0154,  0.0378]],
       dtype=torch.float64)
	q_value: tensor([[7.4004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30592032449026796, distance: 0.953369726028684 entropy -11.030813758357715
epoch: 24, step: 31
	action: tensor([[ 0.0034,  0.0189,  0.0195, -0.0083, -0.0043, -0.0162, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[7.3909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29842383406131245, distance: 0.958504390667444 entropy -11.038807188808233
epoch: 24, step: 32
	action: tensor([[ 0.0036,  0.0191,  0.0075, -0.0082, -0.0031,  0.0195,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[7.3926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30721060730956673, distance: 0.9524831644097276 entropy -11.049520373563217
epoch: 24, step: 33
	action: tensor([[ 0.0034,  0.0192,  0.0149, -0.0080, -0.0108, -0.0139, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.3743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29935756843388317, distance: 0.957866337007136 entropy -11.045158604769297
epoch: 24, step: 34
	action: tensor([[ 3.4876e-03,  1.9371e-02,  8.6480e-05, -7.9915e-03, -1.2604e-02,
         -1.3659e-02,  5.2412e-04]], dtype=torch.float64)
	q_value: tensor([[7.3803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29978482622988056, distance: 0.957574234893035 entropy -11.0512855869554
epoch: 24, step: 35
	action: tensor([[ 0.0035,  0.0194, -0.0020, -0.0080, -0.0162, -0.0097, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[7.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007318528792371, distance: 0.9569264660470264 entropy -11.045466890750069
epoch: 24, step: 36
	action: tensor([[ 0.0033,  0.0198,  0.0127, -0.0077, -0.0023, -0.0076,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[7.3503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30128465304944396, distance: 0.9565481464397835 entropy -11.051491768624928
epoch: 24, step: 37
	action: tensor([[ 0.0036,  0.0189,  0.0051, -0.0084, -0.0063,  0.0099,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[7.3993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30469564617189615, distance: 0.9542104484224411 entropy -11.03998269127985
epoch: 24, step: 38
	action: tensor([[ 0.0034,  0.0191,  0.0147, -0.0082, -0.0110, -0.0108, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[7.3822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999332520781832, distance: 0.957472740162507 entropy -11.040485803799033
epoch: 24, step: 39
	action: tensor([[ 0.0035,  0.0194,  0.0080, -0.0080, -0.0090, -0.0081,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[7.3751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010117373641098, distance: 0.9567349403230709 entropy -11.050662272653998
epoch: 24, step: 40
	action: tensor([[ 0.0035,  0.0192,  0.0097, -0.0081, -0.0024,  0.0082, -0.0180]],
       dtype=torch.float64)
	q_value: tensor([[7.3826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30461763675187226, distance: 0.9542639755688627 entropy -11.045232753655881
epoch: 24, step: 41
	action: tensor([[ 0.0033,  0.0195,  0.0104, -0.0078, -0.0049,  0.0066,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[7.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30438455960316124, distance: 0.9544238865071218 entropy -11.052146595617378
epoch: 24, step: 42
	action: tensor([[ 0.0035,  0.0191,  0.0148, -0.0082,  0.0020, -0.0027, -0.0180]],
       dtype=torch.float64)
	q_value: tensor([[7.3828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30182214490496717, distance: 0.956180159852474 entropy -11.041932536146183
epoch: 24, step: 43
	action: tensor([[ 0.0034,  0.0194,  0.0142, -0.0079, -0.0028,  0.0054,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[7.3689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3040708181659916, distance: 0.954639097777526 entropy -11.052685292148553
epoch: 24, step: 44
	action: tensor([[ 0.0035,  0.0191,  0.0104, -0.0082, -0.0016,  0.0121, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[7.3835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30535797594948355, distance: 0.953755861470914 entropy -11.044501881418507
epoch: 24, step: 45
	action: tensor([[ 0.0034,  0.0193,  0.0082, -0.0079,  0.0014, -0.0002, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[7.3696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026972028365582, distance: 0.955580760026332 entropy -11.05001014527051
epoch: 24, step: 46
	action: tensor([[ 0.0034,  0.0194,  0.0092, -0.0079, -0.0198,  0.0153,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[7.3700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3063195233278758, distance: 0.953095522044113 entropy -11.049762695076794
epoch: 24, step: 47
	action: tensor([[ 0.0034,  0.0193,  0.0061, -0.0079, -0.0011, -0.0017,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[7.3726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023211097181677, distance: 0.955838423485577 entropy -11.04789472598131
epoch: 24, step: 48
	action: tensor([[ 0.0035,  0.0191,  0.0122, -0.0083, -0.0091, -0.0008,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[7.3830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30228709658884045, distance: 0.9558617226423474 entropy -11.039660821889271
epoch: 24, step: 49
	action: tensor([[ 0.0035,  0.0191,  0.0163, -0.0082,  0.0035, -0.0157, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[7.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988578996343051, distance: 0.9582078311840672 entropy -11.043838096908406
epoch: 24, step: 50
	action: tensor([[ 0.0036,  0.0194,  0.0027, -0.0080, -0.0111, -0.0127, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[7.3770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000096198658577, distance: 0.9574205150930026 entropy -11.05200944810689
epoch: 24, step: 51
	action: tensor([[ 0.0035,  0.0195,  0.0086, -0.0080, -0.0100, -0.0102,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[7.3696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30052754959810124, distance: 0.9570662471451633 entropy -11.04593470612003
epoch: 24, step: 52
	action: tensor([[ 0.0035,  0.0192,  0.0026, -0.0081, -0.0112, -0.0052,  0.0441]],
       dtype=torch.float64)
	q_value: tensor([[7.3840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015386912468073, distance: 0.9563742402422954 entropy -11.04538281618875
epoch: 24, step: 53
	action: tensor([[ 0.0035,  0.0189,  0.0128, -0.0085, -0.0136,  0.0088,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[7.3956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042778098311135, distance: 0.9544971170723751 entropy -11.032988501094405
epoch: 24, step: 54
	action: tensor([[ 0.0035,  0.0191,  0.0133, -0.0081, -0.0168, -0.0158,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[7.3843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29896372731075216, distance: 0.9581355143634257 entropy -11.045334336565645
epoch: 24, step: 55
	action: tensor([[ 0.0036,  0.0190,  0.0194, -0.0084, -0.0050, -0.0100, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[7.3999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30016382695377686, distance: 0.9573150499566805 entropy -11.040626247808254
epoch: 24, step: 56
	action: tensor([[ 0.0034,  0.0194,  0.0050, -0.0078, -0.0225,  0.0147,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.3706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061988322147602, distance: 0.9531784313664434 entropy -11.055921844670069
epoch: 24, step: 57
	action: tensor([[ 0.0034,  0.0193,  0.0159, -0.0079, -0.0013, -0.0055, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[7.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30135663630134757, distance: 0.9564988722825846 entropy -11.045349280039398
epoch: 24, step: 58
	action: tensor([[ 0.0035,  0.0193, -0.0003, -0.0080, -0.0030, -0.0267, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967656326776936, distance: 0.9596364525479537 entropy -11.051759391142872
epoch: 24, step: 59
	action: tensor([[ 0.0036,  0.0195,  0.0011, -0.0082, -0.0077, -0.0174,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989874075990544, distance: 0.958119331807899 entropy -11.04433733703909
epoch: 24, step: 60
	action: tensor([[ 0.0036,  0.0193,  0.0125, -0.0082, -0.0007,  0.0002,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[7.3798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027743540991672, distance: 0.9555278946844523 entropy -11.04292454312724
epoch: 24, step: 61
	action: tensor([[ 0.0035,  0.0192,  0.0120, -0.0081, -0.0081,  0.0017,  0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.3795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030391347569834, distance: 0.9553464402793443 entropy -11.046062811938103
epoch: 24, step: 62
	action: tensor([[ 0.0035,  0.0192,  0.0129, -0.0082, -0.0045, -0.0194,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[7.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981232110577574, distance: 0.9587097266108402 entropy -11.044107970607385
epoch: 24, step: 63
	action: tensor([[ 0.0036,  0.0190,  0.0134, -0.0084, -0.0083, -0.0123,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[7.3946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299694326156943, distance: 0.9576361142562607 entropy -11.04129504759175
epoch: 24, step: 64
	action: tensor([[ 0.0036,  0.0192,  0.0113, -0.0082, -0.0035, -0.0013,  0.0334]],
       dtype=torch.float64)
	q_value: tensor([[7.3839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023916416255208, distance: 0.955790106980635 entropy -11.046145746285484
epoch: 24, step: 65
	action: tensor([[ 0.0036,  0.0189,  0.0060, -0.0084, -0.0047, -0.0269,  0.0735]],
       dtype=torch.float64)
	q_value: tensor([[7.3931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29619384851439046, distance: 0.9600265027659012 entropy -11.038356297075266
epoch: 24, step: 66
	action: tensor([[ 0.0037,  0.0186,  0.0009, -0.0090, -0.0105, -0.0020,  0.0257]],
       dtype=torch.float64)
	q_value: tensor([[7.4193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015541157556729, distance: 0.9563636801123316 entropy -11.02130978819177
epoch: 24, step: 67
	action: tensor([[ 0.0035,  0.0192,  0.0152, -0.0083, -0.0201,  0.0203, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.3810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30717046291573846, distance: 0.9525107603182157 entropy -11.03957473474536
epoch: 24, step: 68
	action: tensor([[ 0.0033,  0.0193, -0.0035, -0.0078, -0.0097,  0.0300,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[7.3717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3095721194029808, distance: 0.9508584133616178 entropy -11.051700772776687
epoch: 24, step: 69
	action: tensor([[ 0.0033,  0.0192,  0.0185, -0.0081, -0.0058,  0.0140,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[7.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30563550514258087, distance: 0.9535653161763141 entropy -11.040608051613772
epoch: 24, step: 70
	action: tensor([[ 0.0035,  0.0189,  0.0151, -0.0082, -0.0024, -0.0044,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[7.3891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013588899760997, distance: 0.9564973295504774 entropy -11.04378520743723
epoch: 24, step: 71
	action: tensor([[ 0.0036,  0.0191,  0.0026, -0.0082, -0.0028,  0.0116,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[7.3882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053265715470812, distance: 0.9537774206289463 entropy -11.046510491464918
epoch: 24, step: 72
	action: tensor([[ 0.0034,  0.0193,  0.0044, -0.0080, -0.0066, -0.0142,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[7.3697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29945575607153374, distance: 0.9577992172297288 entropy -11.044754877796716
epoch: 24, step: 73
	action: tensor([[ 0.0036,  0.0191,  0.0101, -0.0083, -0.0123, -0.0054,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[7.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013214191699928, distance: 0.9565229795193292 entropy -11.039573667087216
epoch: 24, step: 74
	action: tensor([[ 0.0035,  0.0191,  0.0209, -0.0083, -0.0092, -0.0079,  0.0567]],
       dtype=torch.float64)
	q_value: tensor([[7.3907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30064059381212327, distance: 0.9569889065914066 entropy -11.04213000801191
epoch: 24, step: 75
	action: tensor([[ 0.0036,  0.0186,  0.0033, -0.0087, -0.0135,  0.0042, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.4187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029902857078387, distance: 0.955379919165927 entropy -11.032578234085657
epoch: 24, step: 76
	action: tensor([[ 0.0034,  0.0196,  0.0108, -0.0078, -0.0070,  0.0208,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[7.3558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30775760205957936, distance: 0.9521070716342002 entropy -11.05011473294587
epoch: 24, step: 77
	action: tensor([[ 0.0034,  0.0189,  0.0111, -0.0083, -0.0031,  0.0131,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[7.3846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052917202120029, distance: 0.953801345539712 entropy -11.04003249971183
epoch: 24, step: 78
	action: tensor([[ 0.0035,  0.0191,  0.0059, -0.0081, -0.0055,  0.0215,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[7.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3075575157631706, distance: 0.9522446605874896 entropy -11.04586365939181
epoch: 24, step: 79
	action: tensor([[ 0.0033,  0.0194,  0.0122, -0.0079,  0.0019,  0.0050,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038632622137728, distance: 0.9547814443415149 entropy -11.047339473091695
epoch: 24, step: 80
	action: tensor([[ 3.5032e-03,  1.9169e-02,  5.0452e-03, -8.0709e-03, -6.9163e-05,
         -1.2635e-02, -4.9450e-03]], dtype=torch.float64)
	q_value: tensor([[7.3803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29976689464910866, distance: 0.9575864959168261 entropy -11.047043671824806
epoch: 24, step: 81
	action: tensor([[ 0.0035,  0.0194,  0.0017, -0.0081, -0.0092, -0.0098,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[7.3696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30067359533091687, distance: 0.9569663270281012 entropy -11.047685931191465
epoch: 24, step: 82
	action: tensor([[ 0.0035,  0.0193,  0.0127, -0.0082, -0.0075,  0.0146,  0.0489]],
       dtype=torch.float64)
	q_value: tensor([[7.3791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30603790089531624, distance: 0.9532889726681048 entropy -11.04262871655121
epoch: 24, step: 83
	action: tensor([[ 0.0034,  0.0187,  0.0064, -0.0085, -0.0119, -0.0227, -0.0177]],
       dtype=torch.float64)
	q_value: tensor([[7.3972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969403288232355, distance: 0.9595172496122287 entropy -11.035444116200443
epoch: 24, step: 84
	action: tensor([[ 0.0035,  0.0195, -0.0004, -0.0079, -0.0002,  0.0326,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3105432897552971, distance: 0.9501894293233749 entropy -11.051031627057984
epoch: 24, step: 85
	action: tensor([[ 0.0033,  0.0193,  0.0048, -0.0079, -0.0098,  0.0145, -0.0302]],
       dtype=torch.float64)
	q_value: tensor([[7.3607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059150623278367, distance: 0.9533733400063178 entropy -11.045173906610898
epoch: 24, step: 86
	action: tensor([[ 0.0032,  0.0197,  0.0161, -0.0076, -0.0083, -0.0093, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[7.3438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30080941644253867, distance: 0.9568733929268242 entropy -11.05547097630588
epoch: 24, step: 87
	action: tensor([[ 0.0035,  0.0193,  0.0033, -0.0081,  0.0037, -0.0266,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[7.3827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966694577001556, distance: 0.9597020706855572 entropy -11.048442883461172
epoch: 24, step: 88
	action: tensor([[ 0.0036,  0.0192,  0.0068, -0.0084,  0.0074, -0.0061, -0.0402]],
       dtype=torch.float64)
	q_value: tensor([[7.3840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30129280371557876, distance: 0.9565425672526691 entropy -11.040655055537338
epoch: 24, step: 89
	action: tensor([[ 0.0033,  0.0196,  0.0033, -0.0077, -0.0113, -0.0119,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[7.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30034013841658824, distance: 0.9571944529831927 entropy -11.05743640897029
epoch: 24, step: 90
	action: tensor([[ 0.0036,  0.0190,  0.0206, -0.0085, -0.0058,  0.0033, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030896313474136, distance: 0.9553118310106176 entropy -11.034172588125013
epoch: 24, step: 91
	action: tensor([[ 0.0035,  0.0192, -0.0030, -0.0080,  0.0008,  0.0038,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[7.3803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036297563017091, distance: 0.9549415626107917 entropy -11.050401585136218
epoch: 24, step: 92
	action: tensor([[ 0.0034,  0.0194,  0.0147, -0.0081, -0.0058, -0.0079, -0.0284]],
       dtype=torch.float64)
	q_value: tensor([[7.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30086055305182835, distance: 0.9568384009255823 entropy -11.042992661362332
epoch: 24, step: 93
	action: tensor([[ 0.0034,  0.0195,  0.0221, -0.0078, -0.0017, -0.0040,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[7.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018851963579354, distance: 0.9561369832398509 entropy -11.054682425079534
epoch: 24, step: 94
	action: tensor([[ 0.0036,  0.0190,  0.0199, -0.0082, -0.0096,  0.0183, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[7.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30676411160529626, distance: 0.9527900478188548 entropy -11.048006978340078
epoch: 24, step: 95
	action: tensor([[ 0.0033,  0.0194,  0.0151, -0.0077, -0.0107, -0.0443,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[7.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926095162024631, distance: 0.9624680017439203 entropy -11.056989985961888
epoch: 24, step: 96
	action: tensor([[ 0.0037,  0.0193,  0.0153, -0.0083, -0.0159,  0.0030,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[7.3960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30344668418015663, distance: 0.9550670789513385 entropy -11.044986932889518
epoch: 24, step: 97
	action: tensor([[ 0.0035,  0.0192,  0.0001, -0.0081, -0.0068, -0.0206,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[7.3830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980169103393936, distance: 0.9587823231661472 entropy -11.046143277128126
epoch: 24, step: 98
	action: tensor([[ 0.0036,  0.0191,  0.0107, -0.0084, -0.0174, -0.0094, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.3898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30040964833147077, distance: 0.9571469040517702 entropy -11.036466098724343
epoch: 24, step: 99
	action: tensor([[ 0.0035,  0.0194,  0.0186, -0.0080, -0.0054,  0.0032,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.3806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034823612755746, distance: 0.9550426196207625 entropy -11.048242840242068
epoch: 24, step: 100
	action: tensor([[ 0.0035,  0.0190, -0.0014, -0.0082, -0.0011, -0.0218,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[7.3891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976523149425836, distance: 0.9590312766302441 entropy -11.04542815045922
epoch: 24, step: 101
	action: tensor([[ 0.0036,  0.0193,  0.0069, -0.0083, -0.0060,  0.0210, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.3756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30761522363872584, distance: 0.9522049799189939 entropy -11.040700147981992
epoch: 24, step: 102
	action: tensor([[ 0.0033,  0.0194, -0.0029, -0.0078, -0.0066,  0.0033,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[7.3631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035790855888628, distance: 0.9549763046822365 entropy -11.050276123395658
epoch: 24, step: 103
	action: tensor([[ 0.0034,  0.0194,  0.0146, -0.0081, -0.0051,  0.0030,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[7.3632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034025111344061, distance: 0.9550973620265734 entropy -11.04263366506824
epoch: 24, step: 104
	action: tensor([[ 0.0035,  0.0191, -0.0023, -0.0081, -0.0145, -0.0003,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[7.3883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025578858481781, distance: 0.9556762149601623 entropy -11.045557596005
epoch: 24, step: 105
	action: tensor([[ 0.0034,  0.0192,  0.0051, -0.0083, -0.0118, -0.0046,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.3752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014948514443109, distance: 0.9564042537873522 entropy -11.037239794316818
epoch: 24, step: 106
	action: tensor([[ 0.0035,  0.0194,  0.0148, -0.0080, -0.0110, -0.0112, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[7.3752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30018667229538365, distance: 0.9572994246084429 entropy -11.046297988914471
epoch: 24, step: 107
	action: tensor([[ 3.5251e-03,  1.9307e-02,  2.8265e-03, -8.0586e-03,  6.5802e-03,
         -6.8462e-05, -2.4696e-03]], dtype=torch.float64)
	q_value: tensor([[7.3810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027989487917695, distance: 0.9555110413728783 entropy -11.048485053924313
epoch: 24, step: 108
	action: tensor([[ 3.4675e-03,  1.9404e-02,  3.9487e-05, -8.0484e-03, -7.7502e-03,
         -1.4173e-03, -5.7968e-03]], dtype=torch.float64)
	q_value: tensor([[7.3651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025236436763943, distance: 0.9556996749913262 entropy -11.046491533397445
epoch: 24, step: 109
	action: tensor([[ 0.0034,  0.0196,  0.0090, -0.0079, -0.0121, -0.0094,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[7.3598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007591547190086, distance: 0.9569077850101647 entropy -11.047109063902218
epoch: 24, step: 110
	action: tensor([[ 0.0036,  0.0189,  0.0032, -0.0085, -0.0088,  0.0180,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[7.3970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064785077362081, distance: 0.952986295955249 entropy -11.0352319652514
epoch: 24, step: 111
	action: tensor([[ 0.0034,  0.0194,  0.0119, -0.0080, -0.0103, -0.0094,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[7.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300507233275384, distance: 0.9570801461382833 entropy -11.044809612197579
epoch: 24, step: 112
	action: tensor([[ 0.0036,  0.0191,  0.0113, -0.0082, -0.0052, -0.0115, -0.0204]],
       dtype=torch.float64)
	q_value: tensor([[7.3876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999644136860745, distance: 0.9574514302501211 entropy -11.042814821020913
epoch: 24, step: 113
	action: tensor([[ 3.4514e-03,  1.9477e-02,  1.6373e-02, -7.8954e-03, -9.2527e-06,
         -2.6856e-04, -1.0174e-02]], dtype=torch.float64)
	q_value: tensor([[7.3716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30279657515487723, distance: 0.9555126679010687 entropy -11.052547941871916
epoch: 24, step: 114
	action: tensor([[ 0.0035,  0.0193, -0.0030, -0.0080, -0.0011, -0.0082,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[7.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30094237819663994, distance: 0.9567824065656594 entropy -11.05143219648848
epoch: 24, step: 115
	action: tensor([[ 0.0035,  0.0194,  0.0068, -0.0081, -0.0025,  0.0061, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[7.3710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30416048255735606, distance: 0.9545775973452998 entropy -11.042865724481757
epoch: 24, step: 116
	action: tensor([[ 0.0034,  0.0196,  0.0221, -0.0078, -0.0113, -0.0096, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[7.3574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006387038360431, distance: 0.9569901996925678 entropy -11.05160200861303
epoch: 24, step: 117
	action: tensor([[ 0.0035,  0.0192,  0.0186, -0.0081, -0.0146,  0.0186, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.3891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30696136761702786, distance: 0.9526544828982803 entropy -11.049652884053717
epoch: 24, step: 118
	action: tensor([[ 0.0034,  0.0193,  0.0077, -0.0079, -0.0090, -0.0241, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[7.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972232351359828, distance: 0.959324178660019 entropy -11.050798687801661
epoch: 24, step: 119
	action: tensor([[ 0.0035,  0.0196,  0.0152, -0.0080, -0.0112, -0.0008, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[7.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027688391967194, distance: 0.9555316736853654 entropy -11.051053155083865
epoch: 24, step: 120
	action: tensor([[ 0.0034,  0.0194, -0.0057, -0.0079, -0.0172,  0.0071,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.3704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30448093669107124, distance: 0.9543577667980166 entropy -11.052170876328265
epoch: 24, step: 121
	action: tensor([[ 0.0033,  0.0196,  0.0117, -0.0079, -0.0066,  0.0151,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[7.3558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3063033547437486, distance: 0.9531066295466678 entropy -11.044185660029228
epoch: 24, step: 122
	action: tensor([[ 0.0034,  0.0191,  0.0140, -0.0081, -0.0168, -0.0049,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[7.3822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30140503786816564, distance: 0.9564657388926809 entropy -11.045156116182545
epoch: 24, step: 123
	action: tensor([[ 0.0035,  0.0191,  0.0161, -0.0082, -0.0074,  0.0232,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.3936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30790562075349337, distance: 0.9520052740702233 entropy -11.043453909764164
epoch: 24, step: 124
	action: tensor([[ 0.0034,  0.0191,  0.0086, -0.0079, -0.0063, -0.0023, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020435588710806, distance: 0.9560285305593651 entropy -11.048610632166532
epoch: 24, step: 125
	action: tensor([[ 0.0034,  0.0194,  0.0178, -0.0079, -0.0045,  0.0033,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[7.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035559141097617, distance: 0.9549921916472655 entropy -11.04896364906011
epoch: 24, step: 126
	action: tensor([[ 0.0035,  0.0188,  0.0046, -0.0085, -0.0108,  0.0015, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[7.4018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30260073276937016, distance: 0.9556468588232468 entropy -11.038423448909194
epoch: 24, step: 127
	action: tensor([[ 0.0034,  0.0196,  0.0120, -0.0078, -0.0087,  0.0016,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[7.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30328924028678983, distance: 0.9551750110936262 entropy -11.051592947898925
LOSS epoch 24 actor 25.103646808854226 critic 11.976532564054882
epoch: 25, step: 0
	action: tensor([[ 0.0038,  0.0190,  0.0045, -0.0083,  0.0021, -0.0073, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[7.0306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301015596426072, distance: 0.9567322992883717 entropy -11.042113106398785
epoch: 25, step: 1
	action: tensor([[ 0.0038,  0.0192,  0.0118, -0.0083,  0.0039,  0.0044, -0.0169]],
       dtype=torch.float64)
	q_value: tensor([[7.0148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303896995020969, distance: 0.9547583110640938 entropy -11.043609314284028
epoch: 25, step: 2
	action: tensor([[ 0.0037,  0.0191, -0.0068, -0.0081, -0.0165, -0.0101, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[7.0176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30065036874779716, distance: 0.9569822186584162 entropy -11.04765880304305
epoch: 25, step: 3
	action: tensor([[ 0.0037,  0.0194,  0.0187, -0.0082, -0.0112,  0.0140,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[7.0106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.306160730322495, distance: 0.9532046041411363 entropy -11.041767149391188
epoch: 25, step: 4
	action: tensor([[ 0.0038,  0.0188,  0.0227, -0.0084, -0.0073,  0.0120,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[7.0352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052034076219754, distance: 0.953861968100911 entropy -11.040257764485508
epoch: 25, step: 5
	action: tensor([[ 0.0038,  0.0189,  0.0143, -0.0083, -0.0031,  0.0204, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.0306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3073351618619794, distance: 0.9523975384978244 entropy -11.044728010648893
epoch: 25, step: 6
	action: tensor([[ 0.0037,  0.0191,  0.0012, -0.0081, -0.0008,  0.0048,  0.0523]],
       dtype=torch.float64)
	q_value: tensor([[7.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30387173378566745, distance: 0.9547756347614291 entropy -11.047265592950373
epoch: 25, step: 7
	action: tensor([[ 0.0037,  0.0186,  0.0039, -0.0088, -0.0141,  0.0073, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[7.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30378837707413375, distance: 0.9548327970536888 entropy -11.025901855502445
epoch: 25, step: 8
	action: tensor([[ 0.0037,  0.0193,  0.0057, -0.0081, -0.0051, -0.0199,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[7.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29834065567322776, distance: 0.9585612087943334 entropy -11.044738336509925
epoch: 25, step: 9
	action: tensor([[ 0.0039,  0.0188,  0.0035, -0.0087, -0.0122,  0.0241, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[7.0392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30805425843239176, distance: 0.9519030398541853 entropy -11.032605381048223
epoch: 25, step: 10
	action: tensor([[ 0.0036,  0.0193,  0.0057, -0.0080, -0.0049, -0.0038, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[7.0013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302036097192116, distance: 0.9560336408774953 entropy -11.046360218791493
epoch: 25, step: 11
	action: tensor([[ 0.0038,  0.0192,  0.0166, -0.0083, -0.0122,  0.0037,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[7.0184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30361013122405345, distance: 0.9549550185779702 entropy -11.042048014524553
epoch: 25, step: 12
	action: tensor([[ 0.0038,  0.0188,  0.0097, -0.0084, -0.0039, -0.0043, -0.0105]],
       dtype=torch.float64)
	q_value: tensor([[7.0386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30157625717760783, distance: 0.95634852115724 entropy -11.040826911359963
epoch: 25, step: 13
	action: tensor([[ 0.0038,  0.0191,  0.0170, -0.0082, -0.0105, -0.0227,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[7.0215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29756735593438377, distance: 0.9590892791580408 entropy -11.045659055666993
epoch: 25, step: 14
	action: tensor([[ 0.0040,  0.0186,  0.0083, -0.0088, -0.0071, -0.0054,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[7.0586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30113723622901056, distance: 0.9566490486526843 entropy -11.033159301760781
epoch: 25, step: 15
	action: tensor([[ 0.0039,  0.0188,  0.0168, -0.0086, -0.0137,  0.0016, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[7.0388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302815554674134, distance: 0.955499662159949 entropy -11.03533860386734
epoch: 25, step: 16
	action: tensor([[ 0.0037,  0.0192,  0.0210, -0.0081, -0.0060,  0.0152,  0.0546]],
       dtype=torch.float64)
	q_value: tensor([[7.0177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30636005376634223, distance: 0.9530676778528862 entropy -11.049219815906227
epoch: 25, step: 17
	action: tensor([[ 0.0038,  0.0183,  0.0026, -0.0088, -0.0048, -0.0097, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[7.0540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997583873469153, distance: 0.9575923128747016 entropy -11.031641007014324
epoch: 25, step: 18
	action: tensor([[ 0.0038,  0.0193,  0.0121, -0.0082, -0.0261, -0.0064,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[7.0116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015019790409724, distance: 0.9563993741660849 entropy -11.043916659382822
epoch: 25, step: 19
	action: tensor([[ 0.0038,  0.0190,  0.0118, -0.0084, -0.0002,  0.0179,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[7.0379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068328583918445, distance: 0.952742803526453 entropy -11.04057624172839
epoch: 25, step: 20
	action: tensor([[ 0.0037,  0.0189,  0.0084, -0.0083, -0.0098,  0.0005, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.0210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30271065439058653, distance: 0.9555715430059312 entropy -11.041699262455051
epoch: 25, step: 21
	action: tensor([[ 0.0037,  0.0192,  0.0070, -0.0082,  0.0017,  0.0031, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[7.0189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303586572005604, distance: 0.9549711717449133 entropy -11.045580715883363
epoch: 25, step: 22
	action: tensor([[ 0.0038,  0.0191,  0.0130, -0.0082, -0.0045, -0.0050,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.0205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016110132630814, distance: 0.9563247251846532 entropy -11.043189538974335
epoch: 25, step: 23
	action: tensor([[ 0.0038,  0.0190,  0.0113, -0.0083, -0.0119,  0.0051, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[7.0318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30387050837495566, distance: 0.9547764751178922 entropy -11.042826002317184
epoch: 25, step: 24
	action: tensor([[ 0.0037,  0.0192,  0.0060, -0.0081, -0.0054,  0.0087, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[7.0162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048989303871672, distance: 0.9540709482875237 entropy -11.048351492833744
epoch: 25, step: 25
	action: tensor([[ 0.0037,  0.0192,  0.0104, -0.0081, -0.0146,  0.0121,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[7.0118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30562769874265205, distance: 0.9535706763951541 entropy -11.04648451063854
epoch: 25, step: 26
	action: tensor([[ 0.0037,  0.0189,  0.0176, -0.0084,  0.0023,  0.0244, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[7.0324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30822970292216234, distance: 0.9517823535628704 entropy -11.038706181909882
epoch: 25, step: 27
	action: tensor([[ 0.0037,  0.0190, -0.0054, -0.0081, -0.0069, -0.0231,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[7.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29755581370126705, distance: 0.9590971589075503 entropy -11.048884940057006
epoch: 25, step: 28
	action: tensor([[ 0.0039,  0.0190,  0.0044, -0.0086, -0.0120,  0.0354, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[7.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.310798027751492, distance: 0.950013876812501 entropy -11.032342594097925
epoch: 25, step: 29
	action: tensor([[ 0.0034,  0.0194,  0.0055, -0.0077, -0.0017, -0.0170, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[6.9920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29903831392169067, distance: 0.9580845426915073 entropy -11.052581264364841
epoch: 25, step: 30
	action: tensor([[ 0.0038,  0.0192,  0.0021, -0.0083, -0.0036, -0.0185,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[7.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29867395332823243, distance: 0.9583335169986414 entropy -11.043700556751674
epoch: 25, step: 31
	action: tensor([[ 0.0039,  0.0188,  0.0162, -0.0087, -0.0162,  0.0099,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[7.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047056480968421, distance: 0.9542035852584484 entropy -11.031086488976452
epoch: 25, step: 32
	action: tensor([[ 0.0038,  0.0185,  0.0038, -0.0087, -0.0156,  0.0092,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[7.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30425391027749116, distance: 0.9545135114460539 entropy -11.032878451847639
epoch: 25, step: 33
	action: tensor([[ 0.0037,  0.0191,  0.0156, -0.0082, -0.0011, -0.0033, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[7.0189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30196335441246, distance: 0.956083459164866 entropy -11.041407381691288
epoch: 25, step: 34
	action: tensor([[ 0.0038,  0.0191,  0.0003, -0.0082, -0.0120,  0.0171,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.0214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067493108459819, distance: 0.9528002189172798 entropy -11.047091213086578
epoch: 25, step: 35
	action: tensor([[ 0.0036,  0.0191,  0.0126, -0.0082, -0.0076,  0.0057,  0.0473]],
       dtype=torch.float64)
	q_value: tensor([[7.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039499175067143, distance: 0.9547220167634253 entropy -11.039741684239129
epoch: 25, step: 36
	action: tensor([[ 0.0038,  0.0185,  0.0022, -0.0088, -0.0041,  0.0102,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[7.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044842272564523, distance: 0.9543555092176651 entropy -11.030047219146017
epoch: 25, step: 37
	action: tensor([[ 0.0037,  0.0190, -0.0123, -0.0083, -0.0078,  0.0033,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[7.0192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034927165612116, distance: 0.955035520176807 entropy -11.038971673541708
epoch: 25, step: 38
	action: tensor([[ 0.0037,  0.0192,  0.0071, -0.0084, -0.0015,  0.0184, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[7.0071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3069790987764951, distance: 0.9526422961486479 entropy -11.034291761628905
epoch: 25, step: 39
	action: tensor([[ 0.0037,  0.0192, -0.0223, -0.0080, -0.0093,  0.0275,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[7.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090736114609717, distance: 0.9512016243998939 entropy -11.04682242584168
epoch: 25, step: 40
	action: tensor([[ 0.0034,  0.0195,  0.0066, -0.0081, -0.0038,  0.0011,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[6.9844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30304040049277337, distance: 0.9553455727867846 entropy -11.03576127713134
epoch: 25, step: 41
	action: tensor([[ 0.0038,  0.0188, -0.0068, -0.0086, -0.0081, -0.0133, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[7.0323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29954184727943056, distance: 0.9577403625422753 entropy -11.034371062327903
epoch: 25, step: 42
	action: tensor([[ 0.0037,  0.0194,  0.0170, -0.0082, -0.0179,  0.0074, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[7.0090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30467034443293417, distance: 0.9542278098584169 entropy -11.042916931102871
epoch: 25, step: 43
	action: tensor([[ 0.0036,  0.0193, -0.0008, -0.0080, -0.0115, -0.0317,  0.0488]],
       dtype=torch.float64)
	q_value: tensor([[7.0145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2958061169159589, distance: 0.9602909089219255 entropy -11.051105654800079
epoch: 25, step: 44
	action: tensor([[ 0.0039,  0.0187,  0.0115, -0.0090, -0.0012, -0.0052,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[7.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010770324845039, distance: 0.9566902531763667 entropy -11.022295688714745
epoch: 25, step: 45
	action: tensor([[ 0.0039,  0.0186,  0.0116, -0.0088, -0.0160, -0.0017,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[7.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30188162160155585, distance: 0.9561394312271928 entropy -11.032910185474615
epoch: 25, step: 46
	action: tensor([[ 0.0038,  0.0186,  0.0095, -0.0087, -0.0102, -0.0323,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[7.0449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29491487336651345, distance: 0.9608983996096767 entropy -11.030750747189135
epoch: 25, step: 47
	action: tensor([[ 0.0040,  0.0189,  0.0120, -0.0087,  0.0029, -0.0027,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[7.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30195898196189863, distance: 0.9560864535786534 entropy -11.034045759894088
epoch: 25, step: 48
	action: tensor([[ 0.0039,  0.0189,  0.0020, -0.0085, -0.0021,  0.0050, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.0298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303804823644531, distance: 0.954821519004716 entropy -11.040171210383308
epoch: 25, step: 49
	action: tensor([[ 0.0037,  0.0193,  0.0109, -0.0081, -0.0129, -0.0042,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019292235901625, distance: 0.9561068329490178 entropy -11.044477765909182
epoch: 25, step: 50
	action: tensor([[ 0.0038,  0.0190,  0.0153, -0.0084, -0.0096,  0.0012,  0.0484]],
       dtype=torch.float64)
	q_value: tensor([[7.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028848436525927, distance: 0.9554521802912852 entropy -11.039627054065921
epoch: 25, step: 51
	action: tensor([[ 0.0038,  0.0184,  0.0064, -0.0088, -0.0030, -0.0214, -0.0001]],
       dtype=torch.float64)
	q_value: tensor([[7.0555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29720659972514696, distance: 0.9593355326618841 entropy -11.03014409669662
epoch: 25, step: 52
	action: tensor([[ 0.0039,  0.0191,  0.0062, -0.0084, -0.0076,  0.0060, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[7.0293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042319748739193, distance: 0.9545285582241605 entropy -11.041681490198016
epoch: 25, step: 53
	action: tensor([[ 0.0036,  0.0194,  0.0282, -0.0080, -0.0077, -0.0051, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.0039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017071879710377, distance: 0.9562588754794388 entropy -11.048381620409367
epoch: 25, step: 54
	action: tensor([[ 0.0038,  0.0189,  0.0026, -0.0083, -0.0018,  0.0092,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[7.0402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047962143048275, distance: 0.9541414378999639 entropy -11.049064207405559
epoch: 25, step: 55
	action: tensor([[ 0.0037,  0.0190,  0.0019, -0.0083, -0.0169, -0.0108, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30027738814097826, distance: 0.9572373758892739 entropy -11.038757953208634
epoch: 25, step: 56
	action: tensor([[ 0.0037,  0.0193,  0.0116, -0.0082, -0.0106, -0.0166,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.0164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29906715063116096, distance: 0.9580648352736086 entropy -11.042210031138472
epoch: 25, step: 57
	action: tensor([[ 0.0039,  0.0189, -0.0044, -0.0085,  0.0029, -0.0060,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.0425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013002340582136, distance: 0.9565374811035976 entropy -11.038128781654672
epoch: 25, step: 58
	action: tensor([[ 0.0038,  0.0192,  0.0007, -0.0084, -0.0081, -0.0285, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[7.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29640417359550875, distance: 0.9598830451255119 entropy -11.038729987676689
epoch: 25, step: 59
	action: tensor([[ 0.0039,  0.0193,  0.0022, -0.0084, -0.0039,  0.0027,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[7.0216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035817951350751, distance: 0.9549744469300551 entropy -11.040801058428034
epoch: 25, step: 60
	action: tensor([[ 0.0038,  0.0190,  0.0065, -0.0084, -0.0157,  0.0090, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[7.0258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046948284745966, distance: 0.9542110095113175 entropy -11.037326627260182
epoch: 25, step: 61
	action: tensor([[ 0.0036,  0.0194,  0.0126, -0.0079, -0.0045, -0.0036,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[7.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021154922857361, distance: 0.9559792637536814 entropy -11.048960101917668
epoch: 25, step: 62
	action: tensor([[ 3.8615e-03,  1.8665e-02, -3.4318e-03, -8.6405e-03, -7.7839e-05,
          5.4879e-03,  3.4120e-03]], dtype=torch.float64)
	q_value: tensor([[7.0449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036954364470481, distance: 0.954896527530545 entropy -11.0351510062027
epoch: 25, step: 63
	action: tensor([[ 0.0037,  0.0192,  0.0044, -0.0083, -0.0211,  0.0086,  0.0282]],
       dtype=torch.float64)
	q_value: tensor([[7.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047317081456973, distance: 0.9541857030298716 entropy -11.040067931670423
epoch: 25, step: 64
	action: tensor([[ 0.0037,  0.0189,  0.0146, -0.0085, -0.0152,  0.0181,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[7.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30664398909525004, distance: 0.952872593007624 entropy -11.035154743780694
epoch: 25, step: 65
	action: tensor([[ 0.0037,  0.0189,  0.0110, -0.0082,  0.0030, -0.0114,  0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999212837186581, distance: 0.9574809246170942 entropy -11.042777159762057
epoch: 25, step: 66
	action: tensor([[ 0.0039,  0.0188,  0.0028, -0.0086, -0.0069,  0.0211,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[7.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30741826425418894, distance: 0.9523404048861072 entropy -11.037550722510233
epoch: 25, step: 67
	action: tensor([[ 0.0036,  0.0188,  0.0069, -0.0085, -0.0088,  0.0076, -0.0387]],
       dtype=torch.float64)
	q_value: tensor([[7.0234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30407714007587905, distance: 0.9546347617357167 entropy -11.033652043777963
epoch: 25, step: 68
	action: tensor([[ 0.0035,  0.0195, -0.0029, -0.0078, -0.0023, -0.0025, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[6.9953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30252112671448284, distance: 0.9557013993920811 entropy -11.05309785843102
epoch: 25, step: 69
	action: tensor([[ 0.0037,  0.0194,  0.0042, -0.0081, -0.0094,  0.0062,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[7.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30440052596657796, distance: 0.9544129330656193 entropy -11.043102828936105
epoch: 25, step: 70
	action: tensor([[ 0.0037,  0.0191,  0.0145, -0.0083, -0.0187,  0.0086,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[7.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30461625738745346, distance: 0.9542649220101235 entropy -11.038535857206101
epoch: 25, step: 71
	action: tensor([[ 0.0037,  0.0189,  0.0144, -0.0083, -0.0101, -0.0032, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[7.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018007580179599, distance: 0.956194804803101 entropy -11.041050961126425
epoch: 25, step: 72
	action: tensor([[ 0.0038,  0.0191, -0.0059, -0.0082, -0.0064, -0.0232,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[7.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976854313491245, distance: 0.959008666714456 entropy -11.047236018014816
epoch: 25, step: 73
	action: tensor([[ 0.0038,  0.0192,  0.0194, -0.0086, -0.0069, -0.0083,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[7.0233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007453694558454, distance: 0.9569172174973499 entropy -11.033590174941073
epoch: 25, step: 74
	action: tensor([[ 0.0039,  0.0186,  0.0135, -0.0087, -0.0075, -0.0235,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[7.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29696832753987124, distance: 0.9594981434679306 entropy -11.0362668246517
epoch: 25, step: 75
	action: tensor([[ 0.0040,  0.0188,  0.0074, -0.0087, -0.0008,  0.0163, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[7.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.306390714008816, distance: 0.9530466138922735 entropy -11.036467167527316
epoch: 25, step: 76
	action: tensor([[ 0.0037,  0.0192,  0.0263, -0.0080, -0.0106,  0.0120,  0.0525]],
       dtype=torch.float64)
	q_value: tensor([[7.0060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055875148843422, distance: 0.953598267929638 entropy -11.046823535613223
epoch: 25, step: 77
	action: tensor([[ 0.0038,  0.0183,  0.0132, -0.0088, -0.0054,  0.0077, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[7.0637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037079870399739, distance: 0.9548879216903458 entropy -11.033140655604589
epoch: 25, step: 78
	action: tensor([[ 0.0037,  0.0192,  0.0156, -0.0080, -0.0101, -0.0104,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[7.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3004557874584628, distance: 0.9571153408300844 entropy -11.050413414804277
epoch: 25, step: 79
	action: tensor([[ 0.0039,  0.0189,  0.0060, -0.0084, -0.0175,  0.0086, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[7.0398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046273376011426, distance: 0.954257319372383 entropy -11.041644820781775
epoch: 25, step: 80
	action: tensor([[ 0.0036,  0.0194,  0.0161, -0.0079, -0.0181, -0.0043,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.0065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019658278003845, distance: 0.9560817652943562 entropy -11.050032182640345
epoch: 25, step: 81
	action: tensor([[ 0.0038,  0.0189,  0.0057, -0.0084, -0.0071,  0.0165, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[7.0359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064582223381853, distance: 0.9530002332053409 entropy -11.040978940534886
epoch: 25, step: 82
	action: tensor([[ 0.0036,  0.0193,  0.0140, -0.0080, -0.0167,  0.0184,  0.0581]],
       dtype=torch.float64)
	q_value: tensor([[7.0050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070900800270048, distance: 0.9525660144187472 entropy -11.046200853879629
epoch: 25, step: 83
	action: tensor([[ 0.0037,  0.0184,  0.0116, -0.0088, -0.0121, -0.0067,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[7.0486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003604324710595, distance: 0.9571805708825477 entropy -11.028863544292488
epoch: 25, step: 84
	action: tensor([[ 0.0038,  0.0190,  0.0067, -0.0084, -0.0007, -0.0018,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[7.0303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022873254089019, distance: 0.955861565901405 entropy -11.039988123932293
epoch: 25, step: 85
	action: tensor([[ 0.0038,  0.0190,  0.0114, -0.0084, -0.0075,  0.0103, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.0254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305006883019049, distance: 0.9539968594437782 entropy -11.038416440560122
epoch: 25, step: 86
	action: tensor([[ 0.0037,  0.0191,  0.0136, -0.0081, -0.0037, -0.0224, -0.0421]],
       dtype=torch.float64)
	q_value: tensor([[7.0163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29761821799574184, distance: 0.9590545554437798 entropy -11.04502094307797
epoch: 25, step: 87
	action: tensor([[ 0.0037,  0.0193,  0.0042, -0.0080, -0.0091, -0.0123, -0.0351]],
       dtype=torch.float64)
	q_value: tensor([[7.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30024767624288806, distance: 0.9572576989694225 entropy -11.05381585576917
epoch: 25, step: 88
	action: tensor([[ 0.0037,  0.0194,  0.0127, -0.0080, -0.0008,  0.0123,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[7.0080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059071606297675, distance: 0.9533787667534138 entropy -11.05067185078649
epoch: 25, step: 89
	action: tensor([[ 0.0038,  0.0187,  0.0117, -0.0085, -0.0117, -0.0172,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[7.0351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29849329143875747, distance: 0.958456942612388 entropy -11.038438550866516
epoch: 25, step: 90
	action: tensor([[ 0.0039,  0.0190,  0.0029, -0.0084, -0.0070,  0.0217,  0.0733]],
       dtype=torch.float64)
	q_value: tensor([[7.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077061531697829, distance: 0.9521424522622999 entropy -11.041907475374972
epoch: 25, step: 91
	action: tensor([[ 0.0036,  0.0183,  0.0021, -0.0089, -0.0073,  0.0233,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[7.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071399377369788, distance: 0.9525317432869129 entropy -11.021810749503121
epoch: 25, step: 92
	action: tensor([[ 0.0036,  0.0191,  0.0091, -0.0081, -0.0075,  0.0058,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[7.0100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3040647810387084, distance: 0.9546432384755986 entropy -11.042526509822633
epoch: 25, step: 93
	action: tensor([[ 0.0038,  0.0190,  0.0164, -0.0083,  0.0046,  0.0260, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[7.0238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308758019043353, distance: 0.9514188383850853 entropy -11.040020701011695
epoch: 25, step: 94
	action: tensor([[ 0.0036,  0.0191,  0.0061, -0.0079, -0.0186,  0.0043, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[7.0079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037746837736184, distance: 0.954842186977465 entropy -11.05093219626831
epoch: 25, step: 95
	action: tensor([[ 0.0037,  0.0193,  0.0109, -0.0080, -0.0074, -0.0073, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[7.0134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30121984023596626, distance: 0.9565925100989151 entropy -11.046847891049378
epoch: 25, step: 96
	action: tensor([[ 0.0038,  0.0192, -0.0027, -0.0082, -0.0047,  0.0222, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[7.0195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30802095863287127, distance: 0.9519259446840929 entropy -11.045633029288451
epoch: 25, step: 97
	action: tensor([[ 0.0036,  0.0194, -0.0013, -0.0079, -0.0025,  0.0045, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[6.9981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3040048653983406, distance: 0.9546843319770858 entropy -11.046357102905867
epoch: 25, step: 98
	action: tensor([[ 0.0037,  0.0192,  0.0080, -0.0082, -0.0010, -0.0211,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[7.0108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979995996318191, distance: 0.9587941447458612 entropy -11.042550820802742
epoch: 25, step: 99
	action: tensor([[ 0.0039,  0.0187,  0.0127, -0.0088, -0.0114,  0.0100, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[7.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30472250358722075, distance: 0.9541920191736792 entropy -11.032583512541494
epoch: 25, step: 100
	action: tensor([[ 0.0037,  0.0191,  0.0189, -0.0081, -0.0187, -0.0069,  0.0360]],
       dtype=torch.float64)
	q_value: tensor([[7.0198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30116763494341825, distance: 0.956628242552425 entropy -11.046753440396762
epoch: 25, step: 101
	action: tensor([[ 0.0039,  0.0186,  0.0092, -0.0087, -0.0070,  0.0027,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[7.0519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30295351490088795, distance: 0.9554051194081484 entropy -11.033411207025784
epoch: 25, step: 102
	action: tensor([[ 0.0038,  0.0190,  0.0184, -0.0083, -0.0108,  0.0048, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[7.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037316020468671, distance: 0.9548717288605907 entropy -11.040812355224775
epoch: 25, step: 103
	action: tensor([[ 0.0037,  0.0191,  0.0045, -0.0081, -0.0172, -0.0105,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30044685980042707, distance: 0.9571214482147559 entropy -11.048962021659687
epoch: 25, step: 104
	action: tensor([[ 0.0038,  0.0191, -0.0048, -0.0083, -0.0149,  0.0096, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30499139842818046, distance: 0.9540074870085815 entropy -11.040814067650128
epoch: 25, step: 105
	action: tensor([[ 0.0036,  0.0194, -0.0030, -0.0081, -0.0059, -0.0156,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[7.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299435602020637, distance: 0.9578129946574256 entropy -11.042697314668787
epoch: 25, step: 106
	action: tensor([[ 0.0039,  0.0188,  0.0121, -0.0087, -0.0056,  0.0057,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[7.0367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30377504146551015, distance: 0.9548419416981394 entropy -11.029382486722367
epoch: 25, step: 107
	action: tensor([[ 0.0038,  0.0189,  0.0088, -0.0083, -0.0133, -0.0082, -0.0310]],
       dtype=torch.float64)
	q_value: tensor([[7.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3007605733326948, distance: 0.9569068143266171 entropy -11.041248417221837
epoch: 25, step: 108
	action: tensor([[ 0.0037,  0.0193,  0.0131, -0.0080, -0.0051,  0.0195,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.0124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3075164470906585, distance: 0.9522728989214743 entropy -11.050787572950034
epoch: 25, step: 109
	action: tensor([[ 0.0037,  0.0190,  0.0051, -0.0082, -0.0161,  0.0173,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[7.0192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066141529235087, distance: 0.9528930945710666 entropy -11.043674666326911
epoch: 25, step: 110
	action: tensor([[ 0.0036,  0.0192,  0.0112, -0.0081,  0.0034,  0.0099, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.0133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30504311272294193, distance: 0.9539719933887493 entropy -11.04184244312688
epoch: 25, step: 111
	action: tensor([[ 0.0037,  0.0190,  0.0143, -0.0082, -0.0093, -0.0010,  0.0487]],
       dtype=torch.float64)
	q_value: tensor([[7.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3024764534837465, distance: 0.9557320050399004 entropy -11.045315072255104
epoch: 25, step: 112
	action: tensor([[ 0.0038,  0.0185,  0.0135, -0.0088, -0.0105, -0.0033, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[7.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013435252095241, distance: 0.9565078473093002 entropy -11.029815085001482
epoch: 25, step: 113
	action: tensor([[ 0.0038,  0.0191,  0.0109, -0.0082, -0.0062,  0.0023,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[7.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30332158975774626, distance: 0.9551528356323353 entropy -11.045003944894571
epoch: 25, step: 114
	action: tensor([[ 0.0038,  0.0191,  0.0129, -0.0083,  0.0004, -0.0294,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[7.0226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2959152590936157, distance: 0.9602164888617386 entropy -11.042378459137812
epoch: 25, step: 115
	action: tensor([[ 0.0040,  0.0189,  0.0055, -0.0085, -0.0001,  0.0017,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[7.0420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30312972168911156, distance: 0.9552843530663481 entropy -11.041188986038247
epoch: 25, step: 116
	action: tensor([[ 0.0038,  0.0189,  0.0065, -0.0085, -0.0097, -0.0121,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[7.0265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29980115295585763, distance: 0.9575630710794986 entropy -11.03641205244978
epoch: 25, step: 117
	action: tensor([[ 0.0039,  0.0189,  0.0118, -0.0085, -0.0075,  0.0062,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[7.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30403099738262007, distance: 0.9546664094163573 entropy -11.037016505248548
epoch: 25, step: 118
	action: tensor([[ 0.0038,  0.0188,  0.0166, -0.0085, -0.0136, -0.0063, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[7.0373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30099475536225817, distance: 0.956746562246296 entropy -11.037466457092423
epoch: 25, step: 119
	action: tensor([[ 0.0037,  0.0193,  0.0033, -0.0080, -0.0085,  0.0067,  0.0367]],
       dtype=torch.float64)
	q_value: tensor([[7.0170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044712702591649, distance: 0.9543643986811674 entropy -11.050360308685175
epoch: 25, step: 120
	action: tensor([[ 0.0037,  0.0188, -0.0073, -0.0086, -0.0095, -0.0187, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[7.0316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982623777613437, distance: 0.9586146763907081 entropy -11.031329511647552
epoch: 25, step: 121
	action: tensor([[ 0.0036,  0.0197,  0.0156, -0.0078, -0.0107,  0.0066,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[6.9873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047482375587576, distance: 0.9541743604851664 entropy -11.051300895826243
epoch: 25, step: 122
	action: tensor([[ 0.0038,  0.0189,  0.0086, -0.0083, -0.0193, -0.0004,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[7.0346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025140186230755, distance: 0.9557062692138475 entropy -11.042104169959776
epoch: 25, step: 123
	action: tensor([[ 3.7537e-03,  1.9080e-02,  1.5565e-05, -8.2973e-03, -9.7805e-03,
          1.0614e-02, -1.5345e-03]], dtype=torch.float64)
	q_value: tensor([[7.0252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051870567295065, distance: 0.9538731918193943 entropy -11.04046093381964
epoch: 25, step: 124
	action: tensor([[ 0.0037,  0.0192,  0.0083, -0.0081, -0.0143, -0.0035, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30203954317877124, distance: 0.9560312808104334 entropy -11.042669928930989
epoch: 25, step: 125
	action: tensor([[ 0.0038,  0.0191,  0.0188, -0.0083, -0.0032, -0.0204, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[7.0264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979660302073197, distance: 0.9588170690795665 entropy -11.04274719915256
epoch: 25, step: 126
	action: tensor([[ 0.0039,  0.0189,  0.0035, -0.0084, -0.0102,  0.0309,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[7.0414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3097865394892707, distance: 0.9507107520484107 entropy -11.044506677407075
epoch: 25, step: 127
	action: tensor([[ 0.0036,  0.0188,  0.0219, -0.0084, -0.0082, -0.0063,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[7.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30070953551217916, distance: 0.956941736232637 entropy -11.034570427795789
LOSS epoch 25 actor 22.66791214395562 critic 12.425231189746846
epoch: 26, step: 0
	action: tensor([[ 0.0053,  0.0190,  0.0051, -0.0089, -0.0040, -0.0132,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[6.8450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009536510783398, distance: 0.9567746920811678 entropy -11.028167641184263
epoch: 26, step: 1
	action: tensor([[ 0.0052,  0.0196,  0.0082, -0.0085, -0.0098, -0.0008,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[6.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043176843496481, distance: 0.9544697637272596 entropy -11.034213659830659
epoch: 26, step: 2
	action: tensor([[ 0.0052,  0.0192,  0.0133, -0.0088, -0.0180,  0.0094,  0.0311]],
       dtype=torch.float64)
	q_value: tensor([[6.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061148590647679, distance: 0.9532361128588709 entropy -11.024890588755795
epoch: 26, step: 3
	action: tensor([[ 0.0051,  0.0194,  0.0094, -0.0085,  0.0007,  0.0015,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[6.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046561834763939, distance: 0.9542375266234346 entropy -11.035105177482908
epoch: 26, step: 4
	action: tensor([[ 0.0052,  0.0194,  0.0084, -0.0085, -0.0051,  0.0035,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[6.8176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051566069483017, distance: 0.9538940930648688 entropy -11.034589578861647
epoch: 26, step: 5
	action: tensor([[ 0.0052,  0.0194,  0.0077, -0.0085, -0.0114, -0.0029, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[6.8206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30370317825879745, distance: 0.9548912190419238 entropy -11.0347474903222
epoch: 26, step: 6
	action: tensor([[ 0.0051,  0.0198,  0.0111, -0.0082,  0.0013, -0.0036,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[6.8092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30381135524668845, distance: 0.9548170399957162 entropy -11.042355570379115
epoch: 26, step: 7
	action: tensor([[ 0.0052,  0.0190,  0.0026, -0.0089,  0.0024,  0.0077,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[6.8418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30572890677644526, distance: 0.9535011801537406 entropy -11.024861705836182
epoch: 26, step: 8
	action: tensor([[ 0.0051,  0.0196,  0.0058, -0.0084, -0.0097,  0.0206, -0.0019]],
       dtype=torch.float64)
	q_value: tensor([[6.8066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30925034504583315, distance: 0.9510799616385415 entropy -11.037540626548106
epoch: 26, step: 9
	action: tensor([[ 0.0050,  0.0198,  0.0129, -0.0081, -0.0092,  0.0105, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[6.7983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30708200601900193, distance: 0.9525715642045113 entropy -11.044155751634317
epoch: 26, step: 10
	action: tensor([[ 0.0051,  0.0197,  0.0028, -0.0081, -0.0092,  0.0215, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[6.8069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3095783038300457, distance: 0.9508541547505157 entropy -11.045204390529866
epoch: 26, step: 11
	action: tensor([[ 0.0050,  0.0199,  0.0181, -0.0080, -0.0117, -0.0075,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[6.7884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30295875478920786, distance: 0.9554015283811214 entropy -11.04476723770283
epoch: 26, step: 12
	action: tensor([[ 0.0052,  0.0193,  0.0104, -0.0086, -0.0115,  0.0126,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[6.8354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30719349892968384, distance: 0.9524949250857558 entropy -11.035127959751774
epoch: 26, step: 13
	action: tensor([[ 0.0051,  0.0193,  0.0099, -0.0086, -0.0157, -0.0041,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[6.8230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3031605980019916, distance: 0.9552631898841771 entropy -11.030734075896929
epoch: 26, step: 14
	action: tensor([[ 0.0052,  0.0195,  0.0043, -0.0085,  0.0002, -0.0125, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[6.8198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015848991749791, distance: 0.9563426044146771 entropy -11.034806855314844
epoch: 26, step: 15
	action: tensor([[ 0.0051,  0.0200,  0.0025, -0.0081, -0.0185, -0.0114,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[6.7993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023679290564195, distance: 0.9558063510852912 entropy -11.046781186058295
epoch: 26, step: 16
	action: tensor([[ 0.0052,  0.0196,  0.0064, -0.0085,  0.0012, -0.0266, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[6.8214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29838943592098544, distance: 0.9585278880195544 entropy -11.033325757080231
epoch: 26, step: 17
	action: tensor([[ 0.0052,  0.0199,  0.0095, -0.0083, -0.0006,  0.0123,  0.0425]],
       dtype=torch.float64)
	q_value: tensor([[6.8082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.307678081666657, distance: 0.9521617560586624 entropy -11.043398677695361
epoch: 26, step: 18
	action: tensor([[ 0.0051,  0.0192,  0.0018, -0.0087, -0.0089, -0.0113,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[6.8235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30155388863262833, distance: 0.956363835609141 entropy -11.030291591934718
epoch: 26, step: 19
	action: tensor([[ 0.0052,  0.0198,  0.0073, -0.0083,  0.0034, -0.0157, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[6.8069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30104701171958137, distance: 0.9567107992637953 entropy -11.037866164500304
epoch: 26, step: 20
	action: tensor([[ 0.0052,  0.0198,  0.0243, -0.0083, -0.0120, -0.0126, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[6.8092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017446923759952, distance: 0.9562331954201165 entropy -11.042190004424029
epoch: 26, step: 21
	action: tensor([[ 0.0052,  0.0196,  0.0168, -0.0083,  0.0023,  0.0100,  0.0204]],
       dtype=torch.float64)
	q_value: tensor([[6.8263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30689929653257175, distance: 0.9526971435597034 entropy -11.046517284565928
epoch: 26, step: 22
	action: tensor([[ 0.0052,  0.0193,  0.0184, -0.0085, -0.0065, -0.0007,  0.0050]],
       dtype=torch.float64)
	q_value: tensor([[6.8236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30410062494788104, distance: 0.954618653869726 entropy -11.03866329537801
epoch: 26, step: 23
	action: tensor([[ 0.0052,  0.0196,  0.0017, -0.0083, -0.0117, -0.0001,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[6.8168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045589537261628, distance: 0.9543042396875592 entropy -11.042353543713245
epoch: 26, step: 24
	action: tensor([[ 0.0051,  0.0198,  0.0076, -0.0083, -0.0135,  0.0122, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[6.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30745136202955814, distance: 0.952317648925504 entropy -11.038124979863639
epoch: 26, step: 25
	action: tensor([[ 0.0051,  0.0198,  0.0124, -0.0081, -0.0129,  0.0172, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[6.8038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30863621908775685, distance: 0.9515026568387844 entropy -11.043543495742387
epoch: 26, step: 26
	action: tensor([[ 0.0051,  0.0197,  0.0031, -0.0081, -0.0198,  0.0168,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[6.8062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3084135509511782, distance: 0.951655870149135 entropy -11.044660151735389
epoch: 26, step: 27
	action: tensor([[ 0.0050,  0.0198,  0.0092, -0.0082, -0.0122,  0.0336,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[6.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3122792217653686, distance: 0.9489924696037909 entropy -11.038574743848246
epoch: 26, step: 28
	action: tensor([[ 0.0050,  0.0195,  0.0046, -0.0082, -0.0152,  0.0027,  0.0514]],
       dtype=torch.float64)
	q_value: tensor([[6.8035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049434899500748, distance: 0.9540403673636522 entropy -11.039589125568664
epoch: 26, step: 29
	action: tensor([[ 0.0051,  0.0193,  0.0138, -0.0087, -0.0039, -0.0115, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[6.8258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013758956074968, distance: 0.9564856884233177 entropy -11.02545407737971
epoch: 26, step: 30
	action: tensor([[ 0.0052,  0.0197,  0.0188, -0.0083, -0.0063,  0.0050,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[6.8133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057845603118581, distance: 0.953462962535502 entropy -11.043115954323317
epoch: 26, step: 31
	action: tensor([[ 0.0052,  0.0195,  0.0095, -0.0083, -0.0054, -0.0038,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[6.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036326532493896, distance: 0.9549395762977297 entropy -11.041675643575688
epoch: 26, step: 32
	action: tensor([[ 0.0052,  0.0196,  0.0051, -0.0084, -0.0114,  0.0035,  0.0441]],
       dtype=torch.float64)
	q_value: tensor([[6.8176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053349485855852, distance: 0.953771669830127 entropy -11.038548075301865
epoch: 26, step: 33
	action: tensor([[ 0.0051,  0.0194,  0.0139, -0.0087, -0.0075, -0.0252,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[6.8226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29832021025072086, distance: 0.9585751742936222 entropy -11.028046588120002
epoch: 26, step: 34
	action: tensor([[ 0.0053,  0.0195,  0.0049, -0.0085, -0.0112, -0.0136,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[6.8307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014736842144109, distance: 0.9564187449298953 entropy -11.037622919070527
epoch: 26, step: 35
	action: tensor([[ 0.0052,  0.0196, -0.0005, -0.0086, -0.0088, -0.0281,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[6.8207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981631261780313, distance: 0.9586824657306425 entropy -11.032175156946021
epoch: 26, step: 36
	action: tensor([[ 0.0053,  0.0195,  0.0061, -0.0088, -0.0101, -0.0075,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[6.8300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302706895545104, distance: 0.95557411858016 entropy -11.026769051033511
epoch: 26, step: 37
	action: tensor([[ 0.0052,  0.0198,  0.0026, -0.0083, -0.0135,  0.0225,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[6.8086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098188610142609, distance: 0.9506884917022733 entropy -11.03969913605407
epoch: 26, step: 38
	action: tensor([[ 0.0050,  0.0197,  0.0144, -0.0083, -0.0043,  0.0053,  0.0039]],
       dtype=torch.float64)
	q_value: tensor([[6.8016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30563346426227067, distance: 0.9535667175377479 entropy -11.036927096249142
epoch: 26, step: 39
	action: tensor([[ 0.0052,  0.0196,  0.0054, -0.0083, -0.0088,  0.0225,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[6.8118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30973336064974555, distance: 0.9507473760231802 entropy -11.041466235846343
epoch: 26, step: 40
	action: tensor([[ 0.0050,  0.0197,  0.0229, -0.0082, -0.0078,  0.0084, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[6.7996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064287529229299, distance: 0.9530204800471226 entropy -11.039800026297415
epoch: 26, step: 41
	action: tensor([[ 0.0051,  0.0196,  0.0069, -0.0081, -0.0139, -0.0090,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[6.8129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025460252592227, distance: 0.9556843409639846 entropy -11.048889687143431
epoch: 26, step: 42
	action: tensor([[ 0.0052,  0.0195, -0.0006, -0.0086, -0.0114, -0.0022,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[6.8234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038952326228075, distance: 0.9547595196950113 entropy -11.030952748239743
epoch: 26, step: 43
	action: tensor([[ 0.0051,  0.0196, -0.0081, -0.0085, -0.0009, -0.0051,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[6.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033111159286689, distance: 0.955160015465811 entropy -11.031768114934067
epoch: 26, step: 44
	action: tensor([[ 0.0051,  0.0194,  0.0123, -0.0088, -0.0120,  0.0260,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[6.8160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31016716270710665, distance: 0.9504485777298274 entropy -11.023303266299992
epoch: 26, step: 45
	action: tensor([[ 0.0050,  0.0193,  0.0077, -0.0085, -0.0132, -0.0088,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[6.8194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30213363796069337, distance: 0.9559668354782613 entropy -11.036227869960504
epoch: 26, step: 46
	action: tensor([[ 0.0052,  0.0196,  0.0109, -0.0084, -0.0136, -0.0070,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[6.8184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029748036251718, distance: 0.9553905296269061 entropy -11.03784834005614
epoch: 26, step: 47
	action: tensor([[ 0.0052,  0.0197,  0.0016, -0.0083, -0.0065, -0.0061, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.8157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30325367099173506, distance: 0.9551993931400525 entropy -11.039127440113774
epoch: 26, step: 48
	action: tensor([[ 0.0051,  0.0199,  0.0089, -0.0082, -0.0053, -0.0030,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[6.7996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3040850280351315, distance: 0.9546293515517201 entropy -11.041513922590738
epoch: 26, step: 49
	action: tensor([[ 0.0052,  0.0195,  0.0128, -0.0085, -0.0114,  0.0082, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[6.8188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30629546880556124, distance: 0.9531120469857295 entropy -11.034666378700004
epoch: 26, step: 50
	action: tensor([[ 0.0050,  0.0199,  0.0116, -0.0079, -0.0120,  0.0102,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[6.7940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071246937735925, distance: 0.9525422217947266 entropy -11.049872077013573
epoch: 26, step: 51
	action: tensor([[ 0.0051,  0.0196,  0.0125, -0.0082, -0.0111,  0.0155,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[6.8127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080891932552008, distance: 0.951879009802565 entropy -11.041373789290546
epoch: 26, step: 52
	action: tensor([[ 0.0051,  0.0196,  0.0192, -0.0082, -0.0082, -0.0069, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[6.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30287984392684797, distance: 0.9554556065475179 entropy -11.041957861582167
epoch: 26, step: 53
	action: tensor([[ 0.0051,  0.0197,  0.0123, -0.0082, -0.0114,  0.0109, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[6.8161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30713317286728425, distance: 0.9525363933870886 entropy -11.047035845587775
epoch: 26, step: 54
	action: tensor([[ 0.0051,  0.0198,  0.0089, -0.0081, -0.0214, -0.0043, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[6.8014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037382472307366, distance: 0.9548671722030081 entropy -11.045333321728046
epoch: 26, step: 55
	action: tensor([[ 0.0051,  0.0199, -0.0011, -0.0081, -0.0086, -0.0028, -0.0383]],
       dtype=torch.float64)
	q_value: tensor([[6.8024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30424315612811315, distance: 0.9545208883766403 entropy -11.045139884103753
epoch: 26, step: 56
	action: tensor([[ 0.0050,  0.0202,  0.0132, -0.0078, -0.0109,  0.0013,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[6.7847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052663118648744, distance: 0.95381878760482 entropy -11.050582411537736
epoch: 26, step: 57
	action: tensor([[ 0.0052,  0.0196,  0.0029, -0.0083, -0.0028,  0.0019, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[6.8132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30505382771959355, distance: 0.9539646390864156 entropy -11.040917777198851
epoch: 26, step: 58
	action: tensor([[ 0.0051,  0.0199, -0.0012, -0.0081, -0.0106,  0.0022, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[6.7950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30534904299509613, distance: 0.9537619940038744 entropy -11.043500501285147
epoch: 26, step: 59
	action: tensor([[ 0.0050,  0.0200,  0.0063, -0.0080, -0.0093, -0.0140, -0.0130]],
       dtype=torch.float64)
	q_value: tensor([[6.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30168121334369113, distance: 0.9562766604519695 entropy -11.04304954958497
epoch: 26, step: 60
	action: tensor([[ 0.0051,  0.0199,  0.0072, -0.0082, -0.0038,  0.0136, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[6.8090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30794844697785206, distance: 0.9519758189674109 entropy -11.0437001415841
epoch: 26, step: 61
	action: tensor([[ 0.0051,  0.0198,  0.0091, -0.0081, -0.0073, -0.0195,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[6.7967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300260386240492, distance: 0.9572490053231882 entropy -11.043958093707024
epoch: 26, step: 62
	action: tensor([[ 0.0053,  0.0195,  0.0021, -0.0086, -0.0119,  0.0162, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[6.8259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3081814488409471, distance: 0.9518155485282366 entropy -11.033052926206116
epoch: 26, step: 63
	action: tensor([[ 0.0050,  0.0199,  0.0174, -0.0080, -0.0045,  0.0110, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[6.7946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30731014065229956, distance: 0.952414740124063 entropy -11.044277287862288
epoch: 26, step: 64
	action: tensor([[ 0.0050,  0.0198,  0.0079, -0.0080, -0.0147,  0.0255,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[6.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31051319992478776, distance: 0.9502101635671016 entropy -11.05014282092095
epoch: 26, step: 65
	action: tensor([[ 0.0050,  0.0197,  0.0128, -0.0081, -0.0124,  0.0040, -0.0353]],
       dtype=torch.float64)
	q_value: tensor([[6.8014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055057755190139, distance: 0.9536543903534359 entropy -11.043446645271928
epoch: 26, step: 66
	action: tensor([[ 0.0049,  0.0199,  0.0038, -0.0078,  0.0125,  0.0059,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[6.7956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062139201134447, distance: 0.9531680670585865 entropy -11.052807380683719
epoch: 26, step: 67
	action: tensor([[ 0.0052,  0.0195, -0.0110, -0.0085,  0.0032,  0.0032, -0.0655]],
       dtype=torch.float64)
	q_value: tensor([[6.8089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30525013477367013, distance: 0.9538298925245616 entropy -11.035353842864998
epoch: 26, step: 68
	action: tensor([[ 0.0048,  0.0205,  0.0123, -0.0075,  0.0002, -0.0158,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[6.7552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014987678258889, distance: 0.9564015725979158 entropy -11.056291991232671
epoch: 26, step: 69
	action: tensor([[ 0.0053,  0.0195,  0.0022, -0.0085, -0.0113,  0.0103,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[6.8228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068675202741006, distance: 0.9527189822359269 entropy -11.037581577913352
epoch: 26, step: 70
	action: tensor([[ 0.0051,  0.0197, -0.0055, -0.0083, -0.0084, -0.0062,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[6.8045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30318326624514025, distance: 0.9552476523626888 entropy -11.03627124129389
epoch: 26, step: 71
	action: tensor([[ 0.0051,  0.0199,  0.0241, -0.0082,  0.0032,  0.0214,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[6.8011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3097322213015752, distance: 0.9507481606705718 entropy -11.03815705951809
epoch: 26, step: 72
	action: tensor([[ 0.0052,  0.0192,  0.0018, -0.0084, -0.0054,  0.0152,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[6.8238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077548204214272, distance: 0.9521089845585621 entropy -11.042568401296068
epoch: 26, step: 73
	action: tensor([[ 0.0050,  0.0198, -0.0107, -0.0082, -0.0091,  0.0026,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[6.7990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30530630223923294, distance: 0.9537913352731132 entropy -11.04078401255535
epoch: 26, step: 74
	action: tensor([[ 0.0050,  0.0200,  0.0033, -0.0082,  0.0068, -0.0091,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[6.7882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30267535545354085, distance: 0.9555957296890312 entropy -11.037946336728156
epoch: 26, step: 75
	action: tensor([[ 0.0052,  0.0197,  0.0272, -0.0084, -0.0151,  0.0002,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[6.8089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30454434703636235, distance: 0.9543142614952212 entropy -11.037971726784024
epoch: 26, step: 76
	action: tensor([[ 0.0052,  0.0193,  0.0101, -0.0085, -0.0140,  0.0096,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[6.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065280333763173, distance: 0.9529522680992347 entropy -11.03951631341373
epoch: 26, step: 77
	action: tensor([[ 0.0051,  0.0197,  0.0114, -0.0082,  0.0033, -0.0028, -0.0386]],
       dtype=torch.float64)
	q_value: tensor([[6.8065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30396330510417313, distance: 0.9547128353152607 entropy -11.040944070857536
epoch: 26, step: 78
	action: tensor([[ 0.0050,  0.0199,  0.0104, -0.0079, -0.0150, -0.0074,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[6.7955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30312506879114876, distance: 0.9552875422059478 entropy -11.05233442072513
epoch: 26, step: 79
	action: tensor([[ 0.0052,  0.0197,  0.0033, -0.0083, -0.0024,  0.0206,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[6.8128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30943149478797916, distance: 0.9509552426569816 entropy -11.040592162032116
epoch: 26, step: 80
	action: tensor([[ 0.0050,  0.0198,  0.0018, -0.0081, -0.0182,  0.0092, -0.0248]],
       dtype=torch.float64)
	q_value: tensor([[6.7980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067455425232255, distance: 0.9528028084955696 entropy -11.042207706300932
epoch: 26, step: 81
	action: tensor([[ 0.0049,  0.0201,  0.0106, -0.0079, -0.0048,  0.0040,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[6.7889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30579850834806144, distance: 0.953453384094891 entropy -11.048540315232945
epoch: 26, step: 82
	action: tensor([[ 0.0052,  0.0195,  0.0125, -0.0084, -0.0110,  0.0114,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[6.8170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30706623049114146, distance: 0.9525824076476392 entropy -11.038815277312693
epoch: 26, step: 83
	action: tensor([[ 0.0051,  0.0195,  0.0084, -0.0083, -0.0074, -0.0081,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[6.8169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30258238358366263, distance: 0.9556594306935988 entropy -11.039274478255988
epoch: 26, step: 84
	action: tensor([[ 0.0052,  0.0197,  0.0144, -0.0083, -0.0047,  0.0201,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[6.8103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30931252637690143, distance: 0.9510371525335811 entropy -11.0400983937928
epoch: 26, step: 85
	action: tensor([[ 0.0051,  0.0196,  0.0037, -0.0082, -0.0003, -0.0126, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[6.8049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30171141726501405, distance: 0.9562559796268996 entropy -11.043429968521249
epoch: 26, step: 86
	action: tensor([[ 0.0051,  0.0199,  0.0106, -0.0081, -0.0122,  0.0091,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[6.8000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30694486801382015, distance: 0.9526658230506715 entropy -11.045851149565694
epoch: 26, step: 87
	action: tensor([[ 0.0051,  0.0195,  0.0003, -0.0084, -0.0139,  0.0087,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[6.8149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30641749470404567, distance: 0.953028214847023 entropy -11.036838211556434
epoch: 26, step: 88
	action: tensor([[ 0.0050,  0.0198,  0.0063, -0.0082, -0.0138, -0.0048,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[6.8037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035431395588385, distance: 0.9550009500965712 entropy -11.038602134840831
epoch: 26, step: 89
	action: tensor([[ 0.0051,  0.0197,  0.0059, -0.0083, -0.0049,  0.0100, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[6.8134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3069430862399751, distance: 0.9526670476530712 entropy -11.039597844409757
epoch: 26, step: 90
	action: tensor([[ 0.0051,  0.0199, -0.0045, -0.0081, -0.0104,  0.0136, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[6.7963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079050246921522, distance: 0.9520056840240065 entropy -11.043617684729844
epoch: 26, step: 91
	action: tensor([[ 0.0049,  0.0202,  0.0141, -0.0078, -0.0075,  0.0013, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[6.7773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30522759723622706, distance: 0.953845363418229 entropy -11.046835143541108
epoch: 26, step: 92
	action: tensor([[ 0.0051,  0.0198,  0.0068, -0.0081, -0.0040,  0.0053,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[6.8066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30591390889791603, distance: 0.9533741321636025 entropy -11.045068277362788
epoch: 26, step: 93
	action: tensor([[ 0.0051,  0.0195,  0.0138, -0.0084, -0.0185, -0.0095,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[6.8121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022473219305718, distance: 0.955888967753739 entropy -11.03565333391193
epoch: 26, step: 94
	action: tensor([[ 0.0052,  0.0197,  0.0199, -0.0083,  0.0023, -0.0048,  0.0690]],
       dtype=torch.float64)
	q_value: tensor([[6.8184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30343749875782333, distance: 0.9550733761473724 entropy -11.04275817606887
epoch: 26, step: 95
	action: tensor([[ 0.0053,  0.0188,  0.0082, -0.0091, -0.0089,  0.0081,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[6.8554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305540452034341, distance: 0.953630581786912 entropy -11.02365227799485
epoch: 26, step: 96
	action: tensor([[ 0.0051,  0.0197,  0.0217, -0.0082, -0.0051,  0.0304,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[6.8034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117318478448676, distance: 0.9493700577359632 entropy -11.040953760649385
epoch: 26, step: 97
	action: tensor([[ 0.0050,  0.0195, -0.0096, -0.0081, -0.0049, -0.0004, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[6.8069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044645300739498, distance: 0.9543690229166235 entropy -11.046490990515332
epoch: 26, step: 98
	action: tensor([[ 0.0050,  0.0201,  0.0203, -0.0081, -0.0134,  0.0015, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[6.7854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051745073744677, distance: 0.9538818059646916 entropy -11.040505273147783
epoch: 26, step: 99
	action: tensor([[ 0.0051,  0.0196,  0.0057, -0.0082, -0.0102,  0.0004,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[6.8176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30469669394256715, distance: 0.9542097294609582 entropy -11.046108865540175
epoch: 26, step: 100
	action: tensor([[ 0.0051,  0.0197,  0.0046, -0.0082, -0.0024, -0.0077,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[6.8084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029054802481119, distance: 0.9554380381327573 entropy -11.04086379819304
epoch: 26, step: 101
	action: tensor([[ 0.0052,  0.0195,  0.0019, -0.0085, -0.0141,  0.0116, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[6.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070964502212874, distance: 0.9525616357510803 entropy -11.033649952320674
epoch: 26, step: 102
	action: tensor([[ 5.0167e-03,  1.9921e-02,  4.7526e-03, -8.0406e-03, -5.3626e-03,
          2.4529e-03,  9.5525e-06]], dtype=torch.float64)
	q_value: tensor([[6.7967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30535541108201547, distance: 0.9537576222736287 entropy -11.043608298599867
epoch: 26, step: 103
	action: tensor([[ 0.0051,  0.0197,  0.0033, -0.0082,  0.0001, -0.0154,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[6.8065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30115169169062994, distance: 0.9566391548109794 entropy -11.040908854559694
epoch: 26, step: 104
	action: tensor([[ 0.0052,  0.0197,  0.0128, -0.0084, -0.0018,  0.0094,  0.0307]],
       dtype=torch.float64)
	q_value: tensor([[6.8140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30681517979556017, distance: 0.952754952867488 entropy -11.038236664049121
epoch: 26, step: 105
	action: tensor([[ 0.0052,  0.0193,  0.0068, -0.0086, -0.0033, -0.0022, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[6.8217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303746259058722, distance: 0.9548616783973309 entropy -11.034072366219613
epoch: 26, step: 106
	action: tensor([[ 0.0051,  0.0198, -0.0062, -0.0082, -0.0118,  0.0015,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[6.8049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30515191209953196, distance: 0.9538973156478202 entropy -11.043528662116502
epoch: 26, step: 107
	action: tensor([[ 0.0050,  0.0199,  0.0016, -0.0082, -0.0113,  0.0232, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[6.7972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3100639641441204, distance: 0.9505196683289008 entropy -11.038939399317611
epoch: 26, step: 108
	action: tensor([[ 0.0049,  0.0201,  0.0056, -0.0078,  0.0080, -0.0066,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[6.7826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303312040048596, distance: 0.9551593819816857 entropy -11.049651101547681
epoch: 26, step: 109
	action: tensor([[ 0.0052,  0.0195,  0.0023, -0.0086, -0.0004,  0.0092,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[6.8173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30655256621755855, distance: 0.9529354117340187 entropy -11.033589935347445
epoch: 26, step: 110
	action: tensor([[ 0.0051,  0.0197,  0.0146, -0.0083, -0.0023, -0.0113,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[6.7999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019540025979488, distance: 0.9560898636174671 entropy -11.038839545425814
epoch: 26, step: 111
	action: tensor([[ 0.0053,  0.0195,  0.0033, -0.0084, -0.0145, -0.0083, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[6.8241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026959041926046, distance: 0.9555816498539673 entropy -11.040268012147292
epoch: 26, step: 112
	action: tensor([[ 0.0051,  0.0199,  0.0164, -0.0081, -0.0198,  0.0084, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[6.8031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067071307197793, distance: 0.9528292045517762 entropy -11.043744291136798
epoch: 26, step: 113
	action: tensor([[ 0.0051,  0.0198,  0.0130, -0.0081, -0.0099,  0.0048,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.8078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30574724748113524, distance: 0.9534885856505025 entropy -11.045460035156664
epoch: 26, step: 114
	action: tensor([[ 0.0052,  0.0195,  0.0123, -0.0084, -0.0050, -0.0004,  0.0343]],
       dtype=torch.float64)
	q_value: tensor([[6.8211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042963927109543, distance: 0.9544843695821205 entropy -11.038160183693224
epoch: 26, step: 115
	action: tensor([[ 0.0052,  0.0193,  0.0091, -0.0086, -0.0237, -0.0091, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[6.8298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021816752268527, distance: 0.9559339331733967 entropy -11.03264910196542
epoch: 26, step: 116
	action: tensor([[ 0.0051,  0.0199,  0.0178, -0.0081, -0.0081,  0.0054,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[6.8059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060272033267132, distance: 0.9532963202126584 entropy -11.046649820137478
epoch: 26, step: 117
	action: tensor([[ 0.0052,  0.0195,  0.0097, -0.0083,  0.0008, -0.0023, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[6.8173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30398205046803217, distance: 0.9546999792689441 entropy -11.041089963659632
epoch: 26, step: 118
	action: tensor([[ 0.0052,  0.0198,  0.0125, -0.0082, -0.0089,  0.0199, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[6.8047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30930749551610703, distance: 0.9510406161310799 entropy -11.043303881703991
epoch: 26, step: 119
	action: tensor([[ 0.0050,  0.0197,  0.0115, -0.0080,  0.0023, -0.0007, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[6.8026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044589319275728, distance: 0.9543728636170397 entropy -11.046352550155513
epoch: 26, step: 120
	action: tensor([[ 0.0051,  0.0199,  0.0154, -0.0081, -0.0061,  0.0193, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[6.7983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30924780197352797, distance: 0.9510817123906895 entropy -11.04756460866182
epoch: 26, step: 121
	action: tensor([[ 0.0051,  0.0197,  0.0058, -0.0081, -0.0048, -0.0050,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[6.8042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30346338167000764, distance: 0.9550556316449609 entropy -11.044729781007259
epoch: 26, step: 122
	action: tensor([[ 0.0052,  0.0197,  0.0109, -0.0083, -0.0121, -0.0070,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[6.8088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30300824115680236, distance: 0.9553676134652275 entropy -11.038525242948266
epoch: 26, step: 123
	action: tensor([[ 0.0052,  0.0196,  0.0055, -0.0085, -0.0009, -0.0014, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[6.8202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30417385244281514, distance: 0.9545684266571424 entropy -11.036126545419602
epoch: 26, step: 124
	action: tensor([[ 0.0051,  0.0198,  0.0153, -0.0082, -0.0064,  0.0032, -0.0187]],
       dtype=torch.float64)
	q_value: tensor([[6.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30545357627390335, distance: 0.9536902287392478 entropy -11.043219232079226
epoch: 26, step: 125
	action: tensor([[ 0.0051,  0.0198,  0.0122, -0.0081, -0.0076, -0.0110,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[6.8023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021779414802024, distance: 0.9559364905799737 entropy -11.047820139086651
epoch: 26, step: 126
	action: tensor([[ 0.0052,  0.0195,  0.0193, -0.0085, -0.0110,  0.0180, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[6.8264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3086050001666053, distance: 0.9515241394156216 entropy -11.036777541127302
epoch: 26, step: 127
	action: tensor([[ 0.0050,  0.0197,  0.0061, -0.0080,  0.0006, -0.0111, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[6.8074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020695165353954, distance: 0.9560107525886004 entropy -11.048294119073821
LOSS epoch 26 actor 21.221681675997743 critic 12.894924084595958
epoch: 27, step: 0
	action: tensor([[ 0.0069,  0.0196,  0.0117, -0.0076, -0.0059,  0.0181,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[6.9114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3109095491294763, distance: 0.9499370117227767 entropy -11.045337261504814
epoch: 27, step: 1
	action: tensor([[ 0.0069,  0.0192,  0.0198, -0.0077, -0.0089, -0.0060,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[6.9246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30483045923496077, distance: 0.9541179376616614 entropy -11.040566479036329
epoch: 27, step: 2
	action: tensor([[ 0.0070,  0.0192,  0.0090, -0.0078, -0.0171, -0.0270, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[6.9343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30032571784502815, distance: 0.9572043172191883 entropy -11.041330449689088
epoch: 27, step: 3
	action: tensor([[ 0.0070,  0.0195,  0.0182, -0.0077, -0.0049,  0.0097,  0.0225]],
       dtype=torch.float64)
	q_value: tensor([[6.9260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30887660170192155, distance: 0.951337226867383 entropy -11.041357784728065
epoch: 27, step: 4
	action: tensor([[ 0.0070,  0.0191,  0.0151, -0.0079, -0.0075,  0.0124, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[6.9383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090344811753638, distance: 0.9512285594430985 entropy -11.038822369134964
epoch: 27, step: 5
	action: tensor([[ 0.0069,  0.0194,  0.0257, -0.0076, -0.0023, -0.0128,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[6.9156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303406921598532, distance: 0.9550943384517062 entropy -11.045380073794774
epoch: 27, step: 6
	action: tensor([[ 0.0071,  0.0189,  0.0114, -0.0081, -0.0084,  0.0066, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[6.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30768158202570517, distance: 0.9521593490048463 entropy -11.03874361052996
epoch: 27, step: 7
	action: tensor([[ 0.0068,  0.0196,  0.0091, -0.0073, -0.0092,  0.0146, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.9028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31002600750186937, distance: 0.9505458142590518 entropy -11.050762769806681
epoch: 27, step: 8
	action: tensor([[ 0.0068,  0.0195,  0.0044, -0.0075, -0.0169,  0.0168,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[6.9125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31036666895937226, distance: 0.9503111283830756 entropy -11.045950191992807
epoch: 27, step: 9
	action: tensor([[ 0.0068,  0.0195,  0.0040, -0.0076, -0.0083, -0.0063,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[6.9118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050246111707642, distance: 0.9539846919067901 entropy -11.040247569609404
epoch: 27, step: 10
	action: tensor([[ 0.0069,  0.0194,  0.0057, -0.0078,  0.0022,  0.0006,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[6.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30662986593972874, distance: 0.9528822976175804 entropy -11.03858582088388
epoch: 27, step: 11
	action: tensor([[ 0.0070,  0.0192, -0.0017, -0.0080,  0.0004, -0.0116, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[6.9258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30368340649078296, distance: 0.9549047763015923 entropy -11.034547048942699
epoch: 27, step: 12
	action: tensor([[ 0.0069,  0.0197,  0.0003, -0.0075, -0.0085, -0.0017,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[6.9057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30645590532835565, distance: 0.9530018251130924 entropy -11.045229746644052
epoch: 27, step: 13
	action: tensor([[ 0.0069,  0.0194,  0.0093, -0.0078, -0.0039, -0.0245, -0.0001]],
       dtype=torch.float64)
	q_value: tensor([[6.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30076814291174514, distance: 0.9569016348408058 entropy -11.035280531999671
epoch: 27, step: 14
	action: tensor([[ 7.0558e-03,  1.9427e-02, -5.1093e-05, -7.8297e-03, -1.1670e-02,
         -4.4029e-03,  4.4301e-03]], dtype=torch.float64)
	q_value: tensor([[6.9272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056390874524155, distance: 0.9535628563941382 entropy -11.04055953579631
epoch: 27, step: 15
	action: tensor([[ 0.0069,  0.0196,  0.0146, -0.0077, -0.0059, -0.0243,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[6.9129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30096732676370863, distance: 0.9567653331783763 entropy -11.039121736674977
epoch: 27, step: 16
	action: tensor([[ 0.0071,  0.0193,  0.0159, -0.0079,  0.0022,  0.0163, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[6.9380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31026019684871753, distance: 0.9503844845580319 entropy -11.038330504710112
epoch: 27, step: 17
	action: tensor([[ 0.0069,  0.0194,  0.0112, -0.0076, -0.0151,  0.0099, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[6.9132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308738649510213, distance: 0.9514321683116027 entropy -11.046721104917342
epoch: 27, step: 18
	action: tensor([[ 0.0068,  0.0195,  0.0111, -0.0075,  0.0004,  0.0221,  0.0368]],
       dtype=torch.float64)
	q_value: tensor([[6.9142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117157936529903, distance: 0.9493811299322756 entropy -11.047160980348668
epoch: 27, step: 19
	action: tensor([[ 0.0069,  0.0190,  0.0141, -0.0080, -0.0101,  0.0198,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[6.9337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3105251567547819, distance: 0.9502019244313848 entropy -11.034957668585752
epoch: 27, step: 20
	action: tensor([[ 0.0069,  0.0193,  0.0082, -0.0077,  0.0028, -0.0008, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[6.9196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061516998134336, distance: 0.9532108072310901 entropy -11.041815926842643
epoch: 27, step: 21
	action: tensor([[ 0.0069,  0.0195,  0.0086, -0.0075, -0.0116,  0.0043, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.9102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30763445605993456, distance: 0.9521917550944095 entropy -11.04782384632905
epoch: 27, step: 22
	action: tensor([[ 6.8584e-03,  1.9491e-02,  2.3589e-05, -7.5544e-03, -1.0653e-02,
          1.2814e-02, -1.2944e-02]], dtype=torch.float64)
	q_value: tensor([[6.9161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30951512696303585, distance: 0.9508976575928065 entropy -11.045068850475937
epoch: 27, step: 23
	action: tensor([[ 0.0068,  0.0197,  0.0054, -0.0074,  0.0001,  0.0124,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[6.8971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3095999580129243, distance: 0.9508392434786661 entropy -11.045772655972863
epoch: 27, step: 24
	action: tensor([[ 0.0069,  0.0194,  0.0074, -0.0077, -0.0076,  0.0138, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[6.9111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309653578043396, distance: 0.9508023192202164 entropy -11.041488794083493
epoch: 27, step: 25
	action: tensor([[ 0.0068,  0.0195,  0.0083, -0.0075, -0.0076, -0.0071, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[6.9090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049358058699271, distance: 0.9540456409653518 entropy -11.043953482721003
epoch: 27, step: 26
	action: tensor([[ 0.0069,  0.0196,  0.0123, -0.0075,  0.0062, -0.0003,  0.0178]],
       dtype=torch.float64)
	q_value: tensor([[6.9084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30662134696090426, distance: 0.9528881513159034 entropy -11.047648246544753
epoch: 27, step: 27
	action: tensor([[ 7.0222e-03,  1.9141e-02,  2.2413e-02, -7.9456e-03,  5.6385e-05,
          2.2156e-02,  4.5338e-03]], dtype=torch.float64)
	q_value: tensor([[6.9300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31146394163630686, distance: 0.9495548094103218 entropy -11.037434952627777
epoch: 27, step: 28
	action: tensor([[ 0.0069,  0.0192,  0.0165, -0.0076, -0.0067, -0.0331, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[6.9218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29867308603106935, distance: 0.9583341095616067 entropy -11.04613672967074
epoch: 27, step: 29
	action: tensor([[ 0.0070,  0.0196,  0.0122, -0.0076, -0.0051,  0.0103, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[6.9250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090850694913544, distance: 0.9511937372049052 entropy -11.050551364129163
epoch: 27, step: 30
	action: tensor([[ 0.0068,  0.0195,  0.0028, -0.0075, -0.0038,  0.0235,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[6.9089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31201690407216387, distance: 0.9491734396935478 entropy -11.047644607097821
epoch: 27, step: 31
	action: tensor([[ 0.0068,  0.0194,  0.0086, -0.0077, -0.0057, -0.0003,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[6.9106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062572167811972, distance: 0.9531383247171138 entropy -11.038693397353269
epoch: 27, step: 32
	action: tensor([[ 0.0070,  0.0192,  0.0076, -0.0079, -0.0092, -0.0091,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[6.9284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042340620250398, distance: 0.9545271265351694 entropy -11.03514832863949
epoch: 27, step: 33
	action: tensor([[ 0.0070,  0.0194,  0.0114, -0.0078, -0.0051, -0.0038,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[6.9235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30561467369832895, distance: 0.9535796198985587 entropy -11.039185687453198
epoch: 27, step: 34
	action: tensor([[ 0.0070,  0.0193,  0.0211, -0.0078, -0.0081, -0.0031, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[6.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30570630671631127, distance: 0.9535166993139462 entropy -11.04148370505094
epoch: 27, step: 35
	action: tensor([[ 0.0068,  0.0195,  0.0053, -0.0074, -0.0114,  0.0101,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[6.9154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089203559050684, distance: 0.9513071123746483 entropy -11.051914721630494
epoch: 27, step: 36
	action: tensor([[ 0.0068,  0.0194,  0.0118, -0.0076, -0.0022, -0.0169,  0.0376]],
       dtype=torch.float64)
	q_value: tensor([[6.9170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025482138108405, distance: 0.9556828415344687 entropy -11.041353297552016
epoch: 27, step: 37
	action: tensor([[ 0.0071,  0.0190,  0.0130, -0.0082, -0.0017,  0.0373,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.9502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3148387546816719, distance: 0.9472248614592257 entropy -11.029754971308021
epoch: 27, step: 38
	action: tensor([[ 0.0067,  0.0192,  0.0160, -0.0077, -0.0100, -0.0005,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[6.9139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060215157929851, distance: 0.9533002266293733 entropy -11.04284886552916
epoch: 27, step: 39
	action: tensor([[ 0.0070,  0.0191,  0.0155, -0.0080,  0.0027,  0.0110, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[6.9410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30869099045590587, distance: 0.9514649660195945 entropy -11.033497096824133
epoch: 27, step: 40
	action: tensor([[ 0.0069,  0.0193,  0.0045, -0.0076, -0.0040,  0.0017, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[6.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068260555259851, distance: 0.9527474787088149 entropy -11.04544212830058
epoch: 27, step: 41
	action: tensor([[ 0.0069,  0.0196,  0.0133, -0.0075, -0.0147,  0.0212,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[6.9068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3115061110267099, distance: 0.9495257312232793 entropy -11.044348371502794
epoch: 27, step: 42
	action: tensor([[ 0.0068,  0.0194,  0.0158, -0.0076, -0.0009,  0.0177, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[6.9155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31054104400598526, distance: 0.9501909768357043 entropy -11.04396859607591
epoch: 27, step: 43
	action: tensor([[ 0.0068,  0.0194, -0.0004, -0.0075, -0.0102,  0.0114, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[6.9104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30914914920755887, distance: 0.951149626392285 entropy -11.047958472063337
epoch: 27, step: 44
	action: tensor([[ 0.0068,  0.0197,  0.0162, -0.0074, -0.0037,  0.0265, -0.0002]],
       dtype=torch.float64)
	q_value: tensor([[6.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128428546397686, distance: 0.948603508699596 entropy -11.04512318198398
epoch: 27, step: 45
	action: tensor([[ 0.0068,  0.0193,  0.0111, -0.0075,  0.0057, -0.0167,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[6.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30245163092575134, distance: 0.9557490105604903 entropy -11.046297211335611
epoch: 27, step: 46
	action: tensor([[ 0.0071,  0.0192,  0.0113, -0.0080, -0.0071,  0.0105,  0.0486]],
       dtype=torch.float64)
	q_value: tensor([[6.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3087862689253129, distance: 0.9513993967530419 entropy -11.037862842406668
epoch: 27, step: 47
	action: tensor([[ 0.0069,  0.0189,  0.0041, -0.0081,  0.0010,  0.0174, -0.0368]],
       dtype=torch.float64)
	q_value: tensor([[6.9436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099690741356549, distance: 0.9505850307057556 entropy -11.030194201425285
epoch: 27, step: 48
	action: tensor([[ 0.0066,  0.0197,  0.0243, -0.0071, -0.0106, -0.0189, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[6.8891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022893679124593, distance: 0.9558601667924675 entropy -11.054411710161773
epoch: 27, step: 49
	action: tensor([[ 0.0069,  0.0194,  0.0030, -0.0076, -0.0034,  0.0147,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[6.9259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30998713368477526, distance: 0.9505725912275106 entropy -11.05115641554377
epoch: 27, step: 50
	action: tensor([[ 0.0068,  0.0192,  0.0072, -0.0079, -0.0056,  0.0042, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[6.9257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070995660101581, distance: 0.9525594940502038 entropy -11.033642391427762
epoch: 27, step: 51
	action: tensor([[ 0.0069,  0.0195,  0.0093, -0.0076, -0.0055,  0.0048,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[6.9107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077140236703255, distance: 0.9521370399225684 entropy -11.043815372927796
epoch: 27, step: 52
	action: tensor([[ 0.0069,  0.0192,  0.0073, -0.0079,  0.0004, -0.0019, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[6.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058698464847692, distance: 0.9534043930338033 entropy -11.037329182079093
epoch: 27, step: 53
	action: tensor([[ 0.0069,  0.0196,  0.0071, -0.0075,  0.0015, -0.0041,  0.0225]],
       dtype=torch.float64)
	q_value: tensor([[6.9098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30573136984562144, distance: 0.9534994887816872 entropy -11.047624154224403
epoch: 27, step: 54
	action: tensor([[ 0.0070,  0.0192,  0.0066, -0.0080, -0.0082, -0.0116,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[6.9296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036560645447288, distance: 0.9549235240226913 entropy -11.034285522022886
epoch: 27, step: 55
	action: tensor([[ 0.0070,  0.0194,  0.0113, -0.0078,  0.0026,  0.0162, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[6.9242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3102793992481131, distance: 0.9503712550841142 entropy -11.038312234308506
epoch: 27, step: 56
	action: tensor([[ 0.0069,  0.0194,  0.0093, -0.0076, -0.0098,  0.0119,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[6.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309168839666621, distance: 0.9511360715806999 entropy -11.04416290946084
epoch: 27, step: 57
	action: tensor([[ 0.0069,  0.0194,  0.0145, -0.0077, -0.0023, -0.0058,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[6.9183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30503501614901385, distance: 0.9539775504833076 entropy -11.040259571442613
epoch: 27, step: 58
	action: tensor([[ 7.0144e-03,  1.9298e-02,  1.7957e-03, -7.7813e-03,  1.6289e-03,
          9.4379e-05,  1.0397e-02]], dtype=torch.float64)
	q_value: tensor([[6.9263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30652167860557067, distance: 0.95295663437474 entropy -11.04201649660467
epoch: 27, step: 59
	action: tensor([[ 0.0069,  0.0194, -0.0073, -0.0078, -0.0110,  0.0155,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[6.9191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31000149864457827, distance: 0.9505626964785873 entropy -11.038215344401095
epoch: 27, step: 60
	action: tensor([[ 0.0067,  0.0197,  0.0065, -0.0075, -0.0210, -0.0334, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[6.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991457716784268, distance: 0.9580111024708948 entropy -11.040727089400546
epoch: 27, step: 61
	action: tensor([[ 0.0070,  0.0196,  0.0136, -0.0077, -0.0157,  0.0233,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[6.9285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3120465162790784, distance: 0.9491530122855825 entropy -11.041517502475378
epoch: 27, step: 62
	action: tensor([[ 0.0068,  0.0193,  0.0094, -0.0076, -0.0046, -0.0144, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[6.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30308571129737527, distance: 0.9553145177728485 entropy -11.044677922941561
epoch: 27, step: 63
	action: tensor([[ 0.0070,  0.0195,  0.0167, -0.0077, -0.0099,  0.0162,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[6.9222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3103498412969278, distance: 0.95032272252783 entropy -11.044345162704314
epoch: 27, step: 64
	action: tensor([[ 0.0069,  0.0193,  0.0085, -0.0076, -0.0113, -0.0177,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[6.9246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30235034452805387, distance: 0.9558183970466814 entropy -11.044081600189992
epoch: 27, step: 65
	action: tensor([[ 0.0070,  0.0194,  0.0092, -0.0077, -0.0030,  0.0033, -0.0539]],
       dtype=torch.float64)
	q_value: tensor([[6.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30736910231703174, distance: 0.9523742045561728 entropy -11.040931876590903
epoch: 27, step: 66
	action: tensor([[ 0.0067,  0.0198,  0.0045, -0.0071, -0.0038,  0.0131, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[6.8877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098255182744659, distance: 0.9506839066770042 entropy -11.058779431740179
epoch: 27, step: 67
	action: tensor([[ 0.0068,  0.0195,  0.0070, -0.0075, -0.0114, -0.0228, -0.0317]],
       dtype=torch.float64)
	q_value: tensor([[6.9107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013751515895141, distance: 0.9564861977403871 entropy -11.043751605374007
epoch: 27, step: 68
	action: tensor([[ 0.0069,  0.0197,  0.0132, -0.0074, -0.0099, -0.0151,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[6.9073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30333192574982415, distance: 0.9551457502335239 entropy -11.049532671958008
epoch: 27, step: 69
	action: tensor([[ 0.0070,  0.0191, -0.0004, -0.0081,  0.0022, -0.0053,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[6.9440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050250716558036, distance: 0.9539843758554893 entropy -11.031847285653628
epoch: 27, step: 70
	action: tensor([[ 0.0070,  0.0193,  0.0004, -0.0080, -0.0081,  0.0068,  0.0639]],
       dtype=torch.float64)
	q_value: tensor([[6.9217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079330471753162, distance: 0.9519864107781015 entropy -11.033045729933992
epoch: 27, step: 71
	action: tensor([[ 0.0068,  0.0190,  0.0208, -0.0083, -0.0100,  0.0277, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[6.9395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31222122180187484, distance: 0.9490324861118573 entropy -11.021264926760892
epoch: 27, step: 72
	action: tensor([[ 0.0068,  0.0193,  0.0012, -0.0074, -0.0165,  0.0007, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[6.9162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30659255725818324, distance: 0.9529079334953258 entropy -11.049955596136869
epoch: 27, step: 73
	action: tensor([[ 0.0068,  0.0196,  0.0098, -0.0075, -0.0054, -0.0051,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[6.9098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054732193134123, distance: 0.953676742594436 entropy -11.043692803939377
epoch: 27, step: 74
	action: tensor([[ 0.0070,  0.0193,  0.0087, -0.0078, -0.0136,  0.0033,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[6.9311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071386139029938, distance: 0.9525326532782372 entropy -11.038598721764213
epoch: 27, step: 75
	action: tensor([[ 0.0069,  0.0192,  0.0157, -0.0079, -0.0122, -0.0369, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[6.9310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297857698653453, distance: 0.9588910442293953 entropy -11.036644876787491
epoch: 27, step: 76
	action: tensor([[ 0.0070,  0.0195,  0.0004, -0.0077, -0.0039, -0.0097,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[6.9329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045489879025979, distance: 0.954311077358171 entropy -11.04644397755646
epoch: 27, step: 77
	action: tensor([[ 0.0070,  0.0194,  0.0306, -0.0079,  0.0005,  0.0266,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.9201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3127280533705016, distance: 0.948682745552119 entropy -11.0356019815178
epoch: 27, step: 78
	action: tensor([[ 0.0069,  0.0189,  0.0063, -0.0078, -0.0034, -0.0067,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[6.9357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045357197203413, distance: 0.9543201807405021 entropy -11.044836770881385
epoch: 27, step: 79
	action: tensor([[ 0.0070,  0.0194,  0.0209, -0.0078, -0.0142, -0.0176, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[6.9217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023620148227698, distance: 0.9558104025404486 entropy -11.038241705306918
epoch: 27, step: 80
	action: tensor([[ 0.0070,  0.0194,  0.0103, -0.0077, -0.0022,  0.0234, -0.0169]],
       dtype=torch.float64)
	q_value: tensor([[6.9305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31202136500785294, distance: 0.9491703624315039 entropy -11.046017017209982
epoch: 27, step: 81
	action: tensor([[ 0.0067,  0.0196,  0.0066, -0.0073, -0.0097, -0.0077,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[6.9002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048519993385459, distance: 0.9541031556860365 entropy -11.049462610131537
epoch: 27, step: 82
	action: tensor([[ 0.0070,  0.0194,  0.0046, -0.0077,  0.0023,  0.0008,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[6.9246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.306699009593299, distance: 0.952834785183113 entropy -11.040294365712404
epoch: 27, step: 83
	action: tensor([[ 0.0070,  0.0194, -0.0012, -0.0078, -0.0063,  0.0100,  0.0225]],
       dtype=torch.float64)
	q_value: tensor([[6.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30877320116248086, distance: 0.9514083900660761 entropy -11.039783688167162
epoch: 27, step: 84
	action: tensor([[ 0.0068,  0.0194,  0.0053, -0.0078, -0.0147, -0.0046,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[6.9151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053131559155411, distance: 0.9537866303267306 entropy -11.034455673218105
epoch: 27, step: 85
	action: tensor([[ 0.0069,  0.0195,  0.0080, -0.0076, -0.0126, -0.0075,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[6.9159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30489496226814117, distance: 0.9540736715329944 entropy -11.041359222089907
epoch: 27, step: 86
	action: tensor([[ 0.0069,  0.0194,  0.0113, -0.0077, -0.0164, -0.0136,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[6.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034498129876245, distance: 0.955064933943701 entropy -11.041301946608685
epoch: 27, step: 87
	action: tensor([[ 0.0070,  0.0194,  0.0168, -0.0077, -0.0119, -0.0220,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[6.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30146778001367425, distance: 0.9564227869224433 entropy -11.040634364539176
epoch: 27, step: 88
	action: tensor([[ 0.0071,  0.0192,  0.0083, -0.0080, -0.0135,  0.0123, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.9423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3091762370595835, distance: 0.9511309792031977 entropy -11.03683818638911
epoch: 27, step: 89
	action: tensor([[ 0.0068,  0.0195,  0.0137, -0.0075, -0.0075, -0.0111,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[6.9124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30397341135981426, distance: 0.9547059042101393 entropy -11.045908384269724
epoch: 27, step: 90
	action: tensor([[ 0.0070,  0.0192,  0.0228, -0.0079, -0.0161,  0.0058,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[6.9392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3076013804427352, distance: 0.9522144988272112 entropy -11.037712928474239
epoch: 27, step: 91
	action: tensor([[ 0.0069,  0.0192,  0.0070, -0.0078, -0.0033,  0.0010,  0.0050]],
       dtype=torch.float64)
	q_value: tensor([[6.9369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065374097190333, distance: 0.9529458257066862 entropy -11.043090857363207
epoch: 27, step: 92
	action: tensor([[ 0.0069,  0.0194,  0.0061, -0.0077, -0.0028, -0.0012, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[6.9210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30620012326211954, distance: 0.9531775445136055 entropy -11.040875724936331
epoch: 27, step: 93
	action: tensor([[ 0.0069,  0.0195,  0.0170, -0.0077, -0.0176,  0.0028, -0.0088]],
       dtype=torch.float64)
	q_value: tensor([[6.9173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30717797165995975, distance: 0.9525055987462395 entropy -11.042797346648323
epoch: 27, step: 94
	action: tensor([[ 0.0069,  0.0195,  0.0131, -0.0075, -0.0096,  0.0229, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[6.9188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31183799719929073, distance: 0.949296845787777 entropy -11.047204695874928
epoch: 27, step: 95
	action: tensor([[ 0.0068,  0.0195,  0.0178, -0.0074, -0.0095,  0.0088,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[6.9057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3085471075099905, distance: 0.9515639756215455 entropy -11.048722254462634
epoch: 27, step: 96
	action: tensor([[ 0.0069,  0.0191,  0.0113, -0.0078, -0.0071, -0.0127, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[6.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30333858948179937, distance: 0.9551411821682342 entropy -11.039131415906132
epoch: 27, step: 97
	action: tensor([[ 0.0070,  0.0195,  0.0144, -0.0077, -0.0042,  0.0255,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[6.9213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.312543583802404, distance: 0.9488100542220654 entropy -11.043214503840671
epoch: 27, step: 98
	action: tensor([[ 0.0068,  0.0192,  0.0081, -0.0077, -0.0168,  0.0069,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[6.9176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078675452360532, distance: 0.9520314609574241 entropy -11.042942396422722
epoch: 27, step: 99
	action: tensor([[ 0.0069,  0.0192, -0.0003, -0.0079, -0.0140, -0.0065, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[6.9287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30480954840164165, distance: 0.9541322875791693 entropy -11.034154715970997
epoch: 27, step: 100
	action: tensor([[ 0.0069,  0.0196,  0.0053, -0.0076, -0.0015, -0.0263,  0.0480]],
       dtype=torch.float64)
	q_value: tensor([[6.9101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006516548030943, distance: 0.9569813387461644 entropy -11.040654222151918
epoch: 27, step: 101
	action: tensor([[ 0.0071,  0.0190,  0.0050, -0.0084,  0.0072, -0.0290,  0.0583]],
       dtype=torch.float64)
	q_value: tensor([[6.9497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993867090323796, distance: 0.9578464173681585 entropy -11.023411178333356
epoch: 27, step: 102
	action: tensor([[ 0.0072,  0.0189,  0.0174, -0.0085, -0.0022, -0.0158,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[6.9537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022706442389631, distance: 0.9558729923771567 entropy -11.021105146370248
epoch: 27, step: 103
	action: tensor([[ 0.0071,  0.0190,  0.0103, -0.0081, -0.0034, -0.0271, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[6.9505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999242693627071, distance: 0.9574788829179209 entropy -11.034109874066363
epoch: 27, step: 104
	action: tensor([[ 0.0071,  0.0195,  0.0137, -0.0078, -0.0177, -0.0098,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[6.9244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044498520804364, distance: 0.9543790929626743 entropy -11.043092801975835
epoch: 27, step: 105
	action: tensor([[ 0.0070,  0.0193,  0.0029, -0.0078, -0.0055,  0.0093, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[6.9368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30857588595463437, distance: 0.9515441732485721 entropy -11.039315430540423
epoch: 27, step: 106
	action: tensor([[ 0.0068,  0.0197,  0.0073, -0.0073, -0.0064,  0.0027, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[6.8965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30737191515467066, distance: 0.9523722707146869 entropy -11.048185881203993
epoch: 27, step: 107
	action: tensor([[ 0.0069,  0.0195,  0.0195, -0.0076, -0.0014,  0.0090, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[6.9148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30861503598220297, distance: 0.9515172335546772 entropy -11.042083783686635
epoch: 27, step: 108
	action: tensor([[ 0.0069,  0.0193,  0.0119, -0.0076, -0.0053,  0.0218,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[6.9227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3114807884113583, distance: 0.9495431927099824 entropy -11.04749928262353
epoch: 27, step: 109
	action: tensor([[ 0.0068,  0.0191,  0.0126, -0.0079, -0.0052,  0.0108, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[6.9256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3086304353366497, distance: 0.9515066368296282 entropy -11.036776864637568
epoch: 27, step: 110
	action: tensor([[ 0.0068,  0.0195, -0.0005, -0.0074, -0.0070, -0.0098,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[6.9071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044183488106904, distance: 0.9544007058698533 entropy -11.048276885436099
epoch: 27, step: 111
	action: tensor([[ 0.0069,  0.0194,  0.0168, -0.0079, -0.0117, -0.0130,  0.0299]],
       dtype=torch.float64)
	q_value: tensor([[6.9218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30338947748265255, distance: 0.9551062971349954 entropy -11.033950305980898
epoch: 27, step: 112
	action: tensor([[ 0.0070,  0.0191, -0.0038, -0.0081, -0.0046, -0.0147, -0.0203]],
       dtype=torch.float64)
	q_value: tensor([[6.9451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029844615403463, distance: 0.9553839107035844 entropy -11.0338001994315
epoch: 27, step: 113
	action: tensor([[ 0.0069,  0.0198, -0.0022, -0.0075, -0.0068, -0.0091, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[6.9038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048219224789812, distance: 0.954123795978609 entropy -11.045351618247954
epoch: 27, step: 114
	action: tensor([[ 0.0069,  0.0196,  0.0080, -0.0076, -0.0093, -0.0091, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[6.9110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304609140847412, distance: 0.9542698049594482 entropy -11.042292766669771
epoch: 27, step: 115
	action: tensor([[ 0.0069,  0.0195,  0.0102, -0.0076, -0.0114,  0.0023, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[6.9219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071196564903226, distance: 0.9525456843342583 entropy -11.043026962450105
epoch: 27, step: 116
	action: tensor([[ 0.0068,  0.0196,  0.0142, -0.0074,  0.0014,  0.0043, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[6.9095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.307634739491883, distance: 0.9521915601962126 entropy -11.049972733950254
epoch: 27, step: 117
	action: tensor([[ 0.0069,  0.0194,  0.0221, -0.0076, -0.0010,  0.0130,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.9190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3094983759532596, distance: 0.9509091918059198 entropy -11.046594597816044
epoch: 27, step: 118
	action: tensor([[ 0.0069,  0.0191,  0.0035, -0.0077, -0.0003, -0.0029, -0.0001]],
       dtype=torch.float64)
	q_value: tensor([[6.9306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056421356981447, distance: 0.9535607633204628 entropy -11.044617474952839
epoch: 27, step: 119
	action: tensor([[ 0.0069,  0.0195,  0.0075, -0.0077, -0.0065,  0.0115, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[6.9120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30925709151214165, distance: 0.951075317087119 entropy -11.041281266897219
epoch: 27, step: 120
	action: tensor([[ 0.0068,  0.0196,  0.0119, -0.0074, -0.0117,  0.0154, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[6.9027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3102169688379882, distance: 0.9504142657790273 entropy -11.047767869127373
epoch: 27, step: 121
	action: tensor([[ 0.0068,  0.0196,  0.0111, -0.0074, -0.0049, -0.0152,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[6.9054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30308246665840255, distance: 0.9553167416095225 entropy -11.048991462822002
epoch: 27, step: 122
	action: tensor([[ 0.0070,  0.0192,  0.0129, -0.0079, -0.0020,  0.0002,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[6.9360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30642436804223805, distance: 0.9530234926247882 entropy -11.038196923906682
epoch: 27, step: 123
	action: tensor([[ 0.0070,  0.0193,  0.0204, -0.0078, -0.0089, -0.0138, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[6.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30317760300014385, distance: 0.9552515341512811 entropy -11.04189612526405
epoch: 27, step: 124
	action: tensor([[ 0.0070,  0.0193,  0.0053, -0.0077, -0.0045, -0.0150,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[6.9343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303071032812402, distance: 0.9553245781893056 entropy -11.045508333099162
epoch: 27, step: 125
	action: tensor([[ 0.0070,  0.0193,  0.0143, -0.0079, -0.0131, -0.0066, -0.0203]],
       dtype=torch.float64)
	q_value: tensor([[6.9289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30495236423173977, distance: 0.954034276887668 entropy -11.037678364782735
epoch: 27, step: 126
	action: tensor([[ 0.0068,  0.0195,  0.0085, -0.0075, -0.0016, -0.0008,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[6.9180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30641493633279515, distance: 0.953029972531018 entropy -11.049325277271937
epoch: 27, step: 127
	action: tensor([[ 0.0070,  0.0193,  0.0085, -0.0078, -0.0071,  0.0050, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[6.9230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30757419180519074, distance: 0.9522331940989773 entropy -11.038620229134308
LOSS epoch 27 actor 21.945371078639536 critic 12.944588106752704
epoch: 28, step: 0
	action: tensor([[ 0.0073,  0.0177,  0.0022, -0.0065, -0.0082,  0.0054,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[7.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30746208730145774, distance: 0.9523102747825886 entropy -11.043162771512225
epoch: 28, step: 1
	action: tensor([[ 0.0073,  0.0175,  0.0189, -0.0067, -0.0121,  0.0073,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.2595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30762525151675646, distance: 0.9521980844540021 entropy -11.035631956138554
epoch: 28, step: 2
	action: tensor([[ 0.0074,  0.0174,  0.0048, -0.0066, -0.0062,  0.0024,  0.0492]],
       dtype=torch.float64)
	q_value: tensor([[7.2696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065189158908872, distance: 0.9529585325917305 entropy -11.042285325462927
epoch: 28, step: 3
	action: tensor([[ 0.0074,  0.0172,  0.0077, -0.0071,  0.0004,  0.0146,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[7.2793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089989892462063, distance: 0.9512529893909014 entropy -11.024767299500146
epoch: 28, step: 4
	action: tensor([[ 0.0073,  0.0175,  0.0072, -0.0066, -0.0176, -0.0285,  0.0238]],
       dtype=torch.float64)
	q_value: tensor([[7.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29963319094197616, distance: 0.957677913155104 entropy -11.040425649483396
epoch: 28, step: 5
	action: tensor([[ 0.0075,  0.0175,  0.0091, -0.0070, -0.0044, -0.0104,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[7.2807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036136465944461, distance: 0.9549526082723456 entropy -11.030343516525473
epoch: 28, step: 6
	action: tensor([[ 0.0075,  0.0174,  0.0015, -0.0068, -0.0031, -0.0024,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[7.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30549197262550365, distance: 0.9536638671238169 entropy -11.036073792840629
epoch: 28, step: 7
	action: tensor([[ 0.0074,  0.0176,  0.0135, -0.0067, -0.0037, -0.0014,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057541299515606, distance: 0.9534838594371516 entropy -11.03748144563828
epoch: 28, step: 8
	action: tensor([[ 0.0074,  0.0174,  0.0087, -0.0068, -0.0110,  0.0064, -0.0160]],
       dtype=torch.float64)
	q_value: tensor([[7.2686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30744201655706016, distance: 0.9523240743430741 entropy -11.037205754346632
epoch: 28, step: 9
	action: tensor([[ 0.0072,  0.0177,  0.0174, -0.0063, -0.0150,  0.0118, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[7.2426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089319662912686, distance: 0.9512991211905245 entropy -11.046133850589593
epoch: 28, step: 10
	action: tensor([[ 0.0073,  0.0175,  0.0221, -0.0065,  0.0116, -0.0040,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[7.2578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30505565199324447, distance: 0.9539633869795859 entropy -11.04429847339097
epoch: 28, step: 11
	action: tensor([[ 0.0076,  0.0170,  0.0062, -0.0071,  0.0026,  0.0205, -0.0373]],
       dtype=torch.float64)
	q_value: tensor([[7.2895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31051100107885443, distance: 0.9502116787259763 entropy -11.035604451235013
epoch: 28, step: 12
	action: tensor([[ 0.0071,  0.0178,  0.0234, -0.0061, -0.0082, -0.0083,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.2260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30428911218406873, distance: 0.9544893639007219 entropy -11.053239609824159
epoch: 28, step: 13
	action: tensor([[ 0.0075,  0.0173, -0.0027, -0.0067, -0.0016, -0.0071, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[7.2739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30442823337601843, distance: 0.9543939245885865 entropy -11.042122923741543
epoch: 28, step: 14
	action: tensor([[ 0.0074,  0.0177, -0.0014, -0.0066, -0.0105,  0.0241, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[7.2475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117792849942481, distance: 0.9493373407018247 entropy -11.039204148846764
epoch: 28, step: 15
	action: tensor([[ 0.0072,  0.0177,  0.0188, -0.0064, -0.0212,  0.0214,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[7.2408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31102587539700255, distance: 0.9498568282789804 entropy -11.041691537161213
epoch: 28, step: 16
	action: tensor([[ 0.0072,  0.0172,  0.0145, -0.0069, -0.0042,  0.0005,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[7.2800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30565883287944595, distance: 0.9535492981427179 entropy -11.034913138618451
epoch: 28, step: 17
	action: tensor([[ 0.0074,  0.0173,  0.0163, -0.0068,  0.0015, -0.0207,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.2750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301025644455884, distance: 0.9567254226619659 entropy -11.037078847451593
epoch: 28, step: 18
	action: tensor([[ 0.0076,  0.0174,  0.0065, -0.0068, -0.0087,  0.0031, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[7.2746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068113155605948, distance: 0.9527576084823861 entropy -11.040337247024175
epoch: 28, step: 19
	action: tensor([[ 0.0073,  0.0177,  0.0071, -0.0064, -0.0014, -0.0174, -0.0344]],
       dtype=torch.float64)
	q_value: tensor([[7.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022672756065131, distance: 0.9558752998484328 entropy -11.045698978156066
epoch: 28, step: 20
	action: tensor([[ 0.0073,  0.0178,  0.0067, -0.0063, -0.0018, -0.0027,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[7.2387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30578960995358306, distance: 0.9534594948402889 entropy -11.048783892718168
epoch: 28, step: 21
	action: tensor([[ 0.0074,  0.0174,  0.0089, -0.0069, -0.0130, -0.0130, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[7.2672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30297419988537166, distance: 0.955390943390401 entropy -11.032749487715362
epoch: 28, step: 22
	action: tensor([[ 0.0074,  0.0177,  0.0012, -0.0065, -0.0061,  0.0016,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066816219491407, distance: 0.9528467334195503 entropy -11.044516648150113
epoch: 28, step: 23
	action: tensor([[ 0.0073,  0.0176,  0.0207, -0.0067, -0.0142,  0.0193,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[7.2522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31052319848215293, distance: 0.9502032738302191 entropy -11.036268911817023
epoch: 28, step: 24
	action: tensor([[ 0.0073,  0.0173,  0.0122, -0.0067, -0.0110,  0.0047,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[7.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068814143731311, distance: 0.9527094333844366 entropy -11.040951633841704
epoch: 28, step: 25
	action: tensor([[ 0.0074,  0.0174,  0.0171, -0.0067, -0.0062,  0.0041,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[7.2687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068546256443022, distance: 0.9527278441070715 entropy -11.038146932898021
epoch: 28, step: 26
	action: tensor([[ 0.0074,  0.0173,  0.0111, -0.0068, -0.0185,  0.0018,  0.0378]],
       dtype=torch.float64)
	q_value: tensor([[7.2753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30630183003932065, distance: 0.9531076769822506 entropy -11.038435004395112
epoch: 28, step: 27
	action: tensor([[ 0.0074,  0.0173,  0.0099, -0.0070, -0.0095, -0.0208, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[7.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30099585686173447, distance: 0.956745808420584 entropy -11.030106010426847
epoch: 28, step: 28
	action: tensor([[ 0.0074,  0.0177,  0.0118, -0.0065,  0.0020, -0.0024, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[7.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057755519178593, distance: 0.9534691487578725 entropy -11.043915520154231
epoch: 28, step: 29
	action: tensor([[ 0.0074,  0.0176, -0.0006, -0.0065, -0.0039,  0.0065,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.2504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077352506468898, distance: 0.9521224425262221 entropy -11.045122156889633
epoch: 28, step: 30
	action: tensor([[ 0.0073,  0.0176,  0.0208, -0.0066, -0.0164, -0.0192,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[7.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016579916041521, distance: 0.9562925602267942 entropy -11.038865145645358
epoch: 28, step: 31
	action: tensor([[ 0.0075,  0.0174,  0.0107, -0.0068,  0.0004, -0.0283,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[7.2841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994521230856003, distance: 0.9578017007749168 entropy -11.038609770578233
epoch: 28, step: 32
	action: tensor([[ 0.0076,  0.0173,  0.0085, -0.0071, -0.0100,  0.0003, -0.0420]],
       dtype=torch.float64)
	q_value: tensor([[7.2858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30597378415084664, distance: 0.9533330099244729 entropy -11.030566713391883
epoch: 28, step: 33
	action: tensor([[ 0.0072,  0.0178,  0.0133, -0.0061, -0.0167,  0.0067, -0.0308]],
       dtype=torch.float64)
	q_value: tensor([[7.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30787876999058517, distance: 0.9520237410746838 entropy -11.053566796990442
epoch: 28, step: 34
	action: tensor([[ 0.0071,  0.0177,  0.0041, -0.0062, -0.0146,  0.0020, -0.0317]],
       dtype=torch.float64)
	q_value: tensor([[7.2432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30670740372154104, distance: 0.9528290169513489 entropy -11.051468968537064
epoch: 28, step: 35
	action: tensor([[ 0.0071,  0.0179,  0.0110, -0.0062, -0.0063, -0.0047, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[7.2345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30523264372749637, distance: 0.9538418992758562 entropy -11.049854653343571
epoch: 28, step: 36
	action: tensor([[ 0.0074,  0.0175,  0.0078, -0.0066, -0.0177,  0.0122,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.2626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30889259545710557, distance: 0.9513262190337818 entropy -11.041375687723585
epoch: 28, step: 37
	action: tensor([[ 0.0073,  0.0176,  0.0007, -0.0065, -0.0099,  0.0064, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[7.2565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30756230767969805, distance: 0.9522413656677484 entropy -11.041057912466817
epoch: 28, step: 38
	action: tensor([[ 0.0073,  0.0177,  0.0188, -0.0065, -0.0144,  0.0251,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[7.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3119952385144146, distance: 0.9491883849808637 entropy -11.04191755378857
epoch: 28, step: 39
	action: tensor([[ 0.0073,  0.0173,  0.0173, -0.0067,  0.0026, -0.0182, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[7.2671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30151749688820717, distance: 0.956388750412425 entropy -11.040071725759052
epoch: 28, step: 40
	action: tensor([[ 0.0075,  0.0175, -0.0041, -0.0066, -0.0117,  0.0351, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[7.2633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3142021255301294, distance: 0.9476648242781451 entropy -11.046131902673094
epoch: 28, step: 41
	action: tensor([[ 0.0071,  0.0179,  0.0084, -0.0063, -0.0076,  0.0150, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3096544996672126, distance: 0.950801684551645 entropy -11.043740307131657
epoch: 28, step: 42
	action: tensor([[ 0.0073,  0.0176,  0.0154, -0.0065, -0.0103,  0.0028,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[7.2486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067176038010745, distance: 0.9528220076687209 entropy -11.04231190454004
epoch: 28, step: 43
	action: tensor([[ 0.0074,  0.0174,  0.0077, -0.0067, -0.0167,  0.0077,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[7.2672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077487786338037, distance: 0.9521131394648709 entropy -11.038834711673664
epoch: 28, step: 44
	action: tensor([[ 0.0073,  0.0176, -0.0086, -0.0065, -0.0093, -0.0191, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.2557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019315207880223, distance: 0.9561052597787523 entropy -11.04179754991217
epoch: 28, step: 45
	action: tensor([[ 0.0073,  0.0180,  0.0206, -0.0065, -0.0037,  0.0310,  0.0454]],
       dtype=torch.float64)
	q_value: tensor([[7.2400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31368053364041404, distance: 0.9480251347684915 entropy -11.04006493514468
epoch: 28, step: 46
	action: tensor([[ 0.0072,  0.0169,  0.0051, -0.0070, -0.0009,  0.0045,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[7.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30635092225391247, distance: 0.9530739512233511 entropy -11.034641666813458
epoch: 28, step: 47
	action: tensor([[ 0.0074,  0.0174,  0.0077, -0.0068, -0.0018, -0.0035,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[7.2595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30512899193898047, distance: 0.9539130480789166 entropy -11.034371396096217
epoch: 28, step: 48
	action: tensor([[ 0.0074,  0.0173, -0.0016, -0.0070, -0.0075,  0.0179, -0.0130]],
       dtype=torch.float64)
	q_value: tensor([[7.2768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099451050458627, distance: 0.9506015404441182 entropy -11.03101734030601
epoch: 28, step: 49
	action: tensor([[ 0.0072,  0.0178,  0.0091, -0.0063, -0.0131, -0.0123,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[7.2351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034595095298318, distance: 0.9550582862822367 entropy -11.044369504698556
epoch: 28, step: 50
	action: tensor([[ 0.0074,  0.0174,  0.0113, -0.0069, -0.0023,  0.0092, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[7.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30808644417788167, distance: 0.9518809007879143 entropy -11.033165994147181
epoch: 28, step: 51
	action: tensor([[ 0.0073,  0.0176,  0.0045, -0.0064, -0.0148,  0.0052,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[7.2508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30735677411538775, distance: 0.9523826802168955 entropy -11.045889742779071
epoch: 28, step: 52
	action: tensor([[ 0.0073,  0.0174,  0.0240, -0.0068, -0.0138, -0.0058, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[7.2684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30446838472893056, distance: 0.9543663783581764 entropy -11.033006294500396
epoch: 28, step: 53
	action: tensor([[ 0.0073,  0.0176,  0.0103, -0.0064, -0.0189,  0.0049,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[7.2587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072364115255507, distance: 0.9524654257503146 entropy -11.049159229581493
epoch: 28, step: 54
	action: tensor([[ 0.0073,  0.0174,  0.0100, -0.0069, -0.0006, -0.0049, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.2714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046918880864098, distance: 0.9542130271489885 entropy -11.033064879646963
epoch: 28, step: 55
	action: tensor([[ 0.0074,  0.0176,  0.0045, -0.0065, -0.0190, -0.0308, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[7.2501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29937454551610077, distance: 0.9578547320333761 entropy -11.044194302238694
epoch: 28, step: 56
	action: tensor([[ 0.0074,  0.0179,  0.0029, -0.0065, -0.0026, -0.0133, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[7.2525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3034140096355771, distance: 0.9550894792580941 entropy -11.04128047798343
epoch: 28, step: 57
	action: tensor([[ 0.0074,  0.0178,  0.0038, -0.0065, -0.0104,  0.0057,  0.0341]],
       dtype=torch.float64)
	q_value: tensor([[7.2434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3076918900609711, distance: 0.9521522605550611 entropy -11.043592908413013
epoch: 28, step: 58
	action: tensor([[ 0.0073,  0.0173,  0.0017, -0.0069, -0.0049,  0.0130,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[7.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30876806760201636, distance: 0.9514119229900289 entropy -11.030112434245638
epoch: 28, step: 59
	action: tensor([[ 0.0073,  0.0176,  0.0216, -0.0066,  0.0016,  0.0092,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.2479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30817243501940617, distance: 0.9518217491913543 entropy -11.037926354904865
epoch: 28, step: 60
	action: tensor([[ 0.0074,  0.0172,  0.0013, -0.0067, -0.0045, -0.0086, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[7.2722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039898060529638, distance: 0.9546946602413209 entropy -11.042582037365849
epoch: 28, step: 61
	action: tensor([[ 0.0073,  0.0179,  0.0107, -0.0063, -0.0012,  0.0153,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[7.2369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099467274845924, distance: 0.9506004229289222 entropy -11.047422121297894
epoch: 28, step: 62
	action: tensor([[ 0.0074,  0.0172,  0.0063, -0.0070, -0.0190,  0.0060,  0.0388]],
       dtype=torch.float64)
	q_value: tensor([[7.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070462279909031, distance: 0.9525961563586786 entropy -11.032476445546369
epoch: 28, step: 63
	action: tensor([[ 0.0073,  0.0173,  0.0188, -0.0070, -0.0013,  0.0178, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[7.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30989519142073074, distance: 0.9506359196734419 entropy -11.02932095477536
epoch: 28, step: 64
	action: tensor([[ 0.0072,  0.0175,  0.0123, -0.0064, -0.0083, -0.0225,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[7.2490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30090265972093877, distance: 0.9568095870138842 entropy -11.048155801259014
epoch: 28, step: 65
	action: tensor([[ 0.0075,  0.0174,  0.0130, -0.0069, -0.0056,  0.0213,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[7.2788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31096274274301694, distance: 0.9499003463202689 entropy -11.034844487777123
epoch: 28, step: 66
	action: tensor([[ 0.0073,  0.0173,  0.0005, -0.0067, -0.0119,  0.0152,  0.0026]],
       dtype=torch.float64)
	q_value: tensor([[7.2666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093173650711779, distance: 0.9510338212250935 entropy -11.038694619192
epoch: 28, step: 67
	action: tensor([[ 0.0072,  0.0177,  0.0241, -0.0065, -0.0012, -0.0242,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[7.2429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30037930896868215, distance: 0.9571676582776276 entropy -11.039488916850278
epoch: 28, step: 68
	action: tensor([[ 0.0076,  0.0172,  0.0189, -0.0070, -0.0111, -0.0120,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[7.2904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030690988741942, distance: 0.9553259036740449 entropy -11.036017803150708
epoch: 28, step: 69
	action: tensor([[ 0.0075,  0.0174,  0.0006, -0.0067, -0.0017,  0.0003,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[7.2743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061561932835356, distance: 0.9532077206549633 entropy -11.041684390383448
epoch: 28, step: 70
	action: tensor([[ 0.0074,  0.0175,  0.0090, -0.0068, -0.0126, -0.0100,  0.0258]],
       dtype=torch.float64)
	q_value: tensor([[7.2555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037539874179056, distance: 0.9548563789396293 entropy -11.033784759805968
epoch: 28, step: 71
	action: tensor([[ 7.4423e-03,  1.7393e-02,  6.1850e-03, -6.9256e-03, -1.0709e-02,
         -2.1034e-05, -1.0382e-02]], dtype=torch.float64)
	q_value: tensor([[7.2736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059324683349255, distance: 0.953361385758176 entropy -11.031744941770041
epoch: 28, step: 72
	action: tensor([[ 0.0073,  0.0177, -0.0041, -0.0064, -0.0068, -0.0244, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[7.2455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30083505600160376, distance: 0.9568558483278459 entropy -11.043637999831704
epoch: 28, step: 73
	action: tensor([[ 0.0074,  0.0178, -0.0016, -0.0067, -0.0046,  0.0100,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[7.2486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3086528492436811, distance: 0.9514912129845217 entropy -11.036716630017178
epoch: 28, step: 74
	action: tensor([[ 7.2978e-03,  1.7446e-02, -2.9402e-03, -6.9026e-03,  7.0033e-03,
          4.0338e-05, -3.7885e-03]], dtype=torch.float64)
	q_value: tensor([[7.2602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058817604861248, distance: 0.9533962109155689 entropy -11.029840458142868
epoch: 28, step: 75
	action: tensor([[ 0.0074,  0.0177,  0.0075, -0.0066, -0.0133,  0.0165,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[7.2453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099890262026478, distance: 0.950571287645404 entropy -11.039639905822499
epoch: 28, step: 76
	action: tensor([[ 0.0073,  0.0173,  0.0075, -0.0068, -0.0033, -0.0021,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[7.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30526114805740856, distance: 0.9538223323641345 entropy -11.034071433270313
epoch: 28, step: 77
	action: tensor([[ 0.0074,  0.0173,  0.0050, -0.0070, -0.0218,  0.0020,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[7.2710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3063273823998538, distance: 0.9530901229678975 entropy -11.030831248378915
epoch: 28, step: 78
	action: tensor([[ 0.0073,  0.0175,  0.0201, -0.0068, -0.0126,  0.0195, -0.0189]],
       dtype=torch.float64)
	q_value: tensor([[7.2675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3104594818193177, distance: 0.9502471784132273 entropy -11.034401450178473
epoch: 28, step: 79
	action: tensor([[ 0.0072,  0.0175,  0.0291, -0.0063, -0.0130,  0.0185,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[7.2534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3103082483109535, distance: 0.9503513792048575 entropy -11.049713042762063
epoch: 28, step: 80
	action: tensor([[ 0.0074,  0.0172,  0.0239, -0.0068,  0.0010, -0.0008,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[7.2798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30546381687817337, distance: 0.9536831979635538 entropy -11.040895469963457
epoch: 28, step: 81
	action: tensor([[ 0.0075,  0.0171,  0.0062, -0.0070, -0.0054, -0.0021, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.2904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30526683052364556, distance: 0.9538184315643824 entropy -11.03606747259803
epoch: 28, step: 82
	action: tensor([[ 0.0074,  0.0176,  0.0124, -0.0066,  0.0184, -0.0130,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[7.2540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303052506540896, distance: 0.955337275670489 entropy -11.041796208715988
epoch: 28, step: 83
	action: tensor([[ 7.6027e-03,  1.6989e-02,  1.3336e-02, -7.2695e-03, -5.8428e-03,
          4.1189e-05,  2.0367e-02]], dtype=torch.float64)
	q_value: tensor([[7.2917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056274339643299, distance: 0.9535708582031116 entropy -11.028317689971944
epoch: 28, step: 84
	action: tensor([[ 0.0074,  0.0173,  0.0148, -0.0069, -0.0100, -0.0090, -0.0524]],
       dtype=torch.float64)
	q_value: tensor([[7.2745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30383632872512323, distance: 0.9547999143812853 entropy -11.036404897844978
epoch: 28, step: 85
	action: tensor([[ 0.0072,  0.0178,  0.0028, -0.0061, -0.0044,  0.0124, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[7.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3092528764742971, distance: 0.9510782188992764 entropy -11.056816295942017
epoch: 28, step: 86
	action: tensor([[ 7.2704e-03,  1.7648e-02,  9.9130e-03, -6.5341e-03,  1.9404e-05,
         -1.9609e-02,  1.9714e-02]], dtype=torch.float64)
	q_value: tensor([[7.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30156979219942015, distance: 0.9563529473799199 entropy -11.040299045438246
epoch: 28, step: 87
	action: tensor([[ 0.0075,  0.0173,  0.0072, -0.0070, -0.0066,  0.0174,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[7.2793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309964395356102, distance: 0.9505882534383168 entropy -11.033206338524717
epoch: 28, step: 88
	action: tensor([[ 0.0073,  0.0176,  0.0161, -0.0065, -0.0106, -0.0176,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[7.2522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019878228374445, distance: 0.9560667021209784 entropy -11.04184136565561
epoch: 28, step: 89
	action: tensor([[ 0.0075,  0.0173,  0.0113, -0.0069, -0.0086,  0.0106, -0.0331]],
       dtype=torch.float64)
	q_value: tensor([[7.2808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3084457128620718, distance: 0.9516337417317915 entropy -11.036657220660416
epoch: 28, step: 90
	action: tensor([[ 0.0071,  0.0178,  0.0146, -0.0061, -0.0160,  0.0042,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[7.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.307229600421866, distance: 0.952470107957155 entropy -11.051419622172935
epoch: 28, step: 91
	action: tensor([[ 0.0073,  0.0175,  0.0172, -0.0066,  0.0016,  0.0052,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[7.2655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30721907469621434, distance: 0.9524773436884834 entropy -11.041718269056876
epoch: 28, step: 92
	action: tensor([[ 0.0075,  0.0173, -0.0115, -0.0068, -0.0128, -0.0054, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.2707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047737094794196, distance: 0.9541568812945067 entropy -11.038069882080517
epoch: 28, step: 93
	action: tensor([[ 0.0073,  0.0180,  0.0204, -0.0064, -0.0057,  0.0211, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[7.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3113082032348897, distance: 0.9496621921573424 entropy -11.039034446768643
epoch: 28, step: 94
	action: tensor([[ 0.0073,  0.0174,  0.0070, -0.0065, -0.0113,  0.0093, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[7.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080768837761987, distance: 0.9518874769940413 entropy -11.044675175233357
epoch: 28, step: 95
	action: tensor([[ 0.0072,  0.0178, -0.0035, -0.0062, -0.0072, -0.0218,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[7.2368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014087791575596, distance: 0.956463177737713 entropy -11.049707992412209
epoch: 28, step: 96
	action: tensor([[ 0.0075,  0.0175,  0.0105, -0.0070,  0.0015, -0.0084,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[7.2728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039641648756646, distance: 0.9547122456659378 entropy -11.02611550078529
epoch: 28, step: 97
	action: tensor([[ 0.0075,  0.0173,  0.0010, -0.0069, -0.0176,  0.0208,  0.0178]],
       dtype=torch.float64)
	q_value: tensor([[7.2717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3106662456958853, distance: 0.9501046983731917 entropy -11.033832777004118
epoch: 28, step: 98
	action: tensor([[ 0.0072,  0.0176, -0.0016, -0.0067, -0.0026,  0.0371,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[7.2555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3144997853242668, distance: 0.9474591424424783 entropy -11.036347325770084
epoch: 28, step: 99
	action: tensor([[ 0.0071,  0.0175,  0.0051, -0.0067, -0.0105,  0.0211, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[7.2489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31063843813026126, distance: 0.9501238616827967 entropy -11.03444439094388
epoch: 28, step: 100
	action: tensor([[ 0.0072,  0.0177,  0.0089, -0.0064, -0.0195, -0.0036,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[7.2415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053420994058784, distance: 0.9537667607970975 entropy -11.04293514706907
epoch: 28, step: 101
	action: tensor([[ 0.0073,  0.0176,  0.0073, -0.0066, -0.0067, -0.0020, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[7.2598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30567344610118796, distance: 0.9535392638099482 entropy -11.038844804682668
epoch: 28, step: 102
	action: tensor([[ 0.0073,  0.0177,  0.0074, -0.0065, -0.0010, -0.0315,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[7.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990192538830124, distance: 0.9580975683708485 entropy -11.043392001127762
epoch: 28, step: 103
	action: tensor([[ 0.0075,  0.0176,  0.0177, -0.0068, -0.0061, -0.0133,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[7.2681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030872466867275, distance: 0.9553134654335794 entropy -11.037795126558054
epoch: 28, step: 104
	action: tensor([[ 0.0075,  0.0173,  0.0113, -0.0069, -0.0135,  0.0415,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[7.2799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3155220100212992, distance: 0.9467524486670693 entropy -11.035538742736986
epoch: 28, step: 105
	action: tensor([[ 0.0071,  0.0175,  0.0102, -0.0065, -0.0189, -0.0102,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[7.2492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035881739320504, distance: 0.9549700734070626 entropy -11.042857561861059
epoch: 28, step: 106
	action: tensor([[ 0.0074,  0.0176, -0.0022, -0.0067,  0.0018, -0.0300,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[7.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993337097652432, distance: 0.9578826457688918 entropy -11.037487032549786
epoch: 28, step: 107
	action: tensor([[ 0.0075,  0.0177,  0.0136, -0.0068, -0.0031, -0.0055,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.2593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049528432378451, distance: 0.9540339481416235 entropy -11.035293073689092
epoch: 28, step: 108
	action: tensor([[ 0.0075,  0.0174,  0.0104, -0.0067,  0.0126,  0.0227,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[7.2699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3113963833040806, distance: 0.9496013928571801 entropy -11.039787561883818
epoch: 28, step: 109
	action: tensor([[ 0.0073,  0.0172,  0.0070, -0.0069, -0.0082,  0.0064, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.2680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071861612184702, distance: 0.9524999691450826 entropy -11.036390250002942
epoch: 28, step: 110
	action: tensor([[ 0.0073,  0.0177, -0.0059, -0.0064,  0.0071,  0.0046, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.2418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3073665651072909, distance: 0.9523759488985809 entropy -11.045238544921531
epoch: 28, step: 111
	action: tensor([[ 0.0073,  0.0178,  0.0144, -0.0066, -0.0056, -0.0042, -0.0130]],
       dtype=torch.float64)
	q_value: tensor([[7.2386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30520934251749443, distance: 0.9538578941868535 entropy -11.037964421066679
epoch: 28, step: 112
	action: tensor([[ 0.0073,  0.0176,  0.0049, -0.0065, -0.0045, -0.0120,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[7.2586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30341558830314663, distance: 0.9550883970014438 entropy -11.045584800349006
epoch: 28, step: 113
	action: tensor([[ 0.0074,  0.0175,  0.0081, -0.0068, -0.0074, -0.0061,  0.0238]],
       dtype=torch.float64)
	q_value: tensor([[7.2611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047144488543537, distance: 0.9541975462758782 entropy -11.035318627321558
epoch: 28, step: 114
	action: tensor([[ 0.0074,  0.0174,  0.0105, -0.0069, -0.0193, -0.0010, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[7.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057086772512375, distance: 0.9535150715110512 entropy -11.032166938757237
epoch: 28, step: 115
	action: tensor([[ 0.0073,  0.0177,  0.0041, -0.0064, -0.0050, -0.0099,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.2496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30400601332423016, distance: 0.9546835446818459 entropy -11.044938970348896
epoch: 28, step: 116
	action: tensor([[ 0.0074,  0.0175,  0.0107, -0.0068, -0.0222, -0.0093,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[7.2670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303918936397815, distance: 0.9547432638104331 entropy -11.034074015709626
epoch: 28, step: 117
	action: tensor([[ 0.0074,  0.0175,  0.0157, -0.0068, -0.0121,  0.0042, -0.0171]],
       dtype=torch.float64)
	q_value: tensor([[7.2707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30698276109212463, distance: 0.9526397789941338 entropy -11.034908113818124
epoch: 28, step: 118
	action: tensor([[ 0.0073,  0.0176,  0.0028, -0.0064, -0.0033,  0.0070,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[7.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077930956771513, distance: 0.9520826624405567 entropy -11.047651869497916
epoch: 28, step: 119
	action: tensor([[ 0.0073,  0.0176,  0.0102, -0.0066, -0.0158,  0.0301,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[7.2541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.312963040367812, distance: 0.9485205483537492 entropy -11.038584402411445
epoch: 28, step: 120
	action: tensor([[ 0.0072,  0.0173, -0.0051, -0.0068, -0.0171,  0.0252, -0.0088]],
       dtype=torch.float64)
	q_value: tensor([[7.2643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31140091902108635, distance: 0.9495982654191488 entropy -11.035591673980386
epoch: 28, step: 121
	action: tensor([[ 0.0071,  0.0179,  0.0040, -0.0063, -0.0099,  0.0143, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[7.2318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30952565245836927, distance: 0.9508904099995639 entropy -11.043194803660683
epoch: 28, step: 122
	action: tensor([[ 0.0072,  0.0178,  0.0066, -0.0062, -0.0144, -0.0038,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[7.2326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054382160894995, distance: 0.9537007743099156 entropy -11.047060643788232
epoch: 28, step: 123
	action: tensor([[ 0.0074,  0.0174,  0.0104, -0.0069, -0.0052, -0.0150,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.2749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30242645668096235, distance: 0.9557662567068066 entropy -11.031775238487176
epoch: 28, step: 124
	action: tensor([[ 0.0075,  0.0175,  0.0220, -0.0068, -0.0051, -0.0048,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[7.2685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049321195423371, distance: 0.9540481708900228 entropy -11.038435945788748
epoch: 28, step: 125
	action: tensor([[ 0.0075,  0.0172,  0.0215, -0.0070, -0.0075,  0.0129, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[7.2854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3087684810978807, distance: 0.9514116384220452 entropy -11.035303768641644
epoch: 28, step: 126
	action: tensor([[ 0.0073,  0.0175,  0.0045, -0.0064, -0.0175,  0.0078,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[7.2558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078522850453511, distance: 0.9520419561317724 entropy -11.046982036509949
epoch: 28, step: 127
	action: tensor([[ 0.0073,  0.0177,  0.0179, -0.0065, -0.0052,  0.0105,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[7.2525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30854454633646167, distance: 0.9515657379384256 entropy -11.041065320489494
LOSS epoch 28 actor 24.24828025382461 critic 12.546094282020672
epoch: 29, step: 0
	action: tensor([[ 0.0074,  0.0154,  0.0093, -0.0050, -0.0152,  0.0023, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[7.5396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058016837115567, distance: 0.953451203485537 entropy -11.033400227163927
epoch: 29, step: 1
	action: tensor([[ 0.0072,  0.0158,  0.0268, -0.0046, -0.0040, -0.0024,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[7.5059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050471461963652, distance: 0.9539692249966839 entropy -11.042294243382017
epoch: 29, step: 2
	action: tensor([[ 0.0075,  0.0153,  0.0050, -0.0050, -0.0097,  0.0108,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[7.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078005077494479, distance: 0.9520775650308274 entropy -11.038016093586924
epoch: 29, step: 3
	action: tensor([[ 0.0072,  0.0157,  0.0248, -0.0048,  0.0020,  0.0087, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[7.5110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074853530656936, distance: 0.9522942782486706 entropy -11.034101261163595
epoch: 29, step: 4
	action: tensor([[ 0.0073,  0.0154,  0.0095, -0.0047, -0.0080,  0.0163, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[7.5234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3092178645227157, distance: 0.9511023222805778 entropy -11.044628860398106
epoch: 29, step: 5
	action: tensor([[ 0.0072,  0.0158,  0.0157, -0.0046, -0.0126,  0.0324, -0.0234]],
       dtype=torch.float64)
	q_value: tensor([[7.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31312177742214276, distance: 0.9484109661445334 entropy -11.041250064575653
epoch: 29, step: 6
	action: tensor([[ 0.0070,  0.0158,  0.0102, -0.0042, -0.0208,  0.0175,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[7.4906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309544239889872, distance: 0.9508776110234858 entropy -11.048656266698988
epoch: 29, step: 7
	action: tensor([[ 0.0072,  0.0157,  0.0096, -0.0047, -0.0040,  0.0026, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.5139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060522402424659, distance: 0.9532791237065081 entropy -11.038606266887328
epoch: 29, step: 8
	action: tensor([[ 0.0073,  0.0158,  0.0175, -0.0046, -0.0102,  0.0046, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[7.5003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067162008442753, distance: 0.9528229717545286 entropy -11.042594003003435
epoch: 29, step: 9
	action: tensor([[ 0.0072,  0.0157,  0.0132, -0.0046, -0.0017,  0.0041,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[7.5087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30651400045808763, distance: 0.952961909897496 entropy -11.044021637514883
epoch: 29, step: 10
	action: tensor([[ 0.0074,  0.0153,  0.0135, -0.0052, -0.0123,  0.0020, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[7.5395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30564383716762666, distance: 0.9535595950068199 entropy -11.030071868852932
epoch: 29, step: 11
	action: tensor([[ 0.0072,  0.0158,  0.0107, -0.0045, -0.0225, -0.0179, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[7.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30163133471873027, distance: 0.9563108116978786 entropy -11.04587894312587
epoch: 29, step: 12
	action: tensor([[ 0.0072,  0.0160,  0.0004, -0.0045, -0.0069,  0.0045,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[7.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30691170437777593, distance: 0.9526886159593637 entropy -11.044839354523537
epoch: 29, step: 13
	action: tensor([[ 0.0073,  0.0155,  0.0066, -0.0052, -0.0076,  0.0099,  0.0425]],
       dtype=torch.float64)
	q_value: tensor([[7.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30748695479800237, distance: 0.9522931769568773 entropy -11.023235647940705
epoch: 29, step: 14
	action: tensor([[ 7.2770e-03,  1.5421e-02,  1.0573e-02, -5.1707e-03, -7.9781e-05,
          1.7663e-03,  2.5249e-03]], dtype=torch.float64)
	q_value: tensor([[7.5354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055563072005286, distance: 0.9536196955821634 entropy -11.027036137923167
epoch: 29, step: 15
	action: tensor([[ 0.0073,  0.0156,  0.0102, -0.0048, -0.0139,  0.0066, -0.0232]],
       dtype=torch.float64)
	q_value: tensor([[7.5147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.307075849781172, distance: 0.9525757957615822 entropy -11.039457532301252
epoch: 29, step: 16
	action: tensor([[ 0.0071,  0.0159,  0.0161, -0.0044, -0.0081,  0.0074,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[7.4916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074447869098528, distance: 0.9523221696099252 entropy -11.046240976627404
epoch: 29, step: 17
	action: tensor([[ 0.0073,  0.0155,  0.0086, -0.0049, -0.0019, -0.0132,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[7.5252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30232668346322844, distance: 0.9558346053893106 entropy -11.035769170131744
epoch: 29, step: 18
	action: tensor([[ 0.0075,  0.0154,  0.0111, -0.0052, -0.0091,  0.0068,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.5379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068831994357072, distance: 0.9527082065762932 entropy -11.027573738439452
epoch: 29, step: 19
	action: tensor([[ 0.0073,  0.0157,  0.0136, -0.0047,  0.0060,  0.0052,  0.0386]],
       dtype=torch.float64)
	q_value: tensor([[7.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30676415161426107, distance: 0.9527900203245021 entropy -11.039771422529215
epoch: 29, step: 20
	action: tensor([[ 0.0074,  0.0153,  0.0083, -0.0052, -0.0117,  0.0200, -0.0300]],
       dtype=torch.float64)
	q_value: tensor([[7.5373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30977626207051034, distance: 0.9507178301602598 entropy -11.028941282684979
epoch: 29, step: 21
	action: tensor([[ 7.0052e-03,  1.5997e-02,  1.3427e-02, -4.1953e-03,  1.3660e-03,
         -3.0287e-03, -8.0932e-05]], dtype=torch.float64)
	q_value: tensor([[7.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305025468021456, distance: 0.9539841038120832 entropy -11.04868047808216
epoch: 29, step: 22
	action: tensor([[ 0.0074,  0.0156,  0.0156, -0.0048, -0.0179, -0.0106,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[7.5185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3031029130422488, distance: 0.9553027278165513 entropy -11.040153687444697
epoch: 29, step: 23
	action: tensor([[ 0.0074,  0.0155,  0.0169, -0.0050, -0.0160, -0.0142,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.5334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30216019021726015, distance: 0.9559486491036827 entropy -11.032054253099377
epoch: 29, step: 24
	action: tensor([[ 0.0074,  0.0156,  0.0065, -0.0049, -0.0080, -0.0099, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[7.5303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30326429549599343, distance: 0.9551921103182522 entropy -11.034807540985403
epoch: 29, step: 25
	action: tensor([[ 7.3186e-03,  1.5813e-02,  1.5195e-02, -4.6303e-03,  9.7340e-05,
         -1.5984e-02, -1.6225e-02]], dtype=torch.float64)
	q_value: tensor([[7.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30199309005365993, distance: 0.956063094863325 entropy -11.040635278757353
epoch: 29, step: 26
	action: tensor([[ 0.0074,  0.0157,  0.0037, -0.0047, -0.0079, -0.0133,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[7.5135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30269518700491516, distance: 0.955582141268849 entropy -11.044206747068246
epoch: 29, step: 27
	action: tensor([[ 0.0074,  0.0157,  0.0151, -0.0049, -0.0004,  0.0063,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.5191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30706418077952957, distance: 0.9525838165252725 entropy -11.032660042677843
epoch: 29, step: 28
	action: tensor([[ 0.0073,  0.0156,  0.0063, -0.0048, -0.0173, -0.0059, -0.0278]],
       dtype=torch.float64)
	q_value: tensor([[7.5152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304201285126001, distance: 0.9545496097217472 entropy -11.039788968156984
epoch: 29, step: 29
	action: tensor([[ 0.0071,  0.0160,  0.0074, -0.0044, -0.0051,  0.0104,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[7.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3082032545761506, distance: 0.9518005480621979 entropy -11.046267193877751
epoch: 29, step: 30
	action: tensor([[ 0.0072,  0.0157,  0.0151, -0.0047, -0.0055, -0.0011, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[7.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30526884381914665, distance: 0.9538170495087726 entropy -11.039025662624207
epoch: 29, step: 31
	action: tensor([[ 0.0073,  0.0156,  0.0054, -0.0047, -0.0038, -0.0080, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[7.5142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30376803249781603, distance: 0.9548467479316686 entropy -11.04053842323407
epoch: 29, step: 32
	action: tensor([[ 0.0073,  0.0158,  0.0038, -0.0046, -0.0074, -0.0264,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[7.5053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997313993096875, distance: 0.9576107659968224 entropy -11.0404885351861
epoch: 29, step: 33
	action: tensor([[ 0.0074,  0.0159,  0.0089, -0.0049, -0.0139, -0.0139, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[7.5131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026271167706036, distance: 0.9556287816416644 entropy -11.033617865253827
epoch: 29, step: 34
	action: tensor([[ 0.0072,  0.0159,  0.0123, -0.0044, -0.0022,  0.0127,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3087895850900446, distance: 0.9513971145349901 entropy -11.045389013883057
epoch: 29, step: 35
	action: tensor([[ 0.0073,  0.0156, -0.0028, -0.0047, -0.0040, -0.0063,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[7.5147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30407779198842044, distance: 0.9546343146038899 entropy -11.040020457366712
epoch: 29, step: 36
	action: tensor([[ 0.0073,  0.0159,  0.0056, -0.0048, -0.0150,  0.0060,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[7.4992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070818994407609, distance: 0.952571637462385 entropy -11.034707619975364
epoch: 29, step: 37
	action: tensor([[ 0.0072,  0.0158,  0.0053, -0.0047, -0.0012,  0.0093,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[7.5106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30773745123387874, distance: 0.9521209292107413 entropy -11.037842252250371
epoch: 29, step: 38
	action: tensor([[ 0.0073,  0.0154,  0.0004, -0.0051, -0.0005, -0.0035, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[7.5315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30442737946101905, distance: 0.954394510416733 entropy -11.027781543551935
epoch: 29, step: 39
	action: tensor([[ 0.0072,  0.0160,  0.0110, -0.0043, -0.0202, -0.0015, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[7.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055444600746785, distance: 0.9536278298661091 entropy -11.04714859741996
epoch: 29, step: 40
	action: tensor([[ 0.0072,  0.0158, -0.0049, -0.0046, -0.0100, -0.0053, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[7.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045224629214184, distance: 0.9543292762259316 entropy -11.042545973202708
epoch: 29, step: 41
	action: tensor([[ 0.0072,  0.0161,  0.0043, -0.0045, -0.0073, -0.0072, -0.0198]],
       dtype=torch.float64)
	q_value: tensor([[7.4880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30423883008803065, distance: 0.9545238558567595 entropy -11.041268261508844
epoch: 29, step: 42
	action: tensor([[ 0.0073,  0.0159,  0.0039, -0.0045, -0.0047, -0.0170, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[7.4983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30190964008951826, distance: 0.9561202440443498 entropy -11.043132958027057
epoch: 29, step: 43
	action: tensor([[ 0.0073,  0.0159,  0.0117, -0.0046, -0.0059,  0.0239,  0.0176]],
       dtype=torch.float64)
	q_value: tensor([[7.4981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31139356302784627, distance: 0.9496033374704768 entropy -11.04097661440885
epoch: 29, step: 44
	action: tensor([[ 0.0072,  0.0155,  0.0093, -0.0048, -0.0093,  0.0028, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[7.5174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059720766906875, distance: 0.9533341826302689 entropy -11.036006900943956
epoch: 29, step: 45
	action: tensor([[ 0.0073,  0.0158,  0.0143, -0.0046,  0.0032,  0.0217, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.5030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3107832881665131, distance: 0.9500240354713307 entropy -11.041089955446477
epoch: 29, step: 46
	action: tensor([[ 0.0072,  0.0157, -0.0002, -0.0045, -0.0214,  0.0169, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[7.5026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093820020958813, distance: 0.950989319284725 entropy -11.043573229809606
epoch: 29, step: 47
	action: tensor([[ 0.0070,  0.0161,  0.0046, -0.0043, -0.0150,  0.0020, -0.0241]],
       dtype=torch.float64)
	q_value: tensor([[7.4852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30624613887272345, distance: 0.9531459346967981 entropy -11.04457972246172
epoch: 29, step: 48
	action: tensor([[ 0.0071,  0.0160, -0.0037, -0.0044, -0.0118,  0.0153,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[7.4885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30928219394916234, distance: 0.9510580353134915 entropy -11.044595762460512
epoch: 29, step: 49
	action: tensor([[ 0.0071,  0.0158,  0.0138, -0.0048, -0.0150, -0.0004,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[7.5014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053982074275554, distance: 0.9537282418033037 entropy -11.03263855734381
epoch: 29, step: 50
	action: tensor([[ 0.0073,  0.0156,  0.0108, -0.0048, -0.0198, -0.0036,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[7.5248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30466649232954646, distance: 0.9542304530500734 entropy -11.03735985435352
epoch: 29, step: 51
	action: tensor([[ 0.0073,  0.0155,  0.0067, -0.0051, -0.0003,  0.0216, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[7.5353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3103848648266434, distance: 0.950298591395345 entropy -11.030095991101918
epoch: 29, step: 52
	action: tensor([[ 0.0072,  0.0158,  0.0089, -0.0045, -0.0067,  0.0152,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[7.4936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3091422037886671, distance: 0.9511544075371391 entropy -11.042635690718429
epoch: 29, step: 53
	action: tensor([[ 0.0073,  0.0154,  0.0001, -0.0051, -0.0168,  0.0219, -0.0317]],
       dtype=torch.float64)
	q_value: tensor([[7.5358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3100995053657982, distance: 0.950495185577193 entropy -11.0287374382508
epoch: 29, step: 54
	action: tensor([[ 0.0069,  0.0161,  0.0086, -0.0042, -0.0023,  0.0267,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[7.4755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3120050666174098, distance: 0.9491816054096696 entropy -11.048109896030484
epoch: 29, step: 55
	action: tensor([[ 0.0072,  0.0156, -0.0081, -0.0047, -0.0039,  0.0171, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[7.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093600174608455, distance: 0.9510044557181714 entropy -11.03855146884321
epoch: 29, step: 56
	action: tensor([[ 0.0071,  0.0161,  0.0083, -0.0045, -0.0129, -0.0049,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[7.4801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30463421088095244, distance: 0.9542526032725642 entropy -11.039219789831696
epoch: 29, step: 57
	action: tensor([[ 0.0074,  0.0156,  0.0106, -0.0050, -0.0055,  0.0277,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[7.5240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31190051626405557, distance: 0.9492537233117028 entropy -11.031412497298675
epoch: 29, step: 58
	action: tensor([[ 0.0072,  0.0154,  0.0077, -0.0051, -0.0150,  0.0217,  0.0411]],
       dtype=torch.float64)
	q_value: tensor([[7.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3100478944497359, distance: 0.9505307378127671 entropy -11.031651136161802
epoch: 29, step: 59
	action: tensor([[ 0.0072,  0.0155,  0.0078, -0.0051, -0.0023,  0.0229, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[7.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31042523615435524, distance: 0.9502707748818989 entropy -11.030031477970613
epoch: 29, step: 60
	action: tensor([[ 0.0071,  0.0159,  0.0053, -0.0043, -0.0130, -0.0050,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[7.4850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30460562682349823, distance: 0.9542722160658313 entropy -11.04689682725916
epoch: 29, step: 61
	action: tensor([[ 0.0073,  0.0157, -0.0062, -0.0048, -0.0092, -0.0067, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.5179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041210305896933, distance: 0.9546046577735824 entropy -11.034258252882612
epoch: 29, step: 62
	action: tensor([[ 0.0072,  0.0161,  0.0161, -0.0046, -0.0095, -0.0119,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.4909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30304495013137, distance: 0.9553424546116511 entropy -11.036309678732632
epoch: 29, step: 63
	action: tensor([[ 0.0074,  0.0156, -0.0023, -0.0049, -0.0014,  0.0143,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[7.5257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3088546756245999, distance: 0.9513523174635119 entropy -11.036899991449129
epoch: 29, step: 64
	action: tensor([[ 7.1928e-03,  1.5806e-02,  1.7817e-03, -4.7522e-03, -1.7073e-02,
          7.4134e-03,  3.4553e-06]], dtype=torch.float64)
	q_value: tensor([[7.4986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072428769159158, distance: 0.9524609811784165 entropy -11.03480239424844
epoch: 29, step: 65
	action: tensor([[ 0.0072,  0.0159,  0.0052, -0.0046,  0.0045, -0.0230, -0.0121]],
       dtype=torch.float64)
	q_value: tensor([[7.4986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003910381475189, distance: 0.9571596347601421 entropy -11.038128076476976
epoch: 29, step: 66
	action: tensor([[ 0.0074,  0.0159,  0.0065, -0.0047,  0.0073,  0.0052, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[7.5039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30707468203642085, distance: 0.9525765984215245 entropy -11.03986484724101
epoch: 29, step: 67
	action: tensor([[ 0.0073,  0.0158,  0.0047, -0.0046, -0.0122, -0.0062,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[7.5010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30426373844026333, distance: 0.9545067696566086 entropy -11.042542616657306
epoch: 29, step: 68
	action: tensor([[ 0.0073,  0.0156,  0.0049, -0.0050, -0.0062,  0.0198,  0.0158]],
       dtype=torch.float64)
	q_value: tensor([[7.5209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3100430840240167, distance: 0.9505340514120021 entropy -11.029678006328353
epoch: 29, step: 69
	action: tensor([[ 0.0072,  0.0156,  0.0210, -0.0048, -0.0021,  0.0158, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[7.5133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3091047934224773, distance: 0.9511801599808305 entropy -11.035369206965276
epoch: 29, step: 70
	action: tensor([[ 0.0072,  0.0156,  0.0067, -0.0046, -0.0123, -0.0117, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[7.5121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302834885353952, distance: 0.955486415603609 entropy -11.04622602065866
epoch: 29, step: 71
	action: tensor([[ 0.0073,  0.0158, -0.0017, -0.0046, -0.0143, -0.0042,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[7.5058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048263892019699, distance: 0.9541207307111061 entropy -11.040950701977982
epoch: 29, step: 72
	action: tensor([[ 0.0073,  0.0157,  0.0009, -0.0050,  0.0001,  0.0131,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[7.5229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30844795394450164, distance: 0.9516321997766437 entropy -11.026575491783657
epoch: 29, step: 73
	action: tensor([[ 0.0073,  0.0156,  0.0097, -0.0050, -0.0037,  0.0068,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[7.5197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30686287672101764, distance: 0.9527221735405382 entropy -11.030065609609702
epoch: 29, step: 74
	action: tensor([[ 0.0073,  0.0157, -0.0014, -0.0047, -0.0079,  0.0046, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[7.5121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30663873909647876, distance: 0.9528762005125666 entropy -11.039727602697074
epoch: 29, step: 75
	action: tensor([[ 0.0072,  0.0159,  0.0138, -0.0046, -0.0092, -0.0308, -0.0759]],
       dtype=torch.float64)
	q_value: tensor([[7.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986376178507084, distance: 0.9583583421531624 entropy -11.037767347868726
epoch: 29, step: 76
	action: tensor([[ 0.0071,  0.0162,  0.0172, -0.0041, -0.0235, -0.0124,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[7.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30314039545676874, distance: 0.9552770371263568 entropy -11.060237930715815
epoch: 29, step: 77
	action: tensor([[ 0.0073,  0.0156,  0.0208, -0.0048, -0.0140, -0.0045,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[7.5293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304459397270319, distance: 0.9543725443615824 entropy -11.03847114678756
epoch: 29, step: 78
	action: tensor([[ 0.0074,  0.0154,  0.0239, -0.0051, -0.0067, -0.0022,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[7.5474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046913628332668, distance: 0.954213387567117 entropy -11.032089782684102
epoch: 29, step: 79
	action: tensor([[ 0.0074,  0.0155,  0.0034, -0.0048, -0.0049,  0.0042,  0.0026]],
       dtype=torch.float64)
	q_value: tensor([[7.5251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064974918593253, distance: 0.9529732525721797 entropy -11.041720655883045
epoch: 29, step: 80
	action: tensor([[ 0.0073,  0.0158, -0.0046, -0.0047, -0.0171, -0.0103,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033655290536458, distance: 0.9551227145576638 entropy -11.037461948290147
epoch: 29, step: 81
	action: tensor([[ 0.0073,  0.0160,  0.0028, -0.0047, -0.0125,  0.0281, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[7.5024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31220136606694193, distance: 0.9490461849944665 entropy -11.034187448898637
epoch: 29, step: 82
	action: tensor([[ 0.0071,  0.0159, -0.0035, -0.0045, -0.0070, -0.0194, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[7.4946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012490903876489, distance: 0.9565724889460232 entropy -11.040051704757195
epoch: 29, step: 83
	action: tensor([[ 0.0073,  0.0160,  0.0173, -0.0048, -0.0086,  0.0015,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[7.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30613437958232115, distance: 0.9532227044487133 entropy -11.034770389640153
epoch: 29, step: 84
	action: tensor([[ 0.0074,  0.0156,  0.0057, -0.0048, -0.0222,  0.0078,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[7.5229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30726799290445306, distance: 0.9524437152311487 entropy -11.038247697466259
epoch: 29, step: 85
	action: tensor([[ 0.0072,  0.0156,  0.0137, -0.0050, -0.0140, -0.0240,  0.0391]],
       dtype=torch.float64)
	q_value: tensor([[7.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998178498914821, distance: 0.9575516539910975 entropy -11.02958631010481
epoch: 29, step: 86
	action: tensor([[ 0.0075,  0.0154,  0.0065, -0.0053, -0.0171, -0.0103,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[7.5455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30300533581435096, distance: 0.9553696046417058 entropy -11.02512099631545
epoch: 29, step: 87
	action: tensor([[ 0.0074,  0.0156,  0.0026, -0.0050,  0.0014,  0.0044, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[7.5289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065108417002771, distance: 0.9529640802170484 entropy -11.030153971481697
epoch: 29, step: 88
	action: tensor([[ 0.0072,  0.0159, -0.0025, -0.0045, -0.0232,  0.0131, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[7.4942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308720585819357, distance: 0.9514445994017191 entropy -11.042839237608806
epoch: 29, step: 89
	action: tensor([[ 0.0071,  0.0160,  0.0129, -0.0045,  0.0002,  0.0198,  0.0438]],
       dtype=torch.float64)
	q_value: tensor([[7.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3103340420878721, distance: 0.9503336079464616 entropy -11.039906096162124
epoch: 29, step: 90
	action: tensor([[ 0.0073,  0.0153,  0.0043, -0.0052, -0.0056, -0.0186,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.5364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30077990182258285, distance: 0.9568935887483707 entropy -11.029195928219938
epoch: 29, step: 91
	action: tensor([[ 0.0074,  0.0158,  0.0067, -0.0048,  0.0045, -0.0096, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[7.5144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30349893781469317, distance: 0.9550312549440781 entropy -11.035478816088512
epoch: 29, step: 92
	action: tensor([[ 0.0074,  0.0157,  0.0074, -0.0048, -0.0021,  0.0010,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[7.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30588891983307875, distance: 0.9533912940934264 entropy -11.039161659755141
epoch: 29, step: 93
	action: tensor([[ 0.0074,  0.0155, -0.0072, -0.0050, -0.0179, -0.0002,  0.0401]],
       dtype=torch.float64)
	q_value: tensor([[7.5220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30540403364479396, distance: 0.9537242419291061 entropy -11.03097765508748
epoch: 29, step: 94
	action: tensor([[ 0.0072,  0.0157,  0.0003, -0.0051, -0.0031, -0.0043, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.5168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30442063409446507, distance: 0.9543991380609301 entropy -11.022527691822303
epoch: 29, step: 95
	action: tensor([[ 0.0072,  0.0160,  0.0070, -0.0045, -0.0136,  0.0095,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[7.4869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308049455165619, distance: 0.9519063437521884 entropy -11.043129894890287
epoch: 29, step: 96
	action: tensor([[ 0.0072,  0.0156,  0.0018, -0.0048, -0.0081,  0.0271,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.5202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3116419118367564, distance: 0.9494320828085263 entropy -11.034208960721836
epoch: 29, step: 97
	action: tensor([[ 0.0071,  0.0158,  0.0102, -0.0046, -0.0065,  0.0073,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[7.4982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071992393459291, distance: 0.9524909790137579 entropy -11.037540329215785
epoch: 29, step: 98
	action: tensor([[ 0.0073,  0.0157,  0.0070, -0.0047, -0.0092, -0.0246,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.5098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29993534445481584, distance: 0.9574713093024564 entropy -11.038653807063493
epoch: 29, step: 99
	action: tensor([[ 0.0074,  0.0158,  0.0134, -0.0049, -0.0064, -0.0090,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[7.5174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036210820866574, distance: 0.9549475101241698 entropy -11.03368725290604
epoch: 29, step: 100
	action: tensor([[ 0.0074,  0.0155,  0.0143, -0.0049, -0.0141,  0.0037,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[7.5287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3063213928518085, distance: 0.9530942377092145 entropy -11.035854084420693
epoch: 29, step: 101
	action: tensor([[ 0.0073,  0.0155,  0.0052, -0.0050, -0.0121,  0.0154,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.5325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30888490430116944, distance: 0.9513315125510354 entropy -11.033882131503736
epoch: 29, step: 102
	action: tensor([[ 0.0072,  0.0157,  0.0173, -0.0048, -0.0010, -0.0034,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[7.5126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30460486511984075, distance: 0.9542727386990781 entropy -11.035811147505589
epoch: 29, step: 103
	action: tensor([[ 0.0074,  0.0153,  0.0192, -0.0052, -0.0002,  0.0016, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[7.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055304367770306, distance: 0.953637458228635 entropy -11.029630780396383
epoch: 29, step: 104
	action: tensor([[ 0.0073,  0.0156,  0.0130, -0.0046,  0.0052, -0.0288,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[7.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988407968629957, distance: 0.9582195177661874 entropy -11.046046838208062
epoch: 29, step: 105
	action: tensor([[ 0.0075,  0.0157,  0.0123, -0.0049, -0.0103,  0.0150, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[7.5218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3091694400127005, distance: 0.9511356583024863 entropy -11.036851341623036
epoch: 29, step: 106
	action: tensor([[ 0.0071,  0.0158,  0.0047, -0.0044, -0.0115,  0.0193,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.4990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31004324550385176, distance: 0.9505339401789038 entropy -11.046508539829517
epoch: 29, step: 107
	action: tensor([[ 0.0072,  0.0157,  0.0091, -0.0046, -0.0157,  0.0093, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[7.5055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30766452933605304, distance: 0.9521710753849872 entropy -11.038526432931437
epoch: 29, step: 108
	action: tensor([[ 0.0072,  0.0158,  0.0042, -0.0046, -0.0125, -0.0092,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[7.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035193163718396, distance: 0.9550172834640457 entropy -11.04179219788003
epoch: 29, step: 109
	action: tensor([[ 0.0073,  0.0158,  0.0084, -0.0047, -0.0170, -0.0224,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[7.5118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30060501935800676, distance: 0.957013245954488 entropy -11.036360334147995
epoch: 29, step: 110
	action: tensor([[ 0.0074,  0.0156,  0.0152, -0.0051, -0.0092, -0.0163,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.5359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30170898306482197, distance: 0.9562576463564711 entropy -11.028144329248054
epoch: 29, step: 111
	action: tensor([[ 0.0074,  0.0157,  0.0049, -0.0048, -0.0105, -0.0079,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[7.5192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30387749146902143, distance: 0.9547716862741941 entropy -11.039011584969723
epoch: 29, step: 112
	action: tensor([[ 0.0074,  0.0156,  0.0128, -0.0050, -0.0046, -0.0023,  0.0700]],
       dtype=torch.float64)
	q_value: tensor([[7.5237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049267708805863, distance: 0.9540518416618184 entropy -11.03131514825352
epoch: 29, step: 113
	action: tensor([[ 0.0074,  0.0152,  0.0090, -0.0055, -0.0222, -0.0059, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[7.5589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30355384460852575, distance: 0.9549936105379508 entropy -11.01812499966655
epoch: 29, step: 114
	action: tensor([[ 0.0072,  0.0160,  0.0172, -0.0045, -0.0135, -0.0064,  0.0191]],
       dtype=torch.float64)
	q_value: tensor([[7.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043121997777898, distance: 0.9544735261110162 entropy -11.043585124011543
epoch: 29, step: 115
	action: tensor([[ 0.0074,  0.0155, -0.0013, -0.0050, -0.0159,  0.0035,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[7.5358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30622032014858414, distance: 0.953163670657903 entropy -11.034847963416354
epoch: 29, step: 116
	action: tensor([[ 0.0072,  0.0158,  0.0157, -0.0048, -0.0100, -0.0099, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[7.5109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032293866596971, distance: 0.9552160392106109 entropy -11.03244106965984
epoch: 29, step: 117
	action: tensor([[ 0.0074,  0.0157,  0.0094, -0.0047, -0.0120,  0.0021, -0.0402]],
       dtype=torch.float64)
	q_value: tensor([[7.5136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061784462152748, distance: 0.9531924349118225 entropy -11.041662850554411
epoch: 29, step: 118
	action: tensor([[ 0.0071,  0.0160,  0.0176, -0.0042, -0.0088, -0.0106, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[7.4807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033595730779025, distance: 0.9551267975278569 entropy -11.050508702206821
epoch: 29, step: 119
	action: tensor([[ 0.0073,  0.0157,  0.0139, -0.0046, -0.0099, -0.0180,  0.0273]],
       dtype=torch.float64)
	q_value: tensor([[7.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30152655481170076, distance: 0.9563825491660244 entropy -11.045768223643345
epoch: 29, step: 120
	action: tensor([[ 0.0075,  0.0155,  0.0107, -0.0051, -0.0052,  0.0012,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[7.5353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30571320479318986, distance: 0.9535119625232351 entropy -11.030143408771442
epoch: 29, step: 121
	action: tensor([[ 0.0074,  0.0155,  0.0149, -0.0049, -0.0043, -0.0020,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[7.5248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30496766608774795, distance: 0.9540237750351834 entropy -11.035011366629726
epoch: 29, step: 122
	action: tensor([[ 0.0074,  0.0155,  0.0025, -0.0050, -0.0119, -0.0017,  0.0388]],
       dtype=torch.float64)
	q_value: tensor([[7.5302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050348141799165, distance: 0.9539776891046592 entropy -11.035781960710676
epoch: 29, step: 123
	action: tensor([[ 0.0073,  0.0156,  0.0099, -0.0051,  0.0014,  0.0033, -0.0162]],
       dtype=torch.float64)
	q_value: tensor([[7.5263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060981947929494, distance: 0.9532475591987497 entropy -11.025206291582043
epoch: 29, step: 124
	action: tensor([[ 0.0073,  0.0158,  0.0253, -0.0045, -0.0182, -0.0048,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[7.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044994941908792, distance: 0.9543450348592779 entropy -11.04423301528528
epoch: 29, step: 125
	action: tensor([[ 0.0074,  0.0154,  0.0153, -0.0050, -0.0023, -0.0121,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[7.5409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30250130865917035, distance: 0.9557149768704695 entropy -11.035560772333495
epoch: 29, step: 126
	action: tensor([[ 0.0075,  0.0155,  0.0079, -0.0050, -0.0159,  0.0087,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[7.5297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074454690723666, distance: 0.9523217005941464 entropy -11.03411516058261
epoch: 29, step: 127
	action: tensor([[ 0.0072,  0.0157,  0.0110, -0.0048, -0.0018, -0.0131, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[7.5150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3024235998755702, distance: 0.9557682138018156 entropy -11.034535678858195
LOSS epoch 29 actor 26.03923481061792 critic 12.217545153450697
epoch: 30, step: 0
	action: tensor([[ 0.0073,  0.0152, -0.0067, -0.0031, -0.0041, -0.0022,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[7.4286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055300084031193, distance: 0.9536377523476048 entropy -11.037534121114035
epoch: 30, step: 1
	action: tensor([[ 0.0072,  0.0152,  0.0047, -0.0034, -0.0030,  0.0164, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[7.4284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309559443399649, distance: 0.9508671420278786 entropy -11.024354352567967
epoch: 30, step: 2
	action: tensor([[ 0.0070,  0.0153,  0.0110, -0.0028, -0.0201,  0.0226, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[7.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31113198173646084, distance: 0.9497836835060706 entropy -11.041357230822522
epoch: 30, step: 3
	action: tensor([[ 0.0070,  0.0153,  0.0003, -0.0028, -0.0053, -0.0049,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[7.4102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047455496696515, distance: 0.9541762049338456 entropy -11.043206378825337
epoch: 30, step: 4
	action: tensor([[ 0.0073,  0.0151, -0.0035, -0.0034,  0.0009,  0.0030,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[7.4366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064027550498787, distance: 0.9530383414228523 entropy -11.025604563460217
epoch: 30, step: 5
	action: tensor([[ 0.0072,  0.0153,  0.0052, -0.0031, -0.0125,  0.0144,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[7.4111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30925072263762343, distance: 0.9510797016890563 entropy -11.033578292626943
epoch: 30, step: 6
	action: tensor([[ 0.0071,  0.0151,  0.0096, -0.0032, -0.0028,  0.0225,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[7.4296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3109515513893746, distance: 0.9499080604369028 entropy -11.033421997668027
epoch: 30, step: 7
	action: tensor([[ 0.0071,  0.0151,  0.0134, -0.0030, -0.0019, -0.0108,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[7.4250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303205842816369, distance: 0.9552321774247986 entropy -11.038638643201164
epoch: 30, step: 8
	action: tensor([[ 0.0074,  0.0149,  0.0102, -0.0034, -0.0131, -0.0115,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[7.4436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030659944541775, distance: 0.9553280313810747 entropy -11.029961349398297
epoch: 30, step: 9
	action: tensor([[ 0.0073,  0.0152,  0.0045, -0.0031, -0.0026,  0.0125,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[7.4307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3088110738767741, distance: 0.9513823256023094 entropy -11.035816101328423
epoch: 30, step: 10
	action: tensor([[ 0.0072,  0.0151,  0.0215, -0.0032, -0.0103,  0.0076,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[7.4277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074245689337983, distance: 0.952336070224589 entropy -11.031593790422708
epoch: 30, step: 11
	action: tensor([[ 0.0073,  0.0149,  0.0154, -0.0033, -0.0098,  0.0030,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[7.4463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062672992537707, distance: 0.9531313985002456 entropy -11.034410753682538
epoch: 30, step: 12
	action: tensor([[ 0.0072,  0.0151,  0.0182, -0.0031, -0.0050,  0.0038,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[7.4324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30664522221929935, distance: 0.9528717456719016 entropy -11.038892373490919
epoch: 30, step: 13
	action: tensor([[ 0.0073,  0.0150,  0.0141, -0.0032, -0.0102, -0.0050, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.4370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045611735416026, distance: 0.9543027166389936 entropy -11.036695509535688
epoch: 30, step: 14
	action: tensor([[ 0.0072,  0.0152,  0.0045, -0.0029, -0.0112,  0.0082,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[7.4214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078024028245937, distance: 0.952076261750642 entropy -11.04268346652982
epoch: 30, step: 15
	action: tensor([[ 0.0071,  0.0152,  0.0003, -0.0031,  0.0023, -0.0130, -0.0202]],
       dtype=torch.float64)
	q_value: tensor([[7.4209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028709081148726, distance: 0.9554617301290963 entropy -11.03431661782993
epoch: 30, step: 16
	action: tensor([[ 0.0072,  0.0154,  0.0024, -0.0029, -0.0048, -0.0091,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[7.4048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30405195288076736, distance: 0.9546520368935212 entropy -11.03995197478206
epoch: 30, step: 17
	action: tensor([[ 0.0073,  0.0151,  0.0038, -0.0033, -0.0056, -0.0013,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[7.4318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055485531202159, distance: 0.9536250195725334 entropy -11.029666534615597
epoch: 30, step: 18
	action: tensor([[ 0.0073,  0.0150,  0.0042, -0.0034, -0.0125, -0.0272,  0.0357]],
       dtype=torch.float64)
	q_value: tensor([[7.4385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995238043393257, distance: 0.957752697569465 entropy -11.02753464756172
epoch: 30, step: 19
	action: tensor([[ 0.0074,  0.0151,  0.0238, -0.0035, -0.0097,  0.0143, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[7.4434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30907577536294495, distance: 0.9512001348716911 entropy -11.02199202102893
epoch: 30, step: 20
	action: tensor([[ 0.0071,  0.0150,  0.0113, -0.0029, -0.0035, -0.0159,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[7.4283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020668971734535, distance: 0.9560125465608448 entropy -11.04435309585118
epoch: 30, step: 21
	action: tensor([[ 0.0074,  0.0150,  0.0136, -0.0034, -0.0081,  0.0100,  0.0372]],
       dtype=torch.float64)
	q_value: tensor([[7.4416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080558505076664, distance: 0.951901944752305 entropy -11.02902181761823
epoch: 30, step: 22
	action: tensor([[ 0.0072,  0.0148, -0.0038, -0.0035, -0.0052, -0.0093, -0.0177]],
       dtype=torch.float64)
	q_value: tensor([[7.4513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033830327257905, distance: 0.9551107152520254 entropy -11.029242709578346
epoch: 30, step: 23
	action: tensor([[ 0.0071,  0.0155,  0.0035, -0.0029, -0.0048, -0.0033,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[7.4034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30536414720332716, distance: 0.9537516248408261 entropy -11.039331352946997
epoch: 30, step: 24
	action: tensor([[ 0.0073,  0.0150,  0.0138, -0.0035, -0.0071,  0.0165, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[7.4439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30945639705521966, distance: 0.9509380965267292 entropy -11.024910873213882
epoch: 30, step: 25
	action: tensor([[ 0.0069,  0.0154,  0.0029, -0.0025, -0.0034,  0.0055, -0.0171]],
       dtype=torch.float64)
	q_value: tensor([[7.3961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30724221222457315, distance: 0.9524614381151952 entropy -11.051465137520378
epoch: 30, step: 26
	action: tensor([[ 0.0071,  0.0153,  0.0035, -0.0028,  0.0067,  0.0081,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[7.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078532250905136, distance: 0.9520413096203993 entropy -11.041205529893245
epoch: 30, step: 27
	action: tensor([[ 0.0072,  0.0150,  0.0066, -0.0032, -0.0023,  0.0020, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[7.4283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30616878999209884, distance: 0.9531990678902924 entropy -11.03330380359921
epoch: 30, step: 28
	action: tensor([[ 0.0072,  0.0152,  0.0050, -0.0030, -0.0143,  0.0057,  0.0387]],
       dtype=torch.float64)
	q_value: tensor([[7.4217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30719939301173804, distance: 0.952490873380715 entropy -11.038275145351237
epoch: 30, step: 29
	action: tensor([[ 0.0072,  0.0150,  0.0087, -0.0035, -0.0138, -0.0096,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[7.4405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30335587608857695, distance: 0.9551293318976364 entropy -11.024917167156124
epoch: 30, step: 30
	action: tensor([[ 0.0073,  0.0152,  0.0140, -0.0031, -0.0146, -0.0082, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.4271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30403220280616206, distance: 0.9546655826713022 entropy -11.03402519338509
epoch: 30, step: 31
	action: tensor([[ 0.0072,  0.0152, -0.0010, -0.0030,  0.0105,  0.0061,  0.0282]],
       dtype=torch.float64)
	q_value: tensor([[7.4294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3073271494611357, distance: 0.952403046911384 entropy -11.039378780477035
epoch: 30, step: 32
	action: tensor([[ 0.0072,  0.0151,  0.0087, -0.0035, -0.0114,  0.0292,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.4313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3123809467140566, distance: 0.9489222813941212 entropy -11.026304804743472
epoch: 30, step: 33
	action: tensor([[ 0.0070,  0.0152,  0.0188, -0.0029, -0.0025,  0.0137,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[7.4207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089152996085094, distance: 0.9513105924960219 entropy -11.03987537135482
epoch: 30, step: 34
	action: tensor([[ 7.2433e-03,  1.4763e-02,  1.0940e-02, -3.4832e-03,  7.7974e-05,
          2.9977e-02,  4.4865e-02]], dtype=torch.float64)
	q_value: tensor([[7.4516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3124325990103217, distance: 0.9488866403403536 entropy -11.030549096958774
epoch: 30, step: 35
	action: tensor([[ 0.0071,  0.0148,  0.0206, -0.0035, -0.0040,  0.0058, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[7.4497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30658636939257333, distance: 0.9529121852906721 entropy -11.02953548833528
epoch: 30, step: 36
	action: tensor([[ 0.0072,  0.0151,  0.0162, -0.0030,  0.0002, -0.0152,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[7.4276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022142458319693, distance: 0.9559116238495599 entropy -11.04447929408677
epoch: 30, step: 37
	action: tensor([[ 0.0074,  0.0149,  0.0082, -0.0035, -0.0124, -0.0221, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.4485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006458211317371, distance: 0.9569853301068515 entropy -11.029847921603192
epoch: 30, step: 38
	action: tensor([[ 0.0073,  0.0153,  0.0081, -0.0031,  0.0056,  0.0223, -0.0276]],
       dtype=torch.float64)
	q_value: tensor([[7.4262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31130099830245417, distance: 0.9496671597161748 entropy -11.0363994324105
epoch: 30, step: 39
	action: tensor([[ 0.0070,  0.0153,  0.0053, -0.0027, -0.0002,  0.0163,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3096998496461346, distance: 0.9507704541427457 entropy -11.046172660828828
epoch: 30, step: 40
	action: tensor([[ 0.0071,  0.0150,  0.0093, -0.0032, -0.0147, -0.0039,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[7.4276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047777673983383, distance: 0.9541540966638313 entropy -11.034512995037534
epoch: 30, step: 41
	action: tensor([[ 0.0073,  0.0150, -0.0037, -0.0035, -0.0069, -0.0238,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[7.4483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002334443116289, distance: 0.957267433525519 entropy -11.026458743848634
epoch: 30, step: 42
	action: tensor([[ 0.0073,  0.0154,  0.0061, -0.0032, -0.0105,  0.0326,  0.0310]],
       dtype=torch.float64)
	q_value: tensor([[7.4222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3135486318050855, distance: 0.9481162295529515 entropy -11.029468893617564
epoch: 30, step: 43
	action: tensor([[ 0.0070,  0.0150,  0.0184, -0.0033, -0.0195,  0.0014, -0.0266]],
       dtype=torch.float64)
	q_value: tensor([[7.4340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057586404637719, distance: 0.9534807620420052 entropy -11.030800542070722
epoch: 30, step: 44
	action: tensor([[ 0.0070,  0.0153,  0.0062, -0.0027, -0.0006, -0.0097,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[7.4127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036913149208015, distance: 0.9548993536108863 entropy -11.04646708361862
epoch: 30, step: 45
	action: tensor([[ 0.0073,  0.0149,  0.0125, -0.0036, -0.0178, -0.0036,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[7.4483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304747310895626, distance: 0.9541749963684205 entropy -11.024505536791604
epoch: 30, step: 46
	action: tensor([[ 0.0072,  0.0149,  0.0030, -0.0036, -0.0140, -0.0282, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.4576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29912681363199367, distance: 0.9580240594421247 entropy -11.023887826662394
epoch: 30, step: 47
	action: tensor([[ 0.0072,  0.0155, -0.0094, -0.0029, -0.0019,  0.0074,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[7.4102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079368602313426, distance: 0.9519837882121014 entropy -11.040348920132406
epoch: 30, step: 48
	action: tensor([[ 0.0071,  0.0153,  0.0003, -0.0032, -0.0200, -0.0023,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[7.4158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30532332876313406, distance: 0.9537796467760628 entropy -11.029235836870004
epoch: 30, step: 49
	action: tensor([[ 0.0071,  0.0154,  0.0020, -0.0030, -0.0067,  0.0240,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[7.4154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3114961669521087, distance: 0.9495325883071001 entropy -11.033135765750414
epoch: 30, step: 50
	action: tensor([[ 0.0070,  0.0152, -0.0026, -0.0031, -0.0003, -0.0189,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[7.4216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014047093058465, distance: 0.9564659638145466 entropy -11.032930255008925
epoch: 30, step: 51
	action: tensor([[ 0.0073,  0.0154,  0.0083, -0.0032, -0.0083, -0.0189,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[7.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30172095873533333, distance: 0.9562494464259844 entropy -11.03134222443896
epoch: 30, step: 52
	action: tensor([[ 0.0073,  0.0151,  0.0023, -0.0033, -0.0081, -0.0149, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[7.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025243019916849, distance: 0.9556992239711217 entropy -11.031163236139507
epoch: 30, step: 53
	action: tensor([[ 7.2358e-03,  1.5340e-02,  4.5088e-03, -2.9920e-03, -1.1157e-02,
          7.7119e-05, -3.0487e-02]], dtype=torch.float64)
	q_value: tensor([[7.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061107579991952, distance: 0.9532389298078141 entropy -11.037094671344184
epoch: 30, step: 54
	action: tensor([[ 0.0070,  0.0154,  0.0012, -0.0027, -0.0142,  0.0265,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[7.4032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31206078305490836, distance: 0.9491431704693981 entropy -11.044842120121931
epoch: 30, step: 55
	action: tensor([[ 0.0070,  0.0152, -0.0008, -0.0030, -0.0098,  0.0067,  0.0223]],
       dtype=torch.float64)
	q_value: tensor([[7.4187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30725930448361594, distance: 0.9524496881079536 entropy -11.03566637007935
epoch: 30, step: 56
	action: tensor([[ 0.0071,  0.0152,  0.0084, -0.0033, -0.0020,  0.0194,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[7.4249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.310264325339638, distance: 0.9503816402538691 entropy -11.028134194778605
epoch: 30, step: 57
	action: tensor([[ 0.0071,  0.0151,  0.0063, -0.0031, -0.0043, -0.0039,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[7.4238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048331448949104, distance: 0.9541160946301008 entropy -11.036389859993077
epoch: 30, step: 58
	action: tensor([[ 0.0073,  0.0151,  0.0010, -0.0032, -0.0014,  0.0108, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[7.4283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3083578483785244, distance: 0.9516941940730438 entropy -11.034465065389991
epoch: 30, step: 59
	action: tensor([[ 0.0071,  0.0153,  0.0054, -0.0029,  0.0048, -0.0090,  0.0495]],
       dtype=torch.float64)
	q_value: tensor([[7.4111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038491610534918, distance: 0.9547911144661251 entropy -11.039026341767027
epoch: 30, step: 60
	action: tensor([[ 0.0074,  0.0148,  0.0111, -0.0037, -0.0043, -0.0058, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[7.4562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041302002460152, distance: 0.95459836829951 entropy -11.020647355247212
epoch: 30, step: 61
	action: tensor([[ 0.0073,  0.0152,  0.0038, -0.0031, -0.0096, -0.0186,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[7.4228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017361730431908, distance: 0.956239028848014 entropy -11.038446928356265
epoch: 30, step: 62
	action: tensor([[ 0.0073,  0.0151,  0.0035, -0.0034, -0.0122,  0.0068,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.4373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3073365191979486, distance: 0.9523966053451667 entropy -11.024711591300019
epoch: 30, step: 63
	action: tensor([[ 0.0071,  0.0152,  0.0071, -0.0031,  0.0027,  0.0081,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[7.4232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30771885737386306, distance: 0.9521337158943711 entropy -11.03502898577899
epoch: 30, step: 64
	action: tensor([[ 0.0072,  0.0150,  0.0147, -0.0033, -0.0123, -0.0057,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[7.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043249972896287, distance: 0.9544647470707472 entropy -11.030891570618797
epoch: 30, step: 65
	action: tensor([[ 0.0073,  0.0151,  0.0126, -0.0032, -0.0057, -0.0181, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.4337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301643325931892, distance: 0.9563026015815125 entropy -11.035542809655336
epoch: 30, step: 66
	action: tensor([[ 0.0073,  0.0152,  0.0015, -0.0031,  0.0028, -0.0165,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[7.4275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30219971757343755, distance: 0.9559215750837569 entropy -11.039579600980685
epoch: 30, step: 67
	action: tensor([[ 7.3769e-03,  1.4947e-02,  2.0798e-02, -3.6904e-03,  7.5708e-05,
         -1.4117e-02,  2.6082e-02]], dtype=torch.float64)
	q_value: tensor([[7.4474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022344570294433, distance: 0.9558977798735367 entropy -11.019462422076373
epoch: 30, step: 68
	action: tensor([[ 0.0074,  0.0148,  0.0060, -0.0035, -0.0072,  0.0401, -0.0049]],
       dtype=torch.float64)
	q_value: tensor([[7.4535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31495777420543036, distance: 0.9471425865574811 entropy -11.031331224723173
epoch: 30, step: 69
	action: tensor([[ 0.0069,  0.0152, -0.0002, -0.0028, -0.0036, -0.0038,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[7.4125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049010665921501, distance: 0.9540694822469946 entropy -11.04176381971977
epoch: 30, step: 70
	action: tensor([[ 0.0072,  0.0153,  0.0101, -0.0031, -0.0103,  0.0158,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[7.4142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30958510890338475, distance: 0.950849468738881 entropy -11.034135251782642
epoch: 30, step: 71
	action: tensor([[ 0.0071,  0.0150,  0.0107, -0.0033, -0.0098, -0.0161,  0.0357]],
       dtype=torch.float64)
	q_value: tensor([[7.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30185434760564356, distance: 0.9561581082088124 entropy -11.03254309828026
epoch: 30, step: 72
	action: tensor([[ 0.0074,  0.0149, -0.0052, -0.0035, -0.0090,  0.0034, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[7.4516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30644004853858353, distance: 0.9530127194912112 entropy -11.025624082225036
epoch: 30, step: 73
	action: tensor([[ 0.0070,  0.0155,  0.0139, -0.0027, -0.0091,  0.0170, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[7.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3100583951307375, distance: 0.9505235045150083 entropy -11.042334591243973
epoch: 30, step: 74
	action: tensor([[ 0.0071,  0.0151, -0.0010, -0.0029, -0.0160,  0.0062, -0.0300]],
       dtype=torch.float64)
	q_value: tensor([[7.4210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072393917245878, distance: 0.9524633770430466 entropy -11.04317248180308
epoch: 30, step: 75
	action: tensor([[ 0.0069,  0.0155,  0.0034, -0.0026, -0.0028,  0.0038, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[7.3916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30696813178229465, distance: 0.9526498338586835 entropy -11.044034276574578
epoch: 30, step: 76
	action: tensor([[ 0.0071,  0.0153,  0.0174, -0.0030, -0.0121,  0.0132,  0.0387]],
       dtype=torch.float64)
	q_value: tensor([[7.4134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089775150199201, distance: 0.9512677703122955 entropy -11.037429024487873
epoch: 30, step: 77
	action: tensor([[ 0.0072,  0.0148,  0.0067, -0.0035, -0.0023,  0.0105,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[7.4566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077722899773817, distance: 0.9520969707326736 entropy -11.029547869447848
epoch: 30, step: 78
	action: tensor([[ 0.0072,  0.0150,  0.0158, -0.0034, -0.0029, -0.0255,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[7.4390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2996564167499045, distance: 0.9576620335992844 entropy -11.029992988218726
epoch: 30, step: 79
	action: tensor([[ 0.0075,  0.0148,  0.0072, -0.0037,  0.0056,  0.0330,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[7.4611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31331800296923284, distance: 0.9482754867045151 entropy -11.022791124723804
epoch: 30, step: 80
	action: tensor([[ 0.0071,  0.0150,  0.0136, -0.0032, -0.0157,  0.0168, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[7.4295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3094055223322022, distance: 0.9509731253221866 entropy -11.033643546347404
epoch: 30, step: 81
	action: tensor([[ 0.0071,  0.0152,  0.0121, -0.0029, -0.0157,  0.0331,  0.0380]],
       dtype=torch.float64)
	q_value: tensor([[7.4187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31347506090802346, distance: 0.948167035695879 entropy -11.041514722224038
epoch: 30, step: 82
	action: tensor([[ 6.9857e-03,  1.4893e-02,  8.2604e-05, -3.4029e-03, -3.7703e-03,
          1.2012e-02,  2.7489e-02]], dtype=torch.float64)
	q_value: tensor([[7.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080688489898976, distance: 0.951893003757115 entropy -11.030850742777005
epoch: 30, step: 83
	action: tensor([[ 0.0071,  0.0151,  0.0102, -0.0033, -0.0171,  0.0047, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.4274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30673673471942775, distance: 0.9528088611602511 entropy -11.028053639718948
epoch: 30, step: 84
	action: tensor([[ 0.0071,  0.0152, -0.0002, -0.0030,  0.0027,  0.0055, -0.0346]],
       dtype=torch.float64)
	q_value: tensor([[7.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071935761616277, distance: 0.9524948719951478 entropy -11.039909840486747
epoch: 30, step: 85
	action: tensor([[ 0.0070,  0.0154,  0.0082, -0.0026,  0.0073,  0.0033, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[7.3962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30682528732617476, distance: 0.9527480066428461 entropy -11.044966183965192
epoch: 30, step: 86
	action: tensor([[ 0.0072,  0.0151, -0.0051, -0.0030, -0.0165,  0.0058,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[7.4197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30720881448404347, distance: 0.9524843968441737 entropy -11.038361673612844
epoch: 30, step: 87
	action: tensor([[ 0.0071,  0.0152,  0.0029, -0.0034, -0.0156,  0.0024,  0.0263]],
       dtype=torch.float64)
	q_value: tensor([[7.4265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062292046096122, distance: 0.9531575675874019 entropy -11.023998751965758
epoch: 30, step: 88
	action: tensor([[ 0.0072,  0.0151,  0.0146, -0.0033,  0.0021, -0.0275,  0.0244]],
       dtype=torch.float64)
	q_value: tensor([[7.4322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992815861077812, distance: 0.9579182742978258 entropy -11.027329006087966
epoch: 30, step: 89
	action: tensor([[ 0.0075,  0.0149,  0.0040, -0.0035, -0.0116, -0.0004, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[7.4532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30567504953385516, distance: 0.953538162788477 entropy -11.029026815960123
epoch: 30, step: 90
	action: tensor([[ 0.0071,  0.0153, -0.0036, -0.0029, -0.0061,  0.0211,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[7.4119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3108443901913728, distance: 0.9499819226802065 entropy -11.040446905619845
epoch: 30, step: 91
	action: tensor([[ 0.0070,  0.0153, -0.0015, -0.0031, -0.0080,  0.0020,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[7.4141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062418198277417, distance: 0.9531489016523638 entropy -11.032362533114817
epoch: 30, step: 92
	action: tensor([[ 0.0072,  0.0151,  0.0149, -0.0033, -0.0021,  0.0213,  0.0387]],
       dtype=torch.float64)
	q_value: tensor([[7.4314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31067938219247926, distance: 0.9500956453516917 entropy -11.02699169822086
epoch: 30, step: 93
	action: tensor([[ 0.0072,  0.0148, -0.0021, -0.0035,  0.0007, -0.0166,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[7.4532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301543486758197, distance: 0.9563709570887573 entropy -11.030111434591003
epoch: 30, step: 94
	action: tensor([[ 0.0074,  0.0150, -0.0005, -0.0036, -0.0085,  0.0119,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[7.4442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3083796082171816, distance: 0.9516792232713452 entropy -11.020154065913415
epoch: 30, step: 95
	action: tensor([[ 0.0071,  0.0153,  0.0111, -0.0030, -0.0020,  0.0132,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[7.4142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089233339681019, distance: 0.9513050626430282 entropy -11.03396280605557
epoch: 30, step: 96
	action: tensor([[ 0.0072,  0.0149,  0.0109, -0.0033, -0.0079,  0.0036,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.4407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30639741704653434, distance: 0.9530420087615729 entropy -11.032767456797915
epoch: 30, step: 97
	action: tensor([[ 0.0072,  0.0151,  0.0189, -0.0031, -0.0113,  0.0168, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[7.4279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30970663058035997, distance: 0.9507657843424114 entropy -11.035771197460965
epoch: 30, step: 98
	action: tensor([[ 0.0071,  0.0151,  0.0066, -0.0029, -0.0058,  0.0042,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[7.4272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066859192049701, distance: 0.9528437804958618 entropy -11.043526821980285
epoch: 30, step: 99
	action: tensor([[ 0.0072,  0.0149,  0.0057, -0.0036,  0.0076,  0.0237,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[7.4520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31097728497883315, distance: 0.9498903223719422 entropy -11.022975978873015
epoch: 30, step: 100
	action: tensor([[ 0.0071,  0.0150,  0.0194, -0.0032, -0.0012,  0.0132,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[7.4263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3086994715763459, distance: 0.9514591296182099 entropy -11.033746421741181
epoch: 30, step: 101
	action: tensor([[ 0.0072,  0.0149,  0.0061, -0.0032, -0.0149, -0.0175,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[7.4389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016865975938884, distance: 0.9562729738529399 entropy -11.036565508418132
epoch: 30, step: 102
	action: tensor([[ 0.0073,  0.0152,  0.0079, -0.0033,  0.0039,  0.0027,  0.0427]],
       dtype=torch.float64)
	q_value: tensor([[7.4330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064880526309055, distance: 0.9529797379860714 entropy -11.028429368283904
epoch: 30, step: 103
	action: tensor([[ 0.0073,  0.0148,  0.0134, -0.0036, -0.0166, -0.0040,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[7.4527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045020496736218, distance: 0.9543432815790955 entropy -11.024606571945275
epoch: 30, step: 104
	action: tensor([[ 0.0073,  0.0150,  0.0122, -0.0034, -0.0153,  0.0026,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[7.4472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30622654451417564, distance: 0.9531593949111249 entropy -11.030311187160402
epoch: 30, step: 105
	action: tensor([[ 0.0072,  0.0150, -0.0007, -0.0032,  0.0005, -0.0013,  0.0134]],
       dtype=torch.float64)
	q_value: tensor([[7.4386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30540999737101715, distance: 0.9537201476334612 entropy -11.033893372330136
epoch: 30, step: 106
	action: tensor([[ 0.0072,  0.0152,  0.0163, -0.0033, -0.0074,  0.0079,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[7.4260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3076423015311279, distance: 0.952186360246115 entropy -11.030386681722957
epoch: 30, step: 107
	action: tensor([[ 0.0072,  0.0149,  0.0103, -0.0033,  0.0011,  0.0101,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.4419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079963789483282, distance: 0.9519428511438655 entropy -11.035360015527443
epoch: 30, step: 108
	action: tensor([[ 0.0072,  0.0150,  0.0143, -0.0032,  0.0001,  0.0013, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[7.4272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060532393325571, distance: 0.9532784374790104 entropy -11.035929720649579
epoch: 30, step: 109
	action: tensor([[ 0.0072,  0.0150,  0.0124, -0.0031, -0.0106, -0.0244, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[7.4298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002283790562541, distance: 0.9572708981060988 entropy -11.03988667791224
epoch: 30, step: 110
	action: tensor([[ 0.0073,  0.0153,  0.0043, -0.0030, -0.0105,  0.0067,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[7.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3076355980608666, distance: 0.9521909698124658 entropy -11.039959414146484
epoch: 30, step: 111
	action: tensor([[ 7.1763e-03,  1.5032e-02,  3.2834e-05, -3.3723e-03, -3.0402e-03,
         -1.6471e-03,  3.7850e-02]], dtype=torch.float64)
	q_value: tensor([[7.4383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052412663444892, distance: 0.953835980288145 entropy -11.02840725081664
epoch: 30, step: 112
	action: tensor([[ 0.0072,  0.0150, -0.0028, -0.0035, -0.0083,  0.0022,  0.0178]],
       dtype=torch.float64)
	q_value: tensor([[7.4408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061221368313898, distance: 0.9532311138700279 entropy -11.023598968242197
epoch: 30, step: 113
	action: tensor([[ 0.0072,  0.0152,  0.0094, -0.0032,  0.0040, -0.0128,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[7.4242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30281523556536916, distance: 0.9554998808311229 entropy -11.029368725600827
epoch: 30, step: 114
	action: tensor([[ 0.0074,  0.0149,  0.0020, -0.0035, -0.0041,  0.0103,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[7.4485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30803763546745444, distance: 0.9519144738112661 entropy -11.027484940998816
epoch: 30, step: 115
	action: tensor([[ 7.1232e-03,  1.5236e-02, -3.9001e-03, -3.0363e-03,  3.5676e-05,
         -2.9884e-02,  1.3432e-02]], dtype=torch.float64)
	q_value: tensor([[7.4157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991041212612344, distance: 0.9580395684254969 entropy -11.03528044871301
epoch: 30, step: 116
	action: tensor([[ 0.0073,  0.0154,  0.0255, -0.0033, -0.0046, -0.0074,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[7.4236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30425117948629454, distance: 0.9545153846686095 entropy -11.026760580462836
epoch: 30, step: 117
	action: tensor([[ 0.0074,  0.0148,  0.0009, -0.0034,  0.0039, -0.0046, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[7.4548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045861420329047, distance: 0.9542855852163233 entropy -11.034196416386818
epoch: 30, step: 118
	action: tensor([[ 0.0072,  0.0153,  0.0142, -0.0029, -0.0029, -0.0105, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.4110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035810453095953, distance: 0.9549749610349241 entropy -11.03947604571543
epoch: 30, step: 119
	action: tensor([[ 0.0072,  0.0151,  0.0022, -0.0030, -0.0076, -0.0155,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[7.4257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30243143978859754, distance: 0.9557628429487287 entropy -11.041580589068039
epoch: 30, step: 120
	action: tensor([[ 0.0073,  0.0153,  0.0049, -0.0032, -0.0123,  0.0086,  0.0694]],
       dtype=torch.float64)
	q_value: tensor([[7.4219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30793096471026116, distance: 0.9519878430650601 entropy -11.031764182771237
epoch: 30, step: 121
	action: tensor([[ 0.0071,  0.0147,  0.0063, -0.0039, -0.0160,  0.0021,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[7.4641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055909017565661, distance: 0.9535959424246292 entropy -11.016033491195705
epoch: 30, step: 122
	action: tensor([[ 0.0072,  0.0150,  0.0058, -0.0036, -0.0030,  0.0099, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[7.4483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30779756085727616, distance: 0.9520795916625777 entropy -11.024147897447623
epoch: 30, step: 123
	action: tensor([[ 0.0071,  0.0152,  0.0080, -0.0029, -0.0068,  0.0290, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[7.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31264299987926514, distance: 0.9487414459571921 entropy -11.040414688758888
epoch: 30, step: 124
	action: tensor([[ 0.0070,  0.0152,  0.0097, -0.0029, -0.0031,  0.0031,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[7.4187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.306472101239039, distance: 0.9529906976140121 entropy -11.04028853150048
epoch: 30, step: 125
	action: tensor([[ 0.0072,  0.0150, -0.0064, -0.0032, -0.0045, -0.0152,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[7.4341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023348363793915, distance: 0.9558290204959333 entropy -11.034598811140016
epoch: 30, step: 126
	action: tensor([[ 0.0073,  0.0153,  0.0034, -0.0032, -0.0228,  0.0243,  0.0470]],
       dtype=torch.float64)
	q_value: tensor([[7.4200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31147322888667606, distance: 0.9495484054007677 entropy -11.028671317655471
epoch: 30, step: 127
	action: tensor([[ 0.0070,  0.0150,  0.0059, -0.0035, -0.0145, -0.0102,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[7.4407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030246259432122, distance: 0.9553563840739994 entropy -11.025131822560203
LOSS epoch 30 actor 25.438906040351956 critic 12.368088726050429
epoch: 31, step: 0
	action: tensor([[ 0.0069,  0.0152,  0.0125, -0.0032, -0.0142, -0.0268,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[7.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29944656384215396, distance: 0.9578055011163638 entropy -11.022476691757417
epoch: 31, step: 1
	action: tensor([[ 0.0071,  0.0151,  0.0044, -0.0033, -0.0115,  0.0103, -0.0150]],
       dtype=torch.float64)
	q_value: tensor([[7.1380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30798393601225926, distance: 0.9519514095606367 entropy -11.022024516965804
epoch: 31, step: 2
	action: tensor([[ 0.0067,  0.0154, -0.0005, -0.0026, -0.0023,  0.0049, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[7.1073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30691336059318697, distance: 0.9526874776783393 entropy -11.035775168779233
epoch: 31, step: 3
	action: tensor([[ 0.0067,  0.0155,  0.0018, -0.0027, -0.0199, -0.0077, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[7.1043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041058952160345, distance: 0.9546150390469541 entropy -11.034541182577184
epoch: 31, step: 4
	action: tensor([[ 0.0067,  0.0155,  0.0099, -0.0026, -0.0065,  0.0028,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.1037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30650374753903686, distance: 0.9529689544558675 entropy -11.035095481028977
epoch: 31, step: 5
	action: tensor([[ 0.0069,  0.0151,  0.0075, -0.0030, -0.0165,  0.0093, -0.0378]],
       dtype=torch.float64)
	q_value: tensor([[7.1295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077101589421132, distance: 0.952139697600082 entropy -11.030498750871159
epoch: 31, step: 6
	action: tensor([[ 0.0065,  0.0155,  0.0148, -0.0024, -0.0030, -0.0162,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[7.0982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30201240633667137, distance: 0.956049865973452 entropy -11.042293533308387
epoch: 31, step: 7
	action: tensor([[ 0.0070,  0.0151,  0.0047, -0.0030, -0.0077, -0.0043, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[7.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047029521329212, distance: 0.9542054351914575 entropy -11.031262562531095
epoch: 31, step: 8
	action: tensor([[ 0.0068,  0.0154, -0.0006, -0.0028,  0.0004, -0.0121, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[7.1128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30304986139925494, distance: 0.9553390885760774 entropy -11.033067922199184
epoch: 31, step: 9
	action: tensor([[ 0.0069,  0.0154,  0.0103, -0.0029, -0.0073,  0.0149, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[7.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30927131149181275, distance: 0.9510655273801865 entropy -11.03075930679722
epoch: 31, step: 10
	action: tensor([[ 0.0067,  0.0152, -0.0018, -0.0027,  0.0007,  0.0126, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[7.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3085564024693698, distance: 0.9515575798292069 entropy -11.036676059038369
epoch: 31, step: 11
	action: tensor([[ 0.0067,  0.0154,  0.0190, -0.0027, -0.0042,  0.0061, -0.0209]],
       dtype=torch.float64)
	q_value: tensor([[7.1105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30712496606336437, distance: 0.9525420346271891 entropy -11.032680747779873
epoch: 31, step: 12
	action: tensor([[ 0.0068,  0.0152, -0.0015, -0.0027, -0.0134,  0.0230,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[7.1203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3108721597830346, distance: 0.9499627826808644 entropy -11.039595791807756
epoch: 31, step: 13
	action: tensor([[ 0.0067,  0.0154,  0.0016, -0.0028, -0.0146, -0.0215,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[7.1101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30081078576406706, distance: 0.9568724559376761 entropy -11.030272589410393
epoch: 31, step: 14
	action: tensor([[ 0.0069,  0.0154,  0.0045, -0.0031, -0.0033, -0.0009,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[7.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30557750728551425, distance: 0.9536051393459286 entropy -11.02408883967854
epoch: 31, step: 15
	action: tensor([[ 0.0069,  0.0151,  0.0139, -0.0033,  0.0050, -0.0006,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[7.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052620393613741, distance: 0.9538217205184765 entropy -11.020223101354233
epoch: 31, step: 16
	action: tensor([[ 0.0070,  0.0150, -0.0006, -0.0032, -0.0113,  0.0209,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31023568844295923, distance: 0.9504013693322322 entropy -11.02895538821949
epoch: 31, step: 17
	action: tensor([[ 0.0067,  0.0154,  0.0011, -0.0028, -0.0048,  0.0119,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[7.1137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30836750894177367, distance: 0.9516875476200778 entropy -11.031292768459002
epoch: 31, step: 18
	action: tensor([[ 0.0068,  0.0153,  0.0130, -0.0029, -0.0195, -0.0030,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[7.1169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048966803660076, distance: 0.9540724924356901 entropy -11.028696092081878
epoch: 31, step: 19
	action: tensor([[ 0.0069,  0.0152,  0.0044, -0.0031,  0.0053, -0.0019,  0.0002]],
       dtype=torch.float64)
	q_value: tensor([[7.1334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051023351209623, distance: 0.9539313450307004 entropy -11.026277563869261
epoch: 31, step: 20
	action: tensor([[ 0.0069,  0.0153,  0.0081, -0.0030, -0.0105, -0.0014,  0.0176]],
       dtype=torch.float64)
	q_value: tensor([[7.1193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053791456869347, distance: 0.9537413281470469 entropy -11.030345037047848
epoch: 31, step: 21
	action: tensor([[ 0.0069,  0.0152,  0.0044, -0.0031, -0.0035, -0.0053, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[7.1312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043314502308352, distance: 0.9544603203491062 entropy -11.027101735006573
epoch: 31, step: 22
	action: tensor([[ 0.0069,  0.0153,  0.0113, -0.0029,  0.0014,  0.0032, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[7.1162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064876275856766, distance: 0.9529800300210034 entropy -11.031175813824904
epoch: 31, step: 23
	action: tensor([[ 0.0069,  0.0152,  0.0090, -0.0029, -0.0106, -0.0105,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[7.1235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30323811241286036, distance: 0.9552100580416929 entropy -11.03307099646895
epoch: 31, step: 24
	action: tensor([[ 0.0069,  0.0153,  0.0123, -0.0029, -0.0112,  0.0088,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[7.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077428436963614, distance: 0.9521172208732056 entropy -11.031487236693247
epoch: 31, step: 25
	action: tensor([[ 0.0068,  0.0152,  0.0148, -0.0029, -0.0072, -0.0052, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[7.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043617591134956, distance: 0.954439528162252 entropy -11.032796991096074
epoch: 31, step: 26
	action: tensor([[ 0.0068,  0.0152,  0.0133, -0.0028, -0.0085,  0.0041, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[7.1217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30663715555372895, distance: 0.9528772886317207 entropy -11.037528329509119
epoch: 31, step: 27
	action: tensor([[ 0.0068,  0.0152,  0.0098, -0.0028, -0.0040, -0.0171,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[7.1235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016787620843938, distance: 0.9562783388258314 entropy -11.035858529697128
epoch: 31, step: 28
	action: tensor([[ 0.0070,  0.0152,  0.0138, -0.0031, -0.0106, -0.0091,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[7.1285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035678219828889, distance: 0.9549840273346552 entropy -11.02725151603148
epoch: 31, step: 29
	action: tensor([[ 0.0070,  0.0150,  0.0040, -0.0033, -0.0060, -0.0080, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[7.1426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30361994198958875, distance: 0.9549482918339425 entropy -11.024419394242056
epoch: 31, step: 30
	action: tensor([[ 0.0069,  0.0153, -0.0100, -0.0029,  0.0006, -0.0125,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[7.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30296741780171377, distance: 0.9553955913715386 entropy -11.031170043511327
epoch: 31, step: 31
	action: tensor([[ 0.0069,  0.0154,  0.0252, -0.0032, -0.0142,  0.0140,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[7.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30892603558250886, distance: 0.9513032031807566 entropy -11.019074394327578
epoch: 31, step: 32
	action: tensor([[ 0.0069,  0.0149,  0.0138, -0.0032, -0.0087,  0.0056,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[7.1509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064972562972216, distance: 0.9529734144204517 entropy -11.029731728894363
epoch: 31, step: 33
	action: tensor([[ 0.0069,  0.0151,  0.0064, -0.0031,  0.0023, -0.0034, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[7.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30470390124777036, distance: 0.9542047839224355 entropy -11.029686273115185
epoch: 31, step: 34
	action: tensor([[ 0.0068,  0.0153, -0.0026, -0.0027, -0.0177,  0.0284,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[7.1112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3121952476688471, distance: 0.9490504061641303 entropy -11.037889884942105
epoch: 31, step: 35
	action: tensor([[ 0.0066,  0.0153,  0.0094, -0.0029, -0.0121, -0.0057, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[7.1168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041736705948823, distance: 0.9545685513910895 entropy -11.028336542535547
epoch: 31, step: 36
	action: tensor([[ 0.0068,  0.0153,  0.0098, -0.0028, -0.0123, -0.0032,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[7.1215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30498151929800565, distance: 0.954014267306436 entropy -11.033632354511933
epoch: 31, step: 37
	action: tensor([[ 0.0069,  0.0152,  0.0013, -0.0031, -0.0049,  0.0299, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[7.1292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3124960026531505, distance: 0.9488428888066291 entropy -11.026851428669927
epoch: 31, step: 38
	action: tensor([[ 0.0067,  0.0153,  0.0173, -0.0027, -0.0015, -0.0028, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[7.1135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049359862121953, distance: 0.9540455171963766 entropy -11.033846181948809
epoch: 31, step: 39
	action: tensor([[ 0.0069,  0.0151,  0.0010, -0.0029,  0.0051, -0.0028, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[7.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049882837242368, distance: 0.9540096247141769 entropy -11.034647332985282
epoch: 31, step: 40
	action: tensor([[ 0.0068,  0.0154,  0.0096, -0.0027, -0.0033,  0.0092,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.1112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079179983935707, distance: 0.9519967610468024 entropy -11.034516926915314
epoch: 31, step: 41
	action: tensor([[ 0.0068,  0.0152,  0.0146, -0.0029, -0.0080, -0.0045, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[7.1253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30453499061991385, distance: 0.954320680978321 entropy -11.03292727611959
epoch: 31, step: 42
	action: tensor([[ 0.0069,  0.0152,  0.0104, -0.0028, -0.0129, -0.0057, -0.0348]],
       dtype=torch.float64)
	q_value: tensor([[7.1206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30441910204396705, distance: 0.9544001891177479 entropy -11.035862503308826
epoch: 31, step: 43
	action: tensor([[ 6.6831e-03,  1.5452e-02,  1.6571e-02, -2.5000e-03, -1.2583e-02,
          2.6143e-02, -6.3551e-05]], dtype=torch.float64)
	q_value: tensor([[7.1041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3118909104289719, distance: 0.9492603490563163 entropy -11.041443409287576
epoch: 31, step: 44
	action: tensor([[ 0.0067,  0.0151,  0.0064, -0.0028, -0.0131,  0.0036,  0.0504]],
       dtype=torch.float64)
	q_value: tensor([[7.1284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062509378344468, distance: 0.9531426380529711 entropy -11.03582102670335
epoch: 31, step: 45
	action: tensor([[ 0.0069,  0.0150,  0.0160, -0.0035,  0.0042,  0.0060,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[7.1420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065877474925328, distance: 0.9529112383746913 entropy -11.016121887264875
epoch: 31, step: 46
	action: tensor([[ 0.0070,  0.0150,  0.0113, -0.0031, -0.0046,  0.0079,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[7.1362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072809043620828, distance: 0.9524348391478655 entropy -11.029736739445914
epoch: 31, step: 47
	action: tensor([[ 0.0068,  0.0152,  0.0139, -0.0029, -0.0085,  0.0021,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[7.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060859564736663, distance: 0.9532559653568397 entropy -11.032942231334301
epoch: 31, step: 48
	action: tensor([[ 0.0069,  0.0151,  0.0062, -0.0030, -0.0076,  0.0007, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[7.1320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057127293456319, distance: 0.9535122890056482 entropy -11.031706001067997
epoch: 31, step: 49
	action: tensor([[ 0.0068,  0.0154,  0.0025, -0.0027, -0.0027,  0.0019,  0.0409]],
       dtype=torch.float64)
	q_value: tensor([[7.1133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30618578299968724, distance: 0.9531873951535098 entropy -11.03586082623617
epoch: 31, step: 50
	action: tensor([[ 0.0069,  0.0150,  0.0144, -0.0034,  0.0002, -0.0051,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[7.1400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041076366491786, distance: 0.9546138446128459 entropy -11.018084974120011
epoch: 31, step: 51
	action: tensor([[ 0.0070,  0.0150,  0.0042, -0.0032, -0.0053,  0.0010,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[7.1394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30570814332860696, distance: 0.9535154381461802 entropy -11.028402763456763
epoch: 31, step: 52
	action: tensor([[ 0.0069,  0.0151,  0.0012, -0.0032,  0.0022, -0.0162,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[7.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30177409946139155, distance: 0.9562130592843757 entropy -11.02455781824999
epoch: 31, step: 53
	action: tensor([[ 0.0070,  0.0152,  0.0218, -0.0033, -0.0008, -0.0036,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.1302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30472413787266417, distance: 0.9541908977343606 entropy -11.022072994669722
epoch: 31, step: 54
	action: tensor([[ 0.0070,  0.0150,  0.0113, -0.0030,  0.0155,  0.0149, -0.0025]],
       dtype=torch.float64)
	q_value: tensor([[7.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30911803783558267, distance: 0.9511710429083483 entropy -11.034371599713674
epoch: 31, step: 55
	action: tensor([[ 0.0069,  0.0151,  0.0069, -0.0029, -0.0117,  0.0256,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[7.1265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3114019945817982, distance: 0.9495975238040251 entropy -11.034891054426677
epoch: 31, step: 56
	action: tensor([[ 6.7095e-03,  1.4917e-02, -5.1410e-05, -3.4770e-03, -1.2481e-02,
          1.5362e-02,  5.6745e-03]], dtype=torch.float64)
	q_value: tensor([[7.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30851643507328974, distance: 0.9515850807918731 entropy -11.020463287454353
epoch: 31, step: 57
	action: tensor([[ 0.0067,  0.0154,  0.0068, -0.0028, -0.0147, -0.0101,  0.0382]],
       dtype=torch.float64)
	q_value: tensor([[7.1156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30334031431169706, distance: 0.9551399997738439 entropy -11.030304221534942
epoch: 31, step: 58
	action: tensor([[ 0.0069,  0.0151,  0.0060, -0.0034, -0.0114, -0.0088,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[7.1413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30335209581926026, distance: 0.9551319233507791 entropy -11.018824986508582
epoch: 31, step: 59
	action: tensor([[ 0.0070,  0.0151,  0.0060, -0.0034, -0.0097,  0.0027,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[7.1391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059942275716009, distance: 0.9533189690057591 entropy -11.017346559429104
epoch: 31, step: 60
	action: tensor([[ 0.0069,  0.0151,  0.0014, -0.0032, -0.0090,  0.0050,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[7.1319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.306639124048657, distance: 0.9528759359954668 entropy -11.025226864529028
epoch: 31, step: 61
	action: tensor([[ 0.0068,  0.0153, -0.0013, -0.0030, -0.0159,  0.0301, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[7.1191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3124493457105183, distance: 0.9488750845155538 entropy -11.02666016729047
epoch: 31, step: 62
	action: tensor([[ 0.0066,  0.0154, -0.0002, -0.0027, -0.0179, -0.0049,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[7.1106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30451553115104213, distance: 0.9543340320768183 entropy -11.03307038305282
epoch: 31, step: 63
	action: tensor([[ 0.0068,  0.0153,  0.0052, -0.0031, -0.0242, -0.0160, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[7.1219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020613325145184, distance: 0.9560163577235006 entropy -11.022051055777343
epoch: 31, step: 64
	action: tensor([[ 0.0068,  0.0156,  0.0093, -0.0026, -0.0095,  0.0213,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[7.1047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3108391741055624, distance: 0.9499855177883215 entropy -11.036121785452888
epoch: 31, step: 65
	action: tensor([[ 0.0068,  0.0150,  0.0030, -0.0033, -0.0034,  0.0126, -0.0317]],
       dtype=torch.float64)
	q_value: tensor([[7.1380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3081311351374648, distance: 0.9518501591168951 entropy -11.023635962178446
epoch: 31, step: 66
	action: tensor([[ 0.0066,  0.0155,  0.0089, -0.0024, -0.0098,  0.0117,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[7.0981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30849489024575416, distance: 0.9515999051331866 entropy -11.03969084068818
epoch: 31, step: 67
	action: tensor([[ 0.0068,  0.0152,  0.0221, -0.0029, -0.0190,  0.0269, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[7.1234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.311780836360141, distance: 0.9493362707177564 entropy -11.03058608662216
epoch: 31, step: 68
	action: tensor([[ 0.0066,  0.0151,  0.0123, -0.0027, -0.0111,  0.0006,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[7.1244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30559448689264546, distance: 0.9535934807808808 entropy -11.039109603564146
epoch: 31, step: 69
	action: tensor([[ 0.0069,  0.0152,  0.0076, -0.0030, -0.0084,  0.0297,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.1271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31247127289882004, distance: 0.9488599537550694 entropy -11.030316103653165
epoch: 31, step: 70
	action: tensor([[ 0.0067,  0.0153,  0.0052, -0.0027, -0.0070,  0.0075,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[7.1173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072771021223998, distance: 0.9524374530360884 entropy -11.033894111017947
epoch: 31, step: 71
	action: tensor([[ 0.0069,  0.0150,  0.0060, -0.0034, -0.0072, -0.0051,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[7.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30405785038413224, distance: 0.9546479919971022 entropy -11.018885073011072
epoch: 31, step: 72
	action: tensor([[ 0.0069,  0.0151,  0.0097, -0.0032,  0.0129, -0.0138,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[7.1315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30224668285465206, distance: 0.9558894055058971 entropy -11.022357706733771
epoch: 31, step: 73
	action: tensor([[ 0.0071,  0.0151,  0.0201, -0.0033, -0.0143, -0.0082,  0.0443]],
       dtype=torch.float64)
	q_value: tensor([[7.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036061665688401, distance: 0.9549577369273992 entropy -11.02529664615956
epoch: 31, step: 74
	action: tensor([[ 0.0070,  0.0149,  0.0088, -0.0035, -0.0096,  0.0084,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.1516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30720438307256503, distance: 0.9524874431037306 entropy -11.02099248070744
epoch: 31, step: 75
	action: tensor([[ 0.0068,  0.0152,  0.0103, -0.0029, -0.0165, -0.0210, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[7.1235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30086304271276243, distance: 0.9568366972559365 entropy -11.032622912058299
epoch: 31, step: 76
	action: tensor([[ 0.0069,  0.0154,  0.0156, -0.0028, -0.0103, -0.0072, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.1158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042011225842268, distance: 0.9545497212153262 entropy -11.033864072636666
epoch: 31, step: 77
	action: tensor([[ 0.0069,  0.0152,  0.0132, -0.0029, -0.0143, -0.0225, -0.0188]],
       dtype=torch.float64)
	q_value: tensor([[7.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005381406150073, distance: 0.9570590014393691 entropy -11.0339671500631
epoch: 31, step: 78
	action: tensor([[ 0.0069,  0.0154,  0.0014, -0.0028, -0.0143, -0.0227,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[7.1158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30076617398301786, distance: 0.9569029820833354 entropy -11.036312554104422
epoch: 31, step: 79
	action: tensor([[ 0.0070,  0.0153, -0.0002, -0.0033, -0.0125,  0.0058,  0.0331]],
       dtype=torch.float64)
	q_value: tensor([[7.1292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3069264823402671, distance: 0.9526784593373512 entropy -11.0182427480801
epoch: 31, step: 80
	action: tensor([[ 0.0068,  0.0152,  0.0023, -0.0033, -0.0061,  0.0250,  0.0385]],
       dtype=torch.float64)
	q_value: tensor([[7.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31115775998684214, distance: 0.9497659123292925 entropy -11.020994995981491
epoch: 31, step: 81
	action: tensor([[ 0.0067,  0.0151,  0.0030, -0.0033, -0.0090, -0.0116, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[7.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026158777158596, distance: 0.9556364821998428 entropy -11.021841245830654
epoch: 31, step: 82
	action: tensor([[ 0.0068,  0.0155, -0.0079, -0.0028, -0.0058,  0.0013,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.1092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30620315027750056, distance: 0.9531754651775232 entropy -11.033139397431357
epoch: 31, step: 83
	action: tensor([[ 0.0068,  0.0154,  0.0060, -0.0030, -0.0063,  0.0179, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[7.1135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098400832947237, distance: 0.9506738752981427 entropy -11.024858354654855
epoch: 31, step: 84
	action: tensor([[ 0.0066,  0.0154,  0.0020, -0.0025, -0.0121,  0.0086,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[7.1092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30765762599596, distance: 0.9521758224654513 entropy -11.038647914061169
epoch: 31, step: 85
	action: tensor([[ 0.0068,  0.0153,  0.0002, -0.0028, -0.0098,  0.0226,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[7.1179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31082745520649113, distance: 0.9499935948123319 entropy -11.03043039552423
epoch: 31, step: 86
	action: tensor([[ 0.0067,  0.0151,  0.0093, -0.0033, -0.0016, -0.0006, -0.0295]],
       dtype=torch.float64)
	q_value: tensor([[7.1330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30508287874115614, distance: 0.9539446994579653 entropy -11.021671885151362
epoch: 31, step: 87
	action: tensor([[ 0.0067,  0.0153,  0.0064, -0.0026, -0.0080, -0.0030, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[7.1100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050875871788281, distance: 0.9539414677079978 entropy -11.04032338812807
epoch: 31, step: 88
	action: tensor([[ 0.0068,  0.0153,  0.0178, -0.0029, -0.0076, -0.0112,  0.0193]],
       dtype=torch.float64)
	q_value: tensor([[7.1208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030635213448467, distance: 0.9553297263970687 entropy -11.032159956931789
epoch: 31, step: 89
	action: tensor([[ 0.0070,  0.0150,  0.0035, -0.0032, -0.0094, -0.0033, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[7.1403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30477262797032556, distance: 0.9541576234477861 entropy -11.027215282177574
epoch: 31, step: 90
	action: tensor([[ 0.0068,  0.0154,  0.0112, -0.0027, -0.0124, -0.0093, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[7.1111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30366074333748294, distance: 0.9549203159120563 entropy -11.035104371071004
epoch: 31, step: 91
	action: tensor([[ 0.0068,  0.0153,  0.0011, -0.0028, -0.0088, -0.0051, -0.0401]],
       dtype=torch.float64)
	q_value: tensor([[7.1202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30458975931188137, distance: 0.9542831032973663 entropy -11.035406095561086
epoch: 31, step: 92
	action: tensor([[ 0.0066,  0.0156, -0.0004, -0.0024, -0.0090,  0.0214, -0.0226]],
       dtype=torch.float64)
	q_value: tensor([[7.0935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31083860572313915, distance: 0.9499859095364731 entropy -11.0410286403151
epoch: 31, step: 93
	action: tensor([[ 0.0066,  0.0155,  0.0041, -0.0025, -0.0054, -0.0256,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[7.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999005820439389, distance: 0.9574950811052575 entropy -11.037640859033909
epoch: 31, step: 94
	action: tensor([[ 0.0070,  0.0152,  0.0064, -0.0033, -0.0263,  0.0356,  0.0400]],
       dtype=torch.float64)
	q_value: tensor([[7.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3136983991565223, distance: 0.9480127957125788 entropy -11.021003087014313
epoch: 31, step: 95
	action: tensor([[ 0.0066,  0.0151, -0.0174, -0.0032, -0.0022,  0.0278,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[7.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31138839800905127, distance: 0.9496068988007413 entropy -11.025082877298987
epoch: 31, step: 96
	action: tensor([[ 0.0066,  0.0156,  0.0123, -0.0029,  0.0006,  0.0268,  0.0422]],
       dtype=torch.float64)
	q_value: tensor([[7.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31190022718625476, distance: 0.949253922707398 entropy -11.025024066425052
epoch: 31, step: 97
	action: tensor([[ 0.0068,  0.0149,  0.0100, -0.0034, -0.0122,  0.0170, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[7.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090145269098078, distance: 0.9512422945208461 entropy -11.023699351026517
epoch: 31, step: 98
	action: tensor([[ 0.0066,  0.0154,  0.0172, -0.0025, -0.0068, -0.0106,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[7.1108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30317302575392235, distance: 0.9552546715463431 entropy -11.03988807977122
epoch: 31, step: 99
	action: tensor([[ 0.0070,  0.0150,  0.0111, -0.0032, -0.0076,  0.0234,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[7.1407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3109193163982713, distance: 0.9499302794258785 entropy -11.028833717898573
epoch: 31, step: 100
	action: tensor([[ 0.0068,  0.0152,  0.0113, -0.0029, -0.0108,  0.0167,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[7.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093470426264915, distance: 0.9510133887857917 entropy -11.032072095504228
epoch: 31, step: 101
	action: tensor([[ 0.0068,  0.0150,  0.0033, -0.0031, -0.0209,  0.0057,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[7.1355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066271341455129, distance: 0.952884174736061 entropy -11.027901894719692
epoch: 31, step: 102
	action: tensor([[ 0.0068,  0.0152, -0.0049, -0.0031,  0.0026, -0.0022,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[7.1275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30500435369489765, distance: 0.9539985954070868 entropy -11.02474474874361
epoch: 31, step: 103
	action: tensor([[ 0.0069,  0.0153,  0.0072, -0.0032, -0.0083, -0.0002, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[7.1208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055409356402591, distance: 0.9536302497434729 entropy -11.021107223483137
epoch: 31, step: 104
	action: tensor([[ 0.0068,  0.0153,  0.0132, -0.0028, -0.0056,  0.0002,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[7.1186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057625420248402, distance: 0.9534780828090582 entropy -11.034292680960393
epoch: 31, step: 105
	action: tensor([[ 0.0069,  0.0150,  0.0024, -0.0032,  0.0051,  0.0163, -0.0348]],
       dtype=torch.float64)
	q_value: tensor([[7.1398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30925108206520247, distance: 0.9510794542444839 entropy -11.026683198738008
epoch: 31, step: 106
	action: tensor([[ 0.0066,  0.0155,  0.0046, -0.0024, -0.0081, -0.0020,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[7.1000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30531744655508164, distance: 0.9537836848549809 entropy -11.041227990358228
epoch: 31, step: 107
	action: tensor([[ 6.8841e-03,  1.5225e-02,  8.5324e-03, -3.0646e-03, -1.2035e-02,
         -6.4774e-05,  3.9699e-02]], dtype=torch.float64)
	q_value: tensor([[7.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055753111957418, distance: 0.9536066472181964 entropy -11.025867512108182
epoch: 31, step: 108
	action: tensor([[ 0.0069,  0.0150,  0.0027, -0.0034, -0.0023,  0.0126,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[7.1427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30823423818456797, distance: 0.9517792336039624 entropy -11.020356082525783
epoch: 31, step: 109
	action: tensor([[ 0.0068,  0.0151, -0.0011, -0.0031, -0.0138,  0.0121,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[7.1292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30820141750351193, distance: 0.9518018118189264 entropy -11.025467173755496
epoch: 31, step: 110
	action: tensor([[ 0.0067,  0.0154,  0.0135, -0.0029, -0.0164,  0.0226,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[7.1127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3108530598630056, distance: 0.9499759472102892 entropy -11.028319652976709
epoch: 31, step: 111
	action: tensor([[ 0.0067,  0.0152,  0.0207, -0.0028, -0.0013,  0.0061,  0.0417]],
       dtype=torch.float64)
	q_value: tensor([[7.1232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30694209045125953, distance: 0.952667732051954 entropy -11.033995301900992
epoch: 31, step: 112
	action: tensor([[ 0.0070,  0.0148,  0.0130, -0.0035, -0.0037, -0.0168,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[7.1562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30122422194422105, distance: 0.9565895109325748 entropy -11.023860828908129
epoch: 31, step: 113
	action: tensor([[ 7.0073e-03,  1.5164e-02,  2.5491e-03, -3.0194e-03,  2.3529e-06,
         -1.8708e-02,  3.5100e-02]], dtype=torch.float64)
	q_value: tensor([[7.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30140733299460676, distance: 0.9564641677307564 entropy -11.0317156014136
epoch: 31, step: 114
	action: tensor([[ 0.0070,  0.0151,  0.0004, -0.0034,  0.0041,  0.0085,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[7.1381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074341485541654, distance: 0.952329483902614 entropy -11.017961755828793
epoch: 31, step: 115
	action: tensor([[ 0.0069,  0.0151,  0.0196, -0.0032, -0.0069,  0.0012,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[7.1286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30569953262932903, distance: 0.9535213509400287 entropy -11.024207319873994
epoch: 31, step: 116
	action: tensor([[ 0.0070,  0.0149,  0.0111, -0.0033,  0.0045,  0.0226,  0.0405]],
       dtype=torch.float64)
	q_value: tensor([[7.1482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3106413992670257, distance: 0.9501218210629866 entropy -11.026733542865541
epoch: 31, step: 117
	action: tensor([[ 0.0068,  0.0149,  0.0061, -0.0034, -0.0203,  0.0342,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[7.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3129324547135235, distance: 0.9485416613363166 entropy -11.02420608478992
epoch: 31, step: 118
	action: tensor([[ 0.0066,  0.0151, -0.0011, -0.0032, -0.0139, -0.0008,  0.0026]],
       dtype=torch.float64)
	q_value: tensor([[7.1331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30497816856290727, distance: 0.9540165669900601 entropy -11.02597147506836
epoch: 31, step: 119
	action: tensor([[ 0.0068,  0.0154,  0.0084, -0.0029, -0.0079,  0.0194,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[7.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3102252082891508, distance: 0.950408589417796 entropy -11.028998114331458
epoch: 31, step: 120
	action: tensor([[ 0.0068,  0.0152,  0.0053, -0.0029, -0.0064, -0.0052,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[7.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30434177815132324, distance: 0.9544532353465661 entropy -11.032399755494003
epoch: 31, step: 121
	action: tensor([[ 0.0069,  0.0152,  0.0092, -0.0030, -0.0091, -0.0051,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[7.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304478003783054, distance: 0.9543597789934707 entropy -11.028476449162882
epoch: 31, step: 122
	action: tensor([[ 0.0069,  0.0152,  0.0017, -0.0030, -0.0079,  0.0232,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[7.1253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3109931710475151, distance: 0.9498793720000951 entropy -11.028946742208253
epoch: 31, step: 123
	action: tensor([[ 0.0067,  0.0153,  0.0292, -0.0028, -0.0078,  0.0016,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[7.1129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059472134292527, distance: 0.9533512588774469 entropy -11.031571454634046
epoch: 31, step: 124
	action: tensor([[ 0.0070,  0.0148,  0.0074, -0.0033, -0.0047,  0.0166,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[7.1578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090693858636937, distance: 0.9512045330953179 entropy -11.028474439699208
epoch: 31, step: 125
	action: tensor([[ 0.0068,  0.0152,  0.0007, -0.0028, -0.0130,  0.0003,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[7.1210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056852074598514, distance: 0.9535311876642847 entropy -11.033481996457668
epoch: 31, step: 126
	action: tensor([[ 6.8257e-03,  1.5320e-02,  1.0318e-02, -3.0257e-03, -9.3747e-03,
         -4.8113e-05,  1.3083e-02]], dtype=torch.float64)
	q_value: tensor([[7.1196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056099080335698, distance: 0.953582892168882 entropy -11.024911312677474
epoch: 31, step: 127
	action: tensor([[ 0.0069,  0.0152, -0.0011, -0.0031, -0.0130, -0.0134, -0.0218]],
       dtype=torch.float64)
	q_value: tensor([[7.1292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025752429936962, distance: 0.9556643229952232 entropy -11.027824749095176
LOSS epoch 31 actor 23.318705348930983 critic 12.709002045893607
epoch: 32, step: 0
	action: tensor([[ 0.0062,  0.0158,  0.0059, -0.0029, -0.0059, -0.0318, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[6.9216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981837833057074, distance: 0.9586683572016125 entropy -11.02865371926666
epoch: 32, step: 1
	action: tensor([[ 0.0064,  0.0158,  0.0304, -0.0031, -0.0072, -0.0204, -0.0313]],
       dtype=torch.float64)
	q_value: tensor([[6.9310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30063646401773614, distance: 0.9569917321497287 entropy -11.027026779963823
epoch: 32, step: 2
	action: tensor([[ 0.0063,  0.0154,  0.0012, -0.0030,  0.0025,  0.0258,  0.0399]],
       dtype=torch.float64)
	q_value: tensor([[6.9484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31116180511575586, distance: 0.9497631236420067 entropy -11.03753446351464
epoch: 32, step: 3
	action: tensor([[ 0.0062,  0.0153,  0.0127, -0.0036, -0.0059, -0.0031,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[6.9465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039795777754316, distance: 0.9547016751142017 entropy -11.01618232720831
epoch: 32, step: 4
	action: tensor([[ 0.0064,  0.0153,  0.0071, -0.0036, -0.0105, -0.0246, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[6.9541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993247828112171, distance: 0.9578887477800948 entropy -11.015247559490959
epoch: 32, step: 5
	action: tensor([[ 0.0064,  0.0157,  0.0118, -0.0031, -0.0002,  0.0009, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[6.9326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30563051708955224, distance: 0.9535687411972191 entropy -11.025838319011097
epoch: 32, step: 6
	action: tensor([[ 0.0063,  0.0155,  0.0003, -0.0031, -0.0080,  0.0021,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[6.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30567351687670385, distance: 0.9535392152108887 entropy -11.02754825224667
epoch: 32, step: 7
	action: tensor([[ 0.0062,  0.0157,  0.0058, -0.0031, -0.0106,  0.0084,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[6.9327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30716259592794104, distance: 0.9525161681197424 entropy -11.022925159576209
epoch: 32, step: 8
	action: tensor([[ 0.0062,  0.0156,  0.0014, -0.0031,  0.0039, -0.0123, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.9361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023678580561896, distance: 0.955806399723013 entropy -11.025733477757218
epoch: 32, step: 9
	action: tensor([[ 0.0063,  0.0157,  0.0109, -0.0030, -0.0058,  0.0002, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[6.9298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053956383582701, distance: 0.9537300055416738 entropy -11.027710556036535
epoch: 32, step: 10
	action: tensor([[ 0.0062,  0.0156,  0.0205, -0.0029, -0.0114,  0.0168,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[6.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3091356184143471, distance: 0.9511589408093389 entropy -11.029817914903811
epoch: 32, step: 11
	action: tensor([[ 0.0063,  0.0153,  0.0039, -0.0032, -0.0114,  0.0158,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[6.9504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3085784205176353, distance: 0.9515424292026121 entropy -11.027480767243917
epoch: 32, step: 12
	action: tensor([[ 0.0062,  0.0155,  0.0057, -0.0033, -0.0038,  0.0069,  0.0172]],
       dtype=torch.float64)
	q_value: tensor([[6.9398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30660267060927204, distance: 0.9529009843864525 entropy -11.02062632104134
epoch: 32, step: 13
	action: tensor([[ 0.0063,  0.0155,  0.0088, -0.0033, -0.0119,  0.0009, -0.0169]],
       dtype=torch.float64)
	q_value: tensor([[6.9431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052647860817499, distance: 0.9538198349988327 entropy -11.020686156223192
epoch: 32, step: 14
	action: tensor([[ 0.0062,  0.0157,  0.0127, -0.0029, -0.0125,  0.0115,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[6.9283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079253453320626, distance: 0.9519917079749165 entropy -11.030219765697591
epoch: 32, step: 15
	action: tensor([[ 0.0062,  0.0155,  0.0090, -0.0031, -0.0079,  0.0090, -0.0356]],
       dtype=torch.float64)
	q_value: tensor([[6.9393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072108358550658, distance: 0.9524830073015499 entropy -11.0263264101946
epoch: 32, step: 16
	action: tensor([[ 0.0060,  0.0157, -0.0003, -0.0026, -0.0061,  0.0105, -0.0523]],
       dtype=torch.float64)
	q_value: tensor([[6.9215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30770087678636204, distance: 0.9521460806775314 entropy -11.035212416588754
epoch: 32, step: 17
	action: tensor([[ 5.8947e-03,  1.5924e-02, -3.0952e-03, -2.4655e-03, -3.4607e-03,
          5.2894e-03,  9.1608e-05]], dtype=torch.float64)
	q_value: tensor([[6.9104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066063791189342, distance: 0.9528984361741341 entropy -11.038335707096047
epoch: 32, step: 18
	action: tensor([[ 0.0062,  0.0157,  0.0072, -0.0031,  0.0063,  0.0077,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[6.9271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30704154021613783, distance: 0.9525993784702042 entropy -11.022026273457382
epoch: 32, step: 19
	action: tensor([[ 0.0063,  0.0154,  0.0123, -0.0033, -0.0076, -0.0103,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[6.9423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30267069198346697, distance: 0.9555989250333176 entropy -11.022061171178754
epoch: 32, step: 20
	action: tensor([[ 0.0064,  0.0155,  0.0079, -0.0032, -0.0147, -0.0252, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[6.9436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995237610561401, distance: 0.9577527271597548 entropy -11.025669621072646
epoch: 32, step: 21
	action: tensor([[ 0.0064,  0.0157,  0.0190, -0.0031, -0.0122, -0.0011,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[6.9339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051291537639127, distance: 0.9539129370029604 entropy -11.024945580709437
epoch: 32, step: 22
	action: tensor([[ 0.0063,  0.0154,  0.0198, -0.0031, -0.0141, -0.0133,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[6.9452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30205120849190525, distance: 0.9560232914961774 entropy -11.02772202432578
epoch: 32, step: 23
	action: tensor([[ 0.0064,  0.0154,  0.0204, -0.0034, -0.0116, -0.0009,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[6.9521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048450790890268, distance: 0.9541079047577923 entropy -11.022076167928262
epoch: 32, step: 24
	action: tensor([[ 0.0064,  0.0153,  0.0118, -0.0034, -0.0103,  0.0024, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[6.9549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30554923056196526, distance: 0.9536245544388212 entropy -11.023841635549559
epoch: 32, step: 25
	action: tensor([[ 0.0062,  0.0156,  0.0021, -0.0030, -0.0049,  0.0167,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[6.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090482627471891, distance: 0.9512190730868868 entropy -11.02944698105783
epoch: 32, step: 26
	action: tensor([[ 0.0062,  0.0156,  0.0039, -0.0030, -0.0026,  0.0098,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[6.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074271577225858, distance: 0.9523342903468069 entropy -11.025754057758084
epoch: 32, step: 27
	action: tensor([[ 0.0063,  0.0155,  0.0016, -0.0034,  0.0017,  0.0021, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[6.9412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055083321762926, distance: 0.9536526349970943 entropy -11.018171253482143
epoch: 32, step: 28
	action: tensor([[ 0.0061,  0.0158,  0.0061, -0.0028, -0.0208, -0.0124,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[6.9201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025578523664031, distance: 0.955676237899511 entropy -11.031807664728662
epoch: 32, step: 29
	action: tensor([[ 0.0063,  0.0156,  0.0002, -0.0033, -0.0054, -0.0089,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[6.9405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30318381062089805, distance: 0.9552472792274432 entropy -11.019956056154014
epoch: 32, step: 30
	action: tensor([[ 0.0064,  0.0154,  0.0018, -0.0037, -0.0159, -0.0105, -0.0272]],
       dtype=torch.float64)
	q_value: tensor([[6.9508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30254352552941965, distance: 0.9556860535863048 entropy -11.008877907155568
epoch: 32, step: 31
	action: tensor([[ 0.0061,  0.0159,  0.0088, -0.0028, -0.0107,  0.0022,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[6.9190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059612961917342, distance: 0.9533415867835476 entropy -11.030823147072095
epoch: 32, step: 32
	action: tensor([[ 0.0063,  0.0156,  0.0152, -0.0032, -0.0012, -0.0145, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[6.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017779930233544, distance: 0.9562103931844906 entropy -11.023717905573802
epoch: 32, step: 33
	action: tensor([[ 0.0064,  0.0155,  0.0010, -0.0032, -0.0071,  0.0201,  0.0293]],
       dtype=torch.float64)
	q_value: tensor([[6.9458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098196280257125, distance: 0.9506879634430082 entropy -11.027942908716339
epoch: 32, step: 34
	action: tensor([[ 0.0062,  0.0155,  0.0222, -0.0034, -0.0116, -0.0038,  0.0400]],
       dtype=torch.float64)
	q_value: tensor([[6.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30400499663344116, distance: 0.9546842419706375 entropy -11.017493205562511
epoch: 32, step: 35
	action: tensor([[ 0.0064,  0.0152,  0.0187, -0.0037, -0.0134, -0.0232,  0.0520]],
       dtype=torch.float64)
	q_value: tensor([[6.9654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29941232514048843, distance: 0.9578289066229472 entropy -11.01713122141532
epoch: 32, step: 36
	action: tensor([[ 0.0065,  0.0151,  0.0207, -0.0039, -0.0014,  0.0019, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[6.9673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30518117654345356, distance: 0.9538772281143613 entropy -11.010696284086288
epoch: 32, step: 37
	action: tensor([[ 0.0063,  0.0154,  0.0173, -0.0030, -0.0006,  0.0031, -0.0530]],
       dtype=torch.float64)
	q_value: tensor([[6.9448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058602308910394, distance: 0.9534109966350937 entropy -11.032746143211167
epoch: 32, step: 38
	action: tensor([[ 0.0060,  0.0157,  0.0010, -0.0025,  0.0042,  0.0143,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[6.9236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30859258606574025, distance: 0.9515326817565574 entropy -11.041862428709964
epoch: 32, step: 39
	action: tensor([[ 0.0063,  0.0155,  0.0084, -0.0034, -0.0092,  0.0296,  0.0437]],
       dtype=torch.float64)
	q_value: tensor([[6.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117717408032701, distance: 0.9493425439472499 entropy -11.01768817642649
epoch: 32, step: 40
	action: tensor([[ 0.0062,  0.0153,  0.0156, -0.0036, -0.0097,  0.0009,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[6.9488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048641881516816, distance: 0.9540947909665496 entropy -11.017445522272414
epoch: 32, step: 41
	action: tensor([[ 0.0064,  0.0153,  0.0166, -0.0036, -0.0071, -0.0088, -0.0387]],
       dtype=torch.float64)
	q_value: tensor([[6.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30285777544195036, distance: 0.9554707296870288 entropy -11.01747046981096
epoch: 32, step: 42
	action: tensor([[ 0.0061,  0.0156,  0.0097, -0.0027, -0.0105,  0.0032,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[6.9311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060025701738601, distance: 0.9533132390931259 entropy -11.037639554781967
epoch: 32, step: 43
	action: tensor([[ 0.0063,  0.0155,  0.0083, -0.0032, -0.0075,  0.0193,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[6.9410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30957247867618953, distance: 0.9508581659657585 entropy -11.02501736172849
epoch: 32, step: 44
	action: tensor([[ 0.0062,  0.0154, -0.0041, -0.0033, -0.0174,  0.0039, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[6.9456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30581395686921986, distance: 0.9534427751240794 entropy -11.021236213211735
epoch: 32, step: 45
	action: tensor([[ 0.0061,  0.0159,  0.0043, -0.0029, -0.0158, -0.0045,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[6.9190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30436456507440257, distance: 0.9544376032230037 entropy -11.025304881036943
epoch: 32, step: 46
	action: tensor([[ 0.0063,  0.0157,  0.0024, -0.0031, -0.0050, -0.0029,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[6.9338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046041090056678, distance: 0.9542732574968884 entropy -11.022015820450404
epoch: 32, step: 47
	action: tensor([[ 0.0063,  0.0155,  0.0053, -0.0034, -0.0142,  0.0039,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[6.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059376051403714, distance: 0.9533578578298434 entropy -11.016843388186803
epoch: 32, step: 48
	action: tensor([[ 0.0063,  0.0155,  0.0096, -0.0034, -0.0092, -0.0160,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[6.9426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013406996091894, distance: 0.956509781526085 entropy -11.016337096496972
epoch: 32, step: 49
	action: tensor([[ 0.0064,  0.0155,  0.0094, -0.0033, -0.0033, -0.0168,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[6.9454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013060343231837, distance: 0.9565335107413409 entropy -11.021023307808676
epoch: 32, step: 50
	action: tensor([[ 0.0064,  0.0155,  0.0164, -0.0033, -0.0038, -0.0067,  0.0254]],
       dtype=torch.float64)
	q_value: tensor([[6.9456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30361787438661625, distance: 0.9549497094883099 entropy -11.02169122728168
epoch: 32, step: 51
	action: tensor([[ 0.0064,  0.0153,  0.0070, -0.0035, -0.0078,  0.0081,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[6.9565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30684738409448997, distance: 0.952732820846711 entropy -11.01998700828552
epoch: 32, step: 52
	action: tensor([[ 0.0062,  0.0156,  0.0185, -0.0031, -0.0019,  0.0170,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[6.9367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30914702444265396, distance: 0.9511510890580609 entropy -11.026401390093492
epoch: 32, step: 53
	action: tensor([[ 0.0063,  0.0151, -0.0014, -0.0036, -0.0140,  0.0021,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[6.9591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052282115074896, distance: 0.9538449417550176 entropy -11.018548924672439
epoch: 32, step: 54
	action: tensor([[ 0.0062,  0.0157, -0.0004, -0.0031, -0.0034,  0.0107,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[6.9280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30772634571306023, distance: 0.9521285663105766 entropy -11.021002193780916
epoch: 32, step: 55
	action: tensor([[ 6.2174e-03,  1.5604e-02,  6.7555e-05, -3.2394e-03, -4.1007e-04,
         -1.7349e-02, -8.4637e-03]], dtype=torch.float64)
	q_value: tensor([[6.9336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30116803315873864, distance: 0.9566279699948737 entropy -11.02003947780167
epoch: 32, step: 56
	action: tensor([[ 0.0063,  0.0158, -0.0089, -0.0031, -0.0192, -0.0364,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[6.9288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974322957232659, distance: 0.959181479154469 entropy -11.024257285649641
epoch: 32, step: 57
	action: tensor([[ 0.0064,  0.0158, -0.0007, -0.0035, -0.0089, -0.0152, -0.0199]],
       dtype=torch.float64)
	q_value: tensor([[6.9331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301917856996063, distance: 0.9561146169978666 entropy -11.008587669757267
epoch: 32, step: 58
	action: tensor([[ 0.0062,  0.0158,  0.0081, -0.0029, -0.0193,  0.0171,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[6.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30932570695553296, distance: 0.9510280780387821 entropy -11.027186063599103
epoch: 32, step: 59
	action: tensor([[ 0.0062,  0.0154,  0.0172, -0.0033, -0.0067,  0.0017,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[6.9445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30528342826047195, distance: 0.9538070377497789 entropy -11.020726831758294
epoch: 32, step: 60
	action: tensor([[ 0.0064,  0.0152,  0.0127, -0.0036, -0.0054, -0.0165,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[6.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010186226254947, distance: 0.9567302282365351 entropy -11.01660799936631
epoch: 32, step: 61
	action: tensor([[ 0.0065,  0.0154,  0.0206, -0.0035, -0.0095, -0.0185,  0.0386]],
       dtype=torch.float64)
	q_value: tensor([[6.9489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30079004576291024, distance: 0.9568866476530745 entropy -11.018880830025827
epoch: 32, step: 62
	action: tensor([[ 0.0065,  0.0152, -0.0014, -0.0037, -0.0053, -0.0082, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[6.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030677727222173, distance: 0.9553268125925123 entropy -11.015283557984123
epoch: 32, step: 63
	action: tensor([[ 0.0062,  0.0158,  0.0016, -0.0030, -0.0013,  0.0109,  0.0232]],
       dtype=torch.float64)
	q_value: tensor([[6.9230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30791483666319264, distance: 0.951998935611113 entropy -11.02631729832259
epoch: 32, step: 64
	action: tensor([[ 0.0063,  0.0154,  0.0014, -0.0034, -0.0021,  0.0103,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[6.9428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30737180588184687, distance: 0.952372345840433 entropy -11.01814815373234
epoch: 32, step: 65
	action: tensor([[ 0.0062,  0.0156,  0.0165, -0.0031, -0.0065,  0.0102,  0.0380]],
       dtype=torch.float64)
	q_value: tensor([[6.9341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3075075863803318, distance: 0.9522789913313823 entropy -11.024134019786215
epoch: 32, step: 66
	action: tensor([[ 0.0063,  0.0152,  0.0111, -0.0036, -0.0140,  0.0039, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[6.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056958711735849, distance: 0.9535238651768676 entropy -11.017271960782287
epoch: 32, step: 67
	action: tensor([[ 6.1800e-03,  1.5630e-02,  2.6376e-05, -2.9320e-03, -5.1155e-03,
         -1.0583e-02,  1.6192e-02]], dtype=torch.float64)
	q_value: tensor([[6.9338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30284011371101616, distance: 0.9554828327841219 entropy -11.030602434114366
epoch: 32, step: 68
	action: tensor([[ 0.0064,  0.0156,  0.0106, -0.0033, -0.0054,  0.0032,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[6.9387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30590548207371804, distance: 0.9533799195521736 entropy -11.017593262077483
epoch: 32, step: 69
	action: tensor([[ 0.0063,  0.0155,  0.0198, -0.0032, -0.0042, -0.0237,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[6.9426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995785091106423, distance: 0.957715298250542 entropy -11.025813590769802
epoch: 32, step: 70
	action: tensor([[ 0.0065,  0.0154, -0.0036, -0.0034, -0.0044,  0.0070,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[6.9539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30678082205909907, distance: 0.9527785642457357 entropy -11.024987520978891
epoch: 32, step: 71
	action: tensor([[ 0.0062,  0.0157,  0.0113, -0.0032,  0.0042, -0.0164,  0.0509]],
       dtype=torch.float64)
	q_value: tensor([[6.9286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30134676476135647, distance: 0.9565056297242256 entropy -11.020017135850741
epoch: 32, step: 72
	action: tensor([[ 0.0065,  0.0152,  0.0096, -0.0039, -0.0022, -0.0114,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[6.9618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302146397216317, distance: 0.9559580963545282 entropy -11.009400519767212
epoch: 32, step: 73
	action: tensor([[ 0.0065,  0.0153,  0.0012, -0.0036, -0.0077,  0.0047, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[6.9547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060356927253213, distance: 0.9532904893376997 entropy -11.014790731439865
epoch: 32, step: 74
	action: tensor([[ 0.0062,  0.0157,  0.0065, -0.0030, -0.0017,  0.0120,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[6.9267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080695620857866, distance: 0.9518925132522711 entropy -11.025825603362113
epoch: 32, step: 75
	action: tensor([[ 0.0062,  0.0155,  0.0044, -0.0032, -0.0100, -0.0099,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[6.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30282547487912503, distance: 0.9554928642556789 entropy -11.02299429003268
epoch: 32, step: 76
	action: tensor([[ 0.0064,  0.0155,  0.0128, -0.0035,  0.0019, -0.0103,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[6.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026679096041469, distance: 0.9556008314755139 entropy -11.01513810104692
epoch: 32, step: 77
	action: tensor([[ 0.0065,  0.0154,  0.0109, -0.0034,  0.0002,  0.0028, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[6.9493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057398818631978, distance: 0.9534936436167668 entropy -11.020641181695064
epoch: 32, step: 78
	action: tensor([[ 0.0063,  0.0155,  0.0098, -0.0030,  0.0015,  0.0351, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[6.9383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3134305469282853, distance: 0.9481977745675998 entropy -11.029356918245924
epoch: 32, step: 79
	action: tensor([[ 0.0061,  0.0155,  0.0016, -0.0029,  0.0045, -0.0088, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[6.9355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30303043515203754, distance: 0.95535240267343 entropy -11.030642731790937
epoch: 32, step: 80
	action: tensor([[ 0.0062,  0.0157,  0.0052, -0.0030,  0.0013, -0.0123, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[6.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.302537887115782, distance: 0.9556899165817636 entropy -11.028036077734644
epoch: 32, step: 81
	action: tensor([[ 0.0061,  0.0158,  0.0191, -0.0027, -0.0017,  0.0055,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[6.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066781132417894, distance: 0.9528491444735583 entropy -11.037882655223182
epoch: 32, step: 82
	action: tensor([[ 0.0064,  0.0152,  0.0074, -0.0034, -0.0128,  0.0127,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[6.9549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078668453998208, distance: 0.9520319422713241 entropy -11.024202685905848
epoch: 32, step: 83
	action: tensor([[ 0.0062,  0.0154,  0.0084, -0.0034, -0.0067,  0.0397, -0.0421]],
       dtype=torch.float64)
	q_value: tensor([[6.9451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3141723556580608, distance: 0.9476853926959521 entropy -11.01955581335812
epoch: 32, step: 84
	action: tensor([[ 0.0058,  0.0158, -0.0045, -0.0024,  0.0012, -0.0221,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[6.9184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001130377654373, distance: 0.957349786923795 entropy -11.038175322978924
epoch: 32, step: 85
	action: tensor([[ 0.0064,  0.0155,  0.0102, -0.0036, -0.0131, -0.0076, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[6.9421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30336396031617985, distance: 0.9551237899680877 entropy -11.010879760072232
epoch: 32, step: 86
	action: tensor([[ 0.0063,  0.0156, -0.0002, -0.0031, -0.0029, -0.0014, -0.0177]],
       dtype=torch.float64)
	q_value: tensor([[6.9381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049635384938262, distance: 0.9540266078651803 entropy -11.02729708612012
epoch: 32, step: 87
	action: tensor([[ 0.0062,  0.0158,  0.0138, -0.0029, -0.0046, -0.0024,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[6.9225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30477587071414125, distance: 0.954155398210069 entropy -11.028258977535938
epoch: 32, step: 88
	action: tensor([[ 0.0064,  0.0153,  0.0020, -0.0035, -0.0153, -0.0001,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[6.9508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049739952446584, distance: 0.9540194312229461 entropy -11.019266332536931
epoch: 32, step: 89
	action: tensor([[ 0.0063,  0.0156,  0.0093, -0.0032, -0.0150, -0.0053,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[6.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039621639655812, distance: 0.9547136179314806 entropy -11.020313069420554
epoch: 32, step: 90
	action: tensor([[ 0.0063,  0.0155,  0.0064, -0.0033, -0.0104, -0.0089,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[6.9460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030749578722134, distance: 0.9553218880217528 entropy -11.019894604584886
epoch: 32, step: 91
	action: tensor([[ 0.0064,  0.0155,  0.0261, -0.0033, -0.0187, -0.0078, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[6.9407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30334503353157194, distance: 0.9551367646768544 entropy -11.018730351382507
epoch: 32, step: 92
	action: tensor([[ 0.0063,  0.0155,  0.0025, -0.0031, -0.0081, -0.0209,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[6.9462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30040778025324566, distance: 0.9571481819596857 entropy -11.031867824350078
epoch: 32, step: 93
	action: tensor([[ 0.0064,  0.0157,  0.0135, -0.0032, -0.0092,  0.0104, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[6.9351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.307752673522962, distance: 0.9521104609716836 entropy -11.022536240056548
epoch: 32, step: 94
	action: tensor([[ 0.0062,  0.0155,  0.0050, -0.0031, -0.0105, -0.0180, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[6.9395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30105707671704673, distance: 0.9567039108703003 entropy -11.027534012979716
epoch: 32, step: 95
	action: tensor([[ 0.0063,  0.0157,  0.0098, -0.0031, -0.0048,  0.0026,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[6.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30596720605150085, distance: 0.9533375278405445 entropy -11.025151874894458
epoch: 32, step: 96
	action: tensor([[ 0.0064,  0.0154,  0.0107, -0.0035, -0.0054,  0.0056, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[6.9494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062486535823702, distance: 0.953144207219963 entropy -11.016642188486058
epoch: 32, step: 97
	action: tensor([[ 0.0062,  0.0156,  0.0073, -0.0030, -0.0060,  0.0163, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[6.9343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30900926193238154, distance: 0.9512459185186452 entropy -11.028846204316622
epoch: 32, step: 98
	action: tensor([[ 0.0061,  0.0156,  0.0021, -0.0029, -0.0078,  0.0007,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[6.9285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305387432340696, distance: 0.9537356391818463 entropy -11.029824124432263
epoch: 32, step: 99
	action: tensor([[ 0.0062,  0.0157,  0.0092, -0.0031, -0.0025, -0.0043,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[6.9316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042209864636365, distance: 0.9545360957292147 entropy -11.02262658749676
epoch: 32, step: 100
	action: tensor([[ 0.0064,  0.0155,  0.0017, -0.0033, -0.0039,  0.0062,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[6.9448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30659057284143054, distance: 0.952909297026345 entropy -11.022902133661438
epoch: 32, step: 101
	action: tensor([[ 0.0062,  0.0156,  0.0030, -0.0032, -0.0039, -0.0219,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[6.9334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30017188921104676, distance: 0.9573095357072411 entropy -11.021400756992554
epoch: 32, step: 102
	action: tensor([[ 0.0065,  0.0154,  0.0071, -0.0036, -0.0122,  0.0229,  0.0514]],
       dtype=torch.float64)
	q_value: tensor([[6.9478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31034293623510634, distance: 0.9503274800274414 entropy -11.013153244619316
epoch: 32, step: 103
	action: tensor([[ 0.0062,  0.0152,  0.0156, -0.0037, -0.0094, -0.0032, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[6.9536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038414658588583, distance: 0.9547963915431213 entropy -11.014495859801896
epoch: 32, step: 104
	action: tensor([[ 0.0062,  0.0156,  0.0124, -0.0030, -0.0099,  0.0080, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[6.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30709027145188406, distance: 0.9525658828397052 entropy -11.031052792843552
epoch: 32, step: 105
	action: tensor([[ 0.0061,  0.0156,  0.0098, -0.0029, -0.0149, -0.0140, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[6.9317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30202275262438427, distance: 0.9560427801715501 entropy -11.031228878048386
epoch: 32, step: 106
	action: tensor([[ 0.0062,  0.0157, -0.0069, -0.0029, -0.0185, -0.0025,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[6.9306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048058897226681, distance: 0.954134798300611 entropy -11.029001815947014
epoch: 32, step: 107
	action: tensor([[ 0.0062,  0.0157,  0.0168, -0.0034, -0.0044, -0.0170,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[6.9337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011434689846041, distance: 0.9566447827414695 entropy -11.013381851076405
epoch: 32, step: 108
	action: tensor([[ 0.0065,  0.0153,  0.0245, -0.0036, -0.0108,  0.0211,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[6.9573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099043653672694, distance: 0.9506296009867872 entropy -11.016833609912505
epoch: 32, step: 109
	action: tensor([[ 0.0063,  0.0152,  0.0015, -0.0033, -0.0088,  0.0126,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[6.9561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30773090485870036, distance: 0.9521254310620213 entropy -11.026868467802771
epoch: 32, step: 110
	action: tensor([[ 0.0062,  0.0157,  0.0061, -0.0031, -0.0001, -0.0202,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[6.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005207034107956, distance: 0.9570709308455402 entropy -11.02352249026022
epoch: 32, step: 111
	action: tensor([[ 0.0065,  0.0153,  0.0130, -0.0038, -0.0024,  0.0287,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[6.9537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31162166708157835, distance: 0.949446044203826 entropy -11.01001059390504
epoch: 32, step: 112
	action: tensor([[ 0.0062,  0.0153,  0.0139, -0.0033, -0.0057,  0.0097,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[6.9477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070742860305492, distance: 0.9525768706195973 entropy -11.022360481109036
epoch: 32, step: 113
	action: tensor([[ 0.0063,  0.0154,  0.0065, -0.0032, -0.0100,  0.0041,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[6.9422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060307639902555, distance: 0.9532938746039669 entropy -11.026118630739713
epoch: 32, step: 114
	action: tensor([[ 0.0063,  0.0156,  0.0125, -0.0032, -0.0054,  0.0049,  0.0642]],
       dtype=torch.float64)
	q_value: tensor([[6.9368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30628330152223293, distance: 0.9531204055396582 entropy -11.022762753255401
epoch: 32, step: 115
	action: tensor([[ 0.0064,  0.0151,  0.0067, -0.0039, -0.0182,  0.0134, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[6.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30760749929769926, distance: 0.9522102913698876 entropy -11.007920471304464
epoch: 32, step: 116
	action: tensor([[ 0.0061,  0.0157,  0.0023, -0.0029, -0.0046,  0.0099,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[6.9299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3075178776694939, distance: 0.952271915286446 entropy -11.02941627840024
epoch: 32, step: 117
	action: tensor([[ 0.0062,  0.0156,  0.0252, -0.0032, -0.0065, -0.0014, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[6.9361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30475542018277546, distance: 0.9541694316998328 entropy -11.023067325253853
epoch: 32, step: 118
	action: tensor([[ 0.0063,  0.0153,  0.0002, -0.0032, -0.0044,  0.0220,  0.0438]],
       dtype=torch.float64)
	q_value: tensor([[6.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31010005568567156, distance: 0.9504948064815854 entropy -11.030220128936923
epoch: 32, step: 119
	action: tensor([[ 0.0062,  0.0154,  0.0137, -0.0036, -0.0153, -0.0028,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[6.9456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30406938874294265, distance: 0.9546400781807514 entropy -11.014536117308342
epoch: 32, step: 120
	action: tensor([[ 0.0064,  0.0154, -0.0081, -0.0034, -0.0204,  0.0163,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[6.9517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308648547788255, distance: 0.9514941729957623 entropy -11.019394836832804
epoch: 32, step: 121
	action: tensor([[ 0.0061,  0.0158,  0.0120, -0.0032, -0.0160,  0.0010,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[6.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30537578337395554, distance: 0.9537436364372068 entropy -11.017950837718077
epoch: 32, step: 122
	action: tensor([[ 0.0063,  0.0154,  0.0008, -0.0035,  0.0074, -0.0037,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[6.9518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041136599016795, distance: 0.9546097133039168 entropy -11.018149567626665
epoch: 32, step: 123
	action: tensor([[ 0.0064,  0.0155,  0.0071, -0.0034, -0.0077, -0.0087,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[6.9425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030978666477454, distance: 0.9553061865953278 entropy -11.01784635953892
epoch: 32, step: 124
	action: tensor([[ 0.0064,  0.0155,  0.0002, -0.0033, -0.0013,  0.0067, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[6.9434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30671654886537036, distance: 0.9528227326009986 entropy -11.02068250884318
epoch: 32, step: 125
	action: tensor([[ 0.0062,  0.0157,  0.0017, -0.0029, -0.0131,  0.0102,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[6.9267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30761369371347336, distance: 0.9522060319363758 entropy -11.028199631688429
epoch: 32, step: 126
	action: tensor([[ 0.0062,  0.0155,  0.0017, -0.0035, -0.0111,  0.0009,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[6.9431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051375643593299, distance: 0.9539071639876604 entropy -11.015950381355992
epoch: 32, step: 127
	action: tensor([[ 0.0063,  0.0156,  0.0144, -0.0032, -0.0055,  0.0199,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[6.9355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098267123009124, distance: 0.9506830843181313 entropy -11.021761741481601
LOSS epoch 32 actor 22.081061300833586 critic 12.767189563701901
epoch: 33, step: 0
	action: tensor([[ 0.0052,  0.0159,  0.0047, -0.0030, -0.0034,  0.0084,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[7.0216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.306263553739906, distance: 0.953133971510027 entropy -11.017217656251665
epoch: 33, step: 1
	action: tensor([[ 0.0052,  0.0160,  0.0158, -0.0030, -0.0106,  0.0029,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[7.0174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050904018292303, distance: 0.9539395357994543 entropy -11.014906967694369
epoch: 33, step: 2
	action: tensor([[ 0.0052,  0.0158,  0.0146, -0.0031,  0.0041, -0.0121,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[7.0293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30146934103919876, distance: 0.9564217182521845 entropy -11.014656113962104
epoch: 33, step: 3
	action: tensor([[ 0.0054,  0.0158, -0.0041, -0.0033,  0.0103,  0.0007, -0.0284]],
       dtype=torch.float64)
	q_value: tensor([[7.0323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30458384589121945, distance: 0.9542871606617059 entropy -11.01265529236191
epoch: 33, step: 4
	action: tensor([[ 0.0050,  0.0164,  0.0086, -0.0025, -0.0152,  0.0203,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[6.9996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3094024832398129, distance: 0.9509752177889681 entropy -11.024342956862942
epoch: 33, step: 5
	action: tensor([[ 0.0051,  0.0160, -0.0019, -0.0028, -0.0124,  0.0119,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[7.0133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3071557205082406, distance: 0.9525208942879182 entropy -11.017575808849742
epoch: 33, step: 6
	action: tensor([[ 0.0051,  0.0161, -0.0032, -0.0030, -0.0070, -0.0115,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[7.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018662448258491, distance: 0.9561499611466704 entropy -11.01116269479806
epoch: 33, step: 7
	action: tensor([[ 0.0052,  0.0162,  0.0088, -0.0029, -0.0142, -0.0017,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043054437645055, distance: 0.9544781606756616 entropy -11.013956019707555
epoch: 33, step: 8
	action: tensor([[ 0.0052,  0.0161,  0.0071, -0.0028, -0.0097, -0.0083, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[7.0169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30270889085915664, distance: 0.9555727513847733 entropy -11.019333978930218
epoch: 33, step: 9
	action: tensor([[ 0.0051,  0.0162,  0.0149, -0.0027,  0.0010,  0.0268, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[7.0136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31095137880967283, distance: 0.9499081793943142 entropy -11.02194764018166
epoch: 33, step: 10
	action: tensor([[ 0.0050,  0.0160,  0.0042, -0.0025, -0.0217, -0.0016, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[7.0137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041351968597388, distance: 0.9545949411010086 entropy -11.02620174916529
epoch: 33, step: 11
	action: tensor([[ 0.0049,  0.0163,  0.0118, -0.0024, -0.0070,  0.0221, -0.0309]],
       dtype=torch.float64)
	q_value: tensor([[6.9997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30985858572670055, distance: 0.9506611319496981 entropy -11.025496023023281
epoch: 33, step: 12
	action: tensor([[ 0.0049,  0.0161,  0.0151, -0.0023, -0.0042, -0.0102,  0.0438]],
       dtype=torch.float64)
	q_value: tensor([[7.0053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30213364917647767, distance: 0.9559668277963338 entropy -11.029110539668562
epoch: 33, step: 13
	action: tensor([[ 0.0054,  0.0157,  0.0132, -0.0034,  0.0010,  0.0061,  0.0155]],
       dtype=torch.float64)
	q_value: tensor([[7.0375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30561197107966354, distance: 0.9535814756114277 entropy -11.007815038016128
epoch: 33, step: 14
	action: tensor([[ 0.0052,  0.0159, -0.0106, -0.0030, -0.0097, -0.0100,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[7.0233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022486025885658, distance: 0.9558880905321617 entropy -11.017526364354156
epoch: 33, step: 15
	action: tensor([[ 0.0052,  0.0163,  0.0121, -0.0031,  0.0017, -0.0020,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[7.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3041219726731407, distance: 0.954604011599757 entropy -11.007517133579256
epoch: 33, step: 16
	action: tensor([[ 0.0053,  0.0159,  0.0156, -0.0030, -0.0010,  0.0212, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30945031972889336, distance: 0.9509422810187649 entropy -11.018241227279606
epoch: 33, step: 17
	action: tensor([[ 0.0051,  0.0160,  0.0016, -0.0027, -0.0089, -0.0162, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[7.0171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008195894482353, distance: 0.9568664317962583 entropy -11.023658222658739
epoch: 33, step: 18
	action: tensor([[ 0.0051,  0.0163,  0.0117, -0.0027, -0.0104,  0.0001,  0.0014]],
       dtype=torch.float64)
	q_value: tensor([[7.0064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3047851855418665, distance: 0.9541490061542169 entropy -11.020820623511487
epoch: 33, step: 19
	action: tensor([[ 0.0052,  0.0161,  0.0184, -0.0028,  0.0047,  0.0106, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[7.0174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070794053945396, distance: 0.9525733517743313 entropy -11.020043525634975
epoch: 33, step: 20
	action: tensor([[ 0.0051,  0.0159,  0.0075, -0.0027, -0.0118,  0.0040,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[7.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053829927053776, distance: 0.9537386870907523 entropy -11.02548677277426
epoch: 33, step: 21
	action: tensor([[ 0.0052,  0.0160,  0.0199, -0.0030, -0.0063,  0.0024, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[7.0168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050209518089354, distance: 0.9539872034853492 entropy -11.014138047147714
epoch: 33, step: 22
	action: tensor([[ 5.1823e-03,  1.5937e-02,  9.1031e-05, -2.7723e-03, -6.6147e-03,
         -1.1848e-02,  3.5974e-02]], dtype=torch.float64)
	q_value: tensor([[7.0234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018334487045249, distance: 0.9561724193372614 entropy -11.023579195632736
epoch: 33, step: 23
	action: tensor([[ 0.0053,  0.0160,  0.0111, -0.0033, -0.0079,  0.0033, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[7.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051533805440968, distance: 0.953896307696519 entropy -11.00675052813081
epoch: 33, step: 24
	action: tensor([[ 0.0051,  0.0161,  0.0135, -0.0027, -0.0065,  0.0274, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[7.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31097495300592004, distance: 0.9498919298056159 entropy -11.02257349952364
epoch: 33, step: 25
	action: tensor([[ 0.0049,  0.0161,  0.0087, -0.0024, -0.0055,  0.0066,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[7.0070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060273875414362, distance: 0.9532961936866403 entropy -11.02868356911293
epoch: 33, step: 26
	action: tensor([[ 0.0052,  0.0160,  0.0204, -0.0029, -0.0198, -0.0198,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[7.0174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299911511702456, distance: 0.957487607070253 entropy -11.0164825559349
epoch: 33, step: 27
	action: tensor([[ 0.0053,  0.0159,  0.0109, -0.0030, -0.0137, -0.0296,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[7.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2978086673385043, distance: 0.9589245238160411 entropy -11.018042012171481
epoch: 33, step: 28
	action: tensor([[ 0.0053,  0.0161,  0.0068, -0.0029, -0.0070,  0.0098, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[7.0222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3069684700672759, distance: 0.9526496013533795 entropy -11.017932418592853
epoch: 33, step: 29
	action: tensor([[ 0.0051,  0.0162,  0.0211, -0.0026, -0.0050,  0.0005, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[7.0095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30471697369726525, distance: 0.9541958137495125 entropy -11.02192257227245
epoch: 33, step: 30
	action: tensor([[ 0.0052,  0.0159,  0.0014, -0.0027, -0.0038, -0.0171,  0.0257]],
       dtype=torch.float64)
	q_value: tensor([[7.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3006122449575317, distance: 0.9570083023871857 entropy -11.025142160138888
epoch: 33, step: 31
	action: tensor([[ 5.3164e-03,  1.6024e-02, -3.5795e-03, -3.1777e-03, -1.2706e-02,
         -5.4187e-05,  2.5945e-02]], dtype=torch.float64)
	q_value: tensor([[7.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045396306142907, distance: 0.9543174974608762 entropy -11.009054531499787
epoch: 33, step: 32
	action: tensor([[ 0.0052,  0.0162,  0.0159, -0.0031, -0.0053, -0.0037,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[7.0100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036506246257217, distance: 0.9549272540016707 entropy -11.009029272088911
epoch: 33, step: 33
	action: tensor([[ 0.0053,  0.0159,  0.0054, -0.0029, -0.0106, -0.0037, -0.0395]],
       dtype=torch.float64)
	q_value: tensor([[7.0257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30369594726892957, distance: 0.9548961772656208 entropy -11.018392078887786
epoch: 33, step: 34
	action: tensor([[ 0.0049,  0.0163,  0.0065, -0.0023, -0.0097,  0.0072,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[7.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30637847025419274, distance: 0.9530550255568992 entropy -11.029480465406527
epoch: 33, step: 35
	action: tensor([[ 0.0052,  0.0160,  0.0072, -0.0029, -0.0179, -0.0074,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[7.0177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30279060112401834, distance: 0.9555167615771272 entropy -11.015817764744936
epoch: 33, step: 36
	action: tensor([[ 0.0052,  0.0161,  0.0112, -0.0029, -0.0031,  0.0060,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[7.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30598243094765776, distance: 0.9533270711700588 entropy -11.015928481644307
epoch: 33, step: 37
	action: tensor([[ 0.0052,  0.0159,  0.0205, -0.0031, -0.0063,  0.0131,  0.0193]],
       dtype=torch.float64)
	q_value: tensor([[7.0255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074206244549267, distance: 0.9523387821776779 entropy -11.014859586551538
epoch: 33, step: 38
	action: tensor([[ 0.0052,  0.0158,  0.0130, -0.0030, -0.0009, -0.0190, -0.0226]],
       dtype=torch.float64)
	q_value: tensor([[7.0301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999171192290223, distance: 0.9574837724493196 entropy -11.018871473869181
epoch: 33, step: 39
	action: tensor([[ 0.0052,  0.0161, -0.0054, -0.0027, -0.0211, -0.0080, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[7.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30295825124004516, distance: 0.9554018734765917 entropy -11.025728711317498
epoch: 33, step: 40
	action: tensor([[ 0.0050,  0.0165,  0.0126, -0.0025,  0.0035, -0.0224, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[6.9957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29959694904775447, distance: 0.9577026913229012 entropy -11.02021330618906
epoch: 33, step: 41
	action: tensor([[ 0.0053,  0.0160,  0.0154, -0.0028, -0.0026, -0.0286, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[7.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29801649275995146, distance: 0.958782608335216 entropy -11.023246586798367
epoch: 33, step: 42
	action: tensor([[ 0.0053,  0.0160, -0.0012, -0.0029,  0.0056, -0.0243,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[7.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991401791112196, distance: 0.9580149247570215 entropy -11.021568363334461
epoch: 33, step: 43
	action: tensor([[ 0.0053,  0.0162,  0.0149, -0.0030, -0.0222,  0.0086,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[7.0168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30667451550308644, distance: 0.9528516167002855 entropy -11.014790789862312
epoch: 33, step: 44
	action: tensor([[ 0.0051,  0.0160,  0.0042, -0.0028, -0.0157, -0.0107, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[7.0174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020888779921497, distance: 0.9559974920349676 entropy -11.019966934467643
epoch: 33, step: 45
	action: tensor([[ 0.0051,  0.0162,  0.0098, -0.0027, -0.0079, -0.0015, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[7.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043647453046042, distance: 0.954437479581737 entropy -11.019897297364556
epoch: 33, step: 46
	action: tensor([[ 0.0051,  0.0161,  0.0034, -0.0027, -0.0005, -0.0051, -0.0422]],
       dtype=torch.float64)
	q_value: tensor([[7.0141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30348654507025985, distance: 0.955039751273994 entropy -11.021748257148761
epoch: 33, step: 47
	action: tensor([[ 0.0049,  0.0163,  0.0088, -0.0024,  0.0009, -0.0189, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[7.0028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300333571305215, distance: 0.9571989451573 entropy -11.0298366824714
epoch: 33, step: 48
	action: tensor([[ 0.0052,  0.0161,  0.0040, -0.0027, -0.0117,  0.0288, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[7.0161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31131639631205965, distance: 0.9496565432743418 entropy -11.022643510040561
epoch: 33, step: 49
	action: tensor([[ 0.0049,  0.0162,  0.0092, -0.0024, -0.0124, -0.0100, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[7.0018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022987957905955, distance: 0.9558537086970936 entropy -11.0256710854226
epoch: 33, step: 50
	action: tensor([[ 0.0051,  0.0162, -0.0094, -0.0027,  0.0038, -0.0022,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[7.0123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30418802940379974, distance: 0.9545587022825117 entropy -11.02201764881683
epoch: 33, step: 51
	action: tensor([[ 0.0052,  0.0161, -0.0034, -0.0033, -0.0098,  0.0106,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[7.0141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068079600673518, distance: 0.9527599144691401 entropy -11.005568547498688
epoch: 33, step: 52
	action: tensor([[ 0.0050,  0.0162,  0.0146, -0.0027, -0.0127, -0.0194,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[7.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30017402066794496, distance: 0.9573080778738561 entropy -11.017136201856276
epoch: 33, step: 53
	action: tensor([[ 0.0053,  0.0160,  0.0122, -0.0030, -0.0066,  0.0085,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[7.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065752381912602, distance: 0.9529198336944043 entropy -11.016752033568062
epoch: 33, step: 54
	action: tensor([[ 0.0052,  0.0159, -0.0083, -0.0029, -0.0043, -0.0228, -0.0099]],
       dtype=torch.float64)
	q_value: tensor([[7.0209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993485436618759, distance: 0.9578725059865458 entropy -11.016785865045547
epoch: 33, step: 55
	action: tensor([[ 0.0052,  0.0164,  0.0093, -0.0028,  0.0019,  0.0212, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[7.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3097990189998071, distance: 0.9507021572736376 entropy -11.017372632298299
epoch: 33, step: 56
	action: tensor([[ 0.0050,  0.0161,  0.0116, -0.0025, -0.0105, -0.0116, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[7.0102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018704412194242, distance: 0.956147087494198 entropy -11.024930401384456
epoch: 33, step: 57
	action: tensor([[ 0.0052,  0.0161,  0.0077, -0.0028,  0.0013, -0.0114,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[7.0175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30202041333288376, distance: 0.9560443822731116 entropy -11.02028410175809
epoch: 33, step: 58
	action: tensor([[ 0.0053,  0.0161,  0.0110, -0.0029, -0.0225,  0.0121,  0.0263]],
       dtype=torch.float64)
	q_value: tensor([[7.0188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30738609363626246, distance: 0.9523625228701297 entropy -11.018378333584073
epoch: 33, step: 59
	action: tensor([[ 0.0051,  0.0160,  0.0049, -0.0030,  0.0068,  0.0157,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[7.0177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080609331995985, distance: 0.9518984486368864 entropy -11.01454329977966
epoch: 33, step: 60
	action: tensor([[ 0.0051,  0.0160,  0.0119, -0.0029, -0.0072,  0.0040, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[7.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053691644020756, distance: 0.9537481804677206 entropy -11.01775893098035
epoch: 33, step: 61
	action: tensor([[ 4.9931e-03,  1.6138e-02, -1.4498e-05, -2.4906e-03, -1.8332e-03,
          4.1060e-02,  2.8549e-02]], dtype=torch.float64)
	q_value: tensor([[7.0095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.314112804572372, distance: 0.947726536027993 entropy -11.02657996596784
epoch: 33, step: 62
	action: tensor([[ 0.0050,  0.0160,  0.0103, -0.0030, -0.0180,  0.0383, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[7.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31299823385816516, distance: 0.9484962540448284 entropy -11.0152130802151
epoch: 33, step: 63
	action: tensor([[ 0.0049,  0.0161,  0.0201, -0.0025, -0.0203,  0.0106,  0.0394]],
       dtype=torch.float64)
	q_value: tensor([[7.0046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068664095449771, distance: 0.9527197455909326 entropy -11.024299145787667
epoch: 33, step: 64
	action: tensor([[ 0.0052,  0.0158,  0.0257, -0.0032, -0.0151, -0.0021,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[7.0306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30363454269965995, distance: 0.9549382807803043 entropy -11.012361453377464
epoch: 33, step: 65
	action: tensor([[ 0.0052,  0.0158,  0.0094, -0.0029, -0.0002, -0.0180, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[7.0326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002515571353128, distance: 0.9572550444449733 entropy -11.022013603992713
epoch: 33, step: 66
	action: tensor([[ 0.0051,  0.0161,  0.0232, -0.0026, -0.0053, -0.0010,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[7.0148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044108185106337, distance: 0.9544058719809966 entropy -11.025952844403514
epoch: 33, step: 67
	action: tensor([[ 0.0053,  0.0158,  0.0136, -0.0030, -0.0103,  0.0045, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[7.0329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054610516118609, distance: 0.9536850964862364 entropy -11.021602460941176
epoch: 33, step: 68
	action: tensor([[ 0.0050,  0.0161,  0.0023, -0.0025,  0.0018,  0.0063, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[7.0102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30606068124685926, distance: 0.9532733259662405 entropy -11.02706548686692
epoch: 33, step: 69
	action: tensor([[ 0.0050,  0.0162,  0.0137, -0.0025,  0.0071, -0.0206, -0.0322]],
       dtype=torch.float64)
	q_value: tensor([[7.0040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29982302988727705, distance: 0.9575481119679867 entropy -11.024160456172845
epoch: 33, step: 70
	action: tensor([[ 0.0052,  0.0160,  0.0060, -0.0026, -0.0018,  0.0094,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[7.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068105891854662, distance: 0.9527581076677195 entropy -11.028244921300203
epoch: 33, step: 71
	action: tensor([[ 0.0051,  0.0160, -0.0044, -0.0029, -0.0129, -0.0013,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[7.0151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30420743196455036, distance: 0.9545453933620647 entropy -11.016485905726872
epoch: 33, step: 72
	action: tensor([[ 0.0051,  0.0163,  0.0065, -0.0028, -0.0022, -0.0200,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[7.0033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000966058771114, distance: 0.9573610251474685 entropy -11.01471637229108
epoch: 33, step: 73
	action: tensor([[ 0.0053,  0.0160,  0.0037, -0.0031, -0.0212,  0.0152, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[7.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080039644860759, distance: 0.9519376336724534 entropy -11.011268835781339
epoch: 33, step: 74
	action: tensor([[ 0.0050,  0.0163,  0.0065, -0.0026, -0.0116, -0.0137, -0.0150]],
       dtype=torch.float64)
	q_value: tensor([[7.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30154847573000565, distance: 0.9563675414743467 entropy -11.02147650838651
epoch: 33, step: 75
	action: tensor([[ 0.0051,  0.0162,  0.0087, -0.0026, -0.0029,  0.0233,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[7.0126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3100955377727257, distance: 0.9504979187051739 entropy -11.022327667058208
epoch: 33, step: 76
	action: tensor([[ 0.0051,  0.0159,  0.0126, -0.0031, -0.0177,  0.0058,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[7.0195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30557728957972696, distance: 0.9536052888264995 entropy -11.014311182857385
epoch: 33, step: 77
	action: tensor([[ 0.0052,  0.0160,  0.0213, -0.0030, -0.0020, -0.0096,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[7.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3021448175547541, distance: 0.9559591783073905 entropy -11.016944934191102
epoch: 33, step: 78
	action: tensor([[ 0.0054,  0.0157,  0.0106, -0.0032, -0.0061,  0.0016,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[7.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046795276595623, distance: 0.9542215085888711 entropy -11.015181222689577
epoch: 33, step: 79
	action: tensor([[ 0.0052,  0.0159, -0.0009, -0.0030, -0.0094, -0.0053,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[7.0234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032631892690403, distance: 0.9551928686106949 entropy -11.015664859981177
epoch: 33, step: 80
	action: tensor([[ 0.0052,  0.0161,  0.0139, -0.0030, -0.0009, -0.0069, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[7.0139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3029353311412758, distance: 0.955417581090472 entropy -11.01195090233112
epoch: 33, step: 81
	action: tensor([[ 0.0052,  0.0160,  0.0133, -0.0028, -0.0035,  0.0133, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[7.0219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077007953695331, distance: 0.9521461366654054 entropy -11.021385780162388
epoch: 33, step: 82
	action: tensor([[ 0.0050,  0.0160,  0.0068, -0.0026, -0.0059,  0.0180,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[7.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3086794323135459, distance: 0.9514729198589126 entropy -11.026439457250405
