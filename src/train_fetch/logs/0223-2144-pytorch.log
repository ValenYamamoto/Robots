epoch: 0, step: 0
	action: tensor([[-153.5916, -257.5337,   14.2033, -179.7429, -530.6224, -323.6447,
         -567.0418]], dtype=torch.float64)
	q_value: tensor([[-34.4814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6256918045047926, distance: 1.4590689028954205 entropy 7.240020140326997
epoch: 0, step: 1
	action: tensor([[ 124.8123, -742.9542, -547.6208,  671.1654,  206.4016, -828.5601,
          -49.2991]], dtype=torch.float64)
	q_value: tensor([[-39.4450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13032423891335443, distance: 1.0671743450729663 entropy 7.67414092720243
epoch: 0, step: 2
	action: tensor([[-444.6443,  124.5369,   30.8092,  447.2840,  130.7039,   57.0727,
          365.9330]], dtype=torch.float64)
	q_value: tensor([[-19.0183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3180659964494942, distance: 0.9449914223709033 entropy 7.17062571570941
epoch: 0, step: 3
	action: tensor([[-185.6830,  277.5695,  -77.1137, -297.5312,  192.5497,  187.3793,
          -19.5720]], dtype=torch.float64)
	q_value: tensor([[-23.6418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6704621615837072, distance: 1.4790232960494967 entropy 7.350226456068635
epoch: 0, step: 4
	action: tensor([[-330.5874,  -90.3420,  664.1204,  241.3662, -187.5444,  -40.0618,
          524.4607]], dtype=torch.float64)
	q_value: tensor([[-30.4887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1631593265262592, distance: 1.6830652274883064 entropy 7.214230068099655
epoch: 0, step: 5
	action: tensor([[-725.0561, -601.6793,  -31.0554, -265.1404,  -41.2425, -491.4884,
          353.0493]], dtype=torch.float64)
	q_value: tensor([[-25.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2184288134376247, distance: 1.7044310684032677 entropy 7.331545582436779
epoch: 0, step: 6
	action: tensor([[ -188.7541,  -628.7982,  -107.8278,   790.4149,   137.9012, -1055.4841,
            97.4534]], dtype=torch.float64)
	q_value: tensor([[-31.0866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9646979024029034, distance: 1.604000812759356 entropy 7.591950682787709
epoch: 0, step: 7
	action: tensor([[ -53.9232, -979.0396, -631.0617, -361.9473,  312.8572,  718.4054,
          312.8897]], dtype=torch.float64)
	q_value: tensor([[-28.0312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.489338076794005, distance: 1.3965398725436846 entropy 7.607188210522736
epoch: 0, step: 8
	action: tensor([[ -84.1001, -295.8651, -717.7595,  -73.9448,  -51.5195, -672.0875,
         -146.9658]], dtype=torch.float64)
	q_value: tensor([[-28.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0497761004578323, distance: 1.6383621486520739 entropy 7.510022443588838
epoch: 0, step: 9
	action: tensor([[ 177.3606, -321.8172,   45.9504,  815.9379,  -40.1483,   60.5190,
          107.8437]], dtype=torch.float64)
	q_value: tensor([[-26.9175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.423462394021876, distance: 1.36530503662724 entropy 7.428601409803286
epoch: 0, step: 10
	action: tensor([[ 134.2585,  223.3843,  434.7885, 1053.1499, -406.2738, -276.1169,
         -180.1623]], dtype=torch.float64)
	q_value: tensor([[-26.9215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32431774899343147, distance: 0.9406497593598498 entropy 7.642833640902258
epoch: 0, step: 11
	action: tensor([[-285.5792, -437.2804,  -40.5683,  481.8236, -638.7865, -362.6216,
         -458.7304]], dtype=torch.float64)
	q_value: tensor([[-31.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34176389429631604, distance: 0.9284265333270763 entropy 7.441843292984515
epoch: 0, step: 12
	action: tensor([[-413.3740,  180.7191, -537.9521,  757.9265,  180.4771,  263.1001,
           74.2632]], dtype=torch.float64)
	q_value: tensor([[-29.4870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030681680219825536, distance: 1.1266522897772844 entropy 7.571841529215562
epoch: 0, step: 13
	action: tensor([[ -84.1320, -476.0323, -265.4172, -428.3999,  871.7590,  147.4909,
           11.1143]], dtype=torch.float64)
	q_value: tensor([[-27.6052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5289344620647238, distance: 1.4149826936709355 entropy 7.500634730726405
epoch: 0, step: 14
	action: tensor([[ 270.5971, -517.5795, -634.8514, -128.6090,  321.5958,  -48.7477,
         -876.8440]], dtype=torch.float64)
	q_value: tensor([[-30.3614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14741450356492747, distance: 1.056636633359203 entropy 7.533843461468704
epoch: 0, step: 15
	action: tensor([[-508.1373,  195.3931,  -74.5470,  864.2346,   72.9640,    7.7259,
          816.7297]], dtype=torch.float64)
	q_value: tensor([[-25.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3596136213328478, distance: 1.3343336754395172 entropy 7.453700669939794
epoch: 0, step: 16
	action: tensor([[-868.5439, -276.5018,  -38.9769,  392.6171,  161.4325,  508.3501,
          547.1926]], dtype=torch.float64)
	q_value: tensor([[-23.5596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5145908981774676, distance: 1.408329785777298 entropy 7.3163743633546705
epoch: 0, step: 17
	action: tensor([[ -20.9104, -237.5447, -781.5475,  415.7519, -415.4487,   66.0093,
         -185.9183]], dtype=torch.float64)
	q_value: tensor([[-27.0170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09147448086414811, distance: 1.0907500925834015 entropy 7.514846225194256
epoch: 0, step: 18
	action: tensor([[-491.2046,  185.1305,  338.2969,  -83.4692,  338.5337,  772.3793,
          913.8877]], dtype=torch.float64)
	q_value: tensor([[-29.2290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.348366535206762, distance: 1.3288032325246213 entropy 7.566415437737879
epoch: 0, step: 19
	action: tensor([[-510.6357,  124.1862,  -42.6606,  823.9461, -209.3809,  153.0737,
          555.2811]], dtype=torch.float64)
	q_value: tensor([[-29.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5181922362426685, distance: 0.7943165112295778 entropy 7.3872053169788074
epoch: 0, step: 20
	action: tensor([[-338.6235, -523.5335,   97.0428, -181.5285, -379.7768, -322.5313,
          450.0601]], dtype=torch.float64)
	q_value: tensor([[-30.4665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9274755758343347, distance: 1.5887337995569104 entropy 7.518899698712046
epoch: 0, step: 21
	action: tensor([[-501.0252, -928.8940,  448.5764,  548.8985,  395.2048,  242.9092,
          331.6164]], dtype=torch.float64)
	q_value: tensor([[-31.8420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21475403386158343, distance: 1.0140504224448563 entropy 7.621280424915326
epoch: 0, step: 22
	action: tensor([[  42.3034, -252.3235,  130.6921,  404.7989,   -2.5925,  408.7919,
         -643.5456]], dtype=torch.float64)
	q_value: tensor([[-27.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.699063732707021, distance: 1.4916314382413645 entropy 7.52277991341529
epoch: 0, step: 23
	action: tensor([[-1102.0060,  -905.6753,  -314.0142,  -471.6980,   265.0955,   592.6270,
           288.0202]], dtype=torch.float64)
	q_value: tensor([[-29.9318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9368984423571305, distance: 1.5926124932209165 entropy 7.75453051868288
epoch: 0, step: 24
	action: tensor([[-530.1535, -606.3985, -476.9678, -358.9549, -583.5430, -339.3754,
         -776.0452]], dtype=torch.float64)
	q_value: tensor([[-21.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2119291983486502, distance: 1.2597817647552365 entropy 7.180531195649492
epoch: 0, step: 25
	action: tensor([[-306.4501, -886.6895,  326.8170, 1149.9965,   38.1957,  355.0938,
          162.5591]], dtype=torch.float64)
	q_value: tensor([[-29.8766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7847217618954772, distance: 1.5287693001254545 entropy 7.448083354867924
epoch: 0, step: 26
	action: tensor([[-766.7429, -539.1867, -244.6778, -480.9804, -142.4144,    0.9437,
           97.9815]], dtype=torch.float64)
	q_value: tensor([[-22.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26203986653009437, distance: 0.9830444228610964 entropy 7.221342221757943
epoch: 0, step: 27
	action: tensor([[-436.7662, -168.8495, -118.9270,   -9.5152, -370.7799, -189.7160,
         -331.9985]], dtype=torch.float64)
	q_value: tensor([[-27.3049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.440664006462206
epoch: 0, step: 28
	action: tensor([[-432.1959, -159.2543, -109.1196,  425.0251, -129.9485,   28.8815,
         -404.4349]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8208619395403369, distance: 1.544170325690833 entropy 6.9332735429202765
epoch: 0, step: 29
	action: tensor([[-354.9332,  -27.5227, 1331.4125,   33.8204, -113.3011,  321.8530,
          513.2525]], dtype=torch.float64)
	q_value: tensor([[-32.2741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18219259093860285, distance: 1.0348614606834465 entropy 7.56694761787933
epoch: 0, step: 30
	action: tensor([[-127.3061, -882.5579,  145.5018,  237.4492, -133.2254, -436.4725,
          721.1887]], dtype=torch.float64)
	q_value: tensor([[-25.6205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02821209393243007, distance: 1.1280865941982978 entropy 7.475897367084239
epoch: 0, step: 31
	action: tensor([[  37.1740,    9.3400,  261.0800,  420.3961,  -45.9007,   14.7518,
         -806.0653]], dtype=torch.float64)
	q_value: tensor([[-25.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.494637630420164
epoch: 0, step: 32
	action: tensor([[-253.5754, -344.9567, -192.5367,   21.6163, -149.9531,  204.0069,
          -94.6144]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7314287750567232, distance: 1.505771277411616 entropy 6.9332735429202765
epoch: 0, step: 33
	action: tensor([[-678.7331, -440.6042,  139.1574, -472.8149,  198.0726,   -2.3095,
          288.0766]], dtype=torch.float64)
	q_value: tensor([[-27.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.194589616694039, distance: 1.6952484396409964 entropy 7.448117107368113
epoch: 0, step: 34
	action: tensor([[-551.5915, -376.0896,  285.2969,  101.2897,  274.1159,  616.2480,
          217.6245]], dtype=torch.float64)
	q_value: tensor([[-29.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9739809734528015, distance: 1.9734484490883888 entropy 7.430382270008231
epoch: 0, step: 35
	action: tensor([[ 257.3274, -573.9436,  291.9282, -579.1309, -244.9099,  224.5683,
         -468.0378]], dtype=torch.float64)
	q_value: tensor([[-29.8550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06750615179866049, distance: 1.1050442801756823 entropy 7.514837814603534
epoch: 0, step: 36
	action: tensor([[  -33.1537,   -77.4798, -1038.1089,   634.9304,   -74.7979,  -225.2940,
           199.2643]], dtype=torch.float64)
	q_value: tensor([[-29.7954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7651845215099136, distance: 1.520378601628057 entropy 7.593820868364576
epoch: 0, step: 37
	action: tensor([[-442.6818, -301.7221,  140.1186,  153.0256,  227.7298,   32.8644,
         -101.2311]], dtype=torch.float64)
	q_value: tensor([[-20.3294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047304027147454186, distance: 1.116950322806633 entropy 7.1630249628912415
epoch: 0, step: 38
	action: tensor([[ 540.1608, -251.4016, -369.7992, -481.7671, -307.5855, -189.5630,
         -190.3536]], dtype=torch.float64)
	q_value: tensor([[-26.9097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37956813029110803, distance: 1.3440897519305144 entropy 7.482788375018563
epoch: 0, step: 39
	action: tensor([[-172.1900, -310.8436,  445.8701,  284.5126,    2.6021, -234.6200,
         -142.4058]], dtype=torch.float64)
	q_value: tensor([[-22.3554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10928095269889515, distance: 1.2052509186936882 entropy 7.126320686001756
epoch: 0, step: 40
	action: tensor([[-1527.9163, -1128.8370,   -82.6450,   427.6170,    43.2718,  -325.1289,
           -89.2931]], dtype=torch.float64)
	q_value: tensor([[-32.1050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8926015688919959, distance: 1.5742956344824215 entropy 7.653841325825879
epoch: 0, step: 41
	action: tensor([[ -530.9839,  -538.1464,  -178.5947,   164.4137, -1105.7282,  -640.3987,
           679.6005]], dtype=torch.float64)
	q_value: tensor([[-23.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6324224104240286, distance: 1.4620861643026994 entropy 7.452307986476062
epoch: 0, step: 42
	action: tensor([[-368.7277, -265.2168,  464.2018, 1061.9626, -652.8246, -126.0016,
          310.2816]], dtype=torch.float64)
	q_value: tensor([[-27.2750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9299894650388572, distance: 1.5897695064150608 entropy 7.540194137962237
epoch: 0, step: 43
	action: tensor([[-1119.5684,  -475.0120,  -177.2261,   205.4598,   286.9909,  -734.3333,
           356.5512]], dtype=torch.float64)
	q_value: tensor([[-26.6055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4332677749574543, distance: 1.3699993512644664 entropy 7.487771771030748
epoch: 0, step: 44
	action: tensor([[-654.3067, -654.0692, -355.4525,   43.6251,  564.6916, -505.6472,
          920.2658]], dtype=torch.float64)
	q_value: tensor([[-28.1093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7925075963406389, distance: 1.5321002931578085 entropy 7.678212708676035
epoch: 0, step: 45
	action: tensor([[-1487.6205,  -777.2664,   551.2749,   419.6164,   572.7688,  -247.4257,
            73.6196]], dtype=torch.float64)
	q_value: tensor([[-31.4901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3991742069503357, distance: 1.353606990488352 entropy 7.710701553044841
epoch: 0, step: 46
	action: tensor([[-1284.0615, -1126.4505,   245.7166,   -23.0439,   759.6467,  -324.3287,
          -244.5953]], dtype=torch.float64)
	q_value: tensor([[-26.3291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9265284180656943, distance: 1.58834340123318 entropy 7.553676310136233
epoch: 0, step: 47
	action: tensor([[-1068.2643,  -760.8283,    65.6301,   662.4813,    65.0973,   485.5830,
          -185.7590]], dtype=torch.float64)
	q_value: tensor([[-22.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04060182602461371, distance: 1.1208723010721011 entropy 7.368799769302489
epoch: 0, step: 48
	action: tensor([[-236.6368, -252.9745, -200.3613,  465.7329, -822.3543, -445.6617,
          393.5637]], dtype=torch.float64)
	q_value: tensor([[-27.5548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0009826878959656, distance: 1.618744697747463 entropy 7.451184556638858
epoch: 0, step: 49
	action: tensor([[-932.6818, -363.8381,  -25.1625, -453.5241, -266.7654,  109.6910,
         -109.2962]], dtype=torch.float64)
	q_value: tensor([[-28.1862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7238924160166522, distance: 1.5024906317721423 entropy 7.515355942578473
epoch: 0, step: 50
	action: tensor([[-308.2991,    7.5720, -552.9515, -426.8529, -195.5862,  873.6376,
           26.1329]], dtype=torch.float64)
	q_value: tensor([[-29.8700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8710220352422287, distance: 1.5652948068829216 entropy 7.582261130079101
epoch: 0, step: 51
	action: tensor([[-235.7577, -372.0134,  -89.2736,  466.9290,   33.9720, 1111.8731,
         -499.4037]], dtype=torch.float64)
	q_value: tensor([[-30.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.910004872246698, distance: 1.5815172411955716 entropy 7.400368141680308
epoch: 0, step: 52
	action: tensor([[-641.0437,    3.6521, -583.5158,  674.9654, -414.7100,  217.7984,
         -275.9866]], dtype=torch.float64)
	q_value: tensor([[-31.2936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.647120431933359
epoch: 0, step: 53
	action: tensor([[-562.1983,  368.3111,  -20.7659,  -80.5472,  180.2990,  423.3006,
         -716.6269]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.9332735429202765
epoch: 0, step: 54
	action: tensor([[  71.2264, -449.4028,  638.1952,  219.9974,  119.8190, -543.6288,
          352.2296]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05449026569282622, distance: 1.1127297396105837 entropy 6.9332735429202765
epoch: 0, step: 55
	action: tensor([[-485.6623, -334.9978, -161.9555,    4.2798, -449.9510,  683.1612,
          -74.4947]], dtype=torch.float64)
	q_value: tensor([[-21.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.286962665562517
epoch: 0, step: 56
	action: tensor([[-302.2256, -553.1143, -142.5676,   18.9950,  220.2693,  860.3744,
         -279.8822]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5650114782267233, distance: 1.431579454191439 entropy 6.9332735429202765
epoch: 0, step: 57
	action: tensor([[-656.2007, -430.8519, -638.1440,  477.6138, -125.8203,  425.5627,
          227.2690]], dtype=torch.float64)
	q_value: tensor([[-26.0513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11740907262861999, distance: 1.0750692070963397 entropy 7.431596680933417
epoch: 0, step: 58
	action: tensor([[ -62.1166, -957.5972,  216.0489,  774.7910,  161.0886,  273.6843,
          571.0903]], dtype=torch.float64)
	q_value: tensor([[-29.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16188227551129009, distance: 1.233496031471411 entropy 7.572105552952712
epoch: 0, step: 59
	action: tensor([[ -750.4001, -1141.6380,  -409.2416,   667.5293,   394.4087,   -40.0356,
             4.4628]], dtype=torch.float64)
	q_value: tensor([[-30.5064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3817904488185264, distance: 1.345171899864286 entropy 7.641774483061504
epoch: 0, step: 60
	action: tensor([[-563.7162,  -69.2477,  152.4786,  212.7027,  495.8780,  -90.3697,
          224.1826]], dtype=torch.float64)
	q_value: tensor([[-25.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4178922343768232, distance: 1.3626311263774051 entropy 7.5580477835141275
epoch: 0, step: 61
	action: tensor([[ -353.0102,  -345.6072, -1058.8735,  -173.7135,  -903.9569,  -156.6941,
          -350.0688]], dtype=torch.float64)
	q_value: tensor([[-29.3101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7590399332803257, distance: 1.5177300840184862 entropy 7.558977979260164
epoch: 0, step: 62
	action: tensor([[ -287.8663, -1015.7881,  -879.5119,  1311.5819,  -174.8847,    -6.5877,
          -644.5980]], dtype=torch.float64)
	q_value: tensor([[-29.8035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17165170449010536, distance: 1.2386709648405627 entropy 7.565088595627216
epoch: 0, step: 63
	action: tensor([[-222.8714, -235.8014, -267.1302,   74.9600, -616.0773,  798.4539,
          488.4852]], dtype=torch.float64)
	q_value: tensor([[-26.9630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14442660508393002, distance: 1.0584865133977155 entropy 7.465842565477193
epoch: 0, step: 64
	action: tensor([[-1226.0034,  -104.9748,   886.2337,  -206.7530,   575.2191,  -911.3276,
           336.6345]], dtype=torch.float64)
	q_value: tensor([[-30.6468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2202502366854555, distance: 1.264099162374844 entropy 7.635596316036772
epoch: 0, step: 65
	action: tensor([[-197.9240, -343.2174, -274.3056,  452.8285,  343.3804,  714.0177,
          -10.7632]], dtype=torch.float64)
	q_value: tensor([[-27.9234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27203053815902845, distance: 1.2906410143801774 entropy 7.637456464203167
epoch: 0, step: 66
	action: tensor([[-994.3970,  774.2202, -235.6503, -779.6363, -291.5456, -711.3296,
          549.3926]], dtype=torch.float64)
	q_value: tensor([[-30.2553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4757010726313562, distance: 1.3901315169720732 entropy 7.645587478427008
epoch: 0, step: 67
	action: tensor([[-531.6104, -108.2094,  152.3124, 1013.0362,  322.4739,   40.9602,
         -272.4717]], dtype=torch.float64)
	q_value: tensor([[-34.0029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3172813470384992, distance: 1.3133968318505 entropy 7.674085139622426
epoch: 0, step: 68
	action: tensor([[-345.1551, -640.6607, -134.9799, -238.9697,  -22.1883,  348.6945,
         -344.0486]], dtype=torch.float64)
	q_value: tensor([[-25.7817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8103275226124045, distance: 1.5396970238837244 entropy 7.374283130219737
epoch: 0, step: 69
	action: tensor([[  586.5719,  -114.0217, -1232.0921,    16.1124,  -118.5370,   -92.8906,
           151.9749]], dtype=torch.float64)
	q_value: tensor([[-31.6526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39458410366854657, distance: 0.8903968261334884 entropy 7.628045808736764
epoch: 0, step: 70
	action: tensor([[-724.0743,   26.9448,  156.1766,  352.2711, -283.3840,    5.4881,
          273.9517]], dtype=torch.float64)
	q_value: tensor([[-20.5480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.190295521098053
epoch: 0, step: 71
	action: tensor([[-204.3019, -269.8652,  -14.4870,  605.5463, -439.7922,    1.0682,
          292.3454]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5099295549236087, distance: 1.4061609600997753 entropy 6.9332735429202765
epoch: 0, step: 72
	action: tensor([[-757.2352,   -6.9829,  556.4584, -132.9096,  589.9631,  312.4464,
          187.9654]], dtype=torch.float64)
	q_value: tensor([[-25.7046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41165048880708865, distance: 1.3596285788303795 entropy 7.440180944109577
epoch: 0, step: 73
	action: tensor([[-250.2963, -272.7243,  620.1436,   12.3080,    3.5477, -249.5451,
         -522.7259]], dtype=torch.float64)
	q_value: tensor([[-29.2263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46534475711027934, distance: 1.3852450299919457 entropy 7.437686469306998
epoch: 0, step: 74
	action: tensor([[-969.7005, -356.2104,  476.4512, -823.1201, -303.5510, 1014.2950,
          666.0998]], dtype=torch.float64)
	q_value: tensor([[-26.0793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08674884909306713, distance: 1.093583143437688 entropy 7.57388974339612
epoch: 0, step: 75
	action: tensor([[-366.5934,  194.2158,  110.1008, -456.4945, -310.7874,  122.2466,
          282.7397]], dtype=torch.float64)
	q_value: tensor([[-26.8392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048290206503936894, distance: 1.1163720698348758 entropy 7.33879626652192
epoch: 0, step: 76
	action: tensor([[ 407.7422, -148.0080, -733.7920, -228.5846, -136.5394,  669.1596,
         -683.5527]], dtype=torch.float64)
	q_value: tensor([[-42.5828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03665027541680943, distance: 1.1651258209133972 entropy 7.607561443992185
epoch: 0, step: 77
	action: tensor([[-176.2824, -738.5720,  483.5465,  500.8538, -685.9216,  281.1187,
         -432.7808]], dtype=torch.float64)
	q_value: tensor([[-24.6912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6055661290527481, distance: 1.450009314787744 entropy 7.434027942019902
epoch: 0, step: 78
	action: tensor([[-752.8218, -524.2068, -358.1508,  478.9793, -292.4523, -216.4247,
          273.7581]], dtype=torch.float64)
	q_value: tensor([[-19.9100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006687963384584661, distance: 1.148164543390424 entropy 7.142568227055455
epoch: 0, step: 79
	action: tensor([[-985.1711,   16.9980, -653.5165,  257.3362, -360.0710,  629.6923,
         -343.0125]], dtype=torch.float64)
	q_value: tensor([[-33.8216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.770359733912028, distance: 1.5226057125705457 entropy 7.627216467973126
epoch: 0, step: 80
	action: tensor([[ -26.1621,  -81.1074, -134.7368,  136.1642,  -13.6094,  787.8795,
         -358.9643]], dtype=torch.float64)
	q_value: tensor([[-26.7573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15157208673840306, distance: 1.0540571715795777 entropy 7.498508872020019
epoch: 0, step: 81
	action: tensor([[-875.1180, -688.7594,  595.2920, -283.9531, 1026.0775,  585.4658,
           46.7310]], dtype=torch.float64)
	q_value: tensor([[-26.0875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9463887544651886, distance: 1.596509424610248 entropy 7.480970489607607
epoch: 0, step: 82
	action: tensor([[ 305.1048, -269.3951, -310.0156,  -14.7395, -301.7447,  336.1616,
         -244.0219]], dtype=torch.float64)
	q_value: tensor([[-29.2152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010729142982268569, distance: 1.1381887821789347 entropy 7.51536405304305
epoch: 0, step: 83
	action: tensor([[-272.3521, -180.8933, -338.6386, -217.6697,  559.3087, -151.7414,
         -418.1083]], dtype=torch.float64)
	q_value: tensor([[-21.1108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.470875029841509, distance: 0.8324071879812922 entropy 7.235296216086238
epoch: 0, step: 84
	action: tensor([[-427.3145, -233.4443, -590.2888, 1013.2898,  225.9533, -198.9609,
           58.9317]], dtype=torch.float64)
	q_value: tensor([[-25.9074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.69471460159088, distance: 1.48972113396009 entropy 7.4580766024818965
epoch: 0, step: 85
	action: tensor([[ -362.1399, -1066.0146,   456.4073,   -21.6121,   155.4785,  -414.4096,
          -221.2728]], dtype=torch.float64)
	q_value: tensor([[-24.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5652532651246998, distance: 1.4316900360600526 entropy 7.369871683818216
epoch: 0, step: 86
	action: tensor([[-598.2134,  650.0613,  301.3290, -272.3853,    1.8184, -297.8812,
         -282.8284]], dtype=torch.float64)
	q_value: tensor([[-30.2251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29329000293413343, distance: 1.3013815745184505 entropy 7.502297867898657
epoch: 0, step: 87
	action: tensor([[-290.6850, -229.6900,  161.8505,  202.5309, -502.3626, -415.2474,
         -978.8468]], dtype=torch.float64)
	q_value: tensor([[-34.7645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.603426428143697, distance: 1.4490427956987921 entropy 7.3911784026803105
epoch: 0, step: 88
	action: tensor([[-250.3521, -774.4684,  116.5567,  -50.8952,  149.7401, -593.8559,
          -68.9575]], dtype=torch.float64)
	q_value: tensor([[-23.2406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.782087463425835, distance: 1.527640630691139 entropy 7.166042086918135
epoch: 0, step: 89
	action: tensor([[-152.6223, -513.8316,  327.3757, -163.8845, -519.8496,  951.9026,
          -78.6355]], dtype=torch.float64)
	q_value: tensor([[-30.1001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7181182569296043, distance: 1.4999722330687322 entropy 7.50906459253513
epoch: 0, step: 90
	action: tensor([[ -46.1740, -169.7412, -200.3186,  161.5526, -244.0857,  415.4988,
           33.6195]], dtype=torch.float64)
	q_value: tensor([[-24.9109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8233152528506804, distance: 1.5452102338657239 entropy 7.429823923838696
epoch: 0, step: 91
	action: tensor([[ -385.3806, -1028.6476,  -608.1854,    49.9589,  -162.8650,  -542.4612,
          -402.8716]], dtype=torch.float64)
	q_value: tensor([[-23.1091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1305986744820187, distance: 1.6703501465583468 entropy 7.329547558160128
epoch: 0, step: 92
	action: tensor([[-437.6946,  112.8855, -438.6990,   72.4716, -232.7241,  331.6008,
          345.6265]], dtype=torch.float64)
	q_value: tensor([[-21.3094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4050069340709328, distance: 1.3564254419307307 entropy 7.172876739359318
epoch: 0, step: 93
	action: tensor([[-327.6264, -392.3272, -126.5839, -105.4718,  -27.8093, -394.8708,
           46.0663]], dtype=torch.float64)
	q_value: tensor([[-18.6530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0606473862582977, distance: 1.6427010492844065 entropy 6.877952472630894
epoch: 0, step: 94
	action: tensor([[-648.0347, -282.3851,    9.3346,  -17.2130,  -43.2620,   74.2985,
          442.0502]], dtype=torch.float64)
	q_value: tensor([[-24.3625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5730087698937323, distance: 1.4352325167755668 entropy 7.461010551010105
epoch: 0, step: 95
	action: tensor([[ -46.7634, -489.2407, -188.5262, 1007.6908,  374.1943,  115.4214,
         -253.2611]], dtype=torch.float64)
	q_value: tensor([[-26.5533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1123647286913543, distance: 1.663187249423048 entropy 7.321749344525343
epoch: 0, step: 96
	action: tensor([[ 118.3229,  370.6922, -184.3277,  -25.7644,  449.7869,  270.9868,
         -225.0170]], dtype=torch.float64)
	q_value: tensor([[-32.6134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.700605306178689
epoch: 0, step: 97
	action: tensor([[  66.2614, -705.0799,  133.0467,  122.4454, -246.2861,   22.7159,
         -413.5556]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.302907691458713, distance: 1.3062115426865233 entropy 6.9332735429202765
epoch: 0, step: 98
	action: tensor([[-1212.7027,  -106.0144,   628.9792,   873.4388,  -350.6858,   235.7104,
           471.7203]], dtype=torch.float64)
	q_value: tensor([[-24.7105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17290613228517548, distance: 1.040720462549432 entropy 7.4053959717448805
epoch: 0, step: 99
	action: tensor([[-453.7986, -243.1612,  243.7843,  110.4879, -507.8850,  500.7324,
          208.2981]], dtype=torch.float64)
	q_value: tensor([[-25.7630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10656404995641355, distance: 1.203774035392608 entropy 7.359905579551617
epoch: 0, step: 100
	action: tensor([[ -30.6633, -687.9680, -414.6590,  295.5311,  237.8251, -248.7222,
          892.5858]], dtype=torch.float64)
	q_value: tensor([[-27.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4170281863548422, distance: 1.3622158768520918 entropy 7.441091229355221
epoch: 0, step: 101
	action: tensor([[-413.0428,  -47.7842,  -68.5617, -518.5594,   89.4858, -473.8310,
          631.9930]], dtype=torch.float64)
	q_value: tensor([[-18.5593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9111100878816483, distance: 1.5819747439025797 entropy 7.228738269888838
epoch: 0, step: 102
	action: tensor([[-304.3511, -358.5102,  243.0596,  853.4519, -379.4844,  -16.4612,
           66.7811]], dtype=torch.float64)
	q_value: tensor([[-26.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.358334468420361
epoch: 0, step: 103
	action: tensor([[-115.8553,  137.1702,  214.0395,  -65.7860, -483.4426,  522.1382,
          331.9232]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.9332735429202765
epoch: 0, step: 104
	action: tensor([[ 152.8417, -307.6836,  183.5429,  114.0803, -484.3663,  -24.2595,
          112.4649]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06091203094197517, distance: 1.1089445518888845 entropy 6.9332735429202765
epoch: 0, step: 105
	action: tensor([[ 123.4775, -339.4037,   86.8455,  177.5328, -118.2071,  253.6510,
         -111.6838]], dtype=torch.float64)
	q_value: tensor([[-12.4102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5994023077566434, distance: 0.7242873745040297 entropy 6.565234686525348
epoch: 0, step: 106
	action: tensor([[-554.3168, -565.5332,  594.8315,  879.2201,  -94.8876,  406.4424,
          340.6364]], dtype=torch.float64)
	q_value: tensor([[-22.3331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2489022638426348, distance: 0.9917561899681775 entropy 7.287995976075858
epoch: 0, step: 107
	action: tensor([[-203.3532,  103.2102,  178.4098,  251.9919,  -11.7924,  215.1437,
         -224.2627]], dtype=torch.float64)
	q_value: tensor([[-25.2393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23111658503016863, distance: 1.2697150994555246 entropy 7.449387247635333
epoch: 0, step: 108
	action: tensor([[  61.4140,    4.3945, -723.5414,  -40.7442, -596.7691,  243.8427,
         -346.4426]], dtype=torch.float64)
	q_value: tensor([[-31.3710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.524161553270525
epoch: 0, step: 109
	action: tensor([[-707.1152, -348.2481,  -39.6772, -126.1562, -165.2245, -194.5665,
          301.3202]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6585377454938075, distance: 1.473734916436533 entropy 6.9332735429202765
epoch: 0, step: 110
	action: tensor([[-5.4565e+02, -5.6960e+02, -1.1432e+03,  5.4724e+02, -4.9827e+02,
         -7.8800e+02,  5.7362e-01]], dtype=torch.float64)
	q_value: tensor([[-31.2649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6341745764946327, distance: 1.4628706214039358 entropy 7.5155324405041
epoch: 0, step: 111
	action: tensor([[ 258.4961, -256.5894,  -64.9333,  203.7355, -441.4416,  289.6105,
         -212.2953]], dtype=torch.float64)
	q_value: tensor([[-23.0841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4205498340405891, distance: 1.3639075395743732 entropy 7.367779105179729
epoch: 0, step: 112
	action: tensor([[-502.0689, -274.4244, -341.0434, -550.9590, -482.8998,  472.4996,
         -235.6284]], dtype=torch.float64)
	q_value: tensor([[-21.8643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.995825886288235, distance: 1.6166574906870201 entropy 7.417072839587754
epoch: 0, step: 113
	action: tensor([[ -12.0044, -925.1620, -220.8800,    6.6024, -360.6303,  -66.9228,
           54.7047]], dtype=torch.float64)
	q_value: tensor([[-28.1655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29402482564530774, distance: 1.3017512321117126 entropy 7.5276217220245645
epoch: 0, step: 114
	action: tensor([[ -24.2063, -127.9176,   23.6820,  -65.6506,  328.5923,   -8.3785,
          168.4271]], dtype=torch.float64)
	q_value: tensor([[-22.7397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.434643767215946, distance: 1.3706568195712512 entropy 7.412831806200722
epoch: 0, step: 115
	action: tensor([[  32.2578,   94.8156,  230.0184,   61.9632, -114.1719,  136.2465,
           38.9503]], dtype=torch.float64)
	q_value: tensor([[-26.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.35133665008687
epoch: 0, step: 116
	action: tensor([[-252.4009, -277.4352,  444.2798,  359.6570, -124.3519, -151.7208,
          412.5278]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7769051593759078, distance: 1.5254178267586413 entropy 6.9332735429202765
epoch: 0, step: 117
	action: tensor([[ -665.7703,   324.5986,  -637.0398,   434.9276, -1133.9436,   -48.6459,
           336.1279]], dtype=torch.float64)
	q_value: tensor([[-24.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20326447544449422, distance: 1.0214421712136612 entropy 7.377648847316803
epoch: 0, step: 118
	action: tensor([[-311.4828, -120.2720, -625.5291,  248.0992, -634.3445,  431.5812,
         1068.0522]], dtype=torch.float64)
	q_value: tensor([[-37.7445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37724839072842586, distance: 1.342959235101063 entropy 7.82002961965147
epoch: 0, step: 119
	action: tensor([[-311.2358, -620.6421, -424.2730, -495.7908, -191.0504, -449.8661,
         -219.9899]], dtype=torch.float64)
	q_value: tensor([[-29.2295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1023399220732335, distance: 1.659236000112899 entropy 7.453449652677526
epoch: 0, step: 120
	action: tensor([[-595.7757, -968.6476, -228.8546, -210.3310,  -69.2734,  395.6952,
          431.1717]], dtype=torch.float64)
	q_value: tensor([[-29.5709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6705433748452314, distance: 1.4790592486285523 entropy 7.583180196354673
epoch: 0, step: 121
	action: tensor([[-666.1381, -247.2972, -296.5319,  298.7314,  454.5651, -326.8414,
         -530.7838]], dtype=torch.float64)
	q_value: tensor([[-31.5100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6831738108564513, distance: 1.4846400631657546 entropy 7.514022380856384
epoch: 0, step: 122
	action: tensor([[ -362.1591, -1255.5854,  -247.1648,   969.1321,   -65.4190,  -451.4879,
           364.6256]], dtype=torch.float64)
	q_value: tensor([[-29.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0895119383646712, distance: 1.194462968366525 entropy 7.521832101072397
epoch: 0, step: 123
	action: tensor([[-705.7609, -603.2937, -115.0849, 1010.5940,   22.5111,  914.2163,
          902.4724]], dtype=torch.float64)
	q_value: tensor([[-28.9837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5866300710897194, distance: 1.4414332433127286 entropy 7.701568393295626
epoch: 0, step: 124
	action: tensor([[ -92.7428,  179.3865,  472.1935,   21.8796, -139.9127,  557.8948,
          508.0733]], dtype=torch.float64)
	q_value: tensor([[-30.2608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18725704378596086, distance: 1.2468926665562825 entropy 7.738490041708649
epoch: 0, step: 125
	action: tensor([[-703.2838, -281.0201, -324.0678, -129.0136,   44.2718, 1309.0375,
          394.1040]], dtype=torch.float64)
	q_value: tensor([[-28.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01831044935443471, distance: 1.1338191226837533 entropy 7.442105001100913
epoch: 0, step: 126
	action: tensor([[-367.3245,  -68.7376,  -83.8416,  345.6143, -577.5392,  525.0123,
          182.6358]], dtype=torch.float64)
	q_value: tensor([[-22.1838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04699786756610114, distance: 1.117129780673466 entropy 7.234434031960862
epoch: 0, step: 127
	action: tensor([[-432.5206,    4.6351, -287.9325,  100.3050, -808.8695,  145.9290,
         -325.7075]], dtype=torch.float64)
	q_value: tensor([[-25.7362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.342143689447085
LOSS epoch 0 actor 328.856733809792 critic 283.05093764398725
epoch: 1, step: 0
	action: tensor([[ -87.0107, -750.8503,  160.9164,    7.7605,  -35.2978,  366.9097,
         -125.6241]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.124789741003837, distance: 1.2136469437060298 entropy 7.079778694432089
epoch: 1, step: 1
	action: tensor([[-778.7133, -500.6202,  299.5372,  254.0042, -103.1349,  463.3720,
         1505.5379]], dtype=torch.float64)
	q_value: tensor([[-40.7571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15325155876584806, distance: 1.05301339612303 entropy 7.626992811375351
epoch: 1, step: 2
	action: tensor([[-621.6670,  605.1942,  657.9471, -249.4269,  496.5478,  901.9051,
         -871.7429]], dtype=torch.float64)
	q_value: tensor([[-40.1655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7530244228927047, distance: 0.5687006147720839 entropy 7.8389721785397155
epoch: 1, step: 3
	action: tensor([[-475.9303, -521.4569,   57.0099,  557.9061,  463.3537,   -2.7437,
         -558.0452]], dtype=torch.float64)
	q_value: tensor([[-38.5692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.462400445686101
epoch: 1, step: 4
	action: tensor([[-666.7517, -241.4063,  -54.2390,  -63.3158,  156.2859,  331.5687,
          477.0989]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7510210203633068, distance: 1.5142667040013231 entropy 7.079778694432089
epoch: 1, step: 5
	action: tensor([[ 2.2185e-02,  2.2872e+02,  2.8450e+02, -4.8532e+02,  3.0937e+02,
         -4.5405e+02, -1.0347e+03]], dtype=torch.float64)
	q_value: tensor([[-38.1286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32984457429755176, distance: 0.9367947804434921 entropy 7.488993406810382
epoch: 1, step: 6
	action: tensor([[ 149.8426, -527.1023,  300.7458,  -86.0627,  151.3849, -146.7609,
         -242.3513]], dtype=torch.float64)
	q_value: tensor([[-36.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23407936341891833, distance: 1.0014945236046193 entropy 7.501827374832972
epoch: 1, step: 7
	action: tensor([[ 8.8634e+01, -1.6432e+02,  2.3706e+02,  2.9595e-01, -2.7566e+02,
         -2.6724e+02,  6.2999e+02]], dtype=torch.float64)
	q_value: tensor([[-31.4000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11542589005956372, distance: 1.2085845930513734 entropy 7.553359206087292
epoch: 1, step: 8
	action: tensor([[-633.3049,  335.4709, -138.8049,  849.3495,  667.4703,  866.6599,
         -549.8890]], dtype=torch.float64)
	q_value: tensor([[-29.3434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.3908859207950695
epoch: 1, step: 9
	action: tensor([[-254.7991, -435.9233, -216.8356,  104.3188,   34.4460, -799.1284,
           -9.9696]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0023984523404173563, distance: 1.1429711025690192 entropy 7.079778694432089
epoch: 1, step: 10
	action: tensor([[-271.8874,  -26.5683, -766.0509,  571.2254, -584.1318,   55.4115,
         -236.2885]], dtype=torch.float64)
	q_value: tensor([[-40.5514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5374712256125771, distance: 0.7782624397582077 entropy 7.695525837103856
epoch: 1, step: 11
	action: tensor([[  16.2359, -682.9820,   46.8196,  847.0641,  428.3909, -863.0432,
          346.4413]], dtype=torch.float64)
	q_value: tensor([[-34.7605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21827338730843682, distance: 1.011775460396749 entropy 7.651463550409593
epoch: 1, step: 12
	action: tensor([[-1319.2151,  -704.1140,   802.0056,   272.0316,  -316.9585,  -206.1007,
           331.2827]], dtype=torch.float64)
	q_value: tensor([[-40.6981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0029760638231324243, distance: 1.1460458097236752 entropy 7.64418323100072
epoch: 1, step: 13
	action: tensor([[   48.3949,   -33.1096,   577.4821,  -234.0680,    45.5496,   851.9257,
         -1454.6689]], dtype=torch.float64)
	q_value: tensor([[-39.4563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5391312424152561, distance: 1.4196932590005853 entropy 7.839686029549491
epoch: 1, step: 14
	action: tensor([[ 233.5359,  545.3439, -630.3593, -671.7889,  -26.4958, -585.7899,
          478.2905]], dtype=torch.float64)
	q_value: tensor([[-34.3681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.550339431391009
epoch: 1, step: 15
	action: tensor([[-401.5189,   99.1047,  -35.4642, -354.4975,   45.1348, -510.7214,
          262.0686]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.079778694432089
epoch: 1, step: 16
	action: tensor([[-329.4197, -187.0388,  -71.5431, -136.3605,  141.4257, -229.4437,
         -386.2617]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4517881775976691, distance: 1.3788223707146567 entropy 7.079778694432089
epoch: 1, step: 17
	action: tensor([[  -90.3035,  -893.8121, -1072.3839,   174.3063,   490.5138,   204.5945,
          1122.3141]], dtype=torch.float64)
	q_value: tensor([[-47.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.345760718830327, distance: 1.7526635227393674 entropy 7.758649545388473
epoch: 1, step: 18
	action: tensor([[-660.3464, -443.0404, -585.3795,  209.2277,  300.3456,  165.4111,
          339.5857]], dtype=torch.float64)
	q_value: tensor([[-35.7634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22096307780483082, distance: 1.2644683367755885 entropy 7.539236174129869
epoch: 1, step: 19
	action: tensor([[-694.4424, -396.7084, -257.7758,  656.1573,  183.7970,  131.7899,
          566.5806]], dtype=torch.float64)
	q_value: tensor([[-35.5448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18394216379199846, distance: 1.245150757040903 entropy 7.586098993304565
epoch: 1, step: 20
	action: tensor([[ -793.9465,  -367.5764,  -638.4793,   662.6373,   179.5182, -1148.2145,
          -319.0160]], dtype=torch.float64)
	q_value: tensor([[-36.4229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20656233437913907, distance: 1.0193259951866087 entropy 7.518588311947838
epoch: 1, step: 21
	action: tensor([[-624.4484, -552.9036, 1558.1824,  146.5972, -398.7947,   84.7795,
          325.2000]], dtype=torch.float64)
	q_value: tensor([[-35.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8501559500099403, distance: 1.5565420642634167 entropy 7.788380013784566
epoch: 1, step: 22
	action: tensor([[ -744.8002, -1191.0141,    43.0463,    89.2666,  -450.9072,  -652.1400,
           757.4567]], dtype=torch.float64)
	q_value: tensor([[-34.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47180667742941407, distance: 1.3882960172045025 entropy 7.700430024760436
epoch: 1, step: 23
	action: tensor([[-1183.4455, -1133.7254,  -338.7899,   -29.5822,   633.3944,  -188.7761,
           636.7263]], dtype=torch.float64)
	q_value: tensor([[-38.9777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4613006696413604, distance: 1.3833321959826401 entropy 7.689740563083568
epoch: 1, step: 24
	action: tensor([[-1040.9101,   136.1264,  -165.0458,     2.0508,   144.3512,   923.3158,
           236.3046]], dtype=torch.float64)
	q_value: tensor([[-31.7797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010389743779374516, distance: 1.1502736144602264 entropy 7.5827066405024235
epoch: 1, step: 25
	action: tensor([[-912.3792,   36.8347, -655.2180,  684.7710, -176.6213,  438.3490,
          612.1384]], dtype=torch.float64)
	q_value: tensor([[-39.5726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10341485842954112, distance: 1.2020598933162379 entropy 7.631423046507339
epoch: 1, step: 26
	action: tensor([[-897.8891,  257.1503, -874.8791,  550.2541, -252.4206, -540.2068,
         -324.6098]], dtype=torch.float64)
	q_value: tensor([[-43.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15070391818364515, distance: 1.0545963248175128 entropy 7.692694451850128
epoch: 1, step: 27
	action: tensor([[-615.0582,  365.6460, -135.9016,   84.7976, -448.8343,  948.4857,
         -348.2820]], dtype=torch.float64)
	q_value: tensor([[-36.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2977477817612413, distance: 1.3036224797996674 entropy 7.600390658571291
epoch: 1, step: 28
	action: tensor([[-755.2081, -648.2169,  297.3905,  722.8607, -417.0092,  187.7432,
         -207.5089]], dtype=torch.float64)
	q_value: tensor([[-48.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.365272269126661, distance: 1.3371075092203306 entropy 7.937048765009502
epoch: 1, step: 29
	action: tensor([[-1439.8319,  -586.6490,  -398.7761,   119.3288,   263.4094,   286.5491,
           878.8157]], dtype=torch.float64)
	q_value: tensor([[-39.2194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15202598734647843, distance: 1.0537751787369507 entropy 7.72983073371158
epoch: 1, step: 30
	action: tensor([[ -26.5659, -178.6241, -279.5049, -121.4925, -148.5763,  908.9168,
         -606.4407]], dtype=torch.float64)
	q_value: tensor([[-40.4542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7585814559821042, distance: 1.5175322800566624 entropy 7.76109908167305
epoch: 1, step: 31
	action: tensor([[-228.0039, -422.3725,  505.7114, 1462.8075, -139.0212, -148.9699,
          432.3014]], dtype=torch.float64)
	q_value: tensor([[-43.0165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03529548828245099, distance: 1.1643642267783445 entropy 7.762023478268493
epoch: 1, step: 32
	action: tensor([[ 271.1276, -685.9892,  776.6134, -472.7691,  220.8469, -151.7488,
         1078.9598]], dtype=torch.float64)
	q_value: tensor([[-38.9944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20594862423264804, distance: 1.019720134394193 entropy 7.692459732534379
epoch: 1, step: 33
	action: tensor([[-112.0765, -166.0278, -200.6950,  624.0006,   56.6915, -229.5853,
          184.8587]], dtype=torch.float64)
	q_value: tensor([[-32.6198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40526031893981496, distance: 1.3565477481556478 entropy 7.483214688381078
epoch: 1, step: 34
	action: tensor([[ -493.2719, -1114.7643,   270.5297,   726.6731,    31.6808,    23.2281,
          -825.1441]], dtype=torch.float64)
	q_value: tensor([[-31.9877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2884905167912464, distance: 0.9652660679964817 entropy 7.519831826499719
epoch: 1, step: 35
	action: tensor([[-311.9935, -101.9057, -412.8202, -311.4649,  -41.8246, -312.2101,
          331.2101]], dtype=torch.float64)
	q_value: tensor([[-35.3165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.095282491766174, distance: 1.6564486805562992 entropy 7.613941005474343
epoch: 1, step: 36
	action: tensor([[-402.0876,   55.6630, -693.5395,  614.8879, -192.0840,  -59.7561,
         -317.1545]], dtype=torch.float64)
	q_value: tensor([[-35.4030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3094584178750389, distance: 0.9509367051041239 entropy 7.517514100474761
epoch: 1, step: 37
	action: tensor([[-1348.1807, -1129.3280, -1197.3333,  -786.4896,   658.8552, -1021.5932,
          -722.2989]], dtype=torch.float64)
	q_value: tensor([[-50.3310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9428445319404686, distance: 1.595055202562332 entropy 7.854114717769016
epoch: 1, step: 38
	action: tensor([[-296.8222,   39.7328,  520.0626, 1168.8613,  571.9801,  324.6423,
         -900.6634]], dtype=torch.float64)
	q_value: tensor([[-36.2932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1477881493445039, distance: 1.2259917888793133 entropy 7.716270593022622
epoch: 1, step: 39
	action: tensor([[-1775.6894,   188.1457,  -426.2649,   422.8984,  -143.5012,   659.2195,
            31.8790]], dtype=torch.float64)
	q_value: tensor([[-39.4266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.459948366674904, distance: 0.8409580558889629 entropy 7.65799892533341
epoch: 1, step: 40
	action: tensor([[-533.8559,  507.7729, 1039.8606,  864.2133, -383.7045, -143.7948,
          138.1888]], dtype=torch.float64)
	q_value: tensor([[-38.4759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.50351782197619
epoch: 1, step: 41
	action: tensor([[ -64.8351,  -54.7654,  -36.7011,   79.1089,  106.5131, -174.4908,
          270.0238]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.079778694432089
epoch: 1, step: 42
	action: tensor([[-820.0569, -288.6227,  359.5777, -214.2578, -106.7580,  468.8642,
          133.6088]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7250124938906701, distance: 1.5029786649068546 entropy 7.079778694432089
epoch: 1, step: 43
	action: tensor([[-120.1804, -498.0482,  369.0824,  -94.0531,  614.1445,   20.7577,
          113.7828]], dtype=torch.float64)
	q_value: tensor([[-30.3084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3716782448679665, distance: 0.9070844914330203 entropy 7.3167130416635
epoch: 1, step: 44
	action: tensor([[-954.9490,  250.1736, -908.7670,  101.7399,  -46.8190,  -23.6746,
         -169.1122]], dtype=torch.float64)
	q_value: tensor([[-33.5107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6470049474997108, distance: 1.4686021009099572 entropy 7.482628817913449
epoch: 1, step: 45
	action: tensor([[-1048.3128, -1150.0735,  -763.2239,   273.2420,   549.8539,    78.0157,
          -626.9552]], dtype=torch.float64)
	q_value: tensor([[-42.2511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7576061917463488, distance: 1.517111429432129 entropy 7.612240305773826
epoch: 1, step: 46
	action: tensor([[-527.4325, -686.5058, -607.6411, -728.3993,  517.3144, -932.5088,
          599.0053]], dtype=torch.float64)
	q_value: tensor([[-45.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5481319168086451, distance: 1.8267017595638482 entropy 7.8864337511055185
epoch: 1, step: 47
	action: tensor([[ -63.3877,  -93.2191, -846.9150, -208.8272, -845.2072,  216.9922,
         -315.3138]], dtype=torch.float64)
	q_value: tensor([[-33.8590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10925350617028151, distance: 1.20523600806245 entropy 7.5936084715756005
epoch: 1, step: 48
	action: tensor([[-1157.4538,  -479.7092,  -288.2920,   883.3219,  -343.9698, -1124.8315,
          -630.9587]], dtype=torch.float64)
	q_value: tensor([[-39.2969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9124901303651805, distance: 1.5825458251765485 entropy 7.668713630005639
epoch: 1, step: 49
	action: tensor([[ -752.9052, -1077.2879,  -355.3479,   421.6016,   531.0334,  -674.6444,
          -155.2687]], dtype=torch.float64)
	q_value: tensor([[-39.5425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1102755405199628, distance: 1.0794050814772733 entropy 7.607858301301391
epoch: 1, step: 50
	action: tensor([[ -380.5091,   483.7849,   171.9915,    81.9272, -1001.9712,  -797.7126,
           275.0026]], dtype=torch.float64)
	q_value: tensor([[-39.9544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03582033829048259, distance: 1.123661954131413 entropy 7.870620138062925
epoch: 1, step: 51
	action: tensor([[ 327.9708,  -75.7298,  103.1285, -701.3602, -143.1847, -825.1960,
         1484.5905]], dtype=torch.float64)
	q_value: tensor([[-40.8866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.782885702623604
epoch: 1, step: 52
	action: tensor([[ -35.3425, -305.9930,  152.2524,   12.5759,  184.5151, -153.1510,
         -119.2840]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38431363886727543, distance: 1.3463995014607486 entropy 7.079778694432089
epoch: 1, step: 53
	action: tensor([[ 205.5202, -299.3122, -558.4609,  826.3746,   33.9840, -346.2733,
         -221.4791]], dtype=torch.float64)
	q_value: tensor([[-33.9338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3424951416982608, distance: 0.9279106863970903 entropy 7.473456597069823
epoch: 1, step: 54
	action: tensor([[ 151.3191,  110.5585,   32.6514,  426.9163, -670.3689,  242.9805,
          761.6207]], dtype=torch.float64)
	q_value: tensor([[-40.1873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11524578633588312, distance: 1.0763859321228626 entropy 7.584514080132925
epoch: 1, step: 55
	action: tensor([[-706.4713, -361.4255, -617.9621,  356.4562,   61.2120,  436.2771,
         -524.4956]], dtype=torch.float64)
	q_value: tensor([[-34.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2951622364805139, distance: 0.9607298301262545 entropy 7.440550806517117
epoch: 1, step: 56
	action: tensor([[ -723.9218, -1524.2537,  -754.3986,   167.5322,  -823.3217,   788.8968,
           336.8962]], dtype=torch.float64)
	q_value: tensor([[-43.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23592182817423302, distance: 1.0002892228823161 entropy 7.840762129395634
epoch: 1, step: 57
	action: tensor([[-448.1294, -299.9576, -760.8227,  650.1293,  233.2972,  161.7126,
         -133.5282]], dtype=torch.float64)
	q_value: tensor([[-38.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07902465380468882, distance: 1.0981981191743828 entropy 7.74088269288559
epoch: 1, step: 58
	action: tensor([[-315.2928, -215.0513,  120.0460, -671.0822, -146.7453,  132.1271,
          291.0342]], dtype=torch.float64)
	q_value: tensor([[-37.1303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.019155575381001, distance: 1.626078787957992 entropy 7.616046734439773
epoch: 1, step: 59
	action: tensor([[ -631.9647, -1368.3533,   468.9886,   493.4239,   298.8180,   316.6980,
          -246.6640]], dtype=torch.float64)
	q_value: tensor([[-32.1875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7932893314659948, distance: 1.5324343408525012 entropy 7.556444580621189
epoch: 1, step: 60
	action: tensor([[-1116.7801, -1256.3273,   582.4026,   182.3862,   -33.5932,   202.2255,
          -394.8146]], dtype=torch.float64)
	q_value: tensor([[-40.2972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4970319999657524, distance: 1.4001424894970802 entropy 7.857152561803283
epoch: 1, step: 61
	action: tensor([[-609.7787, -524.3106,  -98.4367, -135.7008,  791.4866,  430.5628,
          212.3284]], dtype=torch.float64)
	q_value: tensor([[-37.5949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26860615502741036, distance: 0.9786611397411643 entropy 7.716546523595887
epoch: 1, step: 62
	action: tensor([[-843.5299, -545.4031,  346.5308,  618.5098,  267.2517, -520.8077,
         -119.5555]], dtype=torch.float64)
	q_value: tensor([[-35.6226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3740810202524413, distance: 0.90534843188375 entropy 7.578919183513074
epoch: 1, step: 63
	action: tensor([[ 192.2198, -717.2088,  362.5128,  -49.5293, -735.7535,  519.7534,
         -817.0418]], dtype=torch.float64)
	q_value: tensor([[-33.5921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3379085257686165, distance: 0.9311415169141991 entropy 7.7221376476642964
epoch: 1, step: 64
	action: tensor([[-536.5175, -197.6906, -130.3833,  196.5679,  -10.2518,  -80.5835,
           42.7243]], dtype=torch.float64)
	q_value: tensor([[-25.5218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1450435704790889, distance: 1.0581048001940065 entropy 7.319506821378624
epoch: 1, step: 65
	action: tensor([[-373.0426,   31.5696,   45.0770,  199.7355,  700.5450, -621.1090,
          623.7591]], dtype=torch.float64)
	q_value: tensor([[-33.0349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.518393459481669
epoch: 1, step: 66
	action: tensor([[-200.5321,  106.0592,  -62.6784,  306.6753,  -22.0790, -143.3318,
         -245.6454]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13507210820508564, distance: 1.219181655146594 entropy 7.079778694432089
epoch: 1, step: 67
	action: tensor([[ 247.4324, -157.0613, -436.8638,  830.5389,  550.6043,  438.1274,
          144.7215]], dtype=torch.float64)
	q_value: tensor([[-37.4665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7359967492801746, distance: 0.5879783436606063 entropy 7.516725541056504
epoch: 1, step: 68
	action: tensor([[ -173.0732,  -629.5020,    10.8001,  1035.5178, -1005.9625,   250.0893,
           833.6940]], dtype=torch.float64)
	q_value: tensor([[-30.8040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21835716214762635, distance: 1.0117212447693622 entropy 7.558940522642083
epoch: 1, step: 69
	action: tensor([[-1262.2406,  -418.3516,   891.0988,   151.1450,   160.1677,   383.5933,
           584.1296]], dtype=torch.float64)
	q_value: tensor([[-41.5412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0055668146055052, distance: 1.6205978586262484 entropy 7.821407032369936
epoch: 1, step: 70
	action: tensor([[-1122.8735,  -927.0835,  -371.4082,   168.1906,   413.2125,  -431.0831,
           780.5449]], dtype=torch.float64)
	q_value: tensor([[-38.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4203365852326586, distance: 1.3638051628163592 entropy 7.799886870485979
epoch: 1, step: 71
	action: tensor([[-364.5976, -299.2740,    1.2815,  195.2523,  363.4970,  154.8677,
          928.0064]], dtype=torch.float64)
	q_value: tensor([[-31.1690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11128971530233311, distance: 1.078789712563493 entropy 7.599287974586533
epoch: 1, step: 72
	action: tensor([[ 225.1567, -607.8530,  654.7822,  148.3802,  -92.7793,  560.4138,
          672.9741]], dtype=torch.float64)
	q_value: tensor([[-30.5697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18306550581962622, distance: 1.244689681529133 entropy 7.466971691584884
epoch: 1, step: 73
	action: tensor([[-332.5362,  225.5324,  749.0720,  187.2323,   52.0160,  210.8243,
          165.9362]], dtype=torch.float64)
	q_value: tensor([[-34.7605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3427102581713807, distance: 0.9277588812804178 entropy 7.490513551119207
epoch: 1, step: 74
	action: tensor([[  270.3862, -1023.4649,   490.4460,   695.3923,  -261.8509,   758.7645,
          -297.4107]], dtype=torch.float64)
	q_value: tensor([[-39.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.126885211803332, distance: 1.0692822688765964 entropy 7.538851279833369
epoch: 1, step: 75
	action: tensor([[-1060.9228,  -417.4492,   872.3908,   167.3351,  -338.6925,   259.1469,
         -1017.4845]], dtype=torch.float64)
	q_value: tensor([[-37.9592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2764358694027378, distance: 0.9734086651245989 entropy 7.688197397071439
epoch: 1, step: 76
	action: tensor([[-178.1292, -462.6038, -458.3406,  301.2186, -436.6604, -959.2185,
          -17.1278]], dtype=torch.float64)
	q_value: tensor([[-33.4598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06987272350603413, distance: 1.103641146043786 entropy 7.525627086418852
epoch: 1, step: 77
	action: tensor([[-867.4362, -860.1818,   34.1885,   15.0764,  609.7553,  740.1672,
          706.9634]], dtype=torch.float64)
	q_value: tensor([[-42.3542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20020886408061833, distance: 1.0233989959207683 entropy 7.716853008296425
epoch: 1, step: 78
	action: tensor([[-201.4439,   55.1785,  199.0554,  396.4526, -207.5521,  641.6082,
          566.5814]], dtype=torch.float64)
	q_value: tensor([[-39.9569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21932385398558352, distance: 1.011095431290237 entropy 7.73394331647611
epoch: 1, step: 79
	action: tensor([[-216.9454, -229.2290, -634.7054,  123.2778, -719.5456,   78.9653,
          -88.2499]], dtype=torch.float64)
	q_value: tensor([[-40.3990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7521095655920356, distance: 1.5147373127709574 entropy 7.700046978945556
epoch: 1, step: 80
	action: tensor([[1058.4186, -608.8324, -135.3711,   83.6495, -541.4391,  235.4862,
          209.3748]], dtype=torch.float64)
	q_value: tensor([[-38.4042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05872854866592503, distance: 1.1102330120787847 entropy 7.764807333721448
epoch: 1, step: 81
	action: tensor([[-994.6092,   39.5166,   29.1378,  694.5172,  165.8854,  438.8295,
            3.5942]], dtype=torch.float64)
	q_value: tensor([[-32.1520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6171517350200393, distance: 1.4552314727886109 entropy 7.639747641359664
epoch: 1, step: 82
	action: tensor([[-1211.2550,    -5.8900,  -652.5629,   514.2734,  -417.1743, -1328.1494,
          -774.5131]], dtype=torch.float64)
	q_value: tensor([[-49.6304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3511192973709534, distance: 0.9218051339985082 entropy 7.838136589056739
epoch: 1, step: 83
	action: tensor([[-505.3441, -298.7817,  366.5766, -417.7668,  168.3975,  110.5378,
           -8.2466]], dtype=torch.float64)
	q_value: tensor([[-37.7930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.701910588103066, distance: 1.8810146581109957 entropy 7.74364374627963
epoch: 1, step: 84
	action: tensor([[-803.7392,  230.8277, -151.8013,  346.0560,  825.1715, -250.5473,
         -445.1517]], dtype=torch.float64)
	q_value: tensor([[-31.7688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40952015876875847, distance: 1.3586022797513762 entropy 7.465897673505846
epoch: 1, step: 85
	action: tensor([[  8.5721, 422.0887, 179.0601, 368.4001, 111.1659, -11.4467,  52.1984]],
       dtype=torch.float64)
	q_value: tensor([[-41.6101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.633222683144383
epoch: 1, step: 86
	action: tensor([[-262.4638,  -94.3486,   76.7123, -122.7055,   32.9564,   35.1317,
           79.3128]], dtype=torch.float64)
	q_value: tensor([[-36.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1744896505384388, distance: 1.2401701967681853 entropy 7.079778694432089
epoch: 1, step: 87
	action: tensor([[-890.5963, -349.7985,  159.5665, -690.8008, -278.9231,  188.9206,
          273.8441]], dtype=torch.float64)
	q_value: tensor([[-38.0621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02098586822374182, distance: 1.1562894382336932 entropy 7.445644956908105
epoch: 1, step: 88
	action: tensor([[-279.3223, -369.6532,  353.1365, -291.9688, -219.8853,   -6.6063,
          503.7726]], dtype=torch.float64)
	q_value: tensor([[-45.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43608235349413005, distance: 1.3713438591272602 entropy 7.8267583514062835
epoch: 1, step: 89
	action: tensor([[-174.7126, -425.0529,  800.6511, -616.0758,  719.4750, -456.4303,
           86.5000]], dtype=torch.float64)
	q_value: tensor([[-27.7854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8219068408059031, distance: 1.5446133230651717 entropy 7.410031174934773
epoch: 1, step: 90
	action: tensor([[ -754.0585, -1044.9195,  -440.8817,  1400.9085,  -273.0149,  1078.2850,
           511.6789]], dtype=torch.float64)
	q_value: tensor([[-42.4629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5725231664625225, distance: 1.4350109644086895 entropy 7.743015090314545
epoch: 1, step: 91
	action: tensor([[-909.4894, 1117.7358,  214.4053, -164.1247, -888.0169,  565.0599,
          -89.2765]], dtype=torch.float64)
	q_value: tensor([[-40.8837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33289976689747225, distance: 1.3211600697856662 entropy 7.8412923754096875
epoch: 1, step: 92
	action: tensor([[-349.7776, -423.1323,  391.5760, -736.3701, -616.6509, 1351.3528,
          264.2701]], dtype=torch.float64)
	q_value: tensor([[-41.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8754538292990723, distance: 1.5671475272364097 entropy 7.448342982623473
epoch: 1, step: 93
	action: tensor([[  62.0006, -266.9820, -420.0704,  167.7476, -373.3622,  -71.5779,
         -237.7258]], dtype=torch.float64)
	q_value: tensor([[-32.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.14881915742244, distance: 1.6774772033172756 entropy 7.417530143866493
epoch: 1, step: 94
	action: tensor([[-147.2851, -318.2586,  139.1110,  -52.0049,   74.8399, -187.3360,
         -292.5229]], dtype=torch.float64)
	q_value: tensor([[-28.5312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15398062721312022, distance: 1.0525599650146709 entropy 7.445145954153705
epoch: 1, step: 95
	action: tensor([[ 362.7147,  276.3069, -116.2916, -596.0630,  134.2404,  504.9465,
         -291.7118]], dtype=torch.float64)
	q_value: tensor([[-34.3740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38022546453999484, distance: 0.9008937158489652 entropy 7.633491414556611
epoch: 1, step: 96
	action: tensor([[-1234.8324,  -337.9008,   118.8679,  -268.1742,  -826.4392,   -96.4848,
          -332.4081]], dtype=torch.float64)
	q_value: tensor([[-40.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8114422401988783, distance: 1.5401709887329318 entropy 7.686854932524658
epoch: 1, step: 97
	action: tensor([[ -282.2104, -1351.4070,  -441.7320,   866.9684,   606.2075,   764.3750,
          -172.8234]], dtype=torch.float64)
	q_value: tensor([[-38.4623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0409740364291662, distance: 1.634840670539672 entropy 7.71478912692628
epoch: 1, step: 98
	action: tensor([[ -304.1975, -1112.9906,   637.4951,   704.1494,  -830.8258,   400.4901,
           208.2116]], dtype=torch.float64)
	q_value: tensor([[-45.6166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032836843942414484, distance: 1.1253991042887153 entropy 7.878815657885575
epoch: 1, step: 99
	action: tensor([[-249.5637, -495.4791, -137.1358,   96.8564,   44.3741,  714.7140,
          244.2053]], dtype=torch.float64)
	q_value: tensor([[-42.6840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23600485134033422, distance: 1.0002348767189133 entropy 7.816810772940948
epoch: 1, step: 100
	action: tensor([[-372.9877, -102.3644,  175.9279, -720.2005,  172.6970,  -12.6019,
          321.5134]], dtype=torch.float64)
	q_value: tensor([[-34.4977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.702414895075092, distance: 1.8811901939986049 entropy 7.530529122300074
epoch: 1, step: 101
	action: tensor([[-493.0224,    5.1353, -235.1252,  233.3025,  374.4758,  248.3836,
          466.7936]], dtype=torch.float64)
	q_value: tensor([[-29.8269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1294360510311159, distance: 1.6698943466761724 entropy 7.346471022932184
epoch: 1, step: 102
	action: tensor([[-640.7435, -113.8371, -346.1885,  544.4766,   78.0920, -256.6591,
          400.8075]], dtype=torch.float64)
	q_value: tensor([[-37.8603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008826187100228644, distance: 1.149383257875157 entropy 7.715557658189282
epoch: 1, step: 103
	action: tensor([[ -51.1406, -841.2551, -339.6669, -422.2807,  314.4798, -271.7419,
         -960.5864]], dtype=torch.float64)
	q_value: tensor([[-35.3794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.537153259686378, distance: 1.4187807209883767 entropy 7.542569854252556
epoch: 1, step: 104
	action: tensor([[-669.8150, -339.5961,  376.6727,  321.7172, -196.4552,  177.1796,
         -302.9336]], dtype=torch.float64)
	q_value: tensor([[-26.6398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9857566911277955, distance: 1.6125742127946685 entropy 7.251045374909499
epoch: 1, step: 105
	action: tensor([[-540.5216, -527.0888,  107.1355,  861.8266, -761.4539, -180.4525,
          466.9299]], dtype=torch.float64)
	q_value: tensor([[-29.8285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26976220982099974, distance: 1.2894897433169865 entropy 7.470421728162369
epoch: 1, step: 106
	action: tensor([[ 1.7662e+01, -2.0876e+02,  1.8258e-02, -3.2481e+01, -8.3977e+02,
         -4.7293e+02,  5.3272e+02]], dtype=torch.float64)
	q_value: tensor([[-30.9815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4366296579624038, distance: 1.371605150244628 entropy 7.552431220263344
epoch: 1, step: 107
	action: tensor([[-503.7532,   15.5305,   -1.9288,  365.2342, -380.9672,  -90.4558,
         -143.3607]], dtype=torch.float64)
	q_value: tensor([[-34.4490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13298329185895175, distance: 1.2180593391071257 entropy 7.443654372435413
epoch: 1, step: 108
	action: tensor([[-390.6318, -276.1066, -230.1428,   72.4458, -218.6735, -776.6125,
         -370.0408]], dtype=torch.float64)
	q_value: tensor([[-40.3112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12413332718611159, distance: 1.2132927571153747 entropy 7.508072751323261
epoch: 1, step: 109
	action: tensor([[-712.1987, -111.1961,  251.4524,  -16.9557, -261.8622,   64.8733,
         -267.4267]], dtype=torch.float64)
	q_value: tensor([[-39.3615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09978382033532118, distance: 1.2000804374787761 entropy 7.610279698809032
epoch: 1, step: 110
	action: tensor([[ -73.1392, -452.5582, -403.1667,  230.7788, -936.2984, -515.8610,
          145.8097]], dtype=torch.float64)
	q_value: tensor([[-35.7697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0463629272564101, distance: 1.636997525560676 entropy 7.561903028224315
epoch: 1, step: 111
	action: tensor([[-440.4618, -222.8499, -321.1677,  137.4831,  519.4096, -165.7652,
           61.8993]], dtype=torch.float64)
	q_value: tensor([[-32.3143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22778606279312186, distance: 1.2679964651937672 entropy 7.552374507275192
epoch: 1, step: 112
	action: tensor([[-431.6520, -902.7277,  317.3119,  150.4899, -451.7349, -566.0100,
         -155.4179]], dtype=torch.float64)
	q_value: tensor([[-32.4627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.639463095041884, distance: 1.4652357816137451 entropy 7.650291340501688
epoch: 1, step: 113
	action: tensor([[-1041.2023,   640.1509,  -129.0000,   481.7182,   737.9024,  -237.0733,
           510.3787]], dtype=torch.float64)
	q_value: tensor([[-37.4565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20308463102528185, distance: 1.0215574480546055 entropy 7.685625339642931
epoch: 1, step: 114
	action: tensor([[ 183.8883,  175.3838, -137.7289,  752.5395,  731.0192,   69.7408,
         -431.3533]], dtype=torch.float64)
	q_value: tensor([[-47.4941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47282537047457407, distance: 0.8308716562629175 entropy 7.791633866426726
epoch: 1, step: 115
	action: tensor([[-131.2370,  436.9667, -341.3497,  834.1265,  342.2148,  583.0604,
          -88.4977]], dtype=torch.float64)
	q_value: tensor([[-37.3162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15194123885113808, distance: 1.053827835792618 entropy 7.4874264249826625
epoch: 1, step: 116
	action: tensor([[-1266.8654,  1464.0677,  -445.5769,   157.4592,   587.5400,   270.4021,
          -304.0209]], dtype=torch.float64)
	q_value: tensor([[-47.4465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32037626198297897, distance: 0.943389336763794 entropy 7.76794882851329
epoch: 1, step: 117
	action: tensor([[  83.4230,  183.6755, -545.3987,  983.2161,  114.2675,  552.0960,
         1104.0662]], dtype=torch.float64)
	q_value: tensor([[-42.1738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5512929468463805, distance: 0.7665458581307681 entropy 7.765571395743164
epoch: 1, step: 118
	action: tensor([[-1142.0783,   -96.1548,  -433.4611,   424.0241,    61.9670,   688.0780,
           888.4686]], dtype=torch.float64)
	q_value: tensor([[-35.6142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21329925446003872, distance: 1.2604936397523763 entropy 7.514146279173672
epoch: 1, step: 119
	action: tensor([[-401.7281, -305.4526, -548.0227,  977.6893, -463.0631,  126.6002,
          -14.0700]], dtype=torch.float64)
	q_value: tensor([[-38.1786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6625439291009387, distance: 1.4755137398856641 entropy 7.623336727219522
epoch: 1, step: 120
	action: tensor([[ -368.3084,   140.6420,  -247.2297,  -463.9018,   -60.1832, -1321.7592,
           545.2938]], dtype=torch.float64)
	q_value: tensor([[-35.5542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7690205785189388, distance: 1.5220297303703558 entropy 7.607840557189371
epoch: 1, step: 121
	action: tensor([[-151.5532, -426.4878, -320.0585, -362.2435,  703.3448,  -73.4231,
         -237.5340]], dtype=torch.float64)
	q_value: tensor([[-41.9270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.052362093686320055, distance: 1.173922134589466 entropy 7.55167857429641
epoch: 1, step: 122
	action: tensor([[-476.6276, -411.4632, -388.2197,  621.6465, -508.3550,  559.2834,
          448.2317]], dtype=torch.float64)
	q_value: tensor([[-34.3225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8315469582238619, distance: 1.5486943793760184 entropy 7.468801305752568
epoch: 1, step: 123
	action: tensor([[-474.7853, -381.3884,  137.1292,  -93.3749, -264.2838,  286.7102,
           -8.2943]], dtype=torch.float64)
	q_value: tensor([[-28.9043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0634319053939212, distance: 1.6438105521381834 entropy 7.4406606093144205
epoch: 1, step: 124
	action: tensor([[ -67.5906, -565.3633, -486.1062,  -98.3545,  467.3449, -498.5665,
         -202.2022]], dtype=torch.float64)
	q_value: tensor([[-32.9815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1378846238224386, distance: 1.062525566290312 entropy 7.475385482839827
epoch: 1, step: 125
	action: tensor([[-9.0446e+02, -5.1562e+02,  7.5823e+00, -7.6956e+02, -6.5940e-01,
          2.6369e+01,  2.4916e+02]], dtype=torch.float64)
	q_value: tensor([[-42.6560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0161331827475775, distance: 1.624861326281774 entropy 7.640440443462005
epoch: 1, step: 126
	action: tensor([[-1043.5613,   335.6178,  -155.4824,  -408.8758,   374.1720,    34.1952,
           377.9222]], dtype=torch.float64)
	q_value: tensor([[-36.7315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15914753935346204, distance: 1.0493408815431782 entropy 7.545296744349493
epoch: 1, step: 127
	action: tensor([[ -286.2494, -1163.1326,  -151.4176,   768.3215,   116.1650, -1031.0349,
           258.7203]], dtype=torch.float64)
	q_value: tensor([[-45.9269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40646772594230485, distance: 1.357130399481933 entropy 7.681246789197688
LOSS epoch 1 actor 619.6550284445722 critic 73.34955465976459
epoch: 2, step: 0
	action: tensor([[-741.4297,   12.2863, -401.9710,  151.3097,  -26.5093, -797.5312,
          853.6130]], dtype=torch.float64)
	q_value: tensor([[-31.4544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15220258804357356, distance: 1.2283471328637028 entropy 7.6792145589026
epoch: 2, step: 1
	action: tensor([[ -303.0912, -1185.3302,   312.9469,  -614.6965,   -84.9389,   275.3311,
          1105.9079]], dtype=torch.float64)
	q_value: tensor([[-27.0071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11685881194378012, distance: 1.2093606425841181 entropy 7.634601867208198
epoch: 2, step: 2
	action: tensor([[-560.0209,  157.8425,  190.1316, -978.8883, -512.9330,  -85.0689,
          425.3992]], dtype=torch.float64)
	q_value: tensor([[-33.5335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03787515025560362, distance: 1.165813956546168 entropy 7.742939105671299
epoch: 2, step: 3
	action: tensor([[-814.1142, -474.9205, -674.9565,  406.8706, -491.0656,  336.0188,
           42.3148]], dtype=torch.float64)
	q_value: tensor([[-32.5781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5903168397000431, distance: 1.4431069627934343 entropy 7.586946694789267
epoch: 2, step: 4
	action: tensor([[ -93.6375, -481.2539, -584.2224,  -94.8964,  487.9438,  138.6745,
         -237.7475]], dtype=torch.float64)
	q_value: tensor([[-28.6591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5644906283080793, distance: 1.4313412130980947 entropy 7.704735833652834
epoch: 2, step: 5
	action: tensor([[-471.2426, -477.8060,  675.9821,   70.8954,  -35.2104,  549.5133,
          128.4931]], dtype=torch.float64)
	q_value: tensor([[-25.5453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49480355385148944, distance: 1.3990999920429505 entropy 7.579888527757665
epoch: 2, step: 6
	action: tensor([[ -269.0651,  -145.7999,   191.8930, -1064.3108,  -123.7802,  -331.5588,
           661.5535]], dtype=torch.float64)
	q_value: tensor([[-33.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12371797684320196, distance: 1.213068589701761 entropy 7.694670420729549
epoch: 2, step: 7
	action: tensor([[-605.7967, -596.3750,  138.7820,  -87.6790,  390.5778,   32.4172,
          130.1202]], dtype=torch.float64)
	q_value: tensor([[-37.5889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9270316263682443, distance: 1.5885508249581675 entropy 7.843177246281646
epoch: 2, step: 8
	action: tensor([[  84.1265,   18.2068,  103.7979,  677.8946, -657.4905, -199.6510,
         -477.2246]], dtype=torch.float64)
	q_value: tensor([[-31.6545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23682846497613907, distance: 0.9996955871514827 entropy 7.689721338365822
epoch: 2, step: 9
	action: tensor([[-476.0484, -231.8654,  877.5479,  171.4789,   33.6546,  809.9295,
          323.3721]], dtype=torch.float64)
	q_value: tensor([[-34.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.362615889281284, distance: 1.7589490243671435 entropy 7.697570264401521
epoch: 2, step: 10
	action: tensor([[-157.1535, -629.2262,  104.2966,  761.2626,  209.7260,  214.4382,
          644.7739]], dtype=torch.float64)
	q_value: tensor([[-36.8221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0524989606116395, distance: 1.6394499627113248 entropy 7.922821051015471
epoch: 2, step: 11
	action: tensor([[-1227.1691,  1761.7091,   370.2551,    59.6556,  -306.9921,  -583.1352,
         -1182.7084]], dtype=torch.float64)
	q_value: tensor([[-38.8923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3009585487283368, distance: 1.3052341343635308 entropy 7.948264476768884
epoch: 2, step: 12
	action: tensor([[-425.6749, -104.3009,  -87.6500,  713.9323, -968.7170,  486.5107,
          -80.2229]], dtype=torch.float64)
	q_value: tensor([[-30.7729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06966354816372244, distance: 1.1835327811267369 entropy 7.601802856672864
epoch: 2, step: 13
	action: tensor([[ -29.2999, -425.1827,  348.6759,  108.3550, -867.5589,  487.5165,
          455.0749]], dtype=torch.float64)
	q_value: tensor([[-29.1643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0431123213134339, distance: 1.1194048249187196 entropy 7.6111659899050705
epoch: 2, step: 14
	action: tensor([[-490.6530, -378.8751, -687.5228,  371.7920, -398.2337,  871.9283,
          783.9336]], dtype=torch.float64)
	q_value: tensor([[-33.4514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3627133618455649, distance: 1.3358538622541467 entropy 7.853511165113247
epoch: 2, step: 15
	action: tensor([[-1315.1553,  -828.4507,  -640.1598,   -76.1388,   467.6137,   693.6856,
           368.9423]], dtype=torch.float64)
	q_value: tensor([[-35.5407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.272051468236866, distance: 1.724907362178497 entropy 8.090472351986119
epoch: 2, step: 16
	action: tensor([[-471.9360,  303.7159, -202.9265,  519.7927,  122.3171,  457.0566,
          475.5252]], dtype=torch.float64)
	q_value: tensor([[-31.8732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4123193179287761, distance: 1.3599506314592258 entropy 7.778708474774078
epoch: 2, step: 17
	action: tensor([[-612.0662, -419.7434,  775.4750, -534.5762,  -28.1561,   59.7946,
         1273.9199]], dtype=torch.float64)
	q_value: tensor([[-45.4574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0581586716244384, distance: 1.6417087763504727 entropy 7.974893694971755
epoch: 2, step: 18
	action: tensor([[-1842.2482,  -917.3434,  -140.7360,  -454.0201,  -139.2273,   887.4404,
          -437.4630]], dtype=torch.float64)
	q_value: tensor([[-35.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24028736083740254, distance: 1.2744354760871637 entropy 7.9006866712923625
epoch: 2, step: 19
	action: tensor([[-1773.3384, -1442.6958,    71.9785,   849.8543,   119.2068,   989.1589,
          -819.0222]], dtype=torch.float64)
	q_value: tensor([[-35.7868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13852821580899577, distance: 1.221036348071069 entropy 7.884714105926796
epoch: 2, step: 20
	action: tensor([[-458.9994, -712.1850, -419.2031,  618.9120, -367.4437,  424.5451,
         -188.7031]], dtype=torch.float64)
	q_value: tensor([[-33.4412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31998586067379575, distance: 1.314744409690012 entropy 7.81325215775355
epoch: 2, step: 21
	action: tensor([[ -698.9105,  -559.2423,   216.0599,   722.1053, -1085.3601,  1732.1492,
          -628.0492]], dtype=torch.float64)
	q_value: tensor([[-33.5123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3604733875404509, distance: 1.3347554988457098 entropy 7.902383297986374
epoch: 2, step: 22
	action: tensor([[ -132.2772, -1294.5641,   202.1494,  -509.0506,   552.7006,   191.9468,
          -188.8074]], dtype=torch.float64)
	q_value: tensor([[-34.4682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7251287062859846, distance: 1.5030292911515124 entropy 7.81585861582047
epoch: 2, step: 23
	action: tensor([[-534.6401,  159.7403, -234.2904, -125.9814, -288.5928,  -34.6086,
          387.4519]], dtype=torch.float64)
	q_value: tensor([[-23.7890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5799046725596382, distance: 1.438375029565657 entropy 7.431243149156417
epoch: 2, step: 24
	action: tensor([[-341.9649,  -44.2533, -325.4564,  177.0986, -109.6783, -423.8495,
          110.3948]], dtype=torch.float64)
	q_value: tensor([[-27.3921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17637432130300756, distance: 1.0385361838692153 entropy 7.22285859702981
epoch: 2, step: 25
	action: tensor([[-433.0562, -823.4092,  164.9916,   95.0344, -419.6644,  686.8788,
          827.7041]], dtype=torch.float64)
	q_value: tensor([[-37.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20123132969952795, distance: 1.022744621994155 entropy 7.7278225772104605
epoch: 2, step: 26
	action: tensor([[ -405.5221,  -482.9403, -1010.0811,   -52.5578,  -337.8184,   889.2869,
           439.3290]], dtype=torch.float64)
	q_value: tensor([[-29.0899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4961060483198225, distance: 1.3997094109861967 entropy 7.703987000739139
epoch: 2, step: 27
	action: tensor([[   34.8859, -1100.4857,   483.7427,  -112.4571,    37.4834,  -291.5753,
          -154.2565]], dtype=torch.float64)
	q_value: tensor([[-27.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7564881599646676, distance: 1.5166288274057076 entropy 7.601005385917881
epoch: 2, step: 28
	action: tensor([[-257.3096, -189.6169,  136.3575,  310.8594,  -78.3005,  595.5316,
          528.7994]], dtype=torch.float64)
	q_value: tensor([[-24.8858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9928675840532235, distance: 1.615458905417882 entropy 7.418872958680345
epoch: 2, step: 29
	action: tensor([[ -445.6430, -1109.6322,  -732.2464,  -841.2872,   814.5396,  -744.6277,
          1815.1420]], dtype=torch.float64)
	q_value: tensor([[-35.7920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1352724867420139, distance: 1.6721812343063307 entropy 8.000794648792018
epoch: 2, step: 30
	action: tensor([[  454.7044,   636.2280,   325.9994,   451.7687,    27.0669, -1016.7301,
          -187.9256]], dtype=torch.float64)
	q_value: tensor([[-36.9506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5720672921039491, distance: 0.748590711712294 entropy 7.900937456143436
epoch: 2, step: 31
	action: tensor([[-1056.4137,  -161.5448,    47.5654,   465.4529,    80.3973,   384.0853,
           963.9853]], dtype=torch.float64)
	q_value: tensor([[-30.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.081850745309632, distance: 1.6511308367409636 entropy 7.549096015446291
epoch: 2, step: 32
	action: tensor([[-943.5725, -267.6656,  103.6513,  472.1831, -301.4860,  903.8184,
          872.8534]], dtype=torch.float64)
	q_value: tensor([[-31.0009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06296179977213323, distance: 1.1798193697219501 entropy 7.84360773680366
epoch: 2, step: 33
	action: tensor([[-186.4292,   55.9540, -809.7761, -688.6145,  689.2691,  311.2337,
         1164.1866]], dtype=torch.float64)
	q_value: tensor([[-34.4002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9006259716197427, distance: 1.5776295163554055 entropy 7.8626322898765135
epoch: 2, step: 34
	action: tensor([[ 176.8233, -648.2813,  338.8325,  112.2439, -543.3979, -374.9910,
          238.0381]], dtype=torch.float64)
	q_value: tensor([[-28.3589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4025670345905349, distance: 0.8845070209889256 entropy 7.626249039702729
epoch: 2, step: 35
	action: tensor([[ 3.7194e+02, -5.9691e+02,  1.6447e+02, -4.8213e+02, -9.5716e-01,
          1.0904e+03,  5.0853e+02]], dtype=torch.float64)
	q_value: tensor([[-24.7976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1174709484237646, distance: 1.0750315215016348 entropy 7.697729813179039
epoch: 2, step: 36
	action: tensor([[-332.1536, -896.8927, -178.4096,   86.4131,  156.3859, -197.3335,
          287.4063]], dtype=torch.float64)
	q_value: tensor([[-16.9783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8335013646761602, distance: 1.5495204491776307 entropy 7.330571167524092
epoch: 2, step: 37
	action: tensor([[ 541.7913, -768.2759,  180.1780, -238.5878, -350.0087, -170.5274,
         -242.6581]], dtype=torch.float64)
	q_value: tensor([[-27.9282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4458056345031547, distance: 1.3759785054956064 entropy 7.577570523474688
epoch: 2, step: 38
	action: tensor([[ -800.3801, -1627.3708,    84.3439,  1399.2357,  -286.6830,   344.8241,
           333.7427]], dtype=torch.float64)
	q_value: tensor([[-35.0586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2171592752749305, distance: 1.703943302030682 entropy 7.839800984181873
epoch: 2, step: 39
	action: tensor([[ -388.1771,  -606.8846,  -148.3925,   770.6928, -1022.3684,   957.5346,
          -746.5663]], dtype=torch.float64)
	q_value: tensor([[-32.1181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3080155219805789, distance: 1.3087694295549217 entropy 7.7462733370034424
epoch: 2, step: 40
	action: tensor([[-642.1686, -731.7594, -246.5974,  198.6185,  535.0920,  341.8671,
          328.0943]], dtype=torch.float64)
	q_value: tensor([[-35.9045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5365590545419059, distance: 1.418506471076663 entropy 7.869176745350982
epoch: 2, step: 41
	action: tensor([[-139.7227, -240.6308,   -8.5964,  150.8281,  385.9895,   86.4465,
         -717.2335]], dtype=torch.float64)
	q_value: tensor([[-34.3866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.054696804247786, distance: 1.6403275004468152 entropy 7.808742071843647
epoch: 2, step: 42
	action: tensor([[ 499.2781,  483.9149,  541.4929, 1209.4005, -272.1448,  598.3095,
          154.7628]], dtype=torch.float64)
	q_value: tensor([[-38.9563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439226958963248, distance: 0.7728156775322651 entropy 7.893870435728921
epoch: 2, step: 43
	action: tensor([[  -58.5031, -1159.7214,  -847.9712, -1565.3640,  -654.2915,    39.6763,
           434.7710]], dtype=torch.float64)
	q_value: tensor([[-28.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2261115326318095, distance: 1.267131484373774 entropy 7.750536810140271
epoch: 2, step: 44
	action: tensor([[  38.3164,  407.8354,  -16.1149, -163.8247,  -32.3490,  -68.4146,
         -654.1095]], dtype=torch.float64)
	q_value: tensor([[-17.3191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.2688299726905115
epoch: 2, step: 45
	action: tensor([[-235.5971, -629.2203,   75.5214,  707.9554, -302.0885,  144.3640,
           -6.6694]], dtype=torch.float64)
	q_value: tensor([[-30.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.085503611758579, distance: 1.6525787592385788 entropy 7.219796688922144
epoch: 2, step: 46
	action: tensor([[-417.6078,  127.1041,  294.3474,  265.1066,  195.1159,  -71.0146,
           78.4317]], dtype=torch.float64)
	q_value: tensor([[-20.8406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.56504504187414, distance: 1.4315948051237173 entropy 7.349142131109131
epoch: 2, step: 47
	action: tensor([[-389.9022, -692.2856, -623.4903,   45.1311, -381.2398,  145.7380,
         -394.7064]], dtype=torch.float64)
	q_value: tensor([[-29.9048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1194237236986528, distance: 1.6659639096959786 entropy 7.646585379541067
epoch: 2, step: 48
	action: tensor([[-334.2758, -889.0238,  315.3962,  374.1215,  773.1176,  469.9619,
         -784.8729]], dtype=torch.float64)
	q_value: tensor([[-34.1570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2350445925741056, distance: 1.000863272523387 entropy 7.793139237606299
epoch: 2, step: 49
	action: tensor([[  87.8033,  -15.7982, -578.5880,  600.4739, -460.8612, -198.8829,
         -171.5230]], dtype=torch.float64)
	q_value: tensor([[-30.2209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.681811156762137
epoch: 2, step: 50
	action: tensor([[-373.6933,  619.5837, -130.1425,  113.5312,   47.7919,  235.3124,
          244.9605]], dtype=torch.float64)
	q_value: tensor([[-30.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5804858775836376, distance: 0.7411907418826084 entropy 7.219796688922144
epoch: 2, step: 51
	action: tensor([[-952.9262,  -75.4326,   86.8270,  298.6602,  116.3925, 1232.6224,
         -319.6002]], dtype=torch.float64)
	q_value: tensor([[-39.5213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4340462790453954, distance: 1.3703713701309415 entropy 7.730044368579658
epoch: 2, step: 52
	action: tensor([[-193.1142, -236.3088,  152.6471, -384.7612,  571.2392,  891.8986,
          534.6553]], dtype=torch.float64)
	q_value: tensor([[-33.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9863074261491351, distance: 1.612797815094216 entropy 7.849356821869585
epoch: 2, step: 53
	action: tensor([[-252.1998, -862.7857, -255.2921, -479.4600, -116.3789,  585.4434,
         -524.8294]], dtype=torch.float64)
	q_value: tensor([[-24.3065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11860056616294956, distance: 1.074343292872332 entropy 7.46881383123728
epoch: 2, step: 54
	action: tensor([[ -346.5079,  -558.9514, -1121.3453,   357.0704,  -166.2903, -1256.8574,
           355.5149]], dtype=torch.float64)
	q_value: tensor([[-36.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.52446426118287, distance: 1.4129126615149858 entropy 7.940024996294352
epoch: 2, step: 55
	action: tensor([[-1172.4364,   346.1627,   776.9642,   497.9218,   231.7010,   -73.9659,
           493.0950]], dtype=torch.float64)
	q_value: tensor([[-37.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19373193432653424, distance: 1.2502881047957037 entropy 8.028262202519148
epoch: 2, step: 56
	action: tensor([[-1057.2775,   123.7687, -1087.1644,   228.9433, -1295.5494,   478.6557,
           495.3745]], dtype=torch.float64)
	q_value: tensor([[-36.7597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022772977807677397, distance: 1.1312391506096813 entropy 7.926540183058672
epoch: 2, step: 57
	action: tensor([[ -319.7310,  -285.4459,  1036.8461, -1526.8889,  -631.6100,   180.9614,
           435.0459]], dtype=torch.float64)
	q_value: tensor([[-28.9965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5931821990638111, distance: 1.4444064394290486 entropy 7.63933278798856
epoch: 2, step: 58
	action: tensor([[-867.0203, -148.0257, -705.4258, -384.1191,    7.3271,  635.3463,
          734.7910]], dtype=torch.float64)
	q_value: tensor([[-32.4455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39440463091821965, distance: 1.3512978988750497 entropy 7.674572148117998
epoch: 2, step: 59
	action: tensor([[-1.2081e+02, -4.5983e+02, -2.0537e+02, -2.5234e+01, -3.5750e+02,
          3.6850e-01,  2.5028e+02]], dtype=torch.float64)
	q_value: tensor([[-29.5915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9576410947211196, distance: 1.6011175940327822 entropy 7.632127211957491
epoch: 2, step: 60
	action: tensor([[ -714.3429, -1092.5126,   571.6176,   591.1983,  -575.8303,   448.1622,
          -407.6810]], dtype=torch.float64)
	q_value: tensor([[-29.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5452035266056459, distance: 1.4224910369303951 entropy 7.651061487529955
epoch: 2, step: 61
	action: tensor([[-811.9672, -489.6909,   32.2635,  695.2463,  119.0621,  718.5254,
          928.1556]], dtype=torch.float64)
	q_value: tensor([[-28.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07608076038894207, distance: 1.0999519113885452 entropy 7.623430077508077
epoch: 2, step: 62
	action: tensor([[-1096.4500,  -714.3594,  -243.1183,  1430.9403,    88.4675,  -423.4229,
          -351.0609]], dtype=torch.float64)
	q_value: tensor([[-28.6090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23312691243981698, distance: 1.270751354639102 entropy 7.677232986964235
epoch: 2, step: 63
	action: tensor([[-212.9791,  -57.1546, -191.9223,  279.3294, -236.5892, 1629.7431,
          873.5358]], dtype=torch.float64)
	q_value: tensor([[-31.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4570317566962645, distance: 1.3813101467307738 entropy 7.731007787787401
epoch: 2, step: 64
	action: tensor([[-998.9338, -287.0065,  177.9369,  662.6369,  511.9744,  594.6795,
         -753.7419]], dtype=torch.float64)
	q_value: tensor([[-34.5762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1718919050900587, distance: 1.2387979284510677 entropy 7.818865010087365
epoch: 2, step: 65
	action: tensor([[-1559.2555,  -248.4506,   184.8028,   190.6852,   995.3965,  -222.8855,
          1172.8268]], dtype=torch.float64)
	q_value: tensor([[-37.6317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4225357060094058, distance: 0.8695994596750669 entropy 7.912843831850701
epoch: 2, step: 66
	action: tensor([[-628.0915, -749.3507, -697.0931, 1635.9619,  711.9384, -379.3556,
           27.9431]], dtype=torch.float64)
	q_value: tensor([[-38.8228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32333265346281226, distance: 1.316410106132274 entropy 7.898493196289153
epoch: 2, step: 67
	action: tensor([[ -656.2980, -1163.4085,  -870.9108,   478.0375,  -875.7904,   877.2336,
            69.9770]], dtype=torch.float64)
	q_value: tensor([[-38.1505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0961192574071712, distance: 1.1980793897534856 entropy 7.907812978578456
epoch: 2, step: 68
	action: tensor([[ -479.4334, -1312.0135,   119.6846,  1807.1475,  -135.3099,  1305.0132,
           722.3570]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39502746296460756, distance: 1.351599654067218 entropy 7.912321116834467
epoch: 2, step: 69
	action: tensor([[ -303.9270,  -575.5369,  -895.1343,   230.8611,  -221.3764,    36.2461,
         -1498.8836]], dtype=torch.float64)
	q_value: tensor([[-35.4019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4578989124056647, distance: 0.8425522303388964 entropy 7.837869307681637
epoch: 2, step: 70
	action: tensor([[-1625.3516,  -978.8841,  -167.0767,   324.0866,  -348.3922,  -210.5695,
          -467.0974]], dtype=torch.float64)
	q_value: tensor([[-30.3998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06874537903858613, distance: 1.1043097679635987 entropy 7.829878026249197
epoch: 2, step: 71
	action: tensor([[-204.5896,  652.6586, -911.1413, 1933.7944, -764.0761,   86.5507,
          273.8820]], dtype=torch.float64)
	q_value: tensor([[-37.6777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.106270410025385
epoch: 2, step: 72
	action: tensor([[-213.9800,  -52.1243,   26.3925,  266.7361, -364.3398,  261.0951,
          280.0795]], dtype=torch.float64)
	q_value: tensor([[-30.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017547445706050158, distance: 1.134259659151634 entropy 7.219796688922144
epoch: 2, step: 73
	action: tensor([[-617.8197, -808.3649, -449.2587,  284.0014, 1312.2781,  186.7307,
          332.2279]], dtype=torch.float64)
	q_value: tensor([[-28.6664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19257776715206176, distance: 1.0282697154362979 entropy 7.564656498782481
epoch: 2, step: 74
	action: tensor([[-272.1463, -581.4174, -261.4033, 1323.1604, -716.7545,  987.7824,
         -117.3480]], dtype=torch.float64)
	q_value: tensor([[-31.8967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2023232890524046, distance: 1.0220453097038729 entropy 7.826932775590552
epoch: 2, step: 75
	action: tensor([[-783.9457, -616.5766,   58.0260,  939.1450,  481.2108,  753.5680,
         1064.3169]], dtype=torch.float64)
	q_value: tensor([[-32.2842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25443938288777956, distance: 1.2816856837780395 entropy 7.813530686600139
epoch: 2, step: 76
	action: tensor([[ -881.0729, -2473.6364,   116.6573,  1029.6517, -1054.6315,  -181.2322,
           432.6215]], dtype=torch.float64)
	q_value: tensor([[-34.9268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1942539621006909, distance: 1.0272018256176698 entropy 7.975602574995185
epoch: 2, step: 77
	action: tensor([[ -311.4398, -1166.2761,  -597.8401,  -538.7148,    66.8578,   265.7618,
          -311.7557]], dtype=torch.float64)
	q_value: tensor([[-34.7846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3306191352054424, distance: 1.320029313530933 entropy 7.932175129002177
epoch: 2, step: 78
	action: tensor([[ -490.1225,  -713.4345,   -46.3399,    66.0142,  -133.7202, -1004.0328,
          1354.9152]], dtype=torch.float64)
	q_value: tensor([[-33.9440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8914269250775708, distance: 1.573807015091807 entropy 7.886550382022798
epoch: 2, step: 79
	action: tensor([[    6.5232, -1551.5312,    77.4015,   147.5416,   752.4460,   -46.5107,
          -299.8412]], dtype=torch.float64)
	q_value: tensor([[-30.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24520216248788596, distance: 1.276958038583818 entropy 7.862644173089204
epoch: 2, step: 80
	action: tensor([[-504.1345,  449.6709, -452.1133, -382.0554, -840.3934,  821.6899,
         -455.4480]], dtype=torch.float64)
	q_value: tensor([[-32.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14854524398031588, distance: 1.055935719399216 entropy 7.90068868659132
epoch: 2, step: 81
	action: tensor([[-740.1143,  209.0641,  168.3056,   23.0065, -564.2991,  166.6823,
          377.1622]], dtype=torch.float64)
	q_value: tensor([[-27.4274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4515520428535613, distance: 0.8474701395348223 entropy 7.484224328237908
epoch: 2, step: 82
	action: tensor([[  813.0529,  -789.1868, -1065.2424,   571.6760,   402.1054,   292.9002,
          -956.9448]], dtype=torch.float64)
	q_value: tensor([[-33.4524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39159036478989007, distance: 0.89259558603902 entropy 7.575706530187468
epoch: 2, step: 83
	action: tensor([[-901.3708, -750.1627,  748.1931,   -3.8287, -577.2856,  458.4477,
         -829.0941]], dtype=torch.float64)
	q_value: tensor([[-28.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4761554587322049, distance: 0.8282432489588919 entropy 7.82448861144939
epoch: 2, step: 84
	action: tensor([[-635.2888,  191.5160,  256.5450, 1128.6044, -174.2008, -376.7203,
         -250.5433]], dtype=torch.float64)
	q_value: tensor([[-26.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.574033737521945
epoch: 2, step: 85
	action: tensor([[-237.4381, -542.1142, -217.8427,  234.1618,  121.1755, -385.9511,
          312.8380]], dtype=torch.float64)
	q_value: tensor([[-30.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13332634076690997, distance: 1.065330821407008 entropy 7.219796688922144
epoch: 2, step: 86
	action: tensor([[ 369.8632, -388.5791,  320.3420,  831.6762, -112.6768, -810.5993,
         1431.0917]], dtype=torch.float64)
	q_value: tensor([[-35.1807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01872156511998435, distance: 1.1550065394750668 entropy 7.872633992139005
epoch: 2, step: 87
	action: tensor([[-467.6238,  142.5107,  758.9081,   96.3337,  -53.6318,  492.7970,
         -200.3729]], dtype=torch.float64)
	q_value: tensor([[-27.0977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24094396799626172, distance: 1.274772774023938 entropy 7.587343232538754
epoch: 2, step: 88
	action: tensor([[ -251.9042, -1281.0180,   873.7854,   242.1763,   333.0514,   437.8841,
           872.4997]], dtype=torch.float64)
	q_value: tensor([[-35.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21384238058775173, distance: 1.014638896797372 entropy 7.700543680491211
epoch: 2, step: 89
	action: tensor([[-631.4192,  -90.2260, -916.7044, -343.5932,  359.9514,  232.1121,
         1393.9089]], dtype=torch.float64)
	q_value: tensor([[-28.4770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42326410682337, distance: 1.3652099403533688 entropy 7.635041300178693
epoch: 2, step: 90
	action: tensor([[-662.1153, -658.7341, -281.3879,  325.5925,  -42.7113, -121.2248,
           65.4929]], dtype=torch.float64)
	q_value: tensor([[-28.1072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10357049569884003, distance: 1.0834646950880698 entropy 7.616269634049988
epoch: 2, step: 91
	action: tensor([[-1033.4617,  -264.3760,   545.0501,   352.3383,  -648.2078,   929.8672,
           512.2088]], dtype=torch.float64)
	q_value: tensor([[-33.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3128763578666651, distance: 1.311198993242634 entropy 7.8017251557137035
epoch: 2, step: 92
	action: tensor([[-1775.0244,  -467.2909,   210.5733,   543.1631,   304.6448,   430.6570,
           244.3551]], dtype=torch.float64)
	q_value: tensor([[-31.6934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7058797667937948, distance: 1.4946203886656624 entropy 7.875313522656826
epoch: 2, step: 93
	action: tensor([[-618.7495, -625.1383, -597.6845, -722.8572,   69.0133, -204.7914,
         -654.7943]], dtype=torch.float64)
	q_value: tensor([[-40.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6112504191401107, distance: 1.4525738280602647 entropy 7.946381631588679
epoch: 2, step: 94
	action: tensor([[-398.0623, -169.0652,  954.0380,  198.0455,  117.5715, -195.9068,
          654.0920]], dtype=torch.float64)
	q_value: tensor([[-25.5760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41999575013456636, distance: 1.363641518310948 entropy 7.428544367826214
epoch: 2, step: 95
	action: tensor([[   31.3392, -1047.5861,  -164.6922,  -584.8559,  -316.6414,   226.3380,
          -287.2225]], dtype=torch.float64)
	q_value: tensor([[-25.9729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08035586900886305, distance: 1.1894333492560842 entropy 7.676213897770801
epoch: 2, step: 96
	action: tensor([[-218.8355, -376.7609, -444.0382,  691.7676, -431.7693, -781.3144,
         -778.6431]], dtype=torch.float64)
	q_value: tensor([[-26.3679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1900182264581567, distance: 1.0298982411222246 entropy 7.653560885119567
epoch: 2, step: 97
	action: tensor([[-1015.3976,  -591.9173,   312.3013,   123.8784, -1360.4817,  -314.2875,
           901.8622]], dtype=torch.float64)
	q_value: tensor([[-32.8811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21468642314193132, distance: 1.0140940770486706 entropy 7.782991955387868
epoch: 2, step: 98
	action: tensor([[-1218.1710,  -142.5770, -1555.7064,  -301.8352,  -995.4165,   827.8831,
          1095.2687]], dtype=torch.float64)
	q_value: tensor([[-37.2935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6643199378919495, distance: 1.4763016366318538 entropy 7.933048203307337
epoch: 2, step: 99
	action: tensor([[ -79.8598,  -98.4466, -895.2243, -197.4905, -791.0558, -299.6016,
         -329.1250]], dtype=torch.float64)
	q_value: tensor([[-34.9611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06738288710574247, distance: 1.1822703853734382 entropy 8.003078869680763
epoch: 2, step: 100
	action: tensor([[ -419.5369, -2281.1724,  -519.1450,  1243.9290,  -697.1587,    50.6634,
            48.3121]], dtype=torch.float64)
	q_value: tensor([[-38.8148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9061556771937969, distance: 1.5799228372851981 entropy 7.866544080142468
epoch: 2, step: 101
	action: tensor([[-900.3477, -711.2720, -748.7243, -288.7086, -534.1583,  342.6100,
          105.0254]], dtype=torch.float64)
	q_value: tensor([[-28.5857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5920061595703598, distance: 1.4438732321748378 entropy 7.613347102928527
epoch: 2, step: 102
	action: tensor([[-1087.9227,  -783.5482,  -415.4176,   -85.1611,  -259.1784,  -150.6842,
           823.3535]], dtype=torch.float64)
	q_value: tensor([[-32.8772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5925867556508797, distance: 0.730422705264181 entropy 7.746377700550184
epoch: 2, step: 103
	action: tensor([[-363.0081, -110.1644, -572.2864,   57.5511, -463.4598,  373.9518,
          450.0410]], dtype=torch.float64)
	q_value: tensor([[-26.7441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5751346703558735, distance: 1.4362020381552487 entropy 7.561682876722841
epoch: 2, step: 104
	action: tensor([[-572.6450,  512.9580,  658.2517,  468.2276, -490.0037, -377.7459,
          970.4353]], dtype=torch.float64)
	q_value: tensor([[-26.3866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2757882112618044, distance: 1.2925459335182314 entropy 7.534194921861163
epoch: 2, step: 105
	action: tensor([[  22.4142, -516.4486, -917.1306,  323.8176,  -82.8818, -266.8213,
         -691.7515]], dtype=torch.float64)
	q_value: tensor([[-31.5714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.598321062113501
epoch: 2, step: 106
	action: tensor([[-159.1485, -228.7424,  -92.1082,  156.8309,   32.3059, -125.4005,
          -31.9773]], dtype=torch.float64)
	q_value: tensor([[-30.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.219796688922144
epoch: 2, step: 107
	action: tensor([[-334.3903, -276.5328, -372.9964,  107.5521,  -42.6940, -692.9899,
           56.7765]], dtype=torch.float64)
	q_value: tensor([[-30.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6686230957110204, distance: 1.4782089195303822 entropy 7.219796688922144
epoch: 2, step: 108
	action: tensor([[-660.2843,  247.4072, -756.5508,   82.7845,  -93.4019,  245.3196,
          435.0485]], dtype=torch.float64)
	q_value: tensor([[-29.8037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.536721579159559, distance: 1.4185814880847223 entropy 7.5740850765569325
epoch: 2, step: 109
	action: tensor([[-815.8222, -587.4939,  351.4560, -642.6357, -145.8438,  245.2099,
         -262.2284]], dtype=torch.float64)
	q_value: tensor([[-34.3276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5221197962536499, distance: 1.411825788301346 entropy 7.54259099180445
epoch: 2, step: 110
	action: tensor([[  557.9569, -1023.2101,   125.8872,   901.9968,  -557.5281,   788.0357,
          -491.2563]], dtype=torch.float64)
	q_value: tensor([[-28.5951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42213645387122145, distance: 0.8699000232400318 entropy 7.615337870792175
epoch: 2, step: 111
	action: tensor([[ -733.4744, -1003.4675,    89.9351,   130.1469,   351.6789,   208.5897,
          -545.5283]], dtype=torch.float64)
	q_value: tensor([[-22.5770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20282528913730002, distance: 1.021723658355398 entropy 7.375262451025891
epoch: 2, step: 112
	action: tensor([[  45.5833,  -49.5328, -303.9219, 1916.3520, -389.9692,  -74.1028,
          231.7375]], dtype=torch.float64)
	q_value: tensor([[-29.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0854618892427974, distance: 1.1922408092317065 entropy 7.718294211622351
epoch: 2, step: 113
	action: tensor([[ 364.9793,  -52.4340,  142.5632,  251.2608,  337.5130,  899.2251,
         -567.7360]], dtype=torch.float64)
	q_value: tensor([[-25.2396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1677830926243673, distance: 1.2366243244969466 entropy 7.6712321577343525
epoch: 2, step: 114
	action: tensor([[ -281.2150, -1176.0235,  -251.0012,  -199.5792,    75.7964,   189.3256,
          -723.8354]], dtype=torch.float64)
	q_value: tensor([[-25.2742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07596384418886659, distance: 1.1000215052036961 entropy 7.872770166200128
epoch: 2, step: 115
	action: tensor([[-550.9557, -173.9217,  127.3172, -121.9809, -226.5099, -294.2240,
          710.2796]], dtype=torch.float64)
	q_value: tensor([[-20.5996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5599057887465109, distance: 1.8309171154317192 entropy 7.319737545136505
epoch: 2, step: 116
	action: tensor([[ -38.2292, -103.0330, -385.6748, -524.4374, -577.2941,  309.1580,
          286.7541]], dtype=torch.float64)
	q_value: tensor([[-19.6958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.874496390810684, distance: 1.56674745369595 entropy 7.280530596267634
epoch: 2, step: 117
	action: tensor([[-394.4974,  -26.4066,  271.1658,  664.8514,  336.7737,  -57.6516,
          453.5062]], dtype=torch.float64)
	q_value: tensor([[-31.6137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.0091719822514573, distance: 1.9850899837992237 entropy 7.754800138685835
epoch: 2, step: 118
	action: tensor([[-1.3331e+03, -6.8260e+02, -1.0389e+00,  5.3289e+02,  2.7779e+02,
         -5.5277e+02, -1.1282e+03]], dtype=torch.float64)
	q_value: tensor([[-27.3193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6554388718149833, distance: 1.4723574821273517 entropy 7.667300508273164
epoch: 2, step: 119
	action: tensor([[ -676.6704,  -923.5163,  -169.7364,   -31.3956,   358.0076, -1394.2183,
          -493.2399]], dtype=torch.float64)
	q_value: tensor([[-31.4606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1148335137823766, distance: 1.6641588744489586 entropy 7.727738392856883
epoch: 2, step: 120
	action: tensor([[-436.6825, -737.2484,  484.4109, 1199.7752, -271.8789, -940.5337,
          718.0284]], dtype=torch.float64)
	q_value: tensor([[-32.1166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0027355542098930208, distance: 1.142777974234842 entropy 7.757537345142582
epoch: 2, step: 121
	action: tensor([[-1525.3186,  -862.4756,  -798.0829,  -402.3088,  -734.8587,   -11.3713,
          -273.9056]], dtype=torch.float64)
	q_value: tensor([[-29.1577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9932030508096519, distance: 1.6155948677765992 entropy 7.836167698162419
epoch: 2, step: 122
	action: tensor([[ 119.8632,  -44.6555,   24.4075,  821.7967, -658.1195,  448.7232,
         -182.2658]], dtype=torch.float64)
	q_value: tensor([[-19.7645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6540832607018732, distance: 0.6730424898377414 entropy 7.391381694252907
epoch: 2, step: 123
	action: tensor([[-881.7668,   40.4099,  330.4730,  230.8908, -350.2498,   -5.2330,
          206.0324]], dtype=torch.float64)
	q_value: tensor([[-29.4015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7111858109003373, distance: 1.4969430507224062 entropy 7.596738120605188
epoch: 2, step: 124
	action: tensor([[-393.7900, -524.2463, -204.3277,  -80.5685, -257.0476,  210.3518,
         -298.2500]], dtype=torch.float64)
	q_value: tensor([[-33.2371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04257842600550954, distance: 1.1684524948310981 entropy 7.675840818928007
epoch: 2, step: 125
	action: tensor([[-1329.9048,   277.1971,   -36.5551,  1779.0980,  -253.1450,  -158.6562,
          -210.8897]], dtype=torch.float64)
	q_value: tensor([[-34.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31176283805614524, distance: 0.9493486841552102 entropy 7.860387210543353
epoch: 2, step: 126
	action: tensor([[ -957.1583,   237.6106,   395.8168,  -221.6741,  -545.1621,    21.5055,
         -1170.2207]], dtype=torch.float64)
	q_value: tensor([[-32.1109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.841637125710072
epoch: 2, step: 127
	action: tensor([[-437.3347,  197.4656,  487.8035,  235.4829, -101.8126,   94.0130,
          347.0949]], dtype=torch.float64)
	q_value: tensor([[-30.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030904867212857456, distance: 1.1265225756226533 entropy 7.219796688922144
LOSS epoch 2 actor 440.5354692058689 critic 149.61603809095266
epoch: 3, step: 0
	action: tensor([[-976.1141, -133.5820, -621.1126, -627.2157, -312.8182,  310.9670,
         -217.1464]], dtype=torch.float64)
	q_value: tensor([[-23.6772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1284863376976144, distance: 1.669521924646261 entropy 7.786428824414342
epoch: 3, step: 1
	action: tensor([[ -569.9081,  -181.9319, -1206.0613,  1123.9966, -1102.1569,   653.8521,
           370.1029]], dtype=torch.float64)
	q_value: tensor([[-23.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6045896085116078, distance: 1.4495682930131586 entropy 7.828389205182523
epoch: 3, step: 2
	action: tensor([[ -358.9261, -2718.5421,  -685.5891,   828.9893, -1108.9081,   266.7317,
            95.4968]], dtype=torch.float64)
	q_value: tensor([[-26.3121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9814589742703492, distance: 1.610828243290278 entropy 8.076115796716888
epoch: 3, step: 3
	action: tensor([[-1129.4309,   -67.2954,  -557.5182,  -180.6470,   470.0052, -1315.2829,
          -114.6862]], dtype=torch.float64)
	q_value: tensor([[-25.2006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40432655830365083, distance: 1.356096977091262 entropy 8.04926837293461
epoch: 3, step: 4
	action: tensor([[-666.9551, -728.4603, -382.1182,  825.2892,  193.1795,  211.0446,
          215.9212]], dtype=torch.float64)
	q_value: tensor([[-22.6805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0988941453570733, distance: 1.1995949340865781 entropy 7.799743210952288
epoch: 3, step: 5
	action: tensor([[-1062.3077,  -838.6894,   989.0532,   468.4572,   283.5759,   475.3344,
           620.2374]], dtype=torch.float64)
	q_value: tensor([[-23.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016999239721507964, distance: 1.1540297570592908 entropy 7.995407364895086
epoch: 3, step: 6
	action: tensor([[ -646.4871,  -421.3690,  -344.9996,  -190.2894,  -108.3493, -1240.3678,
          1170.4196]], dtype=torch.float64)
	q_value: tensor([[-26.2710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6959130441524828, distance: 1.4902477800656937 entropy 8.006033577944132
epoch: 3, step: 7
	action: tensor([[-1883.2314, -1503.3184,   -35.1035,   386.3924,  -182.8361,  1361.9017,
          -279.4391]], dtype=torch.float64)
	q_value: tensor([[-24.7983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027351245523048573, distance: 1.1285861355987845 entropy 7.896245579029999
epoch: 3, step: 8
	action: tensor([[-189.5171, -329.8890,  388.4773,   25.7441,  513.3110, 1311.9728,
         -407.7111]], dtype=torch.float64)
	q_value: tensor([[-27.7934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23968146312488559, distance: 1.2741241482884542 entropy 8.088066531965163
epoch: 3, step: 9
	action: tensor([[-1420.3660,  1150.9706,  -398.4889,  -259.0987, -1325.1741,  1029.7976,
          -719.8640]], dtype=torch.float64)
	q_value: tensor([[-25.5727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1902661526734799, distance: 1.2484717944477421 entropy 8.003342051688659
epoch: 3, step: 10
	action: tensor([[-1069.0936, -1783.8604, -1060.2785,    -3.0298,   641.7799,   718.9422,
          1239.6922]], dtype=torch.float64)
	q_value: tensor([[-31.1403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.89467634512354
epoch: 3, step: 11
	action: tensor([[ 139.9668, -185.1385, -652.7476,   -0.8833,   82.5102, -475.5563,
         -875.8042]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07820042789811077, distance: 1.1882462248833767 entropy 7.352968730472594
epoch: 3, step: 12
	action: tensor([[-383.5445,  115.3577,  422.9430,  232.4857,  411.1263, -341.2996,
           21.3438]], dtype=torch.float64)
	q_value: tensor([[-21.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.515919879130901
epoch: 3, step: 13
	action: tensor([[-359.4353, -696.1171,  -11.2046,  885.0248, -696.6800,  667.4569,
          517.2912]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.352968730472594
epoch: 3, step: 14
	action: tensor([[ -25.5589, -466.2295,  310.8430,   76.4673, -132.1738, -153.8536,
          626.9306]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.678141861669181, distance: 1.4824191917532255 entropy 7.352968730472594
epoch: 3, step: 15
	action: tensor([[-448.1318,  248.6882,  406.0328, 2121.8120, -543.4973, -335.6210,
         1222.2612]], dtype=torch.float64)
	q_value: tensor([[-25.3580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23851691057498736, distance: 0.9985891083530284 entropy 7.900853511746428
epoch: 3, step: 16
	action: tensor([[-1970.9301, -1296.3753,   414.6679, -1618.9175,  -271.8478,   536.1885,
          -124.0654]], dtype=torch.float64)
	q_value: tensor([[-31.9435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9351950563500202, distance: 1.5919120356020664 entropy 8.226392085676377
epoch: 3, step: 17
	action: tensor([[-1290.2704,  -859.2827,  -479.9248,   541.1570,  -503.3904,    -4.8415,
          -571.2911]], dtype=torch.float64)
	q_value: tensor([[-20.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.367552745861329, distance: 1.3382237592077673 entropy 7.703891364750535
epoch: 3, step: 18
	action: tensor([[-433.3035,  765.3888,  -66.3529,  491.4176, -573.4873, -903.6141,
         -708.1043]], dtype=torch.float64)
	q_value: tensor([[-20.5009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22292698175040382, distance: 1.2654848689771099 entropy 7.735098511365374
epoch: 3, step: 19
	action: tensor([[-793.5477,   27.5277,  777.2393,  266.1495,  550.7322,  639.9456,
         -193.4091]], dtype=torch.float64)
	q_value: tensor([[-28.5133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4906183941741453, distance: 1.3971400150482047 entropy 8.034345109587889
epoch: 3, step: 20
	action: tensor([[-1384.6771,  -412.6539,  -341.0964,  -493.3590,  -852.7908,  -607.5254,
          -290.2452]], dtype=torch.float64)
	q_value: tensor([[-27.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7208139422245385, distance: 1.5011484816472478 entropy 7.9601319082913395
epoch: 3, step: 21
	action: tensor([[ -12.1553, -115.0360, -266.6644,  -11.1488,  566.5497,  -82.3150,
          713.1911]], dtype=torch.float64)
	q_value: tensor([[-24.7061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.709707531030153
epoch: 3, step: 22
	action: tensor([[-189.5666, -488.7330, -807.9483, -253.7279, -156.7992,  891.9815,
          316.8811]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8474537696166944, distance: 1.555404972462523 entropy 7.352968730472594
epoch: 3, step: 23
	action: tensor([[-1898.9539, -1073.0980,   212.6553,  -845.6882,  1269.2555,   129.6448,
           481.7150]], dtype=torch.float64)
	q_value: tensor([[-23.6085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5504792545064008, distance: 1.4249173454085586 entropy 7.954937064255822
epoch: 3, step: 24
	action: tensor([[-2035.6501,  -812.3377,  -122.2494,   487.0611,  -164.9141,  1024.1291,
           990.4387]], dtype=torch.float64)
	q_value: tensor([[-28.5130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4268022081043976, distance: 1.3669057791134345 entropy 8.037123045603296
epoch: 3, step: 25
	action: tensor([[-324.8238,  -32.1588,  731.8119, -342.9537, 1179.6975,  428.6213,
          -70.1634]], dtype=torch.float64)
	q_value: tensor([[-25.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09230525075410645, distance: 1.1959931821605907 entropy 8.028078548848745
epoch: 3, step: 26
	action: tensor([[-1497.9497,  -369.2592,    35.1626,   714.6171,   609.1997,   910.3746,
          -436.3254]], dtype=torch.float64)
	q_value: tensor([[-27.1671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5569904545020374, distance: 1.4279061637315356 entropy 7.967724217961565
epoch: 3, step: 27
	action: tensor([[ -373.9287,  -140.1852,  1189.2904,   151.0655, -1004.7703,   851.6211,
            82.2105]], dtype=torch.float64)
	q_value: tensor([[-25.0704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1673731122599632, distance: 1.0441957203614682 entropy 7.91762994196647
epoch: 3, step: 28
	action: tensor([[-1175.3474,  -233.8632, -2760.8441,  -385.7611,   -32.0864,   192.9623,
          -568.5514]], dtype=torch.float64)
	q_value: tensor([[-24.2526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.573511214018394, distance: 1.4354617165689598 entropy 7.883052184674355
epoch: 3, step: 29
	action: tensor([[-1178.8148, -1065.8410,   132.8017,  -317.0671, -1105.3644,  -442.2965,
           907.9290]], dtype=torch.float64)
	q_value: tensor([[-25.7107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005601603132168398, distance: 1.1475448593078232 entropy 7.972935016711402
epoch: 3, step: 30
	action: tensor([[ 143.3484,  393.2365,  150.3814,  758.4627,  295.2557, -552.6987,
          738.4357]], dtype=torch.float64)
	q_value: tensor([[-24.6491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.02476375908339
epoch: 3, step: 31
	action: tensor([[-271.3399, -534.7224,  257.3546,  773.8775,  283.9368, -101.2495,
         -293.0046]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.301474396136315, distance: 1.7360401584104643 entropy 7.352968730472594
epoch: 3, step: 32
	action: tensor([[-738.9027, -116.6243, -188.9614, -820.3522, 1383.4648, -739.4599,
         1342.7921]], dtype=torch.float64)
	q_value: tensor([[-21.1547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32776101665632074, distance: 1.3186108654180886 entropy 7.668915315259382
epoch: 3, step: 33
	action: tensor([[ 672.3959, -903.5735, 1256.0160, -337.4531,  292.5257,  260.9576,
         -200.6333]], dtype=torch.float64)
	q_value: tensor([[-26.4772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23693465361749833, distance: 0.9996260352851996 entropy 7.802284707930006
epoch: 3, step: 34
	action: tensor([[ -222.7754, -1813.8540,  -211.0499,  1078.7440,   754.0449,  -460.8592,
          -916.3398]], dtype=torch.float64)
	q_value: tensor([[-19.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1373166922340925, distance: 1.6729814751073258 entropy 7.798412239481964
epoch: 3, step: 35
	action: tensor([[-1213.3131,  -516.0012,  -636.4293,   560.7429,   825.6365,   311.8673,
          1227.3499]], dtype=torch.float64)
	q_value: tensor([[-23.9135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8118781925712586, distance: 1.5403563109000207 entropy 7.848978731997299
epoch: 3, step: 36
	action: tensor([[  298.8062, -1005.8079,  -360.8082,   170.6723,    18.9807,  -610.4417,
          -399.3691]], dtype=torch.float64)
	q_value: tensor([[-27.4089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5039849105969543, distance: 1.4033901783356215 entropy 7.931767014366431
epoch: 3, step: 37
	action: tensor([[ -697.9422, -1607.7187,   211.1413,    58.5052,   340.6737,  1304.6840,
          -213.4824]], dtype=torch.float64)
	q_value: tensor([[-24.6089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.580629701256298, distance: 1.438705031613992 entropy 7.92191072476246
epoch: 3, step: 38
	action: tensor([[-516.7475,  536.7522, -608.8463,  835.4162,  -75.0582, -214.8591,
         -495.7406]], dtype=torch.float64)
	q_value: tensor([[-25.4160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23829388820042263, distance: 1.2734108853403254 entropy 7.882776015855334
epoch: 3, step: 39
	action: tensor([[ -68.4904, -902.7994,  534.1012, 1741.7905,  117.1356,  -21.2423,
         -307.9001]], dtype=torch.float64)
	q_value: tensor([[-23.6888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2960955995577974, distance: 0.9600935086527388 entropy 7.758047972082887
epoch: 3, step: 40
	action: tensor([[ -273.9188, -1447.7287,   347.4732,  1748.7733,     2.2626,  -757.5225,
           435.2583]], dtype=torch.float64)
	q_value: tensor([[-21.1187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03385470813660896, distance: 1.163553744676374 entropy 7.861702688410519
epoch: 3, step: 41
	action: tensor([[-1690.2906,  -868.1520,  1270.1160,   525.4886,   719.8334,   992.8622,
          -159.0833]], dtype=torch.float64)
	q_value: tensor([[-28.7719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5808724220598254, distance: 1.4388154908303963 entropy 7.983815341677245
epoch: 3, step: 42
	action: tensor([[-2001.3814, -1003.3192,   791.8963,  -449.1116,    -6.7001,   181.1604,
          1966.7127]], dtype=torch.float64)
	q_value: tensor([[-28.1081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.910213438831766, distance: 1.5816035872177239 entropy 8.139007678334703
epoch: 3, step: 43
	action: tensor([[-142.9094, -913.8438,  -80.2959, -458.3505, -466.3297,   53.0065,
          335.9185]], dtype=torch.float64)
	q_value: tensor([[-19.1479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7066635107011681, distance: 1.4949636910282125 entropy 7.631964753673396
epoch: 3, step: 44
	action: tensor([[-1076.9223,  -403.8050,   657.8588,   160.6165,  -313.9457, -1009.4667,
           654.8598]], dtype=torch.float64)
	q_value: tensor([[-23.5413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0988986725753422, distance: 1.6578774701554215 entropy 7.963719506852323
epoch: 3, step: 45
	action: tensor([[-649.1787, -470.9945, -735.1301,  -72.9484, 1303.6337,  280.9599,
         -222.2533]], dtype=torch.float64)
	q_value: tensor([[-21.8110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1301851922584859, distance: 1.0672596533561245 entropy 7.819802646400035
epoch: 3, step: 46
	action: tensor([[  42.1651, -279.7251, -842.0984,  -42.1515,   44.9863, -472.7668,
          332.7390]], dtype=torch.float64)
	q_value: tensor([[-26.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25798399457033905, distance: 1.2834952065594971 entropy 8.008325543412152
epoch: 3, step: 47
	action: tensor([[ -917.7981,   -38.4812, -1458.3043,  -399.4308,   626.3840,    -9.0747,
          -611.5349]], dtype=torch.float64)
	q_value: tensor([[-24.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03167115872809645, distance: 1.1260771005718337 entropy 7.8686992209484
epoch: 3, step: 48
	action: tensor([[  140.8183, -1336.6451,   112.7723,   145.8475,   839.4772,   311.5069,
           128.2250]], dtype=torch.float64)
	q_value: tensor([[-29.9402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.567670775655438, distance: 1.4327952215379147 entropy 7.971670252212521
epoch: 3, step: 49
	action: tensor([[-1056.5849,   290.3225,   210.4312,  -121.0213,  -184.1538,   322.1599,
          1000.9258]], dtype=torch.float64)
	q_value: tensor([[-19.1648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0877627864478836, distance: 1.1935037606910535 entropy 7.8158571724692125
epoch: 3, step: 50
	action: tensor([[-451.4956,  107.1177,  370.7897,  166.2034, -837.6811,  134.3593,
         -240.1812]], dtype=torch.float64)
	q_value: tensor([[-28.9039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32247655749557635, distance: 1.3159842284416878 entropy 7.849603277588476
epoch: 3, step: 51
	action: tensor([[-1382.0055,  -478.6814,   235.1132,   971.7444, -1256.2332, -2075.4554,
           957.5811]], dtype=torch.float64)
	q_value: tensor([[-25.0772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3817167338993801, distance: 1.345136018678239 entropy 7.856196938049196
epoch: 3, step: 52
	action: tensor([[-844.3684,   42.2475, -259.1345,  -51.4508,   30.7371, -748.2023,
          760.9444]], dtype=torch.float64)
	q_value: tensor([[-27.2052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2420015407748224, distance: 1.713462667743097 entropy 8.001244402356077
epoch: 3, step: 53
	action: tensor([[-726.1221, -904.8425, -772.8219,  430.6724,  100.7166,  312.6520,
          107.2768]], dtype=torch.float64)
	q_value: tensor([[-24.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03620248057944431, distance: 1.1234392563689737 entropy 7.835744213938891
epoch: 3, step: 54
	action: tensor([[  147.3769, -1354.8132,   332.6463,   -53.2137,    -6.9662,   857.5319,
          -314.8502]], dtype=torch.float64)
	q_value: tensor([[-21.2915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1806668736575593, distance: 1.035826339622738 entropy 7.724222406048855
epoch: 3, step: 55
	action: tensor([[-159.0376,  122.3266, -410.9912,  938.6265,  -46.5227, -297.6491,
          508.8283]], dtype=torch.float64)
	q_value: tensor([[-17.9124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13846668951403585, distance: 1.0621668184238915 entropy 7.646417190521315
epoch: 3, step: 56
	action: tensor([[-1038.8070,  -743.9451,   637.0076, -1231.9754,  -895.1051,   319.3445,
          -212.9272]], dtype=torch.float64)
	q_value: tensor([[-25.7621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3951698876564587, distance: 1.7710256647087557 entropy 7.991786670439614
epoch: 3, step: 57
	action: tensor([[ -39.3907, -445.3766, -428.7130,  865.3553, 1042.5954, -414.7521,
         -223.4547]], dtype=torch.float64)
	q_value: tensor([[-25.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2102614896880959, distance: 1.7012906754395118 entropy 8.005765535493046
epoch: 3, step: 58
	action: tensor([[-279.1442, -975.3981, 1241.9070, 1132.8254,  288.8436, -136.5471,
          -93.7913]], dtype=torch.float64)
	q_value: tensor([[-22.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8549493988411416, distance: 1.5585571317773774 entropy 7.855963015378661
epoch: 3, step: 59
	action: tensor([[-542.5137,  -23.1976,  504.2777, -731.0048, -278.3831, -205.0826,
         -975.1145]], dtype=torch.float64)
	q_value: tensor([[-24.5148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.928766815399217
epoch: 3, step: 60
	action: tensor([[-227.7973, -137.2157, -532.2077,  367.3390,  342.1729,  550.8740,
          -30.1470]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3538126985908687, distance: 1.3314841009879652 entropy 7.352968730472594
epoch: 3, step: 61
	action: tensor([[-568.0897, -351.5470, -443.0115,  382.7524, -419.7114, -917.5223,
         -710.6936]], dtype=torch.float64)
	q_value: tensor([[-29.9603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005475903472618837, distance: 1.1474731358212886 entropy 7.9843784463196075
epoch: 3, step: 62
	action: tensor([[ 665.8070, -272.1866, -704.0074, 1801.2839,  -44.9898,  124.8936,
         -479.3569]], dtype=torch.float64)
	q_value: tensor([[-25.9219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01945943395015637, distance: 1.1554247543668297 entropy 7.88180381868595
epoch: 3, step: 63
	action: tensor([[  298.7957, -1701.7266, -1916.8830,   581.1084,  -130.8980,  -714.0705,
           359.8439]], dtype=torch.float64)
	q_value: tensor([[-19.1175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3935529043949578, distance: 0.891154805828188 entropy 7.85097718572785
epoch: 3, step: 64
	action: tensor([[ -903.5438,  -138.9717,   155.2075,   -41.0971, -1200.0766,   196.3494,
           100.0702]], dtype=torch.float64)
	q_value: tensor([[-28.0986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2776373129812013, distance: 1.293482289369394 entropy 7.958991841612483
epoch: 3, step: 65
	action: tensor([[  229.3501, -1422.7041,  -692.4765,  -256.6889,   327.1585,  -348.9450,
             4.2291]], dtype=torch.float64)
	q_value: tensor([[-18.9233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33939330266894374, distance: 1.3243743313168492 entropy 7.7431511231249806
epoch: 3, step: 66
	action: tensor([[-352.0825, -731.4196, -234.4101,  -75.4570, -178.0456, -480.9492,
          190.4376]], dtype=torch.float64)
	q_value: tensor([[-20.4645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4867019468065128, distance: 1.395303386622607 entropy 7.665728151788303
epoch: 3, step: 67
	action: tensor([[ -352.0982, -1269.8315,  -669.3611,    50.5187,     5.9755,  -562.3799,
           157.9655]], dtype=torch.float64)
	q_value: tensor([[-21.5671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2808672211266443, distance: 1.7282505160050872 entropy 7.933097022514453
epoch: 3, step: 68
	action: tensor([[ -972.4777, -1097.8866,  -677.6387,  -523.7285,   245.8782,   220.6929,
          -396.0654]], dtype=torch.float64)
	q_value: tensor([[-22.5076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.531182895837151, distance: 1.4160227402348662 entropy 7.841918947224061
epoch: 3, step: 69
	action: tensor([[-447.0754, -604.7801, -117.3841,  245.0312,  -26.6942,   52.7107,
          586.6958]], dtype=torch.float64)
	q_value: tensor([[-19.2191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0759601043364984, distance: 1.6487932270761745 entropy 7.616611734128114
epoch: 3, step: 70
	action: tensor([[ -441.9992, -1358.4079,   -99.3381,   612.7753,  -375.1960,   791.8376,
          -741.1753]], dtype=torch.float64)
	q_value: tensor([[-21.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24390799966654986, distance: 1.2762942824148706 entropy 7.82321461446339
epoch: 3, step: 71
	action: tensor([[-1175.0915,   912.8906, -1460.7807,  1089.1805,   118.0271,  -177.2435,
           344.7885]], dtype=torch.float64)
	q_value: tensor([[-26.4242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5128642526070231, distance: 1.4075268033283759 entropy 7.915959222429575
epoch: 3, step: 72
	action: tensor([[  86.0947, -400.0830, -731.4782,  507.0590,  -69.4388, -334.1413,
          643.7111]], dtype=torch.float64)
	q_value: tensor([[-24.0569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4618452141519096, distance: 1.3835899169997756 entropy 7.860890061128562
epoch: 3, step: 73
	action: tensor([[-813.8429, -532.8040, 1307.5741,  492.9814,  538.2876,  410.3168,
         -639.5206]], dtype=torch.float64)
	q_value: tensor([[-24.1614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2078118500604489, distance: 1.257639983997886 entropy 7.978852019578902
epoch: 3, step: 74
	action: tensor([[ -91.6084, -677.7641,  -19.4313,  580.3879, -589.5840,  232.1466,
          333.9103]], dtype=torch.float64)
	q_value: tensor([[-25.5521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21342841959723802, distance: 1.0149059969533174 entropy 8.02343600148158
epoch: 3, step: 75
	action: tensor([[-795.4942,  230.0343, -553.0048, 1236.2354,  586.7404, -831.4476,
          207.0173]], dtype=torch.float64)
	q_value: tensor([[-23.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01629893952236272, distance: 1.153632359298336 entropy 7.793992549573794
epoch: 3, step: 76
	action: tensor([[-1091.2947,   147.8473,  -120.3411,  -361.6547,   192.2729,   -66.7431,
           -70.0364]], dtype=torch.float64)
	q_value: tensor([[-26.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5174064874573416, distance: 0.7949639468081987 entropy 7.800882888706697
epoch: 3, step: 77
	action: tensor([[   19.2089,  -990.2743,   425.7987,   814.2559, -1069.6500,  -797.3928,
           604.3774]], dtype=torch.float64)
	q_value: tensor([[-26.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05140792772714908, distance: 1.1733898222983878 entropy 7.8115591233314285
epoch: 3, step: 78
	action: tensor([[  723.3818, -1279.3095,   -10.9441,     9.6080,   234.7147,  1420.7911,
          -143.6050]], dtype=torch.float64)
	q_value: tensor([[-20.6358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.844384647496528
epoch: 3, step: 79
	action: tensor([[-486.8089, -274.8559, -466.4855,  439.2543,    9.6173,   31.7677,
          -10.0713]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.056913887697137, distance: 1.6412122447151014 entropy 7.352968730472594
epoch: 3, step: 80
	action: tensor([[ -836.0283, -1454.3515,   538.8467,  1548.3373,   464.2023,  -507.2048,
          -875.4083]], dtype=torch.float64)
	q_value: tensor([[-24.3015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8081080565071057, distance: 1.5387528982039675 entropy 7.884086669852546
epoch: 3, step: 81
	action: tensor([[-602.9828,   50.8737, -403.1808,  927.0353, -195.2043,  925.4419,
         1310.8464]], dtype=torch.float64)
	q_value: tensor([[-26.4417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11232712008871049, distance: 1.07815988503757 entropy 8.068171283779648
epoch: 3, step: 82
	action: tensor([[ -93.6997, -472.2364, -585.1438, -122.9000, -538.3149, -562.4008,
         -232.9134]], dtype=torch.float64)
	q_value: tensor([[-22.1994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9447413629668799, distance: 1.5958336519014364 entropy 7.836189864702168
epoch: 3, step: 83
	action: tensor([[ -687.2802, -1544.3087,  -116.6327,   739.6924,    58.3176,   140.9115,
           691.5848]], dtype=torch.float64)
	q_value: tensor([[-20.5599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5226108079400273, distance: 1.4120534862349203 entropy 7.666429846689058
epoch: 3, step: 84
	action: tensor([[-2122.9896,   235.7035,   506.4724,   316.0180,  1533.1039,    86.5892,
           395.8998]], dtype=torch.float64)
	q_value: tensor([[-28.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11104459428835634, distance: 1.0789384763403151 entropy 7.986626660915241
epoch: 3, step: 85
	action: tensor([[ -421.9404,  -778.8514,   636.8822,   616.2216, -1565.8356,   -28.5110,
           144.3324]], dtype=torch.float64)
	q_value: tensor([[-27.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4904068426389807, distance: 0.8168992308975233 entropy 7.86357035851723
epoch: 3, step: 86
	action: tensor([[   68.0139, -1377.6798,  -802.6386,   247.9614,   588.0129,   601.5503,
           868.7240]], dtype=torch.float64)
	q_value: tensor([[-21.3355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15803221701994508, distance: 1.2314506553406885 entropy 7.828880177412758
epoch: 3, step: 87
	action: tensor([[-1268.2860,   510.7781,  -173.9915,   358.2278,  -520.6704,   449.5830,
          -218.5890]], dtype=torch.float64)
	q_value: tensor([[-24.9146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31686345034284447, distance: 1.3131884831424132 entropy 8.009112155375584
epoch: 3, step: 88
	action: tensor([[-498.2970,  454.1533,  945.1150, -345.6208,  362.6064,  824.9806,
         -373.7611]], dtype=torch.float64)
	q_value: tensor([[-27.9035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42072993038889106, distance: 0.8709580508661262 entropy 7.855004894427145
epoch: 3, step: 89
	action: tensor([[-1279.8356,   173.0341,  -470.7137,  1556.6844,   526.0504,  -516.6903,
           291.4204]], dtype=torch.float64)
	q_value: tensor([[-25.6811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17352989786115725, distance: 1.040327950849832 entropy 7.9263056527983045
epoch: 3, step: 90
	action: tensor([[ -27.2638, -406.9793, -298.9061,  -93.8026, -196.7347,  835.1601,
          -99.5208]], dtype=torch.float64)
	q_value: tensor([[-24.7870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.924965805014433
epoch: 3, step: 91
	action: tensor([[-441.5837, -329.8455,  127.8435,  408.0987,   16.1695, -190.4714,
          -18.7884]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09448379907847904, distance: 1.1971852624356425 entropy 7.352968730472594
epoch: 3, step: 92
	action: tensor([[-667.2834, -296.4900, -932.6300,  320.3560, -158.1623, 1637.5481,
         1390.5371]], dtype=torch.float64)
	q_value: tensor([[-24.6895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1877165477178494, distance: 1.0313605044269512 entropy 8.02473684706177
epoch: 3, step: 93
	action: tensor([[-1295.5841,  -239.5212, -1312.9549,   301.0004,  -427.6829,  1083.0458,
         -1898.1839]], dtype=torch.float64)
	q_value: tensor([[-27.6035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10032053827586096, distance: 1.0854269399662821 entropy 8.056271142087452
epoch: 3, step: 94
	action: tensor([[-275.8823,  -16.7819,  -79.2633,  463.9451,  312.7945,   50.7617,
          219.3591]], dtype=torch.float64)
	q_value: tensor([[-22.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.80262136891845
epoch: 3, step: 95
	action: tensor([[ 212.0943,  331.5494,  376.3517,  465.2708, -436.8072,  -35.2255,
          325.7742]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.352968730472594
epoch: 3, step: 96
	action: tensor([[-123.9867, -612.7038,  731.4670,   13.5714,  218.5920,  517.4520,
          409.0945]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3821221799856924, distance: 1.3453333601876145 entropy 7.352968730472594
epoch: 3, step: 97
	action: tensor([[  663.5962, -1632.8409,   220.4277,   807.0266,  -507.0481,   412.8173,
          1638.2887]], dtype=torch.float64)
	q_value: tensor([[-24.0354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23696606253315178, distance: 0.9996054620190491 entropy 7.904612745664922
epoch: 3, step: 98
	action: tensor([[-1496.5616,   -57.0988,  -121.8169,   -86.4359,   582.4702,   918.4601,
           881.1723]], dtype=torch.float64)
	q_value: tensor([[-25.5529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6374974808610268, distance: 1.4643571549405194 entropy 7.9204390369786974
epoch: 3, step: 99
	action: tensor([[ -72.5147, -777.2473, -553.5316,  443.0630,  688.3600,  702.9930,
         -394.1092]], dtype=torch.float64)
	q_value: tensor([[-26.3199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7997739393879433, distance: 1.5352025134319396 entropy 8.04784342416236
epoch: 3, step: 100
	action: tensor([[-1328.6973, -1011.6656, -1189.0105,     6.8520,   326.9855,  -366.2482,
           473.1530]], dtype=torch.float64)
	q_value: tensor([[-26.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.434527552548496, distance: 1.7855172087746318 entropy 8.077018149568477
epoch: 3, step: 101
	action: tensor([[-1770.8040,   503.3774,   223.0985,   161.9314, -1409.5003,  -332.0833,
          -601.8041]], dtype=torch.float64)
	q_value: tensor([[-22.6607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02941074526901788, distance: 1.127390660592289 entropy 7.801153567466392
epoch: 3, step: 102
	action: tensor([[  -45.9852, -2323.4411,    14.2934,  -797.4450,   812.9449,   582.7485,
           588.4635]], dtype=torch.float64)
	q_value: tensor([[-29.3275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8700157568534921, distance: 1.5648738246209122 entropy 8.068949765210002
epoch: 3, step: 103
	action: tensor([[ 580.9866, -284.9000,  124.9791, 1557.0746,  687.4394, 1623.6330,
         1567.6708]], dtype=torch.float64)
	q_value: tensor([[-26.6334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4326406501612995, distance: 0.8619573978387438 entropy 7.999818801315096
epoch: 3, step: 104
	action: tensor([[  270.9640,  -963.2646, -1774.7332,   831.8264,   338.8029,  1008.3954,
          -866.7352]], dtype=torch.float64)
	q_value: tensor([[-27.4836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3572450263171857, distance: 0.9174436862239302 entropy 8.031198892783658
epoch: 3, step: 105
	action: tensor([[ -582.8782, -1575.5642,   591.2705,    50.4387,   -32.7364,   584.0163,
           974.9123]], dtype=torch.float64)
	q_value: tensor([[-20.7562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44165757221292057, distance: 1.3740032246725342 entropy 7.782893592476357
epoch: 3, step: 106
	action: tensor([[-564.8231, -246.7023,   55.6372, -419.1081,   22.9216,  408.5213,
          469.6986]], dtype=torch.float64)
	q_value: tensor([[-20.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5012179880305159, distance: 1.4020986562199487 entropy 7.726197695443597
epoch: 3, step: 107
	action: tensor([[ -653.0061, -1026.8662, -1464.6529,  -179.6508,   124.5572,   347.0229,
           405.9941]], dtype=torch.float64)
	q_value: tensor([[-21.0888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1425542720604036, distance: 1.22319335334934 entropy 7.799225731790568
epoch: 3, step: 108
	action: tensor([[-1449.3699,   -29.6760,  -555.8233,    77.7785,  -377.0767,  -320.3955,
           197.7745]], dtype=torch.float64)
	q_value: tensor([[-25.1776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28992547787488654, distance: 1.2996876843883234 entropy 7.922513530937047
epoch: 3, step: 109
	action: tensor([[ -963.7567, -1646.8378,  -686.3304,  -675.1353,   381.0069,   821.9116,
           338.8770]], dtype=torch.float64)
	q_value: tensor([[-27.6243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10349775374478198, distance: 1.0835086537819254 entropy 7.996747280943302
epoch: 3, step: 110
	action: tensor([[-804.2860,  536.4007,  614.1392, -794.7974, -228.5265, -471.3304,
         -342.1736]], dtype=torch.float64)
	q_value: tensor([[-26.4791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.967437095613455
epoch: 3, step: 111
	action: tensor([[-986.5261, -565.7351, -100.6642,  335.4278, -450.7314,  423.4900,
          414.5993]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26283887810481876, distance: 1.2859694905627668 entropy 7.352968730472594
epoch: 3, step: 112
	action: tensor([[-750.0768, -237.8812,  202.0320,  437.5145,  -75.3071,  -35.7975,
          108.5976]], dtype=torch.float64)
	q_value: tensor([[-23.8822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.000301666186692362, distance: 1.1441716359970806 entropy 7.774876529188055
epoch: 3, step: 113
	action: tensor([[-746.1135, -389.6590, -197.0202,   20.0168,   16.5478, -303.1024,
          504.2658]], dtype=torch.float64)
	q_value: tensor([[-19.7760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35411066587428297, distance: 1.3316306193627943 entropy 7.71006014559732
epoch: 3, step: 114
	action: tensor([[ -564.7254,  -491.5283,  -260.2324,  -511.4850,   529.4395, -1041.3004,
           523.2748]], dtype=torch.float64)
	q_value: tensor([[-22.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2331819877397041, distance: 1.2707797321847505 entropy 7.9309643371206855
epoch: 3, step: 115
	action: tensor([[ 150.0628, -591.3803,   -6.8991,  981.5091, -700.0455, -229.8919,
          -71.7048]], dtype=torch.float64)
	q_value: tensor([[-27.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40029691371203935, distance: 0.8861858989137897 entropy 7.925535252910848
epoch: 3, step: 116
	action: tensor([[ 206.9873, -637.9839, -815.3790,  294.3902, -723.9017,  268.8172,
          973.3353]], dtype=torch.float64)
	q_value: tensor([[-25.3966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.516214759866348, distance: 0.7959448927750095 entropy 8.043203680057895
epoch: 3, step: 117
	action: tensor([[ -500.8087,  -345.5804,   -17.6828,   398.9714,  -323.2171,  -272.7151,
         -1026.8320]], dtype=torch.float64)
	q_value: tensor([[-21.6489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3617914135590773, distance: 0.9141932592311713 entropy 7.805016364495235
epoch: 3, step: 118
	action: tensor([[-1468.2683,  -538.1813,   551.2923,   618.0588,  -981.8755,  -596.3891,
           673.9040]], dtype=torch.float64)
	q_value: tensor([[-26.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19318639571674234, distance: 1.250002379684035 entropy 8.088861272342637
epoch: 3, step: 119
	action: tensor([[-1126.7786,   903.0608,  -562.3940,   315.0216,  -914.8628,   660.8015,
          1252.7701]], dtype=torch.float64)
	q_value: tensor([[-23.4877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3838552555158645, distance: 1.3461765685223264 entropy 7.993043181369063
epoch: 3, step: 120
	action: tensor([[ -579.5881,   507.9438,   293.1021,   391.1467,  -767.8931,   261.1294,
         -1189.2210]], dtype=torch.float64)
	q_value: tensor([[-35.5662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04975966260872955, distance: 1.1724697150540262 entropy 8.063260567474895
epoch: 3, step: 121
	action: tensor([[ -405.6240,  -422.2675,   -29.2928,   351.3251, -1168.7535,  2223.8711,
           565.2941]], dtype=torch.float64)
	q_value: tensor([[-28.4469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14489993395127143, distance: 1.0581936796452975 entropy 8.019629962018575
epoch: 3, step: 122
	action: tensor([[ 302.3191, -985.5322,  644.0664,  408.8111, -651.7542,  870.4199,
         -179.2728]], dtype=torch.float64)
	q_value: tensor([[-25.2675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20033179161558068, distance: 1.2537396121217599 entropy 7.948713903507547
epoch: 3, step: 123
	action: tensor([[ -894.8011,   -31.0728,   922.7760,  -417.3853, -1468.0565,   128.4637,
         -1344.3793]], dtype=torch.float64)
	q_value: tensor([[-28.9210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7328998271829057, distance: 1.50641080638928 entropy 8.065838295776905
epoch: 3, step: 124
	action: tensor([[  15.4774, -288.9052,  -66.4899, 1160.5583,  243.9832,  141.1713,
          223.1178]], dtype=torch.float64)
	q_value: tensor([[-24.4628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.760188191271725
epoch: 3, step: 125
	action: tensor([[ 377.5347, -175.3102, -301.5678,   25.8906,   21.7342,   85.9835,
          -97.5383]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.352968730472594
epoch: 3, step: 126
	action: tensor([[  19.9911, -433.3729, -109.8434,  280.6635, -159.3756, -363.5131,
          325.3516]], dtype=torch.float64)
	q_value: tensor([[-24.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07591457987757844, distance: 1.186985980805292 entropy 7.352968730472594
epoch: 3, step: 127
	action: tensor([[ -81.5687,   71.6325,  778.5832,   97.7552,  338.0089,  675.9624,
         -744.3781]], dtype=torch.float64)
	q_value: tensor([[-22.3432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3440245879775605, distance: 1.3266620321905416 entropy 7.973403210431063
LOSS epoch 3 actor 264.84847044373055 critic 329.1028494313239
epoch: 4, step: 0
	action: tensor([[-1466.5035,  -122.3935,  -398.3682,  -508.2874,  -780.3412,  -927.8779,
           376.6414]], dtype=torch.float64)
	q_value: tensor([[-18.3020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4977884059544384, distance: 1.4004961701191097 entropy 7.966558378599383
epoch: 4, step: 1
	action: tensor([[-386.7702,  311.0272, -117.8624,  269.2096, -159.8871,  365.7458,
         -581.1823]], dtype=torch.float64)
	q_value: tensor([[-20.2306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05456330561365497, distance: 1.1126867600098564 entropy 7.697877296549963
epoch: 4, step: 2
	action: tensor([[-331.6625, -619.8698,   64.9428,  857.7450,  126.1795,  202.1536,
         1095.4390]], dtype=torch.float64)
	q_value: tensor([[-22.3544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011680495086879, distance: 0.9566279588041872 entropy 8.073285374553455
epoch: 4, step: 3
	action: tensor([[-283.8693, -858.2299, -260.8739, 1776.1642,  346.8608, -150.5681,
           24.4476]], dtype=torch.float64)
	q_value: tensor([[-23.6657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24015669398508588, distance: 0.9975133442542982 entropy 8.091560045744924
epoch: 4, step: 4
	action: tensor([[ 415.8620, -567.7045,  879.9253,  116.1991,  348.6635, -761.9702,
         -569.1687]], dtype=torch.float64)
	q_value: tensor([[-25.0251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4609293577747564, distance: 0.8401939184494837 entropy 8.064423272751181
epoch: 4, step: 5
	action: tensor([[-757.6075,  -29.4703, -424.6977, 1052.1044,   49.1465,  110.4242,
          -45.4125]], dtype=torch.float64)
	q_value: tensor([[-27.9960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.084920695471894
epoch: 4, step: 6
	action: tensor([[-438.5192, -152.7507, -217.7338,  250.6645,  185.4971, -854.7175,
          263.6377]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7606935263892425, distance: 1.5184432908144925 entropy 7.481410575868874
epoch: 4, step: 7
	action: tensor([[-1291.9161,   765.0555,  -372.5868,  -562.4126,  -649.5736,   120.7306,
           228.5060]], dtype=torch.float64)
	q_value: tensor([[-24.6499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5233823929750061, distance: 0.7900266365080839 entropy 8.104389814332713
epoch: 4, step: 8
	action: tensor([[ 701.3228, -920.6471,  559.4546,  -43.4653,  451.8063, 1163.3754,
          394.8469]], dtype=torch.float64)
	q_value: tensor([[-27.8709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.012392958567379
epoch: 4, step: 9
	action: tensor([[-835.0835, -674.8261,  -97.6620, -522.3554, 1029.4792,   50.3344,
          884.1174]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4970535400029674, distance: 1.4001525624323432 entropy 7.481410575868874
epoch: 4, step: 10
	action: tensor([[  -1.0454, -447.0993,  709.8604, -186.0202, -855.8380,  179.7432,
           92.4941]], dtype=torch.float64)
	q_value: tensor([[-24.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06899334042302785, distance: 1.104162738138014 entropy 8.0472519644328
epoch: 4, step: 11
	action: tensor([[-1754.5195,   242.8560,   254.4882,  -496.0894,   267.6743,  1613.1562,
           999.3714]], dtype=torch.float64)
	q_value: tensor([[-21.7119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32783310456862724, distance: 0.938199620599037 entropy 7.971645117811339
epoch: 4, step: 12
	action: tensor([[   69.9324, -1115.2283,    63.6015,  1497.4702,    -8.2898,  -672.1637,
          -451.9377]], dtype=torch.float64)
	q_value: tensor([[-32.5178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0013785887290700405, distance: 1.14513277237833 entropy 8.14143938839061
epoch: 4, step: 13
	action: tensor([[ 311.2573, -482.6013, -738.0537,  270.6216,  720.4241,  477.0591,
         -671.1353]], dtype=torch.float64)
	q_value: tensor([[-16.1787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39900702044058844, distance: 1.3535261172175874 entropy 7.728745697714373
epoch: 4, step: 14
	action: tensor([[ 533.9181,  -30.8144,   63.8367, -273.9753, -256.8074, -600.9796,
         1360.7953]], dtype=torch.float64)
	q_value: tensor([[-24.1098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27224512348497154, distance: 0.9762234942573178 entropy 8.127529397053667
epoch: 4, step: 15
	action: tensor([[ -892.4290, -1259.7253,  1024.8643,  -960.1317,   -77.2011,   447.7306,
           433.0202]], dtype=torch.float64)
	q_value: tensor([[-20.8137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3703446529760397, distance: 1.3395890782038933 entropy 7.871517440853102
epoch: 4, step: 16
	action: tensor([[-1520.6585,  -958.6783, -1010.2842,   544.5164,   731.2967,   180.1809,
           809.4719]], dtype=torch.float64)
	q_value: tensor([[-20.0471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5871743124211402, distance: 1.4416804402807402 entropy 7.924249080368609
epoch: 4, step: 17
	action: tensor([[ -784.1993,  -960.7356,  -153.7141,  -239.5856,   585.6904,  -204.8122,
         -1077.7374]], dtype=torch.float64)
	q_value: tensor([[-20.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46883217561582646, distance: 1.3868924437576757 entropy 7.93711401395703
epoch: 4, step: 18
	action: tensor([[   34.1650, -2438.5564,   911.5387,   283.9037,  -477.9489,   428.2986,
           436.0637]], dtype=torch.float64)
	q_value: tensor([[-25.4275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17485937085119552, distance: 1.039490870010493 entropy 8.04714138110443
epoch: 4, step: 19
	action: tensor([[ -35.9435,  251.8408,  243.0992,  356.3677, -644.6458, -373.7253,
          227.1012]], dtype=torch.float64)
	q_value: tensor([[-15.8189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.706946631127984
epoch: 4, step: 20
	action: tensor([[ -43.2103, -267.2429,  413.1904,  181.5701, -125.7898, -264.5551,
          348.4371]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.274919688753168, distance: 1.725995773762511 entropy 7.481410575868874
epoch: 4, step: 21
	action: tensor([[-1303.7819,  -109.9897,   970.7776,   450.8278,  -419.3268,   247.1501,
           364.6060]], dtype=torch.float64)
	q_value: tensor([[-18.0124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21280172660166596, distance: 1.2602351730127952 entropy 7.7944415874968
epoch: 4, step: 22
	action: tensor([[ -621.0966,   207.9011,  1284.6901,  1759.5588, -1101.3911,   205.0100,
           360.3705]], dtype=torch.float64)
	q_value: tensor([[-25.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2338441853303328, distance: 1.2711208798607678 entropy 8.138860223183334
epoch: 4, step: 23
	action: tensor([[-734.4166,  -95.9714,  753.1357,  445.3860,  993.9620,  740.5026,
         -237.3982]], dtype=torch.float64)
	q_value: tensor([[-26.2764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5393409526370205, distance: 1.4197899739652566 entropy 8.218030104597045
epoch: 4, step: 24
	action: tensor([[-1981.2097,  -181.7427, -1547.2412,   221.5292,  -110.5342,  1169.0814,
           266.8721]], dtype=torch.float64)
	q_value: tensor([[-25.4609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0464223082019584, distance: 1.6370212764199354 entropy 8.023368572711158
epoch: 4, step: 25
	action: tensor([[ -642.0079, -1475.0807,  -206.7935,  1947.3076, -1238.0882,  -720.7970,
          -660.2671]], dtype=torch.float64)
	q_value: tensor([[-28.6787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8916752631910192, distance: 1.5739103295344226 entropy 8.206484590416933
epoch: 4, step: 26
	action: tensor([[  -96.6261,  -610.4454,  -352.6267,   945.4627,   -69.6597,    91.9317,
         -1306.4406]], dtype=torch.float64)
	q_value: tensor([[-20.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027235241134439203, distance: 1.1598228172248266 entropy 7.924576044506326
epoch: 4, step: 27
	action: tensor([[-1595.1881,  -472.3563,  -354.6565,  -159.3045,   553.2269,   166.0501,
          1050.3434]], dtype=torch.float64)
	q_value: tensor([[-18.7586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3807208295593991, distance: 1.3446511623235362 entropy 7.848151451336385
epoch: 4, step: 28
	action: tensor([[-1228.9587,  -755.3230,   -86.0621,  1682.3849,  -161.0622,  1302.1258,
          1142.6570]], dtype=torch.float64)
	q_value: tensor([[-22.2483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23287463822275, distance: 1.2706213622605618 entropy 7.956943342068311
epoch: 4, step: 29
	action: tensor([[ -85.6484,  352.3853, 1912.7482,  264.5592, 1027.1559, -478.3765,
          171.6727]], dtype=torch.float64)
	q_value: tensor([[-25.1596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11985275350140501, distance: 1.0735798721350371 entropy 8.135836521346224
epoch: 4, step: 30
	action: tensor([[  252.4943,  -275.9604, -2036.8327,  1513.2346,  -725.9862,   614.7252,
           206.9606]], dtype=torch.float64)
	q_value: tensor([[-27.3764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41025666286387863, distance: 1.3589571826095639 entropy 8.120396665857843
epoch: 4, step: 31
	action: tensor([[-1358.7342,  -444.5148,   154.5677,   893.3434,   611.3032,   306.3763,
           414.1097]], dtype=torch.float64)
	q_value: tensor([[-23.8354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6912105097079753, distance: 1.4881802193782117 entropy 8.036159458967651
epoch: 4, step: 32
	action: tensor([[  202.7866,  -706.4701,  -583.1623,  -450.3772, -2412.0839,  -592.9684,
           604.7535]], dtype=torch.float64)
	q_value: tensor([[-25.8200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21177915100558964, distance: 1.015969457748818 entropy 8.124501975344062
epoch: 4, step: 33
	action: tensor([[-740.3549,  407.0717, -849.5241,  144.7921, -516.5032,  -79.3883,
          -14.2926]], dtype=torch.float64)
	q_value: tensor([[-19.1660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0643225911379246, distance: 1.106928999309692 entropy 7.729425084226506
epoch: 4, step: 34
	action: tensor([[-2210.4176,  -556.8200,   919.2905, -1493.9987, -1508.8533,   109.6520,
           488.1099]], dtype=torch.float64)
	q_value: tensor([[-27.1808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7683129477173896, distance: 1.521725284266207 entropy 8.158736737938685
epoch: 4, step: 35
	action: tensor([[ 188.0326, -591.1661,   -4.5698, -253.8360,   55.3532, -903.8306,
          307.7910]], dtype=torch.float64)
	q_value: tensor([[-23.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1363453263930403, distance: 1.2198652457257844 entropy 8.013085846405383
epoch: 4, step: 36
	action: tensor([[-1297.9343,   -85.1639,   -50.4253,   607.5168,  -427.9349,   926.2406,
          1140.2995]], dtype=torch.float64)
	q_value: tensor([[-21.0266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2624974806856102, distance: 1.7212769196980016 entropy 7.841772347017326
epoch: 4, step: 37
	action: tensor([[-2084.2931,   247.2844,  -562.7806,   846.6204,  1535.8264,   203.5640,
            25.0879]], dtype=torch.float64)
	q_value: tensor([[-17.8276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48380917903166154, distance: 0.8221703903523795 entropy 7.819964203968503
epoch: 4, step: 38
	action: tensor([[ -224.8158, -1698.3090,  -482.8060,   404.2339,    12.7387,  -611.4207,
          1044.8789]], dtype=torch.float64)
	q_value: tensor([[-26.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.067335872424602
epoch: 4, step: 39
	action: tensor([[-117.5242,  156.8262,   19.2823, -106.8420,  132.2258, 1186.8258,
           65.2684]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48728232709073693, distance: 0.819399768627207 entropy 7.481410575868874
epoch: 4, step: 40
	action: tensor([[-827.7730,  293.2066,  695.0918,   38.0951,  402.8654,  463.3676,
         -528.0313]], dtype=torch.float64)
	q_value: tensor([[-20.7371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17327741161517096, distance: 1.0404868485764476 entropy 7.707772202779649
epoch: 4, step: 41
	action: tensor([[-346.9201,  629.5349, -766.1535, -503.4929, -625.5252,   26.9411,
          272.1936]], dtype=torch.float64)
	q_value: tensor([[-23.9712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.727487021618123
epoch: 4, step: 42
	action: tensor([[-617.0480, -697.6297, -666.2730,  924.4685, -102.5119,  287.5295,
          -89.2179]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6398434496332837, distance: 1.4654057387291552 entropy 7.481410575868874
epoch: 4, step: 43
	action: tensor([[ 219.1844,  142.9209, -955.3967,  630.7061, -246.4094,  635.6588,
          640.6757]], dtype=torch.float64)
	q_value: tensor([[-21.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4639156029560969, distance: 0.8378635100936899 entropy 8.099383097724447
epoch: 4, step: 44
	action: tensor([[-367.0517,  255.0246, -653.8282, -212.5474, -100.4725,  419.4750,
          331.0960]], dtype=torch.float64)
	q_value: tensor([[-17.0512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5417142221885909, distance: 0.7746845295079952 entropy 7.812295257287624
epoch: 4, step: 45
	action: tensor([[-586.4838, -266.8399,  227.2419, -340.3152,  951.3885,  297.0095,
         -146.8896]], dtype=torch.float64)
	q_value: tensor([[-21.8578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25859004632133953, distance: 1.2838043404023838 entropy 7.7136163697227
epoch: 4, step: 46
	action: tensor([[ -303.2761, -1531.7644,   469.2671,  -540.9082,  -996.5393,   384.1105,
          -926.4235]], dtype=torch.float64)
	q_value: tensor([[-23.5235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0923481886455777, distance: 1.0902254919792649 entropy 8.032457679224617
epoch: 4, step: 47
	action: tensor([[-538.3368, -387.3238, -679.6178,  -38.9915, -871.4858, 2450.9699,
          -31.0096]], dtype=torch.float64)
	q_value: tensor([[-25.3165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18419251832908357, distance: 1.2452823988867707 entropy 8.144498382353774
epoch: 4, step: 48
	action: tensor([[ -34.8013, -609.0071, 1911.2804, -228.1865, -579.3537, -127.9995,
         -159.6389]], dtype=torch.float64)
	q_value: tensor([[-21.9259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23852398687286613, distance: 1.2735291918855352 entropy 7.939547071445873
epoch: 4, step: 49
	action: tensor([[ -39.9800, -636.5564, 1181.1587, -321.1394, -674.8874, -360.0850,
         1510.2948]], dtype=torch.float64)
	q_value: tensor([[-21.4342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24920842454858283, distance: 1.2790106050020948 entropy 7.992581398800761
epoch: 4, step: 50
	action: tensor([[-515.3488, -417.5016,  891.7543,   -8.6333,  518.7401,  897.3087,
         1097.0653]], dtype=torch.float64)
	q_value: tensor([[-24.8441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3087390759875044, distance: 0.951431874816041 entropy 8.153563669516048
epoch: 4, step: 51
	action: tensor([[-698.9399, -953.5983,  536.0772,  839.7438,  255.7601,  463.7577,
         1774.1099]], dtype=torch.float64)
	q_value: tensor([[-24.6475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9827657555595599, distance: 1.6113593300515652 entropy 8.12065358222982
epoch: 4, step: 52
	action: tensor([[-409.3655, -284.6675, -377.6758, -858.2400, -376.9701,  133.6879,
         1318.1029]], dtype=torch.float64)
	q_value: tensor([[-21.1790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05208085647398497, distance: 1.1737652624200714 entropy 8.030249615710455
epoch: 4, step: 53
	action: tensor([[-1715.5444,    50.1428,     4.3197,  1704.0851,    54.6349,  -469.4089,
           137.4062]], dtype=torch.float64)
	q_value: tensor([[-20.9788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7206195822740884, distance: 1.5010637045030646 entropy 7.936480992367206
epoch: 4, step: 54
	action: tensor([[  -54.3682, -1137.6797, -1228.5264,   484.6298,   194.0792,  -646.9868,
           493.1115]], dtype=torch.float64)
	q_value: tensor([[-23.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5532355120675714, distance: 1.426183307307778 entropy 7.982414234755211
epoch: 4, step: 55
	action: tensor([[ -22.0357, 1605.9796, -935.4085, 1227.1446,  328.7878,  724.4627,
          -58.2611]], dtype=torch.float64)
	q_value: tensor([[-26.6618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5391098034417324, distance: 0.7768826607148593 entropy 8.17992523120186
epoch: 4, step: 56
	action: tensor([[ -323.6233,   165.8087,  -422.7290,   172.8271,   388.4055,   194.3365,
         -1408.9517]], dtype=torch.float64)
	q_value: tensor([[-24.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03790480445392741, distance: 1.1658306112619727 entropy 7.938437532187024
epoch: 4, step: 57
	action: tensor([[-1357.2463,   828.2021,   595.8781,  -406.9559,   558.3053,   940.2423,
           433.7268]], dtype=torch.float64)
	q_value: tensor([[-22.9520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13985261700973695, distance: 1.2217463313496475 entropy 7.932261767640866
epoch: 4, step: 58
	action: tensor([[-851.7515, -865.0504, -128.5464,   99.4637,  117.4220, -259.3653,
          121.3549]], dtype=torch.float64)
	q_value: tensor([[-22.2689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9357484078221465, distance: 1.5921396157377496 entropy 7.66415878340798
epoch: 4, step: 59
	action: tensor([[-126.7024, -557.0652, -667.6932, -330.4465,  455.7559,  184.8847,
         2287.6435]], dtype=torch.float64)
	q_value: tensor([[-23.5553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13394769091286518, distance: 1.0649488655579085 entropy 8.132765290482626
epoch: 4, step: 60
	action: tensor([[  29.7506, -975.4890,  885.9461,  484.2705, -277.0317,  253.3452,
         -434.6200]], dtype=torch.float64)
	q_value: tensor([[-21.5038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5477185703546517, distance: 1.4236482219015363 entropy 7.996437641447782
epoch: 4, step: 61
	action: tensor([[ -340.1254, -1141.9197,  -195.8150,   189.7038,  -602.3094,   762.2407,
          -752.9509]], dtype=torch.float64)
	q_value: tensor([[-15.8741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14988471837684791, distance: 1.2271109865803849 entropy 7.701028446582689
epoch: 4, step: 62
	action: tensor([[-413.7355, -228.1842, -637.2923,  349.9067,  579.8265, -630.0105,
         -450.0767]], dtype=torch.float64)
	q_value: tensor([[-21.4604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30807487220325513, distance: 0.9518888606663435 entropy 7.902424445852844
epoch: 4, step: 63
	action: tensor([[  568.6314, -1457.2874,  -138.7599,    64.5468,   508.1416,  1820.2288,
          1583.3362]], dtype=torch.float64)
	q_value: tensor([[-26.9684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1705737500083262, distance: 1.042186831237683 entropy 8.182104120438916
epoch: 4, step: 64
	action: tensor([[-544.2499, -971.7665, -329.1416, -851.3330, -490.7929, -420.7589,
          184.8823]], dtype=torch.float64)
	q_value: tensor([[-17.2210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5892528217774926, distance: 1.4426241192007834 entropy 7.828932933718015
epoch: 4, step: 65
	action: tensor([[ -517.0763,  -828.8229,   629.6318,  1558.9018, -1006.3941,  1270.5081,
          -142.5983]], dtype=torch.float64)
	q_value: tensor([[-23.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1346526129491742, distance: 1.064515370527528 entropy 8.044426721116421
epoch: 4, step: 66
	action: tensor([[ -944.6736, -1168.6972,  -811.8946,   319.2080,  -237.1218,   124.4419,
           542.0330]], dtype=torch.float64)
	q_value: tensor([[-21.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5251053812471107, distance: 1.4132097335618568 entropy 8.00762403418195
epoch: 4, step: 67
	action: tensor([[-312.9635,  188.0715, -917.0836, 2491.2689,  423.5994, -907.2108,
          872.5389]], dtype=torch.float64)
	q_value: tensor([[-26.7111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05177709739714209, distance: 1.1735958041557897 entropy 8.139639249920458
epoch: 4, step: 68
	action: tensor([[ -231.8769,   142.9597, -1366.7848,  -149.3425,   599.0213,   470.3057,
           291.2266]], dtype=torch.float64)
	q_value: tensor([[-23.2699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4045166368924942, distance: 1.35618874929216 entropy 7.939027819220172
epoch: 4, step: 69
	action: tensor([[-613.2610,  881.2372, -166.5785, -839.0852,  -50.7523, -238.8899,
          488.9252]], dtype=torch.float64)
	q_value: tensor([[-27.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05553847228242825, distance: 1.1121127739429757 entropy 7.972315941947271
epoch: 4, step: 70
	action: tensor([[ 8.2596e-01,  4.8588e+01,  4.9120e+02,  3.0679e+02, -1.2777e+03,
         -6.8969e+02,  6.5557e+02]], dtype=torch.float64)
	q_value: tensor([[-21.8682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.89394912560383
epoch: 4, step: 71
	action: tensor([[-752.4120, -204.5550,    7.6606,  476.9914, -112.7772,  344.9622,
          178.9263]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20161788269046965, distance: 1.022497120527643 entropy 7.481410575868874
epoch: 4, step: 72
	action: tensor([[ -677.1201,  -552.7332,    62.9121,   535.2952, -1026.0084,   350.7356,
            39.3562]], dtype=torch.float64)
	q_value: tensor([[-21.3281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.696211531702565, distance: 1.4903789191118835 entropy 7.910956767023942
epoch: 4, step: 73
	action: tensor([[-1143.7909,  -946.9683,  1519.3990,  -632.0220,  -380.1876,  -975.3116,
           -13.3642]], dtype=torch.float64)
	q_value: tensor([[-22.4445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1870331724052523, distance: 1.6923273704221122 entropy 8.059016046541426
epoch: 4, step: 74
	action: tensor([[  339.6658, -1523.1336,   433.4711,   441.0682,   367.2105,   778.0188,
           -74.2593]], dtype=torch.float64)
	q_value: tensor([[-23.8192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3376252928245601, distance: 0.9313406599061568 entropy 7.967724873773124
epoch: 4, step: 75
	action: tensor([[   90.3734, -1080.3496,   595.2497,  1273.2672, -1088.4398,   407.3913,
          -434.3736]], dtype=torch.float64)
	q_value: tensor([[-23.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10654511692905522, distance: 1.0816655713711258 entropy 7.989057172748788
epoch: 4, step: 76
	action: tensor([[-2257.5431, -1438.9125,  -239.2459,  -359.8752,   322.0017,   481.5644,
           103.0540]], dtype=torch.float64)
	q_value: tensor([[-17.8506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0623191954769764, distance: 1.1794626910465709 entropy 7.985508848760501
epoch: 4, step: 77
	action: tensor([[-409.4965, -661.5225,  -65.2254,   75.5194, -449.9923,  -84.3748,
         -338.7965]], dtype=torch.float64)
	q_value: tensor([[-17.8403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4674117091583985, distance: 1.7975356151192394 entropy 7.763683805270233
epoch: 4, step: 78
	action: tensor([[-1091.5621,   -99.6185,  -370.3045,  -362.6730,   700.1914,   238.3958,
          -489.6564]], dtype=torch.float64)
	q_value: tensor([[-21.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22230301252107643, distance: 1.0091643534401173 entropy 7.904527439373865
epoch: 4, step: 79
	action: tensor([[  96.1403, -926.0457, 1079.7274,  240.8927, -705.1792,  978.7402,
          -78.2119]], dtype=torch.float64)
	q_value: tensor([[-22.8509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46180947596453437, distance: 1.3835730043685885 entropy 7.970255932338977
epoch: 4, step: 80
	action: tensor([[-181.2312, -802.6760,  406.6874,  695.8483, -229.0715,   36.1837,
          477.5253]], dtype=torch.float64)
	q_value: tensor([[-18.7764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6755288976541856, distance: 1.48126463590614 entropy 7.907632735002388
epoch: 4, step: 81
	action: tensor([[ -652.1574,  -876.3044,   202.5090,  -511.2506,  1502.5396, -1445.4038,
           367.8217]], dtype=torch.float64)
	q_value: tensor([[-24.3899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6024897930868907, distance: 1.4486195076328052 entropy 8.207320913277343
epoch: 4, step: 82
	action: tensor([[ 104.5751,  -17.5432, -321.8767, 1426.7384, -899.1901, -905.8789,
         1468.7896]], dtype=torch.float64)
	q_value: tensor([[-23.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6545956873590917, distance: 1.4719824676562925 entropy 8.111715898186555
epoch: 4, step: 83
	action: tensor([[ -360.5663,  -369.6388,  -130.8338,   402.7303, -1187.1049,   440.7052,
           666.6952]], dtype=torch.float64)
	q_value: tensor([[-19.0785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18783510161648698, distance: 1.0312852374311028 entropy 7.905136110821361
epoch: 4, step: 84
	action: tensor([[   65.9574, -1272.5460,  -569.4999,   653.7705,   707.1954,     6.3583,
          -303.9984]], dtype=torch.float64)
	q_value: tensor([[-20.6248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.384472241335582, distance: 0.8978018891099464 entropy 7.868427727256055
epoch: 4, step: 85
	action: tensor([[ -824.2746, -1106.8108,   755.9243, -1023.5093,  -379.1557,  -919.7478,
          -409.9423]], dtype=torch.float64)
	q_value: tensor([[-23.2938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5629621670115752, distance: 1.4306418531244243 entropy 8.09925196716397
epoch: 4, step: 86
	action: tensor([[-1026.2674, -1088.7701,  -229.4685,  -189.5153,  -414.0979,  1542.2191,
          1615.7627]], dtype=torch.float64)
	q_value: tensor([[-24.6216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1166114721828553, distance: 1.664858263696246 entropy 8.039595382237332
epoch: 4, step: 87
	action: tensor([[-683.1647, -360.2943, 1394.1249,  160.1092, -562.2359,  354.0946,
         -567.1967]], dtype=torch.float64)
	q_value: tensor([[-22.0787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39855275718924443, distance: 1.3533063516676076 entropy 7.986677387022942
epoch: 4, step: 88
	action: tensor([[-807.8543, -825.3835,  406.6140,  941.6271,  -77.0399,   33.1107,
         -257.3428]], dtype=torch.float64)
	q_value: tensor([[-24.9296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19902302828675933, distance: 1.253055927847057 entropy 8.09900387205143
epoch: 4, step: 89
	action: tensor([[-1348.3289,   108.0825, -1356.5323,   919.6537,   150.6627,   287.8797,
         -1193.8948]], dtype=torch.float64)
	q_value: tensor([[-20.4682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22270227860817737, distance: 1.008905269994132 entropy 8.024431405684028
epoch: 4, step: 90
	action: tensor([[ 105.0277,  -28.7664,  682.6551,   30.9139,  749.5367, -635.1747,
         -347.4107]], dtype=torch.float64)
	q_value: tensor([[-20.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3222539805486009, distance: 1.3158734816479036 entropy 7.779117477563796
epoch: 4, step: 91
	action: tensor([[ -397.6227,   507.0336,  -777.5779,  -104.9384,  -182.9753, -1536.4690,
            39.5056]], dtype=torch.float64)
	q_value: tensor([[-21.1156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17772952132120257, distance: 1.0376814244104249 entropy 7.917334916478375
epoch: 4, step: 92
	action: tensor([[-361.6226,  674.3796, -635.5783, -401.0855, 1004.8912,  459.2672,
          849.3018]], dtype=torch.float64)
	q_value: tensor([[-25.5187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14757080626254515, distance: 1.0565397734390152 entropy 8.100879959621158
epoch: 4, step: 93
	action: tensor([[-1274.2891,  -465.4705,  -439.9134,  -390.8445,    56.9284,   688.7380,
          -702.5626]], dtype=torch.float64)
	q_value: tensor([[-19.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5938455194180372, distance: 1.4447070969702436 entropy 7.870720409601226
epoch: 4, step: 94
	action: tensor([[-815.1585,  548.4447,  168.8006,  208.0879,    6.5937, -268.6337,
          305.6784]], dtype=torch.float64)
	q_value: tensor([[-18.8334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.811503549091381
epoch: 4, step: 95
	action: tensor([[-306.4361, -493.4886,   99.9300,   46.0683, -498.1442,  266.2693,
         -487.3549]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7780541016415876, distance: 1.5259109126689645 entropy 7.481410575868874
epoch: 4, step: 96
	action: tensor([[-791.8807, -863.0491,  -57.7422,  825.8321, -315.9244,  287.6508,
          236.8460]], dtype=torch.float64)
	q_value: tensor([[-19.9827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30625350012600006, distance: 1.3078876137622661 entropy 7.906445302625392
epoch: 4, step: 97
	action: tensor([[   17.6413,  -865.3082,   481.5523, -2158.8814, -2171.0763,  -754.5570,
          -968.0005]], dtype=torch.float64)
	q_value: tensor([[-23.1363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7670112648433887, distance: 1.521165098241607 entropy 8.249321724906505
epoch: 4, step: 98
	action: tensor([[ -146.7797, -1676.9750,   572.8804,   828.4073,   937.3346,   392.0278,
          -640.7049]], dtype=torch.float64)
	q_value: tensor([[-24.4530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17510333830299518, distance: 1.0393371867000276 entropy 7.985170199347479
epoch: 4, step: 99
	action: tensor([[-257.2090, -631.8991, -685.3636,  919.5246,  173.3267, -147.8955,
          191.8364]], dtype=torch.float64)
	q_value: tensor([[-21.5191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31653477861568424, distance: 0.9460517715788872 entropy 7.99106067998997
epoch: 4, step: 100
	action: tensor([[ -118.3550, -2487.2835,   475.7478,    15.9257,    90.8114,   412.2194,
          1788.6496]], dtype=torch.float64)
	q_value: tensor([[-23.7865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9335111934876512, distance: 1.5912193031283346 entropy 8.215902011715412
epoch: 4, step: 101
	action: tensor([[-1194.6857, -1597.4648,   -78.4416,    78.7942,  -116.1744,   410.3331,
          -966.0160]], dtype=torch.float64)
	q_value: tensor([[-26.3380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30093688540229646, distance: 1.3052232670571315 entropy 8.175533983326904
epoch: 4, step: 102
	action: tensor([[-1282.1013,  -653.6010,   530.6080,   758.6932,   -44.5926,  -186.9272,
          -467.5998]], dtype=torch.float64)
	q_value: tensor([[-29.2577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33300757907194223, distance: 1.321213500007251 entropy 8.386131979705123
epoch: 4, step: 103
	action: tensor([[-1172.2621,  -931.8590,    56.8289,  -617.5089,  -190.0959,  -171.2746,
           821.3344]], dtype=torch.float64)
	q_value: tensor([[-26.0567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5413699575405764, distance: 1.4207253782227902 entropy 8.159071538661891
epoch: 4, step: 104
	action: tensor([[-302.3585, -677.7028,  293.6855,  570.1744,   37.0860,  -68.0241,
         -497.3295]], dtype=torch.float64)
	q_value: tensor([[-21.6473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6280171610281438, distance: 1.460112041263055 entropy 7.908717134876858
epoch: 4, step: 105
	action: tensor([[ 133.9020,  450.0962,  -31.9037, -413.6331, -677.7203, 1119.5717,
           59.2705]], dtype=torch.float64)
	q_value: tensor([[-20.4043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.945772982314488
epoch: 4, step: 106
	action: tensor([[ 136.7092, -792.0730, -746.7570,  813.8290,  116.5527,  516.3813,
         -150.1457]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08645783625197856, distance: 1.1927876439171028 entropy 7.481410575868874
epoch: 4, step: 107
	action: tensor([[-1467.6712,  -311.5600,  -300.7003,  1910.9134,  -451.5002,   366.2700,
           573.0059]], dtype=torch.float64)
	q_value: tensor([[-19.4183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24936663516881863, distance: 0.9914495625344133 entropy 7.9992352832804725
epoch: 4, step: 108
	action: tensor([[-933.3730,  292.4821,  176.4259,  366.0745,  958.8818,   72.7199,
          369.6749]], dtype=torch.float64)
	q_value: tensor([[-22.8928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3149965095529913, distance: 0.9471158083425495 entropy 7.987743332730121
epoch: 4, step: 109
	action: tensor([[-1543.8065,  -984.2350,    65.7229,  -462.4157,  1082.7513,   279.6096,
          -196.0768]], dtype=torch.float64)
	q_value: tensor([[-22.9736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5105224976995926, distance: 1.4064370296494288 entropy 8.051674049264976
epoch: 4, step: 110
	action: tensor([[-993.1004,    9.2767, -260.9574, 2061.5701,  674.5423,   89.1253,
          275.6914]], dtype=torch.float64)
	q_value: tensor([[-20.5010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.928715247115977
epoch: 4, step: 111
	action: tensor([[-704.6838, -101.8377, -343.6768,  416.8984, -608.8066, -307.8195,
          846.0957]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.481410575868874
epoch: 4, step: 112
	action: tensor([[ -355.2741,  -555.2154, -1021.4203, -1076.8100,  -483.8517,   139.3860,
          -537.2119]], dtype=torch.float64)
	q_value: tensor([[-22.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8546480441707909, distance: 1.5584305252110533 entropy 7.481410575868874
epoch: 4, step: 113
	action: tensor([[-604.6054, -921.0859, -849.6115,  857.3649,  885.9577,  660.0809,
         -558.8595]], dtype=torch.float64)
	q_value: tensor([[-25.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8361280278199845, distance: 1.5506299688341623 entropy 8.110228043042621
epoch: 4, step: 114
	action: tensor([[-776.6688, -967.4222,  666.9946, -344.6649, -656.6145, -575.9114,
         -370.6762]], dtype=torch.float64)
	q_value: tensor([[-18.3473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37917567699844423, distance: 1.3438985580499077 entropy 7.756791683902668
epoch: 4, step: 115
	action: tensor([[-1485.8048,  -533.6803,  -646.2071,  -449.9311,  -275.0930,   913.0823,
           538.7889]], dtype=torch.float64)
	q_value: tensor([[-23.8482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010875990165382099, distance: 1.1381043027775959 entropy 7.949268416692055
epoch: 4, step: 116
	action: tensor([[-1038.8868, -1459.2909,  -470.9455,   357.5048,    43.8351,   404.9337,
           874.1294]], dtype=torch.float64)
	q_value: tensor([[-20.0460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.895429899068283, distance: 1.575471520006112 entropy 7.837497029075678
epoch: 4, step: 117
	action: tensor([[-611.0536, -594.6240,   70.1463,   23.3171, -239.0647,  526.4126,
          727.1428]], dtype=torch.float64)
	q_value: tensor([[-17.0269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010512828738835744, distance: 1.1383132137166339 entropy 7.688802886268933
epoch: 4, step: 118
	action: tensor([[-1737.0761,  -763.5649,   337.2978,  -144.7337,  -392.7808,    33.5457,
           799.3056]], dtype=torch.float64)
	q_value: tensor([[-20.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0263231811769185, distance: 1.6289623613891742 entropy 7.985818431992961
epoch: 4, step: 119
	action: tensor([[-104.7708, -643.7400, -768.0682, 1704.4431, -698.1481, -813.0574,
          107.6275]], dtype=torch.float64)
	q_value: tensor([[-21.1516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9849814448872654, distance: 1.6122594048072656 entropy 8.052385976501233
epoch: 4, step: 120
	action: tensor([[-398.4626,  329.4624, -199.1439,  107.3543, -683.2755,    8.2528,
          -35.7125]], dtype=torch.float64)
	q_value: tensor([[-19.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7896783255436084, distance: 1.5308906920250902 entropy 7.922023199523047
epoch: 4, step: 121
	action: tensor([[ -432.4914,   629.5221,    38.4761,   499.2284,  -988.2873,  1887.3835,
         -1733.0491]], dtype=torch.float64)
	q_value: tensor([[-28.0395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4799692930851963, distance: 0.8252227412641718 entropy 8.067170673264725
epoch: 4, step: 122
	action: tensor([[-2245.3679,   359.1883,   489.9329,   276.7866,  -830.7832,   687.5053,
           110.9145]], dtype=torch.float64)
	q_value: tensor([[-24.4975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37017161045353486, distance: 0.9081713759964055 entropy 8.11677877825238
epoch: 4, step: 123
	action: tensor([[-476.7498, -952.2409, -525.8492, -236.3167, -728.5771, 1163.6424,
         -154.1414]], dtype=torch.float64)
	q_value: tensor([[-23.2217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44005769969632635, distance: 1.3732406163227233 entropy 7.986213008649334
epoch: 4, step: 124
	action: tensor([[   70.4736, -1503.4422, -1160.2325,   -63.9102,   965.2199,   892.1734,
          -208.1342]], dtype=torch.float64)
	q_value: tensor([[-20.6648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09081201711425724, distance: 1.195175412576157 entropy 7.996889877715335
epoch: 4, step: 125
	action: tensor([[-595.7107, -290.1748, -706.0935, 1306.8928,  962.5353, 1455.5613,
          116.9853]], dtype=torch.float64)
	q_value: tensor([[-20.7876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6414976818944649, distance: 1.4661446843917587 entropy 8.00996158153823
epoch: 4, step: 126
	action: tensor([[-1571.5710, -2042.8169,   -35.8628,  -451.8607, -1713.3187,   222.8154,
          1544.3141]], dtype=torch.float64)
	q_value: tensor([[-26.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6919249537319643, distance: 1.4884945235980944 entropy 8.188165001919023
epoch: 4, step: 127
	action: tensor([[-789.8561, -982.9169, -221.2733, -685.0460,  -23.2096, -808.2812,
          691.1257]], dtype=torch.float64)
	q_value: tensor([[-21.3266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3589172649348018, distance: 1.3339919273200795 entropy 7.976790735719082
LOSS epoch 4 actor 223.17043249931555 critic 312.9372637788895
epoch: 5, step: 0
	action: tensor([[ -570.3210, -2900.0312, -1046.6980,  1319.0606, -1997.9165,   408.2672,
           957.9499]], dtype=torch.float64)
	q_value: tensor([[-31.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1693035973560515, distance: 1.0429845091357 entropy 8.251172721970221
epoch: 5, step: 1
	action: tensor([[-1817.1718,  -402.3743,    99.6810,  -787.7518,  -286.7410,  1528.3124,
            28.5206]], dtype=torch.float64)
	q_value: tensor([[-26.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7014827665702803, distance: 1.492692912121712 entropy 8.277152017334105
epoch: 5, step: 2
	action: tensor([[-7.1862e-01, -6.6093e+02,  5.1639e+02, -3.5966e+02,  9.4379e+02,
         -5.9028e+02, -1.6181e+03]], dtype=torch.float64)
	q_value: tensor([[-24.8152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45145237424374485, distance: 1.3786628984304388 entropy 8.209171567759943
epoch: 5, step: 3
	action: tensor([[-2143.9428,  1427.3122,   626.3160,   900.1454,   394.6550,    44.1363,
          1994.3180]], dtype=torch.float64)
	q_value: tensor([[-24.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5312981988344485, distance: 1.416076054764773 entropy 8.183615327074167
epoch: 5, step: 4
	action: tensor([[ -543.9494,   165.3759,  -144.7386,  1351.7222,   -67.8245,   -99.8451,
         -1611.9345]], dtype=torch.float64)
	q_value: tensor([[-21.4375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08570115777045562, distance: 1.0942102486740741 entropy 7.908528178575561
epoch: 5, step: 5
	action: tensor([[-1741.6597,  -977.6230,  -156.3937,    96.4097,  1240.0773, -1061.9527,
           -21.3383]], dtype=torch.float64)
	q_value: tensor([[-27.7915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3101975941129953, distance: 1.3098606395591852 entropy 8.172567456973868
epoch: 5, step: 6
	action: tensor([[ 776.7480, -608.3006, -672.2106,  166.8460, -636.6605, 1442.8517,
         -747.6697]], dtype=torch.float64)
	q_value: tensor([[-25.6481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.202069368473285
epoch: 5, step: 7
	action: tensor([[ 495.3405, -945.0904, -484.0365,   42.3046, -144.1292, -509.6730,
         -798.3296]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5405313713579409, distance: 1.420338851044194 entropy 7.6054049265184505
epoch: 5, step: 8
	action: tensor([[-581.9498, -628.6951,  888.7800,  264.7540,  543.4707, -256.3974,
          -88.5709]], dtype=torch.float64)
	q_value: tensor([[-15.9569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09122027263871568, distance: 1.0909026795204304 entropy 7.53358085129178
epoch: 5, step: 9
	action: tensor([[ -724.8234, -1208.0221,   856.0215,   611.9644,  -371.7266,  1625.0657,
          -298.9393]], dtype=torch.float64)
	q_value: tensor([[-26.5903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12144077242811702, distance: 1.2118388300456997 entropy 8.196189689004742
epoch: 5, step: 10
	action: tensor([[-1096.2601,   563.4043, -1746.6039,  1722.5272,  -761.3180,  -360.9859,
          -335.2875]], dtype=torch.float64)
	q_value: tensor([[-24.2672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7536599548514495, distance: 0.5679684366534098 entropy 8.267783387158385
epoch: 5, step: 11
	action: tensor([[-1072.1484,  -274.2097,   146.8322,    88.6774,   825.0105,    11.9687,
          -472.9459]], dtype=torch.float64)
	q_value: tensor([[-23.5668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.098318039617386
epoch: 5, step: 12
	action: tensor([[-120.9312, -518.1408, -873.3253,  490.6262, -143.0484, 1157.4833,
           95.5745]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 13
	action: tensor([[-512.5271,  241.1035, -810.2921,  313.0040, -432.1780,  158.6624,
         -197.7973]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08464125474741646, distance: 1.1917900430939699 entropy 7.6054049265184505
epoch: 5, step: 14
	action: tensor([[ -776.1877,  -554.8414,   574.4691,   -46.2226,    95.5312, -1472.3149,
          1187.8503]], dtype=torch.float64)
	q_value: tensor([[-22.7602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20433006812414378, distance: 1.2558259645097585 entropy 7.706980816776999
epoch: 5, step: 15
	action: tensor([[   56.7415, -1499.9656,  -862.8957,   254.1510,  -538.8006,    93.8235,
           482.4000]], dtype=torch.float64)
	q_value: tensor([[-25.5397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0950300735433869, distance: 1.6563489015462785 entropy 8.009318463279824
epoch: 5, step: 16
	action: tensor([[-755.8728, -567.4242,  788.0615, -887.5446,  657.3921, 1807.9376,
          648.0227]], dtype=torch.float64)
	q_value: tensor([[-18.7165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04258014260976806, distance: 1.168453456758591 entropy 7.8766519124105585
epoch: 5, step: 17
	action: tensor([[-1225.5106,  -733.7841,   578.9505, -1062.0876,     4.0231,  -859.6876,
          -933.0867]], dtype=torch.float64)
	q_value: tensor([[-25.9520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3586426970815788, distance: 1.333857154655029 entropy 8.248240020309058
epoch: 5, step: 18
	action: tensor([[  315.5064,   213.4271,   573.9365,  1042.1178, -1329.4120,    87.8360,
           205.8698]], dtype=torch.float64)
	q_value: tensor([[-27.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.29723714287197
epoch: 5, step: 19
	action: tensor([[ -217.3555, -1031.3789,   197.0228,   168.7225,   211.4936,  -170.5910,
            91.5276]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 20
	action: tensor([[-601.8929, -486.9484, -410.6546, -118.5216, -533.1031,   16.4248,
         -435.5209]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 21
	action: tensor([[-557.6199,    9.9915, 1501.4037, -584.2965,  562.1614,   19.0743,
         -729.3700]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 22
	action: tensor([[-128.6352, -230.6862, -281.8271, 1430.6501, -158.3903,  934.3190,
         -568.5044]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4700759768294318, distance: 1.3874795269871827 entropy 7.6054049265184505
epoch: 5, step: 23
	action: tensor([[  659.2759, -1560.5301,   461.4722,  -166.8861,   293.1411,  1155.4883,
          -751.4368]], dtype=torch.float64)
	q_value: tensor([[-21.4552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026791238465652345, distance: 1.1595721345849406 entropy 8.089792415121023
epoch: 5, step: 24
	action: tensor([[ -853.4984,  -649.7965, -1783.2927,  1827.6789,  -488.6349, -1300.6767,
          1095.0498]], dtype=torch.float64)
	q_value: tensor([[-21.0798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1434326597773865, distance: 1.0591011727174622 entropy 8.309358548354735
epoch: 5, step: 25
	action: tensor([[ -757.0870,  -586.0485,   289.8549,   -33.2451,   464.1845, -1961.8720,
         -1017.9787]], dtype=torch.float64)
	q_value: tensor([[-26.3874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.668870776129062, distance: 1.4783186236986874 entropy 8.212679691148422
epoch: 5, step: 26
	action: tensor([[-1932.2446,  -427.6695,   -83.6876,   968.5206,  1373.1158,   -90.5317,
          1153.9711]], dtype=torch.float64)
	q_value: tensor([[-19.9543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09289688695309206, distance: 1.0898959078206822 entropy 7.933408345276121
epoch: 5, step: 27
	action: tensor([[ -393.8277, -2236.1048, -1536.6232, -1229.0122,    97.6113,  -614.1280,
         -1946.1714]], dtype=torch.float64)
	q_value: tensor([[-24.5202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.958990528222343, distance: 1.6016693370260466 entropy 8.283184241686854
epoch: 5, step: 28
	action: tensor([[-2325.7379, -1618.2458,   951.0546,   156.6106,  1120.1086,  1233.8799,
           972.9952]], dtype=torch.float64)
	q_value: tensor([[-27.2596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42429660193891783, distance: 1.3657050406795697 entropy 8.302761703726276
epoch: 5, step: 29
	action: tensor([[ -640.0570, -1025.9001,   533.0247,   957.4685, -1348.2042,  -502.5703,
          -435.7391]], dtype=torch.float64)
	q_value: tensor([[-21.2112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38826183176497464, distance: 0.8950338950772943 entropy 8.053587708938185
epoch: 5, step: 30
	action: tensor([[  161.3272, -2080.5246,  -874.0583,   746.1811,   362.9544,   490.4353,
           992.7168]], dtype=torch.float64)
	q_value: tensor([[-25.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18984929117277627, distance: 1.248253152003592 entropy 8.263546967158137
epoch: 5, step: 31
	action: tensor([[ -933.3425, -1188.9651,  -136.8221,  -692.1934,  -369.0857,   999.2267,
         -1449.0194]], dtype=torch.float64)
	q_value: tensor([[-19.0048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5153386994906906, distance: 1.4086774112988667 entropy 8.00975030003633
epoch: 5, step: 32
	action: tensor([[-800.4012, -340.3258,  491.9905,  571.3881,   49.8507, -441.5266,
         1057.3987]], dtype=torch.float64)
	q_value: tensor([[-21.5387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03246648688032383, distance: 1.125614558945758 entropy 8.050415105755148
epoch: 5, step: 33
	action: tensor([[-1044.2508, -1069.9139,  -847.0295,  -957.9264, -1557.2157,  -506.3773,
           875.3173]], dtype=torch.float64)
	q_value: tensor([[-22.0761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8622920917880137, distance: 1.5616388071433405 entropy 8.090503916183248
epoch: 5, step: 34
	action: tensor([[ -231.8320,  -482.7762,  -284.0808,  1576.5673, -1341.2112,  1589.6588,
           937.1383]], dtype=torch.float64)
	q_value: tensor([[-22.2942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1413270576792867, distance: 1.674550293343207 entropy 8.023738159157164
epoch: 5, step: 35
	action: tensor([[ -356.2704, -1784.2359, -1282.4958,  1287.6002,  -827.5093,  1093.9884,
           653.1527]], dtype=torch.float64)
	q_value: tensor([[-23.7824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.601644138106417, distance: 1.4482372293912196 entropy 8.229605522239089
epoch: 5, step: 36
	action: tensor([[-1130.0729,   503.4311,  -407.4146,  1090.1108, -1439.6324,   264.2575,
          -885.3351]], dtype=torch.float64)
	q_value: tensor([[-24.5863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10437865990099249, distance: 1.0829761933062787 entropy 8.126030616889707
epoch: 5, step: 37
	action: tensor([[-1637.6563, -1157.2724, -1312.4201,  -434.3992, -1021.6489,  -141.3880,
           914.9573]], dtype=torch.float64)
	q_value: tensor([[-22.4496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1463907282352768, distance: 1.6765290578754544 entropy 7.971766445774251
epoch: 5, step: 38
	action: tensor([[ -164.2149, -1520.5787,   453.8099,   381.5451,  -117.1871,   884.6641,
          -564.6670]], dtype=torch.float64)
	q_value: tensor([[-23.7093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022225688083903794, distance: 1.156991287153992 entropy 8.0630758424362
epoch: 5, step: 39
	action: tensor([[-999.5891, -577.2514, -870.0162,  850.0513, -408.8015, 1195.1884,
          853.7693]], dtype=torch.float64)
	q_value: tensor([[-24.8657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11083975116743594, distance: 1.0790627797602268 entropy 8.1572870513052
epoch: 5, step: 40
	action: tensor([[-1173.9807,    55.9836,  -151.5914,   248.8250,   361.2301,   888.1032,
            70.7375]], dtype=torch.float64)
	q_value: tensor([[-24.6872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9033141298292895, distance: 1.5787447855664267 entropy 8.223609739799809
epoch: 5, step: 41
	action: tensor([[ 1714.7887, -1319.1382,  -954.7710,  1198.3619, -1688.5484,  -468.4557,
          1213.4667]], dtype=torch.float64)
	q_value: tensor([[-26.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3724485727175383, distance: 0.9065282742238726 entropy 8.318396139693974
epoch: 5, step: 42
	action: tensor([[ -829.5081, -1210.7972,   -51.6315,  -611.7271,  -156.7929,  -675.5291,
           523.8670]], dtype=torch.float64)
	q_value: tensor([[-22.5396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08243814068805277, distance: 1.19057905096029 entropy 7.90244915293813
epoch: 5, step: 43
	action: tensor([[  26.7872, -385.6400, -199.7656, -416.7693, -499.5294,  394.2332,
          199.8489]], dtype=torch.float64)
	q_value: tensor([[-22.6401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23310071750960215, distance: 1.0021341431626474 entropy 8.017652173514088
epoch: 5, step: 44
	action: tensor([[-1480.7408, -1154.9957,   490.3677,   226.3950,   -26.6990,  -608.9393,
           105.3929]], dtype=torch.float64)
	q_value: tensor([[-22.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3906662778710155, distance: 1.3494852905170707 entropy 8.150797970544371
epoch: 5, step: 45
	action: tensor([[ -855.6605, -1538.3429,   592.8419,   308.9066,  -396.3359,   607.2911,
           163.5104]], dtype=torch.float64)
	q_value: tensor([[-22.8583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20246924275665157, distance: 1.0219518018196414 entropy 7.993134249747492
epoch: 5, step: 46
	action: tensor([[-571.6252, -858.3075, -306.8233, 1243.7562,  117.5483,  909.6492,
           29.7059]], dtype=torch.float64)
	q_value: tensor([[-20.9043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5281538415569258, distance: 1.4146214271940911 entropy 8.022418488197488
epoch: 5, step: 47
	action: tensor([[-1080.0222,   478.6040,  1436.5843,  1495.4981,   606.9592, -1555.6701,
          -122.7872]], dtype=torch.float64)
	q_value: tensor([[-23.7976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.243364044588855
epoch: 5, step: 48
	action: tensor([[ -708.2769,    70.5376, -1703.2782,  -420.8177,    78.4758,    -7.9171,
           -47.9072]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 49
	action: tensor([[  16.1875, -165.0042,   11.1278,  -18.9029,  889.9395,  859.9558,
         -111.3809]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 50
	action: tensor([[ 282.1639, -591.9270,  919.7651,  216.1910,  293.9059,  524.5748,
         -599.1086]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2535517084519888, distance: 0.9886818406844626 entropy 7.6054049265184505
epoch: 5, step: 51
	action: tensor([[-1963.6960,  -872.3972, -1806.7938,   901.5938,  -307.8532,   312.1002,
         -1560.5980]], dtype=torch.float64)
	q_value: tensor([[-21.3741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013743065801616439, distance: 1.1521808204555217 entropy 8.093252777187386
epoch: 5, step: 52
	action: tensor([[-981.2785,  875.9659,  -34.2806, -706.8516, 1855.7268,  318.9300,
          213.8520]], dtype=torch.float64)
	q_value: tensor([[-26.3172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.408831930571779, distance: 0.8798571702067726 entropy 8.324848235002603
epoch: 5, step: 53
	action: tensor([[-1015.6743,    -7.5207,    83.4770,    66.2214,  1222.6164,  1046.5974,
           643.5227]], dtype=torch.float64)
	q_value: tensor([[-30.4478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9761845949526626, distance: 1.6086829098005504 entropy 8.096998297934793
epoch: 5, step: 54
	action: tensor([[-1479.8924,   313.4699, -1187.3203, -1882.6285,    89.4823,  -381.6698,
          -362.3555]], dtype=torch.float64)
	q_value: tensor([[-25.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2852460596918538, distance: 1.2973281264207794 entropy 8.233998931203567
epoch: 5, step: 55
	action: tensor([[-922.1406,  -15.1037,  589.1742,  -89.9436,  -76.0669,  958.8296,
         -934.9205]], dtype=torch.float64)
	q_value: tensor([[-26.2326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2811013268052369, distance: 1.295234589315863 entropy 7.953612043707953
epoch: 5, step: 56
	action: tensor([[  194.4863,   -40.5931,  1409.5301,    -3.9024, -1535.1492,   602.8795,
           131.9452]], dtype=torch.float64)
	q_value: tensor([[-20.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.906704959071409
epoch: 5, step: 57
	action: tensor([[ -91.3022, -646.2846,  399.4547,  613.1106,  811.0865,   87.1426,
         -358.4834]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03135234748734739, distance: 1.1621447482972724 entropy 7.6054049265184505
epoch: 5, step: 58
	action: tensor([[-245.7178, -148.6907,  273.7855, 1030.8011,   70.4879, -468.7733,
          808.5489]], dtype=torch.float64)
	q_value: tensor([[-24.4714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18623209823934717, distance: 1.0323024808512988 entropy 8.170826886178377
epoch: 5, step: 59
	action: tensor([[-703.4919, -726.0651,  -11.1173, -483.5063, 1357.8343,  291.3602,
         2871.6580]], dtype=torch.float64)
	q_value: tensor([[-24.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3825126408444186, distance: 1.3455233806255853 entropy 8.246539321382864
epoch: 5, step: 60
	action: tensor([[-833.5917,  490.2997,  -53.3783, 1141.0373, -221.2472, -815.6261,
         -499.8049]], dtype=torch.float64)
	q_value: tensor([[-19.8798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6844898886670343, distance: 1.4852203716402264 entropy 7.952476921207683
epoch: 5, step: 61
	action: tensor([[-1543.3886,   414.1267,  -986.9912,  -810.3196, -1042.4058,  1984.1292,
          1000.4667]], dtype=torch.float64)
	q_value: tensor([[-27.6628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03128075803957209, distance: 1.1263040776870001 entropy 8.283381455607268
epoch: 5, step: 62
	action: tensor([[ -955.3392,     3.6220,  1487.2137,  -187.7939,   538.5454, -1147.5833,
          -152.4635]], dtype=torch.float64)
	q_value: tensor([[-27.4155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.944315618340302
epoch: 5, step: 63
	action: tensor([[ -687.1215, -1042.4280,   350.1227,   552.9856,  -681.7374,   394.3289,
            57.4177]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 64
	action: tensor([[-739.8565, -468.7508,  554.9072,  848.0265, -502.1633,  187.7305,
          755.2818]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11526267026537385, distance: 1.0763756616372262 entropy 7.6054049265184505
epoch: 5, step: 65
	action: tensor([[-1277.6201, -1075.3634,   201.5219,  1069.0208,    26.3267,   142.1857,
           150.2694]], dtype=torch.float64)
	q_value: tensor([[-24.8685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12872584774940177, distance: 1.0681545833855353 entropy 8.181282218976458
epoch: 5, step: 66
	action: tensor([[ -521.1620,  -130.6260,  -158.9979,   579.9274, -1054.9972,  -292.1429,
           100.7445]], dtype=torch.float64)
	q_value: tensor([[-22.6960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0672789331845847, distance: 1.645342184976414 entropy 8.074470220499366
epoch: 5, step: 67
	action: tensor([[  14.5857,  -98.7158,  -99.9291,  366.2721,  -26.9724,  342.2149,
         -547.3253]], dtype=torch.float64)
	q_value: tensor([[-17.7081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6691475467811879, distance: 0.6582242417314895 entropy 7.840234584903039
epoch: 5, step: 68
	action: tensor([[  10.2671,  878.9218, -294.5806, 1834.2342,  123.5657,  -20.8909,
         -294.7638]], dtype=torch.float64)
	q_value: tensor([[-18.5732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.964103153480208
epoch: 5, step: 69
	action: tensor([[541.4265, -22.4967, 280.3385, 454.2283, 792.8499,  14.2163, 297.0203]],
       dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 70
	action: tensor([[-352.7106, -372.7135, -179.3897,  428.9751,  467.1427,  560.6304,
          730.6448]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.756057569778906, distance: 1.5164429207965304 entropy 7.6054049265184505
epoch: 5, step: 71
	action: tensor([[ -897.1250, -2173.6052,   877.0004,   905.3484,   801.4039,  -473.5699,
           -33.7067]], dtype=torch.float64)
	q_value: tensor([[-25.8421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014873206942045414, distance: 1.1358023391468022 entropy 8.22283002682241
epoch: 5, step: 72
	action: tensor([[-2445.9990,  -682.2647,  -183.9807,   815.5158, -1323.3986,   320.2901,
           655.0482]], dtype=torch.float64)
	q_value: tensor([[-29.4489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8680016093421694, distance: 1.5640308542154686 entropy 8.506486480998802
epoch: 5, step: 73
	action: tensor([[-2862.5933, -1429.5407,   465.7991,   718.6383,   -88.6551,   262.3085,
          -665.6999]], dtype=torch.float64)
	q_value: tensor([[-28.0324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10789238897604991, distance: 1.0808497229111862 entropy 8.248827759728545
epoch: 5, step: 74
	action: tensor([[-958.8692, -703.8166, -475.0730, -529.7411, -616.7135,  275.0214,
         -376.1231]], dtype=torch.float64)
	q_value: tensor([[-22.1501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7707838631422221, distance: 1.5227880887574787 entropy 8.048963496986039
epoch: 5, step: 75
	action: tensor([[-829.2365, -378.5252, -159.9986,  315.7703, -377.5009, -393.3703,
           72.0208]], dtype=torch.float64)
	q_value: tensor([[-19.8595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2789309354662206, distance: 1.2941369565498002 entropy 7.9407639697004955
epoch: 5, step: 76
	action: tensor([[ -764.4860, -2172.3023,  -208.7711,  1177.2834, -1728.8222, -1729.3741,
           792.0815]], dtype=torch.float64)
	q_value: tensor([[-24.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.843737405753104, distance: 1.5538397477016803 entropy 8.197885209781825
epoch: 5, step: 77
	action: tensor([[-929.2282,   12.9257, -313.5461,  399.4857,  348.9487, -185.0106,
          379.6096]], dtype=torch.float64)
	q_value: tensor([[-22.8119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2601508762098954, distance: 0.9843017906348884 entropy 8.134258220922332
epoch: 5, step: 78
	action: tensor([[-703.3832, -536.0822,  704.1982,  -48.4387,  401.1333, -207.4052,
         -727.6032]], dtype=torch.float64)
	q_value: tensor([[-25.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4118609951447687, distance: 1.3597299494479023 entropy 8.27131331908697
epoch: 5, step: 79
	action: tensor([[-1.1894e+03, -6.8118e+02,  4.0506e+02,  1.3320e+03,  9.7948e-01,
          9.3381e+01, -9.8655e+02]], dtype=torch.float64)
	q_value: tensor([[-20.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34897993745303335, distance: 0.9233234794667936 entropy 7.997253889072762
epoch: 5, step: 80
	action: tensor([[ -134.3218, -1267.3377,   754.2964,  -930.3739,   232.3768,  -253.5469,
          -559.7458]], dtype=torch.float64)
	q_value: tensor([[-21.7750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4380728822572748, distance: 1.3722939279540531 entropy 8.06847683954341
epoch: 5, step: 81
	action: tensor([[-1227.9413,  -104.8165,  -214.7671,    51.1740,  -969.0544,  1986.9137,
           718.4638]], dtype=torch.float64)
	q_value: tensor([[-19.0418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4832622000400093, distance: 1.8032999978439794 entropy 7.771809843188245
epoch: 5, step: 82
	action: tensor([[-1577.0596,   151.0499,  -276.1773,   140.4170,  -530.1197,  -568.9781,
           279.1699]], dtype=torch.float64)
	q_value: tensor([[-19.0393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38087350615175275, distance: 0.9004226015380402 entropy 7.9296253523782285
epoch: 5, step: 83
	action: tensor([[-933.5482,  321.1129,  -18.7186, 1305.5222, -155.6352,  581.8183,
          152.4413]], dtype=torch.float64)
	q_value: tensor([[-25.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5819093037775559, distance: 0.7399322302848741 entropy 8.23711440841863
epoch: 5, step: 84
	action: tensor([[  384.4804,  -482.1832,   257.2500,  1403.1507,   580.1865, -1192.2073,
          -575.3922]], dtype=torch.float64)
	q_value: tensor([[-24.9443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13914911338491576, distance: 1.0617460618619994 entropy 8.049059175005436
epoch: 5, step: 85
	action: tensor([[-547.0228, -513.3528,  478.3767, -201.7071,  -15.7343,  -22.0470,
         -546.3318]], dtype=torch.float64)
	q_value: tensor([[-25.6261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40900591368621586, distance: 1.3583544229638986 entropy 8.094437515714063
epoch: 5, step: 86
	action: tensor([[-608.5792, -633.3191, -131.7784,  746.9959,  -48.0493,  328.0060,
          708.1794]], dtype=torch.float64)
	q_value: tensor([[-25.4009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37914092205506233, distance: 1.3438816249596093 entropy 8.150766970519502
epoch: 5, step: 87
	action: tensor([[ -256.9051, -1810.8905,  -173.7792,   537.3026,    21.6305,  1653.4827,
           863.6648]], dtype=torch.float64)
	q_value: tensor([[-21.7080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2083144034300941, distance: 1.2579015998615641 entropy 8.047520519894444
epoch: 5, step: 88
	action: tensor([[-1567.8914, -2095.6136,   593.4129,  -221.8410,  -419.2371,   864.7971,
          1211.2414]], dtype=torch.float64)
	q_value: tensor([[-26.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8854444115541618, distance: 1.571316097123359 entropy 8.376890979817578
epoch: 5, step: 89
	action: tensor([[-128.5569, -206.6437, -856.1381,  247.9177,  574.7666, -741.6199,
         -310.9529]], dtype=torch.float64)
	q_value: tensor([[-23.5384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20429327236933037, distance: 1.0207824817777742 entropy 8.171032799327953
epoch: 5, step: 90
	action: tensor([[-2095.2498, -1391.0657,   446.2047,   589.0995,  -417.8352,   393.5318,
          1044.0148]], dtype=torch.float64)
	q_value: tensor([[-27.4154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8863727161885788, distance: 1.5717028707949539 entropy 8.349496961205775
epoch: 5, step: 91
	action: tensor([[ -812.5461,   -60.1260,  2001.7082,   122.3818,  -114.4103,  -337.2315,
         -1951.4146]], dtype=torch.float64)
	q_value: tensor([[-28.3549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7541234679381144, distance: 1.515607594232946 entropy 8.267159462468303
epoch: 5, step: 92
	action: tensor([[ -795.4535, -2462.0519, -1038.0387,   314.5946, -1548.9586,  -352.8275,
          2716.6210]], dtype=torch.float64)
	q_value: tensor([[-24.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03611626063910833, distance: 1.1234895058754817 entropy 8.261791113757045
epoch: 5, step: 93
	action: tensor([[  -33.9775,  -975.9291, -1403.2214,   608.6136,   614.6148,   796.5272,
         -1208.1733]], dtype=torch.float64)
	q_value: tensor([[-29.2388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17428580235707547, distance: 1.0398520906421733 entropy 8.294334804623862
epoch: 5, step: 94
	action: tensor([[-2332.8005,  -526.7005,  1629.9818,   492.4876, -1628.3988,  -486.1543,
          -977.0346]], dtype=torch.float64)
	q_value: tensor([[-24.3315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1689622506666406, distance: 1.2372485018881347 entropy 8.330487190432203
epoch: 5, step: 95
	action: tensor([[ -606.4371, -1933.7709,   749.1322,  1327.5711, -1976.5556,   565.3919,
          -237.5132]], dtype=torch.float64)
	q_value: tensor([[-27.5560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1635849277647652, distance: 1.2343994990506602 entropy 8.342317214456894
epoch: 5, step: 96
	action: tensor([[ -268.7428, -1363.6176,   327.2316,  1309.3582, -1671.5225, -1259.3147,
           407.8007]], dtype=torch.float64)
	q_value: tensor([[-26.2550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7387238340128635, distance: 1.5089400892668197 entropy 8.219697227157296
epoch: 5, step: 97
	action: tensor([[-370.8981,  508.0244, -738.5231,   62.4210,  200.2189,  227.4096,
          631.0038]], dtype=torch.float64)
	q_value: tensor([[-21.5576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.002180578350576
epoch: 5, step: 98
	action: tensor([[-1.9342e+02, -4.5080e-01, -4.8766e+02, -1.9936e+02, -3.0587e+02,
          1.7675e+01, -2.0061e+02]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0415675127209938, distance: 1.6350783434938738 entropy 7.6054049265184505
epoch: 5, step: 99
	action: tensor([[-477.1804,  868.1266, -518.4693,   18.3326,  488.9747,  -69.8995,
         -625.2999]], dtype=torch.float64)
	q_value: tensor([[-20.8116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23835557900073645, distance: 0.9986948857648654 entropy 7.964682911363481
epoch: 5, step: 100
	action: tensor([[-1539.1524, -1439.3045,   492.6630,  1319.4528,  1220.0441,    62.3914,
         -1255.8032]], dtype=torch.float64)
	q_value: tensor([[-25.1899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.520100969797518, distance: 0.7927415644761425 entropy 8.150020528388897
epoch: 5, step: 101
	action: tensor([[-1031.4701, -1200.7374,   886.7390,   418.7396,  1396.4399,  1204.3749,
            50.7513]], dtype=torch.float64)
	q_value: tensor([[-22.4108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1468999115755284, distance: 1.0569554604615543 entropy 8.117300334630674
epoch: 5, step: 102
	action: tensor([[-1219.2286, -1403.5370,  -671.5412,  -882.7757,   532.4461,   650.3605,
          -454.2731]], dtype=torch.float64)
	q_value: tensor([[-23.3519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7731948969975395, distance: 1.5238244221069162 entropy 8.093619867719282
epoch: 5, step: 103
	action: tensor([[-442.0768,  650.8212,  681.9270,  425.3867, -385.8271,  -48.6545,
         1072.5489]], dtype=torch.float64)
	q_value: tensor([[-23.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.153281138746907
epoch: 5, step: 104
	action: tensor([[-1371.0726,   552.2056,  -612.9293,   477.2856,  1060.3012,   470.3685,
           136.3884]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 105
	action: tensor([[-421.0050, -207.2149, -487.3401,  -60.1692, -412.3534, -538.0319,
         -160.5130]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 106
	action: tensor([[-493.6143, -370.2666,  179.8965,  290.9575,  500.2313,  473.3036,
         -564.3566]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.6054049265184505
epoch: 5, step: 107
	action: tensor([[-147.6555, -770.5322, -440.1623,  426.0805,  221.1125,  826.8938,
         -113.3429]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.464382813877235, distance: 1.384790274946616 entropy 7.6054049265184505
epoch: 5, step: 108
	action: tensor([[  -18.0144,  -134.2225,  1273.3389, -1764.9945,  -397.6904,  1092.6032,
          -246.7482]], dtype=torch.float64)
	q_value: tensor([[-22.0402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24375671797414866, distance: 1.2762166698301949 entropy 8.008915500950947
epoch: 5, step: 109
	action: tensor([[  173.5599,  -430.6891, -1108.1047,  -109.5457,   376.1762,  -306.2861,
          -714.6205]], dtype=torch.float64)
	q_value: tensor([[-18.8593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5200475159441285, distance: 0.7927857132540814 entropy 7.96650847245582
epoch: 5, step: 110
	action: tensor([[ 319.9208,   13.3903,  262.2319,   88.4660,   33.8074, -606.7183,
         -326.1141]], dtype=torch.float64)
	q_value: tensor([[-22.4643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5983778006454596, distance: 0.7252129461628999 entropy 8.075416673999655
epoch: 5, step: 111
	action: tensor([[-1327.6528,  -217.6243,     6.0299,  -607.9215,  1197.2400,   393.5217,
           571.0101]], dtype=torch.float64)
	q_value: tensor([[-26.1858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22415407633219686, distance: 1.0079626382910645 entropy 8.062966028693944
epoch: 5, step: 112
	action: tensor([[ -593.6828, -2400.2198,   -99.5050,   604.5956,   810.9324, -1010.9592,
          -939.0069]], dtype=torch.float64)
	q_value: tensor([[-21.6301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3187781015087396, distance: 1.7425541727983294 entropy 8.090366080743136
epoch: 5, step: 113
	action: tensor([[-533.5336, -889.9502,  112.8817,  155.6583,  419.0021, -314.7926,
          -78.4704]], dtype=torch.float64)
	q_value: tensor([[-21.1062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2731236066276932, distance: 1.2911954256207945 entropy 7.909139325725998
epoch: 5, step: 114
	action: tensor([[-753.6042,   96.8657,   28.6617,  669.7677, -544.5421, 1189.0899,
         1074.7018]], dtype=torch.float64)
	q_value: tensor([[-24.9853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6840816943039321, distance: 1.485040407569165 entropy 8.252569186247163
epoch: 5, step: 115
	action: tensor([[ -354.4960, -2377.4060,   873.4697, -1695.6018,   700.6542,  -422.2129,
           239.7108]], dtype=torch.float64)
	q_value: tensor([[-30.8385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.322909239503165, distance: 1.7441057503903223 entropy 8.289675991031162
epoch: 5, step: 116
	action: tensor([[ 278.4934,  273.2569, 2013.8456, -870.2594, -182.6958, -295.8776,
          841.2026]], dtype=torch.float64)
	q_value: tensor([[-23.9562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.166868168242598
epoch: 5, step: 117
	action: tensor([[ -171.4459,  -575.4863,   693.0800, -1256.6980,  -150.0754,   -21.0111,
         -1401.4576]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11678280054003043, distance: 1.0754505652210562 entropy 7.6054049265184505
epoch: 5, step: 118
	action: tensor([[ -21.1305,  141.3567,  525.8026,  162.8741, -418.7902, -335.0840,
         1602.2576]], dtype=torch.float64)
	q_value: tensor([[-25.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37735715694910077, distance: 1.3430122631959696 entropy 8.157877507511884
epoch: 5, step: 119
	action: tensor([[ 113.7240, -720.7413,  461.1593, -142.9480,   33.0647,  466.0997,
         -285.2649]], dtype=torch.float64)
	q_value: tensor([[-27.0838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.963682905570233
epoch: 5, step: 120
	action: tensor([[-500.4979,  318.6355,  524.9603,  575.0255, -973.6107,  408.1568,
          367.3971]], dtype=torch.float64)
	q_value: tensor([[-23.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2896992619202424, distance: 0.9644458002112335 entropy 7.6054049265184505
epoch: 5, step: 121
	action: tensor([[ -661.8674, -1100.5764,  -817.9412,   336.5314,  -289.7640,  -313.7143,
          1483.4165]], dtype=torch.float64)
	q_value: tensor([[-29.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8164112974815014, distance: 1.5422820018300563 entropy 8.142605618970908
epoch: 5, step: 122
	action: tensor([[ -315.9125, -1206.5708,  -322.6868,  -515.9815,   366.9044,  1197.4503,
            80.2962]], dtype=torch.float64)
	q_value: tensor([[-22.0453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7166316591770863, distance: 1.499323168927025 entropy 8.079874989305805
epoch: 5, step: 123
	action: tensor([[ -558.8702, -1589.0225,  2023.6818,   -88.5775,   508.5527,   554.2103,
          2315.6345]], dtype=torch.float64)
	q_value: tensor([[-26.3352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5998576155728306, distance: 1.4474293001277863 entropy 8.214392315335115
epoch: 5, step: 124
	action: tensor([[-1207.0393, -1657.6261,    -7.4577,  -486.2203,  1198.5054,   462.8260,
           661.1213]], dtype=torch.float64)
	q_value: tensor([[-23.7939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019264363667291007, distance: 1.1332681189791503 entropy 8.145609210235818
epoch: 5, step: 125
	action: tensor([[   30.6621,  -942.3101,  -664.6962,   -46.3203,   917.6680, -1343.2993,
           837.8320]], dtype=torch.float64)
	q_value: tensor([[-25.2488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14068847170148013, distance: 1.0607963393464444 entropy 8.156217258522151
epoch: 5, step: 126
	action: tensor([[-1181.5883,  -690.5077, -1082.8257,  -198.8049,  -851.0722,  -106.0398,
           759.7685]], dtype=torch.float64)
	q_value: tensor([[-19.9422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2124236775743169, distance: 1.2600387403212736 entropy 7.993716703240706
epoch: 5, step: 127
	action: tensor([[ -22.4998, -372.3456, -322.3251,  366.4934, -200.1292, -596.2913,
         -158.9636]], dtype=torch.float64)
	q_value: tensor([[-27.8607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4458350859750828, distance: 1.3759925199609637 entropy 8.18216497055299
LOSS epoch 5 actor 255.37412188951743 critic 372.53201944675936
epoch: 6, step: 0
	action: tensor([[ -46.5931, 1650.0121, -578.0194, -720.5372, -182.8347,  485.0020,
          719.1665]], dtype=torch.float64)
	q_value: tensor([[-26.8991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13462934352247613, distance: 1.064529682985296 entropy 8.086616492285762
epoch: 6, step: 1
	action: tensor([[ 654.2921,  858.3275,  375.2051, -190.6173, -340.3244, 1379.3418,
          782.4836]], dtype=torch.float64)
	q_value: tensor([[-30.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8408504035823274, distance: 0.4565196378683896 entropy 8.135479435152488
epoch: 6, step: 2
	action: tensor([[  -43.7588,  -641.7975, -1476.5864,  -408.2830,  1211.8966,   436.8455,
           243.3316]], dtype=torch.float64)
	q_value: tensor([[-28.5189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.217862531599458
epoch: 6, step: 3
	action: tensor([[-289.9600, -797.5958,   33.5138, -313.9966, -651.6543,  506.6175,
            4.5003]], dtype=torch.float64)
	q_value: tensor([[-28.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.725967362020684
epoch: 6, step: 4
	action: tensor([[   58.2659,   149.0623, -1686.7101,  -657.9344, -1199.4106,   545.4940,
          -535.0576]], dtype=torch.float64)
	q_value: tensor([[-28.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.725967362020684
epoch: 6, step: 5
	action: tensor([[-370.8365, -540.7028,   85.9125, -512.3830,  145.5518,   13.4930,
         -436.8021]], dtype=torch.float64)
	q_value: tensor([[-28.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09780569093951497, distance: 1.086942912196227 entropy 7.725967362020684
epoch: 6, step: 6
	action: tensor([[-1446.9081, -1689.5925,    -3.9944,  1810.9071,   234.9055,  -478.9853,
          -794.7254]], dtype=torch.float64)
	q_value: tensor([[-29.3751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.178025699532111, distance: 1.2420356907782553 entropy 8.184194791239898
epoch: 6, step: 7
	action: tensor([[ -854.2424,  -293.5899,   608.9677,  -544.8244,   208.6273,  -241.0287,
         -1554.5598]], dtype=torch.float64)
	q_value: tensor([[-32.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5927695866377225, distance: 1.444219386548891 entropy 8.384256756339274
epoch: 6, step: 8
	action: tensor([[   57.4932,   355.5883, -2057.2591,   119.1075,  -723.2089,    28.8309,
          1424.7042]], dtype=torch.float64)
	q_value: tensor([[-31.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13189150549568152, distance: 1.2174723131922993 entropy 8.182127568952845
epoch: 6, step: 9
	action: tensor([[-768.3612, -952.0905, -236.9236,  -56.6184, -433.4372,  698.6743,
          535.0019]], dtype=torch.float64)
	q_value: tensor([[-25.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6531406980909513, distance: 1.8639610547745304 entropy 7.922912900998948
epoch: 6, step: 10
	action: tensor([[-1213.6204, -1335.2921,   250.3474,  -631.7185, -1196.7261,   707.2964,
           922.5027]], dtype=torch.float64)
	q_value: tensor([[-20.9180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10515094439363715, distance: 1.2030051674698157 entropy 7.914298365173623
epoch: 6, step: 11
	action: tensor([[  68.0552, -828.2968, 1536.8750,  995.5338,  945.1669,  293.0874,
         -568.8098]], dtype=torch.float64)
	q_value: tensor([[-28.7514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2022044920254853, distance: 1.2547172433290275 entropy 8.264654913154045
epoch: 6, step: 12
	action: tensor([[  130.8660,  -899.5660, -1624.6015,  1740.9663,  -667.4258,  -551.3544,
         -1016.6187]], dtype=torch.float64)
	q_value: tensor([[-31.4360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3666418071901616, distance: 1.3377779837715031 entropy 8.335437356890486
epoch: 6, step: 13
	action: tensor([[-2220.6219,   458.6265,  -177.5637, -1182.7399, -1324.7904,  -596.6811,
           987.3161]], dtype=torch.float64)
	q_value: tensor([[-30.4216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43770225882777347, distance: 0.858103874141899 entropy 8.315876111652882
epoch: 6, step: 14
	action: tensor([[-572.2315,  695.3267, 1390.1405,   42.4531,  448.4440,  396.7205,
         -210.0054]], dtype=torch.float64)
	q_value: tensor([[-36.4690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5368317309442424, distance: 0.7788002687521582 entropy 8.18597792961059
epoch: 6, step: 15
	action: tensor([[-1217.7498,  -462.5148,  -838.4064,  -673.8898, -1261.8605, -1193.9471,
           933.3822]], dtype=torch.float64)
	q_value: tensor([[-30.8962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07004636406240794, distance: 1.1035381247625953 entropy 8.270917633587812
epoch: 6, step: 16
	action: tensor([[ 1083.3197, -1033.6190,   385.5634,  1102.3643,  -466.7138,  -212.9103,
          1312.4989]], dtype=torch.float64)
	q_value: tensor([[-38.7077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33296598368020947, distance: 1.3211928861627829 entropy 8.395567823917846
epoch: 6, step: 17
	action: tensor([[-1733.7742,   623.8884, -1054.1687,   986.6637,  1214.4621,   141.3645,
          1764.4228]], dtype=torch.float64)
	q_value: tensor([[-29.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23092288281802964, distance: 1.0035560608116019 entropy 8.177577758574714
epoch: 6, step: 18
	action: tensor([[ -855.3236, -1794.3606,   627.1590,  -946.5351, -1293.2949,   174.2237,
           203.7808]], dtype=torch.float64)
	q_value: tensor([[-26.9756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6086946642205844, distance: 1.4514213392891768 entropy 8.165617565701648
epoch: 6, step: 19
	action: tensor([[-501.5794, -748.5695, -512.4207, 2606.1281, 1666.3285,  755.1599,
         -179.0610]], dtype=torch.float64)
	q_value: tensor([[-26.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4185396923784803, distance: 1.3629422028159812 entropy 8.256954561265276
epoch: 6, step: 20
	action: tensor([[-2290.6295, -1957.7645,   -19.8347,   -13.3474,   367.6970,  -868.1328,
           687.9594]], dtype=torch.float64)
	q_value: tensor([[-31.9400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7945097105134662, distance: 1.5329556824635027 entropy 8.483788186055985
epoch: 6, step: 21
	action: tensor([[ -300.6900,   -48.0044,   299.2158,   478.0696,  -132.8690,    87.0052,
         -1710.5307]], dtype=torch.float64)
	q_value: tensor([[-32.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4698290823758846, distance: 1.3873630107699155 entropy 8.468160015925518
epoch: 6, step: 22
	action: tensor([[-1862.3845,   100.4854, -1048.9140,   -80.7038,  1270.1059, -1343.4611,
          -228.6956]], dtype=torch.float64)
	q_value: tensor([[-25.1143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0812857386480355, distance: 1.650906766108373 entropy 8.29419642139569
epoch: 6, step: 23
	action: tensor([[ -612.7390, -1600.2922,   198.9523,   246.9058,   787.8224,   784.5568,
          -261.8631]], dtype=torch.float64)
	q_value: tensor([[-28.7112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0607090163131425, distance: 1.6427256141370532 entropy 8.144904856116955
epoch: 6, step: 24
	action: tensor([[ -107.6183, -2844.8743,  -411.1438,  1726.8054,  -831.6152,   464.2053,
           957.8896]], dtype=torch.float64)
	q_value: tensor([[-32.7837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17548528602803204, distance: 1.2406957424338867 entropy 8.384716156047158
epoch: 6, step: 25
	action: tensor([[-2256.3447,  -241.6384, -1151.5782,  -330.9650,   121.1779,   672.7532,
          2230.1168]], dtype=torch.float64)
	q_value: tensor([[-35.0877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0692614980272452, distance: 1.6461309551348846 entropy 8.544822796659426
epoch: 6, step: 26
	action: tensor([[  571.9939,  -948.3217, -1736.3147,  1407.6887,    97.0854,  1821.2304,
          1798.4376]], dtype=torch.float64)
	q_value: tensor([[-27.7220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22799566199422427, distance: 1.26810469239738 entropy 8.270256550765799
epoch: 6, step: 27
	action: tensor([[-3181.4494, -1323.0729,  -524.9650,  -176.2426, -1838.9844,   637.1971,
           762.8004]], dtype=torch.float64)
	q_value: tensor([[-33.6603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.034602162412905, distance: 1.6322867081356356 entropy 8.418981255484814
epoch: 6, step: 28
	action: tensor([[-1412.3474,  -934.5922,  -323.3981,  2105.4615, -1025.1203,  -357.4652,
           885.7475]], dtype=torch.float64)
	q_value: tensor([[-31.0829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24961952102746632, distance: 1.2792210396641268 entropy 8.406288859880958
epoch: 6, step: 29
	action: tensor([[  951.7985,   807.8255,  -317.6431,  1361.2721,   813.5837, -1790.0092,
          1089.4735]], dtype=torch.float64)
	q_value: tensor([[-30.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40393779726070866, distance: 0.8834917225853354 entropy 8.36863332909962
epoch: 6, step: 30
	action: tensor([[-2041.8354, -1763.3634,    52.7856,   518.6623,  1318.8920,  -542.3042,
           953.1608]], dtype=torch.float64)
	q_value: tensor([[-34.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2788813537105568, distance: 1.2941118706743622 entropy 8.510149741082135
epoch: 6, step: 31
	action: tensor([[-1394.3240,   906.5393,   187.0474,   793.7706,  1004.6157,   347.0402,
          2382.8324]], dtype=torch.float64)
	q_value: tensor([[-31.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03374915321868177, distance: 1.163494344668753 entropy 8.33875449696094
epoch: 6, step: 32
	action: tensor([[-1750.9684,  -798.3982,  1002.5203,  -407.5748, -1152.2710, -2131.3467,
           202.5873]], dtype=torch.float64)
	q_value: tensor([[-32.2243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3846370647904065, distance: 1.7671273081278782 entropy 8.291560665170119
epoch: 6, step: 33
	action: tensor([[-1635.2159,  -265.8188,  1707.0368,  -558.0182,  -100.4045,  -154.7407,
           709.9203]], dtype=torch.float64)
	q_value: tensor([[-33.0084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013661082751815545, distance: 1.1521342301455035 entropy 8.38386383410185
epoch: 6, step: 34
	action: tensor([[-1647.5162, -2764.8937,  -551.5046,  -251.1303,  -807.8113,    95.7551,
           513.8815]], dtype=torch.float64)
	q_value: tensor([[-33.2843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3478448460015495, distance: 1.3285461476186053 entropy 8.240829259294438
epoch: 6, step: 35
	action: tensor([[ -258.0212,   329.5082,   846.6389,   718.4446,  1108.0468,  1293.1169,
         -1302.2461]], dtype=torch.float64)
	q_value: tensor([[-29.5632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33644262542221615, distance: 0.9321717416183435 entropy 8.329059845068715
epoch: 6, step: 36
	action: tensor([[-2085.4091, -1161.6588,    27.6473, -1788.4890,    90.2241, -1203.9926,
           950.1588]], dtype=torch.float64)
	q_value: tensor([[-32.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7669510146423579, distance: 1.5211391642574656 entropy 8.202959175833906
epoch: 6, step: 37
	action: tensor([[-1389.4390,  -900.6279, -1454.7310,  -962.2666, -1190.1586,  1002.7187,
           730.5625]], dtype=torch.float64)
	q_value: tensor([[-31.6221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06611072263373663, distance: 1.1058707920964959 entropy 8.300579077067468
epoch: 6, step: 38
	action: tensor([[ -483.0221,   739.0035,   -69.9765,    37.1749,   639.2457, -2256.4127,
          -797.2367]], dtype=torch.float64)
	q_value: tensor([[-32.8852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5827070943786611, distance: 0.7392259327549267 entropy 8.440022770938214
epoch: 6, step: 39
	action: tensor([[   3.5445,  810.3706,  935.1912,  297.0123,  139.1225, -820.2883,
          950.3724]], dtype=torch.float64)
	q_value: tensor([[-32.8110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.120755982782253
epoch: 6, step: 40
	action: tensor([[-9.7798e+01, -9.2405e-01, -4.7375e+02,  9.7627e+02,  1.3973e+02,
         -1.7938e+02,  1.9564e+01]], dtype=torch.float64)
	q_value: tensor([[-28.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18515528329916964, distance: 1.2457885119842944 entropy 7.725967362020684
epoch: 6, step: 41
	action: tensor([[ -780.3021, -1580.5919,   295.5042, -1037.0636,  -284.9780, -2401.8983,
          1445.5064]], dtype=torch.float64)
	q_value: tensor([[-29.6450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9501559266407864, distance: 1.5980536738123778 entropy 8.263025839717903
epoch: 6, step: 42
	action: tensor([[ 165.6272, -886.2700, 1049.0462, -759.8320, 1186.5850,  596.0951,
         -897.7569]], dtype=torch.float64)
	q_value: tensor([[-27.8678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49779040680318687, distance: 1.400497105558342 entropy 8.187680151544148
epoch: 6, step: 43
	action: tensor([[ -319.6394, -1497.0178,  -196.4146,  1346.9493,   926.2341,  1210.9113,
          -202.1780]], dtype=torch.float64)
	q_value: tensor([[-18.9284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8081853945955799, distance: 1.5387858063409676 entropy 7.898666246440199
epoch: 6, step: 44
	action: tensor([[ -984.2868,    47.4325,  1262.7242, -2054.1330,   307.8719,   620.6677,
            56.4098]], dtype=torch.float64)
	q_value: tensor([[-28.8901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4347663413823628, distance: 1.3707153719214984 entropy 8.3030867658759
epoch: 6, step: 45
	action: tensor([[ -602.0580, -1018.3462,   797.0739,  1061.4124,   -38.1525,   396.1581,
          -898.9910]], dtype=torch.float64)
	q_value: tensor([[-28.4080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7690661523004028, distance: 1.522049335626625 entropy 7.972139382816152
epoch: 6, step: 46
	action: tensor([[-461.6828, -210.0805, -213.9408,  194.7566, -356.9358, 1383.6366,
         -520.2559]], dtype=torch.float64)
	q_value: tensor([[-22.6637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18401838028011142, distance: 1.2451908347970244 entropy 8.001789515272211
epoch: 6, step: 47
	action: tensor([[ -312.2963,  -459.8054,  -822.7719, -1093.8271,  -149.6883,  1573.4920,
           281.4050]], dtype=torch.float64)
	q_value: tensor([[-27.6506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6708665037071533, distance: 1.479202287004605 entropy 8.199794207314593
epoch: 6, step: 48
	action: tensor([[-1153.5589, -2058.3363,  -310.6652,  -327.5200,   692.8071,   786.6712,
          -776.7507]], dtype=torch.float64)
	q_value: tensor([[-21.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29370051816372067, distance: 1.301588099956317 entropy 7.9639914026904295
epoch: 6, step: 49
	action: tensor([[-1089.8707,   -37.9225,   -74.4539,  -134.0605,  -887.4282,  -608.4122,
           527.8813]], dtype=torch.float64)
	q_value: tensor([[-26.7089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5926745677070075, distance: 1.4441763074275684 entropy 8.247266313353594
epoch: 6, step: 50
	action: tensor([[ -984.1133,  -157.4386,  -465.8796,   246.6502,   552.6827, -1243.0369,
          1165.4552]], dtype=torch.float64)
	q_value: tensor([[-29.6262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34099796022698503, distance: 1.3251674259008124 entropy 8.13028901017455
epoch: 6, step: 51
	action: tensor([[-5.7893e-01, -4.7121e+02, -6.2523e+02, -9.5764e+02,  8.1517e+02,
          1.7265e+03,  2.4338e+02]], dtype=torch.float64)
	q_value: tensor([[-26.6332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3276642402887201, distance: 0.9383174622061023 entropy 8.077527687440833
epoch: 6, step: 52
	action: tensor([[ -427.2171, -1143.0555,  -350.3775,   889.1403,  1166.6936,   326.1086,
          -376.4016]], dtype=torch.float64)
	q_value: tensor([[-32.5037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9782207385130357, distance: 1.6095114422330747 entropy 8.406720874032882
epoch: 6, step: 53
	action: tensor([[-2917.6068, -1195.9333,  1021.5757,  1006.8407, -1549.1937,  -381.1676,
           287.0175]], dtype=torch.float64)
	q_value: tensor([[-26.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30092680005517214, distance: 0.9567930672094908 entropy 8.21981922419165
epoch: 6, step: 54
	action: tensor([[-1246.9865, -1597.6669, -1170.7848,  2884.5660,    73.1493,  -548.4731,
           206.4530]], dtype=torch.float64)
	q_value: tensor([[-33.2312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3161306653843565, distance: 1.312823062310841 entropy 8.52685171989518
epoch: 6, step: 55
	action: tensor([[-1827.6883, -1627.3933,  -803.8241,  -282.4397, -2473.7906,   -25.6610,
          -511.4170]], dtype=torch.float64)
	q_value: tensor([[-29.8954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6056730400845685, distance: 1.847211624901614 entropy 8.40111764921554
epoch: 6, step: 56
	action: tensor([[  917.9639, -1029.4339,   138.4040,  1240.6819,   516.4050,  1374.8446,
          -284.1097]], dtype=torch.float64)
	q_value: tensor([[-26.0170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4281753727489296, distance: 0.8653426735158193 entropy 8.127950235476183
epoch: 6, step: 57
	action: tensor([[-991.5604, -957.9889, 1740.7395,  524.2314, 1066.7114, -569.5576,
         2054.3456]], dtype=torch.float64)
	q_value: tensor([[-22.3745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18377428303782173, distance: 1.24506247390287 entropy 8.110008654181282
epoch: 6, step: 58
	action: tensor([[  888.4357,  -145.9672,  1247.4251,   194.9105, -1033.6103,  -685.4936,
          -564.2393]], dtype=torch.float64)
	q_value: tensor([[-24.9721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38225368682334615, distance: 0.8994184131770324 entropy 8.189409944332866
epoch: 6, step: 59
	action: tensor([[ -721.0049,  -883.5039,   305.7103,  -338.4247, -1216.9889,  1806.9960,
         -2266.3482]], dtype=torch.float64)
	q_value: tensor([[-34.7851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0945959775540541, distance: 1.6561772926339586 entropy 8.31769052975588
epoch: 6, step: 60
	action: tensor([[-444.0051, -167.7975, -478.2525,  152.2108,   20.7753, 2924.3816,
         -111.1178]], dtype=torch.float64)
	q_value: tensor([[-31.8877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9843586061896501, distance: 1.612006441149985 entropy 8.326336527131714
epoch: 6, step: 61
	action: tensor([[-694.6771, -732.6229, -298.4950,  -71.5500, 1310.2273,  446.3270,
          629.4623]], dtype=torch.float64)
	q_value: tensor([[-30.6633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2612171223332531, distance: 1.7207898112745732 entropy 8.304017765427824
epoch: 6, step: 62
	action: tensor([[ 671.3043, -428.5402, -879.4631,  929.0310,  617.7391, 1534.6724,
         -150.8366]], dtype=torch.float64)
	q_value: tensor([[-32.7212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0034288210080289927, distance: 1.1423806935711167 entropy 8.43534827762622
epoch: 6, step: 63
	action: tensor([[ -993.6271, -1867.7501,  -666.3270,   587.3983,   151.1875,  1731.2907,
           993.9445]], dtype=torch.float64)
	q_value: tensor([[-26.2489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09520743185794855, distance: 1.0885069482579637 entropy 8.191106268893067
epoch: 6, step: 64
	action: tensor([[-833.3088,  -28.3679,  -64.1084,  807.3073, 1218.6295,   66.7936,
         -310.1555]], dtype=torch.float64)
	q_value: tensor([[-26.8681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16985407169209799, distance: 1.0426388766338486 entropy 8.186811435248787
epoch: 6, step: 65
	action: tensor([[ -549.1568, -1715.4618,   277.5532, -1390.8835, -1738.5076, -2344.6020,
         -1017.2958]], dtype=torch.float64)
	q_value: tensor([[-31.3862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5804675425744192, distance: 1.838255583156361 entropy 8.380273801213974
epoch: 6, step: 66
	action: tensor([[ 106.2353, -777.3842, -394.8793, 1001.4789, -518.4845,  -30.6296,
          655.7268]], dtype=torch.float64)
	q_value: tensor([[-23.7566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5961709512130124, distance: 0.7272026808768952 entropy 8.033002852665382
epoch: 6, step: 67
	action: tensor([[-1523.4258,  1256.4978,  1249.4983,   944.6309,   772.5705,   -50.1540,
           -10.4778]], dtype=torch.float64)
	q_value: tensor([[-30.3138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6347167337988247, distance: 0.6916264313060019 entropy 8.26022913160257
epoch: 6, step: 68
	action: tensor([[-1660.4569, -1144.2225,  -594.3927,  -540.1547,   632.9901,  1120.7702,
          1190.7582]], dtype=torch.float64)
	q_value: tensor([[-28.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1864104212722948, distance: 1.6920864106975693 entropy 8.198552241810656
epoch: 6, step: 69
	action: tensor([[ -409.5752, -1416.8492,  1440.8868,  -241.0098,   872.3501,   -52.5369,
           178.7617]], dtype=torch.float64)
	q_value: tensor([[-27.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6424175642810266, distance: 1.4665554348276597 entropy 8.302330302951164
epoch: 6, step: 70
	action: tensor([[-1303.8438, -1664.0358,  -880.1467,   -32.1778,  1509.3587,  -253.7294,
          -128.2575]], dtype=torch.float64)
	q_value: tensor([[-28.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4582116028042378, distance: 1.3818692984456549 entropy 8.305192486980408
epoch: 6, step: 71
	action: tensor([[-7.9615e+02, -4.6432e+02, -1.8627e+03,  5.0458e+01,  4.2630e-01,
          3.5183e+02,  5.3260e+02]], dtype=torch.float64)
	q_value: tensor([[-32.0944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1910129546086452, distance: 1.6938664493050226 entropy 8.20601512390033
epoch: 6, step: 72
	action: tensor([[  744.8140, -1226.0629,  -192.7753,  -199.4030,  -436.8790,  -786.5104,
          1012.3446]], dtype=torch.float64)
	q_value: tensor([[-24.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.399695477381225, distance: 0.8866301611190456 entropy 8.011987506664482
epoch: 6, step: 73
	action: tensor([[ -678.7141,  -379.0367,  -831.0778, -1229.1641,  -884.4012,   996.2816,
          2336.1784]], dtype=torch.float64)
	q_value: tensor([[-32.3842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47971545750016054, distance: 1.3920210367998682 entropy 8.366381376492502
epoch: 6, step: 74
	action: tensor([[-1495.9233, -1165.3213,    12.3146,  1500.1402, -1908.6810,  -158.8400,
          1160.3020]], dtype=torch.float64)
	q_value: tensor([[-28.1052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.708365738716425, distance: 1.8832602892872465 entropy 8.233948053652679
epoch: 6, step: 75
	action: tensor([[ 3.5204e+02, -9.9480e+01,  6.1520e+02, -6.5624e+02,  4.6864e+02,
         -3.9559e-01,  1.0358e+03]], dtype=torch.float64)
	q_value: tensor([[-23.6224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023529916953010876, distance: 1.1308009489152353 entropy 8.027747166441006
epoch: 6, step: 76
	action: tensor([[-1558.7977, -1196.2560, -2472.6845, -2444.9594,   -98.8887,   722.0875,
          -511.3655]], dtype=torch.float64)
	q_value: tensor([[-23.3890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.584727945273676, distance: 1.4405689556401837 entropy 8.027879724023496
epoch: 6, step: 77
	action: tensor([[-1887.2491,  1026.7903, -1073.2758,  1118.9279,   428.0410, -1572.2477,
          -458.7697]], dtype=torch.float64)
	q_value: tensor([[-31.6561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40151535763023094, distance: 0.8852851891669327 entropy 8.378127295848218
epoch: 6, step: 78
	action: tensor([[ -926.0119,  -400.0338,   729.6397,  1035.0890, -1013.4152,   799.0448,
           220.0036]], dtype=torch.float64)
	q_value: tensor([[-28.1837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19073237838709378, distance: 1.248716282902767 entropy 8.098257633802183
epoch: 6, step: 79
	action: tensor([[ -251.6852, -1746.5856,   -22.0897,  -959.7563,    88.4333,   176.8263,
           318.4217]], dtype=torch.float64)
	q_value: tensor([[-26.3106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.836795849946236, distance: 1.5509119347004212 entropy 8.222492296963143
epoch: 6, step: 80
	action: tensor([[-406.9703, -484.6860,  -74.7986,  620.5764,  403.2062, 1725.9484,
         -971.3264]], dtype=torch.float64)
	q_value: tensor([[-25.1714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06646233751210817, distance: 1.1056625890254375 entropy 8.147953516176385
epoch: 6, step: 81
	action: tensor([[  328.6305,  -596.6492,   185.3577,   389.7184, -1852.5361,   508.3837,
           456.8305]], dtype=torch.float64)
	q_value: tensor([[-28.0158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043656347077566826, distance: 1.1690563698748033 entropy 8.381122334345031
epoch: 6, step: 82
	action: tensor([[-2227.8136, -1734.4051,   416.9825,  1585.4049,   133.8922, -1097.6895,
           554.6438]], dtype=torch.float64)
	q_value: tensor([[-28.2594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10858740640281561, distance: 1.204874085392542 entropy 8.296936749813225
epoch: 6, step: 83
	action: tensor([[ -916.0081,  -609.4263,  -947.7475,   309.8996, -2824.7063,   429.4861,
          -407.5336]], dtype=torch.float64)
	q_value: tensor([[-33.9658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5781490351318814, distance: 1.4375756247001186 entropy 8.427467410719283
epoch: 6, step: 84
	action: tensor([[-1634.2862,  -631.4398,   583.0192,  -130.4415,  1000.8830,  -671.3926,
           159.3272]], dtype=torch.float64)
	q_value: tensor([[-27.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6342789034729488, distance: 1.462917316058863 entropy 8.242913381527584
epoch: 6, step: 85
	action: tensor([[ -657.4140,  -815.2538, -1453.7337,   117.0615, -1489.7670,  -522.5194,
           494.0400]], dtype=torch.float64)
	q_value: tensor([[-29.5349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7573175431412, distance: 1.5169868480324733 entropy 8.235792161136331
epoch: 6, step: 86
	action: tensor([[ 1780.3463, -1506.9552, -1442.2218,  -548.4667,  -288.9027,   526.1911,
          1974.0903]], dtype=torch.float64)
	q_value: tensor([[-30.3214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36196423062426586, distance: 1.3354866289697305 entropy 8.441371693040226
epoch: 6, step: 87
	action: tensor([[-2190.7282, -1775.7588,  -248.6689,   369.8316, -1130.3669,   613.5001,
          1640.0502]], dtype=torch.float64)
	q_value: tensor([[-26.7478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08393152716128949, distance: 1.1914000594563054 entropy 8.185508046408856
epoch: 6, step: 88
	action: tensor([[-2258.6086, -1870.4225,   362.4893,  3226.0643,  -536.9474,  -173.6789,
          1110.2028]], dtype=torch.float64)
	q_value: tensor([[-28.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1316551184053969, distance: 1.0663574749819436 entropy 8.30522228682889
epoch: 6, step: 89
	action: tensor([[-1229.1792, -2839.0589,   314.9638,  2431.8030,   320.4278,   416.3710,
          2222.9707]], dtype=torch.float64)
	q_value: tensor([[-31.5630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11829829910246215, distance: 1.2101397466758823 entropy 8.44269929713211
epoch: 6, step: 90
	action: tensor([[-314.2410, -130.3390, -136.6052, 2942.1141, -279.2215, 1253.3355,
          -96.6457]], dtype=torch.float64)
	q_value: tensor([[-23.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6003966806506686, distance: 1.4476731320993408 entropy 8.14628943623213
epoch: 6, step: 91
	action: tensor([[   78.5167, -1073.9967,  1327.3086,  -584.3781,   765.9062, -2078.7692,
           -51.0511]], dtype=torch.float64)
	q_value: tensor([[-31.2895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5683080580536446, distance: 1.4330864186473877 entropy 8.504113128765853
epoch: 6, step: 92
	action: tensor([[ -328.5523, -1055.9956,  -173.2295,   601.8994,  -804.7614,  -316.6809,
           576.2928]], dtype=torch.float64)
	q_value: tensor([[-23.7316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37401359598787964, distance: 1.3413811786863707 entropy 7.893265498337451
epoch: 6, step: 93
	action: tensor([[-1657.8461, -1000.4486,  -212.7067,    88.3089,  -102.2371,  1511.9073,
          -823.9228]], dtype=torch.float64)
	q_value: tensor([[-30.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49802263466461727, distance: 1.4006056727652787 entropy 8.332371876483005
epoch: 6, step: 94
	action: tensor([[-2.0897e+02, -2.5282e+03, -3.2984e+02,  3.6369e+02,  2.2269e+00,
         -6.4958e+02,  2.2501e+03]], dtype=torch.float64)
	q_value: tensor([[-24.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1657697827262249, distance: 1.0452006030229053 entropy 8.203674596139145
epoch: 6, step: 95
	action: tensor([[-1012.2359, -1445.9077,  1045.3658,   105.1639, -1158.4777,   490.6285,
          -570.5867]], dtype=torch.float64)
	q_value: tensor([[-32.1562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0196372259715889, distance: 1.1555255020474997 entropy 8.341494202125759
epoch: 6, step: 96
	action: tensor([[ -992.8463, -1053.7967,    59.0724,   222.7085, -1049.3593,  -887.0547,
          -444.4315]], dtype=torch.float64)
	q_value: tensor([[-30.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38326624363046924, distance: 1.345890050699844 entropy 8.397164451251108
epoch: 6, step: 97
	action: tensor([[-274.6458,  794.1646,   55.0580,   11.1469, -229.2020, -277.6613,
         -975.8654]], dtype=torch.float64)
	q_value: tensor([[-30.5557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.31935919194492
epoch: 6, step: 98
	action: tensor([[-1374.5148,  -260.1473,  -249.6600,  -467.9420,   586.4263,   488.4817,
          -313.8034]], dtype=torch.float64)
	q_value: tensor([[-28.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30703240717841807, distance: 1.3082774963798056 entropy 7.725967362020684
epoch: 6, step: 99
	action: tensor([[-860.7778, -528.8468,  448.1582,  854.6822, -335.9653, 1853.4392,
         -993.1755]], dtype=torch.float64)
	q_value: tensor([[-29.8870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.114638415509698, distance: 1.6640821114300044 entropy 8.365121369212961
epoch: 6, step: 100
	action: tensor([[ -943.7082, -1142.8000, -1719.7285,  -927.3968,  1841.9327,  -957.2425,
          1462.2973]], dtype=torch.float64)
	q_value: tensor([[-28.8562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10236850131222996, distance: 1.201489807530916 entropy 8.316329828394739
epoch: 6, step: 101
	action: tensor([[-1219.1631, -1655.3024,  -222.0305,  -917.2711,  -357.0789,  -110.7321,
           155.8558]], dtype=torch.float64)
	q_value: tensor([[-31.5454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11652923007869975, distance: 1.0756049343421559 entropy 8.213594893883526
epoch: 6, step: 102
	action: tensor([[-188.0362, -735.0818, 1108.6218,  987.9634, -634.7201, 1411.7708,
         -682.4919]], dtype=torch.float64)
	q_value: tensor([[-33.4884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1948488359836027, distance: 1.2508728769400084 entropy 8.3440702263995
epoch: 6, step: 103
	action: tensor([[  522.9568,  -961.8357,    98.3116,   867.9476,  -706.3178, -2243.7314,
           490.8082]], dtype=torch.float64)
	q_value: tensor([[-29.6551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3768996614362745, distance: 0.9033076471946574 entropy 8.335025187276289
epoch: 6, step: 104
	action: tensor([[-1178.5450,   403.5782,  -265.3364,  1151.0465, -1940.9142, -1130.8777,
         -1778.2475]], dtype=torch.float64)
	q_value: tensor([[-27.3761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2603122435447305, distance: 1.2846823897911583 entropy 8.375794390843264
epoch: 6, step: 105
	action: tensor([[ -241.5292,   423.8348,   417.2457,   773.7538,  -536.9796, -1057.0914,
          1529.5531]], dtype=torch.float64)
	q_value: tensor([[-26.7192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16107063995944415, distance: 1.0481402290337785 entropy 8.037996625886924
epoch: 6, step: 106
	action: tensor([[-1730.1085, -2675.7478,   341.1218,  2137.4504, -1054.7652,  2062.9502,
         -1042.5843]], dtype=torch.float64)
	q_value: tensor([[-40.2689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13300834492026792, distance: 1.0655262465843807 entropy 8.587104912271034
epoch: 6, step: 107
	action: tensor([[  496.7610, -1592.4856,  -295.2452,  -916.9273,  -224.3252,  -284.0568,
          -530.9530]], dtype=torch.float64)
	q_value: tensor([[-27.0988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42621372849643624, distance: 1.3666238622744524 entropy 8.212196995456555
epoch: 6, step: 108
	action: tensor([[  -53.6991,  -665.3211,  -393.9309,   270.2432,   376.3542, -1566.4051,
          -578.1112]], dtype=torch.float64)
	q_value: tensor([[-24.0558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0927354600952166, distance: 1.1962286826245185 entropy 8.017149793794497
epoch: 6, step: 109
	action: tensor([[-907.2428,  998.1199, -900.5583,  651.5822, -172.1923,  335.9798,
         -447.9567]], dtype=torch.float64)
	q_value: tensor([[-25.0561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3751017755816044, distance: 1.3419122413853517 entropy 8.101224038514049
epoch: 6, step: 110
	action: tensor([[ 227.8504,  364.3292,  105.7054, 1075.9300, -594.5305, -475.6392,
          316.7495]], dtype=torch.float64)
	q_value: tensor([[-33.0695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3866882209241146, distance: 0.8961843304480867 entropy 8.4735298128226
epoch: 6, step: 111
	action: tensor([[-191.4301, -854.2307,  879.4881, 1444.7303,  559.8805, -825.8373,
          -24.7183]], dtype=torch.float64)
	q_value: tensor([[-30.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8086403254112049, distance: 1.5389793697492975 entropy 8.184888713495155
epoch: 6, step: 112
	action: tensor([[ -518.8596,   395.5508,   377.0487,    21.8626,   668.2369, -1287.7784,
          -355.1716]], dtype=torch.float64)
	q_value: tensor([[-24.7002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04206696369197471, distance: 1.1200161082352784 entropy 8.104919745279075
epoch: 6, step: 113
	action: tensor([[-1134.7401,   393.4672,   969.5099,  -670.0194,  -554.8776,   469.8684,
          -215.0685]], dtype=torch.float64)
	q_value: tensor([[-32.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7942407203177236, distance: 0.5190825252155565 entropy 8.276100298780179
epoch: 6, step: 114
	action: tensor([[  860.9762, -1976.0025,  -281.4282,  -142.2432, -1251.1691,   746.9682,
          1091.8545]], dtype=torch.float64)
	q_value: tensor([[-28.9220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.248001983331092
epoch: 6, step: 115
	action: tensor([[  66.4570,  142.0430, -203.7991,  410.6368, -756.5852, -149.8483,
          239.5751]], dtype=torch.float64)
	q_value: tensor([[-28.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5386374621836305, distance: 0.7772806511326659 entropy 7.725967362020684
epoch: 6, step: 116
	action: tensor([[ 233.3900, -388.8152,  576.6273, -951.0632,  508.3224, 2101.5564,
          -74.6096]], dtype=torch.float64)
	q_value: tensor([[-29.2864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.158113606008348
epoch: 6, step: 117
	action: tensor([[-851.4285,  837.3608,  374.8148,  118.7400,  -61.7459,  536.4338,
         1360.3986]], dtype=torch.float64)
	q_value: tensor([[-28.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6267422753875068, distance: 0.6991350823244625 entropy 7.725967362020684
epoch: 6, step: 118
	action: tensor([[  424.8632, -1107.5248,  -764.7474,  -673.5740,   500.0353,   -49.8979,
           401.7793]], dtype=torch.float64)
	q_value: tensor([[-29.5559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.075123329377018
epoch: 6, step: 119
	action: tensor([[-568.3973, -521.0411,  325.8259, -142.9098, -638.5928,  782.8613,
          -15.6585]], dtype=torch.float64)
	q_value: tensor([[-28.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03643212195376533, distance: 1.1650032194862772 entropy 7.725967362020684
epoch: 6, step: 120
	action: tensor([[-2674.6122,   -78.9126, -1058.5828,   554.9008,   195.0687,   144.1158,
         -1864.1921]], dtype=torch.float64)
	q_value: tensor([[-29.2180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8719558764574498, distance: 1.5656853833784528 entropy 8.288287269850866
epoch: 6, step: 121
	action: tensor([[  334.7318, -1163.4476,  -393.5204,    89.2333,  -404.0479,  -116.3121,
           534.8086]], dtype=torch.float64)
	q_value: tensor([[-28.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29492389982919587, distance: 1.3022033747923754 entropy 8.254048964904479
epoch: 6, step: 122
	action: tensor([[ -807.8604, -1101.8462,   557.6589,   934.2403,  -147.7058,   447.8970,
          -157.9163]], dtype=torch.float64)
	q_value: tensor([[-23.8776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5009698219575864, distance: 1.4019827610930045 entropy 8.102625951518762
epoch: 6, step: 123
	action: tensor([[-1174.6182,  -629.2757,   498.0841,    80.8688,   591.9912, -1238.0816,
         -2832.0681]], dtype=torch.float64)
	q_value: tensor([[-34.0933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16675251885927111, distance: 1.2360785408272743 entropy 8.466863278793339
epoch: 6, step: 124
	action: tensor([[  22.1672, -563.0292, 1384.9817,  -50.2304,  736.4669, 2061.0677,
          257.7479]], dtype=torch.float64)
	q_value: tensor([[-29.9109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32296199461201325, distance: 1.3162257332809164 entropy 8.265007736475386
epoch: 6, step: 125
	action: tensor([[ 1040.4933,  -242.8076,  -447.1708,  -500.4909, -1211.2446,  1016.4382,
          1757.1446]], dtype=torch.float64)
	q_value: tensor([[-28.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3215543175528336, distance: 0.9425713495256297 entropy 8.26878151013842
epoch: 6, step: 126
	action: tensor([[-9.4464e+02, -1.0548e+03,  1.0703e+03,  2.0729e+02, -3.5257e+02,
          9.0239e-01, -6.1481e+02]], dtype=torch.float64)
	q_value: tensor([[-21.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4478260568132222, distance: 1.3769395914962115 entropy 8.035667096322987
epoch: 6, step: 127
	action: tensor([[ -829.9884,   431.5078,    75.2494,  -363.6703,  1272.9379,   665.2501,
         -1046.0822]], dtype=torch.float64)
	q_value: tensor([[-23.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36924315488080883, distance: 0.9088405156146057 entropy 8.129402071421932
LOSS epoch 6 actor 365.3540882873692 critic 117.80883365196345
epoch: 7, step: 0
	action: tensor([[  422.0079,  -442.2231,  -511.7409,  -549.8476, -2087.7639, -1199.5107,
           866.7908]], dtype=torch.float64)
	q_value: tensor([[-42.1559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.404319097810527
epoch: 7, step: 1
	action: tensor([[ -807.4496,  -422.6042,    -2.4683,   796.4512, -1219.9135,  1137.8735,
           984.3842]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1452979393625643, distance: 1.0579473834241078 entropy 7.8412942304246105
epoch: 7, step: 2
	action: tensor([[-1472.6811, -1333.9194,  -481.4883,  -755.9074,   791.4378, -2547.2309,
         -1178.7794]], dtype=torch.float64)
	q_value: tensor([[-31.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8436966798613195, distance: 1.5538225864023882 entropy 8.516361563847518
epoch: 7, step: 3
	action: tensor([[ -415.1620, -1344.8745,  -408.3534,  -104.7522, -1655.7295,   706.5981,
           936.9543]], dtype=torch.float64)
	q_value: tensor([[-35.1987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2034176562870511, distance: 1.0213439749560316 entropy 8.428322260505746
epoch: 7, step: 4
	action: tensor([[   54.5561,  -527.6436, -1781.6554,  -476.8630,  1004.9093,  -399.2432,
          -499.8697]], dtype=torch.float64)
	q_value: tensor([[-32.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0636134319169529, distance: 1.1073483961114101 entropy 8.385646557023323
epoch: 7, step: 5
	action: tensor([[-2289.5858, -1794.9344, -1236.2349,  1819.5639,   832.7577, -2319.0738,
          -523.2812]], dtype=torch.float64)
	q_value: tensor([[-31.9571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06453599433687929, distance: 1.1806926739735693 entropy 8.443514642995604
epoch: 7, step: 6
	action: tensor([[ -421.1235,  -335.0478,  1600.7889,  -513.7215, -1286.5868, -1060.7721,
          -379.1248]], dtype=torch.float64)
	q_value: tensor([[-36.6778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28575057317140184, distance: 1.2975827295386766 entropy 8.503926956877114
epoch: 7, step: 7
	action: tensor([[-1167.4677, -1165.7134,  1243.9739,   638.4879, -1325.1124,   824.4157,
         -1446.8595]], dtype=torch.float64)
	q_value: tensor([[-34.1443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8776186844961393, distance: 1.5680517534399048 entropy 8.29777819223433
epoch: 7, step: 8
	action: tensor([[-1625.3231,   331.7644,  1333.8722,   628.2510,  -244.7838, -1078.5696,
          -112.8853]], dtype=torch.float64)
	q_value: tensor([[-22.4613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2749125076830219, distance: 0.9744328133455188 entropy 8.124462978445278
epoch: 7, step: 9
	action: tensor([[-137.1775, -826.4553, 1045.9653,   71.2857, -834.8688,  630.6161,
          684.0090]], dtype=torch.float64)
	q_value: tensor([[-27.8953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3286496819262027, distance: 0.9376295647168078 entropy 8.184304147836965
epoch: 7, step: 10
	action: tensor([[ -114.2261, -2269.0902,  -692.4891,  -380.2653,  1550.1740,  -390.2044,
           275.0560]], dtype=torch.float64)
	q_value: tensor([[-28.4494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5423972679235614, distance: 1.82464507764017 entropy 8.383422996273982
epoch: 7, step: 11
	action: tensor([[ -873.2657,  -331.6827,   386.2731,   748.7085, -1207.7002,   496.2400,
          -168.7316]], dtype=torch.float64)
	q_value: tensor([[-26.4678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2395952770748243, distance: 0.9978817869643654 entropy 8.209020997693637
epoch: 7, step: 12
	action: tensor([[-1003.7192,  -101.8441,   -95.6362,   107.1765, -3260.0312,  -973.0789,
          -307.9931]], dtype=torch.float64)
	q_value: tensor([[-31.8061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7728041654917379, distance: 1.5236565220594478 entropy 8.499490995448324
epoch: 7, step: 13
	action: tensor([[ -695.4041,  -688.5616,   689.5307, -1022.3811,   558.8733,   733.5124,
           244.5084]], dtype=torch.float64)
	q_value: tensor([[-26.2417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8565832347601012, distance: 1.5592433677874022 entropy 8.232661205426579
epoch: 7, step: 14
	action: tensor([[-1193.2721,  -258.7882,  -469.2701,  1086.2451,   349.5035,   243.6706,
          1149.0912]], dtype=torch.float64)
	q_value: tensor([[-26.3873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21000634958903808, distance: 1.2587819821755448 entropy 8.160787946255088
epoch: 7, step: 15
	action: tensor([[ -172.3011, -1492.4749,   264.5885, -1108.5060, -1321.4204, -2130.3563,
         -1343.4148]], dtype=torch.float64)
	q_value: tensor([[-35.4733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5900616430249164, distance: 1.4429911711269625 entropy 8.580369285443647
epoch: 7, step: 16
	action: tensor([[-1893.0468,  -981.3387, -1335.5362, -1420.3796, -1149.5684,  -967.8778,
          1924.1734]], dtype=torch.float64)
	q_value: tensor([[-29.4658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10944954728882017, distance: 1.0799060087296362 entropy 8.298552958226278
epoch: 7, step: 17
	action: tensor([[-1433.8825,   671.1303,  -763.5862,  -140.4898,   456.6757,  2082.4059,
         -1299.9678]], dtype=torch.float64)
	q_value: tensor([[-33.5684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4641326572609643, distance: 0.8376938723510834 entropy 8.439809856082872
epoch: 7, step: 18
	action: tensor([[-2255.6679,  -398.6471, -1171.8284, -1030.4359,   654.1027,  1141.7936,
         -2098.0810]], dtype=torch.float64)
	q_value: tensor([[-37.1062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0007943080481327, distance: 1.6186684986731181 entropy 8.441611271609554
epoch: 7, step: 19
	action: tensor([[ -261.9292,  -411.0742,   -71.3995,  1478.1682, -1005.1529,  1438.2744,
           -60.5256]], dtype=torch.float64)
	q_value: tensor([[-30.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4954600661156965, distance: 1.3994071981203153 entropy 8.399055563014793
epoch: 7, step: 20
	action: tensor([[ -974.0305,  -177.9971, -1520.2109,   647.7893,  1013.2001,  -173.1773,
          -288.8315]], dtype=torch.float64)
	q_value: tensor([[-30.8384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6870743014838332, distance: 1.4863592776830643 entropy 8.386314287857305
epoch: 7, step: 21
	action: tensor([[-1138.1075, -1130.4280,  1058.0227,   718.8702,  -621.3405,   944.8760,
         -1829.3234]], dtype=torch.float64)
	q_value: tensor([[-23.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2401593870203792, distance: 0.9975115765602219 entropy 8.171187431298305
epoch: 7, step: 22
	action: tensor([[-1154.0051, -1062.7607, -1405.6449,   538.1254,  -545.1550,   147.7056,
          1299.1310]], dtype=torch.float64)
	q_value: tensor([[-26.2446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032157498702755194, distance: 1.1257942807966037 entropy 8.294411511701483
epoch: 7, step: 23
	action: tensor([[ -668.0511, -1997.0055,   866.4090,  -432.9857,   311.5065,  2740.8616,
          1893.9405]], dtype=torch.float64)
	q_value: tensor([[-28.5710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5987916132598199, distance: 1.4469470009295013 entropy 8.409963519691654
epoch: 7, step: 24
	action: tensor([[ 1251.2675,  -624.4649, -1187.8902,   229.4432,  -346.2502, -2663.4321,
          -455.5498]], dtype=torch.float64)
	q_value: tensor([[-34.7500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4895566422104638, distance: 0.817580400396674 entropy 8.501718595475243
epoch: 7, step: 25
	action: tensor([[-1252.1825,  1070.1821,   889.0976,  1677.6964,  -459.2523, -1396.2306,
         -1034.8815]], dtype=torch.float64)
	q_value: tensor([[-30.7899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.4040178849371
epoch: 7, step: 26
	action: tensor([[-1143.6688,  -775.0027,  -309.9709,  -103.9153,  -337.0666,  -459.9012,
            49.3629]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10734519673139542, distance: 1.2041988451067376 entropy 7.8412942304246105
epoch: 7, step: 27
	action: tensor([[  165.3478, -1025.0398,  -749.2118,  1285.3175, -1123.8682,  2137.4656,
         -4321.8169]], dtype=torch.float64)
	q_value: tensor([[-38.2954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21038177702648286, distance: 1.0168696246431845 entropy 8.438541942215554
epoch: 7, step: 28
	action: tensor([[ -169.5024,  -304.3786, -1097.2865,   635.1840, -1156.0917,  -107.8098,
           180.3154]], dtype=torch.float64)
	q_value: tensor([[-20.4357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14052141828066433, distance: 1.060899445751817 entropy 7.999275497210043
epoch: 7, step: 29
	action: tensor([[ -190.6536,  -226.3738,  1411.9040,   339.4866,   938.4124, -2113.1098,
           645.1556]], dtype=torch.float64)
	q_value: tensor([[-27.4516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0776482991810401, distance: 1.0990184157037852 entropy 8.273097937765131
epoch: 7, step: 30
	action: tensor([[1584.2618, -596.1276, -149.2558, 3618.1081,  272.0530, 1428.9938,
         1002.6311]], dtype=torch.float64)
	q_value: tensor([[-33.8535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15363262403894673, distance: 1.229109167220208 entropy 8.538331784089793
epoch: 7, step: 31
	action: tensor([[ -347.3722, -2360.5281,   380.1580, -1266.8044,   239.3293,  -222.8762,
         -1030.5562]], dtype=torch.float64)
	q_value: tensor([[-31.6962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.890705486986018, distance: 1.573506841555356 entropy 8.514184266534146
epoch: 7, step: 32
	action: tensor([[ -366.0767, -1203.0494,  -721.9156,  -565.0141, -1507.3201,  -117.1588,
         -1123.3135]], dtype=torch.float64)
	q_value: tensor([[-29.7698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08485190830148581, distance: 1.191905769200974 entropy 8.257988529688502
epoch: 7, step: 33
	action: tensor([[-1730.9119,  -851.3264, -1522.6722,  -717.9588,   449.6018,   807.4595,
          1033.7730]], dtype=torch.float64)
	q_value: tensor([[-30.6058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12150152203656206, distance: 1.0725738390725017 entropy 8.333145068915034
epoch: 7, step: 34
	action: tensor([[ -640.3501, -2853.8715, -1140.2527, -1555.2466, -1365.6911,  -765.1009,
          -361.6977]], dtype=torch.float64)
	q_value: tensor([[-43.3298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7896834644542825, distance: 1.530892889935396 entropy 8.714736938673045
epoch: 7, step: 35
	action: tensor([[-1700.4583, -3315.2547,  -192.6644,  1457.0205,  1066.4923,   329.0768,
         -1778.3830]], dtype=torch.float64)
	q_value: tensor([[-43.7470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8660080383647504, distance: 1.56319604795833 entropy 8.672790620497187
epoch: 7, step: 36
	action: tensor([[-585.2519, -754.9624, -992.6395,  269.2565,  851.5214, -375.6182,
         -534.2520]], dtype=torch.float64)
	q_value: tensor([[-27.7464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037695866696627, distance: 0.9548456821972359 entropy 8.291988787989405
epoch: 7, step: 37
	action: tensor([[ -458.7474,   383.8958, -2858.9560, -1598.6043,  -617.8934,  -484.1093,
         -1449.7137]], dtype=torch.float64)
	q_value: tensor([[-28.6390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45836204420766946, distance: 0.8421922457164459 entropy 8.43745363486276
epoch: 7, step: 38
	action: tensor([[-1003.4176,   869.0576,   914.2244,   -47.8239,  -182.9634, -1285.2997,
            21.6836]], dtype=torch.float64)
	q_value: tensor([[-35.0769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6242635795173515, distance: 0.7014526174315153 entropy 8.294056722364605
epoch: 7, step: 39
	action: tensor([[-1153.1505, -1506.9804,  1882.8259,   691.1698,  -356.6672, -1715.7102,
           -44.8670]], dtype=torch.float64)
	q_value: tensor([[-42.5564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4558186491132161, distance: 1.3807349956612982 entropy 8.44665424519428
epoch: 7, step: 40
	action: tensor([[ -817.2585,  -881.8372,  -592.3354,   216.7142, -1556.9224,   138.2267,
           613.7896]], dtype=torch.float64)
	q_value: tensor([[-30.2979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4497943913303075, distance: 1.377875255409742 entropy 8.29515975185879
epoch: 7, step: 41
	action: tensor([[-1192.9083,  -544.6363,  -793.0411, -1076.1468,   650.2948,  1215.7993,
           874.6261]], dtype=torch.float64)
	q_value: tensor([[-26.7287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07668661727101056, distance: 1.1874117735426126 entropy 8.354879367301143
epoch: 7, step: 42
	action: tensor([[-2010.2772, -2052.8478, -1441.4504,  1790.1668,   229.7487,  -255.5342,
           -26.0216]], dtype=torch.float64)
	q_value: tensor([[-35.5620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008612903805051753, distance: 1.1394055333317703 entropy 8.512859504801005
epoch: 7, step: 43
	action: tensor([[  622.4944, -1819.5919,  -396.6076, -1005.0061, -1375.3704,   364.9250,
           106.3482]], dtype=torch.float64)
	q_value: tensor([[-33.1155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021301659673895745, distance: 1.156468244865225 entropy 8.606772354197137
epoch: 7, step: 44
	action: tensor([[ -331.4496, -1473.9295,    42.9563,   313.5499,  -260.3187,  -852.4356,
           363.2023]], dtype=torch.float64)
	q_value: tensor([[-17.5619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1937337604392929, distance: 1.0275333605260382 entropy 7.895669749160878
epoch: 7, step: 45
	action: tensor([[-1408.5704,  -167.7132,  1877.5741, -1861.6314,  -678.8730,   168.3999,
          -203.3652]], dtype=torch.float64)
	q_value: tensor([[-32.4444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2423721380132502, distance: 1.7136042774347655 entropy 8.511334912452634
epoch: 7, step: 46
	action: tensor([[-1560.5930, -1329.9628,   665.5255, -1278.0311,    32.2891,   -30.4633,
          1350.9038]], dtype=torch.float64)
	q_value: tensor([[-25.1708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056152841924483776, distance: 1.1117510019407586 entropy 8.185358724789817
epoch: 7, step: 47
	action: tensor([[-1160.9003,  -366.4174,  -288.1818,   618.8241,  -272.2938,  -439.2347,
           713.9198]], dtype=torch.float64)
	q_value: tensor([[-31.6584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3243411626926802, distance: 1.3169116273834165 entropy 8.201306315531648
epoch: 7, step: 48
	action: tensor([[-1941.3769,  -743.7577,   161.7807,   504.2166,   -50.2152,  1398.7482,
          -591.8536]], dtype=torch.float64)
	q_value: tensor([[-32.8795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12144810634942371, distance: 1.0726064466405245 entropy 8.366130590863174
epoch: 7, step: 49
	action: tensor([[-582.1862,  452.3456,  424.2089, -926.5086,  947.7053, -315.0348,
         -866.6985]], dtype=torch.float64)
	q_value: tensor([[-27.8726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34684311144544555, distance: 1.3280523600890604 entropy 8.316678048256792
epoch: 7, step: 50
	action: tensor([[-1912.9304, -1813.0796,  -228.6010,  -530.1700,  1043.9503,   228.4920,
          -520.4163]], dtype=torch.float64)
	q_value: tensor([[-37.1905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6226987247187425, distance: 1.4577251298739862 entropy 8.250681620519307
epoch: 7, step: 51
	action: tensor([[  188.3501, -1420.5532,  1366.7504,  -127.0449,   -49.9527, -1515.1528,
           -37.1313]], dtype=torch.float64)
	q_value: tensor([[-33.4862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.369778195452472
epoch: 7, step: 52
	action: tensor([[ -79.5601, -470.0129, -262.9172,  431.4341, -996.6317,  359.7823,
          -12.0011]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8496055034301817, distance: 1.5563105008128018 entropy 7.8412942304246105
epoch: 7, step: 53
	action: tensor([[-1435.4605, -1141.6465, -2107.6110,   541.1651,    75.6512,  2098.3095,
          -842.7052]], dtype=torch.float64)
	q_value: tensor([[-27.8377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03694588720086878, distance: 1.165291933044045 entropy 8.285440107684497
epoch: 7, step: 54
	action: tensor([[-2639.3587, -1927.7486,   136.8655,  -398.7766, -1362.1757,   554.4073,
         -1313.2875]], dtype=torch.float64)
	q_value: tensor([[-29.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9816200891782527, distance: 1.610893731188843 entropy 8.408730899150818
epoch: 7, step: 55
	action: tensor([[ -64.5788, -610.9114,  647.8053, -501.1575, -968.5735, 1176.6906,
          -75.4006]], dtype=torch.float64)
	q_value: tensor([[-25.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48719483747697967, distance: 1.3955346619737983 entropy 8.125450553996432
epoch: 7, step: 56
	action: tensor([[ -830.1733,  -330.7117,  -178.6489,   794.5509, -1004.2962,  -623.5099,
          1649.7852]], dtype=torch.float64)
	q_value: tensor([[-27.7834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6100141939145465, distance: 1.452016480500702 entropy 8.19353199177644
epoch: 7, step: 57
	action: tensor([[ -164.3583,  -446.4647,  1089.2434,  1123.6741,   809.3273,  2050.2977,
         -1189.4319]], dtype=torch.float64)
	q_value: tensor([[-27.9093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1832455726308233, distance: 1.0341950194478737 entropy 8.188213002230146
epoch: 7, step: 58
	action: tensor([[-1075.3224,   436.6673,   516.7057,   449.6258,  -104.7605, -1597.1845,
          -450.2460]], dtype=torch.float64)
	q_value: tensor([[-34.2200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32966742554271855, distance: 0.9369185883371858 entropy 8.554567362088717
epoch: 7, step: 59
	action: tensor([[-1891.8444, -2698.8010,   -45.5851,  1060.4009,  1256.5860,  1057.7603,
           381.9155]], dtype=torch.float64)
	q_value: tensor([[-37.8332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18075506435464095, distance: 1.0357705914158655 entropy 8.53774171518103
epoch: 7, step: 60
	action: tensor([[-1166.4292,  -885.0563,   469.0704,  -622.0060,   149.6744,   670.6945,
          1020.0018]], dtype=torch.float64)
	q_value: tensor([[-26.5166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5024433187747828, distance: 1.4026707530677158 entropy 8.293869085508442
epoch: 7, step: 61
	action: tensor([[ -833.3685,   792.3572,  -288.1827,  2541.5249, -1828.3083, -1329.0453,
           182.0515]], dtype=torch.float64)
	q_value: tensor([[-31.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5864825573945944, distance: 0.73587425630266 entropy 8.403127032118821
epoch: 7, step: 62
	action: tensor([[-1392.7870,  -168.2962,   -58.4317,   744.7874,  -881.3086, -1009.2549,
           883.3949]], dtype=torch.float64)
	q_value: tensor([[-25.9786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01031087267613251, distance: 1.1502287183594402 entropy 8.10641289635157
epoch: 7, step: 63
	action: tensor([[ 102.6074,  480.8465, 1018.3036, 1754.5796,  347.6257, 1091.0997,
          826.9643]], dtype=torch.float64)
	q_value: tensor([[-32.6680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34699136321287016, distance: 0.9247325737605854 entropy 8.432971336496307
epoch: 7, step: 64
	action: tensor([[-1165.6832, -1026.0913,    -9.2646,  -732.4133,  1877.8262,   566.7453,
          1016.9688]], dtype=torch.float64)
	q_value: tensor([[-29.1179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0215189007087625, distance: 1.1319647784470328 entropy 8.342702450247014
epoch: 7, step: 65
	action: tensor([[ 548.4301, -612.8733,  359.9049, -835.4020, -998.5064, -965.7706,
         -296.7997]], dtype=torch.float64)
	q_value: tensor([[-31.3046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.272401737987149
epoch: 7, step: 66
	action: tensor([[ -819.8637,   225.9925,  -812.6784,   220.8449,    -6.0574,  1340.7947,
         -1035.8306]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3381294291390713, distance: 1.323749333151563 entropy 7.8412942304246105
epoch: 7, step: 67
	action: tensor([[ -716.6667, -1315.5915,   615.5235,   592.4100,  -924.6184,   232.9117,
          -366.3433]], dtype=torch.float64)
	q_value: tensor([[-33.0726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006091160110028637, distance: 1.1478241549226533 entropy 8.160355237049204
epoch: 7, step: 68
	action: tensor([[1274.9003, 1472.2593,  701.3011,   91.0475,  268.2925,  735.8821,
         1617.4663]], dtype=torch.float64)
	q_value: tensor([[-27.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.34185869943251
epoch: 7, step: 69
	action: tensor([[-864.9763, -201.7614,  972.0391,  319.8233, -605.2500,  529.6962,
         -731.4947]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4837234545288449, distance: 1.3939049947106137 entropy 7.8412942304246105
epoch: 7, step: 70
	action: tensor([[ -551.4598,  -752.6839,  2804.9440,  2455.8178, -2658.5386,   738.7647,
          1894.7536]], dtype=torch.float64)
	q_value: tensor([[-30.2923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.069282656340651, distance: 1.1039911622023015 entropy 8.458593511426253
epoch: 7, step: 71
	action: tensor([[-980.1070,  884.4461,  446.1491, 1387.7546, -320.2142,  156.5364,
          831.4590]], dtype=torch.float64)
	q_value: tensor([[-34.7123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14861914510940089, distance: 1.0558898939841457 entropy 8.594790706141607
epoch: 7, step: 72
	action: tensor([[ -446.8464,   449.0439,  -261.5244,  -458.5578, -1149.6186,  -962.6943,
           -19.9037]], dtype=torch.float64)
	q_value: tensor([[-31.0944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.276010706462143
epoch: 7, step: 73
	action: tensor([[ 160.9114, -639.1878,  255.6289,  138.1438,   43.1099,  597.3390,
          238.6604]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3433544319044095, distance: 1.3263312415635178 entropy 7.8412942304246105
epoch: 7, step: 74
	action: tensor([[-1280.9506,  -540.8198,  -434.2598,   874.2260,  -323.8242,   457.3768,
           691.0178]], dtype=torch.float64)
	q_value: tensor([[-20.4146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9843799211315476, distance: 1.6120150987916169 entropy 8.091314335955829
epoch: 7, step: 75
	action: tensor([[-1146.2684,  -279.5040,  -776.2610, -1887.1160,   383.0771,   407.6716,
           306.4788]], dtype=torch.float64)
	q_value: tensor([[-32.2810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41443986614260897, distance: 1.360971207741486 entropy 8.45699390192116
epoch: 7, step: 76
	action: tensor([[ -617.3523, -2145.7072,  -827.1455,  -622.6862,  -627.0957, -1045.2553,
           607.3271]], dtype=torch.float64)
	q_value: tensor([[-31.6397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22090645352508465, distance: 1.264439015479846 entropy 8.392624310311438
epoch: 7, step: 77
	action: tensor([[  -69.3522,  1465.1316,  1459.4008,  -711.9264, -2158.3666,   990.2959,
          1174.0892]], dtype=torch.float64)
	q_value: tensor([[-34.8007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039158637815771336, distance: 1.1217150282389594 entropy 8.384397226059225
epoch: 7, step: 78
	action: tensor([[ -362.6711,  -827.6830, -2172.3985,  -810.5911,   867.8863,  -187.4242,
          1041.6184]], dtype=torch.float64)
	q_value: tensor([[-34.1935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15260090766035095, distance: 1.2285594360149485 entropy 8.363572295695363
epoch: 7, step: 79
	action: tensor([[   77.5703,  -947.9789, -1736.5246,  -206.1536,  1003.1322,  -642.8655,
          -677.1029]], dtype=torch.float64)
	q_value: tensor([[-30.8897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.159942461093868, distance: 1.232465912941811 entropy 8.240457765091524
epoch: 7, step: 80
	action: tensor([[-1029.0627, -1223.1784, -1103.2441,  1396.5870,   959.9895,  -386.0530,
          -947.2558]], dtype=torch.float64)
	q_value: tensor([[-35.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6886494042255029, distance: 1.4870529704208753 entropy 8.41453853448599
epoch: 7, step: 81
	action: tensor([[ -211.3487, -1190.5301,   584.0236,  -608.9887,   554.2577, -1513.9458,
         -2626.2604]], dtype=torch.float64)
	q_value: tensor([[-30.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3071451586396081, distance: 1.3083339245974364 entropy 8.396784040496481
epoch: 7, step: 82
	action: tensor([[-360.8508, -257.2342, -892.2367, 1436.2687, 1065.3215, -686.5713,
          600.7408]], dtype=torch.float64)
	q_value: tensor([[-31.6287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4298367959043814, distance: 1.3683586057152497 entropy 8.262315649348249
epoch: 7, step: 83
	action: tensor([[-1614.9909,   207.3772,  2172.2476,  -591.4913,   597.6799,  -260.0612,
            57.1163]], dtype=torch.float64)
	q_value: tensor([[-33.2433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033218438987502985, distance: 1.1251770687990181 entropy 8.432299416647224
epoch: 7, step: 84
	action: tensor([[ -133.5903,  -959.8302,   899.7664,  -234.0479,  -111.2332, -1112.1592,
          1542.0902]], dtype=torch.float64)
	q_value: tensor([[-34.6179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.09755078737342
epoch: 7, step: 85
	action: tensor([[-252.8336, -460.5300,  165.3460,  618.9763,  605.8069,  -85.2203,
         -198.7449]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3694846954527623, distance: 0.9086664843064413 entropy 7.8412942304246105
epoch: 7, step: 86
	action: tensor([[  654.0984,  -945.3763, -1447.1427,  -272.7454, -1866.7715,  -569.2200,
           719.4835]], dtype=torch.float64)
	q_value: tensor([[-34.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016328350245133, distance: 0.9563097844758087 entropy 8.51645297071864
epoch: 7, step: 87
	action: tensor([[ -166.6129, -1910.3952,  -445.4015,   116.6716,   371.1275,  1001.6472,
           477.4166]], dtype=torch.float64)
	q_value: tensor([[-30.5560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011949587455656108, distance: 1.1511611705133047 entropy 8.415515478398062
epoch: 7, step: 88
	action: tensor([[ -634.5269, -1040.1287,  -789.6356,  -486.8597,  -975.9310,   734.9959,
          -169.6304]], dtype=torch.float64)
	q_value: tensor([[-21.3023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8200848903274647, distance: 1.543840804723118 entropy 8.099913998908988
epoch: 7, step: 89
	action: tensor([[ -729.9234, -1844.6683,  -420.4657,   263.9651,  1927.7090,  1590.3777,
          -206.7270]], dtype=torch.float64)
	q_value: tensor([[-28.1398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5582364500666854, distance: 1.4284773968228563 entropy 8.277000991221085
epoch: 7, step: 90
	action: tensor([[-1647.3386,  -204.3250,  -258.7478,  1814.3040,  -197.3062,   741.0119,
          -426.1421]], dtype=torch.float64)
	q_value: tensor([[-25.9994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6967846578959047, distance: 1.490630686960203 entropy 8.2159417883389
epoch: 7, step: 91
	action: tensor([[  540.9030, -1384.5419, -1247.5493,  -335.5594,  -686.2707,  2704.7881,
          1258.6355]], dtype=torch.float64)
	q_value: tensor([[-32.4430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11730064943476193, distance: 1.2095998348747918 entropy 8.505814057633485
epoch: 7, step: 92
	action: tensor([[  870.8543, -1820.9138,  -909.7281,   -18.0818,   767.0522,  1543.9130,
          -578.6303]], dtype=torch.float64)
	q_value: tensor([[-26.5975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29066315715194513, distance: 1.3000592623030127 entropy 8.344528958244
epoch: 7, step: 93
	action: tensor([[-791.5266, 1082.4829,  182.1770, 1056.3624,  689.9943, -441.0554,
          993.9994]], dtype=torch.float64)
	q_value: tensor([[-22.9001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.164077124777036
epoch: 7, step: 94
	action: tensor([[ -826.3564,  -824.7285,   480.7470,   631.9446,  -249.5078,  -872.7030,
         -1064.8100]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12372728409747757, distance: 1.0712142418460888 entropy 7.8412942304246105
epoch: 7, step: 95
	action: tensor([[ 1441.8061, -1703.8583, -2301.0707, -1396.4668,  -580.3375,  -269.2832,
          -888.7460]], dtype=torch.float64)
	q_value: tensor([[-30.4287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24556157475550267, distance: 0.9939592809132223 entropy 8.39269192437118
epoch: 7, step: 96
	action: tensor([[ -430.1614, -1110.7212, -1515.0091,  1387.1122,   575.3195,  -427.2888,
         -1558.9186]], dtype=torch.float64)
	q_value: tensor([[-27.3582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47904236975944625, distance: 0.8259578686768065 entropy 8.316141653579399
epoch: 7, step: 97
	action: tensor([[-2260.9651, -1098.5858, -1878.4126,  2521.3465,   -75.5691,  1080.5396,
          2230.0807]], dtype=torch.float64)
	q_value: tensor([[-30.3797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06678683492944015, distance: 1.1054704083278728 entropy 8.470141291188286
epoch: 7, step: 98
	action: tensor([[-1717.6603,  -998.7331,  -225.9771,  1833.3695,   663.7183,  1336.2479,
          -677.1750]], dtype=torch.float64)
	q_value: tensor([[-26.9218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7344428110785679, distance: 1.5070813155556881 entropy 8.251684443657133
epoch: 7, step: 99
	action: tensor([[-2199.0717, -1792.2916,  -380.8936,  1847.5462,  -326.9201,  1270.1898,
         -1258.5695]], dtype=torch.float64)
	q_value: tensor([[-32.8484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08067457604418027, distance: 1.0972139702738672 entropy 8.490272348810269
epoch: 7, step: 100
	action: tensor([[ -410.5269, -1286.4580,  -344.7067,   493.0673,  1850.1086,   160.0949,
         -2965.2726]], dtype=torch.float64)
	q_value: tensor([[-29.8578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034766188286976085, distance: 1.1640665452089571 entropy 8.452876053940898
epoch: 7, step: 101
	action: tensor([[  337.0434,  -918.8228,   572.0041,    15.3395, -1251.5997,  -396.1186,
            -7.7991]], dtype=torch.float64)
	q_value: tensor([[-28.8221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.36762169596037
epoch: 7, step: 102
	action: tensor([[ -792.2415, -1327.3727,   132.8725,  1167.0377,     8.3528,   890.3127,
           572.5916]], dtype=torch.float64)
	q_value: tensor([[-31.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5387325524817501, distance: 1.4195093714753186 entropy 7.8412942304246105
epoch: 7, step: 103
	action: tensor([[-8.6680e+01, -1.6393e+03, -1.1992e+03,  1.3243e+02, -1.1675e+02,
         -3.1499e+02,  2.2491e-01]], dtype=torch.float64)
	q_value: tensor([[-30.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07407871385732345, distance: 1.1011430129154176 entropy 8.364179296840444
epoch: 7, step: 104
	action: tensor([[  550.2861,  -358.5657, -1338.0991,   -46.7557,   426.3319,  -716.3565,
         -1341.9510]], dtype=torch.float64)
	q_value: tensor([[-26.5594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33780837587933277, distance: 0.9312119378433795 entropy 8.248960081441432
epoch: 7, step: 105
	action: tensor([[ 581.3080, -931.1161, -133.2876, 1732.6795, -467.0802,  334.6456,
         -759.0871]], dtype=torch.float64)
	q_value: tensor([[-26.4036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.284549505490835, distance: 0.9679356538570468 entropy 8.226177991318322
epoch: 7, step: 106
	action: tensor([[-1431.5037, -1473.7907,    77.5074,  -310.8008,   133.1048,  -201.4466,
           590.1859]], dtype=torch.float64)
	q_value: tensor([[-24.6735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.425588200103757, distance: 1.3663241330803895 entropy 8.355866437845167
epoch: 7, step: 107
	action: tensor([[-1.8693e+03,  1.8296e+02,  6.5301e+02,  8.2873e+02,  1.0575e+00,
         -8.5235e+02, -1.1865e+03]], dtype=torch.float64)
	q_value: tensor([[-26.6714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.466229566952028, distance: 1.385663188743411 entropy 8.185922876666249
epoch: 7, step: 108
	action: tensor([[   -6.6636, -1276.8718,  -706.2985,   469.4021, -2141.0687,  -785.0311,
          1007.1303]], dtype=torch.float64)
	q_value: tensor([[-29.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11694544268018747, distance: 1.0753515399558786 entropy 8.221038552432788
epoch: 7, step: 109
	action: tensor([[  486.9669,  1416.0581,   801.5725, -1108.4587,  -318.8196,  -509.5204,
           229.8677]], dtype=torch.float64)
	q_value: tensor([[-33.2699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41677794440578997, distance: 0.8739239932389262 entropy 8.451909306504458
epoch: 7, step: 110
	action: tensor([[-1898.5524, -1938.1029,  -267.4372,  1066.9686,    40.5252,  -845.1014,
          -151.9180]], dtype=torch.float64)
	q_value: tensor([[-34.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33781538563385893, distance: 0.9312070090703982 entropy 8.39704412635233
epoch: 7, step: 111
	action: tensor([[-3960.8058,  -797.2858,  -984.2208,   158.1471, -2276.6836,  1177.4512,
         -1926.9586]], dtype=torch.float64)
	q_value: tensor([[-35.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22531054651112647, distance: 1.2667175250723726 entropy 8.573562225500675
epoch: 7, step: 112
	action: tensor([[ -391.3012, -2836.2004,   -96.5516, -1261.3580,  -379.7675,  -589.3226,
           340.2855]], dtype=torch.float64)
	q_value: tensor([[-32.4062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3314026628113629, distance: 1.7472913919176358 entropy 8.504730713429458
epoch: 7, step: 113
	action: tensor([[ -503.9949, -1929.6989, -1110.1662, -1558.6320,  -767.9806,  2397.5270,
           218.2946]], dtype=torch.float64)
	q_value: tensor([[-34.9138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5281549020725109, distance: 1.414621918056912 entropy 8.478054439473144
epoch: 7, step: 114
	action: tensor([[  101.7733,    23.7278, -1870.0752, -1185.6513,  -908.4395,  -866.7185,
          1910.2507]], dtype=torch.float64)
	q_value: tensor([[-31.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2000026651220136, distance: 1.0235309117411266 entropy 8.353322300445276
epoch: 7, step: 115
	action: tensor([[   -1.4778, -1266.7955, -1161.1901,  -363.9869,   464.5086,   237.1636,
         -1128.4091]], dtype=torch.float64)
	q_value: tensor([[-30.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6988275262634125, distance: 1.4915277501934303 entropy 8.322211258394985
epoch: 7, step: 116
	action: tensor([[-2024.9721, -1905.7185,  -487.1756,  -332.4546, -1904.6376,  -450.5061,
           821.7122]], dtype=torch.float64)
	q_value: tensor([[-26.6297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22266544407037947, distance: 1.2653495421427376 entropy 8.295246914784588
epoch: 7, step: 117
	action: tensor([[-142.4681,  294.0156, -919.9718, -206.4797,   34.9718, 1804.0886,
         2285.9432]], dtype=torch.float64)
	q_value: tensor([[-36.5188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08897710778367585, distance: 1.09224820159444 entropy 8.437481860060851
epoch: 7, step: 118
	action: tensor([[-1591.8177, -1049.7344,  -503.1980,   326.5710,  1766.7993,   -25.8281,
          1605.0873]], dtype=torch.float64)
	q_value: tensor([[-46.0131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4863797534625958, distance: 1.8044315981883055 entropy 8.510563228798864
epoch: 7, step: 119
	action: tensor([[-2768.1889,  -805.0501, -1292.1504,   866.4356,   473.6165,  -210.0240,
          -194.4296]], dtype=torch.float64)
	q_value: tensor([[-29.3443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1454498041493718, distance: 1.0578533903833334 entropy 8.303353764173552
epoch: 7, step: 120
	action: tensor([[ -859.5289,  1576.3379,   688.4822, -1512.7416,   632.3768,  -946.7428,
         -4112.6759]], dtype=torch.float64)
	q_value: tensor([[-35.3595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10941302805630881, distance: 1.0799281506162663 entropy 8.51970803345077
epoch: 7, step: 121
	action: tensor([[ -242.0167, -1134.5163,  -433.4757,   359.3672,  -492.6555,   640.5492,
          1790.0090]], dtype=torch.float64)
	q_value: tensor([[-43.7269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.314170727912864, distance: 1.7408220988625285 entropy 8.557168618483576
epoch: 7, step: 122
	action: tensor([[ 936.6627, -767.3796, 1049.1421, -375.4473, -569.3859, -931.9312,
         2059.6115]], dtype=torch.float64)
	q_value: tensor([[-29.4626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14513820063157845, distance: 1.2245757206339818 entropy 8.476296156821412
epoch: 7, step: 123
	action: tensor([[-801.2399, -513.8069,  320.8143,   80.7388,  132.0556, -385.7620,
          388.2668]], dtype=torch.float64)
	q_value: tensor([[-27.2070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9914462110584721, distance: 1.6148827057619124 entropy 8.07924731849461
epoch: 7, step: 124
	action: tensor([[ -198.2438,    97.7199, -1108.0748,   540.4000,   320.9265,   540.6281,
           -50.5890]], dtype=torch.float64)
	q_value: tensor([[-25.3656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34322821713685747, distance: 0.9273932618995324 entropy 8.054442797794486
epoch: 7, step: 125
	action: tensor([[-290.6594, -112.0391, -828.8003, 1967.4507,   29.8505,  670.9819,
         -999.8609]], dtype=torch.float64)
	q_value: tensor([[-26.5546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01968759021272337, distance: 1.155554039867082 entropy 8.104406901420457
epoch: 7, step: 126
	action: tensor([[ 789.7626,  220.7669,  163.1199,  979.9733,  956.1038, 1434.8327,
          278.0145]], dtype=torch.float64)
	q_value: tensor([[-28.2759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.486494821280428, distance: 0.8200288033997603 entropy 8.277257164367693
epoch: 7, step: 127
	action: tensor([[ 798.7260, -274.5390, -248.5884,  454.9655,  226.6396, 1394.5307,
         -660.8940]], dtype=torch.float64)
	q_value: tensor([[-25.4285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23329524828179748, distance: 1.0020070350161339 entropy 8.088388582559842
LOSS epoch 7 actor 419.2984939563366 critic 155.05229744650796
epoch: 8, step: 0
	action: tensor([[  677.6524, -1742.8908,  -194.7448, -1690.9039,   150.0090, -2209.2165,
         -2256.7647]], dtype=torch.float64)
	q_value: tensor([[-23.7170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27605071454735586, distance: 1.292678902336959 entropy 8.274654871680719
epoch: 8, step: 1
	action: tensor([[-929.6638, -620.5101, -399.5347, 1477.0218, -824.1100,  875.1083,
         -174.4551]], dtype=torch.float64)
	q_value: tensor([[-23.5457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5851800129021516, distance: 0.7370323154825 entropy 8.107567314255126
epoch: 8, step: 2
	action: tensor([[ -970.6672,  -950.8324, -2283.8469,  1101.0190,  -229.3044,    89.2203,
          2059.5483]], dtype=torch.float64)
	q_value: tensor([[-25.6295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3265538260611973, distance: 1.318011293433178 entropy 8.39215825692071
epoch: 8, step: 3
	action: tensor([[  890.1914, -3126.3962,   915.8746,  2995.4616,   499.0152,   241.1906,
         -2351.3063]], dtype=torch.float64)
	q_value: tensor([[-33.3568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09849185826869988, distance: 1.0865294942820047 entropy 8.771920468904618
epoch: 8, step: 4
	action: tensor([[-2694.0779, -1118.6061,    -4.9705,   720.8003,   310.2702, -3793.0988,
          -190.7326]], dtype=torch.float64)
	q_value: tensor([[-26.4969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29754202805541263, distance: 0.9591065701242427 entropy 8.510869210060267
epoch: 8, step: 5
	action: tensor([[ -683.4304, -2746.2571,  -456.2783,   451.7377,   458.3432,   516.1817,
           -69.0822]], dtype=torch.float64)
	q_value: tensor([[-29.7267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20944203488636615, distance: 1.0174745442484971 entropy 8.434930053325314
epoch: 8, step: 6
	action: tensor([[-1424.4451,   -98.4564,   669.3296,    83.0571,    55.3917,   116.2232,
          -639.4184]], dtype=torch.float64)
	q_value: tensor([[-27.2256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09927159363551485, distance: 1.0860595103152766 entropy 8.457558970395556
epoch: 8, step: 7
	action: tensor([[-1101.2248,   -29.2831,   957.9703, -1393.5345,   -35.6241,  -871.3156,
         -1542.0004]], dtype=torch.float64)
	q_value: tensor([[-31.7596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.637039730821465
epoch: 8, step: 8
	action: tensor([[ -193.9276, -1375.3803,   692.6331,  1708.0709,   317.0322,   600.1040,
           798.6380]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7261061118069492, distance: 1.5034550162288445 entropy 7.951509785056763
epoch: 8, step: 9
	action: tensor([[-1317.6069, -4260.2077,  -341.3444,  1403.3894,  -483.0658,  -592.7854,
           444.3818]], dtype=torch.float64)
	q_value: tensor([[-27.3294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5659585095565514, distance: 1.432012532662329 entropy 8.4530613437485
epoch: 8, step: 10
	action: tensor([[ 1001.0562,   815.1775,  1187.2603,   503.1087, -1165.2911,  3281.1424,
          1795.5971]], dtype=torch.float64)
	q_value: tensor([[-23.8504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3689458874447328, distance: 0.9090546527006531 entropy 8.399926378291012
epoch: 8, step: 11
	action: tensor([[-1979.4094, -4397.4206, -1762.6499,  -773.4373,   430.2549, -2442.4000,
           527.6007]], dtype=torch.float64)
	q_value: tensor([[-26.2641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.543785608615797
epoch: 8, step: 12
	action: tensor([[-635.9765,  -76.5771,  348.5072,  652.5142,  577.6685,  971.4084,
          700.9186]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2144446658145034, distance: 1.0142501585271928 entropy 7.951509785056763
epoch: 8, step: 13
	action: tensor([[-2100.4032, -1202.2293,   -42.7770,  1170.0566, -1400.1288, -1450.4760,
          -197.9426]], dtype=torch.float64)
	q_value: tensor([[-31.1085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1462534652784011, distance: 1.2251718904435185 entropy 8.560825934905365
epoch: 8, step: 14
	action: tensor([[  151.5061, -2443.2092,  -616.6301,  -411.6515,   577.4480,  4105.5567,
           952.7620]], dtype=torch.float64)
	q_value: tensor([[-31.5436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047011806709851056, distance: 1.1709341783798881 entropy 8.61414417578291
epoch: 8, step: 15
	action: tensor([[-515.4022,  921.5315, -111.1832,  681.4364,  205.7825, 2134.0096,
          672.3354]], dtype=torch.float64)
	q_value: tensor([[-21.3685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6540119094600841, distance: 0.6731118995191265 entropy 8.261488390183617
epoch: 8, step: 16
	action: tensor([[ -163.3930,  -185.6871,  1537.7924,  -249.3965, -1581.8570,   868.1652,
          1458.0350]], dtype=torch.float64)
	q_value: tensor([[-30.2631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0135373827430154, distance: 1.6238149733986658 entropy 8.45469316650821
epoch: 8, step: 17
	action: tensor([[-1503.1888,  -636.5019,   514.1408,  -222.4392,   820.8266,  1056.1417,
         -2706.0629]], dtype=torch.float64)
	q_value: tensor([[-31.1705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04859213730522516, distance: 1.11619497075812 entropy 8.387249845731793
epoch: 8, step: 18
	action: tensor([[-1638.2776,  -901.1264, -1010.4214,  -493.2030,   -86.6800,    11.0607,
           -45.4572]], dtype=torch.float64)
	q_value: tensor([[-29.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.399923750745113
epoch: 8, step: 19
	action: tensor([[-408.2266, -594.1048,  600.5687,  473.7533, -393.4555, -225.2977,
          826.9146]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22779928631987378, distance: 1.0055919605183394 entropy 7.951509785056763
epoch: 8, step: 20
	action: tensor([[-3047.6341,  -375.9796, -1317.9627,  2044.6904,  -797.3240,  3981.9491,
           113.9039]], dtype=torch.float64)
	q_value: tensor([[-30.6366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06116047778359257, distance: 1.108797849979403 entropy 8.612973367552376
epoch: 8, step: 21
	action: tensor([[  364.9838, -3600.9132,  1953.1077,  2507.0151,  1014.7746, -1027.7160,
          -439.2066]], dtype=torch.float64)
	q_value: tensor([[-27.3427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.410527811749571, distance: 0.8785942426672483 entropy 8.467093150106562
epoch: 8, step: 22
	action: tensor([[  -66.6704, -1109.8118,  2328.1094,   933.6298, -1469.0985, -1095.6896,
          -298.6107]], dtype=torch.float64)
	q_value: tensor([[-29.7150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2291600832533014, distance: 1.0047055265730294 entropy 8.434605506552954
epoch: 8, step: 23
	action: tensor([[1487.6047, -966.1357, -305.7904,  673.3076, -592.4707, 1065.6553,
         1546.7307]], dtype=torch.float64)
	q_value: tensor([[-30.9836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08217253650447953, distance: 1.0963196986884665 entropy 8.533566117927828
epoch: 8, step: 24
	action: tensor([[-1821.3703,  -912.3281, -2286.6480,   -40.4997,  -797.3553,  -351.4007,
          1161.9670]], dtype=torch.float64)
	q_value: tensor([[-28.6453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07808345077411427, distance: 1.0987591349793868 entropy 8.456234904465584
epoch: 8, step: 25
	action: tensor([[-2155.4195,  -882.6354,   154.5906,   -71.2236,  -982.2507,   851.3381,
          1592.6681]], dtype=torch.float64)
	q_value: tensor([[-19.8970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5029577447726359, distance: 1.4029108648037845 entropy 8.11297367528915
epoch: 8, step: 26
	action: tensor([[  -33.0755,  -127.9844,  -273.5057, -1411.2692,  -189.4573, -1611.3608,
           614.3500]], dtype=torch.float64)
	q_value: tensor([[-28.4030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6437552072936374, distance: 1.4671525206145266 entropy 8.501251199850993
epoch: 8, step: 27
	action: tensor([[-170.5975, -331.6018,  963.6767, -137.6946, 2004.8705, 1309.3288,
         -840.1800]], dtype=torch.float64)
	q_value: tensor([[-31.0604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1256067207867333, distance: 1.6683921991176036 entropy 8.481131062737704
epoch: 8, step: 28
	action: tensor([[ 831.7226, -559.1073, -339.0929, -984.9366, -475.2066, -490.0617,
         1903.9300]], dtype=torch.float64)
	q_value: tensor([[-26.4100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.060444020024613465, distance: 1.17842125436531 entropy 8.486280223263865
epoch: 8, step: 29
	action: tensor([[-1.2324e+03, -2.8878e+03, -8.5832e+02,  3.3419e+03,  2.8365e+02,
          8.4278e-01, -2.0272e+02]], dtype=torch.float64)
	q_value: tensor([[-25.2126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7356857566647992, distance: 1.5076212251388335 entropy 8.32455726046415
epoch: 8, step: 30
	action: tensor([[  780.2957, -1403.4020,   217.4947,   550.4156,  -409.1903,  -941.6805,
           866.7517]], dtype=torch.float64)
	q_value: tensor([[-22.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04097819528845914, distance: 1.167555434449643 entropy 8.28409873732231
epoch: 8, step: 31
	action: tensor([[ 1603.0178, -2993.6431,  -833.9601,   977.3378,  1129.5654,   719.1882,
          1841.4924]], dtype=torch.float64)
	q_value: tensor([[-25.1802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08541650015643787, distance: 1.0943805710152754 entropy 8.502304601288833
epoch: 8, step: 32
	action: tensor([[   19.0909,  -773.9345, -1088.6457,    10.0376,  2227.1783,   406.6356,
         -1036.2031]], dtype=torch.float64)
	q_value: tensor([[-29.1718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.482922915836594
epoch: 8, step: 33
	action: tensor([[ -796.3125, -1131.7458,   623.4151,   278.1877,  -115.4002,    69.0414,
          -264.8345]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3916943692344708, distance: 1.7697402722051179 entropy 7.951509785056763
epoch: 8, step: 34
	action: tensor([[   88.3202, -1076.6365,  -617.0578,   496.2662,  -256.7451,   555.5925,
           129.2899]], dtype=torch.float64)
	q_value: tensor([[-25.4266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11819008074835113, distance: 1.0745934353736313 entropy 8.404877793862422
epoch: 8, step: 35
	action: tensor([[ 1243.6696, -1728.3699,  -713.2041,  3073.9461,   252.2322,   226.5851,
          2329.6058]], dtype=torch.float64)
	q_value: tensor([[-25.7814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5178983541522906, distance: 0.7945587238180346 entropy 8.507018338796865
epoch: 8, step: 36
	action: tensor([[-1187.3142,   284.7201,  1871.5357,  2523.5350,    39.6657,   270.9812,
           569.7145]], dtype=torch.float64)
	q_value: tensor([[-23.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.350857105355288, distance: 0.9219913512301846 entropy 8.517253594792049
epoch: 8, step: 37
	action: tensor([[  768.1066, -1360.7666,  -744.6513,  1300.1674, -2732.2759,  1217.0946,
         -1510.5909]], dtype=torch.float64)
	q_value: tensor([[-29.3548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4852032917093664, distance: 0.8210593931503087 entropy 8.466156184741502
epoch: 8, step: 38
	action: tensor([[-1736.3140,  -674.8772,  -218.0047,  -400.2498,   217.6130,  -874.8329,
          2399.0185]], dtype=torch.float64)
	q_value: tensor([[-26.3091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34163454803488613, distance: 1.3254819250128544 entropy 8.632524538526848
epoch: 8, step: 39
	action: tensor([[ 1418.3803, -1740.4989,   181.0695,   714.7500,  1233.5014,  2627.3659,
           939.9084]], dtype=torch.float64)
	q_value: tensor([[-29.8444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30231509332300877, distance: 0.9558425447864694 entropy 8.480624086951133
epoch: 8, step: 40
	action: tensor([[ -995.4220,  -384.9729, -1992.4452,  1377.4357,  -728.4640,  1906.4564,
         -1046.4419]], dtype=torch.float64)
	q_value: tensor([[-26.9183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18450865938588668, distance: 1.0333950339146014 entropy 8.556028187695873
epoch: 8, step: 41
	action: tensor([[  984.4428, -2473.7660,  -373.2306,  2020.4494, -1628.6044,  2250.5731,
          1754.1738]], dtype=torch.float64)
	q_value: tensor([[-29.2074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09138916316365298, distance: 1.1954915530015222 entropy 8.527506790613813
epoch: 8, step: 42
	action: tensor([[-1555.8574, -2734.5931,   -46.9567,  1425.9348,   972.7426,   534.0555,
          -139.8859]], dtype=torch.float64)
	q_value: tensor([[-23.6557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22433072140490784, distance: 1.0078478849850412 entropy 8.467665412171558
epoch: 8, step: 43
	action: tensor([[  216.8040,   105.8654,   -86.2266,  1050.0791,  2514.7884, -4724.1161,
           636.3974]], dtype=torch.float64)
	q_value: tensor([[-29.7426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5923115521839415, distance: 0.7306693601400323 entropy 8.642119769404859
epoch: 8, step: 44
	action: tensor([[-2373.2345,  -329.3875,    29.4120,  -195.1698,    59.1575,  -666.2887,
          -327.5494]], dtype=torch.float64)
	q_value: tensor([[-27.6350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3543576881179622, distance: 1.3317520744961377 entropy 8.311547809614181
epoch: 8, step: 45
	action: tensor([[-1935.6052, -1963.6389,   504.3728,   707.7845,  1227.8499,   894.9419,
         -1264.8846]], dtype=torch.float64)
	q_value: tensor([[-30.3049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16598547546562226, distance: 1.2356721642635655 entropy 8.537312541384727
epoch: 8, step: 46
	action: tensor([[-1196.6886,  -610.2224,  -873.2308,  -382.1845,  1810.5952,  -110.8532,
           504.2417]], dtype=torch.float64)
	q_value: tensor([[-29.2999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.738584930241314, distance: 1.8937375918718402 entropy 8.650855473868006
epoch: 8, step: 47
	action: tensor([[-1115.0135,  -501.0512, -1026.1920,   683.5673,   636.6743,  1265.5019,
         -2193.8751]], dtype=torch.float64)
	q_value: tensor([[-24.4663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7814333377802238, distance: 1.5273602402366766 entropy 8.325920057016543
epoch: 8, step: 48
	action: tensor([[-3059.7447,  -185.8236, -1782.2741,  1282.7526,   449.2617, -1186.8822,
          2129.7845]], dtype=torch.float64)
	q_value: tensor([[-30.9265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22894851644551817, distance: 1.0048433942114936 entropy 8.657007929069756
epoch: 8, step: 49
	action: tensor([[ -752.4555, -2882.7149, -2307.7525,  3182.8109,  1493.6444,  4792.5721,
          1410.6226]], dtype=torch.float64)
	q_value: tensor([[-36.5582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10712648244242962, distance: 1.2040799171964447 entropy 8.77615015450755
epoch: 8, step: 50
	action: tensor([[-2126.6948,  -749.8398,  1228.0960,   508.2761,  -260.1228, -1165.4418,
          2648.1460]], dtype=torch.float64)
	q_value: tensor([[-30.5848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.034062536815405164, distance: 1.1246857649938953 entropy 8.713499492553114
epoch: 8, step: 51
	action: tensor([[-2330.8019,  2491.0351,  1640.1034,  2660.2161,  1768.6449,  1338.8243,
          1108.6000]], dtype=torch.float64)
	q_value: tensor([[-36.4870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20730818897220415, distance: 1.0188467844299984 entropy 8.789007460562276
epoch: 8, step: 52
	action: tensor([[-1774.5408,   116.9346,   525.3191,  1602.6385, -1450.8189,  2031.7067,
         -1346.7528]], dtype=torch.float64)
	q_value: tensor([[-36.5410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010683902771651654, distance: 1.1504410442592998 entropy 8.658303160407403
epoch: 8, step: 53
	action: tensor([[-1931.4392, -2198.7949,  -841.7054,   842.1414,   492.7666,  2920.0716,
          3028.5584]], dtype=torch.float64)
	q_value: tensor([[-36.0266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0221666155950895, distance: 1.6272907710009032 entropy 8.61709458430383
epoch: 8, step: 54
	action: tensor([[-1202.7898,  -345.5763,  -476.5692,  -847.3939,   436.2166,  1385.6243,
          -474.8008]], dtype=torch.float64)
	q_value: tensor([[-30.5935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4000569415712152, distance: 1.3540339163658512 entropy 8.563127390510552
epoch: 8, step: 55
	action: tensor([[ 1145.0996, -2545.6466,  -503.0943,  1790.2092,   180.8070,   355.5119,
           742.4850]], dtype=torch.float64)
	q_value: tensor([[-27.1976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06710709900550971, distance: 1.1052807020245 entropy 8.451240449198552
epoch: 8, step: 56
	action: tensor([[-1586.1977,  -135.2370,  1496.3486,  1292.6944,  -536.9501,  1179.1584,
           725.3865]], dtype=torch.float64)
	q_value: tensor([[-22.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.656553738305109, distance: 1.4728531832288798 entropy 8.33774073520552
epoch: 8, step: 57
	action: tensor([[-1454.3025,  1526.3050,   894.2789,  -273.8277, -1139.3769, -2030.7749,
          -268.1115]], dtype=torch.float64)
	q_value: tensor([[-33.4405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3980315413268316, distance: 1.353054151503883 entropy 8.699754273220465
epoch: 8, step: 58
	action: tensor([[  722.2009, -2393.6333,   435.5631,   775.7702,    23.9957,  1259.8855,
          1614.5080]], dtype=torch.float64)
	q_value: tensor([[-33.0943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5776248020466392, distance: 0.7437138981360464 entropy 8.506766146701107
epoch: 8, step: 59
	action: tensor([[  276.7065, -1638.0423,   378.0106,  -995.7079,  -493.0948,   371.7674,
           611.9419]], dtype=torch.float64)
	q_value: tensor([[-27.5927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07428171029395902, distance: 1.1860849198487122 entropy 8.445393807451843
epoch: 8, step: 60
	action: tensor([[ -884.4170, -1456.0844, -1282.3442,   776.8056,  -449.0231,  -211.9838,
          1802.3670]], dtype=torch.float64)
	q_value: tensor([[-24.6060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2840562301584826, distance: 1.2967274800605846 entropy 8.518901865072186
epoch: 8, step: 61
	action: tensor([[-2190.3192, -1759.4786, -1418.0229,   557.0826,  1354.6593,   -61.2234,
          1018.3797]], dtype=torch.float64)
	q_value: tensor([[-27.0864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12219702449822911, distance: 1.0721491795715379 entropy 8.604746375396545
epoch: 8, step: 62
	action: tensor([[-2148.8303,   416.4426,  1025.7662,  1046.6908,   385.7082,   449.9068,
          1431.6962]], dtype=torch.float64)
	q_value: tensor([[-29.1802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20833328308588506, distance: 1.2579114270457765 entropy 8.57754828569875
epoch: 8, step: 63
	action: tensor([[ 216.3913, -238.2500,  758.1852,  -73.2217, -909.4127, -989.0720,
          -33.3574]], dtype=torch.float64)
	q_value: tensor([[-29.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31377266250185154, distance: 0.9479615030256808 entropy 8.45485662827478
epoch: 8, step: 64
	action: tensor([[ -550.0215, -1773.7089, -1483.3874,  1852.9659,   740.2993,  2403.5613,
           604.9857]], dtype=torch.float64)
	q_value: tensor([[-25.9964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08786386818759884, distance: 1.0929153432910106 entropy 8.400591790037986
epoch: 8, step: 65
	action: tensor([[-529.4876, -595.8502, -717.2756,  -71.3120,  519.9783, 1572.0500,
          443.5013]], dtype=torch.float64)
	q_value: tensor([[-29.6612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5166990282085611, distance: 1.409309558577179 entropy 8.563116647240511
epoch: 8, step: 66
	action: tensor([[-1290.8780,   104.9952,   247.6807,  -789.1977,  -766.8008, -1321.1007,
          -483.9266]], dtype=torch.float64)
	q_value: tensor([[-27.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2836904708405601, distance: 0.9685165751306922 entropy 8.583628757030649
epoch: 8, step: 67
	action: tensor([[-1603.4021,  -892.7330, -2036.1167,   965.9581,  -801.9274, -1741.8389,
          1417.0122]], dtype=torch.float64)
	q_value: tensor([[-36.4225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06154509000383479, distance: 1.1790328791225315 entropy 8.371816954990894
epoch: 8, step: 68
	action: tensor([[ -912.3767, -3586.0116, -1173.6277,   185.7650,  -972.4727,  -360.8378,
           412.1212]], dtype=torch.float64)
	q_value: tensor([[-27.5140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6437470000107048, distance: 1.4671488578578036 entropy 8.496694245317812
epoch: 8, step: 69
	action: tensor([[-1272.4790,  -752.0323,   860.6434,   793.4082,   829.1469,  -264.8201,
          1651.7915]], dtype=torch.float64)
	q_value: tensor([[-28.3588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1177717527061368, distance: 1.6653145207811986 entropy 8.526368761866845
epoch: 8, step: 70
	action: tensor([[ -695.2462,    31.6920, -1823.1119,  1368.9850, -1921.0992,  -917.1169,
          -658.2826]], dtype=torch.float64)
	q_value: tensor([[-24.2570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6212742470637027, distance: 1.4570851611744087 entropy 8.320027907066333
epoch: 8, step: 71
	action: tensor([[-724.9108, -488.7519, -267.1933, -836.6839, 1009.8366,  279.1018,
         -469.7755]], dtype=torch.float64)
	q_value: tensor([[-26.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6406568931513958, distance: 1.4657691505568722 entropy 8.356195448948748
epoch: 8, step: 72
	action: tensor([[-2057.5181, -1411.5487,  -409.8493,   272.3069, -2115.0276,  1021.0737,
          1920.3930]], dtype=torch.float64)
	q_value: tensor([[-31.1643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0378489813315395, distance: 1.6335885908369054 entropy 8.514859099373236
epoch: 8, step: 73
	action: tensor([[-1242.8584,   -78.0771, -2319.8984,  1236.4973, -1439.0623,  -846.6719,
          2228.5153]], dtype=torch.float64)
	q_value: tensor([[-32.0512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9961326768708332, distance: 1.6167817390591988 entropy 8.607410523449557
epoch: 8, step: 74
	action: tensor([[-1233.3417,   135.7362, -2090.8688,  -408.6734, -2550.7970, -1487.7216,
         -2778.9266]], dtype=torch.float64)
	q_value: tensor([[-26.5769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7019369463718594, distance: 1.4928921224070995 entropy 8.50154777194876
epoch: 8, step: 75
	action: tensor([[ 123.3243,  778.6332,  191.1869, 1502.2290, -213.3067,  164.0580,
          679.0384]], dtype=torch.float64)
	q_value: tensor([[-29.0106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7540928288750508, distance: 0.5674691940629704 entropy 8.133206307536303
epoch: 8, step: 76
	action: tensor([[-1466.7381,  -677.5522,     4.1587,   469.2490,   807.9340,  1079.8627,
          -323.9559]], dtype=torch.float64)
	q_value: tensor([[-20.6270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8732541648268051, distance: 1.5662282270496457 entropy 8.022405524808166
epoch: 8, step: 77
	action: tensor([[  526.8990, -4726.3811,   869.4133,   529.9494, -1186.1291, -1592.7066,
           762.0402]], dtype=torch.float64)
	q_value: tensor([[-33.0233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3991959782696507, distance: 0.8869989563502968 entropy 8.641941149810492
epoch: 8, step: 78
	action: tensor([[-3471.5108,   321.6684,  -231.7026,  1411.3670,   945.8629,   587.0276,
         -2777.4351]], dtype=torch.float64)
	q_value: tensor([[-30.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.459391419779033
epoch: 8, step: 79
	action: tensor([[  117.3544,  -244.5787, -1540.9719,   376.1555,   -37.5950, -1370.6443,
           163.2762]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08688280973696816, distance: 1.0935029341357063 entropy 7.951509785056763
epoch: 8, step: 80
	action: tensor([[ -312.1360, -1652.5157,    22.7319,  -580.0231, -1626.1333,    -5.7181,
           200.9725]], dtype=torch.float64)
	q_value: tensor([[-22.6773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.920514069299685, distance: 0.3226278285653698 entropy 8.25076694547265
epoch: 8, step: 81
	action: tensor([[ -726.5728,  -593.6937, -1372.7227,  -348.8209,   713.7194,   646.4978,
          -259.0725]], dtype=torch.float64)
	q_value: tensor([[-21.2087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.109219433172765
epoch: 8, step: 82
	action: tensor([[-1098.7136,  -887.6101, -1001.8319,  -353.8701,   270.4704,   606.0906,
           372.7014]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.555672907980606, distance: 1.427301879047465 entropy 7.951509785056763
epoch: 8, step: 83
	action: tensor([[-1034.2856,  -986.8865, -1698.0538,  -164.1132,  -274.2487,  -181.8832,
            50.0602]], dtype=torch.float64)
	q_value: tensor([[-30.5097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17942188341723453, distance: 1.0366130185230327 entropy 8.498942973302833
epoch: 8, step: 84
	action: tensor([[ 410.2169, -453.6712,  922.2077, -844.4704,  -90.6611, -413.2837,
          -51.2299]], dtype=torch.float64)
	q_value: tensor([[-26.7706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35105670862487937, distance: 0.9218495899640191 entropy 8.346018048498077
epoch: 8, step: 85
	action: tensor([[-1014.8903,  -472.9751,  -837.7536, -2283.4842,  2046.9993,    20.9677,
          -896.7093]], dtype=torch.float64)
	q_value: tensor([[-29.9595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15985639921274208, distance: 1.0488984778525086 entropy 8.490727305473044
epoch: 8, step: 86
	action: tensor([[ -772.7619,   982.7766,    54.8552,   421.0920, -1426.4351,   107.6813,
          -877.5383]], dtype=torch.float64)
	q_value: tensor([[-30.1476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2943870706875866, distance: 1.3019334233356148 entropy 8.3923199169952
epoch: 8, step: 87
	action: tensor([[-1885.7828, -1563.1393,   661.2523,    74.1217,   128.7598,   472.6434,
           449.2038]], dtype=torch.float64)
	q_value: tensor([[-30.6610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0544561986755876, distance: 1.640231456229793 entropy 8.235188222672802
epoch: 8, step: 88
	action: tensor([[  755.4666, -1885.8633,  1282.8409, -1029.0279,  1034.8279,  2732.3697,
           398.4567]], dtype=torch.float64)
	q_value: tensor([[-33.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5012355162048092, distance: 0.8081732165760852 entropy 8.706091720551093
epoch: 8, step: 89
	action: tensor([[ -495.6610,   -91.6848, -1080.1806,  1530.4492,    11.2815,   399.7332,
          1219.2704]], dtype=torch.float64)
	q_value: tensor([[-25.5381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8443202509326138, distance: 1.554085329431878 entropy 8.464457021201271
epoch: 8, step: 90
	action: tensor([[ -326.0037,  -552.5903,   408.5922, -1601.4605,  1430.4936,  -272.0942,
           -50.1307]], dtype=torch.float64)
	q_value: tensor([[-22.9682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1810078368654876, distance: 1.035610789066141 entropy 8.24120384408146
epoch: 8, step: 91
	action: tensor([[   19.2482, -2002.7515,   901.4294,  1407.6577,   714.9257,   143.2154,
          -608.0689]], dtype=torch.float64)
	q_value: tensor([[-29.2917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3137937008856342, distance: 1.3116569987502853 entropy 8.358776093324384
epoch: 8, step: 92
	action: tensor([[  475.2756,   530.7757, -1173.7531, -2364.6429,  1312.5162,  -522.2771,
          -678.5307]], dtype=torch.float64)
	q_value: tensor([[-22.3257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.251099645717925
epoch: 8, step: 93
	action: tensor([[-867.6617, -704.9831,  734.0329, -525.9982, -222.1008, -308.6172,
         -420.2130]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15187912514949753, distance: 1.2281747011176525 entropy 7.951509785056763
epoch: 8, step: 94
	action: tensor([[  240.3170, -2178.5182,   729.5623, -1266.8568,  1560.9526, -1079.3033,
           758.4371]], dtype=torch.float64)
	q_value: tensor([[-32.7075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30393641484064404, distance: 1.3067271069242465 entropy 8.491129899624319
epoch: 8, step: 95
	action: tensor([[-2204.1383,  1065.9585,   790.9388,   561.5677,   984.5377, -1208.6223,
          1103.8845]], dtype=torch.float64)
	q_value: tensor([[-29.8674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6603208281558628, distance: 0.6669467372051808 entropy 8.456985732901325
epoch: 8, step: 96
	action: tensor([[-2084.4637,  -888.2077,  -691.0431,   999.3563,   283.7202,  1100.9877,
          -211.9375]], dtype=torch.float64)
	q_value: tensor([[-34.2484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2804872345296061, distance: 0.9706796950582276 entropy 8.400545166108424
epoch: 8, step: 97
	action: tensor([[  565.3445, -2900.4172, -1885.7035,   -68.0164,   -67.3475, -1327.0421,
           877.3736]], dtype=torch.float64)
	q_value: tensor([[-28.7978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15673179173433605, distance: 1.230759025444963 entropy 8.507175554899806
epoch: 8, step: 98
	action: tensor([[ 220.8422, -477.6181, -978.5984,  502.4192, -642.5334, 1476.9601,
         -274.0667]], dtype=torch.float64)
	q_value: tensor([[-26.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.419343389951537
epoch: 8, step: 99
	action: tensor([[ -510.0767,    36.8773, -1132.1103,   816.8962,   267.9763,  1917.5381,
            87.8698]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.951509785056763
epoch: 8, step: 100
	action: tensor([[  93.4021,  141.9719, -556.4116,  363.5479, -124.0658, -866.4984,
          194.7334]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.951509785056763
epoch: 8, step: 101
	action: tensor([[-1171.2716,  -402.5523,   238.0438,  -562.3352,  -181.8363,    20.8101,
          1195.4503]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1952991898178298, distance: 1.2511085897378187 entropy 7.951509785056763
epoch: 8, step: 102
	action: tensor([[ -135.2100,  -633.4557, -1511.8004,  -257.5050,   589.2226, -1036.9820,
           510.9579]], dtype=torch.float64)
	q_value: tensor([[-25.5650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024911055160793838, distance: 1.1585099872565103 entropy 8.235970386071758
epoch: 8, step: 103
	action: tensor([[ -605.9121, -1093.0156,  1837.8680,   715.5758,   580.1802, -1214.0992,
          1048.0795]], dtype=torch.float64)
	q_value: tensor([[-29.9790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21888563765983715, distance: 1.2633921471395755 entropy 8.461214200131204
epoch: 8, step: 104
	action: tensor([[-1221.3125, -1037.6557, -1199.1269,   186.2296,  1342.0509, -2203.7956,
         -1214.9417]], dtype=torch.float64)
	q_value: tensor([[-29.2119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19653291002511186, distance: 1.0257481435863596 entropy 8.486377123334863
epoch: 8, step: 105
	action: tensor([[-3584.9215,  2283.3297,   359.9929, -1383.3402, -1117.5119,  1352.2366,
          1355.2826]], dtype=torch.float64)
	q_value: tensor([[-33.8447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.271039552149017, distance: 1.290138174097177 entropy 8.681363985904861
epoch: 8, step: 106
	action: tensor([[-1440.8982,   494.8525,  1390.4142,  1156.8698,   918.8012,   296.3774,
         -1174.4192]], dtype=torch.float64)
	q_value: tensor([[-28.1632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16211880784956434, distance: 1.2336215807027118 entropy 8.278814473961472
epoch: 8, step: 107
	action: tensor([[ -273.7515, -1012.1218,  1050.7576,  -276.0183,  2112.0437, -1900.9912,
          2083.0310]], dtype=torch.float64)
	q_value: tensor([[-29.1158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0454504076500712, distance: 1.6366324977144888 entropy 8.385007309783168
epoch: 8, step: 108
	action: tensor([[ -939.9832, -2653.0359,  1328.6103,  -262.3786,  1106.5743,  -457.1991,
           490.0765]], dtype=torch.float64)
	q_value: tensor([[-27.0007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11143051821972794, distance: 1.0787042500914585 entropy 8.316159528854401
epoch: 8, step: 109
	action: tensor([[ 2234.8387, -1248.9594,  -459.2849,  -704.8428,  2079.5979,   929.2936,
          -853.9784]], dtype=torch.float64)
	q_value: tensor([[-30.8749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.522779075014004
epoch: 8, step: 110
	action: tensor([[ -273.7506,    61.8498,  -268.6860,   -61.8621,   555.3378,   -72.8700,
         -1042.3414]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4340258807052262, distance: 1.3703616237924936 entropy 7.951509785056763
epoch: 8, step: 111
	action: tensor([[  273.4782,   244.7266,   693.0440, -1464.2596, -1724.0059,  1454.0156,
           110.2177]], dtype=torch.float64)
	q_value: tensor([[-27.1594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24502676970412773, distance: 1.2768681025400086 entropy 8.158831702783967
epoch: 8, step: 112
	action: tensor([[-1380.7474, -1559.6661,   714.5104,   477.0438,   844.1203,  1254.4427,
          1927.8672]], dtype=torch.float64)
	q_value: tensor([[-29.7494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1029577627311093, distance: 1.6594797923142182 entropy 8.432341989416095
epoch: 8, step: 113
	action: tensor([[-1382.7953,   410.2589,  -709.0210,  2284.8130,   662.8221, -2363.7511,
           808.7000]], dtype=torch.float64)
	q_value: tensor([[-31.3206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19744386302355987, distance: 1.0251664935338687 entropy 8.624849901180308
epoch: 8, step: 114
	action: tensor([[ -407.3199, -1735.1705, -1070.5983,  2421.7248,  -816.2459, -1975.6538,
           438.9653]], dtype=torch.float64)
	q_value: tensor([[-34.0095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25534359022474495, distance: 1.2821475238847708 entropy 8.593610090524866
epoch: 8, step: 115
	action: tensor([[ -829.6682, -2247.4236,   369.4282,   534.1486,  1207.0255,  1595.3103,
         -1073.8474]], dtype=torch.float64)
	q_value: tensor([[-29.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11519422853546235, distance: 1.2084590817600585 entropy 8.587835044606297
epoch: 8, step: 116
	action: tensor([[-2373.3059,    76.0090, -1157.7592,  1903.6233,  -398.9766,  -162.6549,
           714.2523]], dtype=torch.float64)
	q_value: tensor([[-26.4334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11171063641959589, distance: 1.2065701411861058 entropy 8.401616360453383
epoch: 8, step: 117
	action: tensor([[-2374.5834, -3830.9116, -1273.0256,  -279.4794,  -542.4043,  -812.7152,
          -324.0309]], dtype=torch.float64)
	q_value: tensor([[-27.4470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6116307655529805, distance: 1.4527452628161523 entropy 8.471683292608395
epoch: 8, step: 118
	action: tensor([[-1218.1887, -2415.0986,  4459.0446,  1744.6988,  1587.8873,  2980.6367,
         -1079.8499]], dtype=torch.float64)
	q_value: tensor([[-35.3465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06853816898756149, distance: 1.1829100275246092 entropy 8.608790925153858
epoch: 8, step: 119
	action: tensor([[-1126.7980, -3041.5749,   -60.3517, -1106.4441,  -487.7642,   665.7505,
           665.3530]], dtype=torch.float64)
	q_value: tensor([[-32.8453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9927915549103166, distance: 1.6154280897409101 entropy 8.685202266645977
epoch: 8, step: 120
	action: tensor([[-2747.8295,  -140.3153,  1085.1723,  -398.8928,   196.9313,  1196.7821,
           372.0423]], dtype=torch.float64)
	q_value: tensor([[-29.8946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2680246294475961, distance: 1.2886071532144936 entropy 8.606863181002618
epoch: 8, step: 121
	action: tensor([[ -677.0854, -2870.7282,  -335.6180, -1336.3257,  1412.6383,   371.3730,
            71.1135]], dtype=torch.float64)
	q_value: tensor([[-34.5873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03411903739450062, distance: 1.1246528713911974 entropy 8.638797192709214
epoch: 8, step: 122
	action: tensor([[ -115.4978, -2493.4508, -1026.1380, -1125.4171,  1531.4867,   -60.8752,
          -749.6586]], dtype=torch.float64)
	q_value: tensor([[-34.5682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4043746785094009, distance: 1.3561202106853898 entropy 8.58452044035832
epoch: 8, step: 123
	action: tensor([[-1601.6891,   238.7592, -1787.1299,  -293.4524, -2700.4996,  1934.9910,
           377.1608]], dtype=torch.float64)
	q_value: tensor([[-35.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.581284564496958
epoch: 8, step: 124
	action: tensor([[-646.8782, -240.0103,  435.4939,  691.5774,  -22.8292, -973.5163,
          720.0474]], dtype=torch.float64)
	q_value: tensor([[-30.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15176362617128125, distance: 1.0539381839167676 entropy 7.951509785056763
epoch: 8, step: 125
	action: tensor([[-2306.7092, -1893.0365, -1690.5462,  1128.8835,  -123.8189,   -57.7548,
             6.4930]], dtype=torch.float64)
	q_value: tensor([[-30.1417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3743201314910012, distance: 1.3415307980297113 entropy 8.522885243443474
epoch: 8, step: 126
	action: tensor([[ -454.4931, -1234.0858,   182.5342,   277.9264,   -25.7151,  1766.8895,
          -188.0419]], dtype=torch.float64)
	q_value: tensor([[-24.7784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07738337591539057, distance: 1.1877959176371757 entropy 8.374769489781755
epoch: 8, step: 127
	action: tensor([[-1545.2036,   335.6795,   501.1634,   232.2018,   110.0492,  -238.6517,
           334.8745]], dtype=torch.float64)
	q_value: tensor([[-25.8809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21566420689835064, distance: 1.0134625627188298 entropy 8.371369164348767
LOSS epoch 8 actor 359.3670662070265 critic 210.79482688852912
epoch: 9, step: 0
	action: tensor([[-2107.1983,  -564.3766,  1235.7167, -1176.9852,  1122.2136, -1401.6840,
           476.4631]], dtype=torch.float64)
	q_value: tensor([[-30.8908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.713581458832618
epoch: 9, step: 1
	action: tensor([[ -658.9906, -1772.8416,  -641.6632,   155.5196,   439.1993,  -573.3926,
           844.6529]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.057160553772038
epoch: 9, step: 2
	action: tensor([[ 269.2061,  386.6973,    2.2956,  466.7540, -798.9685,  505.7965,
         1898.1236]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.057160553772038
epoch: 9, step: 3
	action: tensor([[ -625.3986, -1099.7357,   710.7739,   386.4724,   -21.5081,  1701.7225,
           407.2238]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8500494280434951, distance: 1.5564972549844585 entropy 8.057160553772038
epoch: 9, step: 4
	action: tensor([[-4353.9097,   314.6166, -1808.6398,   702.3400,  -638.2796, -1983.4336,
          1585.3626]], dtype=torch.float64)
	q_value: tensor([[-26.0289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19331953556555825, distance: 1.0277972778290974 entropy 8.627952483481748
epoch: 9, step: 5
	action: tensor([[-4712.9797, -1839.9368, -2850.9592,   703.7777,  2299.0529,   855.2090,
           291.4371]], dtype=torch.float64)
	q_value: tensor([[-30.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00657976954078876, distance: 1.140573279994359 entropy 8.698346138893642
epoch: 9, step: 6
	action: tensor([[ -704.3222, -2265.6867,  -925.8897,  1058.2971,  -991.9113,   531.6364,
            78.3076]], dtype=torch.float64)
	q_value: tensor([[-26.7791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09717462532866006, distance: 1.1986560196875184 entropy 8.622345471820935
epoch: 9, step: 7
	action: tensor([[   49.9833,  -382.3240, -1288.4573,   165.0021,  -856.7684, -2490.4476,
          2261.9042]], dtype=torch.float64)
	q_value: tensor([[-32.4365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5310482473870783, distance: 1.4159604780911337 entropy 8.812007917194792
epoch: 9, step: 8
	action: tensor([[-2317.5606,    22.1584,  1326.0661,   867.4862,   122.9677,   649.5308,
           332.0149]], dtype=torch.float64)
	q_value: tensor([[-22.7630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07228561272964273, distance: 1.1022087113809875 entropy 8.549232191636811
epoch: 9, step: 9
	action: tensor([[-1537.1197,  -582.8177,  -295.3152,  -152.7445, -1486.5693,   797.2048,
          1920.6727]], dtype=torch.float64)
	q_value: tensor([[-26.9952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.463164192187403, distance: 1.3842139616097022 entropy 8.554928238916737
epoch: 9, step: 10
	action: tensor([[-1622.3541,   318.2333,   137.7884,   590.9908,   227.8926,  1654.9295,
         -1068.5703]], dtype=torch.float64)
	q_value: tensor([[-23.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.597848514074577, distance: 0.7256906576695514 entropy 8.532842962718693
epoch: 9, step: 11
	action: tensor([[-1298.2986,   267.1084,  -748.4904, -1295.7359,   131.5377, -2534.1390,
           318.4727]], dtype=torch.float64)
	q_value: tensor([[-28.3188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010349428959508744, distance: 1.1384071979819892 entropy 8.483454033856548
epoch: 9, step: 12
	action: tensor([[ -917.8485, -1993.9637,  1013.5976,  2460.7584,  2489.9200,  -468.1099,
          -428.1858]], dtype=torch.float64)
	q_value: tensor([[-27.4278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4570335854367975, distance: 1.3813110135811526 entropy 8.295628793749092
epoch: 9, step: 13
	action: tensor([[-1653.0422,  1943.9060,  -215.5368,  -807.0440,  2904.6147,  -867.5487,
          -664.4603]], dtype=torch.float64)
	q_value: tensor([[-23.3656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0704947130875837, distance: 1.183992514430542 entropy 8.52034811229975
epoch: 9, step: 14
	action: tensor([[-1.3769e+03, -1.3467e+03, -6.9377e-01, -8.6554e+02, -6.3629e+02,
          2.2216e+02, -2.3651e+02]], dtype=torch.float64)
	q_value: tensor([[-28.4216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1347240396749516, distance: 1.2189947105018994 entropy 8.328020525151386
epoch: 9, step: 15
	action: tensor([[ -558.9469, -1198.8278,  -476.0817,   -98.7625,    54.4160,  2622.7066,
           809.4280]], dtype=torch.float64)
	q_value: tensor([[-27.9643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3075318226289194, distance: 1.3085274181071558 entropy 8.593779212898351
epoch: 9, step: 16
	action: tensor([[-673.4288, -773.1295, -334.1393,  274.5924,  147.1199, -239.1398,
         1071.9581]], dtype=torch.float64)
	q_value: tensor([[-29.8252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4489473792676133, distance: 1.7907972550521443 entropy 8.644732355970541
epoch: 9, step: 17
	action: tensor([[-1689.6168,  -175.9758,  -344.3608,  1256.2606,   797.3603,  1275.3960,
           465.1927]], dtype=torch.float64)
	q_value: tensor([[-25.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3335182972605333, distance: 1.3214665755175157 entropy 8.548193178352175
epoch: 9, step: 18
	action: tensor([[  240.8272, -1755.9326, -2564.5924, -1977.3703,  1077.4161, -1971.0555,
            18.3983]], dtype=torch.float64)
	q_value: tensor([[-25.2205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05942162317728372, distance: 1.1778530467610377 entropy 8.519998558370341
epoch: 9, step: 19
	action: tensor([[-1786.7142,    80.0399,   601.4137,  1454.5037,  -722.0792,  1709.2726,
          1896.7784]], dtype=torch.float64)
	q_value: tensor([[-24.1706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2033868387622355, distance: 1.0213637312242474 entropy 8.444496235362086
epoch: 9, step: 20
	action: tensor([[-718.7770,  716.2634,  766.5523, 1558.5626, 1033.3522, 1056.2686,
          586.2220]], dtype=torch.float64)
	q_value: tensor([[-25.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5715849026686932, distance: 0.7490125193176582 entropy 8.472583366486163
epoch: 9, step: 21
	action: tensor([[-2620.7211,   -22.7212, -1248.1800,   823.4064,   857.7771,   287.9931,
           749.1325]], dtype=torch.float64)
	q_value: tensor([[-30.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05055141019977194, distance: 1.1150450655983088 entropy 8.592387142625721
epoch: 9, step: 22
	action: tensor([[ -396.6655,   627.6009, -2343.1392, -1379.3199, -2849.3100,  -438.6783,
           653.9784]], dtype=torch.float64)
	q_value: tensor([[-30.9871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3337545351052189, distance: 0.9340579607509698 entropy 8.721179672933179
epoch: 9, step: 23
	action: tensor([[-2218.1124, -2455.1517, -1146.0131,  -658.7586,  -623.3753,   870.1453,
           635.9134]], dtype=torch.float64)
	q_value: tensor([[-31.2829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8294677746255343, distance: 1.547815085972226 entropy 8.55814494413064
epoch: 9, step: 24
	action: tensor([[-1650.1502, -1115.3899,   431.2289,   877.0265,  -875.1894,   153.6033,
          2288.5909]], dtype=torch.float64)
	q_value: tensor([[-25.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6850996507627614, distance: 1.4854891619579544 entropy 8.530419988447504
epoch: 9, step: 25
	action: tensor([[-1965.7081, -1583.5961, -1067.6320,  4244.3125,  1336.3317,   -30.5471,
          -390.8321]], dtype=torch.float64)
	q_value: tensor([[-29.4800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1280226242984488, distance: 1.2153898310798446 entropy 8.685731994825188
epoch: 9, step: 26
	action: tensor([[-1532.4164,   115.2501,   838.2394,  -537.7961, -1918.4663,  1235.3668,
          -194.3588]], dtype=torch.float64)
	q_value: tensor([[-25.8190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009164393978629892, distance: 1.1390885742096253 entropy 8.605775810862614
epoch: 9, step: 27
	action: tensor([[-1333.5276, -3022.6100, -1305.8122,   716.0156, -2124.6929,   508.2680,
          1857.8606]], dtype=torch.float64)
	q_value: tensor([[-28.7216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29353347204437963, distance: 1.301504064950876 entropy 8.523356350628152
epoch: 9, step: 28
	action: tensor([[  292.8238,  1857.0109, -1177.0000,  2209.4088,  2553.8899,  -538.1613,
          1752.0169]], dtype=torch.float64)
	q_value: tensor([[-26.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39777867802409606, distance: 0.8880445579640995 entropy 8.576991758084048
epoch: 9, step: 29
	action: tensor([[ 1103.0803, -1933.5729,  -691.4637,  -371.5748,   768.8937,  2144.7076,
          1601.8942]], dtype=torch.float64)
	q_value: tensor([[-29.6229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5349925868430347, distance: 1.4177832280701992 entropy 8.662115968038703
epoch: 9, step: 30
	action: tensor([[ -527.7629,  -682.6702,  1391.4011,   225.3065, -2126.5616,  -291.9449,
          -958.6170]], dtype=torch.float64)
	q_value: tensor([[-18.1952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47202959478940687, distance: 1.3884011477119684 entropy 8.250807853773722
epoch: 9, step: 31
	action: tensor([[   85.9251,  -388.2233, -2400.0560,  -685.4664, -2226.0025, -1035.7373,
          -894.0900]], dtype=torch.float64)
	q_value: tensor([[-23.8576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16155953157497338, distance: 1.0478347792121998 entropy 8.66727157380525
epoch: 9, step: 32
	action: tensor([[  871.5212,   -57.2329,   478.7387, -1382.8312, -1123.6692,  -610.1866,
           552.9758]], dtype=torch.float64)
	q_value: tensor([[-23.4483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6113205709640417, distance: 0.7134318130549816 entropy 8.487834603786968
epoch: 9, step: 33
	action: tensor([[  805.8458,   193.9349,  1420.4978, -3036.5652,  -715.3313,   905.4729,
          1316.9708]], dtype=torch.float64)
	q_value: tensor([[-24.2854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6566870634432072, distance: 0.6705046245481746 entropy 8.378484367960839
epoch: 9, step: 34
	action: tensor([[-614.7974,  341.2269, 1367.0432,  497.1288,  463.8873, 2130.7962,
         1155.7919]], dtype=torch.float64)
	q_value: tensor([[-23.7506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06815942603064895, distance: 1.1046571336902633 entropy 8.454880700652655
epoch: 9, step: 35
	action: tensor([[-844.2701,  -30.6481, -972.2770,  202.7745,  362.6701,  280.1355,
          519.9718]], dtype=torch.float64)
	q_value: tensor([[-22.7792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.061092273900207106, distance: 1.1787813862629803 entropy 8.18476333927248
epoch: 9, step: 36
	action: tensor([[ -230.7243, -1182.7994,  1473.1832,  2146.7011,  -443.4767,   -98.4295,
          1832.6630]], dtype=torch.float64)
	q_value: tensor([[-27.6124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25614559498044565, distance: 0.9869625251392309 entropy 8.54132553371414
epoch: 9, step: 37
	action: tensor([[-1419.3925, -1364.8139,  -927.5790,  2081.7270,   583.7509,  1341.1222,
          -687.9876]], dtype=torch.float64)
	q_value: tensor([[-28.7927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09672858836566522, distance: 1.1984123487043357 entropy 8.744693970399947
epoch: 9, step: 38
	action: tensor([[ -971.7356,  -106.6673, -1103.0478, -1135.2958,  -268.0635, -1659.2752,
          1623.3345]], dtype=torch.float64)
	q_value: tensor([[-30.5091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4565391330411122, distance: 1.7935708490988869 entropy 8.794854433514761
epoch: 9, step: 39
	action: tensor([[3286.7361, -576.8998, -345.8241, -199.2203,  360.0240, -211.2743,
          128.2010]], dtype=torch.float64)
	q_value: tensor([[-24.2563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3912692056072947, distance: 0.8928311406981937 entropy 8.6009066421703
epoch: 9, step: 40
	action: tensor([[ -246.8822,  -586.1456,   256.6763,  1018.2419, -1332.3556,  1130.8230,
           846.1066]], dtype=torch.float64)
	q_value: tensor([[-25.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1915580208928986, distance: 1.6940771308702463 entropy 8.642499103585903
epoch: 9, step: 41
	action: tensor([[ 1278.2948, -2639.9616,   372.0096,   446.5576, -1683.9338,  -746.8031,
           980.0103]], dtype=torch.float64)
	q_value: tensor([[-21.3913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42574520095391544, distance: 0.8671795144849198 entropy 8.449291942256918
epoch: 9, step: 42
	action: tensor([[ -623.9104,  -850.2212, -1056.8821,   305.6302,   218.2006,  1173.2818,
           141.3703]], dtype=torch.float64)
	q_value: tensor([[-22.1461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.649762021476866, distance: 1.4698308013885333 entropy 8.35243345050955
epoch: 9, step: 43
	action: tensor([[ 1569.0519,  -145.2198, -3267.2868,   -14.3855, -1158.1605, -2002.4619,
          -519.0786]], dtype=torch.float64)
	q_value: tensor([[-25.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30672378544652334, distance: 0.9528177597453716 entropy 8.708433165776668
epoch: 9, step: 44
	action: tensor([[ -572.0046, -3269.0483, -2262.6505,   518.0668,   450.0338,  2766.3477,
          -110.1998]], dtype=torch.float64)
	q_value: tensor([[-23.3002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9341925380186338, distance: 1.5914996410644386 entropy 8.460988467231866
epoch: 9, step: 45
	action: tensor([[  -14.7108,    80.3911, -1011.8940,  -762.9543, -2627.5442,  1866.6442,
          1228.5813]], dtype=torch.float64)
	q_value: tensor([[-30.3789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8769639252018218, distance: 1.5677783257225284 entropy 8.775613036221019
epoch: 9, step: 46
	action: tensor([[ 2296.3169,  -674.8186,   435.2315,  -181.6032, -1033.4269,   468.9120,
          -572.1570]], dtype=torch.float64)
	q_value: tensor([[-26.6329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39982150867062394, distance: 0.8865370841857342 entropy 8.520373116090889
epoch: 9, step: 47
	action: tensor([[ -323.4663,   551.3033, -1265.3548,  3139.8471, -1486.3357,  1321.1477,
         -2031.4592]], dtype=torch.float64)
	q_value: tensor([[-23.9332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2265074354648977, distance: 1.2673360418067818 entropy 8.578579617279193
epoch: 9, step: 48
	action: tensor([[-799.7922, -495.9083, -916.9277, 2525.3881, -355.7076, -278.8857,
          236.6544]], dtype=torch.float64)
	q_value: tensor([[-26.7598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06407595739754313, distance: 1.1070748765366685 entropy 8.527688392159417
epoch: 9, step: 49
	action: tensor([[ 1347.6860, -1119.9170, -1620.0148,  -433.6894, -3551.3006,   908.1265,
          -532.2586]], dtype=torch.float64)
	q_value: tensor([[-25.6167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1735664327015386, distance: 1.0403049562880287 entropy 8.592605509845118
epoch: 9, step: 50
	action: tensor([[-1732.7027,  -772.2210, -1512.2659,   461.3940,  1321.9767,  1020.0820,
            35.6685]], dtype=torch.float64)
	q_value: tensor([[-20.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1312583176152613, distance: 1.217131734369156 entropy 8.416780924584618
epoch: 9, step: 51
	action: tensor([[  799.0038, -1045.8942,  1563.5019,   390.4276,   234.8534,  -115.5216,
          -344.1223]], dtype=torch.float64)
	q_value: tensor([[-26.6789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7288996140560118, distance: 0.5958291700652264 entropy 8.729177521018473
epoch: 9, step: 52
	action: tensor([[-1090.1846,  -692.9302,   343.5803,   723.3898,  -666.5591,  -621.6290,
          -585.8529]], dtype=torch.float64)
	q_value: tensor([[-25.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9612374250355267, distance: 1.602587604456865 entropy 8.501357950578956
epoch: 9, step: 53
	action: tensor([[-1711.0608,  1336.1673, -1534.0770,   623.3991, -1003.7725,   376.7636,
         -1259.6476]], dtype=torch.float64)
	q_value: tensor([[-23.7156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5088574564071154, distance: 0.8019743363162087 entropy 8.585292384971591
epoch: 9, step: 54
	action: tensor([[-1530.5979, -1448.7031,  1221.0167,  1163.3815,  2813.0605,   682.4902,
          -346.5465]], dtype=torch.float64)
	q_value: tensor([[-29.9024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8380498094072728, distance: 1.551441239273706 entropy 8.633508912651395
epoch: 9, step: 55
	action: tensor([[ 299.0205, -765.5603, 2334.4703, -237.9933, 2124.3166, -408.0075,
         2478.3996]], dtype=torch.float64)
	q_value: tensor([[-27.5348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3098168326776909, distance: 1.30967029397308 entropy 8.689035263990759
epoch: 9, step: 56
	action: tensor([[-2451.9981,  1253.6824,  -854.0954,   304.8258,  2139.6021, -1616.8153,
          1182.7067]], dtype=torch.float64)
	q_value: tensor([[-24.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.525916170173804
epoch: 9, step: 57
	action: tensor([[ -406.4577, -1873.0960,  -785.6592,  -891.8756,  -871.2741,  -216.8612,
          1686.4052]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5229526899791583, distance: 1.4122120062673096 entropy 8.057160553772038
epoch: 9, step: 58
	action: tensor([[   49.8262, -1036.9391, -1010.0778,  -239.3253,   -11.3018,  1895.9824,
         -1589.0600]], dtype=torch.float64)
	q_value: tensor([[-23.8974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5460508017505865, distance: 1.4228809778371307 entropy 8.45135846731391
epoch: 9, step: 59
	action: tensor([[-1628.3075,    88.8380, -1265.0475, -1243.6998,  -682.5072,  -379.3257,
           893.7889]], dtype=torch.float64)
	q_value: tensor([[-19.5595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2587543688275722, distance: 0.9852303155412457 entropy 8.307766085374073
epoch: 9, step: 60
	action: tensor([[ -204.7218, -3346.5691,  -560.1404,  1920.5260,  1839.7291,  2143.6962,
           659.1518]], dtype=torch.float64)
	q_value: tensor([[-36.3721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3952524524219352, distance: 1.351708642392053 entropy 8.698019427993504
epoch: 9, step: 61
	action: tensor([[-1703.2761, -1326.4055,  -286.0638,   103.8395,  -180.1735,   637.5572,
          1176.5292]], dtype=torch.float64)
	q_value: tensor([[-24.5132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4272694259341463, distance: 1.3671295629267572 entropy 8.478712021188674
epoch: 9, step: 62
	action: tensor([[-3346.9564,  -607.5867,   347.5501,   723.9001,  -431.7132,   867.9581,
           455.0274]], dtype=torch.float64)
	q_value: tensor([[-30.9403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058591937781405035, distance: 1.1773917389787287 entropy 8.785115381928597
epoch: 9, step: 63
	action: tensor([[ -497.6238, -1673.1636,  1370.3415,  1259.0972,  -114.2626,  -251.2433,
          1042.4932]], dtype=torch.float64)
	q_value: tensor([[-29.8070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19060249734165913, distance: 1.248648178179113 entropy 8.741673640983167
epoch: 9, step: 64
	action: tensor([[ -769.0402, -1880.3436,  2762.0869,  1256.6871,   -44.7040,   300.6923,
         -1310.7611]], dtype=torch.float64)
	q_value: tensor([[-23.6984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02524239102181658, distance: 1.1298089487015668 entropy 8.48975733872021
epoch: 9, step: 65
	action: tensor([[  674.4751,  2114.4692, -2139.0048,  -573.9998, -2284.4708, -1464.7518,
         -1121.1735]], dtype=torch.float64)
	q_value: tensor([[-28.9048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.71411118438373
epoch: 9, step: 66
	action: tensor([[-1013.1839, -1471.6801,   427.2318,  1921.0122,  1027.9113,   370.3135,
          -146.3095]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8586329791802214, distance: 1.5601038648221408 entropy 8.057160553772038
epoch: 9, step: 67
	action: tensor([[-2550.7605, -2966.4599,   715.4588,   190.6827,  3270.5791,   749.5873,
           197.6582]], dtype=torch.float64)
	q_value: tensor([[-26.4451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9910576660858146, distance: 1.6147251606669355 entropy 8.69169903158665
epoch: 9, step: 68
	action: tensor([[-3060.5009,  -574.9931,  -134.1304,   809.8221,  1889.9335, -1817.8617,
          1385.9152]], dtype=torch.float64)
	q_value: tensor([[-31.9740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6561341534909375, distance: 1.4726666435616236 entropy 8.825502804830261
epoch: 9, step: 69
	action: tensor([[ 1918.3494, -1007.8368, -1846.8211,   -93.2995,  1219.5843, -2713.3336,
          3552.8197]], dtype=torch.float64)
	q_value: tensor([[-27.1323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22375075381056808, distance: 1.265911016898593 entropy 8.751045760930332
epoch: 9, step: 70
	action: tensor([[-1708.9224, -1183.6592,   806.7663,  1027.8033,  -221.8259, -2384.5628,
           -71.4269]], dtype=torch.float64)
	q_value: tensor([[-23.1179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27884376119857424, distance: 0.9717876505664258 entropy 8.461163494513732
epoch: 9, step: 71
	action: tensor([[-1109.3441, -3400.6929,   700.3972,  3115.6260,   782.4465, -1394.1701,
           496.5327]], dtype=torch.float64)
	q_value: tensor([[-28.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09088475179902655, distance: 1.0911040411896182 entropy 8.688243860874824
epoch: 9, step: 72
	action: tensor([[ -814.5097, -1502.3642, -1917.9319,   214.5973,  -816.2942,  -420.1386,
          1373.3609]], dtype=torch.float64)
	q_value: tensor([[-30.6475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45709287159941336, distance: 1.3813391158147472 entropy 8.818920325312396
epoch: 9, step: 73
	action: tensor([[-1562.3007,  -885.1439,  -666.9071, -1215.2061,  3096.5659,  1016.0082,
           -50.6864]], dtype=torch.float64)
	q_value: tensor([[-26.4925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7423629013821922, distance: 1.5105183342890531 entropy 8.72281377761333
epoch: 9, step: 74
	action: tensor([[-2185.8777,   -85.5334,  -201.0255, -1810.0571,    12.2918,  -577.1176,
           -62.0836]], dtype=torch.float64)
	q_value: tensor([[-22.9627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4062567560728132, distance: 0.881771451220125 entropy 8.357056530916248
epoch: 9, step: 75
	action: tensor([[  295.1266, -1775.9869,  -813.6698,  -649.2592,    35.0721,  -346.8484,
          -584.1950]], dtype=torch.float64)
	q_value: tensor([[-23.6680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.413899652453688, distance: 1.3607112866563202 entropy 8.385900481916625
epoch: 9, step: 76
	action: tensor([[-1297.1919, -1152.4469,  -923.0215,  -457.8000,   -74.9097, -2218.4232,
           949.5226]], dtype=torch.float64)
	q_value: tensor([[-26.5540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31422062375170556, distance: 1.3118700957078 entropy 8.565050031229461
epoch: 9, step: 77
	action: tensor([[ 1018.4800, -1964.8357,  -599.9860, -1078.4292,   269.1899,  -285.1724,
          -251.3435]], dtype=torch.float64)
	q_value: tensor([[-25.7129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14028435557307029, distance: 1.0610457451248188 entropy 8.506955872012815
epoch: 9, step: 78
	action: tensor([[-1756.5637, -2172.1884,   394.5476,  1575.3552,  -837.1109, -2384.1609,
          -691.9329]], dtype=torch.float64)
	q_value: tensor([[-25.0228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8970074649526083, distance: 1.5761270159404215 entropy 8.44955844425008
epoch: 9, step: 79
	action: tensor([[-807.2653,  145.8564, -153.5635, -637.8490,   -7.6089, 2879.0855,
          269.7511]], dtype=torch.float64)
	q_value: tensor([[-25.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.533346624299977
epoch: 9, step: 80
	action: tensor([[ -744.8377,  1084.6660,  -458.9048,  1207.1216, -1030.3094,   560.0561,
           951.1912]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.057160553772038
epoch: 9, step: 81
	action: tensor([[1252.8045,  615.5218,  211.4878, -858.0776, -529.3389,  121.4972,
          865.2377]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.057160553772038
epoch: 9, step: 82
	action: tensor([[-1875.6901,   469.3208, -1140.9263,   -87.8066,    70.4614,  -470.8033,
           -79.2994]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.057160553772038
epoch: 9, step: 83
	action: tensor([[-605.6113, -276.0463,  561.4603,  147.4510, -309.8408, -630.8430,
          500.3762]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.057160553772038
epoch: 9, step: 84
	action: tensor([[  567.4461, -1673.3520,   875.1324,  -752.8882,   940.3379,   106.7928,
           342.1106]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.057160553772038
epoch: 9, step: 85
	action: tensor([[-501.6645, -478.8931, -995.1545, -248.6017, -719.1341,  496.4702,
          169.8306]], dtype=torch.float64)
	q_value: tensor([[-27.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012043097729211, distance: 0.9566031402247671 entropy 8.057160553772038
epoch: 9, step: 86
	action: tensor([[  -60.3470, -2918.7471, -1370.2108,  1254.5768,  -716.3575,  2641.3412,
          1793.0126]], dtype=torch.float64)
	q_value: tensor([[-32.1576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7827847592737234, distance: 1.5279394693596708 entropy 8.674651369141861
epoch: 9, step: 87
	action: tensor([[-1461.4094, -1374.4513,  -202.6263,  -810.2199,  -664.9768,  -933.4479,
          1032.9618]], dtype=torch.float64)
	q_value: tensor([[-22.7846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32950879783743725, distance: 0.9370294381302303 entropy 8.434653134840248
epoch: 9, step: 88
	action: tensor([[-1963.6682,   546.0169, -1037.5948,  1121.0108,  1316.8269,  -343.3235,
           392.8603]], dtype=torch.float64)
	q_value: tensor([[-29.0608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2782155103601227, distance: 0.9722108553194276 entropy 8.664807780337599
epoch: 9, step: 89
	action: tensor([[-2460.1333,   344.7178, -1475.9979,   907.2183,  -130.7499,   293.2036,
          3820.0448]], dtype=torch.float64)
	q_value: tensor([[-34.4311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5098348250867577, distance: 0.801175978540391 entropy 8.742068501894746
epoch: 9, step: 90
	action: tensor([[  233.3516,  -899.4307,  -448.8958,   410.1816, -1005.6787,   731.1662,
         -1029.9817]], dtype=torch.float64)
	q_value: tensor([[-26.4026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25415336951153444, distance: 0.9882833058588303 entropy 8.389396485405188
epoch: 9, step: 91
	action: tensor([[-504.5497, -106.3078,  204.6609,  316.0224,  500.1126, 1062.1803,
          335.4916]], dtype=torch.float64)
	q_value: tensor([[-19.6593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5568556272260163, distance: 1.4278443377691745 entropy 8.319283861911828
epoch: 9, step: 92
	action: tensor([[-1281.3843, -2060.4987, -1170.0261,  1192.0251,  1319.6333,  -214.9009,
         -1465.8113]], dtype=torch.float64)
	q_value: tensor([[-31.3412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9261452022061443, distance: 1.5881854205234438 entropy 8.803762910038502
epoch: 9, step: 93
	action: tensor([[-1341.1509, -2308.9851,  1568.5553,  2809.0632,  1868.1581, -1075.0957,
          2171.7030]], dtype=torch.float64)
	q_value: tensor([[-26.4825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17757586563675065, distance: 1.0377783743834177 entropy 8.65839337575037
epoch: 9, step: 94
	action: tensor([[ -646.4272,    46.6618,  -213.4766, -1826.9789, -1518.8770,   384.7587,
          1553.6386]], dtype=torch.float64)
	q_value: tensor([[-27.1179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09000868886735103, distance: 1.1947352381974907 entropy 8.75759340161238
epoch: 9, step: 95
	action: tensor([[-407.0897, -786.1404, -203.8848, 1504.4200, -509.5219,  143.3505,
          720.0156]], dtype=torch.float64)
	q_value: tensor([[-27.2632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20307932952767838, distance: 1.0215608460160241 entropy 8.47083523094601
epoch: 9, step: 96
	action: tensor([[-2607.0853,  -803.0229, -1862.5804,     8.2652,   -16.9257, -1256.3789,
          1503.7929]], dtype=torch.float64)
	q_value: tensor([[-28.4823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18515928599699505, distance: 1.0329827128784088 entropy 8.796973311665244
epoch: 9, step: 97
	action: tensor([[ -197.0808,   395.1014,  2490.2650,  -749.0786,  1342.0537,  -202.3417,
         -2519.8444]], dtype=torch.float64)
	q_value: tensor([[-29.6014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3440246333882693, distance: 1.3266620546025807 entropy 8.759673188141232
epoch: 9, step: 98
	action: tensor([[-2943.6676,  1119.4215,  -244.5712,  1076.7491,   281.7210, -1300.3939,
          1947.5988]], dtype=torch.float64)
	q_value: tensor([[-29.7709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4311013728765911, distance: 0.8631258748783079 entropy 8.55550166102927
epoch: 9, step: 99
	action: tensor([[ -602.3425, -2329.3328,   708.3820, -1149.4972, -1064.0519,  -932.1376,
           756.5753]], dtype=torch.float64)
	q_value: tensor([[-25.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9738113651528133, distance: 1.607716673871937 entropy 8.430568996232218
epoch: 9, step: 100
	action: tensor([[-2133.8590,   651.7279,   506.1743,  1759.2680,  -810.9221,  1843.4971,
          -247.6356]], dtype=torch.float64)
	q_value: tensor([[-25.3446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08517416726204452, distance: 1.0945255478523945 entropy 8.54292594671478
epoch: 9, step: 101
	action: tensor([[-1412.1165,  -477.8184,  -807.3160, -1219.7461,   878.8116,  -579.1459,
          3468.4133]], dtype=torch.float64)
	q_value: tensor([[-31.0632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9058972430757879, distance: 1.5798157317087236 entropy 8.671651581151137
epoch: 9, step: 102
	action: tensor([[ -317.3737, -1633.5171, -1576.0231,   767.1480,    98.7639,  1058.7712,
          1079.5902]], dtype=torch.float64)
	q_value: tensor([[-25.9649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2487111632841037, distance: 1.2787560174878014 entropy 8.333014241651556
epoch: 9, step: 103
	action: tensor([[ -426.5743,  -651.2789,    22.3215,   -65.1816,  -136.1426, -2022.6912,
         -1044.7349]], dtype=torch.float64)
	q_value: tensor([[-29.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6374693537021059, distance: 1.4643445783155877 entropy 8.700944617958415
epoch: 9, step: 104
	action: tensor([[  217.2171, -1897.6752, -2288.9142,  1349.0551,   616.6429,  1398.3525,
           -33.0074]], dtype=torch.float64)
	q_value: tensor([[-26.0009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5229828214326326, distance: 0.7903577258593062 entropy 8.650451078885446
epoch: 9, step: 105
	action: tensor([[ -534.3976, -1408.2631,  3262.8835, -1620.8057,  -268.8842,  2426.6558,
           417.6113]], dtype=torch.float64)
	q_value: tensor([[-22.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7761953378646795, distance: 1.5251131164770912 entropy 8.552637705451332
epoch: 9, step: 106
	action: tensor([[ -699.8097,    32.7844,  -368.5889,  1998.9484,  -443.1046, -1668.5627,
           675.5418]], dtype=torch.float64)
	q_value: tensor([[-26.9258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16641346365357323, distance: 1.0447972937769872 entropy 8.650675444061685
epoch: 9, step: 107
	action: tensor([[-2177.1847, -2421.0986,  -803.2848, -1692.1093,  2931.0048,   603.3565,
          1100.0258]], dtype=torch.float64)
	q_value: tensor([[-31.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6186618481996635, distance: 1.4559107694158762 entropy 8.745363073305292
epoch: 9, step: 108
	action: tensor([[ -874.4362, -2253.1897,  -345.5995,   312.9197,  1688.7721,  1058.3820,
         -2033.3158]], dtype=torch.float64)
	q_value: tensor([[-23.2768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7317683456929784, distance: 1.5059189273324594 entropy 8.534734349801289
epoch: 9, step: 109
	action: tensor([[-1207.2767, -2132.0196,   124.3855, -1015.4731,  -795.6219,  -488.9571,
          -621.1494]], dtype=torch.float64)
	q_value: tensor([[-24.7172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5750519561678549, distance: 1.4361643284129522 entropy 8.50617994679877
epoch: 9, step: 110
	action: tensor([[-365.8105, -194.5149, 1451.3271, 1892.7116,   66.6116, -593.4441,
          991.8764]], dtype=torch.float64)
	q_value: tensor([[-26.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21717505900828638, distance: 1.2625053164428428 entropy 8.529914818488981
epoch: 9, step: 111
	action: tensor([[-1047.3264, -1173.0022, -2112.8970,  -547.4150,   866.4893,  -611.2459,
         -2011.3891]], dtype=torch.float64)
	q_value: tensor([[-25.5894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3868466720042798, distance: 1.347630767176629 entropy 8.565021307107392
epoch: 9, step: 112
	action: tensor([[ -106.1274,  -326.7058,  -104.5578,  -212.1889, -1922.4568,  1156.6803,
            90.2143]], dtype=torch.float64)
	q_value: tensor([[-25.7706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9578248329700028, distance: 1.6011927302876445 entropy 8.453042334237223
epoch: 9, step: 113
	action: tensor([[  924.8172,   -45.6225, -1115.9147,  1467.4829,   227.2378,  1399.9392,
         -1733.4727]], dtype=torch.float64)
	q_value: tensor([[-25.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3245213592749483, distance: 1.3170012171769967 entropy 8.561840384046588
epoch: 9, step: 114
	action: tensor([[  179.2889, -2020.0541,  1055.2045,  1825.4326,  -153.1493,   866.1847,
          -692.7927]], dtype=torch.float64)
	q_value: tensor([[-22.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04625264840983512, distance: 1.1705095959700023 entropy 8.37687204302784
epoch: 9, step: 115
	action: tensor([[-2359.5931, -2411.6724,  -311.5600,  1868.5764, -1105.9671,  -807.8580,
         -4261.9171]], dtype=torch.float64)
	q_value: tensor([[-25.8348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22628003413831577, distance: 1.2672185507715765 entropy 8.672223335328477
epoch: 9, step: 116
	action: tensor([[-1395.4002, -3232.2784,   761.2103,  1961.2341,   626.6957,  -705.2742,
           -74.1502]], dtype=torch.float64)
	q_value: tensor([[-26.9735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10619321309151486, distance: 1.0818785674604257 entropy 8.581922673365801
epoch: 9, step: 117
	action: tensor([[-120.3467, -437.6811,  100.7639,  305.7104, 2066.3198, -999.4132,
         -548.4825]], dtype=torch.float64)
	q_value: tensor([[-23.5745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28158890265598746, distance: 1.295481043291363 entropy 8.541500736798545
epoch: 9, step: 118
	action: tensor([[ -982.7792,  -348.6239, -1129.9457, -1267.7225,  2759.8177,   787.6467,
           875.8168]], dtype=torch.float64)
	q_value: tensor([[-23.8487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39393637878335164, distance: 1.3510709915479582 entropy 8.507302025982366
epoch: 9, step: 119
	action: tensor([[  -19.9763, -1592.0903, -1542.8055, -1422.3891, -1394.5831,   151.3736,
          1164.7965]], dtype=torch.float64)
	q_value: tensor([[-25.4258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9690917439047295, distance: 1.6057934011659887 entropy 8.611701985026752
epoch: 9, step: 120
	action: tensor([[-2148.0716,  1083.9508, -2712.5268,  1545.2145,  -953.5399, -1000.1270,
          1100.4113]], dtype=torch.float64)
	q_value: tensor([[-24.5213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6241294106955191, distance: 0.7015778444072541 entropy 8.621298897607714
epoch: 9, step: 121
	action: tensor([[ -720.1357,   -67.3564,   934.9450,  -660.7036, -1340.6163,  -454.8076,
         -1418.9781]], dtype=torch.float64)
	q_value: tensor([[-25.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5301230223460895, distance: 1.4155325751570536 entropy 8.385026425378035
epoch: 9, step: 122
	action: tensor([[-2023.5763, -3947.8800,  1704.8481, -1423.4180,   376.6641,   901.7730,
         -1798.5517]], dtype=torch.float64)
	q_value: tensor([[-33.9324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0499928635932858, distance: 1.638448774485958 entropy 8.537598313387827
epoch: 9, step: 123
	action: tensor([[ -531.9719, -1864.8248,  -455.1959,  1008.0695,   -66.0367,    93.3204,
          -259.7024]], dtype=torch.float64)
	q_value: tensor([[-25.7116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8780555368349257, distance: 1.568234156619574 entropy 8.488951513267365
epoch: 9, step: 124
	action: tensor([[-795.9503, -618.1134,  488.1228,  870.5012,   90.6144, -328.2884,
         3129.1023]], dtype=torch.float64)
	q_value: tensor([[-30.5251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2759543916374315, distance: 0.9737324765657399 entropy 8.820089476170187
epoch: 9, step: 125
	action: tensor([[  572.9091, -1685.0825,  -771.4099,   -48.2042,   619.6352,   340.9533,
         -1898.6887]], dtype=torch.float64)
	q_value: tensor([[-24.2480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4042297892361464, distance: 1.3560502534499967 entropy 8.634587456683484
epoch: 9, step: 126
	action: tensor([[-1463.9514,   738.3915,  -254.0330,  1584.6441, -3657.7448, -1364.4311,
         -2304.6053]], dtype=torch.float64)
	q_value: tensor([[-28.3099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31355569353612767, distance: 1.3115381832369946 entropy 8.651498280133927
epoch: 9, step: 127
	action: tensor([[-1309.7971,  -709.2443,   818.5833,   558.6140,  -584.7909,  1421.6137,
           895.6013]], dtype=torch.float64)
	q_value: tensor([[-30.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03716812211794707, distance: 1.12287632097275 entropy 8.63948759847698
LOSS epoch 9 actor 301.93814420558635 critic 132.3329977194627
epoch: 10, step: 0
	action: tensor([[  288.5836,  2818.7561,  -750.3949, -2240.1761,  1381.1086,  2063.2462,
         -1838.2555]], dtype=torch.float64)
	q_value: tensor([[-26.0054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6017207342706838, distance: 0.722188456175361 entropy 8.837370000661377
epoch: 10, step: 1
	action: tensor([[ -855.6014,  -326.5759,  -222.2512, -1376.5672,   396.6273,   339.8659,
          3277.5405]], dtype=torch.float64)
	q_value: tensor([[-30.8794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.76255698017919
epoch: 10, step: 2
	action: tensor([[ -413.5591, -2200.0607, -1336.9619,  -741.1665,  -560.0606,  -584.3423,
          1207.9343]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.158707089439574
epoch: 10, step: 3
	action: tensor([[-1871.0703,  -491.8503,   762.7647,   414.9530,  -578.2750,   653.0219,
          -383.4691]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20619519961725896, distance: 1.0195617961463916 entropy 8.158707089439574
epoch: 10, step: 4
	action: tensor([[ 744.6805,  292.4832,  -77.1161,  234.9285,  681.2545, 3085.6238,
         1649.6848]], dtype=torch.float64)
	q_value: tensor([[-23.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5671947046277244, distance: 0.7528405028899161 entropy 8.661738435178702
epoch: 10, step: 5
	action: tensor([[-371.5020, -295.8773, 2094.8081, -433.2835,   53.7406,  144.7381,
         1608.8753]], dtype=torch.float64)
	q_value: tensor([[-23.6848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.663520282037682
epoch: 10, step: 6
	action: tensor([[-926.1241, -676.3807, 1969.9008, 1014.9573, -164.4231,  885.4799,
           98.0377]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2290991650665115, distance: 1.0047452258857048 entropy 8.158707089439574
epoch: 10, step: 7
	action: tensor([[-1818.4684,  -703.7203,  2378.7181,  1224.7657,  -946.3568,  -557.9315,
          4656.0642]], dtype=torch.float64)
	q_value: tensor([[-25.2115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13800771173150472, distance: 1.062449712917097 entropy 8.90285469133455
epoch: 10, step: 8
	action: tensor([[-1389.6191, -2210.6174,  -301.9277,  3066.4123,  -396.0597, -1299.5515,
          4248.2335]], dtype=torch.float64)
	q_value: tensor([[-24.5483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8856985986804369, distance: 1.5714220124343725 entropy 8.826957174035835
epoch: 10, step: 9
	action: tensor([[-1770.3103,  -301.0936,  1734.6130,   212.1392,  -752.2233,  2897.5744,
           974.1215]], dtype=torch.float64)
	q_value: tensor([[-20.2743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13810712355816213, distance: 1.2208105229716049 entropy 8.587337320535028
epoch: 10, step: 10
	action: tensor([[-1124.8661, -1234.4699,  1021.5581,   476.8528,  -594.1271,   992.8836,
          -282.4358]], dtype=torch.float64)
	q_value: tensor([[-21.2862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9392923074779924, distance: 1.5935963657229102 entropy 8.58066797611013
epoch: 10, step: 11
	action: tensor([[-1671.5236, -2660.9737,  -661.8676,  1220.2160,   607.8146,   414.5680,
           872.7355]], dtype=torch.float64)
	q_value: tensor([[-24.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04746093865423462, distance: 1.1711852966358343 entropy 8.7609177844894
epoch: 10, step: 12
	action: tensor([[-3665.8154, -4533.9656, -1995.3561, -1787.7887, -1104.8216,   960.4711,
          2918.0055]], dtype=torch.float64)
	q_value: tensor([[-23.8144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8052946251226429, distance: 1.5375552759017197 entropy 8.80309357405407
epoch: 10, step: 13
	action: tensor([[  831.9511, -2497.8493, -4225.0252,  1195.2405,  -284.6522, -1101.8780,
          1804.3900]], dtype=torch.float64)
	q_value: tensor([[-25.1944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01788164091042166, distance: 1.154530296489162 entropy 8.749704880377069
epoch: 10, step: 14
	action: tensor([[ 156.0973, -791.9295, 1109.2233, -792.4209, 1356.9470, -157.6905,
         4006.7709]], dtype=torch.float64)
	q_value: tensor([[-22.5772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47114791318295435, distance: 0.8321925134128575 entropy 8.783907743058853
epoch: 10, step: 15
	action: tensor([[-1177.8970, -2730.8887, -1242.8315, -1177.7551,   248.4179,  -307.4639,
         -1160.5231]], dtype=torch.float64)
	q_value: tensor([[-22.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6333773117296044, distance: 1.4625137325183049 entropy 8.70413737786789
epoch: 10, step: 16
	action: tensor([[-2137.1726, -2660.9703, -2419.4846,  -325.9887,  3042.1186,  1488.9611,
          2489.1297]], dtype=torch.float64)
	q_value: tensor([[-28.0444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04563026794474523, distance: 1.117931058286614 entropy 8.88482304209251
epoch: 10, step: 17
	action: tensor([[-848.1141, -215.5649,  171.8006, -238.0603,   59.1779,  694.0690,
         1196.3678]], dtype=torch.float64)
	q_value: tensor([[-20.9864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14650963055786137, distance: 1.0571972037750383 entropy 8.516614459405782
epoch: 10, step: 18
	action: tensor([[  861.7764, -4213.1748,  1276.0814,  -198.5469, -1334.9891,  -925.8355,
          1216.7918]], dtype=torch.float64)
	q_value: tensor([[-25.7137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42387427048492365, distance: 1.3655025467054458 entropy 8.832814589325967
epoch: 10, step: 19
	action: tensor([[  295.1582,  -479.3013, -1666.8602,  3316.6198, -1779.5071,   314.3535,
           -57.3082]], dtype=torch.float64)
	q_value: tensor([[-25.7385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10609388959977539, distance: 1.2035182766048436 entropy 8.77243214652016
epoch: 10, step: 20
	action: tensor([[ -737.3010,   231.5445,  -332.2003,  2725.3161,  -192.0463,  1762.3525,
         -1033.1955]], dtype=torch.float64)
	q_value: tensor([[-12.9093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.328537301182168
epoch: 10, step: 21
	action: tensor([[-898.0758,  160.2590, -908.7182,  950.1209, -582.1135, 1271.1506,
          223.1097]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8558003595681292, distance: 0.4345490271658256 entropy 8.158707089439574
epoch: 10, step: 22
	action: tensor([[ -31.5956, -316.7612, -470.8550, -888.8247,  869.6151,  131.7926,
         1931.8108]], dtype=torch.float64)
	q_value: tensor([[-21.5838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.445872467558262
epoch: 10, step: 23
	action: tensor([[-1079.3879,  -987.7457,  1261.6138,   203.4226,  -493.4218,  2735.2425,
          1032.9125]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9549007113727892, distance: 1.5999965477410312 entropy 8.158707089439574
epoch: 10, step: 24
	action: tensor([[-2260.5690,  -990.0823, -2675.7967,   210.8639,  -456.7471,   634.5124,
         -1020.0377]], dtype=torch.float64)
	q_value: tensor([[-23.1571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24088527804989535, distance: 1.2747426287344288 entropy 8.657196473063378
epoch: 10, step: 25
	action: tensor([[-2278.9571, -2940.3013,   805.2892,   852.7813, -1048.4190, -2443.2628,
         -1116.6885]], dtype=torch.float64)
	q_value: tensor([[-26.9984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6608063491864364, distance: 1.4747424840927907 entropy 8.825056058260865
epoch: 10, step: 26
	action: tensor([[ -474.2497,  2542.1780,  1929.4729,   390.7581, -1099.3352,  -663.5131,
          2647.1505]], dtype=torch.float64)
	q_value: tensor([[-22.5659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6744781734762844, distance: 0.6529001225516083 entropy 8.660758878188938
epoch: 10, step: 27
	action: tensor([[-539.7766, -646.9013, 1259.4779, 2213.5269,  965.5237,  -55.4897,
          716.3648]], dtype=torch.float64)
	q_value: tensor([[-23.3629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15041924049037392, distance: 1.0547730564084417 entropy 8.563062000069621
epoch: 10, step: 28
	action: tensor([[-1403.5761, -1614.2161, -1410.5679,   991.4425, -1410.3423,  1255.7274,
          -455.9309]], dtype=torch.float64)
	q_value: tensor([[-22.6459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3327818944377916, distance: 1.3211016513531768 entropy 8.709840524135409
epoch: 10, step: 29
	action: tensor([[  -81.0067, -6022.0810, -1192.2865,  1893.2664, -1653.5894,  1201.5343,
           522.7333]], dtype=torch.float64)
	q_value: tensor([[-30.2935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15587844275328933, distance: 1.230304961356342 entropy 9.008338299830212
epoch: 10, step: 30
	action: tensor([[ 275.2025,  597.7309,  745.6109, 2159.1652, 2796.8833, 2523.6031,
          611.5022]], dtype=torch.float64)
	q_value: tensor([[-23.3484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45735684078521066, distance: 0.842973378299681 entropy 8.656016118260913
epoch: 10, step: 31
	action: tensor([[-1158.0184, -2286.9505,  1351.0187,   422.4681,   612.5851,   912.9138,
           213.1666]], dtype=torch.float64)
	q_value: tensor([[-24.4307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4013441914292837, distance: 1.3546562408811527 entropy 8.703278439624144
epoch: 10, step: 32
	action: tensor([[-4111.8926, -1066.7282,  1390.4230,   286.9922, -2235.5292, -1189.7212,
         -1362.3996]], dtype=torch.float64)
	q_value: tensor([[-25.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13744265007758538, distance: 1.2204540913855584 entropy 8.836822937495231
epoch: 10, step: 33
	action: tensor([[1776.1511, 1390.7780,  832.3660, -179.3531, -396.2798, 1276.5982,
         3096.2494]], dtype=torch.float64)
	q_value: tensor([[-21.6398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.61930945304212
epoch: 10, step: 34
	action: tensor([[ 1017.5532, -1652.9544,   673.2082,    21.6120,  1009.2186,  1374.3120,
           217.6849]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5214075315098209, distance: 0.791661679271314 entropy 8.158707089439574
epoch: 10, step: 35
	action: tensor([[   58.1330, -1133.1085,   326.7600,  -575.2296,   214.2660,   167.1139,
         -1015.8028]], dtype=torch.float64)
	q_value: tensor([[-20.2689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13267173058628356, distance: 1.065733074757668 entropy 8.632175240942644
epoch: 10, step: 36
	action: tensor([[  -74.8067, -1696.2565,   431.1453, -1109.1343,    67.2222,   406.6433,
          1966.8038]], dtype=torch.float64)
	q_value: tensor([[-15.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08694225616825035, distance: 1.0934673385375964 entropy 8.314314106064888
epoch: 10, step: 37
	action: tensor([[-2505.4767,   983.7122,  -638.3803,  2849.1194,   271.2729,   357.8309,
            70.7585]], dtype=torch.float64)
	q_value: tensor([[-20.0802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.568217718040872
epoch: 10, step: 38
	action: tensor([[  -66.8272, -1241.0585, -1306.3083,   304.7756,  -789.3519,  -895.4744,
          -267.5357]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05674655823681918, distance: 1.176365053303599 entropy 8.158707089439574
epoch: 10, step: 39
	action: tensor([[-1839.4211, -1120.7612,   390.3867,  -287.5085,  -483.5869,   230.0425,
         -2276.6391]], dtype=torch.float64)
	q_value: tensor([[-21.3335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33568991732205, distance: 1.32254213479328 entropy 8.554748810657648
epoch: 10, step: 40
	action: tensor([[-1935.4081,  -595.6039, -2244.2309,  2058.5447,    74.0552,  -869.8705,
          -293.2208]], dtype=torch.float64)
	q_value: tensor([[-22.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6033004367337056, distance: 1.4489858643272413 entropy 8.555931905598507
epoch: 10, step: 41
	action: tensor([[ -490.6548,  -465.5091,  1653.7882,  2302.2481, -1022.8154,   340.1990,
           514.7191]], dtype=torch.float64)
	q_value: tensor([[-21.8339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4343450478244464, distance: 0.8606617259020416 entropy 8.621969357612537
epoch: 10, step: 42
	action: tensor([[-1154.5591,   -37.7578,  -993.1594,   -14.6639,  -317.7559,  1270.2092,
          1455.2459]], dtype=torch.float64)
	q_value: tensor([[-18.5774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3198200477856532, distance: 1.7429456384395021 entropy 8.447111299659609
epoch: 10, step: 43
	action: tensor([[ -341.4873, -2146.7544,  2348.0784, -1870.5538, -1130.8849,   642.6107,
         -1160.0124]], dtype=torch.float64)
	q_value: tensor([[-22.7400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42155077087940795, distance: 1.3643879679519997 entropy 8.589710662717666
epoch: 10, step: 44
	action: tensor([[-2424.0835,   867.4615,   941.2839,  -507.6761,   556.0692,   293.9551,
          -187.3987]], dtype=torch.float64)
	q_value: tensor([[-26.1112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3218281039457339, distance: 0.9423811432518724 entropy 8.699653971415062
epoch: 10, step: 45
	action: tensor([[  253.4145,   247.8980,  -102.7256,  1068.7380, -1713.3516, -1211.1310,
          1181.1641]], dtype=torch.float64)
	q_value: tensor([[-31.3207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.689832135913417
epoch: 10, step: 46
	action: tensor([[-2027.7453,  1042.2731,   125.6069,  1318.8545,   590.9976,  1865.3559,
          -199.0116]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05743903277187745, distance: 1.1767504197997591 entropy 8.158707089439574
epoch: 10, step: 47
	action: tensor([[  220.7177,   573.5352,  -404.4091,  -476.1769, -1202.0156, -1190.3697,
           574.6885]], dtype=torch.float64)
	q_value: tensor([[-23.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.449971168858303
epoch: 10, step: 48
	action: tensor([[ 566.1433, 1444.7776, -847.4981,   37.2447, -783.5970, 1440.6555,
         -781.5346]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.158707089439574
epoch: 10, step: 49
	action: tensor([[ -336.1562,   640.9052,  -890.3710,   105.0568,  -463.1018,   249.3363,
         -1797.0260]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.158707089439574
epoch: 10, step: 50
	action: tensor([[1465.8043, -689.5613, -108.0756, -401.6849,  175.0332,  102.2793,
         2219.7988]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.158707089439574
epoch: 10, step: 51
	action: tensor([[ -589.7716, -2177.7939, -1227.7958,  2064.5985,  -265.4399,   443.1042,
          -345.5429]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07573555817602584, distance: 1.100157378683231 entropy 8.158707089439574
epoch: 10, step: 52
	action: tensor([[ -232.0126,  -457.3209,   332.7080,  -294.7887, -1262.6618,   388.5131,
          -844.3410]], dtype=torch.float64)
	q_value: tensor([[-24.1677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9227760716687767, distance: 1.5867958197054561 entropy 8.781504707296465
epoch: 10, step: 53
	action: tensor([[ -628.9041, -1428.4522,     8.3297,  1745.2840,   614.5515,  -491.5013,
          -330.4093]], dtype=torch.float64)
	q_value: tensor([[-18.9596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3962135865479186, distance: 1.3521741315019669 entropy 8.427304581501092
epoch: 10, step: 54
	action: tensor([[-1419.8448,  -293.2872,  -243.9901, -2303.3376,  -168.5073, -1301.9994,
          4384.9273]], dtype=torch.float64)
	q_value: tensor([[-26.4284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9012888367622465, distance: 1.5779046005818305 entropy 8.864091247577457
epoch: 10, step: 55
	action: tensor([[-1851.7668,  -190.4588,  -490.4184,   370.0232,   528.8525,    52.5435,
           -59.0328]], dtype=torch.float64)
	q_value: tensor([[-21.7588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0349634051890377, distance: 1.632431607620655 entropy 8.552739401018146
epoch: 10, step: 56
	action: tensor([[ -525.8118,  -290.1002,  -903.9249,  1446.6743,    98.0813,  2781.0440,
         -1328.1780]], dtype=torch.float64)
	q_value: tensor([[-19.2163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6765803181952821, distance: 1.481729321360258 entropy 8.641451339427894
epoch: 10, step: 57
	action: tensor([[-4754.3064, -3818.1089,  1568.7591,   216.7340, -1315.6112, -1113.9813,
          2465.0559]], dtype=torch.float64)
	q_value: tensor([[-26.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05678049275164854, distance: 1.1113812876946436 entropy 8.898348699520024
epoch: 10, step: 58
	action: tensor([[  752.2400,   243.7893, -2724.9220,  1178.6554, -1663.0826,  1860.3813,
          -870.7000]], dtype=torch.float64)
	q_value: tensor([[-23.9747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7351899935635173, distance: 0.5888760467218526 entropy 8.831345676492408
epoch: 10, step: 59
	action: tensor([[-2383.1672,   643.9237,   338.1753,   447.0466,  -435.1353,   636.3370,
          -359.8546]], dtype=torch.float64)
	q_value: tensor([[-22.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23522396537088563, distance: 1.2718314141350584 entropy 8.52927300189053
epoch: 10, step: 60
	action: tensor([[ -850.7607, -1896.8180, -3309.9947,  -492.2861,  1060.1935,   813.6953,
          2026.4525]], dtype=torch.float64)
	q_value: tensor([[-26.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8522355726019344, distance: 1.5574166151167521 entropy 8.739610598233956
epoch: 10, step: 61
	action: tensor([[ -621.5123, -1202.6814,  1819.2369,   883.2525,  -858.4630,  2731.1966,
           833.9781]], dtype=torch.float64)
	q_value: tensor([[-25.3762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19490127584724148, distance: 1.2509003259701355 entropy 8.721297788949544
epoch: 10, step: 62
	action: tensor([[  131.3535,  -752.8203,  -110.3925,   650.3105, -1013.0488,   326.9673,
          3457.6080]], dtype=torch.float64)
	q_value: tensor([[-24.0294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48870915463033293, distance: 0.8182588320605872 entropy 8.832775452059659
epoch: 10, step: 63
	action: tensor([[-3077.7093, -1087.8484,   669.9172,  3606.8726,   133.5821,  1588.1360,
         -1721.9324]], dtype=torch.float64)
	q_value: tensor([[-27.2876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3294733083404089, distance: 1.3194608372600434 entropy 8.856129171388554
epoch: 10, step: 64
	action: tensor([[ -457.1193, -1674.3679,  1038.3856,  -365.1454,   833.4890,  1700.3204,
          1718.5447]], dtype=torch.float64)
	q_value: tensor([[-24.1793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0377704538050199, distance: 1.6335571157598987 entropy 8.780550844202022
epoch: 10, step: 65
	action: tensor([[-2084.2026,  1293.0624,  -379.3193, -1304.1144,  -428.7515,   625.7191,
           508.9962]], dtype=torch.float64)
	q_value: tensor([[-27.7507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4867425099467503, distance: 0.8198310095432607 entropy 8.85839015693723
epoch: 10, step: 66
	action: tensor([[-2567.4732,  -926.8916,   804.0787,  -284.4753,  1992.1731, -1448.5411,
         -1251.8687]], dtype=torch.float64)
	q_value: tensor([[-36.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057269893002168, distance: 0.9535024968689275 entropy 8.735290797688751
epoch: 10, step: 67
	action: tensor([[-1358.3715, -2568.5856,   766.4114,   963.6578,   895.7170,  1943.5317,
           858.9234]], dtype=torch.float64)
	q_value: tensor([[-24.3520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9785783605804146, distance: 1.6096569191221142 entropy 8.534947931691828
epoch: 10, step: 68
	action: tensor([[-1768.0394,  -824.8691,  -345.2056, -1479.9520,  2000.7087,  -214.8398,
           836.6470]], dtype=torch.float64)
	q_value: tensor([[-20.9351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7273646243527536, distance: 1.5040030046244395 entropy 8.663248689868498
epoch: 10, step: 69
	action: tensor([[-1151.0089,  -254.7206,   665.3133,  -160.8635,  1590.9070,    52.0345,
          1141.3640]], dtype=torch.float64)
	q_value: tensor([[-24.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47932000164854804, distance: 0.8257377521166042 entropy 8.650380347052785
epoch: 10, step: 70
	action: tensor([[  659.9970, -2504.7227,  -109.0781,  1073.2322,   157.4314,   781.3583,
           151.0699]], dtype=torch.float64)
	q_value: tensor([[-21.6882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1023436060988645, distance: 1.201476240600277 entropy 8.554913285123117
epoch: 10, step: 71
	action: tensor([[   76.9702, -3038.3879,   969.1297,   670.5444,   -84.7709,  1084.7844,
          1210.0155]], dtype=torch.float64)
	q_value: tensor([[-16.1974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36664444569398746, distance: 1.3377792751598558 entropy 8.41366142549319
epoch: 10, step: 72
	action: tensor([[ -312.0199, -1474.4480,  1813.9644, -2847.0163,    -6.6248, -1358.0283,
          -512.5933]], dtype=torch.float64)
	q_value: tensor([[-27.1904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04799277557243509, distance: 1.1165465020242704 entropy 8.8641311179926
epoch: 10, step: 73
	action: tensor([[ 1548.7609, -1524.1898,   868.9081,  3062.2507, -2801.8517, -1843.2652,
         -1820.7189]], dtype=torch.float64)
	q_value: tensor([[-23.1659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48120058651422837, distance: 1.3927194184907772 entropy 8.72878442949094
epoch: 10, step: 74
	action: tensor([[ -958.4985,   539.0650,  -287.1136, -2086.1753,  1886.8061, -1574.6660,
         -1215.4675]], dtype=torch.float64)
	q_value: tensor([[-25.7139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.650353981252048
epoch: 10, step: 75
	action: tensor([[ -543.6502,  -216.6188, -1617.7866,  -286.7466,  -141.5789, -1022.3634,
           -61.6138]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43965929701648765, distance: 1.373050644564311 entropy 8.158707089439574
epoch: 10, step: 76
	action: tensor([[ -321.3570, -2754.1911, -2042.9405,    39.8693, -1770.2226,  1025.2538,
          -195.1615]], dtype=torch.float64)
	q_value: tensor([[-21.4637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6815363320030492, distance: 1.4839177199806906 entropy 8.664943456497676
epoch: 10, step: 77
	action: tensor([[-1525.0913,  -701.4861,  1145.6048, -1306.3505,  -506.6049,  -285.8200,
          -127.0923]], dtype=torch.float64)
	q_value: tensor([[-20.9109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7122351407261376, distance: 1.4974019565424992 entropy 8.618363708769381
epoch: 10, step: 78
	action: tensor([[  452.4072, -1628.3959,   777.3027,  -787.0673,  -194.1222,  1920.0922,
           746.6197]], dtype=torch.float64)
	q_value: tensor([[-23.6475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5157231142652691, distance: 0.7963492286280358 entropy 8.739173686888863
epoch: 10, step: 79
	action: tensor([[ 699.9335,  976.4593,  849.0396, 3857.4477, -932.4383, -663.3093,
         1123.9756]], dtype=torch.float64)
	q_value: tensor([[-17.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1478178829499347, distance: 1.0563866432623072 entropy 8.40518991190673
epoch: 10, step: 80
	action: tensor([[-2147.5773,  -871.6700,   710.9131,  1009.6289,   954.2507,   481.1191,
           918.5559]], dtype=torch.float64)
	q_value: tensor([[-19.5309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2567071431729989, distance: 0.9865899175047981 entropy 8.432509045506611
epoch: 10, step: 81
	action: tensor([[-1628.1498, -2492.4720,   147.8944,   122.1153,  -785.1293,   727.2457,
         -1677.5130]], dtype=torch.float64)
	q_value: tensor([[-19.5489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18226321414314084, distance: 1.034816776073283 entropy 8.460739665722185
epoch: 10, step: 82
	action: tensor([[ -890.4008, -2554.1343, -1341.2548,  1950.3258,  2001.4726,  2884.4290,
         -3641.0951]], dtype=torch.float64)
	q_value: tensor([[-23.0771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09977134822783507, distance: 1.2000736326984336 entropy 8.673889384303484
epoch: 10, step: 83
	action: tensor([[-3383.1058, -2839.3942,  -769.9953,  2143.2052,  1753.0728,  2941.5067,
          1186.7827]], dtype=torch.float64)
	q_value: tensor([[-28.7233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05349597590363986, distance: 1.1133146543222625 entropy 8.951737168626016
epoch: 10, step: 84
	action: tensor([[  988.9696, -3665.6262,  1319.7791,   778.8587,   427.9280,  -985.1908,
          -260.7108]], dtype=torch.float64)
	q_value: tensor([[-25.6709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3797967103910177, distance: 0.9012052768412092 entropy 8.75644614830202
epoch: 10, step: 85
	action: tensor([[ -664.8771,  -700.6669, -1173.5968,  -410.3615, -1110.4725,  1823.2882,
          -517.1482]], dtype=torch.float64)
	q_value: tensor([[-22.6847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6472261412814098, distance: 1.4687007146837026 entropy 8.574402023740236
epoch: 10, step: 86
	action: tensor([[  175.2992,  -176.3790,  2048.4113,   865.1945, -3845.4038,   -19.5455,
          1499.7189]], dtype=torch.float64)
	q_value: tensor([[-20.2301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.574863995218886
epoch: 10, step: 87
	action: tensor([[-1233.7840,   223.2204,   154.8422,  -333.3072,    69.1397,  1517.6845,
          -130.4106]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5759269653574434, distance: 0.7452071656832192 entropy 8.158707089439574
epoch: 10, step: 88
	action: tensor([[-2311.0745,   484.8815,  1706.6740,  -440.2422,  1341.3269, -1131.4207,
          2097.5957]], dtype=torch.float64)
	q_value: tensor([[-32.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.584848246399734
epoch: 10, step: 89
	action: tensor([[-1899.3887,   -63.4884,   -40.0423,   665.9002,   614.4618, -1491.3683,
           580.7905]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6654277888703672, distance: 1.4767929033935625 entropy 8.158707089439574
epoch: 10, step: 90
	action: tensor([[-1684.9324, -4162.1580,  -974.5398,   -48.4965, -1614.6228, -1496.3275,
          3944.3939]], dtype=torch.float64)
	q_value: tensor([[-24.8745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9618016524710915, distance: 1.6028181117014773 entropy 8.693579184031659
epoch: 10, step: 91
	action: tensor([[-1840.4316, -1383.9946, -1094.5174,  1080.7122,  -326.7913,  -986.9074,
          -738.7782]], dtype=torch.float64)
	q_value: tensor([[-18.2490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9993438438962776, distance: 1.6180816701606526 entropy 8.403842984480466
epoch: 10, step: 92
	action: tensor([[   61.3057, -1528.0362,    44.6288,  3933.0314,  -674.5205,  3745.3257,
          3551.9848]], dtype=torch.float64)
	q_value: tensor([[-22.9210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3921728689730035, distance: 1.3502160812937207 entropy 8.786927106406157
epoch: 10, step: 93
	action: tensor([[-735.0234, -822.3686, -544.6875, 2296.6637, 1741.5968,  140.5727,
         1190.5457]], dtype=torch.float64)
	q_value: tensor([[-24.7345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03524059369901744, distance: 1.1239997226362834 entropy 8.778003689627498
epoch: 10, step: 94
	action: tensor([[  715.9150, -2189.8144,   766.1604, -1607.0992, -3088.8563,  2191.2313,
         -1019.2400]], dtype=torch.float64)
	q_value: tensor([[-24.4538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3230395308747658, distance: 1.3162643034474977 entropy 8.853472630126232
epoch: 10, step: 95
	action: tensor([[-1945.9525, -2318.8535,  -432.4249,  -231.6563, -1673.8045,  -532.0392,
           285.0334]], dtype=torch.float64)
	q_value: tensor([[-18.6153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5114604034699486, distance: 1.406873600673127 entropy 8.541178939776046
epoch: 10, step: 96
	action: tensor([[-6134.6737, -3977.6583,  -231.7636,  1177.1924, -1024.7191,  -849.1434,
          3669.6531]], dtype=torch.float64)
	q_value: tensor([[-27.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6251917871834909, distance: 1.4588445012502045 entropy 8.851020580159648
epoch: 10, step: 97
	action: tensor([[-1071.4690, -2862.0741,  1001.4480,  1772.6046,  -146.9937,  2263.6543,
           657.5490]], dtype=torch.float64)
	q_value: tensor([[-19.0759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24171743587663053, distance: 1.2751699886558838 entropy 8.448808962048982
epoch: 10, step: 98
	action: tensor([[-2129.1946, -2005.0441,  1332.0571,  2953.2101,  1020.9703,  1451.0382,
          -445.9805]], dtype=torch.float64)
	q_value: tensor([[-26.6702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7085366481959414, distance: 1.4957838599108146 entropy 8.828869745955412
epoch: 10, step: 99
	action: tensor([[  275.9423, -1274.0239, -1361.2772,   496.9652,  1504.8350, -1079.2921,
         -1287.6658]], dtype=torch.float64)
	q_value: tensor([[-24.0803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31373828946113813, distance: 0.9479852443643975 entropy 8.762021404553492
epoch: 10, step: 100
	action: tensor([[  421.3011, -3087.4221,  -598.3011,  1189.7629,  -620.0502,   692.0974,
          -676.1485]], dtype=torch.float64)
	q_value: tensor([[-25.8234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3943909187909981, distance: 0.890538875163833 entropy 8.844229510443204
epoch: 10, step: 101
	action: tensor([[-1874.6582,  -637.3858,  2007.2834, -1813.3617, -2197.0633,  1810.0526,
          1614.8375]], dtype=torch.float64)
	q_value: tensor([[-22.5928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6957946009311355, distance: 1.4901957394170677 entropy 8.716828165223395
epoch: 10, step: 102
	action: tensor([[   90.7634, -3849.4758, -1175.4314,  1502.3606, -2284.9080,   567.1768,
         -1098.1776]], dtype=torch.float64)
	q_value: tensor([[-22.3791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0764302627803074, distance: 1.0997438454829234 entropy 8.669528276210688
epoch: 10, step: 103
	action: tensor([[  856.1304, -2431.3874,  2385.7744,   443.7640,  -366.7986,  1004.3317,
           661.6747]], dtype=torch.float64)
	q_value: tensor([[-21.5147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.051387980657247034, distance: 1.1733786916043902 entropy 8.63761490505806
epoch: 10, step: 104
	action: tensor([[-1230.5059,   218.0017, -1552.4763,  -226.4154, -1154.5267,  2376.8853,
          1042.7243]], dtype=torch.float64)
	q_value: tensor([[-22.0953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.801591034309808
epoch: 10, step: 105
	action: tensor([[ -337.8465,  -538.0644,  -943.3799, -1036.6882,  1015.8585,   292.9317,
           683.2594]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3586341643382651, distance: 1.3338529661076606 entropy 8.158707089439574
epoch: 10, step: 106
	action: tensor([[-1411.1857, -1578.8026, -1904.9123,  1627.1699,  1156.2997,   257.2677,
          2333.0699]], dtype=torch.float64)
	q_value: tensor([[-18.1039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0889266261658006, distance: 1.0922784630143212 entropy 8.535127447602255
epoch: 10, step: 107
	action: tensor([[  273.5321, -1929.6942,   315.8566,   745.5974,  -649.8689,  3445.8630,
          -466.2203]], dtype=torch.float64)
	q_value: tensor([[-24.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4441056422373485, distance: 0.8532038888937546 entropy 8.844904289730641
epoch: 10, step: 108
	action: tensor([[ -163.9329, -2017.4490,  1850.2092,  1902.2745,  -693.7179, -1618.5955,
           783.2437]], dtype=torch.float64)
	q_value: tensor([[-21.6681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09760535249802216, distance: 1.0870635870661511 entropy 8.780633194132177
epoch: 10, step: 109
	action: tensor([[-229.5863,   95.6895, 1668.1552,  671.5509, -611.9015, 2628.2287,
         3258.4132]], dtype=torch.float64)
	q_value: tensor([[-26.4206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5600464919049666, distance: 1.429306813126117 entropy 8.857182863386464
epoch: 10, step: 110
	action: tensor([[ -437.2385,  1028.4819,  -588.0938,  -136.7705, -2080.9563, -1742.1931,
         -2388.2359]], dtype=torch.float64)
	q_value: tensor([[-30.6582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2643763668278545, distance: 1.2867520773986496 entropy 8.952963818843902
epoch: 10, step: 111
	action: tensor([[ -810.5017, -1525.6615,  1103.2722,   637.2442,   -20.3541,  -199.8656,
           297.7893]], dtype=torch.float64)
	q_value: tensor([[-23.3266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5384362408123546, distance: 1.4193726883843296 entropy 8.550936648124631
epoch: 10, step: 112
	action: tensor([[ 1031.7062,   562.8225, -3278.4938,  -909.1543, -3921.6117,   534.3595,
         -4390.0550]], dtype=torch.float64)
	q_value: tensor([[-25.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.895609876824928
epoch: 10, step: 113
	action: tensor([[ -951.4580,  -155.7864,   422.9531,  -266.0429,   834.3209,  -844.5460,
         -1906.3359]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0053364245373158, distance: 1.620504772628866 entropy 8.158707089439574
epoch: 10, step: 114
	action: tensor([[-1526.9750, -4019.1950,   -72.2280,  3526.6430,  2285.2406, -2189.0854,
          3143.9467]], dtype=torch.float64)
	q_value: tensor([[-25.8600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.290659092097588, distance: 1.3000572149773024 entropy 8.761342070751706
epoch: 10, step: 115
	action: tensor([[-2727.2183, -2146.6065,   561.6506,   850.9942, -1441.9151,   760.9561,
             5.2554]], dtype=torch.float64)
	q_value: tensor([[-29.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011023151151205113, distance: 1.1506341078439974 entropy 8.946741116436804
epoch: 10, step: 116
	action: tensor([[-2083.9638, -2485.1109, -1854.9153,  -215.1293,   863.4058,  3084.1251,
          1250.4074]], dtype=torch.float64)
	q_value: tensor([[-28.1707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9316875794440331, distance: 1.590468737419182 entropy 8.90800873480947
epoch: 10, step: 117
	action: tensor([[-2595.9490,  -606.9578,   536.2842,  2057.2435, -1452.0850,  1079.3075,
         -2130.2873]], dtype=torch.float64)
	q_value: tensor([[-23.6379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08332049659637886, distance: 1.0956338807754658 entropy 8.694955297902604
epoch: 10, step: 118
	action: tensor([[ -597.9438, -2190.5534,   997.0243,  -251.5022,  -399.6719,  1407.0854,
         -1747.2627]], dtype=torch.float64)
	q_value: tensor([[-25.7043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7715221860512553, distance: 1.5231055165884593 entropy 8.785095899232571
epoch: 10, step: 119
	action: tensor([[ -972.9797, -1093.9127,  1251.0813,   607.9677, -1488.4242,   432.4267,
          3243.6950]], dtype=torch.float64)
	q_value: tensor([[-22.6784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11940679528612064, distance: 1.0738518215925166 entropy 8.672536887083867
epoch: 10, step: 120
	action: tensor([[-1.4260e+02, -4.2248e+03, -1.4254e+03,  2.4098e+02,  5.6690e+02,
         -4.0463e+00,  9.0970e+02]], dtype=torch.float64)
	q_value: tensor([[-22.8492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.615977497587922
epoch: 10, step: 121
	action: tensor([[-1.1989e+03, -6.4420e+02,  1.3288e+03,  1.1675e+00,  2.0853e+02,
          5.8003e+01,  9.8162e+02]], dtype=torch.float64)
	q_value: tensor([[-25.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3987967030071211, distance: 1.3534243733254538 entropy 8.158707089439574
epoch: 10, step: 122
	action: tensor([[-679.3134,  596.5792, -369.0204,  169.2972,    9.5913, 1547.9826,
         -857.1174]], dtype=torch.float64)
	q_value: tensor([[-23.6296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2693919512901466, distance: 0.9781352705805767 entropy 8.689540201739756
epoch: 10, step: 123
	action: tensor([[ -468.0857, -2467.1001,   559.3572,   -82.6916, -3662.0169,  1487.0631,
          3958.2504]], dtype=torch.float64)
	q_value: tensor([[-29.8240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37336804382076005, distance: 1.341066031418791 entropy 8.927345618531964
epoch: 10, step: 124
	action: tensor([[-1100.2983,  -864.2737,  1029.0945,    74.5056,  -929.5503,   710.6398,
         -1040.7694]], dtype=torch.float64)
	q_value: tensor([[-19.3682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1025883756615994, distance: 1.0840580496623218 entropy 8.492069158073804
epoch: 10, step: 125
	action: tensor([[ -634.5087,   220.6737, -2602.4942,   734.8979, -1325.9977, -3197.3234,
           462.4050]], dtype=torch.float64)
	q_value: tensor([[-23.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43727349664013193, distance: 1.3719124644929754 entropy 8.812369541620214
epoch: 10, step: 126
	action: tensor([[-2271.2565, -3535.0295,  1268.1466,  2514.7104,  1382.8049,   259.6724,
         -1067.7486]], dtype=torch.float64)
	q_value: tensor([[-24.2539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.055073307364284174, distance: 1.1754333570391984 entropy 8.70586693739957
epoch: 10, step: 127
	action: tensor([[-1245.8923,   694.5585,  3338.3890, -1245.8895,  -479.2757,   341.0972,
           178.9017]], dtype=torch.float64)
	q_value: tensor([[-19.9264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09524083844020126, distance: 1.1975992291385953 entropy 8.550059218260538
LOSS epoch 10 actor 250.36572333594182 critic 372.96185574017113
epoch: 11, step: 0
	action: tensor([[  180.1737,   172.4404,   589.0334,  1948.0630, -1760.4235, -1679.9308,
           591.0032]], dtype=torch.float64)
	q_value: tensor([[-27.8605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45458144815731194, distance: 0.8451263569099643 entropy 8.703751297891838
epoch: 11, step: 1
	action: tensor([[ 1372.5111, -1681.1572,   187.7406,   482.9655,   588.6332,  -431.3288,
           758.6471]], dtype=torch.float64)
	q_value: tensor([[-21.6167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6090059912095716, distance: 0.715552897477752 entropy 8.573454621873301
epoch: 11, step: 2
	action: tensor([[  481.4739, -2571.5844,  1278.1803,  2258.6467,  -562.1966,  1096.2967,
          1300.8758]], dtype=torch.float64)
	q_value: tensor([[-19.8956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5541101204331256, distance: 1.4265847841922823 entropy 8.687822684638077
epoch: 11, step: 3
	action: tensor([[-2659.6330, -3124.6935, -3798.5049, -1414.6701,   584.3188, -1812.7687,
           143.7327]], dtype=torch.float64)
	q_value: tensor([[-27.5158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5329354018669825, distance: 1.4168328585857635 entropy 8.9521243001366
epoch: 11, step: 4
	action: tensor([[  680.0639, -1017.0641,   292.9508,   844.3792,  1479.3415,  1473.7110,
          1482.0393]], dtype=torch.float64)
	q_value: tensor([[-22.6449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46103931662210296, distance: 1.383208486256009 entropy 8.665681804304864
epoch: 11, step: 5
	action: tensor([[ 1300.7291, -4322.9298, -2259.4176,   978.6205,   643.6890,  -417.7551,
          -733.8927]], dtype=torch.float64)
	q_value: tensor([[-18.9793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1486701685852383, distance: 1.0558582536322423 entropy 8.848831468148438
epoch: 11, step: 6
	action: tensor([[-3799.8659, -2985.7998, -2399.6911,  2780.7694,  3584.8662, -2392.9786,
         -2473.3481]], dtype=torch.float64)
	q_value: tensor([[-29.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21939063686984195, distance: 1.011052183322658 entropy 8.828716833041332
epoch: 11, step: 7
	action: tensor([[-2679.1479, -1535.2154, -2069.2650,  2386.0257,  1879.1337,  1554.5039,
          2795.6446]], dtype=torch.float64)
	q_value: tensor([[-24.7881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2536723295753911, distance: 1.2812937670397353 entropy 8.952568465117105
epoch: 11, step: 8
	action: tensor([[-2980.4691, -1684.7310,   -19.4404,   434.4567,   172.0467,  -358.0796,
         -2285.8669]], dtype=torch.float64)
	q_value: tensor([[-23.3097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1465631830644596, distance: 1.2253374000397994 entropy 8.746611383836914
epoch: 11, step: 9
	action: tensor([[-1896.2713, -1793.9716, -1978.7903,  -535.9354,  -956.0913,  1297.6750,
          1428.5346]], dtype=torch.float64)
	q_value: tensor([[-21.0372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9397744759867093, distance: 1.593794462272152 entropy 8.72029064062656
epoch: 11, step: 10
	action: tensor([[-1710.9241,  1251.2691,  2001.7798,   718.0107,  1702.1714,  1128.9247,
          -437.1708]], dtype=torch.float64)
	q_value: tensor([[-23.2117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043197338714079825, distance: 1.1687992614565919 entropy 8.739110423202973
epoch: 11, step: 11
	action: tensor([[ 1564.7176,  -389.5623,  -636.9662,  2706.3623,  -534.6730,  2419.1936,
         -4464.1551]], dtype=torch.float64)
	q_value: tensor([[-27.1622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29039602413769583, distance: 0.963972652546811 entropy 8.756274372055113
epoch: 11, step: 12
	action: tensor([[ 4728.1343, -4455.7447, -2100.1166,  2099.8854, -3723.8652,   213.6798,
            97.3291]], dtype=torch.float64)
	q_value: tensor([[-24.7870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5510996074471163, distance: 0.7667109854154767 entropy 8.850511264710239
epoch: 11, step: 13
	action: tensor([[-1842.9730, -1451.5514,  -393.1506,   228.7652,   803.2217,  1160.5438,
          3081.4007]], dtype=torch.float64)
	q_value: tensor([[-23.0859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38374228112632947, distance: 0.8983340872614 entropy 8.920902390659235
epoch: 11, step: 14
	action: tensor([[-5034.1189,   916.8029, -1959.8270,  2161.8289,  -365.3898,   827.1896,
           190.0999]], dtype=torch.float64)
	q_value: tensor([[-28.5043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4291427759086841, distance: 0.8646103776690548 entropy 8.939091303786107
epoch: 11, step: 15
	action: tensor([[ 244.6352, 1994.1559,  314.9721, 3807.2811,  682.2162, 2471.0999,
         -470.7095]], dtype=torch.float64)
	q_value: tensor([[-25.4289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4199181779320633, distance: 0.8715680900006856 entropy 8.717106681256189
epoch: 11, step: 16
	action: tensor([[-1489.2644, -1599.4988,   620.9538,  -368.5081, -2683.7177, -2623.6540,
         -1805.1679]], dtype=torch.float64)
	q_value: tensor([[-22.8633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7451236913743493, distance: 1.5117145756877106 entropy 8.732762396326155
epoch: 11, step: 17
	action: tensor([[-2820.4600,  -804.9347,  -328.6294,  2588.4038,  -294.2731,  2265.2022,
          3129.8323]], dtype=torch.float64)
	q_value: tensor([[-19.1943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3010327016116565, distance: 1.305271332126451 entropy 8.481322563993718
epoch: 11, step: 18
	action: tensor([[-2229.2474,  -822.8693, -1772.0716,   757.6155, -1268.6951,  2280.7944,
          -787.2726]], dtype=torch.float64)
	q_value: tensor([[-22.6246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12014200543790077, distance: 1.0734034467834739 entropy 8.741796159467414
epoch: 11, step: 19
	action: tensor([[ -622.6412, -1766.1315,   637.6671,   152.3771, -1759.6619,  1182.1769,
          1386.4661]], dtype=torch.float64)
	q_value: tensor([[-21.2238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12757229277402982, distance: 1.0688614604660867 entropy 8.687487704682615
epoch: 11, step: 20
	action: tensor([[ -901.8923, -1899.6723,  2077.0201,   171.1306,   298.2924, -3182.0693,
         -2507.3080]], dtype=torch.float64)
	q_value: tensor([[-23.1207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031173802976288134, distance: 1.1263662528734755 entropy 8.885974852083269
epoch: 11, step: 21
	action: tensor([[  122.0001, -1125.8648,   568.4757,  -167.9396,   280.4128, -2366.7923,
          2334.4385]], dtype=torch.float64)
	q_value: tensor([[-26.8524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2709112046396671, distance: 1.2900730344241254 entropy 9.03722612054086
epoch: 11, step: 22
	action: tensor([[-4635.0321, -5762.9252,  1898.8866,   166.1395, -1734.1011,   920.1813,
          -465.1333]], dtype=torch.float64)
	q_value: tensor([[-26.9123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46036186153027314, distance: 0.840636051107718 entropy 8.891983474752617
epoch: 11, step: 23
	action: tensor([[ -775.1102, -3544.7703,   191.6557, -1193.5749,  1660.7669,  1232.7031,
         -1670.5988]], dtype=torch.float64)
	q_value: tensor([[-20.3309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9983867877142645, distance: 1.6176943479807067 entropy 8.702302135745482
epoch: 11, step: 24
	action: tensor([[-1376.1608, -1824.8104,   802.6835, -1056.4522,   233.7334,   210.7090,
           463.1458]], dtype=torch.float64)
	q_value: tensor([[-26.4672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4231468850509621, distance: 1.3651537190156002 entropy 8.841301220443675
epoch: 11, step: 25
	action: tensor([[ -129.8128, -1496.7112, -2297.9665,  4009.6650,  2962.5443,  3198.6789,
           534.2798]], dtype=torch.float64)
	q_value: tensor([[-24.4920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0647464835151879, distance: 1.644334090937832 entropy 8.756143354892773
epoch: 11, step: 26
	action: tensor([[-1408.2609,    89.3909,  2092.5360, -3359.1739,  2105.7331,  -269.6188,
           571.5264]], dtype=torch.float64)
	q_value: tensor([[-27.9734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08122841237553313, distance: 1.1899135719047023 entropy 8.947929789590939
epoch: 11, step: 27
	action: tensor([[-1037.6607, -1437.2130,   419.1824,  2149.3772,  1024.2692,  2573.4191,
          1101.9426]], dtype=torch.float64)
	q_value: tensor([[-23.4586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5014175773033906, distance: 1.402191858723757 entropy 8.446314999412067
epoch: 11, step: 28
	action: tensor([[ -82.9784, 1271.6931, -562.7107,  164.4597, -172.2157,  893.7013,
         2664.8405]], dtype=torch.float64)
	q_value: tensor([[-19.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.747328204430788, distance: 0.5752214553775012 entropy 8.503916882363695
epoch: 11, step: 29
	action: tensor([[ -602.3744, -1031.4529, -1761.9758,  2016.8257, -1251.6339,   445.1339,
           869.4740]], dtype=torch.float64)
	q_value: tensor([[-24.6201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025457354952441502, distance: 1.1588187015822176 entropy 8.727949666496801
epoch: 11, step: 30
	action: tensor([[-3892.8191, -2012.0865,  -922.2546,  -145.1094,  2565.3135, -1044.2383,
          2351.5046]], dtype=torch.float64)
	q_value: tensor([[-28.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8888592448374255, distance: 1.572738402417797 entropy 8.950014481881096
epoch: 11, step: 31
	action: tensor([[-1057.0136,  -204.8160,   404.7373,  1553.7936,   341.5777, -2348.9305,
           146.2445]], dtype=torch.float64)
	q_value: tensor([[-19.9019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024530202126754386, distance: 1.1302216105057175 entropy 8.624969794225832
epoch: 11, step: 32
	action: tensor([[-2482.4469,   596.3887, -3799.0384, -1394.9204,  2231.9072,   167.4748,
         -2419.4021]], dtype=torch.float64)
	q_value: tensor([[-26.8351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5035553913633213, distance: 1.4031897687063066 entropy 8.913117924060714
epoch: 11, step: 33
	action: tensor([[ -389.7722, -1229.1342,  1777.9016,  1508.8745,  -565.5618,    93.7295,
          1612.0980]], dtype=torch.float64)
	q_value: tensor([[-22.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06631391925104746, distance: 1.1057504772829363 entropy 8.55262168569605
epoch: 11, step: 34
	action: tensor([[-2659.8932, -1882.4378,  -724.5160,   212.5947, -1765.2314,    17.3232,
          1734.4651]], dtype=torch.float64)
	q_value: tensor([[-24.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.773816135171389
epoch: 11, step: 35
	action: tensor([[-729.2275, -475.7697, 1294.3294, 1799.9819, -188.9005,   60.3770,
         1594.9675]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009574035755064991, distance: 1.138853082820962 entropy 8.257061183465849
epoch: 11, step: 36
	action: tensor([[-998.2789, 1112.2860,  550.6404, 2201.7720,  401.6098, 2200.8141,
         -337.1108]], dtype=torch.float64)
	q_value: tensor([[-21.2830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1276553069238967, distance: 1.0688106065468572 entropy 8.692594360893196
epoch: 11, step: 37
	action: tensor([[-4217.7309,  -440.5947,  1061.3324,   323.1095, -1347.1224,  1280.0171,
          1993.5737]], dtype=torch.float64)
	q_value: tensor([[-28.5372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8097442025915687, distance: 1.5394489448899003 entropy 8.936687303937125
epoch: 11, step: 38
	action: tensor([[ -953.2489,  -675.0247, -5474.0615, -1439.3849, -1987.9381, -3604.7042,
           122.6032]], dtype=torch.float64)
	q_value: tensor([[-26.7861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9849063928404178, distance: 1.6122289247963577 entropy 8.913160929392538
epoch: 11, step: 39
	action: tensor([[   -8.5251, -1126.6066,  -965.8127,   956.1857, -1192.2618, -1820.1204,
          2345.9415]], dtype=torch.float64)
	q_value: tensor([[-27.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06518501409107991, distance: 1.106418748103254 entropy 8.969990177705588
epoch: 11, step: 40
	action: tensor([[-2402.1622,  2679.2328,  2133.2463,   738.5405,  -355.6190,  -494.9427,
          -541.3738]], dtype=torch.float64)
	q_value: tensor([[-27.5441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33656674449260005, distance: 0.9320845557129414 entropy 8.992198420590729
epoch: 11, step: 41
	action: tensor([[-1304.8974, -1982.2101,    88.0730,   461.2396,  1683.1148, -3325.3003,
          -429.5147]], dtype=torch.float64)
	q_value: tensor([[-31.1740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10536980917240157, distance: 1.2031242835175637 entropy 8.98983114458186
epoch: 11, step: 42
	action: tensor([[-2203.6864, -4613.0143,   470.7691,  1529.6672, -2211.5319,    37.9582,
          -441.1095]], dtype=torch.float64)
	q_value: tensor([[-25.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04641542474844995, distance: 1.1174711045852124 entropy 8.893310945673452
epoch: 11, step: 43
	action: tensor([[-2865.3753,   396.0316,  -629.0668, -1496.4531,   491.3937,  -766.6251,
          3584.5816]], dtype=torch.float64)
	q_value: tensor([[-17.6230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5065536477134445, distance: 1.4045881300103897 entropy 8.57236265174356
epoch: 11, step: 44
	action: tensor([[ 1084.3033, -3232.5271,  -849.7022,   445.8642,   216.3786,  1260.1931,
          -807.0601]], dtype=torch.float64)
	q_value: tensor([[-27.8688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13053966092184355, distance: 1.0670421653059727 entropy 8.761172918089423
epoch: 11, step: 45
	action: tensor([[  243.4211,  2149.3294,  -758.4503, -1813.7976, -4010.0181,  -581.0948,
         -1227.2787]], dtype=torch.float64)
	q_value: tensor([[-23.5133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7935389812321344, distance: 0.5199669335623592 entropy 8.918317507598433
epoch: 11, step: 46
	action: tensor([[ -322.3221, -3170.1304,  1319.4070,   608.3117,   410.1582,  1631.9780,
         -1479.6387]], dtype=torch.float64)
	q_value: tensor([[-22.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006850102062214258, distance: 1.1404180813079168 entropy 8.598458801128146
epoch: 11, step: 47
	action: tensor([[ 1304.3362,  -838.2153, -1832.7752, -3338.6796,  -486.2655, -1731.9108,
           319.2261]], dtype=torch.float64)
	q_value: tensor([[-24.8403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4541671062281156, distance: 1.3799515910262379 entropy 8.915147140324665
epoch: 11, step: 48
	action: tensor([[-2336.5323,  1331.4842,  -720.0889,  -505.7143, -1430.0809,  1334.1400,
         -2260.3037]], dtype=torch.float64)
	q_value: tensor([[-26.3223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.853228019634935
epoch: 11, step: 49
	action: tensor([[ -556.7334, -1027.7063,    74.8075,   940.7659,   447.5639,   -66.2098,
          1631.0749]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.257061183465849
epoch: 11, step: 50
	action: tensor([[  521.8860, -1891.7393,  -376.7848,   517.7928,  -536.8669,   390.5803,
          -656.3769]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18841852417859073, distance: 1.2475024280923488 entropy 8.257061183465849
epoch: 11, step: 51
	action: tensor([[ -485.6243,  1239.3805, -1933.7183,   593.6334,  1102.0394,   654.7886,
          -176.3643]], dtype=torch.float64)
	q_value: tensor([[-18.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.708223235633952, distance: 0.6181331644202567 entropy 8.794276710508134
epoch: 11, step: 52
	action: tensor([[-1288.5971, -2177.7841, -2143.5209, -2642.5028, -1803.2796,  1525.5277,
          -123.7253]], dtype=torch.float64)
	q_value: tensor([[-26.4134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9160461245428404, distance: 1.5840163975449444 entropy 8.79257967311387
epoch: 11, step: 53
	action: tensor([[ 688.4048,  563.4600, -456.2628,  854.3892,  763.2377, 2094.6806,
         3303.4579]], dtype=torch.float64)
	q_value: tensor([[-22.9265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.755619404529849
epoch: 11, step: 54
	action: tensor([[-2155.0238,  -332.0178,  -502.5717,  2085.2356,   770.8084, -2347.7626,
          1843.3031]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.042702715494238985, distance: 1.1685221404479575 entropy 8.257061183465849
epoch: 11, step: 55
	action: tensor([[-2897.2193, -1097.2113, -3017.1916,   745.4286,  1891.2428,  -891.8050,
            32.5539]], dtype=torch.float64)
	q_value: tensor([[-21.7576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8544289245546539, distance: 1.5583384611705606 entropy 8.777342507545132
epoch: 11, step: 56
	action: tensor([[  -34.3971,  -436.3363,  2804.9215,  1246.2384, -1737.8649, -1270.2989,
          2687.5100]], dtype=torch.float64)
	q_value: tensor([[-20.6958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2425999788572959, distance: 0.9959082951478468 entropy 8.713317518117977
epoch: 11, step: 57
	action: tensor([[-2984.4675, -1872.3550, -3719.1648, -2733.3504, -1551.7351,  3290.5475,
          1020.2784]], dtype=torch.float64)
	q_value: tensor([[-25.3492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9855321396311285, distance: 1.612483034406098 entropy 8.975986481864684
epoch: 11, step: 58
	action: tensor([[-2426.9914,  -555.4868, -2726.6551,  1657.4410,   912.6300,  -970.8554,
          -113.6486]], dtype=torch.float64)
	q_value: tensor([[-22.0678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6319242187406313, distance: 1.4618630435143802 entropy 8.673282396581142
epoch: 11, step: 59
	action: tensor([[ -101.8066,  -855.8933,  1510.0226,  1076.1934,   777.0404, -1118.5751,
          -125.4897]], dtype=torch.float64)
	q_value: tensor([[-19.8173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04779100739374853, distance: 1.1166648160522428 entropy 8.68967946303783
epoch: 11, step: 60
	action: tensor([[  142.7497,    42.0363, -2983.3266,   560.7430,  3472.7213,  1381.8873,
         -1953.9296]], dtype=torch.float64)
	q_value: tensor([[-23.4906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.86906723634317
epoch: 11, step: 61
	action: tensor([[ -163.3368, -1088.0184,  -635.3724,  3264.8039,  1388.6992,   795.7572,
          -393.4031]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.209597853244195, distance: 1.017374267587916 entropy 8.257061183465849
epoch: 11, step: 62
	action: tensor([[ 1997.6390, -3007.1251,  -180.7508,  -108.1903,  1945.5715,  -275.5990,
           -11.3847]], dtype=torch.float64)
	q_value: tensor([[-21.6204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4293617466136115, distance: 0.8644445371622319 entropy 8.679462365713857
epoch: 11, step: 63
	action: tensor([[-2894.8719,   235.9754, -2736.4989,   -68.9816,  -278.8821,  4093.5355,
            35.0832]], dtype=torch.float64)
	q_value: tensor([[-17.5861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.572250719207636
epoch: 11, step: 64
	action: tensor([[ -934.0868, -1031.8081,  1000.6276,  -429.4969,  1121.9117, -1013.0185,
           435.5703]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4175953813369915, distance: 1.3624884771843782 entropy 8.257061183465849
epoch: 11, step: 65
	action: tensor([[ -889.6240, -2100.7399, -3821.6682,  -499.1366,  1039.2730,  2095.0868,
         -2078.3873]], dtype=torch.float64)
	q_value: tensor([[-26.2551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10201685879135758, distance: 1.08440318607295 entropy 8.931035172632559
epoch: 11, step: 66
	action: tensor([[-1665.1827, -2102.5961,  1137.7179,  2382.2040,    33.5758,  2773.7686,
          1161.2128]], dtype=torch.float64)
	q_value: tensor([[-26.5343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5260092756898944, distance: 1.4136284597778417 entropy 8.832750183939009
epoch: 11, step: 67
	action: tensor([[-1273.4238, -1121.5808,   450.2694, -2502.2252,  1063.7107,  1237.8354,
          1932.8891]], dtype=torch.float64)
	q_value: tensor([[-20.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2811996094506173, distance: 1.2952842718186228 entropy 8.621278115940159
epoch: 11, step: 68
	action: tensor([[-4567.4363, -4324.6224,   909.8526,  3157.9093,   964.2144,  4350.8691,
          3144.0274]], dtype=torch.float64)
	q_value: tensor([[-27.3196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7919044439154237, distance: 1.531842506870562 entropy 8.88320850043692
epoch: 11, step: 69
	action: tensor([[-4395.7118, -2444.5574,   829.7278,  3524.3993, -4149.4441,  5528.1395,
          2635.7630]], dtype=torch.float64)
	q_value: tensor([[-27.0645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7627943549856011, distance: 1.5193489106872937 entropy 8.901502476159184
epoch: 11, step: 70
	action: tensor([[ -568.7604,  -887.8237, -5039.3342, -2101.1362,  1574.1573,   328.8079,
          2423.6159]], dtype=torch.float64)
	q_value: tensor([[-29.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17313096998351685, distance: 1.239452658421157 entropy 9.017522178334913
epoch: 11, step: 71
	action: tensor([[-2048.9414,  -369.7110,   644.2377,   798.1816, -1082.1723,  2870.5718,
           910.6064]], dtype=torch.float64)
	q_value: tensor([[-24.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15774572241373996, distance: 1.2312983168354177 entropy 8.820949370154583
epoch: 11, step: 72
	action: tensor([[  830.0271,   625.1164,  2637.6073,  2042.0327,   443.2976,   937.7646,
         -2494.9012]], dtype=torch.float64)
	q_value: tensor([[-22.8716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.80608937637477
epoch: 11, step: 73
	action: tensor([[   91.1517, -2105.6650,    46.5909, -1795.0115,  1332.7463,  -701.6443,
          -294.7991]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3364569162487877, distance: 0.9321617036194222 entropy 8.257061183465849
epoch: 11, step: 74
	action: tensor([[ 1036.4945,  -193.9956,  1351.3770, -1532.4300, -1448.7254,  2588.3685,
         -2590.2610]], dtype=torch.float64)
	q_value: tensor([[-27.4352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5409927764373252, distance: 0.7752940542918354 entropy 8.772408246734205
epoch: 11, step: 75
	action: tensor([[-1836.9046, -2922.6087, -1188.9204,   471.9735,   303.8068,   100.7380,
          5232.3308]], dtype=torch.float64)
	q_value: tensor([[-21.7208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5232592110491256, distance: 1.4123541153949943 entropy 8.72707896172284
epoch: 11, step: 76
	action: tensor([[-3878.0101,  -733.5507,  2840.0907,  1699.8506,  -642.2572,  2423.7257,
          1150.5060]], dtype=torch.float64)
	q_value: tensor([[-20.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5725912757002571, distance: 1.4350420407218183 entropy 8.677646079238416
epoch: 11, step: 77
	action: tensor([[ 2866.8653,    24.3824,   316.4260,  2825.0014, -1427.5975,  -460.3585,
           328.6320]], dtype=torch.float64)
	q_value: tensor([[-23.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23809217853667963, distance: 1.2733071661243354 entropy 8.775503078000812
epoch: 11, step: 78
	action: tensor([[ -991.1298,   143.3202,   213.4751,  1226.6522,  -843.9748, -1398.9072,
          2208.5941]], dtype=torch.float64)
	q_value: tensor([[-22.0517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40433741987884186, distance: 0.8831955098157279 entropy 8.678727974381896
epoch: 11, step: 79
	action: tensor([[ -744.5258,  -636.3209, -1047.0980, -2856.1029,  1129.5557,   926.1555,
          1840.4354]], dtype=torch.float64)
	q_value: tensor([[-25.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.86884481243796
epoch: 11, step: 80
	action: tensor([[-656.2246, -504.9507,  327.4687, 1151.5515, -611.8433,   94.7122,
         -238.9083]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.180914589261528, distance: 1.689958431028473 entropy 8.257061183465849
epoch: 11, step: 81
	action: tensor([[-1386.5486, -2146.7794,   845.8370,   114.0672,  1910.7864,  2909.4476,
          -114.1591]], dtype=torch.float64)
	q_value: tensor([[-21.7765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07418717948034803, distance: 1.1010785151753189 entropy 8.750771729423569
epoch: 11, step: 82
	action: tensor([[  154.2759, -2378.1821,  2017.5054, -2861.2737,  2198.0690, -1121.0795,
          1723.9784]], dtype=torch.float64)
	q_value: tensor([[-22.9576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15991845209604594, distance: 1.0488597413932537 entropy 8.810977810963449
epoch: 11, step: 83
	action: tensor([[ -384.3237, -3560.4816,  -958.9464,   912.9452, -1754.5665,  1503.9277,
          -278.6439]], dtype=torch.float64)
	q_value: tensor([[-21.3981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34155529267730156, distance: 1.3254427738537011 entropy 8.639556786733653
epoch: 11, step: 84
	action: tensor([[ -946.2485,  -478.2731,   936.4965,     6.9971, -4494.1093,  2539.9993,
          -792.1394]], dtype=torch.float64)
	q_value: tensor([[-21.4801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9470318482097515, distance: 1.5967731490149935 entropy 8.754086133034752
epoch: 11, step: 85
	action: tensor([[ -797.2103, -3245.7303,  1235.0954,   874.2103,  1876.2331,  4097.6795,
          2550.4923]], dtype=torch.float64)
	q_value: tensor([[-20.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.662748769332955, distance: 1.4756046353087258 entropy 8.628603792823837
epoch: 11, step: 86
	action: tensor([[-1656.0288,  -910.1201,  2571.7664,  1075.7508,  1009.5645,  1053.4299,
          -564.9675]], dtype=torch.float64)
	q_value: tensor([[-24.1426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12187846527865487, distance: 1.2120752943604602 entropy 8.904519254189816
epoch: 11, step: 87
	action: tensor([[-1673.2813,  2602.5382,  2409.2671,  4221.9610,  -119.0369,  1667.2813,
          1328.2786]], dtype=torch.float64)
	q_value: tensor([[-27.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048070016515708414, distance: 1.1165012057007695 entropy 8.941026511790087
epoch: 11, step: 88
	action: tensor([[ -861.8943,  1162.5179,  1863.5585,   567.4755, -1178.9629, -1649.7761,
          3974.9696]], dtype=torch.float64)
	q_value: tensor([[-29.3302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08522257199426053, distance: 1.0944965910198563 entropy 8.882430493358349
epoch: 11, step: 89
	action: tensor([[-4541.5938, -2822.9820, -4218.2833,   264.8799,   192.3038,  -534.8587,
          2074.7643]], dtype=torch.float64)
	q_value: tensor([[-28.3670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11755767779080672, distance: 1.209738957490977 entropy 8.88271488110435
epoch: 11, step: 90
	action: tensor([[-3574.3133, -1983.2885,  2953.0852,  1590.7554,  2567.4044, -1198.7267,
          3576.6267]], dtype=torch.float64)
	q_value: tensor([[-27.7149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07032323698817577, distance: 1.1033738356180658 entropy 9.065177808537538
epoch: 11, step: 91
	action: tensor([[-1182.8142,  -972.7474, -1136.2924,  -218.5668,  2323.5832, -1303.7656,
          1263.0436]], dtype=torch.float64)
	q_value: tensor([[-23.4464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36991721677280753, distance: 1.3393801404284578 entropy 8.891147060780586
epoch: 11, step: 92
	action: tensor([[-2781.9768, -1032.2222,   -29.8835,  -885.0537,  -843.3055,  5001.1567,
          -210.7461]], dtype=torch.float64)
	q_value: tensor([[-19.4108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6975920206060591, distance: 1.49098528013148 entropy 8.537656664025771
epoch: 11, step: 93
	action: tensor([[  765.7890, -2113.8260, -1469.9419, -1930.6509, -4670.0933,  1259.3457,
          1779.2980]], dtype=torch.float64)
	q_value: tensor([[-23.5779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1553656588359914, distance: 1.0516980308610597 entropy 8.719323308460428
epoch: 11, step: 94
	action: tensor([[   81.6666,  -894.6270,  -713.8834,  1652.9518,  1901.5723,  -150.2209,
         -1905.6241]], dtype=torch.float64)
	q_value: tensor([[-19.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6167059233490451, distance: 1.4550308719795801 entropy 8.597918966504963
epoch: 11, step: 95
	action: tensor([[-1012.6568, -2103.5741,  1186.5158,   105.2221, -4054.6539,  2452.7829,
         -2007.0124]], dtype=torch.float64)
	q_value: tensor([[-21.0354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10099042174133688, distance: 1.2007385767282361 entropy 8.672099789468124
epoch: 11, step: 96
	action: tensor([[-2679.4563,   -38.0496, -2826.2158,   650.4886,  -166.8959,  1845.6020,
         -1253.9003]], dtype=torch.float64)
	q_value: tensor([[-23.2489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4164114769462708, distance: 1.3619194174095548 entropy 9.018168485176828
epoch: 11, step: 97
	action: tensor([[   92.5942,  -301.7232,   549.4235,  -621.3257,  1277.8126,  4075.3116,
         -1860.1371]], dtype=torch.float64)
	q_value: tensor([[-29.5990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4219100857059268, distance: 1.3645603901619832 entropy 8.995419571777992
epoch: 11, step: 98
	action: tensor([[-686.6145, -285.9406,   62.4490,  231.1359, -784.0024, 2874.2308,
         -375.3929]], dtype=torch.float64)
	q_value: tensor([[-23.5657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4276821711310348, distance: 1.3673272254567561 entropy 8.773907345381526
epoch: 11, step: 99
	action: tensor([[ -933.5887,   877.7905,    93.6521, -1772.5551,  3834.9676, -1814.1139,
           774.1448]], dtype=torch.float64)
	q_value: tensor([[-25.5237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15892905613359742, distance: 1.2319274120976837 entropy 8.850142886971673
epoch: 11, step: 100
	action: tensor([[-1477.6964, -2989.0594,   -72.7916,  1227.0758,  -106.9026,    80.6193,
         -3922.4969]], dtype=torch.float64)
	q_value: tensor([[-26.8555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3576550014488721, distance: 0.9171510481983975 entropy 8.615745793139888
epoch: 11, step: 101
	action: tensor([[ -748.2862, -2327.0946, -1485.4528, -1375.8464,  -677.2621,   400.5391,
           131.3472]], dtype=torch.float64)
	q_value: tensor([[-24.3697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8607973697538467, distance: 1.561011976232183 entropy 9.00929398438109
epoch: 11, step: 102
	action: tensor([[ 1814.8926, -2682.7346,  -630.9617,   198.4697, -1839.7729, -1032.2888,
           487.5257]], dtype=torch.float64)
	q_value: tensor([[-24.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4180209572110942, distance: 1.362692977960916 entropy 8.755541446042283
epoch: 11, step: 103
	action: tensor([[-1785.1738,  1005.6423, -2549.1352,  -616.3387,  1790.0955, -2835.6618,
          -185.8150]], dtype=torch.float64)
	q_value: tensor([[-21.1395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.81929246114555
epoch: 11, step: 104
	action: tensor([[-2186.8061,  -345.2451,  -436.6189, -1128.5137,    63.0042,  1528.3681,
          -258.7013]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8917951153797736, distance: 1.5739601884118333 entropy 8.257061183465849
epoch: 11, step: 105
	action: tensor([[-1973.8060,   640.0703, -2300.9895, -1030.3979,  1195.1902,  -489.1078,
          -641.0083]], dtype=torch.float64)
	q_value: tensor([[-26.9402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09671844685860786, distance: 1.0875976583016178 entropy 8.78991930024795
epoch: 11, step: 106
	action: tensor([[-2341.8172, -1819.1692,  1825.8570,  -797.4121,  1449.2619,  1531.5060,
          2619.7963]], dtype=torch.float64)
	q_value: tensor([[-32.9537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.69041896829656
epoch: 11, step: 107
	action: tensor([[ -780.8372,  -871.0784, -1736.7758,   264.4051,  -718.6436,    34.6820,
           721.5759]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.257061183465849
epoch: 11, step: 108
	action: tensor([[ -916.5380, -1735.0869,  -688.4874,   950.1276,  -950.8284,   577.3394,
         -1094.3439]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.257061183465849
epoch: 11, step: 109
	action: tensor([[   50.8015, -1612.9724,  -968.9863,  1249.3479,  -623.6230, -1619.5306,
          -559.4947]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.257061183465849
epoch: 11, step: 110
	action: tensor([[-1254.7168,  1143.2653,   992.5043,  1072.6340, -1830.4682, -1027.1870,
           110.3859]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.257061183465849
epoch: 11, step: 111
	action: tensor([[ -623.6656,  -885.0920,    35.4323, -1723.3429,   272.0205,  -617.4619,
           139.2817]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.257061183465849
epoch: 11, step: 112
	action: tensor([[-508.0202,  818.1912,  401.4695,  218.6595,  640.0368, -353.0310,
         1396.5118]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.257061183465849
epoch: 11, step: 113
	action: tensor([[ -521.6626, -1692.6598, -1399.0661,  -100.8916,   256.2159,  2013.4247,
          1587.8473]], dtype=torch.float64)
	q_value: tensor([[-25.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3701738800660921, distance: 1.3395056055343795 entropy 8.257061183465849
epoch: 11, step: 114
	action: tensor([[-3000.6701, -2596.4713,  2315.3047,   686.5522,   410.0048,   801.7457,
         -1524.6390]], dtype=torch.float64)
	q_value: tensor([[-26.9642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30529859132778747, distance: 1.307409474671851 entropy 8.853827342260741
epoch: 11, step: 115
	action: tensor([[-1289.4655, -5226.5918,  3554.7701,   453.4665, -2676.5785,  2386.1708,
         -1530.1995]], dtype=torch.float64)
	q_value: tensor([[-27.6106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060720146165593625, distance: 1.1090578419722439 entropy 8.93908437228524
epoch: 11, step: 116
	action: tensor([[  218.5038,  -271.9462,  -343.1702, -1941.4356, -2281.8409,   726.0977,
           235.8105]], dtype=torch.float64)
	q_value: tensor([[-26.3332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043557471727687425, distance: 1.119144417401918 entropy 8.887407758300398
epoch: 11, step: 117
	action: tensor([[ 1403.9547,  -850.6711, -1165.4595,  -923.3874,  -299.2404,   -25.0128,
            80.5255]], dtype=torch.float64)
	q_value: tensor([[-22.4338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.67053758257018, distance: 0.6568400623392632 entropy 8.887525539062178
epoch: 11, step: 118
	action: tensor([[ 3408.1922, -5779.1696,   168.8458, -1098.9081,  1766.0297,  -555.5386,
          2381.4281]], dtype=torch.float64)
	q_value: tensor([[-24.1542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2353877247057905, distance: 1.0006387712871019 entropy 8.883947485692705
epoch: 11, step: 119
	action: tensor([[-1957.0885,   -11.1810,  -864.5149,  -325.1358,  -937.3619,  -613.0296,
          -868.9751]], dtype=torch.float64)
	q_value: tensor([[-21.1637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3183815471268867, distance: 1.3139451952361862 entropy 8.612805406636346
epoch: 11, step: 120
	action: tensor([[ 3416.6562, -4134.5400, -3007.1812,  1975.8649,  1875.9089,   665.3651,
          2322.0271]], dtype=torch.float64)
	q_value: tensor([[-28.9834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025620033801611486, distance: 1.1589106156027873 entropy 8.775737838016257
epoch: 11, step: 121
	action: tensor([[  358.4775, -2482.2616,  -233.0965,  -466.7688,   -83.2385, -1873.7463,
         -2701.3152]], dtype=torch.float64)
	q_value: tensor([[-23.4409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21480488917199958, distance: 1.26127549738467 entropy 8.839542611075425
epoch: 11, step: 122
	action: tensor([[-459.1590, -781.9612, 1485.0418,  154.4371,  403.3835, -815.7732,
         -304.0186]], dtype=torch.float64)
	q_value: tensor([[-20.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09633392397263663, distance: 1.1981967013485484 entropy 8.543886723647343
epoch: 11, step: 123
	action: tensor([[ -813.2454, -2302.7157,   173.5575,  1111.8983,  2067.3785,  -707.6275,
          -242.8756]], dtype=torch.float64)
	q_value: tensor([[-21.4634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8222087320678795, distance: 1.544741289528589 entropy 8.628430882528184
epoch: 11, step: 124
	action: tensor([[-1419.6056, -1117.6316,  -815.3541,  2144.0863,  -741.4655, -1312.2219,
           782.9040]], dtype=torch.float64)
	q_value: tensor([[-24.7911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29783387166074116, distance: 1.3036657188899803 entropy 8.853973960052192
epoch: 11, step: 125
	action: tensor([[ -106.0347,  1593.4465,   294.3425,  1104.5894,  4157.4217,  2477.3810,
         -1665.6486]], dtype=torch.float64)
	q_value: tensor([[-25.0449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31444163989103946, distance: 0.9474993242345143 entropy 8.875221107871175
epoch: 11, step: 126
	action: tensor([[-200.0650, -419.8748, -182.8114,  596.2141, -684.9205, -531.7217,
         2563.5139]], dtype=torch.float64)
	q_value: tensor([[-23.9458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7691980997552201, distance: 1.5221060963054263 entropy 8.717520709547228
epoch: 11, step: 127
	action: tensor([[-2031.2427, -1728.2683,   -28.3883,   391.8855,  -894.6987,  1284.2786,
           708.8698]], dtype=torch.float64)
	q_value: tensor([[-17.2654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41514749763924597, distance: 1.3613116059770232 entropy 8.495945702851007
LOSS epoch 11 actor 253.97249494239355 critic 289.4407633024244
epoch: 12, step: 0
	action: tensor([[-1692.1646, -3130.5536,     5.7196,  2642.2079,  1261.2660,  3372.5190,
         -1339.5240]], dtype=torch.float64)
	q_value: tensor([[-28.6848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21745388757326234, distance: 1.2626499145370806 entropy 9.089747608780245
epoch: 12, step: 1
	action: tensor([[-2566.0009, -4263.7399,  1743.8317,  4047.8060,  1898.5879, -2491.8245,
         -2146.4641]], dtype=torch.float64)
	q_value: tensor([[-26.2284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24841025378022907, distance: 1.278601933407274 entropy 8.967722731384859
epoch: 12, step: 2
	action: tensor([[-5755.8626, -2700.6053,  -546.8304,  2626.2296,  2270.2763,   472.5104,
           796.6628]], dtype=torch.float64)
	q_value: tensor([[-30.7268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8531464118002907, distance: 1.557799498852189 entropy 9.070315758109329
epoch: 12, step: 3
	action: tensor([[-2216.5708,   671.9204,  1479.1561,   418.3251, -2596.5874,  2100.7738,
           425.2707]], dtype=torch.float64)
	q_value: tensor([[-24.4123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0776907452169655, distance: 1.1879653401352381 entropy 8.894699471959026
epoch: 12, step: 4
	action: tensor([[-4754.0678,   731.0253,  -113.5649,  1066.3141,  -318.0192,  -253.3975,
          1433.7496]], dtype=torch.float64)
	q_value: tensor([[-32.4650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21293691815874216, distance: 1.2603054105478992 entropy 9.054203690141708
epoch: 12, step: 5
	action: tensor([[-1541.9162, -1224.1152,   233.1130,  1543.6728,  -547.0417, -1772.6709,
          1467.9245]], dtype=torch.float64)
	q_value: tensor([[-26.1128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49474716988858214, distance: 1.3990736047809695 entropy 8.745125252833487
epoch: 12, step: 6
	action: tensor([[   -6.0064,  -282.6356,  1114.8210,  2375.2112,  1861.3254, -3372.8434,
         -1357.9179]], dtype=torch.float64)
	q_value: tensor([[-24.9225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5448756090748879, distance: 0.7720079073299669 entropy 8.868390257583735
epoch: 12, step: 7
	action: tensor([[ -211.2167, -3015.5153, -1014.1553,  1245.8794,  -129.7986,  1488.8078,
          1429.3926]], dtype=torch.float64)
	q_value: tensor([[-25.2799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7477735543381188, distance: 1.5128618631221358 entropy 8.733404428834858
epoch: 12, step: 8
	action: tensor([[  751.2099, -3103.4928,   272.6923, -1158.3517,  -687.4484,  3175.5779,
         -2784.1320]], dtype=torch.float64)
	q_value: tensor([[-29.8169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6099122087931584, distance: 0.7147231881090751 entropy 9.091347498440626
epoch: 12, step: 9
	action: tensor([[-1815.8545,  1778.5062,   376.1485,  3616.8497, -1376.4965,   608.5061,
         -1146.7350]], dtype=torch.float64)
	q_value: tensor([[-24.2713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.830709384034426
epoch: 12, step: 10
	action: tensor([[ -182.6705,   -99.2622, -1965.3095,    43.1226,   632.4416, -1237.0547,
          1808.4927]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2537815560195854, distance: 0.9885296107075349 entropy 8.352317888217913
epoch: 12, step: 11
	action: tensor([[  483.4138,  -431.1595, -2612.4916,  -585.7732,  1197.9778,    32.2232,
          -309.7003]], dtype=torch.float64)
	q_value: tensor([[-27.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5616586608292986, distance: 1.4300451529861167 entropy 8.67216408479297
epoch: 12, step: 12
	action: tensor([[-2465.6869, -2426.7677, -2062.6974,  1418.0463,  -746.6709, -2455.2753,
          4626.6241]], dtype=torch.float64)
	q_value: tensor([[-23.5167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7488889734724151, distance: 1.513344536039665 entropy 8.71355570008832
epoch: 12, step: 13
	action: tensor([[  502.0319, -2101.9835, -3177.0692,  4019.5897,   886.3061,   563.7090,
         -1630.7205]], dtype=torch.float64)
	q_value: tensor([[-26.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.857770521902891
epoch: 12, step: 14
	action: tensor([[-1092.5476,   638.0148,  -769.6729,   526.6217,  1315.9062, -1105.6791,
          -925.3899]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22673636199531222, distance: 1.0062838147012052 entropy 8.352317888217913
epoch: 12, step: 15
	action: tensor([[ -997.1754, -1884.4944, -1899.9855,  4661.3067,  1351.9157,  3116.5727,
          -868.3019]], dtype=torch.float64)
	q_value: tensor([[-33.4653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2632612990893364, distance: 1.2861845516795547 entropy 8.989633554117898
epoch: 12, step: 16
	action: tensor([[  540.7625,  2246.2817,  1902.2853,  -135.1314,  1257.7318, -2944.6246,
          2278.0886]], dtype=torch.float64)
	q_value: tensor([[-27.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.967121855130811
epoch: 12, step: 17
	action: tensor([[  668.0492, -1135.8390,  -967.3728,  -957.7657,   -36.8344,  -754.2582,
          -215.9450]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921199199249839, distance: 0.9628010138885457 entropy 8.352317888217913
epoch: 12, step: 18
	action: tensor([[-1517.9663,  -995.2599,  -960.1657,  -664.4607, -1543.8858, -1369.4467,
         -3356.9012]], dtype=torch.float64)
	q_value: tensor([[-24.4904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6121240931281016, distance: 1.452967591800178 entropy 8.677921709756962
epoch: 12, step: 19
	action: tensor([[-2040.2462,   137.7575,  -131.3415,  1677.9153,  -177.2321,  1139.6612,
         -2263.6839]], dtype=torch.float64)
	q_value: tensor([[-28.7716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03731594674860439, distance: 1.165499845985497 entropy 8.863026069704889
epoch: 12, step: 20
	action: tensor([[  646.3034,  -836.8281,   338.7826,  1228.1831, -2331.6318,    -6.2578,
          2340.1872]], dtype=torch.float64)
	q_value: tensor([[-25.0584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.665600030946283
epoch: 12, step: 21
	action: tensor([[  313.7098,   198.4664, -1512.9313,   -80.1324,  -229.6873,  1134.3219,
         -1720.8344]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.352317888217913
epoch: 12, step: 22
	action: tensor([[ 1004.1847,  -273.9434,   758.0574,  -174.2468,   631.1222,   670.7369,
         -1486.2095]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.352317888217913
epoch: 12, step: 23
	action: tensor([[1133.7760, -350.0417, -564.4071, 1733.1556,  277.9091,  762.6339,
         -659.6160]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6417886131165789, distance: 0.6848987709173859 entropy 8.352317888217913
epoch: 12, step: 24
	action: tensor([[-3014.7009,  -639.9010, -2571.9476, -1383.5010,   -79.3795,  2595.1381,
           702.5013]], dtype=torch.float64)
	q_value: tensor([[-20.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04354327282918491, distance: 1.1689930378443807 entropy 8.637319014950851
epoch: 12, step: 25
	action: tensor([[ -685.6989, -1336.7518, -2587.4651,  1304.2236, -1304.0675,  2461.7115,
           172.1391]], dtype=torch.float64)
	q_value: tensor([[-26.7432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5106180293365534, distance: 1.4064815033688336 entropy 8.839188195566555
epoch: 12, step: 26
	action: tensor([[ 2136.0383, -4136.6384,  1157.4711,  1885.2019,  -633.3291,   865.7319,
           412.2112]], dtype=torch.float64)
	q_value: tensor([[-27.7276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.040953671140406156, distance: 1.1675416812936656 entropy 9.063604353361644
epoch: 12, step: 27
	action: tensor([[  568.2951,  2484.1510, -1306.9337,  2348.2409, -1270.7260,  -223.6210,
          2955.3206]], dtype=torch.float64)
	q_value: tensor([[-25.1018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03062723379554333, distance: 1.126683931254581 entropy 8.892140103368181
epoch: 12, step: 28
	action: tensor([[-2751.3236,   184.3937, -2193.3222,  -474.3836,  3117.3871,   390.7019,
          1727.7045]], dtype=torch.float64)
	q_value: tensor([[-24.1167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6442051909453093, distance: 0.6825846176785338 entropy 8.751052603266858
epoch: 12, step: 29
	action: tensor([[ -571.1431,  -996.0787,    78.8018, -1883.5318,   714.0588,   143.8004,
           760.9749]], dtype=torch.float64)
	q_value: tensor([[-22.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20119107378275858, distance: 1.022770393538025 entropy 8.247144240776972
epoch: 12, step: 30
	action: tensor([[-1805.5265, -1184.2676, -1758.4744,  2455.7302,   823.0912, -1699.4001,
          1510.2760]], dtype=torch.float64)
	q_value: tensor([[-31.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6602971313710968, distance: 1.4745163822603102 entropy 8.897441582365518
epoch: 12, step: 31
	action: tensor([[-2001.9009,   141.6747,   320.6276, -3690.5684,  1189.6211,   889.4926,
         -1391.9394]], dtype=torch.float64)
	q_value: tensor([[-22.0929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14645893044526426, distance: 1.0572286037903211 entropy 8.673438826877653
epoch: 12, step: 32
	action: tensor([[ 1530.2781, -2336.2728, -2402.7372,  4424.2294, -1798.1468,   382.1780,
          -465.8502]], dtype=torch.float64)
	q_value: tensor([[-34.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16557349226749807, distance: 1.2354538420128591 entropy 8.898442444550556
epoch: 12, step: 33
	action: tensor([[-2759.9854,  -371.5803,  -855.5284,  2090.8135,   188.9612, -1333.8381,
          -148.9571]], dtype=torch.float64)
	q_value: tensor([[-19.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8623192437642282, distance: 1.5616501913470813 entropy 8.718101969627458
epoch: 12, step: 34
	action: tensor([[   44.0520, -1321.2070,  -424.9237,   801.7508,  1161.7714,  3712.0558,
          1697.9344]], dtype=torch.float64)
	q_value: tensor([[-20.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3532513180297623, distance: 0.9202895051730303 entropy 8.711013439917902
epoch: 12, step: 35
	action: tensor([[-1719.4983,   871.5122,  1635.0384, -1577.5998,  -161.0078,  3170.8911,
          1155.6523]], dtype=torch.float64)
	q_value: tensor([[-25.5027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08632735311698059, distance: 1.1927160151160177 entropy 8.88840729193831
epoch: 12, step: 36
	action: tensor([[-2413.4523,   740.8779,  -485.6451,  -477.4248,   673.8588,  2488.7960,
         -1769.0587]], dtype=torch.float64)
	q_value: tensor([[-27.9693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22049039941297166, distance: 1.0103397211869387 entropy 8.700370918765573
epoch: 12, step: 37
	action: tensor([[-2845.7444,  -872.4142,  1002.4965,   741.1258,  -520.1403, -1195.7426,
           692.1932]], dtype=torch.float64)
	q_value: tensor([[-43.5583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4157311164499462, distance: 1.3615922852579414 entropy 8.978025402171346
epoch: 12, step: 38
	action: tensor([[ -446.2651,   143.1788,  1329.5372,   277.5503, -3273.5925,  5271.3661,
          -291.4618]], dtype=torch.float64)
	q_value: tensor([[-29.8238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20313750133899133, distance: 1.2552040306589254 entropy 8.891393778814981
epoch: 12, step: 39
	action: tensor([[-2808.6477,  -398.3863, -1042.9144,   259.6528, -1236.0310,  2078.9027,
          3771.6008]], dtype=torch.float64)
	q_value: tensor([[-32.8110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33905832804902203, distance: 1.324208711730617 entropy 8.95281918000757
epoch: 12, step: 40
	action: tensor([[   84.5472, -1558.4576, -1332.4561,  -108.9827, -1230.2672, -1475.6532,
           587.9227]], dtype=torch.float64)
	q_value: tensor([[-27.7275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08277858610858768, distance: 1.190766265039722 entropy 8.8657883381331
epoch: 12, step: 41
	action: tensor([[-1418.1730, -4739.7150,   192.7310,    39.6263,  2035.5022,  -292.4343,
          -196.3656]], dtype=torch.float64)
	q_value: tensor([[-26.6498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9233903205048615, distance: 1.587049257864191 entropy 8.829798354288164
epoch: 12, step: 42
	action: tensor([[-1397.7220,  -223.1173,  1742.4040,   828.7523, -1605.2646, -1246.1938,
         -2200.2124]], dtype=torch.float64)
	q_value: tensor([[-23.1883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09149926021371202, distance: 1.0907352177881664 entropy 8.892080399350874
epoch: 12, step: 43
	action: tensor([[  813.3427, -2465.5121, -1314.2204,  -608.8215,    32.3166,   445.3170,
           345.7004]], dtype=torch.float64)
	q_value: tensor([[-29.1811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12697716257846325, distance: 1.0692259624644478 entropy 9.092361898468981
epoch: 12, step: 44
	action: tensor([[ -636.7724,  -780.3048, -1829.0771,  1494.3215,   -87.8264, -1240.1837,
          1306.4129]], dtype=torch.float64)
	q_value: tensor([[-18.3744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.668193558657115, distance: 1.4780186469717767 entropy 8.522018011947441
epoch: 12, step: 45
	action: tensor([[  432.2144,   697.0532,   566.9091, -3061.2745,  -611.1749, -1027.3044,
          2574.6332]], dtype=torch.float64)
	q_value: tensor([[-20.9847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.710806190763074
epoch: 12, step: 46
	action: tensor([[ -436.2867,    65.0378,  1608.3435,  2431.8078,   180.1445, -2345.4474,
         -1374.7791]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914196363706494, distance: 0.9632771305839858 entropy 8.352317888217913
epoch: 12, step: 47
	action: tensor([[-387.5261, -555.0114, -668.3205,  657.2344,  999.6932, 1323.3119,
         -918.3313]], dtype=torch.float64)
	q_value: tensor([[-25.1877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013312918060804702, distance: 1.1519363499489195 entropy 8.828823911187838
epoch: 12, step: 48
	action: tensor([[ -845.2498, -3386.2996,  -280.9696, -1044.8711,  1199.4454,   186.4445,
           122.3611]], dtype=torch.float64)
	q_value: tensor([[-26.2053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.620859898942328, distance: 1.4568989560707253 entropy 8.912508887992
epoch: 12, step: 49
	action: tensor([[ -474.1990, -4233.9919,  3461.0768,   289.0876,  1697.7937,  -278.6105,
           358.3119]], dtype=torch.float64)
	q_value: tensor([[-28.3765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4550327783585486, distance: 1.3803622755672198 entropy 8.973527459941005
epoch: 12, step: 50
	action: tensor([[-1410.8026, -1770.2916, -1082.5115,  -206.3045,  2444.2910,  1569.4930,
          -846.4012]], dtype=torch.float64)
	q_value: tensor([[-22.1794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7439724588962153, distance: 1.5112158654705317 entropy 8.757333550159249
epoch: 12, step: 51
	action: tensor([[-1329.4534, -3170.6833,  1138.0839,  -333.2638,     4.5804,  3291.0944,
           -62.0325]], dtype=torch.float64)
	q_value: tensor([[-25.3290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3368880685325666, distance: 1.3231351804693972 entropy 8.774589657691328
epoch: 12, step: 52
	action: tensor([[-1577.5641, -2434.4164,   451.8651,  1251.0037, -2465.0032,  -864.7630,
           271.8845]], dtype=torch.float64)
	q_value: tensor([[-26.1738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0241712650963821, distance: 1.6280971683411616 entropy 8.739111074348077
epoch: 12, step: 53
	action: tensor([[-2397.9222, -2126.8677, -2770.5492, -3408.8467,  -931.4493, -1963.2550,
          -493.6068]], dtype=torch.float64)
	q_value: tensor([[-23.2800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02764685393102484, distance: 1.1284146220376843 entropy 8.819585486708693
epoch: 12, step: 54
	action: tensor([[-1356.3093,  -702.5930,   276.5104,  2135.6770, -2520.0949,   211.6800,
          1454.6225]], dtype=torch.float64)
	q_value: tensor([[-29.7032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2674425986876696, distance: 1.7231569817815249 entropy 8.982572627807444
epoch: 12, step: 55
	action: tensor([[ 1069.0240,  -518.7973, -1868.6121,  2079.3959, -2096.9020,  2473.0405,
         -1551.4730]], dtype=torch.float64)
	q_value: tensor([[-22.4407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45985293387658877, distance: 0.84103235566923 entropy 8.783627881048973
epoch: 12, step: 56
	action: tensor([[ 227.6976, 1467.9745, -298.8571, -275.7683, 2072.1312, 1970.7763,
         2847.1167]], dtype=torch.float64)
	q_value: tensor([[-24.7545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7152761053421307, distance: 0.6106166624117781 entropy 8.802707966368192
epoch: 12, step: 57
	action: tensor([[ -477.6245, -1340.1111, -1156.7498,  1572.6580,   837.7125,  2857.5568,
         -4037.1552]], dtype=torch.float64)
	q_value: tensor([[-25.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17580888279193185, distance: 1.0388926125337978 entropy 8.735765091757916
epoch: 12, step: 58
	action: tensor([[  -24.1812, -1290.6709,  1368.3355,   335.7895,   640.5096, -1381.4759,
          -473.3165]], dtype=torch.float64)
	q_value: tensor([[-26.3970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9965112247075101, distance: 1.616935035536718 entropy 8.812787002028644
epoch: 12, step: 59
	action: tensor([[ -532.9939, -1823.6863,   387.1138, -1334.8615,  4915.3843, -1592.5844,
         -2935.6410]], dtype=torch.float64)
	q_value: tensor([[-22.5722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43842531560686493, distance: 1.3724620739928586 entropy 8.875929875321045
epoch: 12, step: 60
	action: tensor([[-2012.4500, -3646.7954,  1372.4363,   659.5033,  1308.8288,  1196.8489,
          2590.6726]], dtype=torch.float64)
	q_value: tensor([[-28.6818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2777122645449064, distance: 1.2935202293612127 entropy 8.913534814038583
epoch: 12, step: 61
	action: tensor([[ -103.3549, -1103.2930, -2036.0291,  1465.2272,  2466.5749,  -372.6207,
          -320.9629]], dtype=torch.float64)
	q_value: tensor([[-30.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40797540561626044, distance: 0.8804943382864786 entropy 9.064603914448115
epoch: 12, step: 62
	action: tensor([[ 664.8417, -650.5856,   74.5569, 2383.7714,  903.6465,  752.3901,
         -191.6539]], dtype=torch.float64)
	q_value: tensor([[-26.3838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05963527285243053, distance: 1.109698141042401 entropy 9.02059507854341
epoch: 12, step: 63
	action: tensor([[ -765.5134, -1699.6827,  1231.1065,   805.4242,  2308.1660, -1022.2972,
          2339.8289]], dtype=torch.float64)
	q_value: tensor([[-27.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05249597281917073, distance: 1.17399680407831 entropy 8.916723531783578
epoch: 12, step: 64
	action: tensor([[2782.4509, -807.7891,  158.1996,  198.6613, 2412.9460, 2328.9519,
         1069.5201]], dtype=torch.float64)
	q_value: tensor([[-28.6430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.965945902392777
epoch: 12, step: 65
	action: tensor([[ -853.0254,   194.7316,  -505.7272,  -440.3520,   265.7759,  -393.2453,
         -1098.0217]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06562372415707873, distance: 1.1812957286097137 entropy 8.352317888217913
epoch: 12, step: 66
	action: tensor([[   15.4763,  -749.5606,  2267.5001, -2310.7626,  -679.3897, -1777.9995,
         -1054.3332]], dtype=torch.float64)
	q_value: tensor([[-34.2619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.767951216299938
epoch: 12, step: 67
	action: tensor([[ -376.0503,  -662.0683,  -130.3002,  1543.9706,   693.5283,  1231.0759,
         -1925.9396]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.352317888217913
epoch: 12, step: 68
	action: tensor([[ 2330.1647, -2015.5011,   -80.1039,  -326.0812,  -365.8271,   462.3243,
          -965.0480]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.352317888217913
epoch: 12, step: 69
	action: tensor([[-1232.4258,  -938.1064,  -287.3431,  -456.7529,  -126.8872,  1312.4234,
         -1623.1808]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.352317888217913
epoch: 12, step: 70
	action: tensor([[-1373.3127, -2369.2299, -1534.0035,    26.9990,  -372.7564,  1512.6399,
           792.8296]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6559721610987586, distance: 1.4725946184101961 entropy 8.352317888217913
epoch: 12, step: 71
	action: tensor([[-5124.8789,   283.9114, -1456.3934,  1981.7824,  -509.7646,  2201.6303,
          3010.0423]], dtype=torch.float64)
	q_value: tensor([[-25.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36769837392860516, distance: 0.9099527516313937 entropy 8.799877728377284
epoch: 12, step: 72
	action: tensor([[  655.4565, -1475.7265,  1599.7621,  -540.6729,    60.8245,  -244.2544,
          -864.3185]], dtype=torch.float64)
	q_value: tensor([[-26.1857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.744538524262083
epoch: 12, step: 73
	action: tensor([[ -652.4478, -2119.5687,  -562.3909,  -943.3063,   186.1766,  -161.1296,
            65.5558]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9341443971794421, distance: 1.5914798352272415 entropy 8.352317888217913
epoch: 12, step: 74
	action: tensor([[ -810.8687, -1893.9537, -1479.4490,   207.6698,  3319.1420,   453.2226,
         -1182.8716]], dtype=torch.float64)
	q_value: tensor([[-27.7120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5784965914612838, distance: 1.4377339148838635 entropy 8.921490241848892
epoch: 12, step: 75
	action: tensor([[-1648.6569, -1612.8431,  -811.1651,   629.4041, -1594.6926,  -658.3471,
           604.1196]], dtype=torch.float64)
	q_value: tensor([[-25.2164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4134237608263096, distance: 1.3604822726647752 entropy 8.944142261671905
epoch: 12, step: 76
	action: tensor([[-4179.0010, -3465.0068,  1141.4826,  1196.6697,   360.1534,  -209.8451,
          -348.1981]], dtype=torch.float64)
	q_value: tensor([[-23.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17959521160825043, distance: 1.0365035324517706 entropy 8.816622513295865
epoch: 12, step: 77
	action: tensor([[ -514.5198,  -848.4092,   731.6709, -2794.1609, -1567.7489, -1034.3909,
          2272.8920]], dtype=torch.float64)
	q_value: tensor([[-24.9791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6151757666051922, distance: 1.454342140551257 entropy 8.956962091288496
epoch: 12, step: 78
	action: tensor([[-2309.8226, -1006.3698, -3255.1749,  2011.4252,   754.1160,   770.0815,
          2073.2578]], dtype=torch.float64)
	q_value: tensor([[-28.3091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06967232107313515, distance: 1.1835376345241462 entropy 8.986604590482196
epoch: 12, step: 79
	action: tensor([[  212.0687, -1886.9791,   834.8133,   957.5063,   131.1433, -1682.4779,
          -337.8289]], dtype=torch.float64)
	q_value: tensor([[-25.6218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27786825152381356, distance: 0.9724446981201552 entropy 8.865564071761352
epoch: 12, step: 80
	action: tensor([[  658.1903,  -649.6007, -1065.7078,  2572.9160, -1786.8899,   -34.4056,
         -1521.1956]], dtype=torch.float64)
	q_value: tensor([[-21.3388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.580698529180441, distance: 0.7410028633759761 entropy 8.705278909203715
epoch: 12, step: 81
	action: tensor([[ -789.0221, -2575.7670,  -557.2193,  -841.1770,   -50.7016,   352.2575,
           768.7788]], dtype=torch.float64)
	q_value: tensor([[-22.9801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37901483247849743, distance: 1.3438201905784155 entropy 8.97991933842915
epoch: 12, step: 82
	action: tensor([[ -447.7110,  -792.1386, -1612.4506, -1567.7478,   808.7485, -2102.5104,
          4484.4802]], dtype=torch.float64)
	q_value: tensor([[-19.6413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5098574062677943, distance: 1.4061273645473369 entropy 8.574376009547546
epoch: 12, step: 83
	action: tensor([[-1042.5386,  -670.8482,  -103.7063, -1084.4179, -2503.4632, -2407.9760,
           253.7754]], dtype=torch.float64)
	q_value: tensor([[-32.1828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8042757704440582, distance: 1.5371213393976864 entropy 8.98940295315305
epoch: 12, step: 84
	action: tensor([[  491.5577, -2722.6347,  -554.8262,  -228.1250,   709.2119,  1438.2401,
          1713.9090]], dtype=torch.float64)
	q_value: tensor([[-34.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07412992283226094, distance: 1.186001124705703 entropy 9.16966039681006
epoch: 12, step: 85
	action: tensor([[-4036.2062,  1326.1302, -1458.5613, -3238.9672,  1151.3454,   479.8095,
          2509.4004]], dtype=torch.float64)
	q_value: tensor([[-22.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.740633078891424
epoch: 12, step: 86
	action: tensor([[-1066.7472,   933.4459, -1664.6760,  1980.2171,  -557.7016,   232.9268,
          1802.9363]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.352317888217913
epoch: 12, step: 87
	action: tensor([[ -144.6572, -1574.5035,  -193.2161,  1683.4100,   964.3900,  -420.5738,
             3.4582]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4801691405677264, distance: 1.392234418356032 entropy 8.352317888217913
epoch: 12, step: 88
	action: tensor([[-2096.2009,    69.5898,   -98.5271,  4838.4044,  1832.6672, -1076.2746,
          1999.1892]], dtype=torch.float64)
	q_value: tensor([[-23.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033529063867029585, distance: 1.1249962957660529 entropy 8.87023689180529
epoch: 12, step: 89
	action: tensor([[-1743.0644, -1670.6331, -2942.4617,  1524.8232,  -861.5693,  4342.9388,
           -42.1757]], dtype=torch.float64)
	q_value: tensor([[-27.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07308801500046447, distance: 1.1854257736060798 entropy 8.942763043066536
epoch: 12, step: 90
	action: tensor([[-3330.6600,  1948.2206,   143.8494, -2380.2485,   -50.3378,   834.7911,
         -2796.7062]], dtype=torch.float64)
	q_value: tensor([[-25.7362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27210955234583567, distance: 0.976314418806471 entropy 8.850225991208617
epoch: 12, step: 91
	action: tensor([[ -689.4066,   793.4629,  1859.4851,   305.3005,  -121.4420,   617.3832,
         -1135.7621]], dtype=torch.float64)
	q_value: tensor([[-27.7353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08205790977891969, distance: 1.096388155788979 entropy 8.439058095786397
epoch: 12, step: 92
	action: tensor([[-2210.2928,  -126.6032,   -96.4728,  2140.2843,   -19.3421,  1047.3433,
         -2593.9966]], dtype=torch.float64)
	q_value: tensor([[-29.5579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049287551049043365, distance: 1.1157869652926806 entropy 8.83482763773406
epoch: 12, step: 93
	action: tensor([[ -460.2005, -1465.1811,  2799.1840,  -221.2360,   829.4288, -1257.0228,
         -1685.0412]], dtype=torch.float64)
	q_value: tensor([[-28.2459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.439840132602669, distance: 1.7874643080912562 entropy 8.860352327869041
epoch: 12, step: 94
	action: tensor([[  180.8471,  -529.0082, -1918.7420,  1810.3367,  3468.9564,   816.6774,
          1046.8384]], dtype=torch.float64)
	q_value: tensor([[-25.2562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05202782028288433, distance: 1.114177770472279 entropy 8.822001942325844
epoch: 12, step: 95
	action: tensor([[-4315.6215, -1142.2186, -1358.6329,   480.5026,  3466.1832,  1190.9125,
           656.8524]], dtype=torch.float64)
	q_value: tensor([[-21.8678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2747922421602218, distance: 1.2920413093396634 entropy 8.897534027743749
epoch: 12, step: 96
	action: tensor([[-3260.9411,  -236.7122, -1768.1956,  3522.9672,  -620.0604, -2098.6701,
         -2056.8989]], dtype=torch.float64)
	q_value: tensor([[-27.3108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32922871087945627, distance: 0.937225132213314 entropy 9.104189343225434
epoch: 12, step: 97
	action: tensor([[  156.7057,  -192.3032,  -269.7281, -1825.5009,   958.6833,  3241.2806,
           457.5986]], dtype=torch.float64)
	q_value: tensor([[-26.8433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28454157686855197, distance: 1.2969725249975836 entropy 9.073062295865213
epoch: 12, step: 98
	action: tensor([[-3012.2648,  -904.0437,  1696.9022,  -291.5123,  1325.0462,  1859.6550,
          1087.3116]], dtype=torch.float64)
	q_value: tensor([[-23.9417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6560147751328402, distance: 1.472613565828221 entropy 8.801637796892043
epoch: 12, step: 99
	action: tensor([[  156.8545,   519.6585,  -450.7421,  1984.5572, -2111.1763, -4638.9512,
          -329.5867]], dtype=torch.float64)
	q_value: tensor([[-25.4111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.690146321995579
epoch: 12, step: 100
	action: tensor([[-973.3899, -526.7207,  236.6017, -416.7249,  641.9147, -971.2094,
          379.1543]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20358896538847193, distance: 1.0212341464318426 entropy 8.352317888217913
epoch: 12, step: 101
	action: tensor([[-3268.8644, -1302.9313,  1667.0409,   199.8437,  1381.6093, -2563.0292,
           146.1973]], dtype=torch.float64)
	q_value: tensor([[-32.0827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.758398364188088, distance: 1.5174532803211147 entropy 8.921221244693081
epoch: 12, step: 102
	action: tensor([[ 1202.9402, -1721.7815,   -97.2309,  -307.5924, -1958.1230, -2846.9477,
          -544.9752]], dtype=torch.float64)
	q_value: tensor([[-24.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5006094767900918, distance: 0.8086802591175984 entropy 8.851808452316408
epoch: 12, step: 103
	action: tensor([[-1198.6378,  1455.7459,   775.0431,  1210.5388, -2892.9716,  -145.0250,
          1443.0987]], dtype=torch.float64)
	q_value: tensor([[-30.8363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.961799349446252
epoch: 12, step: 104
	action: tensor([[-1136.7552, -1672.8883, -1148.4062,  1185.5717,  -479.1653,  1779.5696,
          -615.8968]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4853321912648021, distance: 1.3946604652306165 entropy 8.352317888217913
epoch: 12, step: 105
	action: tensor([[ 2025.9313, -2343.4721,   348.6455,  -607.8767,   652.2948,  1065.2915,
          -666.4877]], dtype=torch.float64)
	q_value: tensor([[-24.3902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09568800110053144, distance: 1.1978436808560873 entropy 8.834000636881914
epoch: 12, step: 106
	action: tensor([[-1727.7914, -2986.9660,  -929.0519,  -129.7083,  -758.7490, -1868.0703,
          1762.8716]], dtype=torch.float64)
	q_value: tensor([[-20.8156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21786777677687286, distance: 1.2628645233877858 entropy 8.742933930529281
epoch: 12, step: 107
	action: tensor([[-2337.3756,  -894.8467, -1596.8167,   317.0731,   307.7515,  2010.6245,
          2033.6057]], dtype=torch.float64)
	q_value: tensor([[-26.4834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0779177568016638, distance: 1.649570458662513 entropy 8.79783677478533
epoch: 12, step: 108
	action: tensor([[ 1472.5654, -1979.2357, -1888.1069, -5285.2333,  2741.2608,  -242.6579,
          2010.5808]], dtype=torch.float64)
	q_value: tensor([[-21.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3718054515629571, distance: 0.906992665023977 entropy 8.701313985234068
epoch: 12, step: 109
	action: tensor([[-1061.2301,  -170.5540,   706.5109,  -588.2935,   805.5236,  3013.6010,
           426.4181]], dtype=torch.float64)
	q_value: tensor([[-24.2790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.624632923442225, distance: 1.4585936491955702 entropy 8.720195400681932
epoch: 12, step: 110
	action: tensor([[-1238.8207, -1142.1820,  1902.6371,  2394.7831, -1601.4264,  -842.9710,
          1304.5645]], dtype=torch.float64)
	q_value: tensor([[-26.9982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7150175604685134, distance: 1.4986181182184772 entropy 8.883390762213121
epoch: 12, step: 111
	action: tensor([[ -780.2119,  -184.8980,  1039.6498,   -24.1563,  1156.0961, -2805.4531,
           982.7233]], dtype=torch.float64)
	q_value: tensor([[-23.3384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7618149708025292, distance: 1.518926787418453 entropy 8.741829023884575
epoch: 12, step: 112
	action: tensor([[-3185.3232,  -517.5725, -1546.3116, -2168.8929,   331.0898,   392.2107,
         -2424.3938]], dtype=torch.float64)
	q_value: tensor([[-28.6849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3669040119076372, distance: 0.9105241601140018 entropy 8.963373474627257
epoch: 12, step: 113
	action: tensor([[-2610.7369, -3042.5043,  2162.4055,  -558.3880,  -588.6637, -1554.0468,
           905.4192]], dtype=torch.float64)
	q_value: tensor([[-32.2437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03039763171129861, distance: 1.1268173544699334 entropy 8.945805872178818
epoch: 12, step: 114
	action: tensor([[-2908.7155, -1730.1894,  -742.0379,   820.3350,   784.8440,  1086.9621,
          -248.3038]], dtype=torch.float64)
	q_value: tensor([[-27.7615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9448232417695948, distance: 1.5958672459761887 entropy 8.796202067455138
epoch: 12, step: 115
	action: tensor([[  366.4269, -1296.5597,   652.9865,  4639.2577,   -99.8209,  -671.0557,
          -597.8796]], dtype=torch.float64)
	q_value: tensor([[-24.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07096095132442393, distance: 1.102995339538511 entropy 8.78348537734371
epoch: 12, step: 116
	action: tensor([[-4850.3279,  -334.1838,  -668.8518, -4109.5887,  2668.1630,   115.2819,
          -152.7131]], dtype=torch.float64)
	q_value: tensor([[-21.2253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4083175168600821, distance: 1.3580225573890246 entropy 8.722839283733755
epoch: 12, step: 117
	action: tensor([[-3127.0679,  1272.9883,   452.6915, -2428.5852,  -979.9051,  3163.1487,
          2399.5295]], dtype=torch.float64)
	q_value: tensor([[-27.5168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30803945902677266, distance: 0.9519132194990799 entropy 8.92147219799705
epoch: 12, step: 118
	action: tensor([[-3012.0098,  1840.7678,   650.4161, -1276.4441,   527.2303,  2303.5967,
          1808.5122]], dtype=torch.float64)
	q_value: tensor([[-35.6278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6059592817722879, distance: 0.7183353586746782 entropy 8.67391078929092
epoch: 12, step: 119
	action: tensor([[-2195.5627, -2247.5333,   -82.0923,  -236.6307,  -382.0415,  -903.4879,
         -1675.6116]], dtype=torch.float64)
	q_value: tensor([[-41.4040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.83458173935347
epoch: 12, step: 120
	action: tensor([[-633.9105, -866.8941, -406.0408, 1803.5816, -717.6530,  333.3578,
         -855.0016]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.352317888217913
epoch: 12, step: 121
	action: tensor([[-728.0974, -743.0285, 1444.8912,  297.6797,  993.4316, 1074.6229,
          746.5160]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.363019887229159, distance: 1.3360040956433312 entropy 8.352317888217913
epoch: 12, step: 122
	action: tensor([[-3582.8663, -1215.0527, -1107.4946,   684.4547,  2168.1697, -1152.1824,
         -1403.8737]], dtype=torch.float64)
	q_value: tensor([[-21.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6459990593007008, distance: 1.4681535669979833 entropy 8.697344625498888
epoch: 12, step: 123
	action: tensor([[ -884.2287, -5013.6927, -1631.9833, -5570.9100, -2594.5279,  1027.8693,
         -2417.1511]], dtype=torch.float64)
	q_value: tensor([[-24.5533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9939130607130344, distance: 1.6158825921559656 entropy 8.943567957285977
epoch: 12, step: 124
	action: tensor([[ -941.6504, -1651.0688,    96.6833,  1962.9645,  -172.7999, -2563.0785,
          -759.6007]], dtype=torch.float64)
	q_value: tensor([[-28.2165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24235435019019513, distance: 0.996069771111874 entropy 8.979120722059672
epoch: 12, step: 125
	action: tensor([[-4197.3830,  1111.3872, -2694.4303,  6043.3981, -3541.4659,  3378.7186,
          5081.3958]], dtype=torch.float64)
	q_value: tensor([[-30.5689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.030474891480923
epoch: 12, step: 126
	action: tensor([[-1648.9323, -2317.1422,   443.1233,  -187.6013,  1378.9375, -2011.9375,
          1887.6539]], dtype=torch.float64)
	q_value: tensor([[-29.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06782668679775505, distance: 1.1825161438126774 entropy 8.352317888217913
epoch: 12, step: 127
	action: tensor([[ 3201.6232, -3763.8293,  5282.8255,  3025.8820,  -273.8361,  2129.4288,
          3078.4829]], dtype=torch.float64)
	q_value: tensor([[-34.9092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18664110788682242, distance: 1.032043024352928 entropy 9.018710765531003
LOSS epoch 12 actor 300.1606858098487 critic 292.8664175229309
epoch: 13, step: 0
	action: tensor([[-1034.1250, -1649.6399,  -934.8505, -2868.7448,  2647.0911,  2391.8191,
           -81.4545]], dtype=torch.float64)
	q_value: tensor([[-28.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934144804292408, distance: 0.9619202329666589 entropy 8.84862061302262
epoch: 13, step: 1
	action: tensor([[ -651.1252, -2447.5242,  -401.9396,  -836.2127,  -330.9746,  2947.8030,
          1340.8137]], dtype=torch.float64)
	q_value: tensor([[-28.5535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2565456696030205, distance: 0.9866970755001285 entropy 8.83296701880475
epoch: 13, step: 2
	action: tensor([[   67.0340, -3928.0192,  2041.6204,   813.0400, -1472.1861, -4525.4786,
           207.9486]], dtype=torch.float64)
	q_value: tensor([[-33.3977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0011484802331983568, distance: 1.1436869368389928 entropy 9.009107421127897
epoch: 13, step: 3
	action: tensor([[-1889.2916,  -413.0607,  -819.5307,  1073.9665,  1556.1531,  -485.6842,
          1770.3508]], dtype=torch.float64)
	q_value: tensor([[-25.2344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27633360374191085, distance: 1.2928221821550332 entropy 8.749500855265444
epoch: 13, step: 4
	action: tensor([[-1997.5446,  3122.1934, -1610.9535,  3109.1146, -4807.0729, -3008.5620,
          2448.0510]], dtype=torch.float64)
	q_value: tensor([[-31.8495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08021189619788482, distance: 1.1893540921458574 entropy 9.101342798791272
epoch: 13, step: 5
	action: tensor([[-1819.4591, -1202.7481,  -475.6789, -2689.6206, -2501.5860,   840.1049,
         -4168.2124]], dtype=torch.float64)
	q_value: tensor([[-37.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6623683992378369, distance: 1.4754358461359853 entropy 9.0875306211885
epoch: 13, step: 6
	action: tensor([[-1250.6964,  1406.6316, -2203.0886,  2015.3015,  1556.9458,  -258.4110,
         -4797.5098]], dtype=torch.float64)
	q_value: tensor([[-31.7850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25037983138297215, distance: 1.2796101409428366 entropy 8.981159162235999
epoch: 13, step: 7
	action: tensor([[  473.3993, -2984.4486,   149.9575,  2247.6634, -2521.3829,  2959.5179,
          -596.4894]], dtype=torch.float64)
	q_value: tensor([[-34.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008651978188045861, distance: 1.1492840131020576 entropy 9.078464576761288
epoch: 13, step: 8
	action: tensor([[-2356.2100, -1059.2912, -3511.7207,    33.1264,  4033.4660,  2059.9406,
          2176.5503]], dtype=torch.float64)
	q_value: tensor([[-31.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17730714175732454, distance: 1.037947905555 entropy 9.10034190879152
epoch: 13, step: 9
	action: tensor([[-1215.4839, -2881.0256,  1559.3544,  -935.3108, -2522.0452, -1409.3352,
          2779.7799]], dtype=torch.float64)
	q_value: tensor([[-31.7225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7321844300583142, distance: 1.5060998267264922 entropy 8.98901490284259
epoch: 13, step: 10
	action: tensor([[  447.0255, -2386.0730,   930.1851,  1530.5475, -2416.4324,  -264.5332,
             5.8443]], dtype=torch.float64)
	q_value: tensor([[-32.4184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5627221310790947, distance: 0.7567204002628596 entropy 9.043269846281499
epoch: 13, step: 11
	action: tensor([[  219.9207, -1755.7377,  2542.4812,  -204.6980,  -918.4110,  2596.6043,
         -1354.6065]], dtype=torch.float64)
	q_value: tensor([[-25.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010166523906680047, distance: 1.1385123920853213 entropy 8.667603266479079
epoch: 13, step: 12
	action: tensor([[-1649.4074,   766.0776,   -12.6313,  -545.2776,  3822.0799,  2192.5580,
         -1519.3846]], dtype=torch.float64)
	q_value: tensor([[-22.1114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.741260930186845
epoch: 13, step: 13
	action: tensor([[ -143.2058, -1666.2336,  -768.7688,  2007.5100,   869.3941,  1106.2778,
           795.7754]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12565508011551885, distance: 1.0700352598028482 entropy 8.44647563054673
epoch: 13, step: 14
	action: tensor([[ 564.1575, -302.9425,  458.3925, 2471.5140, -336.5492, -563.0840,
          397.6539]], dtype=torch.float64)
	q_value: tensor([[-29.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4287712573290632, distance: 0.8648916796828684 entropy 8.99320726588835
epoch: 13, step: 15
	action: tensor([[ -917.3387, -4046.9358,  1781.6932, -2442.6744,  -168.4783,  2341.3482,
          3380.5315]], dtype=torch.float64)
	q_value: tensor([[-34.9381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4852311839012813, distance: 1.3946130437270532 entropy 8.954580197066788
epoch: 13, step: 16
	action: tensor([[  316.9815, -2019.7864,   919.9967,  3865.9482,   934.2297,   560.6763,
         -1669.6666]], dtype=torch.float64)
	q_value: tensor([[-25.3261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06021820059310945, distance: 1.1093541393528847 entropy 8.674990609885505
epoch: 13, step: 17
	action: tensor([[-4502.1633, -1596.3429,  -659.1167,   188.3806,   314.1392, -3737.6735,
          2370.3815]], dtype=torch.float64)
	q_value: tensor([[-35.3122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23554307957358778, distance: 1.2719956893068058 entropy 9.0390366979753
epoch: 13, step: 18
	action: tensor([[-2785.0580, -2001.8271, -1688.0973,  3745.5971,  2977.7694, -1433.9092,
         -2872.8344]], dtype=torch.float64)
	q_value: tensor([[-29.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21193376423691745, distance: 1.2597841378383214 entropy 8.888562174244406
epoch: 13, step: 19
	action: tensor([[ -489.4701, -4199.0629,   205.0449, -1230.0856,  1931.3404,  1487.8679,
          1040.5259]], dtype=torch.float64)
	q_value: tensor([[-33.0585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8317023692111414, distance: 1.548760083128788 entropy 9.101567859538148
epoch: 13, step: 20
	action: tensor([[-1271.9703,  -736.8644, -1707.2929,   351.5611,  -206.7968,  1176.7290,
         -2277.2684]], dtype=torch.float64)
	q_value: tensor([[-36.3508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16325972577497827, distance: 1.0467718375106057 entropy 9.09336231535343
epoch: 13, step: 21
	action: tensor([[ -988.9065, -5017.4319, -1696.5406,  3626.8939,  1386.3474,  -461.8667,
          2777.9839]], dtype=torch.float64)
	q_value: tensor([[-30.1733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002190285111395185, distance: 1.1430903469292029 entropy 9.056587264520855
epoch: 13, step: 22
	action: tensor([[ 1118.5658,   -24.5223, -1044.7166, -1240.4016,  2458.0353,  2325.1845,
           550.5640]], dtype=torch.float64)
	q_value: tensor([[-29.8811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7397833262505992, distance: 0.583746451858911 entropy 9.036902597446142
epoch: 13, step: 23
	action: tensor([[ -468.6886, -1933.1884,  1480.1494,  -657.1817,  3128.1107,  1161.3249,
           382.4986]], dtype=torch.float64)
	q_value: tensor([[-26.4619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3045095039531758, distance: 1.3070142331997583 entropy 8.828836049582021
epoch: 13, step: 24
	action: tensor([[ 2482.7227, -2945.2985,   819.5000,  2978.6285,  -472.0331, -2927.9722,
          2132.5832]], dtype=torch.float64)
	q_value: tensor([[-38.8945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013374418572556168, distance: 1.15197130637692 entropy 9.009033808429985
epoch: 13, step: 25
	action: tensor([[  370.5239, -1177.6285,   935.3327,  1054.7882, -1586.0011,  1807.9576,
          -302.6800]], dtype=torch.float64)
	q_value: tensor([[-33.0486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005327864484321654, distance: 1.1412917271532284 entropy 8.979149769152844
epoch: 13, step: 26
	action: tensor([[-4173.8177, -3177.4442,  -100.6003, -3562.9683, -2101.5070,  -917.6280,
         -1425.3621]], dtype=torch.float64)
	q_value: tensor([[-26.6808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6269009709997142, distance: 1.4596114194077285 entropy 9.096516147032759
epoch: 13, step: 27
	action: tensor([[ 2285.9557, -1713.6149, -1410.5040,  -622.8979,   628.0234,  -465.0782,
          2817.0844]], dtype=torch.float64)
	q_value: tensor([[-29.1546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49853881163021774, distance: 0.8103550744806348 entropy 8.902501127368364
epoch: 13, step: 28
	action: tensor([[ -119.5839, -3235.6297, -1382.6531,  1557.2806,   147.7072,   366.3819,
          -438.0002]], dtype=torch.float64)
	q_value: tensor([[-29.8295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5341197607371237, distance: 1.4173800814400264 entropy 8.891775931632692
epoch: 13, step: 29
	action: tensor([[-1141.5352,  1797.2124,  -384.6789,  -767.2428,  -663.7376,   548.0457,
          1050.6281]], dtype=torch.float64)
	q_value: tensor([[-25.7084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5487676538698643, distance: 0.7686998658778319 entropy 8.87072584877746
epoch: 13, step: 30
	action: tensor([[-2082.1572, -3910.5532,  -880.9397,  -490.1483,  -750.1229,  3551.7017,
          -875.9909]], dtype=torch.float64)
	q_value: tensor([[-44.2898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16363017937259539, distance: 1.0465400909726068 entropy 8.91821735533342
epoch: 13, step: 31
	action: tensor([[ 1280.4215, -1622.7330,   168.7526,  -741.5365, -1414.5523,   496.8001,
           949.1273]], dtype=torch.float64)
	q_value: tensor([[-40.0499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29938850565328234, distance: 1.3044462950916749 entropy 9.113856224825804
epoch: 13, step: 32
	action: tensor([[ -945.5916,   294.3601,  3600.0254,  -863.1527, -3020.5413, -1129.5008,
          -288.0884]], dtype=torch.float64)
	q_value: tensor([[-25.0750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5644588965844355, distance: 1.4313266974617282 entropy 8.708794056152088
epoch: 13, step: 33
	action: tensor([[-1136.9408,  -321.0268,  2089.6652,   343.1903,  -320.8536,  -802.7258,
         -1382.5420]], dtype=torch.float64)
	q_value: tensor([[-37.9200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7465734058358122, distance: 1.5123423534028542 entropy 8.843032800366075
epoch: 13, step: 34
	action: tensor([[-1154.3279, -3046.1536, -3446.8800,  1624.6415,   -37.5629,  -713.8782,
             8.3270]], dtype=torch.float64)
	q_value: tensor([[-30.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19687498458984054, distance: 1.2519330030113287 entropy 8.845896545048777
epoch: 13, step: 35
	action: tensor([[-1009.2673, -1236.1109,  1741.3694, -1723.6351, -1541.6151,  4695.8476,
          2313.5959]], dtype=torch.float64)
	q_value: tensor([[-32.6369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8109689424022812, distance: 1.5399697658781673 entropy 9.047290241499747
epoch: 13, step: 36
	action: tensor([[-2222.9533, -1317.4713, -1041.3008,  1337.4676,  -806.0200,  -597.5723,
          2071.7460]], dtype=torch.float64)
	q_value: tensor([[-28.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3989084371279028, distance: 0.8872111871065229 entropy 8.870062610001119
epoch: 13, step: 37
	action: tensor([[ 2934.3468, -5512.0333,   775.9731,   786.1308,  1897.7860,  2099.2056,
         -1393.7577]], dtype=torch.float64)
	q_value: tensor([[-28.8425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40543006575769924, distance: 0.8823850989257981 entropy 9.000424964275625
epoch: 13, step: 38
	action: tensor([[  150.5436,   413.9912, -2783.3518,   231.2435,   888.9616,   868.3693,
          -843.2546]], dtype=torch.float64)
	q_value: tensor([[-34.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4904171811676423, distance: 0.8168909443076956 entropy 8.986729993078013
epoch: 13, step: 39
	action: tensor([[-4482.9269, -1679.1950, -4837.5779,  2707.9678,  2698.7801,  1975.4474,
           179.3508]], dtype=torch.float64)
	q_value: tensor([[-31.4240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13017389884760866, distance: 1.2165482261931875 entropy 8.933809702013805
epoch: 13, step: 40
	action: tensor([[-1556.9599, -3582.0731,   -56.9343,  -380.6980,    77.1251,  2769.7784,
          3726.9007]], dtype=torch.float64)
	q_value: tensor([[-30.6800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4685707885305763, distance: 1.3867690355464377 entropy 9.115513437222335
epoch: 13, step: 41
	action: tensor([[-2076.8372,  -640.8753, -3015.0653, -1263.5376, -1558.4808,  2678.0222,
         -1363.0892]], dtype=torch.float64)
	q_value: tensor([[-26.5342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7947853610310283, distance: 1.5330734148443648 entropy 8.751069224023132
epoch: 13, step: 42
	action: tensor([[  -47.1858, -2300.7391,   294.7027,  1583.8315,   698.1285,  2029.2401,
           193.0863]], dtype=torch.float64)
	q_value: tensor([[-28.0971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.744927534924114, distance: 1.5116296129714204 entropy 8.728645116205563
epoch: 13, step: 43
	action: tensor([[-1371.8189,   218.4332, -1954.1212,   632.2577,  1908.2829, -1843.8680,
           965.7478]], dtype=torch.float64)
	q_value: tensor([[-26.2742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5370253690952195, distance: 0.778637454048178 entropy 8.766166327069238
epoch: 13, step: 44
	action: tensor([[-1645.8402, -3932.8622,    30.1851,  2399.3390, -3826.5976, -2353.4911,
          -821.2556]], dtype=torch.float64)
	q_value: tensor([[-30.8746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.897556950955007, distance: 1.576355269398239 entropy 8.992021159044947
epoch: 13, step: 45
	action: tensor([[-3055.4529,   151.3902,  -343.0292, -2057.8296,  -991.9999,  1430.2444,
         -2447.0429]], dtype=torch.float64)
	q_value: tensor([[-31.1037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6792187312594096, distance: 1.4828947523022904 entropy 9.021012502117413
epoch: 13, step: 46
	action: tensor([[-3707.1636,  1916.6971, -1956.7488,  3054.9675,  1301.0072,   329.0849,
          2615.5987]], dtype=torch.float64)
	q_value: tensor([[-42.2964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.040236529771016216, distance: 1.1210856699817602 entropy 9.037175767818002
epoch: 13, step: 47
	action: tensor([[-2180.0990,  -249.5408,  4486.9096,    32.6993,   253.8167,  3396.9584,
          -449.4218]], dtype=torch.float64)
	q_value: tensor([[-39.5502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8286103052890907, distance: 1.5474523139288217 entropy 9.002731715949471
epoch: 13, step: 48
	action: tensor([[-4288.7646, -2645.6004,   832.7743,   885.8520,  -263.4856, -3353.7939,
           320.8376]], dtype=torch.float64)
	q_value: tensor([[-37.3152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30548634467103786, distance: 1.3075034997765478 entropy 9.14818986667177
epoch: 13, step: 49
	action: tensor([[  948.3962, -5166.1229,   369.3741,  2261.8257,   266.7007, -1166.0803,
          1903.7965]], dtype=torch.float64)
	q_value: tensor([[-33.3422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2474551517464314, distance: 0.9927111201303404 entropy 9.081013678054271
epoch: 13, step: 50
	action: tensor([[-3378.6105, -4694.1844,  -314.7563, -2448.7128, -2191.1309, -1044.6823,
          1250.5409]], dtype=torch.float64)
	q_value: tensor([[-31.5237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6292405341804432, distance: 1.46066053868307 entropy 8.939123689237435
epoch: 13, step: 51
	action: tensor([[ -485.5940,  -700.2466, -1327.9353,  2783.4467,  -338.7371, -2615.6132,
          1152.2406]], dtype=torch.float64)
	q_value: tensor([[-27.5993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46813033020656314, distance: 1.3865610579071788 entropy 8.753576459177093
epoch: 13, step: 52
	action: tensor([[-2092.7212, -6062.7013,  -176.6518, -2602.9094, -2946.6125,  -406.7677,
          4462.7303]], dtype=torch.float64)
	q_value: tensor([[-32.9184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01138276925718018, distance: 1.1378127107451323 entropy 9.061001777666918
epoch: 13, step: 53
	action: tensor([[-224.4010,  235.4188, -317.7516, -648.2145, -709.1043, 2727.7813,
         3213.3911]], dtype=torch.float64)
	q_value: tensor([[-28.6934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.797741624192465
epoch: 13, step: 54
	action: tensor([[  462.4371, -1265.0596,  -299.0809,  1297.0074,  -451.5214,    25.9682,
          1000.0363]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6411653466916464, distance: 0.6854943534440486 entropy 8.44647563054673
epoch: 13, step: 55
	action: tensor([[ -972.5600, -1316.2813, -2138.9662,   797.5246,   400.4961,  -544.3108,
           940.0810]], dtype=torch.float64)
	q_value: tensor([[-22.6360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05195744838534111, distance: 1.1736964196240716 entropy 8.526224708684193
epoch: 13, step: 56
	action: tensor([[ -246.5797, -1916.2205,   812.0601,  1222.3831, -3392.0518, -2144.7384,
           908.2991]], dtype=torch.float64)
	q_value: tensor([[-35.3377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26238298387669623, distance: 1.2857373473338836 entropy 9.163687329604015
epoch: 13, step: 57
	action: tensor([[-1980.9093, -4601.2201,  3838.8964,  2782.3098,  -288.1871, -2282.3056,
           169.7340]], dtype=torch.float64)
	q_value: tensor([[-31.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7798422644423988, distance: 1.5266780129082549 entropy 9.047560035664132
epoch: 13, step: 58
	action: tensor([[ 2278.4483,   543.6128,  -253.8539,  1368.9938, -2653.9730, -2017.4734,
         -1582.7194]], dtype=torch.float64)
	q_value: tensor([[-25.9820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45035181163691973, distance: 0.8483969405371845 entropy 8.854142470336175
epoch: 13, step: 59
	action: tensor([[-2063.1558,   149.5647, -2004.4622,  -876.9208,  1838.1380,  3569.7676,
          1896.2642]], dtype=torch.float64)
	q_value: tensor([[-31.5487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15780011322743626, distance: 1.2313272396504629 entropy 9.025305763048754
epoch: 13, step: 60
	action: tensor([[-4144.1844,  1175.9013,   430.5604,  2757.5459,   178.3666,  -201.7835,
         -1438.0779]], dtype=torch.float64)
	q_value: tensor([[-33.1054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24893477322306257, distance: 0.9917347268971293 entropy 8.920904873068297
epoch: 13, step: 61
	action: tensor([[  307.7055, -3218.4010,  -336.3407,  -768.8428,    52.8178,   377.7033,
          4359.4426]], dtype=torch.float64)
	q_value: tensor([[-38.6193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2024629296643744, distance: 1.02195584659359 entropy 9.06057916156696
epoch: 13, step: 62
	action: tensor([[ 570.1639,  484.3024, 2024.8789, 2009.6571,  724.1412, -767.9805,
         -804.0104]], dtype=torch.float64)
	q_value: tensor([[-27.4977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37052706640839306, distance: 0.9079150676999136 entropy 8.907222867572093
epoch: 13, step: 63
	action: tensor([[-3951.9151, -2338.5075,   100.0894,  1017.1981,  3412.7968,  3743.1013,
           431.9337]], dtype=torch.float64)
	q_value: tensor([[-27.1029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48710606748135643, distance: 0.8195406011753098 entropy 8.650970753365183
epoch: 13, step: 64
	action: tensor([[-4451.2359,  -937.6377, -1240.7322,  -274.5610,    42.7084,   555.0348,
          2320.5386]], dtype=torch.float64)
	q_value: tensor([[-26.7867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3967690563208337, distance: 1.3524430793106121 entropy 8.891382779517143
epoch: 13, step: 65
	action: tensor([[ 3243.8343, -6827.1295,  -637.3254,   674.6347,  1552.3932, -1527.5869,
         -5700.7213]], dtype=torch.float64)
	q_value: tensor([[-33.0803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37517542055890996, distance: 1.3419481746432325 entropy 8.989485419629817
epoch: 13, step: 66
	action: tensor([[-1879.5786,  -766.9174,   585.8207, -1870.8521,  -845.7369,   973.8719,
         -1559.7210]], dtype=torch.float64)
	q_value: tensor([[-28.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7319700636793343, distance: 1.50600663019671 entropy 9.070852959967855
epoch: 13, step: 67
	action: tensor([[-5163.3574,  2013.0332,  1323.5500,  1099.7792,   601.3955,  2687.1279,
           458.1227]], dtype=torch.float64)
	q_value: tensor([[-29.6506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2802003573903198, distance: 0.9708731857620622 entropy 8.97494591197542
epoch: 13, step: 68
	action: tensor([[-2199.0416,  1692.1885,  1043.7162,  -660.4600,   600.2680,  3312.3881,
          -827.0673]], dtype=torch.float64)
	q_value: tensor([[-40.9234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13634958221757754, distance: 1.0634710868155302 entropy 9.1030942009932
epoch: 13, step: 69
	action: tensor([[-1975.5277, -2394.1717,  3281.5259,   694.3498, -1431.6161,  2116.8798,
          -394.5663]], dtype=torch.float64)
	q_value: tensor([[-37.4497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17995855232656366, distance: 1.0362739837886283 entropy 8.975278250704024
epoch: 13, step: 70
	action: tensor([[ 1939.3629, -2438.0406,  -159.3870,   154.4835, -2399.1392,  1998.7037,
          2716.6698]], dtype=torch.float64)
	q_value: tensor([[-30.4650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36388088423395737, distance: 0.9126955153306955 entropy 8.929971505263165
epoch: 13, step: 71
	action: tensor([[-2773.6581, -2148.1557,   -90.9980,   879.5475, -2191.3577,  1113.9172,
          1034.9534]], dtype=torch.float64)
	q_value: tensor([[-33.3156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.352914781285083, distance: 1.3310424738252407 entropy 9.058515631626824
epoch: 13, step: 72
	action: tensor([[-2467.8997,  -635.7523, -1227.1204,  2120.8499,  2755.4797,  2127.4723,
         -1443.3872]], dtype=torch.float64)
	q_value: tensor([[-34.7493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6548004429571368, distance: 1.4720735434789014 entropy 9.162991047540899
epoch: 13, step: 73
	action: tensor([[  402.3725,   814.6452, -3612.1127,  2136.4827,   228.2166, -1734.0117,
         -3592.6359]], dtype=torch.float64)
	q_value: tensor([[-39.9849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843833441989245, distance: 0.9680480474487596 entropy 9.277006619885181
epoch: 13, step: 74
	action: tensor([[  338.4072, -2323.7049,   899.7096,  -840.3170,   338.3578,  2587.2838,
         -1368.7989]], dtype=torch.float64)
	q_value: tensor([[-30.6771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3358855726611405, distance: 1.3226389959551161 entropy 8.83715590485802
epoch: 13, step: 75
	action: tensor([[ -916.1843, -1672.1904,  1893.3222,  1024.3873,  -526.0985,   568.0497,
          1179.4104]], dtype=torch.float64)
	q_value: tensor([[-21.8320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05222422280765637, distance: 1.173845233787682 entropy 8.565043628797534
epoch: 13, step: 76
	action: tensor([[-3415.3214, -2930.4115,   344.4599, -1900.9329,  -362.2294, -2048.8202,
         -2520.6798]], dtype=torch.float64)
	q_value: tensor([[-28.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5131213775782459, distance: 1.407646409205771 entropy 8.909691079106093
epoch: 13, step: 77
	action: tensor([[-2881.0183, -1783.2626,  -321.6493,  2930.2783,  2798.8933, -3407.5996,
          1771.3846]], dtype=torch.float64)
	q_value: tensor([[-37.3890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08942084606125478, distance: 1.0919821647185461 entropy 9.069179467480126
epoch: 13, step: 78
	action: tensor([[  -76.9269, -1185.0261,  1208.3597,   297.3886,   -20.3986,  1524.1143,
          1065.3855]], dtype=torch.float64)
	q_value: tensor([[-29.9676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2633093714021605, distance: 0.982198498527713 entropy 9.025790064553183
epoch: 13, step: 79
	action: tensor([[  304.6878,  -924.0107, -1720.9968,  2186.9428, -2826.5377,  2735.6747,
         -2442.4293]], dtype=torch.float64)
	q_value: tensor([[-33.1971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11946173828498963, distance: 1.0738183205619378 entropy 9.220116781153072
epoch: 13, step: 80
	action: tensor([[-1007.4366,  1194.6653,   160.9314,   414.9719,  -788.9400,  -130.2211,
          1512.4406]], dtype=torch.float64)
	q_value: tensor([[-31.6854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23589301527726647, distance: 1.2721758065368232 entropy 8.904806541122834
epoch: 13, step: 81
	action: tensor([[-2073.1816, -4381.3101, -3730.5085,  5243.3968,  1611.5318,  -414.1962,
          -283.3120]], dtype=torch.float64)
	q_value: tensor([[-40.1471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.152236986157397, distance: 1.0536440667338292 entropy 9.181281033122646
epoch: 13, step: 82
	action: tensor([[-3446.3342, -4133.9448, -5721.6717,   874.8876, -1062.0626,  -891.5169,
          1058.8345]], dtype=torch.float64)
	q_value: tensor([[-31.9690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26235815677846963, distance: 1.285724704069212 entropy 9.080074380769837
epoch: 13, step: 83
	action: tensor([[  -51.3180,   614.2976, -2766.6188,  1501.5520,  1836.7403, -5542.4962,
          -317.9483]], dtype=torch.float64)
	q_value: tensor([[-32.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20425430699719427, distance: 1.0208074750830716 entropy 9.085898036833724
epoch: 13, step: 84
	action: tensor([[-1375.8801, -2326.1790, -4604.4185,   595.9866,  2737.4989,  -579.4508,
          1417.6434]], dtype=torch.float64)
	q_value: tensor([[-38.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12594664717643633, distance: 1.2142709335653181 entropy 9.068085547377207
epoch: 13, step: 85
	action: tensor([[-5549.9982,    83.7312, -2309.1126,  7828.0863, -4082.5498,   362.4142,
            44.9884]], dtype=torch.float64)
	q_value: tensor([[-36.2911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.712406644691798, distance: 1.4974769473942735 entropy 9.223418537843463
epoch: 13, step: 86
	action: tensor([[-1530.2818, -1079.5394,  1980.6922,  2425.3000,  2411.0959,  -961.4762,
         -2458.5118]], dtype=torch.float64)
	q_value: tensor([[-34.7230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49532424288482224, distance: 0.8129482769058897 entropy 9.07603270378924
epoch: 13, step: 87
	action: tensor([[ 1157.7887, -1827.4475,  1097.0093,  -224.7809,  1690.7352, -1362.5247,
          2245.4580]], dtype=torch.float64)
	q_value: tensor([[-32.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21987472916133177, distance: 1.0107386343848388 entropy 9.108531380759269
epoch: 13, step: 88
	action: tensor([[-1013.4312, -1941.4764, -2331.6871,  1118.6505,  1413.3842,  4704.5478,
          2464.7661]], dtype=torch.float64)
	q_value: tensor([[-32.3299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12740556441843964, distance: 1.0689635898452068 entropy 8.987732727326042
epoch: 13, step: 89
	action: tensor([[-2880.8783, -3763.0974, -1496.7778,  2602.9954,  1376.1465,   502.6163,
         -1723.6386]], dtype=torch.float64)
	q_value: tensor([[-29.1083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13445462206852044, distance: 1.064637143750935 entropy 8.888865345462161
epoch: 13, step: 90
	action: tensor([[  -55.5369, -1980.0023,  6110.6384, -1441.3143,  1350.0489,  2334.3322,
          4535.0517]], dtype=torch.float64)
	q_value: tensor([[-31.4462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37375675602030745, distance: 1.3412558027799875 entropy 9.111212298537842
epoch: 13, step: 91
	action: tensor([[-3763.3836, -2198.1164,  -534.5156,  1471.4015,  -410.0945,  1075.2786,
          1478.7255]], dtype=torch.float64)
	q_value: tensor([[-34.0520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5372019541235546, distance: 1.4188031931072502 entropy 9.003888795717405
epoch: 13, step: 92
	action: tensor([[ -916.5376,  -857.4393,  1493.0365,  -443.1346, -2465.6247, -1148.0436,
         -1401.3900]], dtype=torch.float64)
	q_value: tensor([[-30.0207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.93766850526781, distance: 1.592929053426597 entropy 9.084750969147297
epoch: 13, step: 93
	action: tensor([[-1990.6903,  -922.0896,  -584.1911, -1268.1970, -4720.8125, -1043.7353,
          2596.2662]], dtype=torch.float64)
	q_value: tensor([[-37.3418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08110436553938571, distance: 1.189845311930199 entropy 9.09950845374799
epoch: 13, step: 94
	action: tensor([[-1141.9317,   843.9637,  3481.9302,  4108.9977,  1164.1875,  2222.6590,
           355.2549]], dtype=torch.float64)
	q_value: tensor([[-36.4218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16415368493850202, distance: 1.0462125114095127 entropy 9.004671366432591
epoch: 13, step: 95
	action: tensor([[  692.3595,  -382.4450, -3037.3096,  1215.6520, -2665.6004,  -937.4353,
           315.1825]], dtype=torch.float64)
	q_value: tensor([[-35.3493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.95119090163538
epoch: 13, step: 96
	action: tensor([[  927.4622,   571.6751,   176.6445,  3422.3558,   957.4027, -1744.3960,
           967.7322]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4309733327677294, distance: 0.8632229998388165 entropy 8.44647563054673
epoch: 13, step: 97
	action: tensor([[-3004.6996, -1414.0822, -1799.7659,   634.1724, -2506.3477, -1005.7711,
         -2636.4753]], dtype=torch.float64)
	q_value: tensor([[-27.3348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.718586510455031
epoch: 13, step: 98
	action: tensor([[ -759.5125, -2238.8257,  1243.6219,  1658.2775, -1143.4793,  2053.3839,
            53.7495]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008251324416362005, distance: 1.1490557328231492 entropy 8.44647563054673
epoch: 13, step: 99
	action: tensor([[-1583.3251,  -571.2699, -2836.7934, -1348.5368,  1409.7533,  1061.1370,
            -9.9076]], dtype=torch.float64)
	q_value: tensor([[-28.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5702620695346623, distance: 1.4339789077235727 entropy 8.942963498630856
epoch: 13, step: 100
	action: tensor([[  701.5849,    35.4734,  2431.8968,    43.7158, -1686.5252,  2140.5277,
          2530.3590]], dtype=torch.float64)
	q_value: tensor([[-39.1367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.68050382380873, distance: 0.6468290637238246 entropy 9.180909094179688
epoch: 13, step: 101
	action: tensor([[-218.9921,  743.9413, 2023.7268, 2785.1916, 1556.4139, 2196.5956,
         2203.9046]], dtype=torch.float64)
	q_value: tensor([[-26.3780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.843372904023216
epoch: 13, step: 102
	action: tensor([[-849.1624, 1518.5073,  145.7849,  753.4570, -196.4649, -599.0232,
          -32.2532]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.44647563054673
epoch: 13, step: 103
	action: tensor([[ 1223.4860,   673.3927,   260.8971,   909.5802,  -172.4954, -1862.1932,
          1999.3974]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.44647563054673
epoch: 13, step: 104
	action: tensor([[ -515.6901,  -242.0768, -1571.1678,  -170.1832,   367.5012, -1497.5259,
           542.2517]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5788197438410834, distance: 1.4378810749608248 entropy 8.44647563054673
epoch: 13, step: 105
	action: tensor([[-2167.7227,    92.6937,  1205.5482,  -422.0022, -3300.0134, -2497.5481,
         -2586.1067]], dtype=torch.float64)
	q_value: tensor([[-34.0713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08678347562944255, distance: 1.0935624112681182 entropy 8.95790402722017
epoch: 13, step: 106
	action: tensor([[-1024.6339,  -423.3286, -3649.8366,   826.7455,   440.1447,  3016.0360,
          2945.1311]], dtype=torch.float64)
	q_value: tensor([[-38.4906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5009068455037629, distance: 1.4019533491663079 entropy 9.000654154318152
epoch: 13, step: 107
	action: tensor([[-1145.2689,  -417.2755,  -879.7858,   881.8217,   575.6733,  -342.5421,
         -2290.1383]], dtype=torch.float64)
	q_value: tensor([[-23.2578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12692979638392377, distance: 1.0692549677068361 entropy 8.714984029478059
epoch: 13, step: 108
	action: tensor([[ -988.5120,  4668.0807,  3419.9465,  1255.3260,  6000.4842, -1185.0062,
         -2632.1130]], dtype=torch.float64)
	q_value: tensor([[-32.5648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4791031798784603, distance: 0.8259096612419099 entropy 9.158333681708655
epoch: 13, step: 109
	action: tensor([[  296.4591,  -245.3685,  2977.5149,    46.0121, -1466.3182,  -262.9810,
          2508.7491]], dtype=torch.float64)
	q_value: tensor([[-31.6281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.929996147670822
epoch: 13, step: 110
	action: tensor([[ -159.2157,  -392.5308, -1210.2226, -1773.5828,   905.1131,  -169.9338,
         -2238.5996]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03611401112573831, distance: 1.1648244192675847 entropy 8.44647563054673
epoch: 13, step: 111
	action: tensor([[-5441.9770, -1209.2703, -3516.0829,  1735.4754,  1207.4530, -2581.2959,
          2043.9889]], dtype=torch.float64)
	q_value: tensor([[-32.1093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6669629594205708, distance: 1.4774733912469071 entropy 8.978596706537164
epoch: 13, step: 112
	action: tensor([[-3793.9380, -2307.4695, -3169.2456,  1127.9573,  -493.1490,    89.6523,
          -762.0259]], dtype=torch.float64)
	q_value: tensor([[-32.4568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12381827736081097, distance: 1.2131227263690754 entropy 9.084160127284635
epoch: 13, step: 113
	action: tensor([[-5884.6189,  -926.9902, -2502.3508,   781.5565,  -506.1887,  -288.9149,
         -1963.5827]], dtype=torch.float64)
	q_value: tensor([[-33.1586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2072648706171073, distance: 1.0188746225905798 entropy 9.24543234792275
epoch: 13, step: 114
	action: tensor([[ -198.9677, -1915.3897,  -871.1928,  -390.6369, -1939.1991,   290.5690,
          3792.1625]], dtype=torch.float64)
	q_value: tensor([[-37.6760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1907389950600096, distance: 1.6937605472851798 entropy 9.177267367114267
epoch: 13, step: 115
	action: tensor([[-3166.4160,  1052.5617, -1010.0675,   457.1354,   313.7608,  2194.9913,
          3163.9832]], dtype=torch.float64)
	q_value: tensor([[-33.6772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22533903728128557, distance: 1.0071926060388645 entropy 9.159244196307014
epoch: 13, step: 116
	action: tensor([[-1733.7223,   174.9956,  -986.2298,  -438.5449,   646.6532,  4902.4705,
         -3373.3618]], dtype=torch.float64)
	q_value: tensor([[-33.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42218588356252273, distance: 0.8698628173897439 entropy 9.011701108655755
epoch: 13, step: 117
	action: tensor([[1323.4133,  185.5561,  241.8287, -755.6555, 1065.3530,  292.4905,
         -427.4207]], dtype=torch.float64)
	q_value: tensor([[-30.0103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3562497064289811, distance: 0.9181537520732386 entropy 8.614931440174498
epoch: 13, step: 118
	action: tensor([[-5842.3015, -1571.5589,  1678.8088,   398.2411,  1828.3666,  -439.7908,
          2135.2433]], dtype=torch.float64)
	q_value: tensor([[-31.7263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.917785093456718
epoch: 13, step: 119
	action: tensor([[  -79.3071, -1117.5308, -2123.7275,  -523.9214,  2399.8971, -1702.4151,
          -672.1548]], dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1595269708130772, distance: 1.0491040992969818 entropy 8.44647563054673
epoch: 13, step: 120
	action: tensor([[-1276.1547,   784.3623,  1276.8335,  -100.5126,   -23.7481,  4557.4646,
          -361.9812]], dtype=torch.float64)
	q_value: tensor([[-32.0634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28911712522261723, distance: 1.2992803854198096 entropy 8.821079836914157
epoch: 13, step: 121
	action: tensor([[  965.7413,  -627.8501, -1135.4217, -2209.0191, -1942.9465,   784.7082,
          3649.5921]], dtype=torch.float64)
	q_value: tensor([[-36.2619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.713058862624794, distance: 0.6129895924571623 entropy 8.849615104904432
epoch: 13, step: 122
	action: tensor([[-3409.8994, -3220.2362,   523.3228,  1430.4505,  -185.6651, -2252.6636,
          -276.8001]], dtype=torch.float64)
	q_value: tensor([[-24.8558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05469368357756865, distance: 1.1752218728680328 entropy 8.77824863353209
epoch: 13, step: 123
	action: tensor([[-1586.0016, -5680.1348,  2213.9727,   524.4275,   344.1282, -2466.0286,
          1013.5610]], dtype=torch.float64)
	q_value: tensor([[-29.0197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13636977940559514, distance: 1.2198783707939318 entropy 8.930587678094874
epoch: 13, step: 124
	action: tensor([[-2506.5789, -1173.3903, -2842.7212,  1683.5272, -1471.3959, -3930.0822,
         -1303.9077]], dtype=torch.float64)
	q_value: tensor([[-31.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5564265988416455, distance: 1.4276475860769846 entropy 9.054227340567842
epoch: 13, step: 125
	action: tensor([[-1268.7020, -5121.9055,  2748.0639,  -510.8660, -1064.0099,  -538.2738,
          4297.4680]], dtype=torch.float64)
	q_value: tensor([[-34.4362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5858361558199652, distance: 1.441072567225059 entropy 9.244615088398062
epoch: 13, step: 126
	action: tensor([[ 2.8826e+03, -1.1769e+03,  2.9033e+02, -1.5097e+00, -3.1843e+02,
         -3.7006e+03,  1.6760e+03]], dtype=torch.float64)
	q_value: tensor([[-32.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2808703129695507, distance: 0.9704212590802073 entropy 8.994743054221047
epoch: 13, step: 127
	action: tensor([[-3063.6927, -3539.7467, -2142.4120,    45.0551,   862.6564,  -910.4018,
          1851.4017]], dtype=torch.float64)
	q_value: tensor([[-28.7586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9128050051264072, distance: 1.5826760959743613 entropy 8.80524615290339
LOSS epoch 13 actor 459.0981340117147 critic 109.78587275914583
epoch: 14, step: 0
	action: tensor([[ 1884.2432,  1806.9544, -1529.5553,  1965.5158,  1231.4067,  -600.0958,
         -4927.3030]], dtype=torch.float64)
	q_value: tensor([[-29.5441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5355937445451553, distance: 0.7798403883020125 entropy 8.94407854965562
epoch: 14, step: 1
	action: tensor([[ 1595.7648, -2601.6868, -1717.4156,  2004.6704, -2131.6187,  -522.4986,
          1026.6351]], dtype=torch.float64)
	q_value: tensor([[-28.2032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.925683617530913
epoch: 14, step: 2
	action: tensor([[ 2939.7479,  2729.8056,   544.5456,    64.5563, -2191.1679,  1471.6087,
           353.1451]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.53719386307136
epoch: 14, step: 3
	action: tensor([[-2618.9746,  -661.7162,  1231.3283,   741.8564,   323.0814,  -153.1350,
         -1083.9540]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.53719386307136
epoch: 14, step: 4
	action: tensor([[-262.8821, 1600.5373, -129.0836,  771.1504,  579.9220, 1790.3683,
         -736.2032]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29163070205170916, distance: 0.9631336536428302 entropy 8.53719386307136
epoch: 14, step: 5
	action: tensor([[ -986.1966, -1281.1065,  -837.8166,  2666.5104, -1178.6339,  2490.7396,
          1876.0754]], dtype=torch.float64)
	q_value: tensor([[-39.0059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.84157774836771
epoch: 14, step: 6
	action: tensor([[-3777.2188,  -757.9722, -1151.8144,  -513.6055,   126.9941,  -712.9568,
           -40.8517]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19893369025765562, distance: 1.0242145162444285 entropy 8.53719386307136
epoch: 14, step: 7
	action: tensor([[-5026.7263, -2814.4363, -2946.7290,   172.9977,  -890.1365,  2333.1303,
          1531.0077]], dtype=torch.float64)
	q_value: tensor([[-39.5906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27694339536209367, distance: 1.2931309799453772 entropy 9.157914001614786
epoch: 14, step: 8
	action: tensor([[-4999.4040, -3880.8071, -2972.0430,  -252.1831, -3608.5280,   -35.3215,
          1075.0040]], dtype=torch.float64)
	q_value: tensor([[-31.4673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11976827180802263, distance: 1.210934833377253 entropy 9.082875206302075
epoch: 14, step: 9
	action: tensor([[-2031.9342,  1126.7700,  -580.8662,  1908.4674,  2526.6093, -2363.8922,
          1999.0313]], dtype=torch.float64)
	q_value: tensor([[-32.4692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26039059676053533, distance: 1.2847223233205702 entropy 8.984339911675097
epoch: 14, step: 10
	action: tensor([[ -922.6446,   717.3987, -1450.8325,    86.5453,  -917.2357,  1248.1516,
          4315.9518]], dtype=torch.float64)
	q_value: tensor([[-34.6273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35375261285002924, distance: 1.3314545532904092 entropy 8.825637321373533
epoch: 14, step: 11
	action: tensor([[ 154.4939, 1142.4354, 1546.5196, -259.7488, -601.6638, -488.4414,
         1216.8611]], dtype=torch.float64)
	q_value: tensor([[-27.1951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039811797190253295, distance: 1.1213337045008327 entropy 8.574216429578835
epoch: 14, step: 12
	action: tensor([[  401.8564, -3516.7507,   628.9472,   796.9272,   679.9595, -2801.0465,
          -313.9016]], dtype=torch.float64)
	q_value: tensor([[-28.2115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.707878120807807
epoch: 14, step: 13
	action: tensor([[   32.7278,  -636.5113, -1077.5487,  -728.5060,   649.5667,   -23.4897,
          1883.1393]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.53719386307136
epoch: 14, step: 14
	action: tensor([[  772.4396, -1311.1655,  1377.4280,   789.0042, -2252.6651, -1970.0398,
           260.2739]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48855517772354495, distance: 0.8183820334445779 entropy 8.53719386307136
epoch: 14, step: 15
	action: tensor([[-2283.2386, -6244.4304, -2371.4737,   808.0795,  -999.1438,  1858.3973,
          1393.5160]], dtype=torch.float64)
	q_value: tensor([[-32.9489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12612569735688695, distance: 1.0697472471366416 entropy 9.04332255368193
epoch: 14, step: 16
	action: tensor([[  435.3031, -2787.4336,  -897.6837,  4321.0605,   714.2932,  1512.6974,
           657.8270]], dtype=torch.float64)
	q_value: tensor([[-31.4559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22965746122821196, distance: 1.0043813353725473 entropy 9.162459028201011
epoch: 14, step: 17
	action: tensor([[-589.8933,   75.1111,  -29.9544, 1312.8701, -411.1464, 2433.8485,
         1841.1624]], dtype=torch.float64)
	q_value: tensor([[-27.1934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.08626412296917
epoch: 14, step: 18
	action: tensor([[-1252.3361, -1928.5476, -2276.3130,   667.3751,  -320.5403,  3039.4664,
           182.7310]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25543888882730315, distance: 0.9874312505366272 entropy 8.53719386307136
epoch: 14, step: 19
	action: tensor([[-2156.5186,  1766.3782,  1348.9167, -2499.8198,  -440.0022, -1151.7377,
         -1415.8443]], dtype=torch.float64)
	q_value: tensor([[-28.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3926880964378934, distance: 1.3504659080470185 entropy 8.965026512936658
epoch: 14, step: 20
	action: tensor([[ -722.5676,   880.6652,  1202.0775, -1207.7789,  1177.7542, -1157.1108,
          -441.7319]], dtype=torch.float64)
	q_value: tensor([[-33.6751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.55100090013809
epoch: 14, step: 21
	action: tensor([[  305.0279,  1214.2836, -1827.4950,  -533.5372,    17.4810, -1309.6490,
          2247.8782]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.53719386307136
epoch: 14, step: 22
	action: tensor([[-1317.0956, -1727.4891,  -818.4386,  1825.6229,  -516.8973,  -548.7095,
           846.0410]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5453348362303789, distance: 1.4225514764754021 entropy 8.53719386307136
epoch: 14, step: 23
	action: tensor([[ 2583.8582, -2255.1080,   -77.3580,   751.6298,  1563.3657,   583.7568,
          1908.0575]], dtype=torch.float64)
	q_value: tensor([[-31.6454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03827642184862179, distance: 1.1660393029258407 entropy 9.176577295428459
epoch: 14, step: 24
	action: tensor([[-2593.9371, -2237.2025, -2724.4717,   988.5194, -4589.7509,  1217.7188,
           103.0927]], dtype=torch.float64)
	q_value: tensor([[-25.2707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32447225760940457, distance: 1.3169768055085225 entropy 9.091675233026944
epoch: 14, step: 25
	action: tensor([[-2231.8141, -3498.3494,  1219.0101,   386.2920,  -175.2754,   616.2741,
         -3601.9796]], dtype=torch.float64)
	q_value: tensor([[-37.2288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34703364453709395, distance: 0.9247026357563322 entropy 9.34552137313658
epoch: 14, step: 26
	action: tensor([[  67.5298, -487.2046, 2591.7932, 2939.4051, 2494.9128, 3561.7363,
         2017.9705]], dtype=torch.float64)
	q_value: tensor([[-29.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012664542174543714, distance: 1.1370748667647286 entropy 9.125353371529524
epoch: 14, step: 27
	action: tensor([[-4373.0979, -2411.5185,  1831.1796,   383.5255,  2001.8755, -1329.5413,
         -1129.1080]], dtype=torch.float64)
	q_value: tensor([[-34.3634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04157583586969649, distance: 1.167890541558127 entropy 9.149525713579875
epoch: 14, step: 28
	action: tensor([[-5829.3805,  3203.9518, -1196.7225,  1028.3078,  2062.9281, -4972.2013,
          3191.8285]], dtype=torch.float64)
	q_value: tensor([[-32.9039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2535037200808964, distance: 0.9887136208285623 entropy 9.218627381636837
epoch: 14, step: 29
	action: tensor([[ -695.8527, -2328.2421, -3011.7085,   571.9207,  1421.8320,  2250.1933,
          3770.9850]], dtype=torch.float64)
	q_value: tensor([[-38.4320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7704375790061955, distance: 1.5226391877111856 entropy 9.13834444061695
epoch: 14, step: 30
	action: tensor([[-1607.2619, -2178.4574,    70.8593,  2369.0512,  -183.7613,  2005.0773,
          -611.2669]], dtype=torch.float64)
	q_value: tensor([[-32.6689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007591549875197057, distance: 1.1486797145802439 entropy 9.088779866686965
epoch: 14, step: 31
	action: tensor([[-2197.8324,   340.0066,  5447.2689,  3902.1840, -1205.7645,   698.2211,
          7299.6767]], dtype=torch.float64)
	q_value: tensor([[-33.9614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2069067814273191, distance: 1.0191047163395426 entropy 9.205048616886064
epoch: 14, step: 32
	action: tensor([[-1646.3459, -4746.5441,    33.8588, -2362.7924,  1249.9374, -1072.7570,
          -106.8277]], dtype=torch.float64)
	q_value: tensor([[-33.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5443218642739962, distance: 1.4220851565097765 entropy 9.01596395021613
epoch: 14, step: 33
	action: tensor([[  -21.9006,   971.1761,  1514.8194, -1016.5292, -1315.3368,  2199.2616,
           406.5859]], dtype=torch.float64)
	q_value: tensor([[-35.9131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30076554285540824, distance: 0.9569034139329777 entropy 9.067455134419747
epoch: 14, step: 34
	action: tensor([[-1858.4603, -1487.5670,  2002.1578,    99.4145,  -823.9845,  -842.4981,
            91.6175]], dtype=torch.float64)
	q_value: tensor([[-42.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1301782623767926, distance: 1.670185340751679 entropy 9.041002635765485
epoch: 14, step: 35
	action: tensor([[-3658.3545,   -12.2293,   -38.1445,   990.1643,  1526.6203,  2066.4866,
          2214.8250]], dtype=torch.float64)
	q_value: tensor([[-32.1586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3233998186004634, distance: 1.316443512601876 entropy 9.058333188487525
epoch: 14, step: 36
	action: tensor([[ -988.2271,  2163.2082,   452.9685, -3677.0772,   373.6881,   -52.5163,
         -1576.4736]], dtype=torch.float64)
	q_value: tensor([[-32.0198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.698823277523204, distance: 1.491525885048856 entropy 9.122099884894675
epoch: 14, step: 37
	action: tensor([[  891.0391, -2705.2762,  -169.5850,  1784.9178,  3965.7780,   -92.1813,
         -3664.6503]], dtype=torch.float64)
	q_value: tensor([[-37.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18892493760333973, distance: 1.0305930694562857 entropy 8.943759447234408
epoch: 14, step: 38
	action: tensor([[   71.0267,  -538.5675, -1119.1505,  -738.1585,   410.0276,  -530.2270,
          -532.8504]], dtype=torch.float64)
	q_value: tensor([[-23.3988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.271413463496474, distance: 1.2903279250100677 entropy 8.622576393995386
epoch: 14, step: 39
	action: tensor([[ 1327.1451, -4698.6728, -1040.4093,   642.2244,   909.5134, -2555.6196,
         -1837.2464]], dtype=torch.float64)
	q_value: tensor([[-26.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3809407310722963, distance: 0.9003737161473742 entropy 8.781058106083195
epoch: 14, step: 40
	action: tensor([[ 4156.5841,   126.8039,   968.7769,  3908.4853, -3797.5337, -1022.1190,
          1856.5753]], dtype=torch.float64)
	q_value: tensor([[-32.7780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4848206710563838, distance: 0.8213644610773971 entropy 9.179813307242338
epoch: 14, step: 41
	action: tensor([[   83.1682,  -600.3580, -3732.7644,  2127.8430, -2363.1020,  3516.9981,
            12.4676]], dtype=torch.float64)
	q_value: tensor([[-36.9490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5824868899733877, distance: 0.7394209508532482 entropy 9.252606787588759
epoch: 14, step: 42
	action: tensor([[ 1140.2574, -1167.6280,  -354.8880,  1006.3561,   771.8340,  -329.7376,
          -355.4279]], dtype=torch.float64)
	q_value: tensor([[-27.4081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.620569311871106, distance: 1.4567683541052316 entropy 8.952920668032204
epoch: 14, step: 43
	action: tensor([[-2478.9074, -2478.3171,   150.9633,  -637.1553,  1523.7225,  1170.1842,
          -299.7532]], dtype=torch.float64)
	q_value: tensor([[-33.2399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0606117941930253, distance: 1.6426868626322346 entropy 9.095206361098047
epoch: 14, step: 44
	action: tensor([[-1168.0998, -1763.2584, -2610.3853,  1521.9262,  -668.7693,   534.7763,
          1285.1349]], dtype=torch.float64)
	q_value: tensor([[-28.6988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4723940330236187, distance: 1.388573004022084 entropy 9.019736941220108
epoch: 14, step: 45
	action: tensor([[-1913.0942, -6932.4118, -1315.3073,  1979.7607,  5715.4559,  5660.7894,
          3112.3058]], dtype=torch.float64)
	q_value: tensor([[-36.3511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18558909774486487, distance: 1.2460164954450526 entropy 9.349095194734577
epoch: 14, step: 46
	action: tensor([[-3801.6646, -4277.0257, -3634.0735,   838.1806,  -661.2430,  -922.7970,
          1228.0579]], dtype=torch.float64)
	q_value: tensor([[-38.2398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5253643536019403, distance: 1.4133297143585521 entropy 9.313945688741208
epoch: 14, step: 47
	action: tensor([[ 1876.8094, -4177.6722,  -437.4413, -2811.6568, -1605.5423,    54.7803,
          3403.9481]], dtype=torch.float64)
	q_value: tensor([[-36.0825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14478495149994064, distance: 1.2243868291643114 entropy 9.296500817718433
epoch: 14, step: 48
	action: tensor([[-1037.7657, -5434.4216, -2200.5268,  -588.0487, -3791.1457,  -136.0709,
          4392.3155]], dtype=torch.float64)
	q_value: tensor([[-32.9332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6013866636493777, distance: 1.4481208180509895 entropy 9.088389362631004
epoch: 14, step: 49
	action: tensor([[ -362.1208, -1872.3692, -1325.8102,   959.9827,   308.4996,  2996.5877,
          2793.9196]], dtype=torch.float64)
	q_value: tensor([[-39.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9565229770737651, distance: 1.6006602850956848 entropy 9.163518335065051
epoch: 14, step: 50
	action: tensor([[-1180.3706,  -525.8580,   521.4990,    91.7959,   388.3917,  1271.3381,
         -2235.0120]], dtype=torch.float64)
	q_value: tensor([[-27.0297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3002576414522411, distance: 1.3048824815928586 entropy 8.848900182332489
epoch: 14, step: 51
	action: tensor([[-3110.5682, -3219.8652,  1730.3630,  3692.0693, -1055.8389,  -511.3550,
          2423.9870]], dtype=torch.float64)
	q_value: tensor([[-30.1251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2421730978021761, distance: 1.2754039360500617 entropy 9.179549688020554
epoch: 14, step: 52
	action: tensor([[-1271.2590,   687.9042, -3341.8514,  2758.9456,  -498.4468, -2238.7430,
          3346.7818]], dtype=torch.float64)
	q_value: tensor([[-28.6295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15157265262178288, distance: 1.0540568200626574 entropy 9.01790282644342
epoch: 14, step: 53
	action: tensor([[-2460.5007, -5555.8949,   686.8663,  1726.3139,   491.6020,  1933.3590,
          1551.6411]], dtype=torch.float64)
	q_value: tensor([[-32.6040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07642354714233823, distance: 1.0997478438101758 entropy 9.12474722559847
epoch: 14, step: 54
	action: tensor([[-6150.2253,  -225.3552, -3246.8226,  -589.9385,   555.6103,   514.6185,
           694.9410]], dtype=torch.float64)
	q_value: tensor([[-29.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8556374407679785, distance: 1.5588461567047658 entropy 9.04239805317908
epoch: 14, step: 55
	action: tensor([[ -719.3940,  2134.8267,  1346.6531,  2293.7291, -1250.6268, -2498.8853,
          1700.5454]], dtype=torch.float64)
	q_value: tensor([[-30.3527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.106921089308928
epoch: 14, step: 56
	action: tensor([[  300.0643,   461.3786,  -361.9525, -2539.8714, -1059.4785,  1550.9013,
          2805.7184]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.53719386307136
epoch: 14, step: 57
	action: tensor([[-2137.5944,   214.6033,  -268.4352,  1087.1350,  -668.5754,  -341.2823,
           801.6840]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08203719147091348, distance: 1.1903585275262372 entropy 8.53719386307136
epoch: 14, step: 58
	action: tensor([[ 3221.4856, -2728.1630, -1293.8814,  2731.3077,  3106.2962, -4744.9915,
         -1719.2438]], dtype=torch.float64)
	q_value: tensor([[-36.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5672735942267912, distance: 0.7527718877496866 entropy 9.285554134739355
epoch: 14, step: 59
	action: tensor([[ 2955.1047,  -882.7306,   916.6141,  1194.4936, -1212.9137,  1553.9401,
          -218.3556]], dtype=torch.float64)
	q_value: tensor([[-35.3847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3769488445052248, distance: 0.9032719961796635 entropy 9.07366249181383
epoch: 14, step: 60
	action: tensor([[  732.2009, -2193.2954, -3331.9697,  1977.5961, -2730.4202, -1310.3478,
            29.0972]], dtype=torch.float64)
	q_value: tensor([[-28.4782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15529139175261275, distance: 1.0517442667436638 entropy 8.921817172059976
epoch: 14, step: 61
	action: tensor([[  707.4587, -2140.5262, -2202.0725, -2998.4709,  3870.7251, -1224.2379,
         -2543.4126]], dtype=torch.float64)
	q_value: tensor([[-37.3578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21065274325388383, distance: 1.016695134642569 entropy 9.184977843026884
epoch: 14, step: 62
	action: tensor([[  221.9998,  -613.9921, -3823.3602,  2910.4165,   747.2761, -1020.6805,
          4042.3199]], dtype=torch.float64)
	q_value: tensor([[-29.7847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32397838281959923, distance: 1.316731242687724 entropy 9.00425068936787
epoch: 14, step: 63
	action: tensor([[-3408.9707, -3328.4709,  -929.1716,  -900.9405,  -544.9023, -1983.4588,
          3731.2584]], dtype=torch.float64)
	q_value: tensor([[-36.8712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5346752440482314, distance: 1.4176366649687053 entropy 9.127686708252
epoch: 14, step: 64
	action: tensor([[-1085.1934,  -225.6288,  -952.0300, -2928.7034,   688.2420,   -18.2500,
         -2928.0140]], dtype=torch.float64)
	q_value: tensor([[-31.2010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0971574531630903, distance: 1.6571896505339743 entropy 9.06982054253721
epoch: 14, step: 65
	action: tensor([[-3438.0061, -1080.4193, -3606.7079, -3230.6832,  -924.5038,  2900.6551,
          1560.0083]], dtype=torch.float64)
	q_value: tensor([[-35.2889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14113207439584663, distance: 1.2224318296037053 entropy 9.180007794183316
epoch: 14, step: 66
	action: tensor([[-1766.8699,  -280.8185, -3474.9557,  1646.3629,  3877.0850,  3421.9969,
          5432.6404]], dtype=torch.float64)
	q_value: tensor([[-35.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6096671128316726, distance: 1.4518599618134418 entropy 8.99688689472692
epoch: 14, step: 67
	action: tensor([[ -469.8657, -1548.6292,    65.1276,  1144.7762, -1008.4671,  1239.0167,
         -3391.4974]], dtype=torch.float64)
	q_value: tensor([[-29.1139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32105734655350093, distance: 1.3152779170356403 entropy 9.044779086824803
epoch: 14, step: 68
	action: tensor([[-3175.1042,    10.7918, -1790.0873,   -28.0515,  1969.1735,  2823.4432,
          3026.2003]], dtype=torch.float64)
	q_value: tensor([[-33.9953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9187278902098743, distance: 1.585124532515434 entropy 9.173065606576504
epoch: 14, step: 69
	action: tensor([[ -771.4485, -2276.7255,  4728.1417,   114.7735,  3196.6480,  1254.2925,
          2107.6029]], dtype=torch.float64)
	q_value: tensor([[-31.4825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1311480507992151, distance: 1.2170724143581726 entropy 9.158246596271033
epoch: 14, step: 70
	action: tensor([[-1320.4855, -1981.7198,  1806.0586,   498.4956,  2990.0018, -2699.3630,
          2931.2343]], dtype=torch.float64)
	q_value: tensor([[-34.7434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33320280185002815, distance: 1.3213102442159763 entropy 9.206914450492052
epoch: 14, step: 71
	action: tensor([[-2968.8594, -5605.0221, -1309.2925,  -589.1535,   378.5860,   503.5091,
         -2907.0009]], dtype=torch.float64)
	q_value: tensor([[-34.6635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5446382476396514, distance: 1.8254490613453271 entropy 9.240862099515507
epoch: 14, step: 72
	action: tensor([[  533.5630, -3865.0520,  -718.6968,  1708.6491, -2129.6931, -3216.4080,
          2705.5024]], dtype=torch.float64)
	q_value: tensor([[-33.1113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3730886561924722, distance: 1.3409296163052684 entropy 9.01247299806825
epoch: 14, step: 73
	action: tensor([[ -891.1932, -3409.9816, -2719.7715, -1085.2710, -2662.9418,  3963.9559,
         -1742.6380]], dtype=torch.float64)
	q_value: tensor([[-28.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7157418467039527, distance: 1.4989345330252037 entropy 9.122101660300292
epoch: 14, step: 74
	action: tensor([[-1157.8359, -3288.3932, -1324.5648, -1881.7510,  1934.6835,  1312.3308,
          1624.7744]], dtype=torch.float64)
	q_value: tensor([[-30.1974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7710269251944031, distance: 1.522892595949227 entropy 9.039970631825724
epoch: 14, step: 75
	action: tensor([[-1381.1985, -2903.7750,   566.3710, -2007.9756,  1444.1723, -2686.5450,
          1199.8994]], dtype=torch.float64)
	q_value: tensor([[-35.1799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18532839328147088, distance: 1.032875517715787 entropy 9.089532876013307
epoch: 14, step: 76
	action: tensor([[-1099.8237, -3819.1355,  1242.3862,  -368.8120, -1989.9511,  -342.7741,
           528.1761]], dtype=torch.float64)
	q_value: tensor([[-32.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07648036499270106, distance: 1.187298036588205 entropy 8.939606031540311
epoch: 14, step: 77
	action: tensor([[-3954.1001, -2521.2098, -2729.7830, -2332.1862,    34.5849,  -148.5409,
          1404.0649]], dtype=torch.float64)
	q_value: tensor([[-34.4397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7701067762055358, distance: 1.5224969299879005 entropy 8.982158763922945
epoch: 14, step: 78
	action: tensor([[-3487.7210, -4261.6132,  2297.6890,  4436.4528, -6467.1565,  2625.0309,
          5234.1916]], dtype=torch.float64)
	q_value: tensor([[-46.2197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42308916341227065, distance: 1.3651260339925098 entropy 9.40400340376153
epoch: 14, step: 79
	action: tensor([[-1264.5439, -2458.8793,  1250.5696, -2600.0828, -1881.2890,  2387.4795,
          -933.5846]], dtype=torch.float64)
	q_value: tensor([[-32.1530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0054801166543678725, distance: 1.1412043763640396 entropy 9.041488779389967
epoch: 14, step: 80
	action: tensor([[ 1695.8992,   411.5372, -1899.6273,  6811.2434,   587.4960, -2484.8552,
          -594.2617]], dtype=torch.float64)
	q_value: tensor([[-38.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.18630993944616
epoch: 14, step: 81
	action: tensor([[ -865.4099, -1582.8826, -2499.1693,   532.1524,  -215.5000,  -486.4474,
          1974.3800]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029670804741458223, distance: 1.161196966839012 entropy 8.53719386307136
epoch: 14, step: 82
	action: tensor([[-3503.1991, -3735.3197,  3408.6773,   171.6863,  1263.5898, -2557.8570,
           -64.3174]], dtype=torch.float64)
	q_value: tensor([[-26.9509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33826399939585206, distance: 1.3238158935325641 entropy 9.000882098427203
epoch: 14, step: 83
	action: tensor([[-1115.2244, -3692.5093, -1684.3226,  1235.9575,  2928.2775,  2227.4333,
          -425.6111]], dtype=torch.float64)
	q_value: tensor([[-29.9638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2779240099590031, distance: 0.97240715438835 entropy 9.034938779645888
epoch: 14, step: 84
	action: tensor([[-3238.3315, -2401.2248, -4945.3725,  1601.4581,  1429.1732,  -990.9776,
         -2775.8845]], dtype=torch.float64)
	q_value: tensor([[-28.8939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12835192576800192, distance: 1.068383767066365 entropy 8.97251900723738
epoch: 14, step: 85
	action: tensor([[ -928.3217, -1274.5759, -1604.1507,  4297.6285,  -607.3667, -2171.9689,
          2061.5444]], dtype=torch.float64)
	q_value: tensor([[-31.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13652641231311624, distance: 1.0633622098226698 entropy 9.083904450219839
epoch: 14, step: 86
	action: tensor([[ -228.7626, -2548.1504,   691.0232, -1189.5447,   433.2132, -1335.6350,
          1248.0415]], dtype=torch.float64)
	q_value: tensor([[-30.8516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07589067543333816, distance: 1.1869727946293966 entropy 9.097931256053705
epoch: 14, step: 87
	action: tensor([[-1211.7041, -1108.2645,  -173.9171,   407.0147, -1907.1500, -1150.0819,
         -2975.7739]], dtype=torch.float64)
	q_value: tensor([[-27.0654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2328175389211642, distance: 1.0023191457741985 entropy 8.799126485285875
epoch: 14, step: 88
	action: tensor([[-4473.1595, -1166.9665,  1239.7281,   365.1812, -2934.1620, -1836.1525,
          2526.6642]], dtype=torch.float64)
	q_value: tensor([[-34.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2689381091496805, distance: 1.2890712232845678 entropy 9.286209303200982
epoch: 14, step: 89
	action: tensor([[-3247.5567,  -167.0138, -3387.0791,  2726.0865,  1107.9914,  2034.8722,
          1339.1303]], dtype=torch.float64)
	q_value: tensor([[-28.2790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2818120498758565, distance: 1.295593821425634 entropy 8.99976217955614
epoch: 14, step: 90
	action: tensor([[ -292.6777, -2096.8125, -2428.6191,  3191.4113,  -813.9173,  4464.2535,
          -453.6357]], dtype=torch.float64)
	q_value: tensor([[-30.5572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2741774445687297, distance: 1.2917297135931078 entropy 9.017313078546419
epoch: 14, step: 91
	action: tensor([[-4453.6009,  1691.8886,   967.0965,  3465.1209,  -701.5812,   639.4504,
         -6274.6879]], dtype=torch.float64)
	q_value: tensor([[-32.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5091633342497416, distance: 0.8017245672945787 entropy 9.196093256369968
epoch: 14, step: 92
	action: tensor([[-547.9068,  811.9650,  -81.0751,  347.1888, 3844.0648, 3833.0488,
         3417.5517]], dtype=torch.float64)
	q_value: tensor([[-35.7523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42763859592592746, distance: 0.8657487306472385 entropy 9.166892182607985
epoch: 14, step: 93
	action: tensor([[-2050.4409,   364.8611,  1479.4495,  3593.8329, -1421.8777,  1473.7202,
          1847.0858]], dtype=torch.float64)
	q_value: tensor([[-30.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49218106203661016, distance: 0.8154759168507822 entropy 8.971903465527475
epoch: 14, step: 94
	action: tensor([[ -192.4788, -2809.6752,  -499.6754,  3228.9122,  -180.3080,  -636.9598,
         -1200.2816]], dtype=torch.float64)
	q_value: tensor([[-32.8319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2328406090606029, distance: 1.0023040751635746 entropy 9.087295963332428
epoch: 14, step: 95
	action: tensor([[-1565.0236, -2165.6149,  -295.9521,  5795.0627,  3353.8078,   877.4885,
         -1427.3814]], dtype=torch.float64)
	q_value: tensor([[-31.8502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5040713326297301, distance: 1.4034304985838104 entropy 9.118869461114949
epoch: 14, step: 96
	action: tensor([[-4147.5523,   640.8625, -1846.2036,  2162.4677,  -740.7676,  5107.9011,
         -1782.7339]], dtype=torch.float64)
	q_value: tensor([[-29.4875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1452101741927988, distance: 1.0580016997481847 entropy 9.115579882256378
epoch: 14, step: 97
	action: tensor([[-3470.8675,  -907.4117,  -876.2763,  1277.9150, -1846.0067, -2150.9339,
          1718.0451]], dtype=torch.float64)
	q_value: tensor([[-30.6362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2787770795850295, distance: 1.2940591116228615 entropy 8.880778412718104
epoch: 14, step: 98
	action: tensor([[-4654.0305, -2660.0699,  2607.1534,  -738.7274,   622.3437, -2968.1142,
           857.0534]], dtype=torch.float64)
	q_value: tensor([[-33.7371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3490488961542033, distance: 1.3291394203203106 entropy 9.160392947569642
epoch: 14, step: 99
	action: tensor([[-2423.8866, -3157.9827, -3607.7061,   389.0630, -1301.1497, -2722.0284,
           151.8723]], dtype=torch.float64)
	q_value: tensor([[-29.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0978794683655146, distance: 1.6574748969163884 entropy 8.981734746480473
epoch: 14, step: 100
	action: tensor([[ -799.8354,   230.7621,  -319.3286,  -486.5207,  -138.7189, -2776.2119,
          2619.1270]], dtype=torch.float64)
	q_value: tensor([[-26.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4252007386684531, distance: 1.3661384434521064 entropy 8.926960535056837
epoch: 14, step: 101
	action: tensor([[-619.9095, -869.6626, 1622.0499, -103.2038, -124.7027, -292.2140,
          830.1139]], dtype=torch.float64)
	q_value: tensor([[-30.2856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31231399959989536, distance: 1.3109181432728632 entropy 8.632251313243833
epoch: 14, step: 102
	action: tensor([[ -879.4408,  -147.4541,  -158.2593,  -756.3667,  -749.1611,  1135.0260,
         -2928.9652]], dtype=torch.float64)
	q_value: tensor([[-29.3263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37273126381490085, distance: 0.9063240707689932 entropy 8.92975591734396
epoch: 14, step: 103
	action: tensor([[-1181.5773,  -332.2423,   154.1580, -3730.6511, -1834.0846,  4793.9970,
          3033.4017]], dtype=torch.float64)
	q_value: tensor([[-31.4493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8273745736135238, distance: 1.54692935969265 entropy 8.88202884115548
epoch: 14, step: 104
	action: tensor([[-2333.9906, -5545.3824,   765.5876, -2528.8410,  1995.7113,    25.0058,
          1932.9486]], dtype=torch.float64)
	q_value: tensor([[-39.4875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4744633425285104, distance: 0.8295798610375825 entropy 9.112416496506617
epoch: 14, step: 105
	action: tensor([[ -418.8837,  -343.4920, -2373.1999,   801.6270, -1740.8873,  1508.4985,
           220.8921]], dtype=torch.float64)
	q_value: tensor([[-34.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7684385680803187, distance: 1.52177933473501 entropy 8.997164918365323
epoch: 14, step: 106
	action: tensor([[-1194.1125, -1009.2121,   964.3306,  -334.9707,   411.6078, -2157.1595,
           947.9155]], dtype=torch.float64)
	q_value: tensor([[-24.6644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.273681990661871, distance: 1.725526185247084 entropy 8.77407444161516
epoch: 14, step: 107
	action: tensor([[-2581.6967,  -887.9456,  2339.5756, -1380.6849, -1827.5230, -3538.0027,
          3414.1142]], dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17538052161685203, distance: 1.2406404530752286 entropy 9.15120811907361
epoch: 14, step: 108
	action: tensor([[ -500.0115, -1994.2650,   820.2841,  1877.1588, -1290.5221,  2673.4773,
          -824.3054]], dtype=torch.float64)
	q_value: tensor([[-34.3288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8818932506133754, distance: 1.5698356434385177 entropy 9.237561715501055
epoch: 14, step: 109
	action: tensor([[-1700.5295,  -723.3165, -2824.7321, -2188.6143,  2189.8663,  4306.1647,
          4718.4056]], dtype=torch.float64)
	q_value: tensor([[-37.8136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08874949009923017, distance: 1.0923846413488798 entropy 9.315293181032871
epoch: 14, step: 110
	action: tensor([[-1550.0087, -5584.8885, -3186.8067,   509.1701,  -323.1873,  2406.9860,
          1613.3789]], dtype=torch.float64)
	q_value: tensor([[-37.8145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5836878802595113, distance: 1.4400961516836734 entropy 9.196551548195089
epoch: 14, step: 111
	action: tensor([[-1736.7308,  3543.9890,   975.8913,   562.4056, -2335.2371,  5782.1385,
          2106.9661]], dtype=torch.float64)
	q_value: tensor([[-33.6856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28048319409792466, distance: 1.2949220756037656 entropy 9.167046195931572
epoch: 14, step: 112
	action: tensor([[-6408.2019,  -904.6028,  4400.2636,  -477.0306,   274.7905,  1129.1698,
          2506.6225]], dtype=torch.float64)
	q_value: tensor([[-43.2579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8356902141510278, distance: 1.5504450886244172 entropy 9.307753716047984
epoch: 14, step: 113
	action: tensor([[  711.3938,   352.0326,   745.8282,   620.6493,  -596.0730, -1000.8501,
           738.2032]], dtype=torch.float64)
	q_value: tensor([[-29.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.948494508796177
epoch: 14, step: 114
	action: tensor([[-1499.0435, -2552.4389,   -15.0694,   703.2984,   400.7392,  -862.8315,
         -1107.7306]], dtype=torch.float64)
	q_value: tensor([[-35.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5930090575175229, distance: 1.444327950741581 entropy 8.53719386307136
epoch: 14, step: 115
	action: tensor([[  436.5073, -3495.6956, -1939.7900,  3139.0949, -1083.1865,  1006.2994,
          1122.3221]], dtype=torch.float64)
	q_value: tensor([[-31.9227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30435732745691, distance: 0.9544425683498345 entropy 9.189873399015138
epoch: 14, step: 116
	action: tensor([[-3928.6860, -7100.5650, -3589.5506,  3118.4351, -1321.5902, -1531.2793,
          3055.1413]], dtype=torch.float64)
	q_value: tensor([[-32.5061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22466932145049423, distance: 1.2663860346930527 entropy 9.218462917133893
epoch: 14, step: 117
	action: tensor([[-1850.4851, -2399.6867, -2225.3545,  4856.8364,  4694.9111,  1507.9177,
           -36.5188]], dtype=torch.float64)
	q_value: tensor([[-36.6930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6630866704093314, distance: 1.475754562668394 entropy 9.315808137800957
epoch: 14, step: 118
	action: tensor([[1226.8728,  893.8348, 3682.6847,  220.7451,  867.6024, -380.9999,
         -846.0275]], dtype=torch.float64)
	q_value: tensor([[-31.6469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2472320871641126, distance: 0.9928582358038822 entropy 9.052313687374502
epoch: 14, step: 119
	action: tensor([[ -519.6357, -2255.3156, -2459.5814,  3253.0690, -1355.1921,   -83.5967,
         -2784.8940]], dtype=torch.float64)
	q_value: tensor([[-27.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3252275427210962, distance: 1.3173522573919156 entropy 8.776736027559418
epoch: 14, step: 120
	action: tensor([[-870.7601,  421.7810, -401.0782, 3696.5276, 2946.5231, 2434.2671,
          323.5612]], dtype=torch.float64)
	q_value: tensor([[-26.4609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12104883690120938, distance: 1.072850148999639 entropy 9.06807982847853
epoch: 14, step: 121
	action: tensor([[ -125.3557,   350.1314, -1948.6233,  1233.0612, -1297.8383,   465.7601,
          1986.3639]], dtype=torch.float64)
	q_value: tensor([[-34.4138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5789802745390175, distance: 0.7425195881794024 entropy 9.092330369741521
epoch: 14, step: 122
	action: tensor([[-3748.5274,  -859.2508,  2173.3111, -1987.0262,  1277.9481, -1725.5437,
         -3102.3684]], dtype=torch.float64)
	q_value: tensor([[-34.4331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9642390276091783, distance: 1.6038134866255722 entropy 8.998203227160742
epoch: 14, step: 123
	action: tensor([[ 568.4520, -224.5117, -299.7663, 2714.5138, 1631.1291, -357.0812,
         -637.6352]], dtype=torch.float64)
	q_value: tensor([[-30.1624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2516026566581785, distance: 0.9899717732114456 entropy 8.919522104832913
epoch: 14, step: 124
	action: tensor([[-2275.4157, -5982.0479,  1495.4957,  3779.0889,  1608.4507,  -466.3831,
          1201.3945]], dtype=torch.float64)
	q_value: tensor([[-32.3417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6812545049976195, distance: 1.4837933617959018 entropy 9.163164123126617
epoch: 14, step: 125
	action: tensor([[-1101.0733, -2366.7578, -5410.8684, -1099.6076,  -383.9733,  -122.4024,
          -401.9701]], dtype=torch.float64)
	q_value: tensor([[-27.5530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40502406021931225, distance: 1.3564337088908538 entropy 8.965231318894807
epoch: 14, step: 126
	action: tensor([[-1509.4736,  1142.0442,   174.5287,  1672.4590, -1470.8883,   372.3433,
          1261.2926]], dtype=torch.float64)
	q_value: tensor([[-26.9006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3203669083434706, distance: 1.314934163292107 entropy 8.800821927113285
epoch: 14, step: 127
	action: tensor([[-3885.7635,  1094.7956,  2226.4713,  1177.7272,  1091.3250,  2201.9498,
          -671.7044]], dtype=torch.float64)
	q_value: tensor([[-30.0823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22621130809348244, distance: 1.0066253952168402 entropy 8.885046557945076
LOSS epoch 14 actor 451.2390505438046 critic 115.69282543491568
epoch: 15, step: 0
	action: tensor([[ 3149.1759, -1218.4820, -2847.4124,  1829.6290,  2339.1922, -2444.0098,
          4424.5634]], dtype=torch.float64)
	q_value: tensor([[-29.9381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.274147583689714
epoch: 15, step: 1
	action: tensor([[-2417.9686, -1622.7494,  1182.3482,  1604.4659,  -842.1368,   226.7029,
         -1635.9455]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18395826322982578, distance: 1.033743707584019 entropy 8.624138921056396
epoch: 15, step: 2
	action: tensor([[-3270.1448,  1753.0869,   406.9874,  -141.6845, -1893.9777, -2942.2851,
          -447.7525]], dtype=torch.float64)
	q_value: tensor([[-24.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18831573936279022, distance: 1.0309800358828407 entropy 9.127887492653517
epoch: 15, step: 3
	action: tensor([[-2061.2033, -2698.4886,  1591.5578,   359.1811, -1543.6693,  1539.5546,
          1830.0829]], dtype=torch.float64)
	q_value: tensor([[-33.9583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3120850345853028, distance: 1.3108037775473804 entropy 9.231788101166378
epoch: 15, step: 4
	action: tensor([[ 2128.9792,  -705.6520,  2326.4803,    70.4744,  -739.8809,  4476.0455,
         -1082.7491]], dtype=torch.float64)
	q_value: tensor([[-28.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14610088050566805, distance: 1.2250903426873698 entropy 9.298572670565262
epoch: 15, step: 5
	action: tensor([[ 3122.0457, -1448.1099,  -614.3968, -2041.4849,  2963.4941,  1456.1127,
          2504.8174]], dtype=torch.float64)
	q_value: tensor([[-25.5090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47368981558258394, distance: 0.8301901575921309 entropy 9.180178396490984
epoch: 15, step: 6
	action: tensor([[  679.2309,  4425.6972,  2774.3842, -2085.4761, -3112.7202, -2226.4480,
          -413.8767]], dtype=torch.float64)
	q_value: tensor([[-22.9200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5067196101188096, distance: 0.8037178588562787 entropy 9.108789963039989
epoch: 15, step: 7
	action: tensor([[-2190.3173,  2611.6973,   -94.5438,  6313.8515,   204.4968,  2066.2592,
          1114.5127]], dtype=torch.float64)
	q_value: tensor([[-32.0344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19587436785553136, distance: 1.0261684221883711 entropy 9.213161324058742
epoch: 15, step: 8
	action: tensor([[  531.1539,   520.4170,   830.6776,  -258.1926,  2202.9626,  -302.0453,
         -5083.9533]], dtype=torch.float64)
	q_value: tensor([[-32.7858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5601850056109804, distance: 0.7589125050268769 entropy 9.148054853764036
epoch: 15, step: 9
	action: tensor([[ -557.5797, -4184.1590,  -573.8226,  1029.6892, -4072.7282, -1918.5098,
          5444.1990]], dtype=torch.float64)
	q_value: tensor([[-30.8408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.146659797553921
epoch: 15, step: 10
	action: tensor([[-1081.5659,    16.3500, -3258.6553,  1369.1994,  1528.7989,  -219.4504,
          -136.0116]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.624138921056396
epoch: 15, step: 11
	action: tensor([[ 1535.1141,  -478.2350,   -29.8610, -1336.3502,  2259.2564,  1640.7633,
         -1283.7157]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42186730213588486, distance: 0.8701025864969116 entropy 8.624138921056396
epoch: 15, step: 12
	action: tensor([[4448.5519, -264.8044,  275.1031, -128.0492, 1515.6457, -248.2702,
         -227.9246]], dtype=torch.float64)
	q_value: tensor([[-25.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2874895214555788, distance: 1.298459908550458 entropy 9.142211631711287
epoch: 15, step: 13
	action: tensor([[ -275.5620, -1820.1093, -2082.9095,   936.6385, -1508.5804,   972.7997,
           -14.6298]], dtype=torch.float64)
	q_value: tensor([[-25.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48302772456652665, distance: 1.3935781497120578 entropy 8.988539830636485
epoch: 15, step: 14
	action: tensor([[-2724.6540,   651.4353,  2373.6210,  2287.9707, -1515.0600, -2762.5243,
           231.6902]], dtype=torch.float64)
	q_value: tensor([[-24.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1202040679025893, distance: 1.0733655888343936 entropy 9.167430409693141
epoch: 15, step: 15
	action: tensor([[   28.7766, -2624.2754,  3501.5209,  1887.3743,  -343.4836, -2289.2503,
          3102.0240]], dtype=torch.float64)
	q_value: tensor([[-29.7312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3922045845491793, distance: 0.8921449124668746 entropy 9.12380068491298
epoch: 15, step: 16
	action: tensor([[ 1274.6989,  -373.3336,  1498.8970,  2777.1775,  1358.5636, -2601.8957,
           687.3942]], dtype=torch.float64)
	q_value: tensor([[-27.5691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21090680720165955, distance: 1.2592502727015373 entropy 9.218120534843361
epoch: 15, step: 17
	action: tensor([[-4020.3692,  1575.9614,   935.4478,  1591.3811, -4564.9630, -1191.3184,
           422.9306]], dtype=torch.float64)
	q_value: tensor([[-31.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24945810389985024, distance: 0.9913891539511767 entropy 9.310456438483566
epoch: 15, step: 18
	action: tensor([[-4547.6077, -5182.7282,  -120.8596,  3160.4459, -3125.4887, -5690.3473,
           996.8644]], dtype=torch.float64)
	q_value: tensor([[-31.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033624769226084594, distance: 1.1249405926734897 entropy 9.202425884817757
epoch: 15, step: 19
	action: tensor([[  369.3918, -3505.3563,  1512.4599, -1764.8458, -3613.2380, -3765.7718,
          1692.4897]], dtype=torch.float64)
	q_value: tensor([[-30.2682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48636538730240864, distance: 0.8201321450085466 entropy 9.43941918681244
epoch: 15, step: 20
	action: tensor([[-1450.8172, -2362.3071, -2192.8174,  4093.7595, -3550.0601,   315.9981,
           730.2722]], dtype=torch.float64)
	q_value: tensor([[-23.6740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20868637555577552, distance: 1.0179607075179151 entropy 9.015367554320122
epoch: 15, step: 21
	action: tensor([[-3970.2334, -2609.8037, -1921.5499,  -325.6804, -1848.0446,   204.7086,
         -1949.9205]], dtype=torch.float64)
	q_value: tensor([[-22.6376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5462286786183022, distance: 1.4229628284243592 entropy 8.987410167715387
epoch: 15, step: 22
	action: tensor([[-2873.9582, -2293.9773,  -910.0905,  2323.2692,   349.5579,   423.2808,
           912.4360]], dtype=torch.float64)
	q_value: tensor([[-25.1917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5444046098429485, distance: 1.4221232540307391 entropy 9.111757218987446
epoch: 15, step: 23
	action: tensor([[ -349.8687, -3818.3506,  -231.6560,  -695.2319,  1619.6006,  -847.7922,
           286.7543]], dtype=torch.float64)
	q_value: tensor([[-28.0408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4574799788593529, distance: 1.3815225944728309 entropy 9.250746663520582
epoch: 15, step: 24
	action: tensor([[ 3901.2758,   546.1554, -3010.3628,  1178.7059,  -618.0779, -2239.0403,
          3057.8570]], dtype=torch.float64)
	q_value: tensor([[-26.4451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.147126237056815
epoch: 15, step: 25
	action: tensor([[ -945.6448, -1192.7470,  1433.6854,   286.3384, -2130.3280, -2270.4176,
           658.6915]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8917808835033336, distance: 1.5739542679903062 entropy 8.624138921056396
epoch: 15, step: 26
	action: tensor([[-3859.7082,   691.8845,   593.8281,  1586.6773,   634.1967, -1957.4583,
          3389.9588]], dtype=torch.float64)
	q_value: tensor([[-21.7855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09614238069328829, distance: 1.0879444097382454 entropy 8.943634339661546
epoch: 15, step: 27
	action: tensor([[-2672.5259,  1152.4991,    35.1010,  4274.5659, -2308.5516,   966.8774,
          4599.3826]], dtype=torch.float64)
	q_value: tensor([[-30.4088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4065824573261949, distance: 1.3571857517202837 entropy 9.238845927267041
epoch: 15, step: 28
	action: tensor([[ 1611.5654, -4926.9095,   657.9095,  -663.5022, -1015.1167, -1678.8804,
          3710.7201]], dtype=torch.float64)
	q_value: tensor([[-34.4724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08037132500224065, distance: 1.1894418574748782 entropy 9.323723100354588
epoch: 15, step: 29
	action: tensor([[-1422.7122, -1563.4699,  3237.9235,  2848.6774,  5122.1188,  2473.0802,
          1757.7079]], dtype=torch.float64)
	q_value: tensor([[-28.7582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22329837574238032, distance: 1.2656770136457243 entropy 9.165804282926505
epoch: 15, step: 30
	action: tensor([[-765.0820, -164.1018, 1883.3985, -235.3052, 1435.1807, 2899.3367,
         3806.6344]], dtype=torch.float64)
	q_value: tensor([[-27.3251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9933829657085353, distance: 1.6156677813285292 entropy 9.285593373791954
epoch: 15, step: 31
	action: tensor([[-2608.2313, -2996.0871, -1296.3592, -1441.9873,  2770.7160,  2678.9281,
           677.1138]], dtype=torch.float64)
	q_value: tensor([[-28.4666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2999601709292876, distance: 1.304733208762144 entropy 9.155661000077714
epoch: 15, step: 32
	action: tensor([[-2478.7462, -3909.8597,   869.6739, -1481.4855,  2698.6181, -1312.7918,
         -1623.6262]], dtype=torch.float64)
	q_value: tensor([[-32.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7017019435072371, distance: 1.492789049835087 entropy 9.129255073141982
epoch: 15, step: 33
	action: tensor([[ 5006.3181,  2804.6760,  -931.8177,  -364.2501, -2922.2471,  3677.4387,
          3535.5909]], dtype=torch.float64)
	q_value: tensor([[-31.7095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6368131301889832, distance: 0.6896389197060553 entropy 9.295219604535276
epoch: 15, step: 34
	action: tensor([[  -20.1587,   640.5789,  -450.2006,  1852.4747,   610.7446, -2306.2322,
          1917.1394]], dtype=torch.float64)
	q_value: tensor([[-32.3905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7979161391730277, distance: 0.5144255238830208 entropy 8.959971456112715
epoch: 15, step: 35
	action: tensor([[-1845.8478, -1899.5898,  1756.8295,  1899.4588,  2560.2239, -1938.4229,
          3511.1084]], dtype=torch.float64)
	q_value: tensor([[-32.1756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.027701716703223
epoch: 15, step: 36
	action: tensor([[  957.5851,  -765.0139,   332.2374,   340.7020, -1271.0803, -2332.4340,
          -987.5782]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6317511475820919, distance: 0.6944282728466654 entropy 8.624138921056396
epoch: 15, step: 37
	action: tensor([[ 1193.8727,   142.7712, -2820.2068,  2414.9906, -3028.8123,  4280.0052,
          2455.9585]], dtype=torch.float64)
	q_value: tensor([[-21.6265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5251327106433543, distance: 0.7885746659623377 entropy 9.025070394953538
epoch: 15, step: 38
	action: tensor([[-3739.7799, -3888.6733,   453.0340,  2037.1969,  1609.2248,  -853.8275,
          3516.5362]], dtype=torch.float64)
	q_value: tensor([[-21.5084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3159867093946964, distance: 1.3127512632424627 entropy 8.91787039960558
epoch: 15, step: 39
	action: tensor([[-2944.3917, -1183.2726,   721.8670,  -221.3006,  2754.9912,   169.8317,
          1215.4691]], dtype=torch.float64)
	q_value: tensor([[-22.9750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2006584285939854, distance: 1.6975908003856413 entropy 9.040000633245223
epoch: 15, step: 40
	action: tensor([[-3518.8289,   328.7097,  -231.1939, -4175.1479,   339.5901,  5219.5320,
          2408.6564]], dtype=torch.float64)
	q_value: tensor([[-28.0889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40151943812200086, distance: 1.3547409422720913 entropy 9.197469485238978
epoch: 15, step: 41
	action: tensor([[-3698.4455,   819.4512,  1361.2329, -3236.4824, -3463.4591,    25.5653,
          3551.3738]], dtype=torch.float64)
	q_value: tensor([[-33.1267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3959557956890114, distance: 1.3520492959375563 entropy 9.02758303508613
epoch: 15, step: 42
	action: tensor([[ -981.2127,  -979.4842,   810.1565,  1558.8196, -4077.7783,    73.6095,
          1904.2232]], dtype=torch.float64)
	q_value: tensor([[-37.9939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6234996102295742, distance: 1.45808481676511 entropy 8.97358746708987
epoch: 15, step: 43
	action: tensor([[ -651.4061, -1123.6511,  -519.6585,   448.0392,  -684.3736,  2543.8176,
           690.3718]], dtype=torch.float64)
	q_value: tensor([[-21.3418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16741599298799603, distance: 1.0441688316974895 entropy 8.781762427688752
epoch: 15, step: 44
	action: tensor([[-1967.6464, -2826.3975, -3392.3716,   651.8563,  -339.0392,  1666.9822,
          1234.1503]], dtype=torch.float64)
	q_value: tensor([[-23.6524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24177721843749145, distance: 0.9964490737977572 entropy 9.044269257619533
epoch: 15, step: 45
	action: tensor([[  471.1155,  1381.5254,  1817.7790,  2361.4354, -2133.4637,   266.8926,
          1593.8949]], dtype=torch.float64)
	q_value: tensor([[-24.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8531185529181666, distance: 0.4385712559777429 entropy 9.150634612690578
epoch: 15, step: 46
	action: tensor([[-1908.1688, -3432.8901,   363.5805, -1354.0596,  1397.3944,  1904.2960,
          1418.4244]], dtype=torch.float64)
	q_value: tensor([[-27.5819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.20674296911604
epoch: 15, step: 47
	action: tensor([[ 1371.1973, -2047.4548,  -406.2020,  1221.1869, -2519.5255,   681.3458,
           606.3138]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45648150182658276, distance: 0.8436530055043738 entropy 8.624138921056396
epoch: 15, step: 48
	action: tensor([[   17.2316,   885.7827,  1042.5607,    35.4112,  1797.5259, -1948.7594,
          1120.0788]], dtype=torch.float64)
	q_value: tensor([[-22.9688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.041492842472236
epoch: 15, step: 49
	action: tensor([[-554.5332, -565.5997, 1425.1397, 1391.1774,  730.6291,  550.9762,
          683.7205]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6351402219721658, distance: 1.4633027679446813 entropy 8.624138921056396
epoch: 15, step: 50
	action: tensor([[-4057.3047,   730.2667,  1747.8828,   770.3798,   123.2623,  1889.6323,
           877.1656]], dtype=torch.float64)
	q_value: tensor([[-26.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6112406363602857, distance: 0.7135051703685931 entropy 9.113779666038017
epoch: 15, step: 51
	action: tensor([[-2643.8522, -3474.7037, -5520.1839,  2726.1796,  1878.9189,  1623.7944,
         -3929.5992]], dtype=torch.float64)
	q_value: tensor([[-31.4141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06261335687422676, distance: 1.1079395707403943 entropy 9.305854127848486
epoch: 15, step: 52
	action: tensor([[-2024.8749,  -208.2332,   311.3296, -1299.9149, -1417.8750,  1687.8505,
          1428.3974]], dtype=torch.float64)
	q_value: tensor([[-28.4194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9469595795269528, distance: 1.5967435147377884 entropy 9.305686744404172
epoch: 15, step: 53
	action: tensor([[ 3590.9825,   969.3322,  -257.2263, -3919.8170,  3876.9751,  2125.2323,
          3921.0563]], dtype=torch.float64)
	q_value: tensor([[-34.5296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3757144133085296, distance: 0.9041663652663421 entropy 9.293187452240632
epoch: 15, step: 54
	action: tensor([[-1096.2962,  2831.5151,  -691.4336,  2163.5239,   657.5063, -1679.4505,
          -833.0494]], dtype=torch.float64)
	q_value: tensor([[-26.3699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23817943302383315, distance: 0.9988103634809439 entropy 8.972137698602053
epoch: 15, step: 55
	action: tensor([[ -401.7952, -2253.4604, -1579.9590,  1260.1356, -1706.1405, -3655.0879,
         -3039.8269]], dtype=torch.float64)
	q_value: tensor([[-26.9206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21278345962036005, distance: 1.015322004623804 entropy 9.020889212654579
epoch: 15, step: 56
	action: tensor([[-1069.1942, -1182.6810, -1097.7896,  4970.7169, -1882.7361,  -235.2887,
          1079.0188]], dtype=torch.float64)
	q_value: tensor([[-31.1816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.093488529073956, distance: 1.196640807787764 entropy 9.364981300934915
epoch: 15, step: 57
	action: tensor([[ 1173.9600, -6953.1820,  2683.5712,  4844.9017,  -118.8330,   865.7257,
         -1049.7149]], dtype=torch.float64)
	q_value: tensor([[-28.3411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31154075101107737, distance: 0.9495018443226554 entropy 9.308918068099162
epoch: 15, step: 58
	action: tensor([[-1484.9984, -1059.0405,  1950.5552,   892.2428,  1409.2901,   686.4778,
           590.7335]], dtype=torch.float64)
	q_value: tensor([[-21.8183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09839113652946585, distance: 1.1993203508108556 entropy 8.960327531561104
epoch: 15, step: 59
	action: tensor([[ 1930.1454,   537.5911, -5467.9060, -1094.3323,  -476.7540,  2825.3356,
         -4283.7571]], dtype=torch.float64)
	q_value: tensor([[-26.2421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.169522081878224
epoch: 15, step: 60
	action: tensor([[  129.1193,  -523.8776, -4914.4917,   -76.2673,  2922.7500, -1141.3953,
          -784.9432]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5541977229908914, distance: 1.4266249907108712 entropy 8.624138921056396
epoch: 15, step: 61
	action: tensor([[ -964.7900,   663.9048,  -474.3052, -1409.7974,   373.3035,  2345.5167,
         -1462.8344]], dtype=torch.float64)
	q_value: tensor([[-20.5866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5780212109802156, distance: 0.7433648198286598 entropy 8.843731702953667
epoch: 15, step: 62
	action: tensor([[-2624.4284, -1524.4105,  1504.1836,  2552.7823, -3203.0290,   988.4845,
           437.8950]], dtype=torch.float64)
	q_value: tensor([[-42.3447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36017511529665724, distance: 1.3346091739176622 entropy 9.19303723650568
epoch: 15, step: 63
	action: tensor([[ -746.4418, -5307.0467,   296.2280,  -578.4163,    48.1709, -1310.9118,
         -1317.4398]], dtype=torch.float64)
	q_value: tensor([[-27.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7010430699192911, distance: 1.4925000290390638 entropy 9.201538092761952
epoch: 15, step: 64
	action: tensor([[-3054.5940, -2095.1464,  -209.8755,   219.3891,  1193.9949, -3221.4537,
          2603.4330]], dtype=torch.float64)
	q_value: tensor([[-25.9216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.828671301290945, distance: 1.5474781224941658 entropy 9.152170859362043
epoch: 15, step: 65
	action: tensor([[-1473.3753, -2752.3594, -1017.1374,   853.2515,  -254.4066,  2378.6002,
          3029.1225]], dtype=torch.float64)
	q_value: tensor([[-23.9729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27657176618509904, distance: 1.292942796125557 entropy 9.12856212812611
epoch: 15, step: 66
	action: tensor([[-1948.9091, -2924.1581, -3364.3365,  2732.9630,   662.1038,  3132.6320,
          4938.6749]], dtype=torch.float64)
	q_value: tensor([[-25.8261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19985877073996416, distance: 1.2534925539954789 entropy 9.141139574880444
epoch: 15, step: 67
	action: tensor([[ -220.5750, -4550.3545,  1964.2012,  -400.0369,  1722.1744,  -116.0624,
          -791.1266]], dtype=torch.float64)
	q_value: tensor([[-27.7364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.501908856538095, distance: 1.8100577676335003 entropy 9.235938505745905
epoch: 15, step: 68
	action: tensor([[ -119.9138,   349.7525,  -727.2593, -2929.5874,  -895.7038, -1480.6344,
          2463.2574]], dtype=torch.float64)
	q_value: tensor([[-22.3947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47738645451806727, distance: 0.8272695214992375 entropy 8.872346628341448
epoch: 15, step: 69
	action: tensor([[-3251.9534, -3023.1132,  1878.6404,  1470.9533, -3377.9260,   103.0415,
           201.8652]], dtype=torch.float64)
	q_value: tensor([[-33.5417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.98104040786155, distance: 1.6106580973997247 entropy 9.210655729858106
epoch: 15, step: 70
	action: tensor([[-4290.8022, -2807.6401, -1398.8342, -1709.4638,   776.1694,  4197.5879,
          1143.6280]], dtype=torch.float64)
	q_value: tensor([[-21.8162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08665540629511093, distance: 1.0936390890857317 entropy 8.964834476442089
epoch: 15, step: 71
	action: tensor([[-2578.6228, -2095.2177, -4259.5098,  3773.8007, -2120.6525, -6405.3552,
         -1409.2740]], dtype=torch.float64)
	q_value: tensor([[-31.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38048396383132177, distance: 1.3445358184282141 entropy 9.256423615257983
epoch: 15, step: 72
	action: tensor([[-1041.0516, -2722.6864, -3216.7860,  1624.7174,  -227.1129,  2740.8330,
           618.1909]], dtype=torch.float64)
	q_value: tensor([[-25.4890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22421178078385506, distance: 1.266149449506703 entropy 9.162101771773521
epoch: 15, step: 73
	action: tensor([[  187.6531,    16.6267,  -937.0313, -3423.2775,  2893.4622,   604.0950,
          4235.3918]], dtype=torch.float64)
	q_value: tensor([[-25.0383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2718094544838907, distance: 0.9765156576591382 entropy 9.144527257401347
epoch: 15, step: 74
	action: tensor([[  956.7189,   790.2139, -3539.9062,   983.1353,  1369.3992, -3301.0279,
         -2908.7316]], dtype=torch.float64)
	q_value: tensor([[-23.2927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.01362543593672
epoch: 15, step: 75
	action: tensor([[-2894.6993,  -498.5398,    51.9741,  1788.6303,   866.9385,   206.8511,
          1823.7564]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1765793305979383, distance: 1.038406924408976 entropy 8.624138921056396
epoch: 15, step: 76
	action: tensor([[-2826.0426, -2383.7115, -2672.3552, -1007.1237, -4790.5272,  1648.6718,
         -3565.2889]], dtype=torch.float64)
	q_value: tensor([[-27.4286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2789248455049609, distance: 1.2941338753617961 entropy 9.214579334217074
epoch: 15, step: 77
	action: tensor([[-1604.8554,   107.0222, -2458.7814,  -331.1039, -2871.0772, -3417.4266,
          3529.6204]], dtype=torch.float64)
	q_value: tensor([[-32.2673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.278250325977568
epoch: 15, step: 78
	action: tensor([[-1624.6294,   172.8809, -1656.3341, -1739.6948, -2305.9385,   383.0429,
         -1060.1481]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.624138921056396
epoch: 15, step: 79
	action: tensor([[  271.9415, -3157.3730, -1452.0946,  2841.4973,  1292.3214,   496.0513,
           152.0975]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21326993922832016, distance: 1.2604784118989845 entropy 8.624138921056396
epoch: 15, step: 80
	action: tensor([[-1222.9676,  1430.1911, -1971.5062,   -38.1358,  2166.2413,  1303.8141,
           563.2504]], dtype=torch.float64)
	q_value: tensor([[-22.0092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.038628092675376
epoch: 15, step: 81
	action: tensor([[-1587.3171, -2704.3385,  -582.1223,  2566.2849,  -412.4965,  3960.9205,
         -1263.9360]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47355403101008564, distance: 1.3891198769138946 entropy 8.624138921056396
epoch: 15, step: 82
	action: tensor([[-2423.8702,   189.3539,  2103.6691, -1098.4506,  1855.3631,  1220.9488,
          2966.7913]], dtype=torch.float64)
	q_value: tensor([[-28.2156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24315145006319638, distance: 0.9955456632876146 entropy 9.225323557119347
epoch: 15, step: 83
	action: tensor([[ 2443.5045, -3557.2645,  -445.0004,   427.3038, -2948.2676,   154.5699,
           771.3185]], dtype=torch.float64)
	q_value: tensor([[-29.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4949893431650456, distance: 0.8132179659036228 entropy 8.871662367366051
epoch: 15, step: 84
	action: tensor([[ -247.3339, -3216.3529, -1429.3884,  2172.0829,   375.2991, -1016.7846,
          -283.8875]], dtype=torch.float64)
	q_value: tensor([[-22.3031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12011718670488947, distance: 1.0734185857751406 entropy 9.05463256449182
epoch: 15, step: 85
	action: tensor([[-3951.2231,  2855.6005, -1517.5206,  -958.6660,   283.9901,  3847.1491,
          2243.1289]], dtype=torch.float64)
	q_value: tensor([[-24.2451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017932814320857582, distance: 1.134037179733027 entropy 9.169347948460459
epoch: 15, step: 86
	action: tensor([[   66.9108,  -950.0761,  -586.9878, -1377.3054, -1359.0825,  -374.1541,
          2462.9224]], dtype=torch.float64)
	q_value: tensor([[-26.4994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009266905216283483, distance: 1.1496342915611406 entropy 8.920711468204471
epoch: 15, step: 87
	action: tensor([[-1015.8353,   584.2654,  1771.6962, -1238.1322, -3608.4499,   199.1491,
         -3823.6913]], dtype=torch.float64)
	q_value: tensor([[-33.2068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1516116838712599, distance: 1.054032574245964 entropy 9.165140294775993
epoch: 15, step: 88
	action: tensor([[-4539.1586, -3337.7066,  2990.0529,  2555.1689,  -990.3448, -4236.2822,
          2816.8336]], dtype=torch.float64)
	q_value: tensor([[-41.1790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4987645682941746, distance: 1.4009524725368738 entropy 9.073519376638462
epoch: 15, step: 89
	action: tensor([[ 2082.8913,  -624.7939, -4277.7287,  -987.3865,  1025.4594,  -143.3034,
          2301.4975]], dtype=torch.float64)
	q_value: tensor([[-27.1401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06514181722178947, distance: 1.1064443110620474 entropy 9.16204587691527
epoch: 15, step: 90
	action: tensor([[  369.3699,   655.2394,    42.0315,   644.6026,  1052.0123,  6370.6348,
         -2551.4918]], dtype=torch.float64)
	q_value: tensor([[-24.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6676842336479871, distance: 0.6596782516775184 entropy 8.9895520160326
epoch: 15, step: 91
	action: tensor([[-3719.3243, -3741.0340, -1602.2171,  1061.6394,  5770.9968, -1037.7938,
          1380.8441]], dtype=torch.float64)
	q_value: tensor([[-24.6698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2896041576385341, distance: 0.9645103643107058 entropy 8.92514424312011
epoch: 15, step: 92
	action: tensor([[-4440.2022, -1068.1667,  3790.2446, -2635.4634,  -755.4913,  2351.9674,
          2969.9719]], dtype=torch.float64)
	q_value: tensor([[-25.4009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9020658794000711, distance: 1.5782270065941504 entropy 9.162046875447126
epoch: 15, step: 93
	action: tensor([[ -921.9074, -1532.5799,    20.7305, -1194.1985,   256.7168,  -759.1132,
           862.6909]], dtype=torch.float64)
	q_value: tensor([[-25.9569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05443259435722303, distance: 1.1750764008892696 entropy 9.032736620828809
epoch: 15, step: 94
	action: tensor([[   78.4245, -2313.2677,   742.9485,  2664.5649, -2506.3409,  1790.2460,
          1395.6557]], dtype=torch.float64)
	q_value: tensor([[-27.2106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16826527309894912, distance: 1.0436361418590325 entropy 9.21491074396765
epoch: 15, step: 95
	action: tensor([[-2442.0546,   -52.9546,  1264.7427,  1310.8337,   192.1433,  -938.9004,
         -1180.5113]], dtype=torch.float64)
	q_value: tensor([[-20.8584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3320390897261717, distance: 0.9352596915002335 entropy 8.82804432809435
epoch: 15, step: 96
	action: tensor([[ 1077.9859, -2176.0632,  1776.6572,  3033.5239, -1953.2071,  2750.2991,
          5252.6872]], dtype=torch.float64)
	q_value: tensor([[-27.6776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3168067293019776, distance: 0.9458635359390538 entropy 9.238494207449211
epoch: 15, step: 97
	action: tensor([[-1645.5041, -4083.5228,    58.8542,   -48.5987,  2109.7926,  4011.6792,
         -2184.3689]], dtype=torch.float64)
	q_value: tensor([[-27.3247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43534350942364164, distance: 0.8599017951100243 entropy 9.112422581343724
epoch: 15, step: 98
	action: tensor([[-1900.9500, -3964.7308,  3941.1444,   622.5582, -1347.8698,  3943.3306,
         -2872.6999]], dtype=torch.float64)
	q_value: tensor([[-24.8063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11138005700486242, distance: 1.206390734380799 entropy 9.095704142436578
epoch: 15, step: 99
	action: tensor([[ -585.2106, -3760.8179, -2061.1535,  2836.0488,  -133.2312,  -473.3017,
          2420.7269]], dtype=torch.float64)
	q_value: tensor([[-26.6875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10962542758172344, distance: 1.0797993648200803 entropy 9.250686543404044
epoch: 15, step: 100
	action: tensor([[  293.2877, -1007.9245,  1683.6186, -1064.8698,  2380.1851,  1955.6809,
          5363.8049]], dtype=torch.float64)
	q_value: tensor([[-19.8180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039234650870242915, distance: 1.1216706574014237 entropy 8.990552705807326
epoch: 15, step: 101
	action: tensor([[-1527.2828, -1506.5923,   473.9100,   597.4000,  -485.8512,  -252.6364,
          1834.5526]], dtype=torch.float64)
	q_value: tensor([[-19.6362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5840172427119882, distance: 0.7380645698517815 entropy 8.859421022775024
epoch: 15, step: 102
	action: tensor([[ -617.5624, -2486.1884, -4397.1558,   756.8738,   329.9449,   -31.0130,
         -3100.8484]], dtype=torch.float64)
	q_value: tensor([[-21.3454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5482418275385508, distance: 0.7691476229445281 entropy 9.093865835389748
epoch: 15, step: 103
	action: tensor([[  394.8784,   -83.4375, -3069.1441, -1358.1863, -4222.5717,  1142.2825,
          2586.3370]], dtype=torch.float64)
	q_value: tensor([[-26.0466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060383761294953375, distance: 1.1092564179776747 entropy 9.313326432046823
epoch: 15, step: 104
	action: tensor([[-2022.5060, -2113.9421,   539.6751,  2940.2311,  2695.1769,  2443.1055,
            86.1695]], dtype=torch.float64)
	q_value: tensor([[-23.8564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2842293740219066, distance: 0.9681521831697163 entropy 9.065894009279239
epoch: 15, step: 105
	action: tensor([[-1937.2120, -1217.9260, -1864.0333,  3986.4146,   282.3102,  5736.3874,
         -1635.5824]], dtype=torch.float64)
	q_value: tensor([[-27.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3175196199135324, distance: 0.9453699177658995 entropy 9.22621130103717
epoch: 15, step: 106
	action: tensor([[-2908.7994, -3173.4991,  7508.5804,  1677.2102,   201.4543,  1523.7103,
          2957.5842]], dtype=torch.float64)
	q_value: tensor([[-27.9946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26177671744054287, distance: 0.983219679067992 entropy 9.276317312530045
epoch: 15, step: 107
	action: tensor([[-2826.0744,  -786.1953,    66.4795, -1431.3517,  -970.8900,  2775.5024,
          1799.8985]], dtype=torch.float64)
	q_value: tensor([[-24.5587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5547794077698394, distance: 1.4268919349994378 entropy 9.122923073091348
epoch: 15, step: 108
	action: tensor([[  218.3387, -1698.2471,  2143.0913,  -581.7445, -1517.8758,  1737.6321,
          3546.7226]], dtype=torch.float64)
	q_value: tensor([[-29.4717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29138052234488476, distance: 0.9633037169307826 entropy 9.30825187483797
epoch: 15, step: 109
	action: tensor([[ -995.0518,   743.3914, -3859.7300,  1379.3822,  5006.9512, -1463.6296,
         -2154.3102]], dtype=torch.float64)
	q_value: tensor([[-28.7602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14552070943088957, distance: 1.0578095024123337 entropy 9.263034806824171
epoch: 15, step: 110
	action: tensor([[  418.8953, -2826.6983,  1409.4851,  -499.0512,  2174.0018,  -424.7248,
         -2540.6972]], dtype=torch.float64)
	q_value: tensor([[-26.6241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.882284280311106
epoch: 15, step: 111
	action: tensor([[-1310.5832,  -371.6470,  2559.0575,  -676.0086,   610.5605, -1655.1262,
          -428.7636]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04667728324237652, distance: 1.1173176628248056 entropy 8.624138921056396
epoch: 15, step: 112
	action: tensor([[ -725.7122, -4448.2277, -1811.4568, -4851.4792,  4320.6917,   972.9924,
          5930.7296]], dtype=torch.float64)
	q_value: tensor([[-38.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24351501054549096, distance: 0.9953065239396226 entropy 9.245089033045032
epoch: 15, step: 113
	action: tensor([[-3119.2829, -1339.1627,  -291.6616, -1817.7131, -3977.9752,  -431.0686,
           587.0949]], dtype=torch.float64)
	q_value: tensor([[-34.7069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16052517308580283, distance: 1.2327754466118164 entropy 9.201153969215296
epoch: 15, step: 114
	action: tensor([[  -60.8050, -4758.5174,  1623.9793,   519.3033,  -567.4215,  2208.4376,
          -350.3621]], dtype=torch.float64)
	q_value: tensor([[-35.3351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.900168229213067, distance: 1.5774395285756442 entropy 9.236645109799232
epoch: 15, step: 115
	action: tensor([[ -524.7035,  -179.3395,   224.6342,  3991.5022, -3366.7439,  -399.9448,
           857.4703]], dtype=torch.float64)
	q_value: tensor([[-26.0596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7038258135082676, distance: 1.4937203237426386 entropy 9.172728037361448
epoch: 15, step: 116
	action: tensor([[ -565.8258, -1265.9495,  -453.9215, -3323.3095,   480.2163,  -724.9770,
          1967.8842]], dtype=torch.float64)
	q_value: tensor([[-23.9077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8776204826203835, distance: 1.5680525042715892 entropy 9.102930027064016
epoch: 15, step: 117
	action: tensor([[ 2532.3435, -1436.9692,  2418.7726,  4841.7705,  -394.4788, -1288.4946,
         -1879.0126]], dtype=torch.float64)
	q_value: tensor([[-27.4962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3355857883491714, distance: 0.9327733939105306 entropy 9.125118291201138
epoch: 15, step: 118
	action: tensor([[-1008.2243, -1615.6845, -3671.7438,   716.2335,  -853.7545, -3168.4040,
           872.4942]], dtype=torch.float64)
	q_value: tensor([[-23.5225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6233351115065611, distance: 1.4580109457383201 entropy 8.913664601046795
epoch: 15, step: 119
	action: tensor([[-1172.1025, -2287.5859,    62.2750,  1310.9631, -4384.3413,  4144.7846,
          2498.8122]], dtype=torch.float64)
	q_value: tensor([[-22.6050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26236858639513305, distance: 1.2857300153939055 entropy 9.066788307507986
epoch: 15, step: 120
	action: tensor([[ 1408.5039, -2102.7901, -1338.8117,  5830.3916,  3740.8462,  1491.8665,
         -5113.4161]], dtype=torch.float64)
	q_value: tensor([[-25.4211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46908186319312317, distance: 0.8338164791820419 entropy 9.164644315498501
epoch: 15, step: 121
	action: tensor([[-359.4120,  156.9146, 1758.1940,  752.2039, 2359.4438,  827.3803,
         -323.4250]], dtype=torch.float64)
	q_value: tensor([[-27.0369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5080463173133913, distance: 1.4052837796297417 entropy 9.113928581062746
epoch: 15, step: 122
	action: tensor([[ 3357.3685, -4041.6972, -3505.5329,  3343.2700,   227.5607,  -268.1260,
           312.5008]], dtype=torch.float64)
	q_value: tensor([[-29.1683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.393951210921964
epoch: 15, step: 123
	action: tensor([[-1358.4858, -2709.6200,   232.6193,  2433.3208,  -882.2288,  1707.2943,
          1997.6742]], dtype=torch.float64)
	q_value: tensor([[-31.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.743855309934129, distance: 1.511165107689815 entropy 8.624138921056396
epoch: 15, step: 124
	action: tensor([[  958.0767, -2742.9674, -2522.3571,   717.2162,  -427.7246,  3551.6038,
          -957.2271]], dtype=torch.float64)
	q_value: tensor([[-26.6206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2878524147212882, distance: 0.9656988101135602 entropy 9.229547920684869
epoch: 15, step: 125
	action: tensor([[-1606.2030, -3088.6304, -3491.2622,  -523.6270,  1038.8475,  3030.4886,
          3811.2682]], dtype=torch.float64)
	q_value: tensor([[-23.7753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8061708612174132, distance: 1.5379283723401764 entropy 9.249381517186261
epoch: 15, step: 126
	action: tensor([[-1074.9252,   243.1432,  1717.6973,  -710.0455,  3625.9309,  2945.2944,
          -543.6867]], dtype=torch.float64)
	q_value: tensor([[-26.8154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1447898871696489, distance: 1.2243894685956136 entropy 9.198076896007603
epoch: 15, step: 127
	action: tensor([[ -306.1723,  -908.1232,  1212.9202, -1988.4583,  -802.1921,  2826.6124,
           399.7614]], dtype=torch.float64)
	q_value: tensor([[-30.6821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17434206669214058, distance: 1.0398166621710936 entropy 9.196685267578266
LOSS epoch 15 actor 331.98240771045533 critic 235.9898890794567
epoch: 16, step: 0
	action: tensor([[-7142.1164, -2643.9022, -4503.0235,  -442.6715,  2143.1330, -1272.3073,
          2960.7770]], dtype=torch.float64)
	q_value: tensor([[-27.8375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01857123486637846, distance: 1.1336685133389568 entropy 9.298917592138555
epoch: 16, step: 1
	action: tensor([[-2158.3830,  -702.9430, -4977.5757,  -248.5055,  4318.5626,   392.4230,
         -2351.1082]], dtype=torch.float64)
	q_value: tensor([[-22.8324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2947498719355537, distance: 1.302115868752468 entropy 9.253104232956636
epoch: 16, step: 2
	action: tensor([[ 2302.6470,  2475.6175,  3335.2149,  2462.6737,  -103.8151, -1511.7127,
          2916.8270]], dtype=torch.float64)
	q_value: tensor([[-30.0713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.284689577029948
epoch: 16, step: 3
	action: tensor([[ 2243.3494, -1015.4502,   115.5234,  -395.4129, -1739.7818,    10.3950,
          1860.0281]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.708576764229536
epoch: 16, step: 4
	action: tensor([[-2350.4037,  2222.0253,  1406.0858,  1977.1221, -3945.5299,  -831.5804,
           -51.9118]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05499935765073327, distance: 1.112430135061522 entropy 8.708576764229536
epoch: 16, step: 5
	action: tensor([[ -593.2766,  -486.1245,   214.1742,   555.2952, -1938.5867,  2642.1890,
          3370.0345]], dtype=torch.float64)
	q_value: tensor([[-24.3236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6645647417939591, distance: 1.47641020683649 entropy 9.06194309520099
epoch: 16, step: 6
	action: tensor([[ -407.6667, -1352.0352,   959.8794, -2738.1903,  4130.6014,  -258.9467,
         -3205.7107]], dtype=torch.float64)
	q_value: tensor([[-27.8502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0751993341611525, distance: 1.648491085515138 entropy 9.37164275448413
epoch: 16, step: 7
	action: tensor([[-4078.8159, -4850.7558, -1369.1713,  2574.2112,   105.0291, -2009.2106,
          1079.3272]], dtype=torch.float64)
	q_value: tensor([[-24.0547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052638748958498516, distance: 1.113818691974179 entropy 9.17873498123372
epoch: 16, step: 8
	action: tensor([[-6868.6260, -2079.9751,  3770.5392,  2000.0496,   624.7748,  1328.2289,
           -13.1920]], dtype=torch.float64)
	q_value: tensor([[-21.3759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4661232004856546, distance: 1.385612926910807 entropy 9.254524925603988
epoch: 16, step: 9
	action: tensor([[-1766.2269,  2185.4631, -4444.9776,  -186.6924,  -582.4103,  1065.5115,
          -291.8003]], dtype=torch.float64)
	q_value: tensor([[-22.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06422603108484926, distance: 1.180520769037115 entropy 9.226868717869964
epoch: 16, step: 10
	action: tensor([[-2491.9164, -2095.4457,  2458.2262, -1517.4696,  2757.2003,  1571.5883,
         -1644.4626]], dtype=torch.float64)
	q_value: tensor([[-26.5936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8588041778441142, distance: 1.5601757137428545 entropy 9.216694956690093
epoch: 16, step: 11
	action: tensor([[  696.8311, -2201.3581, -1112.9947,  6111.6860,  2261.3525, -4478.9360,
         -2019.9779]], dtype=torch.float64)
	q_value: tensor([[-23.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09876861148860128, distance: 1.1995264133403938 entropy 9.230016830113168
epoch: 16, step: 12
	action: tensor([[-4837.1608, -3860.6744,   957.0526, -3494.5781,  -905.4278, -1508.5562,
          -742.3456]], dtype=torch.float64)
	q_value: tensor([[-20.1776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16440890798798913, distance: 1.0460527703607596 entropy 9.0932574756874
epoch: 16, step: 13
	action: tensor([[-3915.6601,  4371.0109,  2544.4294, -1984.5544,  -438.9826,  2227.6920,
           614.1433]], dtype=torch.float64)
	q_value: tensor([[-24.8492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27593454587963306, distance: 0.9737458212544241 entropy 9.165477832890407
epoch: 16, step: 14
	action: tensor([[-4196.1746, -1828.4463,  2221.8017,   190.3586, -3592.4695,  3045.8514,
         -1234.3515]], dtype=torch.float64)
	q_value: tensor([[-32.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9259781238337961, distance: 1.588116537557275 entropy 9.056551144740935
epoch: 16, step: 15
	action: tensor([[ -442.4007,   943.1787, -4149.9429, -1195.8941,  1215.8604, -1492.1754,
          3854.2546]], dtype=torch.float64)
	q_value: tensor([[-26.6913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6782988781581324, distance: 0.6490572135201347 entropy 9.276498003201358
epoch: 16, step: 16
	action: tensor([[ 2991.9810,  -819.5156, -1049.8533,   115.4673,   833.2631, -5776.0182,
          2074.3302]], dtype=torch.float64)
	q_value: tensor([[-41.8385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.563640242774264
epoch: 16, step: 17
	action: tensor([[-1207.0682,   867.1997,   -51.8174,  -906.1642,   544.7653,   972.7019,
          1666.0250]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.708576764229536
epoch: 16, step: 18
	action: tensor([[  276.3587, -1191.6455, -2045.6889,   157.3494,   842.8218,  -285.9253,
         -1657.4713]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10447737469986051, distance: 1.0829165091974662 entropy 8.708576764229536
epoch: 16, step: 19
	action: tensor([[-4285.3748, -3673.9813,  1149.6240,  4896.9081,  1867.0750,  3049.9924,
         -2515.1301]], dtype=torch.float64)
	q_value: tensor([[-22.8283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19967899538102807, distance: 1.2533986448117862 entropy 9.176577734041997
epoch: 16, step: 20
	action: tensor([[ -996.3557,   348.4836, -1851.0111,  2071.6423,  2982.7901,  2202.7554,
          1558.0373]], dtype=torch.float64)
	q_value: tensor([[-22.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15149969836464794, distance: 1.0541021370126158 entropy 9.170084711498214
epoch: 16, step: 21
	action: tensor([[  -72.4712, -2755.2442,  1823.8438,  1716.8515, -1216.7468, -2050.4863,
           250.8317]], dtype=torch.float64)
	q_value: tensor([[-29.6176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09365858992796527, distance: 1.0894382138201901 entropy 9.34119050096603
epoch: 16, step: 22
	action: tensor([[-2551.3629,  -956.6727, -3022.6983, -2878.3922,  2201.2703, -1184.2871,
         -1427.5130]], dtype=torch.float64)
	q_value: tensor([[-21.5424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9117921111823943, distance: 1.5822570006388936 entropy 9.219345757337948
epoch: 16, step: 23
	action: tensor([[  308.4219, -1336.0612, -2883.4812,  1386.7273,  -435.3153,   -63.4055,
           570.3026]], dtype=torch.float64)
	q_value: tensor([[-21.5876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42244114335556426, distance: 0.8696706573814511 entropy 9.073977924316702
epoch: 16, step: 24
	action: tensor([[ 1702.7323, -1523.9208,  3104.0018,  3521.6362, -1010.9536, -1344.3363,
           -69.2231]], dtype=torch.float64)
	q_value: tensor([[-20.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4024724401636349, distance: 0.8845770423368425 entropy 9.164167457533631
epoch: 16, step: 25
	action: tensor([[-2395.7514, -1820.1567,  -353.7695,  3865.9143, -1416.6427, -1079.3850,
         -2189.3772]], dtype=torch.float64)
	q_value: tensor([[-21.3762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24469623390446216, distance: 0.9945291531818252 entropy 8.986018888813977
epoch: 16, step: 26
	action: tensor([[-2926.2453, -2387.6801, -7038.7546,  -222.6952,  3575.9540,  3268.2861,
           371.9455]], dtype=torch.float64)
	q_value: tensor([[-24.4981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.08740752461238, distance: 1.6533329291548473 entropy 9.364685507060743
epoch: 16, step: 27
	action: tensor([[-1527.9207,  2789.4084, -2380.4214,  2759.2513,  3870.7069,   395.3484,
           895.9031]], dtype=torch.float64)
	q_value: tensor([[-25.1201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24011611557018553, distance: 0.997539979331316 entropy 9.26392424689637
epoch: 16, step: 28
	action: tensor([[-2022.0732,  1446.8812, -2321.9028,  -809.9324, -2128.5519,  1777.5397,
         -1025.3950]], dtype=torch.float64)
	q_value: tensor([[-26.0321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08388808441732332, distance: 1.1913761842367667 entropy 9.183649956659456
epoch: 16, step: 29
	action: tensor([[ 1380.0667, -1732.8684,  -390.3152, -1301.4490,   520.6629, -3206.2146,
          1095.0079]], dtype=torch.float64)
	q_value: tensor([[-25.7380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47798552970894903, distance: 0.8267952334152056 entropy 9.075125654876267
epoch: 16, step: 30
	action: tensor([[-2516.8380, -5499.3176, -2668.8839,   958.8150,  -941.9882,  3928.4462,
          1921.3557]], dtype=torch.float64)
	q_value: tensor([[-23.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36823287718227427, distance: 1.3385564903435336 entropy 9.255223215820962
epoch: 16, step: 31
	action: tensor([[   43.1503, -2145.9623,  1145.8879,   682.9778,  1150.6740,   916.1216,
           844.4386]], dtype=torch.float64)
	q_value: tensor([[-20.7170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006722883541146918, distance: 1.1404911204615158 entropy 9.0403282272958
epoch: 16, step: 32
	action: tensor([[-2237.6570,   956.5817,  3826.1165,  1968.8853,  -657.9506, -3109.9444,
          1141.6631]], dtype=torch.float64)
	q_value: tensor([[-25.6838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5504252180430071, distance: 0.7672866896491882 entropy 9.330170481151653
epoch: 16, step: 33
	action: tensor([[ -668.6292, -4664.5480,  -404.1314,   -14.3156,  2060.6770,   696.8294,
          4490.7060]], dtype=torch.float64)
	q_value: tensor([[-23.3839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5339995695612054, distance: 1.821629126283323 entropy 9.172443933697656
epoch: 16, step: 34
	action: tensor([[-1729.6184,   902.9067, -2214.6948,  -438.1117,   615.6967,  2504.7328,
         -3168.1698]], dtype=torch.float64)
	q_value: tensor([[-18.7501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5815666826539012, distance: 0.7402353517094732 entropy 8.926464973022405
epoch: 16, step: 35
	action: tensor([[-4296.4652, -4640.4152,  2113.4606,  1291.1579,  -121.1010,  1201.7643,
         -3771.0646]], dtype=torch.float64)
	q_value: tensor([[-39.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5951239317787784, distance: 1.4452863755714875 entropy 9.210684159373557
epoch: 16, step: 36
	action: tensor([[ -161.5738,  1740.7421,  1475.5513,  4204.9181,   168.6834, -3287.4836,
          6416.1004]], dtype=torch.float64)
	q_value: tensor([[-23.6712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32052522366847214, distance: 1.3150129928917895 entropy 9.208334895754035
epoch: 16, step: 37
	action: tensor([[-4409.9514, -3236.2631, -2142.7838, -1632.7951, -1313.7921,   779.3340,
          4450.9042]], dtype=torch.float64)
	q_value: tensor([[-29.4757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.079997796734994, distance: 1.650395879730302 entropy 9.343239519889593
epoch: 16, step: 38
	action: tensor([[-1129.6137, -3207.0803,   719.8222,    79.3547,  -993.9823,   602.1668,
          -419.1506]], dtype=torch.float64)
	q_value: tensor([[-20.8971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08591324151708712, distance: 1.1924886597909645 entropy 9.007252086640646
epoch: 16, step: 39
	action: tensor([[  283.6822, -4453.3439, -2963.6089, -2820.4314, -5290.0509,   756.1350,
          3177.2248]], dtype=torch.float64)
	q_value: tensor([[-26.5257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2171026499529478, distance: 1.2624677630209782 entropy 9.332283048013897
epoch: 16, step: 40
	action: tensor([[-1476.5483,   421.3557,  3135.9763,  3665.3442, -2750.1188,  -787.9625,
            55.2370]], dtype=torch.float64)
	q_value: tensor([[-21.8182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.034065421737295765, distance: 1.1246840854684965 entropy 9.115850754338053
epoch: 16, step: 41
	action: tensor([[-1647.0029,  1489.8822, -1661.7209,  1720.8617, -1358.0618, -1307.6130,
         -1038.3603]], dtype=torch.float64)
	q_value: tensor([[-18.0444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014101621692551136, distance: 1.1362470518498275 entropy 8.850560930178263
epoch: 16, step: 42
	action: tensor([[-4167.5205, -4932.8977, -2984.3997, -2559.3228,   979.2718,  3081.3392,
          7801.3971]], dtype=torch.float64)
	q_value: tensor([[-22.2596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0312361484571482, distance: 1.6309359344454513 entropy 9.182819611642634
epoch: 16, step: 43
	action: tensor([[-2571.4081, -4034.1763,  2482.5838,   697.8755,  5531.5823,  2292.7771,
         -5121.0052]], dtype=torch.float64)
	q_value: tensor([[-24.7013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7857195069700658, distance: 1.5291965681034292 entropy 9.174813439412395
epoch: 16, step: 44
	action: tensor([[-4156.1144, -5187.8128, -1131.4594, -1534.6654,  -110.0059, -2822.8642,
           -37.0331]], dtype=torch.float64)
	q_value: tensor([[-22.9255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9615513665515494, distance: 1.602715864973245 entropy 9.248827855654765
epoch: 16, step: 45
	action: tensor([[ -563.9880,  1408.1434,  2252.3619, -2130.3982,  3603.2738,    52.3629,
          -475.8325]], dtype=torch.float64)
	q_value: tensor([[-24.2540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6300944054475276, distance: 0.6959886273047357 entropy 9.210243412188477
epoch: 16, step: 46
	action: tensor([[ 2030.3873, -3374.2469,  -721.8217,  -239.4929,  2292.4444, -2046.7076,
          2282.2773]], dtype=torch.float64)
	q_value: tensor([[-37.2830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05386760192340034, distance: 1.113096072413183 entropy 9.290572369648814
epoch: 16, step: 47
	action: tensor([[-1568.0343,  -615.5024,  3463.3620,  -843.6317,  1211.6814, -1913.6563,
         -1562.7103]], dtype=torch.float64)
	q_value: tensor([[-20.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05250712057403362, distance: 1.1138960674540304 entropy 9.093028109991844
epoch: 16, step: 48
	action: tensor([[ 1813.6796, -3426.0875, -3726.8981,  4039.8019,  1245.3153, -2952.1807,
         -4098.1144]], dtype=torch.float64)
	q_value: tensor([[-23.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10983104437506763, distance: 1.2055497232128258 entropy 9.206665662507286
epoch: 16, step: 49
	action: tensor([[-1625.6334,   979.2074, -1187.9029,  3042.6171,  1570.4768, -1239.4332,
         -1092.5861]], dtype=torch.float64)
	q_value: tensor([[-17.9820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.782481840223495
epoch: 16, step: 50
	action: tensor([[ -796.8042,  -704.5651,   140.1403,  -405.2497,   190.8028, -1564.7754,
         -2491.1164]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4766099589377748, distance: 1.3905595430268607 entropy 8.708576764229536
epoch: 16, step: 51
	action: tensor([[  -98.6704, -7564.8109,  3449.5986,  1617.3737,  1152.2813,  5247.4400,
         -7115.0497]], dtype=torch.float64)
	q_value: tensor([[-32.2460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8876312362501995, distance: 1.5722270752034855 entropy 9.36253676340964
epoch: 16, step: 52
	action: tensor([[-3892.4127, -4265.1775,  3103.9004,  3852.4568,  -957.8555,   581.7800,
         -3067.9437]], dtype=torch.float64)
	q_value: tensor([[-25.6129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8392252079988771, distance: 1.5519372188706662 entropy 9.309578990522473
epoch: 16, step: 53
	action: tensor([[-4764.7988, -5536.8171,  1348.2075,  5272.9560,  1735.3535, -2899.5488,
          1809.8267]], dtype=torch.float64)
	q_value: tensor([[-25.3681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.412267692981092, distance: 1.3599257758826928 entropy 9.402759317341193
epoch: 16, step: 54
	action: tensor([[-1671.2778, -4091.0248,  2824.0327,  2854.5320, -3436.0390,  4232.4946,
         -2836.5105]], dtype=torch.float64)
	q_value: tensor([[-19.6684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.038944311767934225, distance: 1.1664142804741267 entropy 9.13942505290306
epoch: 16, step: 55
	action: tensor([[-2698.7437, -3146.8687, -1289.6456,   612.8916,  -985.4673,  -866.1113,
          1519.7246]], dtype=torch.float64)
	q_value: tensor([[-24.1622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4319805017878182, distance: 1.3693839883863095 entropy 9.3278906004612
epoch: 16, step: 56
	action: tensor([[-3475.8245, -2790.6969,   342.1829,  1159.9671, -1738.0099,  2770.9494,
          -938.9563]], dtype=torch.float64)
	q_value: tensor([[-20.3610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41065172936029093, distance: 1.359147517061545 entropy 9.191854315166077
epoch: 16, step: 57
	action: tensor([[-3493.4931, -1987.9190, -1571.5547, -2311.9174,  2251.9109,  1642.9241,
           849.3215]], dtype=torch.float64)
	q_value: tensor([[-26.1290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4398476179193318, distance: 1.3731404455617222 entropy 9.45041271842654
epoch: 16, step: 58
	action: tensor([[ 8672.1267,  -134.9388,  3108.3540, -1035.9565, -1139.4015,  -587.6022,
          3233.4588]], dtype=torch.float64)
	q_value: tensor([[-26.4454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4759910350918466, distance: 0.8283732227152526 entropy 9.267646563854703
epoch: 16, step: 59
	action: tensor([[-3018.0464, -5102.5812,   707.1227,  2044.7005,  1637.5433, -1363.5012,
          2815.6312]], dtype=torch.float64)
	q_value: tensor([[-27.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5848620580331978, distance: 1.4406299107685243 entropy 9.357605506114163
epoch: 16, step: 60
	action: tensor([[-5675.7034,  1944.6315,  -633.8183, -2733.3336,  -632.7863, -4315.9770,
          1846.5992]], dtype=torch.float64)
	q_value: tensor([[-24.9092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4761099678212096, distance: 0.8282792106984962 entropy 9.350805098006983
epoch: 16, step: 61
	action: tensor([[ 1258.5994,   119.0538,  1554.2833,  3234.4601,   385.6151, -3799.8218,
           261.0969]], dtype=torch.float64)
	q_value: tensor([[-29.3436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6942099629420861, distance: 0.6328027517836754 entropy 9.135595356199987
epoch: 16, step: 62
	action: tensor([[-2461.5595, -3630.2363, -1827.3547,   219.1389,   -44.5993,  2187.2700,
          2635.6254]], dtype=torch.float64)
	q_value: tensor([[-24.8125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29013943865220604, distance: 0.9641469180839854 entropy 9.115932752331583
epoch: 16, step: 63
	action: tensor([[ 2535.4001,  1558.6401, -3285.6204,  4341.9485, -1576.1307, -3317.1289,
         -1212.4710]], dtype=torch.float64)
	q_value: tensor([[-22.4436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2678251528711457, distance: 0.9791835207604462 entropy 9.250786036901998
epoch: 16, step: 64
	action: tensor([[-3421.3592, -3324.0936,  -903.7310,   653.0527, -1319.4401, -2720.0855,
          -789.1040]], dtype=torch.float64)
	q_value: tensor([[-19.5543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.93097594755427
epoch: 16, step: 65
	action: tensor([[  73.7979,  678.6558,  148.9848, 1083.7602, 1480.1833, 1457.6757,
         -620.6352]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.708576764229536
epoch: 16, step: 66
	action: tensor([[-1463.1347,  -204.8841, -1550.5833,  2165.8352,  -479.7643,  1681.1972,
          1933.3457]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9992819544994413, distance: 1.618056626225892 entropy 8.708576764229536
epoch: 16, step: 67
	action: tensor([[-2573.4272,   191.6404,  -537.6591,   265.7246, -1755.9696,  -217.4206,
          1950.8750]], dtype=torch.float64)
	q_value: tensor([[-20.6912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19663387646814723, distance: 1.2518068969342135 entropy 9.152580915310688
epoch: 16, step: 68
	action: tensor([[ -301.9149, -2981.1010,  2834.3238,  2342.3970, -4638.6191,   531.1765,
         -1907.8883]], dtype=torch.float64)
	q_value: tensor([[-26.9795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19602428182732656, distance: 1.0260727629105066 entropy 9.090147785433562
epoch: 16, step: 69
	action: tensor([[ -426.7063,  1463.3433,  -301.5004,  4679.9803, -1660.4033,   -88.1236,
            -6.8041]], dtype=torch.float64)
	q_value: tensor([[-29.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5822423803907966, distance: 1.4394387828782815 entropy 9.06917973708166
epoch: 16, step: 70
	action: tensor([[-3350.5373, -1629.3287,  -628.1249,   672.4951,   -69.1393,  2274.8194,
          1882.4012]], dtype=torch.float64)
	q_value: tensor([[-21.8036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.030146496248465
epoch: 16, step: 71
	action: tensor([[-1688.2093, -3031.8174,  -181.0792,   532.6064,  -978.5708, -1110.3908,
          1806.4814]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4526587908572002, distance: 1.3792357371901538 entropy 8.708576764229536
epoch: 16, step: 72
	action: tensor([[   75.7524, -1265.9251,  4192.2483,  1524.9104,  -877.6568,  3113.6400,
         -1008.2413]], dtype=torch.float64)
	q_value: tensor([[-23.6791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.344626610487503, distance: 1.7522397901276525 entropy 9.384086158114489
epoch: 16, step: 73
	action: tensor([[-2829.4663,  -513.8538,    13.4710,  4460.2083,  -914.6805,   573.3010,
          5420.0543]], dtype=torch.float64)
	q_value: tensor([[-24.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4037071292984067, distance: 0.8836626556875642 entropy 9.302649313591507
epoch: 16, step: 74
	action: tensor([[-2007.3602, -3310.5822,  3415.6603, -3321.8847,    33.4918,   660.5452,
          5076.0392]], dtype=torch.float64)
	q_value: tensor([[-22.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.257889916810756
epoch: 16, step: 75
	action: tensor([[  544.7542,  -973.6768, -1035.8821,   714.5030,  -542.2261,  4874.0265,
           426.8707]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1274351999317036, distance: 1.2150733291945741 entropy 8.708576764229536
epoch: 16, step: 76
	action: tensor([[ -927.7655, -3269.2250, -1656.5167,    90.6370,   -18.3000,  1357.3423,
           970.7660]], dtype=torch.float64)
	q_value: tensor([[-19.3006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.954252150863585
epoch: 16, step: 77
	action: tensor([[  729.1423, -2803.0909,  1843.9662,  1839.2998,   127.2316,   558.8105,
          -580.7982]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40513614332389936, distance: 1.3564878112630636 entropy 8.708576764229536
epoch: 16, step: 78
	action: tensor([[-2132.0891, -2246.3999,   951.4521,  1354.4278,  4801.2305,   512.5879,
          -890.8403]], dtype=torch.float64)
	q_value: tensor([[-23.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.995629266842759
epoch: 16, step: 79
	action: tensor([[-1114.6807,  -843.0404,  -607.1454,  2631.4199, -1683.3317,  1228.8553,
         -1968.1510]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.708576764229536
epoch: 16, step: 80
	action: tensor([[ -291.2856, -1055.0336,  1018.1760,  2635.6497, -1808.2436,  1278.9764,
            18.6105]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.708576764229536
epoch: 16, step: 81
	action: tensor([[-1787.6604,  -643.2133,  -162.1510,   831.6936,  2879.2284, -1472.1969,
          1365.1788]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23531498880175583, distance: 1.2718782737853827 entropy 8.708576764229536
epoch: 16, step: 82
	action: tensor([[-3856.4883, -4073.0021,   291.0970,  3324.3210,  1750.4648,  1817.8397,
           987.4076]], dtype=torch.float64)
	q_value: tensor([[-28.8541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7779566073761206, distance: 1.525869077720629 entropy 9.136574025628255
epoch: 16, step: 83
	action: tensor([[-3207.2156, -2794.3930, -1851.8812,   620.5006, -2796.5881, -1101.0195,
           659.5004]], dtype=torch.float64)
	q_value: tensor([[-25.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28013734273661917, distance: 1.2947471881826478 entropy 9.15425201883638
epoch: 16, step: 84
	action: tensor([[  726.3965, -1047.1315,  2432.1695, -2747.5807,  -427.6727, -2065.0396,
          -277.1562]], dtype=torch.float64)
	q_value: tensor([[-19.2546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12157712721166802, distance: 1.0725276842527949 entropy 8.777210915697069
epoch: 16, step: 85
	action: tensor([[ -833.6302, -3649.3532, -2957.8056,  1684.6599, -1314.7910,   311.3430,
          2266.7576]], dtype=torch.float64)
	q_value: tensor([[-17.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24913963076715806, distance: 0.9915994668765842 entropy 9.080036273763996
epoch: 16, step: 86
	action: tensor([[-5081.0611,  2959.7591,  -724.6483,   818.4023, -2807.8836, -1167.6032,
          2924.1092]], dtype=torch.float64)
	q_value: tensor([[-24.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.086260589954033
epoch: 16, step: 87
	action: tensor([[-2291.2643, -2854.3579,   699.4344,   802.4471, -1076.9268,   510.0903,
           -72.9077]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.708576764229536
epoch: 16, step: 88
	action: tensor([[-1707.1603,  -597.4586,  -710.4389,  -829.1007, -1095.6381,  1137.9994,
           -80.2134]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014177670859706115, distance: 1.1362032277041023 entropy 8.708576764229536
epoch: 16, step: 89
	action: tensor([[-4201.0351,   888.0204, -2631.1985,  4692.0356,  1810.8947,  -163.1315,
           -50.2858]], dtype=torch.float64)
	q_value: tensor([[-22.1646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9378976177597123, distance: 0.28517458831485576 entropy 9.217862171958142
epoch: 16, step: 90
	action: tensor([[ -958.8702,   302.9085,   864.6640,  -917.1785,  -875.3601, -1830.5574,
         -1076.2944]], dtype=torch.float64)
	q_value: tensor([[-19.6652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.841388162474294
epoch: 16, step: 91
	action: tensor([[-1701.2083, -2485.4605,  1554.8679, -1518.4330,  -742.4159,  2159.7149,
          2244.8952]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5028600044770484, distance: 1.402865247036928 entropy 8.708576764229536
epoch: 16, step: 92
	action: tensor([[-3494.2085, -4908.0520,  2523.3344,  5386.4201, -2520.9427,  3408.3245,
           899.3660]], dtype=torch.float64)
	q_value: tensor([[-25.5584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26144533790409885, distance: 1.2852597623199178 entropy 9.295819491037843
epoch: 16, step: 93
	action: tensor([[-2445.9066, -2905.2568,  1131.2322,  2341.5282,  1752.8321,  5240.9915,
         -2005.1948]], dtype=torch.float64)
	q_value: tensor([[-24.7828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13011335147225433, distance: 1.0673037266397885 entropy 9.218049079991607
epoch: 16, step: 94
	action: tensor([[  145.8575, -5221.7801,  1037.5381, -2626.7239, -1482.2474,  1109.1167,
         -2813.0284]], dtype=torch.float64)
	q_value: tensor([[-22.8212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05594181447856217, distance: 1.111875278874923 entropy 9.187379640477543
epoch: 16, step: 95
	action: tensor([[ -644.2854, -1439.0208, -1015.6625,   957.6831, -1071.0586,   128.2251,
          -406.7062]], dtype=torch.float64)
	q_value: tensor([[-16.4596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24275334948827498, distance: 1.2757017886122461 entropy 8.80971013694399
epoch: 16, step: 96
	action: tensor([[-6378.3199, -3303.7089, -2503.8728,  3714.0926, -2200.6677, -6141.2963,
          2842.5636]], dtype=torch.float64)
	q_value: tensor([[-26.4282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35250175155425256, distance: 1.3308392821358597 entropy 9.398118535472628
epoch: 16, step: 97
	action: tensor([[ -568.2838, -2104.9084,  1332.0128, -3178.3518,   706.2737, -2260.1402,
          6500.8248]], dtype=torch.float64)
	q_value: tensor([[-24.9195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9460674687915267, distance: 1.5963776532018326 entropy 9.463784993907081
epoch: 16, step: 98
	action: tensor([[-1336.5460, -2634.3363,  1856.3801,  2109.4406, -5695.1482, -4624.4854,
          4934.9456]], dtype=torch.float64)
	q_value: tensor([[-24.3725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08986502083320391, distance: 1.1946564998713525 entropy 9.25089197291066
epoch: 16, step: 99
	action: tensor([[-2695.6292,  -642.8722,  5021.0924,  4617.6336,  2359.9910,  5758.5180,
          3825.4198]], dtype=torch.float64)
	q_value: tensor([[-26.1496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.608054498592737, distance: 1.451132520494431 entropy 9.52695033617833
epoch: 16, step: 100
	action: tensor([[ 1242.9725, -1157.3278,  2828.7807,   -89.5394, -1055.7188,  -742.2213,
            -5.2349]], dtype=torch.float64)
	q_value: tensor([[-24.5656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6667322423715498, distance: 1.4773711423953073 entropy 9.358796724858337
epoch: 16, step: 101
	action: tensor([[ -144.2623,   -17.4861,   156.8218, -2388.7282,  -705.4342,  1465.6971,
          1209.7304]], dtype=torch.float64)
	q_value: tensor([[-12.5224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6484210500405956, distance: 1.469233320728039 entropy 8.572620785725155
epoch: 16, step: 102
	action: tensor([[-4674.0427, -6307.9738,  -839.8455,  1339.4969,  -177.8112,   103.4199,
          1000.5866]], dtype=torch.float64)
	q_value: tensor([[-24.2406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7007595793972983, distance: 1.4923756561616834 entropy 9.123836773986369
epoch: 16, step: 103
	action: tensor([[  690.6604, -6904.5325, -4378.0901, -1074.7019,  3391.7007, -3110.2494,
         -1540.7899]], dtype=torch.float64)
	q_value: tensor([[-22.4500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30658242356944565, distance: 1.308052270859618 entropy 9.19658028657373
epoch: 16, step: 104
	action: tensor([[  -86.4719, -1499.9126,    74.6548,  1614.6206,   562.7739,  2668.1370,
          -302.3662]], dtype=torch.float64)
	q_value: tensor([[-17.9275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18517204288926514, distance: 1.2457973204633552 entropy 8.872316928108964
epoch: 16, step: 105
	action: tensor([[-1168.0034,   118.2259,  2130.8006, -1075.0251,   857.1080,  1902.4099,
          2998.6464]], dtype=torch.float64)
	q_value: tensor([[-19.9193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5810894493514807, distance: 1.4389142500687928 entropy 9.146699405421858
epoch: 16, step: 106
	action: tensor([[-2074.6052, -3501.7034, -2571.7323,  3041.3124,  1767.6910,  2271.6659,
          1748.4820]], dtype=torch.float64)
	q_value: tensor([[-26.5214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7320343622948755, distance: 1.506034584849228 entropy 9.11465888100739
epoch: 16, step: 107
	action: tensor([[-1383.9315, -3239.5286,   870.2593,  4083.1669,  3431.8345, -2944.3335,
          5468.0839]], dtype=torch.float64)
	q_value: tensor([[-24.7043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04256427440199961, distance: 1.168444564716621 entropy 9.241700526173764
epoch: 16, step: 108
	action: tensor([[-4866.2876, -2794.0665,  2048.0303, -2058.2516, -2661.4500,   143.2432,
          5473.7293]], dtype=torch.float64)
	q_value: tensor([[-24.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8075144005997272, distance: 1.53850026818594 entropy 9.326206603471642
epoch: 16, step: 109
	action: tensor([[ -772.0748, -2215.3523, -2534.1916,  3937.1382,  1791.2889, -2437.0750,
          -268.5762]], dtype=torch.float64)
	q_value: tensor([[-21.1568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4348403079099714, distance: 1.3707507037197182 entropy 9.079482418929201
epoch: 16, step: 110
	action: tensor([[-3989.1075, -2424.9018,  6066.5823,  2210.6617, -1703.0109, -2355.5993,
          1854.7422]], dtype=torch.float64)
	q_value: tensor([[-22.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4838707305911685, distance: 1.3939741732843567 entropy 9.269437961616138
epoch: 16, step: 111
	action: tensor([[-1429.7449, -3139.7449,   102.7798,  6033.5774, -3341.5821,  1742.0201,
          3034.3252]], dtype=torch.float64)
	q_value: tensor([[-24.5213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11013828157713978, distance: 1.079488338875163 entropy 9.354520922996585
epoch: 16, step: 112
	action: tensor([[ -859.3514, -1112.3721,    43.4193, -3840.5034, -1654.5667, -1078.5623,
           113.2208]], dtype=torch.float64)
	q_value: tensor([[-22.5670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9350319142932018, distance: 1.5918449329922664 entropy 9.21224396998191
epoch: 16, step: 113
	action: tensor([[-1023.1463, -4046.0969,  5470.5507,  -790.0082,  2130.2379,  -754.2818,
          -538.1173]], dtype=torch.float64)
	q_value: tensor([[-22.3581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13501323397756082, distance: 1.0642935367088004 entropy 9.154869861688411
epoch: 16, step: 114
	action: tensor([[ 3182.8397,  -868.2559,  4722.0897, -1675.7278, -1640.4406,   787.0038,
          2735.1269]], dtype=torch.float64)
	q_value: tensor([[-26.6295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39979526113501873, distance: 0.8865564693849829 entropy 9.235035519645411
epoch: 16, step: 115
	action: tensor([[-4652.4941, -3718.6366,  1775.6180,   366.6034,  2882.3714,   612.1162,
          3095.9884]], dtype=torch.float64)
	q_value: tensor([[-25.7067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3192864918333649, distance: 1.3143960676635613 entropy 9.248585943519771
epoch: 16, step: 116
	action: tensor([[-1881.2322, -3095.1789, -8356.7456,  2378.1716,  -863.5218, -2667.8375,
          3572.0000]], dtype=torch.float64)
	q_value: tensor([[-26.7785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023180824925119747, distance: 1.1310030638228254 entropy 9.359095524164948
epoch: 16, step: 117
	action: tensor([[-3089.2455, -2921.2943, -1533.5876, -4706.7101,  1648.8315,  6120.4255,
          -972.7990]], dtype=torch.float64)
	q_value: tensor([[-25.4997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14833913602456983, distance: 1.2262860174343158 entropy 9.507477954282725
epoch: 16, step: 118
	action: tensor([[-3406.4241,  -475.0075,   579.8816,  3953.9035,   408.9397,  1718.3841,
           469.6125]], dtype=torch.float64)
	q_value: tensor([[-23.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21071621122746764, distance: 1.0166542598079862 entropy 9.147558349652725
epoch: 16, step: 119
	action: tensor([[-2226.6787, -1395.8965,   793.0524, -1702.6414, -2495.4322, -5044.2852,
           330.8016]], dtype=torch.float64)
	q_value: tensor([[-21.3321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5626765061603549, distance: 1.4305111087425992 entropy 9.124276468159286
epoch: 16, step: 120
	action: tensor([[-2233.0769, -3848.8710,  4253.6712, -1014.2491,  1562.0676,  -487.3310,
          1043.4774]], dtype=torch.float64)
	q_value: tensor([[-21.8963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1697155795608929, distance: 1.2376471054057927 entropy 9.108450353922139
epoch: 16, step: 121
	action: tensor([[-2463.1962,  3266.1296,   780.0991,  1796.5180,   -19.8099,  2597.8645,
          3416.8293]], dtype=torch.float64)
	q_value: tensor([[-27.6919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4801074335212796, distance: 1.3922053974938315 entropy 9.258549795537062
epoch: 16, step: 122
	action: tensor([[  216.2887,   595.3777,   -44.6465,  1140.3556,  -192.4402,  -572.1632,
         -1598.3149]], dtype=torch.float64)
	q_value: tensor([[-21.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.332017349015308, distance: 0.9352749117360096 entropy 8.740582903937312
epoch: 16, step: 123
	action: tensor([[ 2.1578e+02, -5.7876e+03,  1.9594e+00,  4.6003e+03,  4.4441e+02,
         -1.9737e+03, -2.6327e+03]], dtype=torch.float64)
	q_value: tensor([[-27.4305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7696738063858941, distance: 1.5223107164936298 entropy 9.242552785812455
epoch: 16, step: 124
	action: tensor([[-629.1321, -392.5428, -780.8145,  -37.7630, 4058.3772,   -7.9066,
         2221.8260]], dtype=torch.float64)
	q_value: tensor([[-17.5705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8609118356998151, distance: 1.5610599878984783 entropy 8.8989123035358
epoch: 16, step: 125
	action: tensor([[ -499.1265, -1125.6199, -2533.0376,   730.7303, -1385.5054,  3157.3073,
          -887.8612]], dtype=torch.float64)
	q_value: tensor([[-20.8578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0895774160488085, distance: 1.654192038139973 entropy 9.138637039633533
epoch: 16, step: 126
	action: tensor([[-3644.7358,  1670.8648, -1518.9711, -1155.3335,  2286.4189,  2773.3578,
          2925.5858]], dtype=torch.float64)
	q_value: tensor([[-23.3774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03681413392700028, distance: 1.1652179002973657 entropy 9.370688449393953
epoch: 16, step: 127
	action: tensor([[-4004.5883, -4609.4146,   -23.7459,  1397.0879, -1396.7371, -3449.4053,
           352.7148]], dtype=torch.float64)
	q_value: tensor([[-28.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12386896160449568, distance: 1.0711276403260914 entropy 8.999631434109533
LOSS epoch 16 actor 256.59753290551515 critic 247.80736464797906
epoch: 17, step: 0
	action: tensor([[-4626.7858, -4649.3389, -4080.4537,  3106.5788, -5211.0890,  2205.9854,
          1350.8415]], dtype=torch.float64)
	q_value: tensor([[-20.0854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21069247912753486, distance: 1.259138825384594 entropy 9.331027624070742
epoch: 17, step: 1
	action: tensor([[ 2171.8766, -6557.6639, -3745.5136, -1919.6153, -1876.4231,   264.9678,
          4706.8751]], dtype=torch.float64)
	q_value: tensor([[-25.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31045337980890075, distance: 0.950251382956108 entropy 9.476995313135749
epoch: 17, step: 2
	action: tensor([[-1579.2672, -1789.1302, -3376.3927,  -496.7646, -2210.3659, -1633.9322,
          3580.4211]], dtype=torch.float64)
	q_value: tensor([[-20.4464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11654888160402888, distance: 1.0755929716401207 entropy 9.13637698484475
epoch: 17, step: 3
	action: tensor([[-1876.2216, -5658.7109,   722.2135,  6603.4566, -1245.1028,   977.2030,
         -1095.0284]], dtype=torch.float64)
	q_value: tensor([[-28.3806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.837225022408763, distance: 1.5510931115629474 entropy 9.308086709739243
epoch: 17, step: 4
	action: tensor([[-4719.3703,  -593.7910,   626.5490,  3522.2563,  -959.3833,   349.0982,
          2489.3642]], dtype=torch.float64)
	q_value: tensor([[-20.9522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.59976754349199, distance: 1.4473885543755656 entropy 9.210451765141856
epoch: 17, step: 5
	action: tensor([[-5623.0871,  1396.3653,  3438.0619, -1528.0088, -2411.8767, -6057.9602,
          4910.1382]], dtype=torch.float64)
	q_value: tensor([[-23.8239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4730060385882895, distance: 1.3888615565758313 entropy 9.316010266148977
epoch: 17, step: 6
	action: tensor([[-2479.1930, -1556.0577,   601.4688, -1207.2442, -1639.2781,  2426.1008,
           922.8092]], dtype=torch.float64)
	q_value: tensor([[-24.6775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.704281867736158, distance: 1.4939202185573375 entropy 8.98789127149124
epoch: 17, step: 7
	action: tensor([[ -793.5798, -4057.6132, -2003.7030,   -52.5383,  3771.2508,  2574.1101,
          1771.5504]], dtype=torch.float64)
	q_value: tensor([[-25.4675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11026209620265748, distance: 1.205783814683863 entropy 9.199139585752349
epoch: 17, step: 8
	action: tensor([[-2303.9661, -3225.6247, -3252.0262,   -21.2245,  1204.2493, -3232.2813,
          3660.6605]], dtype=torch.float64)
	q_value: tensor([[-23.5917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8194432300776584, distance: 1.5435686447248378 entropy 9.161294121837132
epoch: 17, step: 9
	action: tensor([[  707.7469, -1838.2364,  3763.1257,    -6.4522,    47.3572,  2810.9799,
          -375.3794]], dtype=torch.float64)
	q_value: tensor([[-28.9428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2982044085749518, distance: 1.3038518065740319 entropy 9.480813868535103
epoch: 17, step: 10
	action: tensor([[ -667.5546, -4725.9526,  2836.8922, -1080.5808, -1988.5946,  -104.7712,
          2509.7164]], dtype=torch.float64)
	q_value: tensor([[-21.1537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5888222752914294, distance: 1.4424286944045206 entropy 9.236649059998955
epoch: 17, step: 11
	action: tensor([[-2949.5807, -2327.6984, -4839.3459, -2286.2858,  2422.8115,  4022.7255,
         -1694.2297]], dtype=torch.float64)
	q_value: tensor([[-26.1651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10221261349210753, distance: 1.2014048521603842 entropy 9.463329546405665
epoch: 17, step: 12
	action: tensor([[-3813.1321,  2081.0954, -5883.0718, -5721.9972,  1068.3629,  1996.8233,
          -314.9333]], dtype=torch.float64)
	q_value: tensor([[-26.9810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3180099089093026, distance: 0.9450302832828997 entropy 9.371233387810832
epoch: 17, step: 13
	action: tensor([[-1218.6352, -1529.1295,  2009.7521,  -221.5585, -5027.2090,    54.9567,
         -2268.9566]], dtype=torch.float64)
	q_value: tensor([[-33.7539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44803439449091575, distance: 1.3770386566001793 entropy 9.1480225505605
epoch: 17, step: 14
	action: tensor([[-1021.3802, -1083.6333, -5016.0076,  4498.7773,   600.2183,  1600.1741,
          -299.9029]], dtype=torch.float64)
	q_value: tensor([[-23.6839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9940333022214867, distance: 1.6159313137464544 entropy 9.122327246312334
epoch: 17, step: 15
	action: tensor([[-3067.2412, -6048.9053,  2710.8184,  5786.4415,  -835.8128,    81.5682,
         -1795.1414]], dtype=torch.float64)
	q_value: tensor([[-26.4182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07332764327011976, distance: 1.1015895241334603 entropy 9.442959587086799
epoch: 17, step: 16
	action: tensor([[-2365.9949, -3238.6074,  -258.2721,  4338.6912, -2530.5274,  1629.3651,
          1403.0643]], dtype=torch.float64)
	q_value: tensor([[-24.5878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6786283725990234, distance: 1.4826340606184956 entropy 9.396550487954832
epoch: 17, step: 17
	action: tensor([[ 1678.9920,   463.4802, -2604.5453,   -54.3225, -4539.9985, -3011.4456,
          -400.1148]], dtype=torch.float64)
	q_value: tensor([[-25.4724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.392877938046237
epoch: 17, step: 18
	action: tensor([[-2348.3047, -2585.9589,   -91.6631,   506.7709,   940.8747,  1255.3824,
          1523.4422]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03517615605255542, distance: 1.1240372587758265 entropy 8.791917631038453
epoch: 17, step: 19
	action: tensor([[-3500.0744, -2544.0693, -3600.2999,  -189.7117,  3432.7052, -1745.8287,
         -4557.8572]], dtype=torch.float64)
	q_value: tensor([[-25.4327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5852939771125254, distance: 1.440826203295436 entropy 9.458243336114553
epoch: 17, step: 20
	action: tensor([[-1434.0852, -3573.0075, -1456.9580,  1383.1214,  4401.1327,  -505.7733,
         -1371.2673]], dtype=torch.float64)
	q_value: tensor([[-22.5696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005539208689974151, distance: 1.1475092579663744 entropy 9.350939578757204
epoch: 17, step: 21
	action: tensor([[-1907.1848, -5855.3626,  -406.1957, 10751.6721,  2203.3295,  2841.5816,
          2059.7753]], dtype=torch.float64)
	q_value: tensor([[-22.5943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34426735711234224, distance: 1.3267818432617176 entropy 9.399686390112768
epoch: 17, step: 22
	action: tensor([[-1994.0848,   361.1338, -2031.5513, -5404.1570, -4207.3491, -1210.5780,
          -654.8382]], dtype=torch.float64)
	q_value: tensor([[-24.9472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12520054887850507, distance: 1.213868554104789 entropy 9.430113342213573
epoch: 17, step: 23
	action: tensor([[-1027.2298, -1293.3005, -2644.6790, -2922.7968,  1284.7092, -1523.2123,
          3816.7837]], dtype=torch.float64)
	q_value: tensor([[-30.8332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5264577817116787, distance: 1.4138361827280888 entropy 9.261573104673909
epoch: 17, step: 24
	action: tensor([[  438.4506,  1945.1130,  -134.4582,  3149.3536,  -485.0830,  5316.3415,
         -3396.3168]], dtype=torch.float64)
	q_value: tensor([[-27.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.521995306494938
epoch: 17, step: 25
	action: tensor([[-2108.6519, -2841.7972, -2662.0876,   745.7041,  2904.7559,   793.7964,
         -1593.3681]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35268381605144206, distance: 1.33092885334376 entropy 8.791917631038453
epoch: 17, step: 26
	action: tensor([[-1257.0609, -2842.8543, -2961.7974,  2435.5739, -1678.9858,  5244.1916,
         -3058.7836]], dtype=torch.float64)
	q_value: tensor([[-23.5054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01844112572815737, distance: 1.1548475501900939 entropy 9.349093645891077
epoch: 17, step: 27
	action: tensor([[   12.5539,  3534.9490, -3943.5739,  5950.5623, -1258.8841, -2007.9148,
          1286.2235]], dtype=torch.float64)
	q_value: tensor([[-25.4231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2670773701768673, distance: 0.9796834215924137 entropy 9.459662091962608
epoch: 17, step: 28
	action: tensor([[ -434.6570, -3199.4544, -2860.6572,  2671.3713,  1577.7118, -4449.3708,
         -2092.1974]], dtype=torch.float64)
	q_value: tensor([[-22.4123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8152324245658522, distance: 1.5417814407567998 entropy 9.327400845730622
epoch: 17, step: 29
	action: tensor([[-3324.8924,  -203.1216,  2997.8946,   -31.2623,  -409.0375,   718.0059,
          -676.5314]], dtype=torch.float64)
	q_value: tensor([[-19.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8253574639113348, distance: 1.5460753509367013 entropy 9.223090823600028
epoch: 17, step: 30
	action: tensor([[ -848.4056, -1268.1262,   948.6670,  2124.6978, -4664.1739,   527.7411,
          3391.2636]], dtype=torch.float64)
	q_value: tensor([[-17.5818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7362714211012154, distance: 1.5078755585229247 entropy 9.085613762803485
epoch: 17, step: 31
	action: tensor([[ 2032.5926, -6892.3598,  1820.2322,  6743.1268, -6983.1769,   221.2909,
          3028.5121]], dtype=torch.float64)
	q_value: tensor([[-24.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18800873766852222, distance: 1.2472873297360416 entropy 9.489417783368966
epoch: 17, step: 32
	action: tensor([[   83.5191, -1961.8849, -1809.1202, -3284.1805,  5271.7938,  -614.4966,
          -784.2827]], dtype=torch.float64)
	q_value: tensor([[-20.8876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23000548594152836, distance: 1.2691420027388998 entropy 9.38104820570989
epoch: 17, step: 33
	action: tensor([[ -826.1505, -4250.3481,   770.6373,  1470.8687,  3506.7702,  1488.3043,
          2610.3814]], dtype=torch.float64)
	q_value: tensor([[-18.6418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023072148821703165, distance: 1.157470215171077 entropy 9.009357427219474
epoch: 17, step: 34
	action: tensor([[-1530.2932,  -805.3522, -2157.7455,  3574.1747, -1409.6797,  1722.2891,
           839.7743]], dtype=torch.float64)
	q_value: tensor([[-21.1320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05183990692029605, distance: 1.1142881948714618 entropy 9.206013819562253
epoch: 17, step: 35
	action: tensor([[-3165.4535,  2248.1226,  -626.7700,  2369.7605, -4652.1598,    44.3289,
          1527.2658]], dtype=torch.float64)
	q_value: tensor([[-25.9725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5790068067034128, distance: 0.7424961914583267 entropy 9.54289922298359
epoch: 17, step: 36
	action: tensor([[ -131.5435, -1045.5276,  1617.4506,  2246.3746,   143.1230,  2042.8990,
          3016.7746]], dtype=torch.float64)
	q_value: tensor([[-24.8779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21837768346287023, distance: 1.0117079637758764 entropy 9.356509795552999
epoch: 17, step: 37
	action: tensor([[-3650.4656,   110.4559,  4053.2152,   231.9432, -1139.6135,   487.6620,
         -3653.5627]], dtype=torch.float64)
	q_value: tensor([[-23.4868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06297979168824508, distance: 1.1798293546157197 entropy 9.362262200735527
epoch: 17, step: 38
	action: tensor([[-1322.6814,  1560.5508, -1774.5809,  1237.5372, -2066.9690,  -350.1556,
          1399.4898]], dtype=torch.float64)
	q_value: tensor([[-24.3565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14311714966911349, distance: 1.0592962107165715 entropy 9.398113769967313
epoch: 17, step: 39
	action: tensor([[-5039.4952,  1325.9704, -4938.2114,  3689.9768, -2583.2958,  -301.4142,
          5694.6680]], dtype=torch.float64)
	q_value: tensor([[-24.2107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49288155234055553, distance: 0.8149132851161455 entropy 9.333497158945933
epoch: 17, step: 40
	action: tensor([[-2086.5471, -2713.1965, -3390.2103, -3206.9699, -5313.6330,  6307.4980,
          1818.1387]], dtype=torch.float64)
	q_value: tensor([[-23.6486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.35136224570597
epoch: 17, step: 41
	action: tensor([[  291.3731, -2115.9600,  2207.2989,   413.4696,  -472.3035, -1604.1399,
          1006.4785]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05478658926796798, distance: 1.175273633107434 entropy 8.791917631038453
epoch: 17, step: 42
	action: tensor([[  510.3759, -3212.8068,  1262.9366, -3863.9445,  2716.5176, -2069.4621,
          3648.1352]], dtype=torch.float64)
	q_value: tensor([[-21.1784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04086500060846099, distance: 1.1207185560777608 entropy 9.352706407424693
epoch: 17, step: 43
	action: tensor([[-4935.4372, -2956.2361, -3271.1253,  3657.9379, -1468.2461,  3535.9193,
          1948.5268]], dtype=torch.float64)
	q_value: tensor([[-23.1783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07539754415300759, distance: 1.100358530329754 entropy 9.271973532041317
epoch: 17, step: 44
	action: tensor([[ -400.3202,   446.5307,  -792.4999,   588.3396, -1223.0590,  -606.8321,
          2678.7397]], dtype=torch.float64)
	q_value: tensor([[-22.4512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2584272069756397, distance: 1.2837212869006431 entropy 9.250655071815215
epoch: 17, step: 45
	action: tensor([[ -757.5733, -4625.6828,  3806.5516, -2049.5617,    62.2895, -2956.6351,
         -5690.6073]], dtype=torch.float64)
	q_value: tensor([[-23.3957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11220983909962245, distance: 1.2068410099835984 entropy 9.278970608735662
epoch: 17, step: 46
	action: tensor([[ 1056.8448, -1666.8745, -4630.8332, -2845.3834,   535.5318, -2575.6151,
           725.4223]], dtype=torch.float64)
	q_value: tensor([[-21.4607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011737487075908493, distance: 1.1376085677056627 entropy 9.207947957969983
epoch: 17, step: 47
	action: tensor([[ -969.1009, -3117.2758, -4880.4165,  4336.1457,   617.9880,   452.2190,
          1595.0454]], dtype=torch.float64)
	q_value: tensor([[-22.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8263280268916742, distance: 1.5464863290920792 entropy 9.293513102133444
epoch: 17, step: 48
	action: tensor([[-4843.2784, -1145.7558,  -790.9303, -2202.2026,  6562.9533, -1509.8634,
         -3603.5167]], dtype=torch.float64)
	q_value: tensor([[-21.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.553404344177671, distance: 1.4262608161570407 entropy 9.298370732270499
epoch: 17, step: 49
	action: tensor([[  229.4285, -3830.0592,   712.0917,  5145.6200, -2715.7761,  1482.7905,
          2583.9471]], dtype=torch.float64)
	q_value: tensor([[-27.6740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973709589160952, distance: 0.9592233483188485 entropy 9.55394342372726
epoch: 17, step: 50
	action: tensor([[-1586.0963, -1238.7268,   351.2143,  2749.4463,  4083.9401, -2366.1891,
          3843.2568]], dtype=torch.float64)
	q_value: tensor([[-21.5155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7245663451436704, distance: 1.5027842908332845 entropy 9.22044298464308
epoch: 17, step: 51
	action: tensor([[  782.8979, -1872.7043, -3725.9360,  -487.5900, -1183.0322, -2201.7111,
          6185.2324]], dtype=torch.float64)
	q_value: tensor([[-20.8559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17577701088662845, distance: 1.038912699602302 entropy 9.301410443110063
epoch: 17, step: 52
	action: tensor([[-6237.4173, -1827.4499, -5781.3975,  -515.0517,  2288.7951, -3638.1837,
         -1070.9407]], dtype=torch.float64)
	q_value: tensor([[-20.2142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19217019912305533, distance: 1.2494699738767772 entropy 9.27906113193359
epoch: 17, step: 53
	action: tensor([[ -480.1517,   571.6551, -4937.1176, -4334.5497,   588.7262, -1686.2897,
          2295.1493]], dtype=torch.float64)
	q_value: tensor([[-28.5567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24981336799823772, distance: 1.279320255266702 entropy 9.360543124212995
epoch: 17, step: 54
	action: tensor([[-3836.8484, -4655.1603,  1425.6890,  -264.7845,  4146.5167,  4401.5091,
           866.8617]], dtype=torch.float64)
	q_value: tensor([[-29.5880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3649781948244375, distance: 1.3369634976034313 entropy 9.314073069478246
epoch: 17, step: 55
	action: tensor([[-2238.6649, -4232.8559, -2215.1454, -2638.3812,   192.0116,   464.8882,
           158.9190]], dtype=torch.float64)
	q_value: tensor([[-21.6749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24770973098062032, distance: 0.9925431932523102 entropy 9.036567277107613
epoch: 17, step: 56
	action: tensor([[-3432.3002, -2140.8423,  5938.0614, -1777.3121, -2689.3924,  1959.6301,
         -5011.8694]], dtype=torch.float64)
	q_value: tensor([[-30.8687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2816574709529851, distance: 1.2955156986089282 entropy 9.45939535170891
epoch: 17, step: 57
	action: tensor([[-2150.6861, -2412.5090,  1041.9901, -1638.3637,  -214.9990,  4505.7063,
          2355.7208]], dtype=torch.float64)
	q_value: tensor([[-24.1099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09721188783060297, distance: 1.1986763740331292 entropy 9.226413730238628
epoch: 17, step: 58
	action: tensor([[-5198.3687, -1366.6338,   307.5124,  -342.8577,  -548.5586,   112.2156,
         -2755.9249]], dtype=torch.float64)
	q_value: tensor([[-25.1595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17200731448626883, distance: 1.238858926142306 entropy 9.178504028195826
epoch: 17, step: 59
	action: tensor([[ -356.7973,  -900.3796, -1311.7943, -1700.3059,  1788.1715,   156.3580,
           471.1712]], dtype=torch.float64)
	q_value: tensor([[-25.4025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0804179285489155, distance: 1.1894675114375772 entropy 9.277332431001863
epoch: 17, step: 60
	action: tensor([[-1840.8401, -5400.6392, -3304.5122,  -400.0681, -2664.2612,  3644.9298,
          2145.0142]], dtype=torch.float64)
	q_value: tensor([[-25.5971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5302639884194467, distance: 1.4155977782388676 entropy 9.299631535742902
epoch: 17, step: 61
	action: tensor([[-2945.9758, -3073.9312, -6781.0198, -3242.6251,  1085.1231,  -470.9578,
          1280.8408]], dtype=torch.float64)
	q_value: tensor([[-22.4648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07351598981828555, distance: 1.1856621390291515 entropy 9.216129946327362
epoch: 17, step: 62
	action: tensor([[-2342.6257, -4161.7362,  1298.6067,  1094.2602,  -548.0274, -5065.7746,
          1395.3907]], dtype=torch.float64)
	q_value: tensor([[-25.9829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8706897361339845, distance: 1.5651558001914998 entropy 9.288470458308185
epoch: 17, step: 63
	action: tensor([[ -193.6387, -4378.8817, -1508.3203,  -976.3175,   151.4289, -2373.0503,
          3643.8765]], dtype=torch.float64)
	q_value: tensor([[-21.7568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007796771428900562, distance: 1.1398744291080705 entropy 9.249778748140272
epoch: 17, step: 64
	action: tensor([[-3407.9036,  1765.8475,   816.9250,  3954.2412,  2113.0866, -2575.8650,
           409.8109]], dtype=torch.float64)
	q_value: tensor([[-24.1790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17679693778664218, distance: 1.038269704303278 entropy 9.141145309124704
epoch: 17, step: 65
	action: tensor([[-4324.7284, -5423.8459,   515.3774, -2085.7802, -1737.8762,  1184.7885,
           208.5510]], dtype=torch.float64)
	q_value: tensor([[-23.5894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3194378816102894, distance: 0.9440403975066586 entropy 9.108845004805367
epoch: 17, step: 66
	action: tensor([[-2179.9675,  3384.1253, -1117.3203,  1450.3909,   700.6314, -4390.2111,
         -2710.1169]], dtype=torch.float64)
	q_value: tensor([[-24.5095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5712359631285833, distance: 1.4344235234436167 entropy 9.347167921517665
epoch: 17, step: 67
	action: tensor([[-2553.4890, -1154.8974, -2261.8726, -1883.4025,  2655.7109,  3894.3559,
          -115.2615]], dtype=torch.float64)
	q_value: tensor([[-25.9811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29948022919759865, distance: 1.3044923345654817 entropy 9.355041565483015
epoch: 17, step: 68
	action: tensor([[-1488.0982,   592.4784, -1906.9784,   551.9765, -3523.5977,  3157.9885,
          4466.2270]], dtype=torch.float64)
	q_value: tensor([[-27.8788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18412218940843483, distance: 1.2452454198204326 entropy 9.331028386515944
epoch: 17, step: 69
	action: tensor([[-2723.5659,  -907.3218,  2027.8470,  4562.8925, -1397.1063,  2841.9552,
         -2287.4074]], dtype=torch.float64)
	q_value: tensor([[-23.6523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5286410223823879, distance: 1.4148469023709556 entropy 9.095776551546757
epoch: 17, step: 70
	action: tensor([[ -579.7229,  -470.2919,  2190.1699, -1018.2677,  -105.8764,  -567.2892,
          2858.0459]], dtype=torch.float64)
	q_value: tensor([[-24.7000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3197578271675081, distance: 1.3146308407739105 entropy 9.433818968229096
epoch: 17, step: 71
	action: tensor([[-4391.2056, -1477.7270, -2188.0680,  2611.3150,  -633.0752,   665.5567,
         -2570.4609]], dtype=torch.float64)
	q_value: tensor([[-24.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46556432460133323, distance: 1.3853488087721468 entropy 9.278916446382157
epoch: 17, step: 72
	action: tensor([[-3148.3857, -3325.3340, -4199.6782, -2794.5826,  4627.3278,  3416.0012,
           552.9334]], dtype=torch.float64)
	q_value: tensor([[-22.9291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45646394751267794, distance: 1.381040970368988 entropy 9.289045450192415
epoch: 17, step: 73
	action: tensor([[ 1119.7635,   870.2137,  1515.9477,  1261.6039,  1753.5160,  1000.0733,
         -5996.8993]], dtype=torch.float64)
	q_value: tensor([[-27.1631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5275303790373121, distance: 0.7865813372049709 entropy 9.393572484711708
epoch: 17, step: 74
	action: tensor([[-2498.9484, -2995.6029,  3555.9581,  3816.5682,  -901.6599,  -203.6305,
          5321.7490]], dtype=torch.float64)
	q_value: tensor([[-22.5413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21980553774658163, distance: 1.0107834559590592 entropy 9.293269139755239
epoch: 17, step: 75
	action: tensor([[-2268.5986,  4206.4291,  2093.2759,  -180.7671, -3297.5751, -2645.5676,
           717.6314]], dtype=torch.float64)
	q_value: tensor([[-20.6674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3334249872216103, distance: 0.9342889407617683 entropy 9.301632493420822
epoch: 17, step: 76
	action: tensor([[-2362.8461, -2440.5228,  4085.4356,  -570.9856,  -346.8038,   194.7992,
          3180.0500]], dtype=torch.float64)
	q_value: tensor([[-21.3144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3226195361720663, distance: 1.7439969882846087 entropy 8.978933100177214
epoch: 17, step: 77
	action: tensor([[-4202.7556, -2007.9952,   522.5391,  3939.4333, -2303.6762,   732.6323,
          3024.4596]], dtype=torch.float64)
	q_value: tensor([[-18.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5785698658543832, distance: 1.437767284565516 entropy 9.019442597976072
epoch: 17, step: 78
	action: tensor([[ 2805.7750, -4207.5068, -1864.2368,  1442.8264, -6819.0051,  1912.6371,
           178.0210]], dtype=torch.float64)
	q_value: tensor([[-21.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39740112791084836, distance: 0.8883228848667273 entropy 9.271132122320799
epoch: 17, step: 79
	action: tensor([[ -494.7663,  -583.7031,  2633.3471,  3347.4359, -2993.8378,  -838.5655,
         -2146.6212]], dtype=torch.float64)
	q_value: tensor([[-24.3001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24316781130588538, distance: 0.9955349025797846 entropy 9.343033895532686
epoch: 17, step: 80
	action: tensor([[-2857.2273, -1304.8663, -2459.6914,  6890.7679, -3795.8544, -1543.1175,
          -816.3310]], dtype=torch.float64)
	q_value: tensor([[-21.9012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22140937553113105, distance: 1.264699415746624 entropy 9.488134867456425
epoch: 17, step: 81
	action: tensor([[-1761.8675, -3160.5799,  1779.9476, -1759.7282,   527.0050,  2081.1622,
          -848.3928]], dtype=torch.float64)
	q_value: tensor([[-20.4746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.920305373809557, distance: 1.5857760043131772 entropy 9.326302105195307
epoch: 17, step: 82
	action: tensor([[-1131.2185,  2404.0363, -2467.1843, -1028.2187,  -392.0700,  -836.2635,
          -289.3459]], dtype=torch.float64)
	q_value: tensor([[-19.7042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21289629113864383, distance: 1.2602843035631754 entropy 9.049099244696453
epoch: 17, step: 83
	action: tensor([[ 1519.9079, -3426.0881, -1376.1851, -2073.7402, -2767.1619,  2137.3992,
         -1046.3563]], dtype=torch.float64)
	q_value: tensor([[-29.9488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34407819081414726, distance: 0.9267929658132695 entropy 9.325413501504935
epoch: 17, step: 84
	action: tensor([[-3051.3357, -1904.3990,  -875.8088,  -967.9994,   773.0225,  3026.5120,
         -1238.4068]], dtype=torch.float64)
	q_value: tensor([[-23.9959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1633315538793234, distance: 1.234265094754938 entropy 9.321825918520648
epoch: 17, step: 85
	action: tensor([[-5097.4138, -1499.2822,  3651.0467, -2336.0978,    31.9812, -2060.8740,
           688.2083]], dtype=torch.float64)
	q_value: tensor([[-24.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05919891575507752, distance: 1.1099555775907781 entropy 9.156017771612701
epoch: 17, step: 86
	action: tensor([[ 1580.9058, -1794.4633,    92.6376,  2993.3326,   585.4008,   922.6812,
         -1717.7249]], dtype=torch.float64)
	q_value: tensor([[-25.9761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3580990754853972, distance: 0.9168339651781076 entropy 9.227772259600643
epoch: 17, step: 87
	action: tensor([[-1825.4397,  1682.5181,   584.1329,  -299.6512,   614.9012, -1951.0748,
          2311.0491]], dtype=torch.float64)
	q_value: tensor([[-19.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.183742407361986
epoch: 17, step: 88
	action: tensor([[ 1239.6459,  -181.2548, -1229.5958,  1670.8641,   838.4218,  2225.3043,
         -1391.0818]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09057062304442254, distance: 1.0912925307963182 entropy 8.791917631038453
epoch: 17, step: 89
	action: tensor([[ -919.6815, -5292.2465, -1440.9361,  2312.1391, -4413.3447,   425.8645,
          3424.5479]], dtype=torch.float64)
	q_value: tensor([[-24.5056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49648453784197155, distance: 1.3998864511916334 entropy 9.421098175044778
epoch: 17, step: 90
	action: tensor([[-2381.4363, -7633.0323, -1558.7147,  5225.9320, -1590.5754,   153.3714,
          1283.1764]], dtype=torch.float64)
	q_value: tensor([[-23.6136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08697905912556214, distance: 1.1930737266508744 entropy 9.419844464317734
epoch: 17, step: 91
	action: tensor([[-1830.6309, -6039.9348, -1228.5120,  2756.9367, -2128.7129,  1306.8314,
         -2865.5262]], dtype=torch.float64)
	q_value: tensor([[-21.8487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40065986658421293, distance: 1.3543254377386906 entropy 9.265332006608391
epoch: 17, step: 92
	action: tensor([[ -755.9347, -4202.6953, -1818.5227,  1008.4812, -2460.6307,  -134.8907,
          -515.3429]], dtype=torch.float64)
	q_value: tensor([[-24.6408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37258439818547506, distance: 0.9064301656985233 entropy 9.362408330820386
epoch: 17, step: 93
	action: tensor([[-2080.6620,   287.1112, -9162.4257, -1404.1024,  -829.3630, -2594.7205,
          2552.8537]], dtype=torch.float64)
	q_value: tensor([[-22.5851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41365296675058016, distance: 0.8762621631019356 entropy 9.404556425573253
epoch: 17, step: 94
	action: tensor([[ 1622.6862, -5603.5830,  -549.4264,  2081.4411,    34.5602,  2974.8227,
          2461.3762]], dtype=torch.float64)
	q_value: tensor([[-26.7630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.069091078544098
epoch: 17, step: 95
	action: tensor([[ -319.6179,  -189.9287,  -263.6893,  -437.7042,    25.8666, -1813.3123,
         -3040.7712]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.791917631038453
epoch: 17, step: 96
	action: tensor([[-2594.2531, -2910.0664,  1712.8750,  2095.1143, -2118.4713,    30.0067,
           843.3615]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15439838236406833, distance: 1.0523000616035256 entropy 8.791917631038453
epoch: 17, step: 97
	action: tensor([[ -991.4653, -7750.0643,  2391.2162,  1638.8778,  2049.2729,  -508.4407,
          3793.5168]], dtype=torch.float64)
	q_value: tensor([[-24.9732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03138646890098329, distance: 1.1621639724226567 entropy 9.40634367978925
epoch: 17, step: 98
	action: tensor([[  682.4466, -1740.1618,   -29.8093,  5723.6407, -4567.3655, -3745.0821,
          5333.7803]], dtype=torch.float64)
	q_value: tensor([[-21.6061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955508286865012, distance: 0.9604649581130997 entropy 9.36923228121607
epoch: 17, step: 99
	action: tensor([[ 3215.7238, -3796.5784, -2048.8062,  -128.9006, -1140.3920,   430.4264,
           -45.2102]], dtype=torch.float64)
	q_value: tensor([[-20.2300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09737485433243076, distance: 1.0872024122270398 entropy 9.121541716581426
epoch: 17, step: 100
	action: tensor([[ 4328.1963, -2258.9591, -3383.7210,   260.6777, -6210.8629,   -74.9497,
          6320.6452]], dtype=torch.float64)
	q_value: tensor([[-23.3184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2655628993089898, distance: 0.9806950811266443 entropy 9.271940381007585
epoch: 17, step: 101
	action: tensor([[-1334.0272, -3233.2027,  1368.2872,  1719.4938, -1344.8440, -2783.2715,
          4232.9485]], dtype=torch.float64)
	q_value: tensor([[-24.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7109468267019046, distance: 1.4968385155444142 entropy 9.42129047442396
epoch: 17, step: 102
	action: tensor([[  173.3966,  4163.9200,  2806.9620,   431.7527, -2129.8265,  2358.7879,
          3750.2126]], dtype=torch.float64)
	q_value: tensor([[-20.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4065804235490146, distance: 0.8815310782568277 entropy 9.301535139941551
epoch: 17, step: 103
	action: tensor([[-1934.4311,  1436.9600,  1370.1239,  5282.5425,  2271.6393,   365.8821,
          3674.9631]], dtype=torch.float64)
	q_value: tensor([[-20.8145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07330594130174173, distance: 1.1016024232562611 entropy 9.15204575570989
epoch: 17, step: 104
	action: tensor([[-776.2145, 2485.7821, -317.4478, 3414.6839, -857.3921, 2759.1079,
         2741.1521]], dtype=torch.float64)
	q_value: tensor([[-22.3810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16861732445141153, distance: 1.2370659506857835 entropy 9.262894464917276
epoch: 17, step: 105
	action: tensor([[ 4700.2316,  1957.2748, -2434.0296,  4273.2810, -1490.1631, -3686.2145,
          5627.1360]], dtype=torch.float64)
	q_value: tensor([[-30.0018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.466615742691072, distance: 0.8357507788669764 entropy 9.474151620763667
epoch: 17, step: 106
	action: tensor([[ 1522.2325, -4612.6317, -1447.9339,  2717.8509,   458.9683,   902.9933,
          -222.8493]], dtype=torch.float64)
	q_value: tensor([[-26.2855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.426537329142219
epoch: 17, step: 107
	action: tensor([[ -146.8694,  -347.9500, -2080.7660,  -163.0139,  2735.0097,  1542.3412,
           838.8756]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.791917631038453
epoch: 17, step: 108
	action: tensor([[-780.4349, -726.1018, 1119.7067, 4008.7647, -654.8200, -972.3630,
          731.6324]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24251368090469305, distance: 1.2755787712881406 entropy 8.791917631038453
epoch: 17, step: 109
	action: tensor([[-3885.3559, -4028.9131, -1990.0193,  2009.2758,  3203.9175,  1724.4199,
         -1259.9670]], dtype=torch.float64)
	q_value: tensor([[-18.1824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0246312658335619, distance: 1.1583518462876519 entropy 9.240210293084619
epoch: 17, step: 110
	action: tensor([[-8740.3603,   582.9283, -1631.4383,  4965.9798,  -440.4833,  3539.0198,
          -447.6747]], dtype=torch.float64)
	q_value: tensor([[-23.9658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42012093061825795, distance: 0.87141575958041 entropy 9.413264150312608
epoch: 17, step: 111
	action: tensor([[ 2478.5332, -1605.3018, -2124.3647,   780.0203, -3002.7424,  1067.3704,
          1631.3837]], dtype=torch.float64)
	q_value: tensor([[-23.6509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32938821464026435, distance: 0.9371136934683091 entropy 9.275910814112274
epoch: 17, step: 112
	action: tensor([[  134.4343,   118.9482, -1083.5960, -2868.6555,    80.9899,  -416.6221,
          1563.9998]], dtype=torch.float64)
	q_value: tensor([[-20.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9280714975337141, distance: 0.3069072886715285 entropy 9.226185314275293
epoch: 17, step: 113
	action: tensor([[-3106.5045,  -559.1389,  -486.7941,  1394.9061,  -933.2223, -1557.3778,
           180.3353]], dtype=torch.float64)
	q_value: tensor([[-19.7074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19898425093155947, distance: 1.0241821932065702 entropy 8.97969794765621
epoch: 17, step: 114
	action: tensor([[  233.8063,   623.7237,  2070.7116,  6961.3235,  3868.4843, -2285.4651,
           614.0424]], dtype=torch.float64)
	q_value: tensor([[-25.7450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46090162982009253, distance: 0.8402155265259441 entropy 9.527430417302188
epoch: 17, step: 115
	action: tensor([[   94.5091, -1388.1606,  2101.0455, -3171.0373, -5954.9218,  5549.6188,
          3122.7032]], dtype=torch.float64)
	q_value: tensor([[-25.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38912357199024294, distance: 0.8944032670038979 entropy 9.42592677632312
epoch: 17, step: 116
	action: tensor([[ -879.8818,  1159.7581, -3812.5891,  7921.5858,  2634.4320,  1269.0466,
          -582.6178]], dtype=torch.float64)
	q_value: tensor([[-21.6829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.164967231695373
epoch: 17, step: 117
	action: tensor([[ 2164.4089, -2591.4013,   657.5104,  -224.0314, -3757.9146, -1308.5906,
         -2259.8980]], dtype=torch.float64)
	q_value: tensor([[-26.7733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5242617685508124, distance: 0.7892974871184744 entropy 8.791917631038453
epoch: 17, step: 118
	action: tensor([[1830.1018, 2663.6520,  642.3444, 1081.9384,  594.2502, 1418.8906,
          315.6475]], dtype=torch.float64)
	q_value: tensor([[-23.0784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5256403153826492, distance: 0.7881530836867272 entropy 9.241643836531319
epoch: 17, step: 119
	action: tensor([[-1616.8317,  3558.5527, -2877.0476,  -724.8436, -1911.9536,   344.7074,
           139.7393]], dtype=torch.float64)
	q_value: tensor([[-21.8132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018710976331947515, distance: 1.133587801348052 entropy 9.275958874797817
epoch: 17, step: 120
	action: tensor([[-3696.2228,   569.0955, -1251.5101,   821.8045, -1621.5651,  5147.4813,
          2907.6811]], dtype=torch.float64)
	q_value: tensor([[-27.7214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1305711379407537, distance: 1.0670228501003785 entropy 9.156222687577824
epoch: 17, step: 121
	action: tensor([[-1238.8884, -2042.2595, -3691.3778,   273.4999,  2791.2399,  4591.8140,
         -4052.1264]], dtype=torch.float64)
	q_value: tensor([[-26.2632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38092383778283057, distance: 1.3447500111293784 entropy 8.982170261300684
epoch: 17, step: 122
	action: tensor([[-3286.4786, -1480.5884, -5246.0855, -3424.9944,  -565.7397,    32.7672,
         -1426.0088]], dtype=torch.float64)
	q_value: tensor([[-25.2306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.320698104670674, distance: 1.743275461572006 entropy 9.408409202479419
epoch: 17, step: 123
	action: tensor([[-1380.8317,  1046.8047,  -423.7480,  2590.7815, -1510.5509,  -890.9622,
           965.2659]], dtype=torch.float64)
	q_value: tensor([[-16.6905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009311587389684628, distance: 1.139003962514665 entropy 8.922750381633664
epoch: 17, step: 124
	action: tensor([[-5274.4973, -1599.1293,  1856.0380,  -103.3733,   851.8125, -7138.7362,
         -1474.0486]], dtype=torch.float64)
	q_value: tensor([[-26.4227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14665373864352382, distance: 1.2253857876684129 entropy 9.508386369322087
epoch: 17, step: 125
	action: tensor([[  858.0488, -2079.2656,  7401.5886,  -475.2767, -5551.8528,  9080.4836,
         -3556.8479]], dtype=torch.float64)
	q_value: tensor([[-23.8659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010007892302025478, distance: 1.1500562353800619 entropy 9.531316155509444
epoch: 17, step: 126
	action: tensor([[-2360.1913,  -970.5637,  4971.1226,  2730.1754, -1622.1803,  -423.6783,
          2942.4288]], dtype=torch.float64)
	q_value: tensor([[-21.8310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7476595248517282, distance: 1.5128125107118464 entropy 9.193331644398176
epoch: 17, step: 127
	action: tensor([[-5392.7407, -3725.0411,   534.0364,  2372.7647, -2777.8915, -2309.8542,
          3602.9627]], dtype=torch.float64)
	q_value: tensor([[-18.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6023203386443163, distance: 1.448542913852933 entropy 9.263917066552414
LOSS epoch 17 actor 242.4139114848278 critic 242.04350697495087
epoch: 18, step: 0
	action: tensor([[-6168.3409, -2250.1554,  3547.2334, -1155.3849, -1422.2067,   -63.5686,
          3802.5431]], dtype=torch.float64)
	q_value: tensor([[-21.5802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5705336802639405, distance: 1.8347138632946378 entropy 9.434751942636684
epoch: 18, step: 1
	action: tensor([[ 1018.9992,   101.0997,  -626.5274,  1280.2849, -3890.8349,  2578.2124,
          2292.8523]], dtype=torch.float64)
	q_value: tensor([[-20.4520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6827236414530167, distance: 0.6445781052005177 entropy 9.248091595870672
epoch: 18, step: 2
	action: tensor([[-1800.4151, -1211.0304, -4993.1291,  4525.3854,  -819.1358, -4957.8535,
         -6266.0321]], dtype=torch.float64)
	q_value: tensor([[-22.2091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11622344637922655, distance: 1.0757910604781211 entropy 9.396800888639737
epoch: 18, step: 3
	action: tensor([[  -462.6536,    846.4381,   1216.2724,   -864.3650, -10539.2028,
          -4958.7583,   3643.6747]], dtype=torch.float64)
	q_value: tensor([[-23.8539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12404239082547519, distance: 1.071021620782217 entropy 9.539924091194353
epoch: 18, step: 4
	action: tensor([[ 5455.8949,  1496.0668, -3351.6009,  5996.5518, -2038.8630,   -13.7500,
          1196.6460]], dtype=torch.float64)
	q_value: tensor([[-35.5023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.450071107444273
epoch: 18, step: 5
	action: tensor([[ 2099.8537, -2627.0175,  2941.0778,  3898.5876,  1353.0001, -1612.1968,
          1072.7785]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32907090607253453, distance: 0.9373353709309228 entropy 8.872916168659058
epoch: 18, step: 6
	action: tensor([[-1322.4423,  -322.3559,   672.1719, -1884.3323, -3563.2913, -3538.7841,
         -1350.7621]], dtype=torch.float64)
	q_value: tensor([[-22.2173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08877808862858738, distance: 1.1940606304224004 entropy 9.190795812931102
epoch: 18, step: 7
	action: tensor([[-492.9692, -840.0764, -914.6905, 2927.0607, -558.4834,  649.5142,
         -790.4423]], dtype=torch.float64)
	q_value: tensor([[-18.0852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45931061788776684, distance: 0.8414544544784639 entropy 9.041034055219395
epoch: 18, step: 8
	action: tensor([[-1292.2318, -5746.2523,  3969.7984, -3075.6056, -4390.3819, -2593.9214,
          -937.5074]], dtype=torch.float64)
	q_value: tensor([[-21.0209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0055562669503066, distance: 1.620593597105321 entropy 9.274773290234256
epoch: 18, step: 9
	action: tensor([[-1422.4943,   -12.6426, -1388.5441, -1607.0654,  1771.1776,   356.2704,
          -394.4307]], dtype=torch.float64)
	q_value: tensor([[-18.7599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8332023598545428, distance: 1.5493940972192635 entropy 9.058755530515352
epoch: 18, step: 10
	action: tensor([[-3719.4067, -2640.2035, -1877.6632,  2080.6198,  1338.5176,  3107.7990,
         -1556.6925]], dtype=torch.float64)
	q_value: tensor([[-24.6482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28897995364671214, distance: 1.2992112570699212 entropy 9.163388028184025
epoch: 18, step: 11
	action: tensor([[-2548.6159,  1060.5368,   150.9331, -2343.1630, -4073.6973,   318.6034,
           352.2191]], dtype=torch.float64)
	q_value: tensor([[-19.6727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07672708832985009, distance: 1.1874340898598237 entropy 9.165850070407242
epoch: 18, step: 12
	action: tensor([[-1061.3757, -4244.3940,  2519.2357, -2001.4205,  5965.9846, -1080.7604,
           991.3721]], dtype=torch.float64)
	q_value: tensor([[-32.7460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15038049506951867, distance: 1.2273754946025175 entropy 9.260868922576707
epoch: 18, step: 13
	action: tensor([[-2459.8911, -5660.0774,  -428.0209,   431.1418,    24.5212, -2214.2470,
          1849.7754]], dtype=torch.float64)
	q_value: tensor([[-28.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3857715687512908, distance: 1.3471083146407672 entropy 9.460740811765168
epoch: 18, step: 14
	action: tensor([[-1523.6087,  -489.3287,   351.2438,  3199.9438,  3068.8310,  1396.1250,
          -857.4828]], dtype=torch.float64)
	q_value: tensor([[-22.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1235104436206584, distance: 1.6675693104922222 entropy 9.298839076447633
epoch: 18, step: 15
	action: tensor([[-2606.3801, -3802.6022, -1329.5652, -6674.8359,  8647.3428,  2865.5867,
         -4204.6705]], dtype=torch.float64)
	q_value: tensor([[-25.8246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31428639149396287, distance: 1.3119029203543557 entropy 9.633884740658512
epoch: 18, step: 16
	action: tensor([[-3252.8733,   394.2607,  2092.1404,  -922.2521, -4865.2885,  3096.2406,
         -1287.2933]], dtype=torch.float64)
	q_value: tensor([[-21.7912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06991957766121959, distance: 1.1836744149954446 entropy 9.21253153190788
epoch: 18, step: 17
	action: tensor([[-1268.2671, -4120.6604,  4318.2830,    71.5633, -2001.4377,  2043.7208,
          -997.9356]], dtype=torch.float64)
	q_value: tensor([[-33.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14214465176033675, distance: 1.0598971512217223 entropy 9.287806846472156
epoch: 18, step: 18
	action: tensor([[-4657.4272,  -169.5663,  4225.5405,  1266.1819,  1869.2656, -1726.0998,
          1303.5825]], dtype=torch.float64)
	q_value: tensor([[-24.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13231953840725597, distance: 1.2177024893883437 entropy 9.416704196353802
epoch: 18, step: 19
	action: tensor([[-7560.4504,  2543.0807,   675.9258,   359.0610,   608.7695,  2886.6060,
          5005.7407]], dtype=torch.float64)
	q_value: tensor([[-26.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06564710303164423, distance: 1.1061452570535573 entropy 9.684919187341137
epoch: 18, step: 20
	action: tensor([[-1542.3676, -7282.5875,  1448.0036,  1717.0646,   215.6034,  3556.8051,
         -4397.6548]], dtype=torch.float64)
	q_value: tensor([[-30.2176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36987632830703565, distance: 1.3393601517725693 entropy 9.470265005445189
epoch: 18, step: 21
	action: tensor([[ 1436.5068, -1039.1275, -7703.2118, 10705.5894,   -14.8410,   760.6749,
         -1906.7424]], dtype=torch.float64)
	q_value: tensor([[-25.6257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47570332816522165, distance: 0.8286006005135048 entropy 9.487693552585947
epoch: 18, step: 22
	action: tensor([[-1652.7447, -1688.8433,  2074.5837,  6431.5367,  1878.6858,  2417.9301,
          3263.5653]], dtype=torch.float64)
	q_value: tensor([[-24.2859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0831760095368188, distance: 1.0957202243010298 entropy 9.451638663508161
epoch: 18, step: 23
	action: tensor([[-1187.8699, -1527.0287, -3266.7463,  1561.4056,  1464.8866,  2483.3543,
          3286.5582]], dtype=torch.float64)
	q_value: tensor([[-25.1635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30253836882710383, distance: 0.9556895865518422 entropy 9.458116971795517
epoch: 18, step: 24
	action: tensor([[-4114.9289, -3837.6632, -2287.9810, -3880.3336, -1751.3778, -4794.7288,
           868.7858]], dtype=torch.float64)
	q_value: tensor([[-22.4112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.52431933262049, distance: 1.4128454981602665 entropy 9.269742485269296
epoch: 18, step: 25
	action: tensor([[-1493.0338,  -509.7656,  2843.1172,  5073.3211,   108.6195, -2602.7647,
          3945.5440]], dtype=torch.float64)
	q_value: tensor([[-21.7340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13903304433159014, distance: 1.061817637385982 entropy 9.303860969277498
epoch: 18, step: 26
	action: tensor([[  576.4101, -2540.8159, -3584.6989, -4884.5757,  5183.5480,  9566.9077,
          4501.0566]], dtype=torch.float64)
	q_value: tensor([[-23.7472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31329554057505926, distance: 1.311408300111681 entropy 9.452765339253684
epoch: 18, step: 27
	action: tensor([[-3226.8914, -3425.6280, -1815.8555,  3198.2562, -2718.4769,  -962.4173,
          2375.7557]], dtype=torch.float64)
	q_value: tensor([[-22.3907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1114298996859624, distance: 1.2064177859164655 entropy 9.300802322656198
epoch: 18, step: 28
	action: tensor([[ 1955.0902,  2559.0664, -2106.0032, -1833.5536, -4433.5268,  4566.7944,
          4073.5246]], dtype=torch.float64)
	q_value: tensor([[-21.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.351251279718396
epoch: 18, step: 29
	action: tensor([[-1825.4328, -1668.6017,  -993.0303,  2352.6714,  -416.6113,  -409.5188,
         -1001.8310]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10233242792039099, distance: 1.2014701488745112 entropy 8.872916168659058
epoch: 18, step: 30
	action: tensor([[-1970.6282,  -561.9210,  1267.4121,   145.9612,  3020.1107,  2686.4300,
          3195.5376]], dtype=torch.float64)
	q_value: tensor([[-23.7429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3400915871003134, distance: 1.3247195135247356 entropy 9.502254068376518
epoch: 18, step: 31
	action: tensor([[  3397.7030,  -1888.4715,  -3563.6008,     57.6625,  -1386.4590,
         -11150.3147,   -845.8690]], dtype=torch.float64)
	q_value: tensor([[-26.4372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15382434250978727, distance: 1.0526571799431412 entropy 9.613896143836337
epoch: 18, step: 32
	action: tensor([[-2453.2573,  1796.4198,  -381.3921,   460.4473,  -152.1669,  4816.4207,
          2248.2634]], dtype=torch.float64)
	q_value: tensor([[-23.6655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6522647502757755, distance: 0.6748092884356126 entropy 9.362498489021508
epoch: 18, step: 33
	action: tensor([[ 4658.7214, -2855.5746,  1809.0045,  1198.4223,  1536.0900,  4666.1779,
          1070.4347]], dtype=torch.float64)
	q_value: tensor([[-28.4212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.499050448186296
epoch: 18, step: 34
	action: tensor([[-2570.0467,  1879.3599, -2690.5934,  1071.1201,  -537.5086,   599.6418,
          2939.2473]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11186095742122804, distance: 1.0784429464736445 entropy 8.872916168659058
epoch: 18, step: 35
	action: tensor([[ 1853.3387,   106.3414,  -773.8324,  2844.2547, -2252.6797,  2799.9327,
          -898.5722]], dtype=torch.float64)
	q_value: tensor([[-27.3296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6090477031041344, distance: 0.7155147282700163 entropy 9.061832570390536
epoch: 18, step: 36
	action: tensor([[-2566.5298,  1914.3321,   585.9277,  1652.6381, -3757.1895, -2144.1224,
         -1524.3715]], dtype=torch.float64)
	q_value: tensor([[-25.7390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7990553764887653, distance: 0.5129734508488686 entropy 9.285645910710873
epoch: 18, step: 37
	action: tensor([[-681.8749,  653.9324, -867.3646, -340.6079,  498.9111, -229.5900,
          239.4257]], dtype=torch.float64)
	q_value: tensor([[-21.5566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20488085323347305, distance: 1.0204055193867891 entropy 9.196566785197236
epoch: 18, step: 38
	action: tensor([[ 1716.5092, -3723.7445,  3995.6439, -1899.9767, -2857.9834,  2766.2226,
          3566.0802]], dtype=torch.float64)
	q_value: tensor([[-27.1967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1292088520854371, distance: 1.2160287147012445 entropy 9.213581073231044
epoch: 18, step: 39
	action: tensor([[  454.9859, -4283.0125,   453.1328,   374.2691,  3726.9966,  -387.6618,
           259.9746]], dtype=torch.float64)
	q_value: tensor([[-20.7082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17378373087545473, distance: 1.0401681810905172 entropy 9.260342802964207
epoch: 18, step: 40
	action: tensor([[ -159.4400,  -445.4346, -2233.8454,  -777.8212, -2023.5097,  1632.2495,
           917.2905]], dtype=torch.float64)
	q_value: tensor([[-21.5516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1797132905383717, distance: 1.0364289390108095 entropy 9.193781679809417
epoch: 18, step: 41
	action: tensor([[-4849.4683,   744.0395, -4367.8765,  -640.3534, -5306.9437,  -380.4504,
         -2314.9022]], dtype=torch.float64)
	q_value: tensor([[-19.5691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.206025696597392
epoch: 18, step: 42
	action: tensor([[-1608.0334,  -463.3846,   627.5621, -1214.2928,  1246.4686,   461.6572,
         -2609.7412]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07388143428723692, distance: 1.1012603130863907 entropy 8.872916168659058
epoch: 18, step: 43
	action: tensor([[ -920.0107, -5998.1652, -4830.9883,  2744.3328,   833.5781,  1109.2625,
         -1747.8725]], dtype=torch.float64)
	q_value: tensor([[-27.6889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.916960133904666, distance: 1.5843941632933694 entropy 9.428498976538071
epoch: 18, step: 44
	action: tensor([[-5376.5387, -5961.0447, -2677.2975,    44.6021,  -261.2062, -4033.8124,
          4239.0520]], dtype=torch.float64)
	q_value: tensor([[-22.6516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28959899540020473, distance: 1.299523197326256 entropy 9.317267452698976
epoch: 18, step: 45
	action: tensor([[-4314.2270, -2754.1253, -2110.9353,  5150.5328, -2027.9913,  1457.5533,
          5065.6936]], dtype=torch.float64)
	q_value: tensor([[-23.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00445749380622229, distance: 1.141790951777745 entropy 9.463391861971973
epoch: 18, step: 46
	action: tensor([[  882.5950, -2728.7145,  3195.4235,  5228.7210,   642.8749,  -565.4319,
          3145.8937]], dtype=torch.float64)
	q_value: tensor([[-25.3691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0241931849147754, distance: 1.1581041932490137 entropy 9.417077330053717
epoch: 18, step: 47
	action: tensor([[-4514.8487, -6319.7896,  2146.7104,  2787.5207,  -690.7168, -4150.1392,
          1388.8353]], dtype=torch.float64)
	q_value: tensor([[-24.1684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5141626667849302, distance: 1.4081306779980436 entropy 9.413780821052224
epoch: 18, step: 48
	action: tensor([[ 574.9282,  390.5174, -433.3323, -987.8214, 1490.7525, 1918.8017,
         3169.9066]], dtype=torch.float64)
	q_value: tensor([[-21.5965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.386313938310934
epoch: 18, step: 49
	action: tensor([[ 1500.8971,  -601.0487, -2336.4637,  1267.3725,  -654.7057, -2350.4467,
          1170.3730]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3299579940701305, distance: 0.9367155036497484 entropy 8.872916168659058
epoch: 18, step: 50
	action: tensor([[ 1790.1859, -2064.8034, -1745.9901,   155.2438,  -186.3788, -3037.1844,
          2098.9613]], dtype=torch.float64)
	q_value: tensor([[-18.6481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5578365240895219, distance: 0.7609359911752304 entropy 9.160530273641568
epoch: 18, step: 51
	action: tensor([[  397.7825, -1030.1421, -2840.2290,  1636.2765,  -199.3361,   381.9156,
          2968.9152]], dtype=torch.float64)
	q_value: tensor([[-26.1193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2805877545731048, distance: 0.9706118879407173 entropy 9.283134335351898
epoch: 18, step: 52
	action: tensor([[  415.7424, -1086.3581, -6461.7253,  1861.6185, -1996.8739,  2824.9066,
          4002.5775]], dtype=torch.float64)
	q_value: tensor([[-25.5870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5892552217260225, distance: 0.7334030619217146 entropy 9.324066888795008
epoch: 18, step: 53
	action: tensor([[  336.1563,  -797.9696,  3493.7467,  1759.0814, -3344.3504,  6099.5205,
          3581.5310]], dtype=torch.float64)
	q_value: tensor([[-22.1875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5070171923097297, distance: 0.8034753920883251 entropy 9.272532415650227
epoch: 18, step: 54
	action: tensor([[-4810.2137, -2515.6316,  3532.4548,   701.8025, -1490.0355, -5242.0798,
         -3647.1912]], dtype=torch.float64)
	q_value: tensor([[-21.6438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16330138341544997, distance: 1.0467457800893494 entropy 9.229549278485653
epoch: 18, step: 55
	action: tensor([[-2799.0726, -2140.2930, -3568.1676,   806.0700,  -790.0492, -6923.9170,
         -2440.4276]], dtype=torch.float64)
	q_value: tensor([[-21.2205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5570459575280851, distance: 1.427931614242965 entropy 9.327722140792073
epoch: 18, step: 56
	action: tensor([[  805.9171, -1133.7382,  -110.6973,   658.1558, -1830.5982,  1926.8985,
         -2109.4585]], dtype=torch.float64)
	q_value: tensor([[-23.4150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22303179139125795, distance: 1.0086913992889113 entropy 9.444092564393397
epoch: 18, step: 57
	action: tensor([[ -884.3071, -2815.3808, -2106.1260,  2131.3194,  -676.0197,  1731.3544,
          4321.0654]], dtype=torch.float64)
	q_value: tensor([[-26.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6949554904752944, distance: 1.4898270056244136 entropy 9.488145559681515
epoch: 18, step: 58
	action: tensor([[ -1812.3840, -12665.4936,  -7895.8913,  -4227.1930,  -2561.4640,
          -6905.8758,   5131.9116]], dtype=torch.float64)
	q_value: tensor([[-29.7043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8398338638173526, distance: 1.5521939893515475 entropy 9.657606933124683
epoch: 18, step: 59
	action: tensor([[-2259.1566,   978.1086, -1275.4708,   658.0588,  3264.1235,  -555.9732,
          1183.8133]], dtype=torch.float64)
	q_value: tensor([[-22.8907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23596319373860786, distance: 1.0002621457521177 entropy 9.361467572103567
epoch: 18, step: 60
	action: tensor([[-5394.5227, -3242.9435,   596.0667,  2644.8624, -2075.4228,  1680.0968,
          -382.5846]], dtype=torch.float64)
	q_value: tensor([[-28.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6649765903721208, distance: 1.4765928431204032 entropy 9.596989477129712
epoch: 18, step: 61
	action: tensor([[-6133.4573,   365.6864,  1250.7236,  1724.0605,  1732.0041,  1626.3785,
          4459.7313]], dtype=torch.float64)
	q_value: tensor([[-21.9244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3640839326147418, distance: 0.9125498381038315 entropy 9.345980404273872
epoch: 18, step: 62
	action: tensor([[  772.3328,  -195.6204,  2595.0736,   692.6590,   553.4556, -3545.3074,
           860.2825]], dtype=torch.float64)
	q_value: tensor([[-25.4647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9453917183019169, distance: 0.26741511365908105 entropy 9.372665372978148
epoch: 18, step: 63
	action: tensor([[-3009.4957,  2882.5784, -3073.4440,  5588.0546,   722.6225, -2202.2196,
          -493.2410]], dtype=torch.float64)
	q_value: tensor([[-16.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.916094007018499
epoch: 18, step: 64
	action: tensor([[-1313.3805,  -963.8011,  1117.9504, -2297.3679,  1240.1639, -1257.7577,
           839.0182]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31354705094768853, distance: 1.31153386857282 entropy 8.872916168659058
epoch: 18, step: 65
	action: tensor([[-7727.3229,  5484.8894,   566.3061, -1086.4116,  1724.7018,  1581.4753,
         -1455.5068]], dtype=torch.float64)
	q_value: tensor([[-26.8686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24440851911359718, distance: 0.9947185561185913 entropy 9.383621600141362
epoch: 18, step: 66
	action: tensor([[-4985.3360, -3350.9521,  2617.1689,  1243.7390,   878.4983,  2479.6226,
            75.9297]], dtype=torch.float64)
	q_value: tensor([[-32.9099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2217569852961927, distance: 1.2648793679400667 entropy 9.483195944901675
epoch: 18, step: 67
	action: tensor([[-1564.4134,  1625.1888,  2052.3639,  1393.6776,  2380.9335,  4750.4153,
          -553.7636]], dtype=torch.float64)
	q_value: tensor([[-23.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2636888068822929, distance: 0.9819455233000699 entropy 9.387179987033736
epoch: 18, step: 68
	action: tensor([[ 3219.9199,  -125.8407,   810.0788,  -125.0542, -1745.9006,  -804.1370,
         -2778.6856]], dtype=torch.float64)
	q_value: tensor([[-28.2186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.389807646337497
epoch: 18, step: 69
	action: tensor([[2067.3847,  233.8496, -710.7704, 1373.4023, 2595.3186,   13.6838,
           66.1358]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.872916168659058
epoch: 18, step: 70
	action: tensor([[-3594.9482, -2707.7381,  1556.4510,  -633.6911,  -725.8861, -2336.4822,
         -2672.7950]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16496429158653925, distance: 1.2351309375814798 entropy 8.872916168659058
epoch: 18, step: 71
	action: tensor([[-4276.5814, -2702.3043, -1627.1883,  -547.1959, -3458.0037,  3433.9915,
          4619.3475]], dtype=torch.float64)
	q_value: tensor([[-31.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0373928196236164, distance: 1.1227452896700083 entropy 9.497735673593278
epoch: 18, step: 72
	action: tensor([[-1070.8560,  -437.9143,  -959.9918, -2613.6739,  3832.9681,   901.0632,
          2113.7697]], dtype=torch.float64)
	q_value: tensor([[-27.9486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6552974373558567, distance: 1.472294584446345 entropy 9.468764745529995
epoch: 18, step: 73
	action: tensor([[-4845.6999, -4235.8573, -1235.2352,  3440.8045, -2133.8208,  6738.8657,
           198.0479]], dtype=torch.float64)
	q_value: tensor([[-22.9489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8871176151701767, distance: 1.5720131605648182 entropy 9.391330837830646
epoch: 18, step: 74
	action: tensor([[-3336.9949, -3026.7301,  1794.4134,  3424.6898,   279.9403,  8152.7408,
         -1678.4779]], dtype=torch.float64)
	q_value: tensor([[-26.6118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3903370319809101, distance: 1.3493255329986635 entropy 9.568382869465653
epoch: 18, step: 75
	action: tensor([[-3634.1386, -3781.8542, -3053.0708,  2011.2780,  5531.5461, -2349.3796,
          3430.9765]], dtype=torch.float64)
	q_value: tensor([[-27.4672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10381425484926288, distance: 1.083317375980024 entropy 9.635774318918688
epoch: 18, step: 76
	action: tensor([[-1778.9139,  -472.2352,  -712.1756, -1719.8158,  1158.1941, -2461.3099,
          3672.9345]], dtype=torch.float64)
	q_value: tensor([[-23.0488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5694794580306792, distance: 1.4336215188911081 entropy 9.411924356243475
epoch: 18, step: 77
	action: tensor([[-1338.8322, -4300.2774,  2386.8617,  4073.3269,  2432.5995, -2312.8958,
          4495.3149]], dtype=torch.float64)
	q_value: tensor([[-26.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.623185606255054, distance: 1.4579438045452622 entropy 9.426141036198846
epoch: 18, step: 78
	action: tensor([[ 1514.6113, -5618.2392,  2297.6928,   955.7741,  3635.7230,   573.3262,
            60.6585]], dtype=torch.float64)
	q_value: tensor([[-22.8601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3193835373801579, distance: 1.3144444096803383 entropy 9.42453686239107
epoch: 18, step: 79
	action: tensor([[-2588.4306,  2815.4701,   959.7062,  -769.8037,  4719.4277,  -932.9480,
          2498.3271]], dtype=torch.float64)
	q_value: tensor([[-22.5974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02108428712302901, distance: 1.1322161438818321 entropy 9.529913941671158
epoch: 18, step: 80
	action: tensor([[  -64.3281,    54.4043, -2728.2227,  -411.7589, -1497.1157,  3138.1459,
         -1717.2063]], dtype=torch.float64)
	q_value: tensor([[-32.8776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.372673985453371
epoch: 18, step: 81
	action: tensor([[-1108.2660,   -81.4524,  -808.2106, -1497.8254,  1060.3564,  3384.5951,
          -227.9862]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10926685146877113, distance: 1.080016774040899 entropy 8.872916168659058
epoch: 18, step: 82
	action: tensor([[-5027.7059,  -741.8742, -4764.5665,  2458.5557,  1626.4175, -2894.6577,
          5726.1796]], dtype=torch.float64)
	q_value: tensor([[-30.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0967875796565933, distance: 1.657043505671366 entropy 9.443561299026543
epoch: 18, step: 83
	action: tensor([[-1287.6823,   506.3878, -3666.5947, -1069.5183,  3699.6974,  2591.1310,
         -1831.8344]], dtype=torch.float64)
	q_value: tensor([[-23.1463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38180826864396755, distance: 0.8997426118930447 entropy 9.412395234366654
epoch: 18, step: 84
	action: tensor([[-4002.9612, -4362.9858,  4416.1984, -2696.0521,   966.4962,  1034.6124,
          4686.7663]], dtype=torch.float64)
	q_value: tensor([[-25.0100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07080108861782453, distance: 1.1841619316071021 entropy 9.40824707504011
epoch: 18, step: 85
	action: tensor([[-2236.0817, -4934.3179,  -683.6064,  2243.9306, -1493.8668, -3609.4636,
         -1354.6152]], dtype=torch.float64)
	q_value: tensor([[-28.3955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8790534838447024, distance: 1.5686507595129229 entropy 9.393455886965347
epoch: 18, step: 86
	action: tensor([[-4718.5849, -2542.5446,  2594.6088,  1957.6532,   791.6488, -5353.0179,
         -2773.1235]], dtype=torch.float64)
	q_value: tensor([[-23.9394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9262288074793248, distance: 1.5882198881313299 entropy 9.389234297966137
epoch: 18, step: 87
	action: tensor([[ -882.7346,  2811.3194, -3228.7406,  2160.2686, -4042.3080,   301.2366,
         -1454.4300]], dtype=torch.float64)
	q_value: tensor([[-21.6073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6943064165391806, distance: 0.6327029432417193 entropy 9.373365164982143
epoch: 18, step: 88
	action: tensor([[-3460.4866, -3302.2001,  7102.5915,  3780.8363,   856.1283, -2520.2472,
          -127.4432]], dtype=torch.float64)
	q_value: tensor([[-26.9909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008420447326575653, distance: 1.14915209942157 entropy 9.484853614147042
epoch: 18, step: 89
	action: tensor([[ 1278.9972, -2609.0524,  1059.4100,  1584.3318, -4938.1996,  7452.3661,
           917.8565]], dtype=torch.float64)
	q_value: tensor([[-25.8700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44579157506712297, distance: 0.8519090954470495 entropy 9.555275594653136
epoch: 18, step: 90
	action: tensor([[ 1585.7519, -4063.6112, -1309.7518,  -804.9546,   267.0748, -1672.1770,
         -2944.9319]], dtype=torch.float64)
	q_value: tensor([[-24.6610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5072021557072064, distance: 1.404890405547717 entropy 9.414423566747065
epoch: 18, step: 91
	action: tensor([[ 1604.5034, -2827.5906,  4753.3346,   -86.4358,  3212.6027, -2628.5545,
           521.5643]], dtype=torch.float64)
	q_value: tensor([[-27.5024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09263051796207245, distance: 1.090055918937209 entropy 9.50704184982405
epoch: 18, step: 92
	action: tensor([[  628.8188, -1667.8788,   935.6019,  4337.9081, -1449.6093,  1219.8922,
         -1770.4657]], dtype=torch.float64)
	q_value: tensor([[-28.5882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23255497062526231, distance: 1.002490653029088 entropy 9.46916516512232
epoch: 18, step: 93
	action: tensor([[-3060.2838, -3037.0832,  4713.9124, -2376.3983, -2904.4528,  2347.5002,
          5616.3065]], dtype=torch.float64)
	q_value: tensor([[-21.1099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26487851230990866, distance: 1.2870075680178898 entropy 9.18337959051386
epoch: 18, step: 94
	action: tensor([[-3254.0554, -1625.3442,  2364.3468,    14.4789, -1606.8050,  5023.5401,
         -2597.1877]], dtype=torch.float64)
	q_value: tensor([[-23.4688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45126623981234126, distance: 1.3785744956461259 entropy 9.240951193012704
epoch: 18, step: 95
	action: tensor([[ 1543.2795, -6373.6579,  3893.6007, -8378.3397,   455.5448,  3058.7337,
         -1769.3839]], dtype=torch.float64)
	q_value: tensor([[-22.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3200203444007529, distance: 0.9436363299779789 entropy 9.330378710292546
epoch: 18, step: 96
	action: tensor([[-4839.2953, -2827.5130, -3217.7051,  2248.5330,  3010.8691,  1894.5416,
          3230.1171]], dtype=torch.float64)
	q_value: tensor([[-29.5941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18642597479369982, distance: 1.2464561834699923 entropy 9.636851558981451
epoch: 18, step: 97
	action: tensor([[-2120.1634,  1941.4142, -2181.2893,  2699.8588,  -859.2167, -1971.4734,
         -1898.5740]], dtype=torch.float64)
	q_value: tensor([[-19.2730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22562459361159104, distance: 1.2668798445270797 entropy 9.053929944255012
epoch: 18, step: 98
	action: tensor([[-5947.2127, -6118.7493,   822.8367,  1818.5304, -1002.9333,  -348.6530,
         -1362.9196]], dtype=torch.float64)
	q_value: tensor([[-24.4546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13675683645981196, distance: 1.2200861034961197 entropy 9.257660985902643
epoch: 18, step: 99
	action: tensor([[ -155.7009, -6035.4809,  2362.6411,  3026.0761,   602.2544,  -312.9023,
         -5068.8885]], dtype=torch.float64)
	q_value: tensor([[-20.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13919247018118042, distance: 1.2213924922814696 entropy 9.348888048665453
epoch: 18, step: 100
	action: tensor([[ -171.0630, -1352.8183, -6485.6605,  1852.9975,  3329.0305,  5883.1872,
         -2365.0624]], dtype=torch.float64)
	q_value: tensor([[-19.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02249881251108321, distance: 1.1571458427874441 entropy 9.392763775086603
epoch: 18, step: 101
	action: tensor([[-6167.7874, -5632.9723,  1401.8913, -1465.8570, -1840.2345,  5378.7351,
           635.9618]], dtype=torch.float64)
	q_value: tensor([[-24.6302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7805768075732402, distance: 1.526993011375182 entropy 9.536594797575898
epoch: 18, step: 102
	action: tensor([[ -348.7104,    59.4767, -1987.8982,  2721.0953, -2710.6860, -1107.1996,
          6523.8675]], dtype=torch.float64)
	q_value: tensor([[-20.7398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2321212795486277, distance: 1.0027738719098631 entropy 9.261677379164677
epoch: 18, step: 103
	action: tensor([[ 3321.5066,   690.1341,  -304.0844,  1448.1401, -6558.2939,  5220.5188,
           397.8506]], dtype=torch.float64)
	q_value: tensor([[-25.4388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3084442850466167, distance: 0.9516347241251393 entropy 9.495702624711585
epoch: 18, step: 104
	action: tensor([[ 2889.2531, -4126.7378,   258.0590, -2507.7924,   981.9879, -3354.7293,
         -2906.9866]], dtype=torch.float64)
	q_value: tensor([[-26.3226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.347337772856875
epoch: 18, step: 105
	action: tensor([[-3180.1667, -1174.2474, -1958.7726, -1353.7618,   138.5265,  -178.4105,
           179.5047]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.872916168659058
epoch: 18, step: 106
	action: tensor([[-1192.3002,   660.6503, -1368.3320, -2982.4767, -2310.2617, -1414.9117,
          -891.6310]], dtype=torch.float64)
	q_value: tensor([[-27.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01643845969477331, distance: 1.134899651036597 entropy 8.872916168659058
epoch: 18, step: 107
	action: tensor([[-4384.1544, -2915.9254, -1341.8110, -2405.9150, -2043.4529, -1261.1666,
         -1895.3209]], dtype=torch.float64)
	q_value: tensor([[-23.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2901325090369431, distance: 1.2997919791946981 entropy 9.054438315710618
epoch: 18, step: 108
	action: tensor([[-7344.5306, -3662.2746,  4958.4334,  -266.5411,   770.2482, -4892.1474,
         -3883.7191]], dtype=torch.float64)
	q_value: tensor([[-24.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3847255904846112, distance: 1.3465998210193808 entropy 9.496151907446686
epoch: 18, step: 109
	action: tensor([[-3345.3873,  2312.9363,  2786.7410,   304.3701, -1682.8528,  2332.9891,
         -1970.3779]], dtype=torch.float64)
	q_value: tensor([[-26.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24607751781885923, distance: 1.277406799308033 entropy 9.427316400083951
epoch: 18, step: 110
	action: tensor([[  223.3138, -1165.2870,  1950.1789,  2570.3897,   713.9430, -2353.6973,
          2108.2542]], dtype=torch.float64)
	q_value: tensor([[-24.0255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15548628232203598, distance: 1.0516229308999658 entropy 9.351896785786538
epoch: 18, step: 111
	action: tensor([[-6168.6524, -3679.5537,  -927.5531,  4345.6695, -5123.5322, -1258.6032,
         -3289.3366]], dtype=torch.float64)
	q_value: tensor([[-26.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10751842937084055, distance: 1.0810762380654988 entropy 9.578636162164088
epoch: 18, step: 112
	action: tensor([[-7133.5947, -4242.7791,  -415.9259,  8808.1709, -2058.9325,  3208.1024,
         -1974.3592]], dtype=torch.float64)
	q_value: tensor([[-25.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5946557845359446, distance: 1.4450742740321127 entropy 9.682840890331013
epoch: 18, step: 113
	action: tensor([[ -502.9356, -2756.4090,  -841.0880, -3526.2037,  -969.0207, -1158.4381,
         -6924.5762]], dtype=torch.float64)
	q_value: tensor([[-27.6231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8536340545705106, distance: 1.5580044474980852 entropy 9.561750265198867
epoch: 18, step: 114
	action: tensor([[ 1595.8885, -4293.0236,   802.4648,  2243.8853,  3157.2593,  1411.8967,
          4130.5270]], dtype=torch.float64)
	q_value: tensor([[-20.5201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6122214252702627, distance: 1.4530114526649527 entropy 9.25636221079509
epoch: 18, step: 115
	action: tensor([[-1405.2053,  -168.4835,  1302.6579,  1655.4188, -8264.6340,  1740.0900,
          1593.7065]], dtype=torch.float64)
	q_value: tensor([[-26.6540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29281893156240457, distance: 1.3011445435987088 entropy 9.496123030860023
epoch: 18, step: 116
	action: tensor([[-1346.0102, -3479.9135,  4524.0997,   721.4365, -1941.1750,  3199.9200,
         -4690.6684]], dtype=torch.float64)
	q_value: tensor([[-28.8297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20771258183077668, distance: 1.0185868682061223 entropy 9.654907210365165
epoch: 18, step: 117
	action: tensor([[-3754.6076, -3958.7355,    98.5672, -2886.8265,  -255.1215,  3305.5152,
          3049.3053]], dtype=torch.float64)
	q_value: tensor([[-24.4666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.328652249536431, distance: 1.319053336693112 entropy 9.436034573442344
epoch: 18, step: 118
	action: tensor([[ 2752.0578, -6238.0044,   -22.2940,  5739.4731, -3929.6273,  1811.1366,
          5369.6797]], dtype=torch.float64)
	q_value: tensor([[-25.5034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2416195992844028, distance: 1.2751197514843011 entropy 9.392045636126468
epoch: 18, step: 119
	action: tensor([[ 7790.0379,  1150.3529, -1808.6411,  1724.5180,  4287.3136,  2745.3841,
         10481.0611]], dtype=torch.float64)
	q_value: tensor([[-29.2198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40671675657201545, distance: 0.8814298103639128 entropy 9.617812537860512
epoch: 18, step: 120
	action: tensor([[-3157.7803, -1372.1370,  3859.6866,  1316.0788,  1511.8388,  1869.3259,
          4372.0640]], dtype=torch.float64)
	q_value: tensor([[-25.3019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10537461266826809, distance: 1.0823738778462633 entropy 9.275338266061018
epoch: 18, step: 121
	action: tensor([[-3389.8863, -3282.8739, -2909.3313,  1450.6106, -1449.1868,  6452.1913,
         -4654.2083]], dtype=torch.float64)
	q_value: tensor([[-24.9595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46707700295917687, distance: 1.38606356644812 entropy 9.470351584191178
epoch: 18, step: 122
	action: tensor([[ -805.0137, -3686.8761,  2392.8573, -1403.4065,  3521.0560,  3864.6235,
          1095.5868]], dtype=torch.float64)
	q_value: tensor([[-24.7473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2264581186452299, distance: 1.2673105622990695 entropy 9.386695284045588
epoch: 18, step: 123
	action: tensor([[-3995.4401, -2535.3946,  6173.3150, -4583.8664,  4189.3113, -5624.6046,
         -2058.7765]], dtype=torch.float64)
	q_value: tensor([[-25.1266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.545740454181205, distance: 1.4227381591744723 entropy 9.407864510279085
epoch: 18, step: 124
	action: tensor([[-6701.6075, -2151.1573,  2412.3647,  5599.9033, -2968.0761,  1208.2622,
           321.7402]], dtype=torch.float64)
	q_value: tensor([[-30.2008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1097412441728651, distance: 1.6621541177946928 entropy 9.562902241347867
epoch: 18, step: 125
	action: tensor([[ 4971.5330, -1635.2886, -4827.5474,  6145.8604,  1757.8618,   555.2648,
           186.7498]], dtype=torch.float64)
	q_value: tensor([[-22.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10302891681214277, distance: 1.2018496525759734 entropy 9.328287189392807
epoch: 18, step: 126
	action: tensor([[-1232.0505, -2984.7728, -3015.7089,  3269.8927,  2463.7327,  2077.9659,
         -1424.3542]], dtype=torch.float64)
	q_value: tensor([[-26.1426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23986865578410577, distance: 0.9977023929192134 entropy 9.506011454303755
epoch: 18, step: 127
	action: tensor([[ -976.4503, -2094.3388,    83.8094,  -491.9649,  1159.7151, -1222.7020,
         -3492.6129]], dtype=torch.float64)
	q_value: tensor([[-23.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0097379738553265, distance: 1.6222822354870972 entropy 9.298214939578854
LOSS epoch 18 actor 264.55651584109756 critic 259.38197796088446
epoch: 19, step: 0
	action: tensor([[  785.4493, -1048.8487,  -873.2913, -4491.6522, -2855.4196,  -181.9633,
          2172.6507]], dtype=torch.float64)
	q_value: tensor([[-22.9897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38784398353418803, distance: 1.3481152353553598 entropy 9.471527371191328
epoch: 19, step: 1
	action: tensor([[ 1225.3280,  -170.1133, -7402.2443, -2652.6560, -3538.8577,  1284.3471,
          3361.4284]], dtype=torch.float64)
	q_value: tensor([[-24.2994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17683135385865212, distance: 1.2414059098925059 entropy 9.423437102102795
epoch: 19, step: 2
	action: tensor([[-1538.3229, -6022.1034, -2477.1637, -1882.0641, -3219.1536,   164.7382,
          -437.1942]], dtype=torch.float64)
	q_value: tensor([[-20.2346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2781753184725875, distance: 0.9722379232642021 entropy 9.18606789043621
epoch: 19, step: 3
	action: tensor([[-5052.4697, -2656.8684,  2105.0859, -2494.2461, -3229.9518,  5683.6487,
          3319.8492]], dtype=torch.float64)
	q_value: tensor([[-27.7051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4192038618262697, distance: 1.363261234649167 entropy 9.522823404868051
epoch: 19, step: 4
	action: tensor([[ 1245.9298, -1024.5960, -1857.5410,  3387.1767,   287.8028,  4117.0174,
          5207.8803]], dtype=torch.float64)
	q_value: tensor([[-24.4431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3517262419293671, distance: 0.9213739179826416 entropy 9.392626274762893
epoch: 19, step: 5
	action: tensor([[-3952.2528, -2857.2033,  3675.9273, -2725.3725,  1089.2637,  1325.6979,
         -1699.2335]], dtype=torch.float64)
	q_value: tensor([[-21.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35096675343028794, distance: 0.9219134802043694 entropy 9.2307046884966
epoch: 19, step: 6
	action: tensor([[-2399.4546, -3748.9534, -4565.0237,  6489.1091,   -45.7694, -3524.0656,
             8.8899]], dtype=torch.float64)
	q_value: tensor([[-28.8440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21727125698985406, distance: 1.2625552057593723 entropy 9.52876597941842
epoch: 19, step: 7
	action: tensor([[-164.1287,  325.3880, -728.8557, 3970.0193, 3852.7375, 6371.6549,
         4964.5734]], dtype=torch.float64)
	q_value: tensor([[-22.0211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.536867498533008
epoch: 19, step: 8
	action: tensor([[-2631.2996, -2586.2103,   805.5440, -1758.1508,  1949.1232,  1946.5466,
           312.2983]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20984603412409064, distance: 1.0172145307594427 entropy 8.950801803713295
epoch: 19, step: 9
	action: tensor([[-8921.9399, -1461.4316,  -180.0499, -1393.4224,  2735.7597,  4429.1377,
          2525.6169]], dtype=torch.float64)
	q_value: tensor([[-26.3392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17265243227796012, distance: 1.0408800638687548 entropy 9.390064753513034
epoch: 19, step: 10
	action: tensor([[  -22.0496, -8660.4303,  2106.7097,   840.4308, -1438.5549,  4194.6548,
         -2033.7202]], dtype=torch.float64)
	q_value: tensor([[-27.1468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16505838677844364, distance: 1.2351808178811325 entropy 9.516432017494177
epoch: 19, step: 11
	action: tensor([[-1654.5751, -4294.7178,  2144.7698,   -35.9114,   991.2316,  -335.4987,
          2590.5907]], dtype=torch.float64)
	q_value: tensor([[-21.3436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3232731667703068, distance: 1.7442423684660533 entropy 9.300276509437893
epoch: 19, step: 12
	action: tensor([[ 1107.5019, -1843.9205, -1658.7100,  5741.1733, -3195.2419,  2554.2769,
          1729.1956]], dtype=torch.float64)
	q_value: tensor([[-20.2076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13241932562668657, distance: 1.065888135278328 entropy 9.40261232996086
epoch: 19, step: 13
	action: tensor([[ 1019.9288,   -75.0518,   522.0425,  1586.1719, -1909.8737, -4063.6074,
          4820.4117]], dtype=torch.float64)
	q_value: tensor([[-26.2340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4868957334837982, distance: 0.8197086277114445 entropy 9.414182245387044
epoch: 19, step: 14
	action: tensor([[  911.1808, -3941.2204,  -796.7696,  1198.1527, -7046.2955,  2032.9966,
          1278.4953]], dtype=torch.float64)
	q_value: tensor([[-24.3467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2321771327225215, distance: 1.2702618809519046 entropy 9.223228007957212
epoch: 19, step: 15
	action: tensor([[ 2599.0548,  2304.1393,  -121.6401,  4100.9093, -5009.2961,  9171.0624,
           616.1065]], dtype=torch.float64)
	q_value: tensor([[-31.8907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23037598136762294, distance: 1.003912818745614 entropy 9.65356918105642
epoch: 19, step: 16
	action: tensor([[-4782.7412,  -946.1605, -4566.4209, -1238.4026, -4492.0224, -2773.6722,
         -4324.5838]], dtype=torch.float64)
	q_value: tensor([[-28.2722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6954442024633858, distance: 1.490041773459291 entropy 9.474020562706325
epoch: 19, step: 17
	action: tensor([[-1876.3654,  -410.7333, -1396.6513, -2904.8539,  -555.5611,   162.3961,
          2563.5122]], dtype=torch.float64)
	q_value: tensor([[-18.2911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4341083128425951, distance: 1.3704010094883299 entropy 9.025276918684188
epoch: 19, step: 18
	action: tensor([[ 2077.8047, -2599.4491,  1215.4123,  1664.2782,  -709.2903, -3018.7697,
          5544.5513]], dtype=torch.float64)
	q_value: tensor([[-24.3941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15241030009045575, distance: 1.0535363594333615 entropy 9.455002717009062
epoch: 19, step: 19
	action: tensor([[ -167.4759, -2595.0601, -3965.0494,  2683.6177, -2554.6252, -3839.7009,
           996.3654]], dtype=torch.float64)
	q_value: tensor([[-22.9467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19930367984098796, distance: 1.0239779612526665 entropy 9.292914728369109
epoch: 19, step: 20
	action: tensor([[-9612.8693,  -401.1195,  2148.2393,   940.9503,    93.4792, -3741.1474,
         -4349.0963]], dtype=torch.float64)
	q_value: tensor([[-28.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4616662061516288, distance: 1.3835052017219742 entropy 9.6477075428247
epoch: 19, step: 21
	action: tensor([[-3884.2976, -2686.3285,   603.3090,  2193.8459,  1553.1321,   263.1553,
          3409.4268]], dtype=torch.float64)
	q_value: tensor([[-22.9583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12282724262287725, distance: 1.071764236033449 entropy 9.440197198929452
epoch: 19, step: 22
	action: tensor([[-2689.1161,  -671.3125,  3365.7561,  -959.1630,  6760.0795,  2367.0773,
            54.1850]], dtype=torch.float64)
	q_value: tensor([[-25.0729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8211864505156707, distance: 1.5443079192895757 entropy 9.479722726805349
epoch: 19, step: 23
	action: tensor([[-2006.6941,  5812.5460,  -307.9935,   462.0861,  2736.1148,  -522.6411,
          1060.1429]], dtype=torch.float64)
	q_value: tensor([[-25.0861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21946433333316784, distance: 1.0110044560390923 entropy 9.37990119777667
epoch: 19, step: 24
	action: tensor([[ -605.4505,  -647.2234, -2408.6204,  5535.8216,   364.8795,   510.5022,
          5218.1213]], dtype=torch.float64)
	q_value: tensor([[-29.8922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04500150861333685, distance: 1.1698095216541324 entropy 9.477966364786846
epoch: 19, step: 25
	action: tensor([[-3721.7774,  -334.0443, -1210.1269,  3876.4751,    43.7460,  3155.9521,
         -1832.4971]], dtype=torch.float64)
	q_value: tensor([[-27.7578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18838892125734397, distance: 1.0309335579747174 entropy 9.534893370995922
epoch: 19, step: 26
	action: tensor([[ 6501.0632, -6826.6060,  3128.5730, -1831.0842, -1945.6922,   -20.9145,
          3953.7773]], dtype=torch.float64)
	q_value: tensor([[-27.7681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12408621050807156, distance: 1.2132673300055072 entropy 9.572935695429008
epoch: 19, step: 27
	action: tensor([[-2537.7974,  -145.0686,  3520.9059,   437.7396,  -416.3266, -3000.2875,
           -73.7920]], dtype=torch.float64)
	q_value: tensor([[-19.5868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7288310251435632, distance: 1.504641261103022 entropy 9.203623305758981
epoch: 19, step: 28
	action: tensor([[-3754.2037, -5806.0299,  1033.4966,   847.6969,  1443.0706,  2909.1130,
         -2633.1139]], dtype=torch.float64)
	q_value: tensor([[-27.0517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05548792916959, distance: 1.112142531052066 entropy 9.56126949232202
epoch: 19, step: 29
	action: tensor([[-2650.1934, -6620.1141,   -15.7515,   488.1173, -6876.6645,  2160.5690,
         -7676.1892]], dtype=torch.float64)
	q_value: tensor([[-27.4771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20904884569644278, distance: 1.2582838330603234 entropy 9.60783892020008
epoch: 19, step: 30
	action: tensor([[-7619.2305, -2736.4325, -7392.3763,  3419.2731, -3176.2547,  5344.1110,
         -5350.6190]], dtype=torch.float64)
	q_value: tensor([[-30.1214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3217716870431544, distance: 1.31563347665445 entropy 9.74139121993217
epoch: 19, step: 31
	action: tensor([[ -563.4818, -2052.5621, -4161.4781,  7565.2445, -1495.2586,  2301.7824,
          2947.5516]], dtype=torch.float64)
	q_value: tensor([[-31.5596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013458849008160856, distance: 1.1520192942734178 entropy 9.745912139005219
epoch: 19, step: 32
	action: tensor([[  -381.0163, -11230.4876,   2026.2999,  -2152.6257,   -340.6033,
            988.8622,   -322.0651]], dtype=torch.float64)
	q_value: tensor([[-26.5911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39701644353228316, distance: 1.352562842240378 entropy 9.470750504453614
epoch: 19, step: 33
	action: tensor([[ 2222.2126, -2323.2354,  5180.3268,  5866.6499,  5737.6181,  4152.1643,
          3126.8595]], dtype=torch.float64)
	q_value: tensor([[-28.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15602331628304844, distance: 1.051288509428548 entropy 9.520623645467087
epoch: 19, step: 34
	action: tensor([[-10150.9986,  -1867.2705,   1642.5502,   3498.7334,  -3549.2751,
           3458.8317,  -6874.5124]], dtype=torch.float64)
	q_value: tensor([[-29.6451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06071788901024022, distance: 1.1785734133633383 entropy 9.577387882191493
epoch: 19, step: 35
	action: tensor([[-1536.3916, -1511.7859,  1106.4800,   633.7669,   631.4405,  7089.2365,
          4093.5853]], dtype=torch.float64)
	q_value: tensor([[-26.1036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3293915773744671, distance: 1.3194202789180531 entropy 9.497650258716135
epoch: 19, step: 36
	action: tensor([[-5490.5696, -3932.9694, -5530.9328,   500.5069, -1894.6702, -5752.0789,
         -3682.7661]], dtype=torch.float64)
	q_value: tensor([[-27.7643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13989125328629937, distance: 1.061288297323919 entropy 9.622280240413476
epoch: 19, step: 37
	action: tensor([[-6487.1813, -4423.5053,  -565.9105,    89.0443,  1484.2936,  3818.6938,
         -2693.0464]], dtype=torch.float64)
	q_value: tensor([[-24.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28747923544035947, distance: 1.2984547217101636 entropy 9.508300560023075
epoch: 19, step: 38
	action: tensor([[  583.8659,   933.3082,   583.2189,  5257.9218,  -237.4913,   146.2325,
         -2310.8854]], dtype=torch.float64)
	q_value: tensor([[-24.6453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1226245270921037, distance: 1.0718880717905184 entropy 9.427874684895837
epoch: 19, step: 39
	action: tensor([[ -716.6642, -5497.4491, -3186.2233,   943.3827, -1151.6866, -1640.6288,
           245.4674]], dtype=torch.float64)
	q_value: tensor([[-27.3112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06637582320819535, distance: 1.105713820709498 entropy 9.389486912385243
epoch: 19, step: 40
	action: tensor([[-2043.9880, -5508.7192, -1114.0592,  4483.8081,  2151.6306, -1079.0673,
          4578.0480]], dtype=torch.float64)
	q_value: tensor([[-23.2286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22097125841496323, distance: 1.0100280470928393 entropy 9.549047057654532
epoch: 19, step: 41
	action: tensor([[-5455.1984,  2771.6045,  -393.9504,  5500.9910, -3046.4922,  -457.8927,
         -3146.5621]], dtype=torch.float64)
	q_value: tensor([[-30.0855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2571520430931712, distance: 0.9862946102968665 entropy 9.682021004941225
epoch: 19, step: 42
	action: tensor([[  496.4075, -3281.8790,  -689.7190,  -908.6938,  2730.8107,  -583.0417,
          3033.6019]], dtype=torch.float64)
	q_value: tensor([[-26.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.490694133354301
epoch: 19, step: 43
	action: tensor([[-4102.4387, -1055.0115, -3140.9896,  1935.5850,  -554.0586,  -937.0486,
         -2062.7461]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1790615335440109, distance: 1.2425816296843832 entropy 8.950801803713295
epoch: 19, step: 44
	action: tensor([[ -1514.4744,  -4742.2476,  -4633.8153,   6834.1437,    477.7377,
         -10175.9707,  -1239.6771]], dtype=torch.float64)
	q_value: tensor([[-22.7798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2235428702767246, distance: 1.2658034895969927 entropy 9.409769359743342
epoch: 19, step: 45
	action: tensor([[ -182.4497,   933.9681, -3903.5205,  3165.3228,    60.3070,  1251.5429,
          1282.1334]], dtype=torch.float64)
	q_value: tensor([[-25.0842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3014966895212783, distance: 1.305504061145952 entropy 9.506360367818363
epoch: 19, step: 46
	action: tensor([[ -586.7637, -2111.9078,   644.6430,  6527.9418, -2997.1622,  5140.7554,
           452.3661]], dtype=torch.float64)
	q_value: tensor([[-29.8419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12496479392835791, distance: 1.0704575673013323 entropy 9.534198497771671
epoch: 19, step: 47
	action: tensor([[-1105.6178, -1798.7845,  3365.8401, -7411.3937,    24.5497,  2093.1235,
          3810.5497]], dtype=torch.float64)
	q_value: tensor([[-27.2670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6897971995528325, distance: 1.487558268473437 entropy 9.529530097107832
epoch: 19, step: 48
	action: tensor([[  677.9643,  -842.6814,  4265.1064,  -210.9923,  2180.9443, -1780.0407,
           924.5210]], dtype=torch.float64)
	q_value: tensor([[-27.4697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17376410330329173, distance: 1.040180536121246 entropy 9.494701868480275
epoch: 19, step: 49
	action: tensor([[-6986.7682,  4084.7007,  1249.8228,  -429.4582, -2509.0852, -1590.5031,
           806.6956]], dtype=torch.float64)
	q_value: tensor([[-24.3961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43252518502179715, distance: 0.8620451032480348 entropy 9.4359173483048
epoch: 19, step: 50
	action: tensor([[-3877.0539, -2454.8228, -6071.8438,  2349.5556,  -488.2285,  3989.7402,
          3380.1109]], dtype=torch.float64)
	q_value: tensor([[-34.5975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9576015210282562, distance: 1.6011014106638557 entropy 9.52207842687571
epoch: 19, step: 51
	action: tensor([[ -447.8291,  5088.4061, -4320.4087,  -725.3470,  -523.2798,  1574.9613,
          -329.0127]], dtype=torch.float64)
	q_value: tensor([[-25.8937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5813668322912731, distance: 0.740412104655971 entropy 9.42664195307877
epoch: 19, step: 52
	action: tensor([[-1159.8350, -6149.0760,   304.3082, -5558.0551,  3997.1589, -1494.2810,
          1417.9485]], dtype=torch.float64)
	q_value: tensor([[-31.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07416123347146619, distance: 1.101093943992108 entropy 9.62293170770776
epoch: 19, step: 53
	action: tensor([[-5137.7568,  1635.5161,  2737.9540, -1630.5122,  -781.5749,  4570.4868,
          -851.0712]], dtype=torch.float64)
	q_value: tensor([[-28.7979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10664370527381517, distance: 1.0816058914912363 entropy 9.545257427324971
epoch: 19, step: 54
	action: tensor([[  822.1619, -4800.5262, -2045.9811,  -715.2149, 10501.7276, -2470.7748,
           749.6236]], dtype=torch.float64)
	q_value: tensor([[-45.0703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.694772254042194
epoch: 19, step: 55
	action: tensor([[-1007.1133, -2053.5258,  1895.2253,  -833.7213, -2283.5954,   162.4946,
          -585.0542]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27291788355348956, distance: 0.9757721636927431 entropy 8.950801803713295
epoch: 19, step: 56
	action: tensor([[ 1705.5675,   773.7227, -2356.2337,  2239.7229,   216.6818,  3929.1952,
          3686.1724]], dtype=torch.float64)
	q_value: tensor([[-31.7540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.528198042394175
epoch: 19, step: 57
	action: tensor([[-1209.6224, -1219.7585,  1317.4902,  3852.3125,  2981.0068, -1179.6205,
         -1719.3188]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.354534354351258, distance: 1.3318389304104747 entropy 8.950801803713295
epoch: 19, step: 58
	action: tensor([[-4468.8152,  5062.5369,   256.8296,  -290.2084, -2017.2049,  3035.7184,
          4248.9825]], dtype=torch.float64)
	q_value: tensor([[-27.1014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.193302816922009, distance: 1.0278079284448336 entropy 9.540350747823194
epoch: 19, step: 59
	action: tensor([[-2355.3951, -3310.4283,  2333.1942,   274.6712, -4123.1489,   -57.9391,
         -1344.2593]], dtype=torch.float64)
	q_value: tensor([[-29.4166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9124603448959978, distance: 1.5825335017002886 entropy 9.097378645721044
epoch: 19, step: 60
	action: tensor([[-2788.6910,   192.8228,   418.7687, -1590.4119,   140.3338, -1086.9993,
          -899.8091]], dtype=torch.float64)
	q_value: tensor([[-16.3908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05233389409131184, distance: 1.1739064059962674 entropy 9.092994819428379
epoch: 19, step: 61
	action: tensor([[  329.7693,  -795.4215, -1483.4792, -3723.9882,  2967.8703,  -735.8656,
          2440.0849]], dtype=torch.float64)
	q_value: tensor([[-32.8610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08306079748170969, distance: 1.1909214333692992 entropy 9.334550669660716
epoch: 19, step: 62
	action: tensor([[-2284.9510,   185.9387, -6435.5359,   930.7366,  2925.1124,  2143.3490,
         -1324.6134]], dtype=torch.float64)
	q_value: tensor([[-30.5162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.50286280276758
epoch: 19, step: 63
	action: tensor([[-2651.5042,  -528.3220,  -412.1248,   860.3560, -1549.7530,  1932.5986,
          1522.0853]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9192753852250011, distance: 1.5853506682495984 entropy 8.950801803713295
epoch: 19, step: 64
	action: tensor([[ -633.2641,  2568.0257, -4333.5041,  6411.5683, -2937.3953,  8726.9194,
          -294.6445]], dtype=torch.float64)
	q_value: tensor([[-24.2362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36016808314112847, distance: 0.9153551796954442 entropy 9.419353271537117
epoch: 19, step: 65
	action: tensor([[-6368.5610, -7950.0267, -1757.7023,   859.9322,    21.3543, -2550.2815,
           307.1490]], dtype=torch.float64)
	q_value: tensor([[-31.2100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01418323362125573, distance: 1.1362000220369364 entropy 9.54630741496001
epoch: 19, step: 66
	action: tensor([[ 2498.3334,  2706.9691, -5760.0253, -4755.7510,  -206.5654, -3970.2954,
          5797.8473]], dtype=torch.float64)
	q_value: tensor([[-24.6294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.486141656540283
epoch: 19, step: 67
	action: tensor([[-4918.0687, -1454.6094,  2394.5852,  1197.2008,  1402.9641, -1645.4405,
           167.3556]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4431384667257856, distance: 1.3747087429628302 entropy 8.950801803713295
epoch: 19, step: 68
	action: tensor([[-5407.9309,  2403.2294,  6691.7408,  1478.5427,  1273.2525,  6886.9539,
          -507.2986]], dtype=torch.float64)
	q_value: tensor([[-24.7000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45521473792181744, distance: 0.8446355729798134 entropy 9.494942309543102
epoch: 19, step: 69
	action: tensor([[-2238.8394,  -859.0584,  1840.4861,  -667.0266, -2039.0105,   519.5494,
          2975.7198]], dtype=torch.float64)
	q_value: tensor([[-28.5052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2361879798460449, distance: 1.2723276094828055 entropy 9.487658720397153
epoch: 19, step: 70
	action: tensor([[-158.2112, -256.2068, 7147.9820,  451.3108, 3396.5563, 5669.3598,
         2732.9018]], dtype=torch.float64)
	q_value: tensor([[-28.4451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6277948425404511, distance: 1.4600123429946925 entropy 9.551272355766452
epoch: 19, step: 71
	action: tensor([[ 3487.0418, -5835.3468, -1897.7165,  5086.3987,  1763.6440, -4664.8419,
         -1568.0554]], dtype=torch.float64)
	q_value: tensor([[-26.8117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7310283656724429, distance: 0.5934852559709757 entropy 9.602033293386379
epoch: 19, step: 72
	action: tensor([[ 1179.1456,   972.3608,  -314.5071, -3822.8686,  3954.2898,  2096.7197,
          3390.6020]], dtype=torch.float64)
	q_value: tensor([[-24.1328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31284959900578546, distance: 0.9485988534726154 entropy 9.516156313048523
epoch: 19, step: 73
	action: tensor([[-2954.7638,  -687.7454,    31.6870,  1262.2237,  4333.9528,  1043.7834,
         -1112.3546]], dtype=torch.float64)
	q_value: tensor([[-26.2586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.292209916582426
epoch: 19, step: 74
	action: tensor([[ -507.9815, -3493.6245, -2693.0803,  -201.5542,  3384.6659,  2649.9128,
           760.7088]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37601741145794754, distance: 0.9039469192378035 entropy 8.950801803713295
epoch: 19, step: 75
	action: tensor([[ 1976.3416,  1314.4938,   884.5665,  1256.2667,  -960.8081, -1830.0532,
         -1836.9279]], dtype=torch.float64)
	q_value: tensor([[-28.4259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7950898859630255, distance: 0.5180102946714057 entropy 9.477937405454455
epoch: 19, step: 76
	action: tensor([[-1050.5153,   671.5419,  -492.0590,  5380.8224, -1625.9178,   897.8373,
          1358.9077]], dtype=torch.float64)
	q_value: tensor([[-22.8406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.134795840478315
epoch: 19, step: 77
	action: tensor([[   36.5720, -2673.3473,  -367.7265,  2349.9372,   746.3138,  3129.4538,
         -1862.8044]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004310827926393479, distance: 1.1418750544685947 entropy 8.950801803713295
epoch: 19, step: 78
	action: tensor([[-5252.3276, -2311.5375,  1326.9739,    11.8605,  1569.6390,  3886.8652,
         -4611.4084]], dtype=torch.float64)
	q_value: tensor([[-29.1004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.525766453230757
epoch: 19, step: 79
	action: tensor([[ -277.7880,   834.2223,  -612.7986, -1563.0832,  1076.7707, -2532.6383,
           567.0375]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.950801803713295
epoch: 19, step: 80
	action: tensor([[-2000.5071, -3388.7912,  1124.4150,  1604.9070,   222.6930, -3666.3596,
         -2306.3054]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03290945255223343, distance: 1.1253568595042451 entropy 8.950801803713295
epoch: 19, step: 81
	action: tensor([[-2444.6127,  4570.2029, -1592.2054,  3777.2321, -1045.1859, -1897.4868,
          2537.3052]], dtype=torch.float64)
	q_value: tensor([[-24.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3725718809123859, distance: 0.9064392075340294 entropy 9.391927127681964
epoch: 19, step: 82
	action: tensor([[-2067.7432, -1774.9917, -5372.0154,  1035.7068, -1081.6491,   718.4851,
          3392.8240]], dtype=torch.float64)
	q_value: tensor([[-31.3121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.250371876287407, distance: 1.2796060704048717 entropy 9.565332988734756
epoch: 19, step: 83
	action: tensor([[-4036.9256, -6088.5280,  -849.7448,  -808.3982,   826.2488,  -540.7071,
          2017.9915]], dtype=torch.float64)
	q_value: tensor([[-23.7915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3299224297237786, distance: 1.746736616623604 entropy 9.360589833225902
epoch: 19, step: 84
	action: tensor([[-1297.1226,    52.0009, -1089.6592,    45.3050,  -121.7351, -2030.4734,
           520.1095]], dtype=torch.float64)
	q_value: tensor([[-24.7058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5109846099445987, distance: 1.406652147952407 entropy 9.527202815689398
epoch: 19, step: 85
	action: tensor([[-5756.9187,  1055.2628,  2119.2953,  3179.6853,  2844.2966,  4474.0602,
         -7084.5670]], dtype=torch.float64)
	q_value: tensor([[-30.5610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5797685079376527, distance: 0.7418241897221861 entropy 9.547079162710668
epoch: 19, step: 86
	action: tensor([[-2779.4727, -5586.6659, -1753.6211,  2672.4159,  5548.4852,  7875.9395,
          2535.1359]], dtype=torch.float64)
	q_value: tensor([[-30.3591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12698706426207662, distance: 1.069219898953416 entropy 9.602621004889965
epoch: 19, step: 87
	action: tensor([[ 1432.3001, -9838.7612, -1653.1017,  3298.2439, -4486.1097,   187.5646,
          -104.4273]], dtype=torch.float64)
	q_value: tensor([[-26.4535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3967822056849061, distance: 0.8887789607915045 entropy 9.520329804873645
epoch: 19, step: 88
	action: tensor([[-3762.7367, -5296.8668, -3288.4758,  3227.3829, -2496.1814,  -346.7277,
         -2868.1175]], dtype=torch.float64)
	q_value: tensor([[-24.6905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4014999643222079, distance: 1.3547315303278802 entropy 9.357755219813797
epoch: 19, step: 89
	action: tensor([[-12322.5841,  -3269.0394,  -2005.0385,   -933.4974,  -1294.7739,
           2637.6261,  -1799.6444]], dtype=torch.float64)
	q_value: tensor([[-27.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9043430616325918, distance: 1.57917146270094 entropy 9.689325297946855
epoch: 19, step: 90
	action: tensor([[-4219.1090, -4316.8300,  -507.7677,  -137.1884,  2361.6574,  -480.6480,
         -1286.9742]], dtype=torch.float64)
	q_value: tensor([[-24.2634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4137206817460687, distance: 0.8762115635437556 entropy 9.357399011366658
epoch: 19, step: 91
	action: tensor([[ -717.4620,  -759.3725,  -273.2443,  3985.0183,  -921.3121,  1188.3902,
         -6821.4109]], dtype=torch.float64)
	q_value: tensor([[-22.7627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6850663288670067, distance: 1.4854744745299187 entropy 9.445061217154322
epoch: 19, step: 92
	action: tensor([[-3168.7919, -2352.5333, -2573.5397,  3148.6011,    46.3664, -1440.3710,
          9460.0010]], dtype=torch.float64)
	q_value: tensor([[-23.2375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2840949039989823, distance: 0.9682431210537379 entropy 9.441043163355376
epoch: 19, step: 93
	action: tensor([[-3507.3380,  3457.5948,  1137.9775, -1232.6409,   390.8439,  3234.2863,
           204.3415]], dtype=torch.float64)
	q_value: tensor([[-27.5710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14193253691788343, distance: 1.2228605001099655 entropy 9.621403746203612
epoch: 19, step: 94
	action: tensor([[-2160.1697,  2987.1449,  4253.6367,  3272.2591, -1917.8325,  2243.5619,
         -3813.4707]], dtype=torch.float64)
	q_value: tensor([[-29.2336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2652967186945927, distance: 0.980872780735659 entropy 9.405282556407395
epoch: 19, step: 95
	action: tensor([[-3919.2988,   791.7419,  3530.7697,  -581.1316,  -367.3836, -1137.0283,
          1339.6633]], dtype=torch.float64)
	q_value: tensor([[-27.7014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43633092381341176, distance: 0.8591496113119975 entropy 9.41316292713783
epoch: 19, step: 96
	action: tensor([[ -444.8955, -1689.6188,    91.1833, -1933.8192, -1849.4901,  -328.4314,
         -2061.0687]], dtype=torch.float64)
	q_value: tensor([[-28.6647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26406779566736827, distance: 1.2865950518402574 entropy 9.390699469104815
epoch: 19, step: 97
	action: tensor([[-1.6389e+03,  2.0970e+00,  2.0198e+03,  3.0122e+03,  1.3177e+03,
         -5.3315e+01, -3.9469e+03]], dtype=torch.float64)
	q_value: tensor([[-29.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.496327812254965
epoch: 19, step: 98
	action: tensor([[  169.4786, -2858.5149,  1250.6256,  -645.7185,  -346.0155,   537.5498,
           -36.8440]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14048350402394005, distance: 1.2220843914021056 entropy 8.950801803713295
epoch: 19, step: 99
	action: tensor([[-1005.8797,  -199.1979,  -576.2882,  3707.5029,  5689.9356,  5482.8068,
         -1224.2843]], dtype=torch.float64)
	q_value: tensor([[-23.0393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38516651628919907, distance: 1.3468141968334295 entropy 9.322408951069804
epoch: 19, step: 100
	action: tensor([[ 4471.7027, -4001.4350, -2147.2044,  1769.0060,  -170.5956,   131.2352,
          5755.7523]], dtype=torch.float64)
	q_value: tensor([[-26.0043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5841045200960968, distance: 0.7379871390893451 entropy 9.528967923844943
epoch: 19, step: 101
	action: tensor([[ 1527.4213, -2261.9812, -4905.8179,  1897.5603,  1629.4668,   906.0228,
           216.3363]], dtype=torch.float64)
	q_value: tensor([[-26.0654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19659075108823032, distance: 1.251784339853276 entropy 9.339574318698014
epoch: 19, step: 102
	action: tensor([[-2613.3906, -1440.0967, -3465.1946,   -68.9286, -1277.8262,   111.9016,
          -214.8414]], dtype=torch.float64)
	q_value: tensor([[-30.1123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37859476537657843, distance: 1.3436155018272722 entropy 9.4445666770887
epoch: 19, step: 103
	action: tensor([[ -161.3034, -2518.9179,  1737.8321,  4296.7351, -1654.7732, -4441.5537,
          2755.5468]], dtype=torch.float64)
	q_value: tensor([[-29.5294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1856832757507123, distance: 1.2460659835091383 entropy 9.638626304326788
epoch: 19, step: 104
	action: tensor([[ 1793.1298, -6743.5295,  3092.2260,  3306.7182, -6930.5204,  6478.2766,
          3334.6474]], dtype=torch.float64)
	q_value: tensor([[-29.5338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46902986081578546, distance: 0.8338573135164332 entropy 9.760834690528725
epoch: 19, step: 105
	action: tensor([[ 1405.5799,    54.3365,  3314.3249, -3776.9936, -5059.6893, -1122.6191,
           583.1310]], dtype=torch.float64)
	q_value: tensor([[-26.1459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7372197439006859, distance: 0.5866148583915128 entropy 9.395443610266273
epoch: 19, step: 106
	action: tensor([[ -341.1607,  4305.1691, -3560.8480,   934.4047,  -842.4023, -1084.7946,
         -1072.3250]], dtype=torch.float64)
	q_value: tensor([[-25.7859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.366222176684042
epoch: 19, step: 107
	action: tensor([[ 3031.3844, -5706.3730,  2037.3021,  1152.8445,  1320.2240,  -113.2085,
          3643.4933]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3207233492947408, distance: 0.9431484091244795 entropy 8.950801803713295
epoch: 19, step: 108
	action: tensor([[-3792.5631, -5349.0265,  1801.1282, -1703.4799, -3114.1845,  2853.5418,
          1541.4189]], dtype=torch.float64)
	q_value: tensor([[-22.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16612200475358796, distance: 1.0449799315975927 entropy 9.145945994491772
epoch: 19, step: 109
	action: tensor([[-3358.4295, -7368.5905,  2850.9381,   354.2933, -3023.3618, -4402.8035,
          2531.2268]], dtype=torch.float64)
	q_value: tensor([[-24.5144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8163225398455438, distance: 1.5422443201175107 entropy 9.350071981916146
epoch: 19, step: 110
	action: tensor([[-2380.5302, -5070.7477,  2648.9453,   470.9269, -1614.3575,  1308.3356,
         -4155.5860]], dtype=torch.float64)
	q_value: tensor([[-22.5920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04725377134245057, distance: 1.1169797826232837 entropy 9.420541170014175
epoch: 19, step: 111
	action: tensor([[-9641.2207, -2702.6755,   534.7642, -5697.5197, -2139.9200, -2547.5509,
          6234.7784]], dtype=torch.float64)
	q_value: tensor([[-27.1581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8192502562553416, distance: 1.543486785550832 entropy 9.589906681406948
epoch: 19, step: 112
	action: tensor([[-2636.5984, -6848.0053,  2271.7596, -1914.7356,  2250.3697,  3977.2886,
          -496.3065]], dtype=torch.float64)
	q_value: tensor([[-23.8543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7429992993421535, distance: 1.5107941674762262 entropy 9.509454161088062
epoch: 19, step: 113
	action: tensor([[-6407.2094, -3958.3356, -1963.4262,  2135.8348,  2529.0934,  1981.6248,
          2455.2774]], dtype=torch.float64)
	q_value: tensor([[-30.7335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9777021210796859, distance: 1.6093004507617037 entropy 9.490246605786533
epoch: 19, step: 114
	action: tensor([[ -1039.9115, -11376.0889,    628.7868,  -4151.0027,    364.8255,
           2303.4587,   3957.2190]], dtype=torch.float64)
	q_value: tensor([[-30.3707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5614288469812194, distance: 1.4299399263192734 entropy 9.645143204144285
epoch: 19, step: 115
	action: tensor([[-3913.4766, -1027.1369, -3924.2578, -1748.1464,  -203.0041,  2953.0094,
          1109.8105]], dtype=torch.float64)
	q_value: tensor([[-26.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10563819717227307, distance: 1.2032703362318509 entropy 9.383906729204417
epoch: 19, step: 116
	action: tensor([[-5988.6285, -3424.5983, -2003.7248,  -160.9744, -3956.2980,  -127.6262,
          -539.8179]], dtype=torch.float64)
	q_value: tensor([[-31.3119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5094678427230444, distance: 1.4059459529464198 entropy 9.614737593798925
epoch: 19, step: 117
	action: tensor([[ 1151.1239, -1329.3043, -1796.9905,  1046.1725,  1438.5136, -4094.7435,
           459.2639]], dtype=torch.float64)
	q_value: tensor([[-22.5973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2992934264974425, distance: 1.3043985695958253 entropy 9.302950819948153
epoch: 19, step: 118
	action: tensor([[  231.4558, -5780.4941,  1722.9896, -2351.8412,   524.9593,  1761.1624,
          2576.1246]], dtype=torch.float64)
	q_value: tensor([[-18.1010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4381027359937606, distance: 0.8577982422740605 entropy 8.997332527389093
epoch: 19, step: 119
	action: tensor([[-1958.9188, -1312.2750,  -400.5009,  1939.2648,  -695.0616, -4194.2701,
          2878.3050]], dtype=torch.float64)
	q_value: tensor([[-24.3337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031296355998223224, distance: 1.1262950099848268 entropy 9.453552906616027
epoch: 19, step: 120
	action: tensor([[-2602.5891, -5049.2694, -4588.9489,  -802.4391,  2680.3881,   676.2182,
          2779.4579]], dtype=torch.float64)
	q_value: tensor([[-20.7207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25154561499636685, distance: 0.990009499533985 entropy 9.246685335814218
epoch: 19, step: 121
	action: tensor([[ -729.9436, -4670.5086, -3630.3205,  -685.1700,  2381.5222, -3892.0568,
         -4531.9913]], dtype=torch.float64)
	q_value: tensor([[-27.6234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12203146885477612, distance: 1.0722502796952569 entropy 9.659328468275735
epoch: 19, step: 122
	action: tensor([[ 1291.5737,  -641.4586,  -398.9991,  4976.4849,  -226.5010, -2250.9359,
         -3739.5566]], dtype=torch.float64)
	q_value: tensor([[-28.2095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11546674977298033, distance: 1.0762515124256302 entropy 9.482857029346416
epoch: 19, step: 123
	action: tensor([[-4321.4965,  3408.8765, -2443.1443,  3481.4004,  4563.3037,  2764.7148,
          -114.3069]], dtype=torch.float64)
	q_value: tensor([[-24.3007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47936142555440664, distance: 0.8257049047215025 entropy 9.330391767169052
epoch: 19, step: 124
	action: tensor([[ 6071.9648, -4121.8178,   161.5011,  1533.0068, -2582.2005,  2042.1433,
           256.5353]], dtype=torch.float64)
	q_value: tensor([[-27.7084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4074666587202873, distance: 0.8808725764312226 entropy 9.360970686686489
epoch: 19, step: 125
	action: tensor([[ -241.6254, -3355.0193, -2743.2947,  1179.7133,   119.5109,  -327.7985,
         -1369.4830]], dtype=torch.float64)
	q_value: tensor([[-21.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6891152048920073, distance: 1.4872580522166259 entropy 9.268143001351271
epoch: 19, step: 126
	action: tensor([[  736.3922, -2448.6034,  2048.4446,  1866.0771, -2597.6916, -1353.1492,
          5677.9737]], dtype=torch.float64)
	q_value: tensor([[-22.2303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4499084056556618, distance: 0.8487390761510923 entropy 9.384372688511023
epoch: 19, step: 127
	action: tensor([[-3246.9729, -1796.2508,  1499.3506, -3340.7035,  1224.8821,  4248.5736,
          -197.2934]], dtype=torch.float64)
	q_value: tensor([[-23.2951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6965010087919963, distance: 1.490506088435424 entropy 9.473013539440375
LOSS epoch 19 actor 296.30060319502803 critic 209.74289784191683
epoch: 20, step: 0
	action: tensor([[-4625.3682,  1291.7291,  -195.4802,  1312.3327,  3826.1839, -2231.3083,
         -3648.1462]], dtype=torch.float64)
	q_value: tensor([[-25.3094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22058455628406592, distance: 1.0102786999373676 entropy 9.452763557783925
epoch: 20, step: 1
	action: tensor([[ -148.1103, -6973.4144,   304.1531,   571.1393,   123.9272,  1589.9448,
         -2360.1050]], dtype=torch.float64)
	q_value: tensor([[-28.7868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0022789033461467767, distance: 1.1430395852977775 entropy 9.4785675638794
epoch: 20, step: 2
	action: tensor([[ 3584.7107,  4022.0470, -1176.5941,  2429.4998, -5108.3111,  6546.7531,
          -960.6933]], dtype=torch.float64)
	q_value: tensor([[-31.4745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5115584088592336, distance: 0.7997661374849798 entropy 9.74850413924453
epoch: 20, step: 3
	action: tensor([[-3474.5124, -4002.9781, -2508.8869,  -390.7372, -1793.6285,  3638.8640,
         -5408.4680]], dtype=torch.float64)
	q_value: tensor([[-25.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.351390394907337
epoch: 20, step: 4
	action: tensor([[ -773.5838,   682.3896, -3613.1524, -1470.1567,   465.3508,  -714.0286,
          1668.1915]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.026022937770174
epoch: 20, step: 5
	action: tensor([[  130.6966, -2415.7285,  3724.9438,  1557.8590, -1543.7704, -1575.1671,
          3202.0926]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17519240472380582, distance: 1.2405411683065635 entropy 9.026022937770174
epoch: 20, step: 6
	action: tensor([[  532.3812, -6022.0484, -4160.7721, -1717.2442,  -999.6932, -1657.1792,
         -1769.8121]], dtype=torch.float64)
	q_value: tensor([[-25.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009947251844130922, distance: 1.1386384891220787 entropy 9.408828047846734
epoch: 20, step: 7
	action: tensor([[  593.3000, -3854.3098,   375.1457,  1512.9390,  3191.6163,  3511.1553,
          8485.4395]], dtype=torch.float64)
	q_value: tensor([[-28.0371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946837318899511, distance: 0.9610558878809355 entropy 9.526470587724145
epoch: 20, step: 8
	action: tensor([[-5541.5674, -3832.7863, -2824.0245,  1137.3858,  1820.1161, -7312.2957,
          1661.5610]], dtype=torch.float64)
	q_value: tensor([[-25.5924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2031473494802607, distance: 1.2552091678112902 entropy 9.586617380501716
epoch: 20, step: 9
	action: tensor([[-1249.5278, -7885.2714, -1591.9341,  -194.6765,  1435.2091,  4238.5809,
         -2942.8056]], dtype=torch.float64)
	q_value: tensor([[-30.9978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1785185940718579, distance: 1.689029865321083 entropy 9.813745548746036
epoch: 20, step: 10
	action: tensor([[  346.8907, -1219.8691,  4310.5242,  3251.6288,  -121.7579, -1357.1138,
          3514.3941]], dtype=torch.float64)
	q_value: tensor([[-30.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7161294088536534, distance: 0.6097009817788693 entropy 9.67688648091175
epoch: 20, step: 11
	action: tensor([[-11193.5359,  -3286.5454,  -4947.8818,   6206.9677,   -371.6924,
           8034.2002,  -6721.0264]], dtype=torch.float64)
	q_value: tensor([[-25.3687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.586183765604233
epoch: 20, step: 12
	action: tensor([[  646.5836, -8246.7559,  2155.9810,  -389.9588,  1230.3062, -1760.1737,
          1919.8667]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.026022937770174
epoch: 20, step: 13
	action: tensor([[-2646.6914, -1464.5530, -4087.0222,  -859.0944,  1604.4168,  3444.6589,
          1307.9606]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2088290317725192, distance: 1.2581694452585708 entropy 9.026022937770174
epoch: 20, step: 14
	action: tensor([[-8.6154e+03, -6.5272e+02,  5.0784e+00,  2.0506e+03,  3.9024e+03,
         -3.0687e+03,  2.9387e+03]], dtype=torch.float64)
	q_value: tensor([[-34.9905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.602236433760629, distance: 1.4485049871622118 entropy 9.691626291056155
epoch: 20, step: 15
	action: tensor([[  295.6151,  -585.3664,  -188.6303,  1393.7975, -2664.6884,   745.0087,
          4157.5235]], dtype=torch.float64)
	q_value: tensor([[-22.5743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37232743394203005, distance: 0.9066157654009283 entropy 9.283929610063712
epoch: 20, step: 16
	action: tensor([[ 2696.1358,  -444.8527,  2388.0959,  2904.7516,   410.4183, -3994.4860,
           106.2456]], dtype=torch.float64)
	q_value: tensor([[-20.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.681956179743363, distance: 0.6453572212786669 entropy 9.265916787172825
epoch: 20, step: 17
	action: tensor([[-1302.6967,  -295.0917,   456.2338,  3915.5590,  -821.4051,  -160.2214,
         -5886.7623]], dtype=torch.float64)
	q_value: tensor([[-28.7629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4467674071912815, distance: 1.3764360907008943 entropy 9.327325996178162
epoch: 20, step: 18
	action: tensor([[-1756.6016,   -43.8883, -3710.0749,  2453.4376,  1688.6560,  1366.9006,
          -755.6255]], dtype=torch.float64)
	q_value: tensor([[-22.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24244053571198476, distance: 1.2755412248893911 entropy 9.337132632209231
epoch: 20, step: 19
	action: tensor([[-6414.1826, -5441.0528, -4043.0297,  7142.0799, -3899.4210, -3422.6512,
         -2093.4822]], dtype=torch.float64)
	q_value: tensor([[-27.1662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4458754732031527, distance: 0.8518446104201639 entropy 9.644699788827193
epoch: 20, step: 20
	action: tensor([[ 2.9073e+03, -9.5951e+03,  7.5074e+03,  5.0625e+02,  4.9495e+03,
         -6.1613e+03,  1.3061e+00]], dtype=torch.float64)
	q_value: tensor([[-29.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44196931070287193, distance: 0.8548417707108003 entropy 9.694573198011671
epoch: 20, step: 21
	action: tensor([[-1845.8253, -6055.7668,  -866.8599, -4233.2373,  1952.1409,   114.1001,
          2462.0125]], dtype=torch.float64)
	q_value: tensor([[-22.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1920633591938654, distance: 1.6942724331864145 entropy 9.432252271752912
epoch: 20, step: 22
	action: tensor([[-5384.1206,   300.4254,  -898.4042,  -712.0199,   130.4677,  1347.2185,
          4510.8590]], dtype=torch.float64)
	q_value: tensor([[-19.6407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3815029126296079, distance: 1.3450319344572792 entropy 9.238974486148098
epoch: 20, step: 23
	action: tensor([[ 1507.4597,  1055.2826, -1749.0511, -3250.7117, -2316.7108,  2944.0729,
          4550.4132]], dtype=torch.float64)
	q_value: tensor([[-31.8519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3257349239851537, distance: 0.9396627828340745 entropy 9.316116662255542
epoch: 20, step: 24
	action: tensor([[-2051.1358,  -107.7940,   609.6492,  2822.9632,  6123.9721,  3049.9928,
          3861.4585]], dtype=torch.float64)
	q_value: tensor([[-31.6175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.433671687979642
epoch: 20, step: 25
	action: tensor([[-6983.8726,  -766.6288, -1593.6239,  2436.5286, -1121.5268, -1145.8585,
         -2076.2995]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00028223931529747226, distance: 1.1445057320762952 entropy 9.026022937770174
epoch: 20, step: 26
	action: tensor([[-1213.2940,  4108.2723, -4452.5891,  2351.6957, -3615.7499,  4306.4186,
          6040.7809]], dtype=torch.float64)
	q_value: tensor([[-31.3031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31094815521254204, distance: 0.9499104013838849 entropy 9.807933122823643
epoch: 20, step: 27
	action: tensor([[ -7897.3161, -10182.1985,   4326.6738,   3767.7392,   -479.1922,
           8414.6747,   2641.1978]], dtype=torch.float64)
	q_value: tensor([[-31.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05487798977526637, distance: 1.1125015683184614 entropy 9.76099370046984
epoch: 20, step: 28
	action: tensor([[-4780.3638,  1777.4949,  5502.9119, -1988.3691, -3584.5907,  2411.4722,
          1535.8701]], dtype=torch.float64)
	q_value: tensor([[-30.9888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3854307649396711, distance: 0.8971025708602857 entropy 9.73024168102065
epoch: 20, step: 29
	action: tensor([[-2593.4168, -3400.3898,  6449.3498,  2528.2403,  7743.4184,   725.4238,
         -2116.4879]], dtype=torch.float64)
	q_value: tensor([[-38.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4339403634178205, distance: 1.3703207628298033 entropy 9.67422886952318
epoch: 20, step: 30
	action: tensor([[-3253.2919,  4116.4960,  -683.5922,  1629.0253,  2908.1252, -2209.0177,
          2713.0178]], dtype=torch.float64)
	q_value: tensor([[-33.8713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17096893229297871, distance: 1.0419385253503923 entropy 9.752330721238527
epoch: 20, step: 31
	action: tensor([[-3456.2380, -2998.1156,  3483.7633, -1920.5110,   798.1341,  3518.0852,
          4578.2455]], dtype=torch.float64)
	q_value: tensor([[-26.4175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.895799625236996, distance: 1.5756251697524082 entropy 9.42989097056321
epoch: 20, step: 32
	action: tensor([[-8348.9244,  -998.9327, -2844.6879,  2776.6214, -1122.1469, -1320.7105,
          7897.4480]], dtype=torch.float64)
	q_value: tensor([[-27.8692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03797765095265804, distance: 1.1224041762878458 entropy 9.530047828546472
epoch: 20, step: 33
	action: tensor([[ 1121.9203,  3751.3635, -1062.8960,   743.1353,  3191.7503, -1262.9826,
         -1436.1998]], dtype=torch.float64)
	q_value: tensor([[-25.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18458140793820554, distance: 1.033348939207248 entropy 9.524400846522761
epoch: 20, step: 34
	action: tensor([[ -311.3973,  3124.1193,  1649.3368,  -652.9781,   912.1249,  2718.5177,
         -1143.7544]], dtype=torch.float64)
	q_value: tensor([[-24.4619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06727960695344504, distance: 1.1051785045906155 entropy 9.220090632143524
epoch: 20, step: 35
	action: tensor([[-3406.3266,  -908.6173,  5853.3239,   909.5341,  -259.7027,  4368.2463,
         -2367.8183]], dtype=torch.float64)
	q_value: tensor([[-34.4537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27324085288521993, distance: 1.2912548795282186 entropy 9.388160194298152
epoch: 20, step: 36
	action: tensor([[-7984.5775, -2496.1100, -1969.4725, -7251.2763, -2624.5909, -5781.4885,
          6805.7773]], dtype=torch.float64)
	q_value: tensor([[-31.9008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29733534891632574, distance: 1.3034153134072053 entropy 9.771530403677465
epoch: 20, step: 37
	action: tensor([[-8682.1926, -7929.7361,  4830.8728,  9772.6809,  3345.9819,  -203.5216,
          2110.8011]], dtype=torch.float64)
	q_value: tensor([[-29.9246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2025165671089567, distance: 1.021921480779267 entropy 9.569653749120267
epoch: 20, step: 38
	action: tensor([[-3876.9082, -1533.9551,  -508.1237, -3959.5833,   764.5178, -2097.8736,
         -3538.0523]], dtype=torch.float64)
	q_value: tensor([[-27.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2774720813535554, distance: 1.2933986462680622 entropy 9.701920308247319
epoch: 20, step: 39
	action: tensor([[-3716.4597, -1610.7643,  3216.5200,  3805.4952, -5818.9037, -2262.0110,
          1014.0085]], dtype=torch.float64)
	q_value: tensor([[-23.9578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47120571635855524, distance: 1.3880125570677488 entropy 9.389027259802136
epoch: 20, step: 40
	action: tensor([[-1686.0436,   993.7013,  2507.3319, -2601.0024,  1279.7957, -3877.5977,
          5503.5662]], dtype=torch.float64)
	q_value: tensor([[-22.1638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4540279476989362, distance: 0.8455550728082575 entropy 9.452269354066752
epoch: 20, step: 41
	action: tensor([[-1632.5260,   571.0912, -1131.4780, -1898.7212,  -156.5131,  3651.7436,
          -245.7368]], dtype=torch.float64)
	q_value: tensor([[-28.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35809465971162124, distance: 0.9168371187211725 entropy 9.320848926639178
epoch: 20, step: 42
	action: tensor([[ 1990.3162,  4800.0189, -4843.3684,  3226.4116,  2179.5848,  1834.9770,
          2909.8241]], dtype=torch.float64)
	q_value: tensor([[-33.3875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.281938503988673
epoch: 20, step: 43
	action: tensor([[-4204.0643, -4773.0582,  1825.6860, -1252.8449,   497.7552,   168.4221,
           267.9165]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6165967799072904, distance: 1.4549817567523515 entropy 9.026022937770174
epoch: 20, step: 44
	action: tensor([[ -525.8642, -1214.0629,  1414.7309,  7424.8874,  -973.9336,  2778.6752,
         -3383.7846]], dtype=torch.float64)
	q_value: tensor([[-33.9614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13118761434740933, distance: 1.2170936986109502 entropy 9.672740927079166
epoch: 20, step: 45
	action: tensor([[-4974.1639, -4931.4646,   677.2592, -1888.7878,   920.2682, -3680.1931,
          2912.2447]], dtype=torch.float64)
	q_value: tensor([[-24.8010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2954676943542438, distance: 1.3024767718382166 entropy 9.543146610827847
epoch: 20, step: 46
	action: tensor([[-2157.3093, -1943.4297, -3049.0964,  3617.8672,   -55.2197,  -912.7728,
         -3384.8197]], dtype=torch.float64)
	q_value: tensor([[-33.1352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4466517120658615, distance: 1.376381054165945 entropy 9.640606832777605
epoch: 20, step: 47
	action: tensor([[-1366.1796, -5392.4596,   159.0679,  -233.0237, -3094.1450,  3202.2736,
         -1305.0981]], dtype=torch.float64)
	q_value: tensor([[-25.6033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027035926213333505, distance: 1.1597102912917945 entropy 9.478169789136148
epoch: 20, step: 48
	action: tensor([[-6689.7196,   -21.8423, -1127.0151, -1842.2734,  1928.0735, -4500.5766,
         -6850.7306]], dtype=torch.float64)
	q_value: tensor([[-28.7774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2768393168894927, distance: 1.29307827994306 entropy 9.72649109348445
epoch: 20, step: 49
	action: tensor([[ -678.3872, -5667.6222,  2461.6127,   245.6445,  2178.9774,  1277.6850,
          -123.6834]], dtype=torch.float64)
	q_value: tensor([[-36.1679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7311051933385175, distance: 1.5056305662166083 entropy 9.684807647540453
epoch: 20, step: 50
	action: tensor([[-4158.7366, -4705.4830, -1562.7711,   541.5800,   664.1188,  3285.7894,
            87.8195]], dtype=torch.float64)
	q_value: tensor([[-24.9818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.061855788230498865, distance: 1.1792054090639152 entropy 9.47445509074671
epoch: 20, step: 51
	action: tensor([[-2434.4157, -6033.3594,  4266.7527,  -409.8271,  5514.6651,  1326.0124,
          8610.3840]], dtype=torch.float64)
	q_value: tensor([[-31.0467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.041420952776853, distance: 1.6350196529803074 entropy 9.72576275655582
epoch: 20, step: 52
	action: tensor([[ 2179.1887, -4737.5254,  6184.8164,  -921.3366,  1503.9921,  -877.0915,
           814.2944]], dtype=torch.float64)
	q_value: tensor([[-23.4422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4418621967401003, distance: 0.8549238101915883 entropy 9.513332087407495
epoch: 20, step: 53
	action: tensor([[ -293.3046,   256.6862,  -648.1566,  6011.1198,  2475.0969, -3441.5641,
           865.2548]], dtype=torch.float64)
	q_value: tensor([[-31.6336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.608877548828477
epoch: 20, step: 54
	action: tensor([[-3126.0784, -2307.2524, -1684.8761,   453.8446,  -848.8306,   122.8789,
         -3443.9681]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24487135176924313, distance: 1.2767884036925805 entropy 9.026022937770174
epoch: 20, step: 55
	action: tensor([[-3681.1565, -2706.8772,  2360.0521,  2832.4469,  2057.2964,  2722.0225,
          1384.1355]], dtype=torch.float64)
	q_value: tensor([[-24.0824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5455837967231867, distance: 1.4226660616272124 entropy 9.394755815034571
epoch: 20, step: 56
	action: tensor([[ -4398.7038, -10393.1043,  -1400.9630,  -4162.2096,   3098.1917,
          -3148.2991,   2892.6659]], dtype=torch.float64)
	q_value: tensor([[-32.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14937504573012528, distance: 1.2268390053057638 entropy 9.79519980699368
epoch: 20, step: 57
	action: tensor([[-3609.7550, -1400.3526, -7094.4159,  2524.3556,   725.3407,  2568.3901,
          4751.2403]], dtype=torch.float64)
	q_value: tensor([[-23.7369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1073388258599581, distance: 1.6612074787369648 entropy 9.412794627041027
epoch: 20, step: 58
	action: tensor([[-6075.9186, -3468.9921, -2256.3858,  5407.0834,  2491.3966,   579.5242,
           592.8694]], dtype=torch.float64)
	q_value: tensor([[-27.8618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9351314097441086, distance: 1.5918858571993433 entropy 9.547042918684566
epoch: 20, step: 59
	action: tensor([[-5012.9521, -2315.7739,   275.0745,  7504.6338, -6623.0053, -8471.0528,
          1369.0539]], dtype=torch.float64)
	q_value: tensor([[-31.0106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8490463324756716, distance: 1.5560752318839983 entropy 9.75581586132424
epoch: 20, step: 60
	action: tensor([[ 2695.7083,  3511.7517,  -809.3396,  1521.7104,  2456.3273, -1947.3992,
          1599.7262]], dtype=torch.float64)
	q_value: tensor([[-23.5468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.467164812512834
epoch: 20, step: 61
	action: tensor([[-1.0469e+03, -1.2333e+00,  1.4676e+03,  1.3988e+03, -1.3491e+03,
          6.4017e+02, -1.6667e+03]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5862857200578648, distance: 1.4412768155580113 entropy 9.026022937770174
epoch: 20, step: 62
	action: tensor([[-6005.5569, -5489.1434,  1872.5139, 10889.3087, -1426.7407,  4784.0115,
          1271.4140]], dtype=torch.float64)
	q_value: tensor([[-30.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09494180895350846, distance: 1.0886667148000473 entropy 9.667163715191844
epoch: 20, step: 63
	action: tensor([[-7485.3059, -3660.4454,   840.9086,  9326.7760,  -148.3983, -1657.7381,
          4318.8147]], dtype=torch.float64)
	q_value: tensor([[-28.6933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03999695549215243, distance: 1.1670050281270985 entropy 9.632865893637497
epoch: 20, step: 64
	action: tensor([[ 1353.0750,   205.3128, -1397.7502,  5944.5256,  1339.5785,  1983.0332,
           139.2431]], dtype=torch.float64)
	q_value: tensor([[-28.7968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29037231940796215, distance: 0.9639887534433282 entropy 9.621359173452918
epoch: 20, step: 65
	action: tensor([[ 2167.8288, -7163.1795, -2070.7122, -1126.2975,   301.7436,  3952.0819,
          6070.4474]], dtype=torch.float64)
	q_value: tensor([[-26.8037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27168784150946856, distance: 1.290467147428103 entropy 9.427116530769505
epoch: 20, step: 66
	action: tensor([[ 1362.7937, -5311.5522, -1764.1054,  3281.7929, -1326.7128,  1260.0517,
          1571.3352]], dtype=torch.float64)
	q_value: tensor([[-25.5621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4235259369480747, distance: 0.8688535488294592 entropy 9.392623233488951
epoch: 20, step: 67
	action: tensor([[-3816.6386, -4487.6874,  3156.5369,   889.3638, -2547.6077, -1223.2514,
         -2196.9156]], dtype=torch.float64)
	q_value: tensor([[-26.7472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14513802858899105, distance: 1.0580463473086872 entropy 9.374949193480347
epoch: 20, step: 68
	action: tensor([[-8590.4319, -5831.7944, -4156.6167,  2902.1217,  4032.7203,  2254.4989,
         -2125.9661]], dtype=torch.float64)
	q_value: tensor([[-27.6001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11000619341957918, distance: 1.0795684537581394 entropy 9.762863653840075
epoch: 20, step: 69
	action: tensor([[ 1647.6515,   -63.8593, -4420.7742, -1401.1451,   141.8375, -6618.0393,
         -2837.5427]], dtype=torch.float64)
	q_value: tensor([[-30.1104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2622009959728213, distance: 1.2856446666333945 entropy 9.70563549305065
epoch: 20, step: 70
	action: tensor([[-4673.1369,   669.0216,   497.3893,  1852.5265,  4341.6149, -2912.0330,
         -2473.3779]], dtype=torch.float64)
	q_value: tensor([[-24.3279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.605318497708321
epoch: 20, step: 71
	action: tensor([[  496.2270,  -411.8909,   328.8011, -1896.7501,  -281.5327, -1067.3705,
          1303.7347]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1748295256274114, distance: 1.0395096689618561 entropy 9.026022937770174
epoch: 20, step: 72
	action: tensor([[-7210.4753, -3699.9848,  4559.3973,  1361.5787, -4935.9465,   425.9725,
          4642.6626]], dtype=torch.float64)
	q_value: tensor([[-32.1200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8304680000432574, distance: 1.5482381468381208 entropy 9.678497579209674
epoch: 20, step: 73
	action: tensor([[-2412.3614, -3972.1573,  2791.3741,  4034.8782,   320.9265, -3063.5681,
         -4577.2207]], dtype=torch.float64)
	q_value: tensor([[-26.2261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10244511333641015, distance: 1.2015315571667424 entropy 9.476681202586194
epoch: 20, step: 74
	action: tensor([[-4331.2618, -3370.3245, -3732.6226,  6206.9498, -3620.7761, -1715.2249,
          5045.1907]], dtype=torch.float64)
	q_value: tensor([[-25.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06188464955339845, distance: 1.1083701331106042 entropy 9.533597762185536
epoch: 20, step: 75
	action: tensor([[  462.0869, -2803.7526,  1058.2246, -3386.3634,   643.1443,  1548.9820,
         -1627.9160]], dtype=torch.float64)
	q_value: tensor([[-23.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09901008967650293, distance: 1.1996582170471155 entropy 9.513551842470049
epoch: 20, step: 76
	action: tensor([[-468.0478, -463.2828, 4998.7885,  128.6509, 3901.7507, 1389.9906,
         2099.7516]], dtype=torch.float64)
	q_value: tensor([[-24.9033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07936197455443317, distance: 1.1888861021594734 entropy 9.38176627670552
epoch: 20, step: 77
	action: tensor([[ 971.4357,   11.5606, -909.6716,  940.7451, -965.8247, 1142.5906,
         1707.8585]], dtype=torch.float64)
	q_value: tensor([[-26.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04281968380623491, distance: 1.1195759812640973 entropy 9.476191369386896
epoch: 20, step: 78
	action: tensor([[-3753.6466,  1002.0047,   870.5912, -1185.1694, -2574.9414,  1571.5521,
           858.2576]], dtype=torch.float64)
	q_value: tensor([[-28.6428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.460033769806314
epoch: 20, step: 79
	action: tensor([[-4653.7609,  1696.8259, -1300.7796,   506.7844,  2498.8208,  1764.1561,
          1274.6188]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41108941856249315, distance: 1.3593583550886965 entropy 9.026022937770174
epoch: 20, step: 80
	action: tensor([[-1044.1366, -5847.6299,  1081.4119,   231.6271,  1491.8027, -5447.2833,
           925.2780]], dtype=torch.float64)
	q_value: tensor([[-29.9611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024502192253587518, distance: 1.1582788847582184 entropy 9.28445943037365
epoch: 20, step: 81
	action: tensor([[-3195.3277, -5764.8789,   800.5093, -1623.1739,   -35.9815, -2423.6831,
          2615.8426]], dtype=torch.float64)
	q_value: tensor([[-26.3903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7764490146913745, distance: 1.525222021177685 entropy 9.531506693540127
epoch: 20, step: 82
	action: tensor([[ 1601.7033, -5390.7897,  -573.4838,   533.1053,  1378.4918,  1253.6555,
         -2399.2424]], dtype=torch.float64)
	q_value: tensor([[-24.4334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.410368740294331, distance: 1.3590111817882764 entropy 9.561677069935813
epoch: 20, step: 83
	action: tensor([[-2667.2605, -2744.5392, -2363.7059,  8444.7717, -5358.4386,  6219.7619,
           407.6847]], dtype=torch.float64)
	q_value: tensor([[-29.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3050885506344554, distance: 1.307304280257963 entropy 9.591304248080892
epoch: 20, step: 84
	action: tensor([[-1513.7177, -5046.6825,   355.7973,  7840.3198, -1555.9678, -3835.9008,
          6556.5948]], dtype=torch.float64)
	q_value: tensor([[-27.7908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21362913079642842, distance: 1.0147765007947456 entropy 9.524570271396794
epoch: 20, step: 85
	action: tensor([[-2483.0391, -3435.5784, -3795.1713,  -410.1385,  3817.0088,  6745.3228,
          5725.7227]], dtype=torch.float64)
	q_value: tensor([[-31.9494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4664583366676167, distance: 1.3857712841660985 entropy 9.790046113317528
epoch: 20, step: 86
	action: tensor([[-8721.0503,  3775.8015,  3940.4678,  2537.3630,  4528.8160,  1513.5508,
         -1291.5950]], dtype=torch.float64)
	q_value: tensor([[-27.6309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42288871790540405, distance: 0.8693336199628304 entropy 9.554852045973325
epoch: 20, step: 87
	action: tensor([[   51.2180, -2249.8668,  2966.2316, -1372.6943,  -654.3801,  6325.2226,
          3674.6457]], dtype=torch.float64)
	q_value: tensor([[-29.1120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.566545670961846
epoch: 20, step: 88
	action: tensor([[-4256.2401, -4019.9809,  3494.0173,   197.0141, -2134.3799, -1385.2558,
         -2355.7306]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4083983244806497, distance: 1.3580615177073134 entropy 9.026022937770174
epoch: 20, step: 89
	action: tensor([[-12840.7303,  -3101.4503,    420.7496,    301.3126,  -4381.2316,
          -7064.6016,    301.6939]], dtype=torch.float64)
	q_value: tensor([[-23.4791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05805733096912502, distance: 1.1770944000742598 entropy 9.440543039554184
epoch: 20, step: 90
	action: tensor([[-2413.2343,  -784.8181, -1496.6508, -3622.9188,  5504.9396,  6945.8040,
          -883.0900]], dtype=torch.float64)
	q_value: tensor([[-23.4476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9534426649144159, distance: 1.5993997643759326 entropy 9.525684029428769
epoch: 20, step: 91
	action: tensor([[-7291.9065, -2949.8009, -3830.1629,  -380.8780, -2975.6547,   649.2420,
          1132.1703]], dtype=torch.float64)
	q_value: tensor([[-23.9626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04482556354825151, distance: 1.1697110381232767 entropy 9.362387686415968
epoch: 20, step: 92
	action: tensor([[-1200.2214, -1874.5598, -2422.1961,  4613.9676, -1821.7951, -2436.1487,
          3919.0325]], dtype=torch.float64)
	q_value: tensor([[-34.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8041447876435144, distance: 1.537065544126452 entropy 9.715989005555873
epoch: 20, step: 93
	action: tensor([[-4093.5518, -2502.6204, -3485.2114,   245.2463,  3113.4994,   749.6761,
          3208.1171]], dtype=torch.float64)
	q_value: tensor([[-23.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07332159710708752, distance: 1.185554784078252 entropy 9.46689602024568
epoch: 20, step: 94
	action: tensor([[ 1832.5058,  1912.4436,  2126.3757,  5511.6082, -6642.9519,   425.2893,
          -431.7636]], dtype=torch.float64)
	q_value: tensor([[-27.7236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6526963408068223, distance: 0.6743903896846005 entropy 9.680214848787246
epoch: 20, step: 95
	action: tensor([[-9177.9134, -2772.5035, -4261.5236,  3980.4743,  3474.5507,  8109.3673,
          5139.2671]], dtype=torch.float64)
	q_value: tensor([[-29.9316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14645065205279806, distance: 1.0572337307446058 entropy 9.627278108238398
epoch: 20, step: 96
	action: tensor([[-2671.0126, -3002.9496, -2064.2913, -1811.3409, -1396.7345, -1345.0846,
          2108.5065]], dtype=torch.float64)
	q_value: tensor([[-27.2289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1510585729257938, distance: 1.6783510763500646 entropy 9.519621647584245
epoch: 20, step: 97
	action: tensor([[-3182.2233, -3880.4065, -4993.2863,  1476.9998,  2130.0387,  2487.3690,
         -1687.8821]], dtype=torch.float64)
	q_value: tensor([[-31.1090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14320114133850348, distance: 1.0592442933293524 entropy 9.732697133659695
epoch: 20, step: 98
	action: tensor([[-2243.5943,   428.3479, -3144.5975,  -559.7320,  2396.1444,  2027.9622,
           399.4914]], dtype=torch.float64)
	q_value: tensor([[-27.1586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14888657876131772, distance: 1.0557240441958886 entropy 9.577296581125152
epoch: 20, step: 99
	action: tensor([[ 4340.5391, -2778.2483, -6299.2218, -3812.9701,   626.5775,  -918.5699,
         -1067.8307]], dtype=torch.float64)
	q_value: tensor([[-39.4819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.684520322068831
epoch: 20, step: 100
	action: tensor([[-2022.1931, -2184.9658, -1848.1601,   450.1597,  -920.5570,  1857.0629,
         -1243.6448]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8276979854973168, distance: 1.5470662427410722 entropy 9.026022937770174
epoch: 20, step: 101
	action: tensor([[-1029.2269, -7388.0492,  3127.1709,  6164.1330, -1529.5172, -1002.9649,
          -603.7806]], dtype=torch.float64)
	q_value: tensor([[-27.4005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04531759709929439, distance: 1.1181141717135386 entropy 9.518170921262335
epoch: 20, step: 102
	action: tensor([[-4546.0626, -8820.1568, -4905.9768,  7389.1333, -1056.5985,  6231.2488,
         -3553.0021]], dtype=torch.float64)
	q_value: tensor([[-27.8121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20512234011232033, distance: 1.2562389709521853 entropy 9.693839866944302
epoch: 20, step: 103
	action: tensor([[-1077.1619, -7135.8032,  3640.9488,  9978.6565, -5952.5456,  1295.3228,
         -2431.7610]], dtype=torch.float64)
	q_value: tensor([[-26.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43958292655447884, distance: 1.373014225564694 entropy 9.611005271329974
epoch: 20, step: 104
	action: tensor([[ -574.0633, -1695.5177,   669.8674, -1037.6113,  1772.6483,  8459.8659,
          6500.3634]], dtype=torch.float64)
	q_value: tensor([[-28.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9253887162762506, distance: 1.5878735131159232 entropy 9.651047271224211
epoch: 20, step: 105
	action: tensor([[-1391.4107, -2788.2495, -2397.6750,  3838.4917, -1777.9130,  -247.3933,
          1494.5901]], dtype=torch.float64)
	q_value: tensor([[-33.0691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07143208330249329, distance: 1.1845107778348114 entropy 9.688458959589864
epoch: 20, step: 106
	action: tensor([[ -515.4882, -3391.7033,  4395.7674, -3658.3929, -1270.1110,  4633.3883,
           694.7361]], dtype=torch.float64)
	q_value: tensor([[-24.7405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4651417698504787, distance: 1.3851490809567515 entropy 9.500370476437618
epoch: 20, step: 107
	action: tensor([[-2364.4417, -6035.1509,   881.2077, -1182.7977, -4519.0461,  2349.1389,
          7885.4718]], dtype=torch.float64)
	q_value: tensor([[-29.9816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5532124609875841, distance: 1.4261727244994367 entropy 9.629142771832338
epoch: 20, step: 108
	action: tensor([[-4895.9548,  3316.2690, -2813.6340,  4492.0296, -5188.8564, -2868.5663,
         -6829.7680]], dtype=torch.float64)
	q_value: tensor([[-26.2620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39286755860086475, distance: 0.8916582106165173 entropy 9.491860056167086
epoch: 20, step: 109
	action: tensor([[-5167.3312, -2070.9488,  -601.3326, -4872.0165,  1477.3666,  1107.4589,
          6455.6251]], dtype=torch.float64)
	q_value: tensor([[-25.9397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7726226504157241, distance: 1.5235785174736933 entropy 9.233688911740098
epoch: 20, step: 110
	action: tensor([[ -262.1783, -4115.3637,  -356.8753,  4330.6898, -1045.4420, -3299.1758,
         -1775.0486]], dtype=torch.float64)
	q_value: tensor([[-22.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02235762955794862, distance: 1.1570659527645697 entropy 9.465748829624518
epoch: 20, step: 111
	action: tensor([[ -5020.4660,   1474.3536,    170.6050,  -2749.4730,   4287.1621,
            608.7908, -10285.7963]], dtype=torch.float64)
	q_value: tensor([[-26.6463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5258372880972959, distance: 1.41354879662958 entropy 9.738859593320095
epoch: 20, step: 112
	action: tensor([[ -411.5913,  1699.5109,  2692.1284,  -636.5881,  2689.4628, -3807.9549,
           782.0486]], dtype=torch.float64)
	q_value: tensor([[-30.7690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34701790446186465, distance: 0.9247137808951419 entropy 9.3346938598699
epoch: 20, step: 113
	action: tensor([[-4397.4767, -2634.6476,  4749.6147, -4916.2852, -1314.7072, -4704.2536,
          -507.0556]], dtype=torch.float64)
	q_value: tensor([[-37.6082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09950981421152671, distance: 1.085915882763979 entropy 9.581983448923703
epoch: 20, step: 114
	action: tensor([[  -29.8765, -6577.4628, -7547.1867,  2989.2681, -3426.3084,  3106.9320,
          1581.0535]], dtype=torch.float64)
	q_value: tensor([[-26.0831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.590074625614815
epoch: 20, step: 115
	action: tensor([[-411.0516, -274.4452,  879.1933, 1683.5406, -223.6924,  587.0506,
         -136.7695]], dtype=torch.float64)
	q_value: tensor([[-30.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4545334553119633, distance: 1.7928385058539675 entropy 9.026022937770174
epoch: 20, step: 116
	action: tensor([[-6123.3472,  -300.5429,  1065.9475,  2552.3904,  4835.4421,   700.2689,
          1686.1223]], dtype=torch.float64)
	q_value: tensor([[-24.2939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5643986935428587, distance: 1.431299157252818 entropy 9.306875039678124
epoch: 20, step: 117
	action: tensor([[  321.2994, -7427.1252,  5535.5809,  1187.9559,  -938.0325,  1576.2606,
          -390.4879]], dtype=torch.float64)
	q_value: tensor([[-31.6234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002468666502194017, distance: 1.1457558854885919 entropy 9.767403484552956
epoch: 20, step: 118
	action: tensor([[-2870.0753, -3000.3435,   353.9442,  -228.6032,  3285.6542, -1519.1033,
         -3082.4109]], dtype=torch.float64)
	q_value: tensor([[-24.2146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8091418470296465, distance: 1.53919272834668 entropy 9.421113919048944
epoch: 20, step: 119
	action: tensor([[-5571.4233, -5465.2774,  4310.4696, -2498.4660,  -529.4461,    58.1398,
         -5019.4735]], dtype=torch.float64)
	q_value: tensor([[-20.7748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30829518989467986, distance: 0.9517373018887255 entropy 9.418446697178902
epoch: 20, step: 120
	action: tensor([[-2747.4681,  -717.4396,  2594.7105,  2597.5041, -1333.9492, -2254.4836,
          2096.8711]], dtype=torch.float64)
	q_value: tensor([[-22.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5291162631230806, distance: 1.415066816866217 entropy 9.326816137315248
epoch: 20, step: 121
	action: tensor([[-10914.9311,  -1596.2851,   3449.8688,   4171.5242,   -106.4970,
          -1691.4686,    566.5975]], dtype=torch.float64)
	q_value: tensor([[-24.9011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7061662076023443, distance: 1.494745867116904 entropy 9.474703827679631
epoch: 20, step: 122
	action: tensor([[ -305.1296, -2802.6398, -1585.9029,  1429.9048,  1348.4465,  1986.2742,
          -472.6639]], dtype=torch.float64)
	q_value: tensor([[-21.8385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6420333732015533, distance: 1.4663838979172934 entropy 9.384294289543035
epoch: 20, step: 123
	action: tensor([[-5238.3212,  1066.4356,  -602.7931,  7896.4043,   333.3057, -5422.3655,
          2528.8306]], dtype=torch.float64)
	q_value: tensor([[-27.7003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7123573326101116, distance: 0.6137384709235357 entropy 9.63426453070986
epoch: 20, step: 124
	action: tensor([[-3217.1190, -7469.5940,  5291.5850,  9479.6289, -1090.4113,  6136.1163,
          2048.6276]], dtype=torch.float64)
	q_value: tensor([[-30.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18504046914005468, distance: 1.2457281664638389 entropy 9.55035191638792
epoch: 20, step: 125
	action: tensor([[-7050.4627, -9308.0216, -9520.4020,   138.6437,  4101.6194,  2723.1399,
          1415.1588]], dtype=torch.float64)
	q_value: tensor([[-29.5429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41086430500959303, distance: 1.3592499203607116 entropy 9.633270399760862
epoch: 20, step: 126
	action: tensor([[ -377.2523, -4192.4257,  1899.7009,  3571.7422,   782.3925,  1955.6586,
           304.4094]], dtype=torch.float64)
	q_value: tensor([[-25.4516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4988045911703154, distance: 1.4009711778673026 entropy 9.486916821757132
epoch: 20, step: 127
	action: tensor([[-4617.1288,   588.0835,   535.7718,  3515.1884,  -758.2583, -5972.2753,
         -9851.6528]], dtype=torch.float64)
	q_value: tensor([[-29.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4967569792218237, distance: 0.8117935073849917 entropy 9.730578671780131
LOSS epoch 20 actor 329.78762740948895 critic 248.36807179741362
epoch: 21, step: 0
	action: tensor([[  210.1316, -7786.9876, -5455.9345,  7707.7392,   600.4079,  5582.8047,
         -1309.8410]], dtype=torch.float64)
	q_value: tensor([[-31.2115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33854014503941987, distance: 1.3239524686536577 entropy 9.583116054358772
epoch: 21, step: 1
	action: tensor([[-11123.8923,  -8036.5606,  -2540.8310,   2989.9512,  -1380.9236,
          -4318.2412,  -2501.1921]], dtype=torch.float64)
	q_value: tensor([[-33.3239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00656351149478307, distance: 1.1480935702244905 entropy 9.859348911916909
epoch: 21, step: 2
	action: tensor([[ 2136.6745, -5590.6126,  -957.6899, -2715.6931, -1745.8260,  1711.4975,
         -1957.1595]], dtype=torch.float64)
	q_value: tensor([[-28.5142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15016229374232282, distance: 1.2272590863081043 entropy 9.600532987979525
epoch: 21, step: 3
	action: tensor([[-8212.4452,  -115.1255, -3285.0385,  5222.1062,  5008.5662,  1595.9971,
          2580.1519]], dtype=torch.float64)
	q_value: tensor([[-26.0299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02776133701536887, distance: 1.1601197793407303 entropy 9.540876536593755
epoch: 21, step: 4
	action: tensor([[  512.0919,   734.8371,  1458.9626,   667.5754,  2979.3591,  -214.8748,
         -3364.0270]], dtype=torch.float64)
	q_value: tensor([[-29.6540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16595206137450969, distance: 1.0450864090286742 entropy 9.687688719426811
epoch: 21, step: 5
	action: tensor([[-1314.3045,   342.4972,  1352.4838,   579.6482,  2015.3279,  3771.0169,
         -1625.7125]], dtype=torch.float64)
	q_value: tensor([[-29.0699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5613543164234107, distance: 0.7579029950995243 entropy 9.370080599772995
epoch: 21, step: 6
	action: tensor([[-4731.7621,  1076.2002,  1750.9661, -4975.1456,   932.0697,   757.5675,
         -4266.3289]], dtype=torch.float64)
	q_value: tensor([[-28.2124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4776452727945584, distance: 0.8270646483292412 entropy 9.56526884913912
epoch: 21, step: 7
	action: tensor([[  332.8320,   607.3209,  1503.0044,  1836.1910, -7395.3868, 11083.6687,
           985.9993]], dtype=torch.float64)
	q_value: tensor([[-41.5637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.654029524528282
epoch: 21, step: 8
	action: tensor([[-2385.3276, -1561.8500, -1397.8170,  -309.6434, -1121.3289,  2355.0934,
           860.4451]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09035971426046274, distance: 1.091419066096099 entropy 9.099545669269745
epoch: 21, step: 9
	action: tensor([[ -379.1710, -2990.3492, -5438.9754,   945.5049, -6114.6354, -6447.4925,
          -631.8816]], dtype=torch.float64)
	q_value: tensor([[-36.0784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7797346152300539, distance: 1.5266318436004573 entropy 9.758222942449803
epoch: 21, step: 10
	action: tensor([[-4047.6731,   810.1341, -1781.9565,  1093.3122,  2842.1000, -2030.0611,
         -3434.2695]], dtype=torch.float64)
	q_value: tensor([[-23.8040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3204570544262695, distance: 0.9433332608864015 entropy 9.554797013622679
epoch: 21, step: 11
	action: tensor([[-8465.7377,  1303.6090, 10246.7002, -4873.7129, -2855.9691, 11799.0472,
          1700.1344]], dtype=torch.float64)
	q_value: tensor([[-34.8183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33459173723412006, distance: 1.3219983378858142 entropy 9.768266677730916
epoch: 21, step: 12
	action: tensor([[ -8841.6512,  -1170.9654,  -3906.2370,  -8253.9530,   -548.8065,
         -10193.0209,   7778.0374]], dtype=torch.float64)
	q_value: tensor([[-40.8363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6799800186527007, distance: 1.4832308553776292 entropy 9.714486255146591
epoch: 21, step: 13
	action: tensor([[ 1099.2917, -5246.7477, -2560.9919,  1246.8148, -7197.8159,  4427.1429,
          5814.0499]], dtype=torch.float64)
	q_value: tensor([[-30.1649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09868170919742547, distance: 1.19947897675831 entropy 9.64856193386255
epoch: 21, step: 14
	action: tensor([[-2987.8545, -4333.5333,  1676.0928, -1040.9940, -7340.8037,  1174.6348,
          2511.6818]], dtype=torch.float64)
	q_value: tensor([[-30.2687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22630798423330456, distance: 1.0065625100271767 entropy 9.615353068731583
epoch: 21, step: 15
	action: tensor([[ 2548.3122, -2121.4869,  4122.9637, -6565.2655, -5949.5786, -2500.8402,
          -239.2506]], dtype=torch.float64)
	q_value: tensor([[-37.0691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.834086121663201
epoch: 21, step: 16
	action: tensor([[-1115.9787,   728.5544, -3357.9708, -2701.4651,  -810.1019,  2686.4155,
          1149.8589]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6075051132169879, distance: 0.7169249503947119 entropy 9.099545669269745
epoch: 21, step: 17
	action: tensor([[  465.3865,  -467.7507, -4542.1912,  -473.6193,  1094.6234,  4394.6457,
          1254.9126]], dtype=torch.float64)
	q_value: tensor([[-43.6498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18853912855071076, distance: 1.2475657266807885 entropy 9.583344708042873
epoch: 21, step: 18
	action: tensor([[  -94.2668,  1668.5565,  1980.1357,   903.9289, -4054.1262,  1248.6327,
         -2068.9307]], dtype=torch.float64)
	q_value: tensor([[-26.4430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49050887503778196, distance: 0.8168174456935008 entropy 9.475228743499384
epoch: 21, step: 19
	action: tensor([[-1822.9498, -5146.3655, -2235.5987,  -614.2324,   -81.0002, -1618.8184,
          -794.5455]], dtype=torch.float64)
	q_value: tensor([[-28.4260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1628921671118162, distance: 1.2340319836671219 entropy 9.364429006483912
epoch: 21, step: 20
	action: tensor([[-1885.9522, -8844.3926,  -514.2496,  1301.9362, -1173.8509, -2909.9876,
           -69.3350]], dtype=torch.float64)
	q_value: tensor([[-32.0009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9174401583201608, distance: 1.584592524298018 entropy 9.594816353312554
epoch: 21, step: 21
	action: tensor([[-1654.2137, -5860.6639,   194.6334,  1049.1866,  5558.8459, -5036.9006,
           507.7426]], dtype=torch.float64)
	q_value: tensor([[-23.3316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28231750885735796, distance: 1.2958492430325967 entropy 9.342644952262004
epoch: 21, step: 22
	action: tensor([[  826.0925,  3457.1776,   -44.8360, -1744.8501,  1662.3004,   456.8080,
          -322.2352]], dtype=torch.float64)
	q_value: tensor([[-26.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.577265716009533
epoch: 21, step: 23
	action: tensor([[  643.0494, -2171.7053,  -536.3056,    90.2016, -1889.1602, -1502.3840,
         -1741.3068]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002183044881321927, distance: 1.143094494123788 entropy 9.099545669269745
epoch: 21, step: 24
	action: tensor([[ 1998.1248, -2414.5824, -6190.2117,  8430.2828, -5332.8920,  4467.6465,
          3428.7848]], dtype=torch.float64)
	q_value: tensor([[-29.9909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5019908650753069, distance: 0.8075610197930065 entropy 9.571426777615516
epoch: 21, step: 25
	action: tensor([[ 2445.9053,   503.6107,  3473.3979, -4558.0741,   -82.4643, -8320.5120,
         -8259.2658]], dtype=torch.float64)
	q_value: tensor([[-29.2085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3180775358485789, distance: 0.9449834269642885 entropy 9.604847872985516
epoch: 21, step: 26
	action: tensor([[ 2191.7074,   -28.3399,  5973.8965,   859.5863,  1124.6684, -7032.1232,
         -1879.1730]], dtype=torch.float64)
	q_value: tensor([[-31.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8364998649031599, distance: 0.46271731497685936 entropy 9.563790033996098
epoch: 21, step: 27
	action: tensor([[-2572.2891,  -598.3328,   -36.6331,  -861.3415, -2876.5172,  2387.2463,
          2445.2994]], dtype=torch.float64)
	q_value: tensor([[-26.6501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11327888895859806, distance: 1.2074208750811182 entropy 9.39761640394995
epoch: 21, step: 28
	action: tensor([[-1427.8673, -1335.4845, -1818.7489,  1555.7832, -2372.9501, -2184.5960,
          3659.9432]], dtype=torch.float64)
	q_value: tensor([[-20.3769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09251595323960038, distance: 1.0901247321465164 entropy 9.226091885450046
epoch: 21, step: 29
	action: tensor([[-4806.4588, -3051.0310,  3471.9173, -2212.2688,  2600.7995,  9096.5333,
          -831.3902]], dtype=torch.float64)
	q_value: tensor([[-33.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8295167480695149, distance: 1.5478358027427463 entropy 9.79567850222098
epoch: 21, step: 30
	action: tensor([[-6099.2768, -5085.0495,  4039.6777,   358.7543,  -703.6567,  1631.1681,
          3878.1510]], dtype=torch.float64)
	q_value: tensor([[-29.5061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011238367554931128, distance: 1.1378958046301735 entropy 9.619996811788242
epoch: 21, step: 31
	action: tensor([[-3171.8667, -8450.1340, -2528.6156,  3445.1360,  -808.6872,  1097.6695,
          -398.5390]], dtype=torch.float64)
	q_value: tensor([[-28.9927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5220722647455613, distance: 1.4118037444606004 entropy 9.639717306174116
epoch: 21, step: 32
	action: tensor([[-1462.1906, -5915.8115,  1349.0608, -1278.1935,   233.1738,   373.8964,
            55.2350]], dtype=torch.float64)
	q_value: tensor([[-34.3008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8912590591003282, distance: 1.5737371750913591 entropy 9.855020798097367
epoch: 21, step: 33
	action: tensor([[2158.7916,  740.9574, 3563.1651,  100.0956, -291.8784, -487.3484,
         7532.1081]], dtype=torch.float64)
	q_value: tensor([[-26.8837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.4818957227957
epoch: 21, step: 34
	action: tensor([[-2918.6047, -1035.5545, -3841.2088,   601.8392,  1715.2780,  1271.3784,
         -1771.2530]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11001850030052074, distance: 1.0795609895677927 entropy 9.099545669269745
epoch: 21, step: 35
	action: tensor([[-4855.5466, -4174.9379,  -216.2778, -5007.0154, -3798.2180, -6855.6692,
           727.3163]], dtype=torch.float64)
	q_value: tensor([[-28.1809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9719339596335084, distance: 1.6069518960719589 entropy 9.600437502813406
epoch: 21, step: 36
	action: tensor([[-1218.5412, -5266.0978, -2207.9287,  2074.0383,  6395.5443, -1993.3790,
         -3501.5610]], dtype=torch.float64)
	q_value: tensor([[-32.8268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.729957116062592, distance: 1.50513121269915 entropy 9.67353213215608
epoch: 21, step: 37
	action: tensor([[-2105.0914, -6534.1620, -1520.0979, -1698.2677,  -192.4728, -1969.1490,
          6951.6320]], dtype=torch.float64)
	q_value: tensor([[-29.8044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9433756203366748, distance: 1.595273196697308 entropy 9.720722566927103
epoch: 21, step: 38
	action: tensor([[-3083.5251, -2703.0159, -2154.8440,   862.5557,   574.9961, -4759.7377,
          3899.1994]], dtype=torch.float64)
	q_value: tensor([[-29.1054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03146127268204002, distance: 1.1261991330128607 entropy 9.65340451146368
epoch: 21, step: 39
	action: tensor([[-6370.2323,   582.3987,  1176.3976,  -641.5523,  -794.1338, -1697.0869,
          2632.5967]], dtype=torch.float64)
	q_value: tensor([[-32.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02066472193527602, distance: 1.1324587529149883 entropy 9.835955438612302
epoch: 21, step: 40
	action: tensor([[ 2460.3583,    42.5015, -3163.5238, -2545.0885, -3142.7234,  5005.6247,
          2257.5393]], dtype=torch.float64)
	q_value: tensor([[-30.8209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40880153940923525, distance: 0.8798797860583394 entropy 9.501754825433784
epoch: 21, step: 41
	action: tensor([[-1255.3242, -5039.6919,  4133.8746,  1201.7575, -1452.0340,  -675.6640,
          1599.4604]], dtype=torch.float64)
	q_value: tensor([[-27.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.523759632543346
epoch: 21, step: 42
	action: tensor([[-4701.1834,  1216.4017,  -457.1642, -2681.8646, -2859.2785,   104.1899,
           471.6306]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.099545669269745
epoch: 21, step: 43
	action: tensor([[-1392.7556, -4201.1922,   752.8252,  -227.3266,    48.7001, -1204.8470,
          1507.6788]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.099545669269745
epoch: 21, step: 44
	action: tensor([[ -894.6001,  2551.8351,   328.7503,  5285.5328, -3584.0644,   791.5429,
          1356.0383]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.82589801989745, distance: 0.4774836977718253 entropy 9.099545669269745
epoch: 21, step: 45
	action: tensor([[-3756.5247, -1645.2080,  2917.9536,  -314.9723, -1739.8115, -1585.2094,
         -2442.3404]], dtype=torch.float64)
	q_value: tensor([[-27.2618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5740879714591294, distance: 1.4357247707392358 entropy 9.3629804744189
epoch: 21, step: 46
	action: tensor([[-1318.0046, -2621.6526,  3200.4937, -2132.2756, -3182.6662,   369.4226,
         -3934.3663]], dtype=torch.float64)
	q_value: tensor([[-23.4434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11892238081258677, distance: 1.074147144019848 entropy 9.464100592053983
epoch: 21, step: 47
	action: tensor([[-5029.6055,  1571.8223,   111.8908, -8710.7663,  4028.2784,  1214.3384,
           230.4549]], dtype=torch.float64)
	q_value: tensor([[-30.7990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5835469190903981, distance: 0.7384816918741736 entropy 9.736150022336265
epoch: 21, step: 48
	action: tensor([[ -671.6992, -2460.8219,  1130.2116,  2070.3234,  1815.2087,  -307.1646,
         -1282.8512]], dtype=torch.float64)
	q_value: tensor([[-41.8973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7442642109432549, distance: 1.5113422670893835 entropy 9.580584304712122
epoch: 21, step: 49
	action: tensor([[-3648.3910, -3030.0229, -2129.5032,  2167.1981,  1736.9844, -3091.7515,
          1931.3161]], dtype=torch.float64)
	q_value: tensor([[-24.2898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1120747604462905, distance: 1.078313131077881 entropy 9.389575200870194
epoch: 21, step: 50
	action: tensor([[-2455.6474, -5601.5620, -1693.2849,  3543.7087, -3230.2562,  -305.7957,
         -6473.1061]], dtype=torch.float64)
	q_value: tensor([[-35.1069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1486295862834357, distance: 1.2264410903395992 entropy 9.809960410591819
epoch: 21, step: 51
	action: tensor([[ 2348.1897, -4902.9180,  3343.1817,  -221.5744,  2818.4100,  -623.2351,
         -1580.9359]], dtype=torch.float64)
	q_value: tensor([[-28.1914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045359523016401, distance: 0.9543200211657749 entropy 9.623798938567331
epoch: 21, step: 52
	action: tensor([[-3337.8646,  3090.6918,  1370.7176,   884.8238,   236.7352,  6264.9101,
          4430.0460]], dtype=torch.float64)
	q_value: tensor([[-31.4113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.693079625669819
epoch: 21, step: 53
	action: tensor([[-4277.1973,  1686.2573,   628.1764,   272.8369, -2199.9069, -1562.8419,
         -1225.6812]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.099545669269745
epoch: 21, step: 54
	action: tensor([[-1742.1209, -1512.7128,   299.7942,  -222.5205,   682.8470, -1096.3498,
          4161.6788]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43922660235303734, distance: 1.3728442914587256 entropy 9.099545669269745
epoch: 21, step: 55
	action: tensor([[-3514.5346, -3428.9971,  4341.1121, -2213.4964,   958.8400, -1541.8377,
          1258.3010]], dtype=torch.float64)
	q_value: tensor([[-32.7255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2042138729580567, distance: 1.2557653812733978 entropy 9.71122661024086
epoch: 21, step: 56
	action: tensor([[ -898.5060, -3158.3115,  3666.2916, -2874.2347, -5734.7865,  3435.4138,
         -2369.2192]], dtype=torch.float64)
	q_value: tensor([[-30.9062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3260848544673589, distance: 1.3177782970465983 entropy 9.708611161985173
epoch: 21, step: 57
	action: tensor([[-2929.9318, -4281.3151, -1571.5331,   508.2134,  1863.2423, -4824.4706,
          3471.4237]], dtype=torch.float64)
	q_value: tensor([[-30.4910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0623720755106123, distance: 1.643388346951118 entropy 9.615343205215359
epoch: 21, step: 58
	action: tensor([[ -726.6953, -7491.1737,  1052.1535,  4418.9067,  5384.9834,   344.2940,
          4896.0367]], dtype=torch.float64)
	q_value: tensor([[-29.5824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08247158256130316, distance: 1.1905974422589254 entropy 9.69662439575052
epoch: 21, step: 59
	action: tensor([[-4712.5366, -1842.0031, -2107.0443,   875.5469,  9957.8718, -4769.7308,
         -8986.2434]], dtype=torch.float64)
	q_value: tensor([[-32.7021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7653489646721909, distance: 1.520449418599177 entropy 9.763743593229961
epoch: 21, step: 60
	action: tensor([[-7697.6795, -1626.1548, -4916.5404,  -787.9831,   730.4268, -3225.7383,
          7456.5998]], dtype=torch.float64)
	q_value: tensor([[-29.5623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4775338230270898, distance: 1.390994487656157 entropy 9.714682430273816
epoch: 21, step: 61
	action: tensor([[-3558.5823,  3337.7106, -2063.3265,  3765.8804, -6433.7668, -5654.3463,
          3001.6848]], dtype=torch.float64)
	q_value: tensor([[-33.6517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14936093449587728, distance: 1.2268314741409843 entropy 9.70224416545578
epoch: 21, step: 62
	action: tensor([[-3689.6929, -5493.3763, -1368.6975, -1109.3584,  2141.9265,  1698.7384,
         -1980.4542]], dtype=torch.float64)
	q_value: tensor([[-36.5040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7710687198543429, distance: 1.5229105652984667 entropy 9.639552144855399
epoch: 21, step: 63
	action: tensor([[-4687.0760,   914.8265,  1276.1017, -2328.7441,  -937.2592, -7028.7879,
         -2501.7457]], dtype=torch.float64)
	q_value: tensor([[-26.1935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22553250991071816, distance: 1.2668322519866952 entropy 9.504389774832232
epoch: 21, step: 64
	action: tensor([[ 2715.0330, -1761.4269,    78.2487,  -466.9170,  1360.2034, -4075.2108,
         -3256.8845]], dtype=torch.float64)
	q_value: tensor([[-34.2197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4477411121402636, distance: 0.850409396564028 entropy 9.466614238532708
epoch: 21, step: 65
	action: tensor([[-3985.4216,   552.7372,  5779.2966,  2699.6961, -8946.4195,  -193.4849,
         -9151.4719]], dtype=torch.float64)
	q_value: tensor([[-35.0824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.7411741639278
epoch: 21, step: 66
	action: tensor([[-3117.7523, -5891.6553,  1035.0420,   360.5213,   315.1477,   455.8834,
          -713.1469]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8195146810831047, distance: 1.5435989530210927 entropy 9.099545669269745
epoch: 21, step: 67
	action: tensor([[-1453.0955, -3258.0314,  1745.4770,  3197.2837, -1985.2513,   811.5273,
         -1299.5408]], dtype=torch.float64)
	q_value: tensor([[-24.8648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1172949533539992, distance: 1.075138708237069 entropy 9.417536562607859
epoch: 21, step: 68
	action: tensor([[-7170.8608, -2419.7744, -1464.6254,  1829.1566, -1543.9298,  3862.2545,
         -3138.3582]], dtype=torch.float64)
	q_value: tensor([[-27.8986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41456841404725275, distance: 1.3610330506046673 entropy 9.560101460715634
epoch: 21, step: 69
	action: tensor([[-5987.0277, -1381.0500, -1210.4763,  4320.6109, -6307.0900, -3170.4213,
          3352.2400]], dtype=torch.float64)
	q_value: tensor([[-31.2735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5303900353426942, distance: 1.415656078006645 entropy 9.763924473505138
epoch: 21, step: 70
	action: tensor([[-6123.4308, -1037.2245, -2560.3405, -5488.8840, -2811.9793,  2479.1316,
          4498.2095]], dtype=torch.float64)
	q_value: tensor([[-31.2011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9456737771836259, distance: 1.5962161705492586 entropy 9.697257294799945
epoch: 21, step: 71
	action: tensor([[ -4229.1555, -11608.8168,   2972.6810,   4988.1058,   2198.0319,
           -775.3181,   -358.1714]], dtype=torch.float64)
	q_value: tensor([[-29.0442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39719864703327445, distance: 1.3526510422223457 entropy 9.687835281684135
epoch: 21, step: 72
	action: tensor([[  330.6888, -4842.5860,  -379.5150, -4806.5612,   498.3190,  2570.7736,
          2345.3405]], dtype=torch.float64)
	q_value: tensor([[-20.3372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15022456465794432, distance: 1.2272923083642675 entropy 9.345128926506003
epoch: 21, step: 73
	action: tensor([[ 2981.4253, -1301.4451,  1028.0346, -1406.9916,    28.6298,  -212.9608,
         -4047.1425]], dtype=torch.float64)
	q_value: tensor([[-25.5416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19835290132825945, distance: 1.0245857368688926 entropy 9.489531832738267
epoch: 21, step: 74
	action: tensor([[-2125.4504, -5753.1222, -2137.8338,  3691.9634,  1017.0294,  3213.2250,
         10891.5804]], dtype=torch.float64)
	q_value: tensor([[-31.2959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.510714167664795, distance: 1.4065262581071336 entropy 9.708656579111251
epoch: 21, step: 75
	action: tensor([[ -793.9750,   925.2936,  4120.4267,  2982.5279,   523.4616, -3984.3199,
          6444.4411]], dtype=torch.float64)
	q_value: tensor([[-24.8587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47450452344735683, distance: 1.389567819459945 entropy 9.539193942183413
epoch: 21, step: 76
	action: tensor([[  554.1280, -3883.2913, -1767.6729,  3883.1191,  -205.5811,  9634.5047,
          1433.5398]], dtype=torch.float64)
	q_value: tensor([[-33.1443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2765607139059648, distance: 0.9733246850331683 entropy 9.574591367361768
epoch: 21, step: 77
	action: tensor([[-1224.2242,  2759.3537,  2803.3665,  3868.0157,  4443.4930, -2748.6607,
          1402.7793]], dtype=torch.float64)
	q_value: tensor([[-28.5816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39666359110281546, distance: 0.888866339648735 entropy 9.579268035510045
epoch: 21, step: 78
	action: tensor([[-2135.2524, -5677.7723, -2143.4738, -2409.3159, -8059.4443,  -515.0237,
          4272.9604]], dtype=torch.float64)
	q_value: tensor([[-36.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.656174508768405
epoch: 21, step: 79
	action: tensor([[-3879.5357, -2445.6371, -1014.6549,   878.2104, -1464.0399,  1524.8442,
          1065.5972]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12633387499755955, distance: 1.069619819930069 entropy 9.099545669269745
epoch: 21, step: 80
	action: tensor([[ 5039.1987,  1642.8291,   750.7070,  -856.2798,  1944.9099, -5563.0836,
         -7421.6903]], dtype=torch.float64)
	q_value: tensor([[-31.4606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.77939209170275
epoch: 21, step: 81
	action: tensor([[  237.5192,  -327.7254, -2099.5098, -2311.4504,  5221.3661, -5928.2330,
          2826.5730]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0016101528958487599, distance: 1.1452651680532075 entropy 9.099545669269745
epoch: 21, step: 82
	action: tensor([[-2054.8448, -4113.1083,  3810.6611,   -98.3352, -2116.7551,  2249.7812,
          7154.7630]], dtype=torch.float64)
	q_value: tensor([[-21.5443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7414916779174812, distance: 1.510140639273676 entropy 9.11317742175564
epoch: 21, step: 83
	action: tensor([[ -823.3134,  1086.1663,  -531.0554, -2749.9532,  -421.6184,   499.9082,
         -2339.7479]], dtype=torch.float64)
	q_value: tensor([[-32.4960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6151425827363454, distance: 0.709915443280051 entropy 9.655985373508702
epoch: 21, step: 84
	action: tensor([[-11140.0330,   2352.6485,   9493.2614,  -4927.1205,   -486.7312,
          -3834.9197,   7856.2386]], dtype=torch.float64)
	q_value: tensor([[-35.2433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.674794382368626
epoch: 21, step: 85
	action: tensor([[-6171.7632, -4090.9610,   617.8137,  6198.3045, -1026.3858, -3428.2739,
          1707.6590]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08522040196871128, distance: 1.1921081804398057 entropy 9.099545669269745
epoch: 21, step: 86
	action: tensor([[-3814.3836, -6180.1764, -3080.9445,  4508.4646, -3401.0328, -1044.8828,
         -1768.1893]], dtype=torch.float64)
	q_value: tensor([[-31.0680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46182581988751026, distance: 1.3835807389426833 entropy 9.805728806972672
epoch: 21, step: 87
	action: tensor([[-1529.7625, -6018.8309, -9138.3504, -1802.4798, -6582.2005,  2158.0393,
          9462.7695]], dtype=torch.float64)
	q_value: tensor([[-31.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6788071663410962, distance: 1.4827130175329133 entropy 9.83619912406545
epoch: 21, step: 88
	action: tensor([[-8292.1780, -5201.3489,  3897.7137,  5582.1005, -1131.6814,  2967.5040,
          8876.9289]], dtype=torch.float64)
	q_value: tensor([[-31.8900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00020324471731403282, distance: 1.1442279571283114 entropy 9.692200973665487
epoch: 21, step: 89
	action: tensor([[-6240.2844, -2064.2055, -2536.8450,  -580.3503, -4837.2462, -1374.9590,
          2244.5450]], dtype=torch.float64)
	q_value: tensor([[-29.5493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9925238397268825, distance: 1.6153195763461867 entropy 9.693593249438006
epoch: 21, step: 90
	action: tensor([[-3422.2838, -3926.5403,  -128.5821,  -597.4726,   315.6408,  1757.4167,
          3937.6327]], dtype=torch.float64)
	q_value: tensor([[-21.6974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6996511513124188, distance: 1.491889267398116 entropy 9.330202184773125
epoch: 21, step: 91
	action: tensor([[  844.7998,  2430.7757, -7924.7667, -2940.4058,   880.5943, -6395.6393,
         -1267.3367]], dtype=torch.float64)
	q_value: tensor([[-34.1631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7053808590826586, distance: 0.6211366746684145 entropy 9.693925216640986
epoch: 21, step: 92
	action: tensor([[ -353.3741, -7344.7609,  1369.0255, -2103.0653, -1380.3163, -2264.6508,
          5512.7596]], dtype=torch.float64)
	q_value: tensor([[-38.3591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18632678746237863, distance: 1.246404079398959 entropy 9.615532439638189
epoch: 21, step: 93
	action: tensor([[-6599.6673, -2699.4865,  1629.1870,  -822.7596, -7362.7674, -8417.3675,
         -2681.7475]], dtype=torch.float64)
	q_value: tensor([[-31.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3806762232051828, distance: 1.344629441614612 entropy 9.708983683183765
epoch: 21, step: 94
	action: tensor([[ 378.6505, 2016.2374, 1009.6455, 4176.7197, 2093.8794, 4558.8277,
         3948.1283]], dtype=torch.float64)
	q_value: tensor([[-26.8386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.572872086457796
epoch: 21, step: 95
	action: tensor([[-3496.7395,   759.3308,  2096.9313,   810.8757, -2270.6803,   519.1748,
          4673.9575]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29673412003326294, distance: 1.3031132551354936 entropy 9.099545669269745
epoch: 21, step: 96
	action: tensor([[-4323.6350, -5900.8378, -1811.0239,  2744.7688, -2574.5953, -2852.6293,
          4336.9782]], dtype=torch.float64)
	q_value: tensor([[-33.2020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17155794375759448, distance: 1.2386214018944526 entropy 9.42166694139567
epoch: 21, step: 97
	action: tensor([[-9832.4966, -3680.3575,   719.6094,  2790.4091, -5901.0570,  2772.4366,
          6962.8316]], dtype=torch.float64)
	q_value: tensor([[-27.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20515609297578685, distance: 1.020228891479533 entropy 9.575400208545952
epoch: 21, step: 98
	action: tensor([[  382.9024, -5409.4630, -5708.2790,  4212.0176, -1083.4887,   -15.8317,
          5029.1411]], dtype=torch.float64)
	q_value: tensor([[-31.1720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15218761135425551, distance: 1.0536747490766953 entropy 9.742994507172515
epoch: 21, step: 99
	action: tensor([[-2337.5250, -3170.6660, -5089.4629,  3708.6804,  3741.2344, -2409.2584,
         -1436.2926]], dtype=torch.float64)
	q_value: tensor([[-31.6753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13196593905161114, distance: 1.0661666086615755 entropy 9.829072248743433
epoch: 21, step: 100
	action: tensor([[-4518.2912,   671.6868,  -726.3572, -4360.8242, -7251.5303,  4003.5660,
         -6551.6262]], dtype=torch.float64)
	q_value: tensor([[-33.6845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38813187704946606, distance: 0.8951289583777474 entropy 9.773823456052181
epoch: 21, step: 101
	action: tensor([[-1021.2379, -2197.7547, -3685.8554,    56.2787,  1265.2046,  3149.1661,
         -5120.7060]], dtype=torch.float64)
	q_value: tensor([[-27.9171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0945914035246984, distance: 1.6561754843120264 entropy 9.555788916405744
epoch: 21, step: 102
	action: tensor([[-7200.1193, -6796.2904,   -64.1736,  3002.4464,  -770.2028,  4536.9060,
         -5463.6207]], dtype=torch.float64)
	q_value: tensor([[-26.1308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6447111547467885, distance: 1.4675790795251615 entropy 9.57915485475072
epoch: 21, step: 103
	action: tensor([[-1302.5555, -4015.3496, -1597.6553, -1480.6911, -2274.4663,  5688.5740,
          4569.2479]], dtype=torch.float64)
	q_value: tensor([[-34.9684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9688062180891757, distance: 1.6056769738603536 entropy 9.796537142229692
epoch: 21, step: 104
	action: tensor([[  556.8003, -2604.1097,  1000.7973,  -414.0841,  2784.2915, -6264.6601,
          7436.8387]], dtype=torch.float64)
	q_value: tensor([[-24.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5412088341481731, distance: 0.7751115648162303 entropy 9.398358175879023
epoch: 21, step: 105
	action: tensor([[-5005.1861, -2017.9903, -2925.6853,  3026.1538, -3875.0835,   396.2714,
         -3645.2321]], dtype=torch.float64)
	q_value: tensor([[-33.6025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9079570881902976, distance: 1.5806692134339062 entropy 9.689839321906456
epoch: 21, step: 106
	action: tensor([[-1213.8019,  -659.1340,   -28.6371, -3083.9895,   121.5567,  1284.0501,
          2412.8359]], dtype=torch.float64)
	q_value: tensor([[-24.7310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16686072349120118, distance: 1.236135856462145 entropy 9.50761278168979
epoch: 21, step: 107
	action: tensor([[-6282.3137, -2226.1639,   207.2403,   235.8297,  4015.9523, -3151.5323,
          -594.6522]], dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9185519767766792, distance: 1.585051866898919 entropy 9.681304260219258
epoch: 21, step: 108
	action: tensor([[-4027.0213, -5621.7291,  1725.7508, -3802.9146,  3949.3655,  6295.2755,
           715.3022]], dtype=torch.float64)
	q_value: tensor([[-23.9076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2654118016389061, distance: 0.9807959564235635 entropy 9.491239935550905
epoch: 21, step: 109
	action: tensor([[ 1927.8782, -1029.6064, -4920.5298,   -17.2187, -2369.6216,  5163.3482,
         -1799.4761]], dtype=torch.float64)
	q_value: tensor([[-29.3401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.613872472260056
epoch: 21, step: 110
	action: tensor([[ -595.2595, -2495.4214, -1694.9076,  1170.3295, -1770.7247,   881.2555,
          3118.9183]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35064524822635046, distance: 1.3299255842474162 entropy 9.099545669269745
epoch: 21, step: 111
	action: tensor([[ -113.0160, -2628.9461,   -43.6746, -3961.3597, -5770.9724, -1585.8552,
          5300.5513]], dtype=torch.float64)
	q_value: tensor([[-26.4305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8586057025056959, distance: 1.5600924169994794 entropy 9.491368646978943
epoch: 21, step: 112
	action: tensor([[ 3671.0907, -3376.3419, -2651.9785, -2947.8206,  3433.9058, -1835.0621,
          4781.8245]], dtype=torch.float64)
	q_value: tensor([[-27.8355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3596938887485448, distance: 1.3343730623408419 entropy 9.71076927176804
epoch: 21, step: 113
	action: tensor([[-4526.2592,  3572.1707,  3799.0731, -1205.6324,  2516.5419,  3569.8732,
         -2780.4448]], dtype=torch.float64)
	q_value: tensor([[-27.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20254826289890293, distance: 1.0219011725640075 entropy 9.625879465072668
epoch: 21, step: 114
	action: tensor([[  -36.7105,   435.0420, -4169.3313,  1578.8430,  2990.3706,  6003.4111,
         -4011.8141]], dtype=torch.float64)
	q_value: tensor([[-44.5847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2641504529048514, distance: 0.981637649180396 entropy 9.82030564574794
epoch: 21, step: 115
	action: tensor([[-1145.3920, -3100.8897,  3004.4534,  1716.6812,  4040.0919, -4541.5360,
         -4651.8536]], dtype=torch.float64)
	q_value: tensor([[-29.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42794775573192156, distance: 1.3674543980782394 entropy 9.659392323440667
epoch: 21, step: 116
	action: tensor([[-2909.5840, -4812.7834, -5954.8525,  6945.8186,   213.1670,  4701.8376,
          -567.2559]], dtype=torch.float64)
	q_value: tensor([[-26.5577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09000958917872603, distance: 1.0916290922083702 entropy 9.55251330430671
epoch: 21, step: 117
	action: tensor([[ -571.3819, -2451.2006,  -740.7161,   794.4156,  2962.2101,  -464.8692,
         -2665.1970]], dtype=torch.float64)
	q_value: tensor([[-26.8412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34198998110171264, distance: 0.9282670745747726 entropy 9.59875325355989
epoch: 21, step: 118
	action: tensor([[-3925.7136, -8281.2219,  1517.7860, -5296.4119, -1678.0548, -6590.6529,
          4153.2861]], dtype=torch.float64)
	q_value: tensor([[-27.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.643326038990953, distance: 1.4669609785713964 entropy 9.723095780694267
epoch: 21, step: 119
	action: tensor([[ 1629.2605,  1570.8222, -2848.2330, -6787.7800, -2567.7902,  1098.8646,
          2098.9204]], dtype=torch.float64)
	q_value: tensor([[-23.0302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.253433005535332, distance: 0.9887604494468542 entropy 9.421181741643167
epoch: 21, step: 120
	action: tensor([[ -699.1161, -3413.6597, -4124.3949, -2772.9267,  -421.9768,  1501.3245,
         -2089.4356]], dtype=torch.float64)
	q_value: tensor([[-32.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.481225171519444
epoch: 21, step: 121
	action: tensor([[  950.6554, -2147.5606, -1154.9543,  2367.1245,   462.5349,  2430.5299,
         -3025.5817]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.099545669269745
epoch: 21, step: 122
	action: tensor([[-4975.1211, -3152.4726,  1973.6039,  1855.2432,   804.2380,   858.8147,
           720.0793]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.045402468499495585, distance: 1.1700339240616393 entropy 9.099545669269745
epoch: 21, step: 123
	action: tensor([[-4357.5587,  6105.4061,  1025.4445,   907.8482,  2898.9073, -3266.9143,
          -638.1459]], dtype=torch.float64)
	q_value: tensor([[-27.1341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20434269652739356, distance: 1.0207507790810222 entropy 9.606458683887894
epoch: 21, step: 124
	action: tensor([[ 1148.4121,  3536.9061, -8325.4199,   251.2257, -6247.6859, -5654.5339,
           979.2142]], dtype=torch.float64)
	q_value: tensor([[-36.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47817041004588023, distance: 0.8266488086255129 entropy 9.732264665348765
epoch: 21, step: 125
	action: tensor([[-3663.7695, -2320.4091,  1528.9992, -3173.3166, -1694.6248,  5437.8154,
          1140.7124]], dtype=torch.float64)
	q_value: tensor([[-29.7077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.551701882691995
epoch: 21, step: 126
	action: tensor([[-4085.0876,   237.1937, -2024.7060, -1102.4292, -2579.0408,  3745.0769,
          1898.8376]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.099545669269745
epoch: 21, step: 127
	action: tensor([[-1403.0472, -6389.6056,  4195.1957,  1154.4663,  2181.3138,   789.3901,
          1357.9456]], dtype=torch.float64)
	q_value: tensor([[-31.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.099545669269745
LOSS epoch 21 actor 366.46832545600796 critic 246.97711193006165
epoch: 22, step: 0
	action: tensor([[-1823.5477,  -934.5479, -3586.7107,  1844.9633,  3422.0077,  2551.2201,
           693.4647]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3986825645542489, distance: 1.3533691541091444 entropy 9.173131033128088
epoch: 22, step: 1
	action: tensor([[-6394.6329,  2505.9031,  2939.9693,   140.2042, -1090.8715,   968.0946,
         -1077.7298]], dtype=torch.float64)
	q_value: tensor([[-30.1159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5998448589877894, distance: 0.723887193911162 entropy 9.59756472546395
epoch: 22, step: 2
	action: tensor([[ -927.4356,  1457.7604,  5144.7389,  2158.5408, -2970.4772,  6214.0010,
           656.8372]], dtype=torch.float64)
	q_value: tensor([[-37.3110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5746771926692992, distance: 0.7463044464760455 entropy 9.748382876668703
epoch: 22, step: 3
	action: tensor([[-2.7742e+03, -5.4707e+03, -1.1676e+03, -2.9143e+02, -3.4129e+00,
         -4.5542e+02,  1.6497e+03]], dtype=torch.float64)
	q_value: tensor([[-37.3111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1327925075261778, distance: 1.6712098876651837 entropy 9.762988193378323
epoch: 22, step: 4
	action: tensor([[-7891.2267, -7638.6157,  4318.3393, -2936.8832, -3311.0283, -7346.3473,
         -4346.3598]], dtype=torch.float64)
	q_value: tensor([[-33.7932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013474911264173661, distance: 1.1366081360967526 entropy 9.70668729470039
epoch: 22, step: 5
	action: tensor([[-7503.8974, -2983.5025,  2480.4065,  -256.2827,  2881.5813,  3641.5234,
          1575.0138]], dtype=torch.float64)
	q_value: tensor([[-30.1806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1529003192895837, distance: 1.2287189973575574 entropy 9.69027387489841
epoch: 22, step: 6
	action: tensor([[-7965.2812, -9491.3254, -3845.6182,  -702.3037, -1832.6015, 11642.7228,
          4687.2679]], dtype=torch.float64)
	q_value: tensor([[-36.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.040451533777517534, distance: 1.1209600914309732 entropy 9.804869432996297
epoch: 22, step: 7
	action: tensor([[-2827.9416, -1367.6864,  4575.0727, -1646.9177,  7221.3276,  -894.1857,
         -6473.3003]], dtype=torch.float64)
	q_value: tensor([[-31.5244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7287516691925062, distance: 1.5046067280553281 entropy 9.652243281393805
epoch: 22, step: 8
	action: tensor([[-2712.0920, -2532.4283, -5203.5625,  6909.4062,  -867.7945,   761.4559,
         -3420.8900]], dtype=torch.float64)
	q_value: tensor([[-32.6867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2036305701009415, distance: 1.698736770518042 entropy 9.618200316161536
epoch: 22, step: 9
	action: tensor([[-7654.8296, -3439.3858, -2069.9169, -1363.7094,   428.7525, -4840.2948,
           396.7071]], dtype=torch.float64)
	q_value: tensor([[-27.7243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21390342235334558, distance: 1.260807434969573 entropy 9.648283678135604
epoch: 22, step: 10
	action: tensor([[-3462.9059, -2597.3116,  -573.1690,  4035.9946, -5421.5458,  4831.0557,
         -7902.4735]], dtype=torch.float64)
	q_value: tensor([[-36.1001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5730757730007134, distance: 1.4352630836788407 entropy 9.734228372383782
epoch: 22, step: 11
	action: tensor([[-2754.3839, -9475.1934, -2985.4513, -3343.7927, -1378.8423,  2397.3358,
          4282.5449]], dtype=torch.float64)
	q_value: tensor([[-28.4830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11712103629512305, distance: 1.0752446188996108 entropy 9.594997511767023
epoch: 22, step: 12
	action: tensor([[-3625.1815, -1723.3041, -1520.3913, -3427.8164,  5739.8988,  6861.9511,
         -1299.3872]], dtype=torch.float64)
	q_value: tensor([[-35.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0013926807195172586, distance: 1.143547123276637 entropy 9.829770602578076
epoch: 22, step: 13
	action: tensor([[ 1449.9636,  5739.9428,  5579.2341,  -878.3790,  2562.1881, -6544.2070,
          2658.9699]], dtype=torch.float64)
	q_value: tensor([[-33.6886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5401070596908839, distance: 0.7760417113497806 entropy 9.788008771657877
epoch: 22, step: 14
	action: tensor([[-1967.7621,   852.5711,  4215.8397,  3687.7753,   623.0636,  -758.3654,
         -4435.7989]], dtype=torch.float64)
	q_value: tensor([[-35.5383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.542331408698399
epoch: 22, step: 15
	action: tensor([[ -175.6613,  -824.3384,  -364.8732,  -964.3132, -2885.6080,   913.2834,
          5807.5297]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18954876167722534, distance: 1.2480955015416058 entropy 9.173131033128088
epoch: 22, step: 16
	action: tensor([[-5169.2296, -3271.0507,   205.4960,  1056.9928, -3475.9430,   279.1580,
         -4763.8638]], dtype=torch.float64)
	q_value: tensor([[-28.5471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4257883912609761, distance: 1.3664200641519917 entropy 9.553118553547094
epoch: 22, step: 17
	action: tensor([[ -787.6693, -6104.3741,  -149.4013,  1077.5412,  2294.6024,  1596.0488,
          1434.3444]], dtype=torch.float64)
	q_value: tensor([[-28.0028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20587269205387626, distance: 1.256629999579664 entropy 9.569062558884601
epoch: 22, step: 18
	action: tensor([[-8375.4549, -8108.0983,  3109.6099,  6419.0311, -2470.5863,    88.8429,
         -6455.5149]], dtype=torch.float64)
	q_value: tensor([[-34.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2146584118458036, distance: 1.0141121627134493 entropy 9.788960566454076
epoch: 22, step: 19
	action: tensor([[-7075.5575, -9826.6097,   756.4427,  1733.9174, -8369.0902,  -617.9351,
          4207.6253]], dtype=torch.float64)
	q_value: tensor([[-35.2171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2640066998028745, distance: 1.2865639591302602 entropy 9.869164826762255
epoch: 22, step: 20
	action: tensor([[-1819.4186, -9593.4580,  3318.5254,  4323.0183, -6534.8250, -1447.4625,
          4911.8590]], dtype=torch.float64)
	q_value: tensor([[-34.6752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3243205679209782, distance: 1.316901387725946 entropy 9.771325108070254
epoch: 22, step: 21
	action: tensor([[ 1442.7481,  1495.9935, -9661.0686,  4798.2185, -3665.0569,  1700.0124,
           -35.7921]], dtype=torch.float64)
	q_value: tensor([[-30.6513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6303741667402059, distance: 0.6957253878322804 entropy 9.633329641165165
epoch: 22, step: 22
	action: tensor([[ -409.9368,   610.1838, -1116.8256, -1302.6739,  4006.7932, -3525.3581,
         -1006.7205]], dtype=torch.float64)
	q_value: tensor([[-35.5829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.807131747202732
epoch: 22, step: 23
	action: tensor([[ -761.5731, -4082.3377,    64.3935,  3356.5839, -3788.1133,   614.4274,
          2345.8688]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2087801042881865, distance: 1.0179004185317906 entropy 9.173131033128088
epoch: 22, step: 24
	action: tensor([[ -190.8122, -1019.3301,  -424.9755,  3179.3728,  2901.8383,  6824.8332,
         -3909.0514]], dtype=torch.float64)
	q_value: tensor([[-29.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24550601273800465, distance: 1.2771138286892785 entropy 9.592832598912182
epoch: 22, step: 25
	action: tensor([[-3125.5614, -2772.8130,   465.9882,  4269.2942,  1798.6382,  1598.1252,
           898.6525]], dtype=torch.float64)
	q_value: tensor([[-29.2996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13798545846059085, distance: 1.0624634269775504 entropy 9.610168288120263
epoch: 22, step: 26
	action: tensor([[   138.4047,  -1521.0493,  -5320.2863,  -5361.9620, -10386.9438,
         -11386.6395,    290.2206]], dtype=torch.float64)
	q_value: tensor([[-29.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3391587842019723, distance: 1.3242583818664537 entropy 9.631633612150614
epoch: 22, step: 27
	action: tensor([[ -449.2825, -1402.8448,  1190.1110, -4364.8848,  2270.8221, -2131.8035,
          -267.3376]], dtype=torch.float64)
	q_value: tensor([[-26.0021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.917394709128204, distance: 1.5845737443438517 entropy 9.391624863939295
epoch: 22, step: 28
	action: tensor([[1102.1125,  890.3565, -819.6959, -877.4692, 4963.5293, 2881.9270,
         4878.1211]], dtype=torch.float64)
	q_value: tensor([[-35.4080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4893839566064404, distance: 0.8177186845219317 entropy 9.943749151925513
epoch: 22, step: 29
	action: tensor([[ -521.6818,  -780.7043,  -292.8036, -4201.3697, -2129.7056, -2775.9351,
         -7357.7363]], dtype=torch.float64)
	q_value: tensor([[-39.7360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1732199910246799, distance: 1.0405229818167296 entropy 9.727150199426708
epoch: 22, step: 30
	action: tensor([[-3580.4211, -2140.6287, -5310.9313,  1224.4886, -1710.8837, -3183.6980,
          5475.2670]], dtype=torch.float64)
	q_value: tensor([[-30.2626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8865224753877279, distance: 1.5717652583310302 entropy 9.646190839510703
epoch: 22, step: 31
	action: tensor([[-1373.0748, -2622.6533, -2687.0485,   -29.4187,   179.9597, -2643.3730,
          1971.5951]], dtype=torch.float64)
	q_value: tensor([[-32.4380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8952109173159062, distance: 1.575380509127897 entropy 9.746958351630893
epoch: 22, step: 32
	action: tensor([[-6890.8730, -6430.8908,  3977.3237,  6286.8371,  1037.5119, -2348.6721,
          7092.1105]], dtype=torch.float64)
	q_value: tensor([[-37.2912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.906163534721331, distance: 1.5799260936494477 entropy 9.821353491629603
epoch: 22, step: 33
	action: tensor([[ -916.8339, -2025.5726,  7982.8518,  1346.1878,  4012.1556,  4438.1870,
          3194.3446]], dtype=torch.float64)
	q_value: tensor([[-28.6086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2967200636087952, distance: 1.3031061923296339 entropy 9.53045754752697
epoch: 22, step: 34
	action: tensor([[-2122.8038, -9743.4367, -2089.4172, -8409.2665,  4203.8992, -1118.4153,
           -93.6152]], dtype=torch.float64)
	q_value: tensor([[-37.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0134977460702341, distance: 1.6237989908447636 entropy 9.892409903540216
epoch: 22, step: 35
	action: tensor([[ -7904.5122, -10591.4176,  -2069.2696,  -1927.7677,  -3079.6148,
          -2567.6234,   3727.1580]], dtype=torch.float64)
	q_value: tensor([[-25.3010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21640444803609138, distance: 1.2621055980620832 entropy 9.545898657072007
epoch: 22, step: 36
	action: tensor([[-5058.2080, -6555.5470, -1340.8365,  -935.0045,  -376.9674,  1048.8654,
         -1155.4262]], dtype=torch.float64)
	q_value: tensor([[-36.6117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7059690039695248, distance: 1.494659481091941 entropy 9.748683144483158
epoch: 22, step: 37
	action: tensor([[ 2065.6700,  4961.4429,  -606.2579, -2215.9473, -1769.6992,  3626.7767,
         -2971.2525]], dtype=torch.float64)
	q_value: tensor([[-33.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41466821934735487, distance: 0.875503215913205 entropy 9.665909629990308
epoch: 22, step: 38
	action: tensor([[  837.8429,  -409.7282,   282.2463, -2724.2128, -2274.2190, -1165.9367,
         -4282.2308]], dtype=torch.float64)
	q_value: tensor([[-36.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24804488282562387, distance: 0.9923220753179605 entropy 9.693774909387662
epoch: 22, step: 39
	action: tensor([[-9323.9164,  2050.9254,  4261.3209,  3192.1793,  5101.7800, -1412.0948,
           155.9225]], dtype=torch.float64)
	q_value: tensor([[-32.9001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.68633924245926
epoch: 22, step: 40
	action: tensor([[ -123.4681, -1415.4014, -1545.9430,  -379.8351,  1184.6376,  -377.2577,
          2627.1927]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2956612173667661, distance: 1.3025740532358856 entropy 9.173131033128088
epoch: 22, step: 41
	action: tensor([[-2450.4613, -5390.7518,   352.0038,  1117.5950,  9395.9873,  5974.4796,
         -1582.5506]], dtype=torch.float64)
	q_value: tensor([[-27.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5777870062683752, distance: 1.4374107246466288 entropy 9.583720530969881
epoch: 22, step: 42
	action: tensor([[-7383.5011, -4237.0483,  9410.3240,  2282.9186,  5071.2670,  -879.0818,
          2351.9057]], dtype=torch.float64)
	q_value: tensor([[-27.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6883362502595889, distance: 1.4869150797120168 entropy 9.576140209339957
epoch: 22, step: 43
	action: tensor([[-6503.8813, -2260.3973,  -485.7378,  4352.2082, -3313.7158,  -359.1625,
          6049.1630]], dtype=torch.float64)
	q_value: tensor([[-29.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10174075940871408, distance: 1.084569881874874 entropy 9.645110582841967
epoch: 22, step: 44
	action: tensor([[  1317.2949,  -5663.1070,   5357.6740,   2410.7042, -11591.6880,
          -1809.2839, -10296.3541]], dtype=torch.float64)
	q_value: tensor([[-36.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22791943083189503, distance: 1.2680653312669035 entropy 9.951043910946712
epoch: 22, step: 45
	action: tensor([[ 1130.5032, -2627.4600,  -934.3061,  1558.2206,  3406.1567, -5090.7171,
          3603.4818]], dtype=torch.float64)
	q_value: tensor([[-32.8714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3826815471200442, distance: 1.3456055717011648 entropy 9.629496252450576
epoch: 22, step: 46
	action: tensor([[-1627.9547,  -655.9480, -3918.2259,  8641.5059,  -718.8564,  -250.9793,
          -788.9404]], dtype=torch.float64)
	q_value: tensor([[-41.8428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26940915369372687, distance: 0.9781237552566239 entropy 9.791183192579165
epoch: 22, step: 47
	action: tensor([[-5661.2370, -9794.5504,   665.5127,  2938.0064, -2398.6941,  2076.5912,
          1346.3642]], dtype=torch.float64)
	q_value: tensor([[-36.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046902554580125266, distance: 1.11718564325202 entropy 9.948765023596106
epoch: 22, step: 48
	action: tensor([[-9000.2730, -5960.2397, -7154.1197,  2281.2316, -1383.4611, -3885.5845,
          -866.3335]], dtype=torch.float64)
	q_value: tensor([[-32.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006089116026032904, distance: 1.1408549116685756 entropy 9.676044305470507
epoch: 22, step: 49
	action: tensor([[-3149.1177, -2090.0223, -2943.5043,  6057.5139,  1805.6853,  2065.7265,
          2166.1681]], dtype=torch.float64)
	q_value: tensor([[-27.6641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1623289407204569, distance: 1.233733106617918 entropy 9.611458398421856
epoch: 22, step: 50
	action: tensor([[ -208.1997, -3553.0760,  3177.7237,  3168.0439, -2170.0673,  3747.6195,
         -1706.6815]], dtype=torch.float64)
	q_value: tensor([[-32.7358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5757064727813559, distance: 1.4364626981826414 entropy 9.78958690964144
epoch: 22, step: 51
	action: tensor([[ -5709.3016, -10204.4290,   1040.4869,   8466.6385,  -4054.7680,
           2702.6306,   1844.0045]], dtype=torch.float64)
	q_value: tensor([[-36.4939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16274955688660953, distance: 1.2339563141511216 entropy 9.920034460135685
epoch: 22, step: 52
	action: tensor([[-9306.5994, -5564.4919,  3614.4928,  8350.0575, -6491.8983, -2087.6567,
          7621.6530]], dtype=torch.float64)
	q_value: tensor([[-38.3524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014607414393725371, distance: 1.1526719082435946 entropy 9.968093631739993
epoch: 22, step: 53
	action: tensor([[-3545.1840,   159.9075,  2825.0856,  3144.8029,  4001.2498,  3423.9291,
          -609.1439]], dtype=torch.float64)
	q_value: tensor([[-30.0998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.057600513029568035, distance: 1.176840266448684 entropy 9.707584595750543
epoch: 22, step: 54
	action: tensor([[-5064.6132, -1931.3947, -3653.6116, 11128.1878, -1398.5541,  3269.3500,
          7692.0813]], dtype=torch.float64)
	q_value: tensor([[-33.9221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4475815255271338, distance: 1.3768233071450038 entropy 9.725044872611381
epoch: 22, step: 55
	action: tensor([[-9862.1451,  1315.2481,  4254.1749, -5787.7363,  6224.6391, 13436.0243,
          4171.9459]], dtype=torch.float64)
	q_value: tensor([[-38.2550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009129912044326, distance: 0.9568025169691226 entropy 9.900524607185028
epoch: 22, step: 56
	action: tensor([[-2715.4006,   363.3617, -2900.6403, -2877.4496,  -840.9440, -3801.1447,
          5549.0643]], dtype=torch.float64)
	q_value: tensor([[-32.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40283604759292346, distance: 0.8843078599996663 entropy 9.490852705915843
epoch: 22, step: 57
	action: tensor([[-8486.1269,   674.9442,  1912.1537, -2985.9370,  8331.7191, -3474.5176,
          4973.2110]], dtype=torch.float64)
	q_value: tensor([[-35.7909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2633690126817817, distance: 0.982158739123497 entropy 9.62390955133811
epoch: 22, step: 58
	action: tensor([[-6529.5497, -3172.9228, -1303.1078, -3175.2214,   628.0498,  -771.8213,
         -3622.6061]], dtype=torch.float64)
	q_value: tensor([[-39.8015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05995552698143625, distance: 1.109509163476379 entropy 9.803494022076505
epoch: 22, step: 59
	action: tensor([[ -1837.5085, -11051.8144,   1500.2651,   2768.7162,  -8986.5387,
           3665.1016,  -2187.0764]], dtype=torch.float64)
	q_value: tensor([[-40.8404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7569908009742972, distance: 1.5168458130175047 entropy 9.895151692011265
epoch: 22, step: 60
	action: tensor([[-5.5642e+03,  2.4170e+00, -2.9607e+03,  3.9105e+03, -3.8295e+03,
         -1.5832e+03, -1.2560e+03]], dtype=torch.float64)
	q_value: tensor([[-26.2043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.547543984122298
epoch: 22, step: 61
	action: tensor([[-2970.5342,  -280.9139,   638.1600,  3146.9933,  -579.6430,  3878.7642,
         -1031.9622]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19750918407517637, distance: 1.2522646458840028 entropy 9.173131033128088
epoch: 22, step: 62
	action: tensor([[-7682.9085,   652.4046, -2449.9666,  2293.9427,   120.5496,  -677.9183,
           795.0297]], dtype=torch.float64)
	q_value: tensor([[-33.3149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5837075065670587, distance: 0.7383392960659914 entropy 9.828468030109322
epoch: 22, step: 63
	action: tensor([[-4397.7603,  7890.6707, -2185.4158, -7326.4075, -3015.3303,   542.8917,
          1041.8538]], dtype=torch.float64)
	q_value: tensor([[-35.1124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.655967931973604
epoch: 22, step: 64
	action: tensor([[-1335.3026, -2918.9004,  1325.4061, -4528.2925,  1054.1288,  1426.0524,
          2508.7801]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4859895187916967, distance: 1.394969031663254 entropy 9.173131033128088
epoch: 22, step: 65
	action: tensor([[  -80.1691,  1986.9700, -2694.9468,  7929.8178, -1396.2728, -2536.2764,
         -2004.6637]], dtype=torch.float64)
	q_value: tensor([[-28.7998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7459785608097468, distance: 0.5767556788898026 entropy 9.586221349388069
epoch: 22, step: 66
	action: tensor([[-1852.1648,  5151.6294,  -965.6324,   284.1176, -2537.9423,  1645.3529,
         -1969.6386]], dtype=torch.float64)
	q_value: tensor([[-30.9103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10344703980256698, distance: 1.0835392996717772 entropy 9.511087418586357
epoch: 22, step: 67
	action: tensor([[-1209.5967,  -147.4516,  3315.6879,  2661.0232, -1649.3015,   815.4596,
           691.8137]], dtype=torch.float64)
	q_value: tensor([[-31.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8076924173556601, distance: 1.5385760274948581 entropy 9.595189114355389
epoch: 22, step: 68
	action: tensor([[-10264.1057,   -611.1787,  -6846.8821,   2454.6863,  -4337.6125,
           1635.2151,  -3779.1023]], dtype=torch.float64)
	q_value: tensor([[-37.9989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1113023469388823, distance: 1.2063485569358239 entropy 9.790127058948839
epoch: 22, step: 69
	action: tensor([[-5074.4522, -6652.5309, -2867.4094,  1097.1317, -3002.5087,  2835.6588,
         11235.8891]], dtype=torch.float64)
	q_value: tensor([[-36.4157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008138306677187845, distance: 1.1489913305668047 entropy 9.936173338657296
epoch: 22, step: 70
	action: tensor([[ -496.2156, -6341.5105, -1946.3619,  2171.7936,  1432.2282,  4271.0581,
           804.8503]], dtype=torch.float64)
	q_value: tensor([[-33.1340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08420473686593577, distance: 1.0951053223550058 entropy 9.753944219374821
epoch: 22, step: 71
	action: tensor([[ 2395.4304, -5340.9406, -3621.1947,    59.5633, -6601.2261,  3174.0702,
         -4970.9611]], dtype=torch.float64)
	q_value: tensor([[-30.9777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35646802199751815, distance: 0.9179980517514418 entropy 9.674239946969427
epoch: 22, step: 72
	action: tensor([[-6561.1914,  -503.3756,  1762.3458, -1633.0848, -4194.1738,  3885.1042,
          2483.1474]], dtype=torch.float64)
	q_value: tensor([[-31.8741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48605164074459606, distance: 1.3949981897743022 entropy 9.708867515483206
epoch: 22, step: 73
	action: tensor([[-2102.3659,  2713.7867, -2954.3571,  1231.4999, -3015.5302,  2439.2966,
          -226.2865]], dtype=torch.float64)
	q_value: tensor([[-33.1113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3944972881433644, distance: 0.8904606644779797 entropy 9.767483204864648
epoch: 22, step: 74
	action: tensor([[-6278.9726, -5698.4052,  1135.0183,  2199.4927, -3403.7878,  5020.3003,
         -1858.0739]], dtype=torch.float64)
	q_value: tensor([[-33.6168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6340726616721439, distance: 1.4628250049421507 entropy 9.557668244120961
epoch: 22, step: 75
	action: tensor([[  756.8772, -2582.0428, 12315.5140,  4559.0469, -3285.9945,  6441.4333,
         -2030.1727]], dtype=torch.float64)
	q_value: tensor([[-34.8716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4443932343872581, distance: 0.8529831576687644 entropy 9.784692503053801
epoch: 22, step: 76
	action: tensor([[  -67.3677, -2194.2680,  2139.3046,  3447.3870,  1443.3683,  6120.2440,
          2029.7975]], dtype=torch.float64)
	q_value: tensor([[-26.0852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10528512944712465, distance: 1.2030781984045855 entropy 9.438862378696095
epoch: 22, step: 77
	action: tensor([[-2135.4086, -7308.2509,  5796.0810,  6071.6679, -3478.9862, -3669.8726,
          2149.6504]], dtype=torch.float64)
	q_value: tensor([[-29.4039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1511350606794999, distance: 1.054328609576402 entropy 9.710021657213693
epoch: 22, step: 78
	action: tensor([[ -118.1323,  1057.9874, -5358.9573,  6656.3780, -8503.6787,  5256.8515,
         -8993.8156]], dtype=torch.float64)
	q_value: tensor([[-35.6044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.77132819257755
epoch: 22, step: 79
	action: tensor([[-1079.1167,  -306.8533, -2130.9556,  2536.8434, -2011.0182,  5498.0987,
         -1059.1929]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1408950888502254, distance: 1.0606688000987403 entropy 9.173131033128088
epoch: 22, step: 80
	action: tensor([[-4455.8376, -6017.8483,   338.4601, -3346.8035,  -804.4743,  2171.1072,
         -2770.6609]], dtype=torch.float64)
	q_value: tensor([[-29.8786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8915201588738542, distance: 1.5738458033189717 entropy 9.636886112997704
epoch: 22, step: 81
	action: tensor([[-2907.2286,   294.9838, -1521.4314, -2260.9617,  6488.7100, -3160.1820,
         -2037.5143]], dtype=torch.float64)
	q_value: tensor([[-24.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.37162873710576
epoch: 22, step: 82
	action: tensor([[-1.4952e+03,  1.9699e+03,  1.7702e+02, -2.3504e+00,  1.0229e+03,
          4.7906e+03, -9.3928e+01]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.173131033128088
epoch: 22, step: 83
	action: tensor([[  111.2027,  -760.0132,   230.6507,  3070.8653,  1680.6496,  1621.6159,
         -2981.3793]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44875131657975376, distance: 0.8496312465044131 entropy 9.173131033128088
epoch: 22, step: 84
	action: tensor([[  672.6263, -7490.8098, -5452.3612,  5353.0423,  1395.3488, -1573.2596,
          -186.1894]], dtype=torch.float64)
	q_value: tensor([[-32.2321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3961508143290764, distance: 1.3521437350334928 entropy 9.617379387586658
epoch: 22, step: 85
	action: tensor([[ -195.5433, -8827.8281, -3388.1935,   861.5800,  1777.7215,   478.6126,
          7420.7727]], dtype=torch.float64)
	q_value: tensor([[-40.9553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13638251784609245, distance: 1.0634508086903363 entropy 9.75941212787304
epoch: 22, step: 86
	action: tensor([[ 3834.5017, -2719.1872,  1332.8224,  6354.4995, -3502.6606,  3280.5111,
          2333.1852]], dtype=torch.float64)
	q_value: tensor([[-24.9659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2920629420771167, distance: 0.9628397614302454 entropy 9.463902456531674
epoch: 22, step: 87
	action: tensor([[-4793.2925,   488.3643,  5930.3902,   532.0371,  1572.0171, -1713.3310,
          -115.5436]], dtype=torch.float64)
	q_value: tensor([[-32.1350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18895753393226, distance: 1.0305723599746592 entropy 9.723640353419851
epoch: 22, step: 88
	action: tensor([[-1815.7582, -5504.8156, -3811.4450,  4732.5063,  2319.2297,  -887.6385,
            97.1944]], dtype=torch.float64)
	q_value: tensor([[-36.6115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23214442691425763, distance: 1.2702450224921482 entropy 9.656443432366027
epoch: 22, step: 89
	action: tensor([[-7988.1281, -6700.3058,  6883.5508, 11560.0751, -8295.3961, -2170.3890,
          6183.2088]], dtype=torch.float64)
	q_value: tensor([[-36.1561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38877200453900573, distance: 1.3485658876612774 entropy 9.866414023595079
epoch: 22, step: 90
	action: tensor([[  -542.9195, -10172.9999,  -4469.6296,   3149.4379,   9110.0360,
           8655.6543, -17239.7770]], dtype=torch.float64)
	q_value: tensor([[-34.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27866902283401573, distance: 0.9719053769969711 entropy 9.852528052162231
epoch: 22, step: 91
	action: tensor([[-6471.1029,   862.2342,  8242.7299,    22.3807,   168.8364, -3423.2833,
         -5907.7360]], dtype=torch.float64)
	q_value: tensor([[-31.7421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31503840135940175, distance: 0.9470868471771892 entropy 9.736785465020306
epoch: 22, step: 92
	action: tensor([[-6371.8118, -6014.4932, -3820.7734, -1653.2850,  1859.5719,  6706.8856,
          1045.9642]], dtype=torch.float64)
	q_value: tensor([[-31.5480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8916442948397918, distance: 1.5738974463487299 entropy 9.514704419717292
epoch: 22, step: 93
	action: tensor([[ 2058.7708,   110.3613,  -934.3073, -3084.1665,   -64.7795,  1681.0301,
         -2201.8949]], dtype=torch.float64)
	q_value: tensor([[-29.4559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6923876157459874, distance: 0.6346855360243419 entropy 9.6192299821375
epoch: 22, step: 94
	action: tensor([[-3019.2880, -2620.0270,  2243.8261,  2797.1255, -7693.9568, -3435.4009,
         -3445.1089]], dtype=torch.float64)
	q_value: tensor([[-32.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40263773311609596, distance: 1.3552813193380822 entropy 9.614803996865046
epoch: 22, step: 95
	action: tensor([[  404.1926, -2279.5843, -4707.0016, -2074.5993,  1932.4002, -6756.6014,
         -4990.5938]], dtype=torch.float64)
	q_value: tensor([[-32.7487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0771932979033716, distance: 1.1876911342097032 entropy 9.697770114458235
epoch: 22, step: 96
	action: tensor([[-3889.0033,  2993.9780,  2131.4372,  1325.6089,  1168.8034,  3675.7482,
          4608.7471]], dtype=torch.float64)
	q_value: tensor([[-32.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.688726026283677
epoch: 22, step: 97
	action: tensor([[ -775.6727,  2437.5001,  1208.0619, -1223.9382, -2430.6020,    28.9344,
          -981.6484]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.173131033128088
epoch: 22, step: 98
	action: tensor([[-1829.6196, -6438.5328,  1111.5945,  2862.6326,  1400.3805,  -337.0351,
           284.4557]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7241076912020128, distance: 1.502584442397741 entropy 9.173131033128088
epoch: 22, step: 99
	action: tensor([[-1135.1610, -5390.5201,  5086.8316,  3642.8483,  3080.0472,  9288.2809,
         -1527.8508]], dtype=torch.float64)
	q_value: tensor([[-27.1897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4010488511967283, distance: 1.3545134831012187 entropy 9.563333212955344
epoch: 22, step: 100
	action: tensor([[-3510.0076, -6567.9745,  3268.6559, -1136.0740, -5515.9324, -6587.2653,
          6943.9309]], dtype=torch.float64)
	q_value: tensor([[-33.3300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8946948025518555, distance: 1.5751659861702325 entropy 9.756880783238028
epoch: 22, step: 101
	action: tensor([[ -378.8462, -4220.7777, -1204.1447,  -238.7150,   906.9922, -3980.2462,
          2943.4976]], dtype=torch.float64)
	q_value: tensor([[-26.7009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7457832290488637, distance: 1.512000211180461 entropy 9.517672230385063
epoch: 22, step: 102
	action: tensor([[ -1582.0989, -12644.9306,  -3656.0944,   4605.0432,   2995.8257,
          -3950.2706,   2777.6357]], dtype=torch.float64)
	q_value: tensor([[-33.8203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3182206603741571, distance: 1.313865020105572 entropy 9.89330343747334
epoch: 22, step: 103
	action: tensor([[-1593.9862,   298.6380, -1052.6000,  3662.1361, -1401.1074,  3310.6744,
           237.5539]], dtype=torch.float64)
	q_value: tensor([[-25.8953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18029454279441126, distance: 1.036061669000671 entropy 9.535137576248514
epoch: 22, step: 104
	action: tensor([[-3366.9865, -2534.1535, -3531.7116,  4651.9761,  3806.1467, -1952.0431,
           533.1957]], dtype=torch.float64)
	q_value: tensor([[-28.9442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32967927846861667, distance: 1.3195630427691263 entropy 9.483625036709457
epoch: 22, step: 105
	action: tensor([[-5316.9196,  -678.0954,  -109.5654,  2588.6508,  -935.4218,  6371.5489,
           651.2560]], dtype=torch.float64)
	q_value: tensor([[-27.8920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20718443791220942, distance: 1.2573132935857527 entropy 9.614647970297924
epoch: 22, step: 106
	action: tensor([[-10673.0981,  -7075.6762,   2626.4999,  -7636.9332,  -1418.3499,
           2865.3516,   1527.2583]], dtype=torch.float64)
	q_value: tensor([[-33.0587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44206285245124377, distance: 1.3741963416909506 entropy 9.811765030680723
epoch: 22, step: 107
	action: tensor([[-1931.0944, -2366.2322, -6465.7165,  2692.5457,  1895.8395,   -61.4457,
         -1031.4022]], dtype=torch.float64)
	q_value: tensor([[-26.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8805209829991725, distance: 1.5692631807151225 entropy 9.46584467561925
epoch: 22, step: 108
	action: tensor([[  626.9483, -9012.0589, -5161.4051,  4368.8914,   804.4504, -4850.6908,
          -879.7842]], dtype=torch.float64)
	q_value: tensor([[-25.5810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29103870266766485, distance: 1.300248388301262 entropy 9.60397706934822
epoch: 22, step: 109
	action: tensor([[-5081.3997,   978.8958,  4191.4463,  1177.6818, -1122.9365,  3441.4757,
         -3866.6985]], dtype=torch.float64)
	q_value: tensor([[-28.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6719352435116673, distance: 0.655445342699332 entropy 9.50402810518649
epoch: 22, step: 110
	action: tensor([[-3850.5355, -9390.2328, -1979.4652, -5148.2433,  1627.6664, -2071.6323,
          2523.4158]], dtype=torch.float64)
	q_value: tensor([[-34.4548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26354933467345676, distance: 1.2863311744739874 entropy 9.682438288102372
epoch: 22, step: 111
	action: tensor([[-1283.6702, -3795.8086, -5803.4535,  1824.9775, -6106.8087,  2159.9812,
         -2529.4490]], dtype=torch.float64)
	q_value: tensor([[-31.2138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46071703696150945, distance: 0.8403593634580765 entropy 9.578935772560072
epoch: 22, step: 112
	action: tensor([[ 2300.7780, -2136.2508,  5539.9998,  4228.3866, -1371.1159, -2439.8499,
          1387.3696]], dtype=torch.float64)
	q_value: tensor([[-27.3492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6017484726208105, distance: 0.7221633071570955 entropy 9.631413224418864
epoch: 22, step: 113
	action: tensor([[-3156.3975, -4574.8047,  -789.6784,  6598.2462, -5375.0201,  4919.1376,
          4307.1353]], dtype=torch.float64)
	q_value: tensor([[-30.6859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.610790368606635
epoch: 22, step: 114
	action: tensor([[-4174.8503,   257.9857, -1768.4184,  4296.7945, -1075.9714, -1332.3948,
          2286.2600]], dtype=torch.float64)
	q_value: tensor([[-34.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058466261939259745, distance: 1.177321847037819 entropy 9.173131033128088
epoch: 22, step: 115
	action: tensor([[-4808.8532, -1963.1515, -2734.4705,  6164.6060,  4256.3387, -4840.9470,
          3066.6837]], dtype=torch.float64)
	q_value: tensor([[-38.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16679605587923874, distance: 1.044557500317409 entropy 9.757437907271981
epoch: 22, step: 116
	action: tensor([[ -2049.5149, -10149.1186,  -1251.4108,    -18.7084,  -1372.5624,
          -1957.4488,   -667.1793]], dtype=torch.float64)
	q_value: tensor([[-32.7707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43440574284314737, distance: 1.3705431107640367 entropy 9.742510302970103
epoch: 22, step: 117
	action: tensor([[ -139.9085, -5973.1285,   274.0325, -1002.8942, -1839.3982,  7436.2545,
         -1180.3350]], dtype=torch.float64)
	q_value: tensor([[-32.3124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5948642741325423, distance: 1.4451687373979298 entropy 9.761365668052465
epoch: 22, step: 118
	action: tensor([[ 2777.1515, -4852.6701, -7301.3485,   333.6039, -2402.1496,  4005.1812,
          3886.2601]], dtype=torch.float64)
	q_value: tensor([[-32.1873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2779836594678915, distance: 1.2936575983890952 entropy 9.660833014893258
epoch: 22, step: 119
	action: tensor([[-4688.9130, -7383.8845,  2780.2790,  1990.3656,   835.8068,  4072.6856,
          1935.7434]], dtype=torch.float64)
	q_value: tensor([[-29.4086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0670194698812564, distance: 1.1053326117911617 entropy 9.75404214783971
epoch: 22, step: 120
	action: tensor([[-2320.4501, -5547.7980,  -538.6880, -4558.8871,  3413.5938, -1136.0394,
          6876.6116]], dtype=torch.float64)
	q_value: tensor([[-33.0961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7873509020202623, distance: 1.5298949292348514 entropy 9.758366883013931
epoch: 22, step: 121
	action: tensor([[ -4330.8777,    672.1160, -12158.2132,   2470.3026,  -4961.4027,
           -441.8250,  -1391.2824]], dtype=torch.float64)
	q_value: tensor([[-30.7081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10133532993765482, distance: 1.084814614669328 entropy 9.722038709053166
epoch: 22, step: 122
	action: tensor([[  535.8419, -3971.7724,  2341.9909,  1282.6635, -1243.6765, -1096.6489,
          5637.7343]], dtype=torch.float64)
	q_value: tensor([[-39.2050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14730434589321062, distance: 1.225733378137031 entropy 9.730790403307454
epoch: 22, step: 123
	action: tensor([[-3956.6575,  -630.2562, -3497.3206,  3426.1905,  5225.7535,  3259.6032,
          7146.6031]], dtype=torch.float64)
	q_value: tensor([[-31.2680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0894146275579063, distance: 1.0919858933802034 entropy 9.582958199857197
epoch: 22, step: 124
	action: tensor([[ 2137.5459, -6449.0131, -5754.0138,  3066.3310, -4930.4401, 18157.6234,
         -4666.7058]], dtype=torch.float64)
	q_value: tensor([[-37.9888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05016304796773108, distance: 1.1726949626546634 entropy 9.953630349996175
epoch: 22, step: 125
	action: tensor([[-5097.0614, -5180.5968, -2614.3294, -5353.5602,  1889.0075, -2822.2417,
         -2517.8271]], dtype=torch.float64)
	q_value: tensor([[-34.6902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7810457757478004, distance: 1.5271940877836667 entropy 9.636252180626943
epoch: 22, step: 126
	action: tensor([[-3085.1253, -9792.8761,  1724.8107,  -364.3893,  1261.7889,   504.2597,
         -2603.7146]], dtype=torch.float64)
	q_value: tensor([[-29.2008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.138804055933895, distance: 1.0619588319770996 entropy 9.652651508913356
epoch: 22, step: 127
	action: tensor([[ 2514.0916, -5070.1122, -1146.1396,  -563.7206,  1244.4845,  2715.0602,
         -4510.0756]], dtype=torch.float64)
	q_value: tensor([[-33.8044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4889594985263842, distance: 0.81805848504993 entropy 9.805525522986631
LOSS epoch 22 actor 456.1009869742349 critic 131.68381076394817
epoch: 23, step: 0
	action: tensor([[-6165.1546, -5395.1390, -5299.6387,  3346.1798,   225.4540, -1740.2656,
          3684.4962]], dtype=torch.float64)
	q_value: tensor([[-29.4455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9261558854992451, distance: 1.5881898249231745 entropy 9.711808020595097
epoch: 23, step: 1
	action: tensor([[-8286.2874, -2676.6797,  9032.1661,  1302.2606, -4511.9238, -3086.3149,
         10075.6653]], dtype=torch.float64)
	q_value: tensor([[-33.4054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5419634397784578, distance: 1.420998866792249 entropy 9.882916779164294
epoch: 23, step: 2
	action: tensor([[-11096.2251,   3491.4010,  -1045.2624,   1519.6592,  -9362.4566,
           5600.7880,  -3311.6114]], dtype=torch.float64)
	q_value: tensor([[-35.3195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16308115301386994, distance: 1.0468835298546695 entropy 9.956668720399819
epoch: 23, step: 3
	action: tensor([[-8284.8166, -4380.0173, -3765.6046, -5557.6536, -3176.5679, -4904.3524,
         -2864.7417]], dtype=torch.float64)
	q_value: tensor([[-34.4763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1508119663433716, distance: 1.6782548669089241 entropy 9.742742815764709
epoch: 23, step: 4
	action: tensor([[-10481.9285,   1778.3412,  -1206.1021,   7023.3906,   -408.8710,
           5644.6090,   -887.8604]], dtype=torch.float64)
	q_value: tensor([[-34.3052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15807886369457214, distance: 1.0500074960957793 entropy 9.855923279535043
epoch: 23, step: 5
	action: tensor([[-2905.5651,   643.6861,  6892.4370, -3107.5440, -1447.9605,  1427.6188,
         -2516.5783]], dtype=torch.float64)
	q_value: tensor([[-30.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13518241718553226, distance: 1.219240895179501 entropy 9.57957654842526
epoch: 23, step: 6
	action: tensor([[-9992.5719, -8288.6532, -2228.5408,  2030.5458, -2747.0919,  7262.7426,
          6596.4046]], dtype=torch.float64)
	q_value: tensor([[-40.6221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26369743887212205, distance: 1.2864065595263663 entropy 9.807856627339786
epoch: 23, step: 7
	action: tensor([[-9450.1514, -2544.2622,  2153.7238,  4494.8066,  -900.4928,  1947.3051,
          1906.9865]], dtype=torch.float64)
	q_value: tensor([[-30.3670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.660761950919059, distance: 1.4747227718461606 entropy 9.752544114255883
epoch: 23, step: 8
	action: tensor([[-2258.5209,  4359.1429,   568.6115,  4141.5363, -5910.8923,   872.5575,
          2214.1755]], dtype=torch.float64)
	q_value: tensor([[-32.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28128440473003724, distance: 0.9701418231737089 entropy 9.918575423603187
epoch: 23, step: 9
	action: tensor([[-13054.9286,   4197.6837,  -6640.2112,   6864.2352,   3591.2278,
          -2070.1244,  -1887.6487]], dtype=torch.float64)
	q_value: tensor([[-34.9899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.800015459621235
epoch: 23, step: 10
	action: tensor([[-1734.8822, -4318.6267, -1508.2183, -3664.9073, -3014.9231,    84.0658,
           768.3982]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026774567068824418, distance: 1.159562720906214 entropy 9.244965131231682
epoch: 23, step: 11
	action: tensor([[ 1180.6339, -3520.0579, -9136.0122,  7332.5334, -1532.3427,  3797.2517,
          -856.1509]], dtype=torch.float64)
	q_value: tensor([[-26.6234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.596120901245577, distance: 0.7272477436862015 entropy 9.611402074143882
epoch: 23, step: 12
	action: tensor([[  901.3189, -4809.8761,  1786.5526,     9.9988,  2060.4078,  2936.1739,
          7355.1742]], dtype=torch.float64)
	q_value: tensor([[-22.7615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.315786760031983
epoch: 23, step: 13
	action: tensor([[  445.1908, -3185.8425,   198.2015,  -303.7717,   949.8638,   604.1232,
          1387.2435]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5839349501424929, distance: 0.7381375707457795 entropy 9.244965131231682
epoch: 23, step: 14
	action: tensor([[ 1785.5332,  5374.3930,  2371.8212,    69.0294,  3904.7791, -3272.5745,
         -8312.3574]], dtype=torch.float64)
	q_value: tensor([[-29.7588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47688347209092885, distance: 0.8276675230106856 entropy 9.653425496730183
epoch: 23, step: 15
	action: tensor([[-2923.1867, -8567.3440,   233.4124, -3970.8774, -3856.5362,  7252.6365,
         -4441.0569]], dtype=torch.float64)
	q_value: tensor([[-41.8710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41955672288480583, distance: 1.363430700050948 entropy 9.778678100213819
epoch: 23, step: 16
	action: tensor([[ -349.0802,   229.7752,  3491.8731,  5505.5592,  6082.3314, 10744.5957,
          1739.0593]], dtype=torch.float64)
	q_value: tensor([[-28.5214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35282570402822655, distance: 1.3309986545227792 entropy 9.687609948311373
epoch: 23, step: 17
	action: tensor([[-7152.9444, -1232.6999,   489.9089,  5463.0334,   -52.9279, -3014.0821,
          8006.2151]], dtype=torch.float64)
	q_value: tensor([[-30.2131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22529045797277347, distance: 1.2667071413341624 entropy 9.656117055638564
epoch: 23, step: 18
	action: tensor([[-3739.4964, -4174.8808,  2810.2679,  1599.7840, -5016.5042, -4351.1152,
          2224.8786]], dtype=torch.float64)
	q_value: tensor([[-32.7350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03490499828655924, distance: 1.1641446201692107 entropy 9.900281451665531
epoch: 23, step: 19
	action: tensor([[-3027.4832, -1079.3826, -2804.3357,  2618.7343, -4112.4145, 11624.7951,
          -715.5813]], dtype=torch.float64)
	q_value: tensor([[-34.7360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7217983322967976, distance: 1.5015777855815695 entropy 9.947057513509018
epoch: 23, step: 20
	action: tensor([[-3493.5928, -1507.6728,  -220.8268, -6035.9154,  8827.3456,  5587.5741,
         -4438.0228]], dtype=torch.float64)
	q_value: tensor([[-35.1523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9523725853669862, distance: 1.5989616354624834 entropy 9.910912433745349
epoch: 23, step: 21
	action: tensor([[-7028.3280,  -359.9669, -5967.6014,   515.7016,   366.1699,   888.5199,
          7329.7873]], dtype=torch.float64)
	q_value: tensor([[-25.4780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1926915783353631, distance: 1.2497431632508347 entropy 9.491241464282352
epoch: 23, step: 22
	action: tensor([[ -8785.3428,  -3352.8912, -11759.7376,  -4957.2812,  -3813.9624,
           3452.3257,   1514.1767]], dtype=torch.float64)
	q_value: tensor([[-30.5586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9404235910544865, distance: 1.5940611091113093 entropy 9.792427543699537
epoch: 23, step: 23
	action: tensor([[ 6334.9052, -7406.1911, -3307.5546,  6065.7816, -1751.5238, -2517.4814,
         -2168.5372]], dtype=torch.float64)
	q_value: tensor([[-28.7511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24405955665363166, distance: 1.276372031502663 entropy 9.612622791964538
epoch: 23, step: 24
	action: tensor([[-4333.1192,  3427.0684, -3510.1002,  3819.5892,  1535.7611,  1050.4522,
          1189.1716]], dtype=torch.float64)
	q_value: tensor([[-30.4667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12485496544126662, distance: 1.0705247434820637 entropy 9.667655897447249
epoch: 23, step: 25
	action: tensor([[ 3808.6739, -4347.9188, -2324.4951,  -894.5984, -4033.4829,  1820.0405,
          -869.6990]], dtype=torch.float64)
	q_value: tensor([[-36.4739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17947708039245014, distance: 1.0365781535515133 entropy 9.823697198332004
epoch: 23, step: 26
	action: tensor([[ 4170.7984, -1477.1507,   326.0787, -2764.4182,  2658.9533,  -123.6302,
         -6811.8543]], dtype=torch.float64)
	q_value: tensor([[-29.3479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.212872537809067, distance: 1.260271962815162 entropy 9.627304997245037
epoch: 23, step: 27
	action: tensor([[ -345.1021, -2084.1684,  1943.4778,  1500.8073,  1610.5453, -2160.9404,
          -569.6436]], dtype=torch.float64)
	q_value: tensor([[-29.6731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0025487814674707288, distance: 1.1428849818518003 entropy 9.500315836150236
epoch: 23, step: 28
	action: tensor([[-2299.2958, -6544.3704, -5117.3243,  8635.9538, -1706.5318, -3775.9694,
         12674.6071]], dtype=torch.float64)
	q_value: tensor([[-32.2084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0022686475638546977, distance: 1.143045460053284 entropy 9.752870074358626
epoch: 23, step: 29
	action: tensor([[ 1308.6767,  -699.4059, -6147.3809, -1630.2068, -6127.7432, -7288.6592,
         -5029.5840]], dtype=torch.float64)
	q_value: tensor([[-34.8694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2685505118820277, distance: 0.9786983664342571 entropy 9.835436099797173
epoch: 23, step: 30
	action: tensor([[11135.1621, -1990.2352, -3217.7368,   747.7750,  -491.8538,  -281.1319,
          5111.4299]], dtype=torch.float64)
	q_value: tensor([[-35.3435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309323087972334, distance: 0.9510298811493235 entropy 9.873526012363957
epoch: 23, step: 31
	action: tensor([[  396.6180,  3652.5893, -4947.9022, -6432.1077,  4547.4653, -1699.0125,
         -1092.3737]], dtype=torch.float64)
	q_value: tensor([[-36.3800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6078484022221939, distance: 0.7166113586774859 entropy 9.849190398990162
epoch: 23, step: 32
	action: tensor([[-2126.8643,  3646.8802, -6022.2527,  7435.7748,  -487.2215,  1555.8926,
          3638.3886]], dtype=torch.float64)
	q_value: tensor([[-39.0104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25813604012964986, distance: 0.9856411572119608 entropy 9.811039103185633
epoch: 23, step: 33
	action: tensor([[  852.4830, -4639.1715, -5154.5417,  1341.1586,  2837.6846,  1843.0959,
         -5902.8218]], dtype=torch.float64)
	q_value: tensor([[-37.2119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5420740065682206, distance: 1.4210498123797093 entropy 9.779093035847955
epoch: 23, step: 34
	action: tensor([[-1.3482e+00, -5.3691e+03, -1.6301e+03, -1.3304e+03,  6.5415e+03,
          1.3461e+04,  7.4125e+03]], dtype=torch.float64)
	q_value: tensor([[-32.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.022797646010488, distance: 1.6275446545991665 entropy 9.915627950729917
epoch: 23, step: 35
	action: tensor([[-3225.7765,  1516.3179, -5065.9496,  2008.7048, -6912.9803, -2290.7056,
         -2660.6179]], dtype=torch.float64)
	q_value: tensor([[-29.9070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40902935592449663, distance: 1.3583657226807058 entropy 9.726324668370813
epoch: 23, step: 36
	action: tensor([[-7709.8872, -3505.1333, -6875.3395,  -842.1941, -7673.1067, -6279.8754,
          5819.9037]], dtype=torch.float64)
	q_value: tensor([[-38.2907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7433466664091684, distance: 1.5109447051139666 entropy 9.854491267074803
epoch: 23, step: 37
	action: tensor([[-6509.9264, -6934.3558,  3963.9181,  6394.0783, -5259.5277,  1582.2257,
         -2933.2378]], dtype=torch.float64)
	q_value: tensor([[-29.4943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28527157726641494, distance: 1.2973410050842193 entropy 9.67058669267709
epoch: 23, step: 38
	action: tensor([[ -732.6779, -7859.7514, -4785.6949, -4763.6760, -2216.0127,  6513.0895,
          1901.6208]], dtype=torch.float64)
	q_value: tensor([[-27.5148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5328928071253709, distance: 1.4168131741135166 entropy 9.67713687045584
epoch: 23, step: 39
	action: tensor([[-4981.6176, -3897.2691, -1873.6073,  3386.9458, -1854.1889, -2037.3664,
          2152.4269]], dtype=torch.float64)
	q_value: tensor([[-29.5892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.177288541402774, distance: 1.6885529611877719 entropy 9.701750446101187
epoch: 23, step: 40
	action: tensor([[-9435.0922,  1016.5936,   518.4606, -2280.1348,  6245.4798,  1739.5173,
          -718.6529]], dtype=torch.float64)
	q_value: tensor([[-31.9548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31489158151597274, distance: 1.3122049318435296 entropy 9.794327292380578
epoch: 23, step: 41
	action: tensor([[-8430.2204, -6039.2130, -4314.3579,  2055.9509,  2866.3369,   502.8469,
         -1458.8532]], dtype=torch.float64)
	q_value: tensor([[-41.5618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46845280705092573, distance: 1.3867133295667775 entropy 9.71838351808984
epoch: 23, step: 42
	action: tensor([[-10586.8540,   3895.0781,  -1616.9220,   2885.9796,  -4509.5912,
           4771.6752,  -2148.1917]], dtype=torch.float64)
	q_value: tensor([[-30.9987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5117475638862328, distance: 1.407007239378097 entropy 9.775224255738308
epoch: 23, step: 43
	action: tensor([[-4080.2080, -8566.1687, -9932.7223,  5680.5474, -2995.8060, -2047.1488,
         -3686.4253]], dtype=torch.float64)
	q_value: tensor([[-36.7807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14676549275542905, distance: 1.057038726840117 entropy 9.781318081224873
epoch: 23, step: 44
	action: tensor([[-1787.5798,   -58.2692, -2529.7848, -1270.1438,  3401.2271, -1162.8065,
          7373.5882]], dtype=torch.float64)
	q_value: tensor([[-30.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.711178992609697, distance: 1.496940068403685 entropy 9.756414390836303
epoch: 23, step: 45
	action: tensor([[ -757.3107, -2999.2377,  -708.4090,  5678.4726,  3420.5299,  -729.0419,
          7689.9650]], dtype=torch.float64)
	q_value: tensor([[-36.1714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2802991598973008, distance: 1.2948290175638824 entropy 9.83882301857866
epoch: 23, step: 46
	action: tensor([[-8108.9046, -8907.8287,  -678.8481,   929.9335, -5326.1933,  9303.9609,
          -336.1511]], dtype=torch.float64)
	q_value: tensor([[-33.9337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3493201204100955, distance: 1.329273024348254 entropy 9.808085190190942
epoch: 23, step: 47
	action: tensor([[ 8675.5937, -8845.5699,   744.3629, -3422.2700, -4917.9704, -4942.1168,
         -3638.3719]], dtype=torch.float64)
	q_value: tensor([[-33.0748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16390678077824494, distance: 1.0463670225580404 entropy 9.870982364016095
epoch: 23, step: 48
	action: tensor([[ 3697.2132,  5488.6479, -3025.0043, -3450.6767,  3269.6014,  3991.3711,
           921.4558]], dtype=torch.float64)
	q_value: tensor([[-32.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21640475527268976, distance: 1.0129840079351475 entropy 9.561211649135942
epoch: 23, step: 49
	action: tensor([[-2490.6585,  2140.8749,  2482.1039, 13914.5328,  1684.9850,  6329.8672,
          3533.1084]], dtype=torch.float64)
	q_value: tensor([[-41.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26123685600333957, distance: 1.2851535490491577 entropy 9.857167420579533
epoch: 23, step: 50
	action: tensor([[-2178.8597, -1976.1126,   -93.4062,  7232.3392,  1041.8971,   351.3472,
          3592.9927]], dtype=torch.float64)
	q_value: tensor([[-30.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6965133931633072, distance: 1.4905115287349162 entropy 9.541345987402213
epoch: 23, step: 51
	action: tensor([[-2016.4142, -1927.1031, -1273.9324, -4232.5757,  -539.7998, 10557.2544,
          4837.7514]], dtype=torch.float64)
	q_value: tensor([[-31.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8775505399104322, distance: 1.5680232984592628 entropy 9.849783690536222
epoch: 23, step: 52
	action: tensor([[ 1073.0432, -6748.4593,   692.2277,  2189.1896,  2407.4650, -1863.8634,
          7769.9061]], dtype=torch.float64)
	q_value: tensor([[-25.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23772823673448018, distance: 0.9991060977011045 entropy 9.497094380549052
epoch: 23, step: 53
	action: tensor([[-1758.6154, -5500.9152, -6820.0047,  2611.8743,   811.7853, -1553.0957,
           547.6354]], dtype=torch.float64)
	q_value: tensor([[-33.0694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11306865046012926, distance: 1.2073068612702362 entropy 9.553329302334308
epoch: 23, step: 54
	action: tensor([[  -441.3568,   5568.0379, -10475.2130,   8205.8095,  -7916.9248,
          -4734.6195,   1336.3731]], dtype=torch.float64)
	q_value: tensor([[-32.5422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5839050390182507, distance: 1.4401948828720785 entropy 9.733667330332207
epoch: 23, step: 55
	action: tensor([[ -857.2146, -5065.0205,  6633.0274,  6375.5035, -9150.9986,  -461.2302,
          -521.7322]], dtype=torch.float64)
	q_value: tensor([[-38.9068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31888637980610257, distance: 1.3141967381173967 entropy 9.792575819225902
epoch: 23, step: 56
	action: tensor([[ -301.2381, -4809.1246,  2057.7732,  4659.5979,   249.6956,  1168.8391,
          1540.8369]], dtype=torch.float64)
	q_value: tensor([[-34.4990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06356605745213362, distance: 1.1073764077125552 entropy 9.927070998285062
epoch: 23, step: 57
	action: tensor([[-3595.4465, -2049.3433,  1087.0362,  1778.6065, -1025.8806, -5728.5339,
          4780.4320]], dtype=torch.float64)
	q_value: tensor([[-33.6418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1867678059205462, distance: 1.2466357340045564 entropy 9.889495505426051
epoch: 23, step: 58
	action: tensor([[-2065.0156, -1385.2900,  -715.3303,  1962.9831,  -415.4356,  5022.4194,
          5896.6294]], dtype=torch.float64)
	q_value: tensor([[-30.2105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4066369737340234, distance: 1.3572120524091098 entropy 9.738221834244618
epoch: 23, step: 59
	action: tensor([[-4307.6639, -6082.4690, -3147.6149, -1046.0802, -1906.0123,  1063.3562,
          4570.7790]], dtype=torch.float64)
	q_value: tensor([[-29.5480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2747587840200723, distance: 1.7259347329702497 entropy 9.764267353429032
epoch: 23, step: 60
	action: tensor([[-2647.5732, -2177.3241,  -639.8842,  1962.8189,  -170.5732, -5604.3354,
          1305.9708]], dtype=torch.float64)
	q_value: tensor([[-29.4151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23554417185184895, distance: 1.2719962515587238 entropy 9.79417386598567
epoch: 23, step: 61
	action: tensor([[-1679.1917,  2852.0463, -6240.4228,  7010.8826,  1461.7779, -5782.0211,
         -5678.1334]], dtype=torch.float64)
	q_value: tensor([[-34.2183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2295918284949845, distance: 1.004424120812071 entropy 9.809636046581586
epoch: 23, step: 62
	action: tensor([[ -1878.2784,   -626.2374,   2156.5321,  -1187.0715, -11434.3559,
           6840.3341,  -1780.5615]], dtype=torch.float64)
	q_value: tensor([[-36.6055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8166398872551361, distance: 1.5423790444995873 entropy 9.848538079124689
epoch: 23, step: 63
	action: tensor([[  727.1578, -8686.5905,  4742.5639,  1630.8155,   784.9465, -3956.5847,
          -175.3634]], dtype=torch.float64)
	q_value: tensor([[-31.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901122059334085, distance: 0.9641654119203197 entropy 9.759142107512071
epoch: 23, step: 64
	action: tensor([[-3758.6186, -1691.9128,  -682.9400,  -187.0331,   210.5275, -4155.1166,
         -1646.7269]], dtype=torch.float64)
	q_value: tensor([[-29.6956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08609959106566878, distance: 1.192590974682333 entropy 9.616997336182493
epoch: 23, step: 65
	action: tensor([[  282.5002, -4272.7712,   859.8430,  1510.3142,  5186.5177,  3637.9314,
         -1969.4900]], dtype=torch.float64)
	q_value: tensor([[-30.2985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25680017774688324, distance: 0.9865281720944599 entropy 9.775072948208841
epoch: 23, step: 66
	action: tensor([[-2515.5612, -7381.2091,  3619.6947,  2494.6391,  4440.5038,  4793.1359,
          3402.7039]], dtype=torch.float64)
	q_value: tensor([[-32.6748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.957143028683167, distance: 1.6009139016839002 entropy 9.814305398759318
epoch: 23, step: 67
	action: tensor([[-8307.2211,   875.1031,  3183.0765,  6375.4495,  1872.1470,  5466.3521,
         -6430.4313]], dtype=torch.float64)
	q_value: tensor([[-32.7875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30234358122202964, distance: 1.3059287417323855 entropy 9.914802150341997
epoch: 23, step: 68
	action: tensor([[-7432.5705, -3436.3565, -6785.2571,   255.4838, -2981.9907,  1592.4584,
          2618.1401]], dtype=torch.float64)
	q_value: tensor([[-34.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3172050165997651, distance: 1.3133587786248306 entropy 9.799971159372598
epoch: 23, step: 69
	action: tensor([[-4765.3925, -3559.8384, -1738.4866,  8219.0660, -6536.5989, -5530.1586,
           520.7221]], dtype=torch.float64)
	q_value: tensor([[-31.6669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14789943074152423, distance: 1.2260512191724457 entropy 9.839751856077802
epoch: 23, step: 70
	action: tensor([[-3307.6281,  2172.6347, 10018.3101,  1819.0738, -3176.1711,  3016.4678,
         -3246.1715]], dtype=torch.float64)
	q_value: tensor([[-31.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20980799271004202, distance: 1.2586788016016934 entropy 9.81128131739335
epoch: 23, step: 71
	action: tensor([[-3250.8132,  5010.9580, -1451.6891,  1694.7526, -8273.4731,   287.2371,
          4442.9125]], dtype=torch.float64)
	q_value: tensor([[-38.4017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17459071940017035, distance: 1.2402235560692483 entropy 9.912010569866226
epoch: 23, step: 72
	action: tensor([[ -2493.7061,  -3246.6276,  -3824.6629, -11111.6258,  -1046.7525,
           -860.3115,   1278.4746]], dtype=torch.float64)
	q_value: tensor([[-34.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0162657434294888, distance: 1.6249147426896948 entropy 9.710227592906081
epoch: 23, step: 73
	action: tensor([[-4355.4278, -5733.1049,  5849.7313,  2193.1747,  2541.5827,  6416.4063,
          2669.1265]], dtype=torch.float64)
	q_value: tensor([[-35.8265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3666571920608104, distance: 1.760452740740405 entropy 9.863421030348869
epoch: 23, step: 74
	action: tensor([[ 5831.6769,  1498.6103,  3680.2923,  3145.3825,  1195.7741,  -477.5828,
         -7175.0336]], dtype=torch.float64)
	q_value: tensor([[-31.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5113498403117472, distance: 0.7999368725959557 entropy 9.900670691595666
epoch: 23, step: 75
	action: tensor([[-2783.2570,  -389.8714,   720.9144,  -561.5044, -7160.8633, -3123.4704,
          -973.5679]], dtype=torch.float64)
	q_value: tensor([[-30.8936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8215573307527477, distance: 1.54446515857995 entropy 9.503637891693986
epoch: 23, step: 76
	action: tensor([[-3988.5782, -2687.7652, -4514.2510, -4930.0361,  2238.1694,   -20.5377,
         -1204.5623]], dtype=torch.float64)
	q_value: tensor([[-30.4111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10810357474933563, distance: 1.0807217822949777 entropy 9.562548709355896
epoch: 23, step: 77
	action: tensor([[-5105.7765,  -908.3965,  2234.7463,   374.6088, -1505.0699,  4027.7625,
          1868.7365]], dtype=torch.float64)
	q_value: tensor([[-38.6105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6171671732070882, distance: 1.455238418977262 entropy 9.892313796680613
epoch: 23, step: 78
	action: tensor([[  428.4930, -9517.9653,  4717.3680,  4978.2554,  5028.2249,  2169.6281,
           813.3733]], dtype=torch.float64)
	q_value: tensor([[-34.7478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4148249675938841, distance: 0.8753859808789307 entropy 9.925096426517902
epoch: 23, step: 79
	action: tensor([[-4870.6026, -1473.0070,  5355.0233, -3539.8136, -8157.6885,  1045.0677,
          5076.8135]], dtype=torch.float64)
	q_value: tensor([[-33.2799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.167785966023092, distance: 1.6848641649653189 entropy 9.809667774934828
epoch: 23, step: 80
	action: tensor([[-4433.6755, -1531.3135,  2645.3433,   452.5943,  6257.4136,  3372.3370,
          -289.9174]], dtype=torch.float64)
	q_value: tensor([[-26.6970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5783658736303061, distance: 1.4376743831272207 entropy 9.637280523765867
epoch: 23, step: 81
	action: tensor([[-3677.9290, -3466.5405,  -183.6878, -1402.2507,   581.1545, -1684.5387,
         -3076.4310]], dtype=torch.float64)
	q_value: tensor([[-26.3013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2727324698629885, distance: 1.7251658459161336 entropy 9.580164830453237
epoch: 23, step: 82
	action: tensor([[-5584.2623,   673.7615, -2245.1060,   901.4039,  -577.2022,  3886.9372,
          6489.5479]], dtype=torch.float64)
	q_value: tensor([[-32.4129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3490218509019829, distance: 1.3291260971964693 entropy 9.830858068481204
epoch: 23, step: 83
	action: tensor([[-1566.2336, -7860.7386, -4409.3521, -3145.6923,  1986.2913,  3627.1404,
          3439.0163]], dtype=torch.float64)
	q_value: tensor([[-33.5781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4098416071970017, distance: 1.7764416316239047 entropy 9.635763011633754
epoch: 23, step: 84
	action: tensor([[-4744.5419, -7177.2366,  1797.2649,   107.7642, -2056.4234,  6829.8957,
         -2250.9588]], dtype=torch.float64)
	q_value: tensor([[-29.6204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0430813905161407, distance: 1.6356844586664043 entropy 9.749793815558814
epoch: 23, step: 85
	action: tensor([[-9553.2384, -3831.9809, -2000.6593,  -418.5173,   409.1524,   759.2641,
          3489.4946]], dtype=torch.float64)
	q_value: tensor([[-27.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1625635867682078, distance: 1.6828334512379328 entropy 9.60408797565125
epoch: 23, step: 86
	action: tensor([[   144.0560, -11035.5319,   5824.2554,  10879.5363,  -9053.0000,
           7161.8852,   1766.4987]], dtype=torch.float64)
	q_value: tensor([[-31.1843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035253883184339085, distance: 1.123991981105515 entropy 9.845056364800898
epoch: 23, step: 87
	action: tensor([[-1870.1930,  -985.0401,  3142.2528,  3669.7891,  -350.7628,  1407.7394,
          2692.1590]], dtype=torch.float64)
	q_value: tensor([[-23.8160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05746681794718045, distance: 1.176765879793627 entropy 9.460422718147383
epoch: 23, step: 88
	action: tensor([[-5242.3862, -2495.7062, -9141.4484,  -887.0517,  2387.3587,  1543.1429,
          7546.4861]], dtype=torch.float64)
	q_value: tensor([[-31.9088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0453619685798388, distance: 1.6365971158200032 entropy 9.841504564371471
epoch: 23, step: 89
	action: tensor([[-1745.4416, -3078.5458, -3419.2083,  2303.0985, -2561.4118,  1177.8648,
           849.5784]], dtype=torch.float64)
	q_value: tensor([[-23.1105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8324339127434477, distance: 1.5490693234128199 entropy 9.441408473570052
epoch: 23, step: 90
	action: tensor([[-5012.9372,  5747.3509,  3344.4803, -3854.4043,  1436.9299,  1152.2543,
         -3666.6885]], dtype=torch.float64)
	q_value: tensor([[-32.0374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.666205431572195, distance: 1.4771376446078042 entropy 9.791436200443853
epoch: 23, step: 91
	action: tensor([[  245.3180, -3669.5377, -9860.2429, -5363.3029,  3148.7883,  2476.8155,
          1751.6662]], dtype=torch.float64)
	q_value: tensor([[-33.8652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06298871255036453, distance: 1.107717723624425 entropy 9.689188711020659
epoch: 23, step: 92
	action: tensor([[-2762.7470,    54.4303,   370.4894,  -226.8656, -3036.3737, -3148.4076,
          3140.3463]], dtype=torch.float64)
	q_value: tensor([[-28.3254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12372772278895239, distance: 1.213073850129767 entropy 9.574137043555087
epoch: 23, step: 93
	action: tensor([[-4248.8074,  -328.6452, -1505.6150,  -296.9393,  1051.2509, -9132.1310,
          -126.5474]], dtype=torch.float64)
	q_value: tensor([[-36.5214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3002485941498199, distance: 1.7355777754895236 entropy 9.645961770662728
epoch: 23, step: 94
	action: tensor([[-9937.2336, -4956.4087,   380.6794,  5192.1452,  1421.6124,  1969.4778,
          4853.5133]], dtype=torch.float64)
	q_value: tensor([[-33.6423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9965954789857701, distance: 1.6169691531152885 entropy 9.712402690708018
epoch: 23, step: 95
	action: tensor([[-2.0344e+03, -6.9958e+03,  7.7890e+03,  3.7982e+03, -1.4070e+03,
         -2.6719e+00,  8.1727e+03]], dtype=torch.float64)
	q_value: tensor([[-30.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.796839570539323
epoch: 23, step: 96
	action: tensor([[-5357.2335, -2245.3308,  1325.4033,  -655.1958,  3385.7973,  1294.8818,
         -2309.4357]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.244965131231682
epoch: 23, step: 97
	action: tensor([[ -790.5620,   351.2350,  2642.1603,  3237.3364, -9282.5005,  1462.3979,
         -1247.7885]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11169911058203863, distance: 1.0785412050923429 entropy 9.244965131231682
epoch: 23, step: 98
	action: tensor([[ 4069.8939,  -850.1579, -7377.0576, -6995.6754,  -915.4071, -3227.5047,
          3404.9520]], dtype=torch.float64)
	q_value: tensor([[-36.5839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.761238921385265
epoch: 23, step: 99
	action: tensor([[-4069.5164, -3713.3163,  -295.2383, -1731.2841,  2688.6179,  6158.8427,
          -687.8382]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48203546771275785, distance: 1.3931118675172058 entropy 9.244965131231682
epoch: 23, step: 100
	action: tensor([[ 2781.4656,  -890.1834, -1051.1450,  3098.3641,  -625.4472,  -404.1639,
          2737.3705]], dtype=torch.float64)
	q_value: tensor([[-32.4505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18533550664095844, distance: 1.0328710083951962 entropy 9.689764392856391
epoch: 23, step: 101
	action: tensor([[-6606.5087, -2344.1835, -4105.1770, -1681.6769,  3895.2632,  2192.3135,
         -4505.6082]], dtype=torch.float64)
	q_value: tensor([[-34.6948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3003396544442043, distance: 1.304923633294492 entropy 9.723539262972318
epoch: 23, step: 102
	action: tensor([[ 1601.5357, -4441.8954, -3393.3436,  1864.2297,   171.2518,  1232.5977,
          1837.7591]], dtype=torch.float64)
	q_value: tensor([[-32.2678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3556283385420145, distance: 0.918596760534837 entropy 9.671671909368554
epoch: 23, step: 103
	action: tensor([[-3980.8450,  1195.1732, -3950.3706,  7990.7674,  1469.7292,  3964.3098,
         -2534.2562]], dtype=torch.float64)
	q_value: tensor([[-26.0569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.528084435661418
epoch: 23, step: 104
	action: tensor([[-3200.5810,  -704.6609, -1637.7336,  2037.8930,  2424.8348, -1613.5719,
          2977.6424]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5705597957349153, distance: 1.4341148445409695 entropy 9.244965131231682
epoch: 23, step: 105
	action: tensor([[  2912.1476, -10591.1988,    849.7284,   -756.7815,   4492.6495,
           1469.0804,   4651.7152]], dtype=torch.float64)
	q_value: tensor([[-31.2738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.201105709147261, distance: 1.2541437231865664 entropy 9.799519502009987
epoch: 23, step: 106
	action: tensor([[-4963.4664, -5827.4391,  1631.9414, -1573.8170, -1241.5992, -1993.7569,
          3746.0407]], dtype=torch.float64)
	q_value: tensor([[-24.5275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08093481235003064, distance: 1.1897520045750085 entropy 9.363175043770399
epoch: 23, step: 107
	action: tensor([[-7432.0983,  5723.8651, -6883.0377,  4863.8663,  1199.8141,  5225.0886,
          5537.3189]], dtype=torch.float64)
	q_value: tensor([[-38.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0897273981020954, distance: 1.1945810698512527 entropy 9.90404270391821
epoch: 23, step: 108
	action: tensor([[-4695.7023, -2579.3074, -2621.6177,  2307.0611,  -929.6135,  7183.4770,
         -1765.5500]], dtype=torch.float64)
	q_value: tensor([[-39.0741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10462054855548797, distance: 1.0828299387830949 entropy 9.744192572001252
epoch: 23, step: 109
	action: tensor([[ -603.1674, -4217.8042,  1399.1397,  4587.9139,  -127.9821,  7085.1533,
          7179.5189]], dtype=torch.float64)
	q_value: tensor([[-29.4014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30350115527766186, distance: 1.3065089931669214 entropy 9.717516062721716
epoch: 23, step: 110
	action: tensor([[ -655.2792, -5866.9959,  2129.2391, -3099.7770, -4627.9828, -3736.6956,
         -1635.1261]], dtype=torch.float64)
	q_value: tensor([[-31.7606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8975233590898672, distance: 1.5763413164710427 entropy 9.914215053185515
epoch: 23, step: 111
	action: tensor([[-3260.0483,  1491.3807, -7002.9343,   246.1484, -3664.3264,  -620.6137,
         11443.6720]], dtype=torch.float64)
	q_value: tensor([[-33.3451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22666674036419765, distance: 1.0063291146094184 entropy 9.721042486370282
epoch: 23, step: 112
	action: tensor([[-5024.9055, -2263.8516,   111.6361, -2933.0543, -2057.9709,  -802.8495,
          2814.9161]], dtype=torch.float64)
	q_value: tensor([[-41.3230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38354240730899236, distance: 1.346024394828981 entropy 9.921438274256031
epoch: 23, step: 113
	action: tensor([[  -77.8462, -3298.5878, -1995.3266,  4111.1174, -8708.1683,  8562.9040,
         -2985.3712]], dtype=torch.float64)
	q_value: tensor([[-36.6907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13408187169190522, distance: 1.2186497322825824 entropy 9.816761868280944
epoch: 23, step: 114
	action: tensor([[-2659.1721,  -323.0750, -2800.6636, -2441.1734,   187.1574, -4301.3254,
         -5947.0834]], dtype=torch.float64)
	q_value: tensor([[-32.7666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39885622803055365, distance: 1.353453170133563 entropy 9.833314426782778
epoch: 23, step: 115
	action: tensor([[-5173.4758,  -369.7991, -4929.6029, -6180.0580,  8349.6195, -4386.6685,
          1734.4259]], dtype=torch.float64)
	q_value: tensor([[-34.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03214603954500306, distance: 1.1625918349153372 entropy 9.75111254463761
epoch: 23, step: 116
	action: tensor([[ -7325.1265,   5562.7004, -10209.2289,   2691.5199,   2958.1218,
          -6862.0692,   -457.2012]], dtype=torch.float64)
	q_value: tensor([[-34.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3127798562833257, distance: 0.9486469915424399 entropy 9.767355695285689
epoch: 23, step: 117
	action: tensor([[-2714.9245,   360.1189,  5163.1933,  2393.8542, -1562.5189, -4020.5783,
         -6159.0447]], dtype=torch.float64)
	q_value: tensor([[-30.7994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027869742908776973, distance: 1.1282852834339556 entropy 9.471643954540278
epoch: 23, step: 118
	action: tensor([[ 6871.6927,  -260.9679,  2883.3060,  6139.5694,  3003.6476, -3715.5650,
          2409.9390]], dtype=torch.float64)
	q_value: tensor([[-35.7853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.692825758719994
epoch: 23, step: 119
	action: tensor([[ 5123.0766, -2961.7546,  2375.9652,  -440.2743,  1187.8848,   515.8577,
          -140.9389]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.244965131231682
epoch: 23, step: 120
	action: tensor([[-2442.7759, -2853.0321, -3968.6867,  -113.0031, -2264.7744,  2243.1764,
         -3014.7979]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.244965131231682
epoch: 23, step: 121
	action: tensor([[-3023.2036, -1181.3117,  1808.4126, -1496.7318, -5061.8509,   646.5540,
          1397.9397]], dtype=torch.float64)
	q_value: tensor([[-33.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0016975790396203116, distance: 1.143372534022117 entropy 9.244965131231682
epoch: 23, step: 122
	action: tensor([[ -5608.9371, -14226.0612,  -5795.7373,   7277.6024,   -584.1509,
           2865.4291,   6157.6417]], dtype=torch.float64)
	q_value: tensor([[-32.4813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2211107832106025, distance: 1.2645448186790709 entropy 9.83070714410192
epoch: 23, step: 123
	action: tensor([[ -889.7453,  -247.9699,  -993.8096, -2372.0234, -1974.2593,  -140.8083,
          2987.1301]], dtype=torch.float64)
	q_value: tensor([[-29.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2082987618916069, distance: 1.0182099937428548 entropy 9.704667425658837
epoch: 23, step: 124
	action: tensor([[ 3785.4047, -1538.9019,  -995.2895,  5931.6054,  -690.3457,  4226.9609,
          2613.2943]], dtype=torch.float64)
	q_value: tensor([[-27.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14077436895125162, distance: 1.060743319130656 entropy 9.662727582607685
epoch: 23, step: 125
	action: tensor([[  651.6273, -6586.8799,  -627.9147,  5663.8778,  4796.9131, -3259.9509,
          6055.3446]], dtype=torch.float64)
	q_value: tensor([[-25.0647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3394311073111238, distance: 0.9300702488506271 entropy 9.555639013166516
epoch: 23, step: 126
	action: tensor([[-2076.9173, -4962.1949,  1064.4861,  3148.8128,  -261.9889, -3818.1195,
           343.0298]], dtype=torch.float64)
	q_value: tensor([[-38.9955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1082562281732049, distance: 1.0806292923289886 entropy 9.735731476812896
epoch: 23, step: 127
	action: tensor([[ -6740.1959,  -2996.1858,   8556.0741,   8779.0594,    -94.7899,
          -7579.5165, -10608.4672]], dtype=torch.float64)
	q_value: tensor([[-34.8061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9269877886173772, distance: 1.5885327560042373 entropy 9.882668572813516
LOSS epoch 23 actor 457.1234891330571 critic 101.44335320309153
epoch: 24, step: 0
	action: tensor([[   408.8544,   -326.8642,   3071.4431,   6252.6800, -13152.3706,
            673.9411,  -3129.5194]], dtype=torch.float64)
	q_value: tensor([[-26.7050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13764502534580858, distance: 1.2205626589422944 entropy 9.901939709137707
epoch: 24, step: 1
	action: tensor([[ -619.3006,  2811.7092, -5083.0942, -1877.6111,  1837.1105,  -918.2788,
          1914.4120]], dtype=torch.float64)
	q_value: tensor([[-19.9886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08677727775550914, distance: 1.1929629833837438 entropy 9.623230676371751
epoch: 24, step: 2
	action: tensor([[-11914.7055,  -2257.3579,   1637.2385,    576.4727,     24.5587,
           1260.7515,    592.8124]], dtype=torch.float64)
	q_value: tensor([[-32.7402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09478856259182611, distance: 1.0887588785709494 entropy 9.691133916176302
epoch: 24, step: 3
	action: tensor([[-11763.3041, -13956.8307,  -4847.5543,   3309.0621,  -5105.6725,
           4198.5145,   5991.1361]], dtype=torch.float64)
	q_value: tensor([[-25.4068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37095007634519894, distance: 1.3398849632475884 entropy 9.911424158708897
epoch: 24, step: 4
	action: tensor([[-5852.0308, -4159.8858, -4779.2755,  6338.4907, -1412.7222,  2287.7295,
         -6745.4762]], dtype=torch.float64)
	q_value: tensor([[-31.8756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02349183171816238, distance: 1.1308230009988214 entropy 10.093735953120628
epoch: 24, step: 5
	action: tensor([[10579.3551, -5219.1060,  -721.8604, -2347.9444, -1181.7627, -5894.1269,
            58.5897]], dtype=torch.float64)
	q_value: tensor([[-27.2236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2024018074863254, distance: 1.2548202062421854 entropy 9.94539648610312
epoch: 24, step: 6
	action: tensor([[-2050.1552, -6521.4855, -1547.9994, -4161.5449, -4342.5738, -5027.9082,
          3204.6491]], dtype=torch.float64)
	q_value: tensor([[-22.6804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1840267240107497, distance: 1.0337003443694441 entropy 9.573682863208498
epoch: 24, step: 7
	action: tensor([[-5148.9192,  -851.9876, -4687.3277,  5443.4304,  1954.3439,  1642.9246,
          9699.0552]], dtype=torch.float64)
	q_value: tensor([[-27.3685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46039435283867003, distance: 1.3829031495532769 entropy 9.741901172404386
epoch: 24, step: 8
	action: tensor([[ -244.4422, -7466.4568,   882.8968,  -106.7804, -8845.1790,  -540.8407,
          3279.4017]], dtype=torch.float64)
	q_value: tensor([[-19.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.254803747002402, distance: 1.7183477841036825 entropy 9.56466953716441
epoch: 24, step: 9
	action: tensor([[ 1649.8116, -1514.3929,  2665.9794,   -55.3471, -7023.6310, -3543.3184,
          3339.7378]], dtype=torch.float64)
	q_value: tensor([[-19.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44596527450692935, distance: 0.8517755827445742 entropy 9.551195278770608
epoch: 24, step: 10
	action: tensor([[-4006.9570,   -61.3877,  3363.7506,  3967.6375, -2451.2456,  1954.3255,
          2907.9587]], dtype=torch.float64)
	q_value: tensor([[-29.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5421500856307847, distance: 0.7743160509251196 entropy 9.651486595243018
epoch: 24, step: 11
	action: tensor([[-10627.0298,    205.5464,  -2159.9197,  -5583.9731,  -1181.0768,
           5864.7640,   3146.4175]], dtype=torch.float64)
	q_value: tensor([[-26.1067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36108557316619416, distance: 0.9146986552942953 entropy 9.828409923925468
epoch: 24, step: 12
	action: tensor([[ -909.8746,  -169.9318, -6972.0782,  -603.8389,   579.7756, -4204.7915,
           259.5673]], dtype=torch.float64)
	q_value: tensor([[-26.5476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20239169210139907, distance: 1.0220014871168108 entropy 9.550987994807013
epoch: 24, step: 13
	action: tensor([[  322.3721, -3312.8050,  1242.4534, -4133.0903, -3492.9742, -7246.4588,
         -6776.9341]], dtype=torch.float64)
	q_value: tensor([[-36.0709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2729749855941529, distance: 0.9757338463730559 entropy 9.89152713649638
epoch: 24, step: 14
	action: tensor([[  380.6802,  -667.1094,  3486.7190,    15.6610, -4991.6886, -7630.4598,
         -3435.7190]], dtype=torch.float64)
	q_value: tensor([[-27.6753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32304999021197944, distance: 1.3162695063249181 entropy 9.903555553817407
epoch: 24, step: 15
	action: tensor([[-3436.3752, -3257.1377,  3875.6916,    90.0461,  2257.1858,  5831.0974,
         -1848.9068]], dtype=torch.float64)
	q_value: tensor([[-29.8310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15173105300572065, distance: 1.053958419887803 entropy 9.796493980756072
epoch: 24, step: 16
	action: tensor([[ -705.6854, -1385.8184, -4604.3767, -2966.2477,   205.1188,  1070.7150,
         -4166.3352]], dtype=torch.float64)
	q_value: tensor([[-18.8400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07369514759981888, distance: 1.1857610717598126 entropy 9.478254952652946
epoch: 24, step: 17
	action: tensor([[-1007.4811,  3182.0285, -4111.6051,  5051.6277,  2703.4621,  2656.8880,
          3585.0215]], dtype=torch.float64)
	q_value: tensor([[-26.9059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7010113464771095, distance: 0.6257257779935892 entropy 9.783736468097686
epoch: 24, step: 18
	action: tensor([[-5749.3260,    81.9386,  3560.6754,  2047.0605,  1857.5307,  3110.6347,
          7999.9442]], dtype=torch.float64)
	q_value: tensor([[-30.2596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14575837290391291, distance: 1.2249072723677428 entropy 9.752215615330309
epoch: 24, step: 19
	action: tensor([[ -7784.5878,    584.8743,    -23.5826,   2210.1457, -13700.3271,
           4737.2881,   8037.1817]], dtype=torch.float64)
	q_value: tensor([[-31.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27881942592431785, distance: 0.9718040468188751 entropy 10.03945493943856
epoch: 24, step: 20
	action: tensor([[ -5207.4836,  -5067.1137,   2496.9984,   7333.0611,  -2230.4403,
         -10425.4247,   2156.0112]], dtype=torch.float64)
	q_value: tensor([[-30.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1806045394826934, distance: 1.2433944303340028 entropy 10.035323702535608
epoch: 24, step: 21
	action: tensor([[-13637.4032,  -1029.1642,   -708.3495,   3795.2143,   1625.5845,
           1241.9062,   4685.9716]], dtype=torch.float64)
	q_value: tensor([[-26.3981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11257721545358923, distance: 1.0780079924929753 entropy 9.844813919973815
epoch: 24, step: 22
	action: tensor([[  -19.5547, -3668.5740, -4524.5270, -3379.5228, -1243.5024,  6263.0005,
          1673.2594]], dtype=torch.float64)
	q_value: tensor([[-25.6769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18353760426743615, distance: 1.2449380014462836 entropy 9.842482669585866
epoch: 24, step: 23
	action: tensor([[ 1072.7567, -4762.5974, -4351.9650,  1367.3007,   416.2341,  -229.9631,
         -4743.0723]], dtype=torch.float64)
	q_value: tensor([[-17.9008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17340320403345633, distance: 1.0404076863895748 entropy 9.42173748094983
epoch: 24, step: 24
	action: tensor([[-8800.1445,  1952.0919,  3749.5112,  1358.1215,  4815.9855,  -495.6257,
         -4969.8946]], dtype=torch.float64)
	q_value: tensor([[-28.9500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4506243721622393, distance: 0.8481865621581918 entropy 9.648177482926602
epoch: 24, step: 25
	action: tensor([[-1654.3291, -4431.4423,  -651.4620,   411.4698, -6663.0830,  1399.9045,
           833.2350]], dtype=torch.float64)
	q_value: tensor([[-32.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00917271852998014, distance: 1.1390837891468018 entropy 9.872587872918276
epoch: 24, step: 26
	action: tensor([[ 3414.4627, -7682.8594, -3170.0691,  4996.5928, -4763.0898, -1979.0110,
         14161.6568]], dtype=torch.float64)
	q_value: tensor([[-30.3939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3986501463940777, distance: 1.3533534700671228 entropy 10.020757435358068
epoch: 24, step: 27
	action: tensor([[ 3937.3829, -5215.8218, -3020.7171, 11416.8924, -9062.1882,  5259.6365,
          2960.8347]], dtype=torch.float64)
	q_value: tensor([[-29.4403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01968098356747805, distance: 1.1555502963930797 entropy 9.738179969366815
epoch: 24, step: 28
	action: tensor([[-11520.0938,  -3200.6724,  -1547.1970,   5866.0503,   1676.9334,
           2663.5664,  -3566.1958]], dtype=torch.float64)
	q_value: tensor([[-29.3680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0714751299709756, distance: 1.1026900697308988 entropy 9.814901287552024
epoch: 24, step: 29
	action: tensor([[-2471.3376, -9694.6589,   690.3770,  6457.8747,   364.1953,  1965.2011,
          7211.1252]], dtype=torch.float64)
	q_value: tensor([[-24.2345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21108137274813754, distance: 1.016419055455923 entropy 9.827977865437036
epoch: 24, step: 30
	action: tensor([[ 3166.7674, -3632.8307,     3.9971,  2780.4076,  1914.3365, -3039.1334,
          3482.4762]], dtype=torch.float64)
	q_value: tensor([[-24.6397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6002577553716816, distance: 0.7235136293661895 entropy 9.828185988857141
epoch: 24, step: 31
	action: tensor([[ -543.7945, -5664.4859,  4999.0096,  8093.6764, -1175.9115, -5345.9385,
         -8877.7767]], dtype=torch.float64)
	q_value: tensor([[-27.5047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5010220065563005, distance: 1.4020071324263994 entropy 9.969379138732346
epoch: 24, step: 32
	action: tensor([[-2573.6763, -3676.0494,   854.2553, -6840.0665,  2536.5556,   806.4432,
         -3645.8373]], dtype=torch.float64)
	q_value: tensor([[-25.3784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2324738195676208, distance: 1.2704148002427667 entropy 9.895938982426447
epoch: 24, step: 33
	action: tensor([[-3655.0773,  5609.6077,  -414.5056,  2154.6511,   370.4824, -5782.3862,
         -5262.1887]], dtype=torch.float64)
	q_value: tensor([[-24.1996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08979609161922741, distance: 1.09175714107352 entropy 9.728528690911489
epoch: 24, step: 34
	action: tensor([[-7121.3440, -2851.9628,  1105.1640,  -521.1408, -6416.6293,  2753.3361,
          5134.2730]], dtype=torch.float64)
	q_value: tensor([[-27.1714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6595545626972608, distance: 1.474186606309918 entropy 9.783770050212583
epoch: 24, step: 35
	action: tensor([[ -251.1568, -8158.5955,  5364.8456,   860.9754, -1419.4368,  7559.8120,
          3229.8867]], dtype=torch.float64)
	q_value: tensor([[-21.6050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2679682649597558, distance: 0.979087819680531 entropy 9.634213275836013
epoch: 24, step: 36
	action: tensor([[-5910.0178, -8185.3293,  -164.0828,  3986.8669,  3231.1823, -2276.6869,
          3051.6982]], dtype=torch.float64)
	q_value: tensor([[-23.0273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3058817481889955, distance: 1.307701492021052 entropy 9.72270289423623
epoch: 24, step: 37
	action: tensor([[-2670.3571,  6172.1040,  -396.3238, -1784.6259,  1360.3432,  2850.6889,
          8154.9097]], dtype=torch.float64)
	q_value: tensor([[-23.9072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24398333645322168, distance: 0.9949983882839789 entropy 9.787235810544189
epoch: 24, step: 38
	action: tensor([[-5916.3326,  3777.5978,  6898.8470, -4899.1674,  -847.3355,  7635.3037,
          6502.6083]], dtype=torch.float64)
	q_value: tensor([[-31.3895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3474418762279785, distance: 0.9244135305539121 entropy 9.755700550014081
epoch: 24, step: 39
	action: tensor([[-4541.9881, -5938.3819,   -86.0307, -2801.2910, -4583.2274, -1680.0519,
          6037.7443]], dtype=torch.float64)
	q_value: tensor([[-31.3529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8459873850335118, distance: 1.5547875619810778 entropy 9.608554245347305
epoch: 24, step: 40
	action: tensor([[-4594.0826,  -664.6522,   -70.3048,  1368.0443, -1749.5320,  6743.8594,
          3221.6401]], dtype=torch.float64)
	q_value: tensor([[-30.4302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7510756439344817, distance: 1.5142903227954596 entropy 9.904776235756296
epoch: 24, step: 41
	action: tensor([[ -249.5851, -2374.3762,  4445.9166,  3049.3695, -4314.2406,  2150.6770,
          5846.9271]], dtype=torch.float64)
	q_value: tensor([[-25.2565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7273414453731384, distance: 1.5039929137115418 entropy 9.82567273480561
epoch: 24, step: 42
	action: tensor([[-2581.9574, -4053.0762,  1140.3830,  7573.0492,   201.0037, -9211.6036,
          7616.8121]], dtype=torch.float64)
	q_value: tensor([[-27.8120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4423262744543348, distance: 1.3743218483644284 entropy 9.993043394543735
epoch: 24, step: 43
	action: tensor([[-10917.3826,     69.2052,   8835.8746,  11955.8530,  -2052.9050,
           5625.3172,  -7543.0948]], dtype=torch.float64)
	q_value: tensor([[-27.6052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023055123062832195, distance: 1.1574605839389118 entropy 9.988534103730446
epoch: 24, step: 44
	action: tensor([[ -2652.0188, -11419.9563,   6497.9144,  -2340.3604,   2854.3555,
          -2920.6045,   4275.2381]], dtype=torch.float64)
	q_value: tensor([[-26.7046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9997443197894011, distance: 1.6182437158882805 entropy 9.90733625589162
epoch: 24, step: 45
	action: tensor([[ 2112.4648,  3801.2002,   217.7773,    82.3328,  -765.4069, -1689.4527,
          7272.7564]], dtype=torch.float64)
	q_value: tensor([[-22.9315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.773874316728032
epoch: 24, step: 46
	action: tensor([[  192.2516, -5918.4665,  2480.2077,  -978.8518, -1510.8050, -4043.5808,
          3938.2100]], dtype=torch.float64)
	q_value: tensor([[-27.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27492079044631335, distance: 0.9744272477968982 entropy 9.31433615036473
epoch: 24, step: 47
	action: tensor([[ -7050.6070,   2184.5164, -11961.6043,  11843.3349,   8440.3338,
          -5326.1171,    800.3370]], dtype=torch.float64)
	q_value: tensor([[-30.9405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014903410157874997, distance: 1.1357849276083205 entropy 9.881019458399466
epoch: 24, step: 48
	action: tensor([[-2913.1295, -1539.3985,   174.2280, -4168.0308, -4844.4113, -2469.0217,
         -2568.3043]], dtype=torch.float64)
	q_value: tensor([[-35.1310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10654555998106585, distance: 1.0816653031795391 entropy 9.968369986874636
epoch: 24, step: 49
	action: tensor([[-7487.1686, -6251.2720,  3821.2242,  1335.0622,  -585.2444,  1656.4608,
          8497.0718]], dtype=torch.float64)
	q_value: tensor([[-29.6486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9708571461475299, distance: 1.6065130822473024 entropy 9.85270857570114
epoch: 24, step: 50
	action: tensor([[-5106.0229,  4251.8039,  -603.0128,  9942.3766,   947.9825,   567.6192,
          2425.3167]], dtype=torch.float64)
	q_value: tensor([[-20.1891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1370771384206475, distance: 1.0630230478919374 entropy 9.677984323743596
epoch: 24, step: 51
	action: tensor([[-2121.5674, -9291.6998,  1328.4954,  1823.7083,   583.2527,  1351.3123,
         -2213.6547]], dtype=torch.float64)
	q_value: tensor([[-25.1603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18061715295377234, distance: 1.2434010724725308 entropy 9.656462176761675
epoch: 24, step: 52
	action: tensor([[ -8470.6368, -13418.1069,  -3468.6464,   4788.0357,  -3421.9311,
         -12126.7590,    808.0588]], dtype=torch.float64)
	q_value: tensor([[-27.0809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.055099836443480044, distance: 1.1754481346704697 entropy 9.878505220293542
epoch: 24, step: 53
	action: tensor([[ 2619.2111,  1985.3405, -3647.7457, -2802.3731,  1487.4019, -5683.4891,
          7398.7238]], dtype=torch.float64)
	q_value: tensor([[-28.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.941236180256443
epoch: 24, step: 54
	action: tensor([[-1518.8878, -2094.0816,  1899.3269, -3719.1460,  1174.1041,  4314.3501,
          5685.8754]], dtype=torch.float64)
	q_value: tensor([[-27.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9086695593808263, distance: 1.5809643134044442 entropy 9.31433615036473
epoch: 24, step: 55
	action: tensor([[-3801.1913,   843.7784, -2659.7422,   159.7168, -1872.7815, -3334.4523,
         -4221.2943]], dtype=torch.float64)
	q_value: tensor([[-20.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05610005784285721, distance: 1.176005157726656 entropy 9.53058571997214
epoch: 24, step: 56
	action: tensor([[-6862.1103, -4287.6019,    36.0411,  2049.5495,  2288.8495,  2015.9626,
          5591.4323]], dtype=torch.float64)
	q_value: tensor([[-28.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7683268051426606, distance: 1.5217312467722004 entropy 9.648093937800278
epoch: 24, step: 57
	action: tensor([[ -121.4467, -2679.2065, -8194.4552, -7488.2487,  1870.5784,  6159.2550,
          2388.3976]], dtype=torch.float64)
	q_value: tensor([[-23.3100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15856885126327902, distance: 1.2317359505337764 entropy 9.78719207190269
epoch: 24, step: 58
	action: tensor([[  435.8143, -9697.9117, -1906.0620,  2290.6283,   119.7026,  2557.9591,
          2140.5084]], dtype=torch.float64)
	q_value: tensor([[-23.2086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4483743496360133, distance: 0.8499217035766307 entropy 9.697347058958284
epoch: 24, step: 59
	action: tensor([[ -4113.4945,  -2935.5523,   -167.2828,   6743.8163, -10807.6890,
          -3378.3825,   2445.7188]], dtype=torch.float64)
	q_value: tensor([[-22.0878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2645041857406121, distance: 1.2868171162191095 entropy 9.764739399591319
epoch: 24, step: 60
	action: tensor([[-3574.6569,  -514.2057,   377.2096, -2754.4088,    84.1198,  -884.2947,
         -7805.4423]], dtype=torch.float64)
	q_value: tensor([[-25.9118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9401863530503054, distance: 1.5939636604311782 entropy 9.811214169126604
epoch: 24, step: 61
	action: tensor([[-4620.4542, -2823.4358,  -264.1220, -5729.1087,  -580.2852,  2205.3146,
         -2939.2694]], dtype=torch.float64)
	q_value: tensor([[-25.9205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024808692012334266, distance: 1.130060263759771 entropy 9.682860187755749
epoch: 24, step: 62
	action: tensor([[-1216.3656, -1281.6770, -1647.1253,  3173.7397, -1164.1686, -4806.9548,
          1970.7483]], dtype=torch.float64)
	q_value: tensor([[-25.3181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8033657507629755, distance: 1.5367336527930526 entropy 9.73325156038376
epoch: 24, step: 63
	action: tensor([[-5398.0349,  3765.1516, -6301.7501,  3888.1459,  2833.2719, -1602.0889,
          8055.6713]], dtype=torch.float64)
	q_value: tensor([[-26.3610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.675619255730999, distance: 0.6517547818696336 entropy 9.797779595704442
epoch: 24, step: 64
	action: tensor([[ 2030.2602, -4762.5464,  1257.1690, -2060.7186,  4282.2873,  -641.3013,
          5187.2800]], dtype=torch.float64)
	q_value: tensor([[-25.8816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.624787777572218
epoch: 24, step: 65
	action: tensor([[-3185.1116, -1760.7843, -2016.5085, -4222.4541, -1906.5584,   985.9258,
           555.8372]], dtype=torch.float64)
	q_value: tensor([[-27.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4629487141439781, distance: 1.3841120322774054 entropy 9.31433615036473
epoch: 24, step: 66
	action: tensor([[-394.0580, 6339.0991, 2508.6958, -570.3792,  929.6970, -113.8913,
         8253.1911]], dtype=torch.float64)
	q_value: tensor([[-23.8016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3500925771503949, distance: 1.3296534600127798 entropy 9.782649507509815
epoch: 24, step: 67
	action: tensor([[ 6181.9867, -5734.1901, -2174.4378,  3187.5879, -1313.1229, -4682.3931,
         -2528.6001]], dtype=torch.float64)
	q_value: tensor([[-28.9919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2040058444036823, distance: 1.2556569095368306 entropy 9.799160467093019
epoch: 24, step: 68
	action: tensor([[-3971.6409, -2728.9417, -1445.7981, -3035.0984,  3946.6199,  2242.1600,
          1774.0497]], dtype=torch.float64)
	q_value: tensor([[-25.9484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2063208059797903, distance: 1.2568634658076363 entropy 9.643680847225301
epoch: 24, step: 69
	action: tensor([[-5239.0085, -3267.3942,   438.6640, -1757.4864, -3084.8667,  2890.2644,
          -887.3116]], dtype=torch.float64)
	q_value: tensor([[-23.4264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2704469529289768, distance: 1.2898373872302997 entropy 9.757343064763614
epoch: 24, step: 70
	action: tensor([[ 2229.8079,  3210.4153, -4977.2227,  2793.4323, -4294.6788,  7446.9214,
         -2936.2981]], dtype=torch.float64)
	q_value: tensor([[-29.7132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.86504236985902
epoch: 24, step: 71
	action: tensor([[-4648.5413, -4118.9322,   -23.1166,   767.6633, -1939.1552,   240.2967,
         -4965.9237]], dtype=torch.float64)
	q_value: tensor([[-27.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3682467935716156, distance: 1.7610438590070019 entropy 9.31433615036473
epoch: 24, step: 72
	action: tensor([[ -210.6922, -4346.6036,  7267.2967, -1175.8839,  -228.9538, -5554.9050,
         -2438.2574]], dtype=torch.float64)
	q_value: tensor([[-26.3477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7037240412681776, distance: 1.4936757119248205 entropy 9.85796166873518
epoch: 24, step: 73
	action: tensor([[-5370.0664, -4944.4368,  5517.9725, -5430.4269,  3892.2861, -3202.9953,
          7512.2722]], dtype=torch.float64)
	q_value: tensor([[-33.3737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8706223991274515, distance: 1.5651276304066473 entropy 9.999811320815466
epoch: 24, step: 74
	action: tensor([[-4436.3000,  2989.1216,  -314.8642,  -684.9006, 10997.9715, -9105.0966,
          -124.8030]], dtype=torch.float64)
	q_value: tensor([[-33.4033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.057315425354252714, distance: 1.1766816407368546 entropy 9.982875130570601
epoch: 24, step: 75
	action: tensor([[-3910.4515, -1085.6852, -6251.4288,   207.8625, -6371.4534, -2156.2019,
           992.7421]], dtype=torch.float64)
	q_value: tensor([[-34.2092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7328008834816921, distance: 1.5063677998674845 entropy 9.747272353745444
epoch: 24, step: 76
	action: tensor([[-2214.7577, -3241.0057,  2669.7895,  2726.7405,  4078.1813,  4800.2103,
         -3250.5456]], dtype=torch.float64)
	q_value: tensor([[-20.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11260941478321773, distance: 1.0779884350451792 entropy 9.443808539406827
epoch: 24, step: 77
	action: tensor([[ 2714.0230, -5278.6654,  1742.0337,  5367.5610,  2633.5407, -6087.6097,
          1273.2221]], dtype=torch.float64)
	q_value: tensor([[-24.6769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.828678413395064
epoch: 24, step: 78
	action: tensor([[  345.0432, -2780.6294,  1860.5366,  4255.3372,   738.4673, -1398.4576,
         -1382.0260]], dtype=torch.float64)
	q_value: tensor([[-27.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6004946015083692, distance: 0.7232992577297565 entropy 9.31433615036473
epoch: 24, step: 79
	action: tensor([[  745.9164, -3072.1237,  2605.1249, -6233.7424, -4097.3911, -3292.8551,
          2922.3547]], dtype=torch.float64)
	q_value: tensor([[-22.8864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17511046063506086, distance: 1.039332699761909 entropy 9.751638598402815
epoch: 24, step: 80
	action: tensor([[-2226.2802,  -340.1480, -7283.5874,  3894.5282,   446.9552, -2403.2665,
          6057.6560]], dtype=torch.float64)
	q_value: tensor([[-22.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5835100140563365, distance: 1.4400152798053543 entropy 9.59948957551016
epoch: 24, step: 81
	action: tensor([[-2164.3795,  1528.1653,  -545.2646,  5999.5298,  1848.9760,   -21.6254,
           -33.6567]], dtype=torch.float64)
	q_value: tensor([[-22.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44857958217213934, distance: 0.8497635820190379 entropy 9.617615153730826
epoch: 24, step: 82
	action: tensor([[-1184.7071, -4691.3854, -1234.4438,  7099.1522, -1014.4511, -5360.7377,
          1319.4326]], dtype=torch.float64)
	q_value: tensor([[-27.3986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07712591198261864, distance: 1.099329594121073 entropy 9.715738814533536
epoch: 24, step: 83
	action: tensor([[-3895.8231, -3066.3516,  1842.5278,   480.6195, -6046.6411,  2278.3097,
          1364.9738]], dtype=torch.float64)
	q_value: tensor([[-28.9007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42221832021961925, distance: 1.3647082834151594 entropy 10.007116125662035
epoch: 24, step: 84
	action: tensor([[-8799.0644,  2619.9091,  1504.2891,  8461.7810, -2608.9261,  7077.6282,
          1101.6233]], dtype=torch.float64)
	q_value: tensor([[-27.0339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48907049200995245, distance: 0.8179696426866673 entropy 9.938925447650096
epoch: 24, step: 85
	action: tensor([[-3631.4469, -5753.3448, -1894.7350,  6789.5644, -2081.9626, -6403.5543,
         -2032.2686]], dtype=torch.float64)
	q_value: tensor([[-28.5711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009893618011118877, distance: 1.1499911737209674 entropy 9.917886643405751
epoch: 24, step: 86
	action: tensor([[ 2038.5683, -2952.5325, -4421.2269,  4242.9699,  5889.0526,  4240.3518,
         10373.2392]], dtype=torch.float64)
	q_value: tensor([[-34.1057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1721034767935119, distance: 1.2389097488153606 entropy 10.082915136286902
epoch: 24, step: 87
	action: tensor([[  455.9358, -6508.3262,  1664.0990, 10810.3200,  1711.1576, -5892.5270,
           112.8225]], dtype=torch.float64)
	q_value: tensor([[-29.3266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04434801449292691, distance: 1.1694436929016083 entropy 9.797836250359415
epoch: 24, step: 88
	action: tensor([[ 1518.1755, -1589.7989, -1060.6371,  2570.3882,   882.9532, -6127.7122,
          6982.2027]], dtype=torch.float64)
	q_value: tensor([[-28.1807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4031405886310119, distance: 0.8840823420521223 entropy 9.797199758731336
epoch: 24, step: 89
	action: tensor([[ -2004.0785, -10152.4600,   -738.7811,   -564.1182,  -5135.5100,
          -2179.9673,   -947.9772]], dtype=torch.float64)
	q_value: tensor([[-32.8038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6069346446352961, distance: 1.4506271459815099 entropy 9.988077751973028
epoch: 24, step: 90
	action: tensor([[-1883.8146, -2812.2830,  3240.7762,  6615.8503,  1136.3834,   860.0230,
          8276.6032]], dtype=torch.float64)
	q_value: tensor([[-25.5078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2234556857597726, distance: 1.0084162033838895 entropy 9.783223429348512
epoch: 24, step: 91
	action: tensor([[-8938.0657, -2495.9443,  7995.6441, -3693.9623, -4784.9600,  2608.7369,
         10237.0818]], dtype=torch.float64)
	q_value: tensor([[-24.6831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6196815642555624, distance: 1.4563692907077774 entropy 9.89275606997771
epoch: 24, step: 92
	action: tensor([[ -872.9992, -9366.6198,    45.4728,  2287.3919, -4686.2633, -1999.5855,
         10131.1774]], dtype=torch.float64)
	q_value: tensor([[-28.9564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2529495800680681, distance: 1.2809243770785528 entropy 9.993595333141966
epoch: 24, step: 93
	action: tensor([[-3407.5377, -8226.5832,  2942.7033,  1964.1150, -1886.7527, -5793.0921,
         11785.6003]], dtype=torch.float64)
	q_value: tensor([[-25.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03490162135278707, distance: 1.1641427208439117 entropy 9.855881233971017
epoch: 24, step: 94
	action: tensor([[-4372.0705, -6711.0015,  6033.9285, -2131.1078,  1532.5385,  2197.1171,
          1822.3339]], dtype=torch.float64)
	q_value: tensor([[-32.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7652764455863932, distance: 1.5204181888716748 entropy 9.995137084546528
epoch: 24, step: 95
	action: tensor([[-1559.8812, -1239.3600,  5233.3105,  6251.2189,    17.9626,  -468.0537,
          -952.3131]], dtype=torch.float64)
	q_value: tensor([[-25.0618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4353987374436228, distance: 1.371017420931338 entropy 9.693094874979717
epoch: 24, step: 96
	action: tensor([[-10984.7088,   6261.5103,   4333.4692,   5045.5764,  -1158.5136,
            566.4065,    844.8973]], dtype=torch.float64)
	q_value: tensor([[-26.9931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30665266564422566, distance: 1.3080874309340715 entropy 9.886839794860597
epoch: 24, step: 97
	action: tensor([[-11166.4451,   -648.5771,   5287.0096,   2548.8467,  -8771.1029,
           6320.5802,   2136.7456]], dtype=torch.float64)
	q_value: tensor([[-31.2381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16456773571646544, distance: 1.0459533496735793 entropy 10.016369494453482
epoch: 24, step: 98
	action: tensor([[-5381.8079,  3726.7709,  2832.9536,  7976.0320,   626.5081,  3353.5521,
          -408.4649]], dtype=torch.float64)
	q_value: tensor([[-27.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2002026777921333, distance: 1.2536721809875844 entropy 9.896632034132853
epoch: 24, step: 99
	action: tensor([[-3970.9862,  4149.5405,   356.7492, -1601.9506, -7200.1982,  6860.3883,
         -8380.8671]], dtype=torch.float64)
	q_value: tensor([[-27.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28520857848197456, distance: 0.9674897199756236 entropy 9.812289206005504
epoch: 24, step: 100
	action: tensor([[-5157.7012, -2119.1297,  3531.0211, -2826.5910,  1787.2072,  6762.7554,
         -3750.9698]], dtype=torch.float64)
	q_value: tensor([[-27.3858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1991839256298389, distance: 1.0240545327849606 entropy 9.488147525299592
epoch: 24, step: 101
	action: tensor([[ 6041.9887, -5440.5561, -2552.2864,  8351.6226,   540.8092, 11507.6282,
         -2629.7337]], dtype=torch.float64)
	q_value: tensor([[-26.2366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.469530315913787, distance: 0.833464253257067 entropy 9.839823500010144
epoch: 24, step: 102
	action: tensor([[ 4660.5712,  -610.1610,  1543.8043,  4582.1863, -2548.2049,  1190.6471,
         -2364.5829]], dtype=torch.float64)
	q_value: tensor([[-18.6572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29823675414708395, distance: 0.9586321779095728 entropy 9.493901589541332
epoch: 24, step: 103
	action: tensor([[-6870.9375, -8754.1392,  7367.1836, -3221.9321,  4343.9884,   606.6703,
           942.4558]], dtype=torch.float64)
	q_value: tensor([[-30.1245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36916628917029226, distance: 1.3390129958397952 entropy 9.998569533438067
epoch: 24, step: 104
	action: tensor([[-1040.7741, -3529.2350,  1388.9460,  2073.8112,  -713.2596, -3773.5788,
           -32.1621]], dtype=torch.float64)
	q_value: tensor([[-25.5569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21139818117938813, distance: 1.016214951731239 entropy 9.870849235567848
epoch: 24, step: 105
	action: tensor([[-3510.2478, -4682.8366, -1215.4715, -2470.7906,  -256.8506, 11096.9613,
          -535.1134]], dtype=torch.float64)
	q_value: tensor([[-26.7279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1439910146905032, distance: 1.6755915970094553 entropy 9.858290259478936
epoch: 24, step: 106
	action: tensor([[ 1076.8848, -6776.5843,   537.9680, -6630.5369,  5602.1167, -8431.3279,
          5159.9684]], dtype=torch.float64)
	q_value: tensor([[-25.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4932889550270265, distance: 0.8145858817560093 entropy 9.800830631267926
epoch: 24, step: 107
	action: tensor([[-1324.6553, -2814.7100, -6919.3426,   465.8528, -1386.1084,  7617.7518,
         -2137.5210]], dtype=torch.float64)
	q_value: tensor([[-27.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6077003866045287, distance: 1.4509727337100897 entropy 9.917555296099447
epoch: 24, step: 108
	action: tensor([[ -8672.7931,  -3209.8274, -11393.9169,   1176.1068,   2527.1408,
          -2047.9156,   3009.9251]], dtype=torch.float64)
	q_value: tensor([[-24.6049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0018922546793620487, distance: 1.1432610459462667 entropy 9.8110909357973
epoch: 24, step: 109
	action: tensor([[-5163.7077, -5976.5991,  4489.6324,  4277.4962,  1927.7220, 16969.6034,
         -4055.2397]], dtype=torch.float64)
	q_value: tensor([[-23.7641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18641336402552844, distance: 1.0321875024024234 entropy 9.830779233092967
epoch: 24, step: 110
	action: tensor([[-1373.2550,  4161.1949,  2755.4665,  2877.4187, -3017.1011, -2634.2009,
         -5815.8806]], dtype=torch.float64)
	q_value: tensor([[-27.2690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30005631335258975, distance: 1.3047814555838613 entropy 10.021972689094964
epoch: 24, step: 111
	action: tensor([[ 4128.1152, -1080.2758, -8630.4325, -3048.3791,  1285.8063, -1134.4325,
          9728.1062]], dtype=torch.float64)
	q_value: tensor([[-34.8996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14805962122954308, distance: 1.0562368001728293 entropy 9.996592210317349
epoch: 24, step: 112
	action: tensor([[  592.9640, -1667.9756,  1280.4928,  2768.0248,  3403.2772,  2504.7829,
          8381.2611]], dtype=torch.float64)
	q_value: tensor([[-27.9401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015405623940362823, distance: 1.1531252327609605 entropy 9.815120247390581
epoch: 24, step: 113
	action: tensor([[-8527.3566, -2882.2387,  -951.1895,  1180.3373,  1222.2620, -8998.6753,
          6996.9054]], dtype=torch.float64)
	q_value: tensor([[-23.7140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20751570746451065, distance: 1.018713413941631 entropy 9.816579208782523
epoch: 24, step: 114
	action: tensor([[-8203.2189, -5850.2345,  4384.6573, -6787.0003, -7152.2290, -1160.6316,
         -2211.6622]], dtype=torch.float64)
	q_value: tensor([[-28.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7362233358355017, distance: 1.507854678406478 entropy 10.078211241096605
epoch: 24, step: 115
	action: tensor([[-9702.3980,    -9.7442,  1604.9318,  2469.1102, -3161.4414,    15.8574,
         -3940.6021]], dtype=torch.float64)
	q_value: tensor([[-25.1138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.7797369806827
epoch: 24, step: 116
	action: tensor([[ 3784.2547,  -607.1152, -2228.9347,  1204.9754,  2344.8155,  1586.4650,
         -1580.9177]], dtype=torch.float64)
	q_value: tensor([[-27.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04264812459196099, distance: 1.1684915509462732 entropy 9.31433615036473
epoch: 24, step: 117
	action: tensor([[-1851.3594,   367.6924,  2205.9114,  3331.7622,  1881.6890,  5051.1762,
         -2929.2472]], dtype=torch.float64)
	q_value: tensor([[-27.1726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.664628364382821
epoch: 24, step: 118
	action: tensor([[ 1111.4498, -2005.6060, -1048.1614,  -225.6955,  5418.9571,  2260.1076,
          3764.2696]], dtype=torch.float64)
	q_value: tensor([[-27.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29698506641831857, distance: 0.959486720783132 entropy 9.31433615036473
epoch: 24, step: 119
	action: tensor([[-2517.6706, -2441.1935,  -216.2248,  1076.6224,  7524.3467, -3704.7088,
         -1187.7081]], dtype=torch.float64)
	q_value: tensor([[-25.0807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6662935046976606, distance: 1.4771766837345264 entropy 9.717152970598994
epoch: 24, step: 120
	action: tensor([[ 8212.9970,   304.0435,  -623.9204, -1649.3733,  2563.0192,  4435.0200,
         -4651.2517]], dtype=torch.float64)
	q_value: tensor([[-27.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.912933180144009
epoch: 24, step: 121
	action: tensor([[  583.1364, -2088.4560, -1187.5497,  1698.4159, -4818.1967,  -614.4277,
          2020.2488]], dtype=torch.float64)
	q_value: tensor([[-27.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19818959300465988, distance: 1.0246900937976495 entropy 9.31433615036473
epoch: 24, step: 122
	action: tensor([[-5935.6529, -5322.7596, -7408.4414,  2396.5580,  -174.6412, -5280.1071,
          3190.9046]], dtype=torch.float64)
	q_value: tensor([[-26.6454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5672075380963337, distance: 1.4325835145869534 entropy 9.68126951746707
epoch: 24, step: 123
	action: tensor([[-2963.3608,  2276.9877, -1849.2370,   989.6657,  -830.7267,  6649.6830,
         -1815.5273]], dtype=torch.float64)
	q_value: tensor([[-23.0556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17424555271588105, distance: 1.2400413158644308 entropy 9.732953689225372
epoch: 24, step: 124
	action: tensor([[3350.6388, 1996.2116, -333.0347,  575.4915, 2693.7352,  608.2156,
         4478.9822]], dtype=torch.float64)
	q_value: tensor([[-26.4859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35843998403618904, distance: 0.9165904711772791 entropy 9.75446188156923
epoch: 24, step: 125
	action: tensor([[  850.0131, -2715.4232,  3439.0435, -1512.4167,   813.9285, -4747.7375,
          4930.9841]], dtype=torch.float64)
	q_value: tensor([[-23.7968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.269135181830056, distance: 1.2891713191271628 entropy 9.684143213602301
epoch: 24, step: 126
	action: tensor([[ -510.3629,  2872.3716,   491.0162,  3125.6615, -1767.4595, -1749.2932,
          7564.6188]], dtype=torch.float64)
	q_value: tensor([[-29.6183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5862951364009826, distance: 0.736040999765255 entropy 9.753896222332175
epoch: 24, step: 127
	action: tensor([[-5012.9175, -2614.2030, -6476.2367,  3747.4146, -5622.8438,  4528.0303,
         -7604.0458]], dtype=torch.float64)
	q_value: tensor([[-27.0094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23802521685382805, distance: 0.9989114535020267 entropy 9.709648340048677
LOSS epoch 24 actor 311.45760599566364 critic 184.903864481346
epoch: 25, step: 0
	action: tensor([[-1499.6037,  6597.9746,  2201.8113, -4141.5471, 10336.3114,   345.5062,
          1239.6552]], dtype=torch.float64)
	q_value: tensor([[-19.4134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.839205101114615
epoch: 25, step: 1
	action: tensor([[ -880.2430, -2400.6696, -2054.6090,  4482.8397,  -883.8058,  -523.9327,
          5455.7485]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11744793032034595, distance: 1.2096795560601608 entropy 9.381209324632575
epoch: 25, step: 2
	action: tensor([[-8157.7921,  -105.5145,  5131.8897,  3656.9795,  1061.8538,  3667.3544,
          5835.2404]], dtype=torch.float64)
	q_value: tensor([[-18.5364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07930293230379137, distance: 1.0980321929016628 entropy 9.750623233674139
epoch: 25, step: 3
	action: tensor([[  637.2037, -4678.9233, -2642.7866,  1656.4155,  3609.1227, -4716.0000,
          3607.8429]], dtype=torch.float64)
	q_value: tensor([[-19.0492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2797543392147608, distance: 0.971173936138766 entropy 9.839178060889305
epoch: 25, step: 4
	action: tensor([[-5474.6440, -3902.4394, -7601.1363, -1342.5727, -3180.3142, -2232.2715,
          9763.9694]], dtype=torch.float64)
	q_value: tensor([[-20.7551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5170257844476875, distance: 1.4094613605770392 entropy 9.926216192557444
epoch: 25, step: 5
	action: tensor([[ 1386.8870,   714.4917,   744.7876, -1155.5962,  6407.6899,  1232.3762,
         -1020.6625]], dtype=torch.float64)
	q_value: tensor([[-18.4809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4039868315080223, distance: 0.8834553822148125 entropy 9.681461687787017
epoch: 25, step: 6
	action: tensor([[-3745.1340, -6036.8126,  2064.2270, -3720.6207,   360.7303, -1542.9124,
           -24.1958]], dtype=torch.float64)
	q_value: tensor([[-28.0139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6082118785060056, distance: 1.4512035297552606 entropy 9.909662309569601
epoch: 25, step: 7
	action: tensor([[ -6715.1261, -13098.5420,  14303.5632,  -1383.8960,  -1034.3094,
           4644.5104,   5414.4355]], dtype=torch.float64)
	q_value: tensor([[-27.1776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1692784972372392, distance: 1.2374158508061626 entropy 10.13436213730985
epoch: 25, step: 8
	action: tensor([[ 3755.4727, -9650.3348, -5095.3426,  6457.8996,  1941.6384,   177.5615,
         -3907.8859]], dtype=torch.float64)
	q_value: tensor([[-27.4319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16720842079697773, distance: 1.0442989848730941 entropy 10.091142996748104
epoch: 25, step: 9
	action: tensor([[-5441.9136,  -469.0620,    53.9054, -2419.6799,  5180.1808, -6383.7493,
          3939.0600]], dtype=torch.float64)
	q_value: tensor([[-18.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8980680597871173, distance: 1.576567551526147 entropy 9.531246425167087
epoch: 25, step: 10
	action: tensor([[-9671.2284,  2607.0493, -4063.4498,  1367.9590, -5629.6555, -6725.3805,
          2623.6475]], dtype=torch.float64)
	q_value: tensor([[-23.3061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1627246516065448, distance: 1.0471064760321636 entropy 9.970915320391102
epoch: 25, step: 11
	action: tensor([[ 6673.5056, -9378.3049,   921.7626, -1469.2996,  -295.2618, -1961.9169,
         -3078.1831]], dtype=torch.float64)
	q_value: tensor([[-23.1929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.689813612763563
epoch: 25, step: 12
	action: tensor([[-521.2860, 5320.3977,  516.3220, 1935.7755, 2262.4925, 2455.2479,
         2694.1718]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.381209324632575
epoch: 25, step: 13
	action: tensor([[ 2072.0741, -3886.7130,  3971.4102,   112.2306,  2415.1042, -4653.6266,
          -450.6674]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5119434783812119, distance: 0.7994508221115733 entropy 9.381209324632575
epoch: 25, step: 14
	action: tensor([[-4246.4197, -4458.1961,  1970.7446,   916.6665, -1512.0048, -1702.3402,
         10596.5534]], dtype=torch.float64)
	q_value: tensor([[-21.2636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06146493333393932, distance: 1.1086180497981948 entropy 9.748359562398674
epoch: 25, step: 15
	action: tensor([[ 2733.8924,  4057.5063,  8016.0656, -8866.0447,   828.1191, -4434.5456,
         -2866.1963]], dtype=torch.float64)
	q_value: tensor([[-22.2419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.048807141844978
epoch: 25, step: 16
	action: tensor([[-1083.0560, -3019.3517,  3802.7770, -2265.1488,  1328.3659,   368.2896,
         -3019.5592]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16693063366601946, distance: 1.0444731392709086 entropy 9.381209324632575
epoch: 25, step: 17
	action: tensor([[ 1770.5802, -7907.8287, -6187.6559,  1120.4882,   112.5231,   629.0357,
          4653.4956]], dtype=torch.float64)
	q_value: tensor([[-20.8136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29043019486256916, distance: 0.9639494423887688 entropy 9.954898512825409
epoch: 25, step: 18
	action: tensor([[ 1130.0206, -4039.1331, -7697.3284,   349.7624,  1241.8940, -5963.3595,
          4091.1293]], dtype=torch.float64)
	q_value: tensor([[-20.3900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2670529957110038, distance: 0.9796997118941542 entropy 9.725576966040018
epoch: 25, step: 19
	action: tensor([[  -54.7214, -5522.0555,   245.1852,  2556.4987,  6233.2220, -6937.0727,
          7148.4432]], dtype=torch.float64)
	q_value: tensor([[-22.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34872308664603724, distance: 0.9235056036495927 entropy 9.956259605636498
epoch: 25, step: 20
	action: tensor([[-3084.1107,  4679.8701, -1537.9013,   198.2955, -3706.3553, -8007.0846,
          -786.2558]], dtype=torch.float64)
	q_value: tensor([[-20.4912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.733746579334435
epoch: 25, step: 21
	action: tensor([[  289.9617,   822.5973,  3707.8631,   551.6592, -2537.8399,  -505.0840,
           992.4307]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.381209324632575
epoch: 25, step: 22
	action: tensor([[-2236.4767, -4430.6832,  2306.8194,  5867.7797,  2731.2300,  7160.4829,
          3727.3964]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6151953085653725, distance: 1.4543509385442748 entropy 9.381209324632575
epoch: 25, step: 23
	action: tensor([[-7960.7373, -3413.0719,  4325.6160,    85.3139, -4662.0847, 11120.0722,
         -2567.5374]], dtype=torch.float64)
	q_value: tensor([[-20.0863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17299653556432282, distance: 1.0406635844143024 entropy 9.936391364292163
epoch: 25, step: 24
	action: tensor([[-6485.3668, -4471.2938,  1724.0065,  8667.3706, -2877.7659,  4835.5588,
          3893.5350]], dtype=torch.float64)
	q_value: tensor([[-20.4888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04263015112668134, distance: 1.1196868202193186 entropy 10.001785382252322
epoch: 25, step: 25
	action: tensor([[-4872.3005,  1200.0282, -2713.8198,  2874.9639, -3659.2231, -1897.6969,
         -7448.8656]], dtype=torch.float64)
	q_value: tensor([[-22.5359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13024860373995728, distance: 1.067220749816762 entropy 9.973684340490122
epoch: 25, step: 26
	action: tensor([[-4944.8097,   145.1141, -2625.7309,  5314.8038,  2796.6430,  4788.0489,
          2203.9060]], dtype=torch.float64)
	q_value: tensor([[-24.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24990399805994878, distance: 0.9910946198873926 entropy 10.056706293299843
epoch: 25, step: 27
	action: tensor([[-6757.7639, -4963.0996,  4119.0744,  -585.9178,  4776.7503,   902.2079,
         -1098.0050]], dtype=torch.float64)
	q_value: tensor([[-20.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.475113435869877, distance: 1.3898547082953734 entropy 9.731882601642068
epoch: 25, step: 28
	action: tensor([[ -3073.4006, -11727.8135,  -7019.7554,  -6340.9398,  -5003.8298,
          -5305.0392,   3170.9274]], dtype=torch.float64)
	q_value: tensor([[-21.6629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1113369488745406, distance: 1.20636733745507 entropy 9.863026916907918
epoch: 25, step: 29
	action: tensor([[-3484.9784,    76.6258,  1886.3508, -5569.0863,  2012.3839, -6478.4166,
          2171.0209]], dtype=torch.float64)
	q_value: tensor([[-25.6250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3672270838208189, distance: 1.338064410828041 entropy 9.929534304406365
epoch: 25, step: 30
	action: tensor([[-2434.8727, -8167.3743,   422.9035,  2045.1266, -7001.9943,  1975.5305,
         -2440.8154]], dtype=torch.float64)
	q_value: tensor([[-28.6064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9048731924482272, distance: 1.5793912521962987 entropy 9.922440888320457
epoch: 25, step: 31
	action: tensor([[-4447.4510,  4690.6790, -7020.2698,   356.9881,  4551.2030,   564.5548,
         -3265.6590]], dtype=torch.float64)
	q_value: tensor([[-24.9187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07693570308485675, distance: 1.1875491163493106 entropy 10.029832596178798
epoch: 25, step: 32
	action: tensor([[ -1348.2944, -11745.6177,   4160.5248,   -679.1255,  -1740.4710,
           5803.3837,   5565.0848]], dtype=torch.float64)
	q_value: tensor([[-24.0792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5370199976241083, distance: 1.4187192197224563 entropy 9.984733763739726
epoch: 25, step: 33
	action: tensor([[ -8492.6783,   2345.3370, -11731.1353,  -4660.7238,    696.7098,
            243.9974,  -9286.7911]], dtype=torch.float64)
	q_value: tensor([[-20.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027192454241225072, distance: 1.1286782563795932 entropy 9.96499422921343
epoch: 25, step: 34
	action: tensor([[-4506.8190, -2186.7795, 10191.3173,  3615.8568,  1609.7111,  -806.9208,
          1750.6037]], dtype=torch.float64)
	q_value: tensor([[-26.3539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1084834896353661, distance: 1.661658584618417 entropy 9.705338736576744
epoch: 25, step: 35
	action: tensor([[  596.6647, -2890.0065, -3189.8045,   358.5731, -4529.3893,  5523.2423,
           964.7315]], dtype=torch.float64)
	q_value: tensor([[-19.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17034033097004475, distance: 1.0423334682262826 entropy 9.737292098591125
epoch: 25, step: 36
	action: tensor([[-7392.4316,  7576.8844,  -385.4306,  7243.4687,  8556.1383,  4423.7483,
          1139.0404]], dtype=torch.float64)
	q_value: tensor([[-21.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1774150135167234, distance: 1.037879855288071 entropy 9.88632451121554
epoch: 25, step: 37
	action: tensor([[-9262.7872, -1686.6117, -2554.1720, -4355.0868,  7179.6509, -7349.1170,
         -1727.5832]], dtype=torch.float64)
	q_value: tensor([[-25.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9412700297594847, distance: 1.5944087465849617 entropy 10.064530627245116
epoch: 25, step: 38
	action: tensor([[-2734.8635, -5667.4009, -7788.8786,  -341.8247,  -846.4086, -3256.0619,
         -3687.1716]], dtype=torch.float64)
	q_value: tensor([[-22.6677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5593249849333646, distance: 1.4289762543576447 entropy 9.940637576777528
epoch: 25, step: 39
	action: tensor([[  494.3264, -3160.5266, 16086.9237,  2602.7203,  -662.5088,   549.1253,
          -835.0237]], dtype=torch.float64)
	q_value: tensor([[-21.8140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09566922582812376, distance: 1.088229133144445 entropy 9.936602532778096
epoch: 25, step: 40
	action: tensor([[ 3247.7839, -7172.8046,  1002.0038,  4231.4436,  5681.7897,  3200.5799,
          4491.0047]], dtype=torch.float64)
	q_value: tensor([[-20.7508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10419376787900714, distance: 1.0830879723116396 entropy 9.828719317219054
epoch: 25, step: 41
	action: tensor([[-3316.3318, -4167.1054,  6349.7092,  1420.2305, -4036.1965,  -167.6746,
          2645.3507]], dtype=torch.float64)
	q_value: tensor([[-27.5761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05497225880469325, distance: 1.1753770677316973 entropy 9.956094165206435
epoch: 25, step: 42
	action: tensor([[-1917.7017, -3693.6296,  4668.0546, -4069.6916, -9996.8987,  6521.7562,
          4820.6079]], dtype=torch.float64)
	q_value: tensor([[-20.2335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1523100658532623, distance: 1.6788392404348933 entropy 10.021364655690002
epoch: 25, step: 43
	action: tensor([[-4619.8127,   -24.6888,  3315.4195, -1923.7305,  3363.7087, -2491.9906,
          3431.6732]], dtype=torch.float64)
	q_value: tensor([[-15.7978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21057310555081243, distance: 1.2590767486751824 entropy 9.605748044321464
epoch: 25, step: 44
	action: tensor([[-325.6577, 1446.3112,  273.8915, 5810.4046, 2590.6595, 3966.1615,
         5090.9491]], dtype=torch.float64)
	q_value: tensor([[-25.1022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44491192773301425, distance: 1.375553167756713 entropy 9.937213721559939
epoch: 25, step: 45
	action: tensor([[ 6470.6194, -2285.6184,  5905.7048, -1809.1960, -1422.0929,  7783.5351,
          4320.5242]], dtype=torch.float64)
	q_value: tensor([[-23.0404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.788660641182377
epoch: 25, step: 46
	action: tensor([[  381.3219, -4634.2148, -2057.6975,   526.0422,  1284.6516,   713.4591,
         -1291.3387]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27542403618564215, distance: 1.292361441058667 entropy 9.381209324632575
epoch: 25, step: 47
	action: tensor([[12700.2415,  1256.4249, -3968.7261,  -615.6210, 13362.4489,  5953.9475,
          7348.1440]], dtype=torch.float64)
	q_value: tensor([[-20.5050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39925783623598887, distance: 0.8869532930707958 entropy 9.992264987425514
epoch: 25, step: 48
	action: tensor([[-3171.7571, -6355.2381,  2811.3284,  -251.6024,  1111.4344,  2402.4028,
         -2621.7212]], dtype=torch.float64)
	q_value: tensor([[-21.5514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05570175284541756, distance: 1.1757833733886434 entropy 9.843039016594163
epoch: 25, step: 49
	action: tensor([[ 5979.8022,  1222.6929,  4002.0575,  2187.5326, -3151.7665,    82.7334,
         -1122.6850]], dtype=torch.float64)
	q_value: tensor([[-22.9561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3405069426300381, distance: 0.9293125609140095 entropy 9.831247212949647
epoch: 25, step: 50
	action: tensor([[-11194.4468,  -8186.9761,   1519.9518,   4329.4342,   2757.5055,
            834.5237,   1263.6919]], dtype=torch.float64)
	q_value: tensor([[-19.7051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0669327085793161, distance: 1.182021042388506 entropy 9.679404393529493
epoch: 25, step: 51
	action: tensor([[-8646.3272,  -747.1922, -2708.5419, -3503.5284,  4474.6915, -3256.0723,
          -545.8868]], dtype=torch.float64)
	q_value: tensor([[-17.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49973581923028854, distance: 1.4014063316883762 entropy 9.700167788404611
epoch: 25, step: 52
	action: tensor([[-3861.2375,  2174.8590, -6017.2601, -1343.0243, -5046.3417,  5032.2779,
           545.3365]], dtype=torch.float64)
	q_value: tensor([[-16.7169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08735054366605821, distance: 1.1932775808927236 entropy 9.60426189124704
epoch: 25, step: 53
	action: tensor([[ -122.9451, -4565.5408,  4020.9935,    78.9789, -4148.0801,  2708.2797,
         -4443.3625]], dtype=torch.float64)
	q_value: tensor([[-25.1416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7643657149341223, distance: 1.5200259358356663 entropy 9.649803352806982
epoch: 25, step: 54
	action: tensor([[  266.7090,  -224.6515, -5654.9091,  -642.3821,  3153.2582, -3810.8039,
           837.4823]], dtype=torch.float64)
	q_value: tensor([[-19.6681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.442985033965542, distance: 0.8540634280860403 entropy 9.7116339105269
epoch: 25, step: 55
	action: tensor([[-1775.9026,  2229.0702,  3481.3891,  2972.0622, -1852.3215, -2379.7567,
         -8247.6922]], dtype=torch.float64)
	q_value: tensor([[-24.4425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2226690497457947, distance: 1.0089268347145535 entropy 9.833926824306205
epoch: 25, step: 56
	action: tensor([[-6062.5354, -5194.6040,  3040.3077,  1481.5241,  5629.4980,  -141.5786,
          3758.9551]], dtype=torch.float64)
	q_value: tensor([[-25.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8879596823651521, distance: 1.5723638522931476 entropy 9.776228549870893
epoch: 25, step: 57
	action: tensor([[-1651.0157, -6084.3779, -4471.4642,  7132.5322, -6723.4077, -4169.2585,
         -1048.4264]], dtype=torch.float64)
	q_value: tensor([[-18.4301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4549004681592177, distance: 1.380299514039564 entropy 9.879392570102265
epoch: 25, step: 58
	action: tensor([[-2709.2632,  6887.0005,  1198.0836, -4691.4475,  5097.5810,  -393.1458,
          4198.7429]], dtype=torch.float64)
	q_value: tensor([[-17.8629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05496848422489431, distance: 1.1753749650420449 entropy 9.779540679960409
epoch: 25, step: 59
	action: tensor([[-2209.0566,  -265.6514, -2226.3968,  5787.8052,  2212.8464, 10406.1897,
          1688.3949]], dtype=torch.float64)
	q_value: tensor([[-24.1537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23944045524299673, distance: 1.2740002903065946 entropy 9.71552948675195
epoch: 25, step: 60
	action: tensor([[-7690.1476, -7348.6463,  7602.0357,   461.9489, -7370.6236, -2267.8563,
          5881.6513]], dtype=torch.float64)
	q_value: tensor([[-20.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8942229648311117, distance: 1.5749698413861644 entropy 9.982395738562193
epoch: 25, step: 61
	action: tensor([[-4307.9304,  -883.3706, -2408.7118,  4458.7210,  1732.1316,  4122.8630,
          4380.0310]], dtype=torch.float64)
	q_value: tensor([[-17.1361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06392517508974294, distance: 1.1803538910219014 entropy 9.792866443260602
epoch: 25, step: 62
	action: tensor([[ 4528.2570, -5777.9712, -2412.9615, -6486.1030,  1255.5979,  -107.9869,
          8056.6928]], dtype=torch.float64)
	q_value: tensor([[-20.7852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5175075556726016, distance: 1.4096851484750794 entropy 10.020613960537881
epoch: 25, step: 63
	action: tensor([[  174.5588, -7204.4400,  3932.2492,  4489.1291,  -141.5645, -8257.3950,
           353.1223]], dtype=torch.float64)
	q_value: tensor([[-12.8195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.325383987656363, distance: 0.9399072852725254 entropy 9.458282806149954
epoch: 25, step: 64
	action: tensor([[-1831.0130, -6606.9838,  -785.8639, 12217.1122,   -60.4499,  4188.1432,
          8923.9182]], dtype=torch.float64)
	q_value: tensor([[-24.2069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24478137489450513, distance: 1.2767422609702705 entropy 9.903335681985842
epoch: 25, step: 65
	action: tensor([[-2420.8986, -5818.9647,  4464.1561,  6552.9010, -1718.2822,  3863.7747,
           747.0716]], dtype=torch.float64)
	q_value: tensor([[-20.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6870369611001994, distance: 1.486342828634938 entropy 9.905505429872804
epoch: 25, step: 66
	action: tensor([[-6294.1756,  2643.9748,   344.3939,  7515.2972,  -812.6264,  1087.3804,
         -5623.3793]], dtype=torch.float64)
	q_value: tensor([[-22.4103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19587792691815542, distance: 1.251411432465827 entropy 10.094817410232798
epoch: 25, step: 67
	action: tensor([[-9504.5569,  -965.0957, -7727.0955, -6663.4004,  -778.9416,  1963.3854,
          4813.6556]], dtype=torch.float64)
	q_value: tensor([[-22.3160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8953073395674125, distance: 1.575420583771374 entropy 9.901829361498699
epoch: 25, step: 68
	action: tensor([[-5973.2140,  1368.7528,   148.5099,  6257.4575, -2445.5698, -4071.9062,
         -4478.6043]], dtype=torch.float64)
	q_value: tensor([[-16.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39062766661293036, distance: 0.8933014924223549 entropy 9.562603473978557
epoch: 25, step: 69
	action: tensor([[-1901.5794,   981.7956,   662.5534,  3781.5974,  -850.3589,   544.0193,
         -1039.5916]], dtype=torch.float64)
	q_value: tensor([[-22.4595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.118874303890246, distance: 1.210451361454463 entropy 9.751635433705262
epoch: 25, step: 70
	action: tensor([[-7844.8176, -3100.1169,  2396.9986,  7325.8762, -4834.1203,  8779.0894,
         -1483.6493]], dtype=torch.float64)
	q_value: tensor([[-26.5264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19106283123410694, distance: 1.2488895431722855 entropy 10.154262749709003
epoch: 25, step: 71
	action: tensor([[    49.3229,  -3141.9275, -10415.4636,   2954.4633,  -1913.2307,
           1718.7299,   2391.1061]], dtype=torch.float64)
	q_value: tensor([[-26.0725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09536599773373888, distance: 1.1976676553633272 entropy 10.115970509026756
epoch: 25, step: 72
	action: tensor([[-6714.5608, -8633.4595, -8966.0010, 16432.3071,  3260.7444, -8647.1532,
         -3853.3975]], dtype=torch.float64)
	q_value: tensor([[-23.8263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5073222088335432, distance: 1.4049463562794235 entropy 10.105143306376116
epoch: 25, step: 73
	action: tensor([[ -855.9033, -4558.7602, -1985.4595,  6193.9398,  4424.8868, -1223.4871,
          3531.6249]], dtype=torch.float64)
	q_value: tensor([[-19.3862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5727003718736463, distance: 1.4350918168087683 entropy 9.824030038103897
epoch: 25, step: 74
	action: tensor([[  -87.0468, -3577.3554,  -265.7446,  1085.8601, -8104.5057,  2638.4406,
          4528.6599]], dtype=torch.float64)
	q_value: tensor([[-18.0189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06923192853156812, distance: 1.1832939735471342 entropy 9.786417368185852
epoch: 25, step: 75
	action: tensor([[-8733.0145,  -950.8963, -1469.6615,  -152.6866, -6508.3900,  6895.0970,
         -4930.3134]], dtype=torch.float64)
	q_value: tensor([[-19.4995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4232612163752518, distance: 1.3652085540785068 entropy 9.896654002020162
epoch: 25, step: 76
	action: tensor([[-6919.9970, 11070.6626, -7724.9713,  -793.6391, -8384.1249,  -815.3070,
          2352.0571]], dtype=torch.float64)
	q_value: tensor([[-20.7594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6185456617128656, distance: 0.7067697813226822 entropy 9.907412114152987
epoch: 25, step: 77
	action: tensor([[  2597.9435,    563.1644, -14063.7335,  -7503.9254,   1962.3439,
           3265.8071,   8425.7994]], dtype=torch.float64)
	q_value: tensor([[-29.7929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08159046751029486, distance: 1.0966672763044416 entropy 10.012516685840923
epoch: 25, step: 78
	action: tensor([[-1272.1606,  3085.1423,  4039.3258,  9365.5671, -1490.0179,  1150.3542,
          5583.7168]], dtype=torch.float64)
	q_value: tensor([[-23.1650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.052836481630549326, distance: 1.1741866974076338 entropy 9.732404041654585
epoch: 25, step: 79
	action: tensor([[-1489.9403, -3955.7180,   709.7629, -3629.4519,  3930.9460,  2717.0396,
         -3276.6760]], dtype=torch.float64)
	q_value: tensor([[-20.5302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1269035821239517, distance: 1.21478682457594 entropy 9.597605617367034
epoch: 25, step: 80
	action: tensor([[ 2129.1606, -5981.5508,  3384.6556,  7535.4440, -1659.0372, 10381.1400,
           260.5979]], dtype=torch.float64)
	q_value: tensor([[-26.3089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02722667097032072, distance: 1.1286584066066843 entropy 10.011018437289335
epoch: 25, step: 81
	action: tensor([[-5449.7311, -7245.9934, -3092.3985, -1515.9444,  4681.1275, -6415.6107,
         -2390.5842]], dtype=torch.float64)
	q_value: tensor([[-17.8937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2419575718598609, distance: 0.9963305573847037 entropy 9.82760475812499
epoch: 25, step: 82
	action: tensor([[ 4004.7228, -8194.1615,  2875.9692,   564.3335, -5419.4163, -1341.4023,
         -4164.3342]], dtype=torch.float64)
	q_value: tensor([[-25.5323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10201530861528352, distance: 1.2012973168071461 entropy 9.914027817285557
epoch: 25, step: 83
	action: tensor([[-4619.9544,  3970.8687,  1207.3364,  6623.5138, -1405.3084, -1847.9923,
         13722.5782]], dtype=torch.float64)
	q_value: tensor([[-20.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.88009999153129
epoch: 25, step: 84
	action: tensor([[-5446.4782,   937.3721,  4705.5856,  3056.6427,  1504.2281,  -636.7510,
          4514.9114]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.381209324632575
epoch: 25, step: 85
	action: tensor([[-1708.0702, -5921.4260, -2574.5547,  2135.3550,  1160.4349, -1075.5012,
           326.0063]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.381209324632575
epoch: 25, step: 86
	action: tensor([[-2215.4612,  1501.2966, -2718.8066,  4004.3439,  4282.3575,  2331.2967,
           -92.8627]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08982686640590853, distance: 1.091738684287728 entropy 9.381209324632575
epoch: 25, step: 87
	action: tensor([[-1161.4344, -5046.6081,  1570.1006,   834.0791, -5295.1562,  1519.2107,
           931.6342]], dtype=torch.float64)
	q_value: tensor([[-23.8510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16174480911485012, distance: 1.2334230596765154 entropy 9.712879696973443
epoch: 25, step: 88
	action: tensor([[-10535.9691,  -7660.1043,   9620.4988,   -541.7617,   1914.7259,
          -3360.5219,   2947.3111]], dtype=torch.float64)
	q_value: tensor([[-18.6320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33461020280695153, distance: 1.3220074835186961 entropy 9.840731261357693
epoch: 25, step: 89
	action: tensor([[   32.3747,   -96.7657, -7870.4516, -3531.1135, -2642.3736,  -250.5536,
          6238.2521]], dtype=torch.float64)
	q_value: tensor([[-19.2781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23252825014116363, distance: 1.2704428530263878 entropy 9.765418357632765
epoch: 25, step: 90
	action: tensor([[-2125.3911, -4846.2165, -7723.7558,  -174.5881, -2812.1672,  6413.3336,
          2007.3826]], dtype=torch.float64)
	q_value: tensor([[-22.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07498433906767654, distance: 1.1006043781265225 entropy 9.88948232044467
epoch: 25, step: 91
	action: tensor([[-3916.0335, -9230.2982,  5792.6264,  8147.3336, -8178.3877,  1912.3723,
          4431.9440]], dtype=torch.float64)
	q_value: tensor([[-28.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5604561553035585, distance: 1.4294944665886515 entropy 10.170963234005097
epoch: 25, step: 92
	action: tensor([[ -5327.6402, -12320.0946,  -7666.2336,   5779.7675,   2530.5451,
            788.2692,  -6934.7282]], dtype=torch.float64)
	q_value: tensor([[-18.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3714044380764281, distance: 1.3401069779032453 entropy 9.890493585077321
epoch: 25, step: 93
	action: tensor([[ 3436.0941, -8917.3277, -1523.1811, 10024.5873,   814.6355,  1060.5434,
          4117.6175]], dtype=torch.float64)
	q_value: tensor([[-19.6723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20691442713675245, distance: 1.257172674177413 entropy 9.931112773253213
epoch: 25, step: 94
	action: tensor([[ 1381.9513, -6669.0117,  2010.7122,  1549.1582, -6227.6395,  -685.0742,
         -2139.8447]], dtype=torch.float64)
	q_value: tensor([[-20.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17661138160295975, distance: 1.0383867146239534 entropy 10.016254820976384
epoch: 25, step: 95
	action: tensor([[-3824.8456, -6099.1022, -2205.8711, -3796.2862,  2383.7758, -3417.9941,
          6771.0118]], dtype=torch.float64)
	q_value: tensor([[-20.8496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3791638004323601, distance: 1.343892771646238 entropy 9.850803574967268
epoch: 25, step: 96
	action: tensor([[ 2078.6657, -3399.6997,   284.0028,  1342.7445,  1232.7283, -1590.7865,
          7358.6655]], dtype=torch.float64)
	q_value: tensor([[-23.5713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19061897265695626, distance: 1.0295162435040002 entropy 9.925220502564555
epoch: 25, step: 97
	action: tensor([[-4474.8623, -1903.7710, -2572.8993,  6266.2485, -2439.2178,  8605.4912,
          -265.8453]], dtype=torch.float64)
	q_value: tensor([[-23.2903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3730595679634905, distance: 0.9060868607194156 entropy 9.81816121080501
epoch: 25, step: 98
	action: tensor([[  738.1957, -6048.9232,  2298.6589,  1581.5837,  2417.8696, -2704.9770,
         -9061.5492]], dtype=torch.float64)
	q_value: tensor([[-26.7878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5731838224762338, distance: 0.7476134901121224 entropy 10.102295802355231
epoch: 25, step: 99
	action: tensor([[-2513.7238,  -104.9294,  1397.3790, -1411.3343, -2135.3963,  5861.2518,
         -2261.9336]], dtype=torch.float64)
	q_value: tensor([[-19.6145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5891621005681245, distance: 1.4425829430987926 entropy 9.83648374073991
epoch: 25, step: 100
	action: tensor([[ 4216.7700,   -86.3895, -6230.4182, -1145.3620, -2174.5626,  2719.1494,
         -1153.9170]], dtype=torch.float64)
	q_value: tensor([[-16.7405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08637337040797277, distance: 1.0938079309800868 entropy 9.708904681435838
epoch: 25, step: 101
	action: tensor([[  -470.8268, -10416.1496,  -7597.3119,   5856.6118,  -4287.5162,
          -6248.2978,   -207.9463]], dtype=torch.float64)
	q_value: tensor([[-22.1803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.258839255638178, distance: 1.2839314350698479 entropy 9.837233104986185
epoch: 25, step: 102
	action: tensor([[-4569.1146,  2205.1894, -9676.0362,  4045.1222,  -128.2305,  4387.6044,
          2733.9848]], dtype=torch.float64)
	q_value: tensor([[-19.9133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05676940314413015, distance: 1.1763777686539154 entropy 9.905269574282594
epoch: 25, step: 103
	action: tensor([[  3425.4635, -10547.2870,   -836.2138, -11902.9311,  -6064.8040,
          -4208.5011,   1801.9660]], dtype=torch.float64)
	q_value: tensor([[-21.9722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.9096903318659
epoch: 25, step: 104
	action: tensor([[-1503.4976, -3171.5200,   694.7317,  1918.8173,  2869.3656,  1157.0073,
          5579.2135]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1453101964579424, distance: 1.224667680715665 entropy 9.381209324632575
epoch: 25, step: 105
	action: tensor([[-6658.4264, -5668.4710, -8453.1002, -3943.7271, -1056.2260, -2826.8314,
         -3197.8187]], dtype=torch.float64)
	q_value: tensor([[-21.1488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36955605271024994, distance: 1.3392035721460593 entropy 10.025095780785936
epoch: 25, step: 106
	action: tensor([[  500.0226, -1327.8968, -3967.2088,   248.6745,  -392.7470,  2098.4782,
          3218.0832]], dtype=torch.float64)
	q_value: tensor([[-21.4906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35392156360500016, distance: 1.3315376346568797 entropy 9.825646792904067
epoch: 25, step: 107
	action: tensor([[-5808.5095,  -330.8909,  2230.9114,   158.6986,  -457.2420,  1599.0878,
           -96.9873]], dtype=torch.float64)
	q_value: tensor([[-19.6914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5034168224774449, distance: 1.4031251076621751 entropy 9.98086398028625
epoch: 25, step: 108
	action: tensor([[-10101.8514,  -7896.6306,    201.0606,  -3380.1364,   3188.9197,
           3957.6745,  11978.5116]], dtype=torch.float64)
	q_value: tensor([[-19.8416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005983162247358398, distance: 1.1409157192668462 entropy 9.96029852217689
epoch: 25, step: 109
	action: tensor([[  4974.2272, -11175.2193,  -1849.7522,   3814.6073,   3089.5634,
          -4670.0619,   -323.9877]], dtype=torch.float64)
	q_value: tensor([[-22.4258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5430461232405663, distance: 0.77355799017912 entropy 10.009553007458267
epoch: 25, step: 110
	action: tensor([[-5384.4406, -5844.4600,  2568.8710,   289.3859,  -253.1824, -1494.4665,
         -3904.0757]], dtype=torch.float64)
	q_value: tensor([[-20.6866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24270298835921822, distance: 1.2756759401873252 entropy 9.955611976533708
epoch: 25, step: 111
	action: tensor([[ -658.8496, -7275.0933, -4141.3431,  3558.4231, -5712.5511, -7352.1908,
         -3568.6004]], dtype=torch.float64)
	q_value: tensor([[-21.7818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5368536645235822, distance: 1.4186424522270373 entropy 9.982260607921676
epoch: 25, step: 112
	action: tensor([[-2586.3093, -4416.5037, -2740.8552,  9264.2741,   -46.4265,  8645.7002,
          1349.5147]], dtype=torch.float64)
	q_value: tensor([[-18.1489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6083263107137551, distance: 1.4512551589815001 entropy 9.800827531761254
epoch: 25, step: 113
	action: tensor([[-10334.8504,  -8315.9633,   1917.2250,  -8468.1072,   -115.6463,
           2251.3501,   1190.3721]], dtype=torch.float64)
	q_value: tensor([[-21.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3617068578557445, distance: 1.3353604383829771 entropy 10.042486226875392
epoch: 25, step: 114
	action: tensor([[-3053.2556,  1596.6658, -4410.7749, -3432.0530,  7104.9295,  4995.0858,
          2509.0905]], dtype=torch.float64)
	q_value: tensor([[-23.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22758177152306813, distance: 1.267890969866656 entropy 9.974088238440185
epoch: 25, step: 115
	action: tensor([[-1611.2117,   278.6609, -1321.8880,  5694.8278, -1995.6818,  2539.4985,
          2344.6122]], dtype=torch.float64)
	q_value: tensor([[-25.9025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5351687129137199, distance: 0.7801971675494003 entropy 9.846364912938624
epoch: 25, step: 116
	action: tensor([[-8385.2934,  -223.4394,  4220.2741,  2274.2319,  1475.4227,  5280.6408,
          4817.4852]], dtype=torch.float64)
	q_value: tensor([[-22.7116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40585390412630096, distance: 1.3568342223493512 entropy 9.774378381022727
epoch: 25, step: 117
	action: tensor([[-2066.2625, -1392.5959, -7358.0732, -2865.0453,  2332.3601,  2222.5232,
           848.4582]], dtype=torch.float64)
	q_value: tensor([[-20.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6245040091850673, distance: 1.4585357785085 entropy 9.903719934380266
epoch: 25, step: 118
	action: tensor([[ 1931.6515, -1095.1207,  3205.1154, -1488.5552, -6909.4999,  1918.8716,
         -1737.4754]], dtype=torch.float64)
	q_value: tensor([[-16.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02724628483188707, distance: 1.1286470280765215 entropy 9.59043995284686
epoch: 25, step: 119
	action: tensor([[-1075.0500, -9456.1063, -5888.3511,  8525.4709,  -263.8893, -6503.2298,
          1831.8025]], dtype=torch.float64)
	q_value: tensor([[-20.2700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6857688324427307, distance: 1.4857840891637073 entropy 9.80041714711805
epoch: 25, step: 120
	action: tensor([[-10428.1403,   1054.1996,  -4978.7200,   3688.1661,  -9736.9339,
           2008.3907,  -3827.3213]], dtype=torch.float64)
	q_value: tensor([[-19.3456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21660129567032405, distance: 1.012856962265433 entropy 9.880752321204412
epoch: 25, step: 121
	action: tensor([[-4593.1329,  5807.1512, -1443.4879,  3162.0273, -3979.2373,  1681.3224,
          1137.1012]], dtype=torch.float64)
	q_value: tensor([[-22.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13643932496626088, distance: 1.0634158322264484 entropy 9.846574865362403
epoch: 25, step: 122
	action: tensor([[-3936.1770, -5261.1537, -1780.4989, -2644.2105, -4633.3388, 12479.0966,
         11194.0216]], dtype=torch.float64)
	q_value: tensor([[-25.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10035705847114929, distance: 1.2003931545060316 entropy 10.018299889851033
epoch: 25, step: 123
	action: tensor([[ 1586.9808,   456.6735, -4916.4026,  6780.0657, 12364.0155,  2879.5874,
          5801.5237]], dtype=torch.float64)
	q_value: tensor([[-24.4006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6471278539188532, distance: 0.6797753037959937 entropy 10.040908590961942
epoch: 25, step: 124
	action: tensor([[-6430.2468, -5147.8635, -1750.9025,  2198.7967, -1690.1081,  5306.8439,
         -7058.6916]], dtype=torch.float64)
	q_value: tensor([[-21.6828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14790597917183457, distance: 1.056332038691544 entropy 9.954855084239368
epoch: 25, step: 125
	action: tensor([[ 3298.0134, -5411.5680, -3586.1818,  7971.8601,  -966.8975,  7542.2636,
          8278.5083]], dtype=torch.float64)
	q_value: tensor([[-21.9759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11437979022278444, distance: 1.0769125857599928 entropy 9.999997968163077
epoch: 25, step: 126
	action: tensor([[-610.2256, 2860.2659, 5730.0425, 5270.5724, 4062.3604, 4481.0659,
         5311.8660]], dtype=torch.float64)
	q_value: tensor([[-23.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.898464850173573
epoch: 25, step: 127
	action: tensor([[-1439.6945,   244.4498, -2003.6555,  4193.3334,  2284.2433,  4448.5127,
          -407.1893]], dtype=torch.float64)
	q_value: tensor([[-22.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.039760098946139966, distance: 1.1668721294111246 entropy 9.381209324632575
LOSS epoch 25 actor 204.5933181602127 critic 305.9322460640821
epoch: 26, step: 0
	action: tensor([[-1516.8834, -5558.6165,  2194.0037, -1867.7695, -1147.9153,  2195.1477,
          4330.2574]], dtype=torch.float64)
	q_value: tensor([[-19.5353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.628496164692814
epoch: 26, step: 1
	action: tensor([[  769.6421, -2264.9083,  4351.2684,  1305.4451, -1949.5941,  1679.2762,
          -501.8639]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10511589558640422, distance: 1.202986091238315 entropy 9.446682404733437
epoch: 26, step: 2
	action: tensor([[-1774.4525, -8611.8852,  4372.0879,  6504.1992, -9411.7287,  1107.2092,
         -6298.9932]], dtype=torch.float64)
	q_value: tensor([[-17.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021336357686013496, distance: 1.1320703618027816 entropy 10.020041862264476
epoch: 26, step: 3
	action: tensor([[-4771.4272, -1950.5876,   117.5766, -4558.6031, -6557.0387, -3329.4547,
          -614.0034]], dtype=torch.float64)
	q_value: tensor([[-18.6942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7598444495286962, distance: 1.518077119611086 entropy 10.01341551974709
epoch: 26, step: 4
	action: tensor([[ -8379.2897, -11349.6226,   1299.9056,   5080.0826,     77.2618,
           7217.4102,   1383.4156]], dtype=torch.float64)
	q_value: tensor([[-20.2038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5153693348360566, distance: 1.4086916507229983 entropy 10.021519718460869
epoch: 26, step: 5
	action: tensor([[-8288.7119,   115.7816,   812.7146,  9667.5500, -6798.7251,  1309.7009,
          4070.8928]], dtype=torch.float64)
	q_value: tensor([[-18.4438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1525458578840312, distance: 1.053452108306296 entropy 9.940704204175479
epoch: 26, step: 6
	action: tensor([[-2883.1328,   916.5883,  4042.4470,  -800.1117,  3623.1516,   521.1115,
         -4819.9885]], dtype=torch.float64)
	q_value: tensor([[-17.6759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.567973317957986, distance: 0.7521630209779198 entropy 9.747797639291376
epoch: 26, step: 7
	action: tensor([[ -329.1549,  1899.8419,  8654.6451, -6435.0726, -3147.3607,  1840.8091,
          3823.8182]], dtype=torch.float64)
	q_value: tensor([[-22.7275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3509987651533044, distance: 0.9218907445409515 entropy 9.889022360130651
epoch: 26, step: 8
	action: tensor([[-8331.1739, -3352.5392,  6934.4560, -8025.4246, -4629.8808,  5685.2722,
          5031.4421]], dtype=torch.float64)
	q_value: tensor([[-26.4618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5866247507541997, distance: 1.4414308265821207 entropy 9.9696443075326
epoch: 26, step: 9
	action: tensor([[-1256.2018,  -261.1542, -1215.8265, -7795.2821,  -675.3036, 11085.7045,
         -1030.7186]], dtype=torch.float64)
	q_value: tensor([[-21.7523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19863900182489935, distance: 1.024402887927273 entropy 9.965819750090784
epoch: 26, step: 10
	action: tensor([[-6644.2133, -3832.7897,  -283.7272,  7901.0432, -3278.6729,  4390.9592,
          3693.6012]], dtype=torch.float64)
	q_value: tensor([[-25.7222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8214802939180978, distance: 1.544432499172794 entropy 10.127135345990975
epoch: 26, step: 11
	action: tensor([[-8664.7005, -4672.9995,  1596.7125, -7206.8375, -4998.3648, -5648.6868,
          5688.3476]], dtype=torch.float64)
	q_value: tensor([[-17.3624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29505373852452976, distance: 1.3022686574507862 entropy 9.815648927014143
epoch: 26, step: 12
	action: tensor([[ -8141.5594, -11048.1182,    508.2854,   5514.4266,   2287.4709,
          -9570.2362,   9556.8961]], dtype=torch.float64)
	q_value: tensor([[-21.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8201108929674765, distance: 1.5438518327233823 entropy 9.974066021618397
epoch: 26, step: 13
	action: tensor([[-2034.8704, -3139.4138, -1088.9930,  -552.8032,  2168.4295,  3546.0745,
          2398.2515]], dtype=torch.float64)
	q_value: tensor([[-15.2875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8262257686950656, distance: 1.5464430337162078 entropy 9.678787690987653
epoch: 26, step: 14
	action: tensor([[-1279.4252, -7701.8892, -3631.6693,   696.5505, -2968.4533,  -272.6938,
          2382.0919]], dtype=torch.float64)
	q_value: tensor([[-19.9487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.425508447147657, distance: 1.7822067696843766 entropy 9.889492983066239
epoch: 26, step: 15
	action: tensor([[ -2602.2772,   -462.5180,  -3061.9275,  -4240.3406,  -5835.5109,
           6261.8018, -12345.7910]], dtype=torch.float64)
	q_value: tensor([[-17.9852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48853568432818717, distance: 1.3961636235041146 entropy 9.91746453029394
epoch: 26, step: 16
	action: tensor([[ -5814.5504,  -7464.5093,  -3862.5290,  -1189.5946, -12763.4662,
           2312.2371,  -1572.1640]], dtype=torch.float64)
	q_value: tensor([[-20.1067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03870982859658967, distance: 1.166282646877267 entropy 9.937018190479861
epoch: 26, step: 17
	action: tensor([[-2050.7412, -9596.2184, -4821.2188,  1426.5664,  4837.6698, -4515.4722,
         -9445.4288]], dtype=torch.float64)
	q_value: tensor([[-24.8317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0016131841713158, distance: 1.6189997054805296 entropy 10.052243154616747
epoch: 26, step: 18
	action: tensor([[  2370.0587, -14000.7058,   4096.7649,  -1613.6255,   9392.1995,
            757.6669,  -1086.9699]], dtype=torch.float64)
	q_value: tensor([[-19.3346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.313865051750259, distance: 1.3116926156779114 entropy 9.921989756364214
epoch: 26, step: 19
	action: tensor([[-5828.7935, -4423.3109,  -300.3637,  5215.0726,  1945.2898, -2652.4506,
           265.0263]], dtype=torch.float64)
	q_value: tensor([[-17.3060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.732273390284631, distance: 1.5061385008015247 entropy 9.692361136687069
epoch: 26, step: 20
	action: tensor([[ 3647.5942,  1002.2804, -4603.2689, -3773.9559, -8187.3866,  4908.6971,
          -522.5666]], dtype=torch.float64)
	q_value: tensor([[-15.5284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.761212954175203
epoch: 26, step: 21
	action: tensor([[  816.7876, -4569.9197,  3788.8214, -6036.4519, -4315.4894,  1134.0354,
         10852.2382]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4493672444579859, distance: 0.8491564536871093 entropy 9.446682404733437
epoch: 26, step: 22
	action: tensor([[-10965.0862, -10574.0506,  -6598.0592,  -1663.8349,  -3477.5118,
          -2943.3999,  -4815.8554]], dtype=torch.float64)
	q_value: tensor([[-17.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2685037950652236, distance: 1.2888506019249897 entropy 9.787379033319057
epoch: 26, step: 23
	action: tensor([[-11016.3227,  -2486.6134,  -3487.6073,     70.0081,   9386.5114,
            433.3192,   9327.1045]], dtype=torch.float64)
	q_value: tensor([[-24.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2703999984442902, distance: 1.289813551441914 entropy 10.190139295377248
epoch: 26, step: 24
	action: tensor([[ 1742.4308, -2220.1322,  3618.9991,   806.5296,  5712.2140, -3783.9704,
          1439.6213]], dtype=torch.float64)
	q_value: tensor([[-16.7404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13237664201614918, distance: 1.217733193764489 entropy 9.898925064965718
epoch: 26, step: 25
	action: tensor([[-1182.6372,  1574.1762, -1530.6744, -6233.4054,   197.3549, -7697.9507,
          3379.3483]], dtype=torch.float64)
	q_value: tensor([[-16.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2770763422942465, distance: 0.9729777566127179 entropy 9.628110522845333
epoch: 26, step: 26
	action: tensor([[-1565.8946, -1120.5815,   220.1195,  4087.3905, -4673.9334,  3258.6599,
          3403.0415]], dtype=torch.float64)
	q_value: tensor([[-25.3933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6211711559174473, distance: 1.8526969364336954 entropy 9.87730000156338
epoch: 26, step: 27
	action: tensor([[   63.4921, -9256.9812,  1173.9991,  2585.9126,   496.6067,  -164.1645,
         -7539.6367]], dtype=torch.float64)
	q_value: tensor([[-16.3839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047627994054480594, distance: 1.1712786867666354 entropy 9.613570445495984
epoch: 26, step: 28
	action: tensor([[ 2089.0433,  4656.8101, -3098.6876,  1709.6464, -7311.4446,  3590.6329,
          3343.9134]], dtype=torch.float64)
	q_value: tensor([[-15.7635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.650785954183553
epoch: 26, step: 29
	action: tensor([[ 2487.8275, -1144.4376, -6855.6566,  1785.2347,  4178.6910,   282.5168,
           940.5277]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7687436501631864, distance: 0.5503050858017576 entropy 9.446682404733437
epoch: 26, step: 30
	action: tensor([[ -382.2052,   347.0119,  7238.5232, -7817.4464,  -943.5640,   785.4800,
         -2863.4464]], dtype=torch.float64)
	q_value: tensor([[-16.7362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.703997208156983
epoch: 26, step: 31
	action: tensor([[ 1550.2593, -5561.7594,  2034.9691, -6781.5534,  -502.4884,  4008.5281,
         -2293.0646]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1518293414683327, distance: 1.2281481602606052 entropy 9.446682404733437
epoch: 26, step: 32
	action: tensor([[ 2635.8027,   971.4247,   827.2483, -6597.6882,  4065.1572, -1001.1573,
          3607.3677]], dtype=torch.float64)
	q_value: tensor([[-15.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4460391621209542, distance: 0.8517187832687865 entropy 9.579420062517613
epoch: 26, step: 33
	action: tensor([[-3024.7061, -3365.3254,   782.8171,  7513.1513,  1127.8881,  -573.6684,
          4707.3196]], dtype=torch.float64)
	q_value: tensor([[-24.2323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.868551403270374
epoch: 26, step: 34
	action: tensor([[-1350.4258, -4808.7482,  5183.5601,  2269.0142,  1353.9318, -3674.0354,
         -1423.2274]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.446682404733437
epoch: 26, step: 35
	action: tensor([[ 948.0608,  576.2573, 1942.0031, 2703.6647, 2605.7936, -792.0814,
         1173.9766]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30928462624048514, distance: 0.9510563607857591 entropy 9.446682404733437
epoch: 26, step: 36
	action: tensor([[-4063.2049, -7740.7015, -2671.4131,  9463.5260, -2703.1694, 13171.6027,
          3516.2636]], dtype=torch.float64)
	q_value: tensor([[-18.9995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14006034679255164, distance: 1.2218576534339016 entropy 9.727670297004861
epoch: 26, step: 37
	action: tensor([[-2124.6170,  3224.9964,  1817.1039, -2172.1559,  6721.4121,   880.2825,
          5118.3214]], dtype=torch.float64)
	q_value: tensor([[-19.7688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030147886318036887, distance: 1.1614659467507749 entropy 10.056370196845648
epoch: 26, step: 38
	action: tensor([[-4237.1515, -2589.8147, -5384.6292,  6545.5090, -4740.1303, -1491.5356,
          3679.4400]], dtype=torch.float64)
	q_value: tensor([[-23.6568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1822633348334779, distance: 1.0348166997086188 entropy 9.845758585622134
epoch: 26, step: 39
	action: tensor([[-6421.8990,  -629.9528, -6827.9111,  2550.0631, 11502.5258,  2873.4285,
          3953.9659]], dtype=torch.float64)
	q_value: tensor([[-21.6783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29320543191072646, distance: 1.3013390237484528 entropy 10.126147255098584
epoch: 26, step: 40
	action: tensor([[-9621.2076,  1684.0056,  8667.6869, -2276.6817,  4671.8977, -8116.6647,
           957.6086]], dtype=torch.float64)
	q_value: tensor([[-18.8906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19131148332579695, distance: 1.2490198985084353 entropy 10.018554989926695
epoch: 26, step: 41
	action: tensor([[-10945.3575, -12680.6775,   5657.2377,  -5892.4780,  -7099.4159,
          -1915.2756,   2165.9863]], dtype=torch.float64)
	q_value: tensor([[-29.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13815108575352975, distance: 1.2208341011530157 entropy 10.222444604474848
epoch: 26, step: 42
	action: tensor([[-2360.3902, -8155.6167, -2449.9697,  4349.5147,  3796.3831, -8665.5051,
          1227.2831]], dtype=torch.float64)
	q_value: tensor([[-23.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9740569834442105, distance: 1.6078167017534009 entropy 9.986441769595327
epoch: 26, step: 43
	action: tensor([[-3053.8404, -9036.1064, -4213.1736,  5695.3417, -5807.3656, -3518.6127,
         -2895.3521]], dtype=torch.float64)
	q_value: tensor([[-18.7019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06643036007292635, distance: 1.1817427416059867 entropy 9.963858239910492
epoch: 26, step: 44
	action: tensor([[-3578.0862,  4514.9111,  -399.3269, 10646.5071, -6797.8598,  4304.0870,
         -3480.8006]], dtype=torch.float64)
	q_value: tensor([[-18.6148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05457941754805706, distance: 1.112677278881943 entropy 9.977952383549468
epoch: 26, step: 45
	action: tensor([[1277.5650, 7771.6825, -442.7017, 3800.2978,  745.9067, 9933.9226,
          912.3819]], dtype=torch.float64)
	q_value: tensor([[-20.9599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8424737457442002, distance: 0.4541853967368069 entropy 9.93791652755125
epoch: 26, step: 46
	action: tensor([[-9226.6988, -5732.2367,   -69.5148,  -607.3242, -6576.0637, -3860.1834,
          9068.5140]], dtype=torch.float64)
	q_value: tensor([[-20.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9333936097694935, distance: 1.5911709185295384 entropy 10.026533771747314
epoch: 26, step: 47
	action: tensor([[ -952.2731,  4843.6784, 11828.2667,  3462.1156,  2546.7181, -1463.1703,
          6313.1664]], dtype=torch.float64)
	q_value: tensor([[-18.2250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020487556291921005, distance: 1.1325611814269751 entropy 9.9489738722391
epoch: 26, step: 48
	action: tensor([[-1961.1465, 10547.0277,  3975.8002, -1242.1758, -3014.8555,  6361.8820,
          -423.3414]], dtype=torch.float64)
	q_value: tensor([[-25.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007813256975074956, distance: 1.1398649595104875 entropy 10.156788734723092
epoch: 26, step: 49
	action: tensor([[-10621.2115,  -2523.0396,  -6398.2108,  -6187.2782, -10750.6474,
          -7342.7527,  -8550.0253]], dtype=torch.float64)
	q_value: tensor([[-24.6370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1841694988096304, distance: 1.0336099046334544 entropy 9.903168718652045
epoch: 26, step: 50
	action: tensor([[-9215.1263,   136.1005,  7261.4443,  7027.7484, -7530.7165, -5394.0922,
          7120.9952]], dtype=torch.float64)
	q_value: tensor([[-22.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6803607278479742, distance: 1.4833989072685587 entropy 10.007304978939164
epoch: 26, step: 51
	action: tensor([[ 3018.7356, -5708.8507,   437.3417,  5160.9089,  1891.7500,  8807.9188,
          6693.2298]], dtype=torch.float64)
	q_value: tensor([[-21.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4749552081617757, distance: 0.8291915557336011 entropy 9.952881650658108
epoch: 26, step: 52
	action: tensor([[  452.0605,  -705.4782,  1457.5599, -2334.0800,  1360.1974, -4574.7525,
           -14.6839]], dtype=torch.float64)
	q_value: tensor([[-17.5139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19759989430661373, distance: 1.2523120739397937 entropy 9.946376376820226
epoch: 26, step: 53
	action: tensor([[ 4237.5711, -3983.2385, -3572.2634,    84.5383, -7775.4921,   362.6072,
         -4887.1874]], dtype=torch.float64)
	q_value: tensor([[-21.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48864872328099906, distance: 1.3962166346208493 entropy 9.993212418902896
epoch: 26, step: 54
	action: tensor([[ -439.0646, -5667.1281,  1888.5732,  2902.8988,  9745.6521,  1323.5936,
          6663.7673]], dtype=torch.float64)
	q_value: tensor([[-18.2420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12049318706499657, distance: 1.0731892092626485 entropy 9.929081070270058
epoch: 26, step: 55
	action: tensor([[  428.2147, -6570.3634, -5000.4773,  4254.5322, -5780.3035,  8185.9423,
         -4659.3102]], dtype=torch.float64)
	q_value: tensor([[-19.4357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37760100957240317, distance: 0.9027991323787788 entropy 10.056880972509564
epoch: 26, step: 56
	action: tensor([[-654.6480, -387.8284, 1082.9558, 6779.4885, 7729.4910, -570.8213,
         5976.4231]], dtype=torch.float64)
	q_value: tensor([[-23.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2868969335603122, distance: 0.9663464270893849 entropy 9.979352512486676
epoch: 26, step: 57
	action: tensor([[-8264.1134, -6845.5508,  5186.4656,  9655.1308,  7532.0465,  5985.5715,
           553.0872]], dtype=torch.float64)
	q_value: tensor([[-19.0821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015965358132368657, distance: 1.1351725664696841 entropy 9.85373260354155
epoch: 26, step: 58
	action: tensor([[ -5037.6245, -10070.4888,   6072.3313,   9488.9645,  -1950.7784,
          -3671.1123,    328.0419]], dtype=torch.float64)
	q_value: tensor([[-21.3288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13684702396539616, distance: 1.0631647758926266 entropy 10.053394303989009
epoch: 26, step: 59
	action: tensor([[ -326.0638, -4512.8998,  3841.1536,  9800.6400, -3203.3405,   -94.4449,
         -1409.1820]], dtype=torch.float64)
	q_value: tensor([[-21.7468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08988152825039764, distance: 1.0917059007743513 entropy 10.191114524828418
epoch: 26, step: 60
	action: tensor([[-4599.2391, -1106.2834, -4888.5411,  7972.4673,   601.1485,  4714.6438,
         -3975.0022]], dtype=torch.float64)
	q_value: tensor([[-17.2981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49631387055148957, distance: 1.3998066235576876 entropy 9.82713182982937
epoch: 26, step: 61
	action: tensor([[-5399.8058, -4327.4303,  -441.8081,  4512.4352, -5319.4246,  2904.1482,
         -5732.5547]], dtype=torch.float64)
	q_value: tensor([[-18.4349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.90466280381465, distance: 1.5793040298173229 entropy 9.924908219116702
epoch: 26, step: 62
	action: tensor([[-8638.1694, -8364.8167, -2336.0851, -4256.6857,  8038.5354, -8530.4177,
          8301.7223]], dtype=torch.float64)
	q_value: tensor([[-21.8275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9939482091838996, distance: 1.615896834389751 entropy 10.164383633926823
epoch: 26, step: 63
	action: tensor([[-6316.2010, -3432.2855, -1954.8320,  2688.9425, -2238.7094, -9572.1743,
          3244.3119]], dtype=torch.float64)
	q_value: tensor([[-18.0877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2386935415863718, distance: 1.273616362376268 entropy 9.906298379840605
epoch: 26, step: 64
	action: tensor([[-2843.2061, -5103.9206,  1897.0267,  7720.7125, -1515.5396,  -738.3393,
         -3277.0919]], dtype=torch.float64)
	q_value: tensor([[-18.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3854710130322312, distance: 1.3469622216363273 entropy 10.056946421693764
epoch: 26, step: 65
	action: tensor([[ -3740.4051,  -3309.8575,  -2311.4844,   9172.9117,   3118.8201,
         -11076.3640,   2443.3977]], dtype=torch.float64)
	q_value: tensor([[-19.0445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12753199765557643, distance: 1.21512548909174 entropy 10.069897515815594
epoch: 26, step: 66
	action: tensor([[ 9575.2933,  2912.8424, -2265.3505, -2390.5864,  6234.1937, -2301.6461,
          3599.1519]], dtype=torch.float64)
	q_value: tensor([[-19.4137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.000958824555141
epoch: 26, step: 67
	action: tensor([[-5427.6137,  2262.3348, -2498.5880,  2253.8581,   972.2210,   256.9957,
           265.5022]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1617893675480968, distance: 1.047691151464157 entropy 9.446682404733437
epoch: 26, step: 68
	action: tensor([[ 437.9914,  -33.4189, 6212.3841, -840.7755, -324.4227, 3222.3485,
         6261.5835]], dtype=torch.float64)
	q_value: tensor([[-18.3317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.586511730891406
epoch: 26, step: 69
	action: tensor([[  875.3331, -6566.6600,  1174.8768,  -674.9464, -2808.3239, -2888.6302,
          1910.2818]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3632983639967601, distance: 0.913113315970613 entropy 9.446682404733437
epoch: 26, step: 70
	action: tensor([[-2593.2901, -6660.4350,  6604.5527,  6080.2051, -4014.5910,  9411.6847,
          7720.2436]], dtype=torch.float64)
	q_value: tensor([[-17.9161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3852546945053552, distance: 1.3468570645255105 entropy 9.672348488428538
epoch: 26, step: 71
	action: tensor([[ 3280.4065, -3525.2812,  4031.1677, -3586.9681, -4782.8900,  3283.9089,
         -2503.0798]], dtype=torch.float64)
	q_value: tensor([[-16.3091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5883737169360552, distance: 0.7341896231311384 entropy 9.70045217179819
epoch: 26, step: 72
	action: tensor([[-1510.5173, -2149.0743, -4404.3994,  4414.8438,  9934.5008, -2119.8765,
          9107.7235]], dtype=torch.float64)
	q_value: tensor([[-23.6770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7909761061033387, distance: 1.5314456521282198 entropy 9.979822963963908
epoch: 26, step: 73
	action: tensor([[ 1549.5773, -3732.9975,   665.4519,  4465.1457,  -127.3321, -1973.0093,
          3655.0145]], dtype=torch.float64)
	q_value: tensor([[-19.5462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.658329265355717, distance: 0.6688990574804144 entropy 9.8813377016991
epoch: 26, step: 74
	action: tensor([[ -1046.8074,   -499.8150,  -7622.5626,  -5829.4177,  -3154.8320,
          -4354.4303, -10562.7738]], dtype=torch.float64)
	q_value: tensor([[-18.8158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1030893513144, distance: 1.2018825765499401 entropy 9.899159532830952
epoch: 26, step: 75
	action: tensor([[ 3260.7044, -1260.1272, -4031.2547,  -249.6772,   705.0640,  -571.2856,
          2946.7355]], dtype=torch.float64)
	q_value: tensor([[-18.4816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30652226173863895, distance: 0.9529562337128694 entropy 9.758857852300745
epoch: 26, step: 76
	action: tensor([[ -328.1452,  9597.6773,  4895.9439,  -938.9314, -3769.5681,  2084.1564,
          5018.5251]], dtype=torch.float64)
	q_value: tensor([[-21.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.045753061217870705, distance: 1.170230102566996 entropy 9.919190298542121
epoch: 26, step: 77
	action: tensor([[-3005.6729, -1197.6126,  5870.0546,   950.7955,  1334.5879,  2160.7820,
          3174.3212]], dtype=torch.float64)
	q_value: tensor([[-23.8993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7862318893356768, distance: 1.529415941066149 entropy 9.77411207956624
epoch: 26, step: 78
	action: tensor([[ 2631.9454, -9444.9588, -4805.2667,  2775.9334, -4404.8385,  -467.9581,
          5214.3772]], dtype=torch.float64)
	q_value: tensor([[-17.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19729996867695387, distance: 1.0252583930561194 entropy 9.873298646935599
epoch: 26, step: 79
	action: tensor([[-3540.7055, -5283.6949,  1689.4222,  -296.2093,  3803.5573,  2211.9286,
         -4944.4838]], dtype=torch.float64)
	q_value: tensor([[-18.2122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5459142708907152, distance: 1.422818149548347 entropy 9.6812412600233
epoch: 26, step: 80
	action: tensor([[ -8205.5426,  -4734.8468,   9622.1695,   5603.2472,  -3128.7749,
         -11003.2363,   -890.3797]], dtype=torch.float64)
	q_value: tensor([[-16.0105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4781641902694549, distance: 1.3912911792930227 entropy 9.789196398161788
epoch: 26, step: 81
	action: tensor([[-14521.8496,  -4404.0337,   7996.0082,  -5052.9207,   1661.7687,
           9946.0896,  -1750.5189]], dtype=torch.float64)
	q_value: tensor([[-19.1230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42515554280675305, distance: 1.3661167818391178 entropy 10.045499904403586
epoch: 26, step: 82
	action: tensor([[ -3789.5849,   -325.4769, -13490.3930,  -1533.9660,   -526.2009,
          -2505.8542,   9438.3151]], dtype=torch.float64)
	q_value: tensor([[-21.7881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28090117996051367, distance: 0.9704004323043086 entropy 10.156264649715864
epoch: 26, step: 83
	action: tensor([[-6913.5060, -2418.5693,  3195.6705,  4732.0667,   652.1368, -1480.1077,
          -168.3823]], dtype=torch.float64)
	q_value: tensor([[-18.6204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8746212197574099, distance: 1.5667996202883996 entropy 9.838619429704341
epoch: 26, step: 84
	action: tensor([[ 2342.2205, -3354.1795,  1741.0831, -2173.5648, -1831.4189, -1941.5388,
           525.7682]], dtype=torch.float64)
	q_value: tensor([[-18.3481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10038321890755131, distance: 1.2004074237933917 entropy 9.914272826985812
epoch: 26, step: 85
	action: tensor([[-3000.0862, -6777.3597,  7116.4304, -7205.8555,  2618.9930,  3156.7027,
         -2175.4159]], dtype=torch.float64)
	q_value: tensor([[-20.7529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015928110946522578, distance: 1.153421870602847 entropy 10.047461833415435
epoch: 26, step: 86
	action: tensor([[10769.2617, -5817.8295, -6081.8922,  1022.9918, -3883.5623,  4696.3305,
          9569.7345]], dtype=torch.float64)
	q_value: tensor([[-25.9385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1942060612628339, distance: 1.0272323582511067 entropy 10.128889543417278
epoch: 26, step: 87
	action: tensor([[-13007.4867,  -9282.0640,   8320.4722,   5877.0185,   3179.0886,
           4865.3264,    153.5149]], dtype=torch.float64)
	q_value: tensor([[-14.7792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08393130904186341, distance: 1.0952687922540831 entropy 9.686858242355983
epoch: 26, step: 88
	action: tensor([[   347.9379, -11842.8777,  -5139.0476,  -9631.0523,  -3831.6576,
            -62.5098,  -5045.8463]], dtype=torch.float64)
	q_value: tensor([[-18.9936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4760087384378273, distance: 1.3902764222147979 entropy 10.001727864237646
epoch: 26, step: 89
	action: tensor([[-4947.7676, -2921.5300,  3164.1485,  5216.5182, -4519.2141, -3259.0643,
          2458.5375]], dtype=torch.float64)
	q_value: tensor([[-16.2760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1899731361543724, distance: 1.248318112275194 entropy 9.578356754883092
epoch: 26, step: 90
	action: tensor([[ 5214.7613,  7507.3890,  1870.6378,  2968.6878,  6193.8654, -5489.3994,
          3864.8291]], dtype=torch.float64)
	q_value: tensor([[-19.8181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39577808396104197, distance: 0.8895183879002557 entropy 10.07125055590051
epoch: 26, step: 91
	action: tensor([[  6064.3330, -10344.1769,   5246.4378,   1186.2190,   -841.4242,
          -2588.3406,  -4217.9690]], dtype=torch.float64)
	q_value: tensor([[-20.2948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35341087625081447, distance: 0.92017597667413 entropy 9.87458028335901
epoch: 26, step: 92
	action: tensor([[-2929.2841, -8713.2919, -2294.6333,  5152.1104,  -207.1051,  3024.4279,
           617.3244]], dtype=torch.float64)
	q_value: tensor([[-20.7190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31241211138034275, distance: 0.9489007773497111 entropy 9.830491949775015
epoch: 26, step: 93
	action: tensor([[ -663.5293, -4566.7409,  2348.3184,  5389.5894, -4214.9714,  4176.5051,
          1059.2072]], dtype=torch.float64)
	q_value: tensor([[-17.3181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14081832727122068, distance: 1.2222637680592001 entropy 9.886058624657428
epoch: 26, step: 94
	action: tensor([[-4106.3158, -2608.4752, -7008.2265, -2160.8543,  4780.8733,  1893.7046,
         -1777.3837]], dtype=torch.float64)
	q_value: tensor([[-20.6071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5633237282654109, distance: 1.4308073193019726 entropy 10.090106907519539
epoch: 26, step: 95
	action: tensor([[-11593.2075,  -1596.4725,   5220.3794,   5250.6905,  -5294.4769,
           1769.4267,  -2827.9273]], dtype=torch.float64)
	q_value: tensor([[-18.4749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37736824896931376, distance: 1.3430176709034416 entropy 9.892536344526407
epoch: 26, step: 96
	action: tensor([[-7682.4192,  1068.7256, -7086.8370,   879.2642, -1019.1773, -5752.8210,
          -299.0381]], dtype=torch.float64)
	q_value: tensor([[-18.8767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4467608013154941, distance: 0.8511638397471126 entropy 9.944257603001493
epoch: 26, step: 97
	action: tensor([[ -9775.8847, -10978.0362,   -485.8030,    172.2980,  -2452.9232,
          -2045.2235,  -1986.7968]], dtype=torch.float64)
	q_value: tensor([[-22.4335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12141601270830937, distance: 1.0726260377040875 entropy 9.990248238823526
epoch: 26, step: 98
	action: tensor([[ -329.2770,  -163.2476,  1586.4762,  1900.6627, -5446.5074, -4122.8044,
          1777.5112]], dtype=torch.float64)
	q_value: tensor([[-19.9294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06396022325076145, distance: 1.1803733326581678 entropy 10.072342097991282
epoch: 26, step: 99
	action: tensor([[-2654.7342, -2370.0796,  2035.8910, -9021.3359, -4728.0238,  -579.7089,
         -6846.8568]], dtype=torch.float64)
	q_value: tensor([[-18.7006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9778305232603535, distance: 1.6093526917769578 entropy 10.01197013858909
epoch: 26, step: 100
	action: tensor([[ 1378.2802, -3156.7900,   112.5398, -1580.5625, -6126.2634,  1590.0091,
          9070.3757]], dtype=torch.float64)
	q_value: tensor([[-21.0162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2954207722564659, distance: 1.3024531836406426 entropy 10.046509648283047
epoch: 26, step: 101
	action: tensor([[  577.8604,  4741.8875, -3759.3233,  5150.0281,  -688.8001,  3797.9404,
         -2025.5785]], dtype=torch.float64)
	q_value: tensor([[-18.9095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4504618839923641, distance: 0.8483119864453158 entropy 9.944095481186148
epoch: 26, step: 102
	action: tensor([[-1030.5740, -6198.0262,  2247.5070, -5942.5753,  8302.1999, -3478.4618,
         -3538.4265]], dtype=torch.float64)
	q_value: tensor([[-21.3472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3088081115034329, distance: 0.9513843643695062 entropy 9.934452888294173
epoch: 26, step: 103
	action: tensor([[-7676.8274, -2065.1113,   743.2980,  -672.7933, -5586.0207,  -374.4787,
         -1476.9207]], dtype=torch.float64)
	q_value: tensor([[-22.8014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10789834121958963, distance: 1.0808461171289732 entropy 9.954101806770693
epoch: 26, step: 104
	action: tensor([[-6586.9243, -3798.4750,  1285.1997,  1578.2376, -9202.5188, -1418.0167,
         -2182.2506]], dtype=torch.float64)
	q_value: tensor([[-21.3336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07141531561385261, distance: 1.1845015091252349 entropy 9.9470906750612
epoch: 26, step: 105
	action: tensor([[-4324.8039,  1591.2995,  2052.0241, -1113.2257,  -469.9708,  8928.1784,
         -2354.1676]], dtype=torch.float64)
	q_value: tensor([[-19.7687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1389651198457149, distance: 1.22127060864198 entropy 10.028759025741293
epoch: 26, step: 106
	action: tensor([[  -90.0257,   496.9511,  4489.2873,  6548.3816,  5066.7280,   -23.8291,
         -1545.7413]], dtype=torch.float64)
	q_value: tensor([[-20.4554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15426574506215596, distance: 1.0523825879168884 entropy 9.624000845929464
epoch: 26, step: 107
	action: tensor([[-4036.9665, -1753.7710,  4532.5977, -7007.5506, -5628.7205,  1681.6572,
         -2198.7480]], dtype=torch.float64)
	q_value: tensor([[-21.4554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7228179030810964, distance: 1.5020223028740225 entropy 9.941805005507822
epoch: 26, step: 108
	action: tensor([[-2802.2432,  2458.1911,  6167.9695,  5300.8560, -3017.3616, -3637.6550,
          6602.8849]], dtype=torch.float64)
	q_value: tensor([[-18.3712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14521968297320453, distance: 1.057995815065479 entropy 9.830101326104154
epoch: 26, step: 109
	action: tensor([[-10114.4937,  -1467.9031,  -1977.5684,  -5558.9882,   -550.9261,
          10828.3508,   8375.9148]], dtype=torch.float64)
	q_value: tensor([[-23.7488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26576594292879885, distance: 1.287458966968059 entropy 9.968648314397607
epoch: 26, step: 110
	action: tensor([[-6671.1220, -4791.4496,   530.1472,  7450.3003,  -923.0487,  6192.8402,
          2624.6928]], dtype=torch.float64)
	q_value: tensor([[-20.2864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1894838095820569, distance: 1.0302379427442392 entropy 9.99947935848939
epoch: 26, step: 111
	action: tensor([[-2166.0322,  1830.2920,  -109.9153, -3297.5441, -1930.3310,  9880.6634,
         -4367.2931]], dtype=torch.float64)
	q_value: tensor([[-20.1488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.65768097626609, distance: 0.6695333442570004 entropy 9.964792307113543
epoch: 26, step: 112
	action: tensor([[-3313.3353, -8506.0964,  2587.8893,  8248.4561,  5102.2202,   818.8637,
          8088.9180]], dtype=torch.float64)
	q_value: tensor([[-28.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.974864381560549
epoch: 26, step: 113
	action: tensor([[-3444.9850, -2638.5948,  4022.9341,   639.5061, -5423.8936,  6053.2187,
          -383.2772]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15702243201365906, distance: 1.230913635909933 entropy 9.446682404733437
epoch: 26, step: 114
	action: tensor([[-13281.3319,   3019.6999,  -5136.5116,   -754.8112,  -1295.9479,
           3248.5080,   3909.3747]], dtype=torch.float64)
	q_value: tensor([[-19.9696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5765496020533074, distance: 0.7446598970209645 entropy 9.963476673765001
epoch: 26, step: 115
	action: tensor([[-4104.5558,  2800.0169,  3660.9913,   658.7996,  -540.1736,  8403.1161,
          -620.4258]], dtype=torch.float64)
	q_value: tensor([[-22.4196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07979887784198103, distance: 1.097736418352204 entropy 9.978740848835717
epoch: 26, step: 116
	action: tensor([[-3581.6211,   871.6042, 10242.1349,  1887.4758,   763.1672, -5118.8154,
          1014.5975]], dtype=torch.float64)
	q_value: tensor([[-20.9733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05325191958656261, distance: 1.1134581793123333 entropy 9.875152940488158
epoch: 26, step: 117
	action: tensor([[ 2085.7435, -5485.3060, -2224.0175, -3068.2981,   -14.1253,   595.5922,
         -5645.9548]], dtype=torch.float64)
	q_value: tensor([[-23.4639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.096372634191209
epoch: 26, step: 118
	action: tensor([[-4057.6082, -3074.9375, -3092.6592, -1672.0444, -5084.9824, -2290.5000,
          4767.8365]], dtype=torch.float64)
	q_value: tensor([[-21.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7421844687851087, distance: 1.51044098742581 entropy 9.446682404733437
epoch: 26, step: 119
	action: tensor([[-11059.9375,   3064.3513,    924.7419,    934.2224,   3619.6855,
           5607.7961,    141.7361]], dtype=torch.float64)
	q_value: tensor([[-20.9541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3308024990576264, distance: 1.3201202626670967 entropy 9.922663897404005
epoch: 26, step: 120
	action: tensor([[-1874.1246,  -660.9093,   223.5723,  5168.7710, -3472.6935, -5539.5951,
          4517.5561]], dtype=torch.float64)
	q_value: tensor([[-18.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023766179576739654, distance: 1.1578627504070438 entropy 9.493342195420487
epoch: 26, step: 121
	action: tensor([[-3109.2559,  4094.3550,   400.8235,  4115.1100, -2483.9363,  8977.2071,
         -3214.9534]], dtype=torch.float64)
	q_value: tensor([[-18.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13390775553498147, distance: 1.2185561787401875 entropy 9.967604605322006
epoch: 26, step: 122
	action: tensor([[-2189.7848, -5506.2549,  -426.6771,  8157.0438,  -298.9417, -7645.6449,
           347.6375]], dtype=torch.float64)
	q_value: tensor([[-21.7534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03297439092540899, distance: 1.1630582618406275 entropy 10.03625366347222
epoch: 26, step: 123
	action: tensor([[  5966.7905,  -5428.9465, -16240.2104,  -1385.7870,   5762.3990,
          -3403.2015,  -3817.1045]], dtype=torch.float64)
	q_value: tensor([[-19.1305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18056977196314983, distance: 1.0358877172905072 entropy 9.996896588477881
epoch: 26, step: 124
	action: tensor([[-4134.2629,   -48.6482, -4200.8688,  8845.3972,  3547.7226,  2943.8226,
          1873.1314]], dtype=torch.float64)
	q_value: tensor([[-21.4467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11949645015119081, distance: 1.2107878483719274 entropy 9.838592983061195
epoch: 26, step: 125
	action: tensor([[ 1970.7736, -7698.2177, -1053.1740, -1714.8444,  5366.1614,  3382.9685,
          7530.3624]], dtype=torch.float64)
	q_value: tensor([[-19.8917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3416985528953309, distance: 1.3255135417724093 entropy 10.047024890141468
epoch: 26, step: 126
	action: tensor([[ 4444.2205, -2331.3642, -7370.7849, -7420.4911,  3492.6360,  2769.5931,
         -6461.2311]], dtype=torch.float64)
	q_value: tensor([[-20.2463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23848071139036053, distance: 0.9986128434053309 entropy 9.910443031918332
epoch: 26, step: 127
	action: tensor([[-11734.0992,  -7198.1266,   5774.1821,   -536.8667,  -1225.0045,
           -379.4357,  -4503.1384]], dtype=torch.float64)
	q_value: tensor([[-21.5966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23613491151049537, distance: 1.0001497346499308 entropy 9.84183767273785
LOSS epoch 26 actor 175.24775332027863 critic 303.92965973705236
epoch: 27, step: 0
	action: tensor([[-13224.2406,  -2191.4955,   2786.0991,   5549.7986,  -4580.7241,
           -265.1076,   3486.4101]], dtype=torch.float64)
	q_value: tensor([[-18.8030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0383166500202132, distance: 1.6337760272992092 entropy 9.87381972978135
epoch: 27, step: 1
	action: tensor([[-3901.2641,  -982.9353,   279.1730,  4150.9519, -1663.8891, -2515.6388,
           899.5587]], dtype=torch.float64)
	q_value: tensor([[-17.3669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2642110383128873, distance: 1.2866679474787803 entropy 9.75694389867015
epoch: 27, step: 2
	action: tensor([[-8727.8842, -7496.1160, -6864.0583, -2764.2014, -4099.9922,  -473.2890,
          -535.8197]], dtype=torch.float64)
	q_value: tensor([[-22.0259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.88836415092503, distance: 1.572532271590061 entropy 10.059435466035177
epoch: 27, step: 3
	action: tensor([[-3939.8339, -5729.8863, -7616.9056,  2387.9281, -6032.9191, -8375.3884,
         -4847.4007]], dtype=torch.float64)
	q_value: tensor([[-23.0876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8739052568740078, distance: 1.5665003925056737 entropy 10.122046791960754
epoch: 27, step: 4
	action: tensor([[ -786.9974, -3411.3074,  4656.8415,  4349.5238,  2393.7344, -3812.0518,
          1077.3499]], dtype=torch.float64)
	q_value: tensor([[-21.4397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9707592183641633, distance: 1.6064731696083514 entropy 10.114698072384424
epoch: 27, step: 5
	action: tensor([[-14416.2612,  -9001.4295,  -2097.6832,    566.9927,  -2941.9440,
           6344.9080,  -2675.3708]], dtype=torch.float64)
	q_value: tensor([[-20.8504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6249304839755954, distance: 1.4587272178394448 entropy 10.07603845328208
epoch: 27, step: 6
	action: tensor([[-6972.7770, -3091.0751, -8615.8595,   874.2704, -4270.5074, -3313.8075,
          1691.6385]], dtype=torch.float64)
	q_value: tensor([[-19.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13361005477851018, distance: 1.218396205890533 entropy 9.974018649070027
epoch: 27, step: 7
	action: tensor([[   78.0466,  1144.6428,  2601.8157,  5122.1512, -4923.8254,  4322.3205,
          -731.8602]], dtype=torch.float64)
	q_value: tensor([[-19.8382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8087987881718854, distance: 0.5003823858407225 entropy 9.997725671945592
epoch: 27, step: 8
	action: tensor([[-3533.1763, -6869.4311,  3640.6704,  5613.1249, -1628.4946,  -636.9844,
         -4371.8763]], dtype=torch.float64)
	q_value: tensor([[-21.6898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05785684648341083, distance: 1.176982874763388 entropy 9.991179912037909
epoch: 27, step: 9
	action: tensor([[  2579.2473, -21253.8980,  -6684.8072,   4217.8490,   -472.7990,
          -2936.4285,  -1658.4785]], dtype=torch.float64)
	q_value: tensor([[-20.7425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2574854932476178, distance: 0.986073221113288 entropy 10.087052107882732
epoch: 27, step: 10
	action: tensor([[ 3188.3291,   512.4540, 16924.8471,   678.8928,  -543.6069, -4756.7815,
         -1191.5566]], dtype=torch.float64)
	q_value: tensor([[-19.5458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4571937483460903, distance: 0.8431000474131073 entropy 9.868579927150904
epoch: 27, step: 11
	action: tensor([[-17944.6732,  -3536.4488,   3799.3342,   7110.9419,  10158.8289,
          -9293.5702,    794.3984]], dtype=torch.float64)
	q_value: tensor([[-25.5518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7442319834439817, distance: 1.5113283050398012 entropy 10.03272822837589
epoch: 27, step: 12
	action: tensor([[ -4317.0825, -10996.0040,   8663.9467,  10598.0757,     91.0340,
          -1821.5570,   3649.5570]], dtype=torch.float64)
	q_value: tensor([[-21.3718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10717641466021088, distance: 1.081283361984119 entropy 10.14262465076608
epoch: 27, step: 13
	action: tensor([[-13240.9896,   -880.5728,  -3491.8188, -10608.1810,  -5965.9125,
           2185.7289,   6787.7223]], dtype=torch.float64)
	q_value: tensor([[-21.9129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5144733162165775, distance: 1.4082751184106839 entropy 10.147371625182908
epoch: 27, step: 14
	action: tensor([[-4495.2093, -2458.5883,  6004.4982,  3670.7439,  8217.9177,  1041.8747,
          2056.7025]], dtype=torch.float64)
	q_value: tensor([[-21.2933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020014070058149036, distance: 1.1328348825640677 entropy 10.139578014190343
epoch: 27, step: 15
	action: tensor([[-1335.2314, -8418.9833, -8587.8902,  5847.0309,  4657.5155,  2197.2366,
          7005.8988]], dtype=torch.float64)
	q_value: tensor([[-22.5462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03739532173566551, distance: 1.1655444369154913 entropy 10.103824817470137
epoch: 27, step: 16
	action: tensor([[ -871.2542, -9337.4448, -6240.4039,  8372.8418,  4731.9202, -1579.9525,
          3584.2987]], dtype=torch.float64)
	q_value: tensor([[-20.2911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9126046105328665, distance: 1.5825931894472096 entropy 9.982050707089074
epoch: 27, step: 17
	action: tensor([[ 4034.7077, -5861.8225, -6731.3476,   938.2616,  4642.2757, -6872.7680,
           635.3952]], dtype=torch.float64)
	q_value: tensor([[-19.1757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01744912907228402, distance: 1.1542849825661707 entropy 9.975385290540368
epoch: 27, step: 18
	action: tensor([[ -949.3933, -3308.8191,  4100.5650,  1427.3708,   634.6045, -4138.2441,
          6894.8526]], dtype=torch.float64)
	q_value: tensor([[-19.2950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6565346509346177, distance: 1.472844697849046 entropy 9.945030723420073
epoch: 27, step: 19
	action: tensor([[-2825.1385,  -820.3831, -3813.4652,   666.0805,  5194.1650,  2870.5923,
          7428.5144]], dtype=torch.float64)
	q_value: tensor([[-20.5101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8064636500765943, distance: 1.5380530200289195 entropy 10.089594016916582
epoch: 27, step: 20
	action: tensor([[ -5268.3345,  -2239.9581, -16164.7709,   3821.2585,   -217.3592,
          -2039.6147,  -5814.8753]], dtype=torch.float64)
	q_value: tensor([[-19.5894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16471625247963595, distance: 1.2349994412404055 entropy 10.01963889371909
epoch: 27, step: 21
	action: tensor([[-7146.4841, -2848.8292,  3768.2489, -6907.7251, -2171.3223, -3991.5346,
         -6457.8104]], dtype=torch.float64)
	q_value: tensor([[-24.3975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0051737471067197, distance: 1.6204390417878898 entropy 10.194715757176988
epoch: 27, step: 22
	action: tensor([[8072.3685, 5441.3300, 2111.2044, 7670.9946, 4192.0253, -280.4483,
          807.9093]], dtype=torch.float64)
	q_value: tensor([[-20.1718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24640565818918891, distance: 0.9934030928036469 entropy 9.978466976754333
epoch: 27, step: 23
	action: tensor([[-4876.4064, -3089.2497, -1190.1092,  4305.7296, -1932.6759,  4627.6801,
         10817.6916]], dtype=torch.float64)
	q_value: tensor([[-23.0071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9870569023134699, distance: 1.6131020579077344 entropy 9.941066173517479
epoch: 27, step: 24
	action: tensor([[-8256.8410, -8459.0107, -7594.7189,  2300.6400, -8124.0590,   930.8403,
         -6052.8453]], dtype=torch.float64)
	q_value: tensor([[-21.8467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009416735276891686, distance: 1.1497196224990518 entropy 10.135226103219873
epoch: 27, step: 25
	action: tensor([[-4403.3328, -1442.5243,  1889.6831,  8488.6560, -2560.7368, -3471.6882,
         -2134.2839]], dtype=torch.float64)
	q_value: tensor([[-24.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14233061622243204, distance: 1.0597822636202798 entropy 10.137914802331043
epoch: 27, step: 26
	action: tensor([[ -1102.2577, -15135.2325,  -6352.7399,   2133.2176,  -7853.7925,
          13229.5875,   9525.1280]], dtype=torch.float64)
	q_value: tensor([[-22.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09881116485717467, distance: 1.0863370574442597 entropy 10.169398134896863
epoch: 27, step: 27
	action: tensor([[  5537.4998, -11903.0072,  -1436.5959,   5225.7972,    637.1669,
           -472.0747,   1509.9233]], dtype=torch.float64)
	q_value: tensor([[-18.9158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6267499152755223, distance: 0.6991279272936133 entropy 9.90999119123899
epoch: 27, step: 28
	action: tensor([[-4589.2578, -9339.3246, -2244.6620, -8047.2921,  4126.9806, -1890.4615,
         -1011.1932]], dtype=torch.float64)
	q_value: tensor([[-24.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4004109549652437, distance: 1.3542050936339045 entropy 9.997124842916978
epoch: 27, step: 29
	action: tensor([[-2525.7300, -3474.2389,   478.3493,  6446.0178, -1715.2772, -5935.3003,
         -1622.8873]], dtype=torch.float64)
	q_value: tensor([[-18.8811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7654417841529637, distance: 1.5204893895832094 entropy 9.830143226881857
epoch: 27, step: 30
	action: tensor([[ -3686.5329,   1189.6015,   -637.2630,    589.5023,   2768.6864,
         -11839.7945,   1470.6465]], dtype=torch.float64)
	q_value: tensor([[-18.7170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2066194215619621, distance: 1.2570190196991557 entropy 9.878329589528716
epoch: 27, step: 31
	action: tensor([[ -3673.3003, -13684.8526,  -1381.6341, -16283.7062,  -6006.6486,
          -2688.8774,  -6500.0244]], dtype=torch.float64)
	q_value: tensor([[-24.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21233758108146517, distance: 1.2599940006628505 entropy 10.07375606078367
epoch: 27, step: 32
	action: tensor([[   894.9827,   1433.0104,   6507.9265,  -2105.3885, -10469.0137,
           3472.2746,   -295.2863]], dtype=torch.float64)
	q_value: tensor([[-20.6578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6255350908588031, distance: 0.7002647357778015 entropy 10.020485126300027
epoch: 27, step: 33
	action: tensor([[ 1718.7434,  1508.7502, -2169.6848, -5084.7442,  2357.3095,  5066.9462,
         -2221.2968]], dtype=torch.float64)
	q_value: tensor([[-21.6642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.717452445615129
epoch: 27, step: 34
	action: tensor([[-5682.6711, -2316.1128,  1036.6605,  1055.3744, -1475.3252,  5364.8218,
          -325.0119]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.511449461230601
epoch: 27, step: 35
	action: tensor([[-1086.9109,    86.3312,  3251.2833,  5387.7909,  2692.4115,  3428.6108,
         -3197.9057]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7551868728709975, distance: 0.5662054482169347 entropy 9.511449461230601
epoch: 27, step: 36
	action: tensor([[-2508.1372, -4154.8935,  1690.2899,   725.3652,  2973.5401,  4285.8001,
         -2012.1770]], dtype=torch.float64)
	q_value: tensor([[-20.8160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.892123168150253
epoch: 27, step: 37
	action: tensor([[-3847.2423, -3309.5864, -1733.7147, -1474.5820,  -236.0059,  1109.7789,
          5482.8753]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.511449461230601
epoch: 27, step: 38
	action: tensor([[-314.2772,  582.8020, 2504.9132, -499.3920,  783.3536, 2602.9775,
         5422.9202]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23313210886331137, distance: 1.002113632862275 entropy 9.511449461230601
epoch: 27, step: 39
	action: tensor([[ 8761.9695, -3968.1039, -5896.3380,  3462.1091, -2739.4031,  1223.1478,
           919.5407]], dtype=torch.float64)
	q_value: tensor([[-30.8608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.992684306379727
epoch: 27, step: 40
	action: tensor([[-7275.5887, -2985.5156, -1349.2864, -1459.8820,   590.5765,  1803.6842,
          -527.9619]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.511449461230601
epoch: 27, step: 41
	action: tensor([[-4519.4573, -4416.9974, -6710.7112,  5576.2670, -1886.7553,  4209.8915,
         -1541.0315]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.511449461230601
epoch: 27, step: 42
	action: tensor([[-3267.9684, -4219.5784,    14.2244, -1950.1127,   609.3985,  -297.5241,
         -7194.5082]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19379585755283546, distance: 1.0274937904180237 entropy 9.511449461230601
epoch: 27, step: 43
	action: tensor([[ 6509.7958, -9791.4172, -1844.1935,  1466.6099,   886.2620,  4231.1097,
          1608.4469]], dtype=torch.float64)
	q_value: tensor([[-26.0306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30316370103772305, distance: 1.3063398657612015 entropy 10.071718781159277
epoch: 27, step: 44
	action: tensor([[ 5341.7485, -1717.5501, -4027.3284,   520.2342, -6852.9576, -1182.1807,
           673.4241]], dtype=torch.float64)
	q_value: tensor([[-24.4198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43847893679786587, distance: 0.8575110381520307 entropy 9.877160827562353
epoch: 27, step: 45
	action: tensor([[-5731.4928, -2605.6352, -6617.5507,  2835.7375,  5459.0408,  6250.9705,
         -3036.7081]], dtype=torch.float64)
	q_value: tensor([[-24.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08129313633210011, distance: 1.1899491863784468 entropy 9.883333707832419
epoch: 27, step: 46
	action: tensor([[-3794.5492, -1288.7406,  1032.9145,  6809.0127, -6414.5297, -5797.5547,
          5531.6233]], dtype=torch.float64)
	q_value: tensor([[-20.9686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.527019682971325, distance: 1.414096380961343 entropy 10.067795996607268
epoch: 27, step: 47
	action: tensor([[-4117.5461, -6307.4164, -3761.5385,  9378.6429, 10504.0435, -2583.2221,
          -163.0913]], dtype=torch.float64)
	q_value: tensor([[-21.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020543118331354027, distance: 1.1560386989464526 entropy 10.053629846966313
epoch: 27, step: 48
	action: tensor([[-11839.7579,  -9435.7687,   9536.3003,  10785.7147,  -1972.9146,
          -3985.6376,   3392.1903]], dtype=torch.float64)
	q_value: tensor([[-20.1063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5759548640549481, distance: 1.436575914300249 entropy 10.032403981860051
epoch: 27, step: 49
	action: tensor([[ -1442.0896,   -539.5359,  -4442.6294,   2739.3013,  -3000.4270,
         -17785.4907,   -722.9873]], dtype=torch.float64)
	q_value: tensor([[-20.0313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 7.222997124745056e-06, distance: 1.144340121194909 entropy 10.049692876141629
epoch: 27, step: 50
	action: tensor([[-5622.6851, -6329.9516,  7297.1260,  9056.1006, -2666.2030,   594.8373,
          1952.1416]], dtype=torch.float64)
	q_value: tensor([[-19.5912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5399678566752766, distance: 1.4200790526984446 entropy 10.014735295692926
epoch: 27, step: 51
	action: tensor([[-16476.6772,   2294.9863,   4805.7654,   5205.0214,   1657.6459,
           -282.9105,   2011.0582]], dtype=torch.float64)
	q_value: tensor([[-22.2653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7993413853034707, distance: 0.5126082577700392 entropy 10.119382173139195
epoch: 27, step: 52
	action: tensor([[-1985.2261, -4223.8343, -4406.3475, -2754.0006, -2655.0089,  2151.5222,
          3570.5622]], dtype=torch.float64)
	q_value: tensor([[-19.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05076240699510359, distance: 1.1730295607228862 entropy 9.800883891765336
epoch: 27, step: 53
	action: tensor([[ 2564.3152, -8282.3521, -7079.2640,  1455.8248,   -28.4731, -1805.6837,
         -4858.6476]], dtype=torch.float64)
	q_value: tensor([[-20.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7679525709653252, distance: 0.55124552104679 entropy 9.973324555572395
epoch: 27, step: 54
	action: tensor([[-12993.2346,  -1885.2600, -12674.9374,  12416.4343,  -4718.9728,
           8094.8194,   4885.7206]], dtype=torch.float64)
	q_value: tensor([[-20.7087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.827272882075764
epoch: 27, step: 55
	action: tensor([[  977.4747, -1198.4165,   940.7510, -1959.0300,  3032.3728,  2322.5006,
          -171.4762]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1040202165244204, distance: 1.0831928846657803 entropy 9.511449461230601
epoch: 27, step: 56
	action: tensor([[ 4121.3719, -7384.8945,  2784.7344, -6013.6705,  2831.3796, -3362.4041,
         -1829.3401]], dtype=torch.float64)
	q_value: tensor([[-22.3206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5087544505119799, distance: 1.4056136804070698 entropy 9.99325666653279
epoch: 27, step: 57
	action: tensor([[ -8409.1485, -11904.8452,  -1119.3378,   6713.8742,   4482.0306,
           -484.3360,   9539.1663]], dtype=torch.float64)
	q_value: tensor([[-21.8832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6395643115502196, distance: 1.4652810109821912 entropy 9.916901327230855
epoch: 27, step: 58
	action: tensor([[-6999.0476, -1851.5844,   696.3639,  4865.4315, -1138.3189,  2348.3658,
          6346.6824]], dtype=torch.float64)
	q_value: tensor([[-17.8466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24157612585201194, distance: 1.2750974280941179 entropy 9.835170408456213
epoch: 27, step: 59
	action: tensor([[-7443.4640, -4527.6452, 12721.0560, -8257.7196, -1456.3077, 12768.7024,
          2226.0345]], dtype=torch.float64)
	q_value: tensor([[-20.1771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9175201854144297, distance: 1.58462559156524 entropy 10.033522108020842
epoch: 27, step: 60
	action: tensor([[ -894.4521, -8200.5108, 10637.8731,  5783.7203,  3959.4409,  3110.6741,
          5211.6656]], dtype=torch.float64)
	q_value: tensor([[-20.4395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03663655759562401, distance: 1.1651181119295073 entropy 10.014604090588533
epoch: 27, step: 61
	action: tensor([[ -1422.9647,  -4176.9755, -13481.2782,  -7481.7882, -11471.5458,
           7734.8071,   5532.6676]], dtype=torch.float64)
	q_value: tensor([[-20.7968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3658401198892056, distance: 1.3373855484809554 entropy 10.10331349562959
epoch: 27, step: 62
	action: tensor([[-5128.5665,  3252.8231, -1667.5005,  6312.0035,  3191.9367,  -790.0166,
          7943.6423]], dtype=torch.float64)
	q_value: tensor([[-22.5492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11005690198730189, distance: 1.2056723854558307 entropy 10.015592538685038
epoch: 27, step: 63
	action: tensor([[ 4931.3453, -2730.2438, -6065.7093,  3034.3779, -5069.9172, -9496.8425,
          9470.5171]], dtype=torch.float64)
	q_value: tensor([[-23.5677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24159869662660483, distance: 1.2751090181228069 entropy 10.078486420464868
epoch: 27, step: 64
	action: tensor([[ -812.4635, -7888.3515, -2987.3510, -3392.3790,  5421.2294, -8227.4512,
          5212.1422]], dtype=torch.float64)
	q_value: tensor([[-21.1601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8062922214054504, distance: 1.5379800396949475 entropy 10.024391290924802
epoch: 27, step: 65
	action: tensor([[  5030.8536,   2458.4514,  -6235.9690,   3318.4367,   -325.4099,
          -4341.2827, -13597.4985]], dtype=torch.float64)
	q_value: tensor([[-26.1400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.180896409297644
epoch: 27, step: 66
	action: tensor([[  413.6786, -3720.4409,  6508.3289,  2464.5257, -4466.1249, -1671.1728,
          1772.0244]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22783055745763625, distance: 1.268019440925407 entropy 9.511449461230601
epoch: 27, step: 67
	action: tensor([[-4854.6313,  1189.9901,   283.1501,  1964.6500, -2133.8588, -5838.7447,
           806.8124]], dtype=torch.float64)
	q_value: tensor([[-17.8147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1280008401396926, distance: 1.0685989091829904 entropy 9.746669887214681
epoch: 27, step: 68
	action: tensor([[ -3434.0340,   9596.0255,   7787.2173,   8365.1155,   3159.3511,
         -11705.3667,   -819.2817]], dtype=torch.float64)
	q_value: tensor([[-23.6226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04190108194730979, distance: 1.1680728720979743 entropy 10.03259063549805
epoch: 27, step: 69
	action: tensor([[-1285.2855,  3685.2045, -3035.8475,  6369.6608,  1288.3027, -2222.3889,
          1682.7732]], dtype=torch.float64)
	q_value: tensor([[-24.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5909507142454584, distance: 0.7318878079851231 entropy 10.0842446099828
epoch: 27, step: 70
	action: tensor([[ -469.5916,  -239.7420,  2044.6790, -2756.5713, -4266.6678, -9982.8294,
          -857.7732]], dtype=torch.float64)
	q_value: tensor([[-21.4545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7823659940996581, distance: 1.527760007028895 entropy 9.853551707954747
epoch: 27, step: 71
	action: tensor([[ -4142.9538,  -8530.5151, -10432.6335,   -987.7720,   1949.7887,
          -2679.7064,   6709.6366]], dtype=torch.float64)
	q_value: tensor([[-22.1535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4242904386115123, distance: 1.3657020857836581 entropy 9.862251447624923
epoch: 27, step: 72
	action: tensor([[-4116.0687,  1963.4673, -3299.0436, -1425.7318,  8831.3604, 11523.8685,
         -7932.8499]], dtype=torch.float64)
	q_value: tensor([[-24.0889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17319072087687382, distance: 1.0405414002542765 entropy 10.105127045594545
epoch: 27, step: 73
	action: tensor([[-12503.3553,  -6330.3288,   1287.9988,  10571.1682,   3503.8871,
          -4416.4696,   5219.4780]], dtype=torch.float64)
	q_value: tensor([[-35.6866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.154171832389284
epoch: 27, step: 74
	action: tensor([[-4674.8954, -7347.2596, -2184.9582,  2757.5498,  -127.2934,  2832.3781,
           474.2415]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.511449461230601
epoch: 27, step: 75
	action: tensor([[  633.6970,   980.6379,   493.3449,  3143.1877,  2431.6115, -3278.7298,
         -6632.1036]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.511449461230601
epoch: 27, step: 76
	action: tensor([[-3549.6591, -1036.1802,  3396.4419, -1516.7480,  -343.1329, -2136.4109,
          3591.7461]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.511449461230601
epoch: 27, step: 77
	action: tensor([[-2421.6045,    21.2126, -2954.6152, -2727.3411,  2514.5026,  1233.1549,
         -3208.4586]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.511449461230601
epoch: 27, step: 78
	action: tensor([[  -14.7715, -1002.7585, -3662.0594,  6355.9016,  1198.7544,  -901.2784,
           228.5110]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15170173278678734, distance: 1.2280801264370151 entropy 9.511449461230601
epoch: 27, step: 79
	action: tensor([[  -708.8447,   5155.0625, -10864.2990,     80.5860,   8053.0934,
           8345.1927,  -6104.9800]], dtype=torch.float64)
	q_value: tensor([[-18.3800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03572180600665176, distance: 1.1646039351621882 entropy 9.962068565164335
epoch: 27, step: 80
	action: tensor([[ 1317.6177,  1120.6728,   152.1513,  1525.0979, -7149.3075,  2034.1820,
         -2870.1854]], dtype=torch.float64)
	q_value: tensor([[-22.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6111238396050986, distance: 0.7136123431318193 entropy 10.052634920865156
epoch: 27, step: 81
	action: tensor([[-7255.9925, -1165.9705, -1569.8081,    78.1400, -2890.4953,   626.1941,
         -3404.3423]], dtype=torch.float64)
	q_value: tensor([[-19.3419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11805519374179418, distance: 1.074675620403514 entropy 9.762110131284501
epoch: 27, step: 82
	action: tensor([[-3802.6705, -8325.4950,  8447.9946,   351.2229,   928.9565, -5515.9131,
           573.4061]], dtype=torch.float64)
	q_value: tensor([[-18.4173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25087185175578774, distance: 1.2798618773829036 entropy 9.889120226486336
epoch: 27, step: 83
	action: tensor([[ 5416.7207, -3321.1332, 10026.1282, -3588.4655, -5615.8821, -6271.1061,
          1919.3306]], dtype=torch.float64)
	q_value: tensor([[-20.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1560499802310613, distance: 1.230396249320886 entropy 10.066935530593328
epoch: 27, step: 84
	action: tensor([[ 1571.1728, -5526.6795,    20.9965, -2277.9058,  2461.1470, -9284.4085,
          5156.9030]], dtype=torch.float64)
	q_value: tensor([[-21.9401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38678090640080054, distance: 0.8961166108859255 entropy 10.05560232666995
epoch: 27, step: 85
	action: tensor([[ -8245.4370, -11544.4792,  -8184.5885,  -5273.5002,   5039.7229,
          -5094.2984,  -4457.1766]], dtype=torch.float64)
	q_value: tensor([[-29.1517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22519477060450765, distance: 1.0072863874301383 entropy 10.141360364252746
epoch: 27, step: 86
	action: tensor([[ 6062.3919, -3326.4666,  -897.7104,  2438.1204, -2783.6270, -1686.5827,
          2631.8391]], dtype=torch.float64)
	q_value: tensor([[-25.1776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2673527901356323, distance: 1.2882657356950107 entropy 10.029574999437548
epoch: 27, step: 87
	action: tensor([[-4097.8304,  -486.5445,  -710.2585,  2161.7411, -1191.0331,  2018.2375,
           874.2438]], dtype=torch.float64)
	q_value: tensor([[-17.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.532780074246588
epoch: 27, step: 88
	action: tensor([[-9965.6485, -2986.8825, -3832.5317,   708.4910,  -335.9105, -1901.7544,
          3442.3741]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3999366966409448, distance: 0.8864520067387737 entropy 9.511449461230601
epoch: 27, step: 89
	action: tensor([[ -9360.9983,  -1096.7187,  -4435.0352,  -2933.6546, -10953.8590,
           4571.0967,   -837.4716]], dtype=torch.float64)
	q_value: tensor([[-20.3894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9149482731839456, distance: 1.5835625296271183 entropy 10.051926356055203
epoch: 27, step: 90
	action: tensor([[ -359.1766, -5532.8995, -3762.6094, -2274.1326,  3675.5147,  9840.8981,
         12199.1495]], dtype=torch.float64)
	q_value: tensor([[-21.8128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7313760979038733, distance: 1.505748371373494 entropy 9.966446867285145
epoch: 27, step: 91
	action: tensor([[ -268.9969, -2553.5511,  7881.0319,  -107.8478, -4240.6051,  2844.8978,
         -1544.4983]], dtype=torch.float64)
	q_value: tensor([[-25.3249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.812275727737793, distance: 1.5405252825838887 entropy 10.039137330302632
epoch: 27, step: 92
	action: tensor([[-8057.2970, -5909.3883, -4469.1454,  8389.1741, -8318.4055,  3220.1898,
          2333.9825]], dtype=torch.float64)
	q_value: tensor([[-20.0745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21940277568114963, distance: 1.2636601291431515 entropy 9.9251988312588
epoch: 27, step: 93
	action: tensor([[-3300.0038, -2012.9129, -7620.8905, -5020.9058, -8418.1042, 12282.6668,
         -6553.3109]], dtype=torch.float64)
	q_value: tensor([[-21.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5187810730645326, distance: 1.4102765399486608 entropy 10.162891501058736
epoch: 27, step: 94
	action: tensor([[-1949.3341, -6703.2429, -5222.3078,   783.4063, -4197.7762, -1878.6035,
         -6438.5735]], dtype=torch.float64)
	q_value: tensor([[-20.3527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8564185769201427, distance: 1.5591742226683276 entropy 9.949911047547902
epoch: 27, step: 95
	action: tensor([[-2814.6252, -4014.7152,  7162.6291, -3651.6864, -8077.2137, -1621.8648,
           399.7892]], dtype=torch.float64)
	q_value: tensor([[-16.2384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6731497798745785, distance: 1.480212623341042 entropy 9.778701831395836
epoch: 27, step: 96
	action: tensor([[-17588.8703,  -4816.7200,   2635.8222,  -4909.4297,   6755.9132,
          -3377.4303,  -7413.4512]], dtype=torch.float64)
	q_value: tensor([[-24.0545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09711415152812741, distance: 1.087359407772733 entropy 10.215555354100838
epoch: 27, step: 97
	action: tensor([[-5467.5761,  3155.0888, -2888.8408,  1578.6226,  3880.1651, -7752.5029,
            41.1392]], dtype=torch.float64)
	q_value: tensor([[-25.1682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1537894439959946, distance: 1.2291927043415034 entropy 10.102100494820661
epoch: 27, step: 98
	action: tensor([[  2378.4820,   -351.9191,  -5733.9186,  -5058.8024,  -8660.3401,
         -14059.4010,   2367.2626]], dtype=torch.float64)
	q_value: tensor([[-27.3245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.243149276678084
epoch: 27, step: 99
	action: tensor([[-3035.5866,  -259.5036,  2165.7442, -2735.3901,  -203.9749,  3727.0663,
         -4250.7159]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19468507561026116, distance: 1.0269269872601492 entropy 9.511449461230601
epoch: 27, step: 100
	action: tensor([[  4884.7952,   4173.6074,  -4334.3008,  -4473.5195, -13701.2339,
          -3982.9066,  -6654.8963]], dtype=torch.float64)
	q_value: tensor([[-26.2469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.093835711220608
epoch: 27, step: 101
	action: tensor([[-4655.4596, -2817.5707,   537.5035,   553.8667,  -654.8947,  4331.7848,
          -873.4075]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09735690326231228, distance: 1.087213223110049 entropy 9.511449461230601
epoch: 27, step: 102
	action: tensor([[ -7858.9008,  -6251.8857,   3682.3342,  11266.5485, -12314.5795,
          -3633.3532,   7861.5602]], dtype=torch.float64)
	q_value: tensor([[-22.1569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4452674733258979, distance: 1.3757223966815828 entropy 10.1657110226317
epoch: 27, step: 103
	action: tensor([[  408.5892,  -943.9810, -5227.0512,  -741.4514,  4597.0104,   255.3561,
          1705.7967]], dtype=torch.float64)
	q_value: tensor([[-19.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4123489442441053, distance: 1.3599648952713954 entropy 9.939708803276014
epoch: 27, step: 104
	action: tensor([[ 5010.4986, -2369.4830,  -772.7584, 17498.9516,  2644.4721, -2624.4969,
          1009.1504]], dtype=torch.float64)
	q_value: tensor([[-22.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38997009362779145, distance: 0.8937833427736552 entropy 10.019785990466085
epoch: 27, step: 105
	action: tensor([[-9480.6023, -1369.0968,  4878.4070,  2101.1545, -1945.6098,  -414.9899,
          -958.3991]], dtype=torch.float64)
	q_value: tensor([[-21.9208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07613104064813514, distance: 1.0999219809480376 entropy 9.910164518848209
epoch: 27, step: 106
	action: tensor([[-1885.4344, -1752.7750, -6661.8520, -9127.7950,  3369.1513,  3562.8236,
           264.5498]], dtype=torch.float64)
	q_value: tensor([[-23.2233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.004402445907195, distance: 1.620127356381147 entropy 10.100754262977476
epoch: 27, step: 107
	action: tensor([[-2701.7273, -3269.5091, -4533.8289,  8389.6424, -4099.8741,  -586.3214,
          1582.4175]], dtype=torch.float64)
	q_value: tensor([[-22.2400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1010087409971615, distance: 1.0850117161493238 entropy 10.119666356888422
epoch: 27, step: 108
	action: tensor([[   239.6770, -14003.7773,  -7296.8828,   6042.1847,   1756.4719,
           4849.2868,   6489.5554]], dtype=torch.float64)
	q_value: tensor([[-25.7498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.609643537766265, distance: 1.4518493298577364 entropy 10.243279566828406
epoch: 27, step: 109
	action: tensor([[-2615.8300,   -36.4877, -7145.2939,  -987.8602,  8818.0190,  -470.7403,
         -4679.3545]], dtype=torch.float64)
	q_value: tensor([[-21.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4792622584920787, distance: 1.80184707000395 entropy 10.183498099443455
epoch: 27, step: 110
	action: tensor([[-10743.4758,  -5502.5271,  -9735.1567,    874.8892,  -4320.4381,
            504.1954,  10658.7189]], dtype=torch.float64)
	q_value: tensor([[-23.0348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7197876149669056, distance: 1.500700757649726 entropy 10.074056090778623
epoch: 27, step: 111
	action: tensor([[-5351.1474, -8360.3377,  1587.5436, -2775.7665, -2966.3707,  7336.5199,
         -1589.7146]], dtype=torch.float64)
	q_value: tensor([[-22.7613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9041849916398748, distance: 1.5791059217785162 entropy 10.211659732978436
epoch: 27, step: 112
	action: tensor([[-6000.9647, -7453.9723, -7650.8881,  6994.5985,  7418.0460,  2111.6444,
         -1127.5079]], dtype=torch.float64)
	q_value: tensor([[-19.7543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3202722791815773, distance: 1.3148870425461352 entropy 9.857373352259396
epoch: 27, step: 113
	action: tensor([[-4425.2728,  6396.3994, -4435.5748, -2472.7019, -2127.1106,  5379.4337,
          3806.1325]], dtype=torch.float64)
	q_value: tensor([[-25.0339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.197304312522937, distance: 1.2521575217053131 entropy 10.199439081642623
epoch: 27, step: 114
	action: tensor([[  1449.1718, -12057.8366,   7161.1412,   1111.4838, -10042.7048,
           2212.2305,   3313.0763]], dtype=torch.float64)
	q_value: tensor([[-24.5723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23319516613068503, distance: 1.0020724316025387 entropy 9.96992074849598
epoch: 27, step: 115
	action: tensor([[-1961.3582,   355.4511, -3695.6708, 13475.9319,  2877.7297, -5277.9226,
         -2738.6159]], dtype=torch.float64)
	q_value: tensor([[-22.6028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.126440191800159
epoch: 27, step: 116
	action: tensor([[-4920.6003,  2052.8859,  -367.5565,  3416.6852,  5587.0530,   596.4150,
          1418.3576]], dtype=torch.float64)
	q_value: tensor([[-23.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0166523268854728, distance: 1.1538329123286626 entropy 9.511449461230601
epoch: 27, step: 117
	action: tensor([[-1906.4887,   428.8681,  -591.9238, -4943.3203,  6166.4762,  7859.3102,
          3469.0793]], dtype=torch.float64)
	q_value: tensor([[-25.4565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5159352208990156, distance: 0.7961748145083775 entropy 10.120174840068987
epoch: 27, step: 118
	action: tensor([[-2878.4130, -1569.7175, -3465.2555,  3876.0052,  3695.1125,   544.1128,
          -102.5919]], dtype=torch.float64)
	q_value: tensor([[-25.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09683121484099266, distance: 1.1984684181638185 entropy 9.909557198840307
epoch: 27, step: 119
	action: tensor([[ -948.0113,  1457.2833, 17402.0077,  2660.2524,  -770.7097,  6767.2099,
         -1127.9885]], dtype=torch.float64)
	q_value: tensor([[-23.8869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043233935978579385, distance: 1.168819763081252 entropy 10.142480794405458
epoch: 27, step: 120
	action: tensor([[-2830.2947, -6494.0680,  7390.3887,  5888.6733,  5769.5766,   641.3470,
         -8855.1144]], dtype=torch.float64)
	q_value: tensor([[-25.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01969545439162146, distance: 1.1555584958721672 entropy 10.17471417287658
epoch: 27, step: 121
	action: tensor([[ -855.0088, -6513.3289, -1801.2016,  2546.9773, 10946.5670,  5737.0202,
          7206.8023]], dtype=torch.float64)
	q_value: tensor([[-21.9191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047667763101427396, distance: 1.1167370786087052 entropy 10.087206444023042
epoch: 27, step: 122
	action: tensor([[-2780.3206,  7837.6391,  2730.6072, -2574.2667, -6009.0599, 12130.9410,
         -1262.5732]], dtype=torch.float64)
	q_value: tensor([[-20.7639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6634328523829655, distance: 0.6638845385797556 entropy 9.990448946609671
epoch: 27, step: 123
	action: tensor([[ -6161.4034, -12910.4454,  -2899.8692,   7849.8643,   -222.9087,
          -4909.0903,   4307.7227]], dtype=torch.float64)
	q_value: tensor([[-31.0378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1479817147416881, distance: 1.0562850933517665 entropy 10.068059989128304
epoch: 27, step: 124
	action: tensor([[-10520.4925, -14598.5085,    601.6769,   4280.7527,  -2422.2672,
           1425.4408,   6747.3485]], dtype=torch.float64)
	q_value: tensor([[-27.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8902494834689418, distance: 1.5733170796047244 entropy 10.33289374211714
epoch: 27, step: 125
	action: tensor([[-4937.5410, -5816.8375,  5166.4434,  1343.4706,  1560.8904,  2816.4753,
          2933.4891]], dtype=torch.float64)
	q_value: tensor([[-22.1940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31533965499601413, distance: 1.3124284917769988 entropy 10.156756084504114
epoch: 27, step: 126
	action: tensor([[ -4068.7101, -12346.7580,  -4368.2530,  11079.2795,  -8244.5192,
           8534.6140,   7003.9903]], dtype=torch.float64)
	q_value: tensor([[-23.5559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07016600352624569, distance: 1.103467136838529 entropy 10.160179447481088
epoch: 27, step: 127
	action: tensor([[ -522.8782,  3546.9280, -8202.8453,  4596.5321,  3249.5867, -4455.4798,
          -432.0665]], dtype=torch.float64)
	q_value: tensor([[-21.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33481963268338955, distance: 0.9333110432457687 entropy 10.115176104664187
LOSS epoch 27 actor 206.0809079845274 critic 363.60788224879434
epoch: 28, step: 0
	action: tensor([[  6919.2148,  -4992.4006,  -7829.8094,  -1485.1058,   2312.7863,
           -519.6339, -16246.6804]], dtype=torch.float64)
	q_value: tensor([[-30.3972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11087644858772738, distance: 1.2061173727320222 entropy 10.222278159610184
epoch: 28, step: 1
	action: tensor([[   81.1440, -5709.8252,   191.7940,  8642.5324, -3704.4488,  6989.2547,
          9400.9488]], dtype=torch.float64)
	q_value: tensor([[-22.9577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30272481153540953, distance: 1.3061198676387176 entropy 9.872696971331491
epoch: 28, step: 2
	action: tensor([[ 4084.5004,  2509.5452, -1914.2800,  3384.6862,  3017.6333, -1804.1263,
          1938.4923]], dtype=torch.float64)
	q_value: tensor([[-26.8110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08518035824586079, distance: 1.0945218443051317 entropy 10.195365004342637
epoch: 28, step: 3
	action: tensor([[  513.3331, -6651.8191, -8961.2060,  3867.8639,  3265.4168, -7000.0478,
          3349.9131]], dtype=torch.float64)
	q_value: tensor([[-30.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5464342257760328, distance: 0.770684866439542 entropy 10.111108069660984
epoch: 28, step: 4
	action: tensor([[ 3890.6030,  1030.7738,   469.9280, 10288.7546,  1761.3013,  3239.0297,
         17510.2964]], dtype=torch.float64)
	q_value: tensor([[-27.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.183515591117356
epoch: 28, step: 5
	action: tensor([[-1705.8957, -9817.3155, -2521.4725,  2990.5554,  3247.5631, -5527.7133,
         -6016.9171]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7901257394804841, distance: 1.5310820389891495 entropy 9.575013911065957
epoch: 28, step: 6
	action: tensor([[ -7863.5258, -16001.6398,   4220.5591,  -1619.9190, -10163.9087,
          -5162.5363,  -1118.1946]], dtype=torch.float64)
	q_value: tensor([[-25.3771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3113343483787918, distance: 1.3104287473189824 entropy 10.156720752896836
epoch: 28, step: 7
	action: tensor([[-7956.6706, -2411.8851, -2407.4488, -5320.8425, -6965.3215,  5655.4366,
          6104.5478]], dtype=torch.float64)
	q_value: tensor([[-24.3356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11292353887014328, distance: 1.0777976216025316 entropy 9.968164990423318
epoch: 28, step: 8
	action: tensor([[-3260.3905, -3892.1886, -7219.5695,   543.5842,  -553.1306, 14316.3269,
          1484.4621]], dtype=torch.float64)
	q_value: tensor([[-28.3396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6662144508994778, distance: 1.4771416425451465 entropy 10.122757438304657
epoch: 28, step: 9
	action: tensor([[-5796.0789, -4351.9465,  2489.2250,  1342.7639, -2945.8578, -2722.0115,
          -706.4552]], dtype=torch.float64)
	q_value: tensor([[-19.7723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49474251156161775, distance: 1.3990714246974387 entropy 9.759678174394756
epoch: 28, step: 10
	action: tensor([[-7424.8823, -3750.3440, -1505.2631, -3239.3772,  2771.8608, -5672.6564,
         -1038.3599]], dtype=torch.float64)
	q_value: tensor([[-24.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5352633815486887, distance: 1.4179082812020842 entropy 10.066722511777517
epoch: 28, step: 11
	action: tensor([[-3544.1092,  5554.8132,  6552.6550, -2535.5762,  1137.7954,   649.0528,
          -859.3304]], dtype=torch.float64)
	q_value: tensor([[-25.9399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.792743269896219, distance: 0.5209679595790951 entropy 10.034116381922015
epoch: 28, step: 12
	action: tensor([[ -2871.6132, -15525.1873,  -9317.3440,  -8518.6757,   2230.5479,
           2220.7533,  -4632.1709]], dtype=torch.float64)
	q_value: tensor([[-38.0898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15796931997369512, distance: 1.2314172125489313 entropy 10.17199255255954
epoch: 28, step: 13
	action: tensor([[-2427.5766, -8866.8662, -2606.3562, 10746.4199,  5581.5502,  8991.0957,
         -5268.9988]], dtype=torch.float64)
	q_value: tensor([[-27.9937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3873067090186588, distance: 1.3478542629109587 entropy 9.997962014551302
epoch: 28, step: 14
	action: tensor([[ -9412.7636, -13575.2897,   8070.2089,   1662.1827,    -83.8106,
           6328.4616,  12047.1151]], dtype=torch.float64)
	q_value: tensor([[-24.1452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025328937285243658, distance: 1.158746140079353 entropy 10.15537236910592
epoch: 28, step: 15
	action: tensor([[ -6748.7352,   -425.5031, -11148.8239,   1965.2406,  -1024.7858,
           6750.0086,  -9778.0084]], dtype=torch.float64)
	q_value: tensor([[-27.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19553299705333937, distance: 1.026386215747753 entropy 10.2669435180911
epoch: 28, step: 16
	action: tensor([[-3481.1794,  5625.8814,  2054.6345,  4891.6540,  2517.9864, -2973.1116,
          2600.7006]], dtype=torch.float64)
	q_value: tensor([[-26.8574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48419295416337826, distance: 0.821864701830069 entropy 10.232111871012474
epoch: 28, step: 17
	action: tensor([[-9418.9252,  4519.3905, -6391.2115,  7210.1950,  2073.0075,  4902.2671,
          2750.1099]], dtype=torch.float64)
	q_value: tensor([[-29.1813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11776601685775545, distance: 1.2098517141501643 entropy 10.058452709296308
epoch: 28, step: 18
	action: tensor([[ 1.8509e+03, -1.5329e+04, -6.3064e+00,  7.0823e+03,  6.8492e+03,
          6.0729e+02, -2.4547e+03]], dtype=torch.float64)
	q_value: tensor([[-26.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43193495591380293, distance: 0.8624932921951108 entropy 10.009960790353647
epoch: 28, step: 19
	action: tensor([[-3653.8804,  -997.9480,  5282.4668,  8719.1725,  1749.3252,  5982.4020,
          8462.7815]], dtype=torch.float64)
	q_value: tensor([[-24.4496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6761075422183271, distance: 1.4815203914891746 entropy 9.99595961445073
epoch: 28, step: 20
	action: tensor([[-9707.9269,  -117.8868, -9959.8824,  3161.9513, -3439.0828, -3440.8619,
          5049.8539]], dtype=torch.float64)
	q_value: tensor([[-26.2653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05895253784862575, distance: 1.1775922559933227 entropy 10.164736766103749
epoch: 28, step: 21
	action: tensor([[-11861.3903,   1163.4402,  -1962.8777,  -1235.3657,  -7277.6720,
          -4633.8005,   2375.4388]], dtype=torch.float64)
	q_value: tensor([[-25.1070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27794189899456, distance: 0.9723951088875008 entropy 10.129894121752553
epoch: 28, step: 22
	action: tensor([[ -2983.2180, -11287.6072,  -2192.9990,   2392.4022,   1921.7310,
            -40.4418,  -2833.7417]], dtype=torch.float64)
	q_value: tensor([[-37.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8590416989088394, distance: 1.5602753914721443 entropy 10.07388341474205
epoch: 28, step: 23
	action: tensor([[-1917.5316, -1076.3090, -1775.6096,  -510.5268, -6354.7699,  1601.0669,
         -3425.6360]], dtype=torch.float64)
	q_value: tensor([[-20.6551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.530864344266083, distance: 1.4158754359079209 entropy 9.966843553086063
epoch: 28, step: 24
	action: tensor([[-10628.1845, -18203.3253,  -4001.2159,  10302.2794,   2818.6434,
            611.2262,   -238.2616]], dtype=torch.float64)
	q_value: tensor([[-30.6530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37073491391114377, distance: 1.3397798156468947 entropy 10.255305928559125
epoch: 28, step: 25
	action: tensor([[ -3094.5185,   -226.0982,   -696.1487, -16923.5586,  -3820.0449,
          -4397.3208,   4083.8180]], dtype=torch.float64)
	q_value: tensor([[-25.7293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.593022905532397, distance: 1.4443342284935448 entropy 10.144629754663852
epoch: 28, step: 26
	action: tensor([[ -335.5401, -4336.0182,  -732.9271,  7831.3937, -2556.2839,  7276.7000,
         -5735.6158]], dtype=torch.float64)
	q_value: tensor([[-22.0974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1419598722761377, distance: 1.0600112944186004 entropy 9.78817826312799
epoch: 28, step: 27
	action: tensor([[-7053.6897, -4433.2034, -7433.3249, -3525.2861,  4525.6419,  8949.7263,
          7293.3232]], dtype=torch.float64)
	q_value: tensor([[-23.3664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2545567504555688, distance: 1.2817456407656018 entropy 10.034601127523175
epoch: 28, step: 28
	action: tensor([[-5036.8001,   588.9599,  6714.9161, -7692.3971,   453.5075,  7690.6831,
          3834.4330]], dtype=torch.float64)
	q_value: tensor([[-21.7648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.539277616163593, distance: 0.7767412141575439 entropy 9.861264270722435
epoch: 28, step: 29
	action: tensor([[  551.6087,  -323.2390, -4098.2639,  4305.6854,  9365.0764,  3685.3252,
          2607.6111]], dtype=torch.float64)
	q_value: tensor([[-32.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.9366120137699
epoch: 28, step: 30
	action: tensor([[-1336.5598, -1358.2263, -2068.5480,  6819.3868, -5166.6340, -1789.3740,
          3049.6584]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8904003994817722, distance: 1.5733798845422757 entropy 9.575013911065957
epoch: 28, step: 31
	action: tensor([[  145.3232, -3078.0306, -3989.5259, -3839.8152,  3967.8657,   244.5438,
         -5839.7485]], dtype=torch.float64)
	q_value: tensor([[-24.8723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21013332264865214, distance: 1.0170295917208672 entropy 10.027844942550262
epoch: 28, step: 32
	action: tensor([[ -5701.4496, -11088.2357,   3253.8819,  11840.2112,  -7542.6869,
         -14048.9895, -11181.1986]], dtype=torch.float64)
	q_value: tensor([[-27.5504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.636406650990675, distance: 1.4638693280410744 entropy 10.163623601723945
epoch: 28, step: 33
	action: tensor([[  698.1789, -9426.6049,  -113.5024,  7310.9343,  6963.3387, -1909.2107,
         -2330.4668]], dtype=torch.float64)
	q_value: tensor([[-27.4453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10658070820347698, distance: 1.0816440267803937 entropy 10.241561103860041
epoch: 28, step: 34
	action: tensor([[ -3037.1219,  -3338.8386,  -1171.9883,   4115.4466, -10782.3463,
           -322.6423,   3076.8583]], dtype=torch.float64)
	q_value: tensor([[-24.2675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8175928483258057, distance: 1.5427835370157963 entropy 9.879117172358153
epoch: 28, step: 35
	action: tensor([[  682.8244, -1901.8756, -1595.2311,  2213.4467, 10246.3061,  1135.5151,
          4413.0660]], dtype=torch.float64)
	q_value: tensor([[-23.1224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17603953350091628, distance: 1.0387472347933995 entropy 10.059575428045235
epoch: 28, step: 36
	action: tensor([[-7967.6006, -3638.1117,    85.1098, 12610.4628,  3556.9147,  4338.6841,
          -945.1701]], dtype=torch.float64)
	q_value: tensor([[-25.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06975592369956773, distance: 1.1835838846324742 entropy 10.122231253041466
epoch: 28, step: 37
	action: tensor([[-16730.3394,   2193.2829,  -3895.7167,  -8697.3078,  -9162.4688,
           4536.4460,  14416.1545]], dtype=torch.float64)
	q_value: tensor([[-27.1876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19507644590710282, distance: 1.250992012312494 entropy 10.232910822826971
epoch: 28, step: 38
	action: tensor([[  849.8928,   923.8305, -6187.4086, -3875.2545,  3881.4517,  6024.3196,
          5369.7905]], dtype=torch.float64)
	q_value: tensor([[-29.9488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35358368229490167, distance: 0.9200530063129895 entropy 9.978549447361358
epoch: 28, step: 39
	action: tensor([[-4390.8757,   632.8011,  4993.6700,  5251.4314, 11098.4187, -7939.6125,
         15454.6627]], dtype=torch.float64)
	q_value: tensor([[-31.5721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10223249783420785, distance: 1.0842729755520304 entropy 9.96264703132556
epoch: 28, step: 40
	action: tensor([[ 4752.7995,  -708.9514,   117.0473,  2615.9323,  -910.6573,    26.6685,
         -5097.3983]], dtype=torch.float64)
	q_value: tensor([[-28.4844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.87088001533829
epoch: 28, step: 41
	action: tensor([[-8281.0355, -2281.9431, -1240.5576,  1283.8191, -1747.2434, -5994.7657,
          1142.9756]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.731280919058451, distance: 1.5057069830924243 entropy 9.575013911065957
epoch: 28, step: 42
	action: tensor([[ -1405.0483,  -2390.2438,   4016.8843,   9186.0374, -12606.1979,
          -3139.3116,  -6372.1063]], dtype=torch.float64)
	q_value: tensor([[-25.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05188867622544813, distance: 1.1142595373904038 entropy 10.123370005190983
epoch: 28, step: 43
	action: tensor([[ -5935.4367,  -1064.5401,  -3096.1208,   6121.8293,   5003.4686,
           6193.2907, -13462.3065]], dtype=torch.float64)
	q_value: tensor([[-26.5344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09944867580994488, distance: 1.1998975692517793 entropy 10.182402758810634
epoch: 28, step: 44
	action: tensor([[  2604.3886,  -6256.2765,  -6051.9767,   7363.4467, -10130.2571,
          -6723.9300,  -7285.0205]], dtype=torch.float64)
	q_value: tensor([[-26.7814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.214599511622996
epoch: 28, step: 45
	action: tensor([[  710.8101, -3239.1752, -1882.1998,  1328.3627,  -484.0046, -1767.9435,
          -231.6430]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5637698930886403, distance: 1.4310114773859066 entropy 9.575013911065957
epoch: 28, step: 46
	action: tensor([[ -3814.4026, -12795.8178,    -97.2729,   5958.4218,  -5701.3359,
           7150.1266,  -8367.6108]], dtype=torch.float64)
	q_value: tensor([[-22.9987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16509399302242733, distance: 1.2351996923916473 entropy 10.049776398511474
epoch: 28, step: 47
	action: tensor([[-1279.4651, -1519.2174, 14096.8185,  1967.8817, 16572.5737,  -572.6418,
          4724.0328]], dtype=torch.float64)
	q_value: tensor([[-29.7215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05240869418657135, distance: 1.173948126001289 entropy 10.345135454729311
epoch: 28, step: 48
	action: tensor([[-14451.7416,  -4390.8085,   1239.2375,   6287.6931,  -3826.9997,
           9148.1945,   6430.5843]], dtype=torch.float64)
	q_value: tensor([[-27.4127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008789340713102334, distance: 1.1393041389645546 entropy 10.252121538168609
epoch: 28, step: 49
	action: tensor([[ -5321.9704,   1385.4512,  -1789.3981,   -847.1415, -10981.3365,
          10800.2420,  -7906.3002]], dtype=torch.float64)
	q_value: tensor([[-27.1291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01977592109515458, distance: 1.1329725207974743 entropy 10.1655512795012
epoch: 28, step: 50
	action: tensor([[ 2812.8457,  -436.6295,  3255.7351,  4693.5776,  6558.9340, -8289.1319,
         -2104.5083]], dtype=torch.float64)
	q_value: tensor([[-27.8943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.962589525735266
epoch: 28, step: 51
	action: tensor([[-5033.5020,  -668.6056, -2756.6164, -1980.0659,  4483.7666, -2316.3061,
          1483.5910]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07611761117928473, distance: 1.1870979710808527 entropy 9.575013911065957
epoch: 28, step: 52
	action: tensor([[-4839.4645, -1430.2004,  4605.0248,  -793.7091,   492.1648,   213.8175,
           865.8978]], dtype=torch.float64)
	q_value: tensor([[-29.9310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43049121526336587, distance: 0.8635886125832277 entropy 10.074590457326982
epoch: 28, step: 53
	action: tensor([[-1448.1379, -9847.1210,   895.1700,  3625.4690,   265.9378,  9710.1302,
         10260.8091]], dtype=torch.float64)
	q_value: tensor([[-25.1764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0029098573403932, distance: 1.6195240259672052 entropy 9.992357595114939
epoch: 28, step: 54
	action: tensor([[ -8666.2757, -13515.2888,    453.2056,   -457.3490,   -468.6250,
           -668.6692,   5301.3623]], dtype=torch.float64)
	q_value: tensor([[-24.5062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24198882800681942, distance: 1.2753093328344192 entropy 10.07452696319747
epoch: 28, step: 55
	action: tensor([[-5.5265e+03,  5.8366e+03,  9.4495e+03,  2.0960e+03, -3.0242e+00,
          1.0842e+04,  3.3093e+03]], dtype=torch.float64)
	q_value: tensor([[-28.9998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18240687021368984, distance: 1.0347258762606526 entropy 10.086995475172566
epoch: 28, step: 56
	action: tensor([[ -8957.4178,  -5645.0692,   -702.2371,  12517.9909, -10175.3385,
           9588.3273,  -4778.2097]], dtype=torch.float64)
	q_value: tensor([[-30.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.527098716124393, distance: 1.4141329748084723 entropy 10.12503995678222
epoch: 28, step: 57
	action: tensor([[ -9870.6078, -15527.3430,  -7602.2475,  10756.7057,   5275.4151,
          -7865.1579,   4549.6019]], dtype=torch.float64)
	q_value: tensor([[-30.6431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22383177539596977, distance: 1.0081719798369233 entropy 10.351983507102668
epoch: 28, step: 58
	action: tensor([[-1914.9188, -4176.5777, -6043.5764,  7876.8702,  2027.1795,  4398.6461,
          6445.3466]], dtype=torch.float64)
	q_value: tensor([[-24.9422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6417789718056044, distance: 1.4662702995681172 entropy 10.174850936235574
epoch: 28, step: 59
	action: tensor([[ -6108.5698,  -7985.4951,   -111.2516,   2216.0032,   4363.4651,
         -12166.9420,   3586.8305]], dtype=torch.float64)
	q_value: tensor([[-25.9716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08836252808846157, distance: 1.092616556989117 entropy 10.128477124587578
epoch: 28, step: 60
	action: tensor([[-5486.7277, -8389.8958, -2262.1093,  8452.7203, 11918.2620, -5676.0683,
           806.3905]], dtype=torch.float64)
	q_value: tensor([[-27.2778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37277421151685797, distance: 1.3407760674724936 entropy 10.164438157445746
epoch: 28, step: 61
	action: tensor([[ -885.8711, -5451.0322, -4345.8905, 10944.2332,  -389.3013,  -150.7986,
         10468.6631]], dtype=torch.float64)
	q_value: tensor([[-26.0620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2653295303328713, distance: 1.2872370018601242 entropy 10.120120013307007
epoch: 28, step: 62
	action: tensor([[-11001.6247,   1601.3892,    771.3784,   9731.9874,  -4048.7272,
           3755.0434,   7623.8986]], dtype=torch.float64)
	q_value: tensor([[-24.7597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06549751548681293, distance: 1.1812257723135113 entropy 10.070453101248559
epoch: 28, step: 63
	action: tensor([[ -9193.7694, -16362.3293, -23056.5225,   3847.4440, -11303.3958,
          -1964.4202,    496.0896]], dtype=torch.float64)
	q_value: tensor([[-32.5400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15595969413013222, distance: 1.2303482021555876 entropy 10.296859390081691
epoch: 28, step: 64
	action: tensor([[ -5051.4462,  -2776.6674,  -5222.7997,  -5494.8132, -12514.0985,
           3273.2197,   -136.6966]], dtype=torch.float64)
	q_value: tensor([[-23.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6371736905542393, distance: 1.4642123705670977 entropy 10.02782311827978
epoch: 28, step: 65
	action: tensor([[  424.1341, -7056.9605, 12196.2424,  6127.3047,  5450.2587, -3322.6842,
          3761.2554]], dtype=torch.float64)
	q_value: tensor([[-24.7165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6609035526034922, distance: 0.66637441333843 entropy 10.07408991667412
epoch: 28, step: 66
	action: tensor([[-8364.2880, -3995.0115,  -455.5461,  3905.8214,  2461.5426, 14039.9386,
         -5099.9953]], dtype=torch.float64)
	q_value: tensor([[-27.7406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05586553407258221, distance: 1.17587457517692 entropy 10.115635481135083
epoch: 28, step: 67
	action: tensor([[11627.7031, -6754.5579, -3220.0566,   503.2915,  3379.5312,  6616.1297,
          1946.0986]], dtype=torch.float64)
	q_value: tensor([[-26.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29957904975519045, distance: 0.9577149286276354 entropy 10.179826302802075
epoch: 28, step: 68
	action: tensor([[-1391.2374, -4305.4511, -9190.8176,  3798.1886, -9075.9080, 12036.1731,
         -5201.1171]], dtype=torch.float64)
	q_value: tensor([[-25.1602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26683951118449123, distance: 1.2880048349201931 entropy 10.225976587537973
epoch: 28, step: 69
	action: tensor([[  -290.9016, -11608.9695,  -6924.5565,   3120.6231,   7364.6741,
          -2347.7406,  -3710.6844]], dtype=torch.float64)
	q_value: tensor([[-28.4920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8688650589580347, distance: 1.5643922847980225 entropy 10.325387246715552
epoch: 28, step: 70
	action: tensor([[ 1059.0231, -5372.3324,  -749.6366,  3632.8066, -1008.2983,  2654.5370,
          4712.7991]], dtype=torch.float64)
	q_value: tensor([[-24.6644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005995030345765429, distance: 1.1477693175949757 entropy 10.083751617929254
epoch: 28, step: 71
	action: tensor([[-4554.6766,  1251.1436,  5702.1135,   691.0998,  5843.5062,  6404.2666,
          2209.4073]], dtype=torch.float64)
	q_value: tensor([[-23.5454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15110646835312658, distance: 1.0543463659075143 entropy 10.103672175246428
epoch: 28, step: 72
	action: tensor([[-6941.3396,   207.8281,   701.2226, -1463.3965, -6966.3581, 17460.8516,
          4956.8847]], dtype=torch.float64)
	q_value: tensor([[-29.3791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31488908778064517, distance: 0.9471900684534452 entropy 10.14627737524827
epoch: 28, step: 73
	action: tensor([[-6715.4326, -8062.5308, -1161.0394, -5867.0156,  -763.0096,  -641.0200,
          3063.9830]], dtype=torch.float64)
	q_value: tensor([[-32.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9321772947443099, distance: 1.5906703299347813 entropy 10.00128599862137
epoch: 28, step: 74
	action: tensor([[-12196.0129,   7103.6792,   3000.9378,  -4567.0515,  -1385.1377,
          -1205.9026,  14508.0107]], dtype=torch.float64)
	q_value: tensor([[-33.9176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6807167563194412, distance: 0.6466134838474245 entropy 10.251273138008996
epoch: 28, step: 75
	action: tensor([[  706.4219, -4915.7921,   813.0010,  7788.2395,  2691.1001,  3154.2837,
          4719.8837]], dtype=torch.float64)
	q_value: tensor([[-45.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.242587753793355
epoch: 28, step: 76
	action: tensor([[-5180.7422, -1761.4034, -3079.3485, -2944.9871,  1193.9464, -3650.8143,
          3079.6212]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11701046292729933, distance: 1.0753119495877683 entropy 9.575013911065957
epoch: 28, step: 77
	action: tensor([[-10086.8021,  -2985.6889,   2453.5074,   3777.6534,  -9230.9886,
          -4201.5203,   4162.9893]], dtype=torch.float64)
	q_value: tensor([[-27.7045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9212870454778834, distance: 1.5861812816450416 entropy 10.012618538990791
epoch: 28, step: 78
	action: tensor([[-6806.8378,  4168.6794, -2363.8363,  7880.8087, -6167.4974, -1443.3487,
          4837.7862]], dtype=torch.float64)
	q_value: tensor([[-26.9117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09231387508619582, distance: 1.0902460996413292 entropy 10.1562100711113
epoch: 28, step: 79
	action: tensor([[-11277.2303,  -1430.2294,  -2269.2049,   -542.7110,    497.5081,
          -5347.2267,   7334.6164]], dtype=torch.float64)
	q_value: tensor([[-29.3331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42612559096384617, distance: 1.3665816341308845 entropy 10.09276906075877
epoch: 28, step: 80
	action: tensor([[-7914.8603, -7403.3672,  9122.3729, -1403.6851,  1784.3309, -4860.0319,
          5186.8655]], dtype=torch.float64)
	q_value: tensor([[-24.1159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11191677805148437, distance: 1.0784090552088694 entropy 9.966213790479982
epoch: 28, step: 81
	action: tensor([[-1114.3164, -6628.6790, -3456.3589,  5693.8075,  3912.3497, -6538.6077,
         -3621.3657]], dtype=torch.float64)
	q_value: tensor([[-32.8249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6555506496003678, distance: 1.4724071892121078 entropy 10.129619514114404
epoch: 28, step: 82
	action: tensor([[-4117.0450, -6414.0376,  2737.4160,  5741.4200, -1891.5525,  3666.5980,
         -6140.1250]], dtype=torch.float64)
	q_value: tensor([[-26.3199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28212549516572927, distance: 0.9695739936441937 entropy 10.09130928984021
epoch: 28, step: 83
	action: tensor([[-8201.8894,  1428.4535,  4431.0357, -1062.5334, 14234.9787,  -998.1938,
          3912.0180]], dtype=torch.float64)
	q_value: tensor([[-24.8279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.075683907964885, distance: 1.1868587313893655 entropy 10.129166197602041
epoch: 28, step: 84
	action: tensor([[ 1883.6350, -5395.8333, -1939.3111,  1220.8284, -1480.3567, -3581.6635,
          6989.2268]], dtype=torch.float64)
	q_value: tensor([[-30.5521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5139573965867881, distance: 0.7977996886502801 entropy 10.125192851486263
epoch: 28, step: 85
	action: tensor([[ -741.7266, -6437.9485, -1087.1017,  4593.1050,  3564.4148,  9026.3409,
          1593.9816]], dtype=torch.float64)
	q_value: tensor([[-30.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014116811366197912, distance: 1.1362382987730613 entropy 10.1384712577172
epoch: 28, step: 86
	action: tensor([[-10899.7033,  -9466.4240,  -2678.0302,   -394.5437,  -4921.4321,
           7123.6411,  -3158.2336]], dtype=torch.float64)
	q_value: tensor([[-24.3442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3778991778775149, distance: 1.3432764899250107 entropy 10.081698487052124
epoch: 28, step: 87
	action: tensor([[  1343.2445,  -4475.7355, -11637.4712,  -7597.3100,  -3393.8867,
          -1949.6405,  -1956.2142]], dtype=torch.float64)
	q_value: tensor([[-25.7900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5948370108110217, distance: 0.728402749679689 entropy 10.097750900546052
epoch: 28, step: 88
	action: tensor([[  1799.8076, -10477.3935,   5567.2391,   7582.7224,  -8161.9251,
          -2562.6508,  -5937.1360]], dtype=torch.float64)
	q_value: tensor([[-28.6674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3263596099874648, distance: 1.3179148072056885 entropy 10.038172052589768
epoch: 28, step: 89
	action: tensor([[-7783.4874,   680.5616, -5266.3052,   475.4503, -2391.0404,  2190.3485,
         -7106.7643]], dtype=torch.float64)
	q_value: tensor([[-23.2736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.781077448775594
epoch: 28, step: 90
	action: tensor([[-5425.4362,  1927.3681,  5097.4910, -1054.7049,  -863.1333,   246.1127,
          4430.9064]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.575013911065957
epoch: 28, step: 91
	action: tensor([[ -3679.9242,    339.8718, -10242.6460,  -1956.9657,  -2794.5110,
           5179.1184,  -1928.3468]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7315626555206349, distance: 0.5928955083298986 entropy 9.575013911065957
epoch: 28, step: 92
	action: tensor([[-15020.7031,  -5664.0046,    -40.0504,     78.4326,   9603.0332,
          -3306.2330,   -809.4495]], dtype=torch.float64)
	q_value: tensor([[-29.3176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.863303332905241
epoch: 28, step: 93
	action: tensor([[-3085.9220, -8707.4928,  3137.8551,  -520.1698,  2295.9929, -1597.5916,
         -5303.3221]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.575013911065957
epoch: 28, step: 94
	action: tensor([[-8433.0751,   119.5295,  1870.5149,  8674.8182,  3659.4717, -1735.9131,
          5992.3666]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26578727781241174, distance: 1.287469817185521 entropy 9.575013911065957
epoch: 28, step: 95
	action: tensor([[-1670.2893,  1566.6922,  -902.1606, -6586.0515,  6540.1615,  6891.8414,
         -4517.8802]], dtype=torch.float64)
	q_value: tensor([[-27.9864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14435201323950064, distance: 1.2241552861003084 entropy 10.014869780821797
epoch: 28, step: 96
	action: tensor([[  -38.3867, -9040.3568, -2810.8154, -2750.3727, -1850.1602, -6076.6310,
          3499.5713]], dtype=torch.float64)
	q_value: tensor([[-24.3771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2773982099469119, distance: 1.2933612495374254 entropy 9.895477048768587
epoch: 28, step: 97
	action: tensor([[ -2974.5783,  -1300.6153,   8166.7165,  -3117.3261, -17722.8123,
          -5507.2109,   2388.4541]], dtype=torch.float64)
	q_value: tensor([[-33.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0915733938755241, distance: 1.1955924505426827 entropy 10.171147600899193
epoch: 28, step: 98
	action: tensor([[ 3794.3904, -1505.8168, -3961.3478,  6282.8886, -5349.9587, 11615.2549,
          6632.6294]], dtype=torch.float64)
	q_value: tensor([[-29.8441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047717537005459, distance: 1.1713287415139548 entropy 10.085517468969645
epoch: 28, step: 99
	action: tensor([[-2702.3739, -6940.1549, 11938.9989,  1178.2948, -5916.8098,  3830.9997,
          5543.9540]], dtype=torch.float64)
	q_value: tensor([[-27.5772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018968447054612758, distance: 1.1551464859455527 entropy 10.275255613846168
epoch: 28, step: 100
	action: tensor([[ -6774.1320,    461.3389, -10711.6633,   1690.0037,  -2963.4299,
           2382.8015,   9400.5930]], dtype=torch.float64)
	q_value: tensor([[-26.0043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30445894131503204, distance: 0.9543728571766318 entropy 10.171442059842422
epoch: 28, step: 101
	action: tensor([[-4535.6368,  2344.3990, -2871.9139,  2421.5237,  4351.5620, -5444.2074,
         -1469.8226]], dtype=torch.float64)
	q_value: tensor([[-25.4916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.008402424594147
epoch: 28, step: 102
	action: tensor([[-4487.7347, -2242.2309,  -315.6209,  4703.1603,   800.0414,  2831.9872,
          3625.2063]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.575013911065957
epoch: 28, step: 103
	action: tensor([[-728.4241, 6082.7872, -403.3923, 4944.9150, -416.3839,  649.4352,
         1790.6016]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34704289881469386, distance: 0.9246960829798088 entropy 9.575013911065957
epoch: 28, step: 104
	action: tensor([[ 1244.6550,  1999.7224,  3374.7239,  7782.9007, -1513.5758,   635.3813,
          1096.4785]], dtype=torch.float64)
	q_value: tensor([[-27.7421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22375458558796946, distance: 1.0082221098631037 entropy 9.632147385725466
epoch: 28, step: 105
	action: tensor([[-4432.4571, -1602.9844, -5014.9352,  7161.6581,   856.0134,  4449.2792,
          3148.8355]], dtype=torch.float64)
	q_value: tensor([[-18.7851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.53614769522146
epoch: 28, step: 106
	action: tensor([[-2028.1736, -1153.3347, -2029.6473,  7615.3364, -5132.4593, -1051.0869,
          6322.6161]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8412430194597027, distance: 1.5527882996051658 entropy 9.575013911065957
epoch: 28, step: 107
	action: tensor([[-4262.8376, -6028.4367,  2404.8784,  8651.4790,  4336.7954,  8751.6733,
         -7889.5677]], dtype=torch.float64)
	q_value: tensor([[-22.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4798803917363079, distance: 1.3920986143935263 entropy 9.982231612809866
epoch: 28, step: 108
	action: tensor([[-1986.7106,  1617.1199, -4577.8283,    -8.1093,  5974.0161,  -485.7860,
         -2714.6966]], dtype=torch.float64)
	q_value: tensor([[-25.7573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047752113363635385, distance: 1.1713480692175295 entropy 10.174808051624513
epoch: 28, step: 109
	action: tensor([[  1186.8853,  -4155.1157,   3584.4132,  -1464.5183,  -7413.3198,
         -10029.1359,  12069.7790]], dtype=torch.float64)
	q_value: tensor([[-27.3362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4493710819423824, distance: 0.8491534947001204 entropy 9.920525425499033
epoch: 28, step: 110
	action: tensor([[-4724.1375,  8476.1182, -2849.8112,   842.0062, -3541.9462,  2002.6675,
         10225.0296]], dtype=torch.float64)
	q_value: tensor([[-28.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.017397107627573
epoch: 28, step: 111
	action: tensor([[ 1356.1896, -5941.8917, -2049.8070,  2059.9608, -2508.1980, -2531.8531,
         -5937.7095]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43352991812731123, distance: 0.8612816253398582 entropy 9.575013911065957
epoch: 28, step: 112
	action: tensor([[-3293.3710,  -451.4752,  2999.6515,  9223.8851, -4652.9209, -3366.3873,
         -6126.8389]], dtype=torch.float64)
	q_value: tensor([[-21.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1690753877739305, distance: 1.0431277640681154 entropy 9.88479298468276
epoch: 28, step: 113
	action: tensor([[  -528.4485, -16602.2226,   1158.7337,  -1654.2211,  -1589.0015,
            103.4523,  12196.9501]], dtype=torch.float64)
	q_value: tensor([[-30.2787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4210719488476267, distance: 1.780576106186419 entropy 10.235740363915491
epoch: 28, step: 114
	action: tensor([[-7421.6529, -2470.0918, -1611.1246, -5029.7084,  3448.6610, -3438.8567,
         -1364.2589]], dtype=torch.float64)
	q_value: tensor([[-20.9235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.193274321155662, distance: 1.250048434928883 entropy 9.895425859321028
epoch: 28, step: 115
	action: tensor([[ 6291.7669, -1878.1571, -8537.0226, 11360.5820,  6636.0499, 10716.2826,
         -7525.2796]], dtype=torch.float64)
	q_value: tensor([[-34.4758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21035466864506325, distance: 1.2589631490769624 entropy 10.170577964789775
epoch: 28, step: 116
	action: tensor([[-1565.5260, -6088.9621, -8806.4427,  5705.5666, -7249.1253,  1565.1823,
          -304.3739]], dtype=torch.float64)
	q_value: tensor([[-23.1077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2543047451619751, distance: 1.2816169009103608 entropy 9.736117370565722
epoch: 28, step: 117
	action: tensor([[ -5233.9382, -11684.3432,  -3254.4476,   7769.1030,  -8518.4975,
           6711.6658,    819.7866]], dtype=torch.float64)
	q_value: tensor([[-28.3337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1712412063583375, distance: 1.0417674122607001 entropy 10.245193321879965
epoch: 28, step: 118
	action: tensor([[  4747.0681,   1890.1439, -14941.4658,   4058.0965,  10368.0744,
          20944.7657,   7556.8738]], dtype=torch.float64)
	q_value: tensor([[-25.9941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5084905441799836, distance: 0.8022738412745154 entropy 10.181919750788671
epoch: 28, step: 119
	action: tensor([[ -4627.4952, -13638.9636,   -607.8481,  -7911.1828,   5325.2587,
           2763.1356,  -5087.3991]], dtype=torch.float64)
	q_value: tensor([[-22.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.885603671519656
epoch: 28, step: 120
	action: tensor([[-4420.0955,  2537.0990,  6867.1044,  2428.8277, -1005.3184, -2559.4188,
          -666.2881]], dtype=torch.float64)
	q_value: tensor([[-28.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47178794931781687, distance: 0.8316887860534632 entropy 9.575013911065957
epoch: 28, step: 121
	action: tensor([[-5554.7175, -2086.5009,   195.7197,  5772.8681,  2222.6156,  5211.4801,
         -3831.6374]], dtype=torch.float64)
	q_value: tensor([[-25.0761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49876818115160426, distance: 1.4009541610737548 entropy 9.816985704200803
epoch: 28, step: 122
	action: tensor([[ -1555.8647, -11822.6984,  -5088.1301,    815.7792,  -9681.6934,
           5611.8666,   1898.0304]], dtype=torch.float64)
	q_value: tensor([[-24.6139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16866468029911774, distance: 1.0433855295964252 entropy 10.053329206248105
epoch: 28, step: 123
	action: tensor([[-4262.5370,  -282.1118, -8307.8531,  5398.7737,  1768.5742, -3623.1510,
         -5848.8750]], dtype=torch.float64)
	q_value: tensor([[-23.4379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1091219389016348, distance: 1.0801046239421384 entropy 10.002056353806848
epoch: 28, step: 124
	action: tensor([[-8123.1720,  8214.8531,  6070.9249,  6615.9503,   254.9398,  3458.7167,
          -673.2183]], dtype=torch.float64)
	q_value: tensor([[-24.8598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.746263052674248, distance: 0.5764326190217102 entropy 10.116523200869064
epoch: 28, step: 125
	action: tensor([[-7597.9380,  3648.0396, -5601.0018,   505.6119, -1858.8263,  2428.0356,
          7794.1599]], dtype=torch.float64)
	q_value: tensor([[-27.5567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17802713585119245, distance: 1.2420364479599617 entropy 10.130184078259898
epoch: 28, step: 126
	action: tensor([[ 6929.2476, -5598.2174, -8255.4474,  -737.8146,   809.3689,  3359.6513,
         -1013.7559]], dtype=torch.float64)
	q_value: tensor([[-29.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20071492623546394, distance: 1.0230751704688799 entropy 10.22714367823093
epoch: 28, step: 127
	action: tensor([[   80.8142, -2086.5789, -4455.2835,  1775.8975,  -335.0245,  1361.3937,
          3706.8490]], dtype=torch.float64)
	q_value: tensor([[-23.7398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4301359657767485, distance: 0.8638579162287168 entropy 9.839836021788349
LOSS epoch 28 actor 306.1153634161272 critic 249.94669748187837
epoch: 29, step: 0
	action: tensor([[  760.5991, -8925.8571, 13235.4792,  5142.0744, -1416.0929, -6290.2897,
          2692.6529]], dtype=torch.float64)
	q_value: tensor([[-29.7063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5702404579275806, distance: 0.7501868676821526 entropy 10.059488496392538
epoch: 29, step: 1
	action: tensor([[ -7404.0631,  -1802.9294, -12310.4430,   4125.9209,   2360.2744,
          -5493.3173,  -8146.3537]], dtype=torch.float64)
	q_value: tensor([[-31.9263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1317486413776925, distance: 1.2173954778259954 entropy 10.251607213884485
epoch: 29, step: 2
	action: tensor([[ 1474.9930, -9227.2984, -8345.3546, 11184.3057,  4265.4899, -6232.7359,
         -3044.0559]], dtype=torch.float64)
	q_value: tensor([[-35.5396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39636670504109295, distance: 0.8890850066871229 entropy 10.379607336500465
epoch: 29, step: 3
	action: tensor([[  2829.4445,   3436.9404,  12555.7255,  -1475.3696,   2277.1473,
         -12322.4268,  -4405.1934]], dtype=torch.float64)
	q_value: tensor([[-32.3328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7300446620509784, distance: 0.5945695353207334 entropy 10.20394094335833
epoch: 29, step: 4
	action: tensor([[ -2976.2543, -11643.9556,  -6480.4485,   -387.7991,  -5014.9224,
         -11940.7472,   1944.4923]], dtype=torch.float64)
	q_value: tensor([[-38.6370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03480991768901254, distance: 1.1242505756989536 entropy 10.140984589448266
epoch: 29, step: 5
	action: tensor([[-10307.0867,  -5995.6541,   -351.5168,  -6582.8250,   3469.6879,
          -2393.2279,   6010.5413]], dtype=torch.float64)
	q_value: tensor([[-40.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011260893466897404, distance: 1.1507693858231896 entropy 10.227238738618672
epoch: 29, step: 6
	action: tensor([[ -3603.6670, -12208.9954,  -2054.9921,  20837.7836,   2315.1059,
         -14583.0837,   1452.0786]], dtype=torch.float64)
	q_value: tensor([[-38.7829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7247458847028685, distance: 1.5028625140364709 entropy 10.185717075984053
epoch: 29, step: 7
	action: tensor([[ -8112.6612,  -5262.0989,   2629.4588,  -1698.8394,  -2086.5019,
         -10977.0321,   5795.1660]], dtype=torch.float64)
	q_value: tensor([[-31.0303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22253823756496094, distance: 1.2652837167442248 entropy 10.166983725044572
epoch: 29, step: 8
	action: tensor([[  3591.7275,   4220.5264,   -120.9978,     92.6744,  -3631.4653,
           3324.4763, -16552.9434]], dtype=torch.float64)
	q_value: tensor([[-37.3178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.217702534379075
epoch: 29, step: 9
	action: tensor([[-1162.6796,  1515.0135, -1179.8662,  -546.4520, -1341.4815, -1281.1479,
           224.8038]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29707382432908935, distance: 1.3032839316743463 entropy 9.637696866360656
epoch: 29, step: 10
	action: tensor([[-2382.8114, -8091.8959,  -913.3512,  4877.8088, -1477.5138,   -28.7722,
          8236.8422]], dtype=torch.float64)
	q_value: tensor([[-41.9894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9444074691669688, distance: 1.595696651206889 entropy 10.08312335903438
epoch: 29, step: 11
	action: tensor([[-2365.7728,  1676.7064,    80.3169, 17210.3027,  2311.0250,  -181.9015,
          1547.1275]], dtype=torch.float64)
	q_value: tensor([[-29.0341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4916800140039863, distance: 0.8158781191287641 entropy 10.016828242575816
epoch: 29, step: 12
	action: tensor([[-5953.7565,  -210.0340,  4113.1082, 11459.5985, -5640.8055,  4784.6608,
         -6618.0117]], dtype=torch.float64)
	q_value: tensor([[-34.9965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2839761414507236, distance: 1.2966870399127237 entropy 10.135235826166197
epoch: 29, step: 13
	action: tensor([[  8379.0263, -12104.7536,  -9476.0695,   7433.4871,   3616.2034,
          14471.3034,  -1930.9052]], dtype=torch.float64)
	q_value: tensor([[-35.0643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10610356250084285, distance: 1.0819328233813092 entropy 10.331371766354845
epoch: 29, step: 14
	action: tensor([[-13364.4332,   1870.3065,  -7345.7398,  -2829.0273,  -2690.4606,
           2556.8743,   5380.9498]], dtype=torch.float64)
	q_value: tensor([[-32.6113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.303446805933927
epoch: 29, step: 15
	action: tensor([[-4643.7290, -9063.6102,  7093.6826, -4272.1412, -3766.5912,  1681.3169,
          4186.5938]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05793310078068736, distance: 1.1770252946735105 entropy 9.637696866360656
epoch: 29, step: 16
	action: tensor([[-12076.5985,  -2545.7090,   3884.9763,   3243.8385,   1247.2706,
           5883.5248,    868.9545]], dtype=torch.float64)
	q_value: tensor([[-35.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9144384428875034, distance: 1.5833517140377664 entropy 10.132732600449913
epoch: 29, step: 17
	action: tensor([[-12028.1788,    463.4307,  -4179.3459,   6990.0192,  11816.0151,
           1064.4592,   7831.2257]], dtype=torch.float64)
	q_value: tensor([[-31.9808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21501456080190529, distance: 1.013882188934731 entropy 10.242719150564488
epoch: 29, step: 18
	action: tensor([[-8353.7372,  -321.9846, -8866.7870,   892.2319, -4291.1571, -2918.7481,
         -4773.5689]], dtype=torch.float64)
	q_value: tensor([[-35.0751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0242518311507034, distance: 1.6281295687763404 entropy 10.264082553608388
epoch: 29, step: 19
	action: tensor([[-1855.8080,  2810.6792, -2576.9334,  8143.6880, -6127.3390, 10688.0917,
         -2328.5509]], dtype=torch.float64)
	q_value: tensor([[-27.1974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2577271220215571, distance: 1.283364158979161 entropy 10.049395391591162
epoch: 29, step: 20
	action: tensor([[-1076.8395,  6428.8154, -8008.6732, -6466.1116,  9099.6832, -4918.7338,
         -2192.8147]], dtype=torch.float64)
	q_value: tensor([[-31.8497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0797173239162452, distance: 1.1890817895941743 entropy 10.128133820201906
epoch: 29, step: 21
	action: tensor([[ -5722.1011,  -6775.1008, -11101.1271,   3172.4593,   1410.2569,
           4160.0890,   5424.1588]], dtype=torch.float64)
	q_value: tensor([[-38.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23346331144075116, distance: 1.2709246743240135 entropy 10.039918503048995
epoch: 29, step: 22
	action: tensor([[-15291.1063,  -3562.2893,  -4244.9033,  14677.7823,  -2381.5776,
            840.5270,  12107.8250]], dtype=torch.float64)
	q_value: tensor([[-33.5028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03478734824786078, distance: 1.124263720031528 entropy 10.317772249812228
epoch: 29, step: 23
	action: tensor([[ -2677.7450,  -4160.2589,  -2357.5227,   8547.3229,  -8814.8220,
         -15627.3103,  21387.6953]], dtype=torch.float64)
	q_value: tensor([[-30.3510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49192320454755345, distance: 1.3977513734031484 entropy 10.142170551772436
epoch: 29, step: 24
	action: tensor([[ -7855.5159,  -7763.4765, -11969.0048,  10474.8153,   2921.5622,
          -5198.4852,   1607.5490]], dtype=torch.float64)
	q_value: tensor([[-28.7989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8756177893281163, distance: 1.5672160290396562 entropy 10.120398896687103
epoch: 29, step: 25
	action: tensor([[  2320.4448, -12671.1519,  -6120.4171,   9213.6075,    257.8882,
           2842.6351,   5101.0757]], dtype=torch.float64)
	q_value: tensor([[-27.2180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42349822054404807, distance: 0.8688744354643445 entropy 10.029675068805153
epoch: 29, step: 26
	action: tensor([[-3037.8168,   -28.3164, -1875.2734,   340.2407, -3836.3766,  3081.3367,
         15184.4196]], dtype=torch.float64)
	q_value: tensor([[-26.4421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8482175833514805, distance: 1.555726473582026 entropy 9.92785191686825
epoch: 29, step: 27
	action: tensor([[ -1857.5356, -11698.8002,   5907.1205,  -4533.0366,  -4095.7175,
           -354.6966, -10615.5134]], dtype=torch.float64)
	q_value: tensor([[-31.9132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.207799678444605, distance: 1.2576336471047187 entropy 10.266821081479677
epoch: 29, step: 28
	action: tensor([[-2517.4084,   343.9926, -7280.4024,  4280.8325, -1954.8815,  4184.0792,
          3909.9068]], dtype=torch.float64)
	q_value: tensor([[-31.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5877128284390454, distance: 0.7347787775315071 entropy 10.153820007760661
epoch: 29, step: 29
	action: tensor([[ -291.6721, -7283.1568,   162.4103,  1421.4114, -3661.7839, -3956.0955,
         -2740.2019]], dtype=torch.float64)
	q_value: tensor([[-22.7954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6843674740825925, distance: 1.4851664039853607 entropy 9.820891624503579
epoch: 29, step: 30
	action: tensor([[-11597.1663,  -3081.8582,   5828.1522,   1631.4831,    141.3910,
           3739.3449,  11200.2045]], dtype=torch.float64)
	q_value: tensor([[-26.7265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8187027128324249, distance: 1.5432544949041014 entropy 10.016756608879987
epoch: 29, step: 31
	action: tensor([[ -1921.5851,  -3934.3692, -17450.4844,   4511.5675,  11102.9522,
           2953.7419, -13629.4925]], dtype=torch.float64)
	q_value: tensor([[-34.3924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16474428813659125, distance: 1.2350143048658793 entropy 10.394966546027542
epoch: 29, step: 32
	action: tensor([[ 2071.5573, -5281.4834,  1387.4698,  6105.8431,  8097.3311, -2655.7629,
          7895.4822]], dtype=torch.float64)
	q_value: tensor([[-30.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5879452318729758, distance: 0.73457165347524 entropy 10.193782073967126
epoch: 29, step: 33
	action: tensor([[   84.6738, -6754.6503, 16890.1387,  8637.3389, -5753.3506, 13378.0089,
          8008.3155]], dtype=torch.float64)
	q_value: tensor([[-32.8895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01968409045696684, distance: 1.1330255898636434 entropy 10.23617990352996
epoch: 29, step: 34
	action: tensor([[  5071.2979,  -6786.4401,   2862.5173,   1914.9090, -13564.6532,
          -8313.7179,  -2284.5486]], dtype=torch.float64)
	q_value: tensor([[-35.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28453203825506, distance: 0.9679474695292826 entropy 10.203785280475122
epoch: 29, step: 35
	action: tensor([[ 3534.0715, -6471.2331, -2608.9394,  1731.0948, -2132.0829, -1899.1252,
          5755.1336]], dtype=torch.float64)
	q_value: tensor([[-33.6592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5216481591101956, distance: 0.7914626376929412 entropy 10.134419728337026
epoch: 29, step: 36
	action: tensor([[ -7310.9329,  -1084.3034,  -2650.5372,   3580.6290,   2216.0223,
         -10467.5023,   6517.6979]], dtype=torch.float64)
	q_value: tensor([[-31.0125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.366627648848186, distance: 0.9107228723092823 entropy 10.106784454015878
epoch: 29, step: 37
	action: tensor([[-3574.8795,  -152.1911,  5638.6125, 10320.0264,  1469.7349, -5606.9301,
         -9329.8732]], dtype=torch.float64)
	q_value: tensor([[-29.3514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2709051616074445, distance: 1.2900699673482527 entropy 10.17560410358273
epoch: 29, step: 38
	action: tensor([[-4175.3002, -7201.0569, -1494.4870,  2018.3549,  1517.2589,  2970.2864,
         -3820.6792]], dtype=torch.float64)
	q_value: tensor([[-28.7875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6357174751653365, distance: 1.4635610398795436 entropy 10.076393946532406
epoch: 29, step: 39
	action: tensor([[-7365.7158, -4239.5729, 10912.3081, -2377.3803,  1320.6376,  7665.3523,
         -5223.3470]], dtype=torch.float64)
	q_value: tensor([[-32.8484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6612572976940749, distance: 1.474942684370726 entropy 10.260256975856086
epoch: 29, step: 40
	action: tensor([[ 3556.4282, -5279.9060,  6366.2134, -7462.1210,  -642.7104,  1598.4596,
          7755.9331]], dtype=torch.float64)
	q_value: tensor([[-30.2089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1066805377564799, distance: 1.0815835943175791 entropy 10.02018390335269
epoch: 29, step: 41
	action: tensor([[-6207.0267,  1155.7947, -4539.8416,  7403.3539,  2840.3031, -3015.8369,
         -2399.1321]], dtype=torch.float64)
	q_value: tensor([[-30.9814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3079874236476836, distance: 1.3087553722153902 entropy 10.065906207665506
epoch: 29, step: 42
	action: tensor([[-2848.1825, -6052.0999, -3632.8346,  3208.8977, -3108.2848,  -572.8889,
          1034.9506]], dtype=torch.float64)
	q_value: tensor([[-26.4766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.434485357117232, distance: 1.3705811450794294 entropy 9.752140288856532
epoch: 29, step: 43
	action: tensor([[ 9813.4433, -5255.3029,  5216.5526, -8892.8254,  -193.3436,  2623.9160,
           294.6001]], dtype=torch.float64)
	q_value: tensor([[-26.9987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3227309298714012, distance: 1.3161107841592445 entropy 10.066306315583763
epoch: 29, step: 44
	action: tensor([[11135.8253,  9860.8899, -7839.3363,  2281.4684,  1960.9459,  1942.0334,
         -4507.7227]], dtype=torch.float64)
	q_value: tensor([[-26.9299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5579882438370936, distance: 0.7608054298240479 entropy 9.811388746201436
epoch: 29, step: 45
	action: tensor([[-5996.2987, -1360.9731,   226.9331, -3852.2655, -4974.0276,  2620.2529,
          1256.5403]], dtype=torch.float64)
	q_value: tensor([[-31.4203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22629459860467183, distance: 1.2672260760947627 entropy 10.02253917270487
epoch: 29, step: 46
	action: tensor([[-3998.6010,  2431.8273, -2208.6653,  1439.9283,  5379.3373, -3487.4362,
          7119.3833]], dtype=torch.float64)
	q_value: tensor([[-26.2771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.918476214742837
epoch: 29, step: 47
	action: tensor([[-1024.1539, -2461.7587, -6568.5306,  3438.9249,   963.0987, -9568.1806,
          2284.6782]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09269508185869824, distance: 1.0900171367701612 entropy 9.637696866360656
epoch: 29, step: 48
	action: tensor([[-3150.1509, -4432.5830, -6665.8914,   707.3335, -7956.3843, -1815.5600,
           349.3224]], dtype=torch.float64)
	q_value: tensor([[-29.6830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10289336152580209, distance: 1.083873825128406 entropy 10.17929117316993
epoch: 29, step: 49
	action: tensor([[ -5198.0038,  -1134.5558,  -4713.3041,  -8106.2265, -14951.4721,
          -1368.3778,   9504.1900]], dtype=torch.float64)
	q_value: tensor([[-35.6400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9839114717192052, distance: 1.6118248146404939 entropy 10.325077342113458
epoch: 29, step: 50
	action: tensor([[ 1606.7457, -6228.5453, -5710.9071,  -636.0656,   948.7254, -5651.6062,
          2393.9192]], dtype=torch.float64)
	q_value: tensor([[-32.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4132914944369934, distance: 0.8765322213538547 entropy 10.115915224178378
epoch: 29, step: 51
	action: tensor([[-4105.7981, -4456.0363,  6926.1338, -3046.7654,  5166.3669,  6868.7153,
          6148.9567]], dtype=torch.float64)
	q_value: tensor([[-29.0233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27696510611210257, distance: 1.2931419728853166 entropy 9.851088805632191
epoch: 29, step: 52
	action: tensor([[-4803.8280, -1263.4430,  6806.4068, -5977.2831, 13476.0581, -1325.8057,
         -3690.9549]], dtype=torch.float64)
	q_value: tensor([[-35.5961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10156888111970575, distance: 1.0846736409516768 entropy 10.162439781553715
epoch: 29, step: 53
	action: tensor([[-7882.0401,   455.7588, 10804.7249,  4584.4988,   539.0241,  3260.9809,
          4609.6316]], dtype=torch.float64)
	q_value: tensor([[-34.7357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4218918614692979, distance: 0.8700841051287485 entropy 10.270662084907588
epoch: 29, step: 54
	action: tensor([[-6561.1042, -1457.8027, -1679.4088,  9168.5555,  3298.2400,  5161.6773,
          4394.0330]], dtype=torch.float64)
	q_value: tensor([[-35.2043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11533702443175742, distance: 1.0763304308698955 entropy 10.14819917773349
epoch: 29, step: 55
	action: tensor([[ -2754.2201, -10335.9021, -10022.3525,   7145.5404,   8070.6029,
         -13915.3067,  -2136.3541]], dtype=torch.float64)
	q_value: tensor([[-31.1079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13163534997451398, distance: 1.066369613070457 entropy 10.23371514858759
epoch: 29, step: 56
	action: tensor([[ -1740.4992,    967.5775, -19465.9615,  15764.7948,   6580.0506,
           6037.4012, -10684.6220]], dtype=torch.float64)
	q_value: tensor([[-31.7633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6015164736760299, distance: 0.7223736223985927 entropy 10.265329633442986
epoch: 29, step: 57
	action: tensor([[ -9845.4105, -16064.9657,  -5010.6382,  -5216.9929,  -4091.6802,
           3927.2193,  -4591.1693]], dtype=torch.float64)
	q_value: tensor([[-30.2199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.596231134859837, distance: 1.4457878876817551 entropy 10.120848051623934
epoch: 29, step: 58
	action: tensor([[-5461.5067, -9568.6281, 10401.0128,  1594.3312, -4471.7658,  7114.6056,
          9415.1150]], dtype=torch.float64)
	q_value: tensor([[-29.4759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42968569631130027, distance: 1.3682863023965248 entropy 10.02212953849814
epoch: 29, step: 59
	action: tensor([[ -4677.5305,  -3873.4756,   8140.6482,    630.5102,  -2785.5467,
         -21122.7769,    161.6021]], dtype=torch.float64)
	q_value: tensor([[-34.1727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2875959105047725, distance: 1.2985135552245222 entropy 10.336002506045846
epoch: 29, step: 60
	action: tensor([[ -4329.0134,   2934.4158,  10613.0306,   6778.7527, -14383.5903,
           7989.0789,   7978.1291]], dtype=torch.float64)
	q_value: tensor([[-34.4503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13230054464289065, distance: 1.2176922763476081 entropy 10.337416022525451
epoch: 29, step: 61
	action: tensor([[ -1337.7513, -12255.6281,  15904.9994,   6781.7406,   4508.2782,
           6328.3310,  -8277.3224]], dtype=torch.float64)
	q_value: tensor([[-32.0346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4092365193677252, distance: 1.358465576305636 entropy 10.106124673705905
epoch: 29, step: 62
	action: tensor([[-6985.3853,  -608.4341, -1531.5023,  3305.7645,  4241.1162, -2281.6531,
          1955.1712]], dtype=torch.float64)
	q_value: tensor([[-36.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23647439286140615, distance: 1.2724749940585707 entropy 10.381359417848557
epoch: 29, step: 63
	action: tensor([[ 3382.1627, -9569.0966,  -112.2743, -3878.9552, -5567.6367, -5275.8082,
          -620.1034]], dtype=torch.float64)
	q_value: tensor([[-28.8690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23165666777872573, distance: 1.269993576748817 entropy 10.126106322569338
epoch: 29, step: 64
	action: tensor([[-2436.3804, -2293.1539,   292.1980, -2757.7185,  3833.9416,  1158.8631,
          4107.0384]], dtype=torch.float64)
	q_value: tensor([[-27.7837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7534017578850174, distance: 1.5152957741727522 entropy 9.89382985508505
epoch: 29, step: 65
	action: tensor([[ -8956.4616, -13671.3302,  -4082.0111,   8221.8312,  -4446.5141,
           3439.6886,   5933.8006]], dtype=torch.float64)
	q_value: tensor([[-39.4861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5596519357501097, distance: 1.429126056506107 entropy 10.2697207514339
epoch: 29, step: 66
	action: tensor([[-14912.5732,  -3166.4547,  -7328.2372, -10659.1511,  -8438.2190,
          -3483.8021,  -6437.6540]], dtype=torch.float64)
	q_value: tensor([[-29.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1326953744934266, distance: 1.0657185483860054 entropy 10.197826132194397
epoch: 29, step: 67
	action: tensor([[  4477.0650,   2774.4098, -10752.6482,    388.2529,  -8218.3639,
           5643.0340,   2072.6436]], dtype=torch.float64)
	q_value: tensor([[-41.6058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.334404672902654
epoch: 29, step: 68
	action: tensor([[-2485.5588, -1469.1645,  1686.8866, -4036.3023, -5133.2122, -6110.7696,
         -2621.7277]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.637696866360656
epoch: 29, step: 69
	action: tensor([[ -980.6188, -2790.1268, -2161.6021, -4574.8158, -6499.5409,  3374.5456,
          -287.4343]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06359508898648647, distance: 1.180170772579813 entropy 9.637696866360656
epoch: 29, step: 70
	action: tensor([[ -4721.9474,  -2949.2933,  13737.9790, -10282.4247,  -4541.7603,
          -8629.7843,  -5016.6869]], dtype=torch.float64)
	q_value: tensor([[-35.5543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4420133428454083, distance: 1.3741727516986446 entropy 10.123687620565379
epoch: 29, step: 71
	action: tensor([[ -277.2747, -8768.5243,  5074.8857,  9659.6617, -3037.5237, -6999.1550,
          2751.5092]], dtype=torch.float64)
	q_value: tensor([[-34.7891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5803292923390415, distance: 1.4385683075103008 entropy 10.278529543843998
epoch: 29, step: 72
	action: tensor([[-5684.0398,  4398.0479,  4783.1208, -5777.6073, -2059.9698, 10661.7964,
         10086.2698]], dtype=torch.float64)
	q_value: tensor([[-32.5980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2568042382079736, distance: 1.2828932248086427 entropy 10.204370868645784
epoch: 29, step: 73
	action: tensor([[ -6971.8190, -11647.0669,   2132.9141,  -1215.8790,   3610.3571,
           1435.6900,   7972.6344]], dtype=torch.float64)
	q_value: tensor([[-37.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20364872296838699, distance: 1.0211958322804389 entropy 10.131189888297424
epoch: 29, step: 74
	action: tensor([[ 6315.2715, -8957.8471,  3486.0576,   680.9359, -1855.4953, -8498.1244,
          -866.4978]], dtype=torch.float64)
	q_value: tensor([[-41.6424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14570539714031439, distance: 1.0576951784176778 entropy 10.318493616408011
epoch: 29, step: 75
	action: tensor([[-7228.2729,  2768.0893, -3616.5472,  6648.9696, -4291.6442,   549.8850,
          2981.6325]], dtype=torch.float64)
	q_value: tensor([[-27.7463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1895162833721613, distance: 1.0302173040042497 entropy 10.024158242041192
epoch: 29, step: 76
	action: tensor([[ -1842.8955, -14711.0152,  -3473.0433,    681.8028,  -8250.7620,
          -9293.4154,  -1877.4628]], dtype=torch.float64)
	q_value: tensor([[-38.0878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03259801408825669, distance: 1.1255380479154158 entropy 10.277934936786211
epoch: 29, step: 77
	action: tensor([[-11022.9739, -11848.3943,   -999.6478,  19091.0520,  -5323.4036,
           6887.0415,   5246.1908]], dtype=torch.float64)
	q_value: tensor([[-29.2118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45542046378136924, distance: 1.38054615825992 entropy 10.159290642712508
epoch: 29, step: 78
	action: tensor([[-9315.9737, -5064.5554,  2670.9870,  7051.9149, -7856.9418,  3026.4971,
          3485.6850]], dtype=torch.float64)
	q_value: tensor([[-29.3588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6468732030286422, distance: 1.468543362751162 entropy 10.110348700621014
epoch: 29, step: 79
	action: tensor([[  4199.3897,   5598.6606,  -3065.7254,  -6707.2607,  -9280.7780,
           3571.2603, -10258.3815]], dtype=torch.float64)
	q_value: tensor([[-33.9558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.304836242548843
epoch: 29, step: 80
	action: tensor([[-4548.9218,   504.9837,  -866.1444,  2929.6383, -2465.9138,   833.0411,
          6342.4019]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14371478765420598, distance: 1.0589267400959141 entropy 9.637696866360656
epoch: 29, step: 81
	action: tensor([[-3067.4047, -6405.1648,   217.7275, -1370.8643,  1108.7430, -1139.0626,
          2637.2795]], dtype=torch.float64)
	q_value: tensor([[-32.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9694224456570939, distance: 1.6059282390674516 entropy 10.143805628012496
epoch: 29, step: 82
	action: tensor([[ -4193.7269, -13766.4703,  -4952.6342, -10141.7727,  12982.8412,
           8323.3198,  -9078.2045]], dtype=torch.float64)
	q_value: tensor([[-32.7840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31148945570872666, distance: 1.3105062451341871 entropy 10.168584747585095
epoch: 29, step: 83
	action: tensor([[ -341.4796,  -193.6750,   833.0800, 12817.0732,  8477.6916, -6821.3631,
            66.8521]], dtype=torch.float64)
	q_value: tensor([[-33.4546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33557996685738756, distance: 1.3224876995860437 entropy 10.125538034689116
epoch: 29, step: 84
	action: tensor([[-2000.0446,  3291.1484, -1671.5431, -9171.1925,  8578.2854,  4179.8969,
         -3511.8623]], dtype=torch.float64)
	q_value: tensor([[-30.7215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.126346424959442
epoch: 29, step: 85
	action: tensor([[-4179.8763, -4777.4340,   513.6930,  5553.8270, -2030.5817,     9.6255,
          2780.4737]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.637696866360656
epoch: 29, step: 86
	action: tensor([[-5.4620e+03, -1.0219e+03, -3.8618e+03,  1.1256e+00,  1.6883e+03,
          3.9173e+03, -2.2436e+03]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4130655351237271, distance: 1.3603098577757786 entropy 9.637696866360656
epoch: 29, step: 87
	action: tensor([[-6324.7001,  -322.3386,  -656.8064,  9064.7879,   269.7655,  -849.0575,
         11439.6590]], dtype=torch.float64)
	q_value: tensor([[-32.5567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06931236034430555, distance: 1.1833384787134 entropy 10.169925108329851
epoch: 29, step: 88
	action: tensor([[  700.6193, -3377.4294,  4029.5418,  6235.3153,  7894.6215, -1263.3379,
         12301.0063]], dtype=torch.float64)
	q_value: tensor([[-32.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37151981681542334, distance: 0.9071988425363012 entropy 10.214450994852616
epoch: 29, step: 89
	action: tensor([[ -5370.8821, -13910.5998,  -6353.3011,   7089.1162,   -709.6289,
          -7855.5765,  -1550.4626]], dtype=torch.float64)
	q_value: tensor([[-35.8657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8406885056137201, distance: 1.552554460986046 entropy 10.27687263121747
epoch: 29, step: 90
	action: tensor([[  773.0952, -3405.8525, -4714.5563,  5929.3720, -2364.4789,  6426.3377,
         -8023.4263]], dtype=torch.float64)
	q_value: tensor([[-26.5378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09131768767358062, distance: 1.1954524057630445 entropy 9.997947986874214
epoch: 29, step: 91
	action: tensor([[-3169.4741, -5151.8656, -2572.5173,  1667.6133,  6893.1579, -1634.8782,
          2058.4000]], dtype=torch.float64)
	q_value: tensor([[-26.8061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32058535233955254, distance: 1.315042931389831 entropy 10.011427045077188
epoch: 29, step: 92
	action: tensor([[ -9960.8096, -15564.0940,  -1327.0003,   4772.8476,  -3231.9202,
          -9384.4644,   2817.5388]], dtype=torch.float64)
	q_value: tensor([[-27.8264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3152222800330715, distance: 1.3123699328786365 entropy 10.168932173402796
epoch: 29, step: 93
	action: tensor([[-6517.0732,  6319.5220,  5626.4980, -1743.1900, -6533.3048,  9473.7884,
          4825.4931]], dtype=torch.float64)
	q_value: tensor([[-33.4240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24744065966617068, distance: 1.278105315524349 entropy 10.345737311782257
epoch: 29, step: 94
	action: tensor([[-5235.7603, -5004.3365,  5754.9195,   639.5337,  3450.7083,  1047.3489,
          2829.5786]], dtype=torch.float64)
	q_value: tensor([[-28.1948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3832967746974105, distance: 1.3459049036729016 entropy 10.060973591511926
epoch: 29, step: 95
	action: tensor([[  -311.5151,   3236.8671, -15856.3018,  -2468.2663,  -3570.9813,
          -6098.0170,   2860.9826]], dtype=torch.float64)
	q_value: tensor([[-32.4827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022864086593943123, distance: 1.1573525118022348 entropy 10.278111141806281
epoch: 29, step: 96
	action: tensor([[-6359.0229, -4749.8985,  3602.8567,  6279.0610,  7630.6462, -2491.7704,
          5312.1837]], dtype=torch.float64)
	q_value: tensor([[-38.4746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08300100749817851, distance: 1.0958247940578767 entropy 10.225852930764407
epoch: 29, step: 97
	action: tensor([[-10376.7525,    266.9393,    -54.6950,  -3608.2785,  -7432.2736,
           8053.9304,   2498.7124]], dtype=torch.float64)
	q_value: tensor([[-38.7648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20285692001847067, distance: 1.2550576602982455 entropy 10.405135367484604
epoch: 29, step: 98
	action: tensor([[-10415.4339,   1183.6818, -12635.5790,   2946.2571,  -1868.6785,
           1561.5606,   1982.6320]], dtype=torch.float64)
	q_value: tensor([[-34.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6375052368050268, distance: 0.6889815015710514 entropy 10.012200946032653
epoch: 29, step: 99
	action: tensor([[ -284.5652, -7507.7809, -8772.8207,  -581.6039, 10897.4445,  8925.5439,
          8812.2357]], dtype=torch.float64)
	q_value: tensor([[-38.7475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2026505500487874, distance: 1.254949992745627 entropy 10.363105158888501
epoch: 29, step: 100
	action: tensor([[ 9074.4534, -5799.6959, -4341.5162,  -690.5199,  3224.8433,  9290.3189,
          8402.3722]], dtype=torch.float64)
	q_value: tensor([[-33.6900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08687730672032168, distance: 1.0935062291978557 entropy 10.237026469573495
epoch: 29, step: 101
	action: tensor([[ 1924.4139, -9384.3171,  3520.7883,  3904.8337, -5662.3349,  2091.8889,
          4453.8569]], dtype=torch.float64)
	q_value: tensor([[-29.6340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10146412914488723, distance: 1.2009968612569777 entropy 10.013076844013458
epoch: 29, step: 102
	action: tensor([[-1604.9044, -8254.4720, -5494.3108,  3526.1713,  1094.5986, -5592.7261,
          4376.1863]], dtype=torch.float64)
	q_value: tensor([[-31.2455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13189780084431657, distance: 1.0662084533650706 entropy 9.921900504504702
epoch: 29, step: 103
	action: tensor([[-10905.6698,  -3352.3202,   9130.3121,   5744.3021,   6865.0030,
          -3325.3974,   -130.6098]], dtype=torch.float64)
	q_value: tensor([[-31.8561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13969915141600553, distance: 1.0614068079687609 entropy 10.243870410180884
epoch: 29, step: 104
	action: tensor([[  2063.5688,  -1267.7996, -11957.6263,  -3625.1425,    353.0184,
         -13640.6967,  -1193.7681]], dtype=torch.float64)
	q_value: tensor([[-31.0894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20201829174165753, distance: 1.254620072774715 entropy 10.267656596944107
epoch: 29, step: 105
	action: tensor([[-3111.4684, -1326.3515,   181.2363,  1500.5417,    28.4421,  5513.6978,
         -3599.8999]], dtype=torch.float64)
	q_value: tensor([[-25.9759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9239947436130316, distance: 1.5872986024423918 entropy 9.705585852648738
epoch: 29, step: 106
	action: tensor([[-6515.3923,  2532.3297,  2007.2972,  5746.0090,  1832.6266, -1212.7281,
         18080.0984]], dtype=torch.float64)
	q_value: tensor([[-32.2241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15310290230810053, distance: 1.0531058263915307 entropy 10.260342534275082
epoch: 29, step: 107
	action: tensor([[ 1080.4870, -4980.8048, -6614.6071, -6368.1320, -4642.0074, -2691.2374,
          2046.3988]], dtype=torch.float64)
	q_value: tensor([[-37.6866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021782866480394292, distance: 1.156740659410678 entropy 10.260678967782363
epoch: 29, step: 108
	action: tensor([[ 3613.5581,  4334.5408, -5515.0543, -4072.5278, -7851.8059, -6152.2888,
         -5821.1534]], dtype=torch.float64)
	q_value: tensor([[-34.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22902666534596205, distance: 1.0047924706364069 entropy 10.223357458585017
epoch: 29, step: 109
	action: tensor([[-22256.7864,  -1326.9321,    994.9934,  14368.1813,  -2453.4622,
           -528.8947,  -2470.0636]], dtype=torch.float64)
	q_value: tensor([[-43.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.203106545666225
epoch: 29, step: 110
	action: tensor([[ 6446.6928,  -533.1871, -5379.3104,  1964.9209,  1662.0210, -7855.5181,
          6713.0659]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6141226295974971, distance: 0.7108555334868086 entropy 9.637696866360656
epoch: 29, step: 111
	action: tensor([[  5459.0302, -17497.7118,   1745.2019,   2762.4251,   4414.5241,
           9394.5263,   5428.9783]], dtype=torch.float64)
	q_value: tensor([[-29.2059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07687731209046178, distance: 1.0994776505135588 entropy 10.019252726365059
epoch: 29, step: 112
	action: tensor([[  2972.3162, -14104.4390,   7925.7267,   3323.1255,  -5021.2162,
           3167.5740,    467.3044]], dtype=torch.float64)
	q_value: tensor([[-30.7213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06799577809806578, distance: 1.1047541282433342 entropy 10.18977356980883
epoch: 29, step: 113
	action: tensor([[-6931.1433, -9868.2742, -5077.6875, -1509.4978,  7790.5974,  -771.3977,
         -4336.4000]], dtype=torch.float64)
	q_value: tensor([[-40.3141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02603501782093054, distance: 1.1591450497651232 entropy 10.232463957311126
epoch: 29, step: 114
	action: tensor([[-1125.2276, -6459.9120, -3115.1522, -1147.3353, -3316.8099,  2995.3434,
          6822.4024]], dtype=torch.float64)
	q_value: tensor([[-27.9008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035353424343697926, distance: 1.1643968057541947 entropy 9.91258960162368
epoch: 29, step: 115
	action: tensor([[-9063.6662, -4763.4217, -1103.3996,  3412.6021, -7707.1528,   -21.9148,
          6667.4183]], dtype=torch.float64)
	q_value: tensor([[-30.2910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9474493094755152, distance: 1.596944321154161 entropy 9.941027795772824
epoch: 29, step: 116
	action: tensor([[ -1281.0768, -10880.3922,  10559.3432,   1146.7348,   7178.7755,
           5757.8565,  -6860.5005]], dtype=torch.float64)
	q_value: tensor([[-23.9052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31650354291322946, distance: 1.3130090193609458 entropy 9.931723619763867
epoch: 29, step: 117
	action: tensor([[-11678.9461,   1326.6093,  14403.8526,  -3068.2701,  -2182.7561,
          14487.7439,  11384.2317]], dtype=torch.float64)
	q_value: tensor([[-36.6618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5568333260779688, distance: 0.7617987229640412 entropy 10.413483514216068
epoch: 29, step: 118
	action: tensor([[-9552.0921, -7924.9321,  4794.6072,  2207.4938,  -238.0001, -3486.5700,
         10453.0463]], dtype=torch.float64)
	q_value: tensor([[-37.7809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28756406685293534, distance: 1.298497498296693 entropy 10.161334143116962
epoch: 29, step: 119
	action: tensor([[-18514.9746, -18044.2823,   -622.5680,   1940.2326,   9218.4690,
           2287.6550,   3611.3701]], dtype=torch.float64)
	q_value: tensor([[-30.8149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17309430512006319, distance: 1.0406020682127424 entropy 10.150860064211937
epoch: 29, step: 120
	action: tensor([[-8722.7068,    71.1959,  -686.2906,  6121.0729, -4177.3564,  9782.7555,
         -2776.4985]], dtype=torch.float64)
	q_value: tensor([[-27.5778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1844704521514653, distance: 1.033419241838028 entropy 10.067016004853135
epoch: 29, step: 121
	action: tensor([[  8844.3694,   3105.8047,   1443.8036,    893.9307,   1386.0674,
          -4368.5578, -10160.5850]], dtype=torch.float64)
	q_value: tensor([[-29.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38749838382289803, distance: 0.8955922211396263 entropy 10.020780975898976
epoch: 29, step: 122
	action: tensor([[-3453.1755, -1015.2392, -6513.4627, -5978.4673,  3542.9416,  5354.2354,
          -494.5239]], dtype=torch.float64)
	q_value: tensor([[-34.6775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.171991140677191
epoch: 29, step: 123
	action: tensor([[-9535.3471, -6154.6178,   132.3235,  -202.2080,  2534.4756,  7494.8976,
          4870.4081]], dtype=torch.float64)
	q_value: tensor([[-35.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5288710497049647, distance: 1.4149533502542528 entropy 9.637696866360656
epoch: 29, step: 124
	action: tensor([[-5435.4535, -8574.8994,  6427.0101,  6790.0627, -1191.9036,  1062.0566,
         -3304.4292]], dtype=torch.float64)
	q_value: tensor([[-26.3308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4493953314570478, distance: 1.3776856103916129 entropy 10.024195145929822
epoch: 29, step: 125
	action: tensor([[ -8563.0651,  -1649.1179,  -9847.1160,   7816.6042, -13378.4285,
           -180.5361,  -9513.4014]], dtype=torch.float64)
	q_value: tensor([[-29.1407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36862791260290906, distance: 1.3387497100334957 entropy 10.112466122309197
epoch: 29, step: 126
	action: tensor([[ 9179.8164, -5579.6016,  -809.3355,  6360.1485,  2783.2927, -7186.8956,
         -5375.5742]], dtype=torch.float64)
	q_value: tensor([[-29.3340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3798418236849189, distance: 0.9011724996223858 entropy 10.164369890144958
epoch: 29, step: 127
	action: tensor([[ -8890.8061,  -5596.5775, -11495.5306,  -5353.9978,  -6064.5020,
          -6647.5298,     64.1692]], dtype=torch.float64)
	q_value: tensor([[-33.5397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6000241402486399, distance: 1.447504627588442 entropy 10.129157459334932
LOSS epoch 29 actor 445.40624454456815 critic 119.89995585539707
epoch: 30, step: 0
	action: tensor([[-5512.9448, -9429.2220,  -303.9521,  2239.9272, -7937.4621, -3729.2448,
          1027.6716]], dtype=torch.float64)
	q_value: tensor([[-29.8847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3234372967707262, distance: 1.3164621530550902 entropy 10.131905446945348
epoch: 30, step: 1
	action: tensor([[-13801.5720,  -3585.0970,   1438.5306,   8122.8601,   3616.6393,
          -9053.0747,   5850.5412]], dtype=torch.float64)
	q_value: tensor([[-36.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23047873834195043, distance: 1.2693861344699078 entropy 10.466012297113997
epoch: 30, step: 2
	action: tensor([[ 1781.4857,  5872.4351, -5993.4968,  5382.2106,  6736.5011, 15657.9824,
         -2039.5290]], dtype=torch.float64)
	q_value: tensor([[-31.6935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6483285183784436, distance: 0.6786178342773724 entropy 10.313161924067671
epoch: 30, step: 3
	action: tensor([[-2225.9190, -3689.2960, -3097.2692,  -804.7090,  -843.2362,  3381.1666,
         -7511.5859]], dtype=torch.float64)
	q_value: tensor([[-30.0516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0064675555422378, distance: 1.6209617395400615 entropy 10.024890942058695
epoch: 30, step: 4
	action: tensor([[-4289.3094, -6177.4296,  1928.4543,  5151.7077, -7113.1528,  3807.9337,
          7965.4842]], dtype=torch.float64)
	q_value: tensor([[-26.3089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06000591993718363, distance: 1.1094794243538628 entropy 10.00212214577458
epoch: 30, step: 5
	action: tensor([[ 2285.8724, -7793.7581, -3634.0081, -2206.5910,  3880.5803,   374.5826,
          4233.5916]], dtype=torch.float64)
	q_value: tensor([[-31.5574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2019569401404363, distance: 1.0222799800918683 entropy 10.286380904422183
epoch: 30, step: 6
	action: tensor([[ -3883.9112, -11881.3962,  -1505.2541,   1027.1852,  -6732.8007,
           3082.7110,   8613.3687]], dtype=torch.float64)
	q_value: tensor([[-31.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6018004276453903, distance: 1.4483078876610815 entropy 10.143182767604314
epoch: 30, step: 7
	action: tensor([[ -7925.3586,  -5074.5399, -10886.8866,   3999.8352,  -1311.8837,
          -1989.5124,  -6365.6800]], dtype=torch.float64)
	q_value: tensor([[-33.8316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7553406890007264, distance: 1.5161333581230374 entropy 10.333015397402246
epoch: 30, step: 8
	action: tensor([[-4918.5098, -5276.6163, 11817.3934,  2881.7468,  3972.4796,  5378.5584,
          5326.0425]], dtype=torch.float64)
	q_value: tensor([[-30.3599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02400437306211689, distance: 1.1579974390349579 entropy 10.147354302567765
epoch: 30, step: 9
	action: tensor([[ -6447.6012, -15909.2484,   1774.1081,   7166.3108,  10377.9626,
           5689.7005,   -777.8173]], dtype=torch.float64)
	q_value: tensor([[-34.2937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7106734066291522, distance: 1.4967189086572956 entropy 10.402391803031303
epoch: 30, step: 10
	action: tensor([[ -7415.8376,    682.3677,  14323.5642,  11811.4921, -14605.1759,
          -3372.3621,  -5502.6362]], dtype=torch.float64)
	q_value: tensor([[-34.4134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13344340357329065, distance: 1.0652588711256985 entropy 10.431001256209223
epoch: 30, step: 11
	action: tensor([[  7519.1613,     37.4581,   5931.3746,   2806.6376, -11510.3244,
          36904.1766,   2722.7315]], dtype=torch.float64)
	q_value: tensor([[-47.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4260796701181633, distance: 0.8669269375332795 entropy 10.54733317051316
epoch: 30, step: 12
	action: tensor([[-5085.8380, -4794.6038,  -206.0685,  1701.7544,  -499.5773,  4334.5134,
          6367.2293]], dtype=torch.float64)
	q_value: tensor([[-28.5877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9068506295379319, distance: 1.580210817704438 entropy 9.98639346541835
epoch: 30, step: 13
	action: tensor([[ -8395.8275, -18417.2119,   1674.6946,  14140.8298,   7428.4732,
          -7309.5544,   4738.4808]], dtype=torch.float64)
	q_value: tensor([[-34.5949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2509068112960793, distance: 1.2798797621367206 entropy 10.36544368191004
epoch: 30, step: 14
	action: tensor([[  2639.9336,    948.1842,   4517.1221,  11618.7405, -10064.9002,
         -11189.9122,   7206.5679]], dtype=torch.float64)
	q_value: tensor([[-35.0184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12450311996778263, distance: 1.070739920064073 entropy 10.363780930613583
epoch: 30, step: 15
	action: tensor([[  407.6091, -9818.4285,  4122.2472,  7987.7463,  5580.4371, -4682.6290,
           487.6073]], dtype=torch.float64)
	q_value: tensor([[-28.5942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.867462525231812
epoch: 30, step: 16
	action: tensor([[-4072.7457,    30.0128,  4867.8450,  6110.4746,  -775.0449, -4305.8155,
         -1408.0188]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 17
	action: tensor([[-7113.9125, -9254.8396, -5105.4255,  1654.3465,  -269.6409,  4146.8579,
         -5174.5944]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 18
	action: tensor([[ 1900.8810, -2739.2550,  -830.5950, -2914.8592, -3322.8259,  2461.4661,
          4563.7509]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 19
	action: tensor([[-4286.2710, -2228.6826,  1856.0331,  3532.6584,  4244.8826,  2714.5589,
         -4273.2160]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 20
	action: tensor([[-4750.1520, -4607.4078,   -90.4963,  1148.9727, -8563.8975,  2620.7242,
          3445.0270]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 21
	action: tensor([[  626.7534,   228.4983,  -458.4800,  2941.3710, -3806.7705,  5599.7616,
         -4052.2363]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 22
	action: tensor([[ -227.2990,  4461.8715, -5742.7545,  2917.6573, -2349.8880,  1342.5377,
         -2354.3557]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 23
	action: tensor([[-4309.5528,  1521.1263,  1480.7220,  3344.9929, -5280.0395, -1192.9605,
         -1176.2074]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 24
	action: tensor([[ -1149.1272,  -3821.2384,  -6553.3420,  -2089.5959, -11751.4856,
          -1155.4118,   -398.7704]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1058508547016428, distance: 1.203386048674923 entropy 9.698452301247281
epoch: 30, step: 25
	action: tensor([[ 5727.8613,  5322.3119, -2179.4509, 11876.7528,   839.6198, -6709.6491,
         -8844.2583]], dtype=torch.float64)
	q_value: tensor([[-40.0441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.228779155306782
epoch: 30, step: 26
	action: tensor([[ -229.4925, -6386.1745, -5106.1251, -3678.0174,  4546.7506, -2478.8047,
         -3929.5403]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7431195317343366, distance: 1.5108462739987383 entropy 9.698452301247281
epoch: 30, step: 27
	action: tensor([[-23251.8459,   3014.2968, -12970.0794,  -6465.3987,   3055.4708,
          10917.3753,   7430.4749]], dtype=torch.float64)
	q_value: tensor([[-45.5174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5796582408260988, distance: 0.7419215092317989 entropy 10.386488268657782
epoch: 30, step: 28
	action: tensor([[  6627.7456, -15864.3748,   7115.2639,   3147.5194,   3095.8195,
          -1799.2110, -10792.2449]], dtype=torch.float64)
	q_value: tensor([[-49.0940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.272557555669975
epoch: 30, step: 29
	action: tensor([[-9523.1452, -5571.9371,  9543.4080,  4860.4951,  -413.1419,  1423.7030,
          4197.2806]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.698452301247281
epoch: 30, step: 30
	action: tensor([[-1762.5491,  3932.9219,  8401.5206,  5413.3190,  2890.3798, 11985.4309,
          6839.8091]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34329196400207174, distance: 0.92734825399161 entropy 9.698452301247281
epoch: 30, step: 31
	action: tensor([[ 5425.8449, -4242.4358,  4020.8124,  9439.6634, -2184.4331,  3699.4583,
         11195.4466]], dtype=torch.float64)
	q_value: tensor([[-37.7854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.975944293253587
epoch: 30, step: 32
	action: tensor([[-4784.4089, -3621.1961, -1473.4261,   676.9854,  2987.4548, -3153.8770,
           -22.9919]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27744742698717084, distance: 0.9727280044475087 entropy 9.698452301247281
epoch: 30, step: 33
	action: tensor([[ -1104.0352, -23476.6844,   6977.3140,  -5185.8834,  -4739.2830,
         -10034.8826,  -6636.9446]], dtype=torch.float64)
	q_value: tensor([[-36.3401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7869830247674943, distance: 1.5297374776382244 entropy 10.404363222272744
epoch: 30, step: 34
	action: tensor([[-9707.8360,  7263.7967,  4275.4027,  -591.7259,  5412.7536,  5280.5546,
          3901.4968]], dtype=torch.float64)
	q_value: tensor([[-36.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12473668526726933, distance: 1.070597084391184 entropy 10.278796382549848
epoch: 30, step: 35
	action: tensor([[ -1973.6242, -15034.4290,   -729.2952,    689.0749,  -5686.8483,
           4620.4239,  10524.6719]], dtype=torch.float64)
	q_value: tensor([[-43.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2761247973116372, distance: 1.2927164258598953 entropy 10.31090649609711
epoch: 30, step: 36
	action: tensor([[ -5764.5963, -11302.5920,   -149.0784,    595.6107,  -2657.8302,
          -5036.7720,  -3319.0313]], dtype=torch.float64)
	q_value: tensor([[-29.9937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22720255589868743, distance: 1.2676951209161837 entropy 10.108252733813197
epoch: 30, step: 37
	action: tensor([[-15926.2604,   4277.4726,  -1996.4849,  -2320.4440,  -4448.2307,
           2420.3848,  11465.5829]], dtype=torch.float64)
	q_value: tensor([[-32.5458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23966232782101982, distance: 0.9978377905296753 entropy 10.238349780698902
epoch: 30, step: 38
	action: tensor([[  160.6801, -5964.5282,  3513.1385,  3483.3901,  -313.6488, -5313.1833,
          3245.0312]], dtype=torch.float64)
	q_value: tensor([[-31.3812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3600215826958695, distance: 0.915459966798606 entropy 9.86008609032198
epoch: 30, step: 39
	action: tensor([[-8168.4399, -5888.1089,  4020.1050, -4975.9645,  -309.3823,  2410.4910,
         -3541.5980]], dtype=torch.float64)
	q_value: tensor([[-30.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5435586149571874, distance: 1.421733694909623 entropy 10.041286650703045
epoch: 30, step: 40
	action: tensor([[-8604.4292, -4929.6026, -2501.7915,  4761.2537,  9544.5527,   724.9012,
         14419.3768]], dtype=torch.float64)
	q_value: tensor([[-31.1223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008732354121341768, distance: 1.1493298033929562 entropy 10.148975551302401
epoch: 30, step: 41
	action: tensor([[-7855.8035, -4333.4694,  6307.6902, -2874.6417,  8396.4973,   276.3349,
          2245.6105]], dtype=torch.float64)
	q_value: tensor([[-29.3355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27203896179705467, distance: 1.2906452878132442 entropy 10.136078939066351
epoch: 30, step: 42
	action: tensor([[-9316.2762,   812.4365, -2549.8995, -1520.7870,  7062.2787, -1757.1868,
         13886.3752]], dtype=torch.float64)
	q_value: tensor([[-29.7687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1542954314915267, distance: 1.052364117720521 entropy 10.199754730801716
epoch: 30, step: 43
	action: tensor([[-7797.5988,   630.8027,  -535.7808, -2217.6747, -6994.5848,  -482.5088,
           371.4418]], dtype=torch.float64)
	q_value: tensor([[-40.5786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44314343028044545, distance: 0.8539419860004361 entropy 10.029497544642078
epoch: 30, step: 44
	action: tensor([[ -8909.0536,  -2097.2854,    510.0281,   4250.1311,  -1998.8373,
          12302.4884, -11577.3056]], dtype=torch.float64)
	q_value: tensor([[-46.8404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7601314701619586, distance: 1.5182009094414697 entropy 10.105915010973266
epoch: 30, step: 45
	action: tensor([[ -1633.5301, -10957.4063,   -566.7577,   6529.8529,   7253.3802,
            975.4553,  -1978.1302]], dtype=torch.float64)
	q_value: tensor([[-26.8429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9836650733120367, distance: 1.611724718591663 entropy 10.148917499328524
epoch: 30, step: 46
	action: tensor([[ 3890.9956,  2637.1729,  1328.6319, -2931.4906, -5343.5068, -9047.1454,
          1098.9077]], dtype=torch.float64)
	q_value: tensor([[-32.5703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.290148755728882
epoch: 30, step: 47
	action: tensor([[-4903.1926, -2092.9511, -2333.6472,  8044.8656,   261.9959,  5519.9412,
         -1442.6368]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17230766971782296, distance: 1.239017659760397 entropy 9.698452301247281
epoch: 30, step: 48
	action: tensor([[ -3676.0851,   3259.5270,   1201.4812,  10921.3558, -12124.2231,
          10925.8592,  12063.7766]], dtype=torch.float64)
	q_value: tensor([[-34.3741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07122781328154981, distance: 1.1843978581366004 entropy 10.381681233791758
epoch: 30, step: 49
	action: tensor([[-9852.4827,   591.3602,  -970.0648,  -858.2462, -2461.0078, -4100.5410,
          -471.9950]], dtype=torch.float64)
	q_value: tensor([[-33.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01072674751742031, distance: 1.138190160208798 entropy 10.227611218893784
epoch: 30, step: 50
	action: tensor([[ 2195.6679,  4288.3896, -6163.5563, 13035.5609,  6018.9552,  2818.0135,
         -1232.8871]], dtype=torch.float64)
	q_value: tensor([[-42.8273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46348132515146734, distance: 0.8382028147502171 entropy 10.139151222085092
epoch: 30, step: 51
	action: tensor([[  -896.9954, -10096.9956, -11872.8228,   -816.3932, -11796.9911,
         -10560.5655,  10042.4111]], dtype=torch.float64)
	q_value: tensor([[-26.5960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.997692843700749
epoch: 30, step: 52
	action: tensor([[-9255.1946, -1527.9456, -3567.2690,   219.2625, -1646.4443, -4323.2228,
           769.1652]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1227390473709962, distance: 1.6672663985006118 entropy 9.698452301247281
epoch: 30, step: 53
	action: tensor([[ 2007.9304, -6047.1545,  1101.8043, -1129.9791, -9359.2114,  3978.1311,
         -4254.2096]], dtype=torch.float64)
	q_value: tensor([[-33.8503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13018180242937927, distance: 1.067261733008184 entropy 10.262662266204856
epoch: 30, step: 54
	action: tensor([[-2767.1989, -8962.4240,  -268.9667, -8542.1715,  4955.6457, -1799.6988,
         -5557.0344]], dtype=torch.float64)
	q_value: tensor([[-36.4688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08046465604553288, distance: 1.1894932330757977 entropy 10.17062201899848
epoch: 30, step: 55
	action: tensor([[ -316.5906, -8362.6601,  -176.2005, -5259.7063,  2337.7099, 15681.1179,
         -6694.5193]], dtype=torch.float64)
	q_value: tensor([[-37.5544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20139483177010264, distance: 1.254294658904278 entropy 10.187590647651662
epoch: 30, step: 56
	action: tensor([[-2587.7333,  8791.6987,  6538.5330,  1248.8821, -4893.1500,  9315.1071,
         -4256.4286]], dtype=torch.float64)
	q_value: tensor([[-36.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5371718935475721, distance: 0.7785142308224962 entropy 10.181007857001239
epoch: 30, step: 57
	action: tensor([[-4332.0699, -8846.5694,  -481.8447,  -948.5393, -3369.0014,  8360.0965,
         -2117.1844]], dtype=torch.float64)
	q_value: tensor([[-38.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38042000213270666, distance: 1.3445046700073655 entropy 10.137903157571676
epoch: 30, step: 58
	action: tensor([[ -391.9612, -2672.5304,  3696.7402,  3742.4760, -4944.2474, -4153.9391,
         -5760.7751]], dtype=torch.float64)
	q_value: tensor([[-40.7188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9522993957322177, distance: 1.5989316646168126 entropy 10.257165058450289
epoch: 30, step: 59
	action: tensor([[ 3701.0582, -2553.5632,  -384.4073, -1073.3616,  2059.9396,  8135.4519,
          3275.9114]], dtype=torch.float64)
	q_value: tensor([[-27.9455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31485204009599443, distance: 0.9472156779744397 entropy 10.035057633286911
epoch: 30, step: 60
	action: tensor([[ -3365.6525,   3173.5183,   -289.8788, -12213.2283,  -4530.5836,
           6942.0224,   9035.1020]], dtype=torch.float64)
	q_value: tensor([[-33.9149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.19909283502382
epoch: 30, step: 61
	action: tensor([[-9222.1195, -9435.4482, -3736.1453, -3110.5053,   512.7110,  4435.6443,
          1064.9619]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15491671847175637, distance: 1.0519774931984311 entropy 9.698452301247281
epoch: 30, step: 62
	action: tensor([[ -5421.5958,  -9855.7145,  -8395.1802,   8674.1382,   8692.8748,
         -15634.1688,    560.8022]], dtype=torch.float64)
	q_value: tensor([[-38.0119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4932885900739761, distance: 1.398390827578157 entropy 10.353170623664104
epoch: 30, step: 63
	action: tensor([[ -5552.0449,   4237.6701,  -2376.4873,   4755.1772, -17640.7916,
          16401.6774, -11549.9440]], dtype=torch.float64)
	q_value: tensor([[-36.0947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22346483966989517, distance: 1.0084102597575397 entropy 10.37929953729263
epoch: 30, step: 64
	action: tensor([[-1457.4553,  2585.3206, -3252.7337,  9900.6683,  1267.5372, -1952.4827,
          2570.1233]], dtype=torch.float64)
	q_value: tensor([[-35.8735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38612680876256134, distance: 1.3472809679549744 entropy 10.259082210913192
epoch: 30, step: 65
	action: tensor([[   158.4401, -12374.9951,   4810.6056,   5362.8208,  -6064.5901,
          10060.9145,   2669.6221]], dtype=torch.float64)
	q_value: tensor([[-39.9916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008021992699739355, distance: 1.1397450511085372 entropy 10.331439478286926
epoch: 30, step: 66
	action: tensor([[-5171.4601, -1247.4887, 19903.9156,  3263.5945,  3928.2991,  1916.3805,
         11920.9432]], dtype=torch.float64)
	q_value: tensor([[-28.4920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24382708413492282, distance: 1.276252770619021 entropy 10.211489080424027
epoch: 30, step: 67
	action: tensor([[10926.4427,  -151.9642,  -936.1413,  2396.0933,  -620.9970,  8587.9218,
          5888.9449]], dtype=torch.float64)
	q_value: tensor([[-30.1635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40319752575253076, distance: 0.8840401727377817 entropy 10.196140232727567
epoch: 30, step: 68
	action: tensor([[-18116.3167,  -3777.3778,  -1935.9640,  -1034.3357,   1768.0270,
           7790.3875,   1649.6144]], dtype=torch.float64)
	q_value: tensor([[-29.7936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09672901407391987, distance: 1.1984125812933215 entropy 10.027060369983687
epoch: 30, step: 69
	action: tensor([[-8889.1305, -5172.0777,  2498.4525,  2843.2335,  7949.8011,  4497.5985,
          2604.8506]], dtype=torch.float64)
	q_value: tensor([[-32.1121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4934475017031985, distance: 1.3984652320350275 entropy 10.286869169041834
epoch: 30, step: 70
	action: tensor([[-2306.5210,  4531.3141, -2414.0946,  7265.3047,  8285.2236,  5662.9534,
         18291.1415]], dtype=torch.float64)
	q_value: tensor([[-35.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16597184718571012, distance: 1.2356649428462767 entropy 10.395332314134487
epoch: 30, step: 71
	action: tensor([[ 8974.5298,  1002.8013, -4716.6497, 15254.9473,  4926.4869, -6345.0999,
          -330.4251]], dtype=torch.float64)
	q_value: tensor([[-35.0690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.279055058801605
epoch: 30, step: 72
	action: tensor([[-2718.6122, -3897.2315,  4339.7694,  -621.7936,  5583.1992,  3061.5503,
         -3387.3562]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1767725262387373, distance: 1.0382850987933425 entropy 9.698452301247281
epoch: 30, step: 73
	action: tensor([[-12674.7428,   1517.2292,  -1070.0772,    106.9181,  -6180.6587,
           5889.3341,  -2746.9287]], dtype=torch.float64)
	q_value: tensor([[-40.6940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.666679521894485, distance: 0.6606747230378859 entropy 10.275253858504872
epoch: 30, step: 74
	action: tensor([[ 4867.0532,  3985.4493,  6354.6906,  6325.2961,  -423.9137, -2688.1452,
         13654.9092]], dtype=torch.float64)
	q_value: tensor([[-41.0387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.100371590458463
epoch: 30, step: 75
	action: tensor([[-3223.6487,  -649.4206,  1879.2440, -3132.1617,  2301.7633,   302.5222,
           838.4775]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08203646516644125, distance: 1.0964009624148334 entropy 9.698452301247281
epoch: 30, step: 76
	action: tensor([[-13283.4152,  -2033.1071,    969.0349, -14783.4868,   -312.4100,
          -8940.7204,   3469.9891]], dtype=torch.float64)
	q_value: tensor([[-32.8353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6310750094705544, distance: 1.4614826370049165 entropy 10.194755309087638
epoch: 30, step: 77
	action: tensor([[-4347.2587,   111.5652, -3289.1273,   902.3714, 10167.8933, -3493.2049,
          1393.5495]], dtype=torch.float64)
	q_value: tensor([[-34.3272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11497411720386763, distance: 1.2083398161391636 entropy 10.233727333251718
epoch: 30, step: 78
	action: tensor([[-3688.8769, -4225.0529, -5582.3917,  1925.7137, -5843.9427, -7481.0085,
         -5002.9325]], dtype=torch.float64)
	q_value: tensor([[-30.5181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4834932933381655, distance: 1.3937968764237805 entropy 9.986226823736411
epoch: 30, step: 79
	action: tensor([[ 2662.6809, -9456.2476, -2433.3726, 15062.7208,  -176.7052, -1051.1860,
         -6568.4247]], dtype=torch.float64)
	q_value: tensor([[-33.4659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23160356293639373, distance: 1.0031118583666547 entropy 10.347717154066101
epoch: 30, step: 80
	action: tensor([[  1162.9880, -10252.9665, -10437.1963,   9047.6610,  -6133.4021,
           7328.3343,  -2804.9905]], dtype=torch.float64)
	q_value: tensor([[-31.3846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28455929201173813, distance: 0.9679290337240144 entropy 10.220389705686086
epoch: 30, step: 81
	action: tensor([[-4007.4356, -3624.9732,  5147.6875, -5997.3682, -5034.4066,  1120.6635,
          5801.0455]], dtype=torch.float64)
	q_value: tensor([[-32.7720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8577939177697973, distance: 1.5597516783674716 entropy 10.248247503492419
epoch: 30, step: 82
	action: tensor([[-10682.3720,  -7888.6529,   6187.8101,    964.6874,  -2777.4852,
           -920.5710,   -582.0626]], dtype=torch.float64)
	q_value: tensor([[-25.6140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.345378190123506, distance: 1.7525206114354956 entropy 9.978199850159196
epoch: 30, step: 83
	action: tensor([[ -3300.8570,  -9547.1703, -10910.6655,   7402.4725,  -4892.9962,
           -406.9003,  -9163.0001]], dtype=torch.float64)
	q_value: tensor([[-27.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03945269258973283, distance: 1.1215433709226121 entropy 10.14376529399202
epoch: 30, step: 84
	action: tensor([[ -6718.6265, -10053.2329, -14672.9995,   3978.4482,   1099.4334,
          -3982.4920,   4863.2062]], dtype=torch.float64)
	q_value: tensor([[-33.3242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8500162360047445, distance: 1.5564832922361809 entropy 10.368437721586348
epoch: 30, step: 85
	action: tensor([[  212.3964, -3838.3618, -5845.9030, -2252.4363,  3675.8701, -1009.1677,
          9066.2007]], dtype=torch.float64)
	q_value: tensor([[-29.1019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5452758098764225, distance: 0.7716684109881308 entropy 10.155116952755607
epoch: 30, step: 86
	action: tensor([[ 1579.6892,   372.2514, -3574.3454,  3355.2668,  7503.4769,  2160.7698,
         -3759.0539]], dtype=torch.float64)
	q_value: tensor([[-30.1991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.239546544914236, distance: 0.9979137621447707 entropy 10.021830699401322
epoch: 30, step: 87
	action: tensor([[-9554.7117, -8593.1453, -1924.4981,   771.8514, -2604.6579, -1780.3824,
         -1341.8652]], dtype=torch.float64)
	q_value: tensor([[-29.2763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2821933173063841, distance: 0.9695281916714533 entropy 9.953120689171941
epoch: 30, step: 88
	action: tensor([[  4728.3970, -21117.5645, -10943.7879,  -3518.8604,   7639.2893,
          -2866.5998,    439.9568]], dtype=torch.float64)
	q_value: tensor([[-39.0071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00043741495555571497, distance: 1.144594503282799 entropy 10.471443328069897
epoch: 30, step: 89
	action: tensor([[-10899.6822,  -6769.2533,   4162.1610,   1112.8020,   1464.4329,
          -3027.8313,  -5394.8438]], dtype=torch.float64)
	q_value: tensor([[-32.4256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012133065366988438, distance: 1.1512655250566772 entropy 10.026429720629489
epoch: 30, step: 90
	action: tensor([[  3888.6329,  -7279.8214,  -2800.2748,    854.7966,   6110.6123,
         -18679.9842,  -3012.3764]], dtype=torch.float64)
	q_value: tensor([[-33.8045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40101764254593286, distance: 0.8856532255305885 entropy 10.242236409684264
epoch: 30, step: 91
	action: tensor([[  1966.6423,  -1924.9660,  -4425.1306,   5551.7878,   4830.9640,
         -11674.3134,  12297.3306]], dtype=torch.float64)
	q_value: tensor([[-43.5677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2876513778999693, distance: 0.9658351072265092 entropy 10.332781129433487
epoch: 30, step: 92
	action: tensor([[ -1147.6508, -11065.8531,  -1053.4186,   -261.6857,    -82.2140,
           2299.7529,  -8702.2601]], dtype=torch.float64)
	q_value: tensor([[-33.8344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027947564822936033, distance: 1.1282401212362083 entropy 10.23043417738016
epoch: 30, step: 93
	action: tensor([[-2470.8989,  -191.6557,  6824.3246, -1301.5917, -1582.3916,  2836.6359,
          -319.8808]], dtype=torch.float64)
	q_value: tensor([[-25.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23665825142749042, distance: 1.2725695963943469 entropy 9.891627749317133
epoch: 30, step: 94
	action: tensor([[ -3385.9610,  -4650.8130,   -280.3896,   4749.8355,  -2110.6159,
           6947.1280, -14901.1847]], dtype=torch.float64)
	q_value: tensor([[-29.3139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0401125941337912, distance: 1.121158051347603 entropy 10.008427674337442
epoch: 30, step: 95
	action: tensor([[ -5211.8070, -16845.9927, -10082.9653,   3307.6554,  -2921.6983,
            785.9969,  -5414.0481]], dtype=torch.float64)
	q_value: tensor([[-29.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1012718371925696, distance: 1.08485293632786 entropy 10.209021814706503
epoch: 30, step: 96
	action: tensor([[  8232.5707, -11130.3166,   -874.4391,   8090.7442,   1531.7238,
         -16021.0811,  -6517.0491]], dtype=torch.float64)
	q_value: tensor([[-30.3351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5317376992339709, distance: 0.7830712702089374 entropy 10.220084729386615
epoch: 30, step: 97
	action: tensor([[13876.0320, -6692.4051,  2701.8057,   283.6371, -2923.0181,  8165.5745,
           921.8532]], dtype=torch.float64)
	q_value: tensor([[-32.0588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5663763257387211, distance: 0.7535519288018035 entropy 10.194537868901225
epoch: 30, step: 98
	action: tensor([[-10005.3264,   1099.2025,   -559.9201,   5739.2612,  -3402.9119,
            -85.0098,    152.0825]], dtype=torch.float64)
	q_value: tensor([[-31.6881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.12520493366635
epoch: 30, step: 99
	action: tensor([[-2002.6551,  -867.2870,  3768.7538, -2620.3688,  4196.6875, -3986.1446,
         -1138.7414]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3645544112247249, distance: 0.9122122034370607 entropy 9.698452301247281
epoch: 30, step: 100
	action: tensor([[  4646.4739, -10643.6902,  -2161.5519,   1591.1509,  -9762.4505,
          -8667.1357,   -891.4567]], dtype=torch.float64)
	q_value: tensor([[-35.3511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2577827814646124, distance: 0.9858757991803763 entropy 10.042643878881657
epoch: 30, step: 101
	action: tensor([[ 6173.9039,  1221.5019, -2698.2513,  8673.7315,  1652.7324,  8167.5868,
          -410.5876]], dtype=torch.float64)
	q_value: tensor([[-31.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3197699063866467, distance: 0.943810085656595 entropy 9.879854908665802
epoch: 30, step: 102
	action: tensor([[-1514.3000,  1844.8027, -3106.5204, -6148.8276,  8106.6598, -9110.5884,
         -4285.5170]], dtype=torch.float64)
	q_value: tensor([[-29.6346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.098531664870602
epoch: 30, step: 103
	action: tensor([[ 3138.7651, -4222.6740,   959.3076,  -988.7218,  1843.2950, -2761.3990,
          2897.9253]], dtype=torch.float64)
	q_value: tensor([[-36.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26141563723671446, distance: 1.2852446315424457 entropy 9.698452301247281
epoch: 30, step: 104
	action: tensor([[  -814.7974, -11983.4589,   -713.3108,    -78.9131,  -3238.9302,
           1657.2136,   4167.9995]], dtype=torch.float64)
	q_value: tensor([[-35.0095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3410223484369934, distance: 0.9289493526109449 entropy 10.158536788320633
epoch: 30, step: 105
	action: tensor([[-10167.2949,  -4272.0764,  13901.5368,   4498.9604,  -9904.2341,
            -23.5788,  11526.1652]], dtype=torch.float64)
	q_value: tensor([[-37.8929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.215019845729505, distance: 1.7031210006008648 entropy 10.369090321614141
epoch: 30, step: 106
	action: tensor([[ -7173.8277, -11441.5417, -16502.5426,  -6661.9500,   6052.9034,
           4989.7150, -10149.0109]], dtype=torch.float64)
	q_value: tensor([[-33.9984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26455433901506775, distance: 1.2868426350955238 entropy 10.37391880193967
epoch: 30, step: 107
	action: tensor([[-1949.8080,   504.3517, -5977.9208,  9062.8034,  9114.9170, 13925.1637,
         11108.4968]], dtype=torch.float64)
	q_value: tensor([[-41.3138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1384206371428658, distance: 1.0621952065583429 entropy 10.376548614334752
epoch: 30, step: 108
	action: tensor([[ -7756.2216,  -1651.2696,  -4755.6378,  -5642.7230, -11387.0333,
          12494.5883,    917.0245]], dtype=torch.float64)
	q_value: tensor([[-40.4873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3403802268319287, distance: 1.3248621702287406 entropy 10.197362793364814
epoch: 30, step: 109
	action: tensor([[ 3015.4134,   152.6526,  6073.5680, -1443.9539,  2767.4638, 10090.2296,
          3357.4696]], dtype=torch.float64)
	q_value: tensor([[-34.7340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4321244398762898, distance: 0.8623494334249519 entropy 10.161315627561823
epoch: 30, step: 110
	action: tensor([[ -277.7924, -1789.2721, -4337.6275, 13017.9851,   922.0205, -5828.4594,
         -3796.8002]], dtype=torch.float64)
	q_value: tensor([[-39.1382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7571318194553411, distance: 1.5169066838543184 entropy 10.29473014045276
epoch: 30, step: 111
	action: tensor([[  3330.3000, -19098.1255,  22462.4863,   3447.5047, -12747.5070,
           -158.3108,  -6075.9828]], dtype=torch.float64)
	q_value: tensor([[-31.8907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3643344409558881, distance: 0.9123700786389249 entropy 10.256152578433344
epoch: 30, step: 112
	action: tensor([[-2321.4244, -5304.5732,   405.4033,  5394.3978, -4646.6216,  1541.0660,
          9842.6244]], dtype=torch.float64)
	q_value: tensor([[-31.8841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7230423236750902, distance: 1.5021201291620012 entropy 10.009521224899604
epoch: 30, step: 113
	action: tensor([[ 5199.2113, -1489.3856,   199.4883, -1780.9427, -4994.4192,  1226.3153,
          -330.0087]], dtype=torch.float64)
	q_value: tensor([[-33.4418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2682296883219384, distance: 0.9789129780218107 entropy 10.347029564298227
epoch: 30, step: 114
	action: tensor([[ 2079.6203,  2487.8025,  7695.7946, -5486.1802, -6258.3336,  2265.9012,
         -9656.6679]], dtype=torch.float64)
	q_value: tensor([[-32.5089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4979733770734862, distance: 0.8108118133882084 entropy 10.149727151420736
epoch: 30, step: 115
	action: tensor([[-5513.4948, -5939.7065,  1296.6272,   -42.5169,   236.5610, 12228.2005,
         -6457.4629]], dtype=torch.float64)
	q_value: tensor([[-34.3315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2516885179833983, distance: 0.9899149833743159 entropy 9.911624164360648
epoch: 30, step: 116
	action: tensor([[ -165.9411,   183.4759,   860.0882,  4382.7014, -4216.2075, -2683.0725,
         -7063.3526]], dtype=torch.float64)
	q_value: tensor([[-38.3518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18671046499106003, distance: 1.2466056168599706 entropy 10.269278310558926
epoch: 30, step: 117
	action: tensor([[-4345.6016, -8702.6334,  1147.3907,  5349.8497, -5158.1479,  9007.1490,
         -4467.9900]], dtype=torch.float64)
	q_value: tensor([[-31.4878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013789127834575865, distance: 1.1364271120370817 entropy 9.821215260543537
epoch: 30, step: 118
	action: tensor([[-8138.1411, -2035.8468, -1740.9673,  3840.7768,  2620.0075,  5148.5484,
          5317.4429]], dtype=torch.float64)
	q_value: tensor([[-31.3115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12317699629975076, distance: 1.0715505433185455 entropy 10.263550192214277
epoch: 30, step: 119
	action: tensor([[ 1905.4752,  2671.5857, -6465.0291,  5959.7292, -8939.4474,  5672.9054,
         17351.9315]], dtype=torch.float64)
	q_value: tensor([[-28.6485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5476538342686448, distance: 0.7696480085374987 entropy 10.164696559559996
epoch: 30, step: 120
	action: tensor([[   40.8423,  3268.9569, -3090.5866, -3678.5165, -3992.8449, -2925.5801,
         -2348.2920]], dtype=torch.float64)
	q_value: tensor([[-30.8133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38193260865504075, distance: 0.8996521224650388 entropy 10.133886161732278
epoch: 30, step: 121
	action: tensor([[ -8752.2675, -10166.2352,  -3737.4173, -11364.1376,  -7563.9668,
           1886.4425,   2574.0423]], dtype=torch.float64)
	q_value: tensor([[-37.6882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4003788718760446, distance: 1.3541895812832256 entropy 10.23462409559457
epoch: 30, step: 122
	action: tensor([[-5241.1823, -3534.1267,   248.4060,  8951.8251, -3481.3823, -3651.7193,
          5599.7520]], dtype=torch.float64)
	q_value: tensor([[-31.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9918015137690459, distance: 1.6150267585134768 entropy 10.031494238501962
epoch: 30, step: 123
	action: tensor([[-2360.7307,  4095.5189, -7041.9199,  2935.9725, -4197.0375, -8651.0223,
          6839.5450]], dtype=torch.float64)
	q_value: tensor([[-30.0368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5101709579548591, distance: 0.8009012264998332 entropy 10.215808000084163
epoch: 30, step: 124
	action: tensor([[ -3580.6708,    577.4749, -19786.0700,  -6512.1299,   4651.4883,
          -8327.6375,  10833.5847]], dtype=torch.float64)
	q_value: tensor([[-35.7206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11818513246275741, distance: 1.2100785148410478 entropy 10.218904865806305
epoch: 30, step: 125
	action: tensor([[-11936.5606,   2272.0530,   7656.8010,  -4965.5183,  11563.0177,
          -3980.7184,   3076.0154]], dtype=torch.float64)
	q_value: tensor([[-38.2670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2451113829495838, distance: 0.9942557965559912 entropy 10.010894921503118
epoch: 30, step: 126
	action: tensor([[ -3618.5747,  -4284.8629,  10434.7701,  -4301.1342, -16159.9005,
         -22628.5104,   4078.0316]], dtype=torch.float64)
	q_value: tensor([[-53.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.168136132470165, distance: 1.0437171596355437 entropy 10.32680567226507
epoch: 30, step: 127
	action: tensor([[-11350.1943,   2676.3836,   2593.0378,  12182.4364,   3204.2808,
            138.2685,    431.0145]], dtype=torch.float64)
	q_value: tensor([[-38.7093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5554248215716431, distance: 1.427188067103443 entropy 10.132908014346764
LOSS epoch 30 actor 473.6555872792889 critic 113.51320554103879
epoch: 31, step: 0
	action: tensor([[-1413.4399, -3378.2941, -1226.4195,   151.3186,  2054.8987, -3594.0262,
           -18.0857]], dtype=torch.float64)
	q_value: tensor([[-29.6684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7886254359786133, distance: 1.530440304949776 entropy 9.939205264553253
epoch: 31, step: 1
	action: tensor([[-5475.3901, -1235.7259,  1837.6787,  5144.1127,   -89.8922,  4176.5707,
          9717.3099]], dtype=torch.float64)
	q_value: tensor([[-23.9900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12045836763158302, distance: 1.073210452685567 entropy 10.080471509244173
epoch: 31, step: 2
	action: tensor([[ -7163.6222, -18716.2870,    569.4715, -13282.7774,   2411.5133,
          -2789.6750,   2076.0108]], dtype=torch.float64)
	q_value: tensor([[-29.9298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1218253528620257, distance: 1.666907537594592 entropy 10.537913722293297
epoch: 31, step: 3
	action: tensor([[ -5840.9126, -10265.2877,  -9533.2496,    258.9406,  -4563.2771,
         -11471.0222,   2885.5799]], dtype=torch.float64)
	q_value: tensor([[-30.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7659093578589369, distance: 1.5206907255316897 entropy 10.36178874399967
epoch: 31, step: 4
	action: tensor([[  3072.7662, -19279.3028,  -5903.6240,   6255.3819,  -5898.8068,
         -14198.7369,   -169.9147]], dtype=torch.float64)
	q_value: tensor([[-27.5435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15301576903094305, distance: 1.2287805168735804 entropy 10.308388117276081
epoch: 31, step: 5
	action: tensor([[ -8784.9965,  -3021.0552, -13429.3297,   -471.9703,  -5122.0671,
           7288.0234,  -7961.7633]], dtype=torch.float64)
	q_value: tensor([[-27.7037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4591737221006318, distance: 1.3823250978248531 entropy 10.291248452040128
epoch: 31, step: 6
	action: tensor([[ -619.5169, -5596.3434,  4572.0869, 11625.5329,  4691.8350,  6602.3904,
         -5511.8574]], dtype=torch.float64)
	q_value: tensor([[-27.5946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06469972049674655, distance: 1.1807834660399195 entropy 10.216981323100189
epoch: 31, step: 7
	action: tensor([[   417.6375,   1596.1476,  -4529.7064,   2985.7896,   3232.8359,
          -3392.8144, -13053.0689]], dtype=torch.float64)
	q_value: tensor([[-28.4471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4981641693684149, distance: 0.8106577265925587 entropy 10.407588658959307
epoch: 31, step: 8
	action: tensor([[-13014.8626,  -9398.2088,   -989.7351,   4692.9979,   3289.4952,
           9460.6974,  -7096.4531]], dtype=torch.float64)
	q_value: tensor([[-31.4499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.394076588302925
epoch: 31, step: 9
	action: tensor([[  -43.9741, -2948.9417,  4194.4973, -6701.2880,  2586.7487,  1760.0977,
           524.8756]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.759081311472238
epoch: 31, step: 10
	action: tensor([[-3027.2779,  2287.9161, -2730.2432,   145.6859, -7858.5316,  5717.3728,
          3715.9328]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5349064882300709, distance: 0.780417202340429 entropy 9.759081311472238
epoch: 31, step: 11
	action: tensor([[  560.6790,  -873.6424,  1405.7090,  9933.5811, -2376.8300,  -488.3369,
          5827.2456]], dtype=torch.float64)
	q_value: tensor([[-29.6198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6127231650105409, distance: 0.7121433996275152 entropy 9.962038638737587
epoch: 31, step: 12
	action: tensor([[-5633.7944, -7819.9932, -7654.6981,  -533.6985, -9640.9137,  8843.2650,
          3522.5118]], dtype=torch.float64)
	q_value: tensor([[-28.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4265915816607426, distance: 1.366804883163211 entropy 10.316484161690257
epoch: 31, step: 13
	action: tensor([[-9014.1834, -5691.4385, -1854.7954, 12039.7598,  8215.0259, -1756.0330,
          5215.8992]], dtype=torch.float64)
	q_value: tensor([[-30.1259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17894008290300645, distance: 1.036917296315388 entropy 10.378996598964765
epoch: 31, step: 14
	action: tensor([[ 2080.9003,  3865.9617, 10905.0685, -8439.9160,  8308.2667, 13840.0179,
          2441.2765]], dtype=torch.float64)
	q_value: tensor([[-26.7915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.325141690185337
epoch: 31, step: 15
	action: tensor([[ 2830.7597, -3829.6446,  2597.9284,   -62.2148,  8631.4887,  2722.6010,
         -1556.6522]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4741333594601229, distance: 0.829840265660467 entropy 9.759081311472238
epoch: 31, step: 16
	action: tensor([[   -68.9308,  -4945.6707,  10402.9483,  11531.6085,   3865.9614,
         -14949.5462,  -6065.8682]], dtype=torch.float64)
	q_value: tensor([[-28.6022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15721668288244617, distance: 1.231016959752933 entropy 10.197256618021452
epoch: 31, step: 17
	action: tensor([[-3575.3855,  1817.9925,  6501.2231,  2213.0978,  6409.6172,  -285.6643,
         -7602.1215]], dtype=torch.float64)
	q_value: tensor([[-29.1038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20765046610878768, distance: 1.2575559602773698 entropy 10.26969910178251
epoch: 31, step: 18
	action: tensor([[ -454.2612, -5568.4155,  2776.2704, -1010.8830,  9907.3817,  4481.6555,
          4383.4738]], dtype=torch.float64)
	q_value: tensor([[-30.8688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7849934168707278, distance: 1.5288856437464031 entropy 10.172584214941606
epoch: 31, step: 19
	action: tensor([[-8721.2053,  1666.0348, -6419.8068, 12665.7961,  3247.0985,  -207.4660,
          3001.0441]], dtype=torch.float64)
	q_value: tensor([[-24.4539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7400412044730502, distance: 0.5834571298693454 entropy 10.067282157249524
epoch: 31, step: 20
	action: tensor([[ -6306.6680,  -6124.6110,  -2760.2224,  -1976.0202, -17236.9488,
           8516.9561,  11167.5355]], dtype=torch.float64)
	q_value: tensor([[-26.4431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10795608031541648, distance: 1.204530956485301 entropy 10.205789408276518
epoch: 31, step: 21
	action: tensor([[-1950.5344,   908.0583, 14780.2251,  7538.4656,  -377.4016,  7035.3736,
         -7703.9017]], dtype=torch.float64)
	q_value: tensor([[-29.4719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24032773137305807, distance: 0.9974010700014319 entropy 10.291383202547237
epoch: 31, step: 22
	action: tensor([[-8242.5372, -1512.0870,  2781.1232, -5103.9831,   928.3672,  1281.7800,
          8880.1541]], dtype=torch.float64)
	q_value: tensor([[-32.0604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3020318448032653, distance: 1.3057724350865532 entropy 10.284758005853762
epoch: 31, step: 23
	action: tensor([[-8979.0433, -8608.7142,   737.6951,  5360.3920,  4502.7348, -1845.3565,
         -1167.4233]], dtype=torch.float64)
	q_value: tensor([[-30.3048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09150752380563276, distance: 1.090730257191639 entropy 10.296087850126428
epoch: 31, step: 24
	action: tensor([[ -830.4124, -8893.9185, -2619.6724, -3918.8126,  8949.7562, -1530.1601,
          3447.1084]], dtype=torch.float64)
	q_value: tensor([[-26.8061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8648562380546543, distance: 1.5627135292388883 entropy 10.375590853488848
epoch: 31, step: 25
	action: tensor([[   401.6061, -12499.3950,    678.2140,  -1812.2424,  -5686.4409,
           9100.5709,  15540.1801]], dtype=torch.float64)
	q_value: tensor([[-29.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32720327107142055, distance: 1.3183338853631403 entropy 10.33709465628547
epoch: 31, step: 26
	action: tensor([[-9983.9201, -3012.4345, -4443.9964, -5181.0381, -1812.3837, -9016.4311,
          1075.3211]], dtype=torch.float64)
	q_value: tensor([[-24.3823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36868118036875375, distance: 1.3387757622247296 entropy 10.021421125120865
epoch: 31, step: 27
	action: tensor([[  2420.6955,   3567.7567,   4020.1665,  10003.6696,   1497.1499,
         -10869.3242, -15180.9614]], dtype=torch.float64)
	q_value: tensor([[-34.8791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.374282546591783
epoch: 31, step: 28
	action: tensor([[-6079.8915, -8014.8715,  4074.7518, -2235.2054, -3001.1515,  6157.0903,
         -9675.1296]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2450357048984575, distance: 1.2768726843869365 entropy 9.759081311472238
epoch: 31, step: 29
	action: tensor([[  5068.9952,   -519.1105,   8275.9944,   3238.8238, -19691.6871,
           4185.2410,  10934.8809]], dtype=torch.float64)
	q_value: tensor([[-35.7982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2453357856995222, distance: 0.9941080063301561 entropy 10.443517783255416
epoch: 31, step: 30
	action: tensor([[-1060.2360,  3856.8965, -5900.9433,  7957.3120,  8046.2400, -3058.5076,
          8509.0742]], dtype=torch.float64)
	q_value: tensor([[-26.1605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.07663506157759
epoch: 31, step: 31
	action: tensor([[ 4269.6020,  4601.0812,  9241.8380,    22.0671, -5737.8385,   127.2416,
          3804.2840]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.849134053738855, distance: 0.44448008162725855 entropy 9.759081311472238
epoch: 31, step: 32
	action: tensor([[  -696.5122, -12027.8010,  -3295.1631,   1618.6390,   5136.7944,
           3187.5268,   4224.6060]], dtype=torch.float64)
	q_value: tensor([[-24.6368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.104540013689883
epoch: 31, step: 33
	action: tensor([[-2994.7687, -1361.1930,   375.3106, -1102.7082, -1050.8964,  -621.6294,
         -2446.5223]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4065312590291825, distance: 1.357161051344145 entropy 9.759081311472238
epoch: 31, step: 34
	action: tensor([[-7550.9262,  8062.2210,  2853.5909, -1001.0860, -2685.5691,  3074.1737,
         -1317.7001]], dtype=torch.float64)
	q_value: tensor([[-36.3054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01886030956978857, distance: 1.155085189665842 entropy 10.346965937468793
epoch: 31, step: 35
	action: tensor([[-7667.7069, -7060.0788, -1741.6295,  -958.9632,  1535.0153,  8673.3078,
          5516.0800]], dtype=torch.float64)
	q_value: tensor([[-40.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17261650634735815, distance: 1.2391808544785008 entropy 10.367853013781588
epoch: 31, step: 36
	action: tensor([[ 3088.2600,  1377.3470,  -446.0140,  6155.0742, -9280.7091,  2513.3028,
          1244.3355]], dtype=torch.float64)
	q_value: tensor([[-33.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.344378974304488
epoch: 31, step: 37
	action: tensor([[-3593.6531, -1401.9321,   714.4451,  2638.8378,  3201.2981, -4207.5286,
           914.4184]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5860209063388058, distance: 1.4411565075330548 entropy 9.759081311472238
epoch: 31, step: 38
	action: tensor([[ -8676.8372, -19762.7244,  -5463.5014,  -2864.7564,  -8340.9372,
          11283.9536,   1598.4556]], dtype=torch.float64)
	q_value: tensor([[-30.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017685356272489683, distance: 1.154418973385299 entropy 10.4166515563826
epoch: 31, step: 39
	action: tensor([[  3017.2580,  -7670.5372, -10859.7263,  -5470.6872,  -6160.4480,
          -5127.2257,  -4558.3980]], dtype=torch.float64)
	q_value: tensor([[-33.6294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.504753229441693
epoch: 31, step: 40
	action: tensor([[  875.0137,   618.8053, -5273.8853, -1794.5822,  3975.1810,  1626.1551,
         -1938.1124]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6964495956015969, distance: 0.6304811420316788 entropy 9.759081311472238
epoch: 31, step: 41
	action: tensor([[ -4824.9699, -10112.0933,  -2982.2596,   -620.4479,   4712.4043,
           3416.5930,   5788.8439]], dtype=torch.float64)
	q_value: tensor([[-40.1109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9220729469361915, distance: 1.5865056617872328 entropy 10.22871195285133
epoch: 31, step: 42
	action: tensor([[-18201.3998, -13890.5299,   7353.0678, -18452.2296,   8515.8109,
         -11350.8352,   5564.9564]], dtype=torch.float64)
	q_value: tensor([[-33.5692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8301167696919747, distance: 1.5480896016802144 entropy 10.333350801893946
epoch: 31, step: 43
	action: tensor([[  1841.7345,   2374.1386,   6557.5472,   4915.7031, -22620.3428,
           7496.3680,    642.1929]], dtype=torch.float64)
	q_value: tensor([[-31.9320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.292792454651858
epoch: 31, step: 44
	action: tensor([[ 2882.5281, -2817.6024,  -579.1713,  3512.5183,  1338.3036,  -955.5525,
           140.6669]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2151772048391123, distance: 1.2614687616106843 entropy 9.759081311472238
epoch: 31, step: 45
	action: tensor([[  -823.4241, -13732.2031,  -3136.6585,  -2202.1058,   3162.8177,
           1097.0498,   2459.6598]], dtype=torch.float64)
	q_value: tensor([[-24.2782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05159184819740448, distance: 1.173492447057644 entropy 9.99647606427472
epoch: 31, step: 46
	action: tensor([[ -4026.5250, -14740.2990,  -3233.2950,   4496.2060,   9840.1207,
          -9505.1225,   4472.2033]], dtype=torch.float64)
	q_value: tensor([[-33.2681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8194048877213462, distance: 1.5435523803067457 entropy 10.44464119742501
epoch: 31, step: 47
	action: tensor([[ 3261.4598, 10092.0502,  5826.6151, 12792.5906, -7926.7999,  1438.9671,
         -1895.6625]], dtype=torch.float64)
	q_value: tensor([[-28.4519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8495440211733928, distance: 0.44387574935136914 entropy 10.291333318497069
epoch: 31, step: 48
	action: tensor([[ -3994.3689, -10865.8865,  -2001.9806,  -1860.8085,   5215.8558,
           3444.2205,    256.9065]], dtype=torch.float64)
	q_value: tensor([[-32.2925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.31577104455677
epoch: 31, step: 49
	action: tensor([[   235.2964,  -2451.6858,   1073.9608,   4169.8768,  -9169.1712,
         -11401.2412,   2841.6473]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.759081311472238
epoch: 31, step: 50
	action: tensor([[  873.0589, -6413.7919,  2846.9973, -3693.4749,  7070.8828, -1032.6614,
          2937.3618]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.759081311472238
epoch: 31, step: 51
	action: tensor([[-1390.0506, -6385.1254,  3001.3873,  4146.4251, -5173.8038,  2265.5245,
           446.0283]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5038540372210396, distance: 1.4033291170826783 entropy 9.759081311472238
epoch: 31, step: 52
	action: tensor([[ 1498.9282, -9234.6927,   976.6959,  7062.3279,  1272.2653,   975.1348,
         -4080.2714]], dtype=torch.float64)
	q_value: tensor([[-28.3882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33097429486801655, distance: 1.3202054683274957 entropy 10.394305731129299
epoch: 31, step: 53
	action: tensor([[  2845.2259,  -6047.0463,  -3976.0810,   3234.0658, -26411.7050,
           5685.0460,   6185.7701]], dtype=torch.float64)
	q_value: tensor([[-35.9914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27017805688841023, distance: 1.2897008799484702 entropy 10.439545840695867
epoch: 31, step: 54
	action: tensor([[  -804.2092, -18117.6913,   1284.7030,   2257.5755,   5771.2084,
           7939.1307,  -3149.6948]], dtype=torch.float64)
	q_value: tensor([[-38.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2380158228438034, distance: 1.273267901782128 entropy 10.413586337019897
epoch: 31, step: 55
	action: tensor([[  2832.6931,    834.1898, -12031.3948,  -3316.9591,   3300.8717,
          12525.6994,   8619.7976]], dtype=torch.float64)
	q_value: tensor([[-32.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8260293627658563, distance: 0.47730355648102973 entropy 10.495980431533539
epoch: 31, step: 56
	action: tensor([[ 269.5750, 3026.5915,  997.9780, -910.0433, 3734.4773, 6525.6114,
          283.8998]], dtype=torch.float64)
	q_value: tensor([[-30.1609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4418496996684772, distance: 0.8549333812905363 entropy 10.056377545181846
epoch: 31, step: 57
	action: tensor([[-13997.6721, -10387.6518,  -5267.7709,   6469.9956,  -7813.4960,
          -5546.4560,  -6337.1315]], dtype=torch.float64)
	q_value: tensor([[-31.5182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6971164013604529, distance: 1.4907763986796836 entropy 10.096038169883355
epoch: 31, step: 58
	action: tensor([[-5444.3775, -4562.0503, -7325.8842, 10603.6925,  1440.6290, 13835.0427,
         -3479.6967]], dtype=torch.float64)
	q_value: tensor([[-28.4601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5992724291766747, distance: 1.4471645598796958 entropy 10.313481847369488
epoch: 31, step: 59
	action: tensor([[-17751.7946, -10814.0742,  -6019.4525,  12253.2387,   -114.3535,
           4493.7207,  13637.2575]], dtype=torch.float64)
	q_value: tensor([[-24.7024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08048934568762633, distance: 1.1895068235223996 entropy 10.18486150486561
epoch: 31, step: 60
	action: tensor([[-5724.3420, -6104.5237,  -437.2217,  1737.6449,   912.1137,  6475.3896,
           415.4994]], dtype=torch.float64)
	q_value: tensor([[-25.7170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23917839016998765, distance: 0.9981552905571224 entropy 10.279881540911614
epoch: 31, step: 61
	action: tensor([[-2301.0430, -7412.7953, -4576.1495, 13542.1461,  3222.2084,  3750.6201,
         -7634.3284]], dtype=torch.float64)
	q_value: tensor([[-25.7792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12395212947941725, distance: 1.071076800029434 entropy 10.213938352367945
epoch: 31, step: 62
	action: tensor([[-13154.1739, -10554.4331,   1268.2524,  10149.9093,  -2475.4975,
           7175.0651,   5223.8515]], dtype=torch.float64)
	q_value: tensor([[-28.3082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12920535513354015, distance: 1.0678606124814742 entropy 10.338305401093754
epoch: 31, step: 63
	action: tensor([[  4412.2160, -13443.5986,   -552.5301,   2571.2943,   -559.6696,
            674.1117,  -2599.3250]], dtype=torch.float64)
	q_value: tensor([[-27.0056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3824663187350811, distance: 0.8992636073025863 entropy 10.313512261467311
epoch: 31, step: 64
	action: tensor([[ -7433.0876, -10567.5143,  -7584.6646,   7903.2468,   1856.0775,
           6559.6400,   6091.2908]], dtype=torch.float64)
	q_value: tensor([[-25.2668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4629376924405475, distance: 1.38410681838955 entropy 10.107439790488298
epoch: 31, step: 65
	action: tensor([[-15614.6519,  -3505.1957,  -2608.5004,   1387.1788,   5974.1497,
         -18719.1306,  12736.9466]], dtype=torch.float64)
	q_value: tensor([[-29.2087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4579919532324921, distance: 1.3817652194389067 entropy 10.38082420511743
epoch: 31, step: 66
	action: tensor([[   570.5340,   -226.1345,    297.1866,   1153.7314, -20939.3053,
          -2093.6504,  -3947.3126]], dtype=torch.float64)
	q_value: tensor([[-25.3708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4666611026724532, distance: 0.83571524121728 entropy 10.264732013766817
epoch: 31, step: 67
	action: tensor([[  1976.2265, -12029.4728,  -6727.4839,   5114.7667,   2805.5893,
          -8319.8341,   1247.9269]], dtype=torch.float64)
	q_value: tensor([[-29.7057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3757565743866319, distance: 0.9041358333485818 entropy 10.116217615067509
epoch: 31, step: 68
	action: tensor([[ 4703.2670,  6511.7887,  9961.8894, -6445.1230, -7638.5658, 10524.7174,
          5192.6854]], dtype=torch.float64)
	q_value: tensor([[-34.1290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6691295766328442, distance: 0.6582421171115007 entropy 10.429320392478543
epoch: 31, step: 69
	action: tensor([[-18601.4826,  -9447.4697,  -2489.2308,   4799.7802,   8743.0901,
           3209.4755,  -6773.1290]], dtype=torch.float64)
	q_value: tensor([[-33.4901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.316742216042824
epoch: 31, step: 70
	action: tensor([[ 3675.8343,   868.2384,  2766.2179,  8959.5299, -3913.9581, 10282.5714,
          4590.9942]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.759081311472238
epoch: 31, step: 71
	action: tensor([[-3103.3487, -3681.0019,  5592.7433,  4419.1391,  1172.4693,  -435.2793,
          5034.7393]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.759081311472238
epoch: 31, step: 72
	action: tensor([[-11122.8336,   2755.6344,  -1813.3731,  -6911.7820,    883.1471,
          -4210.7636,  -3256.0171]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.759081311472238
epoch: 31, step: 73
	action: tensor([[-2342.5295,  4491.9276,  8134.0097,  2920.2196, -1736.3494, -2347.4320,
           879.5348]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02434183633078879, distance: 1.1303307296281575 entropy 9.759081311472238
epoch: 31, step: 74
	action: tensor([[-16695.6719,  -1374.5592,  -9289.8195,  -1182.4485,  -3002.1687,
          -6554.5257,   6410.1924]], dtype=torch.float64)
	q_value: tensor([[-36.9103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.510437996266393, distance: 1.4063976897498067 entropy 10.404546247905355
epoch: 31, step: 75
	action: tensor([[  2503.9208, -13147.7355,  -1477.0912,  -3150.9753,  -2442.7829,
           8535.2329,  11232.9355]], dtype=torch.float64)
	q_value: tensor([[-27.7940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5452872048625982, distance: 0.7716587422634904 entropy 10.280147556491718
epoch: 31, step: 76
	action: tensor([[-3638.9168, -5767.0688, -4562.1580,  -236.2311, -5432.1709,  8019.7790,
           -20.5546]], dtype=torch.float64)
	q_value: tensor([[-27.4587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23411898718119795, distance: 1.0014686178565035 entropy 10.109210340044893
epoch: 31, step: 77
	action: tensor([[ 13805.4286,   3112.8887,   9382.6010, -21295.6298,  -3240.0393,
          13859.4675,    781.6032]], dtype=torch.float64)
	q_value: tensor([[-31.2284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.660917435053548, distance: 0.6663607726703152 entropy 10.307923011436127
epoch: 31, step: 78
	action: tensor([[-10272.3903,   6315.4864,  -1938.8025,  -9754.5881,   2703.5990,
          18846.0399,  13423.0225]], dtype=torch.float64)
	q_value: tensor([[-32.9892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.000558116019631405, distance: 1.144024871000523 entropy 10.235843264569516
epoch: 31, step: 79
	action: tensor([[-6648.7021, -7503.9410, -5961.5699,  -690.0282,   -53.4269,  7047.3522,
         -2754.5264]], dtype=torch.float64)
	q_value: tensor([[-33.1461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1499814225789965, distance: 1.055044801649586 entropy 10.322442551287144
epoch: 31, step: 80
	action: tensor([[-2539.5251, -7814.5412,  -254.4268, 22226.1945,  9922.4516, -6597.3168,
          5412.6694]], dtype=torch.float64)
	q_value: tensor([[-34.4958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5435206445990481, distance: 1.4217162080228132 entropy 10.441194003716303
epoch: 31, step: 81
	action: tensor([[11597.1483, -6661.5720, -9575.3734,  1313.7634,  6568.8487,  3079.8994,
          3574.6765]], dtype=torch.float64)
	q_value: tensor([[-28.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06571428053075579, distance: 1.1813459206157866 entropy 10.37656921626527
epoch: 31, step: 82
	action: tensor([[ -1103.1337,    854.8295,  -6855.1290, -13374.5613,  -2183.7564,
          11920.5908,  -4984.2720]], dtype=torch.float64)
	q_value: tensor([[-30.8932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.28159705777824
epoch: 31, step: 83
	action: tensor([[-3505.7822,    -8.0437,  -598.1697,    55.1073,  7766.8459,  2921.9791,
           524.6434]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009773886299088597, distance: 1.1499230009496297 entropy 9.759081311472238
epoch: 31, step: 84
	action: tensor([[-8256.8006, -5573.0966,  5069.8268,  9293.3499,  -488.7777,    49.7225,
          5508.6589]], dtype=torch.float64)
	q_value: tensor([[-26.4841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42895187717609273, distance: 1.367935105085142 entropy 10.269700786939959
epoch: 31, step: 85
	action: tensor([[-5259.6071,  8290.3341, 13138.3549,  7586.2552, -7440.6274, -3274.1943,
         11335.0425]], dtype=torch.float64)
	q_value: tensor([[-23.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6775181388703599, distance: 0.6498443375758003 entropy 10.239799176790417
epoch: 31, step: 86
	action: tensor([[  2824.3161,   9082.9199,  -3950.8699,  11048.4459, -10144.9324,
           6580.9945,  10942.3001]], dtype=torch.float64)
	q_value: tensor([[-30.2596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6893480062881416, distance: 0.6378135859952548 entropy 10.222096005106154
epoch: 31, step: 87
	action: tensor([[-3615.4840, -8355.4792,  -516.1316,  3995.1639, -3403.1213,  1602.5410,
          5484.9591]], dtype=torch.float64)
	q_value: tensor([[-30.5312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.32816819436957
epoch: 31, step: 88
	action: tensor([[-2089.7750,   317.8744,  -869.7145, -7313.6559,  -943.9477,  4051.1704,
           628.9403]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.759081311472238
epoch: 31, step: 89
	action: tensor([[  634.9544, -2562.9264, -1918.9736,  -510.4867,  1201.1048,  7783.8822,
          3126.2617]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36334849124183954, distance: 0.9130773707544223 entropy 9.759081311472238
epoch: 31, step: 90
	action: tensor([[-2921.8918, -1298.0249,   -23.9851,  2167.6879,  7511.9641, -7128.1000,
         -3011.3259]], dtype=torch.float64)
	q_value: tensor([[-30.6033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1527679699218255, distance: 1.053314047899066 entropy 10.238039665766143
epoch: 31, step: 91
	action: tensor([[  715.5611, -8115.0721,  1501.6639, 18626.6536,  1329.5825, 16078.0060,
          3407.4763]], dtype=torch.float64)
	q_value: tensor([[-30.0941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3190712802781357, distance: 0.9442946284310362 entropy 10.35656704204283
epoch: 31, step: 92
	action: tensor([[-12901.0338,   4035.1122,   4556.2409,   7057.6628,   4399.2947,
          -3246.6936,   4172.7003]], dtype=torch.float64)
	q_value: tensor([[-29.3841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5095339100879956, distance: 0.8014218638983354 entropy 10.335759507913846
epoch: 31, step: 93
	action: tensor([[  5263.1207,  -4040.4860,   5562.1615,    782.3259, -10642.8874,
          -2854.5220,  -2783.5623]], dtype=torch.float64)
	q_value: tensor([[-25.8981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.007801047161662
epoch: 31, step: 94
	action: tensor([[-3723.9404, -2110.5788, -1579.5803,  5087.2955,  2849.6368, -5924.3539,
         -3781.4812]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.759081311472238
epoch: 31, step: 95
	action: tensor([[-6831.6170,   100.4468, -9671.1603,  1959.4261, -2914.3976,  9689.3882,
          6590.2726]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6577014674610082, distance: 1.4733633217779398 entropy 9.759081311472238
epoch: 31, step: 96
	action: tensor([[-1859.3381,  7496.2944, -4948.8085,  3709.8594,  4211.3465,  5565.7623,
         11153.8915]], dtype=torch.float64)
	q_value: tensor([[-23.7048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035373384475305025, distance: 1.123922365561001 entropy 9.88146136698398
epoch: 31, step: 97
	action: tensor([[-18930.3285, -11377.7227,  -3172.5535,   3870.9617,  -4222.8012,
          11873.0701,   6282.0496]], dtype=torch.float64)
	q_value: tensor([[-32.3286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11697254903247978, distance: 1.075335035267051 entropy 10.317060604164055
epoch: 31, step: 98
	action: tensor([[-5001.0740,   687.6896, -3412.0422, -1675.7870, -2459.6441, -6088.4374,
          8946.8342]], dtype=torch.float64)
	q_value: tensor([[-26.4781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3643669801117344, distance: 0.9123467266327575 entropy 10.251845823333756
epoch: 31, step: 99
	action: tensor([[ 2678.4980, -4771.6961, -1587.5503,   549.1119,  3632.5143, 11107.1339,
          2378.8482]], dtype=torch.float64)
	q_value: tensor([[-33.1680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.200813298332847
epoch: 31, step: 100
	action: tensor([[-7322.7744,  1658.2372, -4203.9108,  1498.7892,  -691.2295, -6485.8819,
         -4770.5726]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4521022536120758, distance: 0.8470449358419374 entropy 9.759081311472238
epoch: 31, step: 101
	action: tensor([[-11861.9866,   5861.4676,  -3327.3342,    274.1829,  -9667.1508,
          -2913.6986,  10548.7832]], dtype=torch.float64)
	q_value: tensor([[-28.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05001072199200929, distance: 1.1153625161248069 entropy 10.117440347234421
epoch: 31, step: 102
	action: tensor([[-10378.1686,  -9432.5889,     12.8406,   7633.4868,  -8026.0711,
           3313.1607,  -6771.3343]], dtype=torch.float64)
	q_value: tensor([[-32.8795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8721571556503853, distance: 1.5657695550791704 entropy 10.28418427257063
epoch: 31, step: 103
	action: tensor([[-5671.1777, -7221.0002, -1824.9958, 12277.5585,  9546.6489,  2116.6289,
           527.2896]], dtype=torch.float64)
	q_value: tensor([[-26.0000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8605215641482042, distance: 1.5608962860944218 entropy 10.294364244745164
epoch: 31, step: 104
	action: tensor([[ 7282.0805, -9097.1575,  8713.8195,  5476.4108,  8488.6356, 10147.5964,
          8966.1520]], dtype=torch.float64)
	q_value: tensor([[-28.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43995371125770355, distance: 0.8563842177081352 entropy 10.333099393886213
epoch: 31, step: 105
	action: tensor([[ -4878.4703,  -4912.8561,  -5906.6338,   5332.6721,   5481.9097,
         -10372.2080,   2003.5664]], dtype=torch.float64)
	q_value: tensor([[-23.1938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02129956692570778, distance: 1.1320916404812285 entropy 10.037072936044321
epoch: 31, step: 106
	action: tensor([[ 1232.9457,  2760.4248, -2697.4377,  4534.0341, -9106.0957,  -568.2047,
         11747.8514]], dtype=torch.float64)
	q_value: tensor([[-26.4950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20839902044208558, distance: 1.0181455202487806 entropy 10.277087097043935
epoch: 31, step: 107
	action: tensor([[-3071.7830,  4240.1807,  1967.1270,  4118.8483, -3021.0047,  4300.2907,
         -4437.6063]], dtype=torch.float64)
	q_value: tensor([[-30.1653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48366611947588956, distance: 0.822284312547206 entropy 10.27049611485955
epoch: 31, step: 108
	action: tensor([[-3903.9977, -8628.1690,  9769.0445,  5627.4068,   361.3639,  1919.1724,
          -903.6090]], dtype=torch.float64)
	q_value: tensor([[-29.1930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7860928682085271, distance: 1.5293564232436099 entropy 10.16869263574674
epoch: 31, step: 109
	action: tensor([[-2509.7658, -5528.7677, -5058.8337,  3280.3565, -3213.4906,  8648.5328,
         -5637.2879]], dtype=torch.float64)
	q_value: tensor([[-29.2400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9128452154437283, distance: 1.5826927311184074 entropy 10.424599578638166
epoch: 31, step: 110
	action: tensor([[  4653.7188,  -9901.8093,   -549.6728,  15173.3318,   4131.4551,
           2828.5786, -16356.8181]], dtype=torch.float64)
	q_value: tensor([[-28.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31810871921440054, distance: 1.3138092332782891 entropy 10.408577578561971
epoch: 31, step: 111
	action: tensor([[-1552.0420,  2137.8744,  1593.7850, -5964.7049,  -440.1443, -6855.9222,
         -7301.1109]], dtype=torch.float64)
	q_value: tensor([[-28.6248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05178237293179411, distance: 1.17359874743036 entropy 10.462958787755635
epoch: 31, step: 112
	action: tensor([[-6279.9638,   470.9930,  5548.5112, -8105.2325, -1782.3905,  4697.7253,
          1855.5514]], dtype=torch.float64)
	q_value: tensor([[-32.2625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19765281823435454, distance: 1.0250330275892476 entropy 9.886817555075053
epoch: 31, step: 113
	action: tensor([[ 3566.4214,  6826.9787,  2454.1983, 15374.0970,  -413.3808,  -852.7385,
          -598.1475]], dtype=torch.float64)
	q_value: tensor([[-36.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.217033654184261
epoch: 31, step: 114
	action: tensor([[-8631.5375,  -542.3258,  3459.8748,  2140.3440,  1456.6651, -6697.1651,
         -7351.3306]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1117425834347041, distance: 1.6629423058664403 entropy 9.759081311472238
epoch: 31, step: 115
	action: tensor([[  4376.2857,  -4037.6084, -15350.4069,  -3092.3654,   4208.0259,
          -6802.4705,  -3601.7950]], dtype=torch.float64)
	q_value: tensor([[-27.1239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23080158663145034, distance: 1.0036351963893242 entropy 10.198844648582792
epoch: 31, step: 116
	action: tensor([[-8279.9829,   606.7781, -1461.0924,  1506.7492, -1609.1316,  -631.0805,
          7665.7574]], dtype=torch.float64)
	q_value: tensor([[-29.6722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.175883154400003
epoch: 31, step: 117
	action: tensor([[  -80.6140, -7519.3827, -5394.9739,  1516.4104, -5649.1895, -1698.3840,
         -8253.2229]], dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13314413557045135, distance: 1.0654428004711531 entropy 9.759081311472238
epoch: 31, step: 118
	action: tensor([[-2049.8676,  5102.5626,  6714.8041,  3166.5795, -4630.5592,  5205.9274,
         -1069.6239]], dtype=torch.float64)
	q_value: tensor([[-27.4866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28670634233177006, distance: 0.9664755562831717 entropy 10.229792265713817
epoch: 31, step: 119
	action: tensor([[ -7184.6449,  -3671.8494,   -882.4835,   1078.8424, -10729.1292,
           -561.8384,  10067.4371]], dtype=torch.float64)
	q_value: tensor([[-28.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44719008463954635, distance: 0.8508335473965655 entropy 10.166401843185941
epoch: 31, step: 120
	action: tensor([[-14800.6480, -15163.6636,  -1880.4409,   5867.5353,  11129.4012,
          -4169.3040,  -1513.7901]], dtype=torch.float64)
	q_value: tensor([[-28.6183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11140072631590892, distance: 1.0787223333052998 entropy 10.42196538040965
epoch: 31, step: 121
	action: tensor([[-12274.1009,  -4321.4654,   -633.8568,   2659.2742,   8404.5666,
           2734.6780, -10375.6025]], dtype=torch.float64)
	q_value: tensor([[-26.7899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8739589366106006, distance: 1.5665228292660003 entropy 10.28473837202514
epoch: 31, step: 122
	action: tensor([[ -7600.1426,  -3635.2182,    -15.3049,   2713.3457,   -478.1114,
          -1855.5333, -13680.7152]], dtype=torch.float64)
	q_value: tensor([[-27.1048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8552284749497356, distance: 1.558674369396133 entropy 10.361014635316776
epoch: 31, step: 123
	action: tensor([[   707.4803,  -7416.0690,  -5394.2959,   3769.8989,  -1261.4808,
         -15500.4209, -17996.1613]], dtype=torch.float64)
	q_value: tensor([[-26.7541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017895365332214697, distance: 1.1545380799124567 entropy 10.308106799634116
epoch: 31, step: 124
	action: tensor([[-9561.1901, -1498.2219, -1646.9724,  7583.3801,  9402.4947, -4992.5091,
          -729.2393]], dtype=torch.float64)
	q_value: tensor([[-27.6623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18163334820115096, distance: 1.0352152358165425 entropy 10.289839938279574
epoch: 31, step: 125
	action: tensor([[-18726.0809, -10609.7327,   1131.2253,  14651.7246,  14439.2016,
           4047.9381,   4471.2760]], dtype=torch.float64)
	q_value: tensor([[-27.7117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22210887213094344, distance: 1.0092903069534203 entropy 10.349927999204132
epoch: 31, step: 126
	action: tensor([[-7724.4075, -2472.5959,  2161.3759, -1784.5635,  9414.3335,  5033.5245,
         19160.3660]], dtype=torch.float64)
	q_value: tensor([[-26.0581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1928149193819737, distance: 1.2498077820708113 entropy 10.245725789092521
epoch: 31, step: 127
	action: tensor([[  -500.2559,  -1915.4186,  -9480.9011,   6469.7541,  -4285.9092,
          -8003.3521, -13566.9815]], dtype=torch.float64)
	q_value: tensor([[-26.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5140931484326849, distance: 1.4080983525252766 entropy 10.29598858543183
LOSS epoch 31 actor 344.32007887580744 critic 226.5472018274027
epoch: 32, step: 0
	action: tensor([[ -7676.1799, -10193.3300,   -303.2578,   6789.6986,  -1762.8808,
          13891.0084,  11404.3834]], dtype=torch.float64)
	q_value: tensor([[-22.3266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8460680946967949, distance: 1.5548215505689829 entropy 10.250498860832229
epoch: 32, step: 1
	action: tensor([[ -9974.9915,  -5686.1929,   9232.5252,  12250.5862, -14359.8839,
           9942.3509,   -276.0808]], dtype=torch.float64)
	q_value: tensor([[-26.6006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35285420038150384, distance: 1.3310126727396214 entropy 10.417394628952845
epoch: 32, step: 2
	action: tensor([[ -3304.1246, -20547.2179, -18582.9864,  15602.2328, -14388.9569,
         -10344.5815,   5295.5496]], dtype=torch.float64)
	q_value: tensor([[-28.6738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20630333557203473, distance: 1.256854364581564 entropy 10.57468586242435
epoch: 32, step: 3
	action: tensor([[ -3530.4423,  -5703.2448,   3939.7382,   1109.6057, -10365.2989,
          -1513.9021, -10796.4726]], dtype=torch.float64)
	q_value: tensor([[-24.4688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3066214289016653, distance: 1.308071795319648 entropy 10.32857999954014
epoch: 32, step: 4
	action: tensor([[ -9845.2379,  -2662.1904,  -2122.4291,   3171.7817,  -1219.0650,
         -13532.2938,    449.4653]], dtype=torch.float64)
	q_value: tensor([[-24.4644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35196779649711596, distance: 1.330576554744435 entropy 10.335780636361312
epoch: 32, step: 5
	action: tensor([[  6881.5006,  -7749.8809, -10074.4696,  10030.0399,  -7213.0574,
         -11080.5136,  -4453.3377]], dtype=torch.float64)
	q_value: tensor([[-25.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27976620600001456, distance: 1.2945594883695966 entropy 10.406988283048346
epoch: 32, step: 6
	action: tensor([[-12020.4122,  -3018.7392,  -3303.8739,   7977.3808,  -3499.3633,
         -11965.3983,  -2486.6066]], dtype=torch.float64)
	q_value: tensor([[-29.4349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32632087215563366, distance: 1.3178955614692545 entropy 10.307783424656401
epoch: 32, step: 7
	action: tensor([[-4439.5620, -3681.3814,   696.2569,  -660.8771, -7393.0121,  4975.0213,
           330.9834]], dtype=torch.float64)
	q_value: tensor([[-23.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3072970100748833, distance: 1.308409917157763 entropy 10.265496013402299
epoch: 32, step: 8
	action: tensor([[ -4696.2582, -14134.5656,    -23.5655, -10905.9626,   6045.7707,
          -4614.9783,  13124.6079]], dtype=torch.float64)
	q_value: tensor([[-26.3832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4555736218137565, distance: 1.3806187957472027 entropy 10.385808386858738
epoch: 32, step: 9
	action: tensor([[   289.6327,  -6879.0232, -14605.3910,   3030.1722,   1881.0427,
          13523.9890,   9268.3487]], dtype=torch.float64)
	q_value: tensor([[-27.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4700110573620264, distance: 1.3874488906704872 entropy 10.247589879107512
epoch: 32, step: 10
	action: tensor([[-5399.6860, -3751.0133,   322.5733,  9442.9905, -1876.8813,  8270.3809,
          5451.4621]], dtype=torch.float64)
	q_value: tensor([[-25.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07156271686776372, distance: 1.1845829859346828 entropy 10.195496999538424
epoch: 32, step: 11
	action: tensor([[-7580.7969, -4949.2573, 11182.2575,  3342.5162,  6840.9468,  8540.8423,
         11510.6130]], dtype=torch.float64)
	q_value: tensor([[-26.2627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007438652559655035, distance: 1.1485925578806546 entropy 10.42690601238815
epoch: 32, step: 12
	action: tensor([[-15194.2727,  -6268.4939,   -480.0498, -10076.2298,   3433.9492,
           8525.5349,  -1778.5142]], dtype=torch.float64)
	q_value: tensor([[-26.2938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19715659473757752, distance: 1.2520802766643064 entropy 10.482811295889473
epoch: 32, step: 13
	action: tensor([[ -6888.4755,   -737.9442, -10483.2215,  15666.7385,  -4480.1289,
           3746.4136,   3199.2810]], dtype=torch.float64)
	q_value: tensor([[-30.2981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2692349350805433, distance: 0.978240371285478 entropy 10.446593626362754
epoch: 32, step: 14
	action: tensor([[  1032.8862,  -8909.7639, -12353.2291,  13367.2568,   5078.3452,
          -1283.6552,  -7193.5741]], dtype=torch.float64)
	q_value: tensor([[-25.4039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07846842689913036, distance: 1.0985296997866827 entropy 10.425699457398519
epoch: 32, step: 15
	action: tensor([[   879.0920, -13979.8751,   -992.8021,  -8824.4654,  -3100.2036,
          -1702.1403,   6135.9233]], dtype=torch.float64)
	q_value: tensor([[-23.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25095318954005275, distance: 1.2799034881349922 entropy 10.114213632719794
epoch: 32, step: 16
	action: tensor([[  5665.0556, -13178.3492,   -699.2785,   2902.1669,  -1356.5213,
           7136.5672,   2948.9454]], dtype=torch.float64)
	q_value: tensor([[-27.7777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4979147979158991, distance: 0.810859116942801 entropy 10.360026359606868
epoch: 32, step: 17
	action: tensor([[-4252.1154, -6585.2613,  2177.7255, 13689.0426,   499.9294,  8039.8973,
         -3563.8529]], dtype=torch.float64)
	q_value: tensor([[-26.7486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3399162061952643, distance: 1.3246328261253963 entropy 10.297620971560692
epoch: 32, step: 18
	action: tensor([[-13981.7073, -10692.3092, -14699.4193,     50.7248,  -8298.1519,
           4236.1899,  12522.3813]], dtype=torch.float64)
	q_value: tensor([[-26.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6012392291341224, distance: 1.4480541546054748 entropy 10.542383029593333
epoch: 32, step: 19
	action: tensor([[ 2162.6438, -3334.5649,  3729.7151,  3037.4968, -7632.6185,  5888.2587,
          9190.7248]], dtype=torch.float64)
	q_value: tensor([[-26.6568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35109608733990794, distance: 0.9218216200292779 entropy 10.454378375110576
epoch: 32, step: 20
	action: tensor([[-18477.7394,    457.4750,   -405.4822,   2960.4171,  -7049.2989,
           1123.5384,  -7812.0539]], dtype=torch.float64)
	q_value: tensor([[-25.2596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09240187445178871, distance: 1.09019324916365 entropy 10.360436475387091
epoch: 32, step: 21
	action: tensor([[-11871.8683,    -83.8639,   8551.9147,   2525.3465, -13537.9324,
            -39.9842, -11599.6551]], dtype=torch.float64)
	q_value: tensor([[-26.7172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04713568126156087, distance: 1.1710034443879112 entropy 10.371148392613062
epoch: 32, step: 22
	action: tensor([[-4079.2217, -4051.4322,  8913.9369,   755.2031, -7997.2608, -9139.5970,
          8073.8242]], dtype=torch.float64)
	q_value: tensor([[-23.7252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7181295273194093, distance: 1.4999771527661896 entropy 10.233011985312638
epoch: 32, step: 23
	action: tensor([[-10177.1200,   4529.0258,  -1902.5490,  -5148.0334,  -1772.5879,
           2760.0462,   8081.6455]], dtype=torch.float64)
	q_value: tensor([[-21.1167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5159577460862455, distance: 1.40896511856622 entropy 10.168114295283113
epoch: 32, step: 24
	action: tensor([[-4208.7892,  3611.5873,  2455.9828, -5407.0304,  -181.8045, -4950.7643,
          3004.4362]], dtype=torch.float64)
	q_value: tensor([[-24.5455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0744413195006094, distance: 1.1009273791594165 entropy 9.871232779336156
epoch: 32, step: 25
	action: tensor([[-6060.0104, -7049.4590,  -522.5010,   505.1258, 14393.9663,    15.0085,
          7760.3252]], dtype=torch.float64)
	q_value: tensor([[-36.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7225545920649417, distance: 1.8881869499492903 entropy 10.185672357698971
epoch: 32, step: 26
	action: tensor([[  572.0039,  2849.5240, -5577.6209,  3283.5163, -4757.2759, -3118.4609,
         -1788.2972]], dtype=torch.float64)
	q_value: tensor([[-16.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7605016827304327, distance: 0.5600256598922424 entropy 9.892641497823785
epoch: 32, step: 27
	action: tensor([[ -513.7812, -6320.3337, -2169.8274,  4531.0748, -2252.0864,  3729.4239,
         -7477.8584]], dtype=torch.float64)
	q_value: tensor([[-23.9136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19403835403336545, distance: 1.2504485630700024 entropy 10.079165233427252
epoch: 32, step: 28
	action: tensor([[ -3742.5909,    300.6682,  -2840.6389,   3773.1493, -25348.9413,
          14190.3254,   9205.7789]], dtype=torch.float64)
	q_value: tensor([[-28.0970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4249134971441557, distance: 1.3660007675962222 entropy 10.516574309716438
epoch: 32, step: 29
	action: tensor([[-1328.4145,  4526.6850, -6830.6862, -3509.4193,  4102.7166,  -619.7651,
          6869.6075]], dtype=torch.float64)
	q_value: tensor([[-28.3751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18182598416720075, distance: 1.2440374673761851 entropy 10.428416145182334
epoch: 32, step: 30
	action: tensor([[ 4769.8599, -5288.0349,  2450.4813,   807.3121, -6164.8272,  3229.3814,
         10130.3966]], dtype=torch.float64)
	q_value: tensor([[-25.7664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18624314684215149, distance: 1.0322954729939149 entropy 10.123859425224481
epoch: 32, step: 31
	action: tensor([[ -9735.6985,  -4185.0412, -10618.8618,  10002.9212,    745.1527,
           5515.6200,   3284.8706]], dtype=torch.float64)
	q_value: tensor([[-22.8972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8086466295170307, distance: 1.5389820518415354 entropy 10.212203414573029
epoch: 32, step: 32
	action: tensor([[-13728.6546,  21875.8545,  12477.4825,   4533.7905,  -1182.7407,
          -3028.1304,   4699.2306]], dtype=torch.float64)
	q_value: tensor([[-25.9568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6022811986199557, distance: 0.7216801402591111 entropy 10.475085507738829
epoch: 32, step: 33
	action: tensor([[-14095.1036,   -794.4445,  -5146.4699,   1153.4905,  -9424.1633,
            360.3091, -11858.3499]], dtype=torch.float64)
	q_value: tensor([[-27.6517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2542898922303246, distance: 0.9881928520757629 entropy 10.279631576627052
epoch: 32, step: 34
	action: tensor([[-15702.6304,  -4045.4670,   4128.6925,    577.1477,  -4822.8469,
           1581.7071,   2234.1712]], dtype=torch.float64)
	q_value: tensor([[-22.1027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28855347972183143, distance: 1.2989963097540267 entropy 10.214677422279175
epoch: 32, step: 35
	action: tensor([[-18355.4666,   6965.6276,   6152.8104,   9613.8269,  -3702.4282,
          10656.5012,   5197.0114]], dtype=torch.float64)
	q_value: tensor([[-27.4619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.68192997353132, distance: 0.6453838088410319 entropy 10.48589995740185
epoch: 32, step: 36
	action: tensor([[-13502.1001,  -3302.3156,  -6873.1983,  -9438.6525,   -197.0351,
         -14403.4780,   7479.9254]], dtype=torch.float64)
	q_value: tensor([[-25.1326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.328209651024638
epoch: 32, step: 37
	action: tensor([[-1649.4981,  1103.5491, -2966.0616,  2246.3189, -6588.2284,  1338.7698,
           103.9004]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6652153286834386, distance: 0.6621242220892993 entropy 9.820261937036452
epoch: 32, step: 38
	action: tensor([[-7820.7034,  6611.7484,  2906.8536, 11387.2719, -5600.6757, -2520.3045,
          5921.1406]], dtype=torch.float64)
	q_value: tensor([[-28.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2611046062205997, distance: 0.9836671606424006 entropy 10.329543054044198
epoch: 32, step: 39
	action: tensor([[  9671.1122,  -7094.6052, -10043.6336,   -970.6079,   1755.3724,
          11356.1167,  -1066.8313]], dtype=torch.float64)
	q_value: tensor([[-34.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5859346854225942, distance: 0.7363615772568223 entropy 10.584551497221998
epoch: 32, step: 40
	action: tensor([[  7139.9747,  -4359.3186, -13093.1825,   2925.3036,  -2604.4137,
          -7284.4691,   5457.2255]], dtype=torch.float64)
	q_value: tensor([[-24.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.056710133489330294, distance: 1.1763447792043225 entropy 10.256275887493874
epoch: 32, step: 41
	action: tensor([[-10962.0616,   2103.5035,   3027.0496,  -6391.5281,    219.5611,
         -15590.6123, -14465.9164]], dtype=torch.float64)
	q_value: tensor([[-35.6724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.42953739855596
epoch: 32, step: 42
	action: tensor([[-2630.7204, -8778.1924, -3559.2155,   670.1146,  5560.2127,  9489.5268,
         -5711.7580]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16197470238378375, distance: 1.0475753187638785 entropy 9.820261937036452
epoch: 32, step: 43
	action: tensor([[ -3161.0248, -10269.1641,   -304.4543,   2320.2563,  23236.4790,
          -3829.6563,  10238.2842]], dtype=torch.float64)
	q_value: tensor([[-25.9478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024062574401254477, distance: 1.130492484927241 entropy 10.45697914411622
epoch: 32, step: 44
	action: tensor([[ 2381.3566, -7160.6442,  3865.5505, 13348.9440, -7560.5253, -3039.8143,
          9417.9730]], dtype=torch.float64)
	q_value: tensor([[-25.8851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0550115294663287, distance: 1.1753989438545975 entropy 10.509063677487134
epoch: 32, step: 45
	action: tensor([[  2876.8032,   3873.0072,  -7421.0021, -14934.7815,   -187.9590,
           3958.1723,   5502.2571]], dtype=torch.float64)
	q_value: tensor([[-24.7861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4303739862639593, distance: 0.8636774895439127 entropy 10.22563590148023
epoch: 32, step: 46
	action: tensor([[-311.5645,  211.0415, -828.9421, 2799.7684, 7271.5997,  -39.9511,
         9662.1355]], dtype=torch.float64)
	q_value: tensor([[-24.2045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4336396604989956, distance: 0.861198193160894 entropy 9.916640931626048
epoch: 32, step: 47
	action: tensor([[11028.9363,  4471.7292,  7669.5386, -1931.2355,   -57.7151, -1017.0213,
          1133.7051]], dtype=torch.float64)
	q_value: tensor([[-20.8765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1639184288338973, distance: 1.0463597337871944 entropy 9.884685873494393
epoch: 32, step: 48
	action: tensor([[-23824.6371,    677.9459,   1843.0687,  12790.8774,    920.2631,
          -1245.2585,   2173.2645]], dtype=torch.float64)
	q_value: tensor([[-31.6880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24985549916603478, distance: 0.9911266599301095 entropy 10.301189158803533
epoch: 32, step: 49
	action: tensor([[  2638.0904, -16058.0801,   3144.7218,  15355.3941,  -2333.2588,
           -201.0214,  -6826.3023]], dtype=torch.float64)
	q_value: tensor([[-32.6055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4510912665872149, distance: 0.8478260640469658 entropy 10.420626310861584
epoch: 32, step: 50
	action: tensor([[-15257.8625,   2235.4168,   4899.2305,  14725.9456,   5377.9501,
         -10952.9942,   7555.6778]], dtype=torch.float64)
	q_value: tensor([[-27.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026734643620093923, distance: 1.1289438074215075 entropy 10.602421091667008
epoch: 32, step: 51
	action: tensor([[-2387.3784, -3421.9701, -2419.4484, 16811.8671, -4666.8643, -6719.0918,
         10839.3630]], dtype=torch.float64)
	q_value: tensor([[-29.9912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42367970111863507, distance: 1.3654092470203452 entropy 10.41298589555885
epoch: 32, step: 52
	action: tensor([[ -8426.1630,  -3209.7893,   8931.5723,   6802.3469,   5140.0556,
           9470.0360, -14200.4022]], dtype=torch.float64)
	q_value: tensor([[-23.9738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024249013605820435, distance: 1.1581357569037463 entropy 10.29148625499282
epoch: 32, step: 53
	action: tensor([[-3997.9692, -8925.7852, -9753.1787, 16533.2913,  5504.2013,  8311.1359,
          7146.8084]], dtype=torch.float64)
	q_value: tensor([[-24.4504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1053488879161103, distance: 1.0823894394433218 entropy 10.345086170373124
epoch: 32, step: 54
	action: tensor([[-20733.5887,  -5265.3440,  -3337.1009,   6709.4853,  -5971.1202,
           8418.9448,  15749.4984]], dtype=torch.float64)
	q_value: tensor([[-25.6336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5963186011271209, distance: 1.4458274984670538 entropy 10.433318767847751
epoch: 32, step: 55
	action: tensor([[  1167.0859,  10790.2209,  -3640.3756,  12125.2723,  18507.1138,
          -7771.8580, -11734.5834]], dtype=torch.float64)
	q_value: tensor([[-25.6494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4959329281772692, distance: 0.8124578838711175 entropy 10.463773378076542
epoch: 32, step: 56
	action: tensor([[ -6976.0109,  -2095.4906, -12381.3934,   2546.3178,  -9132.6385,
          -9423.8256,   7181.5227]], dtype=torch.float64)
	q_value: tensor([[-32.2351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1137901335076672, distance: 1.077271036858339 entropy 10.395677562101861
epoch: 32, step: 57
	action: tensor([[-8936.7810, -7974.2754, -9960.2609, -2625.1153, 11121.3711,  3363.7014,
         -3255.7020]], dtype=torch.float64)
	q_value: tensor([[-27.0348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2116497420712844, distance: 1.7018248769583442 entropy 10.508074978802568
epoch: 32, step: 58
	action: tensor([[ -4621.7153, -13744.6554,    397.7228,  -2858.8122,  -2005.6616,
           2930.7281,   8263.2807]], dtype=torch.float64)
	q_value: tensor([[-21.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3484054359532669, distance: 1.3288224005562843 entropy 10.058444248361017
epoch: 32, step: 59
	action: tensor([[-9171.5086, -9273.6569,  3200.0423, 14470.9919,  -161.2798,  3042.6969,
         12955.2453]], dtype=torch.float64)
	q_value: tensor([[-26.8605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7940903360549658, distance: 1.5327765471765638 entropy 10.373966274139931
epoch: 32, step: 60
	action: tensor([[-5928.7336, -6150.6705,  4693.9758,  1854.8424,   661.2648, -2551.0334,
          3298.8414]], dtype=torch.float64)
	q_value: tensor([[-23.6674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6232769241546539, distance: 1.457984814794797 entropy 10.269416060439015
epoch: 32, step: 61
	action: tensor([[-17297.7130,   5788.9534,   5401.4242,   5437.2015,   4154.0652,
          -1146.6368,   6617.3749]], dtype=torch.float64)
	q_value: tensor([[-22.0058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16671538613197656, distance: 1.2360588711321905 entropy 10.205843795820416
epoch: 32, step: 62
	action: tensor([[-12631.0156,   1635.9761,   4223.8812,  -2107.9649,  -7522.0675,
          10003.5979,   3524.7815]], dtype=torch.float64)
	q_value: tensor([[-28.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05577309758036375, distance: 1.1119746285604093 entropy 10.25450425113274
epoch: 32, step: 63
	action: tensor([[-5454.5316, -1960.8061, -2131.2233,  3852.7008,  4925.6561, -1641.3521,
          3255.2700]], dtype=torch.float64)
	q_value: tensor([[-29.1044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8584348714225278, distance: 1.5600207185210488 entropy 9.999187213635778
epoch: 32, step: 64
	action: tensor([[-17000.6483,  -9156.3447,  -8276.1016,  -3545.5208,   1213.6209,
          -1980.7110,   8195.3658]], dtype=torch.float64)
	q_value: tensor([[-21.0648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0415155634714206, distance: 1.167856750172134 entropy 10.195145350684088
epoch: 32, step: 65
	action: tensor([[ -715.5544, -5560.1041,  6135.7084, -1681.6633, -1540.7422,  3430.6594,
         17062.0300]], dtype=torch.float64)
	q_value: tensor([[-24.3690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009215794821236245, distance: 1.1390590280005863 entropy 10.237359419885204
epoch: 32, step: 66
	action: tensor([[17290.1020, 12331.4805,   307.6947,   571.5366, 14291.0393,  7333.1814,
          3584.0322]], dtype=torch.float64)
	q_value: tensor([[-33.9031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.557818934430367
epoch: 32, step: 67
	action: tensor([[ 4360.1677, -1940.1028, -2196.4037,  9087.1267,  2064.6127, -3297.5189,
           280.0206]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11158832108313343, distance: 1.2065037632602407 entropy 9.820261937036452
epoch: 32, step: 68
	action: tensor([[  5150.4590,  -5667.2052,  -3892.8582,   2690.3378,   -701.8988,
         -20995.6389,  10871.0514]], dtype=torch.float64)
	q_value: tensor([[-29.9469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06706119836194113, distance: 1.1053078929654212 entropy 10.426050754563724
epoch: 32, step: 69
	action: tensor([[  7154.3889,   4878.8807, -10820.4735,   1913.1820,  -5302.9744,
           1162.2885,   8220.3745]], dtype=torch.float64)
	q_value: tensor([[-34.3930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3738290162389072, distance: 0.90553066671463 entropy 10.36292309765507
epoch: 32, step: 70
	action: tensor([[-5372.7895, -6601.8476, -4365.4581, -1282.5999, -8771.9085,  2576.9369,
          6322.8680]], dtype=torch.float64)
	q_value: tensor([[-25.4259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.213949421831858
epoch: 32, step: 71
	action: tensor([[-4263.1221, -4668.6938, -8268.5463, -4673.9111,  3669.0208,  6401.6338,
         -3746.7477]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.820261937036452
epoch: 32, step: 72
	action: tensor([[ 1169.0953, -3219.1646, -1763.9709,  3498.3340,    -8.2667, -3422.1912,
          6235.3945]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.820261937036452
epoch: 32, step: 73
	action: tensor([[-2981.3984,   513.8513,  5511.8738,  -365.7729, -6133.5802,   319.5971,
         -5258.1095]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27373593964809273, distance: 1.2915059007958314 entropy 9.820261937036452
epoch: 32, step: 74
	action: tensor([[-2249.6931, -1601.4604, -5155.9084,   931.0257, -4712.1393, -7315.1855,
         -5878.7579]], dtype=torch.float64)
	q_value: tensor([[-33.2916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.084579223164305, distance: 1.6522124701890084 entropy 10.131407496407377
epoch: 32, step: 75
	action: tensor([[-14846.2153,  -6072.7369,  -4547.1544,    425.4251,  -2186.5923,
          14176.9975,  -5070.8572]], dtype=torch.float64)
	q_value: tensor([[-22.4380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15886607732284042, distance: 1.0495164919608142 entropy 10.209028550772976
epoch: 32, step: 76
	action: tensor([[-5589.1754,   648.7612, 19844.5090,  6407.8969, -3237.4716, -4907.6297,
         18812.9915]], dtype=torch.float64)
	q_value: tensor([[-23.0172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4468128179308266, distance: 1.3764576921311036 entropy 10.2576020952451
epoch: 32, step: 77
	action: tensor([[ -3161.9040, -11809.4598,  -2896.3493,   6687.7220,  13520.2020,
           -576.1522,   1257.0883]], dtype=torch.float64)
	q_value: tensor([[-26.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07714967264407657, distance: 1.1876670838102061 entropy 10.303597321788393
epoch: 32, step: 78
	action: tensor([[  5354.6803,    836.8471, -15122.0385,  -3659.2728,  -3063.7388,
          -7767.7514, -13210.7751]], dtype=torch.float64)
	q_value: tensor([[-21.6525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.248076029744865
epoch: 32, step: 79
	action: tensor([[  766.2503,  3222.7340,  1501.2660,  4557.6467,   889.5417, -4092.2125,
         -6919.0346]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5305696462927302, distance: 0.7840473246684501 entropy 9.820261937036452
epoch: 32, step: 80
	action: tensor([[-4996.2965, -8318.8229, -1354.6102,  5857.9750,  2473.1344, -3151.5975,
         -6396.3684]], dtype=torch.float64)
	q_value: tensor([[-27.0117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1367373123956277, distance: 1.2200756258192258 entropy 10.294059891407116
epoch: 32, step: 81
	action: tensor([[ -1126.2413,   5122.7557,  -6693.8110,   7412.1283, -14791.4210,
           3340.9623,  -3680.5016]], dtype=torch.float64)
	q_value: tensor([[-26.2657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07834469868375937, distance: 1.0986034436286247 entropy 10.496739849436574
epoch: 32, step: 82
	action: tensor([[-18702.1680,  -1354.6533,  -4426.2754,   3665.9320,    117.3845,
           3489.4417, -10864.0536]], dtype=torch.float64)
	q_value: tensor([[-28.4033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06878154905973854, distance: 1.1830447350833089 entropy 10.342587048191335
epoch: 32, step: 83
	action: tensor([[ -3155.8350,   9612.5083, -18710.1729,  -2158.2216, -11148.5048,
          -6595.5887,   2203.3665]], dtype=torch.float64)
	q_value: tensor([[-27.3159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08665022970367064, distance: 1.1928932505802088 entropy 10.514699811651786
epoch: 32, step: 84
	action: tensor([[-9473.0618, -7397.5071, -5310.6616,  4515.8964,  7596.8410,  6792.9285,
          5149.4411]], dtype=torch.float64)
	q_value: tensor([[-28.6917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.054734542714761414, distance: 1.175244636864551 entropy 10.228051475572489
epoch: 32, step: 85
	action: tensor([[-15799.1373,  -4264.8758,  15571.8889,  -1090.7573,   8527.5001,
          18284.8305,   1218.9632]], dtype=torch.float64)
	q_value: tensor([[-24.8093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7303964537303036, distance: 1.5053223211528053 entropy 10.36625172303815
epoch: 32, step: 86
	action: tensor([[ -1939.6232, -11369.3831,  -4572.6368,    778.8124,  -2866.7878,
           7921.0034,  -5516.6598]], dtype=torch.float64)
	q_value: tensor([[-23.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3286173504190104, distance: 1.3190360130858294 entropy 10.192031479527156
epoch: 32, step: 87
	action: tensor([[ -3219.7973, -13008.7440,  -4798.9484,  -3597.3992,   1382.7822,
           5000.4979,  -5586.9551]], dtype=torch.float64)
	q_value: tensor([[-23.8008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6513605224014694, distance: 1.4705427092302525 entropy 10.283056390870472
epoch: 32, step: 88
	action: tensor([[-1538.9991,  -543.7898,  2114.6716, -9980.0131, 11434.3257,   972.4891,
         -9980.6823]], dtype=torch.float64)
	q_value: tensor([[-22.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4677359198642641, distance: 1.3863747969382754 entropy 10.105877766794743
epoch: 32, step: 89
	action: tensor([[  7112.1257, -19176.8201,   2946.8903,   5475.6383,  -7668.3964,
          -3305.0425,   3885.9822]], dtype=torch.float64)
	q_value: tensor([[-32.5611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26955176454752205, distance: 0.9780282860399171 entropy 10.488807102488611
epoch: 32, step: 90
	action: tensor([[-13561.6175,   1128.9499,  -5560.3977,   9971.8135,  -9064.8403,
          10289.5619,    -91.0004]], dtype=torch.float64)
	q_value: tensor([[-26.2398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.724449166111975, distance: 0.6006998978507327 entropy 10.404648755606416
epoch: 32, step: 91
	action: tensor([[ -4152.3157,   9850.6668,  -5362.1000,   1897.8950, -15543.4860,
           5945.6593,  -5219.3636]], dtype=torch.float64)
	q_value: tensor([[-28.2607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5314520760432371, distance: 0.7833100564812395 entropy 10.345730080918486
epoch: 32, step: 92
	action: tensor([[  4819.2588,  -1624.2076, -12118.4171,  15951.2378,  11236.9244,
           -747.6716, -14059.9553]], dtype=torch.float64)
	q_value: tensor([[-26.4793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5133131182055577, distance: 0.798328279009502 entropy 10.272509346442746
epoch: 32, step: 93
	action: tensor([[-5308.4504,   579.3197, -2395.9513,  2038.8556,  5156.6616,  5774.4313,
          6376.5628]], dtype=torch.float64)
	q_value: tensor([[-29.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19672782275717315, distance: 1.2518560348981043 entropy 10.270610006509528
epoch: 32, step: 94
	action: tensor([[-5264.5630, -1457.4099, -5819.4302, -4867.7729, -1154.0492,  6815.7354,
         -5253.2264]], dtype=torch.float64)
	q_value: tensor([[-23.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4020925972941358, distance: 1.3550179283792267 entropy 10.226662431735308
epoch: 32, step: 95
	action: tensor([[   410.0991,  -3544.2002,  -1668.1079,   1181.5607,  -1921.8753,
         -15295.6838,  -5790.1872]], dtype=torch.float64)
	q_value: tensor([[-23.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4460573207886258, distance: 1.3760982656626481 entropy 10.172790776850901
epoch: 32, step: 96
	action: tensor([[ -4615.0469,  -2550.5573, -23851.8711,  -5941.9007,    357.9888,
         -22000.1069, -17433.4030]], dtype=torch.float64)
	q_value: tensor([[-23.2580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04240089887126097, distance: 1.1198208725733902 entropy 10.305684742155396
epoch: 32, step: 97
	action: tensor([[ -5877.4189, -14206.8524,  -6938.4728,  16433.6612,   9102.2763,
           7848.0913,   6898.9143]], dtype=torch.float64)
	q_value: tensor([[-26.5843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41067467806554836, distance: 1.359158572429879 entropy 10.326296656053769
epoch: 32, step: 98
	action: tensor([[ -3828.1323, -10107.1211,  -7225.1454,  -8957.2185,  -5140.9113,
          -9135.1145,   1654.0529]], dtype=torch.float64)
	q_value: tensor([[-24.4618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13775500490172532, distance: 1.2206216552425873 entropy 10.405541899902575
epoch: 32, step: 99
	action: tensor([[  -757.1538,  -3911.5907,    166.5347,  11515.3904,   4357.9660,
          11362.0333, -15595.8532]], dtype=torch.float64)
	q_value: tensor([[-24.8404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6668317852659027, distance: 1.4774152585406957 entropy 10.279623134745881
epoch: 32, step: 100
	action: tensor([[ 11061.4072, -27720.5932,  -4660.2767,  12937.5025,  13150.6064,
         -13803.9172,   4769.9018]], dtype=torch.float64)
	q_value: tensor([[-25.7403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4387330109508176, distance: 1.3726088586761032 entropy 10.504461476635624
epoch: 32, step: 101
	action: tensor([[-4131.4431,  1348.0915, -6695.5637, -7221.0853,  -370.4604, -4945.4354,
          6489.8063]], dtype=torch.float64)
	q_value: tensor([[-27.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.211928745251656
epoch: 32, step: 102
	action: tensor([[-3239.0638,  2380.0695, -4908.7651,  -368.7290,  3423.4177,   685.9541,
           330.2626]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1703653267736378, distance: 1.2379907984068375 entropy 9.820261937036452
epoch: 32, step: 103
	action: tensor([[ 1432.6424, -1259.1702, -2500.7581,  2064.1041,  -567.8959, -1717.6077,
          4707.3999]], dtype=torch.float64)
	q_value: tensor([[-30.4722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.075873823478073
epoch: 32, step: 104
	action: tensor([[-3850.0563,  2726.8323,  7132.8537, -3584.8684, -9818.7069, -6847.5337,
          6051.2599]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.820261937036452
epoch: 32, step: 105
	action: tensor([[ 5665.9212,  -675.9420, -4080.6100,  6175.8560,  -719.7727, -6741.0479,
          2625.2128]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.820261937036452
epoch: 32, step: 106
	action: tensor([[ 5650.8227,  4762.3745, -4029.7613,  8207.4751,  -246.8659,  4287.4330,
         -4533.2510]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.820261937036452
epoch: 32, step: 107
	action: tensor([[-10315.7435,  -6457.0957,   -126.1510,  -1135.1404,    887.1624,
           6175.5503,  -3638.1028]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.820261937036452
epoch: 32, step: 108
	action: tensor([[  752.5443,  -783.1305,  3841.7749, 13812.5275,  5255.1338, -2150.0736,
          1425.1920]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41535706360303926, distance: 0.8749878988572197 entropy 9.820261937036452
epoch: 32, step: 109
	action: tensor([[ -4319.4894, -10304.2535,  -8943.1895,   6819.4930,   6504.7359,
           4422.7524,  -8055.1053]], dtype=torch.float64)
	q_value: tensor([[-30.5397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5278063501882753, distance: 1.414460580599174 entropy 10.315737819271353
epoch: 32, step: 110
	action: tensor([[-2320.5202,  -829.1304,  1364.9484, -5438.7344, -1126.1826, 10287.8862,
          2994.4177]], dtype=torch.float64)
	q_value: tensor([[-25.7179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6503672608865585, distance: 1.470100391165514 entropy 10.380618752353238
epoch: 32, step: 111
	action: tensor([[ 4153.0157,   360.8741, 15861.7896, -5023.2990,  5552.0487, 15882.0327,
          3836.4455]], dtype=torch.float64)
	q_value: tensor([[-27.3929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5199896977225515, distance: 0.7928334638972884 entropy 10.33233734528487
epoch: 32, step: 112
	action: tensor([[-2324.7696, -3944.8987,  4192.2885, -1058.0432,  7096.0778, 20779.0108,
         -7917.3600]], dtype=torch.float64)
	q_value: tensor([[-33.3310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18239092217218777, distance: 1.0347359679371515 entropy 10.404997451517806
epoch: 32, step: 113
	action: tensor([[-4770.2890,  -377.6255,  -485.2531,  3357.6947, -4372.0400,  4681.5364,
         -6424.7091]], dtype=torch.float64)
	q_value: tensor([[-27.7476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5408198556099362, distance: 1.420471833149563 entropy 10.340887116370352
epoch: 32, step: 114
	action: tensor([[ 6931.3780,  5286.9602, -3961.5855, -1629.8092, -2756.5482, -2881.5370,
         -2100.7853]], dtype=torch.float64)
	q_value: tensor([[-21.7464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.269699712072882
epoch: 32, step: 115
	action: tensor([[-4578.3707, -6476.5675,  4861.9760, -2845.9605,   722.0313,  4466.3491,
           724.3532]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33820972215429923, distance: 1.3237890476323437 entropy 9.820261937036452
epoch: 32, step: 116
	action: tensor([[ -909.7782,  5636.5751, -1625.4870, -2697.7999,   -76.6543,  4098.3191,
          2165.1939]], dtype=torch.float64)
	q_value: tensor([[-25.3864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03846118016137323, distance: 1.1661430449864805 entropy 10.172417768429515
epoch: 32, step: 117
	action: tensor([[ -4558.2759,  -4498.0076,  -1628.1923, -16895.1751,  17081.0874,
          -3737.1580,   6879.2852]], dtype=torch.float64)
	q_value: tensor([[-33.4112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19479065464073142, distance: 1.2508424218951868 entropy 10.352139990540035
epoch: 32, step: 118
	action: tensor([[-13721.2840,   3114.4327,  -7984.9361,  -4266.4313,   7718.3632,
             43.1172,   6134.3956]], dtype=torch.float64)
	q_value: tensor([[-31.6667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7911531616258336, distance: 1.5315213494555144 entropy 10.408982276588274
epoch: 32, step: 119
	action: tensor([[-12621.2699,   5983.1201,  -6750.9291,  -6807.1256,  -5755.4616,
          -5458.0696,   4175.9419]], dtype=torch.float64)
	q_value: tensor([[-32.3045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25051260760062266, distance: 0.9906924633348948 entropy 10.243121100857362
epoch: 32, step: 120
	action: tensor([[-10419.7790,  -6919.8016,    439.4311,   4646.3239, -13261.7486,
          -5064.5945,   -773.6793]], dtype=torch.float64)
	q_value: tensor([[-37.2251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6544086655637475, distance: 1.4718992749507984 entropy 10.358678240042588
epoch: 32, step: 121
	action: tensor([[ -1573.5169,  -2257.6261,   8004.5138,   1912.7862,    958.5778,
           6151.5217, -11152.9933]], dtype=torch.float64)
	q_value: tensor([[-22.3194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8881991180295605, distance: 1.5724635546468075 entropy 10.180475890013613
epoch: 32, step: 122
	action: tensor([[  9064.6600, -15411.3161, -10408.3183,   1671.8049, -13820.7805,
         -11032.5615,  -6958.3417]], dtype=torch.float64)
	q_value: tensor([[-25.2634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05666409046303611, distance: 1.111449863107562 entropy 10.420485670840062
epoch: 32, step: 123
	action: tensor([[ -6165.2457,  -7363.0959,  -4749.9114,   5465.0732,   5114.1205,
           1933.5624, -19871.8108]], dtype=torch.float64)
	q_value: tensor([[-25.9429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06713321340354261, distance: 1.1052652318981018 entropy 10.272280347087205
epoch: 32, step: 124
	action: tensor([[-15512.0893,   5423.2864,   9240.5720,   7347.3188,   -831.4329,
          -5380.6780,   4693.8827]], dtype=torch.float64)
	q_value: tensor([[-24.2037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35500306402309156, distance: 0.9190423386936215 entropy 10.36700860003397
epoch: 32, step: 125
	action: tensor([[-15593.7244,  -4433.2430,   4830.6553,   6321.2263,  -1894.8249,
           2978.2060,  -5540.1664]], dtype=torch.float64)
	q_value: tensor([[-30.3960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.053743635258598, distance: 1.6399469842870966 entropy 10.384175529174698
epoch: 32, step: 126
	action: tensor([[  8996.0096, -14479.5188, -10977.0996,   3680.1033,  -2074.1997,
          -4493.8524,  -7238.5172]], dtype=torch.float64)
	q_value: tensor([[-25.2669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4792820523564991, distance: 0.8257678431439663 entropy 10.380912147114978
epoch: 32, step: 127
	action: tensor([[ -2464.3545, -12102.5256,   -769.7406,  -3785.7467,  -3061.5317,
           3254.3224,  -5448.3966]], dtype=torch.float64)
	q_value: tensor([[-22.6622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2179911320779231, distance: 1.2629284782324555 entropy 10.115083309222507
LOSS epoch 32 actor 295.8782772462155 critic 201.51845868953558
epoch: 33, step: 0
	action: tensor([[-3950.7096, -9077.0674, -9491.7036,  3855.7760,   243.3004,  5528.6533,
          1038.4645]], dtype=torch.float64)
	q_value: tensor([[-24.2470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.728614734611909, distance: 1.5045471368557735 entropy 10.316938160402557
epoch: 33, step: 1
	action: tensor([[ 3616.7708, -6770.0856, -1759.7337,   565.6811,   371.7939, -8806.0878,
          7550.8267]], dtype=torch.float64)
	q_value: tensor([[-22.4164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49178903554410536, distance: 0.8157906220193272 entropy 10.33820106850592
epoch: 33, step: 2
	action: tensor([[ -963.9178, -9571.2429,  1335.8041,  -708.4775,  1215.1171, -3266.1693,
         -2808.0976]], dtype=torch.float64)
	q_value: tensor([[-23.2003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7104836304085076, distance: 1.4966358859321982 entropy 10.093582004363478
epoch: 33, step: 3
	action: tensor([[-1968.1443,  7255.4386, -4214.5497,  6774.5149, -9285.6832, -5033.1139,
         -7443.5352]], dtype=torch.float64)
	q_value: tensor([[-24.2276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7094180806324613, distance: 0.6168662180953012 entropy 10.363310716994434
epoch: 33, step: 4
	action: tensor([[-2316.3425,   -58.7743, 17998.6218, 13915.2406,  4848.8988,   301.0346,
          1181.9544]], dtype=torch.float64)
	q_value: tensor([[-26.7746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.3074614462689
epoch: 33, step: 5
	action: tensor([[-7144.2078, -2210.8773,   202.8227,  4352.1271,  5677.0562,   430.2304,
         -3096.8822]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.879674460782558
epoch: 33, step: 6
	action: tensor([[-7343.4181,  5449.6202, -1577.9050,  8385.0907,  1019.3163,  2427.4333,
          3621.0544]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2876664375274782, distance: 1.298549117251483 entropy 9.879674460782558
epoch: 33, step: 7
	action: tensor([[-5552.6750,  6853.9394,  6310.9258, -2276.8562,  3315.2866, -9018.0834,
          4086.3424]], dtype=torch.float64)
	q_value: tensor([[-24.8915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46175331472806713, distance: 0.839551564457068 entropy 10.168805356570692
epoch: 33, step: 8
	action: tensor([[ -5413.9871,  -5619.9549,  -5319.5829,   4290.4618,  -4649.9883,
         -19758.4254,   8132.6189]], dtype=torch.float64)
	q_value: tensor([[-38.8182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4187869578092993, distance: 1.363060984760567 entropy 10.586122107014353
epoch: 33, step: 9
	action: tensor([[  4830.3135, -11821.4102,  -6910.9545, -13057.9845,  12104.1409,
           5680.4164,  -6430.3745]], dtype=torch.float64)
	q_value: tensor([[-24.5949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3503376561174346, distance: 0.9223601698189285 entropy 10.455477793190914
epoch: 33, step: 10
	action: tensor([[-19089.1119,  -1833.4846,  -7943.6202,   9049.1276,   -513.3850,
           4034.4360,   7389.0076]], dtype=torch.float64)
	q_value: tensor([[-26.5996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4972837764443352, distance: 1.4002602251649268 entropy 10.442351319205846
epoch: 33, step: 11
	action: tensor([[-13519.6888,  -5762.2117,    -26.1135,   9406.2824,   6369.3176,
         -10770.5527,   1385.1680]], dtype=torch.float64)
	q_value: tensor([[-21.9330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023418462746454427, distance: 1.157666102693302 entropy 10.26157173184345
epoch: 33, step: 12
	action: tensor([[  -332.7915,   -360.6954,  -5350.0215,  10003.3348,   3358.9452,
         -10774.6216,  -9209.1971]], dtype=torch.float64)
	q_value: tensor([[-23.4375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08905702750725497, distance: 1.0922002916628601 entropy 10.480014404228301
epoch: 33, step: 13
	action: tensor([[  1519.5127,   -807.8090,  -2366.4688,  -4574.0896,  -1666.1696,
         -11262.8831,   8838.3418]], dtype=torch.float64)
	q_value: tensor([[-27.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1489311584618771, distance: 1.0556963954261218 entropy 10.670441559427326
epoch: 33, step: 14
	action: tensor([[ 7800.8420, -6743.6656, -4410.9593, -4126.3100,  2482.3526,  9272.2238,
         -7832.3895]], dtype=torch.float64)
	q_value: tensor([[-25.2455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11395307549248657, distance: 1.0771719966891629 entropy 10.244091938938384
epoch: 33, step: 15
	action: tensor([[  5311.2692, -11350.5468,  -5795.4218,  17450.7994,   6398.5867,
          -1437.0203,   -467.6668]], dtype=torch.float64)
	q_value: tensor([[-27.1140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5414854074252806, distance: 1.4207785839794838 entropy 10.371246532581178
epoch: 33, step: 16
	action: tensor([[-26480.5683,  11528.1863,   2378.2683,    115.4299,  -1442.4184,
           4101.9094,   6744.2381]], dtype=torch.float64)
	q_value: tensor([[-30.6687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.357415758513879
epoch: 33, step: 17
	action: tensor([[ -439.9121, -2982.3133, -1828.6025,   679.0024,  1100.2879, -1530.8834,
          2651.1823]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18245273713661136, distance: 1.2443672963189591 entropy 9.879674460782558
epoch: 33, step: 18
	action: tensor([[ -1999.0515,   4011.3744, -14777.1365,  -3655.7777,  -4373.7247,
           4427.0048,    968.9076]], dtype=torch.float64)
	q_value: tensor([[-19.6310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20937501054822838, distance: 1.2584535450972651 entropy 10.051964031759017
epoch: 33, step: 19
	action: tensor([[  -353.7389, -12306.3355,    390.8077,   3716.8689,   1583.4906,
          -2501.5376,   -928.2174]], dtype=torch.float64)
	q_value: tensor([[-22.3179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.611916296455985, distance: 1.4528739477806738 entropy 9.867672616633085
epoch: 33, step: 20
	action: tensor([[-7399.6300, -4294.6742,  8512.8276, -5143.2783,    79.9778, 16774.7114,
          2999.0853]], dtype=torch.float64)
	q_value: tensor([[-21.8568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4511722238191247, distance: 1.3785298414861888 entropy 10.289376839380585
epoch: 33, step: 21
	action: tensor([[-16753.8709,  -1658.7815,   8297.8297,   7109.4957,  -4833.3424,
          14115.6984,    497.9268]], dtype=torch.float64)
	q_value: tensor([[-23.2076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11128155615878477, distance: 1.2063372724078352 entropy 10.269490809550348
epoch: 33, step: 22
	action: tensor([[-9913.4493, -3803.0353, -6284.7030, -2189.6923,  1187.3804,  9016.8720,
         -2615.7300]], dtype=torch.float64)
	q_value: tensor([[-24.6401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8817530263652866, distance: 1.5697771562944178 entropy 10.464517725724637
epoch: 33, step: 23
	action: tensor([[ 4247.7100,  -240.6605,  3744.7616, 10583.4736, -6261.0920,  4502.8342,
         12610.1626]], dtype=torch.float64)
	q_value: tensor([[-25.8905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2823477945805376, distance: 0.9694238612614364 entropy 10.376272054693455
epoch: 33, step: 24
	action: tensor([[-7125.0024, -8457.1202, 13095.9919,  7176.9557,  1802.5390, -8780.9186,
          -455.0426]], dtype=torch.float64)
	q_value: tensor([[-23.8248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20726222146726925, distance: 1.2573537997086774 entropy 10.35436162681584
epoch: 33, step: 25
	action: tensor([[  1225.9274, -12270.2012,  -1246.7021,  28408.1426,  -7196.8582,
           -343.7638,   1866.1212]], dtype=torch.float64)
	q_value: tensor([[-24.1244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.330414663575609, distance: 0.9363962383545676 entropy 10.438755239873503
epoch: 33, step: 26
	action: tensor([[ -9107.7430, -15743.3209,   1569.5438,  -5505.7957,  18071.1791,
          -9047.6534,   5054.4356]], dtype=torch.float64)
	q_value: tensor([[-27.3809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13453072260847998, distance: 1.2188908691665539 entropy 10.499793462086416
epoch: 33, step: 27
	action: tensor([[-14368.8846, -16152.5501,   -774.6763,  10651.2175,   6299.9995,
            686.3696,  -5759.5105]], dtype=torch.float64)
	q_value: tensor([[-27.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024124770654928684, distance: 1.1304564613483679 entropy 10.495546534808167
epoch: 33, step: 28
	action: tensor([[-15721.7625,  -5577.6116,  -6247.4689,  12200.8392,  -7465.1523,
         -10555.7810,  -2990.4131]], dtype=torch.float64)
	q_value: tensor([[-23.4814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.78023784028851, distance: 1.5268476581085213 entropy 10.46216272083397
epoch: 33, step: 29
	action: tensor([[  2308.7743, -12842.2788,   4126.6380,   7476.8224,  -4717.5196,
         -14189.7130,   6740.2493]], dtype=torch.float64)
	q_value: tensor([[-21.5902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10495648101470878, distance: 1.2028993218643258 entropy 10.379117559681728
epoch: 33, step: 30
	action: tensor([[-7503.1084, -8494.1922, -1907.6368,  5534.2301, -5544.8699, -5158.2952,
          8562.1741]], dtype=torch.float64)
	q_value: tensor([[-23.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10886806091666068, distance: 1.0802585143925567 entropy 10.268448064183316
epoch: 33, step: 31
	action: tensor([[ -6895.7482, -15063.5795,    200.7041,  -5362.9429,  -3179.8632,
          18964.0050,  11975.4448]], dtype=torch.float64)
	q_value: tensor([[-28.0115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6641368836403603, distance: 1.4762204470972247 entropy 10.716373839067746
epoch: 33, step: 32
	action: tensor([[-11706.8102,   8255.0149,   7521.9454,   4870.8146,  -5131.9783,
          -1626.4133,   4637.3274]], dtype=torch.float64)
	q_value: tensor([[-23.2502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3847466833477481, distance: 0.8976017177738209 entropy 10.207850987892527
epoch: 33, step: 33
	action: tensor([[ 2318.8680,  3968.9141, -7578.3146,  4041.4339,  -244.1895,  8971.3955,
          6596.4023]], dtype=torch.float64)
	q_value: tensor([[-24.8920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31928371856612603, distance: 0.9441473149332015 entropy 10.155905502602819
epoch: 33, step: 34
	action: tensor([[-13612.0238, -12907.6487,   2707.8229,   1414.7152,  -1629.1397,
            129.0358,  -5456.9965]], dtype=torch.float64)
	q_value: tensor([[-23.7518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.552392600387537, distance: 1.4257962733718077 entropy 10.139837702723293
epoch: 33, step: 35
	action: tensor([[-6749.3386, -8579.9962,  8518.6051,  5347.1469,  3291.1276, -5415.7413,
          1432.3477]], dtype=torch.float64)
	q_value: tensor([[-24.4925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7297386660826624, distance: 1.5050361796314322 entropy 10.467590958735457
epoch: 33, step: 36
	action: tensor([[-14841.8895,  -9874.0231,   -436.5603,   9338.7265,   1557.2350,
           3461.4257,   2009.4541]], dtype=torch.float64)
	q_value: tensor([[-21.0258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25989875977036114, distance: 0.9844694853157435 entropy 10.362407215620767
epoch: 33, step: 37
	action: tensor([[-17289.5215,   2741.9272,  -1390.4970,   8294.0965, -14767.2919,
           4219.2497,   2927.3650]], dtype=torch.float64)
	q_value: tensor([[-25.3279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10577999867879573, distance: 1.2033474953225942 entropy 10.542431966118471
epoch: 33, step: 38
	action: tensor([[-4007.0290,  9864.2323,  3819.7739, -1640.0987,  1567.2387,  5897.3773,
         -2519.7269]], dtype=torch.float64)
	q_value: tensor([[-28.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1851104027195094, distance: 1.033013697353303 entropy 10.412764914298473
epoch: 33, step: 39
	action: tensor([[  908.1270,  -523.1612,   322.3098,  7363.0122, -2965.4002, -1878.2661,
          1781.3039]], dtype=torch.float64)
	q_value: tensor([[-30.1202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.234322730475883
epoch: 33, step: 40
	action: tensor([[  4294.3648,  -6551.6933,    225.4821,  -2604.3501, -10547.1510,
          -3372.5876,   6956.3790]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27857290107109667, distance: 1.293955798223607 entropy 9.879674460782558
epoch: 33, step: 41
	action: tensor([[ -6308.4045, -15144.0924,   7322.6064,  13908.3568,   1129.6496,
           7116.3715,   5089.7301]], dtype=torch.float64)
	q_value: tensor([[-24.7041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5043807857574536, distance: 1.4035748646161244 entropy 10.23404090921181
epoch: 33, step: 42
	action: tensor([[  7965.6876,  -6578.0407,  -6568.6838,  -5185.3802,   7881.8588,
         -12080.2062,   2027.5323]], dtype=torch.float64)
	q_value: tensor([[-21.9831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16232277904006676, distance: 1.047357739183455 entropy 10.313632387874392
epoch: 33, step: 43
	action: tensor([[-14673.7892, -13407.9989,  -7432.2865,  -7022.8472,  -7828.5185,
          13487.7353,  -2508.9779]], dtype=torch.float64)
	q_value: tensor([[-27.6081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6970776215991368, distance: 1.4907593661758638 entropy 10.441200332653798
epoch: 33, step: 44
	action: tensor([[-8635.5955,  3822.0689, -8743.4466,  4323.7999, -8232.7958, -9540.4141,
         10573.3046]], dtype=torch.float64)
	q_value: tensor([[-25.8766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17130056854126274, distance: 1.238485340352059 entropy 10.30748167566212
epoch: 33, step: 45
	action: tensor([[ 7983.1364, -5698.1399,  2641.2274,  3474.2948, -2309.2344,   972.2460,
          1233.7547]], dtype=torch.float64)
	q_value: tensor([[-26.7340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4115611458164026, distance: 1.359585552880797 entropy 10.349117743288483
epoch: 33, step: 46
	action: tensor([[ 2203.7478,  2943.4757, 25079.7347,  3655.5292, -6812.7707, -4713.3442,
          4001.4148]], dtype=torch.float64)
	q_value: tensor([[-29.3396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.750493838109434, distance: 0.5716067268428944 entropy 10.449155888084446
epoch: 33, step: 47
	action: tensor([[-12070.3819,  -4715.6853,   3824.4672,   5941.0407,   3905.3352,
          -5802.8004,   7317.4282]], dtype=torch.float64)
	q_value: tensor([[-29.5020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24147886195910517, distance: 0.9966451031201049 entropy 10.402718949063344
epoch: 33, step: 48
	action: tensor([[-12891.9378,   3559.3203,  -2516.3814,  10417.5800, -10471.0763,
           6611.0322,  -5601.6458]], dtype=torch.float64)
	q_value: tensor([[-24.3796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17536999441279977, distance: 1.2406348972130168 entropy 10.575548403539646
epoch: 33, step: 49
	action: tensor([[ -1734.4137,  -1737.1989, -17017.7256,  -9287.4164,   7290.3971,
          -3289.7138,  17652.1765]], dtype=torch.float64)
	q_value: tensor([[-33.8755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6951836948746213, distance: 1.4899272954656289 entropy 10.648997119818194
epoch: 33, step: 50
	action: tensor([[-1699.5294, -4862.6429,  6978.3984, 20512.2220, 11386.0932, 10103.8632,
         -5321.7128]], dtype=torch.float64)
	q_value: tensor([[-29.0959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30706538107992465, distance: 1.3082939989320745 entropy 10.524281327353505
epoch: 33, step: 51
	action: tensor([[-10265.9888, -12923.4180,   7823.1140,  -3107.6994,   -904.3688,
          -9352.9252, -14506.0401]], dtype=torch.float64)
	q_value: tensor([[-22.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17318749927897104, distance: 1.239482520620386 entropy 10.360620974314608
epoch: 33, step: 52
	action: tensor([[   738.9069,  -2813.8040,  -7619.0978,   -220.3842,  -4347.7663,
          10995.7892, -10569.3741]], dtype=torch.float64)
	q_value: tensor([[-20.1630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4057385407238022, distance: 0.8821561695670533 entropy 10.117557938888385
epoch: 33, step: 53
	action: tensor([[-1665.2837, -2011.7246,  6277.9423, 13346.7150, -1171.8209, -6368.0672,
          5792.1160]], dtype=torch.float64)
	q_value: tensor([[-24.2128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21458353542416164, distance: 1.0141605055499976 entropy 10.216687903441754
epoch: 33, step: 54
	action: tensor([[-14859.5958,   -500.0460,   -593.1434,  -9083.1666, -14955.1965,
           4659.0610,  -4428.2078]], dtype=torch.float64)
	q_value: tensor([[-24.0189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3981734143452613, distance: 1.3531228041061478 entropy 10.342090742061808
epoch: 33, step: 55
	action: tensor([[ 5742.6034,  4390.7025, -3542.6292, 10588.5702, -4644.0159, 17276.6116,
         15321.0961]], dtype=torch.float64)
	q_value: tensor([[-24.0583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.3511838212591
epoch: 33, step: 56
	action: tensor([[ -1781.3163, -13744.8292,  -1480.6519,  -2257.7094,    646.1348,
           1896.5745,   5432.0475]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.501294568703341, distance: 1.4021344179447632 entropy 9.879674460782558
epoch: 33, step: 57
	action: tensor([[-10375.7941, -16236.8430, -13304.2448,   3385.3608, -14870.6706,
          -1952.6516, -13258.2612]], dtype=torch.float64)
	q_value: tensor([[-27.9429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29177531076186214, distance: 1.3006192667686345 entropy 10.419437522549657
epoch: 33, step: 58
	action: tensor([[  7314.1730,  -7265.1130, -17916.6542,  12911.1350,  -6286.5441,
          -4221.4530, -23695.2379]], dtype=torch.float64)
	q_value: tensor([[-22.9367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4729168940555559, distance: 0.8307995286927649 entropy 10.354481820775485
epoch: 33, step: 59
	action: tensor([[-10753.1246,  -9874.1052,    528.3498,  10452.1271,  -2374.3503,
          -4861.2352,   3170.7489]], dtype=torch.float64)
	q_value: tensor([[-32.6997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.408825322667411, distance: 1.3582673706430233 entropy 10.470293645011614
epoch: 33, step: 60
	action: tensor([[-17362.4822,  -3507.0055,  -7168.0541,   -737.6446,   5664.3173,
           -160.5417,   8946.5111]], dtype=torch.float64)
	q_value: tensor([[-23.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6476127248668575, distance: 1.4688730475120366 entropy 10.369269026573903
epoch: 33, step: 61
	action: tensor([[ -3640.0177, -10692.3128,   5892.3948,   2270.6141,  -5449.5916,
          -6422.3330, -12606.8127]], dtype=torch.float64)
	q_value: tensor([[-24.8088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07757845561319454, distance: 1.0990600255984426 entropy 10.408291161762081
epoch: 33, step: 62
	action: tensor([[-16356.5385,   8839.4647,   2269.8202,   8074.4738,  11674.4029,
            996.7955,  -6378.1252]], dtype=torch.float64)
	q_value: tensor([[-24.7423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18743408512145798, distance: 1.0315398109507918 entropy 10.550604404169453
epoch: 33, step: 63
	action: tensor([[-2600.1729, -1482.8779,  5795.3305, -5055.8589, -2375.7323, -3803.5413,
           143.6469]], dtype=torch.float64)
	q_value: tensor([[-24.7140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.199374885253816
epoch: 33, step: 64
	action: tensor([[ -767.6187,  4754.9632,  4044.5569, -1073.6467, -3622.3724,  1789.6669,
         -2999.1809]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6592049730663079, distance: 0.6680413078800889 entropy 9.879674460782558
epoch: 33, step: 65
	action: tensor([[-11843.9573,  -3856.9916,  -3811.2378,  20243.0225,  -1919.5631,
         -12193.8029,   5574.0996]], dtype=torch.float64)
	q_value: tensor([[-31.5052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02966812915800532, distance: 1.1272411681932335 entropy 10.303866102274513
epoch: 33, step: 66
	action: tensor([[  9357.4124, -16706.0655,   1100.2815,   3868.6652, -10575.8097,
          13743.8784,  -6589.6524]], dtype=torch.float64)
	q_value: tensor([[-29.0639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6257951284299985, distance: 0.7000215530917232 entropy 10.5168213701369
epoch: 33, step: 67
	action: tensor([[  1884.3541,  -3708.6289,  10359.6555,  11934.2298, -13516.6508,
          12726.5075,  -9099.8797]], dtype=torch.float64)
	q_value: tensor([[-25.2198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34601449965497844, distance: 0.9254239887881699 entropy 10.271199693007564
epoch: 33, step: 68
	action: tensor([[  2287.4016,    226.0388,   3077.4306,  -1945.5502, -21999.8078,
          -4582.6323,   -458.9387]], dtype=torch.float64)
	q_value: tensor([[-25.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.835707253596707, distance: 0.46383753290013396 entropy 10.393636247539282
epoch: 33, step: 69
	action: tensor([[  878.4898, -4679.2758,  -317.4050, 10020.8698,  3034.8341, -1091.5374,
          -652.6677]], dtype=torch.float64)
	q_value: tensor([[-24.5691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35489143707144644, distance: 1.3320144687155069 entropy 10.190320012724944
epoch: 33, step: 70
	action: tensor([[  4169.8596,   2043.8526,   2249.0873,   5585.2387,   2042.8484,
         -10283.6523,   -153.5225]], dtype=torch.float64)
	q_value: tensor([[-31.0529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8753219126880223, distance: 0.40406548867787134 entropy 10.353657894677204
epoch: 33, step: 71
	action: tensor([[ -5304.0011,  -7510.5651,  -3901.4001,  -4874.6154, -13573.3222,
           2726.5018,   4549.5192]], dtype=torch.float64)
	q_value: tensor([[-28.9945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.318253169894714
epoch: 33, step: 72
	action: tensor([[  23.2959,  181.4863, 7626.0269,  414.5742, 5695.9198, 8439.5817,
         8440.3132]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.879674460782558
epoch: 33, step: 73
	action: tensor([[ 2632.8893, -5589.3984,  7448.0028,   -60.8752,  1914.8719,  -146.5047,
         -7527.9940]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05576583405790181, distance: 1.1119789055181712 entropy 9.879674460782558
epoch: 33, step: 74
	action: tensor([[-2416.5690,  2651.0778, -3217.5295, 20014.0052, -4945.5793, 14134.1981,
           765.4970]], dtype=torch.float64)
	q_value: tensor([[-20.8170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016326279635336394, distance: 1.1349643696511045 entropy 10.352617437776003
epoch: 33, step: 75
	action: tensor([[  -52.8309,  3195.2392, -3256.1429,   161.4935,  3874.2094,   162.1005,
         11679.3865]], dtype=torch.float64)
	q_value: tensor([[-24.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08513554289264347, distance: 1.1920615709325388 entropy 10.260298259251774
epoch: 33, step: 76
	action: tensor([[-1804.4246, -4461.9657, -3406.8379,  7601.0874,  4547.2429, -1966.3133,
         -2545.0716]], dtype=torch.float64)
	q_value: tensor([[-28.5110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35150102919257065, distance: 1.3303468439299706 entropy 10.475686409370624
epoch: 33, step: 77
	action: tensor([[ -2403.2765,  -2832.2575,  -2362.6251,  16480.7795, -10066.4141,
          11981.1856,    783.0723]], dtype=torch.float64)
	q_value: tensor([[-20.9518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10337788658431912, distance: 1.2020397545876946 entropy 10.361096696694574
epoch: 33, step: 78
	action: tensor([[-4073.8313, -9048.8030,  4604.4970,  4196.5489,  7304.1402,  5655.9517,
          2147.9678]], dtype=torch.float64)
	q_value: tensor([[-22.9907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.875591934478612, distance: 1.567205227192275 entropy 10.360995384207884
epoch: 33, step: 79
	action: tensor([[ -4616.3860, -11441.7888,   7437.0847,   1579.6328,  -6700.0270,
          11539.1716,   8331.9127]], dtype=torch.float64)
	q_value: tensor([[-26.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24876655654093094, distance: 1.2787843802017083 entropy 10.495675730900633
epoch: 33, step: 80
	action: tensor([[-13927.7113,   3305.7228,   5679.6769,  -2278.2923,  13395.6320,
           7090.5884,   1655.3600]], dtype=torch.float64)
	q_value: tensor([[-25.4446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13739768022540377, distance: 1.0628255939881783 entropy 10.5221243878029
epoch: 33, step: 81
	action: tensor([[  4624.0189,  -3639.3719, -11367.5898,    698.0333,  12130.7555,
          -3709.0919,    650.4420]], dtype=torch.float64)
	q_value: tensor([[-31.2077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5945358097048241, distance: 0.7286734493238715 entropy 10.298958633428214
epoch: 33, step: 82
	action: tensor([[-10119.4305,   4415.9043,  15597.2617,   9516.0170,  -6254.0444,
          -6399.0831,  -4305.9402]], dtype=torch.float64)
	q_value: tensor([[-24.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1711702488547977, distance: 1.0418120088509342 entropy 10.496264751133099
epoch: 33, step: 83
	action: tensor([[-1856.8993,  -881.6293,  2038.3504, 21057.7109, -3199.0992, 30218.7937,
          4863.0238]], dtype=torch.float64)
	q_value: tensor([[-32.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058840871323871724, distance: 1.110166767615953 entropy 10.582737205509428
epoch: 33, step: 84
	action: tensor([[-6682.9678, -8531.9515, -3560.7832,  3359.7390,  2461.3734,  5625.0032,
         -1679.3322]], dtype=torch.float64)
	q_value: tensor([[-23.9852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19410176604437357, distance: 1.2504817665282681 entropy 10.379838754939337
epoch: 33, step: 85
	action: tensor([[-6692.6306, -4086.6844,  3730.0707, -7910.4057, -3742.2665,  1605.8582,
         17357.2069]], dtype=torch.float64)
	q_value: tensor([[-24.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6416875356509963, distance: 1.4662294682543817 entropy 10.466531661743188
epoch: 33, step: 86
	action: tensor([[-22361.8786,   -157.0540,  -4805.8347,   4849.4494, -11776.4777,
            455.4475,  -4669.0830]], dtype=torch.float64)
	q_value: tensor([[-30.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2724403733044505, distance: 1.2908489132604504 entropy 10.60407196542222
epoch: 33, step: 87
	action: tensor([[-3037.1195,  1412.3553, -9279.1956, -6713.7573, -8936.0615,  1781.6809,
         -5983.4373]], dtype=torch.float64)
	q_value: tensor([[-23.7272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888812384552242, distance: 0.9650009962265872 entropy 10.377105232070816
epoch: 33, step: 88
	action: tensor([[-3453.1869,  -876.7276, -6185.4039, 10099.2931,  8267.8143, -7824.0684,
          4902.1377]], dtype=torch.float64)
	q_value: tensor([[-28.7370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1747440395602864, distance: 1.2403044970584436 entropy 10.213558471471742
epoch: 33, step: 89
	action: tensor([[ -3627.0491, -11953.5586,   7184.9145,   5289.4051,   8079.1123,
         -14526.8169,  10370.7611]], dtype=torch.float64)
	q_value: tensor([[-22.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47942498487618757, distance: 1.3918844011221874 entropy 10.405895548676869
epoch: 33, step: 90
	action: tensor([[ -259.7530, -2308.1909,   819.0443,  1873.8408,  -157.0034, -8544.5117,
          -231.3376]], dtype=torch.float64)
	q_value: tensor([[-25.9463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23796107904332642, distance: 1.2732397501651394 entropy 10.647193292621122
epoch: 33, step: 91
	action: tensor([[-3903.5258, -3853.3357, -4231.9033, -1425.9902,  1798.8951, -2741.6865,
          3211.7225]], dtype=torch.float64)
	q_value: tensor([[-23.3342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4700867246529725, distance: 1.3874845989556053 entropy 10.363726057460514
epoch: 33, step: 92
	action: tensor([[ -4142.3471, -13608.9688,    -83.5518,  -1473.4053,   -124.2226,
          -4035.2448,  -2317.3691]], dtype=torch.float64)
	q_value: tensor([[-24.4027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19762570922035072, distance: 1.0250503439085534 entropy 10.376395868495768
epoch: 33, step: 93
	action: tensor([[-16514.6570,    955.3998,  -1235.2840,   7056.7789, -10759.5471,
           -541.5401,  -9651.9891]], dtype=torch.float64)
	q_value: tensor([[-29.2138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4593156413601913, distance: 1.382392318670984 entropy 10.413382282451822
epoch: 33, step: 94
	action: tensor([[   505.2873, -14582.2810,  -3559.8609,  17970.1932,   6345.0077,
          -4280.4833,    461.1563]], dtype=torch.float64)
	q_value: tensor([[-34.8355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3078623426977336, distance: 1.3086927935253954 entropy 10.51644609647906
epoch: 33, step: 95
	action: tensor([[-12851.5562,   2453.9857,  -2581.1031,   5967.6784,   5368.2151,
          -4796.6420,  -2234.5640]], dtype=torch.float64)
	q_value: tensor([[-27.8760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23338474223016126, distance: 1.0019485534542287 entropy 10.442102216166505
epoch: 33, step: 96
	action: tensor([[  2234.6953,  -5814.3807, -13368.6338,   -812.6923,    759.8911,
          -8354.3149,  -8993.4070]], dtype=torch.float64)
	q_value: tensor([[-29.8916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.46145301414579
epoch: 33, step: 97
	action: tensor([[ 4796.0867,   391.4719,  1775.7967, 16878.7895,  1328.4723,  -763.5619,
          2560.1493]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5848079946173624, distance: 0.7373627335666754 entropy 9.879674460782558
epoch: 33, step: 98
	action: tensor([[ 3.3386e+03, -6.2315e+03,  7.0771e+03,  5.3142e+03,  3.2947e+03,
         -1.3361e+00, -1.3419e+03]], dtype=torch.float64)
	q_value: tensor([[-30.6098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2014539254308514, distance: 1.0226021061079076 entropy 10.414190395572703
epoch: 33, step: 99
	action: tensor([[ -3906.7909,   8192.6460,    835.0511, -15369.1908, -23711.0086,
            721.2085,   9472.1668]], dtype=torch.float64)
	q_value: tensor([[-32.9654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.430288513156722
epoch: 33, step: 100
	action: tensor([[  530.7285, -5437.8200,   299.8323,  7220.1880,  2013.9926,  4128.1437,
          2354.9385]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.879674460782558
epoch: 33, step: 101
	action: tensor([[ -2783.3295, -14462.4121,  -1014.4121,   3417.5552,  -6905.0576,
          11681.1716,    715.7303]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0719008019089874, distance: 1.1024372827337519 entropy 9.879674460782558
epoch: 33, step: 102
	action: tensor([[ -7640.4649,  -4333.5149,   8996.7333,   -842.5733,  -2981.2306,
         -10509.5102,   5183.6269]], dtype=torch.float64)
	q_value: tensor([[-24.7228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8319360701974379, distance: 1.548858880624735 entropy 10.487467738247204
epoch: 33, step: 103
	action: tensor([[-15155.4745,  -4641.6302,  -4453.4845,    275.4091,   2185.9188,
           9741.6136,   4954.8568]], dtype=torch.float64)
	q_value: tensor([[-22.6942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39079161082275915, distance: 1.3495460999171625 entropy 10.319503672108926
epoch: 33, step: 104
	action: tensor([[-11017.2715, -10287.0617,  -1338.3875,   3889.3868,   4230.1296,
          -1133.5389,  21588.2751]], dtype=torch.float64)
	q_value: tensor([[-21.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6539528207956544, distance: 1.4716964823089984 entropy 10.272716363237167
epoch: 33, step: 105
	action: tensor([[ -1806.6078,  -9020.1666,   5548.0052,  -5324.2554,   -385.5701,
         -15381.6711,   8551.8371]], dtype=torch.float64)
	q_value: tensor([[-19.9548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6629455585940174, distance: 1.4756919529359664 entropy 10.2561750437865
epoch: 33, step: 106
	action: tensor([[-6403.2380,  3583.7137,   651.4416,  1847.3927, 13284.2763,  3392.3989,
         12406.2763]], dtype=torch.float64)
	q_value: tensor([[-23.5006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4033870652300011, distance: 0.8838997797051602 entropy 10.289065878107612
epoch: 33, step: 107
	action: tensor([[ 7152.1879, -5823.7176,  3432.0638, 11911.3451, -4326.8399,   318.8158,
          -953.1776]], dtype=torch.float64)
	q_value: tensor([[-27.6663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.32054957996525
epoch: 33, step: 108
	action: tensor([[ -206.6475, 12058.9375,  5386.6792,  5676.1237,   110.8636, -2037.1436,
          2980.5760]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17975516031165362, distance: 1.2429470733583097 entropy 9.879674460782558
epoch: 33, step: 109
	action: tensor([[  4205.5268,   6388.4690, -10375.6062,   9920.4996, -18584.2502,
            212.7164,   9606.9709]], dtype=torch.float64)
	q_value: tensor([[-26.3759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36047624487429286, distance: 1.3347569005049007 entropy 10.356715692489901
epoch: 33, step: 110
	action: tensor([[-4669.6205,  -864.5550,  6609.1647,  3360.8421,  6527.2458,  -526.1717,
          2492.0048]], dtype=torch.float64)
	q_value: tensor([[-21.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9143547931732379, distance: 1.5833171220753628 entropy 9.96474059095381
epoch: 33, step: 111
	action: tensor([[ 10675.2830, -11192.6220,   2666.8862,  -6171.5276,  -2351.3771,
           2629.2529,   3115.1979]], dtype=torch.float64)
	q_value: tensor([[-21.4308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12476038015379609, distance: 1.070582592838932 entropy 10.345088308932507
epoch: 33, step: 112
	action: tensor([[-12001.0684,  14051.4409,  -2110.0468,  -1914.5204,   8693.3325,
           -230.5356,   8007.0293]], dtype=torch.float64)
	q_value: tensor([[-24.3259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.335855259104651
epoch: 33, step: 113
	action: tensor([[-3056.3828, -3089.1875,   412.7876,  5861.8823, -6079.2178,  3519.8605,
         -5723.9113]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9297093046764395, distance: 1.5896541154832147 entropy 9.879674460782558
epoch: 33, step: 114
	action: tensor([[ 7383.1760,   759.2038,  1192.1630, -4270.9490,  1011.8712,  2722.1980,
         19238.3249]], dtype=torch.float64)
	q_value: tensor([[-25.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.442140853217143
epoch: 33, step: 115
	action: tensor([[ 5054.6553, -2173.3760,   742.8922, 12040.8735,  8174.5847, -3495.2327,
          5389.0998]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.480734021745163, distance: 0.8246157543610577 entropy 9.879674460782558
epoch: 33, step: 116
	action: tensor([[ -5368.1328,   2924.7066, -11229.3441,   2797.2623,  -7150.9810,
           5959.0240,   6440.4292]], dtype=torch.float64)
	q_value: tensor([[-22.8343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.216055218100566
epoch: 33, step: 117
	action: tensor([[-1452.3343, -2952.9648, -2273.4177,  1529.2274,  4142.2860,   728.0415,
           314.8374]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46829208603052463, distance: 1.3866374401412063 entropy 9.879674460782558
epoch: 33, step: 118
	action: tensor([[-6187.3106,   763.9975,  5740.6640,  5203.3682, -4395.8209, -7740.7279,
         -7353.8322]], dtype=torch.float64)
	q_value: tensor([[-22.6943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06245541173472502, distance: 1.1080329080524662 entropy 10.285456914573412
epoch: 33, step: 119
	action: tensor([[-4355.3969, 11696.6566,  3907.4297, -1759.9202,  1125.6829, 28690.7415,
          8688.6129]], dtype=torch.float64)
	q_value: tensor([[-29.2090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19426447363059485, distance: 1.0271951253067686 entropy 10.425260774976573
epoch: 33, step: 120
	action: tensor([[-12194.4699, -10217.0063,  15582.3011,   4314.2160,   2554.5527,
           4822.8749,  -1715.2257]], dtype=torch.float64)
	q_value: tensor([[-29.7738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8594382484345191, distance: 1.560441792654911 entropy 10.136790868168529
epoch: 33, step: 121
	action: tensor([[-2303.0269,  4071.8236,  4452.2377, -5038.9419, 21748.2105,  2123.1006,
          8595.7063]], dtype=torch.float64)
	q_value: tensor([[-24.8144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4830481084315241, distance: 0.8227762702065354 entropy 10.485158581386155
epoch: 33, step: 122
	action: tensor([[-9964.2367, -6793.5928,  9764.7133,  4985.4738, -6447.3786, -2131.3978,
          1939.8325]], dtype=torch.float64)
	q_value: tensor([[-34.1685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05994023905908308, distance: 1.178141307140123 entropy 10.350420660628838
epoch: 33, step: 123
	action: tensor([[-8833.2074, 10097.9137, -1165.9243, -8465.0820,  3292.6321, 17937.6860,
         -2307.9163]], dtype=torch.float64)
	q_value: tensor([[-21.4373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1250154812007156, distance: 1.0704265632002101 entropy 10.296823472435898
epoch: 33, step: 124
	action: tensor([[ -5123.4962,  -7970.1457,  -7548.1330,  -4329.3129, -11022.4278,
          -9650.7787,  10799.7327]], dtype=torch.float64)
	q_value: tensor([[-26.8804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4999344161867496, distance: 1.4014991166360886 entropy 10.239745940121486
epoch: 33, step: 125
	action: tensor([[ -5284.0018,  -9258.1868, -12172.7772,  -4430.3885,  -4817.3992,
          -4265.2699,  18125.9929]], dtype=torch.float64)
	q_value: tensor([[-31.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08836665001124389, distance: 1.1938349971728708 entropy 10.490840668381582
epoch: 33, step: 126
	action: tensor([[ 3920.4472, 11728.4874,  3725.8585, 11517.7388, -4228.2745, -4224.8071,
         -2976.2272]], dtype=torch.float64)
	q_value: tensor([[-30.8599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.467988247600568
epoch: 33, step: 127
	action: tensor([[-3510.6112, -5724.1874, -5164.2414,  -657.8040, -1870.3176,  1829.0152,
          4634.9334]], dtype=torch.float64)
	q_value: tensor([[-28.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10828975568234345, distance: 1.080608977553504 entropy 9.879674460782558
LOSS epoch 33 actor 277.9959554661443 critic 294.7013237911244
epoch: 34, step: 0
	action: tensor([[  3594.3190,  17827.0317, -12087.2429,  -1675.0420, -10114.8040,
           1530.3156,  12968.9548]], dtype=torch.float64)
	q_value: tensor([[-29.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.436570071503516
epoch: 34, step: 1
	action: tensor([[ 8182.8808, -6877.4754,  1421.4919, -5143.6148,  -563.0939,  4155.3487,
          -351.1034]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32157581122962664, distance: 0.9425564187176627 entropy 9.937238692383604
epoch: 34, step: 2
	action: tensor([[    72.5265, -18321.1385,    861.6569,  17186.8815,    551.8460,
           -731.0507,   2061.0178]], dtype=torch.float64)
	q_value: tensor([[-27.5322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38352227022602237, distance: 1.3460145992840802 entropy 10.31380718768811
epoch: 34, step: 3
	action: tensor([[-9871.1287, -8614.3126,   233.7809,  7040.9769,  6906.7481,  2329.5127,
         -4739.1026]], dtype=torch.float64)
	q_value: tensor([[-23.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5348249179566213, distance: 0.7804856359479048 entropy 10.12416640074912
epoch: 34, step: 4
	action: tensor([[-6803.5659,  4513.2649, -5424.6742,  9601.9676, -5139.5086, -4793.2349,
          3386.1079]], dtype=torch.float64)
	q_value: tensor([[-22.2483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07009038448561067, distance: 1.18376889480663 entropy 10.254622160347557
epoch: 34, step: 5
	action: tensor([[-7127.0079,  4673.4267, -2159.0640, -2660.5530,  9895.6330,  1600.0746,
          4259.3118]], dtype=torch.float64)
	q_value: tensor([[-32.5544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3414058351741025, distance: 0.9286790159780353 entropy 10.511452792628683
epoch: 34, step: 6
	action: tensor([[-1756.6544, -4504.9516,  8398.3279, -5121.3245,  2459.1170,   948.4924,
           625.8863]], dtype=torch.float64)
	q_value: tensor([[-27.4788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29882205087700675, distance: 1.3041619342640285 entropy 10.21354240337138
epoch: 34, step: 7
	action: tensor([[-17164.7444,  -5646.8939,  -8336.1051,    -25.6280,  -2847.2634,
           8959.7908,   7997.8824]], dtype=torch.float64)
	q_value: tensor([[-34.1284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6496106086108575, distance: 1.4697633503265362 entropy 10.63210537338597
epoch: 34, step: 8
	action: tensor([[-12889.1865, -14779.3260,   2303.4504, -12202.8565,  12772.1273,
           9698.6655,   7218.6597]], dtype=torch.float64)
	q_value: tensor([[-25.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0020639456453694915, distance: 1.1431627118554843 entropy 10.343974912579755
epoch: 34, step: 9
	action: tensor([[-18482.9510,  -5541.5527, -11944.9169,  -9779.0485,  13440.0644,
           1073.6854,  22195.7240]], dtype=torch.float64)
	q_value: tensor([[-33.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25740463658562596, distance: 1.2831996190024069 entropy 10.51017345739337
epoch: 34, step: 10
	action: tensor([[-10204.7548,   1039.7356,   1720.5895,   4080.2623,   -548.9776,
           3129.1103,  -6621.3180]], dtype=torch.float64)
	q_value: tensor([[-29.9784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04068419561612324, distance: 1.1673905485967881 entropy 10.451765333973254
epoch: 34, step: 11
	action: tensor([[  1427.8177, -16205.0774,  -3708.9942,  14803.0765,   6138.1396,
          -9579.3115,   5991.7995]], dtype=torch.float64)
	q_value: tensor([[-30.8581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1764918786168359, distance: 1.0384620653199814 entropy 10.40388170893726
epoch: 34, step: 12
	action: tensor([[  4635.3065, -11831.8478, -16615.3298,  -7126.8678,   -193.8194,
         -18163.1845,  -6406.4172]], dtype=torch.float64)
	q_value: tensor([[-24.9543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0890891221665635, distance: 1.1942311731555597 entropy 10.521476309315915
epoch: 34, step: 13
	action: tensor([[-6868.8024, -6985.5402,  1078.4295,  4429.7504, -5759.1126, 13386.3345,
          4558.7977]], dtype=torch.float64)
	q_value: tensor([[-28.1265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9790355855401944, distance: 1.6098428942699736 entropy 10.444795606663982
epoch: 34, step: 14
	action: tensor([[-14409.0891,  -6364.9449,   4662.6641,  10953.5921,  13546.5953,
          10877.6280, -13712.9442]], dtype=torch.float64)
	q_value: tensor([[-27.6521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13181239192476524, distance: 1.0662609019548641 entropy 10.611869650100816
epoch: 34, step: 15
	action: tensor([[ -6291.1519, -15133.5545,  10764.0429,   2495.7255,   1566.9986,
          -3194.3131,   5164.9529]], dtype=torch.float64)
	q_value: tensor([[-26.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09569699650645735, distance: 1.197848597889047 entropy 10.556607096374924
epoch: 34, step: 16
	action: tensor([[ 3539.8993, -2000.5780, -1412.8289,  3509.8684, -8949.4937, 12281.3355,
         15766.0881]], dtype=torch.float64)
	q_value: tensor([[-27.1164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5267465811846143, distance: 0.7872335117101192 entropy 10.453427094013003
epoch: 34, step: 17
	action: tensor([[ -6849.9325,  -5625.2117,  -9118.0778, -13604.1894,  -5725.9136,
          -4610.6084,   7707.7988]], dtype=torch.float64)
	q_value: tensor([[-26.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.543082509122786, distance: 1.4215144133400102 entropy 10.406691717041534
epoch: 34, step: 18
	action: tensor([[-2944.8452, -8106.5852,  8698.4171,   -23.5249,  3209.3248, -2005.4766,
          -349.7720]], dtype=torch.float64)
	q_value: tensor([[-25.0639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.406836764331638
epoch: 34, step: 19
	action: tensor([[-3236.0610,  1345.4991,  1016.8843,  1397.1557,  3459.0135, 10557.6777,
          5972.3687]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.033572380307901994, distance: 1.1633948606330178 entropy 9.937238692383604
epoch: 34, step: 20
	action: tensor([[-13198.1776,   5618.9491,   6151.9183,   2095.2241,   -360.1253,
           -191.1097,   4167.4851]], dtype=torch.float64)
	q_value: tensor([[-31.5795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6234981860998412, distance: 0.7021667005607912 entropy 10.355449835172474
epoch: 34, step: 21
	action: tensor([[-12846.9376,  -6628.1548,  -9449.7927,  -9081.1715,  10516.7669,
          -7507.1276, -22169.6251]], dtype=torch.float64)
	q_value: tensor([[-29.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15543354522063435, distance: 1.2300681664213022 entropy 10.535314538879266
epoch: 34, step: 22
	action: tensor([[-18432.0426,  -2425.7213,   -650.9647,  -6100.9191,   2163.6465,
            -55.5515,  -3130.8639]], dtype=torch.float64)
	q_value: tensor([[-24.8658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8343015176285775, distance: 1.5498585231342257 entropy 10.376934907371679
epoch: 34, step: 23
	action: tensor([[ -5785.9136, -13444.5966, -13082.7338, -11022.9012,  -1601.3861,
          -3022.4220,  -4640.1766]], dtype=torch.float64)
	q_value: tensor([[-29.7417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09815085930885048, distance: 1.1991891657019291 entropy 10.480636419178591
epoch: 34, step: 24
	action: tensor([[-2776.0909, -6869.2040,  6899.5491, -5323.9321,  3697.8361, -4700.9890,
         -1176.5955]], dtype=torch.float64)
	q_value: tensor([[-29.5675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.504418828240881, distance: 1.4035926111655594 entropy 10.45882682797618
epoch: 34, step: 25
	action: tensor([[  2740.2260, -18908.5500,  -2479.8666, -13078.0945,  -4405.8768,
         -10104.6560,   9531.6646]], dtype=torch.float64)
	q_value: tensor([[-30.6759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04574118265960758, distance: 1.117866094675963 entropy 10.544248099553291
epoch: 34, step: 26
	action: tensor([[ -2435.9846, -12544.1277,  -2728.4006, -11793.3632,  -4254.1835,
          -4046.3421, -11471.6419]], dtype=torch.float64)
	q_value: tensor([[-26.5186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.052230028163608555, distance: 1.1738484719663267 entropy 10.302212701756696
epoch: 34, step: 27
	action: tensor([[-13323.1982,   -662.8552,  12848.2994,  21104.2468,  -3618.7869,
           5880.2027,   7151.8116]], dtype=torch.float64)
	q_value: tensor([[-33.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7023642067985956, distance: 1.4930795011755655 entropy 10.568925338596147
epoch: 34, step: 28
	action: tensor([[  3593.4355,  -4168.8036, -11856.6908,  13267.3177,  -8999.3085,
         -10939.7315,    873.8675]], dtype=torch.float64)
	q_value: tensor([[-21.4614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5547181949286468, distance: 0.7636145027629225 entropy 10.242140472965971
epoch: 34, step: 29
	action: tensor([[ 8579.4750, -6846.8881, 15736.8231,   818.3772, -4129.0498,  1068.4289,
          3438.1189]], dtype=torch.float64)
	q_value: tensor([[-22.0141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.187933277669458, distance: 1.0312229036770626 entropy 10.276750380270055
epoch: 34, step: 30
	action: tensor([[-3716.8797, -6185.5087,   461.5629, 21050.2972,   -23.8394,  3554.2806,
         14804.1252]], dtype=torch.float64)
	q_value: tensor([[-25.0608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22674746957215142, distance: 1.2674600479825546 entropy 10.325158042355332
epoch: 34, step: 31
	action: tensor([[-9105.8828, -6000.6232, -6474.4744, 11467.4300,  5225.0387,  7398.5092,
         -3438.1833]], dtype=torch.float64)
	q_value: tensor([[-29.5261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28143272691314314, distance: 1.2954021065597612 entropy 10.467409298232509
epoch: 34, step: 32
	action: tensor([[ -1650.3019,   5480.6092, -15872.3413, -10618.1957,   2672.7480,
          12512.4350,   6019.8287]], dtype=torch.float64)
	q_value: tensor([[-28.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.625543155305389, distance: 0.7002571953142963 entropy 10.664381856220519
epoch: 34, step: 33
	action: tensor([[ 6151.3953,  5477.7222,   636.5017,   -26.9484,  6946.3529, -8966.8063,
          4560.8959]], dtype=torch.float64)
	q_value: tensor([[-33.3950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.302659664215431
epoch: 34, step: 34
	action: tensor([[ 3753.8819, -6182.9421,  3550.6714, -3393.0498, 10756.7911, -5663.7756,
          2249.9933]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4419991246925363, distance: 0.854818934525776 entropy 9.937238692383604
epoch: 34, step: 35
	action: tensor([[  1641.6179, -10594.7697, -11346.3356,   -547.0275,   3389.2999,
           3327.2205,    -59.5833]], dtype=torch.float64)
	q_value: tensor([[-23.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14740860822895974, distance: 1.0566402864926467 entropy 10.168088654207505
epoch: 34, step: 36
	action: tensor([[ -9118.2463, -12133.2961, -11195.4684,  -1117.2308,  -3022.4888,
          -6895.7637,   -806.4064]], dtype=torch.float64)
	q_value: tensor([[-31.2700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19402909558942827, distance: 1.0273451506646747 entropy 10.442523682425147
epoch: 34, step: 37
	action: tensor([[ -2847.0911,   2456.9152, -11577.9587,  12134.4152,   8438.7316,
          -8290.1135,   1561.8555]], dtype=torch.float64)
	q_value: tensor([[-26.9829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3435950533637362, distance: 0.9271342305498432 entropy 10.349757257452865
epoch: 34, step: 38
	action: tensor([[-12027.6685,  -3158.2197,   2175.8268,  10746.7775,   2653.7725,
           -317.4865,   2769.2120]], dtype=torch.float64)
	q_value: tensor([[-30.5049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17172979503908925, distance: 1.2387122428430613 entropy 10.380267463609831
epoch: 34, step: 39
	action: tensor([[-4457.3054, -8114.2244, 10681.9183, 10486.8777, -8984.3947,  1430.4479,
         18315.9565]], dtype=torch.float64)
	q_value: tensor([[-27.3160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1668896076580494, distance: 1.0444988574388772 entropy 10.508847721743871
epoch: 34, step: 40
	action: tensor([[  8349.1027,  -6471.9461, -10550.0532,   8812.8047,   2958.9509,
          -8042.6131,   1795.0725]], dtype=torch.float64)
	q_value: tensor([[-23.5465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44191513568379814, distance: 0.8548832647954244 entropy 10.300465955133811
epoch: 34, step: 41
	action: tensor([[ -709.1589,  -364.7667, 10630.3585,  2879.0891,  9482.8059, -6470.5194,
         -1304.9299]], dtype=torch.float64)
	q_value: tensor([[-22.8275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.527275150486213, distance: 1.4142146638440718 entropy 10.272751640128364
epoch: 34, step: 42
	action: tensor([[ -1397.3899,   2372.5547,   -235.6841,   9513.6199,   6231.3617,
         -16778.1125,  -1828.3157]], dtype=torch.float64)
	q_value: tensor([[-28.3415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04380423022119395, distance: 1.1190000406116043 entropy 10.50336031141414
epoch: 34, step: 43
	action: tensor([[-23816.6912,  -7555.8315,  -3477.6379,  16388.1716,   3737.9296,
           1111.2676,  -6315.3422]], dtype=torch.float64)
	q_value: tensor([[-31.8951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14509791220961488, distance: 1.0580711726560181 entropy 10.49289242551433
epoch: 34, step: 44
	action: tensor([[-17288.6135, -23054.0731,  -6840.6694,  19505.3343,  -7398.3173,
            671.2162,  -6724.2136]], dtype=torch.float64)
	q_value: tensor([[-26.7417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4852239594535819, distance: 1.394609651891036 entropy 10.572314348082745
epoch: 34, step: 45
	action: tensor([[-16325.8978,   2337.3611,   -942.2226,  -7034.9952,   8518.0738,
           5662.3416,  11724.1616]], dtype=torch.float64)
	q_value: tensor([[-28.1926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11732691610326507, distance: 1.0751192426748308 entropy 10.621064286813482
epoch: 34, step: 46
	action: tensor([[ -6767.9226, -10151.0354,  -9728.4109,   3726.0652,  -4021.7244,
          -4113.9955,  -1416.6055]], dtype=torch.float64)
	q_value: tensor([[-29.7286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9281627123668781, distance: 1.5890169626150252 entropy 10.202340916946381
epoch: 34, step: 47
	action: tensor([[ -7267.8818, -18672.5010,   4287.3199,   6067.3325,   4480.8327,
           -942.2273,  -5708.3279]], dtype=torch.float64)
	q_value: tensor([[-22.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7712564843200962, distance: 1.5229912908250895 entropy 10.41896479985221
epoch: 34, step: 48
	action: tensor([[-6700.0165, -3686.7449, -9694.8107,   185.2997,  7818.2557,  1172.8878,
          9167.1027]], dtype=torch.float64)
	q_value: tensor([[-28.6591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4324780616087287, distance: 1.3696218726575968 entropy 10.526469003469598
epoch: 34, step: 49
	action: tensor([[  -481.8160,  -3192.1476,  -5275.4837,   2018.4525,   4219.5384,
         -10401.7303,   -166.5377]], dtype=torch.float64)
	q_value: tensor([[-25.3808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24141474922665085, distance: 0.9966872221115808 entropy 10.431758010014873
epoch: 34, step: 50
	action: tensor([[-14218.0106, -13757.8141,  -5973.2353,  -4997.8226,  14408.2041,
          12159.2841,  -2543.1294]], dtype=torch.float64)
	q_value: tensor([[-27.8634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8772300680362226, distance: 1.5678894728180426 entropy 10.672693870971942
epoch: 34, step: 51
	action: tensor([[  2152.6911,   -735.4988,  -4094.4193,   2511.0538, -13905.2972,
          17537.1167,  -1086.2074]], dtype=torch.float64)
	q_value: tensor([[-24.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5606426322657572, distance: 0.758517578770059 entropy 10.353081336044102
epoch: 34, step: 52
	action: tensor([[  4229.4514, -22321.6947,  -3146.4351,   5418.6396,   5622.7001,
           4287.0225,   -327.1265]], dtype=torch.float64)
	q_value: tensor([[-28.4958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5615555641950503, distance: 0.7577291146840494 entropy 10.332381346674527
epoch: 34, step: 53
	action: tensor([[  307.7671, -9140.8935, -2136.3439,  4458.2677, -6737.5688, -8804.5110,
         -5176.0652]], dtype=torch.float64)
	q_value: tensor([[-29.3085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30480562074679163, distance: 0.9541349828815899 entropy 10.268688846001892
epoch: 34, step: 54
	action: tensor([[ -7630.0493,   -147.4452, -10622.4304,   5511.0887,  12964.5672,
          -5593.4283,  -7638.0614]], dtype=torch.float64)
	q_value: tensor([[-26.9972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03083198794516906, distance: 1.1265649340011414 entropy 10.402692237229035
epoch: 34, step: 55
	action: tensor([[-24563.0762,  -4562.5511, -12061.3035,  -3657.1906, -12363.7305,
         -11201.5264,  -7718.5914]], dtype=torch.float64)
	q_value: tensor([[-30.3088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8384008862729817, distance: 1.5515893987845042 entropy 10.52249921658661
epoch: 34, step: 56
	action: tensor([[  3545.1208, -18583.6699,  -9907.2768,   3522.3614,   7807.5409,
          11523.6467, -11583.3876]], dtype=torch.float64)
	q_value: tensor([[-26.6061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5459529871611268, distance: 0.7710936108962364 entropy 10.468773361545384
epoch: 34, step: 57
	action: tensor([[-18270.0160,   5422.3044,   1985.4479,   -703.9992,  -3301.9787,
          10994.8120,   5676.1958]], dtype=torch.float64)
	q_value: tensor([[-29.3215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.55516282308304
epoch: 34, step: 58
	action: tensor([[  4672.2250, -10914.7643,   9056.9000,   2457.7836,  -1986.5378,
          10361.3826,   6804.7932]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027610678145144396, distance: 1.1600347456199809 entropy 9.937238692383604
epoch: 34, step: 59
	action: tensor([[ -5047.1975,   1862.2956,  13384.7629,  14970.1352, -13448.7714,
          14513.2839,  -6761.2203]], dtype=torch.float64)
	q_value: tensor([[-24.9168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.415765378055891
epoch: 34, step: 60
	action: tensor([[-7048.2722,  3177.5812,   332.0739,  1093.4068, -1721.9629, -9734.0682,
           670.1305]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.937238692383604
epoch: 34, step: 61
	action: tensor([[-3673.9846, -3562.2251,  5188.0316,   880.6412,    17.8409, -2695.7694,
          3229.3094]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.937238692383604
epoch: 34, step: 62
	action: tensor([[-2986.9545,   186.5837,  3712.3398,  -225.7703, -2006.5559,  2678.0270,
          5898.1499]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.937238692383604
epoch: 34, step: 63
	action: tensor([[ -474.5374, -4835.7133,    30.2118,  4891.4883,  2838.8833,  3864.3641,
         10124.6746]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44144070502293786, distance: 1.3738998757805996 entropy 9.937238692383604
epoch: 34, step: 64
	action: tensor([[-1582.3542, -7196.9389,  -468.9309,  5857.2005,  1780.9527, -2569.5578,
         -7133.8602]], dtype=torch.float64)
	q_value: tensor([[-22.0966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.155599145560414
epoch: 34, step: 65
	action: tensor([[-2421.9801, -7611.7683, -1998.4537, -3122.4725, -5533.8064,  5574.4311,
         -4516.3005]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18328686359084978, distance: 1.0341688772908797 entropy 9.937238692383604
epoch: 34, step: 66
	action: tensor([[-1501.3065, -4622.1207, -7173.4415,  6198.8783, -7013.5485, -2585.8558,
         -5598.1652]], dtype=torch.float64)
	q_value: tensor([[-32.7615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05644952682830873, distance: 1.1761997147353864 entropy 10.446426208606118
epoch: 34, step: 67
	action: tensor([[-15348.4803,  -9686.4936,  -1421.3252,  16498.5542,  -8152.2716,
          -2301.2938,   2937.7683]], dtype=torch.float64)
	q_value: tensor([[-27.2215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11029051828393721, distance: 1.0793959960000723 entropy 10.445317283871756
epoch: 34, step: 68
	action: tensor([[-11968.8227,  -4269.5013,    217.2012, -11642.7670,   8701.5367,
          13688.8566,   6895.9815]], dtype=torch.float64)
	q_value: tensor([[-30.7315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8529861997040578, distance: 1.5577321583236337 entropy 10.73806920532474
epoch: 34, step: 69
	action: tensor([[-7323.3532, -5479.9956,  6507.5744, 17447.9639,  4171.8882, -3224.5510,
         17041.2367]], dtype=torch.float64)
	q_value: tensor([[-33.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0165660872922153, distance: 1.62503576220286 entropy 10.596126096668886
epoch: 34, step: 70
	action: tensor([[-1587.7845, -6131.5157, -9870.1587,  8354.4777,  1803.5142, -3371.5593,
          3883.7436]], dtype=torch.float64)
	q_value: tensor([[-23.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6574263992152358, distance: 1.4732410765227315 entropy 10.366394315372649
epoch: 34, step: 71
	action: tensor([[ -1345.6655, -12468.5635,  -4387.0919,  -4318.4986,   2987.3096,
          17696.4478,  -6906.9518]], dtype=torch.float64)
	q_value: tensor([[-21.6613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5786124862895681, distance: 1.4377866938596737 entropy 10.305049852334134
epoch: 34, step: 72
	action: tensor([[-2769.9113, -8855.1683, -1970.5137,  2302.6877,  1731.3542, -1695.1316,
         -3596.1900]], dtype=torch.float64)
	q_value: tensor([[-26.7144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48274062462769096, distance: 0.8230209281455112 entropy 10.421642778918839
epoch: 34, step: 73
	action: tensor([[-16246.4475,  11360.6745,  13233.9356,   -873.4995,  10891.5668,
          -2644.6695,  -4836.0562]], dtype=torch.float64)
	q_value: tensor([[-28.3011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13869440521262366, distance: 1.062026436130594 entropy 10.674533908345584
epoch: 34, step: 74
	action: tensor([[ 1.3745e+02, -2.5857e+03,  4.5899e+03,  1.3281e+03, -1.9704e+04,
          1.2296e+01,  2.4722e+03]], dtype=torch.float64)
	q_value: tensor([[-32.2801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36627117388115227, distance: 0.9109791230267588 entropy 10.458239842205192
epoch: 34, step: 75
	action: tensor([[ -5325.4100, -21440.8705,  -4080.6050,   2179.7169,  -1339.6305,
           1310.4170,  -3735.2381]], dtype=torch.float64)
	q_value: tensor([[-28.1395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07022618047988982, distance: 1.1838440034034532 entropy 10.463449309712473
epoch: 34, step: 76
	action: tensor([[-21851.0524, -12894.3310,  -7271.3656,  15903.0893,  -8547.3271,
          -6276.5398,   6999.9330]], dtype=torch.float64)
	q_value: tensor([[-28.6816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1143847819460917, distance: 1.2080204314239438 entropy 10.723901516903124
epoch: 34, step: 77
	action: tensor([[-9.1540e+03, -3.0175e+03,  1.4844e+04,  1.0956e+01,  3.3889e+03,
          4.6553e+03, -4.5499e+03]], dtype=torch.float64)
	q_value: tensor([[-25.2553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.571195443731364
epoch: 34, step: 78
	action: tensor([[ -263.3319, -5225.7336, -5548.3469, -1725.3936, -5867.3436,  2827.1031,
         -8876.0329]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.937238692383604
epoch: 34, step: 79
	action: tensor([[-9241.0049,  4360.2218, -1813.3045,  3032.8005,  2781.3698,  9065.2860,
          5684.2874]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.937238692383604
epoch: 34, step: 80
	action: tensor([[ -5301.4106,  -4243.5356, -11180.3661,   6111.1816,    178.4236,
           3478.8942,    911.9066]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35109215587304465, distance: 1.3301455919646223 entropy 9.937238692383604
epoch: 34, step: 81
	action: tensor([[ -1184.0487,  -2228.3062,   4598.8143,   5176.0367,   7071.3481,
         -13724.3793,   9126.4474]], dtype=torch.float64)
	q_value: tensor([[-28.2548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15804253385658362, distance: 1.0500301503498815 entropy 10.515773681875661
epoch: 34, step: 82
	action: tensor([[-15534.4078, -11476.0237, -14682.1598,  13086.2315,  -5853.4369,
          12915.5102,    661.8514]], dtype=torch.float64)
	q_value: tensor([[-30.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21718802153604466, distance: 1.262512039065245 entropy 10.611135914037764
epoch: 34, step: 83
	action: tensor([[-19992.1859,   7944.6653,  13365.5079,   8898.1735,  13427.5255,
           8566.7198, -12483.7573]], dtype=torch.float64)
	q_value: tensor([[-28.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15122366198071457, distance: 1.0542735847308176 entropy 10.502028547866038
epoch: 34, step: 84
	action: tensor([[ -326.2773,  7598.4055,  2837.2244,  1352.0773, -2443.1025,  7372.1912,
         -1205.9062]], dtype=torch.float64)
	q_value: tensor([[-26.6303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07022984363348694, distance: 1.1838460294231805 entropy 10.274537197960427
epoch: 34, step: 85
	action: tensor([[  3551.6374, -12135.1971,  -1653.1120,   1349.4743,    215.7856,
          -1679.2046,  14189.4961]], dtype=torch.float64)
	q_value: tensor([[-29.4312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03769394899864953, distance: 1.1225696634958737 entropy 10.423493892526452
epoch: 34, step: 86
	action: tensor([[-17964.3244,  -9541.0719,   1539.7412,   -846.0746, -13997.9041,
         -10886.5639,  11379.2502]], dtype=torch.float64)
	q_value: tensor([[-29.0687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5390602132332591, distance: 1.4196604999961773 entropy 10.475314391771462
epoch: 34, step: 87
	action: tensor([[ 9717.4028,  1573.8846,  1332.2070,  1324.8930,  1344.5184, 10537.4812,
          3932.8054]], dtype=torch.float64)
	q_value: tensor([[-21.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.180348600439014
epoch: 34, step: 88
	action: tensor([[   671.6933, -14239.3679,  -3610.3080,  11487.4970,   -547.6610,
           5961.5139,   8313.2851]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.937238692383604
epoch: 34, step: 89
	action: tensor([[-6613.9131, -6695.7827, -2506.9671,  -654.9450,  5218.7519, -8924.3595,
         -8590.4468]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24509093370546653, distance: 1.2769010046080942 entropy 9.937238692383604
epoch: 34, step: 90
	action: tensor([[ 5636.0285, -2912.3503,  5886.6038,   131.6074,  -686.7622, -8708.3471,
          5184.0302]], dtype=torch.float64)
	q_value: tensor([[-28.9847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18644153407148245, distance: 1.032169632749834 entropy 10.445382953062522
epoch: 34, step: 91
	action: tensor([[ 13479.5841, -17809.1185,  -1542.9114, -10301.2097,    316.3091,
            961.7124,   5241.0572]], dtype=torch.float64)
	q_value: tensor([[-21.0230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6418036755252314, distance: 0.6848843711327898 entropy 10.156315070907045
epoch: 34, step: 92
	action: tensor([[   664.2877, -15974.0373,   1743.8595,  -9096.4139,   7910.0233,
            670.9997, -22251.5093]], dtype=torch.float64)
	q_value: tensor([[-27.9507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15028806653364046, distance: 1.0548544809307923 entropy 10.418241895551459
epoch: 34, step: 93
	action: tensor([[ 1534.4614,  -736.3133,  6503.2211, -4871.9707,  1183.4153, 19490.4999,
         11102.0789]], dtype=torch.float64)
	q_value: tensor([[-31.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4132175579383768, distance: 0.8765874495303106 entropy 10.560728800554333
epoch: 34, step: 94
	action: tensor([[ -5213.1814,   2242.2651, -10911.8875,  -3941.1781, -10000.8851,
           5172.6092,  10241.6474]], dtype=torch.float64)
	q_value: tensor([[-27.6098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43369885887712134, distance: 0.8611531839459161 entropy 10.367274246222864
epoch: 34, step: 95
	action: tensor([[10027.3869, -4175.7239,  5393.4485, -9560.5043, -7643.2075,   719.3722,
         10302.4190]], dtype=torch.float64)
	q_value: tensor([[-32.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4768079283406672, distance: 0.8277272829763509 entropy 10.2319035903129
epoch: 34, step: 96
	action: tensor([[ 1978.9612, -4772.7716,  1529.5507,   100.3386, 14628.8976, -8500.3965,
         11130.8384]], dtype=torch.float64)
	q_value: tensor([[-27.5056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036328158284133, distance: 0.9549394648241276 entropy 10.406904613989159
epoch: 34, step: 97
	action: tensor([[-22679.1749,  -7259.1191,   -815.5878,  13388.4678,  -5483.2434,
           3852.5288,  18535.6429]], dtype=torch.float64)
	q_value: tensor([[-29.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0549632151175401, distance: 1.1753720297957801 entropy 10.393576093424297
epoch: 34, step: 98
	action: tensor([[ -4213.8000, -25380.8909,  -2706.3747,    961.6994,  -1701.6507,
          -7676.3673,  -6963.0703]], dtype=torch.float64)
	q_value: tensor([[-21.0299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4639202412804875, distance: 1.3845715423101705 entropy 10.160416812237397
epoch: 34, step: 99
	action: tensor([[-10785.9270,  -3026.6112,   7380.8001,   9713.7809,   7610.2671,
          21032.9938,  -7001.1237]], dtype=torch.float64)
	q_value: tensor([[-24.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2829885752073571, distance: 1.2961882725923117 entropy 10.381534141498614
epoch: 34, step: 100
	action: tensor([[ -2615.6860,  -4686.1284,  -1178.5269, -10435.6566,  18618.5312,
           7352.9574, -24976.9633]], dtype=torch.float64)
	q_value: tensor([[-27.5209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7834694389067074, distance: 1.5282328442624435 entropy 10.605696877205888
epoch: 34, step: 101
	action: tensor([[-11838.4658,  -8321.5710,   6531.8092,  15012.9335,  -8456.5973,
          -4793.7379,   7632.4493]], dtype=torch.float64)
	q_value: tensor([[-25.7365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24775950554423853, distance: 1.2782686469707463 entropy 10.381675014042434
epoch: 34, step: 102
	action: tensor([[ -364.5028,  6150.7348,  7000.4843, -1632.5582,   319.3181, 10591.7755,
          4156.6933]], dtype=torch.float64)
	q_value: tensor([[-24.2798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6216104737850763, distance: 0.7039247683713602 entropy 10.560762225415587
epoch: 34, step: 103
	action: tensor([[-1456.9998,    45.0160, -4691.4987,  7916.0749,   861.2076, 19012.9007,
          8032.7367]], dtype=torch.float64)
	q_value: tensor([[-29.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020831033079369177, distance: 1.1323625915125808 entropy 10.398802052576885
epoch: 34, step: 104
	action: tensor([[ 2406.6911, -6416.2365,  7539.0343,  3545.4348, -5671.2583, -8904.2322,
         -2006.3004]], dtype=torch.float64)
	q_value: tensor([[-29.1723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45541573803346147, distance: 0.8444797432257525 entropy 10.447612027716923
epoch: 34, step: 105
	action: tensor([[ -4047.5752,     62.4940,   6608.4369, -10298.9590,   3920.1353,
          -1491.5941,   6026.4969]], dtype=torch.float64)
	q_value: tensor([[-27.8637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29087368302499783, distance: 1.300165287236652 entropy 10.345586726612222
epoch: 34, step: 106
	action: tensor([[-5026.0569,  -396.9616, -7084.8483,  -989.5922,   592.1845, -5594.3013,
          3202.2665]], dtype=torch.float64)
	q_value: tensor([[-25.6499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.044900597006447995, distance: 1.1697530383822956 entropy 10.0716694279236
epoch: 34, step: 107
	action: tensor([[-4131.7530, -8958.9655,  9791.6581, 11083.4983, -6445.1233,  4663.6509,
           169.6295]], dtype=torch.float64)
	q_value: tensor([[-33.2397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4358559265298494, distance: 1.3712357450394064 entropy 10.515844445932052
epoch: 34, step: 108
	action: tensor([[-18751.0796, -27938.5515,  -1059.7523,  -3688.4438,  -3957.9335,
            422.8429,   3070.0772]], dtype=torch.float64)
	q_value: tensor([[-24.8103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8562787681912323, distance: 1.5591155100823637 entropy 10.328711783297324
epoch: 34, step: 109
	action: tensor([[ 10625.1710,   2417.7980,  -9596.4287,   7909.5547, -20389.4454,
           1293.3881,  -2763.3488]], dtype=torch.float64)
	q_value: tensor([[-27.1028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.368869045193623
epoch: 34, step: 110
	action: tensor([[-6455.7343, -9946.1790, -7826.2462,  3586.3146,  -878.6765,  2484.4395,
          3958.8681]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4865757227125027, distance: 1.3952441532824476 entropy 9.937238692383604
epoch: 34, step: 111
	action: tensor([[  9848.7382,   6157.2937,  -6299.4379,  -1733.9919, -11248.1308,
           8842.4909,   8731.3714]], dtype=torch.float64)
	q_value: tensor([[-27.0758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.470900515182615
epoch: 34, step: 112
	action: tensor([[ 5449.8795, -8230.6748,  4784.2186,  8840.2098, -8198.1344,  -813.4394,
          8352.5951]], dtype=torch.float64)
	q_value: tensor([[-30.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026814304980009918, distance: 1.1288976046882537 entropy 9.937238692383604
epoch: 34, step: 113
	action: tensor([[-13949.7639,  -3695.1880,   -421.5690,  -2216.4917,  -4710.6721,
           2745.6375,  -4401.5297]], dtype=torch.float64)
	q_value: tensor([[-28.8762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5961065786260473, distance: 1.445731478116948 entropy 10.554143068760306
epoch: 34, step: 114
	action: tensor([[-7170.9528, -2002.1486, 11701.0039, 13892.5598,  4370.5715,  7900.7759,
          1020.1288]], dtype=torch.float64)
	q_value: tensor([[-26.8080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7604981677571085, distance: 1.5183590486596925 entropy 10.467252475310877
epoch: 34, step: 115
	action: tensor([[-9197.8608,  4669.1556, -9926.0492, 10020.0879, -3947.7813, 12687.3811,
          1935.8983]], dtype=torch.float64)
	q_value: tensor([[-27.4109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6767341622841367, distance: 0.6506337672265536 entropy 10.573374257052501
epoch: 34, step: 116
	action: tensor([[-3886.5537,   931.3141,  6427.7123,  5246.6694,  2657.1612, -3767.8847,
          5864.3882]], dtype=torch.float64)
	q_value: tensor([[-26.1049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7020754051852229, distance: 0.6246113504195734 entropy 10.380635615093047
epoch: 34, step: 117
	action: tensor([[-14753.4050,  -3295.3038,  -1195.5474,  16758.9961,   9891.8838,
         -21382.2599,  -2493.3036]], dtype=torch.float64)
	q_value: tensor([[-28.3960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08251117813138253, distance: 1.0961174309679897 entropy 10.393542151522917
epoch: 34, step: 118
	action: tensor([[-12633.0911,   1044.3495,   1941.5128,   7695.1857,  15776.6444,
           9244.1366,   -875.5006]], dtype=torch.float64)
	q_value: tensor([[-24.9578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04808483829985333, distance: 1.116492513567644 entropy 10.474219280728253
epoch: 34, step: 119
	action: tensor([[ -3992.0536,  -1709.9324,   2159.5041,   7345.8702,  -2276.0309,
         -13124.2140,  16737.6942]], dtype=torch.float64)
	q_value: tensor([[-25.2608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5941695404689271, distance: 1.4448539404745548 entropy 10.23646626320341
epoch: 34, step: 120
	action: tensor([[-12562.5219,   2512.7609,  -3667.2081,   2978.0225, -18751.1399,
           3580.5999,   5276.2808]], dtype=torch.float64)
	q_value: tensor([[-23.1833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14664776131270507, distance: 1.2253825937897675 entropy 10.464645184936623
epoch: 34, step: 121
	action: tensor([[ -7562.6182, -11588.5646,  -7470.8728, -15961.7257, -22706.8947,
           5952.7229,  10124.9452]], dtype=torch.float64)
	q_value: tensor([[-33.2935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9981699826765582, distance: 1.6176065937483166 entropy 10.63120160295217
epoch: 34, step: 122
	action: tensor([[ -8228.5758, -11113.7354,   -652.2626,  18592.9228,  -2949.1071,
           7856.0381, -11133.1536]], dtype=torch.float64)
	q_value: tensor([[-25.8277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8212619107280055, distance: 1.5443399128826416 entropy 10.40432436441733
epoch: 34, step: 123
	action: tensor([[  -877.5978,   1637.6111, -11958.0934,   5243.4943,   6153.4112,
           6499.0920,  22615.4221]], dtype=torch.float64)
	q_value: tensor([[-25.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4790646979007377, distance: 0.8259401682914657 entropy 10.451790879323024
epoch: 34, step: 124
	action: tensor([[-14886.0765,   9309.4938,  -3306.8716,   8306.9208,   4262.0427,
         -11884.7463,   4688.1635]], dtype=torch.float64)
	q_value: tensor([[-31.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0967269564263249, distance: 1.1984114570812823 entropy 10.497438108402338
epoch: 34, step: 125
	action: tensor([[ -8931.3447,  -3199.1164,  -2222.2880,   1435.7410, -10927.1583,
          -5498.9667,  -1727.8228]], dtype=torch.float64)
	q_value: tensor([[-30.1869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.338773323154669, distance: 1.324067782078797 entropy 10.425259210285821
epoch: 34, step: 126
	action: tensor([[ -7243.0767,  -5109.5257,  -6546.7592,  -7538.4746, -12352.7254,
          -1757.5274,   8847.9088]], dtype=torch.float64)
	q_value: tensor([[-24.6768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4551850674172908, distance: 1.3804345105624172 entropy 10.407403396943858
epoch: 34, step: 127
	action: tensor([[ -9138.1568,   4903.3820, -19828.0216,    928.9524,  -2536.2298,
          -1760.6266,  -1056.9082]], dtype=torch.float64)
	q_value: tensor([[-22.2081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5693451965052809, distance: 0.7509678438902118 entropy 10.213401841050436
LOSS epoch 34 actor 321.6097600465588 critic 203.8611842276237
epoch: 35, step: 0
	action: tensor([[   783.4838,  -5127.4736,   6475.2723,   3690.6825,  -3083.5396,
          -1700.9606, -15450.1285]], dtype=torch.float64)
	q_value: tensor([[-31.7104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.406091177638492
epoch: 35, step: 1
	action: tensor([[ -364.7642, -4938.4888, -3157.9088, -1020.1943,   527.6970,  2329.8205,
         -4136.0083]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6690547119138324, distance: 1.4784000884256594 entropy 9.993769838042994
epoch: 35, step: 2
	action: tensor([[  -578.2782, -10703.9845,   3323.5403,  -5987.4783,   3229.2755,
           1529.0334,  12946.6258]], dtype=torch.float64)
	q_value: tensor([[-35.5255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22999003776917437, distance: 1.2691340328614544 entropy 10.594489965773773
epoch: 35, step: 3
	action: tensor([[ -5159.4377, -12415.0235,  17341.1443,   7323.4350,  15006.3413,
          11894.6129,  11739.3135]], dtype=torch.float64)
	q_value: tensor([[-33.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6560974873970737, distance: 1.4726503413701195 entropy 10.584153354850631
epoch: 35, step: 4
	action: tensor([[10884.7033, -9988.1975, -4926.0058, 14201.6443,  4933.0614,   135.5649,
         10107.9610]], dtype=torch.float64)
	q_value: tensor([[-27.6889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3878565403812938, distance: 0.8953303368246901 entropy 10.457114565766137
epoch: 35, step: 5
	action: tensor([[ -1564.9920,  -8423.6544, -11613.5970,   5891.7247, -11579.4947,
           9063.5421,  -7102.1955]], dtype=torch.float64)
	q_value: tensor([[-28.0950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16762056062660124, distance: 1.0440405465798892 entropy 10.348211438078767
epoch: 35, step: 6
	action: tensor([[13299.4411,  4836.1725, -1684.1810,  8306.0547, -1157.0931, -8664.9091,
         -2710.8162]], dtype=torch.float64)
	q_value: tensor([[-27.0188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19619429382684095, distance: 1.0259642683992307 entropy 10.52853326249049
epoch: 35, step: 7
	action: tensor([[-16546.9943, -13429.5358,  18559.7861,   6977.0550,   -831.6838,
           -450.9763, -10591.4213]], dtype=torch.float64)
	q_value: tensor([[-33.1981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19996161577947924, distance: 1.0235571710361282 entropy 10.482319261947367
epoch: 35, step: 8
	action: tensor([[ -8463.9571,  -7454.6637,    182.1062,   4867.3883, -11125.1551,
           4743.4327,   5574.2655]], dtype=torch.float64)
	q_value: tensor([[-26.7316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6210237566754313, distance: 1.456972595417168 entropy 10.590390150149167
epoch: 35, step: 9
	action: tensor([[ 1983.2974,  -887.0197, -9646.7394, -3052.2397, 10415.3451, -1951.6246,
         -2836.2027]], dtype=torch.float64)
	q_value: tensor([[-29.1846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2756109202349425, distance: 0.9739634078779833 entropy 10.488126984648238
epoch: 35, step: 10
	action: tensor([[-4591.3440, -7991.7155,  1364.2121,  6204.8988,  2952.2050,  9274.5591,
          3913.1059]], dtype=torch.float64)
	q_value: tensor([[-29.2504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1849388214575689, distance: 1.245674738711839 entropy 10.586194089198315
epoch: 35, step: 11
	action: tensor([[-2373.8294,  3897.2083,  1722.5754, -3382.0156, -8752.2378,  9363.2391,
         10132.6451]], dtype=torch.float64)
	q_value: tensor([[-24.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4257292904629034, distance: 0.8671915275811338 entropy 10.38312336460888
epoch: 35, step: 12
	action: tensor([[-4649.3054, -3291.1448,   459.2733,  9994.0792, -3898.2991,  1255.9551,
          5172.4615]], dtype=torch.float64)
	q_value: tensor([[-31.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6559291067632909, distance: 1.4725754749740005 entropy 10.2612288300524
epoch: 35, step: 13
	action: tensor([[  5429.3465, -15829.4692, -15729.7873,  11695.1910,   2963.8458,
            483.3285,    775.1298]], dtype=torch.float64)
	q_value: tensor([[-29.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3686141822337795, distance: 0.9092935375639886 entropy 10.589872367740087
epoch: 35, step: 14
	action: tensor([[ 15482.9046,   2443.9010,   9788.8152,  14494.2064, -19119.0003,
         -11135.4343,   -692.5090]], dtype=torch.float64)
	q_value: tensor([[-34.1430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29916681173681414, distance: 0.9579967223330589 entropy 10.539812726733347
epoch: 35, step: 15
	action: tensor([[-7794.5278, -3217.0123,  2132.8444,  9339.9305,   482.6360, -5706.1913,
          1506.0627]], dtype=torch.float64)
	q_value: tensor([[-31.5338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39720854310339115, distance: 1.352655832488169 entropy 10.317648235383505
epoch: 35, step: 16
	action: tensor([[-14453.6596,  -5747.8538,  -5992.3248,   4275.1563,  14988.1772,
           1249.3583,   7326.7527]], dtype=torch.float64)
	q_value: tensor([[-29.1167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25327458453548535, distance: 1.2810904967526915 entropy 10.550962665519764
epoch: 35, step: 17
	action: tensor([[  3229.8148,   9548.1997, -10511.6984,   9445.0815,  -4089.2448,
          -2963.0230,  -5883.7541]], dtype=torch.float64)
	q_value: tensor([[-31.1541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2656932356095326, distance: 0.9806080581262444 entropy 10.559692415602713
epoch: 35, step: 18
	action: tensor([[-22297.0059,  -9152.2695,  -4140.0918,  -8889.8248,  -4126.0498,
           6567.2100,   1316.1648]], dtype=torch.float64)
	q_value: tensor([[-27.7344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.184377984258449
epoch: 35, step: 19
	action: tensor([[-1479.8241,  4597.2713,   921.0529, -8940.3507, -3781.4704,  7818.4519,
          2889.7406]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2956223820710121, distance: 0.9604161779693419 entropy 9.993769838042994
epoch: 35, step: 20
	action: tensor([[ -6272.3955,  -6370.4621, -15766.4492,   1696.8972,   8213.4788,
           3806.6113,   6175.5857]], dtype=torch.float64)
	q_value: tensor([[-37.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2763977550200516, distance: 1.2928546717616058 entropy 10.326110481874418
epoch: 35, step: 21
	action: tensor([[-11884.9751, -20099.7369,  12036.9664,  10904.5598,   6628.8170,
          17825.5030,    315.0708]], dtype=torch.float64)
	q_value: tensor([[-28.3782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993087560966622, distance: 0.9578997027285316 entropy 10.504889165802767
epoch: 35, step: 22
	action: tensor([[  -585.6256,   6735.2947, -18100.8672,  -6424.3559,  14826.4766,
          10062.0286,   -175.4331]], dtype=torch.float64)
	q_value: tensor([[-27.7091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09702985649259654, distance: 1.087410165496318 entropy 10.483806666599987
epoch: 35, step: 23
	action: tensor([[-18440.7287,  -4458.6641,   1559.2797,  -3556.9996,   1344.2697,
          -6652.3924,  -3343.7071]], dtype=torch.float64)
	q_value: tensor([[-32.1569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.215760026064341
epoch: 35, step: 24
	action: tensor([[ 1854.5189, -2337.8045, -3275.1312,  2756.6646, -1730.1692,  6219.4389,
          6601.3909]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15760663246597906, distance: 1.2312243513871632 entropy 9.993769838042994
epoch: 35, step: 25
	action: tensor([[-11166.4476, -13242.3640,  15060.2832,   -561.6121,   2927.4089,
         -13252.6174,   5204.2420]], dtype=torch.float64)
	q_value: tensor([[-26.9639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09018845161201172, distance: 1.194833751188092 entropy 10.55581570599222
epoch: 35, step: 26
	action: tensor([[-12585.9273,   1196.0005,  13187.4429,    364.7572,   1806.6310,
          -3167.8599,  11780.5229]], dtype=torch.float64)
	q_value: tensor([[-27.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31417431683946695, distance: 0.9476840377022339 entropy 10.407467041624455
epoch: 35, step: 27
	action: tensor([[ -6421.5960, -17163.4948,   -182.1192,   3799.5367,   4941.8313,
         -19017.2480,  -7811.8727]], dtype=torch.float64)
	q_value: tensor([[-33.0746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15912539028477768, distance: 1.049354701904831 entropy 10.548067123963238
epoch: 35, step: 28
	action: tensor([[ -8079.5689, -20505.4305,   9670.8739,   7879.0640,   1333.1710,
           9778.4463,  18874.0656]], dtype=torch.float64)
	q_value: tensor([[-27.5349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20853440938858425, distance: 1.0180584488912214 entropy 10.65815938545538
epoch: 35, step: 29
	action: tensor([[ -7758.6162, -12488.0323,   3353.1699,  -8254.3005,  13468.6705,
          13011.4425,  15914.0119]], dtype=torch.float64)
	q_value: tensor([[-27.2057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6507336781034403, distance: 1.4702635791451988 entropy 10.417059553554541
epoch: 35, step: 30
	action: tensor([[-13220.0947,  -2755.7949,  -3370.7186,  13354.0664,  -7921.0150,
           6815.0049,   3408.1165]], dtype=torch.float64)
	q_value: tensor([[-29.8613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5062995182345738, distance: 1.404469660182242 entropy 10.546883046526817
epoch: 35, step: 31
	action: tensor([[  9210.6025,   1851.0114, -12884.3333,  -9581.0853,   2867.6282,
           9666.8953,  27044.6018]], dtype=torch.float64)
	q_value: tensor([[-34.8010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.738584006036033
epoch: 35, step: 32
	action: tensor([[ 4135.8344, -2350.5135, -2607.5456, -5239.6209, 11717.1502, -3326.0098,
         -1859.4582]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02754751336327499, distance: 1.1599990927850867 entropy 9.993769838042994
epoch: 35, step: 33
	action: tensor([[ 3010.2202,  3897.0691,  1495.5984,  1071.3775, -3972.8732, -2786.8876,
          4596.3925]], dtype=torch.float64)
	q_value: tensor([[-22.4637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2763386002526156, distance: 0.9734740908745275 entropy 10.078486738690527
epoch: 35, step: 34
	action: tensor([[-11431.0029,  -3291.8212,   8237.7341,   9547.1432,  -8298.0628,
           1212.5768,  13738.4374]], dtype=torch.float64)
	q_value: tensor([[-30.6801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.393363199043408
epoch: 35, step: 35
	action: tensor([[-6252.6731, -9758.1585, -2924.5325,   656.5282, -2500.0744, -3774.0516,
          1970.5711]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45426608070010044, distance: 1.3799985518107247 entropy 9.993769838042994
epoch: 35, step: 36
	action: tensor([[ -6153.9547, -18356.2363,   -166.5756, -12037.6035,  16317.9498,
          -6835.8383,   -348.1504]], dtype=torch.float64)
	q_value: tensor([[-26.2664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30513587975465817, distance: 1.3073279847799435 entropy 10.582359189735921
epoch: 35, step: 37
	action: tensor([[  -985.1676, -13911.5883, -14501.1625,  -3320.4673,   1426.4868,
         -20497.2460,   6857.8744]], dtype=torch.float64)
	q_value: tensor([[-30.0942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09379664857235781, distance: 1.0893552362085615 entropy 10.56204446286696
epoch: 35, step: 38
	action: tensor([[-14181.6666,   1864.0973,  -1414.9230,   4461.6866, -10499.0984,
           1306.2832,   2999.9223]], dtype=torch.float64)
	q_value: tensor([[-31.3626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5352884275062795, distance: 1.4179198468807388 entropy 10.593436379370214
epoch: 35, step: 39
	action: tensor([[ -704.4721, -8532.2856, -2250.8259,  1870.9303,  4986.2535,  4095.1354,
         -4764.9739]], dtype=torch.float64)
	q_value: tensor([[-28.7053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49235433802721373, distance: 1.3979533187452158 entropy 10.28284491971844
epoch: 35, step: 40
	action: tensor([[  -98.0235, -5551.8326,  3526.7994,  1188.3560,   170.4359,  7008.7982,
         -7996.4733]], dtype=torch.float64)
	q_value: tensor([[-28.8281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05354097348042852, distance: 1.1132881900623157 entropy 10.703080032705232
epoch: 35, step: 41
	action: tensor([[  -957.9450, -20449.8931,  -1119.5480,  11169.7861,   -954.3221,
         -10044.0189,   4342.1601]], dtype=torch.float64)
	q_value: tensor([[-31.9777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5576692329383326, distance: 1.428217381535644 entropy 10.68551740892742
epoch: 35, step: 42
	action: tensor([[   190.7943, -12145.5929,  11364.0889,  -1826.7342,  -6135.6002,
           6977.3592,  11310.9584]], dtype=torch.float64)
	q_value: tensor([[-24.5398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44953186511652254, distance: 0.8490295096007013 entropy 10.42414639014055
epoch: 35, step: 43
	action: tensor([[-16435.8146,  -4783.4835,    222.1570,  -2954.5105, -16281.9769,
           4699.1016,  -3763.8801]], dtype=torch.float64)
	q_value: tensor([[-31.1846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08383069996135961, distance: 1.191344646211729 entropy 10.517042718517162
epoch: 35, step: 44
	action: tensor([[ -4426.2434, -17560.1512,  -1515.5266,   5487.1309,   1904.1992,
          -1376.6162,  -5012.1485]], dtype=torch.float64)
	q_value: tensor([[-29.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8581241313750012, distance: 1.5598902912499604 entropy 10.515587315595036
epoch: 35, step: 45
	action: tensor([[-8510.9008,   644.6403, -3667.4183, -4786.5018,  9440.4116, -3034.9316,
         -9777.2638]], dtype=torch.float64)
	q_value: tensor([[-27.8673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5352350979418306, distance: 0.780141453506807 entropy 10.529486964326058
epoch: 35, step: 46
	action: tensor([[  6151.9099, -18839.0404,   3701.4814,  11507.4810,  -1055.2287,
         -16441.3926,  -9550.9038]], dtype=torch.float64)
	q_value: tensor([[-38.4355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.523512316985533
epoch: 35, step: 47
	action: tensor([[-2031.5263,  8154.8735,  5076.6849, -7979.4427, -5047.8623, -7432.6235,
          2686.2036]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6255880261288007, distance: 0.7002152384656684 entropy 9.993769838042994
epoch: 35, step: 48
	action: tensor([[  2315.4810,  -9962.2162,   1977.8944, -29856.6088,   3701.5676,
          -9493.2917,  15645.3941]], dtype=torch.float64)
	q_value: tensor([[-43.7003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.546953208150565
epoch: 35, step: 49
	action: tensor([[-3995.1476, -3622.9123,  1906.7953, -2942.9287,    11.8513,  3330.3869,
          2973.0908]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7937776677474717, distance: 1.5326429776674018 entropy 9.993769838042994
epoch: 35, step: 50
	action: tensor([[ -3508.2690,  -9259.7432,  -1545.0821,  12371.7842,  -2632.0109,
          10911.4905, -12863.5486]], dtype=torch.float64)
	q_value: tensor([[-29.4236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31036679101562203, distance: 1.309945213523252 entropy 10.523291980387203
epoch: 35, step: 51
	action: tensor([[-6704.4743, 11380.5924, -3832.0595, -3822.5670,  1533.2279, 10978.6622,
          2244.1521]], dtype=torch.float64)
	q_value: tensor([[-24.1947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2827088777007236, distance: 1.2960469773399534 entropy 10.273357244232418
epoch: 35, step: 52
	action: tensor([[ -9910.7905,   5119.8184, -11782.4167,   -425.4405,  -8649.0683,
           5485.6532,   3595.1069]], dtype=torch.float64)
	q_value: tensor([[-34.7549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3390179507240564, distance: 0.9303610622598365 entropy 10.227677936577654
epoch: 35, step: 53
	action: tensor([[-11167.2558,  -6955.8457, -12149.6410,   6845.0870,   4027.2983,
          -5671.5972,    788.3124]], dtype=torch.float64)
	q_value: tensor([[-41.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1117756312339493, distance: 1.6629553179562901 entropy 10.494553556535795
epoch: 35, step: 54
	action: tensor([[-12081.5281,   1559.1145, -11659.6595, -12178.1702,   4074.6060,
         -18683.4923,  -9799.3049]], dtype=torch.float64)
	q_value: tensor([[-26.6519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3460927767036627, distance: 0.9253686040538545 entropy 10.435195698027945
epoch: 35, step: 55
	action: tensor([[-12329.7541, -20257.7163,  13484.1327,  11074.5045,   2824.7801,
          -2775.1618,  13190.4050]], dtype=torch.float64)
	q_value: tensor([[-38.3332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22481954327447973, distance: 1.2664637017755878 entropy 10.576364492593612
epoch: 35, step: 56
	action: tensor([[ -4064.9691,   3134.8338,   5936.5251, -13333.3646,   1019.8912,
           -435.8273,   7149.7166]], dtype=torch.float64)
	q_value: tensor([[-26.0024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2525963943552143, distance: 1.2807438286821002 entropy 10.53426388050505
epoch: 35, step: 57
	action: tensor([[  2862.1061, -17582.3733,   1689.4529,  -4039.7864,   5660.7844,
          -4699.5083,   2199.4543]], dtype=torch.float64)
	q_value: tensor([[-37.0539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4778644280172736, distance: 0.8268911316141575 entropy 10.34017803510928
epoch: 35, step: 58
	action: tensor([[  897.5020, -7984.4034, -5629.2252, 12781.0972,  4642.8279, -7256.6955,
         -6172.7238]], dtype=torch.float64)
	q_value: tensor([[-26.3973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.277739290342883, distance: 1.2935339093705982 entropy 10.272276724849316
epoch: 35, step: 59
	action: tensor([[-3691.4253, -3619.3841,  5400.3492,  1038.9267,  1905.8931, -1065.1265,
         -1757.4774]], dtype=torch.float64)
	q_value: tensor([[-29.2935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19358944052740146, distance: 1.0276253195349359 entropy 10.332589169548344
epoch: 35, step: 60
	action: tensor([[-11464.1364,   4106.4241,   9039.0242,   7737.1156,  -4766.4321,
          10805.3511,   5278.9326]], dtype=torch.float64)
	q_value: tensor([[-32.9089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11536156110215057, distance: 1.2085497417409905 entropy 10.524617692677962
epoch: 35, step: 61
	action: tensor([[ 13797.2150,   6724.0311,  -7680.8579, -23131.5853,   3047.7529,
           5848.4813, -22480.1718]], dtype=torch.float64)
	q_value: tensor([[-36.2443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5982381123335059, distance: 0.7253390534414924 entropy 10.594524709593538
epoch: 35, step: 62
	action: tensor([[ 4530.6012, -6787.5204, -3846.5181,  2145.6076,  3255.5756,  1548.5439,
         -7602.6290]], dtype=torch.float64)
	q_value: tensor([[-33.2761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5680711014139039, distance: 0.7520778951455964 entropy 10.343366761699153
epoch: 35, step: 63
	action: tensor([[-4563.6291, -2828.7062,  4139.9315,  1567.1473, -7159.8308,  8896.6986,
         -7191.4815]], dtype=torch.float64)
	q_value: tensor([[-28.0410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7078143972731885, distance: 1.495467669599462 entropy 10.356074743927232
epoch: 35, step: 64
	action: tensor([[ -2765.7730,   9231.7809, -21762.4545,   7220.0273,  -3507.3308,
          -6036.7903,  10566.1519]], dtype=torch.float64)
	q_value: tensor([[-33.6781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21176990305301546, distance: 1.0159754177597815 entropy 10.616970626479812
epoch: 35, step: 65
	action: tensor([[  -244.2703, -14991.5443,  -8253.9054,   4453.3514,   -877.5276,
          -6476.2465,  25165.9193]], dtype=torch.float64)
	q_value: tensor([[-33.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008759480249435736, distance: 1.139321299743303 entropy 10.595955256507404
epoch: 35, step: 66
	action: tensor([[ 1324.6414, -3058.9986,  3239.7207,  4158.2131, 14958.6067, 12223.8844,
         12287.2563]], dtype=torch.float64)
	q_value: tensor([[-34.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14792299913040452, distance: 1.2260638055989335 entropy 10.632978264625944
epoch: 35, step: 67
	action: tensor([[-19710.5435,  -1316.1820,  -5133.7797,   7632.4480,  -1801.3477,
          12219.4658,  -9188.7267]], dtype=torch.float64)
	q_value: tensor([[-28.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8389836804196702, distance: 1.5518353151058053 entropy 10.675526989902577
epoch: 35, step: 68
	action: tensor([[-5079.8424, -9559.7097,  5583.5286,  -901.3752,  2851.0475, -3704.6013,
         -6252.7272]], dtype=torch.float64)
	q_value: tensor([[-30.3583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0544520895771323, distance: 1.6402298159232445 entropy 10.63533662879193
epoch: 35, step: 69
	action: tensor([[ 2792.0331,  8194.9971, -4970.5493,  1086.9761,  6317.1987, 11638.8576,
          4462.0153]], dtype=torch.float64)
	q_value: tensor([[-28.8670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16804431873100456, distance: 1.0437747561575164 entropy 10.501739347408332
epoch: 35, step: 70
	action: tensor([[ 4082.0426, -1811.9903,   373.4508, 16182.0415, -6603.3126, -8062.0143,
          3082.7904]], dtype=torch.float64)
	q_value: tensor([[-32.1632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15468683742178357, distance: 1.2296706316855677 entropy 10.332658071390199
epoch: 35, step: 71
	action: tensor([[  5522.5448,   3531.2809, -17486.8785,   3561.7731,    615.1343,
          15720.3862,  16726.7819]], dtype=torch.float64)
	q_value: tensor([[-31.4986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5802776979947735, distance: 0.7413746232323333 entropy 10.516871012723731
epoch: 35, step: 72
	action: tensor([[-13325.4244,   3929.9528,   4367.5895,   -916.8896,  -2210.0250,
           3386.5144,  11525.9137]], dtype=torch.float64)
	q_value: tensor([[-28.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39752592370435536, distance: 0.8882308960623606 entropy 10.285595676242716
epoch: 35, step: 73
	action: tensor([[-7470.5309, -6965.9199,   908.4172,  2768.2398, -2730.5461,  5622.4036,
          8771.6072]], dtype=torch.float64)
	q_value: tensor([[-30.3481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33616397533787445, distance: 1.3227768097999133 entropy 10.295562376849796
epoch: 35, step: 74
	action: tensor([[-6433.7061,  8072.5554, 14309.9934, 13669.1897, 10676.8866, -2000.9240,
           -31.0035]], dtype=torch.float64)
	q_value: tensor([[-33.4740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20290415110160964, distance: 1.0216731191659074 entropy 10.62055794396022
epoch: 35, step: 75
	action: tensor([[ -3993.8973,  -9778.6643, -10001.7034,   6855.6712,   2304.3908,
           7874.6542,  -5462.5766]], dtype=torch.float64)
	q_value: tensor([[-35.5281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17620858641970472, distance: 1.2410773965996325 entropy 10.60488924461157
epoch: 35, step: 76
	action: tensor([[-24406.6238,  -5111.0886,  -2481.8428,  10316.8612,   3825.7433,
           -204.2189,  -6317.7437]], dtype=torch.float64)
	q_value: tensor([[-27.4530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.552285059623087, distance: 1.4257468870642729 entropy 10.551244297419556
epoch: 35, step: 77
	action: tensor([[ -2651.6110,   4140.4764, -15122.3516,   5580.1579,   9803.7462,
            524.2905,  -1369.2038]], dtype=torch.float64)
	q_value: tensor([[-24.9263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018332866064247888, distance: 1.133806177328812 entropy 10.541969108735582
epoch: 35, step: 78
	action: tensor([[ -7521.1772, -15883.3735, -15139.5617,  14688.1671,  -4369.2882,
         -10370.6843,  -7735.9771]], dtype=torch.float64)
	q_value: tensor([[-32.5324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023136746099386984, distance: 1.1575067562130643 entropy 10.457333314960485
epoch: 35, step: 79
	action: tensor([[    25.6734, -22552.7162,  -5002.2072,   6425.4000,  18462.2828,
          15327.4605,  11906.4089]], dtype=torch.float64)
	q_value: tensor([[-31.5260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7360209483033451, distance: 1.5077667922818354 entropy 10.746196158999037
epoch: 35, step: 80
	action: tensor([[ -7742.8863,  -4353.4166,  -2426.2150,   5525.3040, -18465.9825,
           3883.9784,   -350.5053]], dtype=torch.float64)
	q_value: tensor([[-28.1196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06399356603700235, distance: 1.1071236045283686 entropy 10.541361102047988
epoch: 35, step: 81
	action: tensor([[  4422.4247,   4284.5207,   5099.8433,   7510.7001,   -754.9568,
         -10318.3463,    409.0283]], dtype=torch.float64)
	q_value: tensor([[-26.8289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29923599005391655, distance: 0.9579494398715177 entropy 10.402180787516778
epoch: 35, step: 82
	action: tensor([[ -8273.2266, -25416.0829,   3030.6912,   8937.2552,   4844.5746,
          -3287.0047,   2155.4619]], dtype=torch.float64)
	q_value: tensor([[-28.6221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30209085674516967, distance: 1.3058020254895466 entropy 10.360606499338404
epoch: 35, step: 83
	action: tensor([[  495.5145,  3722.1424, -7458.2584, 29115.5427, -6242.8893,    32.0117,
         -4576.6657]], dtype=torch.float64)
	q_value: tensor([[-29.5104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1457584270017941, distance: 1.0576623499887796 entropy 10.514175786558791
epoch: 35, step: 84
	action: tensor([[-5124.7383, -7582.6725, 12664.9639,  4842.4655,  1924.4213, -4112.5618,
           282.1249]], dtype=torch.float64)
	q_value: tensor([[-30.2267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45940446336890517, distance: 1.3824343880461007 entropy 10.321871446706087
epoch: 35, step: 85
	action: tensor([[   332.1633,  -4667.0194,  -9263.4772,  -3833.4306,   7286.3770,
         -10719.5474,  -3311.5733]], dtype=torch.float64)
	q_value: tensor([[-24.8730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04278923529676515, distance: 1.168570619370179 entropy 10.42184322443505
epoch: 35, step: 86
	action: tensor([[-9711.0259, -6690.6295, -1211.1696, -8341.4736,   568.6440,  3943.2604,
         11201.7331]], dtype=torch.float64)
	q_value: tensor([[-23.9863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1370587961116102, distance: 1.063034345653599 entropy 10.385702520224157
epoch: 35, step: 87
	action: tensor([[  4484.0114, -17558.2025,  -6373.4156,   9887.0819,  -3513.8655,
           4202.1632, -12430.7752]], dtype=torch.float64)
	q_value: tensor([[-32.7991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20178589121156776, distance: 1.2544987815819237 entropy 10.626515050940684
epoch: 35, step: 88
	action: tensor([[ -6759.8223,   8563.0320,   3250.7874, -10691.9588,   8984.3846,
         -13136.4170, -18091.1974]], dtype=torch.float64)
	q_value: tensor([[-26.4316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.498027899951182
epoch: 35, step: 89
	action: tensor([[ -4062.1610,  -5542.8404,  -5640.2157, -10001.9154,   3334.1715,
            111.7891,   -157.3361]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13765942646628937, distance: 1.0626643309680348 entropy 9.993769838042994
epoch: 35, step: 90
	action: tensor([[-19083.8706,  -7206.2029,  -9106.6108,   9857.9496,   2654.8513,
           -103.7373,  -6011.2089]], dtype=torch.float64)
	q_value: tensor([[-38.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0036440031547578, distance: 1.6198208086331038 entropy 10.64066460209759
epoch: 35, step: 91
	action: tensor([[ -2856.6956,   -289.7439, -15983.7227,  -9418.1230,   3974.0506,
          -1828.1143,  11803.8884]], dtype=torch.float64)
	q_value: tensor([[-27.1697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4382825122445482, distance: 1.37239394495573 entropy 10.577278111602482
epoch: 35, step: 92
	action: tensor([[ 7244.2302, -1904.6140, -7860.8704,   737.5393, -1398.7847, -6632.1057,
         -8094.3134]], dtype=torch.float64)
	q_value: tensor([[-32.5073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21977998616793992, distance: 1.0108000075386694 entropy 10.543705164975247
epoch: 35, step: 93
	action: tensor([[-4551.5049, -6461.6422, -1703.3660,  8014.9321, -7187.1580,  3280.0671,
         -6116.3011]], dtype=torch.float64)
	q_value: tensor([[-25.9932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003062140447625983, distance: 1.1425908392572033 entropy 10.252747007173856
epoch: 35, step: 94
	action: tensor([[-15691.3961,   4133.5717,   9233.5030,   4153.4627,   4155.5490,
           5889.3126,   9934.0488]], dtype=torch.float64)
	q_value: tensor([[-28.6420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0001150456961862556, distance: 1.1444100780475501 entropy 10.588619724408408
epoch: 35, step: 95
	action: tensor([[ 1947.2654, -3222.0539, -5841.8313, 11851.1691,  7664.1855,  1779.0344,
         12134.0596]], dtype=torch.float64)
	q_value: tensor([[-35.9453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3982215877911749, distance: 0.887717937219341 entropy 10.611113996440652
epoch: 35, step: 96
	action: tensor([[  131.9108, -5408.2436,  4497.4734, -8515.6161,  2861.8744, -6416.4154,
         -1769.4752]], dtype=torch.float64)
	q_value: tensor([[-32.8137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2090872618975479, distance: 1.2583038231959498 entropy 10.448327295013986
epoch: 35, step: 97
	action: tensor([[-20995.5575,  -6207.6677,   1569.2348,  17041.5457,   6199.3137,
           5818.6629,    918.3601]], dtype=torch.float64)
	q_value: tensor([[-34.4475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5264153657742383, distance: 1.4138165393405038 entropy 10.607924384689813
epoch: 35, step: 98
	action: tensor([[  4406.2448,   -529.0210, -14016.4653,  -5844.3170,   -291.2696,
         -23088.9129,   8086.9439]], dtype=torch.float64)
	q_value: tensor([[-27.9227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0213994680646461, distance: 1.1320338597105755 entropy 10.555303142702583
epoch: 35, step: 99
	action: tensor([[  9804.2311, -19274.6774,  -5178.5627,  -5632.5277,    792.0874,
          12207.6275,   6588.8259]], dtype=torch.float64)
	q_value: tensor([[-23.7524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48367240243515597, distance: 1.3938810136984137 entropy 10.26414650677681
epoch: 35, step: 100
	action: tensor([[ 4079.5033, -9883.3961,  3761.2063,  2004.9994, -4470.1538,  7148.8347,
          7363.8340]], dtype=torch.float64)
	q_value: tensor([[-29.3079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3398422280973259, distance: 1.324596258476945 entropy 10.43040800846443
epoch: 35, step: 101
	action: tensor([[ -1076.7327,  -5429.8824,   1743.9574, -12548.0543,   -871.9943,
          -9346.3441,  -6694.2122]], dtype=torch.float64)
	q_value: tensor([[-26.9752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36981060951388556, distance: 0.9084316085932215 entropy 10.465444714016332
epoch: 35, step: 102
	action: tensor([[ -7247.4257, -13844.6643,  20159.5987,  25161.4222, -11439.8380,
          -6081.1169,  17497.1690]], dtype=torch.float64)
	q_value: tensor([[-35.9996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11025880764725149, distance: 1.2057820289389332 entropy 10.674403815454168
epoch: 35, step: 103
	action: tensor([[-19011.0499,   1333.2419,  -4344.3165,     25.2917, -10173.0213,
          -2774.8403,  15104.5493]], dtype=torch.float64)
	q_value: tensor([[-30.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017012343982546585, distance: 1.1345685095250824 entropy 10.57407012564013
epoch: 35, step: 104
	action: tensor([[-11783.8362, -13113.9087,   9199.4493,   2108.9343, -14656.5828,
          16361.5863,  -1337.7045]], dtype=torch.float64)
	q_value: tensor([[-33.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09551200752521316, distance: 1.1977474758881026 entropy 10.428070390544317
epoch: 35, step: 105
	action: tensor([[  9136.8377,  -2499.5621,  -4599.6832,   3546.1474,  18121.5247,
           1668.2777, -12773.8376]], dtype=torch.float64)
	q_value: tensor([[-30.2094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.213656531838607, distance: 1.014758820729895 entropy 10.518275379213629
epoch: 35, step: 106
	action: tensor([[ -3387.2151, -11175.0411,   9883.3218,  13237.6373,  -6390.2508,
           5723.5666,   3015.3401]], dtype=torch.float64)
	q_value: tensor([[-28.3265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0826883564569868, distance: 1.0960115890092876 entropy 10.223305098711778
epoch: 35, step: 107
	action: tensor([[-4931.5439,  4415.6581,  1712.0228, 19468.7798,  1086.5122, 14574.2536,
         10157.6522]], dtype=torch.float64)
	q_value: tensor([[-25.8160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11121661591882437, distance: 1.0788340786762451 entropy 10.374696366422848
epoch: 35, step: 108
	action: tensor([[-13590.2784,  -7199.1407,   1905.3750,  -8456.1085,   1081.5661,
           4466.7720,   3960.2075]], dtype=torch.float64)
	q_value: tensor([[-32.1380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9315125011544612, distance: 1.5903966598071282 entropy 10.530362155788579
epoch: 35, step: 109
	action: tensor([[-21840.9478, -10047.6027,  -6826.7404,   4519.9590, -11569.9203,
           3842.5434,  -9043.0615]], dtype=torch.float64)
	q_value: tensor([[-30.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3580725916715142, distance: 1.3335772727658406 entropy 10.545796520906967
epoch: 35, step: 110
	action: tensor([[  3988.2171, -13136.5643, -20444.2055,   9583.6169,  -4083.3567,
          -4588.9237,   8068.4965]], dtype=torch.float64)
	q_value: tensor([[-33.9356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3968882864887604, distance: 0.8887008078151287 entropy 10.779999972879521
epoch: 35, step: 111
	action: tensor([[-5935.2219,  9693.9329,  9275.0122,  5169.6587, -1929.1138,  1448.5557,
         10028.5535]], dtype=torch.float64)
	q_value: tensor([[-33.0578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18920777551225076, distance: 1.0304133596989522 entropy 10.404632238264066
epoch: 35, step: 112
	action: tensor([[-8990.4420,  7272.1888, 17504.4849, 16031.6235,  7978.7014,  4041.1389,
         -4900.7536]], dtype=torch.float64)
	q_value: tensor([[-27.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0528069948295955, distance: 1.174170254563727 entropy 10.283439065757957
epoch: 35, step: 113
	action: tensor([[ -4945.1461, -14548.2763,    721.6572,   1930.4302, -13117.2815,
          -5764.9828,  -1406.0950]], dtype=torch.float64)
	q_value: tensor([[-28.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.379005834997276, distance: 1.3438158066456372 entropy 10.349835174607113
epoch: 35, step: 114
	action: tensor([[ -9366.5930, -24794.0183,  -5142.8382,  -5882.3683, -10511.7655,
          10537.6612,  12570.5736]], dtype=torch.float64)
	q_value: tensor([[-27.0567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9949384950399143, distance: 1.6162980487088423 entropy 10.464092364088575
epoch: 35, step: 115
	action: tensor([[  -441.5393, -10034.2712,  -2589.6088,  -7448.3837,   8953.4483,
           8125.3200,   3764.1390]], dtype=torch.float64)
	q_value: tensor([[-29.4642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20867370547429154, distance: 1.2580886095591066 entropy 10.476839915585117
epoch: 35, step: 116
	action: tensor([[-11901.5746,  -2524.1291,    597.0304,   2865.8400, -10385.9955,
           -849.3090,  -3626.5707]], dtype=torch.float64)
	q_value: tensor([[-28.1684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7381684279454097, distance: 1.5086990672590412 entropy 10.306320647056316
epoch: 35, step: 117
	action: tensor([[ -5587.2370,  -8898.5923,  -2158.8750,   -280.5397,   6027.1866,
           1032.9796, -10499.1101]], dtype=torch.float64)
	q_value: tensor([[-21.7751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44493747837986164, distance: 1.3755653297844328 entropy 10.195770424315011
epoch: 35, step: 118
	action: tensor([[ -8015.9790, -16151.1023,  -8566.1155, -12518.7468,   3425.0552,
           1548.7327,   5250.0046]], dtype=torch.float64)
	q_value: tensor([[-26.8640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.704985696955216, distance: 1.4942286640631512 entropy 10.382576545843849
epoch: 35, step: 119
	action: tensor([[  3502.7095,  10091.9486,  -9887.8095,   5685.2180, -12638.6577,
         -13121.1775, -10165.5398]], dtype=torch.float64)
	q_value: tensor([[-35.4487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.622072888701435
epoch: 35, step: 120
	action: tensor([[ 8088.7244, -1332.6009,  5454.3182,  7191.9582, -6906.7572,  1621.4627,
          1498.7875]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21946404599833014, distance: 1.011004642127177 entropy 9.993769838042994
epoch: 35, step: 121
	action: tensor([[  406.5300, -8533.3445,  8016.9134, 15681.3235, -7609.3698, 12973.7201,
         -5646.2768]], dtype=torch.float64)
	q_value: tensor([[-23.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38234643129356083, distance: 0.8993508941861651 entropy 10.202571069758275
epoch: 35, step: 122
	action: tensor([[ -8395.4115,  -2912.6037,  -7740.6915, -16418.4670,   -933.9648,
          -8487.2844,   8618.1470]], dtype=torch.float64)
	q_value: tensor([[-33.6076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8211807559971318, distance: 1.5443055049028036 entropy 10.490126778459643
epoch: 35, step: 123
	action: tensor([[ -815.1898,  1804.1973,  8319.3724, -5978.0847, -1839.9664,  6940.5359,
         -7528.9117]], dtype=torch.float64)
	q_value: tensor([[-25.5450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35615886933061525, distance: 1.3326373390911002 entropy 10.32720928189334
epoch: 35, step: 124
	action: tensor([[  -389.2733,   5924.0068,  -2271.7493,  10727.9398, -14363.3461,
           -564.2898,  15005.3241]], dtype=torch.float64)
	q_value: tensor([[-34.9713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18504282624790902, distance: 1.033056528906094 entropy 10.289743667234703
epoch: 35, step: 125
	action: tensor([[ 1628.8212, -7671.8914, -8983.7848, -4305.7534,  2959.0574,  3343.2193,
         -1707.0915]], dtype=torch.float64)
	q_value: tensor([[-27.3164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.128046470619967
epoch: 35, step: 126
	action: tensor([[-2640.4397,  1160.3148, -4109.3936,  -820.6089,  3113.4151,  -148.2540,
         12448.3544]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.993769838042994
epoch: 35, step: 127
	action: tensor([[ -2242.0137, -19940.2585,  -1831.2183,  -3556.5370,  -8396.8596,
          -4847.5864,  -5850.8090]], dtype=torch.float64)
	q_value: tensor([[-32.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.993769838042994
LOSS epoch 35 actor 377.87422758794014 critic 162.07288288389736
epoch: 36, step: 0
	action: tensor([[   209.3791, -12671.1274,  -3556.5350,  -4157.0036,   1244.4737,
          -3920.7406,   4743.3824]], dtype=torch.float64)
	q_value: tensor([[-31.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18725741669367613, distance: 1.0316519437689622 entropy 10.048990259157362
epoch: 36, step: 1
	action: tensor([[-3640.6323,  -873.3166, -8240.6152,  4657.1861, -1012.3208, 10896.3026,
         -3064.1533]], dtype=torch.float64)
	q_value: tensor([[-23.6052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26029760926596657, distance: 1.2846749311415706 entropy 10.216853539024829
epoch: 36, step: 2
	action: tensor([[ -6912.9887,   5757.8903,   6286.7891,   8382.9278, -13121.8119,
          -8704.0887,   5360.4893]], dtype=torch.float64)
	q_value: tensor([[-26.5963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.166497192186277, distance: 1.0447448207958603 entropy 10.489000318776363
epoch: 36, step: 3
	action: tensor([[ -9469.0359, -24051.8260,  -8272.3076,  22045.7775,   -680.2828,
         -10711.5841,  -2519.7466]], dtype=torch.float64)
	q_value: tensor([[-36.5204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12493036194258655, distance: 1.2137228062572907 entropy 10.75093113121829
epoch: 36, step: 4
	action: tensor([[ -9749.7917, -12956.6505, -14122.1606,    289.5253,  -4723.6634,
           2845.2384,   4480.4793]], dtype=torch.float64)
	q_value: tensor([[-28.2427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1522442491434044, distance: 1.0536395533153509 entropy 10.619379138885739
epoch: 36, step: 5
	action: tensor([[-14998.2964,   6297.3451,   4884.3884,  15325.7828, -10037.4088,
          -2775.1794,   2020.0573]], dtype=torch.float64)
	q_value: tensor([[-28.1542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24600000541866973, distance: 0.9936704266193529 entropy 10.509887108909199
epoch: 36, step: 6
	action: tensor([[ 7629.9895,   263.7355,  6356.8379, 11496.5539,  5942.9437,   625.5484,
           199.3629]], dtype=torch.float64)
	q_value: tensor([[-37.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6695952449290415, distance: 0.6577787477836682 entropy 10.714369886891772
epoch: 36, step: 7
	action: tensor([[  9387.5706, -14714.9875, -11837.3281,  -3667.2119,   -875.4668,
          11797.9112,  19951.2159]], dtype=torch.float64)
	q_value: tensor([[-26.4151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10575432517783057, distance: 1.2033335258519746 entropy 10.51759846387348
epoch: 36, step: 8
	action: tensor([[ -4490.1863, -13568.6150,   -314.8895,   4338.1901,  -9980.5653,
          14848.4714,   8144.0035]], dtype=torch.float64)
	q_value: tensor([[-26.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.569576993348768, distance: 1.4336660644115358 entropy 10.418617579210073
epoch: 36, step: 9
	action: tensor([[  4245.2747, -21751.5407,  -8520.8831,  -1944.8938,  -9500.8066,
         -13028.6523,  13533.7384]], dtype=torch.float64)
	q_value: tensor([[-27.8223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18436137831830046, distance: 1.2453711812737407 entropy 10.622685212391861
epoch: 36, step: 10
	action: tensor([[-2423.3138,  4261.9240,  4211.6411, 12069.8575,  8100.2477, -2978.0363,
          8177.4734]], dtype=torch.float64)
	q_value: tensor([[-23.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.201727288851444
epoch: 36, step: 11
	action: tensor([[-1374.0811, -6111.8846, -1475.6693, -6028.2168, -7545.6691, -4238.1896,
         -5351.5719]], dtype=torch.float64)
	q_value: tensor([[-31.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15273231398192832, distance: 1.0533362121385612 entropy 10.048990259157362
epoch: 36, step: 12
	action: tensor([[-13166.1823,   1696.0901, -11408.2283,   -174.1416,   5539.9795,
          -4959.9168,  -1084.4302]], dtype=torch.float64)
	q_value: tensor([[-26.0991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20571877606068867, distance: 1.2565497997260864 entropy 10.403668152419268
epoch: 36, step: 13
	action: tensor([[ -7758.4491, -12661.1030,   1677.7018,   3653.7711,   8064.3179,
          17250.3966, -21732.3603]], dtype=torch.float64)
	q_value: tensor([[-38.5562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7939333547521017, distance: 1.5327094874198115 entropy 10.667603030175162
epoch: 36, step: 14
	action: tensor([[   404.0783,   5226.9607,  -5721.7917,    -51.8165,   1363.1425,
            768.9602, -12267.0676]], dtype=torch.float64)
	q_value: tensor([[-27.7094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.527985009735502
epoch: 36, step: 15
	action: tensor([[4481.2966, -283.4468, 5042.8447, 4147.1243,  -10.3177, 5052.9638,
          -57.8712]], dtype=torch.float64)
	q_value: tensor([[-31.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43725859273971823, distance: 0.8584423394185104 entropy 10.048990259157362
epoch: 36, step: 16
	action: tensor([[ -4845.1731, -11904.0700,   1152.1638,  13029.7108,   3910.5059,
           8186.8752,   8208.3692]], dtype=torch.float64)
	q_value: tensor([[-28.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5295983132420388, distance: 1.415289847446733 entropy 10.472200062896807
epoch: 36, step: 17
	action: tensor([[-13391.2540, -11441.6609,  -6817.6529,  -5644.0445,  -3630.6858,
           7485.1663, -12520.5435]], dtype=torch.float64)
	q_value: tensor([[-32.4772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8749191541908881, distance: 1.5669241214426783 entropy 10.80806090205823
epoch: 36, step: 18
	action: tensor([[  -699.2013, -20393.1254,  -5746.6978,  16113.7603,  -1407.3023,
         -13232.7212,  -7632.8082]], dtype=torch.float64)
	q_value: tensor([[-25.7643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42025984015747664, distance: 1.3637683170627801 entropy 10.451812637654546
epoch: 36, step: 19
	action: tensor([[  4220.4186,   4477.6786,   6674.6241,  -3073.5303,   1736.4024,
         -10357.7152,  19009.0868]], dtype=torch.float64)
	q_value: tensor([[-27.6595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.649211614407857
epoch: 36, step: 20
	action: tensor([[-4637.9830,  3348.0437,  8388.9786,  -767.9610,  -322.2233,  2162.3632,
         -4090.6328]], dtype=torch.float64)
	q_value: tensor([[-31.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14846862194524713, distance: 1.2263551530638004 entropy 10.048990259157362
epoch: 36, step: 21
	action: tensor([[ -4120.1516, -11945.7891,  -1781.7462,  10476.9192,   3559.0292,
          13996.6617,  13438.0547]], dtype=torch.float64)
	q_value: tensor([[-30.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4913223641442439, distance: 1.3974698877111014 entropy 10.284062945997416
epoch: 36, step: 22
	action: tensor([[-13149.1374, -27564.7481, -18178.8716,   1057.7720, -37873.4232,
          -4965.1666, -17607.2105]], dtype=torch.float64)
	q_value: tensor([[-32.5351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01034968012929749, distance: 1.1502508090939287 entropy 10.787690469325236
epoch: 36, step: 23
	action: tensor([[-14395.9519,  -8869.4679,   -453.5256,  10957.1491,    140.4059,
          -9487.6884,   6621.4908]], dtype=torch.float64)
	q_value: tensor([[-24.9885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021197931546935944, distance: 1.1321504212744977 entropy 10.6043545300316
epoch: 36, step: 24
	action: tensor([[ -2931.4596, -11423.3571,  11035.0830,  13195.2454,  -4153.0073,
          22224.6218,   2204.3737]], dtype=torch.float64)
	q_value: tensor([[-25.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15066601432201088, distance: 1.0546198577337964 entropy 10.560461129655769
epoch: 36, step: 25
	action: tensor([[-1620.0334,  9027.6125, -1836.5642,  1758.8669, -1591.0674,  -190.8630,
           724.7669]], dtype=torch.float64)
	q_value: tensor([[-26.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3999574256860253, distance: 0.8864366954689443 entropy 10.44898810314358
epoch: 36, step: 26
	action: tensor([[  1057.4640, -16513.0235,  16168.6258,  -1672.0842,   3993.2427,
          -1967.2804,    -85.2673]], dtype=torch.float64)
	q_value: tensor([[-31.1082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0953573778842387, distance: 1.19766294290408 entropy 10.638789510488118
epoch: 36, step: 27
	action: tensor([[ -7043.7475, -14273.0736,  16966.9062,   1239.5804,  -1157.9240,
          -9012.0253,   9291.2370]], dtype=torch.float64)
	q_value: tensor([[-30.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13910453041350346, distance: 1.2213453487794892 entropy 10.718844026622547
epoch: 36, step: 28
	action: tensor([[  9307.1218, -13971.4731,  19843.8610,  -7157.8887,   2236.1716,
         -15035.8846,  -9763.8652]], dtype=torch.float64)
	q_value: tensor([[-29.6553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09815523441577689, distance: 1.0867323308350683 entropy 10.655388917627903
epoch: 36, step: 29
	action: tensor([[ -1682.8173, -14055.4911, -22642.1732,   -738.1498,  -1792.0602,
           6331.2354,   1162.3993]], dtype=torch.float64)
	q_value: tensor([[-25.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1588390260484953, distance: 1.2318795607286819 entropy 10.190766067671953
epoch: 36, step: 30
	action: tensor([[ 3648.3265, -8878.1484,  7083.8818, -4522.5474, 21205.2067, 14228.5514,
          5044.0707]], dtype=torch.float64)
	q_value: tensor([[-27.7686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.437381764635406, distance: 0.8583483870672335 entropy 10.47628587858002
epoch: 36, step: 31
	action: tensor([[ -5034.8861,   2038.6069, -19235.7188,   6292.5832,   2287.6039,
            302.2055,   1584.8636]], dtype=torch.float64)
	q_value: tensor([[-29.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.474413036397404
epoch: 36, step: 32
	action: tensor([[-1844.2282, -8120.4769,   465.4631, -1170.6431,  4578.1616,  6252.9031,
         -1318.0786]], dtype=torch.float64)
	q_value: tensor([[-31.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15086828878167768, distance: 1.2276356879027726 entropy 10.048990259157362
epoch: 36, step: 33
	action: tensor([[ -1239.8175, -12743.9777,  -2317.7357,   3444.8353,  -4397.8757,
          -5608.0779,  18406.4847]], dtype=torch.float64)
	q_value: tensor([[-26.9407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8823004504127543, distance: 1.5700054729927306 entropy 10.429917775320506
epoch: 36, step: 34
	action: tensor([[ -5141.2543,   1851.7650,  -9459.1939,  -3985.6523,  -2161.1388,
         -10139.1088,  17312.8757]], dtype=torch.float64)
	q_value: tensor([[-26.2336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32071998921587974, distance: 0.9431507417890206 entropy 10.451679956015008
epoch: 36, step: 35
	action: tensor([[  2199.4067,  13241.0637, -27565.6033,  13629.5296,   2188.2620,
           -356.5302,   9268.4022]], dtype=torch.float64)
	q_value: tensor([[-36.4836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.854018608864181, distance: 0.4372254589027983 entropy 10.657308704364706
epoch: 36, step: 36
	action: tensor([[ 1138.6855, -2175.5634,  6378.8110,  2972.4801, -3051.1974, -8159.9826,
         -6864.7509]], dtype=torch.float64)
	q_value: tensor([[-31.1517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.363720770672236
epoch: 36, step: 37
	action: tensor([[  5043.0514,   3386.3218,  -2487.9112,  -2725.9971, -10116.4112,
           4002.9617,   6748.1445]], dtype=torch.float64)
	q_value: tensor([[-31.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.048990259157362
epoch: 36, step: 38
	action: tensor([[ -847.7311, -7195.7988, -2941.3437, -1053.2099,  1215.0291,  9663.7048,
         -2971.4060]], dtype=torch.float64)
	q_value: tensor([[-31.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3002277503286186, distance: 0.9572713281482118 entropy 10.048990259157362
epoch: 36, step: 39
	action: tensor([[-20664.6344,  15648.0918, -13977.6652,    426.5505,  -4825.2701,
           6859.3544, -11478.7950]], dtype=torch.float64)
	q_value: tensor([[-28.5132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010945211375738007, distance: 1.1505897557970954 entropy 10.486846806150572
epoch: 36, step: 40
	action: tensor([[-11048.4504,   4925.6743,   2911.0048,   1047.1662,    171.7578,
         -11308.8982,   6955.3996]], dtype=torch.float64)
	q_value: tensor([[-33.2534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4441261581731004, distance: 0.8531881445014701 entropy 10.338979484877191
epoch: 36, step: 41
	action: tensor([[-4324.8933,  -309.4775, 13782.7018, 25348.7223,  7410.5392,  8996.5898,
          5217.7585]], dtype=torch.float64)
	q_value: tensor([[-33.9406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8607226659731488, distance: 1.5609806416364198 entropy 10.56911931113126
epoch: 36, step: 42
	action: tensor([[  -924.1615,   1581.6506,  -5687.0964,  -5608.6688,   7966.9588,
         -12315.1482, -16407.3938]], dtype=torch.float64)
	q_value: tensor([[-31.3396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3594591436105412, distance: 0.9158621501968812 entropy 10.689264035451075
epoch: 36, step: 43
	action: tensor([[ -2442.3547, -11742.1215, -12576.8864,  20998.9086,   3512.0223,
         -11139.0312,  12849.4023]], dtype=torch.float64)
	q_value: tensor([[-35.3310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5747632696938121, distance: 1.436032707289604 entropy 10.448368534967605
epoch: 36, step: 44
	action: tensor([[-10602.0057,   9450.4547,   5996.6655,  -3068.6106, -16826.9583,
          10793.8764,   -188.3389]], dtype=torch.float64)
	q_value: tensor([[-23.8132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4448408261929807, distance: 0.8526395106708233 entropy 10.410913819284598
epoch: 36, step: 45
	action: tensor([[-14145.5150, -16974.9125, -12313.3916,   8590.8707,   7891.9548,
          -1949.9959, -13897.9081]], dtype=torch.float64)
	q_value: tensor([[-27.4324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4541088392959354, distance: 1.3799239441516526 entropy 10.302053861849467
epoch: 36, step: 46
	action: tensor([[ 15171.7772, -13006.0936,   2769.8383,  15140.0332,   1087.0314,
          -6685.3524,  14396.5348]], dtype=torch.float64)
	q_value: tensor([[-25.6145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4365839658302373, distance: 0.8589567451748044 entropy 10.409682961957598
epoch: 36, step: 47
	action: tensor([[ -3691.9435, -13722.4645,  -2902.1843,   3779.9832,  -3747.6192,
           -353.1201,  10144.8341]], dtype=torch.float64)
	q_value: tensor([[-28.1513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3366955672560935, distance: 0.9319940571309805 entropy 10.375149126510282
epoch: 36, step: 48
	action: tensor([[-12566.3173, -12957.6638,   -869.7937,  -1697.1485,   1951.3219,
          21118.9649,   2326.3551]], dtype=torch.float64)
	q_value: tensor([[-29.7326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7249406124359965, distance: 1.502947349939962 entropy 10.759118825252
epoch: 36, step: 49
	action: tensor([[-7860.2265, -9565.8489,  5299.8860, -2719.4195,  9594.1958, -2673.0214,
         -5604.3881]], dtype=torch.float64)
	q_value: tensor([[-27.3162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15215123568012245, distance: 1.0536973529680258 entropy 10.597567161918281
epoch: 36, step: 50
	action: tensor([[ 1298.5304, -3651.4308, -8142.5670, 17162.5997, -1041.8574, -4085.5182,
         17753.7614]], dtype=torch.float64)
	q_value: tensor([[-29.3959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36063787520391577, distance: 0.9150190714522405 entropy 10.576326736480524
epoch: 36, step: 51
	action: tensor([[ -5826.4557,  -3871.3102,  -8850.4779,   -525.9721,   9023.2834,
         -10012.2209,   -824.3326]], dtype=torch.float64)
	q_value: tensor([[-25.3796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06253732127546285, distance: 1.1079845047817605 entropy 10.096956554978116
epoch: 36, step: 52
	action: tensor([[-3172.0674, -5664.2416,  1054.2923, -6623.3387, -4335.7033, -6699.6547,
         15648.3457]], dtype=torch.float64)
	q_value: tensor([[-28.3069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07945865701214372, distance: 1.0979393295811075 entropy 10.486099549778377
epoch: 36, step: 53
	action: tensor([[-15634.5852, -10253.7590,  14408.6976,  -2039.9426, -15855.8632,
          10794.2114,    721.0608]], dtype=torch.float64)
	q_value: tensor([[-36.3401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21220990875050016, distance: 1.0156918093455511 entropy 10.70688369719777
epoch: 36, step: 54
	action: tensor([[  6863.2061, -10979.6715, -13573.6646,   6548.4645,   4979.9705,
         -10950.2258,   1324.8299]], dtype=torch.float64)
	q_value: tensor([[-32.6931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15562782022845978, distance: 1.2301715741245414 entropy 10.674669226935107
epoch: 36, step: 55
	action: tensor([[-18234.2970, -22921.7407,  -4417.0862,   -882.6303,   4993.5323,
         -11825.5524,   -752.8430]], dtype=torch.float64)
	q_value: tensor([[-24.9981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010441403324744747, distance: 1.1383542971322873 entropy 10.610460308910296
epoch: 36, step: 56
	action: tensor([[-15165.6296, -24053.5884,   8794.5180,   8725.2575,  12437.7282,
           9808.5897,   8685.0326]], dtype=torch.float64)
	q_value: tensor([[-29.1883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7919871333791351, distance: 1.5318778507680968 entropy 10.512694299681824
epoch: 36, step: 57
	action: tensor([[ -2978.5049,    252.1954,   1840.9415,   7895.3136,  -5957.8591,
           7535.3669, -12294.8925]], dtype=torch.float64)
	q_value: tensor([[-30.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06901037856705439, distance: 1.183171375113438 entropy 10.69359201474858
epoch: 36, step: 58
	action: tensor([[-2012.2475, -6403.5130, -1800.5016,  -769.4701, -4176.0498,  8289.3386,
         -1116.3570]], dtype=torch.float64)
	q_value: tensor([[-25.9894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6713093757881268, distance: 1.4793983092451792 entropy 10.42158817170828
epoch: 36, step: 59
	action: tensor([[-14676.3994,  14408.9244,  -1362.3556,  -2807.3113,   4308.0834,
         -16561.9883,  -7823.8935]], dtype=torch.float64)
	q_value: tensor([[-30.3591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06998547694877955, distance: 1.183710867314482 entropy 10.640391024540408
epoch: 36, step: 60
	action: tensor([[  2656.8819,  -7339.9487,  -6860.4731,   2849.5381,   8441.9238,
           5999.7645, -12840.2865]], dtype=torch.float64)
	q_value: tensor([[-38.5568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05411932952854681, distance: 1.112947987616102 entropy 10.569672804136317
epoch: 36, step: 61
	action: tensor([[ 1.0937e+01, -9.8398e+03, -1.8523e+04,  1.1448e+04,  5.6092e+03,
          5.7945e+00, -7.3143e+02]], dtype=torch.float64)
	q_value: tensor([[-26.7019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.513219860426286
epoch: 36, step: 62
	action: tensor([[-15959.4022,   4517.9329,   2791.6360,   4827.6173,   1026.3795,
           5324.6757,     47.9600]], dtype=torch.float64)
	q_value: tensor([[-31.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13267906215453873, distance: 1.065728570401263 entropy 10.048990259157362
epoch: 36, step: 63
	action: tensor([[ -3099.5780,  -1577.2398,   9707.4031,  -7394.0420,   3801.9852,
          -7530.8414, -14856.2179]], dtype=torch.float64)
	q_value: tensor([[-33.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08291206346216184, distance: 1.190839657436774 entropy 10.300209373663787
epoch: 36, step: 64
	action: tensor([[-16096.0290,  -3329.8506,   7522.7555,   4103.8802,   6392.4453,
           6213.7350,   6554.4565]], dtype=torch.float64)
	q_value: tensor([[-30.9310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6963687165532144, distance: 1.49044797286976 entropy 10.494173477647376
epoch: 36, step: 65
	action: tensor([[-1237.9158, -8422.4792,  8413.7957,  4556.4766, -8789.9435, -8158.7415,
           900.5597]], dtype=torch.float64)
	q_value: tensor([[-27.4484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6114037393257561, distance: 1.4526429369922362 entropy 10.515559704416388
epoch: 36, step: 66
	action: tensor([[ 7497.9399, -6054.0725,  9165.7033, -5524.8889, -5629.5181, -3141.4814,
         19561.5354]], dtype=torch.float64)
	q_value: tensor([[-26.2366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05233787440160387, distance: 1.1739086260650866 entropy 10.46413054113819
epoch: 36, step: 67
	action: tensor([[ -1085.0652, -12145.0115,   4092.9259,  -6536.9759,  10004.5245,
           3088.9019,   4523.5296]], dtype=torch.float64)
	q_value: tensor([[-25.2299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35295100443543026, distance: 1.3310602924748678 entropy 10.346051124521267
epoch: 36, step: 68
	action: tensor([[ 2410.2619, -9995.8336, -3681.7382, -3172.5845, -4512.7665,  3045.1019,
         17535.4057]], dtype=torch.float64)
	q_value: tensor([[-32.0687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0014285561649469702, distance: 1.1435265818535496 entropy 10.60199092444974
epoch: 36, step: 69
	action: tensor([[12479.0593,  6213.7594, -5313.8688, -1830.1146,   803.2249, -7521.6236,
         12819.8617]], dtype=torch.float64)
	q_value: tensor([[-28.5459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4966323697867333, distance: 0.8118940064133069 entropy 10.446732866980302
epoch: 36, step: 70
	action: tensor([[ 9028.3501, -8926.6192, 12795.8817,  3097.7402,  8641.6432,  3136.4783,
         -6935.6692]], dtype=torch.float64)
	q_value: tensor([[-34.5482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17211795402767205, distance: 1.2389173999875944 entropy 10.484187075716566
epoch: 36, step: 71
	action: tensor([[  7015.0149,  12955.7325,  -1270.4052,  -7916.2331,  -1417.2619,
         -10070.1593,  -7723.5223]], dtype=torch.float64)
	q_value: tensor([[-38.3454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28990544477654634, distance: 0.9643058125892673 entropy 10.547973449425948
epoch: 36, step: 72
	action: tensor([[ -5135.9031,   6133.0127,   2110.3565,  -3487.1937, -19207.6016,
          -2592.4384,   6400.6656]], dtype=torch.float64)
	q_value: tensor([[-37.8979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18794076427575357, distance: 1.0312181501405215 entropy 10.576687092416773
epoch: 36, step: 73
	action: tensor([[ 1910.2304, -5862.3249,  2970.2727, -2655.6817,  1289.2970, -5420.6720,
         19168.3808]], dtype=torch.float64)
	q_value: tensor([[-37.3769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0014593515182641514, distance: 1.1451789498446063 entropy 10.381043591978692
epoch: 36, step: 74
	action: tensor([[ -6226.4399,  -8824.0083,  -5838.6838,   6765.3223, -16254.1389,
            739.1074,  -8070.0087]], dtype=torch.float64)
	q_value: tensor([[-28.4431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9484385525900434, distance: 1.5973498684511644 entropy 10.448881599739062
epoch: 36, step: 75
	action: tensor([[-7296.2910, -6538.8805,  -785.6372, 13255.6899, 10023.6621,  1790.7450,
         -7593.2621]], dtype=torch.float64)
	q_value: tensor([[-25.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34402472587762367, distance: 1.326662100249852 entropy 10.480849200474609
epoch: 36, step: 76
	action: tensor([[ -3675.3629,   6439.4572,  14720.1319,  12787.0397,  -2611.0368,
         -14761.8827,   6950.2271]], dtype=torch.float64)
	q_value: tensor([[-31.8346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07979146627621914, distance: 1.0977408390864738 entropy 10.682470824933555
epoch: 36, step: 77
	action: tensor([[  3724.7571,   1550.1074,   6641.6201,   2573.0807,  12399.2233,
         -13234.8229,  -7379.7758]], dtype=torch.float64)
	q_value: tensor([[-33.2857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44779163321299986, distance: 0.8503704976177597 entropy 10.592675301031639
epoch: 36, step: 78
	action: tensor([[  -803.8072, -10092.6599, -16880.6018,  11789.1473, -15992.7114,
           4068.5373,   7602.1319]], dtype=torch.float64)
	q_value: tensor([[-36.9069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22397751314060987, distance: 1.2660282972502495 entropy 10.59181843466878
epoch: 36, step: 79
	action: tensor([[ -9747.2814,    314.5005,    190.6353,  -6901.0455, -11056.4377,
            585.3258,  18382.5022]], dtype=torch.float64)
	q_value: tensor([[-27.1965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5426488263961968, distance: 1.4213146412717927 entropy 10.571111283283695
epoch: 36, step: 80
	action: tensor([[ -3066.1621,   3047.1992,   1625.3233,   9136.6615, -12392.5299,
          11179.0205,  16574.7661]], dtype=torch.float64)
	q_value: tensor([[-30.4930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027332516006240537, distance: 1.159877731104595 entropy 10.466931537020447
epoch: 36, step: 81
	action: tensor([[  9450.2012,   8577.9287,   3912.0496, -18195.1854,  -6415.2964,
          14314.2426,   5337.6078]], dtype=torch.float64)
	q_value: tensor([[-33.5072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7122166612989902, distance: 0.6138885266214577 entropy 10.63912736814382
epoch: 36, step: 82
	action: tensor([[ -8789.8274, -10781.1005, -16772.4698,  -7693.1285,  14803.2165,
          -9669.3543,   9776.2738]], dtype=torch.float64)
	q_value: tensor([[-30.4410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14653111939490138, distance: 1.057183894848399 entropy 10.342111611288189
epoch: 36, step: 83
	action: tensor([[-16579.6209,   8194.0255,  -8456.4971, -16156.7832,    716.3575,
         -20309.9961,  25334.6790]], dtype=torch.float64)
	q_value: tensor([[-33.7603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23713908632385594, distance: 0.9994921214953755 entropy 10.592587931817674
epoch: 36, step: 84
	action: tensor([[  -313.0155, -12884.9067,  -9597.8432, -21319.1643,   8249.4814,
           8813.5724,   5011.3037]], dtype=torch.float64)
	q_value: tensor([[-34.5168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7944427673133276, distance: 1.5329270891615199 entropy 10.44446122217583
epoch: 36, step: 85
	action: tensor([[ -2889.4204, -22601.0397,   7862.2699,  -3604.3043,   2531.5628,
          15658.6563,  15730.1355]], dtype=torch.float64)
	q_value: tensor([[-28.9970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8049239827757819, distance: 1.5373974312093845 entropy 10.62835488676626
epoch: 36, step: 86
	action: tensor([[ -8098.3843, -25309.2193,  -7254.3906,  -9452.9637,  -8709.7481,
          13216.9275,   -935.8222]], dtype=torch.float64)
	q_value: tensor([[-32.5309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3895852088040297, distance: 1.3489606605382958 entropy 10.61713405361101
epoch: 36, step: 87
	action: tensor([[ -6348.0993, -11896.9516,   8121.3018,  11804.7440,   9731.6344,
           6456.1064,   4086.9615]], dtype=torch.float64)
	q_value: tensor([[-28.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13242071240390674, distance: 1.2177568897018227 entropy 10.554494877224826
epoch: 36, step: 88
	action: tensor([[ -6918.1019,   3912.1003, -20019.9597,  19719.5888, -12315.1290,
         -10273.1400,   8123.3204]], dtype=torch.float64)
	q_value: tensor([[-30.5900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0851577434262889, distance: 1.1920737649242619 entropy 10.701295272531139
epoch: 36, step: 89
	action: tensor([[-3843.8851, -5023.1421, 15312.0791,  6107.6722,  3118.4243,   953.3479,
         -5952.7698]], dtype=torch.float64)
	q_value: tensor([[-30.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7451941145258627, distance: 1.5117450774312635 entropy 10.516405012230473
epoch: 36, step: 90
	action: tensor([[-14092.0971,  -5285.7063,   1388.1320,   2565.4073,  -3445.1512,
          -1358.8012,   6299.5197]], dtype=torch.float64)
	q_value: tensor([[-27.3748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2022444356188382, distance: 1.2547380873282739 entropy 10.615567829010981
epoch: 36, step: 91
	action: tensor([[ -4884.5381,   4121.6590,   1268.3306,  13380.3127, -16336.6822,
         -15607.3524,  -3309.0576]], dtype=torch.float64)
	q_value: tensor([[-25.4645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4317576984347934, distance: 0.8626278467357918 entropy 10.626890834092235
epoch: 36, step: 92
	action: tensor([[ 16451.8039,   3069.3361, -11945.2337,   4016.3087,   4531.3602,
           2620.2186,   9197.0720]], dtype=torch.float64)
	q_value: tensor([[-33.1676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2830357515067079, distance: 0.9689590945103579 entropy 10.60886263323076
epoch: 36, step: 93
	action: tensor([[-2508.4574, -9525.1117,  5687.9349, 11034.9706, -1768.5229, -3267.7816,
         -7002.5980]], dtype=torch.float64)
	q_value: tensor([[-29.2299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14878112266496535, distance: 1.055789446224181 entropy 10.44325227513917
epoch: 36, step: 94
	action: tensor([[-14012.5451, -10387.7050,  -1659.7226,  22802.0484,  -9870.5307,
         -10983.0209,  14378.0259]], dtype=torch.float64)
	q_value: tensor([[-31.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8169458418096358, distance: 1.5425089211001748 entropy 10.799618926819218
epoch: 36, step: 95
	action: tensor([[ 1196.0715, -2860.7020, -2776.1068,  4350.9282,  4978.9726, -5983.8892,
          1336.7602]], dtype=torch.float64)
	q_value: tensor([[-26.8332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49761746549951025, distance: 0.8110991748218009 entropy 10.521564576464609
epoch: 36, step: 96
	action: tensor([[-24264.3601,   -670.9801,  -2591.6073,  -6239.4150, -19768.7666,
          -4250.2788,  18391.9651]], dtype=torch.float64)
	q_value: tensor([[-32.8252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7996641354213356, distance: 1.535155681467925 entropy 10.510267017114577
epoch: 36, step: 97
	action: tensor([[  -553.7770,  -1889.8419,  -1285.9958, -12190.8650,   4327.1278,
           3082.6979,  -1090.9727]], dtype=torch.float64)
	q_value: tensor([[-26.1790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18725390003908005, distance: 1.2468910157269417 entropy 10.490349067888186
epoch: 36, step: 98
	action: tensor([[-1.2874e+01, -1.3556e+04,  4.1580e+03,  2.3373e+03, -1.9707e+04,
         -9.4000e+03,  1.5189e+04]], dtype=torch.float64)
	q_value: tensor([[-28.5548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4963771541122064, distance: 1.399836224235924 entropy 10.5714167204121
epoch: 36, step: 99
	action: tensor([[-2.0879e+04, -1.2567e+04, -1.2151e+04, -1.3282e+04, -1.3125e+00,
         -3.4307e+03, -4.4637e+02]], dtype=torch.float64)
	q_value: tensor([[-30.5146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03421929541217039, distance: 1.1245945006405762 entropy 10.781746677907012
epoch: 36, step: 100
	action: tensor([[ -7218.0130,  -6927.2863,  -7056.7333,   7925.8824,   1482.4454,
         -15925.0838,  11357.0470]], dtype=torch.float64)
	q_value: tensor([[-28.8636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6207905293835629, distance: 1.456867779557821 entropy 10.483732393397563
epoch: 36, step: 101
	action: tensor([[-9076.8595, -4322.9644,  2473.8779,  -717.8167, -2701.1102, -6485.3430,
          -693.5266]], dtype=torch.float64)
	q_value: tensor([[-24.9798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20271362670779658, distance: 1.021795213655346 entropy 10.420045359417772
epoch: 36, step: 102
	action: tensor([[-1.3552e+04, -2.8354e+04,  6.3454e+02, -1.8368e+01, -7.4369e+03,
          3.9381e+03, -2.6597e+04]], dtype=torch.float64)
	q_value: tensor([[-32.6763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11523525346829167, distance: 1.0763923392111718 entropy 10.615337563637398
epoch: 36, step: 103
	action: tensor([[-16960.7592, -12801.2403,   5981.1946,   5895.9651,  -9923.9236,
           7892.0612,  -6079.7867]], dtype=torch.float64)
	q_value: tensor([[-26.2882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9356685755803378, distance: 1.592106784667145 entropy 10.349930624564054
epoch: 36, step: 104
	action: tensor([[ -9284.6188,  -7361.1184, -13930.3084,  -1140.4592,    549.1698,
          -1459.4201,   1000.0866]], dtype=torch.float64)
	q_value: tensor([[-29.6141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9416674607696127, distance: 1.594571947747102 entropy 10.695665729040698
epoch: 36, step: 105
	action: tensor([[ -5411.2944,   -188.3314,    902.1285,    207.4197, -25869.3171,
           2089.0170,  -6441.5275]], dtype=torch.float64)
	q_value: tensor([[-27.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5930294235088407, distance: 1.4443371832931322 entropy 10.476453751916063
epoch: 36, step: 106
	action: tensor([[ -7911.9661, -18604.5506,   1022.3018,  -3692.6518, -14730.8138,
          10763.0836,   4889.6304]], dtype=torch.float64)
	q_value: tensor([[-25.0853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15855553655428423, distance: 1.0497102111612655 entropy 10.489554172991276
epoch: 36, step: 107
	action: tensor([[ -4848.1318, -14056.0566,  -5061.2041,   3178.3486,   2335.2062,
           2983.8140,   9308.1437]], dtype=torch.float64)
	q_value: tensor([[-34.4266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5744153793704345, distance: 1.435874077263433 entropy 10.664866180148854
epoch: 36, step: 108
	action: tensor([[-15100.9015, -17975.0961,  13917.7853,   8469.0457, -12715.4311,
           5836.7844,   5965.0767]], dtype=torch.float64)
	q_value: tensor([[-30.1197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03105730895889669, distance: 1.1264339693463015 entropy 10.703836392195141
epoch: 36, step: 109
	action: tensor([[  560.1865, -3078.7538,  1000.9608, -6332.0100,  3800.0371,  4645.9949,
         -3112.6542]], dtype=torch.float64)
	q_value: tensor([[-30.3932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21686550979115837, distance: 1.0126861465120964 entropy 10.597094324910246
epoch: 36, step: 110
	action: tensor([[-6696.8050, -1181.5982,  -693.8169, 15478.1256, -6672.8181, -7770.2929,
          3916.9718]], dtype=torch.float64)
	q_value: tensor([[-27.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5508837295368447, distance: 1.425103193075359 entropy 10.423773165512879
epoch: 36, step: 111
	action: tensor([[ -5147.9769,  -4269.6203,  -4739.2315,  -3168.0131,   2173.3234,
         -14285.6509,   5968.5585]], dtype=torch.float64)
	q_value: tensor([[-29.6188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3170484308705612, distance: 1.3132807120551995 entropy 10.726645482172936
epoch: 36, step: 112
	action: tensor([[ -8249.3570, -14433.4069,  -4257.0767,   2350.6426,  -2804.0025,
          -2342.4336,  11537.5487]], dtype=torch.float64)
	q_value: tensor([[-26.6768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3794973862458697, distance: 1.344055289129018 entropy 10.425521908990946
epoch: 36, step: 113
	action: tensor([[ -8549.0451,  -6131.1435,  18087.7796,  24377.8809,   5122.9737,
         -14383.3724,  -4240.7981]], dtype=torch.float64)
	q_value: tensor([[-29.9649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1452408295783567, distance: 1.0579827279846918 entropy 10.677709077462064
epoch: 36, step: 114
	action: tensor([[  3193.4598, -13029.3721,  -9682.3188,  10146.0793,   -412.5898,
           6480.8140,  -6775.3528]], dtype=torch.float64)
	q_value: tensor([[-26.6074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38916013321906306, distance: 0.8943765013846153 entropy 10.5882024922027
epoch: 36, step: 115
	action: tensor([[-7897.0196, -3562.6869, -9871.8111,  -972.6549, -3952.2515,  7611.8475,
          1285.3130]], dtype=torch.float64)
	q_value: tensor([[-30.0193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0028775052665306, distance: 1.619510946204205 entropy 10.510449333500324
epoch: 36, step: 116
	action: tensor([[ -1888.9377,  -5347.9450,   5996.9155,   3690.3949, -19769.4294,
          -1176.0610,  -2037.6088]], dtype=torch.float64)
	q_value: tensor([[-25.1505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12146394684418382, distance: 1.2118513512206288 entropy 10.413505995721632
epoch: 36, step: 117
	action: tensor([[ 11588.9126, -19273.7751, -13899.6749,  21017.0111,  -6872.9312,
          22547.0461,   5576.3703]], dtype=torch.float64)
	q_value: tensor([[-31.4566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5695938666673317, distance: 0.750750999379025 entropy 10.870060207845134
epoch: 36, step: 118
	action: tensor([[-6597.0794, -4834.7599,  1350.8487, 12105.4575, 11589.3367,  4651.8308,
         15656.8182]], dtype=torch.float64)
	q_value: tensor([[-31.1830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31243375446725175, distance: 1.3109779556527814 entropy 10.576454347940127
epoch: 36, step: 119
	action: tensor([[-14906.0519,   7863.9149, -16660.2712,   5863.2832,  -6670.4644,
          11586.2186,  -7732.2896]], dtype=torch.float64)
	q_value: tensor([[-27.9983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41305431893506805, distance: 0.8767093714734376 entropy 10.666389132625266
epoch: 36, step: 120
	action: tensor([[ 5784.3669,  8919.1538, -4816.9943,  9548.0198, 13473.0512,  2684.7340,
         -8408.5620]], dtype=torch.float64)
	q_value: tensor([[-34.5336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.843372492158715, distance: 0.452887894312121 entropy 10.642078301773955
epoch: 36, step: 121
	action: tensor([[   933.1609,   8381.0244,   2604.7293,   6664.5260, -12576.7937,
          17184.5102,  15929.3345]], dtype=torch.float64)
	q_value: tensor([[-32.2021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8237587821731643, distance: 0.4804082267104154 entropy 10.494158807709377
epoch: 36, step: 122
	action: tensor([[ -1411.4280,   6532.8488,  -8102.6555,   2137.7752,  -4704.7993,
         -15715.8446,   5067.7966]], dtype=torch.float64)
	q_value: tensor([[-35.7783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7811697198692353, distance: 0.5353162185016157 entropy 10.654755651855526
epoch: 36, step: 123
	action: tensor([[-13426.9103,  -1248.5188, -13798.7130,   5046.1297,  -2107.0654,
         -10255.6846,  -3794.2906]], dtype=torch.float64)
	q_value: tensor([[-34.2862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033826550061917704, distance: 1.1248231417620251 entropy 10.586511626607868
epoch: 36, step: 124
	action: tensor([[-8333.5118, -7100.6500, -7805.4312,  6931.6919, -7422.8397, 10688.0290,
          7050.6548]], dtype=torch.float64)
	q_value: tensor([[-29.5826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5493146896246606, distance: 1.4243821172178102 entropy 10.734543718409247
epoch: 36, step: 125
	action: tensor([[-1831.9766, 10909.9159,  9172.2835,  1642.0738, -4206.2141, -2855.1537,
         -7050.2307]], dtype=torch.float64)
	q_value: tensor([[-28.7971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32668585734704136, distance: 0.9389999337597517 entropy 10.676700152475403
epoch: 36, step: 126
	action: tensor([[-28859.2644,  -3632.9334,  -5103.2954,   7313.6237,  -8027.2141,
          -7454.2741,  -7046.9001]], dtype=torch.float64)
	q_value: tensor([[-32.8562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10667033191563025, distance: 1.0815897726440167 entropy 10.637199095132479
epoch: 36, step: 127
	action: tensor([[  5027.7228, -19745.5353, -21537.8883,  21921.9334,  10115.3469,
          -9583.8015,   -355.8777]], dtype=torch.float64)
	q_value: tensor([[-29.1191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22811382611398845, distance: 1.0053871362423483 entropy 10.732864915628795
LOSS epoch 36 actor 391.09002099671943 critic 116.62760603835017
epoch: 37, step: 0
	action: tensor([[ 1328.2387, -5511.3870, -2579.7814, -7017.3293, 14234.5353, -2810.3559,
         -6035.1237]], dtype=torch.float64)
	q_value: tensor([[-24.6012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31547608487321555, distance: 0.9467842093794258 entropy 10.492587807981282
epoch: 37, step: 1
	action: tensor([[  2683.6288,  -5807.0150, -17768.6233,   -530.8840,  17716.2007,
          24893.9942,   2851.7020]], dtype=torch.float64)
	q_value: tensor([[-29.3887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5436467169913182, distance: 0.7730494630851494 entropy 10.81276587561715
epoch: 37, step: 2
	action: tensor([[ -4469.2317, -16539.9280,  -4018.0462,  -8370.2186,   4546.3516,
           8697.0108,   7160.1707]], dtype=torch.float64)
	q_value: tensor([[-24.7935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32030852901061646, distance: 0.9434363458427205 entropy 10.555871222305557
epoch: 37, step: 3
	action: tensor([[ -3865.1426,  -4249.4279,  13287.0009,   9407.3745,  -2297.6369,
         -20509.1675, -10040.5322]], dtype=torch.float64)
	q_value: tensor([[-27.7160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25858022201986075, distance: 1.283799329833005 entropy 10.763268592124764
epoch: 37, step: 4
	action: tensor([[-10200.2657,   6207.1025,   5669.6568,   4008.6704,   -656.3316,
          11513.9147,  -8661.3775]], dtype=torch.float64)
	q_value: tensor([[-20.9610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01567864484658199, distance: 1.1353379292214096 entropy 10.563385584017263
epoch: 37, step: 5
	action: tensor([[ -1494.0379,  -7228.0472, -10606.0784,   7897.4585,   6934.2697,
           4611.9287,   1808.3300]], dtype=torch.float64)
	q_value: tensor([[-27.9796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4131097508012389, distance: 1.3603311400694835 entropy 10.81090280034199
epoch: 37, step: 6
	action: tensor([[ -1958.1226,  12313.4375, -10683.4738,  -1255.7382,   4296.1904,
          16047.4156,  12336.2910]], dtype=torch.float64)
	q_value: tensor([[-24.1107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6269521937415546, distance: 0.6989384595671488 entropy 10.6905845544612
epoch: 37, step: 7
	action: tensor([[ 13140.4125, -10822.5194,   8944.5908,  14643.1996, -13288.1528,
          -8670.2946,  -3534.4248]], dtype=torch.float64)
	q_value: tensor([[-25.7520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4013089935718508, distance: 1.354639228181155 entropy 10.678044980260541
epoch: 37, step: 8
	action: tensor([[-18172.0185,  -2573.3993,   1235.2835,   5971.0916,  -5901.8306,
          21242.0749,   3088.5821]], dtype=torch.float64)
	q_value: tensor([[-24.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0689675128219005, distance: 1.1831476531589602 entropy 10.652912592766898
epoch: 37, step: 9
	action: tensor([[  3312.4422, -30265.5872,  -5831.8817,   4017.7699,  19533.0634,
           -564.4006, -11199.7423]], dtype=torch.float64)
	q_value: tensor([[-26.9975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6895111037717084, distance: 0.6376461326223952 entropy 10.88269548414861
epoch: 37, step: 10
	action: tensor([[ -8015.3359,  -8715.6655,   4435.4611,   -529.6369,   4325.5295,
          16774.3112, -16480.5294]], dtype=torch.float64)
	q_value: tensor([[-22.5849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.057160828128212327, distance: 1.1765956123089374 entropy 10.666784606834062
epoch: 37, step: 11
	action: tensor([[-15493.5996, -15425.8453, -13546.1775,  21659.9187,   3810.0639,
           4997.1075,   4510.4400]], dtype=torch.float64)
	q_value: tensor([[-21.2210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28715911443539377, distance: 1.2982932866904948 entropy 10.495679467948232
epoch: 37, step: 12
	action: tensor([[ 3077.5014, -6487.8519, -9648.2239, -7416.1180,  1032.6603, -1507.9460,
         10512.3660]], dtype=torch.float64)
	q_value: tensor([[-23.0991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20919153497387755, distance: 1.017635732363939 entropy 10.594161778861563
epoch: 37, step: 13
	action: tensor([[ 13970.6255,  -6469.0473,  -3525.9653, -13016.9891,  -6843.8604,
           7888.8974,   1807.0204]], dtype=torch.float64)
	q_value: tensor([[-24.1767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4786927510401938, distance: 0.8262349755687012 entropy 10.410109997857017
epoch: 37, step: 14
	action: tensor([[-2361.3812, -1898.9077, 17535.2047, -6480.6860, -6381.1593, -6532.5645,
          8670.6314]], dtype=torch.float64)
	q_value: tensor([[-24.3277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0031006453251445354, distance: 1.146116983742754 entropy 10.566597426386855
epoch: 37, step: 15
	action: tensor([[-12223.3105, -11864.7922, -18276.1053,   5050.0553,   4888.7484,
          -1483.1926, -13440.9472]], dtype=torch.float64)
	q_value: tensor([[-24.5363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8829017059947015, distance: 1.570256203205239 entropy 10.603474339059677
epoch: 37, step: 16
	action: tensor([[-6611.5708, -7450.2422, -3268.3352,  8768.3283,  2224.3017,  3523.8720,
         10882.3324]], dtype=torch.float64)
	q_value: tensor([[-18.3705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17198719957998532, distance: 1.0412984420381644 entropy 10.301735522126268
epoch: 37, step: 17
	action: tensor([[  971.4369,   155.9982, -5255.1194,  1981.0893, -8714.5703, -3881.9175,
          3113.5613]], dtype=torch.float64)
	q_value: tensor([[-21.5897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6189008030332253, distance: 0.7064406965836563 entropy 10.515439908018008
epoch: 37, step: 18
	action: tensor([[-7913.1738, 11863.7924, 18114.1749, 12489.2277,  3797.4875, 14624.8787,
         -9495.1686]], dtype=torch.float64)
	q_value: tensor([[-21.2983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34111922832016695, distance: 0.9288810651565043 entropy 10.477743835802144
epoch: 37, step: 19
	action: tensor([[-12714.5265,  -2899.2285, -14506.6065,   2052.4661, -12925.6682,
           -254.8248,  18061.7364]], dtype=torch.float64)
	q_value: tensor([[-25.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03497732786931085, distance: 1.164185300532919 entropy 10.55185657709423
epoch: 37, step: 20
	action: tensor([[  2045.3654,  -4304.5938,   3797.5376,  -6527.9575, -31055.9410,
          -9491.7717,  21919.6140]], dtype=torch.float64)
	q_value: tensor([[-20.0696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4831897095723303, distance: 0.8226635768982345 entropy 10.590665119301972
epoch: 37, step: 21
	action: tensor([[  4277.6381,   8341.2719,   5946.0702,  15098.5023,  -4436.2309,
         -11897.6188,  14958.6303]], dtype=torch.float64)
	q_value: tensor([[-23.3960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3864536622527742, distance: 0.8963556851477293 entropy 10.418474757535474
epoch: 37, step: 22
	action: tensor([[-4.5206e+00,  6.1535e+02,  1.8154e+04,  1.7295e+04, -1.7641e+03,
         -1.7167e+03, -9.5212e+03]], dtype=torch.float64)
	q_value: tensor([[-27.2677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15744547436152834, distance: 1.050402389680101 entropy 10.443494526358487
epoch: 37, step: 23
	action: tensor([[-16221.5450,  -2696.3997, -15897.9163,  -8334.7293,  -4344.1205,
         -10523.5235,   -186.2216]], dtype=torch.float64)
	q_value: tensor([[-27.6074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6921300040622038, distance: 1.488584718806207 entropy 10.465806345655752
epoch: 37, step: 24
	action: tensor([[-18618.3632,   2873.6557, -14725.7680,  11878.3915,   8366.6117,
          -6698.5126, -19803.9583]], dtype=torch.float64)
	q_value: tensor([[-26.8829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05128866091657813, distance: 1.1146120629958227 entropy 10.722497501346968
epoch: 37, step: 25
	action: tensor([[ 17689.9150,  -9851.1794,   7954.3887,  -6602.8773,  14212.3044,
           5727.2860, -15084.8862]], dtype=torch.float64)
	q_value: tensor([[-28.0676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.71070560221729
epoch: 37, step: 26
	action: tensor([[ -7449.6083, -12660.0221,  -1371.1960,   1214.7787,  -4730.5285,
            576.6131,   4775.2144]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21970990789744715, distance: 1.0108454008409304 entropy 10.102850280637927
epoch: 37, step: 27
	action: tensor([[  -227.5245, -19793.3875,  -6713.7474,  -4524.6358,   5731.2006,
          -4404.4309,    161.2868]], dtype=torch.float64)
	q_value: tensor([[-22.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07369311680254576, distance: 1.1013722728496294 entropy 10.54554045858942
epoch: 37, step: 28
	action: tensor([[  180.4616, -7047.3782, -5649.7575, 10112.0415,  2179.3339, -6295.6016,
         -4324.1893]], dtype=torch.float64)
	q_value: tensor([[-22.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38263085462271185, distance: 0.8991437992683461 entropy 10.564901782239929
epoch: 37, step: 29
	action: tensor([[ -9694.3022, -13933.8893,  -8097.7108,  -2009.4468,  -5162.1861,
           4418.8523, -15804.2132]], dtype=torch.float64)
	q_value: tensor([[-21.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0044790806643870695, distance: 1.1417785726915732 entropy 10.570588370619296
epoch: 37, step: 30
	action: tensor([[  862.6390, -4600.2822,  3144.5464, 10915.2832, -7840.7545, 18500.3648,
          4444.8772]], dtype=torch.float64)
	q_value: tensor([[-23.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28064271979467625, distance: 0.9705748084241529 entropy 10.621293933103269
epoch: 37, step: 31
	action: tensor([[ 6920.7706,  4505.4783,  3888.0111, -1261.1330, -6242.2987,  2389.5031,
         16288.0273]], dtype=torch.float64)
	q_value: tensor([[-28.3675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6548242889924902, distance: 0.6723212022277362 entropy 10.569085522477618
epoch: 37, step: 32
	action: tensor([[-6528.9366,  4897.1175, -4745.5736, 18304.8228,  -383.1940, 17061.0921,
         -5649.9610]], dtype=torch.float64)
	q_value: tensor([[-23.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4811047293658176, distance: 0.8243213523249731 entropy 10.530820671401008
epoch: 37, step: 33
	action: tensor([[ -9537.3734, -16890.8849,  -8704.4743,  -3477.4106, -13480.6632,
           2833.9707,  10433.9783]], dtype=torch.float64)
	q_value: tensor([[-24.1595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2321867293311286, distance: 1.0027311355189203 entropy 10.58620020849086
epoch: 37, step: 34
	action: tensor([[ -8279.2213,   8911.9847, -21146.0853,  17306.9512,   6625.9621,
         -16506.5345,   1859.4038]], dtype=torch.float64)
	q_value: tensor([[-27.1875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2458962658411593, distance: 0.9937387816628165 entropy 10.796051298360897
epoch: 37, step: 35
	action: tensor([[-12132.1642,  -9077.0463,   5423.4941,  21886.3463,  -2384.3169,
         -15203.5457,  10626.0051]], dtype=torch.float64)
	q_value: tensor([[-30.6184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2558783036101283, distance: 0.9871398332721651 entropy 10.833585117641935
epoch: 37, step: 36
	action: tensor([[ 2419.9110, -7388.0530,   -42.0518,  2987.3784, 10130.5307, -7088.8201,
         -2385.0815]], dtype=torch.float64)
	q_value: tensor([[-24.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43118415747712, distance: 0.8630630727137644 entropy 10.758268731996461
epoch: 37, step: 37
	action: tensor([[-17584.4548,   5626.0830,   9702.6677, -18088.3136,   4815.5122,
            482.2542,   1363.5611]], dtype=torch.float64)
	q_value: tensor([[-30.3433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.596896624668412
epoch: 37, step: 38
	action: tensor([[ -2946.8835, -14066.6638,   2142.1279,  13825.4364,   4778.7158,
          -1235.8251,   1290.9673]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1346410252167597, distance: 1.0645224978836365 entropy 10.102850280637927
epoch: 37, step: 39
	action: tensor([[-15176.2502,  -4840.5605,   6713.3607,  13243.5288,  -1506.8690,
           2594.3138,   9876.5764]], dtype=torch.float64)
	q_value: tensor([[-23.2003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39166499298216517, distance: 1.3499697739212948 entropy 10.730651360495141
epoch: 37, step: 40
	action: tensor([[-13266.7109, -13119.9230,  16522.4236,   6165.0512,  -9321.8543,
           5662.6960,  10330.6893]], dtype=torch.float64)
	q_value: tensor([[-22.9324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7271909449532183, distance: 1.5039273920695115 entropy 10.657458468697973
epoch: 37, step: 41
	action: tensor([[-10789.7694,   2988.0398,  -3765.9568,  10611.7167,  -9184.0801,
          -4017.9046,   3488.4516]], dtype=torch.float64)
	q_value: tensor([[-25.4226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06835121421412715, distance: 1.1045434494973023 entropy 10.73931740081064
epoch: 37, step: 42
	action: tensor([[ -9628.7526, -14963.8807,  -9731.4838,   6037.5085,  -3338.8056,
           5043.2155,  -5890.1355]], dtype=torch.float64)
	q_value: tensor([[-27.4814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029308883016786025, distance: 1.1609928728007688 entropy 10.67013516062501
epoch: 37, step: 43
	action: tensor([[-16248.8857, -27728.0122, -20065.9826,   -492.6740,  11957.3880,
         -11086.9825,  10986.6430]], dtype=torch.float64)
	q_value: tensor([[-26.4111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9525718076038618, distance: 1.5990432132797074 entropy 10.823929149863929
epoch: 37, step: 44
	action: tensor([[ -4824.2312,  -5128.7999, -11999.7480,  -9480.6655,  -3808.3307,
           5249.2045,  14071.2410]], dtype=torch.float64)
	q_value: tensor([[-22.2590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8211655369099262, distance: 1.5442990522294004 entropy 10.539304841490111
epoch: 37, step: 45
	action: tensor([[-26539.6928,  -7477.3414,  -5370.1245,  18130.7733,   5040.1785,
           5217.7086,   9256.2842]], dtype=torch.float64)
	q_value: tensor([[-22.1950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1563215034219667, distance: 1.230540733342847 entropy 10.634436487173858
epoch: 37, step: 46
	action: tensor([[-13284.4529,  -5122.5185,  -3930.3378,   7238.2007,  -9009.6711,
          11594.1798,   3567.3525]], dtype=torch.float64)
	q_value: tensor([[-21.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0264337076934531, distance: 1.1593702343343906 entropy 10.446973713148017
epoch: 37, step: 47
	action: tensor([[ -2073.6684, -18988.6867,   8596.8044,  23194.4815,   5633.1066,
           1361.8757,  11393.8346]], dtype=torch.float64)
	q_value: tensor([[-24.9993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26424068454237815, distance: 1.2866830338166102 entropy 10.629289613405053
epoch: 37, step: 48
	action: tensor([[-11031.7280, -22211.1620,   5460.8600,    689.7123,   5563.4892,
          13758.6514,  11923.2063]], dtype=torch.float64)
	q_value: tensor([[-26.4699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4350280535050228, distance: 1.3708403805942715 entropy 10.722689173016851
epoch: 37, step: 49
	action: tensor([[-17393.5607, -26067.2729,   3447.6680,   1090.3611,  24405.2804,
          -8043.1880,  -2959.6172]], dtype=torch.float64)
	q_value: tensor([[-25.6983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5288851929972611, distance: 1.4149598949698916 entropy 10.76850880234073
epoch: 37, step: 50
	action: tensor([[ -9558.5252,   2942.4221,   9595.3080,   9137.3818,  10827.9692,
         -11374.4016,  10238.8841]], dtype=torch.float64)
	q_value: tensor([[-20.0362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39361483981765455, distance: 0.8911092985949289 entropy 10.523435012288529
epoch: 37, step: 51
	action: tensor([[ -2816.8501, -14911.9840,  13931.7813,  10573.1065,  -7211.7888,
           5133.0513,   3040.9617]], dtype=torch.float64)
	q_value: tensor([[-26.8673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05691354286744488, distance: 1.1113028994600647 entropy 10.605348973222252
epoch: 37, step: 52
	action: tensor([[  1685.5779,   5493.6124, -23137.9096, -18467.7453,  -4122.5272,
          15719.7974,   5760.5265]], dtype=torch.float64)
	q_value: tensor([[-23.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.59003344725845
epoch: 37, step: 53
	action: tensor([[ -9497.7227,   4852.0998, -11043.1741,    494.0114,   3181.5425,
           9492.8281,  -7690.5952]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06290422736610801, distance: 1.1797874184516537 entropy 10.102850280637927
epoch: 37, step: 54
	action: tensor([[ -7795.0176,  -9770.8739,  22938.1368,  -7216.7238, -18469.9736,
           8171.2026,   1047.2118]], dtype=torch.float64)
	q_value: tensor([[-28.2720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3099763856763851, distance: 1.3097500591182136 entropy 10.697726617518526
epoch: 37, step: 55
	action: tensor([[ 5211.6475, 10166.8480, -2875.5241,   232.3324, -2470.2452, -9275.0594,
         -1531.8576]], dtype=torch.float64)
	q_value: tensor([[-23.8386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.607636871961642
epoch: 37, step: 56
	action: tensor([[ 5506.2817, -7170.0284,  8620.0250,  4485.0660,   128.7107, -6535.1317,
          8513.3608]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3557930685951807, distance: 0.9184793359377054 entropy 10.102850280637927
epoch: 37, step: 57
	action: tensor([[ 2099.2752, -6125.1682, -8968.2327, 21333.7141, -2287.3270, -3864.1666,
         12120.3159]], dtype=torch.float64)
	q_value: tensor([[-24.2429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022701657296230193, distance: 1.1572606152238885 entropy 10.453982783517635
epoch: 37, step: 58
	action: tensor([[-19176.2329,  -4852.0799,   3493.7492,  11875.3432,  -1367.0747,
          -6747.3490,  -8245.3383]], dtype=torch.float64)
	q_value: tensor([[-23.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0648006405146897, distance: 1.1066461911965602 entropy 10.50204142164841
epoch: 37, step: 59
	action: tensor([[ 1412.0637,  4804.0875, -1185.4926, -3871.0879,  7691.6404,  1755.9736,
          7299.6835]], dtype=torch.float64)
	q_value: tensor([[-22.9636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.606892359753308
epoch: 37, step: 60
	action: tensor([[  4945.5243, -13268.9415,   8435.9891,   7120.7363,  -5724.7434,
           -342.1537,  -5967.9952]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028005304158648214, distance: 1.1282066123429781 entropy 10.102850280637927
epoch: 37, step: 61
	action: tensor([[-13870.1204,  -6227.1798,   8268.1431,   3032.4536,   8581.4710,
           5931.8722, -10176.6463]], dtype=torch.float64)
	q_value: tensor([[-21.6381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33988512430451356, distance: 1.3246174623543765 entropy 10.542532828250492
epoch: 37, step: 62
	action: tensor([[ -4459.4368, -26473.8468,  -9843.4735,  24507.3625,  -3784.0398,
          14956.1428,  -2146.9805]], dtype=torch.float64)
	q_value: tensor([[-21.9731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47328587721390103, distance: 1.3889934768265504 entropy 10.62790852612086
epoch: 37, step: 63
	action: tensor([[-15174.7565,  -5515.7737,  -5016.2471,   8496.3164,  -7731.3689,
          -3680.4418,   2583.4283]], dtype=torch.float64)
	q_value: tensor([[-23.4476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16580380524678928, distance: 1.0451792895288319 entropy 10.634252511808853
epoch: 37, step: 64
	action: tensor([[-14270.4488,   8569.0167,   4858.9331,   4973.4730,  -1205.8886,
         -15563.8080,  13159.0990]], dtype=torch.float64)
	q_value: tensor([[-23.6950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5411119879916977, distance: 0.7751933695897844 entropy 10.765050171417002
epoch: 37, step: 65
	action: tensor([[ -5645.5905, -15418.8092, -10440.4174,  14996.7883,   2858.7206,
           4799.9222,   6900.3493]], dtype=torch.float64)
	q_value: tensor([[-25.8034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.525095803325765
epoch: 37, step: 66
	action: tensor([[ -2755.4561, -11705.7564,   8123.2455,  -2747.8577,   6295.6671,
           3031.9131,   4158.5785]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.102850280637927
epoch: 37, step: 67
	action: tensor([[-13448.6669,  -1733.2340, -10706.8803,  -6274.4941,   5668.0984,
           3932.3062,   2810.6974]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19262211252329875, distance: 1.2497067683900835 entropy 10.102850280637927
epoch: 37, step: 68
	action: tensor([[  529.9972,  5995.6387,  -136.5505, 12395.9021,  1625.6839,  5027.5436,
         -2096.4812]], dtype=torch.float64)
	q_value: tensor([[-22.5674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.488815160052384
epoch: 37, step: 69
	action: tensor([[ 6449.3170, -6306.3236,  9082.4477,    50.2389,   536.7466, -7261.8594,
          6578.9419]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15923914651197246, distance: 1.2320922122144682 entropy 10.102850280637927
epoch: 37, step: 70
	action: tensor([[  7150.3823,  -3338.4825,  -2473.0426,  15932.8937,   -287.4737,
         -17186.9610,   7483.2999]], dtype=torch.float64)
	q_value: tensor([[-23.6606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18929482681417098, distance: 1.2479622779670743 entropy 10.5480493421661
epoch: 37, step: 71
	action: tensor([[ 3033.1471, -2737.0505, -6961.9179,  3417.2086, -5525.9032,  4176.0661,
         -1352.4848]], dtype=torch.float64)
	q_value: tensor([[-30.9470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2503345792429781, distance: 1.2795869857304367 entropy 10.47807812881272
epoch: 37, step: 72
	action: tensor([[-10135.8717,    206.6718,   7789.5704,  -1433.9495,  -6702.2363,
          -7210.6929,   2287.5930]], dtype=torch.float64)
	q_value: tensor([[-28.0922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.656114217267843
epoch: 37, step: 73
	action: tensor([[-10098.4040,   -978.1554,  -2899.5931,   6320.6886,  11696.3662,
           3837.0292, -11368.1116]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37213206976747903, distance: 1.340462443793714 entropy 10.102850280637927
epoch: 37, step: 74
	action: tensor([[-3.6994e+04, -1.3152e+04, -5.1213e+03,  8.1945e+03,  1.3069e+01,
         -6.6752e+03, -2.4672e+03]], dtype=torch.float64)
	q_value: tensor([[-26.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5389668650179908, distance: 1.419617446195439 entropy 10.896194053976615
epoch: 37, step: 75
	action: tensor([[ 20543.3672,  -9982.4385, -12899.4966,   -693.2539,  -7737.5411,
         -17791.8757,    131.5824]], dtype=torch.float64)
	q_value: tensor([[-23.2250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14842542428708427, distance: 1.056010014270575 entropy 10.656120049610346
epoch: 37, step: 76
	action: tensor([[ -1418.5527,  -8436.5499,   4342.7594,  10894.4619,  -3022.7847,
           -770.8455, -12035.3450]], dtype=torch.float64)
	q_value: tensor([[-23.2206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04889284071577682, distance: 1.1160185636813498 entropy 10.570029778362406
epoch: 37, step: 77
	action: tensor([[ -5419.3685, -20213.9705,  -9356.9818, -15507.8644,  -9361.8815,
          -4467.0199,   -216.9185]], dtype=torch.float64)
	q_value: tensor([[-25.2380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026834643182385376, distance: 1.128885808446272 entropy 10.67077373336353
epoch: 37, step: 78
	action: tensor([[-16600.3651,  -2753.1734,  -2242.7348,   4324.1243,  -3281.6509,
          23425.3862,    916.6765]], dtype=torch.float64)
	q_value: tensor([[-24.1385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8257461707120077, distance: 1.5462399592462137 entropy 10.626704450530935
epoch: 37, step: 79
	action: tensor([[ -1519.9340,  -3568.4000, -11816.7293,  -3954.6138, -19877.6606,
          -5912.3761,   2525.1286]], dtype=torch.float64)
	q_value: tensor([[-20.5710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9078229163419789, distance: 1.5806136343436057 entropy 10.53380687662467
epoch: 37, step: 80
	action: tensor([[-14204.2544,  -9297.8493,  -3178.1584,  10887.7529,  -1852.1270,
           3289.5019,   4617.6019]], dtype=torch.float64)
	q_value: tensor([[-22.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10357899827792116, distance: 1.0834595567776661 entropy 10.509976481645015
epoch: 37, step: 81
	action: tensor([[-1276.1356, -5787.5028,  1505.4735, -7131.1387, -8490.7224, 10229.2903,
         -2613.1987]], dtype=torch.float64)
	q_value: tensor([[-21.8457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3622399111229062, distance: 1.3356217826627745 entropy 10.504648001026116
epoch: 37, step: 82
	action: tensor([[ -1607.6903, -26905.0519, -10453.3120,  -2510.0449,    678.3009,
           2566.2860,  29357.8750]], dtype=torch.float64)
	q_value: tensor([[-23.7000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8279294264572346, distance: 1.5471641919549866 entropy 10.617067067418862
epoch: 37, step: 83
	action: tensor([[ 3500.3948,  8366.9474, 11521.6384,  1328.9184, -1486.9955,  1420.4043,
         -7534.3366]], dtype=torch.float64)
	q_value: tensor([[-22.7258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.622912908176506
epoch: 37, step: 84
	action: tensor([[  1115.6925,     82.0544, -10131.8411,   3933.3179,  12948.6644,
           8456.6002,   -905.3411]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.102850280637927
epoch: 37, step: 85
	action: tensor([[-2980.4842,  4863.7258, -4778.6873, -4443.2545,   360.6090,  1645.6707,
         -4975.3027]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.102850280637927
epoch: 37, step: 86
	action: tensor([[  993.6483, -6627.9165, -3318.6993, -5004.4156, 11299.3823,  1240.3586,
         -5158.1425]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6123133292853857, distance: 0.7125201128793502 entropy 10.102850280637927
epoch: 37, step: 87
	action: tensor([[ 2514.4860,   664.4855, -3916.2133, 16755.3772, -5013.0830,  3190.3975,
           401.5847]], dtype=torch.float64)
	q_value: tensor([[-22.5658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6809529071957309, distance: 0.6463743128459725 entropy 10.484731923282949
epoch: 37, step: 88
	action: tensor([[-10563.2113,   1824.8523,   4891.4519,   4669.2045,   2137.0844,
          -3752.9580,   5114.6797]], dtype=torch.float64)
	q_value: tensor([[-22.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.282697184589953
epoch: 37, step: 89
	action: tensor([[ 3930.2522, -4995.9867, -4027.0391, -1296.4118,  6152.4833,  -659.5366,
         -2537.6526]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49423841165802085, distance: 0.8138223532981662 entropy 10.102850280637927
epoch: 37, step: 90
	action: tensor([[    45.3405, -10796.6455,   9467.7030,   5526.0504,   6170.7763,
           9714.6715,   4179.5862]], dtype=torch.float64)
	q_value: tensor([[-24.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5442424610354767, distance: 0.7725447115789387 entropy 10.41126768447501
epoch: 37, step: 91
	action: tensor([[-7668.2993, -9694.7582, -4502.1607,   310.8151,  2517.5846, -3567.0337,
          7803.5195]], dtype=torch.float64)
	q_value: tensor([[-19.5140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23602676877757722, distance: 1.27224464476209 entropy 10.225732591560485
epoch: 37, step: 92
	action: tensor([[  3662.6471,   -691.6072, -27939.0888,    188.1297,  22555.8737,
           1289.9466,   7967.1425]], dtype=torch.float64)
	q_value: tensor([[-24.3412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45933485500324256, distance: 0.8414355946136206 entropy 10.671345016188342
epoch: 37, step: 93
	action: tensor([[ -5715.2388, -11928.6656,  -2451.0850,  -8800.7865,   6919.2525,
           -726.4783,  14127.8599]], dtype=torch.float64)
	q_value: tensor([[-28.1081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7541127053190069, distance: 1.5156029446365602 entropy 10.47218958557669
epoch: 37, step: 94
	action: tensor([[-10715.2038,   2736.0876,  -8707.5973,   1500.1901,  -8106.5847,
           2381.7409,   5231.7619]], dtype=torch.float64)
	q_value: tensor([[-20.4541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42022547194171167, distance: 0.8713372060786335 entropy 10.361337485198634
epoch: 37, step: 95
	action: tensor([[  3377.4542,  -6768.6679, -29486.0221,  16585.6276,   5233.6063,
           -450.8535,  10583.6186]], dtype=torch.float64)
	q_value: tensor([[-23.5488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23714428029941137, distance: 0.9994887189442746 entropy 10.502055113730458
epoch: 37, step: 96
	action: tensor([[-12389.9581,   -848.2877,  -7421.6913,  11076.1162,  -5025.9474,
          -4169.1184,  19568.9126]], dtype=torch.float64)
	q_value: tensor([[-25.4567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8016221564330157, distance: 1.5359905733445955 entropy 10.56721428460975
epoch: 37, step: 97
	action: tensor([[ -6500.9773,   6462.3179,   -344.4306,   1748.7678,   -476.6799,
         -17420.5464,    476.3501]], dtype=torch.float64)
	q_value: tensor([[-25.0757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6278659233226567, distance: 0.6980819576917702 entropy 10.697716710682341
epoch: 37, step: 98
	action: tensor([[   -87.0385,  -7556.8625,  -6250.3170, -11690.8573,   5669.2535,
         -15178.6943,  -3321.1196]], dtype=torch.float64)
	q_value: tensor([[-24.8652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8846211297412736, distance: 1.5709730010328729 entropy 10.515857282042864
epoch: 37, step: 99
	action: tensor([[-22645.2344,   3776.3053,  -7242.6036, -13182.4325,   6637.8669,
           3104.7465,  -4167.6799]], dtype=torch.float64)
	q_value: tensor([[-23.9817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06822686666919797, distance: 1.1046171589711216 entropy 10.633917336805666
epoch: 37, step: 100
	action: tensor([[-10384.8026, -12433.2517, -12132.5491,  -4680.6272,   1913.6041,
          -2959.8932,   3377.1593]], dtype=torch.float64)
	q_value: tensor([[-30.8499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6118931360533089, distance: 1.4528635101212828 entropy 10.665732165186508
epoch: 37, step: 101
	action: tensor([[  -99.0991,  6407.4741, -1759.2561,  7962.1786, -7871.7673, -1177.5766,
         17783.2258]], dtype=torch.float64)
	q_value: tensor([[-27.9578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07383960645299159, distance: 1.1012851818318437 entropy 10.766491524458504
epoch: 37, step: 102
	action: tensor([[ 3579.2742,  -963.7545,   541.6428,  1925.3799, 16253.1322, -8496.4404,
         11776.2640]], dtype=torch.float64)
	q_value: tensor([[-24.9636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.404233266441187
epoch: 37, step: 103
	action: tensor([[-6130.1791,  4520.7926, -2859.3890,  6119.8849,  3040.7700,   816.2717,
         -1967.2904]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.102850280637927
epoch: 37, step: 104
	action: tensor([[-1268.6551, -1697.2057, -1442.2721, 12590.8877, -1559.8059,  6422.1938,
          5536.0153]], dtype=torch.float64)
	q_value: tensor([[-25.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03692325413277198, distance: 1.165279215757678 entropy 10.102850280637927
epoch: 37, step: 105
	action: tensor([[-9932.9346,   631.7426, 13444.1698,  5847.2585,   149.9897, -4166.5241,
         -5834.8795]], dtype=torch.float64)
	q_value: tensor([[-24.3313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1534240109465983, distance: 1.2289980313821676 entropy 10.647292999590409
epoch: 37, step: 106
	action: tensor([[-15243.2546, -17618.5058,   4056.1531,   6561.8547,  -4861.8378,
           9430.6579,  22961.3268]], dtype=torch.float64)
	q_value: tensor([[-28.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.266202889588973, distance: 1.2876811653763165 entropy 10.731530904450361
epoch: 37, step: 107
	action: tensor([[ -8334.7586,   9092.2164,  19990.0545,  19403.7608,   6937.1166,
         -18282.1988,   7217.5779]], dtype=torch.float64)
	q_value: tensor([[-25.4850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13503427632030163, distance: 1.219161337351844 entropy 10.790992036134964
epoch: 37, step: 108
	action: tensor([[ 11696.9147, -12990.9108, -10721.4369,   6851.5829,   -233.8891,
           4116.0129,  -6450.6074]], dtype=torch.float64)
	q_value: tensor([[-28.5575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15082321087369333, distance: 1.2276116452772967 entropy 10.73889691082421
epoch: 37, step: 109
	action: tensor([[ 1530.8071,  5747.2766,  1479.9563,  2445.9443, 13086.0431,   132.1416,
         13364.5195]], dtype=torch.float64)
	q_value: tensor([[-24.4631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4652643381810596, distance: 0.8368088555385579 entropy 10.716639027796123
epoch: 37, step: 110
	action: tensor([[-7058.8707, -7001.4646, -7500.8662,  8941.9659,  8319.1413,  2590.1950,
         -9653.5220]], dtype=torch.float64)
	q_value: tensor([[-25.1656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17336907592946849, distance: 1.0404291640769985 entropy 10.652832683316008
epoch: 37, step: 111
	action: tensor([[ -1639.8560, -25654.0455, -11762.7373,   -515.4847, -10893.5791,
           8893.0439,  -1800.3863]], dtype=torch.float64)
	q_value: tensor([[-24.8134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4086239664616049, distance: 1.3581703020665108 entropy 10.555732302606861
epoch: 37, step: 112
	action: tensor([[ -1634.9953, -10761.5232,  -5563.8136, -10740.6081, -10121.7467,
         -20105.7927,    327.7556]], dtype=torch.float64)
	q_value: tensor([[-20.8037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -5.960167208129974e-05, distance: 1.1443783559073624 entropy 10.559696377916868
epoch: 37, step: 113
	action: tensor([[-15027.9479,   8566.6018,   4928.3534,  12925.2748,  -5002.4496,
         -13001.8933,   6520.8538]], dtype=torch.float64)
	q_value: tensor([[-22.4578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.040061953984278365, distance: 1.167041495725191 entropy 10.503320770015112
epoch: 37, step: 114
	action: tensor([[ -9241.6727, -23112.6884,  11206.8687,  12992.6066,  -2963.7596,
          -2231.7880,  -3986.9075]], dtype=torch.float64)
	q_value: tensor([[-23.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07533996598675008, distance: 1.1003927913462987 entropy 10.428852706843596
epoch: 37, step: 115
	action: tensor([[-11374.4464,  -9776.2438, -12502.4428,  13559.3277,  12568.2234,
          -1916.6035,  -8229.2220]], dtype=torch.float64)
	q_value: tensor([[-21.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.172560500108915, distance: 1.2391512613904183 entropy 10.672302252688288
epoch: 37, step: 116
	action: tensor([[-12523.3178, -13729.3072,   6620.7285,   5114.6127,  15836.1153,
          -5233.7701,  -4805.3993]], dtype=torch.float64)
	q_value: tensor([[-25.0634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14367905472821219, distance: 1.0589488344609383 entropy 10.591255534180274
epoch: 37, step: 117
	action: tensor([[ -2663.4975,  -3287.8866,  -9956.8209,   5119.7536,  -5591.2365,
         -13312.5033, -12346.6237]], dtype=torch.float64)
	q_value: tensor([[-23.8266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2710373028785269, distance: 1.290137032562674 entropy 10.748541724925563
epoch: 37, step: 118
	action: tensor([[ -7566.1482,   4605.5915,  -3757.6168,   2126.6016,   6361.3983,
         -18006.5260,  16504.2225]], dtype=torch.float64)
	q_value: tensor([[-23.6531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11010090211113022, distance: 1.0795110110910267 entropy 10.542516448013586
epoch: 37, step: 119
	action: tensor([[-9358.1114, -4146.5007, -4183.1817, -5009.0591, -1883.1068,  6079.7150,
         11939.9006]], dtype=torch.float64)
	q_value: tensor([[-26.7627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5626058183426919, distance: 1.4304787537249142 entropy 10.594493944000794
epoch: 37, step: 120
	action: tensor([[  1891.7533,  13339.4706,  -7076.1243,  -1414.4184, -11017.3409,
          14334.9009,   -499.7089]], dtype=torch.float64)
	q_value: tensor([[-23.3545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44015124142891726, distance: 0.8562331796215035 entropy 10.586640533250108
epoch: 37, step: 121
	action: tensor([[ -8862.6410, -19130.9490, -17237.6642,  13750.0215, -23625.5988,
          10758.9149,   -336.3108]], dtype=torch.float64)
	q_value: tensor([[-28.5814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1458391762889506, distance: 1.224950464244543 entropy 10.6624779023519
epoch: 37, step: 122
	action: tensor([[-20655.2150, -17666.7075,   7722.5628,  17045.7059,  24655.4024,
          -5412.6220,  -3900.2215]], dtype=torch.float64)
	q_value: tensor([[-23.3513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.029911360654196, distance: 1.6304039932314576 entropy 10.69488446428915
epoch: 37, step: 123
	action: tensor([[-8413.7315, -9842.1971,    31.1613, -3554.5820, -4822.8778,  4623.3507,
         27645.0742]], dtype=torch.float64)
	q_value: tensor([[-20.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10929049180552175, distance: 1.20525610087602 entropy 10.512999713149481
epoch: 37, step: 124
	action: tensor([[ 11208.6774,  -7232.2128,  -4914.3538, -14779.0508,   2920.7083,
           3909.9627,  -9493.1864]], dtype=torch.float64)
	q_value: tensor([[-21.2982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2482509600898013, distance: 1.2785203577724245 entropy 10.484416576295848
epoch: 37, step: 125
	action: tensor([[ 3279.9797, -6824.0287,  6144.9221, 11050.3033, -3709.2747, -4509.5464,
          3028.3310]], dtype=torch.float64)
	q_value: tensor([[-21.4558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2329529049213701, distance: 1.0022307144517046 entropy 10.353219053846386
epoch: 37, step: 126
	action: tensor([[  5326.1365,  -8868.7579, -12467.0210,  -3498.9332, -12927.8682,
          11953.3470,  -4253.1815]], dtype=torch.float64)
	q_value: tensor([[-22.7322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11460289844310279, distance: 1.0767769276178394 entropy 10.399853768040174
epoch: 37, step: 127
	action: tensor([[   205.2337,  -3574.3613,   2392.5707,   5971.5595,  18316.9278,
           2038.7321, -15019.7184]], dtype=torch.float64)
	q_value: tensor([[-23.9172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43882346400677996, distance: 0.8572479302876828 entropy 10.530432097657924
LOSS epoch 37 actor 249.33840132537892 critic 277.8639507676513
epoch: 38, step: 0
	action: tensor([[-18479.0094,  -7058.3664,  -1802.4819,  -4844.4908,   2932.3784,
          -3808.7343,   -681.0990]], dtype=torch.float64)
	q_value: tensor([[-20.4312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.519574735512006
epoch: 38, step: 1
	action: tensor([[-4948.8651,  4789.8414,  -474.0858,  8834.0736,  5949.1646, -5836.6964,
          1653.9029]], dtype=torch.float64)
	q_value: tensor([[-22.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06771411750640999, distance: 1.1824538122965436 entropy 10.155140047487347
epoch: 38, step: 2
	action: tensor([[-1937.7209, -1452.7390, -3971.9599,  -634.9472,  1227.9082,  7210.5221,
          5915.3952]], dtype=torch.float64)
	q_value: tensor([[-23.1438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3571471253009786, distance: 1.3331228083809408 entropy 10.707604582374325
epoch: 38, step: 3
	action: tensor([[ -2333.7545,  -8904.9978,  15726.4501, -11634.8765,   5033.4599,
         -24670.6491,  -1767.6129]], dtype=torch.float64)
	q_value: tensor([[-21.6736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07045368405649932, distance: 1.103296423265121 entropy 10.77585033972409
epoch: 38, step: 4
	action: tensor([[ -1432.8512, -11907.6713,   1323.6972,   3322.1990,  -8399.0984,
          -5348.3934,  11257.8857]], dtype=torch.float64)
	q_value: tensor([[-21.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3844329929557433, distance: 1.3464575427951104 entropy 10.571138651353738
epoch: 38, step: 5
	action: tensor([[-6125.2017, -4035.4387, -7686.7970,  9874.3818, 11615.1279,  1801.0973,
         -3078.5678]], dtype=torch.float64)
	q_value: tensor([[-19.0318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1911700209231817, distance: 1.2489457388097374 entropy 10.619420091028976
epoch: 38, step: 6
	action: tensor([[-25887.0100, -14320.0303,   9011.3105,   3668.8755,   6802.3169,
           3731.2093,  -8446.8215]], dtype=torch.float64)
	q_value: tensor([[-20.8939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29489892733210965, distance: 1.302190818291267 entropy 10.837502225511983
epoch: 38, step: 7
	action: tensor([[-14843.7844,  -2218.4461,  -7047.1638,  20993.6246,   2544.2194,
           8619.6596,  16710.8708]], dtype=torch.float64)
	q_value: tensor([[-23.3820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010497707265942235, distance: 1.1503350680890874 entropy 10.846350902137786
epoch: 38, step: 8
	action: tensor([[  2497.8098, -10159.3153,  -5389.4053,   5493.6238,   8383.1824,
           3454.1242,     31.5903]], dtype=torch.float64)
	q_value: tensor([[-22.5290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006426155147734258, distance: 1.1480152327339423 entropy 10.761563119731028
epoch: 38, step: 9
	action: tensor([[ -7768.1901,   -969.1815,   1285.3323,  -2124.2801,   1414.5507,
         -13984.4861,  -8361.6366]], dtype=torch.float64)
	q_value: tensor([[-22.4873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7845886853473365, distance: 1.5287123032483356 entropy 10.500432988605628
epoch: 38, step: 10
	action: tensor([[ -1857.0562, -19288.6894,   1226.4556,  -1842.2923,   1545.1980,
           6134.7604,  -6545.7015]], dtype=torch.float64)
	q_value: tensor([[-17.9334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8588733633453123, distance: 1.5602047486825241 entropy 10.459930449450038
epoch: 38, step: 11
	action: tensor([[-8880.2973, -7103.6972, 19760.3982,  -759.1678, -6024.1747,  1165.6244,
         14054.2557]], dtype=torch.float64)
	q_value: tensor([[-17.4350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05590528294505903, distance: 1.1758967083213165 entropy 10.478575338681143
epoch: 38, step: 12
	action: tensor([[ 5619.7999, -2376.2537, -2935.5913,  -912.6987,  7728.9560, -1665.4303,
           574.8448]], dtype=torch.float64)
	q_value: tensor([[-20.2014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15807248439105093, distance: 1.0500114740834672 entropy 10.522415254468354
epoch: 38, step: 13
	action: tensor([[ -9027.1670, -13757.6603,   9015.6890,  -1475.5163, -11937.6589,
          -4238.1553, -13159.9240]], dtype=torch.float64)
	q_value: tensor([[-18.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04209483958761018, distance: 1.119999811856461 entropy 10.440431147812813
epoch: 38, step: 14
	action: tensor([[-10736.1401, -11263.1821,  19936.2769,  -9824.5936,   6489.1718,
         -11530.1749,   6028.4756]], dtype=torch.float64)
	q_value: tensor([[-23.4302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018510840013928886, distance: 1.133703394469263 entropy 10.702778586006323
epoch: 38, step: 15
	action: tensor([[ -9458.0899, -10048.4866,   1829.5248,  13094.9770,  18649.6388,
           -299.5195, -27427.6154]], dtype=torch.float64)
	q_value: tensor([[-23.3842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9314663408987482, distance: 1.5903776556445666 entropy 10.791681158756573
epoch: 38, step: 16
	action: tensor([[  2572.3268,  -8130.8952, -13971.7844,   2804.7184,  -2116.1379,
           4271.1193,  12027.7811]], dtype=torch.float64)
	q_value: tensor([[-17.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08256298754239544, distance: 1.1906477088161236 entropy 10.461133968444368
epoch: 38, step: 17
	action: tensor([[-19073.0502,  -9822.3229,  -4879.7584,   -620.9830,  10570.7160,
           4857.9915, -13004.1952]], dtype=torch.float64)
	q_value: tensor([[-25.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8393540419277881, distance: 1.5519915729180114 entropy 10.616408926966358
epoch: 38, step: 18
	action: tensor([[ -2916.8944,  -2446.1647, -10782.6903,   8503.2210,  12743.7247,
          21073.8759,  -7150.3694]], dtype=torch.float64)
	q_value: tensor([[-20.8851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03539809368061242, distance: 1.123907970661525 entropy 10.640347393487142
epoch: 38, step: 19
	action: tensor([[-2447.9018, -4855.8344, -4449.6302,  -530.5838,  3482.6371, -7907.7382,
         17266.7039]], dtype=torch.float64)
	q_value: tensor([[-19.3525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5924458727783324, distance: 1.444072617803401 entropy 10.560389646812235
epoch: 38, step: 20
	action: tensor([[  843.0233, -3851.1108,  3743.8633, -2552.1580,  -606.1719, -9729.9191,
          -828.3820]], dtype=torch.float64)
	q_value: tensor([[-17.8790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02600041060475844, distance: 1.159125501152805 entropy 10.494905300176201
epoch: 38, step: 21
	action: tensor([[ 14036.2438,   2405.9412,  -1442.3627,  17177.2125,   7950.0625,
           6813.6799, -12084.7481]], dtype=torch.float64)
	q_value: tensor([[-21.1937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6627863342143425, distance: 0.664521866815441 entropy 10.687210853574285
epoch: 38, step: 22
	action: tensor([[-14093.2727,  -1543.7837,   -263.2279,  -4381.0151,   7679.9300,
          17510.5768,  -3934.2396]], dtype=torch.float64)
	q_value: tensor([[-19.8195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31068649577594876, distance: 0.9500907429870014 entropy 10.539923974580322
epoch: 38, step: 23
	action: tensor([[-15432.8814,  -5994.6008,   7923.1725,    574.5260,  11826.1181,
         -12307.9048,   6800.4583]], dtype=torch.float64)
	q_value: tensor([[-19.7909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.878291531013474, distance: 1.5683326847244785 entropy 10.506582680984023
epoch: 38, step: 24
	action: tensor([[  3636.2032, -11554.8197,   3892.1542,  11880.3996,  -1245.2562,
           -828.7264,  -2613.6293]], dtype=torch.float64)
	q_value: tensor([[-17.9716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30663542824648604, distance: 1.3080788027430141 entropy 10.462581635811677
epoch: 38, step: 25
	action: tensor([[-2885.7899, -7862.2902, -6792.5596, -9298.9206,  7065.0685, -2176.4073,
          3690.7180]], dtype=torch.float64)
	q_value: tensor([[-21.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.762229976579823, distance: 1.5191056729102446 entropy 10.503059134127353
epoch: 38, step: 26
	action: tensor([[-22044.8587, -14461.2565,  -3174.9932,     44.9611,   2583.5271,
         -16792.2062,   1580.7809]], dtype=torch.float64)
	q_value: tensor([[-19.0964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6174652143392823, distance: 1.4553725117697471 entropy 10.577847578742462
epoch: 38, step: 27
	action: tensor([[-14898.1992,   3192.8634,   6754.1045,   2657.6189,   4606.5173,
          -1171.9737,   4335.1989]], dtype=torch.float64)
	q_value: tensor([[-19.9290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05953485750909204, distance: 1.1097573881313436 entropy 10.640264230814122
epoch: 38, step: 28
	action: tensor([[ -9182.1019, -23809.9500,   3848.9670,  16493.0383,   8540.1942,
          -1275.9875, -10059.2119]], dtype=torch.float64)
	q_value: tensor([[-20.0724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17120820311966223, distance: 1.238436507678811 entropy 10.45131077373407
epoch: 38, step: 29
	action: tensor([[-11024.7085,  -7423.1150,  -2720.3652, -21237.8228,   2402.0869,
           7498.4180, -10897.8424]], dtype=torch.float64)
	q_value: tensor([[-18.1310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0551039363536856, distance: 1.1754504184482486 entropy 10.66113831005268
epoch: 38, step: 30
	action: tensor([[-1061.6658, -9603.7277, -5637.5607,  5088.0460, 10309.7688, 11642.7956,
          4429.6449]], dtype=torch.float64)
	q_value: tensor([[-18.5985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14948364760485977, distance: 1.055353676096021 entropy 10.471387688481185
epoch: 38, step: 31
	action: tensor([[-13009.9467,  -4066.2718,   6723.6928,   7600.6139, -25096.8590,
           8577.2049,  12013.7823]], dtype=torch.float64)
	q_value: tensor([[-18.9816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5967084306634256, distance: 1.4460040270926584 entropy 10.639870160260925
epoch: 38, step: 32
	action: tensor([[ -1635.8928,  -7067.5039,   1483.0109,    334.8348,  -1173.1349,
         -13106.8567,   6977.1559]], dtype=torch.float64)
	q_value: tensor([[-19.4293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4337816802003367, distance: 1.3702449392707112 entropy 10.630042943442254
epoch: 38, step: 33
	action: tensor([[ 3561.0238, -3705.2316, -7029.9661, 19033.3304, -2660.6009,  4696.0940,
         23166.2469]], dtype=torch.float64)
	q_value: tensor([[-21.7764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.691490694070723
epoch: 38, step: 34
	action: tensor([[-4897.9795, -5799.5284,   210.1048,  -931.8523,  3346.7999, -6129.6750,
          5228.6976]], dtype=torch.float64)
	q_value: tensor([[-22.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6259967852356536, distance: 1.459205757565878 entropy 10.155140047487347
epoch: 38, step: 35
	action: tensor([[ -5471.2495, -20521.4752,  11866.0534,  -3910.7420, -30218.8455,
           2967.6541, -11232.1990]], dtype=torch.float64)
	q_value: tensor([[-24.0442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20220664942332522, distance: 1.0221200308450484 entropy 10.769376099372703
epoch: 38, step: 36
	action: tensor([[ 18879.3587,   -272.6336, -25190.4343,   8593.0852,   5229.3349,
           3846.0051,   2798.8421]], dtype=torch.float64)
	q_value: tensor([[-27.5372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32735437815835533, distance: 0.9385336604014536 entropy 10.926934638831815
epoch: 38, step: 37
	action: tensor([[  -989.1410, -14220.9471,  -3492.3714,   6944.5792,  -6253.7441,
          -2439.1120,   6567.5428]], dtype=torch.float64)
	q_value: tensor([[-19.5517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24557245618676748, distance: 0.993952112840764 entropy 10.589122451627267
epoch: 38, step: 38
	action: tensor([[-4931.8720, -2115.7384, -7770.0525,  7267.9961, 16190.2723, 10544.0723,
         -4547.2327]], dtype=torch.float64)
	q_value: tensor([[-20.1622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35405198175204045, distance: 1.3316017641064626 entropy 10.720279587556135
epoch: 38, step: 39
	action: tensor([[-23063.4167,  19923.8848,  27658.7688,   1652.7981,   3277.3924,
           -847.6707,    324.2601]], dtype=torch.float64)
	q_value: tensor([[-21.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27321665580406007, distance: 0.9755716611675342 entropy 10.704552301488118
epoch: 38, step: 40
	action: tensor([[ 2822.0994, 11985.0945, -8281.2290, 11701.0681, -3013.9983,  8062.4400,
         -7908.4668]], dtype=torch.float64)
	q_value: tensor([[-24.1944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012782154881406, distance: 0.9565525529718394 entropy 10.821396926489784
epoch: 38, step: 41
	action: tensor([[-2.2142e+04, -1.0876e+04,  1.9579e+04,  7.7638e+03, -1.2890e+01,
          7.2984e+02,  9.0680e+03]], dtype=torch.float64)
	q_value: tensor([[-21.1463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049639749169906655, distance: 1.1155802705791475 entropy 10.46696330841321
epoch: 38, step: 42
	action: tensor([[-14054.0364,   5851.1762,   6559.9736,  -1671.1919, -24859.6305,
          -2331.5190,  -2471.9051]], dtype=torch.float64)
	q_value: tensor([[-18.3874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5363209276469572, distance: 0.7792295988146707 entropy 10.550598833858519
epoch: 38, step: 43
	action: tensor([[ -9066.2489,  -1143.1753,   6713.1468,  15757.6861,  -2331.8242,
         -11593.9504,  -1928.2029]], dtype=torch.float64)
	q_value: tensor([[-26.1321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36013368575199567, distance: 1.3345888483467863 entropy 10.790659069450289
epoch: 38, step: 44
	action: tensor([[ -2446.1283,  -9706.9094, -10113.9806,   3503.4751, -17631.9683,
          -6769.0357,   8500.5973]], dtype=torch.float64)
	q_value: tensor([[-20.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04989356123962185, distance: 1.172544487923173 entropy 10.62604081427266
epoch: 38, step: 45
	action: tensor([[-6081.7946, -2521.0879,  3770.8329, 23529.7828, -6244.4497, -8427.9340,
          6070.1802]], dtype=torch.float64)
	q_value: tensor([[-20.0095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7147379864100321, distance: 1.4984959644180456 entropy 10.643625896834404
epoch: 38, step: 46
	action: tensor([[-13851.7274,   4086.6060,  -1983.7073,   2853.0075,  11652.5490,
          -2220.1643,   3107.2960]], dtype=torch.float64)
	q_value: tensor([[-19.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0691674307809571, distance: 1.1040594987727386 entropy 10.53790139666697
epoch: 38, step: 47
	action: tensor([[-14034.3076, -28011.5282, -14086.1609,  17914.9297,   6895.3786,
            299.5929, -35254.4067]], dtype=torch.float64)
	q_value: tensor([[-26.4459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5110568509230959, distance: 1.4066857739444694 entropy 10.876100199005943
epoch: 38, step: 48
	action: tensor([[-20188.4036, -25920.5930,  -2402.2819,   1866.8785,     31.2759,
           -103.7340,  13967.9517]], dtype=torch.float64)
	q_value: tensor([[-21.5795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.004701097651742, distance: 1.6202480496681666 entropy 10.723058672793792
epoch: 38, step: 49
	action: tensor([[ -7927.0695,  -5781.0037, -18611.8069,  -5891.9463,  -7974.7953,
          11524.5905,   4332.5491]], dtype=torch.float64)
	q_value: tensor([[-14.9632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5079947752427338, distance: 1.405259764500388 entropy 10.436078171681839
epoch: 38, step: 50
	action: tensor([[-11641.0392,  -2445.8499,  -8249.5405,   -890.0517, -13107.9677,
          11536.7919,   2544.3356]], dtype=torch.float64)
	q_value: tensor([[-18.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47753527867557755, distance: 1.3909951728514778 entropy 10.528739243219222
epoch: 38, step: 51
	action: tensor([[ -6949.2111,  -7302.5412, -11774.4540, -13345.5226,  10369.5337,
          -6970.1096,   3250.7715]], dtype=torch.float64)
	q_value: tensor([[-19.5219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5325807058316812, distance: 1.4166689331944724 entropy 10.486222002972289
epoch: 38, step: 52
	action: tensor([[-12982.1397,   4238.7977,   5003.3497,  -2446.8946,   1193.7969,
            146.7399,  -2544.3442]], dtype=torch.float64)
	q_value: tensor([[-23.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.034902850383978334, distance: 1.1241964505308697 entropy 10.785595613142219
epoch: 38, step: 53
	action: tensor([[ -5242.6103, -11696.0854,  -5650.4139,   -991.6939,  11709.7308,
          11531.0964,   4825.0082]], dtype=torch.float64)
	q_value: tensor([[-21.7881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1761099410663074, distance: 1.038702853279199 entropy 10.480828947385763
epoch: 38, step: 54
	action: tensor([[-16314.2466,  -6712.2924,   -879.8191,  -2737.8518, -18426.4820,
          -1302.6905,  10693.8213]], dtype=torch.float64)
	q_value: tensor([[-25.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09158063972738617, distance: 1.1955964187006904 entropy 10.73823856533532
epoch: 38, step: 55
	action: tensor([[ -2715.8324,  -6523.4733, -23560.3469,  22931.2651,    -64.6430,
         -11619.0151, -15475.0357]], dtype=torch.float64)
	q_value: tensor([[-22.2990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25365344025081193, distance: 1.2812841142523028 entropy 10.635021837937563
epoch: 38, step: 56
	action: tensor([[ -7364.7682,  -8598.1930,   1019.2079,   5565.2267,  -1978.5169,
         -12280.5752, -19805.9641]], dtype=torch.float64)
	q_value: tensor([[-17.3774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4980813862473821, distance: 1.4006331379688732 entropy 10.49479742060704
epoch: 38, step: 57
	action: tensor([[-10047.2864,   2601.6361,  -6629.7338,  -5835.3860,   2506.9025,
          11320.4221,   4141.6557]], dtype=torch.float64)
	q_value: tensor([[-19.3122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16284834829995143, distance: 1.2340087337363657 entropy 10.553048996074226
epoch: 38, step: 58
	action: tensor([[ -4997.6742,   1418.1426,   1213.0579,  -3380.0059,  13850.3690,
          -5488.6650, -11102.2232]], dtype=torch.float64)
	q_value: tensor([[-18.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33858065813540783, distance: 1.3239725042901662 entropy 10.088429786759123
epoch: 38, step: 59
	action: tensor([[ 3005.5315, -4872.7332,  -623.9321,  4749.7210, -8920.9998, -7626.0763,
          7556.1355]], dtype=torch.float64)
	q_value: tensor([[-25.3504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21660391268007428, distance: 1.2622090731541074 entropy 10.533938729025094
epoch: 38, step: 60
	action: tensor([[  2917.8058, -10584.7063,     49.1716,     22.9184,  -6349.5073,
           2797.4646,   -997.5562]], dtype=torch.float64)
	q_value: tensor([[-18.5565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5429463619281856, distance: 0.7736424264348578 entropy 10.41061153692931
epoch: 38, step: 61
	action: tensor([[-2867.1116, -2434.4648,  6161.5070, -3818.3074,  1041.4298, -1503.5380,
         -3794.5096]], dtype=torch.float64)
	q_value: tensor([[-23.8385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4254059013037086, distance: 1.3662367701191727 entropy 10.664200194662246
epoch: 38, step: 62
	action: tensor([[ -6113.6019, -15485.2253,  -6532.4188,  14431.9076,   6413.0034,
          17967.8200, -11500.2244]], dtype=torch.float64)
	q_value: tensor([[-18.8071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12183374764150656, distance: 1.2120511376959875 entropy 10.56781156790753
epoch: 38, step: 63
	action: tensor([[ 17853.4137, -13864.1732,    333.2288,  -8030.5940, -20005.2846,
          16467.0147,  -9113.6041]], dtype=torch.float64)
	q_value: tensor([[-22.2605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06163763958461432, distance: 1.179084274311236 entropy 10.755595322051477
epoch: 38, step: 64
	action: tensor([[ -8843.6940,  -6454.2096, -10019.3975,  23967.3115,  -5135.4110,
           -475.7360,   7824.3137]], dtype=torch.float64)
	q_value: tensor([[-17.9584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27803382218383554, distance: 0.9723332104720126 entropy 10.285526875573742
epoch: 38, step: 65
	action: tensor([[ 3685.3446, -1338.0723,  7055.6661, -9776.0027,  3208.9415, -1567.9886,
         19313.2650]], dtype=torch.float64)
	q_value: tensor([[-22.5007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2083730851111496, distance: 1.2579321444301304 entropy 10.834011920825203
epoch: 38, step: 66
	action: tensor([[ -8569.2445,   2090.4496, -13788.9831,  -4645.2854,  14157.8791,
           6650.4298,   2163.9937]], dtype=torch.float64)
	q_value: tensor([[-18.4808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10536971663787997, distance: 1.2031242331586107 entropy 10.360031154754969
epoch: 38, step: 67
	action: tensor([[  7212.8608, -12338.5234,  -3501.9727,   5890.8453,  16746.4909,
           7782.1068,   3738.4736]], dtype=torch.float64)
	q_value: tensor([[-25.5502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.695188736735108
epoch: 38, step: 68
	action: tensor([[-2979.5567, -2301.2597, -8187.1532,  4293.0259, -7177.7254, -6088.9942,
         11546.9586]], dtype=torch.float64)
	q_value: tensor([[-22.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011148316394515412, distance: 1.1379476202049812 entropy 10.155140047487347
epoch: 38, step: 69
	action: tensor([[  5406.7970,  -2122.6592, -19565.9499,   8988.4343,   7802.9445,
           3222.5935,   5453.7537]], dtype=torch.float64)
	q_value: tensor([[-19.7117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3857285573070868, distance: 0.8968851969232089 entropy 10.696946618497417
epoch: 38, step: 70
	action: tensor([[-12415.8942,  -8702.7406,   5556.5695,  15328.5919,   9814.2649,
          18362.0902, -10545.3508]], dtype=torch.float64)
	q_value: tensor([[-19.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1478749398727306, distance: 1.0563512780634414 entropy 10.404009951701132
epoch: 38, step: 71
	action: tensor([[ -6537.5225,   8289.8547, -15853.1253,  10869.9672,  -4060.1087,
          14432.6509,   4047.2319]], dtype=torch.float64)
	q_value: tensor([[-19.6716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08401849908695325, distance: 1.1914478559620694 entropy 10.579155306312304
epoch: 38, step: 72
	action: tensor([[ -7965.9395, -14719.5478, -12461.5363,   8807.8109,  26283.2237,
         -12818.5107,  10336.7081]], dtype=torch.float64)
	q_value: tensor([[-20.0772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5049708894789373, distance: 1.4038501185786185 entropy 10.542862541006928
epoch: 38, step: 73
	action: tensor([[   830.7447, -13455.8003,  -5716.5973,  10452.9188,  -6959.6858,
           7210.0669,   2731.7026]], dtype=torch.float64)
	q_value: tensor([[-17.9925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5246608145168893, distance: 0.7889663890134828 entropy 10.620545791499323
epoch: 38, step: 74
	action: tensor([[ -6994.1907,   5641.2789,  -5553.3701,   8426.9767, -10936.3979,
          -1160.3746,   1617.5431]], dtype=torch.float64)
	q_value: tensor([[-20.0017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11880318383258304, distance: 1.0742197998011493 entropy 10.502932935594922
epoch: 38, step: 75
	action: tensor([[ 5925.8801, -1099.8451, -3863.4164,  2909.4037, -5873.1435, -9739.6522,
         10542.4282]], dtype=torch.float64)
	q_value: tensor([[-21.6740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7100040970741217, distance: 0.616243887244539 entropy 10.56355725654148
epoch: 38, step: 76
	action: tensor([[ -1507.9074,    744.9136, -17485.1571,  -3140.0471,   6849.0110,
          18582.0650,  -6340.3342]], dtype=torch.float64)
	q_value: tensor([[-23.2561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.416858266605937
epoch: 38, step: 77
	action: tensor([[  3660.6405,  -2705.3176,   1979.9458,  -1902.3698, -14834.5216,
           1382.2514,   9062.2605]], dtype=torch.float64)
	q_value: tensor([[-22.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3105036287295311, distance: 1.310013608582637 entropy 10.155140047487347
epoch: 38, step: 78
	action: tensor([[-1067.5409, -3636.5807,  2827.6537,  -324.5010,  2172.0356, -6615.1209,
          1093.5553]], dtype=torch.float64)
	q_value: tensor([[-17.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06642645156924476, distance: 1.105683840103346 entropy 10.333616944626272
epoch: 38, step: 79
	action: tensor([[ -9167.5636, -21701.7400,  -1857.0836,  -4753.3093,   1522.4612,
          -4575.7521,  -1440.7013]], dtype=torch.float64)
	q_value: tensor([[-21.1070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7102775763133673, distance: 1.4965457367966477 entropy 10.5828418079749
epoch: 38, step: 80
	action: tensor([[-16263.4393,   4480.7740,  -5015.3274,   2172.7506,   3574.9420,
          11408.7138,   3316.5366]], dtype=torch.float64)
	q_value: tensor([[-22.1833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31198547073733995, distance: 1.310754043292172 entropy 10.655228938099045
epoch: 38, step: 81
	action: tensor([[ 5296.8052, -2690.9955, -1287.3019, 16478.9067, -6359.4515,  -915.9948,
          -190.0476]], dtype=torch.float64)
	q_value: tensor([[-19.5482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20814788875188373, distance: 1.0183070083824413 entropy 10.245321349825646
epoch: 38, step: 82
	action: tensor([[-9334.7463, -1867.8255, -5988.4307, 11721.4512,  9296.9325, -3564.1763,
         22651.3543]], dtype=torch.float64)
	q_value: tensor([[-21.0711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01583338468041917, distance: 1.135248685547764 entropy 10.511754795634518
epoch: 38, step: 83
	action: tensor([[ -7488.5146,  -5010.9338, -17973.7688,   5085.4340,  -2105.4441,
           4270.4472,   3663.2440]], dtype=torch.float64)
	q_value: tensor([[-18.9742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0853159111384123, distance: 1.1921606374141196 entropy 10.685987295161173
epoch: 38, step: 84
	action: tensor([[-24137.8202,   8350.6755,   1971.1249,  -9796.8154, -21754.5728,
           7459.2865,  -2781.5931]], dtype=torch.float64)
	q_value: tensor([[-18.9755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1952903601542002, distance: 1.2511039687657195 entropy 10.539117345679033
epoch: 38, step: 85
	action: tensor([[  -582.5011,   -403.4509, -16461.4693,   7043.9007,   5903.8891,
          -5135.7326,  12879.4953]], dtype=torch.float64)
	q_value: tensor([[-23.7630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14498866710708425, distance: 1.0581387741994823 entropy 10.507487399317567
epoch: 38, step: 86
	action: tensor([[-24549.6753,  -8331.1935,  -1319.3813,   1475.8900,  -2299.6227,
           8212.6207,  -3629.9679]], dtype=torch.float64)
	q_value: tensor([[-19.4979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2071521322600498, distance: 1.0189470693405234 entropy 10.668720422753982
epoch: 38, step: 87
	action: tensor([[  9183.0561, -11808.9226,  -1776.2374,   8859.1000, -12415.0618,
           4037.2449,  -2768.8454]], dtype=torch.float64)
	q_value: tensor([[-18.5901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25273167505523675, distance: 0.9892247648162784 entropy 10.574169661668547
epoch: 38, step: 88
	action: tensor([[-19753.7162, -17187.9921,   5555.0656,  11650.2368,   2586.2963,
          -3896.8853,   1581.8326]], dtype=torch.float64)
	q_value: tensor([[-24.0053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3460739812367831, distance: 0.9253819030428774 entropy 10.547796614961399
epoch: 38, step: 89
	action: tensor([[ -4434.5934,   5333.0230, -12658.4646, -14998.0278, -25558.3440,
          -5928.1385,   2131.9544]], dtype=torch.float64)
	q_value: tensor([[-20.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6312822351506766, distance: 0.6948702599079698 entropy 10.708381091182108
epoch: 38, step: 90
	action: tensor([[ -9333.6728, -11400.0760,   8785.9234,   5578.2614,  -2958.7215,
          -8366.9339,  -5788.4908]], dtype=torch.float64)
	q_value: tensor([[-24.2050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23649214742209268, distance: 1.2724841297727976 entropy 10.617345902554218
epoch: 38, step: 91
	action: tensor([[  7246.9118, -27508.6264,   6903.5547,  13652.9537,  -9450.0044,
           5936.7789, -11627.0422]], dtype=torch.float64)
	q_value: tensor([[-22.9514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45721754284480665, distance: 0.8430815681096817 entropy 10.774890175809734
epoch: 38, step: 92
	action: tensor([[-8031.2938,  -390.8858, 19862.3629, 10706.1661,  2448.9637, 16415.5640,
         12461.7044]], dtype=torch.float64)
	q_value: tensor([[-22.0724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16276870217262773, distance: 1.0470789305897885 entropy 10.762941524702368
epoch: 38, step: 93
	action: tensor([[ -7132.7014, -18542.1736,  -4157.2833,  13182.5807,  15029.4795,
          12787.8234,   1976.2946]], dtype=torch.float64)
	q_value: tensor([[-21.8972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07998887680016697, distance: 1.0976230846808888 entropy 10.725707184675551
epoch: 38, step: 94
	action: tensor([[   294.7234,  22494.4408,  24695.7836, -14706.0439,  -7537.5253,
           2336.1024,   6142.1441]], dtype=torch.float64)
	q_value: tensor([[-22.3258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.766502705973556
epoch: 38, step: 95
	action: tensor([[ 1804.8260, -1090.9022,   680.7251, -6920.0048, -1020.3789,  3397.7313,
          5992.1092]], dtype=torch.float64)
	q_value: tensor([[-22.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.534442648352154, distance: 0.7808062621772698 entropy 10.155140047487347
epoch: 38, step: 96
	action: tensor([[-15036.4123, -17104.4149,  -8411.2242,  21281.6927,   5999.3159,
          19831.2626,  -2576.8120]], dtype=torch.float64)
	q_value: tensor([[-22.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12885102997159548, distance: 1.215836032700898 entropy 10.791491342498698
epoch: 38, step: 97
	action: tensor([[ 1765.1718, -8431.1119, -3905.2090,  6424.5341, -3612.2357,  -161.3544,
         -7348.7241]], dtype=torch.float64)
	q_value: tensor([[-19.5780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3184775015017738, distance: 1.3139930101104684 entropy 10.58320898338494
epoch: 38, step: 98
	action: tensor([[ 1536.5833,  4487.7802, -5561.0449, 10171.3841,  8456.5144, -2897.3163,
          7448.4158]], dtype=torch.float64)
	q_value: tensor([[-25.6463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5816632958674863, distance: 0.7401498892931087 entropy 10.728047416995015
epoch: 38, step: 99
	action: tensor([[ -9173.2710, -16518.4888, -17895.2048, -18718.9288,   5332.6613,
          -2015.7920,   9098.6223]], dtype=torch.float64)
	q_value: tensor([[-24.3491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.65281372567525
epoch: 38, step: 100
	action: tensor([[-12097.6945, -13183.3662,   2244.1863,  10992.1660,  -2835.9402,
           9637.9340,   3676.1759]], dtype=torch.float64)
	q_value: tensor([[-22.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5426293869863024, distance: 1.4213056860240167 entropy 10.155140047487347
epoch: 38, step: 101
	action: tensor([[-19916.8752,   -825.0423,  -1171.6154,   4481.8223, -14204.5538,
           8086.2498,  -2193.0496]], dtype=torch.float64)
	q_value: tensor([[-19.3272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6829535471563966, distance: 1.4845429184114416 entropy 10.613200447856864
epoch: 38, step: 102
	action: tensor([[-17123.0684, -13897.6233,  14868.6194,   -876.7300,   5633.8804,
         -12139.2347,  -7359.1095]], dtype=torch.float64)
	q_value: tensor([[-20.0880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7527450066152541, distance: 1.5150119642517599 entropy 10.675134429759341
epoch: 38, step: 103
	action: tensor([[-7103.7921, -4029.6855,  2408.2169,  2174.6128, 12634.5750,  2054.2801,
         -4653.8760]], dtype=torch.float64)
	q_value: tensor([[-21.4352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11507830621394688, distance: 1.208396271614851 entropy 10.683230973396032
epoch: 38, step: 104
	action: tensor([[  1506.9257,  -2936.3055,  -4425.5406,  22393.8085, -10373.0400,
          -4164.4429, -16821.4942]], dtype=torch.float64)
	q_value: tensor([[-21.7602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40741004054163166, distance: 1.3575849529831254 entropy 10.750837785310823
epoch: 38, step: 105
	action: tensor([[-15859.4508,   6548.4856, -10762.2814,  -7486.5236,    734.7067,
          -1400.0680,  16230.6869]], dtype=torch.float64)
	q_value: tensor([[-19.1615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05143666993535967, distance: 1.1145251139598824 entropy 10.675093411857933
epoch: 38, step: 106
	action: tensor([[-18475.6125,   8068.0196,   5374.0877,   6220.9604,  12013.6796,
           3713.6111,  -5749.4040]], dtype=torch.float64)
	q_value: tensor([[-21.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27252508769482564, distance: 0.9760357016789645 entropy 10.416794244054696
epoch: 38, step: 107
	action: tensor([[-3807.7007, -2589.1825,  -370.6922, -1125.1102, -9958.3952,  4840.2308,
          5174.7468]], dtype=torch.float64)
	q_value: tensor([[-18.6952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.179339370848481, distance: 1.6893480140713375 entropy 10.331733508980042
epoch: 38, step: 108
	action: tensor([[ -8909.2886, -11465.6456,    209.9368,  13947.5220,  18539.3411,
           2749.5511,   1005.1860]], dtype=torch.float64)
	q_value: tensor([[-16.9782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40656827273835927, distance: 1.3571789084780197 entropy 10.301275748146542
epoch: 38, step: 109
	action: tensor([[13370.4062, 14979.2496, -4271.5800,  2216.8007,   296.3662,  3327.1279,
          6714.4379]], dtype=torch.float64)
	q_value: tensor([[-20.4144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.654603488225831
epoch: 38, step: 110
	action: tensor([[  791.2960, -4083.2379,  2213.2860, -4703.7403,   301.4070,  3376.8949,
         -7200.9921]], dtype=torch.float64)
	q_value: tensor([[-22.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3274941541378139, distance: 0.9384361415348103 entropy 10.155140047487347
epoch: 38, step: 111
	action: tensor([[ 13972.9065,  -1162.5549,   4614.3106, -13343.5895,  13695.9996,
           8296.1807,   2624.9670]], dtype=torch.float64)
	q_value: tensor([[-20.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3471050453770108, distance: 0.9246520769876203 entropy 10.5086920629638
epoch: 38, step: 112
	action: tensor([[ -2162.3542,  -1958.6504,  -7800.8753,   3945.5071,  -4931.2368,
         -20045.8845,  -7482.0942]], dtype=torch.float64)
	q_value: tensor([[-21.9998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3073281121836686, distance: 1.3084254813582574 entropy 10.623637140854948
epoch: 38, step: 113
	action: tensor([[-21402.1667,   8575.0932,    -55.0714,  -3882.2378, -10453.2370,
          -7896.6947,  -3696.0915]], dtype=torch.float64)
	q_value: tensor([[-20.0821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43051873787256234, distance: 0.8635677450393937 entropy 10.479922839957311
epoch: 38, step: 114
	action: tensor([[-12444.0385,  17608.1964,   8814.7878,  -4551.3796, -10270.3743,
          -8240.1347,  -7969.3876]], dtype=torch.float64)
	q_value: tensor([[-26.4700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14121074756077312, distance: 1.2224739679878365 entropy 10.58118650177373
epoch: 38, step: 115
	action: tensor([[  -715.6758, -12541.7727,   1684.1066,   3216.4567,  12805.7506,
          -2213.1827, -16143.2698]], dtype=torch.float64)
	q_value: tensor([[-28.8281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9676888144644702, distance: 1.6052212550848712 entropy 10.711342877725977
epoch: 38, step: 116
	action: tensor([[ -4343.4486,   2082.0917,   3425.0563, -10377.5868,  -7449.5289,
           4359.6221,  16801.8893]], dtype=torch.float64)
	q_value: tensor([[-16.7713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10490250175589333, distance: 1.0826594348670193 entropy 10.402708775587937
epoch: 38, step: 117
	action: tensor([[-10913.5889,    127.6709, -20049.9217,    632.1136,  20480.7601,
           8963.8730,   8676.3276]], dtype=torch.float64)
	q_value: tensor([[-25.4030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049335485386931666, distance: 1.1722328107349353 entropy 10.563789578736316
epoch: 38, step: 118
	action: tensor([[-10598.1587, -14869.4733, -10634.6427,   4991.3425,   5252.2290,
          -9026.1458,  10498.7665]], dtype=torch.float64)
	q_value: tensor([[-22.5272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3407674189511207, distance: 1.3250535111411506 entropy 10.509221821313679
epoch: 38, step: 119
	action: tensor([[   468.0705,   3929.6100,  14662.1280, -17814.3153,   4502.1499,
         -22579.6722,  -1864.1406]], dtype=torch.float64)
	q_value: tensor([[-18.9218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.550922060815415
epoch: 38, step: 120
	action: tensor([[ -6152.6582,   -656.2226,  11607.6789,   4853.9910,   -983.0704,
         -10222.3824,   9798.7804]], dtype=torch.float64)
	q_value: tensor([[-22.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24212952050341507, distance: 0.9962175509105805 entropy 10.155140047487347
epoch: 38, step: 121
	action: tensor([[ -1908.6532, -33368.5949,  14916.9556,   6406.3970,    903.4216,
          13418.9987,   7784.0774]], dtype=torch.float64)
	q_value: tensor([[-21.5290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49751395059221504, distance: 1.4003678504940056 entropy 10.800110772249182
epoch: 38, step: 122
	action: tensor([[ -9394.7940, -17088.2641, -25177.9615, -12176.8446,   5373.4192,
          -1928.4902, -10141.5818]], dtype=torch.float64)
	q_value: tensor([[-19.4499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2408017187598357, distance: 1.274699708416098 entropy 10.600090074244852
epoch: 38, step: 123
	action: tensor([[ -1756.3003, -10423.8087,  -7146.2840,   5483.5574,  -5135.9496,
          -2278.9417,   8804.8088]], dtype=torch.float64)
	q_value: tensor([[-17.3145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10528529088997085, distance: 1.2030782862680498 entropy 10.404291202098554
epoch: 38, step: 124
	action: tensor([[-16635.4052, -25459.7458, -19635.0382,   5692.5231,   1864.0555,
         -16202.0915,   5012.0358]], dtype=torch.float64)
	q_value: tensor([[-21.5857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16124080755648817, distance: 1.2331554817297563 entropy 10.871898124101895
epoch: 38, step: 125
	action: tensor([[-22211.4658, -11838.6289, -12037.3479,   7044.7466, -17996.8625,
           3058.5182,   8847.4523]], dtype=torch.float64)
	q_value: tensor([[-19.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5827642592252296, distance: 1.4396761519153278 entropy 10.684459888602822
epoch: 38, step: 126
	action: tensor([[-13798.8489,   -631.4798,   2797.2465,  -4827.8069,   -804.4928,
           2014.8951,   4443.0609]], dtype=torch.float64)
	q_value: tensor([[-19.8155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8505744557957791, distance: 1.5567180994311567 entropy 10.632425498890173
epoch: 38, step: 127
	action: tensor([[  4096.9743,  -5466.7619,   8002.7289,   7580.4984,  -1211.9441,
         -11014.5304,  14621.8276]], dtype=torch.float64)
	q_value: tensor([[-16.5247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11344827282205938, distance: 1.207512725306608 entropy 10.371880730305211
LOSS epoch 38 actor 176.73852143321358 critic 304.4818044976589
epoch: 39, step: 0
	action: tensor([[-12142.1184, -14327.5373,  -2481.5994,  15374.6684, -13697.2272,
          12765.6556,    -72.5708]], dtype=torch.float64)
	q_value: tensor([[-27.4912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1364023685483976, distance: 1.0634385866318643 entropy 10.643403612332365
epoch: 39, step: 1
	action: tensor([[ -7667.6517,   4000.0522,    610.5776,   1901.5097,   -450.8760,
          10140.3846, -16958.0733]], dtype=torch.float64)
	q_value: tensor([[-21.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17784967381983174, distance: 1.0376056071616366 entropy 10.7881887807139
epoch: 39, step: 2
	action: tensor([[-12675.2523,  -9473.0214, -10776.0711,  20854.9769,   4896.2154,
          -3680.2412,  13450.0002]], dtype=torch.float64)
	q_value: tensor([[-22.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5728793919785562, distance: 1.4351734925626505 entropy 10.617194746451336
epoch: 39, step: 3
	action: tensor([[ 6.8211e+00, -1.6989e+04,  5.1812e+03, -4.1578e+03, -5.7520e+03,
         -6.1590e+03,  2.7541e+04]], dtype=torch.float64)
	q_value: tensor([[-22.4743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4396574046094701, distance: 1.373049742138461 entropy 10.757467355212912
epoch: 39, step: 4
	action: tensor([[-1781.7617,  6655.4518, -5286.5346, -9428.2371,  5723.0218, -1239.1485,
         31602.0366]], dtype=torch.float64)
	q_value: tensor([[-20.4995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3190138008352602, distance: 0.9443344831046551 entropy 10.795384576297948
epoch: 39, step: 5
	action: tensor([[  4221.9393,  -1137.6745, -23287.8554,   8459.3224,  24397.4169,
           8314.0552,   5898.5053]], dtype=torch.float64)
	q_value: tensor([[-33.3303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.834057659075052
epoch: 39, step: 6
	action: tensor([[-1639.7566,  -147.4645,  6888.4415, -1423.8151,  -381.3657,  5988.4145,
          -204.0867]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 7
	action: tensor([[ 1.1104e+04, -1.6968e+04, -4.3216e+03,  3.5981e+03, -1.2476e+01,
         -3.2703e+03, -3.7169e+03]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 8
	action: tensor([[ 2125.7599, -1851.4296, -4193.5388, 12321.7668,  9967.7675,  -679.5612,
           -84.9634]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 9
	action: tensor([[-12932.2597, -13225.8409,   -662.1916,  -1027.1600, -10543.6296,
           9599.5237,    113.3674]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 10
	action: tensor([[-11118.0640,   3155.1070,  -8379.5474,   8751.8644,    432.2444,
          -6274.3967,   4029.5225]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18481019660939735, distance: 1.2456071280176992 entropy 10.205756468212739
epoch: 39, step: 11
	action: tensor([[ -2343.4187, -18589.6570,   7380.1383,  13734.4318, -22332.5314,
          -1123.3430,   1681.8703]], dtype=torch.float64)
	q_value: tensor([[-25.0368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2864487415965373, distance: 1.297934978396985 entropy 10.756661335804438
epoch: 39, step: 12
	action: tensor([[  -564.5817, -33053.8276, -19540.9558,  -1018.2954,   1076.1806,
           3729.6964, -13544.5356]], dtype=torch.float64)
	q_value: tensor([[-21.8513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.703879325595401, distance: 1.4937437802086357 entropy 10.924286190273923
epoch: 39, step: 13
	action: tensor([[  -220.7984,  -8929.9452,   3342.6892, -16895.7258,   1668.4219,
           5034.4193,  10198.5490]], dtype=torch.float64)
	q_value: tensor([[-18.9699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07048725710895687, distance: 1.1032764988339634 entropy 10.710176274416629
epoch: 39, step: 14
	action: tensor([[ -9164.8390, -14919.3107,  10131.0805,   -658.7296,  -1817.7559,
           3450.9427,   7484.6175]], dtype=torch.float64)
	q_value: tensor([[-21.8533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3669747357544446, distance: 1.3379409223627707 entropy 10.710551088871261
epoch: 39, step: 15
	action: tensor([[ -9620.8795,   8122.5604,   7149.9812,  -7342.9752,  -1012.3952,
          -4986.0257, -25450.5507]], dtype=torch.float64)
	q_value: tensor([[-20.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30637819004561406, distance: 0.95305521806402 entropy 10.676556233059879
epoch: 39, step: 16
	action: tensor([[-24113.9375,  -7945.6797, -15571.8618,   6165.0450, -16704.7720,
          17679.4404,  -8451.2983]], dtype=torch.float64)
	q_value: tensor([[-27.2201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0322864712538387, distance: 1.6313575466266257 entropy 10.744659811019316
epoch: 39, step: 17
	action: tensor([[  -551.1938, -12900.4028,  -1799.3017,   5639.4342,  -1170.0392,
         -10039.8595, -12842.4136]], dtype=torch.float64)
	q_value: tensor([[-19.6749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7344589005664153, distance: 1.5070883057285906 entropy 10.641585450854153
epoch: 39, step: 18
	action: tensor([[ 14631.6384,  -3356.8814,   -333.4164, -17224.3173,   5977.4480,
          -4546.8100,  -2306.6087]], dtype=torch.float64)
	q_value: tensor([[-18.7254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29846321744447435, distance: 1.303981767462001 entropy 10.730114953342914
epoch: 39, step: 19
	action: tensor([[ -4051.6120, -14635.7670,  -8173.9225, -11351.7953,   3981.6811,
          -2407.2103,  -2566.7225]], dtype=torch.float64)
	q_value: tensor([[-18.9685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010022287683997821, distance: 1.1385953397472597 entropy 10.38637908478839
epoch: 39, step: 20
	action: tensor([[-16465.5951, -14447.4620,   5636.3205,  10364.9672,   5196.1247,
          -9890.5495,  10315.5574]], dtype=torch.float64)
	q_value: tensor([[-22.1662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6770437959733218, distance: 1.4819341135646722 entropy 10.677195988827696
epoch: 39, step: 21
	action: tensor([[-3500.0352, -2842.6990,  6260.5414, -6334.9874,  6474.4212, -1734.8056,
         -2217.4823]], dtype=torch.float64)
	q_value: tensor([[-17.3130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8283924900068942, distance: 1.5473601486382529 entropy 10.438497831169537
epoch: 39, step: 22
	action: tensor([[  -137.5043,     16.1768,  -7573.7202,   2785.5192, -16088.9119,
           2735.6392,  10586.9352]], dtype=torch.float64)
	q_value: tensor([[-22.2603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.731649450202772
epoch: 39, step: 23
	action: tensor([[ -444.2089, -2942.8733, -1047.9319,  1811.4922,  1407.9158,  2605.2092,
          8177.4242]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1059589302841546, distance: 1.2034448511210973 entropy 10.205756468212739
epoch: 39, step: 24
	action: tensor([[ -4920.8717,     53.5579,  10352.8712,   8261.1690,  -5481.4665,
         -11058.6979,   4468.7208]], dtype=torch.float64)
	q_value: tensor([[-18.7242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2524122921191252, distance: 1.2806497055997437 entropy 10.593134628339772
epoch: 39, step: 25
	action: tensor([[ -5361.6975,  -2594.6854, -12567.5269,   1033.3341,  -9337.5150,
          12262.0607,   5404.2229]], dtype=torch.float64)
	q_value: tensor([[-19.9988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11447537471637026, distance: 1.0768544689054882 entropy 10.736191278582412
epoch: 39, step: 26
	action: tensor([[-2353.0065, -3302.6730,  6889.0228,  5235.1277,   727.3135,  9211.4582,
         -4718.0307]], dtype=torch.float64)
	q_value: tensor([[-19.1189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11875805541967399, distance: 1.0742473062634614 entropy 10.650730589579949
epoch: 39, step: 27
	action: tensor([[ 1520.9117,  5096.0363, 10799.4732, 28785.4139, -2211.4608, 14400.0151,
         -1408.8997]], dtype=torch.float64)
	q_value: tensor([[-21.4186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41722193879860014, distance: 0.8735912801458132 entropy 10.868783541605918
epoch: 39, step: 28
	action: tensor([[ -3192.9739,   1567.7787, -11688.8232,   8644.9035,  -3183.0691,
          14150.0604,  15063.0346]], dtype=torch.float64)
	q_value: tensor([[-19.6497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04709726237921674, distance: 1.1170715227759551 entropy 10.536475091834674
epoch: 39, step: 29
	action: tensor([[ -2568.4496, -13660.3113,   5905.6269,   1179.3438,   -544.1880,
          15534.3818,   7292.0215]], dtype=torch.float64)
	q_value: tensor([[-24.1264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6945266221298432, distance: 1.4896385109799373 entropy 10.781952352868501
epoch: 39, step: 30
	action: tensor([[-10587.2129, -19521.0475,   5084.5024,  29254.3159, -13339.1650,
          22232.6330,  27971.8544]], dtype=torch.float64)
	q_value: tensor([[-23.6978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10565134762393347, distance: 1.0822064592493725 entropy 10.865228982123048
epoch: 39, step: 31
	action: tensor([[  -530.2696,  -6977.2754,  -6158.5549,  11526.0809,   8604.3119,
         -14773.3715,   7056.0933]], dtype=torch.float64)
	q_value: tensor([[-20.1849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030293814693277854, distance: 1.16154820913211 entropy 10.610536731982211
epoch: 39, step: 32
	action: tensor([[ -9281.0540, -21803.0905,  11279.7671,  -2436.4334,  -1370.0564,
          -9949.8875, -20346.7220]], dtype=torch.float64)
	q_value: tensor([[-20.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.569240686231814, distance: 1.4335124631760379 entropy 10.682847147054117
epoch: 39, step: 33
	action: tensor([[-3011.0953, -2224.1490, -2280.2305, 12515.9854,  9404.9810, -4470.7118,
         -2988.1542]], dtype=torch.float64)
	q_value: tensor([[-18.5080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21885613126981007, distance: 1.2633768551524087 entropy 10.482973395500427
epoch: 39, step: 34
	action: tensor([[ -2350.2403,  -1293.0658, -18066.2910,   7566.8436,  20614.6042,
           2922.8382,  17665.5986]], dtype=torch.float64)
	q_value: tensor([[-21.3056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11638776873058765, distance: 1.0756910438315073 entropy 10.750437181140885
epoch: 39, step: 35
	action: tensor([[  9644.2909,  10693.3708,   7776.5203,  13573.9950,  -9567.5771,
          -6402.0601, -20575.6978]], dtype=torch.float64)
	q_value: tensor([[-20.6902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7960862263219588, distance: 0.5167493918727591 entropy 10.787824405001944
epoch: 39, step: 36
	action: tensor([[  1132.5033,   -791.8887, -10464.4056,   4182.0070,    319.4801,
           7538.1444,  17779.3707]], dtype=torch.float64)
	q_value: tensor([[-23.9455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1445053680887064, distance: 1.058437790811425 entropy 10.675677260366825
epoch: 39, step: 37
	action: tensor([[-10992.9865, -13480.6077,  16165.5383,  -1747.6815, -10436.8935,
         -10772.2023,   3110.7759]], dtype=torch.float64)
	q_value: tensor([[-21.9545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0345732709840223, distance: 1.1639580284015973 entropy 10.852271954308646
epoch: 39, step: 38
	action: tensor([[-14248.5377,  13305.5668,  -6999.7901,   7366.9581,   1632.5681,
          -6105.3328, -11937.7854]], dtype=torch.float64)
	q_value: tensor([[-18.5079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6565996032926122, distance: 0.6705900257883096 entropy 10.558918847379749
epoch: 39, step: 39
	action: tensor([[-12834.1544, -10519.7614,  -7460.9078,  -2037.5462,   4392.1287,
          18868.6733,   1374.2718]], dtype=torch.float64)
	q_value: tensor([[-21.6459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9440582965167212, distance: 1.5955533688306909 entropy 10.57929847253878
epoch: 39, step: 40
	action: tensor([[-16586.2336, -14442.9870,   3586.0903,  23299.7690,  -6732.9269,
            514.5288,  -6507.5486]], dtype=torch.float64)
	q_value: tensor([[-19.0065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8800719039984801, distance: 1.5690757950849226 entropy 10.517422320612102
epoch: 39, step: 41
	action: tensor([[ 4644.2288,  6448.2827,  5741.3375,  6213.4691, 22990.4333, 26149.0540,
         -4953.3733]], dtype=torch.float64)
	q_value: tensor([[-23.6894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38194811958141617, distance: 0.8996408336269758 entropy 10.915782632961312
epoch: 39, step: 42
	action: tensor([[-14351.4974,  15743.9381,   1876.3752, -10915.2742, -12501.4550,
          20968.2727,  -5109.8367]], dtype=torch.float64)
	q_value: tensor([[-21.2195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6149794720230033, distance: 0.7100658659336088 entropy 10.65741638684311
epoch: 39, step: 43
	action: tensor([[ -5091.9880, -15554.5726,   -634.4454,  11785.1318,   -185.4571,
          19048.7846,  11541.1643]], dtype=torch.float64)
	q_value: tensor([[-27.8547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8795646651991254, distance: 1.5688641144011541 entropy 10.686214094590929
epoch: 39, step: 44
	action: tensor([[  3688.4685,  -1026.3966,   5810.8949,  -4750.9509,   7344.4066,
          22617.0731, -18779.3873]], dtype=torch.float64)
	q_value: tensor([[-22.7030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18884329998065352, distance: 1.2477253552391785 entropy 10.818121372938027
epoch: 39, step: 45
	action: tensor([[ -1644.4401, -12380.5201,  -3823.9772,  13021.0165,   1655.4339,
           8848.4370,   4532.4217]], dtype=torch.float64)
	q_value: tensor([[-16.6822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.523553181893925, distance: 1.4124903925629433 entropy 10.336782053497448
epoch: 39, step: 46
	action: tensor([[-15263.6580, -14767.6378,  -9579.2768,  14143.4118,  10633.5957,
          -1836.5081,  15356.4630]], dtype=torch.float64)
	q_value: tensor([[-24.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08330939973341667, distance: 1.09564051235291 entropy 10.940354621809789
epoch: 39, step: 47
	action: tensor([[  3566.9839,  14317.8554,  -1612.1176,   9728.6026, -13280.0499,
         -11414.4164,  16968.6681]], dtype=torch.float64)
	q_value: tensor([[-25.4034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40333779926455593, distance: 0.8839362734471955 entropy 10.776044831041116
epoch: 39, step: 48
	action: tensor([[  2124.2877,   4284.1622, -20396.7388,  12762.7671,   9144.6425,
           8524.3428, -16201.7756]], dtype=torch.float64)
	q_value: tensor([[-28.6474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8207821065441487, distance: 0.48444823437367496 entropy 10.699905955745022
epoch: 39, step: 49
	action: tensor([[   362.5035, -13144.0123, -14654.4657,   -788.5590, -12229.9368,
         -11591.5038,  -2310.2100]], dtype=torch.float64)
	q_value: tensor([[-24.7114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.349356500159284, distance: 1.3292909438517677 entropy 10.777939044930937
epoch: 39, step: 50
	action: tensor([[  8547.9894,   6143.3975, -16503.3201,  10633.9882, -10186.1463,
          -7787.8248,  12153.6398]], dtype=torch.float64)
	q_value: tensor([[-21.4260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7805271602875123, distance: 0.5361015769775775 entropy 10.770503524997142
epoch: 39, step: 51
	action: tensor([[ 1683.9172,  -349.8308, 13387.8350, 12970.4170,  8023.0543, -7590.2086,
          4467.5000]], dtype=torch.float64)
	q_value: tensor([[-26.8829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6233826511193946, distance: 0.7022744272574513 entropy 10.690042093501726
epoch: 39, step: 52
	action: tensor([[  4358.8922, -18258.3930,  -9174.6413, -13103.9465,   7101.0483,
         -14790.2643,   2154.0362]], dtype=torch.float64)
	q_value: tensor([[-21.0169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33894807309753183, distance: 1.3241541944740953 entropy 10.552054540455092
epoch: 39, step: 53
	action: tensor([[-14017.7219,  18005.3319,  -1416.5333,  -5897.5030,   2899.7387,
         -11767.0838, -16573.3891]], dtype=torch.float64)
	q_value: tensor([[-20.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5685348308297644, distance: 0.7516740621259602 entropy 10.417918161804902
epoch: 39, step: 54
	action: tensor([[ 5431.5681,  5456.8530, -9084.8730,  6783.9770, -2392.5235,  3170.1735,
         12404.8612]], dtype=torch.float64)
	q_value: tensor([[-31.3987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926622500403745, distance: 0.9624321265245768 entropy 10.908481145535388
epoch: 39, step: 55
	action: tensor([[-10953.2025,  -4855.0594,    117.1765,  -1211.8291,  11132.1653,
           9436.3683,  -1357.5305]], dtype=torch.float64)
	q_value: tensor([[-18.8513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.290375162978497
epoch: 39, step: 56
	action: tensor([[-1.1137e+04, -6.9277e+02, -6.7985e+00,  3.7035e+03, -7.6023e+03,
         -6.6881e+03,  2.0563e+03]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 57
	action: tensor([[-10060.6872,  -6519.6997,  -2988.2441,  -4277.4611,   4008.3247,
          11886.8268,  -8062.5276]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 58
	action: tensor([[-6382.8753,  1325.7734, -4029.0999,  1368.4616, -7434.9245, -4274.2451,
           721.9812]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 59
	action: tensor([[-11755.3948, -15073.9064,   4322.7147,  -2223.0286,  -5034.2490,
          -6011.6982,  -4587.9323]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 60
	action: tensor([[ 1918.9978,  1923.7032, -1396.8022,  1674.4573, -8256.9072,  -815.4788,
          -421.8645]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 61
	action: tensor([[-15124.4260,  -7618.6665,  -1036.3752,   5453.8208,  -1514.4302,
            733.7302,   -599.9088]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05142563038974224, distance: 1.114531599459927 entropy 10.205756468212739
epoch: 39, step: 62
	action: tensor([[-13402.7137,   6731.2667, -10077.4788,  -9047.7828,  -3855.8287,
           9913.4691,   1439.0223]], dtype=torch.float64)
	q_value: tensor([[-19.8063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.056215205697908166, distance: 1.176069266605136 entropy 10.64754969787846
epoch: 39, step: 63
	action: tensor([[-15519.1432,  -8103.9457,  -1974.4181,     41.1036,  14868.1253,
           8206.7196,  -2647.3522]], dtype=torch.float64)
	q_value: tensor([[-22.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7762853271297796, distance: 1.5251517501980938 entropy 10.457506623325175
epoch: 39, step: 64
	action: tensor([[-21118.1587, -10612.3739, -14707.9965,  10379.6385,   3792.9379,
           3347.5599,  -1049.3156]], dtype=torch.float64)
	q_value: tensor([[-20.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.699464198669399, distance: 1.4918072151747617 entropy 10.674839167186661
epoch: 39, step: 65
	action: tensor([[-19529.1264, -23370.9457,   1412.1995,  -2110.7378, -14588.5445,
           1338.1795,  16671.6873]], dtype=torch.float64)
	q_value: tensor([[-20.5572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0250250962946068, distance: 1.6284405122114751 entropy 10.699968789423748
epoch: 39, step: 66
	action: tensor([[-1634.8865, -2702.7555, -5104.1761, -5002.4424,  1071.1215, -3437.8742,
           703.0109]], dtype=torch.float64)
	q_value: tensor([[-17.8382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4707218617424469, distance: 1.387784291399758 entropy 10.55435340648465
epoch: 39, step: 67
	action: tensor([[ -6094.0434, -37034.7768,   2972.6539,   9440.3384, -10526.1142,
          -9017.9095,  11559.2422]], dtype=torch.float64)
	q_value: tensor([[-24.7232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7165778647087073, distance: 1.4992996764310496 entropy 10.834200108064609
epoch: 39, step: 68
	action: tensor([[23441.5535, -4964.1504, -6992.6504, -5490.1707, -1230.9887, -2336.7237,
          4666.0581]], dtype=torch.float64)
	q_value: tensor([[-19.1017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41739420296625596, distance: 0.8734621575799663 entropy 10.663698218140592
epoch: 39, step: 69
	action: tensor([[ 5749.8743, -6597.2489, -5399.9441,  3015.6744, -1659.1697, 14928.9621,
         23510.3258]], dtype=torch.float64)
	q_value: tensor([[-21.5757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3150172239293556, distance: 1.3122676231933548 entropy 10.547020691531827
epoch: 39, step: 70
	action: tensor([[-14359.9305,  -8595.7871,  -5302.8282,  20164.1205,   6827.8552,
          13616.2154, -11453.2389]], dtype=torch.float64)
	q_value: tensor([[-24.5983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2777977454972287, distance: 1.293563497902509 entropy 10.545346510034802
epoch: 39, step: 71
	action: tensor([[  2062.7261,  10906.5970,   1205.9753,  17922.3823,    716.2409,
           8218.8318, -11176.7607]], dtype=torch.float64)
	q_value: tensor([[-20.2611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5282646848198307, distance: 0.7859698525778726 entropy 10.654569344172542
epoch: 39, step: 72
	action: tensor([[-4930.4348, -4097.3499,  3655.9915,  2411.8790,  3094.1141, -1412.4025,
          2422.5414]], dtype=torch.float64)
	q_value: tensor([[-19.6278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08775250804403933, distance: 1.1934981218961003 entropy 10.405947892367468
epoch: 39, step: 73
	action: tensor([[-14403.8758,  -2838.3968,  -6991.5569,  20945.7145,    688.7420,
          15550.9406,  -4011.4898]], dtype=torch.float64)
	q_value: tensor([[-19.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7873880247794642, distance: 1.5299108168890614 entropy 10.64180364903942
epoch: 39, step: 74
	action: tensor([[-11364.6283,  -2970.7245,  21599.0213,   1802.9966,  -3510.3992,
         -21120.6683,  16934.9828]], dtype=torch.float64)
	q_value: tensor([[-22.8654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05168723661610519, distance: 1.1735456687761099 entropy 10.891990747773038
epoch: 39, step: 75
	action: tensor([[ -8945.2759,  -9598.4777, -10739.2992,  32332.9028,  -2854.7516,
          13223.9999, -12101.8518]], dtype=torch.float64)
	q_value: tensor([[-25.4254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47571782213345815, distance: 1.3901394060849592 entropy 10.805379766010136
epoch: 39, step: 76
	action: tensor([[-34130.0337,  -1147.0187, -11756.5973,  -3187.8571,  -9554.9592,
          25246.3167, -31649.4138]], dtype=torch.float64)
	q_value: tensor([[-20.5362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5012715702242276, distance: 1.4021236781861255 entropy 10.794626934648432
epoch: 39, step: 77
	action: tensor([[  2276.8161, -10015.7373,  14194.9024,   9424.0286,   -487.0519,
         -15129.1495, -11030.4745]], dtype=torch.float64)
	q_value: tensor([[-19.5072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5244624781274414, distance: 0.7891309708848983 entropy 10.533118965750017
epoch: 39, step: 78
	action: tensor([[ 7214.0281, -6645.2634, 18136.8124,  2646.1976, -3698.4732, 13826.9872,
         21709.8855]], dtype=torch.float64)
	q_value: tensor([[-25.8997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.275036353356664, distance: 1.2921650105365112 entropy 10.656013133244057
epoch: 39, step: 79
	action: tensor([[-6446.6852,  2420.4115, -2951.3133,  6078.4653, -7528.5852,  4975.9221,
          9299.1621]], dtype=torch.float64)
	q_value: tensor([[-33.2635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.81557967581556
epoch: 39, step: 80
	action: tensor([[ -8806.4149, -12069.2201,  -7372.2395,  -6746.7081,  13082.9647,
           -829.1578,  -2501.3759]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048374378004908, distance: 0.9541131486203764 entropy 10.205756468212739
epoch: 39, step: 81
	action: tensor([[-18935.6095, -24136.4211,  -6549.8382,  11649.2608,  11781.6152,
          25539.8683,   2103.9344]], dtype=torch.float64)
	q_value: tensor([[-22.4576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40105587454842473, distance: 1.3545168781336756 entropy 10.653396271723555
epoch: 39, step: 82
	action: tensor([[-27348.2513, -17265.7827,  -6526.2022,  -6962.8318,  15024.2426,
          13749.7795,   4763.8612]], dtype=torch.float64)
	q_value: tensor([[-20.6344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5724789039214495, distance: 1.4349907683059944 entropy 10.69560839306521
epoch: 39, step: 83
	action: tensor([[ -3457.9256,  -5251.2044, -29791.4517,  -3035.4067, -18851.3090,
          -6213.7653,  -3059.6122]], dtype=torch.float64)
	q_value: tensor([[-20.4896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47160084221999354, distance: 1.3881989357747337 entropy 10.642726590087122
epoch: 39, step: 84
	action: tensor([[-11264.8078, -21555.5629,  -7472.0813,   8271.2249,   7761.8497,
           -116.5445,  10014.6315]], dtype=torch.float64)
	q_value: tensor([[-25.1840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4552378531252288, distance: 1.3804595474301482 entropy 10.82249274697545
epoch: 39, step: 85
	action: tensor([[-19026.5699, -29366.2697,   1404.3241, -13582.0440,  -9950.5397,
          -6376.0982,  -5035.6532]], dtype=torch.float64)
	q_value: tensor([[-18.8916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3157644431219009, distance: 1.3126403986533468 entropy 10.641092489049441
epoch: 39, step: 86
	action: tensor([[ -7105.7285, -13405.0568,  -8876.1681,   2961.9379,   5548.2064,
         -12665.3899,   1186.2329]], dtype=torch.float64)
	q_value: tensor([[-26.8710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9399601656879122, distance: 1.5938707454016643 entropy 10.910905831901347
epoch: 39, step: 87
	action: tensor([[ 1956.7183,  4012.5866, -8692.7762, -5096.8892,  9487.7609,   555.9265,
          4804.0233]], dtype=torch.float64)
	q_value: tensor([[-19.4019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.527937864999146
epoch: 39, step: 88
	action: tensor([[  2292.1826, -12425.4158, -11503.9141,  -5127.3114,   6080.5881,
           3502.9704,  -5274.3671]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1923824656110107, distance: 1.249581203296344 entropy 10.205756468212739
epoch: 39, step: 89
	action: tensor([[ 12309.5449,   2626.9480, -14338.0708,   2454.4610,   1530.1791,
           9612.8572,   9692.1105]], dtype=torch.float64)
	q_value: tensor([[-20.9011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.738576430842739, distance: 0.5850986056079535 entropy 10.566411314062067
epoch: 39, step: 90
	action: tensor([[ -4503.9885,   2918.7235,  -1700.0489,   4125.7325,  11099.3971,
          -3475.4041, -11671.1912]], dtype=torch.float64)
	q_value: tensor([[-23.1683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2344108620221964, distance: 1.0012777714045078 entropy 10.735592435192219
epoch: 39, step: 91
	action: tensor([[-22203.4671, -11754.9332,  -5379.9731, -10376.2927, -20871.3411,
           6494.0277,  -6901.6077]], dtype=torch.float64)
	q_value: tensor([[-24.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8514418448397763, distance: 1.5570828840074464 entropy 10.882538360896572
epoch: 39, step: 92
	action: tensor([[-13325.9870, -29227.4374, -13367.8782,   2301.8287,  -9153.1403,
          -8853.3595,  -9366.6598]], dtype=torch.float64)
	q_value: tensor([[-23.2479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0840046492301203, distance: 1.1914402447289816 entropy 10.740402818890578
epoch: 39, step: 93
	action: tensor([[ 23871.9389,  -5470.0334,    467.0226,   3534.9078, -22137.2708,
         -16660.4477, -17966.5774]], dtype=torch.float64)
	q_value: tensor([[-25.4433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23421519525177792, distance: 1.2713119749021746 entropy 10.853789672296065
epoch: 39, step: 94
	action: tensor([[11891.5867, -4245.3513, -2517.3512,  1831.4471, 10674.4823, -5160.8575,
         11823.0946]], dtype=torch.float64)
	q_value: tensor([[-26.9343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3705212079846998, distance: 1.3396753716568814 entropy 10.769969189954805
epoch: 39, step: 95
	action: tensor([[ -5112.6847,  -5142.5762,  -2441.6577, -26389.0065,  -2146.2704,
          -4345.6932, -13428.2753]], dtype=torch.float64)
	q_value: tensor([[-28.5835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49806449697053523, distance: 1.4006252426209571 entropy 10.690924517809236
epoch: 39, step: 96
	action: tensor([[  5842.5472,   7266.8321, -12795.8378,   -130.0238,  -9819.0716,
           5069.0598,   7190.7655]], dtype=torch.float64)
	q_value: tensor([[-22.5290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.703919290571411
epoch: 39, step: 97
	action: tensor([[  418.4060, -8293.1503, -3049.0758,   619.7727, -1777.2991,  1184.6980,
         -4758.9830]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6030981892681054, distance: 0.7209385245424411 entropy 10.205756468212739
epoch: 39, step: 98
	action: tensor([[ 3903.7201, -9871.0399,  4976.0398,   501.5312, -5236.2100,   -67.7017,
         -5494.7786]], dtype=torch.float64)
	q_value: tensor([[-20.3068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7413862289313402, distance: 0.5819457714373182 entropy 10.58076818091242
epoch: 39, step: 99
	action: tensor([[-12762.3972, -26838.8312,  17585.6463, -11704.7634,  14794.8577,
         -20953.6216,   5733.2776]], dtype=torch.float64)
	q_value: tensor([[-24.7406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23983822265163734, distance: 1.2742047031691943 entropy 10.826191518354538
epoch: 39, step: 100
	action: tensor([[  -421.7087, -17229.1589,   5886.8848, -14845.4772,  19990.5670,
            362.0194,  -3287.0338]], dtype=torch.float64)
	q_value: tensor([[-20.8240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1643032175575765, distance: 1.0461189236938995 entropy 10.783854139453704
epoch: 39, step: 101
	action: tensor([[  5273.5494, -21383.7707,  17033.1710,   7746.2789,  -5960.6869,
          14719.3169,   4974.6302]], dtype=torch.float64)
	q_value: tensor([[-22.3025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026798104583591775, distance: 1.128907000897371 entropy 10.676124888319716
epoch: 39, step: 102
	action: tensor([[-2902.4589, 10869.3749,  1595.5325, 19793.2805,  9443.6916,  7161.3577,
         -1828.9715]], dtype=torch.float64)
	q_value: tensor([[-20.8876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.503171610562074
epoch: 39, step: 103
	action: tensor([[-1324.1543, -3278.0583,  3475.8576, 11013.3935, -1499.7970, -6667.4164,
         -1312.6848]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026920903907758698, distance: 1.159645348925557 entropy 10.205756468212739
epoch: 39, step: 104
	action: tensor([[ -7261.8528, -11186.4565,  -3511.9114,  -8627.1305,   4762.0892,
          10122.9044,    400.3048]], dtype=torch.float64)
	q_value: tensor([[-20.7833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9265270061732616, distance: 1.5883428192094688 entropy 10.529197343960691
epoch: 39, step: 105
	action: tensor([[ 8645.6398,  3656.8423,  9981.7403,  3393.6536, -8407.5269,  3321.8249,
         -1209.2493]], dtype=torch.float64)
	q_value: tensor([[-17.0764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.376269996129327
epoch: 39, step: 106
	action: tensor([[  3755.0822,   -175.7958,  -1874.5924,  -2559.3332,    180.4144,
         -10064.3634,   5939.7532]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.205756468212739
epoch: 39, step: 107
	action: tensor([[-14001.4992,   -732.2799,  12312.8076,   1473.0927,   -794.6766,
          17631.8410,   4508.2803]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32678291595053843, distance: 1.3181250958362558 entropy 10.205756468212739
epoch: 39, step: 108
	action: tensor([[-15350.3450,   2637.3615,  10382.4798,  -5652.5254,  36376.2061,
           -699.4749,   8620.6270]], dtype=torch.float64)
	q_value: tensor([[-21.6542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34640345894049707, distance: 0.9251487489015002 entropy 10.897969985179547
epoch: 39, step: 109
	action: tensor([[-9225.8008, -7839.2835, -6271.1556, -9348.4887,  1215.3153, 15883.8057,
          8936.1925]], dtype=torch.float64)
	q_value: tensor([[-25.7083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06969423260958474, distance: 1.1037470350187888 entropy 10.671187478953675
epoch: 39, step: 110
	action: tensor([[  8108.2594,   9974.2640,  -4552.1649,  -9655.5906,  -2491.9171,
          -1517.4418, -12572.2343]], dtype=torch.float64)
	q_value: tensor([[-23.2494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2454811699116013, distance: 0.9940122454819033 entropy 10.80708977249618
epoch: 39, step: 111
	action: tensor([[-18061.3873, -22060.6958, -14073.3175, -37812.2743,  -7292.2030,
          16491.8059,   -335.9687]], dtype=torch.float64)
	q_value: tensor([[-26.9818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0784677227032583, distance: 1.09853011951182 entropy 10.731596892027255
epoch: 39, step: 112
	action: tensor([[ -5885.7761,   1802.0463,  24703.7370,    209.2356, -15184.2657,
            341.3269,  22981.4910]], dtype=torch.float64)
	q_value: tensor([[-24.0796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45978522031241587, distance: 0.8410850704919702 entropy 10.849860375928552
epoch: 39, step: 113
	action: tensor([[ 11154.9650, -24765.2177, -13770.1660,  -9790.6876,  -3739.4384,
           1233.4120,  -3400.8326]], dtype=torch.float64)
	q_value: tensor([[-28.6512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.794550772596853
epoch: 39, step: 114
	action: tensor([[ -6158.3406,  -7993.2718, -14309.8469,   -952.7111,  -1771.9249,
           6555.1277,   1958.4177]], dtype=torch.float64)
	q_value: tensor([[-22.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6656736720231138, distance: 1.4769019159535142 entropy 10.205756468212739
epoch: 39, step: 115
	action: tensor([[-11397.1075,  -4130.6552,    586.6287,  10670.5297,   6023.0602,
           9344.8411,  -5891.7432]], dtype=torch.float64)
	q_value: tensor([[-25.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8958332955959263, distance: 1.5756391616397214 entropy 10.78010573846917
epoch: 39, step: 116
	action: tensor([[ -1770.0795, -23801.6142, -11796.4953, -10566.9870,  -6621.2571,
         -17953.7276,   6044.2538]], dtype=torch.float64)
	q_value: tensor([[-21.7784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05082407715756554, distance: 1.1148849422483735 entropy 10.84619846618297
epoch: 39, step: 117
	action: tensor([[  2708.9116, -25757.5381,   3552.6062,  -2671.0013, -27937.5276,
          -5214.5732,   -263.7218]], dtype=torch.float64)
	q_value: tensor([[-25.8705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24760531415551368, distance: 0.9926120726578593 entropy 10.85043182362421
epoch: 39, step: 118
	action: tensor([[ -4495.6249, -13459.7723,   7230.7821,  18863.5809, -23854.1982,
           6343.4624,  -9937.6800]], dtype=torch.float64)
	q_value: tensor([[-23.8597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8557184243059164, distance: 1.5588801718353025 entropy 10.718158128745586
epoch: 39, step: 119
	action: tensor([[-12352.6860, -13565.2216, -18505.2308,  13910.6475, -12342.5818,
         -14048.9924,   5371.4763]], dtype=torch.float64)
	q_value: tensor([[-22.4551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10120085723840222, distance: 1.2008533215714396 entropy 10.841286951082893
epoch: 39, step: 120
	action: tensor([[  5443.1904,    463.2465,   8147.0718,   1067.6649, -12143.6422,
           2127.1701,  12421.1715]], dtype=torch.float64)
	q_value: tensor([[-20.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5527688340148558, distance: 0.765284158522849 entropy 10.802954299930482
epoch: 39, step: 121
	action: tensor([[-19131.6081,  -7068.7340, -12131.7347,   3204.1635,  -7133.3151,
          -5795.7348,  12922.7449]], dtype=torch.float64)
	q_value: tensor([[-21.2793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06149364675200042, distance: 1.1086010912112778 entropy 10.588685050313968
epoch: 39, step: 122
	action: tensor([[-21052.9155,   8049.9798,   6257.8119,   8277.3218,  11849.5597,
         -24646.7966,  27513.2631]], dtype=torch.float64)
	q_value: tensor([[-23.4370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5459869938003151, distance: 0.7710647341582839 entropy 10.84111929516293
epoch: 39, step: 123
	action: tensor([[ -4206.5584,   1344.3972,   7985.8434,  31333.7910, -13758.0688,
          16059.2694,   7803.3629]], dtype=torch.float64)
	q_value: tensor([[-21.2577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6699999543183428, distance: 0.6573757711309 entropy 10.541596685054316
epoch: 39, step: 124
	action: tensor([[ -7709.2424, -29967.3641,   5871.8756,  10047.9077,   1449.8210,
           7122.7828,   8630.7175]], dtype=torch.float64)
	q_value: tensor([[-22.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08901109574308608, distance: 1.1941883927924197 entropy 10.589479089111387
epoch: 39, step: 125
	action: tensor([[-14636.7429, -15480.2330,  15632.5357,   8701.9142,  -1020.7045,
          -1771.6568,  15900.2276]], dtype=torch.float64)
	q_value: tensor([[-20.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17307487213172446, distance: 1.0406142956584927 entropy 10.817826278288916
epoch: 39, step: 126
	action: tensor([[ -8189.7963, -12808.5756, -28656.3133,  10537.4728, -20399.2804,
            908.9757, -14835.3790]], dtype=torch.float64)
	q_value: tensor([[-22.3226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15897790354111652, distance: 1.2319533739224982 entropy 10.921527981860644
epoch: 39, step: 127
	action: tensor([[-13625.6369,  11139.1264, -14275.3734,  -2643.3381,   8582.8369,
          13445.8565,   4771.9603]], dtype=torch.float64)
	q_value: tensor([[-21.2020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4469586606249647, distance: 0.8510116219088032 entropy 10.695693234447992
LOSS epoch 39 actor 214.46175543767265 critic 354.65712762314104
epoch: 40, step: 0
	action: tensor([[  8785.2173, -14371.6016, -11621.5932,  -2073.4365,  -5291.3641,
           7482.5434,  20941.4991]], dtype=torch.float64)
	q_value: tensor([[-29.2246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20538827638278834, distance: 1.02007987006837 entropy 10.636682374710267
epoch: 40, step: 1
	action: tensor([[-1467.7066,  9182.5007,  2913.3276,  -930.9910, 14218.2864,  8818.3889,
         -4973.0432]], dtype=torch.float64)
	q_value: tensor([[-23.4069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.482534017031783
epoch: 40, step: 2
	action: tensor([[ -4374.2934,  -4854.4036,  -1276.5697, -12465.0765,   -210.4245,
            -30.9827, -11997.1234]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20807903372381098, distance: 1.0183512805593447 entropy 10.256455048509496
epoch: 40, step: 3
	action: tensor([[ 11932.5524,  -8677.8288, -15228.3514,   3829.9941,  11683.2970,
           4990.5596,  12141.9147]], dtype=torch.float64)
	q_value: tensor([[-26.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06400879913390645, distance: 1.1071145955145636 entropy 10.77217935383446
epoch: 40, step: 4
	action: tensor([[-11840.6116, -12243.0530,   2608.6089,  34577.8607,   2766.4205,
           9657.3677,   1472.4361]], dtype=torch.float64)
	q_value: tensor([[-30.3169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04802957415865272, distance: 1.1715031543988323 entropy 10.91735335318954
epoch: 40, step: 5
	action: tensor([[14114.3766, -7003.4382,  4009.9288, -3611.8714, -6561.6183, -3040.3123,
          9060.6651]], dtype=torch.float64)
	q_value: tensor([[-27.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32904790595877187, distance: 1.3192497210656136 entropy 10.820178334032516
epoch: 40, step: 6
	action: tensor([[-12412.0335,  -1952.9371,  14802.7768,   2782.9339,   3524.7221,
          -1672.6078,  -5043.5704]], dtype=torch.float64)
	q_value: tensor([[-23.8114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5701293897017017, distance: 1.4339183241735942 entropy 10.520412757533908
epoch: 40, step: 7
	action: tensor([[-19212.7237, -17044.6734,  15617.7119,   7143.2615,  12539.6727,
          -7811.3660, -11049.4045]], dtype=torch.float64)
	q_value: tensor([[-21.9594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5766226942334676, distance: 1.4368802653811492 entropy 10.652830449373951
epoch: 40, step: 8
	action: tensor([[-14164.3762,    851.1796,  23386.6551,  -3118.5810,   9242.8988,
          10399.8057, -14472.9623]], dtype=torch.float64)
	q_value: tensor([[-25.7632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39791929925230984, distance: 0.8879408708289683 entropy 10.983554026927193
epoch: 40, step: 9
	action: tensor([[  4278.9128,  12845.5311, -12275.9229,   5832.9370,   1381.7339,
          12726.2391,  23704.2531]], dtype=torch.float64)
	q_value: tensor([[-29.1626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3508620117152159, distance: 0.9219878669203319 entropy 10.586522998808801
epoch: 40, step: 10
	action: tensor([[-19143.1674, -13726.3261,   4716.9627,   9575.9101,  -8057.8562,
          -8370.9211,   9508.4116]], dtype=torch.float64)
	q_value: tensor([[-24.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16800352334408764, distance: 1.0438003468674575 entropy 10.612728829223302
epoch: 40, step: 11
	action: tensor([[-17520.7741,  -3106.1887,   5370.8363,  16781.8057, -23363.7485,
           1868.8949,  -2236.2033]], dtype=torch.float64)
	q_value: tensor([[-25.4693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7986046999847571, distance: 1.5347037533200039 entropy 10.985055088373796
epoch: 40, step: 12
	action: tensor([[-17956.3754,  -6729.6995,  -4151.4802,  10680.8831,   -241.3373,
          -7022.8929,  -6559.1299]], dtype=torch.float64)
	q_value: tensor([[-25.1868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2836514288697102, distance: 0.9685429689444895 entropy 10.909561438452302
epoch: 40, step: 13
	action: tensor([[-16525.9918,    200.7940,  -7357.8670,  19524.6500,  -7003.0119,
         -12895.2686,  14198.1273]], dtype=torch.float64)
	q_value: tensor([[-23.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0021188761802714007, distance: 1.143131249216616 entropy 10.81073818224933
epoch: 40, step: 14
	action: tensor([[ -3785.2122, -10351.1068,    -72.7724,  -1771.9726,   5578.6549,
          -2669.8514, -27602.3623]], dtype=torch.float64)
	q_value: tensor([[-26.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33181876912078856, distance: 1.3206242227486154 entropy 10.746996712798348
epoch: 40, step: 15
	action: tensor([[ -1197.4880, -10709.0950, -11411.9601,   4170.7575,   8677.4282,
          -7051.9188,   3216.7472]], dtype=torch.float64)
	q_value: tensor([[-20.8503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8777276610038653, distance: 1.5680972574408067 entropy 10.505605698485606
epoch: 40, step: 16
	action: tensor([[  6240.8463,  -5565.8875,  -9430.0689,  10265.2517, -18333.5059,
           7256.5797,   1526.9116]], dtype=torch.float64)
	q_value: tensor([[-21.3413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.617079355258669
epoch: 40, step: 17
	action: tensor([[  2630.7362, -14849.2690,   7726.7396,   4409.6890,   -487.7812,
           4597.8882,   4474.3494]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15411525716222407, distance: 1.052476212953507 entropy 10.256455048509496
epoch: 40, step: 18
	action: tensor([[  2941.2985, -12316.3600,  -2206.9866,  19021.1445, -17745.7164,
           2830.2696,   -593.9056]], dtype=torch.float64)
	q_value: tensor([[-25.6071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017588857018237647, distance: 1.134235753836501 entropy 10.733279986858827
epoch: 40, step: 19
	action: tensor([[  7485.7802,  -6222.8448, -10860.5610,   2423.4529,  -4184.3150,
           5561.9758,   8940.3321]], dtype=torch.float64)
	q_value: tensor([[-27.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4598380577550719, distance: 0.8410439369719579 entropy 10.874256786332987
epoch: 40, step: 20
	action: tensor([[-10997.4828,  -4755.7857,   4681.7397,  16139.1669,  11648.8509,
          29893.1344,   1460.2185]], dtype=torch.float64)
	q_value: tensor([[-25.5092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13390422978340866, distance: 1.0649755864033892 entropy 10.82246737533819
epoch: 40, step: 21
	action: tensor([[-20199.1038,  -6863.3577,  10176.3846,  -5693.7867,  -4329.8122,
         -13072.2873,  -8491.8891]], dtype=torch.float64)
	q_value: tensor([[-27.3300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8939214936414919, distance: 1.5748445058438885 entropy 10.810145153941269
epoch: 40, step: 22
	action: tensor([[-18861.9650,  -9410.2283,  21839.6474,   3593.7383,  -5068.9827,
           9278.6608,    807.3950]], dtype=torch.float64)
	q_value: tensor([[-25.7984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7702601962782243, distance: 1.5225629080779612 entropy 10.883172141312167
epoch: 40, step: 23
	action: tensor([[  6772.1033, -19035.0453, -13492.2563,   9973.0021,   4622.3959,
          14115.5880, -19128.2880]], dtype=torch.float64)
	q_value: tensor([[-25.7464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23825806600263344, distance: 1.2733924661641725 entropy 10.886593311501896
epoch: 40, step: 24
	action: tensor([[-22237.1976,  -7110.1140,  25770.1979, -17930.6938,  15123.6442,
          19511.0002,  -4702.4460]], dtype=torch.float64)
	q_value: tensor([[-26.0128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7776398744280368, distance: 1.5257331591792904 entropy 10.908476963066454
epoch: 40, step: 25
	action: tensor([[-12110.6766, -14989.5042, -13843.9909,  -3525.9801,   4054.8911,
         -32574.5916, -10159.5864]], dtype=torch.float64)
	q_value: tensor([[-27.6035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16553216010612926, distance: 1.2354319367316389 entropy 10.772418674829042
epoch: 40, step: 26
	action: tensor([[  1930.4789,   1944.0233, -12897.0500,  24559.8739,   3814.8250,
          -6785.2018,   6025.3938]], dtype=torch.float64)
	q_value: tensor([[-29.3238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.84630412884816
epoch: 40, step: 27
	action: tensor([[ -9492.2558, -22244.8567,  -1517.9711,  10398.4365,   -378.8501,
          11466.9223,  16083.1260]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44987769445623327, distance: 1.377914840218406 entropy 10.256455048509496
epoch: 40, step: 28
	action: tensor([[ 10399.4996, -23714.7696, -23143.1710,  32835.7739,   4694.0391,
         -14607.5799, -11709.4643]], dtype=torch.float64)
	q_value: tensor([[-28.3350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37553903182712944, distance: 1.3421255757296637 entropy 10.848254029164147
epoch: 40, step: 29
	action: tensor([[-12128.1668,   4351.0471,  -6820.3603,  13814.1287, -15513.4333,
         -10078.4272,  17388.3204]], dtype=torch.float64)
	q_value: tensor([[-25.7660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09138590016510939, distance: 1.195489765880006 entropy 10.77789269930154
epoch: 40, step: 30
	action: tensor([[-14566.1871, -47953.0010,  -1939.2578,  16356.6857, -19410.4343,
          16011.2869, -10320.8721]], dtype=torch.float64)
	q_value: tensor([[-30.6397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10806731435174033, distance: 1.2045914198375225 entropy 10.988612808260518
epoch: 40, step: 31
	action: tensor([[  5524.5722,  -6368.0660, -21248.8547,  15173.0160, -27640.8311,
          -4404.3864,  -4705.4173]], dtype=torch.float64)
	q_value: tensor([[-26.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2802205864391778, distance: 0.9708595430912846 entropy 10.87640208442276
epoch: 40, step: 32
	action: tensor([[-12215.8877,  -2644.3682,   8447.7482,  -1634.7135,  10440.1746,
          -6818.0563,  16520.4943]], dtype=torch.float64)
	q_value: tensor([[-24.4041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.660089351081744, distance: 1.4744241141292567 entropy 10.505613377334646
epoch: 40, step: 33
	action: tensor([[ -6503.6851, -14338.5399, -12378.5283,  -7333.3839, -18493.7169,
           6061.5331,   3137.5992]], dtype=torch.float64)
	q_value: tensor([[-25.3277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20908998062769202, distance: 1.017701071835112 entropy 10.863618204192694
epoch: 40, step: 34
	action: tensor([[ 10377.3631,  -6328.7463,  -9849.8933,  14057.4860, -12117.6464,
           5648.3745,   -988.8317]], dtype=torch.float64)
	q_value: tensor([[-29.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1358361037562632, distance: 1.2195918901754388 entropy 10.907621992769922
epoch: 40, step: 35
	action: tensor([[  -893.4067, -20521.4881,  -3096.5582,   3045.9606,   6399.6390,
          14114.8546,  11096.8538]], dtype=torch.float64)
	q_value: tensor([[-31.1884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32331858504493605, distance: 1.3164031087017176 entropy 10.789516878108653
epoch: 40, step: 36
	action: tensor([[-14431.4915,  10828.8852,  16866.3024,   -569.3021, -22653.0599,
          12564.7963,  -5487.4705]], dtype=torch.float64)
	q_value: tensor([[-26.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.600849871334256, distance: 0.7229775805307268 entropy 10.961499298920645
epoch: 40, step: 37
	action: tensor([[ 10897.2507, -22784.2984,  -8371.2575,  -6945.8569,  -4502.7200,
           1490.1378,  -2793.5235]], dtype=torch.float64)
	q_value: tensor([[-27.8520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4930631674818713, distance: 0.8147673489493437 entropy 10.749151139491923
epoch: 40, step: 38
	action: tensor([[ -9733.5283, -18205.7150, -28828.5260, -12213.5074,   4810.3600,
          20064.9197,  -6755.7374]], dtype=torch.float64)
	q_value: tensor([[-27.4299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08635071273242656, distance: 1.0938214939557902 entropy 10.892728331998585
epoch: 40, step: 39
	action: tensor([[-11971.0929,  19152.9345,  -1477.4991,  -4805.7269,  -5207.9059,
          27014.6438,  25684.7225]], dtype=torch.float64)
	q_value: tensor([[-28.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5505753916399175, distance: 0.7671585287395164 entropy 10.824476921160255
epoch: 40, step: 40
	action: tensor([[ 7673.8999,   549.7678, -9330.9250, 13511.0811, -3091.0516,  6215.5226,
         -1544.5171]], dtype=torch.float64)
	q_value: tensor([[-32.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1869915133461557, distance: 1.031820691708022 entropy 10.624163097614263
epoch: 40, step: 41
	action: tensor([[ -179.1589, -3590.5321, -6142.5854,  4866.6928,   950.1035,   587.3675,
          4921.4915]], dtype=torch.float64)
	q_value: tensor([[-19.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24402487324626798, distance: 0.9949710544884323 entropy 10.197003553157105
epoch: 40, step: 42
	action: tensor([[ -8532.4868, -15070.3108,   9826.5086,  -5380.9895,  -1379.3064,
          -6704.9975,   6656.7695]], dtype=torch.float64)
	q_value: tensor([[-24.8508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0819472056435724, distance: 1.651169087991061 entropy 10.673282854376478
epoch: 40, step: 43
	action: tensor([[ 17528.7860, -13915.3961, -15328.4756, -10959.3902, -14732.1152,
            448.9721,  13329.1102]], dtype=torch.float64)
	q_value: tensor([[-25.5218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1461992578857445, distance: 1.2251429203418112 entropy 10.842552739599823
epoch: 40, step: 44
	action: tensor([[ -5619.2530, -22817.3842,  -2434.1732,   5998.1764,   9814.1882,
         -10747.1221,   5076.3473]], dtype=torch.float64)
	q_value: tensor([[-24.8925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9940968582993157, distance: 1.6159570659337124 entropy 10.736361357493267
epoch: 40, step: 45
	action: tensor([[ 11946.0169, -15430.1266,  -9136.1411,  -4547.0787,  11722.8427,
         -26514.4142,  -8989.2408]], dtype=torch.float64)
	q_value: tensor([[-24.5771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5502941340997543, distance: 0.7673985416018085 entropy 10.836829171214475
epoch: 40, step: 46
	action: tensor([[  2388.3726,  -6297.1754,  12038.8628,  27760.9234,   8279.5218,
         -13612.7379,   5092.6326]], dtype=torch.float64)
	q_value: tensor([[-27.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10474440914697802, distance: 1.202783881414097 entropy 10.82126994790751
epoch: 40, step: 47
	action: tensor([[ -1765.0638, -14817.9612, -13094.4219,   3950.6516,  12921.7084,
          35935.2536, -21274.7567]], dtype=torch.float64)
	q_value: tensor([[-28.0601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.732876592488491
epoch: 40, step: 48
	action: tensor([[-4090.5257, -9272.3783,   407.7810,  4011.4201, -6736.2373,  4007.3477,
         -3427.7132]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14502936705230973, distance: 1.058113589327065 entropy 10.256455048509496
epoch: 40, step: 49
	action: tensor([[ 16520.6604,  -1312.4440,   7001.0029,  -3383.1231,  -5385.1845,
          10233.6459, -25451.1990]], dtype=torch.float64)
	q_value: tensor([[-19.8975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04240652563706382, distance: 1.1683561636154363 entropy 10.527950661503839
epoch: 40, step: 50
	action: tensor([[  534.1138, -1226.0355, -9405.9723,  7565.9774, 23461.7898,  4034.0589,
          -784.2827]], dtype=torch.float64)
	q_value: tensor([[-23.1411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28670904465276636, distance: 0.966473725529713 entropy 10.441269357752233
epoch: 40, step: 51
	action: tensor([[ -373.9085, -9801.5213,  -329.8314,  9982.9304,  3214.7196,  9023.8849,
          3679.5712]], dtype=torch.float64)
	q_value: tensor([[-26.2903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.321621295260047, distance: 1.315558627837936 entropy 10.515270612487564
epoch: 40, step: 52
	action: tensor([[ -9143.1652, -16381.4037, -18994.5807, -23300.7239,   -367.2693,
          -1467.7673,  13811.9034]], dtype=torch.float64)
	q_value: tensor([[-25.3490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8295893389367932, distance: 1.5478665096572728 entropy 10.988021522752993
epoch: 40, step: 53
	action: tensor([[  -261.8299,  -9544.6217, -10944.6034, -22824.6840,   5886.3530,
          14991.5245,  -5782.9968]], dtype=torch.float64)
	q_value: tensor([[-24.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6367345854705111, distance: 1.4640159997678714 entropy 10.797146434426361
epoch: 40, step: 54
	action: tensor([[-11400.8297,  -3466.6124,  -9640.8016,  11411.2612, -23347.8681,
          -2676.0357,   7525.4106]], dtype=torch.float64)
	q_value: tensor([[-28.0228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6841686120943986, distance: 1.4850787295715966 entropy 10.782690370868181
epoch: 40, step: 55
	action: tensor([[-16794.6429,  11978.9383,  -6527.8123,   7983.9512,   -697.2776,
         -12114.1182,  -5083.7831]], dtype=torch.float64)
	q_value: tensor([[-21.3681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.657616556347939, distance: 0.6695963399587473 entropy 10.576742878387309
epoch: 40, step: 56
	action: tensor([[  -439.7608, -23894.6848,  11883.7977,  10589.7504,  -6168.8942,
           -638.4680,  -7700.1637]], dtype=torch.float64)
	q_value: tensor([[-24.2603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021250951480541236, distance: 1.156439534864248 entropy 10.55038651974708
epoch: 40, step: 57
	action: tensor([[-15144.0682, -11981.3918,   3647.0110,   -788.0022,    873.8162,
            633.0431,  15474.5318]], dtype=torch.float64)
	q_value: tensor([[-28.9286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6256776424287991, distance: 1.4590625476042063 entropy 10.838034944310648
epoch: 40, step: 58
	action: tensor([[ -3941.5447,  -1468.8192,   4326.6668,  14059.2063,  10610.8154,
           5049.3623, -19105.3975]], dtype=torch.float64)
	q_value: tensor([[-24.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14570957498081905, distance: 1.057692592139393 entropy 10.631088767305865
epoch: 40, step: 59
	action: tensor([[  7984.5423,   5947.7114,  23074.8795,  -3302.6062,  -1574.3550,
         -10906.0195,  17551.4565]], dtype=torch.float64)
	q_value: tensor([[-23.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.874527078110576
epoch: 40, step: 60
	action: tensor([[ 4030.8921, -6988.6284, -5376.0552, -6162.9232,  1162.8725,  4948.8728,
          3096.7806]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3231991019024598, distance: 0.9414280985510642 entropy 10.256455048509496
epoch: 40, step: 61
	action: tensor([[-2904.3448, 10576.2632, -5242.3144,  9088.6289,  5226.7041,  1365.3507,
         -1368.9227]], dtype=torch.float64)
	q_value: tensor([[-27.3527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.679096670618035
epoch: 40, step: 62
	action: tensor([[-1200.9114,  -311.9733, -1173.3028, -7219.4186,  -229.0658, -8143.4638,
         -3405.7191]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8342042788504584, distance: 1.5498174425526152 entropy 10.256455048509496
epoch: 40, step: 63
	action: tensor([[ -7693.1416, -16916.1391,  -9663.7963,  -8971.5469,   9962.7301,
         -22660.1061,  -3103.4636]], dtype=torch.float64)
	q_value: tensor([[-30.1062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38144826643186214, distance: 1.3450053324113296 entropy 10.95136242117563
epoch: 40, step: 64
	action: tensor([[-18461.6355,  11378.3726,  11177.3001,  15830.3302,  17203.0716,
          -3988.3250,   1539.2934]], dtype=torch.float64)
	q_value: tensor([[-27.4791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31745662947819897, distance: 0.945413543846903 entropy 10.838444403935041
epoch: 40, step: 65
	action: tensor([[ -5011.2430,  -4993.3861,  -1447.1611,    526.2975,   9600.6167,
         -10137.7044,   4985.1050]], dtype=torch.float64)
	q_value: tensor([[-24.4798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.647561958127963, distance: 1.468850417658621 entropy 10.41201794790507
epoch: 40, step: 66
	action: tensor([[-35309.0481, -27130.1787,    258.1957,    898.3798,   8546.1389,
          27076.4813,   8263.6915]], dtype=torch.float64)
	q_value: tensor([[-23.3831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08526862136226065, distance: 1.192134664504649 entropy 10.8187019564913
epoch: 40, step: 67
	action: tensor([[   276.5164, -28776.0087,  -3148.4165,   2890.3585,   9222.5993,
         -18337.8923,   2362.3230]], dtype=torch.float64)
	q_value: tensor([[-23.1245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.577082894766979, distance: 0.7441908375850014 entropy 10.68509912028287
epoch: 40, step: 68
	action: tensor([[  3607.1793, -20759.7018,  16746.5523,  11059.7260,  -1309.2066,
          23072.2357,   7520.9919]], dtype=torch.float64)
	q_value: tensor([[-21.7643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04531802096486992, distance: 1.1181139235000186 entropy 10.51481635595018
epoch: 40, step: 69
	action: tensor([[ -5533.7007,  -9705.6304, -12791.1190,  -6519.3352,  -8787.5499,
          17767.6358,  -9967.3881]], dtype=torch.float64)
	q_value: tensor([[-30.2606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4503143754345009, distance: 1.3781223280303492 entropy 10.711605490991563
epoch: 40, step: 70
	action: tensor([[-17351.8577, -10237.2601, -17790.2508,   9762.1314,  11134.3266,
          11577.7680, -13842.8604]], dtype=torch.float64)
	q_value: tensor([[-26.5076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06235061497246497, distance: 1.1080948331160911 entropy 10.742789990041283
epoch: 40, step: 71
	action: tensor([[-7348.0227, -9346.4714, -6770.2127,  1461.6709,  6212.8878, 14877.2301,
          8551.9141]], dtype=torch.float64)
	q_value: tensor([[-27.5120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15324675736226911, distance: 1.0530163816231357 entropy 10.770055326424679
epoch: 40, step: 72
	action: tensor([[-14172.8224,  -4170.2094,   2234.7357,   4084.8080,     44.3967,
          14899.8203,  14468.6632]], dtype=torch.float64)
	q_value: tensor([[-27.6778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027600610018543525, distance: 1.1600290628232097 entropy 10.84245374436127
epoch: 40, step: 73
	action: tensor([[  2787.1103,    180.7944,   1498.5545,   1141.2721,   8853.0974,
          27651.3110, -26854.6081]], dtype=torch.float64)
	q_value: tensor([[-24.1938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4152052309058848, distance: 0.8751015093535216 entropy 10.805082406179736
epoch: 40, step: 74
	action: tensor([[ -5363.3049, -24609.8631,  -9628.9975,  10518.6204, -25627.2212,
          -7109.9634,  18939.2500]], dtype=torch.float64)
	q_value: tensor([[-21.9553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24766514652449778, distance: 0.9925726043351347 entropy 10.707985680007313
epoch: 40, step: 75
	action: tensor([[   820.6179, -23429.8020,  -7238.9229,  22064.1859,  -3422.1475,
          15867.8135, -17099.8549]], dtype=torch.float64)
	q_value: tensor([[-31.5131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06308519116251421, distance: 1.1076606945250025 entropy 10.855156865570669
epoch: 40, step: 76
	action: tensor([[-11373.5432,  24370.5045,   1246.4760,  19558.0062, -20603.0458,
          -6714.1143,  27051.4881]], dtype=torch.float64)
	q_value: tensor([[-26.7380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.185390863912418, distance: 1.2459123223586939 entropy 10.840582411891305
epoch: 40, step: 77
	action: tensor([[ 18489.1391,  -3945.1498,  -9818.4622, -15117.3148,   6446.1999,
          -9558.5542,  -2134.8167]], dtype=torch.float64)
	q_value: tensor([[-28.8228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3790418507583959, distance: 0.9017535459534536 entropy 10.784048597289441
epoch: 40, step: 78
	action: tensor([[  2917.0021,   -693.1535,  -4844.6815, -12483.0801,  11193.3932,
           9864.7004,   8583.5357]], dtype=torch.float64)
	q_value: tensor([[-25.4654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48203101090391043, distance: 0.8235852743981569 entropy 10.505697157911742
epoch: 40, step: 79
	action: tensor([[ -6274.9209,  -6923.8566,   1744.3817,  25524.4644, -12717.4745,
          18921.0341,    187.0579]], dtype=torch.float64)
	q_value: tensor([[-24.9306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.473330804150238, distance: 1.38901465491111 entropy 10.703097307765452
epoch: 40, step: 80
	action: tensor([[-14872.3000, -13629.0867,  -5194.3041,   1751.5008,  -9918.1573,
           9219.6255,  -6276.9018]], dtype=torch.float64)
	q_value: tensor([[-20.5585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2162398034880193, distance: 1.262020179993202 entropy 10.542664818365596
epoch: 40, step: 81
	action: tensor([[-10171.4720,  -5170.4643, -13989.1290,   7425.3418, -31114.0773,
          -4444.1008,   5769.1876]], dtype=torch.float64)
	q_value: tensor([[-28.0860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11166862328073224, distance: 1.2065473419629231 entropy 10.917479966163441
epoch: 40, step: 82
	action: tensor([[-21321.8067,  -8640.6397,  -1511.9900,   8153.1094,  22440.6394,
           8969.7714,  -2691.0968]], dtype=torch.float64)
	q_value: tensor([[-23.4620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1494729343608443, distance: 1.2268912471888198 entropy 10.845687127575857
epoch: 40, step: 83
	action: tensor([[  5594.3708,   -326.9590, -19224.0290,  19850.4638,  22221.6980,
          -4205.9593,   2696.0914]], dtype=torch.float64)
	q_value: tensor([[-27.6523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1384511145859053, distance: 1.2209950030308363 entropy 10.918889801197986
epoch: 40, step: 84
	action: tensor([[-26659.5468,  -2051.2294,  -4706.9642,  -1449.4054, -17863.3284,
          12601.1936, -16795.5305]], dtype=torch.float64)
	q_value: tensor([[-28.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2268942562157656, distance: 1.7076799950499892 entropy 10.712990395175876
epoch: 40, step: 85
	action: tensor([[ 10930.3687,  -2306.2943,  -2715.0190, -11567.8850,  -7455.9504,
         -18906.1935,  16525.3877]], dtype=torch.float64)
	q_value: tensor([[-21.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38728176436716044, distance: 1.347842145220697 entropy 10.569015852259607
epoch: 40, step: 86
	action: tensor([[  2243.3132, -11462.2835,  -5132.7355,   7690.4313, -18842.4552,
          14481.4255,  25411.7761]], dtype=torch.float64)
	q_value: tensor([[-23.6347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1890507607928762, distance: 1.2478342185450872 entropy 10.601344886234784
epoch: 40, step: 87
	action: tensor([[  9475.6664, -13729.4922,   8265.4708,  -4551.4600,   5470.7883,
           3293.2923,    355.8874]], dtype=torch.float64)
	q_value: tensor([[-25.8674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3765482816659147, distance: 0.9035623086663782 entropy 10.49507387225803
epoch: 40, step: 88
	action: tensor([[  2451.8147, -12963.5570,    901.1764,  12651.1603,   2163.8616,
           9401.3943,   2764.9327]], dtype=torch.float64)
	q_value: tensor([[-27.7237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030741086694337394, distance: 1.1618003079082904 entropy 10.798902872012206
epoch: 40, step: 89
	action: tensor([[-19225.6052,   4428.1378,  -6748.8473,   2549.8373,   5440.3985,
         -13378.7592,  -5645.4532]], dtype=torch.float64)
	q_value: tensor([[-33.8412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02408201978547042, distance: 1.1304812224376346 entropy 10.739532301374842
epoch: 40, step: 90
	action: tensor([[-24075.2155, -30511.1086,  -7893.0090,  12144.6673,   8818.0635,
           6372.1771,  -7065.4727]], dtype=torch.float64)
	q_value: tensor([[-28.7409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.972671465700752, distance: 1.6072523691119776 entropy 10.930363071102462
epoch: 40, step: 91
	action: tensor([[-5016.7929,  2589.9972,  -174.3370,  4459.1906,  2961.8784, 22257.6779,
         -3808.7237]], dtype=torch.float64)
	q_value: tensor([[-27.0147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6784169419162716, distance: 0.6489381011503845 entropy 10.945552835853396
epoch: 40, step: 92
	action: tensor([[ 16799.4622,  -4252.6935, -14380.5402,   -346.2678,   4228.1925,
          -2222.2396,  15116.3712]], dtype=torch.float64)
	q_value: tensor([[-28.1239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.762065542730031
epoch: 40, step: 93
	action: tensor([[ -220.0398, -2620.0698,  1847.9840,  -142.6798, -6895.4497,  -849.7683,
           245.8427]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47747592530805893, distance: 1.3909672340678694 entropy 10.256455048509496
epoch: 40, step: 94
	action: tensor([[-3915.7771, -3907.4439,  2060.4810,  5685.1474, -1545.1667,  5072.2271,
         -5613.8377]], dtype=torch.float64)
	q_value: tensor([[-18.5532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21531817792282115, distance: 1.013686095002668 entropy 10.572366510326788
epoch: 40, step: 95
	action: tensor([[16365.0018, -7228.0873,  9372.9908, -7390.1000, -3652.2566,  -236.2810,
          9796.5504]], dtype=torch.float64)
	q_value: tensor([[-22.9387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3119552737405479, distance: 0.9492159527757569 entropy 10.746578407870286
epoch: 40, step: 96
	action: tensor([[  3459.4591,  -2402.8415,  -2876.3315, -13969.2247,  -4138.0242,
          11749.6157,  15948.8121]], dtype=torch.float64)
	q_value: tensor([[-21.2175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1326542046260818, distance: 1.0657438422500685 entropy 10.592992904213798
epoch: 40, step: 97
	action: tensor([[-7152.9416, -9400.4946, 21163.3709,  8832.8107,  4153.3456, -1353.2957,
          6959.5692]], dtype=torch.float64)
	q_value: tensor([[-27.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8086135440814335, distance: 1.5389679755361942 entropy 10.7513401441479
epoch: 40, step: 98
	action: tensor([[-7205.0441, 11177.0339,  1689.7705, -9313.8273,  8897.8959, -1711.8260,
         -1592.3970]], dtype=torch.float64)
	q_value: tensor([[-22.2670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04173649200999863, distance: 1.1202092854596002 entropy 10.523327289914976
epoch: 40, step: 99
	action: tensor([[-22099.4731,  -8572.7741,  -4357.0937,  12076.3284, -13810.8383,
          -6606.8852,  -1270.5276]], dtype=torch.float64)
	q_value: tensor([[-32.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8399308258021958, distance: 1.552234890280196 entropy 10.712583048269567
epoch: 40, step: 100
	action: tensor([[ -6646.8081,  14711.2805, -15103.0841,  -5162.6409,   2614.8504,
          -7549.0775,   5206.6407]], dtype=torch.float64)
	q_value: tensor([[-21.1597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5160070178616423, distance: 0.7961157676042827 entropy 10.61324492390067
epoch: 40, step: 101
	action: tensor([[-16104.3980,  14194.9513, -14283.6741,  -2692.7507,  17497.7896,
          -7429.7109,   -281.6737]], dtype=torch.float64)
	q_value: tensor([[-33.2326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35834553711471817, distance: 0.9166579363872943 entropy 10.764388662930298
epoch: 40, step: 102
	action: tensor([[-14899.3041,  -4441.7548,  -4715.7395,  -4332.7104,   9399.0207,
           7663.4229,  -6268.8063]], dtype=torch.float64)
	q_value: tensor([[-35.3204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6618772788982006, distance: 1.4752178829933935 entropy 10.711222746321306
epoch: 40, step: 103
	action: tensor([[ -2523.1004,   4196.9419, -16313.4685,  15809.4621,   -459.9321,
           6512.6600,  10941.4649]], dtype=torch.float64)
	q_value: tensor([[-25.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15822670960558138, distance: 1.2315540626392796 entropy 10.818046459021389
epoch: 40, step: 104
	action: tensor([[-14007.5911, -16419.7474,   -355.0333,   -194.6891,  11048.3630,
          -4596.5859,  11620.9011]], dtype=torch.float64)
	q_value: tensor([[-28.3214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9290653321265054, distance: 1.5893888477997713 entropy 10.782706494267583
epoch: 40, step: 105
	action: tensor([[ 21264.8822,  -5579.2929, -31028.2784,   6871.7899,  -1015.3410,
         -23694.5148,  -3441.9979]], dtype=torch.float64)
	q_value: tensor([[-30.4483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15075693785100663, distance: 1.0545634062510558 entropy 10.918284434810147
epoch: 40, step: 106
	action: tensor([[-8673.7825, 12767.1239, -3785.8580,  9959.8514, -5743.1516,  1328.7509,
           875.9530]], dtype=torch.float64)
	q_value: tensor([[-27.2419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21083860013845945, distance: 1.016575433894304 entropy 10.79142999789636
epoch: 40, step: 107
	action: tensor([[ -4437.4183,  -8449.5898,  -9498.2156,  -4937.8493, -35057.1813,
          15587.6223,  -3130.8688]], dtype=torch.float64)
	q_value: tensor([[-27.8938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.613585422231745, distance: 1.4536259725324987 entropy 10.81726956892842
epoch: 40, step: 108
	action: tensor([[  7236.4276, -19941.5567,  17670.3539,  22450.2902,  -4560.2995,
          -1507.0576,  -8017.2748]], dtype=torch.float64)
	q_value: tensor([[-26.8278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37986901564666964, distance: 0.9011527426332054 entropy 10.761043052518696
epoch: 40, step: 109
	action: tensor([[-20258.5292,   9196.9229,  -7955.5746,  -1739.8168,  -6966.2790,
          15200.6265,   4694.6305]], dtype=torch.float64)
	q_value: tensor([[-29.9549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.559870383265762
epoch: 40, step: 110
	action: tensor([[-4297.6792, -1959.9808,  -613.6984,    40.8215, -2025.0586, 10408.1550,
          7814.4508]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020481558291116864, distance: 1.1560038317954364 entropy 10.256455048509496
epoch: 40, step: 111
	action: tensor([[-36713.1795,  -4546.7330,  -6295.4230, -14980.5141,   3472.5253,
           7859.8062,  -3681.1746]], dtype=torch.float64)
	q_value: tensor([[-22.8502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20594675197433543, distance: 1.2566685876037305 entropy 10.881990400406892
epoch: 40, step: 112
	action: tensor([[  3274.8149, -11033.1941, -12949.4276,  -4389.2260,   9521.7602,
          10220.8807, -18636.1730]], dtype=torch.float64)
	q_value: tensor([[-22.0186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4339239543748805, distance: 0.8609820197396569 entropy 10.606969704937937
epoch: 40, step: 113
	action: tensor([[-21781.8613, -13919.5548,   7979.3038, -10072.7972,   9698.8548,
           4497.7351,   2649.9239]], dtype=torch.float64)
	q_value: tensor([[-26.9342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.055365991706027406, distance: 1.1755963822433044 entropy 10.668689309882497
epoch: 40, step: 114
	action: tensor([[ -8260.6710, -11120.7606,  23599.3467,  12276.2749,   8299.7594,
           1671.3512,  24040.9825]], dtype=torch.float64)
	q_value: tensor([[-27.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6540136642292425, distance: 1.4717235514740235 entropy 10.777798706524303
epoch: 40, step: 115
	action: tensor([[  -944.8579, -23837.7822,   8474.1394,   9248.1563,  14519.9702,
          -7661.2457,  -4566.3973]], dtype=torch.float64)
	q_value: tensor([[-20.8718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7969121348902903, distance: 1.533981471931202 entropy 10.688410205929182
epoch: 40, step: 116
	action: tensor([[-9362.4369, -1807.0006,   615.2373, 17538.4082, -2557.4314,  7225.1376,
         -9240.3745]], dtype=torch.float64)
	q_value: tensor([[-23.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13407938097971406, distance: 1.064867895553268 entropy 10.801277570068327
epoch: 40, step: 117
	action: tensor([[-26832.9404, -11940.9268,  10889.6143,  13555.9742,  -5385.9542,
          -5070.8633,  -1116.8203]], dtype=torch.float64)
	q_value: tensor([[-22.5175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03848324802494418, distance: 1.1221091941354708 entropy 10.736153149080423
epoch: 40, step: 118
	action: tensor([[  -117.1642,  -6332.2846,  -7641.8576,   6484.4974, -13176.5170,
           3567.8096,  13948.5202]], dtype=torch.float64)
	q_value: tensor([[-22.8333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01220327066148541, distance: 1.1513054523826711 entropy 10.719558479472584
epoch: 40, step: 119
	action: tensor([[-15203.5502,  -8979.8706,  -5295.7206,  12854.5593,  -3442.2723,
          -5479.3004,  -7048.9462]], dtype=torch.float64)
	q_value: tensor([[-20.8053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.602564743396597, distance: 1.4486533840455298 entropy 10.599511719399615
epoch: 40, step: 120
	action: tensor([[-23910.7432,   5895.5317,  17681.2615, -12964.8256,  17864.2848,
          19963.4519,  -8004.7379]], dtype=torch.float64)
	q_value: tensor([[-24.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25284661681349796, distance: 0.9891486826362464 entropy 10.66385426969954
epoch: 40, step: 121
	action: tensor([[  746.4849,  2085.2352, -3516.9868, -7428.6960, -3069.0269, -7391.9831,
         22284.9057]], dtype=torch.float64)
	q_value: tensor([[-24.5151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5360980010089246, distance: 0.7794168944799139 entropy 10.352942388441337
epoch: 40, step: 122
	action: tensor([[ -3397.7221, -14407.7799,    974.8626,  15909.1552, -10256.9876,
         -15265.0429,   8654.5344]], dtype=torch.float64)
	q_value: tensor([[-29.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.725234011111768
epoch: 40, step: 123
	action: tensor([[ 8721.7057,  -958.6018, -5835.0146,  2561.8991,  4905.1594, -8233.3334,
         -3713.9094]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.256455048509496
epoch: 40, step: 124
	action: tensor([[-12475.3258,   6632.2695,   1423.3128,  -9443.5161,   3463.9341,
          -9486.1250,   1488.5928]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6122657565025247, distance: 0.7125638279814958 entropy 10.256455048509496
epoch: 40, step: 125
	action: tensor([[  4537.0778, -18433.5733,  10891.5212,  -3481.3988, -10036.4269,
         -13599.7554,  12574.5407]], dtype=torch.float64)
	q_value: tensor([[-39.1729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.879342144520534
epoch: 40, step: 126
	action: tensor([[-12714.0761,   3111.5730,  -9048.4297,   3745.9446,   7483.7476,
          -5011.4479,  17449.1752]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.256455048509496
epoch: 40, step: 127
	action: tensor([[-2437.2139, -1426.0767,     3.2031,  2866.6080,  1735.7271,  2867.8183,
         -2538.1204]], dtype=torch.float64)
	q_value: tensor([[-26.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.256455048509496
LOSS epoch 40 actor 277.7634283920173 critic 257.57198334199165
epoch: 41, step: 0
	action: tensor([[-11512.2433,  -9154.4500,   3614.5592,  10846.9697,   1381.1629,
          11118.6216,   8652.4806]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -3.0266384991843767, distance: 2.296296765353866 entropy 10.306647925309766
epoch: 41, step: 1
	action: tensor([[   753.9154,   4059.9204,  -6003.3345,  -7702.8157,  -5019.8242,
         -10532.5011,  -9312.0055]], dtype=torch.float64)
	q_value: tensor([[-24.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -3.5129185142502166, distance: 2.431002689012889 entropy 10.420161561593046
epoch: 41, step: 2
	action: tensor([[ -5452.1690,  -1782.5156, -10946.1930,   3956.2339,  11151.0260,
           6811.8720, -14808.4418]], dtype=torch.float64)
	q_value: tensor([[-25.3465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -7.633891133580422, distance: 3.36248207153174 entropy 10.591565991739035
epoch: 41, step: 3
	action: tensor([[-12207.6554, -23748.6289,  25106.2847,   1705.5254,  -3184.9119,
           5303.3797,  -1206.6295]], dtype=torch.float64)
	q_value: tensor([[-34.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -5.640451107799769, distance: 2.9488690340923047 entropy 10.980998835023701
epoch: 41, step: 4
	action: tensor([[-20593.5958,   6902.2335,  -9622.4185,   9173.8216,   8894.4726,
          -7790.6876,   3580.8865]], dtype=torch.float64)
	q_value: tensor([[-31.3501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -3.3280347360244464, distance: 2.38068569353588 entropy 11.06714851858494
epoch: 41, step: 5
	action: tensor([[  2300.6343,    739.5370,   2179.5362,  20869.5705, -14569.7736,
          -4036.2912,   6681.5156]], dtype=torch.float64)
	q_value: tensor([[-27.2424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.1734682446816036, distance: 2.0385612831179816 entropy 10.878648066337226
epoch: 41, step: 6
	action: tensor([[-10104.4869, -17892.5502,   1610.4995,  -6809.3644,   7572.7294,
          -7842.7885,  -4511.2265]], dtype=torch.float64)
	q_value: tensor([[-26.0193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -3.932468805758197, distance: 2.5414927019619125 entropy 10.525958754763085
epoch: 41, step: 7
	action: tensor([[  4663.1985,  12999.8006, -16854.5865,  -2560.1481, -13272.7397,
           3178.0653,  11165.5928]], dtype=torch.float64)
	q_value: tensor([[-31.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.081654834647808, distance: 2.008855460711634 entropy 10.888182282489675
epoch: 41, step: 8
	action: tensor([[-17336.9004, -17279.2883, -29868.0477,   7740.0086,  10754.3925,
          19523.5644,  -4132.1779]], dtype=torch.float64)
	q_value: tensor([[-34.2571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4229436945459142, distance: 1.7812642604084339 entropy 10.821271524208482
epoch: 41, step: 9
	action: tensor([[-17183.1204,  13170.4003,    118.0553,   5635.4559, -14172.7914,
           6243.9766,  20436.0792]], dtype=torch.float64)
	q_value: tensor([[-28.7708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8828096210615164, distance: 1.942963645557562 entropy 10.861118571966498
epoch: 41, step: 10
	action: tensor([[-1803.6251,  6234.1813, -4341.6571, 12252.8575, -2467.3311, 22392.4386,
         -4054.5654]], dtype=torch.float64)
	q_value: tensor([[-36.5718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7854607498101278, distance: 1.9098761915134959 entropy 10.958457638152419
epoch: 41, step: 11
	action: tensor([[-28501.1738,  10248.5461, -10776.7369,  16164.5262, -24000.2419,
          15556.5711,   7313.1869]], dtype=torch.float64)
	q_value: tensor([[-34.5960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.777614556450804, distance: 1.9071843880938721 entropy 10.861118656321674
epoch: 41, step: 12
	action: tensor([[ 16484.3384,   2951.8789,  18650.8487, -12175.5226,   2029.7788,
          -8528.2140,  42396.0571]], dtype=torch.float64)
	q_value: tensor([[-33.7155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -3.2196829072466633, distance: 2.350696721382571 entropy 10.833937783515392
epoch: 41, step: 13
	action: tensor([[ 24617.1582,  -1349.6534,  -6055.8794,   8734.0054, -14765.7619,
           2073.9913,    289.9626]], dtype=torch.float64)
	q_value: tensor([[-39.5008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.817237565218132
epoch: 41, step: 14
	action: tensor([[-16038.6082, -12510.8666,    913.1538,  -4079.5224,   2564.6101,
          -5292.7369,   6777.1670]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07925670542236696, distance: 1.0980597578677354 entropy 10.306647925309766
epoch: 41, step: 15
	action: tensor([[  -535.1837, -23063.4010,  23556.2586,   5347.3274,   5517.7848,
         -18666.6638, -12575.8623]], dtype=torch.float64)
	q_value: tensor([[-37.8584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8109870135460284, distance: 1.5399774493190395 entropy 10.925226580144882
epoch: 41, step: 16
	action: tensor([[-16438.7581, -20644.0630,  -3495.7260,   2880.0359,   5996.5851,
          14740.2476,  18415.7769]], dtype=torch.float64)
	q_value: tensor([[-31.6183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3344289140443526, distance: 1.321917692042616 entropy 10.840910126903802
epoch: 41, step: 17
	action: tensor([[-29792.8037,   4077.3188,  12956.8217,   6898.8797,  -9134.7853,
           3772.3081,  -8134.1634]], dtype=torch.float64)
	q_value: tensor([[-37.4356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09721843030914923, distance: 1.1986799477743808 entropy 10.983650535846909
epoch: 41, step: 18
	action: tensor([[-3087.0526, 12571.7572,  -418.3686, 24101.5257, -8211.8317, 15897.6200,
         -2662.2763]], dtype=torch.float64)
	q_value: tensor([[-34.3805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14348026916754186, distance: 1.0590717390353408 entropy 10.872733980686602
epoch: 41, step: 19
	action: tensor([[ -2904.9985,  -8244.5910, -16959.9129,  -5271.3328,    228.9780,
          -2779.6602, -10079.4461]], dtype=torch.float64)
	q_value: tensor([[-31.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7689785218049203, distance: 1.5220116378873365 entropy 10.805088154479716
epoch: 41, step: 20
	action: tensor([[ -9273.4086,    114.0983,   5094.9046,   7958.9206, -16364.7242,
          -2236.9902,  -7211.2112]], dtype=torch.float64)
	q_value: tensor([[-30.3008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10622276799841113, distance: 1.0818606804376767 entropy 10.828310697429052
epoch: 41, step: 21
	action: tensor([[-16844.5108,  15022.3702,  18086.1735,  11671.0789,  14499.1003,
           1555.5466,  -8492.5412]], dtype=torch.float64)
	q_value: tensor([[-31.6985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16395376869716216, distance: 1.2345951276533766 entropy 10.826259553883583
epoch: 41, step: 22
	action: tensor([[-10174.1896, -21175.6100,  10160.9043,  25353.7550,   3695.6803,
           1819.1007,  34022.6076]], dtype=torch.float64)
	q_value: tensor([[-34.2385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11926595897507053, distance: 1.0739376905737126 entropy 11.023477490097752
epoch: 41, step: 23
	action: tensor([[ -9052.3389,  -4076.9130, -14815.5454,   5477.1953,   4706.8824,
         -14206.0722,  24516.4644]], dtype=torch.float64)
	q_value: tensor([[-33.7010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8169998517521238, distance: 1.5425318469896236 entropy 10.899575598981498
epoch: 41, step: 24
	action: tensor([[  7511.9792,   4040.7333, -20543.3174,  -1595.4152,   8695.8197,
          -2690.8637,  -1932.4825]], dtype=torch.float64)
	q_value: tensor([[-27.1815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.671527773168483
epoch: 41, step: 25
	action: tensor([[ -3881.3630, -13054.6435,   3724.9005,  -8256.3793,  -1719.8107,
         -10138.3370,  -9489.6254]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5940919299071183, distance: 1.4448187694064705 entropy 10.306647925309766
epoch: 41, step: 26
	action: tensor([[  2455.0677,  -8318.0737, -13978.7742,   3964.0184, -14037.4555,
           4316.8272,    738.0278]], dtype=torch.float64)
	q_value: tensor([[-28.1838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5502220598757661, distance: 0.767460034504872 entropy 10.65007923044885
epoch: 41, step: 27
	action: tensor([[-31745.8994,  -6795.8600,    972.9053,   1137.7104,  -3041.7598,
          -8286.3309,  -6765.8897]], dtype=torch.float64)
	q_value: tensor([[-34.7700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4664788163809266, distance: 1.3857809605744964 entropy 10.79638061679554
epoch: 41, step: 28
	action: tensor([[ -1201.7027,   2842.2562,  15985.7996,   2017.4598,  20379.6602,
         -13168.5969,  14226.8045]], dtype=torch.float64)
	q_value: tensor([[-28.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20557668598719303, distance: 1.0199589278313734 entropy 10.83121720677307
epoch: 41, step: 29
	action: tensor([[-12713.5189, -15511.0721,  -1637.6502,  23158.2102,   4580.8408,
          10532.1104,   3662.7583]], dtype=torch.float64)
	q_value: tensor([[-34.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.969737294210413
epoch: 41, step: 30
	action: tensor([[-1329.5455, -1422.4126, -5652.0318, -8151.3982, -5421.9752,  2855.6613,
         -3375.2431]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.306647925309766
epoch: 41, step: 31
	action: tensor([[ -4604.8499,  -2408.6869,   8100.7628, -10606.5410,  -8318.1341,
          -2221.5180,   8166.1121]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.306647925309766
epoch: 41, step: 32
	action: tensor([[ -7580.3970,  -5446.3208,  -2729.2382,  -2163.1540,    617.4958,
         -10290.0605,    663.8498]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048067835596050656, distance: 1.1715245388075424 entropy 10.306647925309766
epoch: 41, step: 33
	action: tensor([[-19634.9426,   7779.8837, -27909.2081,  12881.6240,  -6310.7258,
          -4872.1214,   4838.1372]], dtype=torch.float64)
	q_value: tensor([[-36.8216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6381520755173984, distance: 0.6883665149009054 entropy 10.903208164652787
epoch: 41, step: 34
	action: tensor([[   731.3570,  25324.3729,  -7198.5089,  -3195.7568,  12504.3481,
         -23126.8711,  -4642.5187]], dtype=torch.float64)
	q_value: tensor([[-31.3274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21733936966991152, distance: 1.0123797215283674 entropy 10.703526675126508
epoch: 41, step: 35
	action: tensor([[-4821.0039, -6059.5990,  7366.8251,  9892.0706, -8200.6065,  4872.5342,
          4955.4636]], dtype=torch.float64)
	q_value: tensor([[-32.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.54748112004161
epoch: 41, step: 36
	action: tensor([[ 8259.6015,   390.9983,  2160.6937,  -191.5964,  -431.6963, -6202.2012,
         -4526.6267]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8297038402512956, distance: 0.47223603146500465 entropy 10.306647925309766
epoch: 41, step: 37
	action: tensor([[  2189.7669, -10235.6270,   1230.9081,   1014.2018,   -960.3141,
         -10969.3877,   2618.4637]], dtype=torch.float64)
	q_value: tensor([[-34.7726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15565725218612347, distance: 1.2301872392576094 entropy 10.632566434792823
epoch: 41, step: 38
	action: tensor([[-9922.0743, -9682.4800, -1062.7738,  5964.0360,   216.3505, 11781.7843,
         16257.5249]], dtype=torch.float64)
	q_value: tensor([[-30.0516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6226208752657598, distance: 1.457690162056683 entropy 10.499059503482528
epoch: 41, step: 39
	action: tensor([[ -2238.9712, -24209.3956, -16716.1164,  29074.6467,  24741.9344,
          19838.7099,  12343.5784]], dtype=torch.float64)
	q_value: tensor([[-34.3585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09283573843825332, distance: 1.0899326425601044 entropy 10.954202382512621
epoch: 41, step: 40
	action: tensor([[-15304.9656,   -332.4632,  -3523.8715,  18101.3835,  11927.0578,
          22529.3092, -35695.6813]], dtype=torch.float64)
	q_value: tensor([[-32.1136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07341871073306327, distance: 1.1856084170806982 entropy 11.059655250855599
epoch: 41, step: 41
	action: tensor([[ -3712.0520,  13465.6091, -15063.6426,  -1590.4358,  -4518.3899,
           5290.0006, -13450.6101]], dtype=torch.float64)
	q_value: tensor([[-31.3599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09552613170073954, distance: 1.088315226111307 entropy 10.77930209236467
epoch: 41, step: 42
	action: tensor([[ -2065.7004, -21847.4189,  -3291.6633,  10041.5873, -14335.8573,
           4545.5414,   1842.3588]], dtype=torch.float64)
	q_value: tensor([[-37.9548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33361170762718784, distance: 1.3215128577905113 entropy 10.701124174460206
epoch: 41, step: 43
	action: tensor([[ -8797.7448,  -2993.9551, -14350.4598,   8952.4450,   3417.2705,
          -1460.8724,  -2135.6148]], dtype=torch.float64)
	q_value: tensor([[-32.9138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44729424650691496, distance: 1.3766866819983512 entropy 10.869899713631911
epoch: 41, step: 44
	action: tensor([[-3042.6041,  5895.6596, -8991.2524,  3342.0377, 15178.6915,  2415.1585,
         10477.6894]], dtype=torch.float64)
	q_value: tensor([[-26.7328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20859202440743718, distance: 1.018021393253603 entropy 10.738686380979042
epoch: 41, step: 45
	action: tensor([[ -6957.2798, -29587.1493,  -9007.2384,  27042.4047,   8260.1114,
           7263.8666,   -755.4845]], dtype=torch.float64)
	q_value: tensor([[-31.3380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5265932855183171, distance: 1.4138989345236375 entropy 10.84670234771626
epoch: 41, step: 46
	action: tensor([[  2629.6445, -21513.8022, -18425.3888,  10598.3578, -15976.2870,
          10361.3155,  11964.8790]], dtype=torch.float64)
	q_value: tensor([[-31.5689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18144865180681524, distance: 1.2438388539276342 entropy 10.899932412425128
epoch: 41, step: 47
	action: tensor([[-10329.5222,   6829.8189, -10826.9478,   9900.2246,    266.3727,
           4580.4331,  -1720.7628]], dtype=torch.float64)
	q_value: tensor([[-40.3542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026232923914518036, distance: 1.159256834842521 entropy 10.78767844420597
epoch: 41, step: 48
	action: tensor([[  9437.1160, -42058.6896, -15463.4815,  -4621.0278,    441.2800,
          -8125.5050,   2883.2885]], dtype=torch.float64)
	q_value: tensor([[-32.8203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38197013099693167, distance: 1.3452593572499165 entropy 10.837257317613167
epoch: 41, step: 49
	action: tensor([[ -7319.1723,  -1967.2747,   4871.0183,  -7166.3170,  20919.3628,
         -26795.9821,  15661.5154]], dtype=torch.float64)
	q_value: tensor([[-26.5313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7302462882609337, distance: 1.505257003075902 entropy 10.442355986478304
epoch: 41, step: 50
	action: tensor([[-4909.8884,  2183.4699, -7145.7881,  3555.8425,  7511.0346, 14040.6198,
         18807.7827]], dtype=torch.float64)
	q_value: tensor([[-35.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4677247282682572, distance: 0.8348815014126271 entropy 10.835630758883799
epoch: 41, step: 51
	action: tensor([[  2985.7118,    552.3479,  -1742.9428,  12402.8773, -10539.7110,
          16298.2273, -17855.0211]], dtype=torch.float64)
	q_value: tensor([[-33.4270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.781123101185614
epoch: 41, step: 52
	action: tensor([[ -9354.1546,  -8373.8174,  -1085.6772,   5489.2228, -11992.2050,
           6907.8379,  -7811.5539]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7880352447322279, distance: 1.5301877850603154 entropy 10.306647925309766
epoch: 41, step: 53
	action: tensor([[ -7593.7482, -19376.2607,  -1358.4927,   -482.6697, -12688.6335,
         -25416.6306,   1431.6835]], dtype=torch.float64)
	q_value: tensor([[-30.4886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1000072449541172, distance: 1.658315231773367 entropy 10.818966819150452
epoch: 41, step: 54
	action: tensor([[-11663.6235, -22429.3014, -14229.0152,  29363.4027, -10051.2454,
          11306.1836,  -2655.0057]], dtype=torch.float64)
	q_value: tensor([[-29.1830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3340043752752686, distance: 1.3217073961074626 entropy 10.848703432299772
epoch: 41, step: 55
	action: tensor([[-28617.0911,  -4033.5297,   4291.4971,  17782.5507,   7815.1378,
           3907.9596,  -6211.3146]], dtype=torch.float64)
	q_value: tensor([[-36.8079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11939401920927661, distance: 1.2107324551740286 entropy 10.97474099141425
epoch: 41, step: 56
	action: tensor([[-7.2646e+03,  3.6588e+03, -2.9871e+04,  6.1496e+03,  2.7134e+01,
         -2.2106e+04, -6.0599e+03]], dtype=torch.float64)
	q_value: tensor([[-30.1015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33479415051087014, distance: 0.9333289200250752 entropy 10.883393905607905
epoch: 41, step: 57
	action: tensor([[  5601.8051, -17373.2740,  -2733.0616,   8566.1640,   9082.9599,
          21908.8866,   8848.9243]], dtype=torch.float64)
	q_value: tensor([[-33.9630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.797043461047041
epoch: 41, step: 58
	action: tensor([[-15276.8125, -10797.2398,   5313.6791,   9362.0088,  16596.3225,
           5302.3704,  17907.7951]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8627399845785031, distance: 1.5618265877594664 entropy 10.306647925309766
epoch: 41, step: 59
	action: tensor([[  5418.1893,  -8107.0242, -18412.7465, -10790.0534,   2408.5264,
           4619.3376,   6852.6959]], dtype=torch.float64)
	q_value: tensor([[-29.9187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4037754784462515, distance: 0.8836120100037815 entropy 10.865402000279005
epoch: 41, step: 60
	action: tensor([[14154.0349, -8253.0934,  7563.2091, 12969.8814,  1389.9795, 13917.1092,
         -3183.1492]], dtype=torch.float64)
	q_value: tensor([[-31.9625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5386105577529567, distance: 0.7773033144254251 entropy 10.801775380832172
epoch: 41, step: 61
	action: tensor([[-12450.0707,  12744.4430,  -6184.9370,   6804.3129,   4741.5840,
         -10547.2604,  24495.8696]], dtype=torch.float64)
	q_value: tensor([[-33.6480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14855236525088933, distance: 1.0559313036509712 entropy 10.857697614734066
epoch: 41, step: 62
	action: tensor([[ -1769.2850, -25295.5577,  22778.8626,  27117.9229,   6088.4562,
           9506.8353,   7999.2610]], dtype=torch.float64)
	q_value: tensor([[-34.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0989643065196143, distance: 1.086244751301131 entropy 10.971650035663616
epoch: 41, step: 63
	action: tensor([[-14550.9468,   -592.6803,  12798.2610,  17754.7274, -16956.6623,
           4070.6983, -25743.9557]], dtype=torch.float64)
	q_value: tensor([[-31.9769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02763929868040671, distance: 1.128419005958662 entropy 10.808473235292894
epoch: 41, step: 64
	action: tensor([[  -192.3235,   5408.0073,   1776.1971,  11769.6845,   6562.0407,
         -14912.9677,   3731.3359]], dtype=torch.float64)
	q_value: tensor([[-31.3111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08178685465696067, distance: 1.1902208207075013 entropy 10.82491955518278
epoch: 41, step: 65
	action: tensor([[-3296.3393,  2044.0271, -3375.1559, 15190.5264, 23059.8177,   704.1779,
         19221.1261]], dtype=torch.float64)
	q_value: tensor([[-32.7660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31521004438438527, distance: 0.9469681755342005 entropy 10.771139962101554
epoch: 41, step: 66
	action: tensor([[-3493.7666, -1971.5418,  1486.6302, -9672.6370, 10461.9555,  6116.0823,
          1836.2968]], dtype=torch.float64)
	q_value: tensor([[-26.5262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9383363231569726, distance: 1.5932035314584119 entropy 10.615746085426096
epoch: 41, step: 67
	action: tensor([[    85.9088,  -4545.9889,  -9120.5511,  24433.8762, -17406.1419,
           9095.8692,  -4496.0804]], dtype=torch.float64)
	q_value: tensor([[-27.2450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3537202415100632, distance: 0.9199558178994663 entropy 10.689711084696068
epoch: 41, step: 68
	action: tensor([[ -2568.7266,   -196.6936, -13101.9413,  14845.9026,  15831.4940,
         -11022.4094,  25966.4579]], dtype=torch.float64)
	q_value: tensor([[-34.2293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27353845354896844, distance: 1.291405776295381 entropy 10.933372326012712
epoch: 41, step: 69
	action: tensor([[-21216.7395, -18947.5302,   -396.5674,  18773.1108,   3043.6109,
         -14410.3561,  -9285.7495]], dtype=torch.float64)
	q_value: tensor([[-28.6799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09103997718096868, distance: 1.091010887834167 entropy 10.960889571797805
epoch: 41, step: 70
	action: tensor([[  9233.9802, -32023.5042,  -6542.2420,   7403.8965, -18804.5419,
          -4504.5479,  28820.7560]], dtype=torch.float64)
	q_value: tensor([[-29.8142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09819253382573478, distance: 1.1992119199321785 entropy 10.823660295475479
epoch: 41, step: 71
	action: tensor([[ -1469.6551,  -6296.0694,  14528.4371,   -699.9538,    474.6370,
          -2519.5409, -14501.9940]], dtype=torch.float64)
	q_value: tensor([[-29.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6397675139427246, distance: 1.4653718093058965 entropy 10.633028502281144
epoch: 41, step: 72
	action: tensor([[ -9361.5916,  12988.9278,  -5763.5236,  44228.8876,  10053.2207,
           9058.6723, -21431.5868]], dtype=torch.float64)
	q_value: tensor([[-32.2741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1650407780450308, distance: 1.0456571854671692 entropy 11.00186590603252
epoch: 41, step: 73
	action: tensor([[-21614.3561,   7639.7520,  25595.4787,  19889.3629,   -292.7601,
            421.6599,  22322.0998]], dtype=torch.float64)
	q_value: tensor([[-29.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004002721091303396, distance: 1.142051712157737 entropy 10.744408247058276
epoch: 41, step: 74
	action: tensor([[-11372.7843,  -7148.9158,  17021.8421,   6832.9585,   3003.4097,
          -6075.1694,  27087.7829]], dtype=torch.float64)
	q_value: tensor([[-34.2598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8913764063481018, distance: 1.5737859972928434 entropy 10.868165863791853
epoch: 41, step: 75
	action: tensor([[-16673.2028,   1223.4541,   7234.4532,  27311.1186,  -4794.7117,
           5687.8669,  -2190.0374]], dtype=torch.float64)
	q_value: tensor([[-25.8667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018902487450400685, distance: 1.1551090980174445 entropy 10.67563213828871
epoch: 41, step: 76
	action: tensor([[-5634.9374, -5237.9441,  8353.4372, -2473.1569, -4036.5938,   979.5023,
          3340.8317]], dtype=torch.float64)
	q_value: tensor([[-28.5934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16451622779903063, distance: 1.2348933892405214 entropy 10.740819994680237
epoch: 41, step: 77
	action: tensor([[-22920.4286, -25215.6373,  15670.9852, -18246.3817,   1080.6507,
         -15331.9845,  24941.3547]], dtype=torch.float64)
	q_value: tensor([[-33.2480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0909908460901202, distance: 1.0910403730862077 entropy 11.0055634428592
epoch: 41, step: 78
	action: tensor([[  -461.0112, -12394.9026, -16763.0642,   6393.3167,  -5626.9631,
           4148.8444,    182.3782]], dtype=torch.float64)
	q_value: tensor([[-34.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5465722963732522, distance: 1.4231209318592444 entropy 10.84189659099091
epoch: 41, step: 79
	action: tensor([[-11299.9194,   -133.6346,  17462.1180,  11972.9100,   5274.0476,
           1621.9403, -11948.8164]], dtype=torch.float64)
	q_value: tensor([[-28.9887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15610710002139783, distance: 1.2304266455446182 entropy 10.714683341120088
epoch: 41, step: 80
	action: tensor([[ -4746.7221, -15045.2768, -10816.0164,  -1106.8015,   8793.7742,
         -21934.3568,  10332.0282]], dtype=torch.float64)
	q_value: tensor([[-34.6945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5853292138706088, distance: 1.4408422160233532 entropy 10.972579060741337
epoch: 41, step: 81
	action: tensor([[-27413.2911,    266.3038,  17182.5792,  14002.8696,  15918.1877,
         -42413.3813,  22091.7561]], dtype=torch.float64)
	q_value: tensor([[-31.4524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1946712562044045, distance: 1.0269357983845369 entropy 10.976134828185725
epoch: 41, step: 82
	action: tensor([[-23878.7696, -16976.1735,   5050.2837,   4107.0722,    -43.9904,
          -9839.4523,  12897.3712]], dtype=torch.float64)
	q_value: tensor([[-33.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7143545592627747, distance: 1.4983284180756682 entropy 10.881723604334336
epoch: 41, step: 83
	action: tensor([[   207.8833,  -6768.3635,   1210.7279,   4915.1755, -12196.1244,
          -3703.0833, -15012.2014]], dtype=torch.float64)
	q_value: tensor([[-28.5353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16124526654143012, distance: 1.2331578492905626 entropy 10.8411982602775
epoch: 41, step: 84
	action: tensor([[-13207.5377,  -3412.3140, -16377.8718,  -5682.0114,  -5376.3325,
           -871.2209,  17765.0750]], dtype=torch.float64)
	q_value: tensor([[-27.1071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6377103557378974, distance: 1.4644523351537067 entropy 10.731247784079825
epoch: 41, step: 85
	action: tensor([[ -7642.5591, -10491.1210, -20568.0198,   -932.0797,  28417.0865,
         -14405.4244,  -1393.8945]], dtype=torch.float64)
	q_value: tensor([[-32.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5935982877234143, distance: 1.4445950436904356 entropy 10.88120717885639
epoch: 41, step: 86
	action: tensor([[ -592.2040, 12722.8239, 12869.9388, -7179.9206, 22082.0676, 14142.0224,
         14261.0987]], dtype=torch.float64)
	q_value: tensor([[-32.7741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12948990985403563, distance: 1.0676861226589403 entropy 10.908291710580361
epoch: 41, step: 87
	action: tensor([[ -5309.2945, -25252.0293,  -2674.8415, -20361.7717,   1152.8230,
         -15906.8377,  10432.1987]], dtype=torch.float64)
	q_value: tensor([[-40.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9089705302174269, distance: 1.5810889566129704 entropy 10.876062200928285
epoch: 41, step: 88
	action: tensor([[  921.7159,   818.3543,  4938.2835,  7597.3235, 15693.6369, 41579.1200,
           248.5412]], dtype=torch.float64)
	q_value: tensor([[-34.8010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.995798798115322
epoch: 41, step: 89
	action: tensor([[-4699.7290,  1672.9739, -4986.0764, -7745.0342, -1270.5025, -8900.7818,
         12758.8280]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.306647925309766
epoch: 41, step: 90
	action: tensor([[ -2310.0989, -19523.2926,  -7410.0367,   8714.4490,   4083.8590,
          -8784.8491,   1570.6077]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.807248364003397, distance: 1.5383870429645405 entropy 10.306647925309766
epoch: 41, step: 91
	action: tensor([[-14499.6234,  -2584.2539,  -1640.3611,  17794.6184,  20502.2921,
           5182.5454,  22037.7130]], dtype=torch.float64)
	q_value: tensor([[-26.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1732587545545513, distance: 1.04049858910281 entropy 10.77457539058579
epoch: 41, step: 92
	action: tensor([[   723.0027,   3931.1422, -11684.3985,  23593.3877,  -6960.0535,
          12079.0233,   8767.6770]], dtype=torch.float64)
	q_value: tensor([[-27.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8324491550531661, distance: 0.4684141483986897 entropy 10.805531227616381
epoch: 41, step: 93
	action: tensor([[-13404.3699, -13631.3137,  -8450.1967,    114.4146,  11539.9264,
          -7049.9678,   9489.1465]], dtype=torch.float64)
	q_value: tensor([[-33.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.762936449033717
epoch: 41, step: 94
	action: tensor([[-14931.4763,   4291.2449,   6433.1677,   6655.1633,   7385.7044,
          12046.5445,  -9913.1501]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5035375574081894, distance: 0.8063060028994962 entropy 10.306647925309766
epoch: 41, step: 95
	action: tensor([[-16630.7475,  -7911.3810,  19683.3519,  -6366.7926,  10084.8947,
          20988.9544, -32400.4860]], dtype=torch.float64)
	q_value: tensor([[-31.7481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9402593578567209, distance: 1.5939936487654545 entropy 10.835062152819054
epoch: 41, step: 96
	action: tensor([[  1646.2438,   -823.9361,  -5375.7047,  17635.5023, -13168.3169,
           -539.9050,   1451.6517]], dtype=torch.float64)
	q_value: tensor([[-26.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14032488776021257, distance: 1.2219994058675852 entropy 10.700763838333888
epoch: 41, step: 97
	action: tensor([[-19765.5917,  -9503.0826, -10946.1594,   1246.0103,  -6376.1938,
           4011.0295,   3310.0731]], dtype=torch.float64)
	q_value: tensor([[-27.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4084488438770504, distance: 1.3580858743938133 entropy 10.794549004780123
epoch: 41, step: 98
	action: tensor([[-11214.0273,  -8356.9006, -10924.0454,    392.8240,  10325.6576,
           3630.5785,  17813.2615]], dtype=torch.float64)
	q_value: tensor([[-28.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4574817398701252, distance: 1.381523429089934 entropy 10.772914147102638
epoch: 41, step: 99
	action: tensor([[-10695.5910,  14467.0099,  -9994.7721, -22840.9135,   3903.6226,
          16774.9283,   9883.0817]], dtype=torch.float64)
	q_value: tensor([[-32.8516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1581112564415601, distance: 1.2314926798566506 entropy 11.002714742428198
epoch: 41, step: 100
	action: tensor([[-13131.5346, -17160.2018,   2398.6979,  15146.0726,  -2965.5341,
           1990.0436,   7151.2269]], dtype=torch.float64)
	q_value: tensor([[-31.5529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06173973037797409, distance: 1.1084557397915382 entropy 10.589804574856586
epoch: 41, step: 101
	action: tensor([[ -7114.3951,  -7074.3736,  16860.2622,  21619.2673,  -8883.8816,
           6790.2518, -24243.9036]], dtype=torch.float64)
	q_value: tensor([[-29.1858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022201932950845604, distance: 1.1315696234376478 entropy 10.916225185527665
epoch: 41, step: 102
	action: tensor([[-10480.4273,   8733.2942, -17453.1979, -15496.4732,  15721.4209,
          11422.5249,  -1094.4209]], dtype=torch.float64)
	q_value: tensor([[-33.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02938170752712843, distance: 1.1274075249012654 entropy 10.95910344614531
epoch: 41, step: 103
	action: tensor([[-14874.6311,  12570.4872, -24804.6739,  10796.5944, -19475.7514,
         -12568.6443,   1921.9917]], dtype=torch.float64)
	q_value: tensor([[-32.7975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35050969978260194, distance: 0.9222380319630622 entropy 10.603653810311659
epoch: 41, step: 104
	action: tensor([[ 19089.5635, -10794.6700,  15148.3004,  18632.1142,   2297.5182,
         -26510.9576,  -6765.5547]], dtype=torch.float64)
	q_value: tensor([[-29.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4715864452422458, distance: 0.8318474086177887 entropy 10.596901850334419
epoch: 41, step: 105
	action: tensor([[   721.0530, -24673.6899,   6291.9813,   9831.8569,   5722.6211,
          -7251.6547,  -8456.8925]], dtype=torch.float64)
	q_value: tensor([[-30.5835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18005327239799396, distance: 1.2431041034578199 entropy 10.77466973803457
epoch: 41, step: 106
	action: tensor([[ -8788.1281, -12068.9371,   3537.3057, -10958.9801,  19391.6818,
          32250.6771,  -1012.2488]], dtype=torch.float64)
	q_value: tensor([[-36.1407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7285260797383328, distance: 1.5045085547634762 entropy 10.692264589529563
epoch: 41, step: 107
	action: tensor([[ -879.2178, -1652.2051, -8466.1425, -1141.5320,   383.5090,  -764.5759,
         11324.2886]], dtype=torch.float64)
	q_value: tensor([[-24.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1892720497712903, distance: 1.0303725167025584 entropy 10.394065094790522
epoch: 41, step: 108
	action: tensor([[  1497.6343, -13625.7078,   -975.8990,   2456.8632,  10150.9958,
          24535.3244,   3023.3940]], dtype=torch.float64)
	q_value: tensor([[-30.9643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25689064538465334, distance: 1.2829373244674982 entropy 10.73121353350646
epoch: 41, step: 109
	action: tensor([[-14087.5427, -11400.5494,   7451.0559,  19773.9406,  14592.6922,
         -28106.5012,   3217.3045]], dtype=torch.float64)
	q_value: tensor([[-24.6709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052242647884429894, distance: 1.1140515169252925 entropy 10.680683258043148
epoch: 41, step: 110
	action: tensor([[-3292.3677,  2739.7085, -9087.1941,   152.0171,  4445.3041,  6717.1851,
          2037.5953]], dtype=torch.float64)
	q_value: tensor([[-29.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06910717154439616, distance: 1.1040952349018853 entropy 10.83805456727026
epoch: 41, step: 111
	action: tensor([[   942.0327,  -5461.4005,  -3389.2942, -14188.3739,   1882.4586,
           1751.1227,   6419.5179]], dtype=torch.float64)
	q_value: tensor([[-33.8117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4671840981008518, distance: 0.8353053869435404 entropy 10.896352578763466
epoch: 41, step: 112
	action: tensor([[  2835.1624,  -3741.4500,    110.0721,  -1282.4789, -16692.2028,
           -504.6817,  43091.3366]], dtype=torch.float64)
	q_value: tensor([[-31.3220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44695513088903427, distance: 0.8510143376565171 entropy 10.965491368974517
epoch: 41, step: 113
	action: tensor([[-15089.4175,   3987.6184,  13571.7266,  30087.2709,  28382.7568,
           8133.4164,  -5153.3423]], dtype=torch.float64)
	q_value: tensor([[-33.4549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.918894311516397
epoch: 41, step: 114
	action: tensor([[ -7983.2209,   3281.7951,   9698.6360,  -2627.1927,    915.2163,
         -15463.6621,  -8901.1812]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3403295721073212, distance: 0.9294375217239966 entropy 10.306647925309766
epoch: 41, step: 115
	action: tensor([[-21456.7568,   4556.5922,    148.6028, -14279.9719,  21638.4383,
          15383.4815, -11698.6043]], dtype=torch.float64)
	q_value: tensor([[-40.9397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.941047521546968
epoch: 41, step: 116
	action: tensor([[ -2203.8637, -12066.3888,  -1678.9622,  21449.1813,   5230.0432,
          -7973.9712,    712.1921]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1744271925896438, distance: 1.0397630578849537 entropy 10.306647925309766
epoch: 41, step: 117
	action: tensor([[-15515.1685, -11808.4878,  -9475.6430,  -5977.8598,   9519.1920,
         -17983.5828,  11026.8452]], dtype=torch.float64)
	q_value: tensor([[-31.1383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03130584492827582, distance: 1.126289493663707 entropy 10.905283736387437
epoch: 41, step: 118
	action: tensor([[  5373.3058,  -5210.4788, -10774.9099,   2741.5202,    507.6787,
           3034.4072,   8291.6968]], dtype=torch.float64)
	q_value: tensor([[-24.8589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7539129257428441, distance: 0.5676767333955994 entropy 10.518479357554824
epoch: 41, step: 119
	action: tensor([[ 2153.5742,  4160.5821,  4126.5203,  4685.1599, -5740.3685, -7479.4770,
          8458.0337]], dtype=torch.float64)
	q_value: tensor([[-29.1055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6358161908491924, distance: 0.6905847922867234 entropy 10.54313242909994
epoch: 41, step: 120
	action: tensor([[ -2913.6027, -11108.4072, -18511.7883,  10115.2924,  13110.3070,
            -55.5506,   9478.2223]], dtype=torch.float64)
	q_value: tensor([[-34.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2746597173772294, distance: 0.9746026588508487 entropy 10.651269950576692
epoch: 41, step: 121
	action: tensor([[-26192.7618, -26833.3685,   3543.4665,   8485.1848,  18900.7384,
          18773.3061, -12981.6365]], dtype=torch.float64)
	q_value: tensor([[-29.6539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030296292385893153, distance: 1.1268762383643742 entropy 10.952150832721726
epoch: 41, step: 122
	action: tensor([[-15903.1655,  -4122.1297,  17264.6837, -12562.4540,  -6270.6376,
          -1900.1114,  -8216.5305]], dtype=torch.float64)
	q_value: tensor([[-28.5343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1733033941412867, distance: 1.2395437410598635 entropy 10.729624612184612
epoch: 41, step: 123
	action: tensor([[-10451.5191, -20173.3543,  -4510.5922,  18229.2445,  -7371.7209,
          -6990.7015,   8806.2789]], dtype=torch.float64)
	q_value: tensor([[-24.5877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10171789646073592, distance: 1.2011352026652482 entropy 10.56328847621968
epoch: 41, step: 124
	action: tensor([[  7007.5925,  -2530.1903,  16622.9444,  13091.5199, -11443.5706,
           5395.9499,   2819.0917]], dtype=torch.float64)
	q_value: tensor([[-28.3159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10151089046176809, distance: 1.2010223544152547 entropy 10.862270461160152
epoch: 41, step: 125
	action: tensor([[-24900.9503,   3228.1757, -11095.5344,   1258.9831,   6437.5532,
          -7584.1724,   8928.1414]], dtype=torch.float64)
	q_value: tensor([[-30.7987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28342725509224653, distance: 0.9686945047755657 entropy 10.872562807905066
epoch: 41, step: 126
	action: tensor([[ 10733.8158, -16165.5219,  13284.9729,  -3427.9695,  20386.9402,
          28469.0456,  13623.5157]], dtype=torch.float64)
	q_value: tensor([[-31.0644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.829778000892507
epoch: 41, step: 127
	action: tensor([[ -2246.7850,  -4811.8228,    -72.7773,  13378.4744,  -9438.4333,
         -11954.5525,  19259.8032]], dtype=torch.float64)
	q_value: tensor([[-31.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5502777294410433, distance: 1.424824739882945 entropy 10.306647925309766
LOSS epoch 41 actor 390.8787321903376 critic 223.7885097063516
epoch: 42, step: 0
	action: tensor([[ 18766.8044, -11513.2697,  -3899.3010,  -7683.5161,  22978.9251,
           3119.8662,  -6187.0308]], dtype=torch.float64)
	q_value: tensor([[-32.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21119012522831204, distance: 1.0163489963274892 entropy 10.980669992974597
epoch: 42, step: 1
	action: tensor([[-10035.8261, -13803.6772,  -2219.6299,  -1683.7896, -10251.5167,
          -7031.3779,   9679.7728]], dtype=torch.float64)
	q_value: tensor([[-35.6679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.780181359225889
epoch: 42, step: 2
	action: tensor([[ 8150.6633, -2254.2395,  6994.6259,  1797.0087,  7239.7109,  -290.9985,
         -1540.4820]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.35628821808297
epoch: 42, step: 3
	action: tensor([[ -2353.0291, -15278.3715,  -1309.9497, -10691.5262,  -4594.3362,
          -9567.7035,  10958.2416]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4351803194072956, distance: 1.3709131062494397 entropy 10.35628821808297
epoch: 42, step: 4
	action: tensor([[-13970.7058, -12330.7417, -11299.9039,   9479.2628, -25684.2474,
          -5112.5367,    224.2490]], dtype=torch.float64)
	q_value: tensor([[-39.8469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43406961926704835, distance: 1.3703825220169894 entropy 10.947780443763689
epoch: 42, step: 5
	action: tensor([[ 15863.4813,  -5505.0040,    619.4221,  21046.0551, -14988.3180,
          11236.5038,   3478.2391]], dtype=torch.float64)
	q_value: tensor([[-32.4052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3990146522119753, distance: 1.353529809046025 entropy 10.857943080563606
epoch: 42, step: 6
	action: tensor([[ -9854.2855,   1860.2442,  -3649.8298,   8911.0588,  10183.1319,
          -8477.9246, -10075.8322]], dtype=torch.float64)
	q_value: tensor([[-35.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09464977777749084, distance: 1.1972760356931245 entropy 11.111979643340572
epoch: 42, step: 7
	action: tensor([[-15561.0012, -16109.2449, -12791.6192, -11962.6437,  53142.9086,
           2473.2142,  25934.9939]], dtype=torch.float64)
	q_value: tensor([[-37.1869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2255350094122162, distance: 1.7071587505247212 entropy 10.9178570285756
epoch: 42, step: 8
	action: tensor([[ -8108.5740, -19728.9131,   9115.1664,   3087.2688,  12417.7896,
          11349.2720,   7925.0376]], dtype=torch.float64)
	q_value: tensor([[-30.6403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6433943233128014, distance: 1.4669914562102404 entropy 10.65562489958131
epoch: 42, step: 9
	action: tensor([[ -7905.6273,   9904.4822,  16938.6952,  17334.1240, -18857.2100,
          12085.3188,   6674.0229]], dtype=torch.float64)
	q_value: tensor([[-33.5222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05100969771255426, distance: 1.17316758538699 entropy 10.92355670047186
epoch: 42, step: 10
	action: tensor([[-26386.8118,   6513.4258, -22521.4001,  -2956.3471,   1938.6942,
           1816.9099,   9846.6295]], dtype=torch.float64)
	q_value: tensor([[-31.6414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6529278903553974, distance: 0.6741655420301236 entropy 10.815582205629937
epoch: 42, step: 11
	action: tensor([[23471.1214,  5867.2292, -4620.4672, 10969.1952, -7006.9231,  3916.5152,
         18954.9193]], dtype=torch.float64)
	q_value: tensor([[-44.8945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.894613523921137
epoch: 42, step: 12
	action: tensor([[-8097.3208, -5812.6401, -2318.2563, -1775.4788,  -806.4205, -9098.2623,
          4806.4152]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09699508855729921, distance: 1.1985579443077168 entropy 10.35628821808297
epoch: 42, step: 13
	action: tensor([[-19513.3370,  -8342.9833,  10118.5208,  -5600.5475,  -5994.9207,
           6761.2917,  -1387.9048]], dtype=torch.float64)
	q_value: tensor([[-36.3535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8440367328341185, distance: 1.5539658739707614 entropy 10.836850349209941
epoch: 42, step: 14
	action: tensor([[  7184.0028, -11439.6606, -16110.6543,  15526.4015,  -8825.3906,
           8525.2694,  -9919.7006]], dtype=torch.float64)
	q_value: tensor([[-32.1748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19119655484656195, distance: 1.2489596491858426 entropy 10.741451970540307
epoch: 42, step: 15
	action: tensor([[  2959.1863,  -7160.9824,  -1319.3963,   9753.4022,    911.6060,
          -2457.0739, -19529.7467]], dtype=torch.float64)
	q_value: tensor([[-35.8598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5045463479983634, distance: 0.8054863965662609 entropy 10.753144295492405
epoch: 42, step: 16
	action: tensor([[-2501.2208, -9065.6116,  9678.1714,  9833.5768, -9965.6170, 10811.9665,
         11941.5160]], dtype=torch.float64)
	q_value: tensor([[-37.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20137876324617643, distance: 1.0226502305695189 entropy 10.549138687841921
epoch: 42, step: 17
	action: tensor([[ -7181.3534, -11412.5643,   5127.6016,  13936.9430, -10373.9058,
         -17661.5188,   1633.6271]], dtype=torch.float64)
	q_value: tensor([[-34.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5306618866166646, distance: 1.4157818076606214 entropy 10.821884939049763
epoch: 42, step: 18
	action: tensor([[  -292.1516,  16987.5084, -15922.3420,   8504.8670, -14253.9968,
          -6026.7279,   3517.0245]], dtype=torch.float64)
	q_value: tensor([[-31.3165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04809673256304947, distance: 1.1715406891326647 entropy 10.71649620422039
epoch: 42, step: 19
	action: tensor([[ 5726.2510,  -624.6453, -9174.8554,  5219.0610,  5784.3043, -3717.8916,
         -5802.5168]], dtype=torch.float64)
	q_value: tensor([[-36.4273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.606553589369794, distance: 0.7177934435446347 entropy 10.80642912308783
epoch: 42, step: 20
	action: tensor([[-5535.6231, -2452.1635, -4102.9549, -7770.9829,  4347.6187, 15151.3614,
         13324.2622]], dtype=torch.float64)
	q_value: tensor([[-35.0863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1008800664884473, distance: 1.6586598169845377 entropy 10.955250817425807
epoch: 42, step: 21
	action: tensor([[ -6713.3930,  -7978.4376,    175.3192, -12721.5773,  -3182.4885,
         -10387.6488,   7381.7741]], dtype=torch.float64)
	q_value: tensor([[-30.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010208169963324076, distance: 1.1501702538732967 entropy 10.658595968188447
epoch: 42, step: 22
	action: tensor([[-16398.4185,  -5043.0079, -11651.0555,  -5902.5791,   8268.2876,
          -6391.6376,  19625.8180]], dtype=torch.float64)
	q_value: tensor([[-37.8083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05613555605978027, distance: 1.1760249218271734 entropy 10.89977184629924
epoch: 42, step: 23
	action: tensor([[-9876.1750, 23131.7477, 31056.2349,  4488.1766, -8712.0485,  6164.3539,
         34116.3060]], dtype=torch.float64)
	q_value: tensor([[-43.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26084915010790355, distance: 1.2849560049072335 entropy 10.991975352983218
epoch: 42, step: 24
	action: tensor([[ -8987.4003, -19209.4023,  -5578.9280,  33505.6962,  -7202.5719,
           9773.9716,  10867.7879]], dtype=torch.float64)
	q_value: tensor([[-34.9335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6620945197672596, distance: 1.4753143002037632 entropy 10.753805078233711
epoch: 42, step: 25
	action: tensor([[-35118.9552, -23960.1816,  -2517.5809,  20405.9219, -21623.2696,
         -22128.9325,  22695.6038]], dtype=torch.float64)
	q_value: tensor([[-33.6347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11451098301119733, distance: 1.0768328177081048 entropy 10.956337430622694
epoch: 42, step: 26
	action: tensor([[ -3010.1573,  -6802.7582,  -2907.4698, -28371.3886,  -3423.6293,
           4200.2486,   5620.7781]], dtype=torch.float64)
	q_value: tensor([[-35.6366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.726437140794189, distance: 1.5035991740326193 entropy 11.004682827417847
epoch: 42, step: 27
	action: tensor([[-12001.5342,   7923.9395,   4735.0111,  12880.6389,   1554.7954,
          -3576.3942,  -8156.5264]], dtype=torch.float64)
	q_value: tensor([[-29.9864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2543163394035579, distance: 0.9881753284167685 entropy 10.602617620133888
epoch: 42, step: 28
	action: tensor([[ -1270.3294,   5790.7847,  -9474.1912,  -2130.3711, -19536.8838,
           7045.7901,  19501.3586]], dtype=torch.float64)
	q_value: tensor([[-40.8862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3086582338412084, distance: 0.9514875076049573 entropy 10.879856369237796
epoch: 42, step: 29
	action: tensor([[-1264.6675, -6998.8426, -2371.1168, 13824.0326,  1860.8975, 18275.6019,
         10539.5187]], dtype=torch.float64)
	q_value: tensor([[-34.6292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1585123039774483, distance: 1.2317058909586847 entropy 10.724567253992388
epoch: 42, step: 30
	action: tensor([[   618.2217,   3286.0631,   3655.8054,  22195.6775,  11480.8362,
         -11453.3007,  18892.3421]], dtype=torch.float64)
	q_value: tensor([[-37.2903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.99304839402447
epoch: 42, step: 31
	action: tensor([[ -6203.4188, -17822.9900,   9615.6328,   3755.6888,    939.7169,
          -5810.7747,  -6042.1223]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03857130643801632, distance: 1.122057809998283 entropy 10.35628821808297
epoch: 42, step: 32
	action: tensor([[ 1544.7113,  9144.5789,  5423.9299, 21018.1435, 17994.7200,  7679.4006,
         -3440.0365]], dtype=torch.float64)
	q_value: tensor([[-36.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3244794389785879, distance: 0.9405372044233771 entropy 11.026351111664974
epoch: 42, step: 33
	action: tensor([[ -6390.6957,  -2463.7839, -11820.5019,   7607.0352,   5593.1371,
           5920.5733,  -9920.8961]], dtype=torch.float64)
	q_value: tensor([[-32.0171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2847614575448072, distance: 1.2970835245160943 entropy 10.638709527168961
epoch: 42, step: 34
	action: tensor([[ 12082.2710,    366.5961,  15402.4983,    -51.4334,    889.0893,
         -20219.2280,   2607.9281]], dtype=torch.float64)
	q_value: tensor([[-32.2836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.932302165886322
epoch: 42, step: 35
	action: tensor([[  5100.3094,  -2511.4475, -11488.3304,   3629.1645,   5591.3829,
          -7428.6969,  -8382.9432]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.35628821808297
epoch: 42, step: 36
	action: tensor([[ -8118.2635,  -6438.8705,  -3614.2454,  -7258.3137, -11932.0177,
          12985.2416,   6108.5982]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.35628821808297
epoch: 42, step: 37
	action: tensor([[ -3970.3153,  -6594.3265,  -5431.5796,   1044.5794,  -7460.2116,
         -16421.3533,   7800.5522]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3008566491385656, distance: 1.305183016121719 entropy 10.35628821808297
epoch: 42, step: 38
	action: tensor([[-15318.2124, -28383.5313, -10224.6336, -11959.6105,  -4172.0336,
         -24114.1622, -25060.1410]], dtype=torch.float64)
	q_value: tensor([[-34.1098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028304194383081516, distance: 1.1280331361624116 entropy 11.028515070316628
epoch: 42, step: 39
	action: tensor([[ -5679.4562, -28611.1464,    471.5307,  28662.0118,  12778.9633,
          16633.0579,   4775.2105]], dtype=torch.float64)
	q_value: tensor([[-35.4858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7051836324916283, distance: 1.4943153956906619 entropy 10.905851050471801
epoch: 42, step: 40
	action: tensor([[  4518.4370, -15000.1521,  -3777.3813,  21739.0543,    499.4887,
          10021.5291, -15001.8620]], dtype=torch.float64)
	q_value: tensor([[-25.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13983558291272324, distance: 1.0613226426088715 entropy 10.612018506203354
epoch: 42, step: 41
	action: tensor([[-12789.1132,   2516.7370, -11124.3562,  -2511.8540,  -6991.3682,
           5967.2565, -23085.6201]], dtype=torch.float64)
	q_value: tensor([[-37.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.904549315566348
epoch: 42, step: 42
	action: tensor([[ -9464.5196, -15957.2081,  -3383.5656,  10298.6413,   -822.8089,
          12858.4608,   8846.9469]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048589394692878374, distance: 1.116196579578071 entropy 10.35628821808297
epoch: 42, step: 43
	action: tensor([[ -8699.4727,   6596.9006, -16506.7732,  28888.3355,   3014.0052,
           9079.6131, -21044.3693]], dtype=torch.float64)
	q_value: tensor([[-32.2858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16826932609505585, distance: 1.2368817462578927 entropy 10.960793279552105
epoch: 42, step: 44
	action: tensor([[-15196.6426,  10182.3116,   1745.1331,  16444.7393,   4526.2528,
           -609.1604, -31944.0839]], dtype=torch.float64)
	q_value: tensor([[-35.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05589834645020719, distance: 1.1758928459413325 entropy 10.942690784762691
epoch: 42, step: 45
	action: tensor([[-14328.0848,    600.3015,   9088.3699,  21083.6503,   6668.3029,
         -20959.4125,   2513.8519]], dtype=torch.float64)
	q_value: tensor([[-36.8393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38781014488257115, distance: 0.8953642655629542 entropy 10.835108973698548
epoch: 42, step: 46
	action: tensor([[16324.0923, -9461.5535,  5950.8596, 29710.9595, -7528.0915, -1761.6307,
            98.6479]], dtype=torch.float64)
	q_value: tensor([[-41.0851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14810441030567512, distance: 1.0562090350302025 entropy 10.89567998347584
epoch: 42, step: 47
	action: tensor([[ -1328.6799, -24342.2823, -10379.7277,  -9955.3080,  14611.3981,
          14591.5923,  -8495.7387]], dtype=torch.float64)
	q_value: tensor([[-37.5031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03489456696783422, distance: 1.1641387531600669 entropy 10.730334559982309
epoch: 42, step: 48
	action: tensor([[ -6540.0874, -12425.0825, -20960.9396,  19496.6797,  -5752.6682,
         -10878.5769,  -5172.9345]], dtype=torch.float64)
	q_value: tensor([[-34.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013491320722661682, distance: 1.152037749754872 entropy 10.817754921396292
epoch: 42, step: 49
	action: tensor([[-4643.4703, -2058.0644, 15096.8844, 19308.3061, -5486.8460, 22666.2623,
          -218.9364]], dtype=torch.float64)
	q_value: tensor([[-33.1658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14372918951549596, distance: 1.223822112079688 entropy 10.88992593908333
epoch: 42, step: 50
	action: tensor([[-9026.8170, -6086.0326, 23954.8826,  2053.2357, -8186.3522, 15200.8244,
         11061.2478]], dtype=torch.float64)
	q_value: tensor([[-29.6145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11326506653806034, distance: 1.2074133794162532 entropy 10.875099832002752
epoch: 42, step: 51
	action: tensor([[-14749.5681, -17003.8373, -16258.7599,  11898.8357,   7138.0008,
          10385.2652,  19127.0350]], dtype=torch.float64)
	q_value: tensor([[-33.4312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.040801998295952546, distance: 1.12075536356643 entropy 11.003966107097678
epoch: 42, step: 52
	action: tensor([[ 11018.7936,  -5172.2276, -18200.1552,  14152.8561,  19035.6349,
           9946.2286,  -5632.1015]], dtype=torch.float64)
	q_value: tensor([[-33.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2341670743730373, distance: 1.0014371778751723 entropy 10.826801079183628
epoch: 42, step: 53
	action: tensor([[ 7792.3574,  4957.4026,  8530.9224,   229.6548,  7308.2168, -5953.3598,
         22962.1692]], dtype=torch.float64)
	q_value: tensor([[-40.8358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6438495022191452, distance: 0.6829257229989031 entropy 10.97132068948704
epoch: 42, step: 54
	action: tensor([[-12156.3746, -30637.1397,  17700.8003,  -1537.2541,   6127.7460,
          -2052.7509,   1496.0469]], dtype=torch.float64)
	q_value: tensor([[-41.1686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21039227388711335, distance: 1.258982706669232 entropy 10.936224769094153
epoch: 42, step: 55
	action: tensor([[ -3441.9693,  -2410.2334,  10028.7913,   7525.1283,   3808.5382,
         -19825.4072,  20719.6186]], dtype=torch.float64)
	q_value: tensor([[-34.3806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6370190862115803, distance: 1.4641432334508144 entropy 10.724902906955363
epoch: 42, step: 56
	action: tensor([[ -5293.0459, -25983.0197, -13947.0177,  11439.2656,   3903.6793,
          21289.7039,   1074.4849]], dtype=torch.float64)
	q_value: tensor([[-32.8125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02720831019925496, distance: 1.1286690580812944 entropy 10.751512130958062
epoch: 42, step: 57
	action: tensor([[-16794.9244, -27449.7860,  -9994.1412,  -2097.3124,   3154.7612,
           6815.9319,  12560.6545]], dtype=torch.float64)
	q_value: tensor([[-34.3237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6863068451881402, distance: 1.4860211640793002 entropy 10.83473252182051
epoch: 42, step: 58
	action: tensor([[ 14234.8136, -19395.1846,  12230.8497,  14412.2865,  -9260.6654,
          10967.1223, -14298.4454]], dtype=torch.float64)
	q_value: tensor([[-30.4858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4904082765138572, distance: 0.8168980816158946 entropy 10.678029338466885
epoch: 42, step: 59
	action: tensor([[ -6543.0853,  12401.4943, -22368.7240,   9739.5887, -28526.2291,
          -1876.6111,  13830.8588]], dtype=torch.float64)
	q_value: tensor([[-32.6816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19455578498926251, distance: 1.2507194717524692 entropy 10.893833342991156
epoch: 42, step: 60
	action: tensor([[-11050.2112, -28737.4482,  -2641.4601,   6279.3487,   8244.0559,
           5883.6001,    597.4135]], dtype=torch.float64)
	q_value: tensor([[-33.8080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03219299388389241, distance: 1.1257736366121138 entropy 10.817010668551973
epoch: 42, step: 61
	action: tensor([[ -4586.7455, -11033.2542,  -4898.4148,  10033.0464,  -6809.3608,
         -17500.0235,   7722.5818]], dtype=torch.float64)
	q_value: tensor([[-29.4453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027607603515111823, distance: 1.160033010196045 entropy 10.878698953732666
epoch: 42, step: 62
	action: tensor([[   511.0143,  13424.4350,   4153.3406,   8466.9475,  -8737.3773,
         -10267.5989,  14273.2545]], dtype=torch.float64)
	q_value: tensor([[-38.3366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29035727257009614, distance: 0.963998973524918 entropy 10.852500414707759
epoch: 42, step: 63
	action: tensor([[ -7359.2618, -11022.3563,  14655.6950,   1884.9750,  -2052.3082,
          -2277.4390,   -963.3993]], dtype=torch.float64)
	q_value: tensor([[-38.7944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12020509584359995, distance: 1.073364961781693 entropy 10.696267802955802
epoch: 42, step: 64
	action: tensor([[  5074.3244, -23038.6344,  -3950.8046,   2532.2590,   2489.7846,
          19892.6638, -13439.9632]], dtype=torch.float64)
	q_value: tensor([[-42.7624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03666602330694002, distance: 1.1651346706692738 entropy 10.993009049221744
epoch: 42, step: 65
	action: tensor([[ -2740.5122,   -672.1338, -15974.7506,   3566.1511,  -1838.0700,
          -1174.4098,  -5174.8308]], dtype=torch.float64)
	q_value: tensor([[-37.0947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3753149568250109, distance: 0.9044555897554402 entropy 10.694751008782413
epoch: 42, step: 66
	action: tensor([[  7698.2749, -31622.9023, -17944.2769,  -2377.3170,  -8689.4631,
          -8009.9342, -15822.9787]], dtype=torch.float64)
	q_value: tensor([[-43.0960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3306963373193673, distance: 0.936199261237307 entropy 10.947623752434785
epoch: 42, step: 67
	action: tensor([[-12756.1738, -16903.8494,  -2501.7093,  -1641.5391, -19590.9783,
          11230.2836,  -2386.3622]], dtype=torch.float64)
	q_value: tensor([[-34.8615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1254422831683728, distance: 1.2139989388147994 entropy 10.471137509525475
epoch: 42, step: 68
	action: tensor([[ -7671.6209,   3318.3706, -12341.1919,  19579.7012, -11862.7999,
         -10266.1147,  -2381.9383]], dtype=torch.float64)
	q_value: tensor([[-27.5386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888104522019094, distance: 0.9650490241431927 entropy 10.564141453244416
epoch: 42, step: 69
	action: tensor([[-6665.7526,  1209.0302,  7956.4906, 14810.0734, -4233.7239, -5020.9912,
          9615.3167]], dtype=torch.float64)
	q_value: tensor([[-34.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5659429550064093, distance: 0.753928390947591 entropy 10.569989636916167
epoch: 42, step: 70
	action: tensor([[-22834.6978,   3412.6813,   9519.2980,  10196.8642,  -4081.1478,
          19003.2783,  -9946.5961]], dtype=torch.float64)
	q_value: tensor([[-35.8315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6302209960345408, distance: 0.6958695250940132 entropy 10.75383840938734
epoch: 42, step: 71
	action: tensor([[  831.7381, -2282.1551, -8582.7662, -4616.8311, 23025.1220, 25936.6905,
         11272.5194]], dtype=torch.float64)
	q_value: tensor([[-36.6057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.849494175136641
epoch: 42, step: 72
	action: tensor([[-7734.8260, -7127.6882, -5130.9407, -3269.6588, -3096.3465,  8883.2414,
          3085.5602]], dtype=torch.float64)
	q_value: tensor([[-35.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6017720188526667, distance: 1.448295044344179 entropy 10.35628821808297
epoch: 42, step: 73
	action: tensor([[  7111.0243,  -2584.1415, -25102.8712,  10367.5912,   5935.4487,
          -3743.4988, -18044.4560]], dtype=torch.float64)
	q_value: tensor([[-35.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12966913987228157, distance: 1.2162765280875232 entropy 10.85506651493188
epoch: 42, step: 74
	action: tensor([[-21713.7225, -37096.3951,  -2850.4967, -21924.1995,   8065.4831,
         -15029.2508,   -543.8676]], dtype=torch.float64)
	q_value: tensor([[-31.1433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6465375726646223, distance: 1.4683937116251604 entropy 10.885094708594579
epoch: 42, step: 75
	action: tensor([[-10012.5690, -30719.3099, -14289.5347,   5793.5457,  -1811.1406,
          19398.1486,  19462.0206]], dtype=torch.float64)
	q_value: tensor([[-38.4496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3474773143325818, distance: 1.328365000628549 entropy 11.053423885868899
epoch: 42, step: 76
	action: tensor([[-15030.9228,   3927.8815,  -7215.6948,   6610.8139,  15438.0876,
          19303.6387, -26593.4131]], dtype=torch.float64)
	q_value: tensor([[-34.2466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19210439357875542, distance: 1.0285710970227417 entropy 10.979710391620745
epoch: 42, step: 77
	action: tensor([[-12822.1146, -11175.0019,  16418.8508,  19080.7159,  -7457.6503,
          14100.2529,   5627.5212]], dtype=torch.float64)
	q_value: tensor([[-39.5136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06521339806828219, distance: 1.1064019507672114 entropy 11.007419985845996
epoch: 42, step: 78
	action: tensor([[  1502.7391,  13161.0160,   8619.3383,  14771.1772, -22572.6112,
          27453.8245,  -6773.0973]], dtype=torch.float64)
	q_value: tensor([[-35.4047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36191274754319613, distance: 0.9141063534880378 entropy 10.888457195174174
epoch: 42, step: 79
	action: tensor([[-32457.1961,  -6814.0730,  13274.0916,  13071.6248, -14545.1938,
         -14786.3067,  -4459.1540]], dtype=torch.float64)
	q_value: tensor([[-35.6295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8877886845654155, distance: 1.572292643980277 entropy 10.744272358413635
epoch: 42, step: 80
	action: tensor([[-14256.5664,   3827.9512, -12344.9740,   7773.2224,  -1544.7363,
          -8000.0884,   1245.4242]], dtype=torch.float64)
	q_value: tensor([[-31.5043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6444132453423985, distance: 0.6823850145720983 entropy 10.814933058677246
epoch: 42, step: 81
	action: tensor([[-11621.3486,  -9356.4996,  -3010.7601,  -8128.3458,   -121.9664,
           -836.8203,  12985.3960]], dtype=torch.float64)
	q_value: tensor([[-34.5426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.054082655578334, distance: 1.6400823352625056 entropy 10.708061059841087
epoch: 42, step: 82
	action: tensor([[23526.1584, -9419.6956, -7242.9301, -4642.7792, 14150.5654,    71.1280,
          2257.0807]], dtype=torch.float64)
	q_value: tensor([[-37.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35085639445192385, distance: 1.3300295337463437 entropy 11.038898362782604
epoch: 42, step: 83
	action: tensor([[ -6619.5891,   6796.7447,  10839.5832,  17158.4770, -10121.0052,
          -4801.2259,  -4222.1347]], dtype=torch.float64)
	q_value: tensor([[-28.2438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.435929733167272, distance: 0.8594553061710919 entropy 10.646446437428569
epoch: 42, step: 84
	action: tensor([[ 1030.5370, -2950.7715, 11773.9363, 13701.5693,  7271.3642, 12749.7268,
         -8904.8469]], dtype=torch.float64)
	q_value: tensor([[-34.7245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5184397926422302, distance: 0.7941124221556257 entropy 10.62611050935578
epoch: 42, step: 85
	action: tensor([[  1392.5739, -29522.5420,  13697.6175,   9138.9581,  -3293.4241,
           9931.7393,  -4414.8500]], dtype=torch.float64)
	q_value: tensor([[-32.1648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5070755244822799, distance: 0.8034278550840022 entropy 10.689773835956263
epoch: 42, step: 86
	action: tensor([[-10904.0736,  -1358.2004, -17999.9483,  -6373.7010, -17716.7292,
           7572.8820, -16756.2806]], dtype=torch.float64)
	q_value: tensor([[-36.1351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3874223748523349, distance: 1.347910449995107 entropy 10.65722324767458
epoch: 42, step: 87
	action: tensor([[ -8753.2122,   4340.4634,  10874.2711,   3508.9346,  -2998.2999,
         -19301.3063,  -1784.0276]], dtype=torch.float64)
	q_value: tensor([[-29.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1376530579235966, distance: 1.2205669679521254 entropy 10.584668314555746
epoch: 42, step: 88
	action: tensor([[-14969.9327, -11754.5304,  12016.8164,   3954.1599,  -2795.6025,
          -6186.5023, -11491.5099]], dtype=torch.float64)
	q_value: tensor([[-36.5603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3235326533038563, distance: 1.3165095791802506 entropy 10.803401590228175
epoch: 42, step: 89
	action: tensor([[ -5569.3266, -14111.8730,  23260.0656, -13120.4465, -11674.4161,
          -1658.6625,  -5802.3347]], dtype=torch.float64)
	q_value: tensor([[-31.6162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0009588830445530494, distance: 1.1448927686922923 entropy 10.935581838301792
epoch: 42, step: 90
	action: tensor([[-12236.6945,  11026.7054,    278.3513,  23715.5694,  19512.1134,
           2578.2775, -21569.8226]], dtype=torch.float64)
	q_value: tensor([[-34.0572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01671093083399766, distance: 1.1538661676453426 entropy 10.898330904427992
epoch: 42, step: 91
	action: tensor([[  8591.4996, -17836.8385,   -276.2421,   1235.4120, -12284.5498,
          -9897.8990,   9820.1516]], dtype=torch.float64)
	q_value: tensor([[-29.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42713904018770277, distance: 0.8661264600113431 entropy 10.55429310112938
epoch: 42, step: 92
	action: tensor([[   -86.8811, -10746.2565,  10233.1770,   7142.1273,   2961.1414,
         -17228.3235,   -866.8206]], dtype=torch.float64)
	q_value: tensor([[-33.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46530828592340634, distance: 1.3852277910983433 entropy 10.695349813004432
epoch: 42, step: 93
	action: tensor([[-6626.8063,  9541.3468,  6834.8844,  2117.5288, -9787.7620,  2767.2544,
         -5512.8548]], dtype=torch.float64)
	q_value: tensor([[-31.4568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.560447633453464, distance: 0.75868588551741 entropy 10.698819127848138
epoch: 42, step: 94
	action: tensor([[-1291.1723, -1793.9394, 12889.1731,  1292.6511, 14735.1178,  5898.9876,
          3632.9910]], dtype=torch.float64)
	q_value: tensor([[-37.4829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30789302306758937, distance: 1.3087081433608307 entropy 10.878966932679353
epoch: 42, step: 95
	action: tensor([[ -1922.8961, -13045.9003,  -2153.3310,  -6240.4208,   6300.1657,
           -938.9153,  21838.6289]], dtype=torch.float64)
	q_value: tensor([[-33.4269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7480329146366764, distance: 1.5129741092846787 entropy 10.986741669201242
epoch: 42, step: 96
	action: tensor([[  6773.1638,  -8779.4122,  14947.0735,   3860.1538, -23771.3127,
           2102.2497,   9746.3363]], dtype=torch.float64)
	q_value: tensor([[-35.5130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45446881581036536, distance: 0.8452136143482859 entropy 10.834215564841912
epoch: 42, step: 97
	action: tensor([[-23532.0428, -27638.0864,  10889.7865,    536.9333,  -1825.9220,
           4534.8107, -27849.1156]], dtype=torch.float64)
	q_value: tensor([[-32.2163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5135038624158512, distance: 1.4078243094708394 entropy 10.832690326521154
epoch: 42, step: 98
	action: tensor([[   167.5130,  -4042.3153,   1952.1037,  -8809.6841,   8446.7735,
         -19570.2998,   2672.4173]], dtype=torch.float64)
	q_value: tensor([[-36.5747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5120700415825725, distance: 1.4071572989085344 entropy 10.971161042152275
epoch: 42, step: 99
	action: tensor([[  7170.7543, -19968.4542, -12275.6806, -19974.6318,   8310.2803,
          11521.9191,   9319.2299]], dtype=torch.float64)
	q_value: tensor([[-32.7553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48384632085477364, distance: 0.8221408107325022 entropy 10.737384336520591
epoch: 42, step: 100
	action: tensor([[ -7032.7025, -19683.3780,  -5338.7761,  26853.4217,   4964.6891,
           6403.5608,   7039.2221]], dtype=torch.float64)
	q_value: tensor([[-33.5365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32649676200858146, distance: 1.3179829448380547 entropy 10.798285287641383
epoch: 42, step: 101
	action: tensor([[  5787.9789, -11964.3438,  -4830.0371, -18750.0624,  21503.2496,
          19833.4769, -11129.3828]], dtype=torch.float64)
	q_value: tensor([[-31.8769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0811011049404633, distance: 1.0969594098393791 entropy 10.729781033878623
epoch: 42, step: 102
	action: tensor([[ -6578.4985, -15098.5026,  11399.0268, -15252.9910,  -5013.5422,
          -4158.4658,  -8922.0560]], dtype=torch.float64)
	q_value: tensor([[-43.4200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018703798689102102, distance: 1.1549964678156341 entropy 10.919485833778184
epoch: 42, step: 103
	action: tensor([[-1054.5256,   127.6169, -8502.0643,  7913.0506, 14190.9777,  5686.6128,
          4254.7342]], dtype=torch.float64)
	q_value: tensor([[-34.4178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47611172166344606, distance: 1.3903249221489233 entropy 10.685564053075309
epoch: 42, step: 104
	action: tensor([[-29279.1411,  -5810.7717,   2583.4871,   5840.7125, -12433.2628,
          -2273.4819,  13982.0186]], dtype=torch.float64)
	q_value: tensor([[-33.2996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8667517658743795, distance: 1.5635075353851162 entropy 10.667035878388445
epoch: 42, step: 105
	action: tensor([[ -6163.2923,   5212.7021,   3765.3533,  21469.3887,  40267.3427,
         -13453.7734,  27971.7422]], dtype=torch.float64)
	q_value: tensor([[-34.7199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24294000233607171, distance: 1.2757975857516435 entropy 10.829960145156589
epoch: 42, step: 106
	action: tensor([[  1799.8779,  -3913.0855, -13410.5841,   7557.4057,  19487.0324,
         -10840.2854,  -9371.8709]], dtype=torch.float64)
	q_value: tensor([[-35.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4337621641889393, distance: 0.8611050495784418 entropy 10.78743026766281
epoch: 42, step: 107
	action: tensor([[ -6305.1073,  -7057.6921,  -2826.5539,  -9842.7602,  -8518.9461,
         -26717.6525,    462.2245]], dtype=torch.float64)
	q_value: tensor([[-49.9015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7994821988252638, distance: 1.5350780814166287 entropy 10.922598311880531
epoch: 42, step: 108
	action: tensor([[-28018.1350, -14499.5162,   7393.7653,   2350.4808, -29983.8842,
          -8954.0156,  -8504.9808]], dtype=torch.float64)
	q_value: tensor([[-32.9093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1566652185594744, distance: 1.6805369294430916 entropy 10.810294970274658
epoch: 42, step: 109
	action: tensor([[ 5708.5535, -8395.8355, 12880.6768, 18530.1252, 13107.4849,  -357.8968,
          -243.1743]], dtype=torch.float64)
	q_value: tensor([[-31.6476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1958665301525334, distance: 1.0261734231380981 entropy 10.887719124609902
epoch: 42, step: 110
	action: tensor([[  4747.4381,  -4399.4299, -22510.6310,   9816.4162,    537.8986,
         -11750.0293,  11710.6945]], dtype=torch.float64)
	q_value: tensor([[-36.3544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34082097839714565, distance: 0.9290912756938228 entropy 10.701348091960261
epoch: 42, step: 111
	action: tensor([[-27599.9268, -12792.5224,  27882.2632,  -5150.5780,  -3492.6920,
          25781.1787,  -3272.7944]], dtype=torch.float64)
	q_value: tensor([[-43.1330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1285219129766544, distance: 1.6695358766879318 entropy 10.954593070664012
epoch: 42, step: 112
	action: tensor([[  1514.7759,   -785.5904, -12687.7314,  11657.4136,  -5151.4894,
         -14631.9885,  26327.1603]], dtype=torch.float64)
	q_value: tensor([[-29.8516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42961574478941533, distance: 1.3682528282978923 entropy 10.725010425012744
epoch: 42, step: 113
	action: tensor([[-24309.7223, -10341.3294,  -5883.0354,  10353.1067, -10579.2643,
         -12630.6581,   -891.3638]], dtype=torch.float64)
	q_value: tensor([[-32.0776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015851110059783458, distance: 1.1352384622766647 entropy 10.70829713229755
epoch: 42, step: 114
	action: tensor([[  5913.4109,  -4620.6757,  11756.3632,  -1180.4746,  -6990.4510,
         -20577.1576,   5241.9651]], dtype=torch.float64)
	q_value: tensor([[-32.3982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2023488717680213, distance: 1.2547925842194763 entropy 10.887220172959575
epoch: 42, step: 115
	action: tensor([[-12794.6311, -19042.7793,  19354.9314, -17238.1471,  14611.1756,
          -4107.1862,   4426.6216]], dtype=torch.float64)
	q_value: tensor([[-35.3014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.84572419306205, distance: 1.554676720977041 entropy 10.74908327036018
epoch: 42, step: 116
	action: tensor([[17429.0487, -1818.5086, 35641.6513, -4729.2022, -6308.3511,  5508.4343,
         16779.0171]], dtype=torch.float64)
	q_value: tensor([[-43.7870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3610949273493096, distance: 0.9146919593349653 entropy 11.005007069984362
epoch: 42, step: 117
	action: tensor([[  -123.8393,  -7873.8418,  14290.7962,   4011.6865,   6445.6927,
            503.2632, -20327.5799]], dtype=torch.float64)
	q_value: tensor([[-34.5235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07175819541334671, distance: 1.1025219766323995 entropy 10.920079423835901
epoch: 42, step: 118
	action: tensor([[  -793.2924, -15843.0768, -21454.2975,  -3944.5143,    801.2994,
          30093.4738,  17884.1643]], dtype=torch.float64)
	q_value: tensor([[-36.0562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008815985591345998, distance: 1.1392888259613152 entropy 10.783710515948615
epoch: 42, step: 119
	action: tensor([[-28948.2660, -30010.2663, -27098.9209,  -6663.1621,   9774.0301,
         -18780.0157,   8517.9363]], dtype=torch.float64)
	q_value: tensor([[-38.2700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3931701041831228, distance: 1.3506995851771635 entropy 11.092499889441063
epoch: 42, step: 120
	action: tensor([[-17430.1155,  -7895.1428,  -6975.6239,  -9477.2476,   2752.3444,
           8342.5943,  11931.2962]], dtype=torch.float64)
	q_value: tensor([[-39.0869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28462939844094426, distance: 1.297016859965714 entropy 10.928326743742295
epoch: 42, step: 121
	action: tensor([[-2932.0487, -5349.9704,  9672.8075,  2062.6898, -8186.6445,  5640.4037,
          1935.8280]], dtype=torch.float64)
	q_value: tensor([[-31.1091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5204526848519091, distance: 1.4110524208909205 entropy 10.613571920872133
epoch: 42, step: 122
	action: tensor([[ -9038.0923, -17735.0446,  11656.1484,  -4143.7744,   8641.4125,
          -5588.8121,  -4490.4691]], dtype=torch.float64)
	q_value: tensor([[-29.3625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10784582690539546, distance: 1.2044710231757614 entropy 10.740790467747289
epoch: 42, step: 123
	action: tensor([[ -4439.2320, -12503.9442,   5100.3688,   8113.2190,   7234.9881,
          -5662.8233,  -6289.4677]], dtype=torch.float64)
	q_value: tensor([[-33.6060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43801278322102455, distance: 1.372265252632646 entropy 10.792988555085502
epoch: 42, step: 124
	action: tensor([[  2226.1338,   4209.1107,  -4653.6058, -15044.9689,  16887.4255,
          15956.2068,   -355.1180]], dtype=torch.float64)
	q_value: tensor([[-32.8906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26398794713956597, distance: 0.9817460361270056 entropy 10.885531492621453
epoch: 42, step: 125
	action: tensor([[-9092.1216, 11648.3379,  1436.7545, 21449.9401,  6322.4268,  6925.2980,
          4065.9121]], dtype=torch.float64)
	q_value: tensor([[-33.6177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39595821317749047, distance: 0.8893857874520819 entropy 10.733354905170527
epoch: 42, step: 126
	action: tensor([[-25111.8232,  31224.9578,   1951.9265,   2933.3809,   2991.2047,
         -11631.9552,  25234.7560]], dtype=torch.float64)
	q_value: tensor([[-35.5165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18209009519378783, distance: 1.2441764665446082 entropy 10.875049514076272
epoch: 42, step: 127
	action: tensor([[ 4801.7675,  3929.7516, -6871.8429, 14959.8760,  3940.3208, 10693.6396,
         31674.7281]], dtype=torch.float64)
	q_value: tensor([[-36.6965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937763108258443, distance: 0.9616739099533526 entropy 10.875567475520182
LOSS epoch 42 actor 522.7785119340307 critic 63.23687076626456
epoch: 43, step: 0
	action: tensor([[ 6988.0681, 10051.7191, 18977.2823,  9040.2033, -2993.5470, -8618.9831,
         -3728.7897]], dtype=torch.float64)
	q_value: tensor([[-29.6977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6820780209343189, distance: 0.6452335926783398 entropy 10.737649373459474
epoch: 43, step: 1
	action: tensor([[ -8308.1298, -19615.1291,  19538.1363,  27246.7987,  16915.2991,
          -8735.6860,   2220.7624]], dtype=torch.float64)
	q_value: tensor([[-30.3051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19689932978614033, distance: 1.2519457355022396 entropy 10.955411169026693
epoch: 43, step: 2
	action: tensor([[-30039.8137, -19306.1274,  -9482.7443,   1583.4350, -16395.7853,
         -17582.9579,   4693.2343]], dtype=torch.float64)
	q_value: tensor([[-29.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1380073748495978, distance: 1.220757023200418 entropy 11.082996255609114
epoch: 43, step: 3
	action: tensor([[ -2774.7118,   4683.4221, -12176.8891, -13997.2413,  -2037.5171,
          12296.5785,  16726.3731]], dtype=torch.float64)
	q_value: tensor([[-30.5839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35632756415083766, distance: 1.3327202209830258 entropy 11.113323895461152
epoch: 43, step: 4
	action: tensor([[-16107.6798,  -2204.5998, -21238.6464,  22514.9956,   8939.1771,
         -12958.9515,  25848.8506]], dtype=torch.float64)
	q_value: tensor([[-34.7007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5309122753045796, distance: 1.4158976011144864 entropy 10.65081683019514
epoch: 43, step: 5
	action: tensor([[  5026.8290,  12105.8491,  -7932.0769,   8300.0268,    983.7731,
         -16564.1731,  -7908.6335]], dtype=torch.float64)
	q_value: tensor([[-26.7265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1875567976614655, distance: 1.0314619171778348 entropy 10.923543989494098
epoch: 43, step: 6
	action: tensor([[-18407.4864, -15436.9569, -30887.4327,  -8286.3465, -16017.0186,
          -6422.6609,  13222.5056]], dtype=torch.float64)
	q_value: tensor([[-32.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41012296272985416, distance: 1.3588927627529495 entropy 11.05180276017657
epoch: 43, step: 7
	action: tensor([[-16558.4645,   3058.0022,  -3153.9111,  30272.0210,  13283.0373,
          15111.0342,   7132.3273]], dtype=torch.float64)
	q_value: tensor([[-28.2538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011758284327471058, distance: 1.137596597578431 entropy 10.93698497812221
epoch: 43, step: 8
	action: tensor([[ -2103.8500, -17633.8198,   3887.6342,    986.8381,  -6454.8008,
          16443.4138,  20249.6220]], dtype=torch.float64)
	q_value: tensor([[-24.5020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2455897567200529, distance: 1.2771567625646483 entropy 10.639725929439033
epoch: 43, step: 9
	action: tensor([[ 12162.2300,  -9658.1321,   1326.0643,   3461.5563, -20276.1598,
           5669.0631,  22788.3990]], dtype=torch.float64)
	q_value: tensor([[-27.5402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14935955754323993, distance: 1.0554306611686546 entropy 10.974036236846077
epoch: 43, step: 10
	action: tensor([[  8838.1033,  -6148.1660, -23990.8530,   8351.7822, -24158.4011,
          -5672.0329,  11640.0826]], dtype=torch.float64)
	q_value: tensor([[-27.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.642473937973119
epoch: 43, step: 11
	action: tensor([[ -8554.6915,  -4659.3313,  -5948.7575,  10480.8824,   2949.0189,
         -13498.3493,   6447.3381]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.40462102339717
epoch: 43, step: 12
	action: tensor([[ 6744.5764, -8681.2338, -4593.7615, -3997.1562,  9805.2342,  8224.0286,
          3104.9617]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18856798493815052, distance: 1.030819825581441 entropy 10.40462102339717
epoch: 43, step: 13
	action: tensor([[ 6317.9064, -9772.7822, 15791.3605, -4835.4487,  3457.8879,  2747.3077,
         11128.5461]], dtype=torch.float64)
	q_value: tensor([[-29.1555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017801799606552038, distance: 1.1341128215039538 entropy 10.793617161014996
epoch: 43, step: 14
	action: tensor([[ -3548.3472,  -7253.0008,  -1729.8236,  10750.9736, -18755.5803,
          15024.6440,   4523.9575]], dtype=torch.float64)
	q_value: tensor([[-31.7196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31014591563090965, distance: 1.309834806708702 entropy 10.813634333460147
epoch: 43, step: 15
	action: tensor([[ -4698.5997,  -5225.5655, -21355.7844,  -4954.7448,  20078.4375,
          -1638.9727,  -2473.7371]], dtype=torch.float64)
	q_value: tensor([[-21.1635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.340151322971582, distance: 1.3247490385188656 entropy 10.487325224917852
epoch: 43, step: 16
	action: tensor([[ -6133.9065, -23846.9302, -12429.8038,  26703.5059,   1484.3183,
          -8129.1183,  18651.1045]], dtype=torch.float64)
	q_value: tensor([[-30.9635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5796643234031151, distance: 1.4382656162003034 entropy 10.95820457549355
epoch: 43, step: 17
	action: tensor([[ -4833.8624, -19445.0081, -12146.2718,  26602.3191,  -9039.6583,
            765.0215,   9047.4060]], dtype=torch.float64)
	q_value: tensor([[-28.5929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6724940997333648, distance: 1.4799225593140481 entropy 10.914087069280367
epoch: 43, step: 18
	action: tensor([[  3571.7095, -30803.3650,   6202.5705,  -5498.4193, -16086.1249,
          12428.5900, -11790.4885]], dtype=torch.float64)
	q_value: tensor([[-25.5228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16709660710703844, distance: 1.0443690882499097 entropy 10.83847711015963
epoch: 43, step: 19
	action: tensor([[-10681.4142, -27017.5753,  -2746.1187, -10291.1541,   7412.6164,
          -5254.1021,   1501.2900]], dtype=torch.float64)
	q_value: tensor([[-24.6746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22372168697215222, distance: 1.008243474730323 entropy 10.575265738605493
epoch: 43, step: 20
	action: tensor([[ -2199.6040,  -4407.6218,   2886.9966,  -1381.9473,  -6335.5752,
         -10446.7089,  11663.7501]], dtype=torch.float64)
	q_value: tensor([[-29.5217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6517139205871303, distance: 1.4707000520144264 entropy 10.782652968957008
epoch: 43, step: 21
	action: tensor([[ -6645.2931, -16821.5012,  17684.4578,  18928.2302,  -8029.6179,
           5645.0993,  12571.1201]], dtype=torch.float64)
	q_value: tensor([[-35.0710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.250087860099762, distance: 1.279460733851469 entropy 11.072152159288297
epoch: 43, step: 22
	action: tensor([[  7190.0316,  -7175.7849,  -3088.8763,  11714.1037,  -4099.0185,
         -13034.8017,   7451.7677]], dtype=torch.float64)
	q_value: tensor([[-29.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08433452840890365, distance: 1.095027717429739 entropy 10.961389389962457
epoch: 43, step: 23
	action: tensor([[ -9171.3619, -31396.5079, -15919.0626,   1367.0813,   4988.3166,
           3989.5711, -12023.8774]], dtype=torch.float64)
	q_value: tensor([[-33.2362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44914727038059177, distance: 0.8493260528925067 entropy 10.897495216809675
epoch: 43, step: 24
	action: tensor([[  7890.4132,   2074.6572, -17580.5872,  14474.2571,  -2679.5255,
           4510.0666,  -2907.5294]], dtype=torch.float64)
	q_value: tensor([[-26.4238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4584779564483037, distance: 0.8421021249886342 entropy 10.848830740534364
epoch: 43, step: 25
	action: tensor([[ -6158.8795, -12963.4048,   -807.5450, -16662.4121,  -2055.0216,
          13646.5799,  10298.1812]], dtype=torch.float64)
	q_value: tensor([[-27.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.880080729174283
epoch: 43, step: 26
	action: tensor([[-19049.2274,   8508.5117,  -2327.2490,   3068.3433,  -5529.9027,
         -23226.5527,   6028.8497]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08321016294800132, distance: 1.095699815295445 entropy 10.40462102339717
epoch: 43, step: 27
	action: tensor([[ -1813.5889, -18372.0723,   5650.4915,  -1313.4715, -21239.3051,
          29736.2637,  14298.6332]], dtype=torch.float64)
	q_value: tensor([[-32.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6804599537021687, distance: 1.4834427042189346 entropy 10.95680988765283
epoch: 43, step: 28
	action: tensor([[ -8886.7502,  -1579.2240, -13440.0600,   2595.0414, -26722.4916,
           5770.6282,   6287.4664]], dtype=torch.float64)
	q_value: tensor([[-32.5071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08983593699680881, distance: 1.0917332442569898 entropy 10.962777178164547
epoch: 43, step: 29
	action: tensor([[  -560.5289, -14651.5123,   5756.2757, -12835.1694, -12165.1711,
          -8164.4796,   7786.5328]], dtype=torch.float64)
	q_value: tensor([[-27.9345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1613456454693516, distance: 1.2332111456920964 entropy 10.88893856989561
epoch: 43, step: 30
	action: tensor([[-13983.4869,  -2240.7586,  -6006.7409,   6342.9892, -11256.7598,
           2020.6444,   7644.4501]], dtype=torch.float64)
	q_value: tensor([[-27.9430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9016949128140253, distance: 1.5780730955124396 entropy 10.876332189442047
epoch: 43, step: 31
	action: tensor([[ -3928.5191,  -2746.3285,   -253.6073,   6812.2719, -17693.6810,
           2320.0084,  -4140.0851]], dtype=torch.float64)
	q_value: tensor([[-29.8810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008249531496384521, distance: 1.1396143268945067 entropy 11.027043797318196
epoch: 43, step: 32
	action: tensor([[ -5161.7481, -15147.0221,   2032.5949,  15019.6407, -19374.7976,
         -15317.7325,  -1517.1137]], dtype=torch.float64)
	q_value: tensor([[-26.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11177819425701574, distance: 1.0784931938263265 entropy 10.894580678468802
epoch: 43, step: 33
	action: tensor([[ 29775.6757, -13605.7557,   -426.5058,  21098.5474,  11895.6693,
          -1895.3018,   7322.1588]], dtype=torch.float64)
	q_value: tensor([[-36.8780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5738458405502871, distance: 0.7470334679198686 entropy 11.103579636915638
epoch: 43, step: 34
	action: tensor([[  6886.2900, -10629.2353,  21257.7698, -12232.1982,  10423.5670,
          26237.0935,   7302.6713]], dtype=torch.float64)
	q_value: tensor([[-34.8976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24889390285141055, distance: 1.2788495822686397 entropy 10.85313957533127
epoch: 43, step: 35
	action: tensor([[ 4055.9534, -4217.1092,  3323.4384,  9533.1193, -3983.1023, -1477.7748,
         16628.9366]], dtype=torch.float64)
	q_value: tensor([[-28.3033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5102164811114007, distance: 0.8008640090260548 entropy 10.607402026871725
epoch: 43, step: 36
	action: tensor([[  6237.9098,  -3968.0631,  17817.1128,  22029.8719,   2183.5144,
           2369.4300, -11265.0054]], dtype=torch.float64)
	q_value: tensor([[-33.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2792348432622621, distance: 0.9715241152613472 entropy 10.812439056318754
epoch: 43, step: 37
	action: tensor([[ -8835.8055, -13228.4311,   3372.4141,   7000.7849, -12958.1807,
            612.2680,  46018.9990]], dtype=torch.float64)
	q_value: tensor([[-33.2118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5099738415938966, distance: 1.4061815815022722 entropy 10.984072798879833
epoch: 43, step: 38
	action: tensor([[-17310.5701,  -6136.4533,  13634.4197,   6469.0884, -10323.8684,
         -17851.0145,   5159.2778]], dtype=torch.float64)
	q_value: tensor([[-26.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7815139704656351, distance: 1.5273948061566 entropy 10.95373634407525
epoch: 43, step: 39
	action: tensor([[ 6829.3961,  5138.0900,  1295.7347,   756.8499, -2212.3894, 14902.9487,
         -5506.8875]], dtype=torch.float64)
	q_value: tensor([[-23.7477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.765962992072128
epoch: 43, step: 40
	action: tensor([[ 5354.1877, -1056.5291,  7277.3321,  3426.8066, -9493.8016,  2839.7707,
          2956.4489]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4062850719205827, distance: 0.8817504249565612 entropy 10.40462102339717
epoch: 43, step: 41
	action: tensor([[ 11994.8327, -23061.6142, -18918.4319,   1242.8292,    637.5217,
          15493.0371,   3478.0494]], dtype=torch.float64)
	q_value: tensor([[-26.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036849633802663595, distance: 1.1230620197572165 entropy 10.712354323339605
epoch: 43, step: 42
	action: tensor([[-14983.4437,   2881.6656, -34330.5904,   3564.3682, -13880.9550,
           5163.8099,   7390.4394]], dtype=torch.float64)
	q_value: tensor([[-36.1782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.99299231093847
epoch: 43, step: 43
	action: tensor([[-11025.8264,   -931.3531,   3792.4094,  10561.4734, -15196.7472,
          -5960.8320,  -9927.7591]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37269336939517395, distance: 0.9063514466798874 entropy 10.40462102339717
epoch: 43, step: 44
	action: tensor([[ -4802.9369, -19080.8713, -10775.3856,  31970.1735,  28699.7076,
          -5052.4043,   3711.4542]], dtype=torch.float64)
	q_value: tensor([[-28.6518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26038734794449003, distance: 1.2847206675524308 entropy 10.990504999050765
epoch: 43, step: 45
	action: tensor([[-23919.8239, -14327.4053,   7333.4368,    859.8988,  -5079.7063,
         -16735.2480,   2801.0283]], dtype=torch.float64)
	q_value: tensor([[-29.3810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21258664362140467, distance: 1.0154489195929923 entropy 11.027430578340857
epoch: 43, step: 46
	action: tensor([[ -9853.3963, -15520.7740,   5758.6768,   2674.9486,   8130.8790,
          39338.5735,   -274.4318]], dtype=torch.float64)
	q_value: tensor([[-30.3208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19457183345023465, distance: 1.0269991871777289 entropy 11.051569464830397
epoch: 43, step: 47
	action: tensor([[  2360.8188, -23940.1320,   1551.6514,   8247.9560,   8418.6517,
           -364.9583,  -3027.2814]], dtype=torch.float64)
	q_value: tensor([[-30.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3562434531391345, distance: 0.9181582114635098 entropy 10.985161982882202
epoch: 43, step: 48
	action: tensor([[ 2070.8735, -8018.9739, -4894.9685, 13956.3973,  8166.8285, 18524.3799,
         19487.2419]], dtype=torch.float64)
	q_value: tensor([[-28.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.425779745477065, distance: 0.8671534313303824 entropy 10.68842471305974
epoch: 43, step: 49
	action: tensor([[-21210.9107, -12081.5617, -24868.3770,  16742.1180, -29207.3029,
         -13821.5275,  12871.7321]], dtype=torch.float64)
	q_value: tensor([[-31.7537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2880867963096595, distance: 0.9655398819277915 entropy 10.954561277755769
epoch: 43, step: 50
	action: tensor([[ 14643.8333, -20578.9506,  21235.9943, -39301.4388,   7084.2109,
         -14353.8559,    717.0878]], dtype=torch.float64)
	q_value: tensor([[-32.2012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06573258533827708, distance: 1.1813560660258806 entropy 11.091870859325935
epoch: 43, step: 51
	action: tensor([[22200.9035,  6415.0668, 10098.3962,   572.9597, 15918.6954,  4720.3843,
         -3815.9755]], dtype=torch.float64)
	q_value: tensor([[-33.2814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46470326923925975, distance: 0.8372477493696712 entropy 10.9254162962871
epoch: 43, step: 52
	action: tensor([[-22241.0746,   2438.0448,  -3225.9809,   4387.5911, -25121.9650,
          -4896.6706,  -1849.0750]], dtype=torch.float64)
	q_value: tensor([[-32.2054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2194220187157645, distance: 1.0110318600955557 entropy 10.986100062831499
epoch: 43, step: 53
	action: tensor([[-19635.5504,  -4770.2967, -15801.7185,   8114.5556,  13122.1440,
           7949.5531,   8352.8887]], dtype=torch.float64)
	q_value: tensor([[-30.8382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39564333833386556, distance: 1.3518979725966926 entropy 10.91000323932427
epoch: 43, step: 54
	action: tensor([[-4.2583e+03, -4.0594e+04,  5.9193e+00,  1.4946e+04,  4.5803e+03,
          1.7009e+04, -9.8486e+03]], dtype=torch.float64)
	q_value: tensor([[-31.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21800487686045555, distance: 1.0119492097312122 entropy 11.070824114629508
epoch: 43, step: 55
	action: tensor([[-20914.6608,  -2430.3060,  -5282.0356,  -3879.5301, -12771.0224,
           5451.0469, -21538.8403]], dtype=torch.float64)
	q_value: tensor([[-28.8208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4416168679024213, distance: 1.3739838274697105 entropy 10.97745806175891
epoch: 43, step: 56
	action: tensor([[  2703.9818, -13858.8601,  11576.0162,  -4299.8006,  14121.3528,
           -976.9070,  12571.5063]], dtype=torch.float64)
	q_value: tensor([[-27.9357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43336924317501513, distance: 0.8614037646873912 entropy 10.824308846390204
epoch: 43, step: 57
	action: tensor([[ -1108.7934,    516.3814, -16899.2647,   4755.6155,   3400.0831,
          13845.4357,  13458.9600]], dtype=torch.float64)
	q_value: tensor([[-34.6149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.929407764487276
epoch: 43, step: 58
	action: tensor([[  4901.7885,  -8278.5908,  -8839.0435,  18189.7953, -11842.7937,
           -878.6773,   9809.6311]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15464487781828307, distance: 1.0521466760836546 entropy 10.40462102339717
epoch: 43, step: 59
	action: tensor([[  2694.0993,   1622.4416,  -2803.1709,  -7254.2966,  -1052.3668,
         -18591.3443,   5863.2755]], dtype=torch.float64)
	q_value: tensor([[-30.9166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5294433632413621, distance: 0.7849873258525525 entropy 10.619706896892396
epoch: 43, step: 60
	action: tensor([[-19409.1788, -22252.8923,   3747.3241,  25700.8884,  18641.2050,
           7314.1971,  24385.6151]], dtype=torch.float64)
	q_value: tensor([[-34.7222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14979612818050048, distance: 1.2270637157125266 entropy 10.894761552294016
epoch: 43, step: 61
	action: tensor([[-15092.9323, -17268.8548,   1075.8792,   3873.1564, -22058.6876,
          12187.0520,  16692.4511]], dtype=torch.float64)
	q_value: tensor([[-23.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027335745450766114, distance: 1.159879554154964 entropy 10.768146741091837
epoch: 43, step: 62
	action: tensor([[ 7813.6697, -9015.9999, -4196.4264, -6234.2996, 16254.6770,  7419.6890,
          9910.7297]], dtype=torch.float64)
	q_value: tensor([[-30.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027409041832410885, distance: 1.128552603924502 entropy 10.96210465729216
epoch: 43, step: 63
	action: tensor([[ -2745.3579, -13426.0949,   2852.3151,  -9718.3577, -18812.8594,
          -1051.4527,   8890.0758]], dtype=torch.float64)
	q_value: tensor([[-25.1832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5036392415761183, distance: 0.8062234259041706 entropy 10.641682624223845
epoch: 43, step: 64
	action: tensor([[  3765.4217,  -1846.9698,    951.2923,  -6008.0825, -10778.9169,
           -934.9690,  29631.4843]], dtype=torch.float64)
	q_value: tensor([[-27.1759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07312814857968031, distance: 1.1017080932443197 entropy 10.687171093717021
epoch: 43, step: 65
	action: tensor([[-1.3080e+04, -1.3819e+04, -4.6496e+02,  4.2214e-01, -1.0023e+04,
         -1.8274e+04, -1.7898e+03]], dtype=torch.float64)
	q_value: tensor([[-33.9976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10736420569931049, distance: 1.2042091808534594 entropy 10.866228183588357
epoch: 43, step: 66
	action: tensor([[  7805.2181,  -3350.6088,  -8873.1847,   7176.5419,  -3349.7456,
           8349.8465, -10723.0446]], dtype=torch.float64)
	q_value: tensor([[-23.7171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18730714178244479, distance: 1.0316203841028382 entropy 10.624046646434063
epoch: 43, step: 67
	action: tensor([[ 30931.5758, -10115.0886,   2063.7423,    770.3813,  30636.7414,
          13799.9282,  20158.7746]], dtype=torch.float64)
	q_value: tensor([[-27.4687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3703807584886777, distance: 0.908020574577134 entropy 10.832626521567176
epoch: 43, step: 68
	action: tensor([[-11946.6380, -11546.9572,   5290.3342,    649.5094, -18458.0208,
          -1065.4285,  -5659.0000]], dtype=torch.float64)
	q_value: tensor([[-29.2883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20007507857765883, distance: 1.2536055373113053 entropy 10.979153582164411
epoch: 43, step: 69
	action: tensor([[-18898.8152, -17034.0417,   3453.0386,   1221.5923,   8511.1119,
           7014.7972,   8831.5301]], dtype=torch.float64)
	q_value: tensor([[-28.5824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3040815295160686, distance: 1.3067998175266542 entropy 10.905992962248833
epoch: 43, step: 70
	action: tensor([[-19328.1607, -32064.1697, -20809.1194,  16764.3707,  -1449.1752,
          -1052.2529,  -7905.5522]], dtype=torch.float64)
	q_value: tensor([[-30.6062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21232514613093478, distance: 1.015617519286949 entropy 11.185596455723424
epoch: 43, step: 71
	action: tensor([[-16531.4906,   4943.4003,   6506.1233,   6505.6788, -13472.4509,
           9162.9148,   2893.9336]], dtype=torch.float64)
	q_value: tensor([[-31.3428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2117300354641105, distance: 1.016001110751141 entropy 10.927244657110174
epoch: 43, step: 72
	action: tensor([[-12473.3343, -11604.2741,  11959.0395,   7263.2636,  -7512.8473,
          21090.1141,   3622.0395]], dtype=torch.float64)
	q_value: tensor([[-32.6372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12065133320374444, distance: 1.2114122174453794 entropy 11.030201167830043
epoch: 43, step: 73
	action: tensor([[-4596.8343, -4924.1379, 11163.2375,   824.5828, 15651.6682,  8539.0881,
         -5068.3719]], dtype=torch.float64)
	q_value: tensor([[-31.4789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01592852320489202, distance: 1.1351938124736796 entropy 11.076567642204278
epoch: 43, step: 74
	action: tensor([[-33171.5110,  13536.3675,  15603.2708,  17123.0205,   1306.2196,
            786.3904,   5402.9128]], dtype=torch.float64)
	q_value: tensor([[-28.2468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4934735803292696, distance: 0.8144374669267601 entropy 11.032351024469948
epoch: 43, step: 75
	action: tensor([[17559.4716, 21444.5672, -3918.0738,  1288.8860, 16990.3767, -4253.6060,
         28278.9820]], dtype=torch.float64)
	q_value: tensor([[-26.8138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7423314112471917, distance: 0.5808813494876666 entropy 10.781877033351021
epoch: 43, step: 76
	action: tensor([[ -8340.5798,  -4687.8825,  14418.8454, -21465.2640, -14801.1874,
         -20101.3306,   4388.6217]], dtype=torch.float64)
	q_value: tensor([[-32.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2692232751235717, distance: 1.289216060372369 entropy 10.718944368322607
epoch: 43, step: 77
	action: tensor([[  3054.7534,  16942.1670,  26336.1864,  -3831.3013,  -1380.4920,
         -14989.6833,  -9683.8187]], dtype=torch.float64)
	q_value: tensor([[-32.8139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.97073449633271
epoch: 43, step: 78
	action: tensor([[-4812.6932, -5362.1589, -8655.2780,  6759.9330, -2302.0127,  7826.9056,
         -5893.0756]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24115502168701086, distance: 1.2748811729795915 entropy 10.40462102339717
epoch: 43, step: 79
	action: tensor([[-12416.3848,   1809.9394,  13762.6077, -27345.9406,  22556.2962,
         -19422.0113, -27458.9195]], dtype=torch.float64)
	q_value: tensor([[-31.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1879276832225919, distance: 1.0312264557939312 entropy 11.028391375119456
epoch: 43, step: 80
	action: tensor([[-23410.9723,  11024.0156,   1518.7009,   9828.2546,  14208.4142,
          17699.0362,  16949.8557]], dtype=torch.float64)
	q_value: tensor([[-31.2186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.71492053650971
epoch: 43, step: 81
	action: tensor([[  -948.3260,  -2746.2813,  -4463.0917,  -1314.7667,  11699.7351,
          11047.1392, -10310.8085]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6445815939736126, distance: 1.4675212747171917 entropy 10.40462102339717
epoch: 43, step: 82
	action: tensor([[-27099.0903,  15069.8845,   -904.2128,  14979.4916,  12389.5359,
          -2490.8547,   6200.6638]], dtype=torch.float64)
	q_value: tensor([[-31.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5384153798493925, distance: 0.7774677052593065 entropy 10.816816632052616
epoch: 43, step: 83
	action: tensor([[ -2669.2067, -23301.9226,  -9299.5356,  -4967.2381,  -5859.1749,
           9621.2419,   4215.5900]], dtype=torch.float64)
	q_value: tensor([[-29.5170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14788031145149572, distance: 1.0563479485738503 entropy 10.600581522430529
epoch: 43, step: 84
	action: tensor([[-19243.6024,  11053.5247,  22349.4174,   5121.7594,  -5100.8455,
          14309.5536, -18192.5908]], dtype=torch.float64)
	q_value: tensor([[-32.5837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5707527169878714, distance: 0.7497396354909209 entropy 10.946607687836732
epoch: 43, step: 85
	action: tensor([[ 17677.3077,  -7651.3405,   1084.1317,  20406.2325,   5002.6969,
           9715.9478, -19536.8317]], dtype=torch.float64)
	q_value: tensor([[-32.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5365337313649872, distance: 0.7790507660874316 entropy 10.777494711708831
epoch: 43, step: 86
	action: tensor([[  5283.1530, -13873.1499, -27405.2304,  14887.1246,  20274.9446,
         -10247.2247,    165.1986]], dtype=torch.float64)
	q_value: tensor([[-30.2813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1946868735638574, distance: 1.2507880958121567 entropy 10.955011952938081
epoch: 43, step: 87
	action: tensor([[-23387.0806, -32820.7783, -18416.8388,  16231.2395,   3071.2718,
         -19276.1360,   3232.9406]], dtype=torch.float64)
	q_value: tensor([[-38.0940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6274441257714627, distance: 1.4598550509385309 entropy 10.907331847636783
epoch: 43, step: 88
	action: tensor([[-9573.3396, 14186.6150,   954.1004, 18674.7161,  1108.9377,  9258.7943,
          2978.9784]], dtype=torch.float64)
	q_value: tensor([[-29.5187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3149458563384142, distance: 0.9471508253709968 entropy 11.047589199841946
epoch: 43, step: 89
	action: tensor([[-11400.8238,  11098.8557,   1138.6826,   1472.4824,  10070.8980,
           9157.8351, -37184.1323]], dtype=torch.float64)
	q_value: tensor([[-31.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39123588596049297, distance: 0.892855575482389 entropy 10.994421981397311
epoch: 43, step: 90
	action: tensor([[ -4736.0859,  10644.8678,  -5185.4369,    401.3632,  13714.9564,
         -12322.5822,  -5184.5320]], dtype=torch.float64)
	q_value: tensor([[-26.3204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6237114573922836, distance: 0.7019677995101777 entropy 10.746930849588992
epoch: 43, step: 91
	action: tensor([[-33952.2935,  19748.8446,  -3960.5986,  -1884.9203,  -9159.3619,
          15984.4776,  -5740.6282]], dtype=torch.float64)
	q_value: tensor([[-30.1569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5097620467874053, distance: 0.8012354544721647 entropy 10.832799284311577
epoch: 43, step: 92
	action: tensor([[-13803.1526,  -2195.3749,   3431.5353,  -1642.9232,   2135.6378,
           1457.1680,  10436.5097]], dtype=torch.float64)
	q_value: tensor([[-31.3839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47027456706903203, distance: 1.387573240030289 entropy 10.600392914824964
epoch: 43, step: 93
	action: tensor([[-3.7380e+04,  1.9840e+02, -1.7014e+04,  4.9272e+03, -1.9475e+03,
         -2.0530e+01, -4.9480e+03]], dtype=torch.float64)
	q_value: tensor([[-27.9661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5825049507116546, distance: 1.4395582140820167 entropy 10.690685604013847
epoch: 43, step: 94
	action: tensor([[-5378.2633,  3328.3626, -2064.9321, -7261.2705, -8855.3631,  9895.1176,
         11642.1072]], dtype=torch.float64)
	q_value: tensor([[-19.6376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5986490190676226, distance: 0.7249680339926293 entropy 10.300066186447753
epoch: 43, step: 95
	action: tensor([[ 6281.7000, -7102.8310,  -184.4489, 26164.0154,  3703.3236,   151.9303,
         18591.7595]], dtype=torch.float64)
	q_value: tensor([[-36.3998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.840215568893639
epoch: 43, step: 96
	action: tensor([[  127.5714,  3455.1401, -5078.2799, -3314.8088,  6712.3277,   746.1738,
         13627.1195]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.40462102339717
epoch: 43, step: 97
	action: tensor([[-13816.4059,  -3019.2772,  -1358.7922,  -4441.4423,  -2319.7013,
           3246.0607,  18923.1019]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01891306663614256, distance: 1.1551150947061024 entropy 10.40462102339717
epoch: 43, step: 98
	action: tensor([[ -6429.6359,  -8833.1357, -14224.5527, -12973.1853,  -4846.4635,
           3209.8055, -12586.1951]], dtype=torch.float64)
	q_value: tensor([[-34.4159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17109714965852074, distance: 1.2383777922768155 entropy 10.921435980502983
epoch: 43, step: 99
	action: tensor([[  6623.5518,  -1592.5003, -14721.6869,   -158.0902, -25535.4312,
         -15552.7950, -14536.4032]], dtype=torch.float64)
	q_value: tensor([[-31.4553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35545908845047536, distance: 0.9187173915210941 entropy 10.839374414237229
epoch: 43, step: 100
	action: tensor([[ -6159.7214,    895.5934,  -2733.9462,  -4577.6673, -13016.4518,
          -3576.1407,   7909.2232]], dtype=torch.float64)
	q_value: tensor([[-31.5462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25737209443454123, distance: 1.2831830140270308 entropy 10.819482954361083
epoch: 43, step: 101
	action: tensor([[ -6517.2366, -18642.0130, -19393.0398,  26210.0201,  -7492.8474,
          17695.3645,  25702.0892]], dtype=torch.float64)
	q_value: tensor([[-40.2306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9742937720537275, distance: 1.6079131278617653 entropy 10.83108610604281
epoch: 43, step: 102
	action: tensor([[   262.1949, -20173.0234,  34560.5523, -15569.0244, -17463.3144,
          32631.2874, -20892.1025]], dtype=torch.float64)
	q_value: tensor([[-33.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4869022322291796, distance: 0.819703436666589 entropy 11.133604119586233
epoch: 43, step: 103
	action: tensor([[ -7816.5272, -13995.1770,   3803.7835,   3590.3308,   6504.3317,
          -6536.2835,  -6146.4187]], dtype=torch.float64)
	q_value: tensor([[-32.2450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5824979761437736, distance: 1.4395550417988776 entropy 10.922831631544554
epoch: 43, step: 104
	action: tensor([[ -3256.7015,   -992.0814, -32956.3745, -14743.6693, -21773.1452,
          -3941.0555,  -7407.3042]], dtype=torch.float64)
	q_value: tensor([[-28.6897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7241709135155279, distance: 1.5026119917191456 entropy 10.825742535923851
epoch: 43, step: 105
	action: tensor([[-23194.4649, -15723.4122, -14529.3044,  -1530.2206,  28319.0049,
         -24461.5386,  16058.9300]], dtype=torch.float64)
	q_value: tensor([[-35.7762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21688687734478496, distance: 1.0126723310222823 entropy 11.033388406915018
epoch: 43, step: 106
	action: tensor([[-16900.6849,   -542.9355,   9157.7359,  -1117.7624,   6362.6074,
          16372.6750, -18226.8311]], dtype=torch.float64)
	q_value: tensor([[-36.2437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13405734799197022, distance: 1.0648814430260567 entropy 10.962641314969767
epoch: 43, step: 107
	action: tensor([[-18103.9856, -22641.0558, -20629.2874,  39570.0050,  13651.1432,
         -19501.0620,  20856.3705]], dtype=torch.float64)
	q_value: tensor([[-38.0637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7313194109830612, distance: 1.5057237213458465 entropy 11.069328302218137
epoch: 43, step: 108
	action: tensor([[ -6002.7818, -27631.7368, -14379.6319,   6266.1455,  -8390.7291,
          -5609.4655,  12920.1220]], dtype=torch.float64)
	q_value: tensor([[-28.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014680414677038178, distance: 1.1359134736701415 entropy 10.999057073408178
epoch: 43, step: 109
	action: tensor([[  5187.6299, -14878.8280,  -7718.9610,   8108.8469,   4257.2065,
          38238.9910,   6143.5543]], dtype=torch.float64)
	q_value: tensor([[-32.8910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24645481016785276, distance: 1.2776001738487148 entropy 11.135391512130656
epoch: 43, step: 110
	action: tensor([[-24265.8874, -16031.4541, -14600.1876,  32951.4050,   6307.3363,
            160.5063,  14484.9889]], dtype=torch.float64)
	q_value: tensor([[-40.6409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06398778297263097, distance: 1.1803886201423148 entropy 11.1676652091284
epoch: 43, step: 111
	action: tensor([[-11397.0175, -20791.0726,  24561.3295,  24136.1027,   1571.2069,
           6173.5282,  21244.7110]], dtype=torch.float64)
	q_value: tensor([[-28.5657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23544785094009124, distance: 1.0005994272783907 entropy 10.9484679392088
epoch: 43, step: 112
	action: tensor([[   584.5063, -10076.5519,   3184.8562, -30007.3077, -19129.6706,
         -17277.6650,   6161.1144]], dtype=torch.float64)
	q_value: tensor([[-25.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1849973494666668, distance: 1.2457055023094463 entropy 10.81440775627132
epoch: 43, step: 113
	action: tensor([[ -8424.7182,   7860.3856,   6775.3749,  31679.8393, -16787.0706,
           3977.9341, -14351.2437]], dtype=torch.float64)
	q_value: tensor([[-29.7183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.848760279742011
epoch: 43, step: 114
	action: tensor([[ 8731.6801,  5864.6610,  7976.2381,  5731.5837, 11978.2953, -7938.7154,
          1237.9630]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44540316773597455, distance: 0.8522075659947153 entropy 10.40462102339717
epoch: 43, step: 115
	action: tensor([[ -7475.5608, -23613.3656,  13641.7588,  27568.7356,   6626.1140,
          -4299.0027,  -6088.8630]], dtype=torch.float64)
	q_value: tensor([[-32.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13803375880053903, distance: 1.062433660621471 entropy 10.880156724810348
epoch: 43, step: 116
	action: tensor([[ -2421.1605, -43064.4274,  -7834.2374,  36253.9046,   5704.8655,
          25360.9956,   8795.4127]], dtype=torch.float64)
	q_value: tensor([[-31.7767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04330964269707005, distance: 1.1192894018120516 entropy 11.012977368757573
epoch: 43, step: 117
	action: tensor([[ -9976.0718, -13957.0246,  -4585.7890,   5466.8660,  -5304.6375,
          -4620.6257,  -1342.7877]], dtype=torch.float64)
	q_value: tensor([[-26.2752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12443562907220784, distance: 1.0707811902302653 entropy 10.80842531785631
epoch: 43, step: 118
	action: tensor([[ -4978.1242,  -9603.6896, -18748.5215,    535.1532,  -1332.8391,
          15514.6472,  16937.9554]], dtype=torch.float64)
	q_value: tensor([[-27.0002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24076734603501393, distance: 1.2746820524098683 entropy 10.896933699573136
epoch: 43, step: 119
	action: tensor([[ -5777.1512,   2310.7333,  11290.6848, -11820.0023,  10286.5330,
          18032.2223,  -3837.9647]], dtype=torch.float64)
	q_value: tensor([[-25.5480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5182726951800989, distance: 0.7942501854710015 entropy 10.78613010961375
epoch: 43, step: 120
	action: tensor([[-11577.5215,  -3482.5929,  -1422.4310,  16387.6468,   4389.9742,
           -506.9140,  20878.7084]], dtype=torch.float64)
	q_value: tensor([[-30.8428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6142084145713858, distance: 1.4539065620937874 entropy 10.587107618152206
epoch: 43, step: 121
	action: tensor([[-19133.5276, -22541.5226,   2372.1940,  10699.4660,   -466.0088,
          14825.3422,  -3437.0035]], dtype=torch.float64)
	q_value: tensor([[-25.6959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23067512072559804, distance: 1.0037176981310312 entropy 10.90903552015173
epoch: 43, step: 122
	action: tensor([[  7858.5686, -26049.2505,   2318.8064,  21653.4342,   4127.7644,
         -23297.0555,  15236.0269]], dtype=torch.float64)
	q_value: tensor([[-28.0009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25849666151109263, distance: 1.2837567116892004 entropy 11.035032031503007
epoch: 43, step: 123
	action: tensor([[ -5893.9819,   1118.4562,    675.5076,   4261.5838,   4826.1718,
         -10832.8362,   6763.9617]], dtype=torch.float64)
	q_value: tensor([[-31.3944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10157204511910989, distance: 1.0846717310057148 entropy 10.707985662719278
epoch: 43, step: 124
	action: tensor([[  5296.8009,  -4921.8356,  -2221.3570, -23765.6479,   5197.8032,
          -2701.1001,   2272.1092]], dtype=torch.float64)
	q_value: tensor([[-36.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2553468191112914, distance: 0.9874922997259752 entropy 11.10828197160353
epoch: 43, step: 125
	action: tensor([[-27345.9173, -26077.7304,  30281.1538,  19955.2578, -16100.6175,
          10850.8256,  11267.6614]], dtype=torch.float64)
	q_value: tensor([[-35.3067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3312939273588522, distance: 1.3203639819938924 entropy 10.8727070887477
epoch: 43, step: 126
	action: tensor([[  7137.0401,  18121.0182,   3949.7104,  -2366.0099,  11684.7765,
         -14541.5475,  15391.8479]], dtype=torch.float64)
	q_value: tensor([[-30.6661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.121486808080636
epoch: 43, step: 127
	action: tensor([[-9205.4154, -4522.4192, 12366.2611, 11596.2123, 10147.6394, -6068.7511,
          2783.5057]], dtype=torch.float64)
	q_value: tensor([[-32.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9432374864999507, distance: 1.5952165002149352 entropy 10.40462102339717
LOSS epoch 43 actor 389.03106087906707 critic 173.0697691880615
epoch: 44, step: 0
	action: tensor([[ -3954.8719, -18658.5045, -14120.6675,  -2872.3387,   6589.6777,
         -37329.0406,   1916.0824]], dtype=torch.float64)
	q_value: tensor([[-17.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.744626868101665
epoch: 44, step: 1
	action: tensor([[-15377.3600,  -6448.4335,   2760.2633,   3718.7367,  -2476.1382,
           6273.4886,  -7111.8776]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31979236577041703, distance: 0.9437945044818628 entropy 10.451788480589126
epoch: 44, step: 2
	action: tensor([[-17292.9928,  17225.2270,   6908.3835,   1533.6391, -20900.3517,
          32282.5883,   3134.9515]], dtype=torch.float64)
	q_value: tensor([[-21.4018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48614719550060026, distance: 0.8203063224354483 entropy 10.974817190913466
epoch: 44, step: 3
	action: tensor([[-35441.4334, -35849.7572,   3740.1908, -20200.5436, -32687.7368,
          18733.7212,  15354.9969]], dtype=torch.float64)
	q_value: tensor([[-29.5179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.148096167279377
epoch: 44, step: 4
	action: tensor([[  -333.6644,   5715.3224,   1047.5963,   5900.7368,  15696.9484,
          -3296.0965, -15068.3297]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16219734949049514, distance: 1.2336632670714167 entropy 10.451788480589126
epoch: 44, step: 5
	action: tensor([[15280.0827,  6039.5009, -9964.7988,  7188.0780, 27723.4943, 10379.7045,
         12535.6146]], dtype=torch.float64)
	q_value: tensor([[-24.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18711097431748847, distance: 1.0317448826406044 entropy 10.895544729401275
epoch: 44, step: 6
	action: tensor([[  4459.0002,  19136.4166,   8297.6495,  12791.8115,  -6163.6193,
         -13384.3270,  10728.7122]], dtype=torch.float64)
	q_value: tensor([[-21.5483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.691508286449473
epoch: 44, step: 7
	action: tensor([[ -3401.1462, -13446.6123, -13908.0307,  -6870.7229,    406.0041,
           5995.3530,   9365.1249]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13514825231009475, distance: 1.0642104691103462 entropy 10.451788480589126
epoch: 44, step: 8
	action: tensor([[-10083.0804, -21716.6189,   5166.1217, -14298.4226, -14031.1851,
            224.8711,  34513.8181]], dtype=torch.float64)
	q_value: tensor([[-28.5449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7461181770264778, distance: 1.5121452512734068 entropy 11.089837061167785
epoch: 44, step: 9
	action: tensor([[-13860.5704, -26352.0593,   8467.7626,   -686.3835,  -3993.8132,
          -4446.9979,   6096.8492]], dtype=torch.float64)
	q_value: tensor([[-23.0547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4871240758699924, distance: 1.395501461397964 entropy 10.78987637417166
epoch: 44, step: 10
	action: tensor([[  -869.1274, -23079.1427, -25125.0685,  17899.8590,  15797.0046,
          14651.7326, -33913.2456]], dtype=torch.float64)
	q_value: tensor([[-24.8961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12436369496883204, distance: 1.2134170703250904 entropy 11.065162853282658
epoch: 44, step: 11
	action: tensor([[  1202.4100, -11876.5837,  10430.9118,  36642.3361,  -5649.9562,
          23955.2085,   2149.0604]], dtype=torch.float64)
	q_value: tensor([[-20.2875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5945257918811118, distance: 0.7286824509536649 entropy 10.942224021858536
epoch: 44, step: 12
	action: tensor([[ -6796.3370, -14642.0177,   2495.7762,   3879.3801,   7805.3527,
          -3455.4944, -11855.7043]], dtype=torch.float64)
	q_value: tensor([[-23.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3391702044220595, distance: 1.3242640284298446 entropy 10.789579706251983
epoch: 44, step: 13
	action: tensor([[  2974.5230, -12433.6608, -14107.3447,   7802.1875,  11247.8434,
           8822.6434,  14520.2681]], dtype=torch.float64)
	q_value: tensor([[-21.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.856341656264986
epoch: 44, step: 14
	action: tensor([[ -7484.3570, -19720.2872,   2520.8535,  11929.0621,  -6677.5223,
           2529.7921,   8223.2405]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5179802615012514, distance: 1.4099046908748218 entropy 10.451788480589126
epoch: 44, step: 15
	action: tensor([[ -2739.3230, -19980.9729,  17001.3665,  -2499.9567,  -8448.6551,
           -504.8322,   5239.5774]], dtype=torch.float64)
	q_value: tensor([[-19.4124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0556597976367401, distance: 1.6407118490024035 entropy 10.88416785141625
epoch: 44, step: 16
	action: tensor([[ -8087.7073, -23444.6197, -10360.2918,  -4610.0226,  -4370.9024,
           -718.9267,  -9640.3589]], dtype=torch.float64)
	q_value: tensor([[-19.7801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6575024225673145, distance: 1.4732748636763626 entropy 10.74377319405996
epoch: 44, step: 17
	action: tensor([[ -6355.3678, -16727.1320,  -1519.9968,  15137.4206, -16638.3925,
           4052.4470,   8553.3518]], dtype=torch.float64)
	q_value: tensor([[-26.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6483343251795033, distance: 1.4691946714000657 entropy 10.959597950481227
epoch: 44, step: 18
	action: tensor([[-18664.5886,  -7567.1139,  -4862.6042,   3670.0684,  -4217.1579,
          22675.1232,   8895.6550]], dtype=torch.float64)
	q_value: tensor([[-18.0256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39065123424822445, distance: 1.3494779914249384 entropy 10.79797622883251
epoch: 44, step: 19
	action: tensor([[-24562.7572, -22585.8836, -15893.4351,   1336.4045,   5740.3387,
           3512.0979, -12837.4590]], dtype=torch.float64)
	q_value: tensor([[-22.5257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29519552370295576, distance: 1.3023399430410985 entropy 10.997445201300497
epoch: 44, step: 20
	action: tensor([[  6590.1882,  -2232.1871, -43544.8545,   7228.1638,    727.8728,
           9200.9682,   1024.9603]], dtype=torch.float64)
	q_value: tensor([[-24.7126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1404059910913441, distance: 1.060970682273937 entropy 11.071232344601636
epoch: 44, step: 21
	action: tensor([[  3213.6009,  -3414.3939, -11503.9446,   2428.9641, -17022.7638,
          13227.8478, -31199.5955]], dtype=torch.float64)
	q_value: tensor([[-20.8528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2860290982785595, distance: 0.966934262543097 entropy 10.94158985340015
epoch: 44, step: 22
	action: tensor([[-18115.0670,   8710.5248,  -4098.3195,  44147.9373,  -7400.4797,
         -23641.3935,   5392.2537]], dtype=torch.float64)
	q_value: tensor([[-24.6915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0699174149043762, distance: 1.1036146314689397 entropy 10.989829864884346
epoch: 44, step: 23
	action: tensor([[  -900.0805,  -3843.0101, -11549.5670,  12871.2604,   -259.4165,
            999.3013,     47.6622]], dtype=torch.float64)
	q_value: tensor([[-23.3584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41489044535863884, distance: 1.36118796369044 entropy 10.832888518684394
epoch: 44, step: 24
	action: tensor([[-21538.4054, -24159.5069,  -1518.0761,  16174.9149,  -2895.5933,
           6768.1389,  -9092.4944]], dtype=torch.float64)
	q_value: tensor([[-22.5397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2511146121831136, distance: 1.2799860646618357 entropy 11.0960483074806
epoch: 44, step: 25
	action: tensor([[-15605.8249,   5067.4321,  23709.8449,   1924.5887,  -9541.2002,
          23583.9422,  13255.1437]], dtype=torch.float64)
	q_value: tensor([[-22.9553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12627406853796397, distance: 1.214447473610061 entropy 11.074994552523735
epoch: 44, step: 26
	action: tensor([[-26998.1429,   6212.1737,  20733.0549,  19144.7440, -16940.5959,
          25197.0961,  16669.4161]], dtype=torch.float64)
	q_value: tensor([[-30.6165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08480732752046038, distance: 1.191881278929262 entropy 11.223580735589746
epoch: 44, step: 27
	action: tensor([[ -8215.9645,   6822.9042,  -4328.6922,   5916.2611,  14048.6171,
         -11697.8403,  -7010.8497]], dtype=torch.float64)
	q_value: tensor([[-24.0601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6701356459516261, distance: 0.6572406051443758 entropy 10.898522521214511
epoch: 44, step: 28
	action: tensor([[ -6150.5516,  -2983.7164,   1516.0089,  25942.5504,  17263.5619,
          34683.7797, -20196.5983]], dtype=torch.float64)
	q_value: tensor([[-22.4337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28832175745384747, distance: 1.2988795043945494 entropy 10.786399890686429
epoch: 44, step: 29
	action: tensor([[-35243.5959,  14830.2720,  -1414.7794,  46747.4734,   4501.0105,
         -52164.6094,  10118.4907]], dtype=torch.float64)
	q_value: tensor([[-24.0230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.13594873764602
epoch: 44, step: 30
	action: tensor([[ -7045.7297,  -9189.9740,  -5232.1212, -12129.5047,   9181.2340,
            759.0418,  -2362.2171]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5427758965132383, distance: 1.4213731778925964 entropy 10.451788480589126
epoch: 44, step: 31
	action: tensor([[ -1697.4740,   8473.9579,  -3877.0874,   3631.9134, -33010.8549,
          11430.8025,   9480.6576]], dtype=torch.float64)
	q_value: tensor([[-25.2606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3667922665028355, distance: 0.9106045131753249 entropy 10.9835738027744
epoch: 44, step: 32
	action: tensor([[-2638.7650, -5110.5247, -6725.3037,   753.5708, -5859.5856, 11359.6524,
          8205.0926]], dtype=torch.float64)
	q_value: tensor([[-25.5207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7514171540336498, distance: 1.514437980673062 entropy 10.681243308473155
epoch: 44, step: 33
	action: tensor([[-12971.0045,  14287.6643,  -5839.9405,  -9178.0032,  17273.1213,
         -20646.4577,  37531.6555]], dtype=torch.float64)
	q_value: tensor([[-23.3755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6484860709773366, distance: 0.6784658032102826 entropy 11.09086368577181
epoch: 44, step: 34
	action: tensor([[-10463.8523,  -6004.1247,   6997.1882,   2012.3337,   -494.4593,
           4486.8989,  -2529.8422]], dtype=torch.float64)
	q_value: tensor([[-33.4266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8571156372388911, distance: 1.5594669197168203 entropy 10.91316683387643
epoch: 44, step: 35
	action: tensor([[-14233.5999, -29756.2274, -18247.8774,   3837.2239,  -5722.6214,
          -2942.2967,  -6686.1207]], dtype=torch.float64)
	q_value: tensor([[-20.8996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2306661701839936, distance: 1.269482809980969 entropy 10.966220585540189
epoch: 44, step: 36
	action: tensor([[ -9616.8347, -22664.3781, -24305.1005,  12590.6781,   1945.5722,
         -20719.9810,  15417.6324]], dtype=torch.float64)
	q_value: tensor([[-20.1663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13458527278816734, distance: 1.2189201719943132 entropy 10.93937383831987
epoch: 44, step: 37
	action: tensor([[-17276.7871, -30021.4793,  -6946.1942,  20195.3395,  -3887.2759,
           9350.9423,   8026.9261]], dtype=torch.float64)
	q_value: tensor([[-22.9049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40162881784224136, distance: 1.3547938057180755 entropy 11.099050560706518
epoch: 44, step: 38
	action: tensor([[-42537.3891,   4871.6999,  24382.4259,    930.1190,  29187.8347,
         -34815.7224,  26403.8199]], dtype=torch.float64)
	q_value: tensor([[-26.4811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1315347007303057, distance: 1.0664314109486492 entropy 11.178042376983525
epoch: 44, step: 39
	action: tensor([[-13605.8443,  -5503.0645,  -8711.4789, -22342.7083,   8902.6493,
          -2201.1567,  -2622.8750]], dtype=torch.float64)
	q_value: tensor([[-27.6226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4443910274909415, distance: 1.3753051974292356 entropy 11.099675015597485
epoch: 44, step: 40
	action: tensor([[-15254.6095,  -5336.3320,  -4757.8844,  21797.6596,   8874.2386,
          -6047.0905,  14539.0202]], dtype=torch.float64)
	q_value: tensor([[-27.8926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08900701462327898, distance: 1.0922302734171456 entropy 11.109455766589534
epoch: 44, step: 41
	action: tensor([[-15275.5054,  16218.2450,  -2276.3299, -15439.9765,  -4629.7288,
          20517.4668, -16661.8369]], dtype=torch.float64)
	q_value: tensor([[-20.8493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05563452538618496, distance: 1.1757459356430726 entropy 10.977455895183892
epoch: 44, step: 42
	action: tensor([[-17989.7455,  -1326.8655,  -2127.1465,  -1774.6364,  -9339.7219,
         -28644.0756,   6604.2841]], dtype=torch.float64)
	q_value: tensor([[-27.3528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.69602172480362
epoch: 44, step: 43
	action: tensor([[-13419.2850,  -7179.8460,   2854.6042,   3943.7347,   3104.7599,
           6291.4112,   -961.9351]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 44
	action: tensor([[ -3588.4500,  -2595.6363,  14858.6754,   4059.2719, -12574.9092,
         -17423.9840,   6001.0497]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 45
	action: tensor([[  5934.1047,  -1237.3848, -14829.6300,    -92.3483, -14003.0743,
            259.3727,   3772.4508]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 46
	action: tensor([[ 5034.0497,  6356.1085,  2799.9426,  8404.9436, -8208.1636, 21924.2707,
         -4091.7032]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 47
	action: tensor([[-21582.3782,   2766.4652,   7675.2724,   7089.5057, -11035.4493,
           1390.0283,   9601.8856]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22887949142302633, distance: 1.004888370316956 entropy 10.451788480589126
epoch: 44, step: 48
	action: tensor([[ -3458.0691, -16885.6900,    203.8507,  -7869.2375,  10790.9721,
          -8152.1218, -10091.1004]], dtype=torch.float64)
	q_value: tensor([[-26.6320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43251222456836125, distance: 1.3696382045160145 entropy 10.63520447286589
epoch: 44, step: 49
	action: tensor([[-26293.1794,  16376.8862,   1672.7561,  13801.3530,   7137.1474,
           8700.0527,  16094.6819]], dtype=torch.float64)
	q_value: tensor([[-29.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07693418770200422, distance: 1.1875482808341562 entropy 11.09048439565515
epoch: 44, step: 50
	action: tensor([[ -6270.5965,    207.0421,  13055.2742, -13630.3174,  -5715.5758,
         -12943.8877,   5418.1864]], dtype=torch.float64)
	q_value: tensor([[-22.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25592893230064084, distance: 1.2824464091597703 entropy 10.81015712266783
epoch: 44, step: 51
	action: tensor([[  7007.5587, -15948.7919,   -547.0365, -22616.0972,   1470.5546,
            857.6909,  -2721.9447]], dtype=torch.float64)
	q_value: tensor([[-27.8285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15468321331390134, distance: 1.2296687019577919 entropy 10.825002059249115
epoch: 44, step: 52
	action: tensor([[ 5095.7831,  5643.9205, -4987.9458, -6004.7658, 15396.9973,  8354.9156,
          -711.9811]], dtype=torch.float64)
	q_value: tensor([[-24.9984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4789373641517689, distance: 0.8260411056167509 entropy 10.835515513889067
epoch: 44, step: 53
	action: tensor([[-12135.7137, -12240.4832, -14399.2186,  32359.6248,  14642.9923,
          10829.6024,   -342.2480]], dtype=torch.float64)
	q_value: tensor([[-29.3221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.912250642752527
epoch: 44, step: 54
	action: tensor([[-14699.2611, -25033.8077,  -6899.0427,  10503.8249,   7240.4861,
          -5643.9093,  -2760.7217]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 55
	action: tensor([[ -4724.6772, -15915.5636,  11631.2260,  -4027.2348,   -303.6195,
           3886.2864, -20744.8474]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2307521557678287, distance: 1.269527158040831 entropy 10.451788480589126
epoch: 44, step: 56
	action: tensor([[-27124.0153,  -8542.9430, -12226.9494, -17230.8411,   7091.6297,
           -513.1469, -19853.0841]], dtype=torch.float64)
	q_value: tensor([[-24.4854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07420773535527325, distance: 1.101066291452891 entropy 10.880227783934851
epoch: 44, step: 57
	action: tensor([[  4886.2912,  -1050.2420, -12685.8967,   7086.2706,   4601.4484,
           -486.2195, -20715.6751]], dtype=torch.float64)
	q_value: tensor([[-24.9013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891234510568067, distance: 0.9648366387937065 entropy 10.96497086785028
epoch: 44, step: 58
	action: tensor([[ -3566.8892, -20841.4821,   -280.1350,  10378.5343,   4338.2141,
          -7018.1613,   1766.8057]], dtype=torch.float64)
	q_value: tensor([[-21.2768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.041065227621854605, distance: 1.1676042409234302 entropy 10.657760516478078
epoch: 44, step: 59
	action: tensor([[ -9814.7353, -26229.6461, -10570.6384,  14524.1889,  -1996.1794,
          -1248.1665,  -2447.9277]], dtype=torch.float64)
	q_value: tensor([[-24.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07138006026654331, distance: 1.1027465193420714 entropy 11.027456280354977
epoch: 44, step: 60
	action: tensor([[-2472.6752,  2796.7622, -9470.9299, -7650.1986, -2706.5115, 18246.9308,
         53756.0025]], dtype=torch.float64)
	q_value: tensor([[-24.7577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4407973114235433, distance: 0.8557389869981553 entropy 11.210039845588202
epoch: 44, step: 61
	action: tensor([[ -8375.5492,  -1065.4120,   -899.4037, -18528.3655,  -7793.9756,
         -10494.8438,    651.2365]], dtype=torch.float64)
	q_value: tensor([[-32.1613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.973918123875105
epoch: 44, step: 62
	action: tensor([[-19427.4145,  15402.3550,   8177.4360,  -3073.7032,   4909.0174,
           7889.6698,  -8023.5107]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29244376514804493, distance: 0.9625807547041826 entropy 10.451788480589126
epoch: 44, step: 63
	action: tensor([[  3743.0058,     40.3694,  -1028.6407,  16042.3213, -36044.0390,
          13485.5565,   7061.6793]], dtype=torch.float64)
	q_value: tensor([[-37.5968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7341901378461249, distance: 0.5899867229293521 entropy 11.007002243425948
epoch: 44, step: 64
	action: tensor([[-11372.3556,  -4121.5419, -21990.1214,   1392.5771,  10091.7120,
           -331.2552, -22461.6377]], dtype=torch.float64)
	q_value: tensor([[-23.2382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43097514351269606, distance: 1.3689031985821662 entropy 10.733353911305466
epoch: 44, step: 65
	action: tensor([[ -3555.3474, -12232.2947,   1031.4211,  -3321.6214,   6935.8889,
           3876.2436,    121.0342]], dtype=torch.float64)
	q_value: tensor([[-20.5569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.208424952169306, distance: 1.70058371531631 entropy 10.802513499562107
epoch: 44, step: 66
	action: tensor([[-1334.9378,  4896.8375,  8083.8946,  1003.4145,  5840.9280,  4435.5157,
          1598.8724]], dtype=torch.float64)
	q_value: tensor([[-18.0424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4091455671573524, distance: 0.8796237408118304 entropy 10.67177794181609
epoch: 44, step: 67
	action: tensor([[-38928.9713,  -5631.7054,  27124.7383,  -9764.7193,   4126.3825,
          39179.3034, -30558.0107]], dtype=torch.float64)
	q_value: tensor([[-25.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6508501020787225, distance: 1.4703154260647864 entropy 11.130340454212837
epoch: 44, step: 68
	action: tensor([[ -9234.5508, -30282.9802, -29336.9716,  11180.8622,     33.1187,
          21062.9889,  15248.0123]], dtype=torch.float64)
	q_value: tensor([[-24.5231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6722262361338145, distance: 1.479804043721826 entropy 10.938624387253883
epoch: 44, step: 69
	action: tensor([[-11322.5788, -30652.8217,  25201.6360, -20226.8950,  18816.9486,
          -3663.2312, -13317.1336]], dtype=torch.float64)
	q_value: tensor([[-22.5130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5076426533493068, distance: 1.4050956884617218 entropy 11.010758088591462
epoch: 44, step: 70
	action: tensor([[   183.4017, -33200.7331,  -7178.8877,   6244.2004,   9500.0971,
          -5073.1940,   8886.8880]], dtype=torch.float64)
	q_value: tensor([[-23.5838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14192327413689665, distance: 1.06003390060334 entropy 10.962444853858262
epoch: 44, step: 71
	action: tensor([[  7721.9525, -14437.9810,  15645.5592,  10099.2205,  14371.0015,
           7999.6744,  -4413.1796]], dtype=torch.float64)
	q_value: tensor([[-24.4557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020713757152763357, distance: 1.132430401512968 entropy 10.881612075911438
epoch: 44, step: 72
	action: tensor([[  6901.7026,   7677.5837,  22409.8746,  -4581.8496,   1181.6531,
          -5154.9435, -23983.2139]], dtype=torch.float64)
	q_value: tensor([[-25.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5363211898440383, distance: 0.7792293784987588 entropy 10.865589512746618
epoch: 44, step: 73
	action: tensor([[-11513.3891,  -1781.0444, -21537.6730,   8946.6423,   4497.4964,
          11887.4689,   8281.0445]], dtype=torch.float64)
	q_value: tensor([[-21.9007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.463984282117623
epoch: 44, step: 74
	action: tensor([[ -2911.5517,  -9711.6713,  16565.7712,   5722.0111,  -8882.9571,
           7456.7276, -12816.2185]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4306421055820273, distance: 1.368743893502875 entropy 10.451788480589126
epoch: 44, step: 75
	action: tensor([[  4281.1570,  -9824.3590, -11056.4185, -36608.4287,   7951.0055,
          -9232.8101,  -8456.9934]], dtype=torch.float64)
	q_value: tensor([[-20.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2279931166562028, distance: 1.2681033781595283 entropy 10.959030880860144
epoch: 44, step: 76
	action: tensor([[ -9009.5582,   3026.8848,   7338.1153,  11681.9157,   5097.0145,
         -12592.2122,   9988.9758]], dtype=torch.float64)
	q_value: tensor([[-27.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.85829011707524
epoch: 44, step: 77
	action: tensor([[  4810.2124,  -7118.7103,   2393.3397, -11605.9764,  -2732.3471,
          -4746.9873,   -245.8913]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 78
	action: tensor([[-15532.9298,   1827.0770, -11707.8514,  -3550.6102,  -5813.2758,
           -422.6463,  12628.0502]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 79
	action: tensor([[-7037.3247, -6155.4020,  5623.5284, 20322.8397,  2259.0048, -1866.8593,
          3494.6051]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9718407241963123, distance: 1.6069139063023552 entropy 10.451788480589126
epoch: 44, step: 80
	action: tensor([[  9852.9147,  10346.4159,   7795.3401,  -3262.0079, -17913.9958,
           1534.1356,  11331.2932]], dtype=torch.float64)
	q_value: tensor([[-22.3463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9261897752533621, distance: 1.5882037965664657 entropy 11.0963357656504
epoch: 44, step: 81
	action: tensor([[-19418.1762,   1653.6137, -10455.5529,   -762.5009,  21628.0789,
         -12037.4369,   2122.1290]], dtype=torch.float64)
	q_value: tensor([[-21.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10840758767869785, distance: 1.2047763629639703 entropy 10.766562814689406
epoch: 44, step: 82
	action: tensor([[ -4056.8873, -13397.8317,  -1218.2551,  31077.6712, -29933.1780,
         -15161.5296, -14702.9759]], dtype=torch.float64)
	q_value: tensor([[-26.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6579377087235649, distance: 0.6692822288212515 entropy 11.022349549346961
epoch: 44, step: 83
	action: tensor([[ -7876.6233, -11239.8372,  -6339.4696,  -2501.1374,   8767.3662,
         -25380.9789, -37242.1660]], dtype=torch.float64)
	q_value: tensor([[-24.0771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.986296448081044
epoch: 44, step: 84
	action: tensor([[  294.3059, 11458.8247,  2044.8846,  5625.2509, -2757.0114, -7533.2492,
         12265.8103]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 85
	action: tensor([[-13899.8257,    363.8447,  -8987.3981,   4353.8125,  -3314.3642,
          -4519.7956,   8849.7675]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01546512876113848, distance: 1.153159019999997 entropy 10.451788480589126
epoch: 44, step: 86
	action: tensor([[-3590.7251, -8608.1733,  1368.2244, 24761.8544, -3450.2689,   416.6226,
         -2570.7820]], dtype=torch.float64)
	q_value: tensor([[-26.9367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24940606663348508, distance: 0.9914235213113249 entropy 10.979925502679636
epoch: 44, step: 87
	action: tensor([[  2754.2814, -22799.0332,  -4387.8152,  -4428.7344,   2019.5236,
           7646.4791,  -6984.1524]], dtype=torch.float64)
	q_value: tensor([[-19.6566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43667836253551606, distance: 0.8588847858425277 entropy 10.897638749760947
epoch: 44, step: 88
	action: tensor([[ 6568.8438,  6810.2298,  3356.6969,  8434.0971,  -718.1514, -8625.2643,
         23954.2184]], dtype=torch.float64)
	q_value: tensor([[-23.7018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7133362938389372, distance: 0.6126931839761858 entropy 10.989554178788548
epoch: 44, step: 89
	action: tensor([[ -9303.0802, -13711.1218, -18915.2370,   9905.4982, -10089.3444,
           9346.6441,   8347.2884]], dtype=torch.float64)
	q_value: tensor([[-28.3754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42254377427244116, distance: 1.3648644213403416 entropy 11.03927303258379
epoch: 44, step: 90
	action: tensor([[-14572.0309, -17306.0781,  33670.8697, -14197.6715, -11733.8790,
           7944.0909,  18167.2014]], dtype=torch.float64)
	q_value: tensor([[-26.0789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1186048040193528, distance: 1.2103055737951338 entropy 11.101807022511723
epoch: 44, step: 91
	action: tensor([[20990.9802,  7810.0271,  2098.8079, -8613.2535,  2963.7453,   105.1908,
         15067.5754]], dtype=torch.float64)
	q_value: tensor([[-25.4509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6418517864645054, distance: 0.6848383746573353 entropy 11.117119655691283
epoch: 44, step: 92
	action: tensor([[ -2473.1607,  -6340.4804,   5173.0381,   8513.4329,  -8037.8438,
          -7438.5785, -11154.7013]], dtype=torch.float64)
	q_value: tensor([[-26.3222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3335805347519749, distance: 1.3214974126666015 entropy 10.706771317554987
epoch: 44, step: 93
	action: tensor([[ 3133.9186, 10257.1987,   474.1358, 10192.7414, -2733.5948, -5581.8042,
         -7685.0379]], dtype=torch.float64)
	q_value: tensor([[-21.4848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.861941945078152
epoch: 44, step: 94
	action: tensor([[ -1720.7567,   -724.0518,    274.8791,   4442.2396,   -856.0762,
           6365.6381, -13173.9718]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18849955394489504, distance: 1.247544956505173 entropy 10.451788480589126
epoch: 44, step: 95
	action: tensor([[ 20245.3491, -28408.9966,  39985.9354, -29165.2661,   8770.0929,
          16997.8575,  11089.9135]], dtype=torch.float64)
	q_value: tensor([[-22.5433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11891864575132205, distance: 1.210475346782462 entropy 11.035922447699672
epoch: 44, step: 96
	action: tensor([[ 3346.3314, -2656.7280,  8410.7338, -1240.7956, -3035.0163, 20346.5618,
          7308.1791]], dtype=torch.float64)
	q_value: tensor([[-18.6235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2902824544488929, distance: 1.299867511041133 entropy 10.527507602570916
epoch: 44, step: 97
	action: tensor([[ 10105.5117,  -1544.7990, -23094.6251,  17882.1514,   5349.0344,
           3694.0569,  10719.7805]], dtype=torch.float64)
	q_value: tensor([[-23.3891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03714252031179999, distance: 1.1228912495763093 entropy 10.783483694995553
epoch: 44, step: 98
	action: tensor([[ -9704.8749,   -721.2084,  -5361.2849,  -6546.6434, -15245.3378,
          -7646.9637,   2777.2321]], dtype=torch.float64)
	q_value: tensor([[-27.8527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03934210689323259, distance: 1.1216079294739445 entropy 10.863457265370995
epoch: 44, step: 99
	action: tensor([[  743.5268,  2797.4989, -2942.4281, 27727.9423, 12299.6003, -8129.4403,
         21257.1678]], dtype=torch.float64)
	q_value: tensor([[-26.1393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.9983738565404
epoch: 44, step: 100
	action: tensor([[  9105.7055, -10610.9587,  -1485.8638,  -6228.3160,  -6050.4070,
          18185.7109,   1699.3639]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.451788480589126
epoch: 44, step: 101
	action: tensor([[-18856.4605,   -914.6759,   6263.3350,   5333.1529, -10308.2280,
            679.9204,   5417.6639]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8952283130200209, distance: 1.57538773913857 entropy 10.451788480589126
epoch: 44, step: 102
	action: tensor([[-10513.2921,   2729.9271,   8430.6783, -21872.6009, -33924.3517,
          -8025.9790,   8744.3037]], dtype=torch.float64)
	q_value: tensor([[-21.4737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3899142877581341, distance: 0.8938242237333714 entropy 10.970367621822788
epoch: 44, step: 103
	action: tensor([[ 1326.2391,  4032.6370,  8946.4579, -5324.6495, -2104.0302,  9741.8713,
         15395.9127]], dtype=torch.float64)
	q_value: tensor([[-31.3865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.973272911227076
epoch: 44, step: 104
	action: tensor([[-3957.9732, -4285.7588, -8991.8662, -4449.5690,  5898.5511,  8292.0928,
         -4818.9132]], dtype=torch.float64)
	q_value: tensor([[-26.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20352953238467508, distance: 1.25540851222061 entropy 10.451788480589126
epoch: 44, step: 105
	action: tensor([[-30075.4297,  -6335.5022,  -6551.0624,  17444.8033,   4366.7623,
         -24017.5138,  -1485.9986]], dtype=torch.float64)
	q_value: tensor([[-24.1834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9896429882079649, distance: 1.614151414886649 entropy 10.801897739492835
epoch: 44, step: 106
	action: tensor([[ 2197.1808, -3031.6126,  2933.2266, -2515.0758,  8166.4806,  4003.9139,
         10582.9749]], dtype=torch.float64)
	q_value: tensor([[-18.8943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21706302267831146, distance: 1.262447210701917 entropy 10.755669979379887
epoch: 44, step: 107
	action: tensor([[ -2570.2484,  -7942.0638,  12263.7392, -18237.2562, -10061.1478,
          -4333.1938,   3224.4677]], dtype=torch.float64)
	q_value: tensor([[-22.1066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27043176232173205, distance: 0.9774389752254815 entropy 10.795398477587112
epoch: 44, step: 108
	action: tensor([[-19897.2634, -18195.2063,  -4377.4339,  30468.8358,  13183.0990,
           2230.5560,   2795.8858]], dtype=torch.float64)
	q_value: tensor([[-24.0130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.301310670488514, distance: 1.3054107619148798 entropy 10.896560973523574
epoch: 44, step: 109
	action: tensor([[  5087.2517, -13460.0669,  22064.3714,  -1908.3349,   2297.3510,
         -16155.2069,  20625.8403]], dtype=torch.float64)
	q_value: tensor([[-18.1502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40610043615307156, distance: 0.8818875193797286 entropy 10.646524529561788
epoch: 44, step: 110
	action: tensor([[-29236.1803,   -633.2986,   -129.4991,  22453.6175,   -492.4791,
          28760.4242,  -3007.9619]], dtype=torch.float64)
	q_value: tensor([[-22.8895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5550626177298017, distance: 1.427021886426898 entropy 10.854103584108937
epoch: 44, step: 111
	action: tensor([[-10703.8019,  -3631.6971,  20330.5926, -15311.1023, -12133.0600,
         -11815.5390, -26582.6168]], dtype=torch.float64)
	q_value: tensor([[-26.0917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07441373373470173, distance: 1.1009437853037898 entropy 11.21216797282681
epoch: 44, step: 112
	action: tensor([[-14037.5910,  -8891.9991,  37435.8565,  11310.3250,    584.1315,
           9521.0323,  11583.9531]], dtype=torch.float64)
	q_value: tensor([[-26.0332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8431887294838327, distance: 1.5536085275641556 entropy 11.036497183774454
epoch: 44, step: 113
	action: tensor([[16261.0221,  3748.7420, -3049.6929,  1962.1248, -4492.1071, 18489.9625,
         14964.5070]], dtype=torch.float64)
	q_value: tensor([[-17.7806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42591598401733677, distance: 0.8670505555392684 entropy 10.687125303077861
epoch: 44, step: 114
	action: tensor([[-20686.2871,  11891.0725,   8281.6087,  -6456.2767, -20560.6262,
            562.8431,  -5860.8172]], dtype=torch.float64)
	q_value: tensor([[-21.5508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15462680663925643, distance: 1.2296386667201193 entropy 10.790485970179015
epoch: 44, step: 115
	action: tensor([[ -7162.3083, -13842.0440,    253.3027,  28335.7862,  -4191.1037,
           3623.2397,  11909.5888]], dtype=torch.float64)
	q_value: tensor([[-27.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09590287828684851, distance: 1.0880885408712393 entropy 10.843281897876592
epoch: 44, step: 116
	action: tensor([[ -3353.9410, -37605.0700, -24903.1919,   7520.4253,  14754.1178,
           6461.3218,   3533.1434]], dtype=torch.float64)
	q_value: tensor([[-23.5649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29829300731044683, distance: 1.3038962978920665 entropy 11.044411338340707
epoch: 44, step: 117
	action: tensor([[-21411.4062, -20135.3064,   7780.9197,   7560.7364, -14221.2979,
          17891.1985,  -2576.0938]], dtype=torch.float64)
	q_value: tensor([[-24.1258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25260781470048466, distance: 1.2807496671562701 entropy 11.12842501153007
epoch: 44, step: 118
	action: tensor([[  5624.6672, -19370.6040, -18867.3057,  12382.7057,   2888.3294,
           3892.3064,  21077.9530]], dtype=torch.float64)
	q_value: tensor([[-24.5742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5667534717070415, distance: 0.7532241550760412 entropy 11.069686540893985
epoch: 44, step: 119
	action: tensor([[ 8966.8493, -5133.4012,  6656.1546, 12025.8096,  8396.6075,  6860.7406,
          2007.8663]], dtype=torch.float64)
	q_value: tensor([[-24.1624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32377522248766977, distance: 1.3166302146714686 entropy 10.87097971463187
epoch: 44, step: 120
	action: tensor([[ -8441.8504, -36360.6665,  -7466.2368,  36017.5084,  -6781.5380,
         -18004.0532,  46589.1484]], dtype=torch.float64)
	q_value: tensor([[-30.3532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24624383347524015, distance: 1.2774920450726976 entropy 11.031050566603616
epoch: 44, step: 121
	action: tensor([[ 21008.0049,  -2262.4107,  13788.0749,   4212.2114,  -1114.7467,
            931.5127, -12081.6638]], dtype=torch.float64)
	q_value: tensor([[-21.6827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4514194882184992, distance: 1.378647279936679 entropy 11.051763298894608
epoch: 44, step: 122
	action: tensor([[ 12645.6892, -24083.1520,  11164.1172,   6420.2563,   2924.9910,
           5135.0979,  24423.9892]], dtype=torch.float64)
	q_value: tensor([[-29.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06166661489497771, distance: 1.1084989280733955 entropy 11.00896085877952
epoch: 44, step: 123
	action: tensor([[-3459.1754, -8766.5750, 19789.4844,  6116.9247, 12147.3854, 14626.2144,
          8148.5698]], dtype=torch.float64)
	q_value: tensor([[-25.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5066284169571702, distance: 1.4046229839597997 entropy 11.073458954593551
epoch: 44, step: 124
	action: tensor([[-20147.1374,   2709.2926,  -7028.3037,  27343.9095,  18616.7069,
          13294.7461,  14385.3286]], dtype=torch.float64)
	q_value: tensor([[-22.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33637449051867396, distance: 0.9322195986717101 entropy 11.026516955028852
epoch: 44, step: 125
	action: tensor([[-11005.4012,  -8198.9190, -12026.8042,   1054.7295,   3213.0262,
           8556.5526,  -3089.6587]], dtype=torch.float64)
	q_value: tensor([[-25.0108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.774088896527936, distance: 1.5242085103575518 entropy 11.020587186522048
epoch: 44, step: 126
	action: tensor([[40311.0355,  9583.7915, -3390.2904,  9385.5528, -8392.4435,  1691.3287,
         -1237.9628]], dtype=torch.float64)
	q_value: tensor([[-22.8309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.51147012630974, distance: 0.7998384104085203 entropy 11.04231623328567
epoch: 44, step: 127
	action: tensor([[-10684.3274,  -8536.8678, -15048.6783,  -2150.6321,   6399.2102,
           6019.4629,  -4574.5670]], dtype=torch.float64)
	q_value: tensor([[-23.4661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.9107930292622
LOSS epoch 44 actor 245.15574225796388 critic 381.0916340842043
epoch: 45, step: 0
	action: tensor([[ 2524.1473, -1307.4500,  3131.1048,  8525.3191, -4359.5882,  4111.8035,
         -1975.3641]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3286423995242368, distance: 0.9376346501220609 entropy 10.499788867035775
epoch: 45, step: 1
	action: tensor([[  146.6244, -2339.5448, -8529.3319,  7211.3945, -3455.6811, 10351.6865,
           694.0147]], dtype=torch.float64)
	q_value: tensor([[-22.0017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4498207645108484, distance: 0.8488066844389944 entropy 10.850452512067802
epoch: 45, step: 2
	action: tensor([[ -2050.2091, -24234.9509,   6126.8352,  19943.0764, -12587.2040,
           5843.0383,  45641.6999]], dtype=torch.float64)
	q_value: tensor([[-25.5044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12777180737932525, distance: 1.0687392350574894 entropy 11.162397076232423
epoch: 45, step: 3
	action: tensor([[ -7568.9415, -11483.9074,  -5644.9112,  16599.1239,  16435.7311,
          19766.3143,  13672.8886]], dtype=torch.float64)
	q_value: tensor([[-23.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08962831774696522, distance: 1.0918577558267455 entropy 11.028677898343023
epoch: 45, step: 4
	action: tensor([[-32913.3050, -25198.8427,    -34.8010, -14822.2427,   6641.3794,
          -9906.8446,  -4190.8900]], dtype=torch.float64)
	q_value: tensor([[-24.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5641903985002625, distance: 1.4312038674636216 entropy 11.074293790344512
epoch: 45, step: 5
	action: tensor([[  3284.4804,   4949.3619,  -4162.6729, -17200.4154,   9048.8031,
          33673.9887,  11805.8077]], dtype=torch.float64)
	q_value: tensor([[-20.8673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2738502033450432, distance: 0.9751463585061405 entropy 10.903539287556393
epoch: 45, step: 6
	action: tensor([[-10859.0761,  23764.3846,  -7964.4647,  19844.0225,  -3162.3751,
         -15877.5759,   8348.7355]], dtype=torch.float64)
	q_value: tensor([[-26.6454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08984521122739386, distance: 1.0917276820665844 entropy 10.876022621779414
epoch: 45, step: 7
	action: tensor([[ -4713.3709, -27271.2265, -28707.9837,  14839.0043,   7708.0727,
          21309.5042,  16680.3077]], dtype=torch.float64)
	q_value: tensor([[-23.5351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002091834765060252, distance: 1.145540518276876 entropy 10.852890601503303
epoch: 45, step: 8
	action: tensor([[ -3333.0733, -32856.6429, -12126.8060,  17373.6002,   6160.2004,
           8391.4684, -17097.4015]], dtype=torch.float64)
	q_value: tensor([[-21.8986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47595978817345186, distance: 1.390253368502141 entropy 10.98843934355793
epoch: 45, step: 9
	action: tensor([[ -6997.2275, -12589.0016,  26257.8983,  -3067.5361,  13952.4730,
         -13305.5322,   7174.6126]], dtype=torch.float64)
	q_value: tensor([[-22.0705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5480818410034223, distance: 1.4238152869222602 entropy 11.043461521745416
epoch: 45, step: 10
	action: tensor([[  -496.6227,   4367.1189,   9914.3503, -10389.6367,   2065.6555,
           1210.5778, -21421.5632]], dtype=torch.float64)
	q_value: tensor([[-22.1127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1110526009109647, distance: 1.2062129963073305 entropy 10.927205426233655
epoch: 45, step: 11
	action: tensor([[  -960.3855, -19626.1067, -19746.3866, -11665.8694,   1033.5742,
          -3493.5618,  -6296.6570]], dtype=torch.float64)
	q_value: tensor([[-34.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.538726973779132, distance: 1.4195067982445735 entropy 10.919783629621529
epoch: 45, step: 12
	action: tensor([[ 3004.4600, 12037.8166,  4396.2672, -2736.5405, 10068.6571, -3912.5690,
          2105.0960]], dtype=torch.float64)
	q_value: tensor([[-25.5244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.031136906926843
epoch: 45, step: 13
	action: tensor([[11844.7053, -5626.7544, -1280.7943, -6340.1126, -3608.7742, -4082.4310,
          5228.8618]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058565798927756596, distance: 1.1103289898766466 entropy 10.499788867035775
epoch: 45, step: 14
	action: tensor([[  7767.0970, -18187.2256,  -2776.0471,   3556.1755,   5578.0276,
          27583.7777,   1685.4177]], dtype=torch.float64)
	q_value: tensor([[-21.9760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2679232035900838, distance: 0.9791179538653886 entropy 10.877567197389324
epoch: 45, step: 15
	action: tensor([[-4704.1694, -2365.5831, -5606.0627,  5137.1738,  1031.5577, -1105.8623,
         -5715.8861]], dtype=torch.float64)
	q_value: tensor([[-22.7818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6573251152413602, distance: 1.4731960615554123 entropy 10.77411752469745
epoch: 45, step: 16
	action: tensor([[ -3162.8669,  -5870.4388,  -8877.1754,   3519.8249,   6410.0444,
          -3539.4330, -13314.1610]], dtype=torch.float64)
	q_value: tensor([[-19.6578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.69222670405404, distance: 1.488627252202976 entropy 10.900499494121542
epoch: 45, step: 17
	action: tensor([[-9533.0933, -8368.1132, 13040.7061, 11578.6207, -3535.6351,  8773.7614,
         -3131.7536]], dtype=torch.float64)
	q_value: tensor([[-19.5141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24403392438621352, distance: 1.2763588824224672 entropy 10.868245060601478
epoch: 45, step: 18
	action: tensor([[  9788.1853, -20353.5340, -39145.2396, -14051.6953,   3651.1264,
           1168.8204,   4602.4055]], dtype=torch.float64)
	q_value: tensor([[-24.3489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051718297421947335, distance: 1.1143596509919267 entropy 11.135384645821693
epoch: 45, step: 19
	action: tensor([[-11051.4126,   5987.8987,   3672.2754,  23238.0527, -26780.5102,
         -13700.5715,  17872.3227]], dtype=torch.float64)
	q_value: tensor([[-21.5293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08607182688685877, distance: 1.1925757313653522 entropy 10.895087110475663
epoch: 45, step: 20
	action: tensor([[-31682.3507,  -4557.4247,   6817.6798,   2674.6172,  20163.7492,
           7738.0205,  -6643.8047]], dtype=torch.float64)
	q_value: tensor([[-26.7213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19613587437734026, distance: 1.0260015505299285 entropy 11.110174099700867
epoch: 45, step: 21
	action: tensor([[ -8254.4904,   3480.0221, -11247.1252,   8821.9011, -19348.2549,
         -15362.3271,    816.0802]], dtype=torch.float64)
	q_value: tensor([[-22.1922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17389715614099455, distance: 1.0400967800505987 entropy 11.031794130425562
epoch: 45, step: 22
	action: tensor([[    86.7312, -23378.5819, -22857.5711,  19920.2653,  22075.8163,
         -20522.5068,  18733.0574]], dtype=torch.float64)
	q_value: tensor([[-25.9339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27765501660634795, distance: 0.9725882618535809 entropy 11.108504105778382
epoch: 45, step: 23
	action: tensor([[-13764.8691, -23311.2551,   1025.0055, -20167.2586, -16360.4885,
          11864.8431,   3463.6560]], dtype=torch.float64)
	q_value: tensor([[-25.1518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5046719985686376, distance: 1.403710707617859 entropy 11.12587751587636
epoch: 45, step: 24
	action: tensor([[ -8384.1554,    924.9799,   6296.9667,   5134.3448,   4270.3030,
          13382.5964, -10970.1477]], dtype=torch.float64)
	q_value: tensor([[-20.0401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5592640271490437, distance: 0.7597066759449103 entropy 10.910942939491576
epoch: 45, step: 25
	action: tensor([[-19109.6948, -14553.6122,    197.9493,  -8020.3436,   8551.1321,
         -16997.8267,  18459.9341]], dtype=torch.float64)
	q_value: tensor([[-24.7196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47190658378939454, distance: 1.388343135229623 entropy 11.057193258065256
epoch: 45, step: 26
	action: tensor([[11038.6132, -9580.1910,   378.1069,  6074.3566, -6492.6095, 10785.3570,
          7661.4013]], dtype=torch.float64)
	q_value: tensor([[-19.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4829819587280163, distance: 0.8228289101776929 entropy 10.827864462103857
epoch: 45, step: 27
	action: tensor([[   575.1870, -19527.1011,     53.0744,   8510.5779,   9065.8623,
          -4292.3571,    813.9519]], dtype=torch.float64)
	q_value: tensor([[-22.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13694369438605447, distance: 1.2201863770919108 entropy 10.911806268912681
epoch: 45, step: 28
	action: tensor([[ -4368.2835, -16002.3766, -45860.0809,   2223.9747,  -6200.5401,
         -19037.4553,  13673.6088]], dtype=torch.float64)
	q_value: tensor([[-30.4746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04246794247867114, distance: 1.119781671333027 entropy 11.114743179794322
epoch: 45, step: 29
	action: tensor([[  1530.5721,  -6136.6652,   4758.1739,  -1205.3745, -15510.8076,
         -10721.3855,  26207.5528]], dtype=torch.float64)
	q_value: tensor([[-22.8308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06809389389429887, distance: 1.1046959757793755 entropy 10.97089069965993
epoch: 45, step: 30
	action: tensor([[18389.9559,  1883.5133, -8123.3501, 16003.0816, -9897.5519,  1459.5174,
         15173.2788]], dtype=torch.float64)
	q_value: tensor([[-24.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3853026838113155, distance: 0.8971960476542554 entropy 10.833545424876196
epoch: 45, step: 31
	action: tensor([[-11135.1653,  -5068.9992,  -9759.6984,  14989.0855,   2588.7814,
           5656.5943, -11068.8663]], dtype=torch.float64)
	q_value: tensor([[-22.7143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30598597292673646, distance: 0.953324638473107 entropy 10.78878966190382
epoch: 45, step: 32
	action: tensor([[ -2801.0033, -16521.8252, -16244.7938,  30349.6508,  -9349.3173,
          25168.1391,  -5397.2993]], dtype=torch.float64)
	q_value: tensor([[-20.3309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09651366191056032, distance: 1.0877209371436727 entropy 10.886908213403443
epoch: 45, step: 33
	action: tensor([[   430.5835, -30999.6730,  -5358.1271, -30476.9951,  21663.7007,
          32962.1501,   7469.8425]], dtype=torch.float64)
	q_value: tensor([[-22.4357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09452803229050744, distance: 1.0889155459859532 entropy 11.059969654649155
epoch: 45, step: 34
	action: tensor([[ -7122.2967,  -5442.7806, -16825.4559,  11482.1143,  -8639.0897,
           7589.2310, -14095.2031]], dtype=torch.float64)
	q_value: tensor([[-21.0073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1879112119832551, distance: 1.0312369139091042 entropy 10.710205953195343
epoch: 45, step: 35
	action: tensor([[  2355.1293, -18579.7799, -12656.9844,  -4523.2976,   2284.6348,
          19702.9149, -10235.9334]], dtype=torch.float64)
	q_value: tensor([[-21.2396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5330149151902097, distance: 0.7820026020227789 entropy 10.942081968792246
epoch: 45, step: 36
	action: tensor([[ -7635.2094, -13295.1801, -13834.6056,  -3759.3428,  -9390.9228,
           8105.7647,   7829.0263]], dtype=torch.float64)
	q_value: tensor([[-23.4952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2035023199839433, distance: 1.021289697419547 entropy 10.884023219469285
epoch: 45, step: 37
	action: tensor([[  4327.1495,  19048.2284,  -5440.7740,   4735.2044,  11673.5447,
         -13663.9483, -10649.4032]], dtype=torch.float64)
	q_value: tensor([[-24.8665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.980425833755572
epoch: 45, step: 38
	action: tensor([[-10503.0104,  -1159.0284, -10645.3993,   1544.3849,  -2913.6580,
         -13782.6695, -13479.9281]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08118692624550983, distance: 1.18989074352629 entropy 10.499788867035775
epoch: 45, step: 39
	action: tensor([[  4990.6601, -11334.9671,   4185.4956, -17753.5933,  18417.0740,
          32149.6953,   5440.7207]], dtype=torch.float64)
	q_value: tensor([[-20.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11617521946845888, distance: 1.2089904810356895 entropy 11.0041457349757
epoch: 45, step: 40
	action: tensor([[-1066.6209, -3699.1853, -6948.1545, 10720.3533, -3539.2826,  3647.5067,
          -373.8382]], dtype=torch.float64)
	q_value: tensor([[-17.9858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4137532327282196, distance: 1.3606408289929999 entropy 10.48845430166487
epoch: 45, step: 41
	action: tensor([[-17188.9783, -21006.3373,   -901.2849, -20967.7262, -13247.3834,
           1817.6265,  -4249.5524]], dtype=torch.float64)
	q_value: tensor([[-22.1419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.602833175258843, distance: 1.4487747047112771 entropy 10.978090491925517
epoch: 45, step: 42
	action: tensor([[ -3442.8680, -33237.3283,  -9265.7788, -18112.1978, -22341.5523,
          17629.9357, -22765.0534]], dtype=torch.float64)
	q_value: tensor([[-25.9838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3017293975973636, distance: 1.3056207682271548 entropy 11.0718216995854
epoch: 45, step: 43
	action: tensor([[ 14360.8469,  -5728.7112,  16240.4663,  -6407.1501,   9154.7175,
         -10603.0327,  -3050.2240]], dtype=torch.float64)
	q_value: tensor([[-24.5790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36714036508226267, distance: 0.9103541816516406 entropy 10.940903662982638
epoch: 45, step: 44
	action: tensor([[-13781.3593,    654.2777, -14587.4742,  27586.4515, -29079.0458,
          34068.6790,  14019.9718]], dtype=torch.float64)
	q_value: tensor([[-26.0810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.01435964922723
epoch: 45, step: 45
	action: tensor([[-13332.4782, -25729.1056,  12538.2830,   7243.3968,  -6777.8570,
          -6105.6388,  -3704.4545]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03341738489552537, distance: 1.1250612924438028 entropy 10.499788867035775
epoch: 45, step: 46
	action: tensor([[  1457.4615,  17327.0096,  20519.5565,  13175.2464,   -811.1899,
         -37328.6802,   5087.9750]], dtype=torch.float64)
	q_value: tensor([[-22.1590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.063826548811946
epoch: 45, step: 47
	action: tensor([[-21554.6781,  -2816.0349,  -5865.3109,    269.4525,   -323.2539,
          -9356.5134,  -1592.0179]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02744884669889236, distance: 1.1285295097629873 entropy 10.499788867035775
epoch: 45, step: 48
	action: tensor([[-1203.4195, -8004.0217, 14220.2314,  1762.0979,  3209.0217, 10584.2513,
          -855.4531]], dtype=torch.float64)
	q_value: tensor([[-25.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33688284404579694, distance: 1.3231325950962605 entropy 11.252145489143755
epoch: 45, step: 49
	action: tensor([[ -1447.9367,   8281.0511, -24296.9173, -10145.3353,  -9191.8668,
          -7081.9523,  21275.2800]], dtype=torch.float64)
	q_value: tensor([[-23.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12082950252948965, distance: 1.211508513194759 entropy 11.005648860257404
epoch: 45, step: 50
	action: tensor([[  4943.8233, -13553.9946,   -386.4521, -14073.3153,   9069.7815,
          -2544.8619,  -7487.3331]], dtype=torch.float64)
	q_value: tensor([[-23.4590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19800023267017575, distance: 1.0248110851207908 entropy 10.620296485319823
epoch: 45, step: 51
	action: tensor([[-29376.9660,  -5465.8329, -16084.7644,  24788.4065,   -855.7564,
           2007.4604, -29716.8820]], dtype=torch.float64)
	q_value: tensor([[-26.0450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8696274326186677, distance: 1.5647113366902745 entropy 11.110802098428625
epoch: 45, step: 52
	action: tensor([[-7348.7113, -9754.1889, -3812.2598, 16511.2725,  9052.0279, -3951.5876,
         -4417.7648]], dtype=torch.float64)
	q_value: tensor([[-20.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.304645524470333, distance: 0.9542448404043006 entropy 10.927326981183827
epoch: 45, step: 53
	action: tensor([[  5033.8383,  12165.4257, -22105.9512,   3793.0536,   7414.8923,
          20233.0265, -11265.7205]], dtype=torch.float64)
	q_value: tensor([[-22.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3782992707446976, distance: 0.9022925710765478 entropy 11.073999208734364
epoch: 45, step: 54
	action: tensor([[  5998.1753,  -3507.7838, -13671.0549, -21357.7434,  -3581.4886,
           5432.6995,  21762.8306]], dtype=torch.float64)
	q_value: tensor([[-22.6800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22614495547484292, distance: 1.2671487547635252 entropy 10.915421068543866
epoch: 45, step: 55
	action: tensor([[-2288.9393,  -842.1659, 14793.2563, 23827.6954, 12269.8888, -3137.9744,
         15517.4994]], dtype=torch.float64)
	q_value: tensor([[-22.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37320860627848207, distance: 1.3409881853967385 entropy 10.746551787838442
epoch: 45, step: 56
	action: tensor([[ 11634.4445,  -9337.7111, -25568.7165,    753.9724,  42100.0173,
          21856.3928,  19480.7026]], dtype=torch.float64)
	q_value: tensor([[-23.7981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042923404271849264, distance: 1.1195153207563247 entropy 11.034892103644202
epoch: 45, step: 57
	action: tensor([[-31410.3725,  15311.5537,   4159.6004,  -4230.3476, -23717.4898,
         -30798.8647, -16026.9530]], dtype=torch.float64)
	q_value: tensor([[-25.5833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.145517176487472
epoch: 45, step: 58
	action: tensor([[-11570.3969,   6815.7282,   1476.0640, -10103.7640,  -2663.6305,
          13151.3594,  13282.0253]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.499788867035775
epoch: 45, step: 59
	action: tensor([[-10953.4390,  -3093.7731,   7618.6465,  -4373.8870, -10576.1658,
          -7894.1418,  -5163.0382]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.499788867035775
epoch: 45, step: 60
	action: tensor([[ -8832.8065,  -4526.7454,   1039.6918, -14901.2746,   4163.8637,
           2431.4107,     64.9416]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04450562746256259, distance: 1.1185895558817904 entropy 10.499788867035775
epoch: 45, step: 61
	action: tensor([[ 6315.2656,  7618.8537,  -574.9930, -1020.3866, 12617.9631, 14480.4224,
         21895.4316]], dtype=torch.float64)
	q_value: tensor([[-31.1923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4587547516902811, distance: 0.8418868801454983 entropy 11.15082434103738
epoch: 45, step: 62
	action: tensor([[ -5143.2050,  -2002.8335,   3552.4896,  13372.2726,  15160.0974,
         -34054.7711,   7879.2326]], dtype=torch.float64)
	q_value: tensor([[-27.5661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.923161795487022
epoch: 45, step: 63
	action: tensor([[  3154.6671,  -7252.8840,   2627.1565,   5484.3815,   6930.1571,
         -16120.0906,   4496.0207]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25796787053997317, distance: 0.9857528660194124 entropy 10.499788867035775
epoch: 45, step: 64
	action: tensor([[-1.0879e+04, -2.0294e+04,  5.4262e+02,  2.0036e+01, -2.0499e+04,
          2.5331e+04,  5.4062e+01]], dtype=torch.float64)
	q_value: tensor([[-22.3629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7512393670636646, distance: 1.5143611131568366 entropy 10.889486636964595
epoch: 45, step: 65
	action: tensor([[ 22169.5933,  -2251.0765,   8572.3064,  25950.9557, -15719.2282,
          -5642.1223, -13916.5727]], dtype=torch.float64)
	q_value: tensor([[-25.7773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4608737070655351, distance: 0.8402372858461667 entropy 11.193700460802392
epoch: 45, step: 66
	action: tensor([[  8400.0521,  -5091.2993,   9402.0196, -12064.3093, -12689.7874,
          -2828.0375,  -2756.3017]], dtype=torch.float64)
	q_value: tensor([[-25.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14589634342073565, distance: 1.224981020902345 entropy 10.929631923692464
epoch: 45, step: 67
	action: tensor([[ -3118.2999,  -1625.5855,  33580.1078,  21940.6578, -11500.4300,
           4662.1252,   7750.0402]], dtype=torch.float64)
	q_value: tensor([[-27.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06595721303451851, distance: 1.1059616780347707 entropy 10.963728190435177
epoch: 45, step: 68
	action: tensor([[ -8280.3638, -24099.6208, -35445.3554,  26567.9018, -16233.0246,
         -11689.7563, -42159.5533]], dtype=torch.float64)
	q_value: tensor([[-26.6483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21085118957022198, distance: 1.0165673251866931 entropy 11.214560711212007
epoch: 45, step: 69
	action: tensor([[ -8769.7262,  -7068.9103, -29414.9884,  16075.9725,   9163.2070,
         -14690.9277, -22123.6816]], dtype=torch.float64)
	q_value: tensor([[-22.3571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06556909803012001, distance: 1.1061914296837978 entropy 11.12502153559378
epoch: 45, step: 70
	action: tensor([[   577.8795, -14602.5356, -14958.3221,   4199.2960, -12295.3489,
         -15435.5342, -18349.5318]], dtype=torch.float64)
	q_value: tensor([[-24.9278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30741317709669125, distance: 0.9523439024492717 entropy 11.0822726934686
epoch: 45, step: 71
	action: tensor([[-24301.5396,  15051.5142, -11448.4205,   3526.5671, -12496.7763,
            538.2809,  23262.6878]], dtype=torch.float64)
	q_value: tensor([[-25.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.970111664460708
epoch: 45, step: 72
	action: tensor([[ 1794.8758, -4254.6378, 10101.0371, -6554.3093,  4725.7775,   110.3522,
          4801.8594]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.285017071076236, distance: 0.9676193165490108 entropy 10.499788867035775
epoch: 45, step: 73
	action: tensor([[-17889.3830, -29105.9010,   -874.6615, -16279.1448, -10276.9127,
           6974.2719, -19598.3906]], dtype=torch.float64)
	q_value: tensor([[-22.2569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21527068507730374, distance: 1.0137167711998984 entropy 10.875824119381537
epoch: 45, step: 74
	action: tensor([[ -3413.2452,  -8487.3840,  -8783.8838,  28256.6740,   -589.4317,
          33622.2324, -12143.0347]], dtype=torch.float64)
	q_value: tensor([[-24.9018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35218766786835665, distance: 1.3306847465958713 entropy 10.963533448632559
epoch: 45, step: 75
	action: tensor([[  -350.7043,  13117.2121,  10100.2381, -11257.9403, -11043.0209,
          -9281.7649, -10175.8751]], dtype=torch.float64)
	q_value: tensor([[-18.4492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5763447965769986, distance: 0.7448399558771446 entropy 10.738419433243246
epoch: 45, step: 76
	action: tensor([[-11249.4824,  14898.9880,   6095.2190,  14246.0206, -24021.4562,
          38889.5657,  39777.2319]], dtype=torch.float64)
	q_value: tensor([[-37.1349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14044935798973235, distance: 1.2220660966842798 entropy 11.184843470564884
epoch: 45, step: 77
	action: tensor([[  -380.0874,  -6441.9524, -10110.7325,  15136.8275,  12434.0505,
          20079.4905,  15102.2592]], dtype=torch.float64)
	q_value: tensor([[-27.9724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09617479969240827, distance: 1.0879248987088668 entropy 10.807884715831117
epoch: 45, step: 78
	action: tensor([[  5350.4011, -29999.1164,   -237.4489,  17369.5615,  14868.7521,
          -2651.3946,   6551.4108]], dtype=torch.float64)
	q_value: tensor([[-22.8083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.097116754251259
epoch: 45, step: 79
	action: tensor([[-6162.6079, -6665.4057,   764.7654, 12608.5164,  -265.8325, -1159.5729,
         -7300.3634]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41905621808392457, distance: 1.3631903208674983 entropy 10.499788867035775
epoch: 45, step: 80
	action: tensor([[  1853.0907, -19226.5310,  -8772.7257,  10600.2573,  16446.3542,
           5561.4824,  42716.8734]], dtype=torch.float64)
	q_value: tensor([[-21.5494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03575191985135573, distance: 1.1237018210961849 entropy 11.092875395296625
epoch: 45, step: 81
	action: tensor([[  8662.8437, -14474.4918,  17713.1113, -11708.0368,  -3811.3534,
          -3062.3483,  10791.8984]], dtype=torch.float64)
	q_value: tensor([[-24.6873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052888571176949783, distance: 1.1136718234997776 entropy 11.15671801668421
epoch: 45, step: 82
	action: tensor([[-11332.9723, -36057.1224,  -4768.5336,  18469.9612,   -875.7488,
          29675.7106,   4786.1230]], dtype=torch.float64)
	q_value: tensor([[-26.0391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.350192562883862, distance: 1.3297026951235225 entropy 10.93497299586531
epoch: 45, step: 83
	action: tensor([[-11441.7789,   1071.8286,   9394.8943, -13527.5398, -11386.2776,
           -192.2905, -16319.8978]], dtype=torch.float64)
	q_value: tensor([[-22.6442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35556371134637943, distance: 1.3323448892878886 entropy 11.009475834036357
epoch: 45, step: 84
	action: tensor([[-20043.6650, -13376.3311,  -6170.0039, -18364.8886,  -3877.4534,
         -16605.6136,   6834.1941]], dtype=torch.float64)
	q_value: tensor([[-24.3667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18507302239010137, distance: 1.0330373901023593 entropy 10.777632558994386
epoch: 45, step: 85
	action: tensor([[ -2574.9672, -11652.7422,  -9764.3851,  17685.8969,  10787.5833,
          12626.6231,  24611.9423]], dtype=torch.float64)
	q_value: tensor([[-25.1503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6739400578234469, distance: 1.4805621560942428 entropy 10.926245251931181
epoch: 45, step: 86
	action: tensor([[  2154.1597,  -2956.9508, -11015.1228,  11639.1817,   1179.2825,
          -1873.3146,  -7975.7742]], dtype=torch.float64)
	q_value: tensor([[-18.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6989375195931641, distance: 0.6278920885379291 entropy 10.718912879934825
epoch: 45, step: 87
	action: tensor([[  -494.2508,  -2878.9884,    601.8668,   8056.4844, -20307.7553,
          23690.9078,  -2064.5397]], dtype=torch.float64)
	q_value: tensor([[-20.2081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1526523112792676, distance: 1.0533859412595858 entropy 10.890967143828103
epoch: 45, step: 88
	action: tensor([[ -6338.9260, -10697.6203, -21147.0650,    -92.1927, -25455.9446,
           -927.2302, -19021.7574]], dtype=torch.float64)
	q_value: tensor([[-22.4851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08399624993642574, distance: 1.1914356288466017 entropy 11.092881964279917
epoch: 45, step: 89
	action: tensor([[ 32731.7865, -36526.7277, -13139.2775,  33721.5140,  -3869.4940,
         -20171.1083,   1373.6705]], dtype=torch.float64)
	q_value: tensor([[-25.7144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11009802911807554, distance: 1.205694720070616 entropy 11.302236158500879
epoch: 45, step: 90
	action: tensor([[ -6359.4277, -24399.7943,  39748.1702,  -6477.0177,  -1965.2977,
           9957.3692,  11111.5516]], dtype=torch.float64)
	q_value: tensor([[-22.8080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2542655211071254, distance: 1.281596861679534 entropy 10.965405278094002
epoch: 45, step: 91
	action: tensor([[  4565.0824,  -3174.5854,  10335.6920,  -7321.5214,  -6087.4990,
         -32068.0391, -18188.3478]], dtype=torch.float64)
	q_value: tensor([[-20.4615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24264684682067272, distance: 0.9958774812355782 entropy 10.952517472062317
epoch: 45, step: 92
	action: tensor([[-10430.6484, -26412.8657,   7270.0242, -16833.2379, -11193.7063,
          -1584.0673,  -1288.4547]], dtype=torch.float64)
	q_value: tensor([[-21.3922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5634717201365584, distance: 1.4308750413091205 entropy 10.793378538292965
epoch: 45, step: 93
	action: tensor([[  1496.8921,   1533.9730, -31610.4875,  33504.4254,  -6399.7489,
          19650.6427, -34201.4730]], dtype=torch.float64)
	q_value: tensor([[-24.7463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.98735559926162
epoch: 45, step: 94
	action: tensor([[ 8693.7859, 14073.9238,  7897.3494, 11766.1186,  7684.2839, -7153.9918,
          4152.5392]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.499788867035775
epoch: 45, step: 95
	action: tensor([[ 4038.4337,    85.5135,  5230.5784, 14527.3776,  9403.4412,  5470.9549,
         12935.4748]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.499788867035775
epoch: 45, step: 96
	action: tensor([[ 8204.5107,  2802.7993, -8537.5610,  6081.0717,  9655.2198, -2893.9662,
         10282.4116]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.499788867035775
epoch: 45, step: 97
	action: tensor([[ 3807.4907, -3856.2252, -8494.6369,  2060.1516,  -694.7059,  9655.9916,
         20480.4377]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0663795505062117, distance: 1.105711613542446 entropy 10.499788867035775
epoch: 45, step: 98
	action: tensor([[  4124.9256, -17644.0952,  -6913.8533,     91.9062,   4137.9690,
          14258.5494,   1065.9154]], dtype=torch.float64)
	q_value: tensor([[-22.2170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0053230157926015265, distance: 1.1412945088562672 entropy 10.902097213981154
epoch: 45, step: 99
	action: tensor([[  2051.6253,  10297.8664,  -7030.0454,  -5535.7330,  20030.4876,
           7200.0933, -14735.1520]], dtype=torch.float64)
	q_value: tensor([[-26.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7381436301998527, distance: 0.5855827362675765 entropy 11.165052003345027
epoch: 45, step: 100
	action: tensor([[-17663.0898, -16320.7976, -10489.8936,  -3353.1523, -18716.3402,
          -4158.6003,   -204.5709]], dtype=torch.float64)
	q_value: tensor([[-23.0539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32635381539001385, distance: 0.9392314370998955 entropy 10.580374263127041
epoch: 45, step: 101
	action: tensor([[-28378.4462, -10463.3277,  -9754.3001,  27926.5131,   4394.8674,
          40448.8800,   6760.8882]], dtype=torch.float64)
	q_value: tensor([[-27.6118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0104284190435147, distance: 1.6225608789700239 entropy 11.044125977140272
epoch: 45, step: 102
	action: tensor([[-16667.9279, -18919.9649,   6551.3057,  26118.9355, -11536.9220,
          15942.3350,  -1508.4034]], dtype=torch.float64)
	q_value: tensor([[-25.0219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18077776179556992, distance: 1.0357562431400158 entropy 11.14887641049543
epoch: 45, step: 103
	action: tensor([[ -5461.6963, -20696.3377,   4295.8682,   7853.4146, -18151.3994,
          -9006.8690, -12107.6890]], dtype=torch.float64)
	q_value: tensor([[-22.4010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49789764677742276, distance: 1.4005472416069469 entropy 11.053856250505145
epoch: 45, step: 104
	action: tensor([[   238.8764,  -3877.1156, -19910.9761,    668.9213,   7674.9764,
          17026.6110,  22738.2938]], dtype=torch.float64)
	q_value: tensor([[-19.8620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2540071575303525, distance: 0.988380170129044 entropy 10.9714669869817
epoch: 45, step: 105
	action: tensor([[-12902.3604,  11277.3142,  -1052.3657,  -3201.8806,  -2945.4086,
          23642.0252,     56.4446]], dtype=torch.float64)
	q_value: tensor([[-23.3980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.033878705131267
epoch: 45, step: 106
	action: tensor([[13955.0232, -5718.8259,  5644.8880, 13812.0220, 10204.0665, -3248.9582,
         15960.5126]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3730579069154838, distance: 1.3409146016713431 entropy 10.499788867035775
epoch: 45, step: 107
	action: tensor([[ -3503.4521, -26366.6141, -13873.0731,  15636.7305,  16440.9868,
          22083.6061,  11406.9820]], dtype=torch.float64)
	q_value: tensor([[-23.3583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13358795966137793, distance: 1.0651700159497115 entropy 11.053164695363336
epoch: 45, step: 108
	action: tensor([[ 20596.3484, -35553.6397,  -9267.0919,  -4670.0428, -14026.7212,
         -31513.8356,  28518.7737]], dtype=torch.float64)
	q_value: tensor([[-24.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35911643417755146, distance: 1.3340896817729702 entropy 11.126456215474326
epoch: 45, step: 109
	action: tensor([[-17095.2249, -10713.0640,  12404.2392,   4981.3594,  16578.5243,
           5797.3734,  -7882.4625]], dtype=torch.float64)
	q_value: tensor([[-20.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46136804662373754, distance: 1.383364086638886 entropy 10.789366905179634
epoch: 45, step: 110
	action: tensor([[   953.4243,   -952.9944,  -7256.0084,    892.4702, -17149.3354,
         -13416.8106,  21719.8798]], dtype=torch.float64)
	q_value: tensor([[-25.2973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26022905597496593, distance: 0.9842497837369459 entropy 11.183299237181242
epoch: 45, step: 111
	action: tensor([[ -7889.3939,  11562.3051,  -5550.8894,  -2982.3752,  -1803.3230,
         -11952.9596,   3287.6201]], dtype=torch.float64)
	q_value: tensor([[-23.2328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4898922938817968, distance: 0.8173115484747399 entropy 11.005839335620296
epoch: 45, step: 112
	action: tensor([[-5033.7566,  4571.2426, 31815.2740,  7759.7020,  6474.6647,  4837.1908,
          5269.4959]], dtype=torch.float64)
	q_value: tensor([[-24.8950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7605132744794756, distance: 1.518365563116741 entropy 10.878582417025246
epoch: 45, step: 113
	action: tensor([[-22850.1733,  -5846.9550, -24344.3850,   7633.7577, -11766.8819,
          34695.9439,  26631.9041]], dtype=torch.float64)
	q_value: tensor([[-28.7014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27096914972123676, distance: 1.2901024434555717 entropy 11.143856826483828
epoch: 45, step: 114
	action: tensor([[-10261.9449, -15567.1315,  -9907.1206,  -3690.4248, -29411.6014,
         -25961.1799,  -1906.3581]], dtype=torch.float64)
	q_value: tensor([[-24.8504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5630838971687189, distance: 1.4306975642831696 entropy 11.158577425015972
epoch: 45, step: 115
	action: tensor([[ -3909.3765,   1105.8274, -20721.3664,  22974.9138,  -3452.1337,
           2488.9378,  11143.4586]], dtype=torch.float64)
	q_value: tensor([[-21.7522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36486010917686207, distance: 0.9119927551051187 entropy 10.902179587274087
epoch: 45, step: 116
	action: tensor([[  7514.9651,  -9833.0180,  -7385.6824, -11747.5891,  -8900.0383,
           8401.8234, -25748.6293]], dtype=torch.float64)
	q_value: tensor([[-23.5644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34518737994877946, distance: 0.926009012883722 entropy 11.015389708156002
epoch: 45, step: 117
	action: tensor([[-13746.9679,  -8722.7732,   2820.5972,  33358.8907,  29656.0129,
          -8368.3813,   1796.1626]], dtype=torch.float64)
	q_value: tensor([[-23.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.032316181877417, distance: 1.631369471242946 entropy 10.933185932827131
epoch: 45, step: 118
	action: tensor([[-7210.4334,   560.5799,  1971.3479, -6318.4648, 15967.1421,  -671.0993,
         -5367.9498]], dtype=torch.float64)
	q_value: tensor([[-18.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1319352422329061, distance: 1.2174958347718892 entropy 10.763639953332495
epoch: 45, step: 119
	action: tensor([[ 26159.9617, -16482.3903,  -5396.5522,   2407.9990, -12675.3209,
          -8315.8642,   6121.4797]], dtype=torch.float64)
	q_value: tensor([[-32.1978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6116164891887019, distance: 0.7131601783114768 entropy 10.965568609399588
epoch: 45, step: 120
	action: tensor([[-44023.8513,  -7342.8803,   3448.1279,  -8159.4689,   6256.2894,
         -19439.7090,  -4084.7162]], dtype=torch.float64)
	q_value: tensor([[-23.0506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37224395753445805, distance: 1.3405170953449395 entropy 11.044385390381468
epoch: 45, step: 121
	action: tensor([[27959.3798, 19816.4002, 15477.1063, 28345.7265,  3310.8068, -7555.2172,
         16439.2669]], dtype=torch.float64)
	q_value: tensor([[-25.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.021714206094178
epoch: 45, step: 122
	action: tensor([[-6547.2600,  8368.5804, -8880.4001,  4199.1862, -3488.0399,  1726.1523,
          4638.7317]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18832222299070844, distance: 1.2474518825996996 entropy 10.499788867035775
epoch: 45, step: 123
	action: tensor([[-9524.8612, -6874.0556, -2301.5861, 28224.2766, -3767.8985,  1797.1894,
          3470.3377]], dtype=torch.float64)
	q_value: tensor([[-25.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6400638349361736, distance: 1.465504206371787 entropy 10.840584843250497
epoch: 45, step: 124
	action: tensor([[ 10891.2499, -11145.7209,  -8577.1230,  18900.9250,   7761.0389,
         -17054.7752,  13102.3253]], dtype=torch.float64)
	q_value: tensor([[-23.4684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17368315045901683, distance: 1.0402314922167297 entropy 11.10278328372809
epoch: 45, step: 125
	action: tensor([[ -9119.4941, -19487.2713,  -4804.2000,  -3777.5724, -18528.0032,
          -5036.4800,  26648.0929]], dtype=torch.float64)
	q_value: tensor([[-22.7157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8084628993009997, distance: 1.5389038816004452 entropy 10.873768932243115
epoch: 45, step: 126
	action: tensor([[  7606.9885,   3563.5778, -10158.5120,   1924.9153, -11117.8645,
          -9257.9968,   9186.9450]], dtype=torch.float64)
	q_value: tensor([[-19.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.784748655899264
epoch: 45, step: 127
	action: tensor([[-5266.2647, -6648.9818, -4731.2380,   369.0245, 14857.8927, 16110.7994,
          5045.7649]], dtype=torch.float64)
	q_value: tensor([[-27.0014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8015510759828002, distance: 1.5359602728795434 entropy 10.499788867035775
LOSS epoch 45 actor 240.99840248322874 critic 348.0167075456226
epoch: 46, step: 0
	action: tensor([[ -9116.1013, -10241.5758, -14396.8271,  -2645.2459,  13210.5817,
          29531.2624,  18515.1350]], dtype=torch.float64)
	q_value: tensor([[-25.5727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2550700963472565, distance: 1.282007849535202 entropy 10.952824698534522
epoch: 46, step: 1
	action: tensor([[  1731.5129,  -4864.2269, -12479.8636,  17945.0894,  -4645.5534,
          24335.1046,   5492.0845]], dtype=torch.float64)
	q_value: tensor([[-31.6770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10730756957702137, distance: 1.204178385831305 entropy 11.144881249656207
epoch: 46, step: 2
	action: tensor([[  -810.0525,  -2862.5785,   9658.3469,   9758.2458, -15756.1344,
          32126.0444,   8162.5705]], dtype=torch.float64)
	q_value: tensor([[-27.3421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12680473822806548, distance: 1.214733547215993 entropy 11.023535337828301
epoch: 46, step: 3
	action: tensor([[   -39.6801,   2426.8475,  -5785.1011,   3585.5634, -19572.0229,
         -14296.2380,  15167.0613]], dtype=torch.float64)
	q_value: tensor([[-25.6158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6424554556199498, distance: 0.6842609738206809 entropy 10.98467596571778
epoch: 46, step: 4
	action: tensor([[-12911.9514,   3705.8412, -23557.9260, -10473.7882,  27188.5884,
         -35566.8983,  12267.8621]], dtype=torch.float64)
	q_value: tensor([[-24.6563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06459771106003975, distance: 1.180726898944436 entropy 10.829753702282721
epoch: 46, step: 5
	action: tensor([[-16291.6947, -16005.7004, -25656.1216,  10542.1706,  14461.2760,
          -1839.6053,  -7145.0048]], dtype=torch.float64)
	q_value: tensor([[-36.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06141770348605102, distance: 1.1789621344126107 entropy 11.012430218253337
epoch: 46, step: 6
	action: tensor([[ -1771.0117, -14433.7546,  12000.5164,  20515.1187,  16037.1960,
          -6440.0324,   1191.0298]], dtype=torch.float64)
	q_value: tensor([[-27.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010343159126565915, distance: 1.13841080410917 entropy 11.223040349067478
epoch: 46, step: 7
	action: tensor([[ 7522.6929, -7188.5923, 11425.9214,  8639.7679, -7227.8766,  4162.3694,
          2866.8660]], dtype=torch.float64)
	q_value: tensor([[-27.2393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18375071940577437, distance: 1.2450500820387997 entropy 11.179183073541365
epoch: 46, step: 8
	action: tensor([[  7681.7096, -11524.9361, -27542.4439,  10621.2133,  -6850.1049,
          -5148.9944,  -2794.1599]], dtype=torch.float64)
	q_value: tensor([[-27.7601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18307342528541115, distance: 1.0343040023570604 entropy 11.077366765135206
epoch: 46, step: 9
	action: tensor([[  8758.7650,   6411.2445, -36308.1102,  15174.8566, -25417.3136,
         -10026.8912, -18394.2417]], dtype=torch.float64)
	q_value: tensor([[-33.5588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7565389600972019, distance: 0.5646397251579662 entropy 11.117305275676449
epoch: 46, step: 10
	action: tensor([[  8274.1381,  -1683.1860, -16494.6385,   7082.7144,   -783.3125,
          27956.2845,   3750.0152]], dtype=torch.float64)
	q_value: tensor([[-31.8832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08842142945160236, distance: 1.092581259161778 entropy 11.143988456288588
epoch: 46, step: 11
	action: tensor([[-17278.8294, -15724.8911,  16972.4640, -13610.8189,  24402.0932,
          -9692.7373,  11451.2199]], dtype=torch.float64)
	q_value: tensor([[-29.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9616504631945249, distance: 1.602756348683918 entropy 11.139810879986712
epoch: 46, step: 12
	action: tensor([[-13942.2787,  -3690.7306, -16234.2018, -21317.8842,   1299.3564,
          25827.5009, -20667.8668]], dtype=torch.float64)
	q_value: tensor([[-26.7080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6719861729759042, distance: 1.4796978203009843 entropy 11.010599745203434
epoch: 46, step: 13
	action: tensor([[-14854.3247, -27186.0577,   9071.5545,  38943.0336,   -940.0364,
          -1351.2770,   5096.1416]], dtype=torch.float64)
	q_value: tensor([[-31.6849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9399091178499175, distance: 1.593849774818791 entropy 11.05028623113497
epoch: 46, step: 14
	action: tensor([[-5496.1866, 10897.7781,  3231.0869, 23126.0454,  7929.4198, 13833.7692,
         11694.6578]], dtype=torch.float64)
	q_value: tensor([[-21.2276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1435527815554417, distance: 1.0590269079337153 entropy 10.875117365337127
epoch: 46, step: 15
	action: tensor([[-14233.6846,   6383.4602,  -1900.4882,  -7468.2649,  -6655.1853,
          -4701.0037,   2642.7517]], dtype=torch.float64)
	q_value: tensor([[-26.7882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.86874092619827
epoch: 46, step: 16
	action: tensor([[-13448.1076, -14507.9709,  -8483.7717,   6351.1643,  -8214.7602,
         -12378.7218,  20061.9475]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11064929654102507, distance: 1.20599405297085 entropy 10.54737174593475
epoch: 46, step: 17
	action: tensor([[-22751.6541, -22583.9824,  29967.2724,  -1045.1948,  16498.2827,
          37670.8702,  -1299.4265]], dtype=torch.float64)
	q_value: tensor([[-26.0563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5344712179448201, distance: 1.4175424285823464 entropy 11.154403584910813
epoch: 46, step: 18
	action: tensor([[-15695.4908, -15012.6588,  28855.9740,   2905.3448,  -2926.1495,
           1167.1401,   7909.1782]], dtype=torch.float64)
	q_value: tensor([[-26.3369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5608914537217278, distance: 1.4296938356284303 entropy 10.980414593768414
epoch: 46, step: 19
	action: tensor([[-4.9450e+04,  3.7889e+00,  3.8865e+03,  2.0057e+04, -2.9500e+04,
          7.9117e+04,  2.5701e+04]], dtype=torch.float64)
	q_value: tensor([[-29.2028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.220299192557182
epoch: 46, step: 20
	action: tensor([[  2169.9834,   3257.1476,  -5795.7271,  -4138.8135, -12953.7164,
            444.8129,   2694.0937]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.54737174593475
epoch: 46, step: 21
	action: tensor([[-4883.9055,  7626.6458,  1255.2165, 26304.8668,  3118.3230, -8490.0737,
          3965.2174]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17092423537726353, distance: 1.2382863647164308 entropy 10.54737174593475
epoch: 46, step: 22
	action: tensor([[-25629.4733, -15166.6944,  -9190.0818,  17955.1787,  12592.2032,
          -9189.2673,   3501.4001]], dtype=torch.float64)
	q_value: tensor([[-28.3747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16587740562073905, distance: 1.2356148985444169 entropy 10.963675730826639
epoch: 46, step: 23
	action: tensor([[-32017.5290, -18384.9568, -16919.4924,  -5464.6905,   7612.2682,
          -9599.7073, -28077.4014]], dtype=torch.float64)
	q_value: tensor([[-27.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7734980944448706, distance: 1.5239546954139631 entropy 11.12846938632283
epoch: 46, step: 24
	action: tensor([[-18662.5297,   9939.8481,  -3204.8598, -13646.4008,  19650.9560,
          54674.3477,   4889.9683]], dtype=torch.float64)
	q_value: tensor([[-28.2058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18069373904609698, distance: 1.0358093574549878 entropy 11.180353971291515
epoch: 46, step: 25
	action: tensor([[-15422.3647,  -8858.2812,   8866.4013, -19723.4855,  -6951.8066,
         -12699.1343, -10527.6780]], dtype=torch.float64)
	q_value: tensor([[-40.8146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4108177394186636, distance: 1.3592274891468084 entropy 11.06273408861001
epoch: 46, step: 26
	action: tensor([[  5036.3073,   -205.9170, -19901.3347,  38607.1967,  -6837.2127,
           2513.3135,  10375.4746]], dtype=torch.float64)
	q_value: tensor([[-31.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11558080886758082, distance: 1.0761821197653783 entropy 11.236560835977867
epoch: 46, step: 27
	action: tensor([[-5410.2006, -5582.0962, 10899.2653, 35646.0412, -4787.5804, 10405.8285,
         17254.9596]], dtype=torch.float64)
	q_value: tensor([[-30.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12051742575962343, distance: 1.0731744209265244 entropy 11.024325024942147
epoch: 46, step: 28
	action: tensor([[-10501.9143,   -141.1567,  -1134.4923,  -5717.1269,  -2212.8977,
             78.8552,   7548.6314]], dtype=torch.float64)
	q_value: tensor([[-26.5732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5820774060622445, distance: 1.439363738548792 entropy 11.05655572856242
epoch: 46, step: 29
	action: tensor([[-20905.2862, -46517.9878,   -279.9523,  -9515.5771,  42529.3374,
         -10702.5966, -11789.2310]], dtype=torch.float64)
	q_value: tensor([[-27.5056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4826308996336919, distance: 1.3933916921211158 entropy 11.114441471603715
epoch: 46, step: 30
	action: tensor([[-14749.8277,  -4633.7104,  14955.4482, -13685.7371,  12704.7637,
          12738.8497,  23236.5796]], dtype=torch.float64)
	q_value: tensor([[-29.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8612271572976518, distance: 1.561192238946147 entropy 11.103812186147413
epoch: 46, step: 31
	action: tensor([[-17100.7191,  27941.6331, -18065.3409,  16998.4273,  -5045.1821,
          -5936.0763,  -4666.1454]], dtype=torch.float64)
	q_value: tensor([[-26.3029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2019100268213052, distance: 1.0223100272438104 entropy 10.994868277773621
epoch: 46, step: 32
	action: tensor([[-23958.2153,  -3671.2378, -12286.7201,  -6392.3069, -13671.8649,
          -8393.4627,   2610.5198]], dtype=torch.float64)
	q_value: tensor([[-28.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1865701762866191, distance: 1.0320880247014623 entropy 10.88017004678922
epoch: 46, step: 33
	action: tensor([[ -1742.3727, -22745.1726,   2245.8610,  10388.1424,   3427.0111,
         -12854.0114,  14041.3500]], dtype=torch.float64)
	q_value: tensor([[-31.2176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022323116731878923, distance: 1.1314995005057744 entropy 11.182093161596358
epoch: 46, step: 34
	action: tensor([[-17660.2973, -23285.9150,  -1957.0681, -17927.1896,   5383.2963,
         -16477.8824,  13084.1817]], dtype=torch.float64)
	q_value: tensor([[-28.3107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5463092151799764, distance: 1.4229998860222908 entropy 11.138985414463013
epoch: 46, step: 35
	action: tensor([[ 11450.9771, -14262.6117,  11200.1900,  27152.9681, -10080.0053,
         -14351.7316,  35309.1869]], dtype=torch.float64)
	q_value: tensor([[-38.3991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11696974215991685, distance: 1.0753367443452935 entropy 11.292900421143349
epoch: 46, step: 36
	action: tensor([[-10609.8249, -12152.9077,  18892.1743,  -7838.6015,  45688.1776,
          11130.9002,  -8772.8143]], dtype=torch.float64)
	q_value: tensor([[-24.6253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37684331990614206, distance: 0.9033484853883289 entropy 11.11131007886152
epoch: 46, step: 37
	action: tensor([[ -6694.3249, -14722.3093,   8688.1583,  -2530.2700,  -1242.6710,
          11414.8066, -27356.1181]], dtype=torch.float64)
	q_value: tensor([[-33.7871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2691953196329888, distance: 1.2892018623716712 entropy 11.136513958916611
epoch: 46, step: 38
	action: tensor([[-50454.1828, -24881.0326,  -1744.0606, -25706.2341,  -3493.2495,
          34347.5777,  28464.8228]], dtype=torch.float64)
	q_value: tensor([[-30.9493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8082201940588503, distance: 1.538800613634143 entropy 10.98528262164076
epoch: 46, step: 39
	action: tensor([[ 25390.2344, -19873.5668, -21929.1377,  12217.8797,  -3290.1079,
         -13920.6668,   -592.6578]], dtype=torch.float64)
	q_value: tensor([[-29.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010149832896346411, distance: 1.138521991094878 entropy 11.136148291320273
epoch: 46, step: 40
	action: tensor([[-26473.8729, -23029.1535,    256.3135,  33284.0907,   7424.3370,
          17789.3341, -12040.9666]], dtype=torch.float64)
	q_value: tensor([[-26.8958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6546618916291322, distance: 1.472011916104252 entropy 10.99258126865986
epoch: 46, step: 41
	action: tensor([[-20305.2821, -27010.2678,   5485.5520,  41511.6474,  -1643.8943,
          20999.9384,  10242.5783]], dtype=torch.float64)
	q_value: tensor([[-28.6888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26765063305327286, distance: 1.2884171056558815 entropy 11.215864896899971
epoch: 46, step: 42
	action: tensor([[ -8596.4296, -17506.0290, -37877.7351,   2028.5163, -23816.9844,
           8009.5666,  10793.9993]], dtype=torch.float64)
	q_value: tensor([[-27.5675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7566597930658359, distance: 1.5167029233650193 entropy 11.094404483049148
epoch: 46, step: 43
	action: tensor([[-12169.5936, -34089.6726,  -4139.9617,  -1857.2157,  -1140.9243,
         -11572.4043,  -7734.2446]], dtype=torch.float64)
	q_value: tensor([[-27.4117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9168574013886142, distance: 1.5843517077991878 entropy 11.106878158897539
epoch: 46, step: 44
	action: tensor([[ -4739.7377,   1363.3679, -15036.3120,  10519.2173,  22077.3958,
         -15340.7181,  14601.3433]], dtype=torch.float64)
	q_value: tensor([[-25.2946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5998311534634723, distance: 0.7238995905638244 entropy 10.942639252004316
epoch: 46, step: 45
	action: tensor([[-18107.9688, -17707.8693,  23842.7940,  -2081.6544,  10089.1541,
         -15102.8865, -11113.4135]], dtype=torch.float64)
	q_value: tensor([[-27.8630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6443520844314543, distance: 1.4674188711974139 entropy 10.976799090979116
epoch: 46, step: 46
	action: tensor([[  -743.4657, -27249.5460, -15781.6245,  16565.9007,   4930.8969,
           5728.2010, -14407.2111]], dtype=torch.float64)
	q_value: tensor([[-27.8920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22707129584998476, distance: 1.006065858622079 entropy 11.01028832662223
epoch: 46, step: 47
	action: tensor([[22493.0771, -6731.2779,  5872.3398,  7532.0449,  4605.9856,   747.2554,
         39054.4983]], dtype=torch.float64)
	q_value: tensor([[-29.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49442096886756626, distance: 0.8136754633895366 entropy 11.189337026241864
epoch: 46, step: 48
	action: tensor([[  4736.7327, -12118.6855, -10242.3784,  -1517.7235,  16458.4996,
           5164.7755,   4864.7992]], dtype=torch.float64)
	q_value: tensor([[-26.0165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27440472814140904, distance: 1.2918449157002803 entropy 11.03580499202103
epoch: 46, step: 49
	action: tensor([[ -1309.5863,  -4525.6063,   1208.5355,   -130.6046,   9514.9350,
         -11910.5001, -20205.0860]], dtype=torch.float64)
	q_value: tensor([[-28.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6221304183784593, distance: 1.4574698431198987 entropy 10.923043849888192
epoch: 46, step: 50
	action: tensor([[ -7788.0034, -21646.0125,  19848.8863, -14379.4809,  14895.9543,
          15209.9946,   2061.9329]], dtype=torch.float64)
	q_value: tensor([[-26.7287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36291772428400737, distance: 1.3359540256949323 entropy 11.007141609754203
epoch: 46, step: 51
	action: tensor([[ -3255.6949,   8954.7867, -15184.8033,  10069.1298,  -3018.9594,
         -16979.9582,   5398.8577]], dtype=torch.float64)
	q_value: tensor([[-26.8310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6591215112822536, distance: 0.6681231055908664 entropy 10.884389249605226
epoch: 46, step: 52
	action: tensor([[  2562.6151, -15957.2899, -11631.4102,   9774.7232,  15866.3098,
         -18451.4950,   3441.8784]], dtype=torch.float64)
	q_value: tensor([[-29.9447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.009895271160676
epoch: 46, step: 53
	action: tensor([[ 10797.2667,   9155.4883,   1420.2706, -10842.3593,  -9344.9008,
          -9421.5778,  13209.5937]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.54737174593475
epoch: 46, step: 54
	action: tensor([[ 5190.1708, -2770.4146,  2375.0704,   129.2772,  4987.4235,  2229.4229,
         10516.1755]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2866634785902874, distance: 0.9665045949095535 entropy 10.54737174593475
epoch: 46, step: 55
	action: tensor([[ 10055.9592,  -6994.5086,  -2371.5072,   -352.0913, -16867.3574,
         -20796.2823,  -6296.0770]], dtype=torch.float64)
	q_value: tensor([[-25.8062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21865373914616126, distance: 1.0115292889916523 entropy 10.850914784382708
epoch: 46, step: 56
	action: tensor([[   952.3150, -22683.3710, -15005.8213,  44193.9641, -19584.1163,
           -372.4024, -19706.5845]], dtype=torch.float64)
	q_value: tensor([[-28.0795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08702985091982784, distance: 1.0934148860515074 entropy 10.921756537165056
epoch: 46, step: 57
	action: tensor([[-1.0762e+04,  1.9673e+00,  6.5010e+03,  1.2234e+04, -2.4353e+04,
         -2.4235e+04,  2.0169e+04]], dtype=torch.float64)
	q_value: tensor([[-28.0901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.037143660926963
epoch: 46, step: 58
	action: tensor([[-1594.1416,  2616.1543,  3679.6715, 17383.5604, 11808.1012,  2942.6414,
         -5907.2745]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.54737174593475
epoch: 46, step: 59
	action: tensor([[-3886.8011, 11762.5599, -4069.8342, -6367.0754,  2882.3916, 10417.8705,
          8774.1810]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.54737174593475
epoch: 46, step: 60
	action: tensor([[-10026.0585,   3285.2472,  24496.2081,  -4222.8045,   -836.7788,
           -816.2442,  -2938.4053]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.54737174593475
epoch: 46, step: 61
	action: tensor([[-7085.6466,  7561.2559,  8757.4781,  1712.7851, -5434.6770, -3546.8080,
          4141.6282]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9642621225309009, distance: 1.6038229151720818 entropy 10.54737174593475
epoch: 46, step: 62
	action: tensor([[ -3953.9735, -13167.6712,  -4067.7951,   9704.6043,  -8551.9116,
         -14811.3269, -27266.8058]], dtype=torch.float64)
	q_value: tensor([[-29.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6962718037319378, distance: 1.4904053979331717 entropy 11.217115960015482
epoch: 46, step: 63
	action: tensor([[-16343.8070,  17505.2185, -11192.0062,  17889.4198,  12851.7668,
          26406.9233, -18844.5884]], dtype=torch.float64)
	q_value: tensor([[-25.1174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.12079776546402
epoch: 46, step: 64
	action: tensor([[  5491.1259,   1176.4209, -10304.0673,   1019.3466,   1302.0646,
         -11113.4248,  -7785.7440]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5744329938964075, distance: 0.7465186608036194 entropy 10.54737174593475
epoch: 46, step: 65
	action: tensor([[-18162.9687,  16990.1814,  -6215.6285,  -6350.8057,  16958.4685,
           6461.1732,  -3378.1559]], dtype=torch.float64)
	q_value: tensor([[-26.3225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21532664839207116, distance: 1.01368062372767 entropy 10.686823294549507
epoch: 46, step: 66
	action: tensor([[-15301.3010,   3865.8819, -17035.8112,  11901.9053,   7996.3171,
         -13959.9225,  10956.9499]], dtype=torch.float64)
	q_value: tensor([[-24.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5211050285158116, distance: 0.7919118317643267 entropy 10.708311104268896
epoch: 46, step: 67
	action: tensor([[ 7186.5710, -7094.4317, 12984.5321, -4083.9040,  5603.3597, -5975.7024,
         24683.1700]], dtype=torch.float64)
	q_value: tensor([[-31.6225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.190383124941807
epoch: 46, step: 68
	action: tensor([[ 8371.0726, -1469.7635,  8048.1595, -1085.2751, -5492.8946,   492.5661,
         -6270.4038]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.54737174593475
epoch: 46, step: 69
	action: tensor([[-4941.6243, -2893.7866, -2726.6424,  1690.5264, 10539.0575, -6431.2339,
         -7110.7262]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24556933867370234, distance: 1.277146294771099 entropy 10.54737174593475
epoch: 46, step: 70
	action: tensor([[ 11440.9645,   3411.4639,   5087.2653,   6383.3693, -16388.3410,
          -5297.3952, -10925.5015]], dtype=torch.float64)
	q_value: tensor([[-26.4604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3553983409989274, distance: 0.9187606846808332 entropy 11.069909392197134
epoch: 46, step: 71
	action: tensor([[-19816.8009,   4738.0590,  22865.3830,  -9198.9867,  15956.0461,
         -23658.3766, -12677.0509]], dtype=torch.float64)
	q_value: tensor([[-25.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4425287629205751, distance: 0.8544131535585021 entropy 10.888612350800187
epoch: 46, step: 72
	action: tensor([[ 1425.0674, -8944.4615, -4611.2766, -9866.7079, 18280.4975, -3032.0441,
         12328.6931]], dtype=torch.float64)
	q_value: tensor([[-33.3931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14506590204222625, distance: 1.2245370630751937 entropy 11.100238519467794
epoch: 46, step: 73
	action: tensor([[-18794.1921, -11318.3035,   7762.6216,  -1037.2988,   7514.5305,
           9886.9969,  -2547.9336]], dtype=torch.float64)
	q_value: tensor([[-28.4011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9510265157603746, distance: 1.5984103357813628 entropy 11.00981058979758
epoch: 46, step: 74
	action: tensor([[-13374.4769,  -3919.6608,   9495.2762,  23722.5072,  -1313.0808,
          28997.3443, -16804.8785]], dtype=torch.float64)
	q_value: tensor([[-27.4952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07982125342096746, distance: 1.0977230720104856 entropy 11.106059855553115
epoch: 46, step: 75
	action: tensor([[ -6711.3728,  26965.8752,    -86.8325, -14279.4012,   7853.3592,
           3079.8801,   7740.8121]], dtype=torch.float64)
	q_value: tensor([[-24.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13166662608679425, distance: 1.0663504090436313 entropy 10.987986135660739
epoch: 46, step: 76
	action: tensor([[ -3903.4652,  -7581.0971,  -9713.9785, -21684.6510,  19991.9165,
         -23341.7066,   3483.4597]], dtype=torch.float64)
	q_value: tensor([[-33.8400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15261697805504237, distance: 1.0534079033975376 entropy 10.761954025097495
epoch: 46, step: 77
	action: tensor([[-14584.2316,  -6031.2756,   4365.2379,  25538.7396,   1399.8484,
           8533.4796,   1256.5979]], dtype=torch.float64)
	q_value: tensor([[-30.4697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45294438543759563, distance: 1.3793713102839835 entropy 11.132833781225495
epoch: 46, step: 78
	action: tensor([[-29193.4915, -33200.0658,   3576.5017,  -3984.0385, -21436.9751,
          -6361.1233,  29950.8775]], dtype=torch.float64)
	q_value: tensor([[-26.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14087951545801491, distance: 1.2222965458903277 entropy 10.951982141743027
epoch: 46, step: 79
	action: tensor([[  -633.3742, -34366.6336,  -4468.8563,  10830.4371,  -5050.0629,
           3182.2966,  26157.3945]], dtype=torch.float64)
	q_value: tensor([[-25.6836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2765314170456157, distance: 1.7266070796676558 entropy 10.821826926670486
epoch: 46, step: 80
	action: tensor([[-15066.8808,  -4074.5933, -20881.8133,   3530.7864, -18689.4728,
          -2561.0696,  14265.8305]], dtype=torch.float64)
	q_value: tensor([[-21.8934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23821833521389224, distance: 0.9987848611459242 entropy 10.863567808217926
epoch: 46, step: 81
	action: tensor([[-29195.0887,  20717.9829,   1011.8771, -26079.9642, -15794.3216,
           7133.1831, -51343.2132]], dtype=torch.float64)
	q_value: tensor([[-29.7491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07922898913292031, distance: 1.1888128600991792 entropy 11.330569334749617
epoch: 46, step: 82
	action: tensor([[-7447.2708,  3762.2662,  8811.2169,  7739.7779, 10143.8643, 22647.2070,
         14222.0023]], dtype=torch.float64)
	q_value: tensor([[-29.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13874200357457522, distance: 1.0619970903226537 entropy 11.082566770922186
epoch: 46, step: 83
	action: tensor([[-39398.2997,  -8764.4396,  13957.0768,   7750.9670,  12895.8440,
           5309.4430,  -6096.5414]], dtype=torch.float64)
	q_value: tensor([[-32.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18395568138671226, distance: 1.2451578652408486 entropy 11.23066808324252
epoch: 46, step: 84
	action: tensor([[-14361.4978, -14431.4430,  10062.2442,  15415.7834,  14168.4505,
          19360.6179,  27182.6513]], dtype=torch.float64)
	q_value: tensor([[-27.9886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021816829653073966, distance: 1.131792434419756 entropy 11.182086400755496
epoch: 46, step: 85
	action: tensor([[  2708.3538,   2235.7954,  17273.3198, -37069.8327,  12108.2083,
            738.6514,   3568.9353]], dtype=torch.float64)
	q_value: tensor([[-29.7749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.155674887347944
epoch: 46, step: 86
	action: tensor([[-9262.9630, -8373.9592, -3652.0194, -7364.6631,  2668.6650,  4437.4127,
         -2345.9085]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33826463206643664, distance: 1.3238162064526147 entropy 10.54737174593475
epoch: 46, step: 87
	action: tensor([[  537.5071, 27029.7300,  2925.0120,  -424.7628,  1424.3005, 26717.1658,
         -2761.3228]], dtype=torch.float64)
	q_value: tensor([[-28.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.02894174609165
epoch: 46, step: 88
	action: tensor([[  8886.2353, -20998.2021,   7182.4855,  -6209.5256,   2783.2633,
         -17179.2494,  -8638.1367]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45189575918045966, distance: 0.8472045400624274 entropy 10.54737174593475
epoch: 46, step: 89
	action: tensor([[ -1647.3726, -38633.8503,  -7484.9605, -10956.6762,   9858.0876,
         -17820.3852,  42385.9091]], dtype=torch.float64)
	q_value: tensor([[-24.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02954280181233948, distance: 1.1273139626599302 entropy 10.83324089577142
epoch: 46, step: 90
	action: tensor([[ 13148.3023,  -6154.0622,  -2691.9737,    180.3207, -22501.5558,
         -11126.8003,  -3032.4064]], dtype=torch.float64)
	q_value: tensor([[-30.2764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41439522382548244, distance: 0.875707357054749 entropy 10.967975479975733
epoch: 46, step: 91
	action: tensor([[-10262.2585,  -5481.9988,  -3064.3640,  27036.2048,  36956.0937,
           3995.5984,   1384.7157]], dtype=torch.float64)
	q_value: tensor([[-26.4673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09323917152884764, distance: 1.1965043598875809 entropy 11.05576007193247
epoch: 46, step: 92
	action: tensor([[ 11423.7431,  38483.2019, -16401.0853,  27412.6295,  20483.9159,
         -12728.4503,  17170.8981]], dtype=torch.float64)
	q_value: tensor([[-28.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.12783997257283
epoch: 46, step: 93
	action: tensor([[-10178.4517, -13217.5500,    892.4053,   9571.0369, -36024.2256,
           1471.3889,  18511.4900]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9730438786130986, distance: 1.6074040753891528 entropy 10.54737174593475
epoch: 46, step: 94
	action: tensor([[-42198.6510,   -149.1506,  -4472.0557,  23800.1869,  12944.5323,
          -4071.1524,  -6296.4361]], dtype=torch.float64)
	q_value: tensor([[-27.6254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44429098350052265, distance: 1.3752575671823088 entropy 11.086823211280924
epoch: 46, step: 95
	action: tensor([[-16483.0378, -10043.6724,  15640.2733,  28530.0953, -11194.4997,
           8288.8713, -10387.9981]], dtype=torch.float64)
	q_value: tensor([[-23.5406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048530425296718915, distance: 1.1717830504141014 entropy 10.935912611633684
epoch: 46, step: 96
	action: tensor([[  8883.3418, -36237.8276,  10633.7393,  10788.8355,  33343.9981,
           4826.5225,  12096.0053]], dtype=torch.float64)
	q_value: tensor([[-27.0989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48550758134931193, distance: 0.8208166985164067 entropy 11.130718270517773
epoch: 46, step: 97
	action: tensor([[-25611.5363,   1636.3925, -19472.7235,  -2267.8983, -12525.1284,
           7733.5930,  14262.5366]], dtype=torch.float64)
	q_value: tensor([[-31.0866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.945344178089176
epoch: 46, step: 98
	action: tensor([[-14048.9384, -10463.1002,   6451.3977,  15582.2437, -11161.9217,
         -14646.5715,   -717.4579]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07586235892835069, distance: 1.186957174481838 entropy 10.54737174593475
epoch: 46, step: 99
	action: tensor([[   779.8723, -17671.8428,  14437.4260,  -3497.7054,  15122.6696,
          -9657.1876,  20454.5023]], dtype=torch.float64)
	q_value: tensor([[-26.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3509002742418451, distance: 0.9219606938802319 entropy 11.175234817298048
epoch: 46, step: 100
	action: tensor([[ -1849.1315,  -6124.5762,  16188.2345,  39128.0591,   7661.5871,
         -10437.9219,   4211.3375]], dtype=torch.float64)
	q_value: tensor([[-24.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29296007713175565, distance: 0.9622294874460096 entropy 10.931117131080738
epoch: 46, step: 101
	action: tensor([[ -3886.9864, -23975.1854,  23631.3202,  26876.1485, -15233.5001,
          -3233.8429, -11593.4108]], dtype=torch.float64)
	q_value: tensor([[-28.5262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12947476804981306, distance: 1.0676954083778092 entropy 11.268361750055979
epoch: 46, step: 102
	action: tensor([[ 19120.2608,  14552.8600,  13027.9261,  26885.3570,  -1869.6034,
         -10261.7442,   7866.0539]], dtype=torch.float64)
	q_value: tensor([[-27.4335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4890939504800643, distance: 0.8179508646200552 entropy 11.232989908479407
epoch: 46, step: 103
	action: tensor([[-1009.6559,  2605.4401, 12370.9103, 14429.1415,  -546.4275, 18889.1223,
         10296.8916]], dtype=torch.float64)
	q_value: tensor([[-29.1405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2804525154583548, distance: 0.9707031141638865 entropy 11.087743174163297
epoch: 46, step: 104
	action: tensor([[-15512.0202, -14250.5760,   1956.4783,  18146.2762,   -259.9602,
         -13363.2399,  -3899.3847]], dtype=torch.float64)
	q_value: tensor([[-29.6269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 2.8101750899400813e-05, distance: 1.1443281748484515 entropy 10.99392984503808
epoch: 46, step: 105
	action: tensor([[-33789.2095, -17718.8320,  -4084.3384, -36196.3594,  -3486.6710,
         -31665.9533,  16222.4042]], dtype=torch.float64)
	q_value: tensor([[-28.4629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0018190051929678486, distance: 1.1453845652023542 entropy 11.26219477989081
epoch: 46, step: 106
	action: tensor([[-12851.8690, -13258.6107,  17884.9995,  -7135.7793, -12683.5256,
          21455.6783,  15365.1201]], dtype=torch.float64)
	q_value: tensor([[-26.1815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23585857655669407, distance: 1.272158081534823 entropy 10.892167580421713
epoch: 46, step: 107
	action: tensor([[ 15529.2924, -32845.0296,   -636.7845,  25667.7920,  16618.4184,
           5552.5373,  43225.6752]], dtype=torch.float64)
	q_value: tensor([[-39.8680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1810323414251045, distance: 1.2436196871225422 entropy 11.31959395323177
epoch: 46, step: 108
	action: tensor([[12409.0926,   127.0949,   695.8741, 25460.5894, -4987.9613,  3845.4533,
         -2796.1561]], dtype=torch.float64)
	q_value: tensor([[-26.6534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6056676866378385, distance: 0.718601098164024 entropy 10.976379579715458
epoch: 46, step: 109
	action: tensor([[  6294.8366,  -1798.5992, -10836.4205,   9855.0129, -22358.7886,
          -5243.4934,  11662.9443]], dtype=torch.float64)
	q_value: tensor([[-26.2448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3576006343854021, distance: 0.9171898604833292 entropy 10.877905080911244
epoch: 46, step: 110
	action: tensor([[-15860.7489,  14297.3943,   8877.4471, -19334.4488, -15409.1455,
          29864.2128,  -1639.3388]], dtype=torch.float64)
	q_value: tensor([[-31.1982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.20496700401093
epoch: 46, step: 111
	action: tensor([[ 6210.3788, -2200.8023,  2947.3019,  5901.5833, -4256.1501, -5220.4544,
          1591.6144]], dtype=torch.float64)
	q_value: tensor([[-32.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17742230125688463, distance: 1.0378752576990187 entropy 10.54737174593475
epoch: 46, step: 112
	action: tensor([[-33944.0797, -24679.6375,  24017.3631,  14249.0932,  29506.4704,
           6877.2264,   3111.3850]], dtype=torch.float64)
	q_value: tensor([[-29.4213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23401908127163962, distance: 1.0015339343801146 entropy 11.088271813675819
epoch: 46, step: 113
	action: tensor([[  4734.4820, -15050.3854, -11199.4465,  -1125.1266, -16044.5587,
            868.0367, -16638.5643]], dtype=torch.float64)
	q_value: tensor([[-26.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35320244328474104, distance: 0.9203242776174387 entropy 10.927330498814118
epoch: 46, step: 114
	action: tensor([[-20013.8383,  10230.9236,   7132.2984,  -3012.3627,   6853.4056,
          25906.1867,   6480.7914]], dtype=torch.float64)
	q_value: tensor([[-30.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2832024552526944, distance: 0.9688464400079808 entropy 11.078444761864757
epoch: 46, step: 115
	action: tensor([[  8101.4419,  -6081.5158,  -1886.0268,  -2285.8756, -21185.7392,
          -2072.7509, -12602.7921]], dtype=torch.float64)
	q_value: tensor([[-32.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.347584589616182, distance: 0.9243124411673584 entropy 10.844634893015169
epoch: 46, step: 116
	action: tensor([[-20365.7760,  -8019.6964, -35234.1409,   2468.2303,   2617.1710,
          20936.0867,   4786.3721]], dtype=torch.float64)
	q_value: tensor([[-27.4875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7042254951674778, distance: 1.493895511137123 entropy 10.907391978334443
epoch: 46, step: 117
	action: tensor([[ -5858.9992,  -1709.1462,  -3675.9308, -13353.4629,  16609.7653,
         -26957.5681,  -3425.9309]], dtype=torch.float64)
	q_value: tensor([[-24.6316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24388938424920048, distance: 1.2762847323357556 entropy 10.930954745413903
epoch: 46, step: 118
	action: tensor([[-14984.9156, -24419.8635,   8164.7294, -10588.0487,  -9954.3870,
         -26285.3757, -13313.5563]], dtype=torch.float64)
	q_value: tensor([[-33.0021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3923806346070362, distance: 1.3503168295703316 entropy 11.149765693194837
epoch: 46, step: 119
	action: tensor([[-4651.4465, -9080.9807,  3067.2458, 34177.6054, -3073.6390,  -227.2961,
         16889.2280]], dtype=torch.float64)
	q_value: tensor([[-32.7113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5501526068434168, distance: 1.424767240066439 entropy 11.123047674112831
epoch: 46, step: 120
	action: tensor([[-15265.3209, -20124.2477, -16790.4470, -11937.5548, -13144.9436,
          -1774.7781,   5299.5006]], dtype=torch.float64)
	q_value: tensor([[-21.0040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12591787296627788, distance: 1.0698744429774256 entropy 10.768481131563876
epoch: 46, step: 121
	action: tensor([[-17636.4962, -17012.6954,   7646.8281,  15221.7657,  -2766.8215,
          31771.2131, -14288.0787]], dtype=torch.float64)
	q_value: tensor([[-30.2456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0203766801323395, distance: 1.6265704074359955 entropy 11.042207916828172
epoch: 46, step: 122
	action: tensor([[-14399.3111,  -5181.4500, -13463.0525,  13486.9341,  17463.3159,
           4339.7474,  11235.1945]], dtype=torch.float64)
	q_value: tensor([[-24.2142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.036435089408906274, distance: 1.1650048872714824 entropy 10.907669556728306
epoch: 46, step: 123
	action: tensor([[ -6155.5524, -22098.4062, -18890.6279,   9258.7623,    -60.3598,
          19988.9084,  -7730.4597]], dtype=torch.float64)
	q_value: tensor([[-27.2643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35400877034328415, distance: 1.3315805164557155 entropy 11.092422782942958
epoch: 46, step: 124
	action: tensor([[-24542.3207,  22399.7591,  -4988.8964,  12474.1523, -20222.5309,
          -6254.0177,  11748.7712]], dtype=torch.float64)
	q_value: tensor([[-29.1108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18921132937788698, distance: 1.0304111014418227 entropy 11.171853742324858
epoch: 46, step: 125
	action: tensor([[-13245.0070,  11567.2624,   4351.9521,  13642.2509,  -6987.9537,
         -11963.7285,  15047.5803]], dtype=torch.float64)
	q_value: tensor([[-29.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5771511389509503, distance: 0.7441307918399007 entropy 11.04356687523671
epoch: 46, step: 126
	action: tensor([[-12878.2978, -29817.3176, -11977.6280,  -1310.0308,   6736.6404,
           1214.7125,  -6293.5667]], dtype=torch.float64)
	q_value: tensor([[-27.8603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8365191810119192, distance: 1.5507951265882398 entropy 10.951168356219268
epoch: 46, step: 127
	action: tensor([[-14214.4221, -38931.3342,  -4738.8766,   7565.1186,    340.8670,
          11838.9518,   9526.4450]], dtype=torch.float64)
	q_value: tensor([[-26.3081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39944927976817723, distance: 1.3537400411795595 entropy 10.890440271315553
LOSS epoch 46 actor 331.5329072425044 critic 198.0610785987675
epoch: 47, step: 0
	action: tensor([[  1051.4398, -11119.0656,  -9758.8472,   8125.8669,  12591.3832,
          -1532.2175, -49187.8874]], dtype=torch.float64)
	q_value: tensor([[-31.5792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27321489630077733, distance: 0.9755728420700103 entropy 11.098843677629779
epoch: 47, step: 1
	action: tensor([[-18643.3971,  -4010.1892,  30987.2000,  -1897.7971,  26247.5416,
          36266.3563,  18796.1780]], dtype=torch.float64)
	q_value: tensor([[-30.4516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04079391398693111, distance: 1.1674520854270085 entropy 11.092422493502811
epoch: 47, step: 2
	action: tensor([[-11459.8108, -20135.0641,  -3464.5117,  34415.9265,   4535.2407,
         -11756.9146,  26881.8104]], dtype=torch.float64)
	q_value: tensor([[-34.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5221791712800687, distance: 1.411853324365267 entropy 11.210251559320213
epoch: 47, step: 3
	action: tensor([[ 10346.7987,  13443.5774,   6209.4045, -11706.5402,   1591.9846,
          17825.0263,  21179.8548]], dtype=torch.float64)
	q_value: tensor([[-31.7452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.302549534548133
epoch: 47, step: 4
	action: tensor([[  3524.1493,  -9144.6486,   -800.4123,  -4018.2896, -16392.7359,
          -7140.1862,  -8483.3314]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.401397583486625, distance: 0.885372291298452 entropy 10.594180570233785
epoch: 47, step: 5
	action: tensor([[-18520.9858, -20768.1611,  13340.8112,  -7491.8336,  14228.2812,
           5828.6232,  18485.3599]], dtype=torch.float64)
	q_value: tensor([[-31.1503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01786372714415929, distance: 1.1340770680677048 entropy 10.934396518600412
epoch: 47, step: 6
	action: tensor([[-13793.5925, -11717.2791, -21746.3847,  -3043.7292,  -6651.1530,
          -1582.3764,  12647.8909]], dtype=torch.float64)
	q_value: tensor([[-36.1390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30054237343172074, distance: 1.3050253461383874 entropy 11.211187471211174
epoch: 47, step: 7
	action: tensor([[-11186.4958, -14539.3547,  -4260.2864,  17912.7483,  10959.4676,
          21367.9427, -13151.1958]], dtype=torch.float64)
	q_value: tensor([[-38.0293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.649063087128166, distance: 1.4695194158520002 entropy 11.182190305250117
epoch: 47, step: 8
	action: tensor([[  -344.6615, -17378.3405,  10722.6826,  10885.3598, -22707.0960,
          16075.1204,  14732.6830]], dtype=torch.float64)
	q_value: tensor([[-27.5247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02579007626133789, distance: 1.1294915022812688 entropy 10.889057713009848
epoch: 47, step: 9
	action: tensor([[-27276.3983,   2159.0759,    962.8522,  -7392.1706,   1082.4185,
          -8679.1532,  22850.9823]], dtype=torch.float64)
	q_value: tensor([[-32.4748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2968457068799515, distance: 1.303169321822059 entropy 11.168653885638848
epoch: 47, step: 10
	action: tensor([[-10365.1564, -16843.6708,   -323.5643,    998.3829,   9521.7754,
          -6786.0030,  14206.5459]], dtype=torch.float64)
	q_value: tensor([[-28.4637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15707761508991935, distance: 1.0506316674774585 entropy 10.814479736150375
epoch: 47, step: 11
	action: tensor([[  1376.6455,  12989.7093, -26345.7738, -40198.1665,   6775.7180,
          -2139.5909, -19498.0098]], dtype=torch.float64)
	q_value: tensor([[-32.4609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.305999637686986
epoch: 47, step: 12
	action: tensor([[-2905.5819,   874.0267, -1021.0479,   -40.7257,  3378.2347, -7387.7624,
          8507.2480]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17402670010434074, distance: 1.2399257527584007 entropy 10.594180570233785
epoch: 47, step: 13
	action: tensor([[ -1928.2390, -34220.8047, -13755.2417,  19237.5979, -26287.5875,
         -14301.7845,  16636.5658]], dtype=torch.float64)
	q_value: tensor([[-32.8957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004683565202651785, distance: 1.1416613034056164 entropy 11.158681233071652
epoch: 47, step: 14
	action: tensor([[  8605.8990,  -9267.4703, -19908.9141,   3314.3839, -13153.3829,
          27600.7702,  -5204.5307]], dtype=torch.float64)
	q_value: tensor([[-28.8903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22489895318921116, distance: 1.2665047560456184 entropy 11.155992143153187
epoch: 47, step: 15
	action: tensor([[-29221.8172, -15051.5666, -16864.1920,  -5156.0195, -25556.0078,
         -11836.6562,   5070.5729]], dtype=torch.float64)
	q_value: tensor([[-41.1745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5036002178842145, distance: 1.4032106856765199 entropy 11.259680433622902
epoch: 47, step: 16
	action: tensor([[ -5905.2450, -20964.8946,  10287.7205,   3894.8883,   1485.3008,
         -27775.1346,  28781.9190]], dtype=torch.float64)
	q_value: tensor([[-27.9737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018457169221054226, distance: 1.1548566463052417 entropy 10.995330925637036
epoch: 47, step: 17
	action: tensor([[-13684.9535,  -5592.1878,  -4383.7783,  -8290.9582, -35097.9845,
          22324.9320,   -888.3511]], dtype=torch.float64)
	q_value: tensor([[-31.5412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9491083415852037, distance: 1.5976243947975108 entropy 11.130763433454302
epoch: 47, step: 18
	action: tensor([[  9476.4512,   9267.4726,  -4458.1618,  15264.5237,   4311.9321,
            176.1310, -20623.2380]], dtype=torch.float64)
	q_value: tensor([[-29.9682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.909443252537173
epoch: 47, step: 19
	action: tensor([[-10249.1045, -14560.7316,    540.3644,   9865.7577, -13565.7894,
           4956.9206,   7592.1802]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00010644948811522337, distance: 1.1442833449489895 entropy 10.594180570233785
epoch: 47, step: 20
	action: tensor([[ -3301.5429, -18086.1473,  15659.1022,  26169.5156,  17706.9864,
            389.5995, -14260.0235]], dtype=torch.float64)
	q_value: tensor([[-29.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24010139141763953, distance: 0.9975496438734761 entropy 11.027156265702118
epoch: 47, step: 21
	action: tensor([[ -1025.0262,  -1633.7619, -16690.8569,  39625.0291,  10430.8317,
         -25860.4095,   6867.8202]], dtype=torch.float64)
	q_value: tensor([[-31.9651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17817445555944422, distance: 1.2421141077698894 entropy 11.154089635729068
epoch: 47, step: 22
	action: tensor([[-12526.0042, -18299.8220, -34038.6954,    381.8304, -22375.4623,
         -14665.9874,  11139.7593]], dtype=torch.float64)
	q_value: tensor([[-30.1305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5606810219163936, distance: 1.4295974601902923 entropy 11.07683620305049
epoch: 47, step: 23
	action: tensor([[-24683.0988, -48001.2583, -27302.1244,    481.7106, -30352.6403,
          -4362.9240,  17301.0413]], dtype=torch.float64)
	q_value: tensor([[-30.1898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12323239135346098, distance: 1.212806463364633 entropy 11.198465953452711
epoch: 47, step: 24
	action: tensor([[   418.4388, -24764.3359,   5775.2597,  -7587.6939,   7698.3333,
          19257.5905,   6197.9205]], dtype=torch.float64)
	q_value: tensor([[-29.7678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010148260504431295, distance: 1.1385228953741935 entropy 11.179289918734563
epoch: 47, step: 25
	action: tensor([[-12313.9125, -16662.3919,    799.7889, -11804.1629,   4694.1381,
          26188.0575,  -8950.3888]], dtype=torch.float64)
	q_value: tensor([[-29.8016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5945376287584094, distance: 1.4450207367612398 entropy 10.91162307134773
epoch: 47, step: 26
	action: tensor([[-23067.4199,  -2934.4256, -24273.3048,   5538.2466, -12037.8292,
          -2521.5168, -20727.6867]], dtype=torch.float64)
	q_value: tensor([[-38.0375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9471177588110902, distance: 1.596808376540513 entropy 11.15594073336438
epoch: 47, step: 27
	action: tensor([[  118.4730, -8902.0522,  3424.0034, -2011.6941, 23162.7055,  -764.7775,
         17866.5557]], dtype=torch.float64)
	q_value: tensor([[-26.1180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4704711041532026, distance: 1.3876659779759277 entropy 10.903593485812932
epoch: 47, step: 28
	action: tensor([[  5008.3259, -20332.3201,  23041.1066,  13982.2224,  -9536.6116,
           2040.9509,   4008.4743]], dtype=torch.float64)
	q_value: tensor([[-35.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20164055305530004, distance: 1.2544229227895867 entropy 11.1563103298874
epoch: 47, step: 29
	action: tensor([[  8988.3048, -13649.4376,   1385.4900,  15436.0892, -16898.3628,
           5488.3780,  -8249.1046]], dtype=torch.float64)
	q_value: tensor([[-32.3067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3710056933746926, distance: 1.3399121413582393 entropy 10.847421892524045
epoch: 47, step: 30
	action: tensor([[-18993.3943,   9785.3538,   -604.4666,  31430.5025, -19753.7763,
          30950.9689,  -8867.0014]], dtype=torch.float64)
	q_value: tensor([[-40.6195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19237531161630728, distance: 1.0283986231127598 entropy 11.185429230494858
epoch: 47, step: 31
	action: tensor([[ -3345.6156,  -8932.0107,  -2912.0264,   7840.9117,   -692.7012,
          22207.9917, -19425.5104]], dtype=torch.float64)
	q_value: tensor([[-34.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11822237460162865, distance: 1.0745737581848709 entropy 11.057241940021834
epoch: 47, step: 32
	action: tensor([[ -9471.2687,  -8198.5011,  27572.1807,   4533.0948, -20449.1124,
          37505.3593,  -3049.9339]], dtype=torch.float64)
	q_value: tensor([[-37.9424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20070704892320435, distance: 1.2539355738506044 entropy 11.370514686057493
epoch: 47, step: 33
	action: tensor([[-10770.9348,  -6061.5691,  47546.3391, -39140.4378,  11654.4865,
         -20420.9476,  16136.4665]], dtype=torch.float64)
	q_value: tensor([[-33.5051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.411405539633358, distance: 1.3595106125391105 entropy 11.262406745942782
epoch: 47, step: 34
	action: tensor([[-12080.7788, -16685.7399,   1426.7329,  22873.4227, -16200.2764,
          15193.0323, -32589.2214]], dtype=torch.float64)
	q_value: tensor([[-27.1315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.817718142910282, distance: 1.5428367114908184 entropy 11.014011798324645
epoch: 47, step: 35
	action: tensor([[ -8468.4006,  -7270.9632, -12837.5881,  16375.8356,  -5328.2464,
          28556.2565,  -4180.2175]], dtype=torch.float64)
	q_value: tensor([[-31.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1416842121965758, distance: 1.0601815541321598 entropy 11.139163082194932
epoch: 47, step: 36
	action: tensor([[ -6476.9144, -20244.6514,  14386.5264,  -1754.9368,  -9112.1969,
          13108.0629,  33693.4127]], dtype=torch.float64)
	q_value: tensor([[-33.1982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2920593089558272, distance: 1.3007622301850124 entropy 11.21035161350045
epoch: 47, step: 37
	action: tensor([[-16166.4252, -11564.7846,   1000.6228,  -4290.6815,   3408.4230,
         -17942.6309,     76.0393]], dtype=torch.float64)
	q_value: tensor([[-35.7778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8966733920432217, distance: 1.5759882277355206 entropy 11.185798358799802
epoch: 47, step: 38
	action: tensor([[ -1432.7039,  20028.1295,   5154.2141,   1103.9824, -31084.3404,
         -15290.0965,  -4891.9775]], dtype=torch.float64)
	q_value: tensor([[-35.1361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07189684417737352, distance: 1.1847676557961162 entropy 11.270701871997412
epoch: 47, step: 39
	action: tensor([[-33322.8197, -17669.5101,  17057.5435,  34031.8079,    295.9578,
          -1635.1439,   5015.2642]], dtype=torch.float64)
	q_value: tensor([[-33.8390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4398046181447157, distance: 1.373119941596658 entropy 11.10603338732327
epoch: 47, step: 40
	action: tensor([[   707.2390, -20658.3675,   4447.9403,   6574.0964,  17733.2101,
         -11037.6738,  11329.4823]], dtype=torch.float64)
	q_value: tensor([[-28.8960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1743344335199961, distance: 1.0398214686901837 entropy 11.151056523158555
epoch: 47, step: 41
	action: tensor([[  4151.0155, -16095.4223,  -1153.5870,   5250.1473, -18841.5323,
         -17585.6122,   7749.8447]], dtype=torch.float64)
	q_value: tensor([[-30.4340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4131003119433744, distance: 0.8766750213686043 entropy 10.943202318951124
epoch: 47, step: 42
	action: tensor([[-10602.5244, -15237.4170, -15889.6485,  22938.5200, -10072.1758,
         -13481.5658,  -5639.7806]], dtype=torch.float64)
	q_value: tensor([[-37.7785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01938788950494874, distance: 1.133196747909962 entropy 11.116982813516179
epoch: 47, step: 43
	action: tensor([[-9405.4339,   187.7015, -4107.8163,  7644.9920,  3501.9098,  -419.6987,
           578.8999]], dtype=torch.float64)
	q_value: tensor([[-28.3961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24146112074138282, distance: 0.9966567584277097 entropy 11.053619135366139
epoch: 47, step: 44
	action: tensor([[-2185.4393, 10186.9707, -2896.6566,  5006.1102,   473.9172, 29828.8010,
         -3800.7772]], dtype=torch.float64)
	q_value: tensor([[-28.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7199077910515528, distance: 0.6056297597340641 entropy 11.006488155887292
epoch: 47, step: 45
	action: tensor([[-24708.8095,   4186.4011, -27807.2237,  12105.0322, -24821.2057,
          -6408.0701,  14666.5151]], dtype=torch.float64)
	q_value: tensor([[-33.4214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05657255501933034, distance: 1.1762681995549875 entropy 11.043226151751673
epoch: 47, step: 46
	action: tensor([[-22280.0113, -40380.2513, -12153.5113,  48065.7914,  15305.3687,
          -7601.5096,   2188.4276]], dtype=torch.float64)
	q_value: tensor([[-33.0604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5772025580858378, distance: 1.437144475180143 entropy 11.154522953599137
epoch: 47, step: 47
	action: tensor([[-18982.8543, -18144.3209, -34798.0999, -19055.5711,  11568.5851,
         -16047.0718,  -3348.2534]], dtype=torch.float64)
	q_value: tensor([[-30.9716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14826768604423557, distance: 1.2262478669123882 entropy 11.245922326156286
epoch: 47, step: 48
	action: tensor([[ -8956.0998, -17967.3643, -14179.1151,   2553.5326,   5073.1651,
           3052.5068,   9096.8935]], dtype=torch.float64)
	q_value: tensor([[-29.3914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34478930980048084, distance: 1.3270393999658643 entropy 10.9736123185953
epoch: 47, step: 49
	action: tensor([[ -8416.9990, -16397.2850,  13838.4546,  24473.1766,  16105.5382,
           3213.9097,   7924.7776]], dtype=torch.float64)
	q_value: tensor([[-32.8706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31698850562925807, distance: 1.3132508348096346 entropy 11.198846759993186
epoch: 47, step: 50
	action: tensor([[-30057.9032, -11608.6425,   1793.2917,  11392.5005, -31577.4002,
          37689.1724, -10317.9012]], dtype=torch.float64)
	q_value: tensor([[-33.6209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2142841028397411, distance: 1.2610051142723002 entropy 11.168699867147145
epoch: 47, step: 51
	action: tensor([[ 12729.3896, -12324.9228, -11241.0717,  -8429.5734,  -4576.3083,
         -19839.5546, -21965.8829]], dtype=torch.float64)
	q_value: tensor([[-32.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19072612239675557, distance: 1.2487130025824347 entropy 11.149447980143336
epoch: 47, step: 52
	action: tensor([[-20981.9401, -19511.2431,  28853.4404,  23041.6302, -24811.0452,
          -6828.2932, -38014.8716]], dtype=torch.float64)
	q_value: tensor([[-26.3454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4652191492361015, distance: 1.3851856578163964 entropy 10.851591584776626
epoch: 47, step: 53
	action: tensor([[-31369.7228,   -263.4188, -23097.3317, -19075.2557,   3755.0953,
          14872.2803,  10591.0405]], dtype=torch.float64)
	q_value: tensor([[-27.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3977117823367493, distance: 1.3528994067938598 entropy 10.99438098765451
epoch: 47, step: 54
	action: tensor([[-20736.7351,   3330.8954,  -5065.7766,   2547.3634,  -9290.5114,
          -4568.6142,   5312.3376]], dtype=torch.float64)
	q_value: tensor([[-32.7773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4916654692546074, distance: 1.3976306348093113 entropy 11.041690856318894
epoch: 47, step: 55
	action: tensor([[-4182.6756, 14251.1252, -9834.2666, 18788.6935, -3122.6926, -3680.3360,
          8654.0825]], dtype=torch.float64)
	q_value: tensor([[-34.3417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12566667398926623, distance: 1.070028165408776 entropy 11.16886931656504
epoch: 47, step: 56
	action: tensor([[ 20812.5108, -16186.5268,   9075.6487, -15318.1059, -12623.7784,
          17177.4613,  29133.8378]], dtype=torch.float64)
	q_value: tensor([[-32.7948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15677779881849263, distance: 1.230783500898758 entropy 11.072324315093294
epoch: 47, step: 57
	action: tensor([[-13670.4517,   3661.5800,  12648.8653, -20079.0905, -24286.2439,
           7837.1683,   1436.7690]], dtype=torch.float64)
	q_value: tensor([[-36.3949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.183141825086677
epoch: 47, step: 58
	action: tensor([[-4010.4804, -6560.9831,  -804.5482, 23468.9005,  5546.4251,  -584.0087,
         -4375.3244]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.061547813358216574, distance: 1.1790343915040253 entropy 10.594180570233785
epoch: 47, step: 59
	action: tensor([[  9725.6916,  -5231.3537,     83.9424,   5459.8629,   1984.2337,
         -38025.4691,   9261.2178]], dtype=torch.float64)
	q_value: tensor([[-28.3514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4009745261452966, distance: 0.8856851008368137 entropy 11.095319190403568
epoch: 47, step: 60
	action: tensor([[-19187.9386, -30725.4140,  16678.2609,  12059.3226,  15810.8681,
         -15771.5025,  -8794.4473]], dtype=torch.float64)
	q_value: tensor([[-29.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22533296477856746, distance: 1.0071965536805725 entropy 11.084970955877461
epoch: 47, step: 61
	action: tensor([[ 10436.5625,  -1890.0835,   4659.5899,   7978.4292, -12622.6199,
         -14722.5836,   4059.0947]], dtype=torch.float64)
	q_value: tensor([[-34.0583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21503821184203953, distance: 1.2613966156153391 entropy 11.19108479396635
epoch: 47, step: 62
	action: tensor([[ -6002.2932, -39891.6482, -16226.6185,  -5626.0498,  -7227.9169,
         -36825.1418,  13662.1902]], dtype=torch.float64)
	q_value: tensor([[-28.9361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.792077670132215, distance: 1.5319165478916437 entropy 11.044731663243057
epoch: 47, step: 63
	action: tensor([[  4845.9628,   4631.7238,   8166.9503,   3347.8754, -13646.1349,
          -7389.9204,   5271.1476]], dtype=torch.float64)
	q_value: tensor([[-29.1504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.167567641038923
epoch: 47, step: 64
	action: tensor([[ -1214.4483,  13313.9938,   -572.0106,  -4592.6406, -15959.1960,
         -13192.2956,   8972.7323]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10272549887859894, distance: 1.0839752252359687 entropy 10.594180570233785
epoch: 47, step: 65
	action: tensor([[ -2652.0163, -20591.7683, -35429.1990,  -2926.5449,  -3598.7484,
           7291.5629,   7005.9347]], dtype=torch.float64)
	q_value: tensor([[-44.6492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027575448423963533, distance: 1.1284560542783109 entropy 10.934839286292313
epoch: 47, step: 66
	action: tensor([[-10747.2629, -23795.8006,  -5250.2258,  11454.3064,    703.3432,
           1876.6364,  -1770.8238]], dtype=torch.float64)
	q_value: tensor([[-32.5457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16506674946540456, distance: 1.2351852508829897 entropy 10.952045221209707
epoch: 47, step: 67
	action: tensor([[  -222.0613,  -7929.7948, -14539.6217,   6603.4127,   7039.2100,
          -3869.4374,   7093.8671]], dtype=torch.float64)
	q_value: tensor([[-28.4906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4160240667933084, distance: 1.3617331518004887 entropy 10.960022594427146
epoch: 47, step: 68
	action: tensor([[ -3429.4199, -13805.1597, -14712.2393,  13052.7523,  -4262.8147,
         -31287.2765,  19578.6112]], dtype=torch.float64)
	q_value: tensor([[-27.9643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03300083071451765, distance: 1.1253036920571864 entropy 11.029910106121886
epoch: 47, step: 69
	action: tensor([[-12378.7570,   3071.4028, -19333.4139,  -3560.4271,  -4148.6825,
          23980.3000,   2754.8764]], dtype=torch.float64)
	q_value: tensor([[-30.9753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6346942005230136, distance: 1.463103179883165 entropy 11.256664314528267
epoch: 47, step: 70
	action: tensor([[ 3674.2778,  8069.6247, -2883.1126,  5824.0089,  9377.7023, -7330.3746,
         14226.0736]], dtype=torch.float64)
	q_value: tensor([[-26.8777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7051279502001818, distance: 0.6214032175743893 entropy 10.793940006708448
epoch: 47, step: 71
	action: tensor([[-20000.2468,   8606.2360, -14279.8599,   8340.5688,  16777.5810,
          13123.0541, -35207.9250]], dtype=torch.float64)
	q_value: tensor([[-31.6278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.98787304985214
epoch: 47, step: 72
	action: tensor([[  6368.8498,  -2747.7344, -14113.2922, -11082.5528,  15288.8953,
         -14137.1704,  15796.1544]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19675911861833217, distance: 1.2518724035559479 entropy 10.594180570233785
epoch: 47, step: 73
	action: tensor([[  9392.9017, -39761.7374,   3835.8957,    227.9806, -21007.2649,
          10441.2169,  28657.0340]], dtype=torch.float64)
	q_value: tensor([[-33.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13712175235065593, distance: 1.0629955678870777 entropy 10.999007741310143
epoch: 47, step: 74
	action: tensor([[ 24991.6985, -12723.9153, -10308.0457,    784.3260,  -5271.1105,
          -3587.3087, -14747.0662]], dtype=torch.float64)
	q_value: tensor([[-34.8052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.529157777909476, distance: 1.4150860259346596 entropy 11.157982671626371
epoch: 47, step: 75
	action: tensor([[-12617.2273,   4069.3494, -20185.3638,   7707.8077,  -8860.5708,
         -18786.6134, -12684.8164]], dtype=torch.float64)
	q_value: tensor([[-36.4308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0980206726337951, distance: 1.0868134019851259 entropy 10.973956765803434
epoch: 47, step: 76
	action: tensor([[-20488.1757, -21728.7478,   3060.3981,   -694.0038,   6716.0606,
          18036.6506, -30458.5248]], dtype=torch.float64)
	q_value: tensor([[-30.8233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9718928462953131, distance: 1.6069351441161417 entropy 11.046089212599968
epoch: 47, step: 77
	action: tensor([[ -5763.1723, -12900.1762,   9666.9562,   5456.7667, -21488.6646,
           -920.6613,  24077.2164]], dtype=torch.float64)
	q_value: tensor([[-26.7264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14786529212331057, distance: 1.2260329876076574 entropy 10.908223109563908
epoch: 47, step: 78
	action: tensor([[ -1102.7694,  -4349.7237,  20436.8019,   5313.3074, -10464.5875,
          10968.7231,   8784.9833]], dtype=torch.float64)
	q_value: tensor([[-27.9183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2997855990937872, distance: 1.3046455994170638 entropy 11.019502011105663
epoch: 47, step: 79
	action: tensor([[-29371.8816, -49748.5979, -18350.9737,   6996.3238, -17142.6085,
           9876.5839,  14381.3269]], dtype=torch.float64)
	q_value: tensor([[-35.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2031105990006854, distance: 1.2551899973035507 entropy 11.28477924177038
epoch: 47, step: 80
	action: tensor([[ -8428.5032,  -7777.1666,  -2044.3503,  21450.7894, -24280.8007,
          25743.5377,  -4201.7483]], dtype=torch.float64)
	q_value: tensor([[-34.0315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28031901090274003, distance: 1.294839055670226 entropy 11.183253989362624
epoch: 47, step: 81
	action: tensor([[-43229.7582, -17576.8768,  17563.1773,  27893.3617,  34456.5399,
         -21154.6153,  21837.3189]], dtype=torch.float64)
	q_value: tensor([[-36.1832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16233928299013511, distance: 1.233738595413589 entropy 11.298873913122561
epoch: 47, step: 82
	action: tensor([[  4571.5595, -25201.2089,   7044.0661,  -1070.8318, -10223.1506,
           8443.1397,  -1690.0124]], dtype=torch.float64)
	q_value: tensor([[-28.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11550395185793272, distance: 1.0762288794600599 entropy 11.098768951184974
epoch: 47, step: 83
	action: tensor([[ -3907.1746, -11831.8800,  11226.0263,  -5281.7125, -22410.6188,
           7218.4368,  -6994.7621]], dtype=torch.float64)
	q_value: tensor([[-24.3754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09706094122371434, distance: 1.0873914482974358 entropy 10.536013701016143
epoch: 47, step: 84
	action: tensor([[-25224.2707, -15658.4900, -18484.7157,  17167.8778,  21997.0073,
           1976.4658,  17814.2453]], dtype=torch.float64)
	q_value: tensor([[-34.3952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6205319671905545, distance: 1.456751568997109 entropy 10.971957411751804
epoch: 47, step: 85
	action: tensor([[  7865.7421,  -4681.7116, -34304.5504,  11276.2487,   1620.2772,
           3993.9235,  10884.2689]], dtype=torch.float64)
	q_value: tensor([[-26.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6131921880749535, distance: 0.711712037821989 entropy 10.777403891259665
epoch: 47, step: 86
	action: tensor([[-20826.2425,   5365.9330,  12249.0810,   9692.8131,  -2608.9209,
          13618.7412,  -7298.0303]], dtype=torch.float64)
	q_value: tensor([[-33.4568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25169972338477, distance: 0.9899075717305655 entropy 10.946473618156508
epoch: 47, step: 87
	action: tensor([[-2815.8953, 38214.9766, -6565.2702, 17207.9851,  3421.7358, 41496.9718,
          1591.6487]], dtype=torch.float64)
	q_value: tensor([[-42.6083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6081856052255037, distance: 0.7163031928149435 entropy 11.31448866010913
epoch: 47, step: 88
	action: tensor([[-20793.7231, -17801.1725,   6036.4334,  46142.6499,  17604.1992,
         -12024.0625,   8036.7919]], dtype=torch.float64)
	q_value: tensor([[-34.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4326872378970834, distance: 1.369721867892957 entropy 11.18370617968309
epoch: 47, step: 89
	action: tensor([[-19347.9508, -29227.2118,  18890.2464,  21925.6001,   6353.2988,
         -40799.6162, -19088.0010]], dtype=torch.float64)
	q_value: tensor([[-30.2661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09979306691225176, distance: 1.2000854823851292 entropy 11.222915865804035
epoch: 47, step: 90
	action: tensor([[-20913.0705, -26374.9270,  15996.2823,  37739.9764,   7263.0677,
           8113.7656,   7390.3036]], dtype=torch.float64)
	q_value: tensor([[-31.6234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20463813249157325, distance: 1.0205612537111215 entropy 11.122382533554248
epoch: 47, step: 91
	action: tensor([[ 5167.3950, 27914.7675,  5691.9669, 23841.5698, -1778.1964, -6406.4409,
         12175.1435]], dtype=torch.float64)
	q_value: tensor([[-30.6763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21336630043804716, distance: 1.0149460720488765 entropy 11.067209027404607
epoch: 47, step: 92
	action: tensor([[-10202.1690,  -7950.4768, -16261.1882,   2999.4667, -11246.1404,
         -11528.5309,  18460.4065]], dtype=torch.float64)
	q_value: tensor([[-26.7058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.742641398727004
epoch: 47, step: 93
	action: tensor([[-12065.9625,   6930.1885,   5408.5830,  -8961.0597,  11327.6374,
           4540.0900,   1215.1899]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.594180570233785
epoch: 47, step: 94
	action: tensor([[ -4866.6564,   6707.2535,   1803.0901,  15083.9876,   7062.7540,
         -11173.1009,   5586.4076]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42008441993911994, distance: 0.8714431924384708 entropy 10.594180570233785
epoch: 47, step: 95
	action: tensor([[-20066.8237,  -8525.3478, -17546.8508,   1210.2732,   -862.0722,
         -15065.0592,  -3376.7844]], dtype=torch.float64)
	q_value: tensor([[-29.9235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02021693061508234, distance: 1.132717626079999 entropy 10.867078928133427
epoch: 47, step: 96
	action: tensor([[-13297.2344,   2568.5810, -11858.8531,   2896.1313,  21124.3139,
            532.6408,   7274.3611]], dtype=torch.float64)
	q_value: tensor([[-29.8107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10852206465420478, distance: 1.0804682078840664 entropy 11.175639998957902
epoch: 47, step: 97
	action: tensor([[-5604.8091, 11713.3637, 31417.0418, 22977.3041,  9443.9141, 18880.5260,
          7629.1576]], dtype=torch.float64)
	q_value: tensor([[-33.8057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11534661607131624, distance: 1.0763245959916383 entropy 11.138241601163916
epoch: 47, step: 98
	action: tensor([[  4905.6034, -19028.6298, -15045.5882,  20937.6396,  11836.4328,
          -7654.6527,  27938.7598]], dtype=torch.float64)
	q_value: tensor([[-32.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30649129640436956, distance: 1.3080066552246943 entropy 11.012992672384476
epoch: 47, step: 99
	action: tensor([[-23834.0583, -14224.7525,  15833.1548,   6676.1258, -24784.8885,
         -20480.7765,  51716.2983]], dtype=torch.float64)
	q_value: tensor([[-35.1570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08229157850837199, distance: 1.190498445999484 entropy 11.127809205071896
epoch: 47, step: 100
	action: tensor([[  2793.2908, -14111.0775,  22054.8198,   1336.0179,  -8673.4700,
          27162.7446,  10630.5279]], dtype=torch.float64)
	q_value: tensor([[-30.2033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.139376547063398
epoch: 47, step: 101
	action: tensor([[ -8081.8635,  -7613.2628,   8527.7859, -15160.1553,  13713.8244,
           4126.1634, -11054.4674]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023377982949985388, distance: 1.1576432075857184 entropy 10.594180570233785
epoch: 47, step: 102
	action: tensor([[-15070.9948,   6851.1610, -23636.2156,  33956.2931,  22120.1622,
          11819.6191,  23615.3373]], dtype=torch.float64)
	q_value: tensor([[-34.6262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19341813966735144, distance: 1.0277344598181093 entropy 11.0203253779071
epoch: 47, step: 103
	action: tensor([[  9492.3079,  -8760.5081, -33422.1390,   9514.0388,  10079.2672,
         -31693.0644,  -8347.3556]], dtype=torch.float64)
	q_value: tensor([[-39.6668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.176158418146299
epoch: 47, step: 104
	action: tensor([[-8562.1744, -1534.5918, -2772.8156, -8781.6464,  8416.5784, -8081.2284,
          2573.6424]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47856018881845963, distance: 1.3914775294959079 entropy 10.594180570233785
epoch: 47, step: 105
	action: tensor([[  4033.6723,  15486.7113, -13612.1018,  14085.2860,  17743.2563,
          -9079.2616,    444.7306]], dtype=torch.float64)
	q_value: tensor([[-38.9180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.205052740436317
epoch: 47, step: 106
	action: tensor([[-21026.1049,  -9425.8924,  -2295.5116,   2770.7847,  -6229.2531,
           3535.2953,  -4315.5031]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5688505374588757, distance: 1.4333342502669724 entropy 10.594180570233785
epoch: 47, step: 107
	action: tensor([[-25594.5518, -49480.1909,  22597.0232,  -4240.3740,  -1399.5938,
         -50513.1543, -14516.1769]], dtype=torch.float64)
	q_value: tensor([[-34.7153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5530334159712762, distance: 1.4260905218116742 entropy 11.222794041227257
epoch: 47, step: 108
	action: tensor([[-32340.4977, -21862.5321,   2203.7549,   7481.1268,   2079.2947,
           1348.7337,   5246.9864]], dtype=torch.float64)
	q_value: tensor([[-29.3624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19847665660595992, distance: 1.0245066479614244 entropy 11.016739722151096
epoch: 47, step: 109
	action: tensor([[-19233.5347,   2471.1198,   6964.3991,  31191.7170,   5601.7189,
           -649.4051,  11562.2736]], dtype=torch.float64)
	q_value: tensor([[-29.1870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09719798424620918, distance: 1.1986687793510495 entropy 11.042660789905081
epoch: 47, step: 110
	action: tensor([[  5698.9268,  -7121.6026, -10733.0052, -24011.8869,  -6909.6307,
           4978.4493,  33403.0467]], dtype=torch.float64)
	q_value: tensor([[-33.6082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35663505831633235, distance: 0.9178789054059496 entropy 11.204646547434008
epoch: 47, step: 111
	action: tensor([[  1839.0186, -11386.4625,   7148.0725,  -6704.0512,  18622.1345,
          -1178.5934,  22715.5083]], dtype=torch.float64)
	q_value: tensor([[-30.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5683140932125672, distance: 0.7518663157968722 entropy 10.937149267385124
epoch: 47, step: 112
	action: tensor([[  3746.6746, -23145.1466,    472.7201,  10331.8044,   2157.7509,
          -1435.0420,   5454.0937]], dtype=torch.float64)
	q_value: tensor([[-31.4726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3089542112194714, distance: 1.3092389604690386 entropy 11.087602780465675
epoch: 47, step: 113
	action: tensor([[-8145.1666,  3211.7198, 13603.2377, 21970.1816, 17869.0456,  7811.7089,
           148.6003]], dtype=torch.float64)
	q_value: tensor([[-38.7746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28280836782172103, distance: 0.9691127339866664 entropy 11.047088729096233
epoch: 47, step: 114
	action: tensor([[ -3899.1918, -16768.0903,  -2015.6264,  11183.3963, -20711.0638,
         -15250.2683,  -5196.5876]], dtype=torch.float64)
	q_value: tensor([[-28.7509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41292498823559, distance: 0.8768059555310528 entropy 10.90606007371707
epoch: 47, step: 115
	action: tensor([[ -8804.3022, -14099.2400,  19103.3932,  -1489.1433,  12697.1251,
          14325.3331,   7892.7936]], dtype=torch.float64)
	q_value: tensor([[-25.8732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5570698178092242, distance: 1.4279425550630074 entropy 11.02751589747244
epoch: 47, step: 116
	action: tensor([[  2206.6068,   9422.1113,  -2475.2487, -18684.8962,  -5942.4513,
          10595.1028,  12561.4307]], dtype=torch.float64)
	q_value: tensor([[-29.1272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.957298832211565
epoch: 47, step: 117
	action: tensor([[-15401.9541, -11821.3537,  -4943.2661,  19032.6347,  -3261.1392,
          15707.7807,  11329.6825]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12206299502528939, distance: 1.0722310282942011 entropy 10.594180570233785
epoch: 47, step: 118
	action: tensor([[ -9983.2463, -12628.7194, -22188.6381,  18696.8753, -33218.9034,
          26963.6826,   1839.0889]], dtype=torch.float64)
	q_value: tensor([[-30.0077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0409171846217431, distance: 1.1206880679882014 entropy 11.012080306328386
epoch: 47, step: 119
	action: tensor([[ 16688.8924,   9946.8637, -16961.1116,    980.2075,   2151.6199,
          14232.2954,   8128.8733]], dtype=torch.float64)
	q_value: tensor([[-32.0446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.137075616343777
epoch: 47, step: 120
	action: tensor([[ -3330.0887, -13971.5078,  -3355.9418, -11952.9382,   8982.4723,
           4406.6263,   2648.0885]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07383929385901544, distance: 1.1012853676825274 entropy 10.594180570233785
epoch: 47, step: 121
	action: tensor([[18584.8974, 12322.3890, 24031.0274, 12501.3941, 19326.7142, 16788.0844,
         22290.7571]], dtype=torch.float64)
	q_value: tensor([[-36.3297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.08097577879795
epoch: 47, step: 122
	action: tensor([[ -8065.2387, -22253.0006,  -2393.2028,   9500.7289,  -1194.0469,
          -3620.9333,   1033.4669]], dtype=torch.float64)
	q_value: tensor([[-36.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.853368992089595, distance: 1.557893049218745 entropy 10.594180570233785
epoch: 47, step: 123
	action: tensor([[ -4944.1564,  -3000.9480, -12499.7430,  -5530.3354,  -9848.8284,
          -1213.3006,  -8545.8865]], dtype=torch.float64)
	q_value: tensor([[-25.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9744426104269883, distance: 1.6079737355249701 entropy 10.935434358999439
epoch: 47, step: 124
	action: tensor([[ 17379.7571, -10933.0585,    -63.0396, -28023.8176,  15579.3774,
          12140.0121,  15992.8333]], dtype=torch.float64)
	q_value: tensor([[-33.0765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42079142337764774, distance: 0.8709118209313467 entropy 11.137816015284514
epoch: 47, step: 125
	action: tensor([[   403.7158, -10136.8584,  -7636.2842,  27657.4001,  -4923.1012,
           1617.5363,  11227.0403]], dtype=torch.float64)
	q_value: tensor([[-33.6432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16846687855337228, distance: 1.2369863191912391 entropy 11.0619672390149
epoch: 47, step: 126
	action: tensor([[-21388.2412,   -881.3088, -10781.2014, -38767.3096,  -1882.5428,
          -1525.6155,  37684.7431]], dtype=torch.float64)
	q_value: tensor([[-33.7879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5035770875482048, distance: 1.403199892628245 entropy 11.035642253222525
epoch: 47, step: 127
	action: tensor([[ -7411.8015, -18453.6548, -28808.0644,  -4272.3403, -10521.7818,
          23212.9672, -14918.3444]], dtype=torch.float64)
	q_value: tensor([[-34.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.253057059303776, distance: 0.9890093715013193 entropy 11.071498857829395
LOSS epoch 47 actor 437.1534368293883 critic 144.00726039429256
epoch: 48, step: 0
	action: tensor([[-17768.3178, -28710.5751,  -2367.7140,   5188.1958,   5418.8171,
            549.4905,  13263.7111]], dtype=torch.float64)
	q_value: tensor([[-37.0301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3653804588249523, distance: 1.3371604870751896 entropy 11.174012730091691
epoch: 48, step: 1
	action: tensor([[-10119.3466, -17634.1752,   9603.5255,   3845.9439,  23154.8271,
          -4084.8189,  14955.8543]], dtype=torch.float64)
	q_value: tensor([[-31.9980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06541471944561894, distance: 1.1062828035959542 entropy 11.170457812474535
epoch: 48, step: 2
	action: tensor([[-13654.2091,  -6524.0466,  -3723.2546,  22061.1355, -22348.6244,
           8047.2867,  45883.6165]], dtype=torch.float64)
	q_value: tensor([[-31.0625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02709302218091203, distance: 1.1597425267079364 entropy 11.19548950767982
epoch: 48, step: 3
	action: tensor([[-38330.0671,  21587.1673,  -7106.4921,  21151.1730,   6134.3715,
          10935.6113, -15791.5851]], dtype=torch.float64)
	q_value: tensor([[-35.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4561444760137148, distance: 0.8439145318159758 entropy 11.269935802617619
epoch: 48, step: 4
	action: tensor([[-3928.1855, 11078.1898, -7130.6068, 20229.8558, -8399.9116, 16481.1060,
         15200.5968]], dtype=torch.float64)
	q_value: tensor([[-35.3276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17069855971291603, distance: 1.0421084156315734 entropy 11.108010599702226
epoch: 48, step: 5
	action: tensor([[ -9974.4286,  17059.0927, -18551.5659,  22698.2031,  33914.1701,
          -2703.5168,   1651.2915]], dtype=torch.float64)
	q_value: tensor([[-32.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1428023539697768, distance: 1.0594907713094937 entropy 11.103583103300512
epoch: 48, step: 6
	action: tensor([[ 19292.8879, -17126.3413,  -2857.6984, -40147.8543,   6370.6756,
         -26640.1231,   3015.0983]], dtype=torch.float64)
	q_value: tensor([[-36.5856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13172140770489915, distance: 1.217380830426236 entropy 11.331454680746209
epoch: 48, step: 7
	action: tensor([[-4811.3945, -3251.3280, -6645.6498,  5160.0789,  3207.2003, -1019.0110,
         23853.0176]], dtype=torch.float64)
	q_value: tensor([[-26.9430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1573805173856222, distance: 1.0504428794321712 entropy 10.701568406601172
epoch: 48, step: 8
	action: tensor([[ -2928.0599, -25649.2783,  11815.3909,  15612.6066,  30556.3947,
          12125.6963,   5126.0886]], dtype=torch.float64)
	q_value: tensor([[-30.9425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22546026005303155, distance: 1.2667949091181026 entropy 11.280233602845609
epoch: 48, step: 9
	action: tensor([[-26352.6719, -12418.2850,  20603.1337,   4723.9304,  11633.9504,
          -2489.3665,   4996.7891]], dtype=torch.float64)
	q_value: tensor([[-33.8890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44555816133309045, distance: 0.8520884741262248 entropy 11.20400575588943
epoch: 48, step: 10
	action: tensor([[-30035.3283,   6372.5585,  -3276.3123,  12229.2983,  15097.7014,
          -2506.0660,   5123.8549]], dtype=torch.float64)
	q_value: tensor([[-27.7532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21099378555974502, distance: 1.2592954973025312 entropy 11.18462434761534
epoch: 48, step: 11
	action: tensor([[-37238.9580,   7172.4515,  -4070.1020, -13240.0986,  -4868.9900,
            414.6104,   9352.4576]], dtype=torch.float64)
	q_value: tensor([[-33.2032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0061435850461339925, distance: 1.1408236502639975 entropy 11.17617024228193
epoch: 48, step: 12
	action: tensor([[-12681.2329,   2264.1230,  -5880.0012,   5787.0439, -10776.7914,
         -32300.2523,  20137.9263]], dtype=torch.float64)
	q_value: tensor([[-31.6495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09278133723054971, distance: 1.196253793445557 entropy 11.001457635358415
epoch: 48, step: 13
	action: tensor([[  7521.9731,    642.4200, -19161.6739,  24839.1591,  -2852.1271,
          -8719.3662,  17171.1583]], dtype=torch.float64)
	q_value: tensor([[-28.1713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7532974032422266, distance: 0.5683862375051407 entropy 10.910119267531817
epoch: 48, step: 14
	action: tensor([[-22357.0380,   2299.8068, -21682.2683,   1233.1875,  13033.3727,
         -15467.1316,   9243.4524]], dtype=torch.float64)
	q_value: tensor([[-28.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39217778063435904, distance: 0.8921645841458614 entropy 10.87573879229578
epoch: 48, step: 15
	action: tensor([[  5265.2349, -47108.6208, -23113.3720,   6655.5152, -14507.5998,
          15860.1306,   6794.5801]], dtype=torch.float64)
	q_value: tensor([[-30.4272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21162510605632756, distance: 1.0160687302546962 entropy 11.020911166380829
epoch: 48, step: 16
	action: tensor([[-11416.5781, -30271.3524,  19554.6136,  21616.4738, -10535.3161,
           6000.5788, -20364.3231]], dtype=torch.float64)
	q_value: tensor([[-38.4680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07791025317136135, distance: 1.1880863185168755 entropy 11.1131774375765
epoch: 48, step: 17
	action: tensor([[-35719.1778, -16835.1283,  10491.4431,  10817.8060,  32527.7419,
          -5149.1941,   2368.6694]], dtype=torch.float64)
	q_value: tensor([[-30.2500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.087399539670518, distance: 1.6533297669117808 entropy 11.058257738192875
epoch: 48, step: 18
	action: tensor([[ -6107.7708,  12025.7199, -12988.2336,   9628.3094, -27013.5226,
          -8977.4651,  -4545.0787]], dtype=torch.float64)
	q_value: tensor([[-26.8765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3092168149211766, distance: 0.9511030448514368 entropy 11.032331262057436
epoch: 48, step: 19
	action: tensor([[-19459.3738,  -8195.1142,  24811.6949,  16475.3511, -18278.4557,
          29431.9117, -22393.1473]], dtype=torch.float64)
	q_value: tensor([[-36.4395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.050399997798736385, distance: 1.1728272536362785 entropy 11.246598453401536
epoch: 48, step: 20
	action: tensor([[  4656.5798, -23295.9704, -25288.6483,  -1534.8510,  15588.8567,
          -2474.5828,  -4119.2847]], dtype=torch.float64)
	q_value: tensor([[-30.9025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6677932746092576, distance: 1.4778413106014443 entropy 11.078867326008611
epoch: 48, step: 21
	action: tensor([[ -2046.5314, -11998.2025,  -9535.1018,  12922.6256,  -6564.9418,
          -1184.8999,   8501.1431]], dtype=torch.float64)
	q_value: tensor([[-27.2055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6138507244211611, distance: 1.4537454686239841 entropy 10.816203105328714
epoch: 48, step: 22
	action: tensor([[-16921.2080, -11239.3661, -14761.5798,  -4913.9077,   5946.9755,
           9523.9928, -11882.1169]], dtype=torch.float64)
	q_value: tensor([[-30.1836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8704456347818397, distance: 1.5650536803412736 entropy 11.102122568561269
epoch: 48, step: 23
	action: tensor([[-18483.7548,  -6873.5807,   3297.4365,  29255.9273, -27134.6038,
          20698.2984,    781.2600]], dtype=torch.float64)
	q_value: tensor([[-36.4889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.095241780414368, distance: 1.6564325880740627 entropy 11.237359204713826
epoch: 48, step: 24
	action: tensor([[ -6990.8220, -30391.2684, -33893.3030,  26050.7945,  13915.6538,
          -2624.6459,  22340.7701]], dtype=torch.float64)
	q_value: tensor([[-32.9545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7351992483225671, distance: 1.5074099191161812 entropy 11.192307405467842
epoch: 48, step: 25
	action: tensor([[ -9188.8268, -14316.5956,  21062.4486,  -3377.5117, -32469.4328,
         -18574.9275, -24175.8056]], dtype=torch.float64)
	q_value: tensor([[-31.9284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0580084060998636, distance: 1.6416488449329456 entropy 11.350348113173911
epoch: 48, step: 26
	action: tensor([[-17882.9213,  -7618.9548,   8139.2329,  -2734.4055,   5597.8789,
          19483.2354,  -4135.8880]], dtype=torch.float64)
	q_value: tensor([[-28.8159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0307441944875109, distance: 1.6307384207308213 entropy 11.066197327767567
epoch: 48, step: 27
	action: tensor([[-27882.3194, -11269.5671,  11199.0883,   8391.8184,   -819.1548,
         -24545.1775,  29714.9913]], dtype=torch.float64)
	q_value: tensor([[-30.4892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17863370367711684, distance: 1.2423561699642225 entropy 10.937011837801396
epoch: 48, step: 28
	action: tensor([[-40238.6544,  -2545.8901,    737.1771,   8193.8950,  -6614.1760,
           5754.1323,   5223.2365]], dtype=torch.float64)
	q_value: tensor([[-28.9839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10523692753400749, distance: 1.2030519647651785 entropy 11.187782736096242
epoch: 48, step: 29
	action: tensor([[-7242.5080, 13100.9366, -8120.4297, 24785.6794,  1562.7056, 17336.8645,
         19700.9479]], dtype=torch.float64)
	q_value: tensor([[-31.4783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21181808852882988, distance: 1.2597240149570452 entropy 11.195659114030375
epoch: 48, step: 30
	action: tensor([[-22701.7280,  -1705.6937,    413.3653,   9820.2254,  17742.7186,
         -47029.4812,  51178.1462]], dtype=torch.float64)
	q_value: tensor([[-44.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0300800472739673, distance: 1.630471735505904 entropy 11.411371889861817
epoch: 48, step: 31
	action: tensor([[-26816.6344,  -2058.5733, -13927.5227, -13001.0272,  -8617.3399,
           8542.7154,  11313.0802]], dtype=torch.float64)
	q_value: tensor([[-28.2746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0171085135698736, distance: 1.6252543027302522 entropy 11.168858288388003
epoch: 48, step: 32
	action: tensor([[ -8071.7593, -20344.9556,  37880.4704,  -3806.8925, -29350.6480,
          59903.6034,  37466.8118]], dtype=torch.float64)
	q_value: tensor([[-41.0401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018468901209881095, distance: 1.1548632978987807 entropy 11.421122610208224
epoch: 48, step: 33
	action: tensor([[-21757.3803,  -6418.0335, -10388.6648,  21675.9828,  -3838.8206,
          25062.8978,  21676.1738]], dtype=torch.float64)
	q_value: tensor([[-36.6207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3034246081315415, distance: 1.3064706307141178 entropy 11.108838810354845
epoch: 48, step: 34
	action: tensor([[ -5969.3572, -19979.5398,    344.0794,   3250.1784,  14833.2856,
           3052.3367, -15579.7384]], dtype=torch.float64)
	q_value: tensor([[-32.5495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08385948491774164, distance: 1.1913604662913733 entropy 11.09013711958267
epoch: 48, step: 35
	action: tensor([[ 22655.6256,  17976.7681,  24141.2829, -13282.8191, -19236.4297,
            789.4888, -31595.2997]], dtype=torch.float64)
	q_value: tensor([[-34.5189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.792716835777973, distance: 0.5210011813942765 entropy 11.287879943717765
epoch: 48, step: 36
	action: tensor([[ -9892.6373,   2722.5658, -22164.7223, -18376.8968, -22795.7792,
          -1384.9605,  -5494.9101]], dtype=torch.float64)
	q_value: tensor([[-38.4380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.086370914184187
epoch: 48, step: 37
	action: tensor([[ 13389.1284,  -2292.8501, -11380.0742, -16840.1867, -20860.0476,
         -11395.6495,   3671.3470]], dtype=torch.float64)
	q_value: tensor([[-36.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36029953965709194, distance: 0.9152611428235576 entropy 10.639766172775174
epoch: 48, step: 38
	action: tensor([[  2835.8070, -33212.8450,  -4565.5167,    198.0564, -12286.1714,
         -13704.1622,   -157.4859]], dtype=torch.float64)
	q_value: tensor([[-29.8876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14612903286987577, distance: 1.057432895927499 entropy 10.957549727501391
epoch: 48, step: 39
	action: tensor([[ -4637.6777, -15755.4751, -22095.7053,  31329.9184,   7917.9442,
         -19925.5569, -10933.7918]], dtype=torch.float64)
	q_value: tensor([[-30.4568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22439093918492425, distance: 1.0078087629378962 entropy 11.135175229981057
epoch: 48, step: 40
	action: tensor([[  4524.5399,  -8593.5994,   9292.0938,   3029.0038, -10173.4378,
          10575.4822, -12034.8267]], dtype=torch.float64)
	q_value: tensor([[-30.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6291442766244162, distance: 0.696881902202616 entropy 11.03441939979695
epoch: 48, step: 41
	action: tensor([[-14871.5722, -13605.0326,  17603.3814,  -3643.3480,  13992.7804,
          28907.4680,  -2302.9966]], dtype=torch.float64)
	q_value: tensor([[-36.5163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.683352293026602, distance: 1.4847187759981506 entropy 11.124358027917314
epoch: 48, step: 42
	action: tensor([[ 8630.1726,  4902.8396, 20141.5682,   940.9858, -3755.5512, 29137.8633,
          -897.1972]], dtype=torch.float64)
	q_value: tensor([[-27.7465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.034025838627288
epoch: 48, step: 43
	action: tensor([[-4.1335e+03, -1.3850e+04,  1.0722e+04, -3.9886e+00, -2.8159e+03,
          4.0780e+04, -1.5618e+04]], dtype=torch.float64)
	q_value: tensor([[-36.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.639766172775174
epoch: 48, step: 44
	action: tensor([[ -8394.4385, -11970.3733,  -5052.0705,  12611.0444,   3490.5823,
           1781.2551,   1208.8296]], dtype=torch.float64)
	q_value: tensor([[-36.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.639766172775174
epoch: 48, step: 45
	action: tensor([[11410.2537, -7010.3068, -9912.1445, -2161.1762,  3990.7594,  4078.0493,
          2881.1765]], dtype=torch.float64)
	q_value: tensor([[-36.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.639766172775174
epoch: 48, step: 46
	action: tensor([[ -2719.9291,  -5037.4696,  -4694.0954, -14584.7189,   8256.9979,
           -746.6095, -13921.3685]], dtype=torch.float64)
	q_value: tensor([[-36.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009457067986375689, distance: 1.1497425916270159 entropy 10.639766172775174
epoch: 48, step: 47
	action: tensor([[-17760.8484,  -4467.9423,  19027.7363,  -3167.9881, -11728.5801,
          22082.0219,  25492.4813]], dtype=torch.float64)
	q_value: tensor([[-33.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15970332450547353, distance: 1.0489940284907813 entropy 11.157826422898664
epoch: 48, step: 48
	action: tensor([[  7973.6488,  14333.7407,  -7907.0561,  26193.0780, -17234.9573,
           1042.1523,  -2752.1126]], dtype=torch.float64)
	q_value: tensor([[-33.1770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.123713489399739
epoch: 48, step: 49
	action: tensor([[-17768.2539, -13019.9489,  -3378.9120, -17028.4267,  -7959.9912,
           2365.2791,  -1748.5620]], dtype=torch.float64)
	q_value: tensor([[-36.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20288761215511064, distance: 1.2550736722423268 entropy 10.639766172775174
epoch: 48, step: 50
	action: tensor([[-24052.5278, -20808.4097,   -440.6805, -19365.5735,  12040.4243,
            470.8066,  26198.5420]], dtype=torch.float64)
	q_value: tensor([[-35.4538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09359293943501168, distance: 1.1966979362866648 entropy 11.044373191143348
epoch: 48, step: 51
	action: tensor([[-11332.7343,  -5671.4211,  11411.7883, -14875.8369,   -950.0847,
          23224.4978, -26909.7996]], dtype=torch.float64)
	q_value: tensor([[-38.1737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38499191505977115, distance: 1.3467293107122822 entropy 11.169905995370843
epoch: 48, step: 52
	action: tensor([[  2848.6739,  -2495.9310,   5933.1989,  -8022.0271,   7301.8327,
         -11210.5022, -15851.0421]], dtype=torch.float64)
	q_value: tensor([[-33.2781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.593895562237915, distance: 0.729248527614018 entropy 11.12203618061139
epoch: 48, step: 53
	action: tensor([[-33545.7977,   8598.7193, -22830.3412,  -4438.5543,  25826.9466,
           2250.2944,   -979.4565]], dtype=torch.float64)
	q_value: tensor([[-29.9003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14536448499624233, distance: 1.057906197639807 entropy 11.003680617948666
epoch: 48, step: 54
	action: tensor([[ 12960.3401, -28135.9491, -29626.3496,   9482.6134,  -3666.1502,
          17802.0255,  -3650.9247]], dtype=torch.float64)
	q_value: tensor([[-41.7821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.248039135279454
epoch: 48, step: 55
	action: tensor([[ -9969.9675,  -9468.9460, -21843.5496,   9052.1612,   8497.6407,
          -2934.1211,  -8476.3015]], dtype=torch.float64)
	q_value: tensor([[-36.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05804960503219414, distance: 1.1770901024934803 entropy 10.639766172775174
epoch: 48, step: 56
	action: tensor([[-15232.7206, -11662.3903, -37004.7633,   -164.8171, -10887.2118,
          12759.5455, -11261.9142]], dtype=torch.float64)
	q_value: tensor([[-31.6769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1363456462451071, distance: 1.672601389526605 entropy 11.162354268452281
epoch: 48, step: 57
	action: tensor([[-18857.3228,  -7024.2059,  13908.9714,  27587.7759,   4880.9740,
            406.1875,  22210.2626]], dtype=torch.float64)
	q_value: tensor([[-37.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025842841211057843, distance: 1.1294609142283891 entropy 11.201636150743607
epoch: 48, step: 58
	action: tensor([[  5154.7920, -31957.0477, -15816.6279,  -4577.9979, -21506.1184,
          -4813.4415, -11078.3722]], dtype=torch.float64)
	q_value: tensor([[-31.5221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16562535047350324, distance: 1.2354813253537482 entropy 11.111691567291201
epoch: 48, step: 59
	action: tensor([[ 29779.0469,  -5780.5964,   3629.0835,  20161.2154, -17793.5351,
           1219.2548,  -3414.8438]], dtype=torch.float64)
	q_value: tensor([[-27.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31535480224678347, distance: 0.946868080364222 entropy 10.848262837614115
epoch: 48, step: 60
	action: tensor([[ -9009.4491, -31466.0622,   3020.2740,   6515.0213,  -2042.0296,
          -1093.8094,  -4825.3702]], dtype=torch.float64)
	q_value: tensor([[-32.5976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6568416177826584, distance: 1.472981155468959 entropy 11.152538224779212
epoch: 48, step: 61
	action: tensor([[ -5582.5585, -16454.4476, -22929.8963,  -6571.6791,   4298.7708,
          12332.0105,  19935.9991]], dtype=torch.float64)
	q_value: tensor([[-27.6519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9642713580231879, distance: 1.6038266855641752 entropy 11.059668796614572
epoch: 48, step: 62
	action: tensor([[ -3366.0997, -20020.1438,  -6084.7247,   8139.8298,  16518.3058,
          24946.4184,   1288.1449]], dtype=torch.float64)
	q_value: tensor([[-33.9406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14692545525240075, distance: 1.0569396365674892 entropy 11.08483659168063
epoch: 48, step: 63
	action: tensor([[  257.5333, -8663.6825,  8380.1309, 20522.8046, -8160.4752, 11846.1170,
         16330.7248]], dtype=torch.float64)
	q_value: tensor([[-33.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4315510460923455, distance: 0.8627846881574418 entropy 11.217682854450972
epoch: 48, step: 64
	action: tensor([[-21416.0408,  10073.8829,  11593.0994,  25217.6028,   3156.2047,
            233.3353,   6632.3230]], dtype=torch.float64)
	q_value: tensor([[-30.1753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7406688104244599, distance: 0.5827523980926773 entropy 11.029390980441763
epoch: 48, step: 65
	action: tensor([[-18104.1832, -31294.7145,  20974.1047, -19344.6687,   4488.7637,
          23316.1140,  -9067.6985]], dtype=torch.float64)
	q_value: tensor([[-33.3253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22015868760686108, distance: 1.264051742066791 entropy 11.149080386055042
epoch: 48, step: 66
	action: tensor([[-5242.7825, -6376.2838, -2046.6494, 21380.5154, 23144.5989, 12506.2202,
          3624.1689]], dtype=torch.float64)
	q_value: tensor([[-33.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40657439822609365, distance: 1.3571818636753847 entropy 11.135323847439148
epoch: 48, step: 67
	action: tensor([[-14928.2695,   2737.0508, -26830.2953,   8038.1425,   8229.9176,
           4555.5212,  -4695.0630]], dtype=torch.float64)
	q_value: tensor([[-35.2931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10100319661067181, distance: 1.085015061963293 entropy 11.259300791288606
epoch: 48, step: 68
	action: tensor([[-28761.6404, -42875.9827,   9118.0784,  18547.2335,  32098.8927,
          16373.1138,  20640.9392]], dtype=torch.float64)
	q_value: tensor([[-34.6972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4899976400712329, distance: 1.396849071802239 entropy 11.051066686750405
epoch: 48, step: 69
	action: tensor([[ 17280.4089, -16108.8792,  -3882.9804,   8905.4161,   4417.6999,
         -27900.9685,  24045.8785]], dtype=torch.float64)
	q_value: tensor([[-32.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3742559553764674, distance: 0.9052219072752018 entropy 11.13514598880206
epoch: 48, step: 70
	action: tensor([[ -5906.7079, -10952.8166,   7876.5130, -11906.8808,   5651.1589,
          22090.7459,   6984.1664]], dtype=torch.float64)
	q_value: tensor([[-28.7872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2641970559903568, distance: 1.2866608321098147 entropy 10.851220874211052
epoch: 48, step: 71
	action: tensor([[ -3455.1187,   8988.6304,   1316.1910,  21572.6188, -11979.9961,
         -12176.1444,  13259.8840]], dtype=torch.float64)
	q_value: tensor([[-35.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22255320066064566, distance: 1.265291459857205 entropy 11.170911341442164
epoch: 48, step: 72
	action: tensor([[ -1175.8931, -10038.1699,  -8913.4688,  15106.3635, -27609.2786,
         -32738.2859,   4224.0435]], dtype=torch.float64)
	q_value: tensor([[-38.3496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1980345488611599, distance: 1.2525393091663937 entropy 11.246125600642594
epoch: 48, step: 73
	action: tensor([[-11227.3836, -10795.8412,   5006.5475,   4646.6303,  21604.1299,
         -28286.9557,  19310.3613]], dtype=torch.float64)
	q_value: tensor([[-29.1326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09754948147742404, distance: 1.0870972388681466 entropy 11.141459118956584
epoch: 48, step: 74
	action: tensor([[ -8373.1516, -48413.9044, -23453.2208,  11904.2824,  -1666.6939,
         -19524.4042,    885.4512]], dtype=torch.float64)
	q_value: tensor([[-34.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05464143389017462, distance: 1.1751927621712714 entropy 11.381089668881105
epoch: 48, step: 75
	action: tensor([[  3580.5651, -45864.9430,   1915.2556,  12331.8939,   3903.4615,
         -13727.5331, -17408.4454]], dtype=torch.float64)
	q_value: tensor([[-29.9775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15581292934269453, distance: 1.0514195341328498 entropy 11.193310436103415
epoch: 48, step: 76
	action: tensor([[ -9833.1385, -12343.8344, -10829.7793,   5242.3776,  12975.6445,
          -7531.7624,  44902.0990]], dtype=torch.float64)
	q_value: tensor([[-32.0960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15941328567336566, distance: 1.2321847502590528 entropy 11.206546113455506
epoch: 48, step: 77
	action: tensor([[ -5195.0673, -20391.0832,  -5625.7323,  16381.6653,  -9515.6743,
          25537.5603,  26463.3091]], dtype=torch.float64)
	q_value: tensor([[-31.8105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5368424761047126, distance: 1.4186372883017933 entropy 11.216775403866365
epoch: 48, step: 78
	action: tensor([[ 12592.9595,  23454.7983,  35805.8989,  25489.7625, -44417.1909,
         -44003.0203, -23046.5808]], dtype=torch.float64)
	q_value: tensor([[-37.3102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3617656369225487, distance: 0.9142117207419731 entropy 11.424407721165098
epoch: 48, step: 79
	action: tensor([[-12536.1556,  15611.5838, -23139.4697,  -3898.7891,  14112.3215,
         -12841.3205,  -1477.0033]], dtype=torch.float64)
	q_value: tensor([[-30.2156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47139277295004844, distance: 0.8319998375568805 entropy 11.014322372819871
epoch: 48, step: 80
	action: tensor([[-13842.2114,  22798.6235, -17131.4243, -10260.7712,  -6414.8674,
          -2049.8616, -23605.5715]], dtype=torch.float64)
	q_value: tensor([[-32.1206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2528858272157558, distance: 0.9891227271714914 entropy 10.935401893097723
epoch: 48, step: 81
	action: tensor([[  3092.6672,  -1886.8048, -14029.4918,  23766.3493,   9794.4422,
           9330.5074,  24788.3392]], dtype=torch.float64)
	q_value: tensor([[-37.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4521893085083757, distance: 0.8469776401294944 entropy 11.080005885599743
epoch: 48, step: 82
	action: tensor([[  3466.6610,  -2715.4705, -20230.7706, -15711.9812,   6464.3689,
         -27515.2086,  14246.6688]], dtype=torch.float64)
	q_value: tensor([[-27.8733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0036141688715485287, distance: 1.1464103156003582 entropy 10.757076804516213
epoch: 48, step: 83
	action: tensor([[-2109.8641, -6434.4576, 13125.9110,  7312.7317, 34003.9861, 18424.3885,
          3503.0459]], dtype=torch.float64)
	q_value: tensor([[-28.7532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08739582622001296, distance: 1.1933024275689863 entropy 11.046827651094366
epoch: 48, step: 84
	action: tensor([[-17887.4726, -13375.9168,  -5004.2735, -12315.4983, -27539.6459,
          13730.2927,  -4238.6616]], dtype=torch.float64)
	q_value: tensor([[-27.6100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.066903734383986, distance: 1.6451928683028254 entropy 11.02599967092164
epoch: 48, step: 85
	action: tensor([[-12552.7946,   -869.6408,  -9590.1091,  10100.4447,     75.0400,
         -14717.1471, -16950.6044]], dtype=torch.float64)
	q_value: tensor([[-29.5568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9411623806537084, distance: 1.5943645386559548 entropy 11.101206087787585
epoch: 48, step: 86
	action: tensor([[-33250.0711, -28223.2719,  16096.4632,   9479.3402,  11445.0975,
          -3110.3421, -14236.2733]], dtype=torch.float64)
	q_value: tensor([[-31.0046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11701553072097903, distance: 1.0753088637828454 entropy 11.213352860878052
epoch: 48, step: 87
	action: tensor([[-12633.1334, -23146.7759, -33416.8930,  -8597.2711,  -5408.3116,
          -7393.9324,  -8433.1568]], dtype=torch.float64)
	q_value: tensor([[-31.6316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28875467577135394, distance: 1.2990977190974486 entropy 11.324504370194168
epoch: 48, step: 88
	action: tensor([[-19097.7586, -52749.8873, -17348.6260,  -4886.2523,  23329.0673,
          11125.4217, -11387.7844]], dtype=torch.float64)
	q_value: tensor([[-34.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.152517986415019, distance: 1.2285152422658134 entropy 11.16559288970359
epoch: 48, step: 89
	action: tensor([[-38229.0573, -27028.3732, -12471.7265,   5530.5425,  11407.2260,
          21698.9062,   3299.3058]], dtype=torch.float64)
	q_value: tensor([[-38.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7830455739920692, distance: 1.5280512311767513 entropy 11.162123726617532
epoch: 48, step: 90
	action: tensor([[  -108.8157, -26176.5628, -17085.8907,   -274.1856, -13359.0022,
          -7985.2481, -23292.3967]], dtype=torch.float64)
	q_value: tensor([[-31.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09067860640338499, distance: 1.0912277402065655 entropy 11.162367056383975
epoch: 48, step: 91
	action: tensor([[-20703.3777, -25969.4051, -13844.7251,  13027.7918, -10616.8334,
           3620.5586,   6609.5323]], dtype=torch.float64)
	q_value: tensor([[-29.9510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0321856288412823, distance: 1.6313170720001702 entropy 10.978524098095036
epoch: 48, step: 92
	action: tensor([[ -3672.5117,  18164.6474, -28066.4114,  11033.9436, -37426.7209,
          44782.3611, -12928.2400]], dtype=torch.float64)
	q_value: tensor([[-29.7834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034597057782457474, distance: 1.163971409124183 entropy 11.08525337623629
epoch: 48, step: 93
	action: tensor([[-43197.2690,   2956.7261,  11975.0389,  16750.8967,  -9804.4605,
          -3083.3422, -15640.8325]], dtype=torch.float64)
	q_value: tensor([[-36.2928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23136744641361584, distance: 1.00326596702732 entropy 11.217167737024706
epoch: 48, step: 94
	action: tensor([[ 18809.8423,   4299.1117,  20022.5777, -17095.0784,   4077.5761,
         -22884.2854,  15700.1371]], dtype=torch.float64)
	q_value: tensor([[-36.2913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4284542491762884, distance: 0.8651316358199619 entropy 11.28556400529323
epoch: 48, step: 95
	action: tensor([[  9130.2808, -12209.8196,   3508.4225,   -237.2726, -18700.7935,
           4375.6994,    180.5334]], dtype=torch.float64)
	q_value: tensor([[-33.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23550359718011715, distance: 1.000562947966468 entropy 10.806707463680198
epoch: 48, step: 96
	action: tensor([[ 24869.1286, -27783.8751,   5132.0689,  16034.3214,   3002.3557,
            433.5975, -29632.6650]], dtype=torch.float64)
	q_value: tensor([[-34.8984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21252819644120757, distance: 1.2600930509494934 entropy 11.126716797898679
epoch: 48, step: 97
	action: tensor([[-12759.2878, -29722.6284,  13909.1831,  -3895.1631,   3199.9041,
          -4213.7209,  -7427.1295]], dtype=torch.float64)
	q_value: tensor([[-30.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16201823571286567, distance: 1.0475481089448586 entropy 10.95991490541022
epoch: 48, step: 98
	action: tensor([[ 10943.5854,  10619.4723,  11097.5013,  18723.4472,    749.0031,
         -12549.6168,  16376.1737]], dtype=torch.float64)
	q_value: tensor([[-34.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3833220208358774, distance: 0.898640346955558 entropy 11.08862403964379
epoch: 48, step: 99
	action: tensor([[ -4816.6761, -33471.6316, -13471.3432,  19093.3431,  11058.3947,
          20423.0956,   3690.8593]], dtype=torch.float64)
	q_value: tensor([[-34.1411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31634212991965893, distance: 0.9461850942360992 entropy 11.064384891829576
epoch: 48, step: 100
	action: tensor([[-17762.0722, -28295.7153,  -2838.7173,   1494.6824, -11686.3858,
           3536.4741,   7859.6180]], dtype=torch.float64)
	q_value: tensor([[-28.9069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20188862180869227, distance: 1.0223237364827689 entropy 11.045405285441104
epoch: 48, step: 101
	action: tensor([[-14166.9260,  30043.0580, -12804.8629,  21076.7540, -17766.0752,
          19509.5932, -12817.1093]], dtype=torch.float64)
	q_value: tensor([[-29.8621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6806576831681354, distance: 0.6466732986518041 entropy 11.006276409223636
epoch: 48, step: 102
	action: tensor([[ -8540.4869, -30340.8064, -13008.0110, -18868.7654,  19472.0953,
         -11858.3094,  -5176.3247]], dtype=torch.float64)
	q_value: tensor([[-35.0895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27004965751969223, distance: 1.289635691858075 entropy 11.120858844038036
epoch: 48, step: 103
	action: tensor([[-22990.2503, -13756.4296,   9846.9982,  -4961.8210, -25714.6499,
           3509.8024,  15561.0477]], dtype=torch.float64)
	q_value: tensor([[-26.4550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24567633749254458, distance: 0.99388367909996 entropy 10.877726911492157
epoch: 48, step: 104
	action: tensor([[ -4898.6863,  -6979.2352,  -1834.1467, -29352.2492, -10447.5850,
          -8934.0521,  -7878.8522]], dtype=torch.float64)
	q_value: tensor([[-34.8753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42990883356765375, distance: 1.3683930754236857 entropy 11.269059334046943
epoch: 48, step: 105
	action: tensor([[  7637.8434,  -5686.1434, -12849.7971,  -4386.8875,  18589.2542,
           7320.8328,  19706.2384]], dtype=torch.float64)
	q_value: tensor([[-31.3065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32845540287951114, distance: 0.9377652231027813 entropy 11.078586053160095
epoch: 48, step: 106
	action: tensor([[  8130.7772, -11470.3744, -15965.1571, -10671.7046, -50996.3072,
          15781.4142,  31194.6344]], dtype=torch.float64)
	q_value: tensor([[-34.3895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4977642850439591, distance: 0.8109806457080254 entropy 11.17154332495831
epoch: 48, step: 107
	action: tensor([[ -8801.5580, -22231.1302,  -5160.3056,   9334.4061,  14558.9787,
           2205.0780,   1153.0565]], dtype=torch.float64)
	q_value: tensor([[-34.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5348147766673461, distance: 1.417701109241735 entropy 11.039968753393547
epoch: 48, step: 108
	action: tensor([[  2309.4989, -12506.3828,  -2413.2700,   9227.6310,  -2673.6958,
          12368.6528, -13487.7777]], dtype=torch.float64)
	q_value: tensor([[-29.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955444071475012, distance: 0.9604693357526671 entropy 11.125327446979426
epoch: 48, step: 109
	action: tensor([[-19441.7529,  -9187.8300, -18292.4513, -19624.4794,   8821.0200,
          38871.9457,   4093.9669]], dtype=torch.float64)
	q_value: tensor([[-37.6791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4233743495406026, distance: 1.3652628123180233 entropy 11.196480669338808
epoch: 48, step: 110
	action: tensor([[ -7601.0687, -10898.5939, -26756.0787, -11831.2630,  18884.3798,
           3898.9070,  -1047.8233]], dtype=torch.float64)
	q_value: tensor([[-31.3447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1387211920651481, distance: 1.0620099213378855 entropy 11.099107392919606
epoch: 48, step: 111
	action: tensor([[ -1574.4084, -37042.3889,  -6838.1600, -19199.3117,  15551.0975,
         -10127.5786, -13335.0355]], dtype=torch.float64)
	q_value: tensor([[-42.5525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020509651489433844, distance: 1.156019743705422 entropy 11.313861919786817
epoch: 48, step: 112
	action: tensor([[-31842.0865, -21919.8631,  17950.7155, -24266.1769,   5164.0533,
         -10728.2189,    757.3120]], dtype=torch.float64)
	q_value: tensor([[-41.3622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16535680170320421, distance: 1.0454592813813153 entropy 11.2609831466542
epoch: 48, step: 113
	action: tensor([[  6188.6633, -16967.0607,  -4070.6604,   3632.6531,   7614.0607,
           9358.4244,  23980.2527]], dtype=torch.float64)
	q_value: tensor([[-37.3589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4467771161096018, distance: 0.851151289420591 entropy 11.186643597118815
epoch: 48, step: 114
	action: tensor([[ 4572.1348,  4820.7309,  2473.6476, 36981.0256,  -207.4435, 14643.6690,
         -4505.4408]], dtype=torch.float64)
	q_value: tensor([[-30.5198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40730114228143655, distance: 0.8809955979571387 entropy 10.921401010078357
epoch: 48, step: 115
	action: tensor([[ -1543.7502,   6103.1338,  15732.6040,  -1042.6464,  18304.0699,
          -8065.5712, -16359.4020]], dtype=torch.float64)
	q_value: tensor([[-31.9040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030152790316478884, distance: 1.161468711315208 entropy 10.939621169468365
epoch: 48, step: 116
	action: tensor([[-14344.5937, -12665.0217,   1678.8975, -11676.4785,   5642.5057,
           5247.9993,  17118.2462]], dtype=torch.float64)
	q_value: tensor([[-29.2003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7642102441316974, distance: 1.5199589642163103 entropy 10.960499251757465
epoch: 48, step: 117
	action: tensor([[21183.9152, -8981.7127,  5070.0561, 10146.8851, 34648.7532, 27369.1537,
         16762.6434]], dtype=torch.float64)
	q_value: tensor([[-30.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22703436677445965, distance: 1.2676082486077416 entropy 11.139429025706319
epoch: 48, step: 118
	action: tensor([[ -2625.9716, -16038.3479,   -934.0518,  30558.1662,   5033.5008,
          41754.2620,  35959.3726]], dtype=torch.float64)
	q_value: tensor([[-32.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2861439857407212, distance: 0.9668564630890131 entropy 11.028047430106003
epoch: 48, step: 119
	action: tensor([[-13704.7254, -24272.1027,  14845.2455,   8239.3420, -13015.1352,
         -26526.3726, -11432.4525]], dtype=torch.float64)
	q_value: tensor([[-29.4074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.193952024070709, distance: 1.027394269781618 entropy 10.980201509326784
epoch: 48, step: 120
	action: tensor([[-32359.5226,  -6957.9699,  -2896.4507, -15464.2833,  12575.7468,
         -31533.2692,   2204.0048]], dtype=torch.float64)
	q_value: tensor([[-30.7527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8538124061831267, distance: 1.5580793991643203 entropy 11.314779174820213
epoch: 48, step: 121
	action: tensor([[15797.9257, -9197.3930,  9051.8786, 13266.0937, 24398.0893,  9391.6211,
         -2222.2994]], dtype=torch.float64)
	q_value: tensor([[-30.4264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5686211563597423, distance: 0.7515988626517037 entropy 11.108262765091869
epoch: 48, step: 122
	action: tensor([[  1436.3611,  -4692.3484, -21683.5330,  10589.9187,   2544.1400,
          11298.1042,   1375.1274]], dtype=torch.float64)
	q_value: tensor([[-30.7677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4027687793889462, distance: 0.8843576655230477 entropy 11.040656759323495
epoch: 48, step: 123
	action: tensor([[-12141.4635,  -2627.0765,   4559.8673, -12180.5120,  21258.4437,
          46441.4752,   5716.9478]], dtype=torch.float64)
	q_value: tensor([[-32.6879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0822478758190297, distance: 1.0962747023939874 entropy 10.910341086028081
epoch: 48, step: 124
	action: tensor([[ -7709.0242,  -3078.0393,  -2525.4627,   2514.6229,   1184.5468,
           4252.7744, -27832.8398]], dtype=torch.float64)
	q_value: tensor([[-30.3943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9189082409262586, distance: 1.58519902760903 entropy 10.97838158645119
epoch: 48, step: 125
	action: tensor([[-11291.5756,  -3276.3262,  -8011.1285,    925.8485,  24017.6128,
          13563.7650,  -1792.7891]], dtype=torch.float64)
	q_value: tensor([[-33.3409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7433696774286969, distance: 1.5109546768149646 entropy 11.221360128662107
epoch: 48, step: 126
	action: tensor([[-11313.1691, -11583.2343,  -2638.6047, -10098.9358,  -7403.0056,
           6936.2594,  27996.9112]], dtype=torch.float64)
	q_value: tensor([[-34.6879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8789407328682528, distance: 1.5686036960419705 entropy 11.25145742356008
epoch: 48, step: 127
	action: tensor([[  7752.7821, -10505.0804, -14120.4831,  -3424.0763,   6365.6288,
          -4149.9618,   3623.5105]], dtype=torch.float64)
	q_value: tensor([[-29.2608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11408246421460533, distance: 1.0770933445360797 entropy 10.907094247996303
LOSS epoch 48 actor 459.4612262940549 critic 70.06127252832127
epoch: 49, step: 0
	action: tensor([[-12144.4725,  17616.2410,  -1847.2389,  27506.6222, -16414.0035,
          -1583.1397,   8669.3526]], dtype=torch.float64)
	q_value: tensor([[-27.3376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.094652717725324
epoch: 49, step: 1
	action: tensor([[  3932.1677, -11065.0182,   5585.5050,  27887.6854,  15600.5721,
          14726.2625,  -3394.3673]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27226519827178997, distance: 0.9762100298242267 entropy 10.683789007502837
epoch: 49, step: 2
	action: tensor([[ 29974.5612, -11114.5452,  15301.1741,  19268.5324,  12193.9186,
         -28599.6211,   5685.8210]], dtype=torch.float64)
	q_value: tensor([[-22.5822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09451661906670283, distance: 1.1972032121356269 entropy 10.96001755657922
epoch: 49, step: 3
	action: tensor([[ -8205.0888,  -1109.5114,  -1050.5271, -10613.9725, -11103.5233,
          16085.9558, -32108.4980]], dtype=torch.float64)
	q_value: tensor([[-26.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7829922509169382, distance: 1.5280283823497272 entropy 11.072924215430243
epoch: 49, step: 4
	action: tensor([[ -9941.4249,  -9751.0623, -12515.4727,  13764.0422,  16966.9748,
          14293.3788,   5900.0885]], dtype=torch.float64)
	q_value: tensor([[-20.7774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5061646946126193, distance: 1.4044068041826383 entropy 10.87377990915628
epoch: 49, step: 5
	action: tensor([[-1683.7714, 13494.7707,  -199.9306, 16129.4720, 17351.0394, 28458.6094,
         -7607.6614]], dtype=torch.float64)
	q_value: tensor([[-24.5058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6188086310554513, distance: 0.7065261206675785 entropy 11.202738791595712
epoch: 49, step: 6
	action: tensor([[  3976.0473, -11232.3779,   8712.8798,  19234.0909,  -9451.7417,
          -7527.0094,   3129.7816]], dtype=torch.float64)
	q_value: tensor([[-27.6481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.261396611374789
epoch: 49, step: 7
	action: tensor([[  -532.7238, -20229.8807,   2561.1180,   3722.1074, -15323.5923,
          -6528.1756,  12371.0014]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2832537357664786, distance: 1.296322209969928 entropy 10.683789007502837
epoch: 49, step: 8
	action: tensor([[-22502.3402, -19546.9910,  -3118.5552,   5652.7511,    686.9564,
         -10026.7260, -35398.4782]], dtype=torch.float64)
	q_value: tensor([[-22.6601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10850561243509804, distance: 1.0804781778547048 entropy 11.160922611403873
epoch: 49, step: 9
	action: tensor([[ 17451.5743,  16382.6125,  14650.5098,  17521.1382, -17460.5795,
          28704.5425,  22515.9511]], dtype=torch.float64)
	q_value: tensor([[-24.3753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32705676741591694, distance: 0.9387412636336363 entropy 11.259545445700585
epoch: 49, step: 10
	action: tensor([[-35400.8911,   -426.4938, -25841.1203,   1265.4497, -14476.4040,
           5813.7306, -31661.5598]], dtype=torch.float64)
	q_value: tensor([[-26.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0766939568898165, distance: 1.0995868369404567 entropy 11.158847247805124
epoch: 49, step: 11
	action: tensor([[-16756.4622, -19809.4989,   8323.3635,   2189.3706,  -5870.3179,
           6880.6836,   2221.2062]], dtype=torch.float64)
	q_value: tensor([[-26.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08716953117624038, distance: 1.1931782536162843 entropy 11.261276535013248
epoch: 49, step: 12
	action: tensor([[ -3006.6406, -44712.7027,  -1499.3663, -14128.7442, -10898.8329,
           3862.2627,   3562.0447]], dtype=torch.float64)
	q_value: tensor([[-28.1742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7479659496276669, distance: 1.5129451289106948 entropy 11.411170995356672
epoch: 49, step: 13
	action: tensor([[ -7665.7771,  19736.5017, -10458.4245,   4494.0357,   1885.2783,
          19856.2121,  32807.5081]], dtype=torch.float64)
	q_value: tensor([[-26.4501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08221224603181854, distance: 1.0962959824620164 entropy 11.22248318719606
epoch: 49, step: 14
	action: tensor([[-12965.9419, -44607.9955, -20674.5178,  -7405.6674,  13690.1066,
          33640.8657,   6978.1045]], dtype=torch.float64)
	q_value: tensor([[-28.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6210756816372778, distance: 1.4569959302511861 entropy 11.248065651608735
epoch: 49, step: 15
	action: tensor([[ -5649.1460, -34681.6168, -24610.5080,  38582.6127,  16563.6642,
         -12666.6824,   6626.8348]], dtype=torch.float64)
	q_value: tensor([[-30.5842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4922005097983744, distance: 1.3978812680869375 entropy 11.290480317062416
epoch: 49, step: 16
	action: tensor([[-12117.9017, -51945.4999,  30774.2779,  13599.0024,  17969.6141,
          -5193.6571,  36432.8309]], dtype=torch.float64)
	q_value: tensor([[-24.2670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7549276943418459, distance: 1.5159549905224972 entropy 11.128711998182522
epoch: 49, step: 17
	action: tensor([[  1808.6097,  -4299.6098,   -633.5247,  22174.4718,  -1487.9641,
            837.7709, -35117.8014]], dtype=torch.float64)
	q_value: tensor([[-24.1198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17253295285247028, distance: 1.040955219261293 entropy 11.280965577447152
epoch: 49, step: 18
	action: tensor([[ 12929.6307, -33796.0069, -32401.5754,  18050.9813,  -9526.0021,
           8625.4970,  -1163.8109]], dtype=torch.float64)
	q_value: tensor([[-24.9910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5553176918271205, distance: 0.7631002905190615 entropy 11.019680026996998
epoch: 49, step: 19
	action: tensor([[-27929.9184,  19884.2058, -10246.8505,  -9672.5450,  -9212.3745,
         -30198.8037,   2866.4415]], dtype=torch.float64)
	q_value: tensor([[-28.2936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.035704505125848
epoch: 49, step: 20
	action: tensor([[-15334.5448,   1123.2493,  -2145.9310,   3984.0080,  -9103.9058,
          -9093.3935,  -5147.1817]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14083389046243666, distance: 1.2222721051709944 entropy 10.683789007502837
epoch: 49, step: 21
	action: tensor([[-15436.8179,   8989.6314,  -6768.2570, -22584.8672,   7004.3412,
          25879.5667, -13709.1324]], dtype=torch.float64)
	q_value: tensor([[-24.8664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0548601186190657, distance: 1.112512086323069 entropy 11.008015741362641
epoch: 49, step: 22
	action: tensor([[ -9130.1515, -11956.0883,   4909.4677,   2150.5155,  12495.6504,
          23763.8226, -15015.5974]], dtype=torch.float64)
	q_value: tensor([[-33.0963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1502531907124306, distance: 1.0548761285841584 entropy 10.9344553465986
epoch: 49, step: 23
	action: tensor([[  8396.7090,  -7986.7806, -14100.1165,  30347.0177,     40.0640,
         -33644.3801,  33531.2651]], dtype=torch.float64)
	q_value: tensor([[-26.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24097336759640997, distance: 0.9969771403887229 entropy 11.28140999157237
epoch: 49, step: 24
	action: tensor([[  5768.5210, -13432.3878,  19663.8144,   6291.8275,  11165.7376,
          27580.1186, -13273.0035]], dtype=torch.float64)
	q_value: tensor([[-25.8669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5663838690780096, distance: 0.753545374359635 entropy 11.138988823210436
epoch: 49, step: 25
	action: tensor([[ 25947.9053, -15921.5047, -21729.2077, -40125.1628,  45246.5686,
         -18689.0650,  47361.4677]], dtype=torch.float64)
	q_value: tensor([[-27.9735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3439726053380585, distance: 1.3266363763872135 entropy 11.265316800776464
epoch: 49, step: 26
	action: tensor([[ -2311.3179, -18828.9627,   8424.4237,  43474.9338, -15841.1733,
         -20166.1133,  -4764.3415]], dtype=torch.float64)
	q_value: tensor([[-25.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07491303794666693, distance: 1.1864333844179602 entropy 10.996377033343862
epoch: 49, step: 27
	action: tensor([[ -2370.5780, -15103.2160,  16485.5826,  38144.8013,  27004.2823,
          -8265.7894,  -7343.8324]], dtype=torch.float64)
	q_value: tensor([[-23.0184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6940304855565551, distance: 1.4894204210892947 entropy 11.114024740339902
epoch: 49, step: 28
	action: tensor([[-13169.9679, -13986.1897,   2625.4314,  34770.3834,  -9243.4858,
         -18819.0338,  13339.5809]], dtype=torch.float64)
	q_value: tensor([[-22.4942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6091774913872339, distance: 1.4516391348390687 entropy 11.150439970668327
epoch: 49, step: 29
	action: tensor([[  1918.5187,   8966.4758,   8213.5217,  13644.4505, -10023.6105,
          19989.5838,   3529.3562]], dtype=torch.float64)
	q_value: tensor([[-21.9301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.120308761821258
epoch: 49, step: 30
	action: tensor([[-19552.0775,  -6648.7661,  -8871.6759,  28027.4900,   1150.1288,
         -10627.5568,  -2570.6898]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.683789007502837
epoch: 49, step: 31
	action: tensor([[-14282.5137,  10916.8624, -17285.2737,  -2126.1065,    -65.4938,
           2474.7999, -16250.0289]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.683789007502837
epoch: 49, step: 32
	action: tensor([[ 16576.8442, -12955.7386,   2194.9315,  15077.5529,   6664.1666,
           2340.5780,  11352.3241]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17143737222079958, distance: 1.0416441126355358 entropy 10.683789007502837
epoch: 49, step: 33
	action: tensor([[-12072.0720,   8863.4476, -10131.1396,  16546.3588,  -8142.7068,
          23929.7943,   6593.6121]], dtype=torch.float64)
	q_value: tensor([[-28.5872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19852763063015844, distance: 1.024474069960157 entropy 11.185188463555265
epoch: 49, step: 34
	action: tensor([[  2892.8346,  -1882.8401,  20643.8975,  33473.3365, -11561.3175,
          43030.0668, -10532.2294]], dtype=torch.float64)
	q_value: tensor([[-28.8024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40032759112968697, distance: 0.8861632324949952 entropy 11.272735993503405
epoch: 49, step: 35
	action: tensor([[ 24839.6571, -34855.3611,  -5890.3440,  34142.8801,  -4319.8055,
          27816.6908,  36179.3964]], dtype=torch.float64)
	q_value: tensor([[-25.1831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.339930060100732, distance: 1.3246396740500772 entropy 11.21081995991277
epoch: 49, step: 36
	action: tensor([[ 17963.1707, -33514.9925, -31973.5986,  44331.6458,   9641.0130,
          24937.3882,  -4471.8695]], dtype=torch.float64)
	q_value: tensor([[-28.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3426291177439713, distance: 1.3259731317837495 entropy 11.36696618605883
epoch: 49, step: 37
	action: tensor([[-11448.8416,  24531.4170,  21392.9146,  14723.5265, -22185.4865,
           4242.3436,  27377.6163]], dtype=torch.float64)
	q_value: tensor([[-31.4387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43112833643114323, distance: 0.8631054202522367 entropy 11.183842470292237
epoch: 49, step: 38
	action: tensor([[ 33825.6807, -28664.5375,  -3855.5440,  18646.6666, -29922.4431,
          22787.7903, -19467.2436]], dtype=torch.float64)
	q_value: tensor([[-24.0094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44538077320699887, distance: 0.8522247718217805 entropy 11.029672172711914
epoch: 49, step: 39
	action: tensor([[  5716.3672,  -5843.1358,  36609.8019,  44380.1395,  23144.8749,
         -17605.7478,  14852.7387]], dtype=torch.float64)
	q_value: tensor([[-28.6627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37918564708599434, distance: 0.9016491296121566 entropy 11.256137975814115
epoch: 49, step: 40
	action: tensor([[-33390.8904, -23253.7828, -14440.7737, -14291.3750,  20195.8697,
          43165.2494, -37478.0494]], dtype=torch.float64)
	q_value: tensor([[-31.4152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31013523818815947, distance: 1.3098294692441916 entropy 11.27894455068638
epoch: 49, step: 41
	action: tensor([[-16278.4035, -20358.1550,  11726.6457,  14080.7712,  18444.9594,
           4963.9043,  -2357.8598]], dtype=torch.float64)
	q_value: tensor([[-23.9834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2621174602095109, distance: 1.2856021222660818 entropy 11.07569911702625
epoch: 49, step: 42
	action: tensor([[-12800.4913, -11381.9860, -27912.9807,  21562.9924,  12792.6081,
          -1641.4735,  25567.8930]], dtype=torch.float64)
	q_value: tensor([[-26.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2890325843997714, distance: 1.2992377810545137 entropy 11.283986087027566
epoch: 49, step: 43
	action: tensor([[  2674.3819,   4226.9393, -14002.9686,   4710.2321,  20602.3291,
         -27625.3557, -17022.4862]], dtype=torch.float64)
	q_value: tensor([[-25.3566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7982234106374073, distance: 0.5140342793388021 entropy 11.338615931260064
epoch: 49, step: 44
	action: tensor([[-15023.0083,  12787.2556,   2546.4550,  -5174.1006, -17851.6273,
          15342.2305,   2690.5990]], dtype=torch.float64)
	q_value: tensor([[-23.8746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.968685104558935
epoch: 49, step: 45
	action: tensor([[-1094.3823, 13224.1642,  1024.9279, 14120.0279,  -146.2526,  3491.0443,
          9637.7760]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.683789007502837
epoch: 49, step: 46
	action: tensor([[ -1023.5910,  -3727.9329,   3803.0500,  -5070.4725, -15787.8022,
          13361.1312,   4848.3596]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2797461555265761, distance: 1.294549347207804 entropy 10.683789007502837
epoch: 49, step: 47
	action: tensor([[ 20145.9109,  11809.9269, -15642.9897, -42366.9286,  12378.9587,
          13490.2025,    339.3801]], dtype=torch.float64)
	q_value: tensor([[-28.4506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.203803177623195
epoch: 49, step: 48
	action: tensor([[-10646.1084, -11395.5008,  -8018.1919,   7540.7445,   1785.6913,
            460.5655,  -2923.2848]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2813105117395309, distance: 1.2953403313432204 entropy 10.683789007502837
epoch: 49, step: 49
	action: tensor([[-27946.4766,  11492.9657,   6559.2002,  25521.4124,  11590.5617,
          -5059.7842,  33827.4381]], dtype=torch.float64)
	q_value: tensor([[-23.5875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40109764730621245, distance: 0.8855940761755936 entropy 11.140969111413638
epoch: 49, step: 50
	action: tensor([[-23765.4198, -26452.6794,   9402.3970,  27093.9116,  -2007.3832,
          19650.2283, -62100.4996]], dtype=torch.float64)
	q_value: tensor([[-28.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9415414150954355, distance: 1.59452019013098 entropy 11.23762324596072
epoch: 49, step: 51
	action: tensor([[10872.7878, -9377.8270, 28204.3101, 19704.7698, 12030.1601, 25229.0425,
         -7858.3891]], dtype=torch.float64)
	q_value: tensor([[-28.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.062457915688852506, distance: 1.1080314284079456 entropy 11.435636753141386
epoch: 49, step: 52
	action: tensor([[-25758.8891, -19167.6739,   4906.6587, -23773.2630,  -3142.8951,
         -27936.6930, -45218.0826]], dtype=torch.float64)
	q_value: tensor([[-24.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24754548635876117, distance: 1.2781590161703074 entropy 11.171623060352674
epoch: 49, step: 53
	action: tensor([[ -3533.0175, -11795.3639, -21466.4629,  -3750.9802,  -2713.4037,
         -16792.3472,   7914.5486]], dtype=torch.float64)
	q_value: tensor([[-23.0576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17855866203335446, distance: 1.037158116727547 entropy 11.102513974393323
epoch: 49, step: 54
	action: tensor([[-20618.2529, -51715.4949, -23455.0863,  21076.4128, -10950.9624,
          -4640.1350,  35932.5333]], dtype=torch.float64)
	q_value: tensor([[-33.6828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7754821949471151, distance: 1.5248069190711728 entropy 11.331441186008991
epoch: 49, step: 55
	action: tensor([[ -2039.7933, -18965.3833, -10540.2048,  36219.2196, -17946.8216,
          14390.4350,  -5330.0831]], dtype=torch.float64)
	q_value: tensor([[-21.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1436176482035123, distance: 1.2237624344682358 entropy 10.954542103884934
epoch: 49, step: 56
	action: tensor([[-1.5513e+04, -1.2022e+04,  8.9132e+03,  2.6773e+01, -2.1513e+04,
          2.7534e+04,  1.5422e+03]], dtype=torch.float64)
	q_value: tensor([[-27.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08615080975115386, distance: 1.192619094673548 entropy 11.355874609845511
epoch: 49, step: 57
	action: tensor([[ 30393.2514, -61432.3564, -15447.0788,  12628.9459,   9895.3969,
          54337.4641,   4494.2778]], dtype=torch.float64)
	q_value: tensor([[-26.6192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3318526598728846, distance: 0.9353901992589717 entropy 11.339517376895788
epoch: 49, step: 58
	action: tensor([[-23958.2521, -38133.9311,  -7911.5255,  25217.9664, -17298.6530,
          22075.4917,    148.2774]], dtype=torch.float64)
	q_value: tensor([[-25.2217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04413723929914892, distance: 1.1188051696274068 entropy 11.253698282406408
epoch: 49, step: 59
	action: tensor([[-30266.8975,  11064.8621, -10140.4565, -13965.9694,  21701.6381,
           9626.9465,  30917.6442]], dtype=torch.float64)
	q_value: tensor([[-26.0583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12397525386297581, distance: 1.0710626637276257 entropy 11.285060709201934
epoch: 49, step: 60
	action: tensor([[-14224.7484, -19102.6039, -17290.1402,  -8223.9538,  18193.4271,
         -16878.9511,     58.8542]], dtype=torch.float64)
	q_value: tensor([[-31.8578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3441198356552799, distance: 1.3267090399760255 entropy 10.930773227459394
epoch: 49, step: 61
	action: tensor([[-24442.7735,  -6708.2380,   3864.7298,  25160.1901, -42493.0752,
          14888.5971,  23014.9435]], dtype=torch.float64)
	q_value: tensor([[-30.4889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4837775732753924, distance: 1.3939304157899322 entropy 11.32749441323011
epoch: 49, step: 62
	action: tensor([[-19545.2144, -27427.7051, -24102.1588,  -4936.2443,  -7444.3042,
          -2872.4733,  26510.1882]], dtype=torch.float64)
	q_value: tensor([[-20.0985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3859254772046301, distance: 1.347183119754919 entropy 10.868567304517244
epoch: 49, step: 63
	action: tensor([[-15802.0539, -15586.0015,  10775.0982,   4542.7116,  -2887.5431,
           3015.8998, -12388.7606]], dtype=torch.float64)
	q_value: tensor([[-21.2856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9135017887066961, distance: 1.582964332962287 entropy 10.93144993575728
epoch: 49, step: 64
	action: tensor([[-12360.3222,   6020.7194, -31551.9127,  52081.2578, -16475.3656,
          -3586.4832, -25671.2840]], dtype=torch.float64)
	q_value: tensor([[-23.5436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19444441442735438, distance: 1.027080419783738 entropy 11.166292923151989
epoch: 49, step: 65
	action: tensor([[-7336.9995,  6858.7199,  -225.8463, 16230.2469, -5974.7416, -3413.1820,
          8307.2711]], dtype=torch.float64)
	q_value: tensor([[-24.7660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5291230100480082, distance: 0.7852544886030047 entropy 11.121484354489235
epoch: 49, step: 66
	action: tensor([[  3181.4137,  -8273.1300,   2346.6509, -10054.6433,    814.8730,
         -18781.5556,  -9027.1511]], dtype=torch.float64)
	q_value: tensor([[-24.0606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.990012639375061
epoch: 49, step: 67
	action: tensor([[  4967.3076,   6158.0647,  -3136.1529,  -4056.9237,   2277.9456,
         -15399.5391,  11228.8757]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4539805612877673, distance: 0.8455917660335022 entropy 10.683789007502837
epoch: 49, step: 68
	action: tensor([[-20004.4029, -13072.8138, -20062.9310,   5837.6301,   4188.2204,
          16700.2054,   2604.4812]], dtype=torch.float64)
	q_value: tensor([[-25.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.97997867376708
epoch: 49, step: 69
	action: tensor([[-10670.0033, -20332.7791, -16790.9033,  17407.9624,  -5301.8337,
          15485.2465,    698.0327]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37671518572137996, distance: 1.3426992449954442 entropy 10.683789007502837
epoch: 49, step: 70
	action: tensor([[  7519.8649,  -5049.5299,  26828.9833,  17386.6946, -11752.6021,
          15322.2503,  -7506.2105]], dtype=torch.float64)
	q_value: tensor([[-25.0498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3170233600368694, distance: 0.9457135642054185 entropy 11.211454444202626
epoch: 49, step: 71
	action: tensor([[ -8567.3083, -10921.6979, -15739.4250,  11074.6308,  -7359.7393,
           4956.1304,   3011.3880]], dtype=torch.float64)
	q_value: tensor([[-26.3556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7648729231321196, distance: 1.5202444036422411 entropy 11.270506146654679
epoch: 49, step: 72
	action: tensor([[-31820.3677, -46484.1745, -11720.7583,  -3046.7997, -16017.1860,
           5459.0782, -24585.8736]], dtype=torch.float64)
	q_value: tensor([[-25.3916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4007217061729891, distance: 1.3543553343630423 entropy 11.244982616849457
epoch: 49, step: 73
	action: tensor([[-13862.7076,  -8455.0524,  21563.8147, -11962.8597,   8054.0434,
         -17390.0067,  -7070.5506]], dtype=torch.float64)
	q_value: tensor([[-24.4356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13366210951585422, distance: 1.0651244349282418 entropy 11.03185592280288
epoch: 49, step: 74
	action: tensor([[  508.3026, 15151.8626, -4125.8098, 26437.0028,  8237.5024, 23420.1287,
          5800.9777]], dtype=torch.float64)
	q_value: tensor([[-26.4573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.078272772937021
epoch: 49, step: 75
	action: tensor([[  7140.3635, -17329.7508, -11431.2230,  17947.4218,   4734.1606,
          -1122.2318,  11449.8944]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05345140160692141, distance: 1.1745295452630282 entropy 10.683789007502837
epoch: 49, step: 76
	action: tensor([[  4172.5213,  -1689.3109,  -7441.2196,  10467.9848, -29983.1296,
          31394.5647, -19597.8452]], dtype=torch.float64)
	q_value: tensor([[-25.6344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4392944963410562, distance: 0.8568880825076045 entropy 11.196048190421592
epoch: 49, step: 77
	action: tensor([[-26893.0631,   8416.3857,  20949.8831,   8423.7868, -16452.4458,
          17607.3805,   5249.4949]], dtype=torch.float64)
	q_value: tensor([[-24.4230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24381196191789878, distance: 0.995111155457308 entropy 11.060360447055972
epoch: 49, step: 78
	action: tensor([[  7101.2453,   3142.1567,  -2274.5981,  10972.7272, -22466.2120,
          24048.9437,   2356.8402]], dtype=torch.float64)
	q_value: tensor([[-28.1134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.239253714603183
epoch: 49, step: 79
	action: tensor([[ -6810.0611, -20363.8847,  10714.8132,  16579.7672, -15128.2929,
         -14856.2089,   6666.1703]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05286046963171154, distance: 1.113688345139173 entropy 10.683789007502837
epoch: 49, step: 80
	action: tensor([[-10836.0865, -24521.7878, -15879.4012,   9652.4008,   4355.0294,
         -11730.2879, -14034.8816]], dtype=torch.float64)
	q_value: tensor([[-26.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42516241065725646, distance: 1.366120073505972 entropy 11.222152870289007
epoch: 49, step: 81
	action: tensor([[-10515.4470, -12869.3063,  22887.1411,  -2814.2862,  21077.5093,
          20092.7634,  24174.5346]], dtype=torch.float64)
	q_value: tensor([[-22.6485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31707224377457655, distance: 1.3132925843924086 entropy 11.162288995874574
epoch: 49, step: 82
	action: tensor([[22028.8146,  4553.5195, -1357.9470,  7455.5530, 19092.0605, 20027.8040,
          4099.4866]], dtype=torch.float64)
	q_value: tensor([[-31.0989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.413324116897442
epoch: 49, step: 83
	action: tensor([[  5293.6880, -14537.7062, -10064.0944,   9308.8209,   -184.6756,
           5168.6838,  10089.6053]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.566304793170332, distance: 0.7536140809457266 entropy 10.683789007502837
epoch: 49, step: 84
	action: tensor([[ -4279.0197, -15452.3692,  -4041.1934,  14321.5808,   8554.3673,
          17463.4125,  -9418.8997]], dtype=torch.float64)
	q_value: tensor([[-25.0856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6773750848696705, distance: 1.4820804794719373 entropy 11.079415333666185
epoch: 49, step: 85
	action: tensor([[  2386.2142, -23392.8069, -12365.6572,   9964.3688,  -3794.2039,
          13258.4628,  -6299.4533]], dtype=torch.float64)
	q_value: tensor([[-24.9831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3554992748713085, distance: 0.9186887505624222 entropy 11.235851545450604
epoch: 49, step: 86
	action: tensor([[ 5654.3505, 11080.0008, -7457.0042,  8753.7364, -5778.7018,  2649.9911,
           911.4890]], dtype=torch.float64)
	q_value: tensor([[-24.3346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3686390333663453, distance: 0.9092756426446191 entropy 10.954326579220563
epoch: 49, step: 87
	action: tensor([[ -4524.0778, -16003.4292,   9016.7792,  -3911.2542,   9549.4414,
          -1590.1947,  -6384.8998]], dtype=torch.float64)
	q_value: tensor([[-26.5561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15384389359960582, distance: 1.2292217080155496 entropy 10.979482376248162
epoch: 49, step: 88
	action: tensor([[-22111.4046,  14353.3051, -12706.9142,   3159.4436, -10028.0742,
          15137.8932,   -388.8648]], dtype=torch.float64)
	q_value: tensor([[-19.6359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012499189664717636, distance: 1.137170077723797 entropy 10.796619320702604
epoch: 49, step: 89
	action: tensor([[ -2102.9538, -16290.4065,   -463.1574,   8448.6657,  -4913.6656,
           3054.5132, -20689.5376]], dtype=torch.float64)
	q_value: tensor([[-23.7653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03241892496234988, distance: 1.1256422250310558 entropy 10.731902734110866
epoch: 49, step: 90
	action: tensor([[  3370.0505, -25676.8772,  -4144.0277,   1465.1731,  18203.8244,
           7897.1512,   3895.9782]], dtype=torch.float64)
	q_value: tensor([[-22.9757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12321968752027979, distance: 1.2127996048842886 entropy 11.078002069020561
epoch: 49, step: 91
	action: tensor([[-14288.2857,  -8011.2559,   6111.7517,   4745.4207,  -1862.2164,
          -6483.3692, -24839.4456]], dtype=torch.float64)
	q_value: tensor([[-27.9968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14138998333884145, distance: 1.222569963609461 entropy 11.079677038555904
epoch: 49, step: 92
	action: tensor([[-19996.6016,  -5330.6642,  16449.7444,  12714.9850,  -7223.8385,
           1770.7549,  -1541.6092]], dtype=torch.float64)
	q_value: tensor([[-26.4697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7172461261505427, distance: 1.4995914857020292 entropy 11.375764344803569
epoch: 49, step: 93
	action: tensor([[-34798.1104, -15364.2127,   5808.5279,  17257.2527,   1203.2329,
         -34760.2493,   4234.7895]], dtype=torch.float64)
	q_value: tensor([[-25.3217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15411190533143693, distance: 1.0524782981777108 entropy 11.268442048936143
epoch: 49, step: 94
	action: tensor([[-31024.7672, -13779.0176,  -8387.3301, -10741.1883, -29811.0902,
          23659.5114,   2981.3486]], dtype=torch.float64)
	q_value: tensor([[-28.4502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5183559139397109, distance: 1.4100791336477796 entropy 11.469030962528423
epoch: 49, step: 95
	action: tensor([[-12613.8541,  -5454.3710,  -6961.2790,   5510.8768,  31953.1935,
          13493.7327,  -5401.0997]], dtype=torch.float64)
	q_value: tensor([[-26.3513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9024685250698845, distance: 1.5783940440901665 entropy 11.198922300671622
epoch: 49, step: 96
	action: tensor([[ 9.8309e+03, -1.9014e+04,  3.8959e+03,  5.5835e+04, -9.5272e+03,
         -2.9491e+04,  5.3897e+01]], dtype=torch.float64)
	q_value: tensor([[-27.3180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.374450759239506
epoch: 49, step: 97
	action: tensor([[-3.3127e+04, -2.3697e+04,  2.0708e+01,  1.3989e+04,  8.0324e+03,
          1.7859e+03,  1.1407e+04]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5743433339149726, distance: 1.435841223993986 entropy 10.683789007502837
epoch: 49, step: 98
	action: tensor([[  -598.6764, -28061.0474, -25400.0648,  65348.8730,   -750.0906,
           -635.9206,   7352.6903]], dtype=torch.float64)
	q_value: tensor([[-27.6974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.051476133560823145, distance: 1.1734278811388472 entropy 11.352226187677493
epoch: 49, step: 99
	action: tensor([[ 26410.6978,  -6340.5116, -26067.4966, -22065.4761,  42674.7655,
         -17694.0933,  -6172.4791]], dtype=torch.float64)
	q_value: tensor([[-25.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09670720792747423, distance: 1.1984006672790959 entropy 11.340812408700591
epoch: 49, step: 100
	action: tensor([[-20781.2132,  -6720.7996,   1281.9438,  17663.3339,  11565.2908,
         -10210.3006,  -6803.7244]], dtype=torch.float64)
	q_value: tensor([[-22.0108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15566356322361452, distance: 1.0515125464014947 entropy 10.764365675726125
epoch: 49, step: 101
	action: tensor([[-24387.9799,  11019.1622, -50731.0902,  23753.3961,  28887.0866,
         -11078.7719,  -5658.7885]], dtype=torch.float64)
	q_value: tensor([[-22.6602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22735597253437212, distance: 1.0058805699483104 entropy 11.176651813794694
epoch: 49, step: 102
	action: tensor([[-66673.9380,   7338.9777, -19307.0697,   4738.9271,  -3309.1701,
          14252.0592, -28621.9023]], dtype=torch.float64)
	q_value: tensor([[-29.7950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5047759009473861, distance: 0.8052997764868056 entropy 11.366543057090434
epoch: 49, step: 103
	action: tensor([[-27167.4064, -41934.5396, -27532.2167,  33001.6972,   -605.1398,
         -15089.2807, -19450.1613]], dtype=torch.float64)
	q_value: tensor([[-28.9894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.211542781600459
epoch: 49, step: 104
	action: tensor([[ 13538.5192,  -3540.4352,  12823.5078, -10325.2865,  13445.3378,
          -8754.2383,   4820.1272]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.683789007502837
epoch: 49, step: 105
	action: tensor([[  1580.1821, -11655.7687, -11511.1409, -10495.9469,   5264.7157,
         -15946.2951,  22270.7586]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1383190774843559, distance: 1.0622578084545113 entropy 10.683789007502837
epoch: 49, step: 106
	action: tensor([[ 16158.6281, -11099.8205,  -1207.4343,  25279.0792,  22089.1140,
          12919.5850,   5852.8713]], dtype=torch.float64)
	q_value: tensor([[-25.8117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.443440510455124, distance: 1.3748525962524474 entropy 10.979496884942062
epoch: 49, step: 107
	action: tensor([[-13992.1789, -39537.0333, -10514.3352, -12956.8466,  32678.4604,
          21985.0776,  -5336.7663]], dtype=torch.float64)
	q_value: tensor([[-26.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12025972355669545, distance: 1.0733316378881808 entropy 11.20322349239493
epoch: 49, step: 108
	action: tensor([[-29651.2792,  -8555.0851,   7325.8218,  10122.3448,  17054.8011,
          29714.5221, -22717.4716]], dtype=torch.float64)
	q_value: tensor([[-29.9404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6878512789731921, distance: 1.486701507649361 entropy 11.194397245390359
epoch: 49, step: 109
	action: tensor([[  6443.9570,  -7131.7413,  32074.9253, -22822.4082,   5182.0423,
           2749.1378, -17629.1293]], dtype=torch.float64)
	q_value: tensor([[-23.9800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08594021424063647, distance: 1.1925034696606651 entropy 11.226129373962081
epoch: 49, step: 110
	action: tensor([[ -2657.3858, -18953.3256,  -6968.9210,  22239.2539,  11495.1501,
          -3087.6155,   8509.9771]], dtype=torch.float64)
	q_value: tensor([[-23.9655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6279205815099955, distance: 1.4600687312131901 entropy 10.979307041508944
epoch: 49, step: 111
	action: tensor([[ 4.8863e+01, -2.5095e+04, -5.5830e+03,  1.1872e+04, -7.6138e+03,
          3.1835e+02, -4.9130e+04]], dtype=torch.float64)
	q_value: tensor([[-23.1427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31532055504466605, distance: 1.3124189629031273 entropy 11.282378132312088
epoch: 49, step: 112
	action: tensor([[-34825.8307,   8086.0192, -14581.9481,  36148.8538,  -7705.9135,
           4651.4655, -10302.9152]], dtype=torch.float64)
	q_value: tensor([[-24.4330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5877524373150607, distance: 0.7347434811864513 entropy 11.26467826821067
epoch: 49, step: 113
	action: tensor([[ 26778.8763, -34427.4234,  15209.3620,  14119.7584,   3092.5738,
         -20801.1213,  26120.0074]], dtype=torch.float64)
	q_value: tensor([[-28.5702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.369373894131568
epoch: 49, step: 114
	action: tensor([[-1.1922e+04,  8.0946e+03,  5.3595e+03,  2.0645e+04,  1.9038e+04,
         -2.9072e+03,  2.0702e+00]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6979982657066383, distance: 0.6288707738005915 entropy 10.683789007502837
epoch: 49, step: 115
	action: tensor([[ 18618.5019, -43529.5437,  -6081.5709,     82.4409,  -1723.9459,
           7902.0083, -13916.9147]], dtype=torch.float64)
	q_value: tensor([[-25.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.120970104227991
epoch: 49, step: 116
	action: tensor([[-11632.8989,   7496.4836,   7076.4643,   9568.5095,  -1597.5353,
          29975.6267, -14431.2068]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03412284109684527, distance: 1.1637046202691679 entropy 10.683789007502837
epoch: 49, step: 117
	action: tensor([[-10434.1807,   1519.4308, -21874.4404,  18941.6946,   6344.0721,
           2657.7360,  13878.7350]], dtype=torch.float64)
	q_value: tensor([[-29.0029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007544691692433991, distance: 1.1400192184966476 entropy 10.883778380948502
epoch: 49, step: 118
	action: tensor([[-28338.3545,   5166.5448,  -8512.7049,  15883.6026,  15256.7337,
         -24610.3454,  34688.0871]], dtype=torch.float64)
	q_value: tensor([[-29.8719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06851088342234346, distance: 1.1044487951931667 entropy 11.238841147029875
epoch: 49, step: 119
	action: tensor([[  6014.2539, -44576.5277,   8850.2408,  23701.8395,    764.0718,
           7093.4106,   -923.1482]], dtype=torch.float64)
	q_value: tensor([[-26.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.102487547627439
epoch: 49, step: 120
	action: tensor([[  -599.6117,   -464.5565,  -9813.2630,  -2558.3141,  -9958.9214,
           -175.6486, -13969.5297]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12220005913951859, distance: 1.0721473263134198 entropy 10.683789007502837
epoch: 49, step: 121
	action: tensor([[  2127.8244, -37367.5506,    262.3171,   1899.9324, -15447.2596,
         -17322.4858,  -5404.0438]], dtype=torch.float64)
	q_value: tensor([[-26.6900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5291679483473619, distance: 0.7852170171996432 entropy 10.988794852290201
epoch: 49, step: 122
	action: tensor([[ -4634.1414,   3909.7261,   5870.6752,  11439.8467, -12696.0428,
          24084.8360,  16710.5528]], dtype=torch.float64)
	q_value: tensor([[-22.3876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.052319058143889
epoch: 49, step: 123
	action: tensor([[-18266.2707, -27807.2572,  13481.2712,   2823.6733,   7486.9172,
          -3369.2180,  15997.0603]], dtype=torch.float64)
	q_value: tensor([[-29.5177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06441789338125092, distance: 1.1806271783954516 entropy 10.683789007502837
epoch: 49, step: 124
	action: tensor([[ -2831.4720, -32549.5698, -17785.6070,  18839.1506,   1272.8268,
           8111.8245,  -4743.4081]], dtype=torch.float64)
	q_value: tensor([[-24.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20963860168166382, distance: 1.2585906816845212 entropy 11.33822535221412
epoch: 49, step: 125
	action: tensor([[-10896.9108, -24378.6655,  13448.0880,  19066.8901,  17574.1369,
         -14306.7498,   1839.4692]], dtype=torch.float64)
	q_value: tensor([[-26.7796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19517152760610879, distance: 1.251041776526572 entropy 11.287335294190209
epoch: 49, step: 126
	action: tensor([[ -3864.1449, -52294.1943, -17143.3120,  12984.7271, -20103.3233,
           5385.7515, -11547.5452]], dtype=torch.float64)
	q_value: tensor([[-24.7990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5248426014521359, distance: 1.4130879783812973 entropy 11.169927899988298
epoch: 49, step: 127
	action: tensor([[ -1565.9480, -33413.4890,  -6824.7913,  13717.0857,  20696.5331,
          37683.9664,   1347.9357]], dtype=torch.float64)
	q_value: tensor([[-27.1718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1671438335646589, distance: 1.044339479442888 entropy 11.346157815570743
LOSS epoch 49 actor 282.5302116163563 critic 323.9759466301942
epoch: 50, step: 0
	action: tensor([[ 13379.0132,   5073.5312,  -9629.9039,  31552.8369,   6554.9271,
         -13247.9889,  22599.8294]], dtype=torch.float64)
	q_value: tensor([[-20.7476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7202158488774029, distance: 0.605296618881638 entropy 11.242148141843975
epoch: 50, step: 1
	action: tensor([[ -4854.8681, -45320.7493,  26528.3414,  26080.5682,  16517.4707,
          17174.9389,  41257.9966]], dtype=torch.float64)
	q_value: tensor([[-23.3356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5978746004536458, distance: 0.725667120583094 entropy 11.1824317435621
epoch: 50, step: 2
	action: tensor([[ 36334.0296,  21966.5546,   2036.2659,  -5162.3968,  14805.7637,
         -19142.0941,  25968.7770]], dtype=torch.float64)
	q_value: tensor([[-22.5444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.345951004173192, distance: 0.9254689124262355 entropy 11.277772436922415
epoch: 50, step: 3
	action: tensor([[ -5887.1551,  -6892.3336, -14246.6071,  17139.6298,  -9447.9743,
         -19611.6296,   -159.1410]], dtype=torch.float64)
	q_value: tensor([[-25.9786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3119178911810141, distance: 0.9492417386284396 entropy 11.082087427572192
epoch: 50, step: 4
	action: tensor([[ 15966.0304, -10407.1883,  15163.4518,  25586.1227,  17500.5014,
         -29505.2288,  -1528.5529]], dtype=torch.float64)
	q_value: tensor([[-23.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12763085690734577, distance: 1.0688255847166794 entropy 11.365581314976676
epoch: 50, step: 5
	action: tensor([[ -8061.4150, -39458.8822, -20080.2955,  33076.9951,  24725.5845,
          21581.2961,  -1722.3225]], dtype=torch.float64)
	q_value: tensor([[-28.0784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35929751691808975, distance: 0.9159776919549706 entropy 11.185893922606242
epoch: 50, step: 6
	action: tensor([[-14898.6301, -27372.6024, -16285.7247,    301.9979,  22926.6233,
          29588.1926,   2516.3081]], dtype=torch.float64)
	q_value: tensor([[-19.0112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11099629745868889, distance: 1.2061824330305622 entropy 11.088580728903958
epoch: 50, step: 7
	action: tensor([[ 26420.7578, -37211.3528, -13611.5738,  -6801.9927,  38302.5181,
           5232.6171,  13961.5422]], dtype=torch.float64)
	q_value: tensor([[-21.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006335218286006361, distance: 1.1407136593898655 entropy 11.307620886959999
epoch: 50, step: 8
	action: tensor([[ -4359.0818,  15227.5625,  -5563.0001,  24388.6517, -16594.8195,
         -20232.8737,  10464.7566]], dtype=torch.float64)
	q_value: tensor([[-17.6329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.954857869596992
epoch: 50, step: 9
	action: tensor([[ 9780.7883, 12206.4181,  -223.8547,  4629.6843, -7403.4164,  2222.2973,
          6704.5128]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6910430355650026, distance: 0.6360711354466673 entropy 10.727661287749038
epoch: 50, step: 10
	action: tensor([[-26938.8040,   3782.2957, -30546.6407,    157.0400,   7673.8668,
         -19393.7444,   2346.5825]], dtype=torch.float64)
	q_value: tensor([[-22.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5478325017765857, distance: 0.7694959958939782 entropy 11.180398797733803
epoch: 50, step: 11
	action: tensor([[-34911.6904, -25042.9434,  20896.7398,  -1044.2656, -13911.4839,
         -36666.1219, -13945.2247]], dtype=torch.float64)
	q_value: tensor([[-22.2748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30941694980930423, distance: 1.3094703596467927 entropy 11.255918560683073
epoch: 50, step: 12
	action: tensor([[-22950.5007,   4210.4079, -15191.6411,  32889.1111,   4454.3589,
         -17495.7168, -21017.8896]], dtype=torch.float64)
	q_value: tensor([[-23.9554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6195588658031672, distance: 0.7058305100724367 entropy 11.262570062868106
epoch: 50, step: 13
	action: tensor([[  1073.1304, -10802.3206, -15843.7459,  12889.9885,  22084.9474,
           2810.5718,  -1793.5671]], dtype=torch.float64)
	q_value: tensor([[-22.2691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5424525257303939, distance: 0.7740602652380045 entropy 11.058909299916861
epoch: 50, step: 14
	action: tensor([[ -8863.7504, -39192.0203,  20069.0698,  10997.6990,   7713.9741,
          10362.1698, -31101.5778]], dtype=torch.float64)
	q_value: tensor([[-23.6520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.669550112133557, distance: 1.4786194776957369 entropy 11.176671489282478
epoch: 50, step: 15
	action: tensor([[ -3088.1184, -23393.9133, -25129.3744,  40130.7898,  20226.7123,
         -32144.7613,  -7552.6937]], dtype=torch.float64)
	q_value: tensor([[-20.6431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18777170737373972, distance: 1.2471628947793638 entropy 11.203189714705875
epoch: 50, step: 16
	action: tensor([[-12377.5799,  -9966.7734,  -9135.9260,  29324.3190,  -1457.9355,
         -23825.6770,  10246.3666]], dtype=torch.float64)
	q_value: tensor([[-21.7547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4707230046477522, distance: 1.3877848306267058 entropy 11.216314584420308
epoch: 50, step: 17
	action: tensor([[  -126.0492, -24191.1399, -10428.1202,   8863.3536,  10820.4018,
         -10197.5937,  20332.7772]], dtype=torch.float64)
	q_value: tensor([[-20.5273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2005968431184948, distance: 1.2538780268618643 entropy 11.146108828656793
epoch: 50, step: 18
	action: tensor([[-17166.6579, -21452.5656,  -3344.1723,  -1396.5381,  -6628.9662,
           -675.2903,  -7744.1127]], dtype=torch.float64)
	q_value: tensor([[-20.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5778728579047261, distance: 1.4374498308072474 entropy 11.13419496086985
epoch: 50, step: 19
	action: tensor([[ 13422.8607, -26807.7140, -14227.9967, -12795.6897,   1133.5713,
          10852.0688,  -9576.5791]], dtype=torch.float64)
	q_value: tensor([[-22.6122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.492135312711391, distance: 0.815512649069612 entropy 11.306507084545048
epoch: 50, step: 20
	action: tensor([[-27194.7583, -41845.3592, -19283.8348, -19664.9035,  31961.6433,
           1607.2601,   3167.6919]], dtype=torch.float64)
	q_value: tensor([[-26.9066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030781380686767612, distance: 1.1265943466611625 entropy 11.366154411472602
epoch: 50, step: 21
	action: tensor([[-17937.9276, -20618.2613,  -3286.8171,  18008.3393,  -2585.6645,
         -31802.8676,  10232.5095]], dtype=torch.float64)
	q_value: tensor([[-23.7743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7689198922771376, distance: 1.5219864155452045 entropy 11.255678209606392
epoch: 50, step: 22
	action: tensor([[-32587.3410, -40391.4061, -31332.0662,  19637.6675,  -4940.2847,
          -2971.7516, -14653.8776]], dtype=torch.float64)
	q_value: tensor([[-23.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1599762515023595, distance: 1.0488236588764421 entropy 11.31575970495314
epoch: 50, step: 23
	action: tensor([[-17458.5035,  -4921.8113,   3913.1694,  16401.2931, -18692.7150,
         -39946.4421, -22234.7879]], dtype=torch.float64)
	q_value: tensor([[-21.5799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8566460676453118, distance: 1.5592697525270685 entropy 11.256890020011394
epoch: 50, step: 24
	action: tensor([[ 3194.9055,  3377.2148,  8381.9442, 21799.0956,   -23.9640, -8809.9018,
         -4603.5492]], dtype=torch.float64)
	q_value: tensor([[-18.7048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29497845626435926, distance: 0.9608550728680354 entropy 11.141213345783322
epoch: 50, step: 25
	action: tensor([[-14321.2731, -10424.4480,  -6960.3002,   4727.4993, -31746.3625,
          -5919.3716, -12012.9785]], dtype=torch.float64)
	q_value: tensor([[-21.4312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12541452214106563, distance: 1.2139839660054306 entropy 11.034343934976516
epoch: 50, step: 26
	action: tensor([[-36292.5572, -27644.3941,   7421.8483,  10750.8019,  35953.6009,
           6288.7050,   4532.7348]], dtype=torch.float64)
	q_value: tensor([[-21.6863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18415265160646255, distance: 1.0336205767927673 entropy 11.254547301153774
epoch: 50, step: 27
	action: tensor([[  7439.1395,   8257.8609, -38211.6791,   3681.5423,  18876.7777,
          -8949.5392,  12351.5697]], dtype=torch.float64)
	q_value: tensor([[-20.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42736271814553695, distance: 0.8659573506407805 entropy 11.294078872804766
epoch: 50, step: 28
	action: tensor([[-12457.1618, -10806.1409,   -553.5728,   7065.1652,  26340.2377,
           2817.0962,  16585.3753]], dtype=torch.float64)
	q_value: tensor([[-26.0340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35445315228154306, distance: 1.3317990090536624 entropy 11.161416530671529
epoch: 50, step: 29
	action: tensor([[ -6729.9945,  41388.1260,   8895.4068,  26595.2982, -36753.6240,
          -3353.1294,   1006.1196]], dtype=torch.float64)
	q_value: tensor([[-22.1753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07187022905612939, distance: 1.102455440477738 entropy 11.351228495916393
epoch: 50, step: 30
	action: tensor([[  5986.0064,  -8323.5937,   6100.1157,  11337.7555,    374.7670,
         -21446.5626, -10374.7784]], dtype=torch.float64)
	q_value: tensor([[-24.6363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5933380611399672, distance: 0.7297489129609608 entropy 11.324226528794897
epoch: 50, step: 31
	action: tensor([[ -8121.5098,  -8986.5343, -15061.9475,  18081.3892, -30027.2682,
           9954.7650, -10750.1782]], dtype=torch.float64)
	q_value: tensor([[-22.8271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17563683029479493, distance: 1.0390010429312737 entropy 11.269029906640228
epoch: 50, step: 32
	action: tensor([[ 21464.6100, -35464.5258,   7296.6126,  -7113.2762,   -374.1265,
          33713.3049,  -6087.3190]], dtype=torch.float64)
	q_value: tensor([[-19.9492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1309696322702265, distance: 1.2169764247866988 entropy 11.1698060569084
epoch: 50, step: 33
	action: tensor([[16581.3675,  1199.5609, -7877.8264,  8962.5898, -7053.6680, -7215.9044,
          9265.0007]], dtype=torch.float64)
	q_value: tensor([[-19.4905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13868478149956198, distance: 1.062032369337807 entropy 11.031731362821075
epoch: 50, step: 34
	action: tensor([[-14080.8041,  22104.1632,  17980.7468,   9388.9906,   2355.9979,
          -3722.4943,  11673.7985]], dtype=torch.float64)
	q_value: tensor([[-23.3788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31647033320867923, distance: 0.9460963731565609 entropy 11.149356677330871
epoch: 50, step: 35
	action: tensor([[  9337.4757,  -9221.5587, -21664.4755,  18682.7362,   7344.6462,
         -11864.1601,   -549.5199]], dtype=torch.float64)
	q_value: tensor([[-20.3950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93871067861319
epoch: 50, step: 36
	action: tensor([[ 15677.5648, -13352.1178,  -2629.0582,  14015.4621,   4183.5616,
           3116.1004,  -7964.1089]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.727661287749038
epoch: 50, step: 37
	action: tensor([[-1854.5757, -7624.7726, -7201.5232,  3597.2849,  4901.9478, -1787.8179,
          6489.9793]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6099702765577546, distance: 1.4519966765881176 entropy 10.727661287749038
epoch: 50, step: 38
	action: tensor([[-21354.9307, -25193.9426,  -8085.5717,  -1788.3025,  -4240.2007,
           2947.9216,  27421.7369]], dtype=torch.float64)
	q_value: tensor([[-18.9446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4787236811564284, distance: 1.3915544589397464 entropy 11.104847173843057
epoch: 50, step: 39
	action: tensor([[  5222.6847, -26947.5368,  -8917.1556,  -6359.3731,   3088.1959,
          26693.0836,  -5483.4801]], dtype=torch.float64)
	q_value: tensor([[-17.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43271000767878154, distance: 0.8619047107256418 entropy 11.025194834514243
epoch: 50, step: 40
	action: tensor([[-24328.9066,   9357.5380,   7412.9562,  10891.9350,  -3396.3820,
          50595.7056, -27486.9893]], dtype=torch.float64)
	q_value: tensor([[-22.4933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.25367119843389
epoch: 50, step: 41
	action: tensor([[  5511.4421, -10971.4114,  -9062.6820,  -1370.1915,  -3989.7263,
         -15208.1706,  28907.7318]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40103158627659474, distance: 0.8856429168948217 entropy 10.727661287749038
epoch: 50, step: 42
	action: tensor([[-26265.2689, -16366.1210,  10503.3510,  18117.1905,  29915.4962,
         -22228.1430, -10408.0192]], dtype=torch.float64)
	q_value: tensor([[-23.4447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37724460436914753, distance: 1.3429573890543771 entropy 11.137453743092852
epoch: 50, step: 43
	action: tensor([[ 12687.7384, -16822.7278, -16183.7261,  32157.2157,  -8233.9148,
           1612.0015, -13343.2847]], dtype=torch.float64)
	q_value: tensor([[-21.8956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3525431651850661, distance: 1.3308596571426723 entropy 11.243200620837857
epoch: 50, step: 44
	action: tensor([[-27214.1442,  17468.5898, -40668.8653,  33068.7358,  12075.1274,
          23591.7054,  26307.3368]], dtype=torch.float64)
	q_value: tensor([[-25.4080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5487630614670271, distance: 0.7687037775770853 entropy 11.408384691768072
epoch: 50, step: 45
	action: tensor([[  6974.2890, -21137.9424,   1778.1047,  35682.2717,  35878.5552,
          11821.9001,  -5595.8137]], dtype=torch.float64)
	q_value: tensor([[-21.8706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24957226201302385, distance: 0.9913137554826427 entropy 11.154411931304766
epoch: 50, step: 46
	action: tensor([[  8769.2572, -10526.3915, -26540.8122,  14809.6952,  15033.7668,
          27640.0442,  19070.6238]], dtype=torch.float64)
	q_value: tensor([[-20.6353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3147744534435609, distance: 0.9472693081517541 entropy 11.053887026275419
epoch: 50, step: 47
	action: tensor([[-26527.2177, -20443.0687,    858.6280,  26587.3158,   5609.8858,
           3412.2063, -11095.8565]], dtype=torch.float64)
	q_value: tensor([[-22.0082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21704321118425984, distance: 1.012571245463249 entropy 11.199600892369501
epoch: 50, step: 48
	action: tensor([[ -4527.2159, -18383.8957,  13506.0573,  16145.4916,  17753.1804,
          -7553.7356,   9817.5031]], dtype=torch.float64)
	q_value: tensor([[-19.6225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09771245682893692, distance: 1.086999073899213 entropy 11.22786161591543
epoch: 50, step: 49
	action: tensor([[-41987.9625, -25121.1515,  21309.9275, -64197.1454, -22438.6806,
         -41747.7530,  37429.5752]], dtype=torch.float64)
	q_value: tensor([[-25.3986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9177453407544598, distance: 1.584718622252409 entropy 11.604149174841242
epoch: 50, step: 50
	action: tensor([[ -6018.5958, -39648.4699,   1890.7629,  37360.0186,   4602.4904,
         -15696.6648,  26484.5718]], dtype=torch.float64)
	q_value: tensor([[-23.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05856489868965853, distance: 1.11032952074764 entropy 11.295899682774243
epoch: 50, step: 51
	action: tensor([[ -7273.4214, -46572.8263,   8106.3525,  10625.7490, -28782.2876,
         -18910.8119,  -6231.3226]], dtype=torch.float64)
	q_value: tensor([[-21.8194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7229956208276607, distance: 1.502099771634612 entropy 11.394692419428695
epoch: 50, step: 52
	action: tensor([[-7731.0562, -4984.3672, 22616.0188,  5868.6761, 14680.5791, 33978.5017,
         -6621.9843]], dtype=torch.float64)
	q_value: tensor([[-19.7264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26323481152205774, distance: 1.286171067502349 entropy 11.164223582040838
epoch: 50, step: 53
	action: tensor([[ -8277.9496,  16292.4967,   7888.8703, -16611.8712,  23836.0726,
         -10662.4081,   3114.8361]], dtype=torch.float64)
	q_value: tensor([[-22.2100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21862817693141812, distance: 1.011545835252076 entropy 11.359161709809174
epoch: 50, step: 54
	action: tensor([[  1282.6930, -11389.9721, -23264.8980,  -6032.4532, -29515.8579,
          11753.9572, -11575.2303]], dtype=torch.float64)
	q_value: tensor([[-34.2481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2777049119210403, distance: 0.9725546709541246 entropy 11.316597386829232
epoch: 50, step: 55
	action: tensor([[17200.8660, -8156.5258, 32367.1808, 20325.4398,  6071.9297, -9502.1892,
          1704.2728]], dtype=torch.float64)
	q_value: tensor([[-25.7941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26831889214066007, distance: 1.2887566641174903 entropy 11.28143170361969
epoch: 50, step: 56
	action: tensor([[  2043.2399,  30367.9427, -22997.5841,  -6559.5422,  34610.1464,
            191.1341,  17287.7214]], dtype=torch.float64)
	q_value: tensor([[-26.5480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2872767338606055, distance: 1.2983526037632964 entropy 11.289846586920529
epoch: 50, step: 57
	action: tensor([[-27409.7720,  -1395.9304, -22006.5247,  25311.7358,  16789.3972,
           1013.3315,   8757.9072]], dtype=torch.float64)
	q_value: tensor([[-26.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.231158894536517
epoch: 50, step: 58
	action: tensor([[  6348.5685,  -4152.7892, -20035.9038, -10038.8471,   1846.7329,
           7094.7940, -15341.0930]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2692680412591606, distance: 1.2892387958174278 entropy 10.727661287749038
epoch: 50, step: 59
	action: tensor([[ 21271.4176,  10659.5798, -24531.6031, -14575.4904,   7097.6348,
          -5343.6335,  -4933.8351]], dtype=torch.float64)
	q_value: tensor([[-19.3238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24391281921031138, distance: 0.99504479130018 entropy 10.939844412376967
epoch: 50, step: 60
	action: tensor([[  -907.6420,  10702.8947,   1188.7181, -16500.1014,    811.5537,
          23128.4786,  10651.4765]], dtype=torch.float64)
	q_value: tensor([[-27.6095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.146460402241031
epoch: 50, step: 61
	action: tensor([[ -3897.0281,  -6642.1838,  -9095.4047,     62.8203,   1092.7266,
         -26206.2977,    327.5274]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3276964146133645, distance: 1.3185787866061291 entropy 10.727661287749038
epoch: 50, step: 62
	action: tensor([[-19068.9933,  -4666.8784,  -7782.1735,  15889.2024,   6593.5182,
          30474.0685,   8281.1168]], dtype=torch.float64)
	q_value: tensor([[-18.5641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05439835721839148, distance: 1.1750573235292419 entropy 11.07427645714907
epoch: 50, step: 63
	action: tensor([[-38357.6356, -12149.9010,  38944.1208,   6231.8419, -10974.1695,
          16707.7961,   6788.8213]], dtype=torch.float64)
	q_value: tensor([[-20.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2656526748276895, distance: 1.2874013610197435 entropy 11.233907439996399
epoch: 50, step: 64
	action: tensor([[-10490.6910, -12695.2901,  12635.3944,  15888.6246,   8628.7112,
           -158.8787,   3581.6650]], dtype=torch.float64)
	q_value: tensor([[-21.8453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3435992799705587, distance: 1.3264521087135 entropy 11.362885758677654
epoch: 50, step: 65
	action: tensor([[-22854.5315, -15713.2352,  15611.3654,   9531.7834,  -7944.9802,
          -1776.4733,  10306.0192]], dtype=torch.float64)
	q_value: tensor([[-17.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08409903668215146, distance: 1.0951685185183127 entropy 11.103555308756844
epoch: 50, step: 66
	action: tensor([[  6277.8654,  12759.4423, -16802.7260,   4174.0125,    300.0802,
           8200.3114,  22650.6440]], dtype=torch.float64)
	q_value: tensor([[-23.0063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7555239000216465, distance: 0.5658155746575638 entropy 11.46807890295706
epoch: 50, step: 67
	action: tensor([[-20323.6977, -23800.8534, -19195.3895,  -8287.4930,  12328.9345,
         -36014.0599,  -9725.3886]], dtype=torch.float64)
	q_value: tensor([[-20.3132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.085178126210776
epoch: 50, step: 68
	action: tensor([[-10336.1853,   3178.6781,  10435.7017,    409.7771,  -6045.3624,
           6180.0024,    437.3431]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02247788354368052, distance: 1.1314099384510012 entropy 10.727661287749038
epoch: 50, step: 69
	action: tensor([[  9226.6881,    118.9287, -20402.1601, -20156.2087,  19498.4307,
           2722.8981,  11593.2327]], dtype=torch.float64)
	q_value: tensor([[-22.0002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4224788682679722, distance: 0.8696422544016035 entropy 11.093580136734262
epoch: 50, step: 70
	action: tensor([[  -604.6409,   2086.1320,  -5300.7499,  17783.1557,  -4747.4249,
         -13123.7848,  10522.9380]], dtype=torch.float64)
	q_value: tensor([[-21.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6336960706474521, distance: 0.6925920178147149 entropy 10.785288501428795
epoch: 50, step: 71
	action: tensor([[-1363.1570, -6486.6914, -3100.7270,  5599.5282,  1937.3530,  3101.2464,
         16109.0080]], dtype=torch.float64)
	q_value: tensor([[-19.5026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1942946988873706, distance: 1.2505827835708827 entropy 10.981540350801344
epoch: 50, step: 72
	action: tensor([[-11512.9090,   9631.8626, -26693.9191, -21581.5093,  40428.0215,
           8681.4596,   1530.1405]], dtype=torch.float64)
	q_value: tensor([[-22.0196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37463662077618987, distance: 0.9049465238460049 entropy 11.314929482023402
epoch: 50, step: 73
	action: tensor([[-11062.8914, -32396.0167,  25569.7850,  30235.9192,   1766.1997,
         -46152.7704, -12994.1366]], dtype=torch.float64)
	q_value: tensor([[-33.6878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2460451882529553, distance: 1.2773902279974252 entropy 11.291580200173433
epoch: 50, step: 74
	action: tensor([[ 8705.5550, -2831.4028,  3043.2296,  7341.6521, 23441.5755, 31237.3624,
         12973.7036]], dtype=torch.float64)
	q_value: tensor([[-20.2594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5425030446212207, distance: 0.774017531152799 entropy 11.265384507965027
epoch: 50, step: 75
	action: tensor([[-16056.0425, -34852.3160, -10858.1898,  28154.8381,  -1325.2773,
         -15459.4522,  16771.7823]], dtype=torch.float64)
	q_value: tensor([[-23.6008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7985020121555177, distance: 1.5346599422351057 entropy 11.121971741760843
epoch: 50, step: 76
	action: tensor([[-12409.9152, -14132.1076,  23654.0307,  -3421.4212,  -3096.6559,
          -5692.3892,  28289.5591]], dtype=torch.float64)
	q_value: tensor([[-17.8316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.057037258458816575, distance: 1.1765268452037603 entropy 11.06294892767564
epoch: 50, step: 77
	action: tensor([[  4655.7596, -36212.7091,  15731.2033,  20625.4467,  42790.0768,
          30231.5086,  24540.1217]], dtype=torch.float64)
	q_value: tensor([[-21.2639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47442380751433577, distance: 0.8296110642222712 entropy 11.237073544773406
epoch: 50, step: 78
	action: tensor([[ 15244.6478,  -3894.3403,    612.3913,   -890.5886,  23462.4470,
           8366.2219, -14470.1097]], dtype=torch.float64)
	q_value: tensor([[-23.0845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018069439781320584, distance: 1.1339582932050665 entropy 11.293703314224548
epoch: 50, step: 79
	action: tensor([[ -4034.1466,   2654.6000, -20806.1108,  -2303.6022,   1278.9886,
          42125.5640,  -9136.9572]], dtype=torch.float64)
	q_value: tensor([[-23.7028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.121707908971372
epoch: 50, step: 80
	action: tensor([[  9712.3073,   5008.4750,  -7816.7049,   7519.7845,    -56.5336,
         -10314.0945,   3738.6247]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4423212227524955, distance: 0.8545721828830258 entropy 10.727661287749038
epoch: 50, step: 81
	action: tensor([[-13296.7713,   9496.2525,   7212.0165,   5672.7939, -19613.1081,
           2450.7856,  19913.5128]], dtype=torch.float64)
	q_value: tensor([[-20.7025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23826130646272614, distance: 0.9987566905820642 entropy 10.877037069079728
epoch: 50, step: 82
	action: tensor([[-14875.7327,  14601.7951, -23894.7921,  20837.3765,  22680.4176,
          18546.6721,  -3408.3891]], dtype=torch.float64)
	q_value: tensor([[-22.8282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3095443024056136, distance: 0.9508775679759629 entropy 11.238784254380949
epoch: 50, step: 83
	action: tensor([[  -587.3158,   8730.1505,   2883.3352, -10871.3914, -16638.6259,
         -12330.8194,   4417.4692]], dtype=torch.float64)
	q_value: tensor([[-25.8012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3705024076997696, distance: 0.907932850667746 entropy 11.336231964433756
epoch: 50, step: 84
	action: tensor([[-13521.4272,  16621.1277, -11739.6836,   3879.3478,   1882.1755,
           7250.9043,  16522.0059]], dtype=torch.float64)
	q_value: tensor([[-20.4043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026437357365937775, distance: 1.1593722955088486 entropy 10.920535701994641
epoch: 50, step: 85
	action: tensor([[-36012.1464,    326.1323, -69424.1223,  29326.7243,  -9005.1675,
         -29719.7404,  41171.8068]], dtype=torch.float64)
	q_value: tensor([[-30.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48463794972476837, distance: 0.8215101069666175 entropy 11.524278471540404
epoch: 50, step: 86
	action: tensor([[-34586.4109, -13146.6162, -13110.5653,   8214.0723,   3222.9295,
         -12027.7634, -54099.7209]], dtype=torch.float64)
	q_value: tensor([[-24.1649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5584029192257529, distance: 1.4285536981698836 entropy 11.230915759489173
epoch: 50, step: 87
	action: tensor([[ 16002.9516, -40029.3818,   -272.9255,  23399.4805,  11412.8897,
         -12353.3341,  14403.0306]], dtype=torch.float64)
	q_value: tensor([[-21.7494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17285912058818342, distance: 1.0407500392043523 entropy 11.336880637795561
epoch: 50, step: 88
	action: tensor([[  -731.4843,   8563.9219, -39816.0545,  55974.8910,  -4772.8655,
         -12786.3432,  30154.2654]], dtype=torch.float64)
	q_value: tensor([[-25.5576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44159517385660485, distance: 0.8551282909868075 entropy 11.394075105150693
epoch: 50, step: 89
	action: tensor([[ -5444.0604,   8072.7106, -29361.2642,  23303.3568,  20063.2696,
          50837.0416,   8558.7766]], dtype=torch.float64)
	q_value: tensor([[-23.9645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4980064235463588, distance: 0.8107851266445868 entropy 11.294392865030188
epoch: 50, step: 90
	action: tensor([[  5141.4660, -29316.8616,  16131.0511,  15681.3449,  18412.6327,
          12886.5894,  33886.4095]], dtype=torch.float64)
	q_value: tensor([[-21.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13384844206692348, distance: 1.0650098849194531 entropy 11.073501822032062
epoch: 50, step: 91
	action: tensor([[ 24250.6167, -55244.2555,   4453.0162,   8726.1538, -45999.6694,
         -37818.2329,   6324.2975]], dtype=torch.float64)
	q_value: tensor([[-23.1084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1720621462293821, distance: 1.0412513149917828 entropy 11.204798284454919
epoch: 50, step: 92
	action: tensor([[-21424.7363, -40387.7602,   2855.3380,  -2064.1218, -27302.8566,
           4226.3221,  -7149.5041]], dtype=torch.float64)
	q_value: tensor([[-23.6301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1567997852547487, distance: 1.050804798942267 entropy 11.268044323406533
epoch: 50, step: 93
	action: tensor([[ 1830.6182,  6314.8298,  6454.6422, 16136.0629, 17786.2545, -5338.6384,
         13832.1319]], dtype=torch.float64)
	q_value: tensor([[-20.8611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7162549730565394, distance: 0.6095661226426284 entropy 11.19409633288329
epoch: 50, step: 94
	action: tensor([[-15002.3196,   6759.1706, -16909.9284,  -5098.9521,  -3628.1230,
         -28211.5472,  29623.6904]], dtype=torch.float64)
	q_value: tensor([[-23.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3410667277662941, distance: 0.9289180716993313 entropy 11.239709933498517
epoch: 50, step: 95
	action: tensor([[  -324.4191, -10740.0552, -33570.2552, -10457.2433,  -9604.9148,
          -2635.8674, -21521.3657]], dtype=torch.float64)
	q_value: tensor([[-30.4881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.280355811782377
epoch: 50, step: 96
	action: tensor([[ -7986.2139,  -2815.7797,  -8472.5758,  -3944.6563,  -4596.3903,
         -10449.8440,    900.1615]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.727661287749038
epoch: 50, step: 97
	action: tensor([[ -9027.8954, -23707.0755,  -3388.5345, -23815.9113,  -3791.6316,
          -6364.7942,  -2164.5475]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5116278769954041, distance: 1.40695154103959 entropy 10.727661287749038
epoch: 50, step: 98
	action: tensor([[-21082.4958, -23489.6075,  -2328.0596,  26244.0594,  30131.2308,
          23948.9370,  19322.9750]], dtype=torch.float64)
	q_value: tensor([[-28.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8926014404876947, distance: 1.5742955810780659 entropy 11.402088046233006
epoch: 50, step: 99
	action: tensor([[-20670.8577,   1267.7932, -24669.7769,  37987.7506, -16215.2302,
         -30249.7998,  11549.2936]], dtype=torch.float64)
	q_value: tensor([[-23.0941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2117131584414227, distance: 1.2596694746701118 entropy 11.458666661107314
epoch: 50, step: 100
	action: tensor([[  2177.1836,   2454.3421,   3551.5949,  -3980.5858,  16423.4877,
         -13048.4994,  14659.5508]], dtype=torch.float64)
	q_value: tensor([[-24.0983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3501671371560706, distance: 0.9224812092947087 entropy 11.246087857576441
epoch: 50, step: 101
	action: tensor([[  3223.9992,  25186.7757,   -246.0597,  -5477.2036, -14568.1597,
         -17565.5054, -12079.2319]], dtype=torch.float64)
	q_value: tensor([[-22.2374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2835352469063126, distance: 0.9686215079868473 entropy 10.978218106102775
epoch: 50, step: 102
	action: tensor([[ 6973.8178, 23890.4455,   761.9446, 11745.9488,  7848.0417,   145.4949,
         27588.0732]], dtype=torch.float64)
	q_value: tensor([[-32.2643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34528643309288387, distance: 0.9259389717983698 entropy 11.392274888919117
epoch: 50, step: 103
	action: tensor([[-20471.6974, -22439.2329, -15879.8747,   -917.7549,   5827.1439,
           4506.7122,   4867.6039]], dtype=torch.float64)
	q_value: tensor([[-25.1733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.054951106241445
epoch: 50, step: 104
	action: tensor([[  3800.0487,  -9797.4845,  -6550.1597,   6401.2789,  -2865.4277,
          12186.5577, -17060.3087]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3261860047090116, distance: 0.9393484148667343 entropy 10.727661287749038
epoch: 50, step: 105
	action: tensor([[-9882.5851,  6302.7048,  1182.4167, -6057.1474, 43240.0822, 24681.8171,
         11686.0958]], dtype=torch.float64)
	q_value: tensor([[-22.3005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.315960830042519
epoch: 50, step: 106
	action: tensor([[  4030.3700, -24778.6728,    887.4146, -27054.1299, -10854.4474,
           6488.1273,  33433.0009]], dtype=torch.float64)
	q_value: tensor([[-25.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41784684887916135, distance: 0.8731227806793006 entropy 10.727661287749038
epoch: 50, step: 107
	action: tensor([[-19986.1967,  -7881.2417, -30176.8653,  -2276.1123,  -5107.3503,
          -5151.2396, -12235.9077]], dtype=torch.float64)
	q_value: tensor([[-23.5912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011968649715762836, distance: 1.1374755119761126 entropy 11.249602557909332
epoch: 50, step: 108
	action: tensor([[  5479.3605,   2131.4779,   6742.0829,  -3825.5637,  19081.0347,
         -27236.8976,   4450.8151]], dtype=torch.float64)
	q_value: tensor([[-22.0015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3592692598941374, distance: 0.9159978905017943 entropy 11.088157773296343
epoch: 50, step: 109
	action: tensor([[ -3399.6614, -21394.0890,  24361.8806,  34210.7926,   6032.1625,
           2459.2144,  -3930.6758]], dtype=torch.float64)
	q_value: tensor([[-26.0147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2666643888945637, distance: 0.979959394745235 entropy 11.149906738559205
epoch: 50, step: 110
	action: tensor([[-41295.6086, -26100.1616,   7005.6261,   5375.6728,   5709.6086,
           9124.5450,  -9105.7464]], dtype=torch.float64)
	q_value: tensor([[-18.2237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.659642165164755, distance: 1.4742255145455843 entropy 11.132537057209708
epoch: 50, step: 111
	action: tensor([[-27651.0631,  12418.9095,  18883.4839,  35177.8003,   7304.4915,
            852.3873,  21752.4206]], dtype=torch.float64)
	q_value: tensor([[-21.0712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6535361917063045, distance: 0.6735744895565918 entropy 11.253232713897962
epoch: 50, step: 112
	action: tensor([[ -5358.9507,   4198.2527, -33642.6697,   8534.6853,  -9113.4118,
          36013.5776, -14616.7007]], dtype=torch.float64)
	q_value: tensor([[-23.7910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010755808330342509, distance: 1.1504819678529727 entropy 11.278821717708272
epoch: 50, step: 113
	action: tensor([[-40283.7563,  32096.7006,  -2829.4345,  20223.0841,  -8766.8642,
          37549.7117,   6749.6634]], dtype=torch.float64)
	q_value: tensor([[-30.2346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18283733341013675, distance: 1.244569646950636 entropy 11.507419195668914
epoch: 50, step: 114
	action: tensor([[-16845.0130, -34893.1023,  -4984.4730, -12636.2022,  14162.2674,
         -14317.9702, -30801.5603]], dtype=torch.float64)
	q_value: tensor([[-26.4072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12781659602143192, distance: 1.2152788332584026 entropy 11.360173462143747
epoch: 50, step: 115
	action: tensor([[-10808.1863, -44351.9488,   1125.2932,  19975.9923, -10864.9291,
          45151.3192,  -4590.2481]], dtype=torch.float64)
	q_value: tensor([[-22.0389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6980170646247061, distance: 1.4911719253041111 entropy 11.15584642553111
epoch: 50, step: 116
	action: tensor([[-23215.4567, -16010.9789, -28208.1975,   4949.6307,  -3141.1202,
         -29561.7716,  27749.5953]], dtype=torch.float64)
	q_value: tensor([[-19.0117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23811870612356079, distance: 1.2733208071060536 entropy 11.250275424640394
epoch: 50, step: 117
	action: tensor([[ 16434.8499, -13342.1339,  -1490.1185,  38497.5168, -15173.0060,
         -12875.7061, -14824.5368]], dtype=torch.float64)
	q_value: tensor([[-22.3294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2789577082935204, distance: 1.2941505020521233 entropy 11.359922600798102
epoch: 50, step: 118
	action: tensor([[ 17953.2483, -40081.9304,  -2806.6906,  18094.8401, -16885.3691,
          22376.1410,    225.5790]], dtype=torch.float64)
	q_value: tensor([[-23.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4201280198028281, distance: 1.3637050270260964 entropy 11.287245641992637
epoch: 50, step: 119
	action: tensor([[-23940.7328, -16042.7948,   7474.7454,  23861.0815,  -3554.6293,
           2819.5796,  -7496.0754]], dtype=torch.float64)
	q_value: tensor([[-26.9567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4619601215637372, distance: 1.3836442940332514 entropy 11.313929238079853
epoch: 50, step: 120
	action: tensor([[ -1635.5509, -19961.6421,   7846.3170,  -8181.2900,  28752.6097,
          15694.6289,  11665.5159]], dtype=torch.float64)
	q_value: tensor([[-20.6345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9384351432791569, distance: 1.5932441432343862 entropy 11.246404719159882
epoch: 50, step: 121
	action: tensor([[-30042.1490,  -3015.5105, -18423.4050,  -3601.3217,   4109.7823,
          -6635.7447,   5756.5704]], dtype=torch.float64)
	q_value: tensor([[-21.1135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13304347069342104, distance: 1.0655046617097723 entropy 11.110308948288443
epoch: 50, step: 122
	action: tensor([[ 5338.4303, -3801.9768, 42595.2794, 35387.9777, 21500.5308, -8946.0577,
         10878.6571]], dtype=torch.float64)
	q_value: tensor([[-23.5617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1869542895579035, distance: 1.246733675671442 entropy 11.25776163053803
epoch: 50, step: 123
	action: tensor([[ -7045.2324, -14651.1252, -33726.0708,   2874.2595,  13984.1584,
         -15486.2951,   6230.7567]], dtype=torch.float64)
	q_value: tensor([[-19.0770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04816042756619532, distance: 1.1164481837109317 entropy 11.033419755802308
epoch: 50, step: 124
	action: tensor([[-32280.9502,   7671.1828, -15575.7433,  24668.9322,  -8919.1166,
         -10056.2944,  20093.0922]], dtype=torch.float64)
	q_value: tensor([[-23.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1291659743564917, distance: 1.0678847586481421 entropy 11.361303006130774
epoch: 50, step: 125
	action: tensor([[-36832.3143,  -2463.4084,   4204.9386,  -7507.0584, -46573.4722,
          23040.2336, -35301.7982]], dtype=torch.float64)
	q_value: tensor([[-24.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8075693029332125, distance: 1.5385236335913168 entropy 11.217687630822699
epoch: 50, step: 126
	action: tensor([[-11714.4747, -18643.6622,  -8483.9442,  -4909.0402,  10050.0092,
           5976.6424, -37718.3308]], dtype=torch.float64)
	q_value: tensor([[-23.3234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08523281782998338, distance: 1.0944904616252473 entropy 11.277979939970805
epoch: 50, step: 127
	action: tensor([[-2737.3180, 14454.1504,  4930.6803, 29538.5704, 11981.0078,  2342.1997,
         34052.4982]], dtype=torch.float64)
	q_value: tensor([[-25.0748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15864624276424943, distance: 1.2317770893500135 entropy 11.275988295168405
LOSS epoch 50 actor 222.56291164377006 critic 287.5455205785711
epoch: 51, step: 0
	action: tensor([[ 18954.1903,  14319.9301,  -1796.1485,  20170.5651,   2257.9776,
         -27796.4086,  13178.9473]], dtype=torch.float64)
	q_value: tensor([[-26.5230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4983339929513677, distance: 0.8105205498106448 entropy 11.119382877122518
epoch: 51, step: 1
	action: tensor([[-4.3567e+00, -1.7508e+04,  7.5519e+03,  1.4445e+04,  5.2744e+03,
          3.3654e+04, -1.9817e+03]], dtype=torch.float64)
	q_value: tensor([[-26.3132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.330281699037304
epoch: 51, step: 2
	action: tensor([[ 18604.7369,   5787.6519, -15033.4400, -11969.2997,  -5982.0370,
           2188.7266,  13460.4349]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29635384236921103, distance: 0.9599173767865229 entropy 10.771082707966375
epoch: 51, step: 3
	action: tensor([[ -1790.7866,    630.1584, -18114.4086,   2624.7523,   6905.0098,
          12952.1700,  -3632.3928]], dtype=torch.float64)
	q_value: tensor([[-24.8878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6446012729374786, distance: 0.682204574528108 entropy 11.141615483368076
epoch: 51, step: 4
	action: tensor([[ -7677.1609,   5016.0655, -10882.9923,  -4098.3112,   4418.4170,
         -19519.4482,  19083.8482]], dtype=torch.float64)
	q_value: tensor([[-18.9704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45547545059434613, distance: 1.380572237018022 entropy 10.745572495630796
epoch: 51, step: 5
	action: tensor([[ -5787.7989, -19216.3553, -18004.9230,  -5961.0336,   8007.6683,
            494.1638,  -4070.4988]], dtype=torch.float64)
	q_value: tensor([[-27.2184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039068259394292926, distance: 1.1217677822385972 entropy 10.997334740950238
epoch: 51, step: 6
	action: tensor([[ -3733.1950,  -6077.6598,  -8473.7861, -24663.3291,  11033.3874,
          -3642.7525,  -4323.3620]], dtype=torch.float64)
	q_value: tensor([[-25.2245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10792366227321126, distance: 1.0808307778700312 entropy 11.394848477613973
epoch: 51, step: 7
	action: tensor([[-20835.3605,   3703.9794,  -7791.4198, -30210.1339,   3649.7618,
          14316.9087,  25477.4651]], dtype=torch.float64)
	q_value: tensor([[-21.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2017974319561049, distance: 1.2545048050405567 entropy 11.203429509938719
epoch: 51, step: 8
	action: tensor([[   507.8206, -33330.8189, -27964.0572,   8155.2407,  27454.2908,
          25254.2911,   5207.9610]], dtype=torch.float64)
	q_value: tensor([[-30.1453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10819301204816067, distance: 1.2046597415318918 entropy 11.02990525218348
epoch: 51, step: 9
	action: tensor([[ -9013.9295, -25562.2048, -29557.5449, -33363.1645, -11235.5858,
          37592.8280,   4547.3826]], dtype=torch.float64)
	q_value: tensor([[-23.5335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09214644394419369, distance: 1.195906238164988 entropy 11.24540759569594
epoch: 51, step: 10
	action: tensor([[-14837.0287,  -3283.0528, -21455.5496, -20183.3646,  -5974.2216,
          12858.6830, -21002.2619]], dtype=torch.float64)
	q_value: tensor([[-26.2807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.663333402763755, distance: 1.4758640286692875 entropy 11.324381919085642
epoch: 51, step: 11
	action: tensor([[  7254.3532,  16140.9869,  -5141.6076,  32505.9059,  -4849.4501,
          45232.5729, -11787.8386]], dtype=torch.float64)
	q_value: tensor([[-25.8171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.353109949517137
epoch: 51, step: 12
	action: tensor([[-3164.3712, -8677.3589, 14293.7869, 17243.2777,  5956.1813, -7606.2119,
         11640.5118]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7100835076700172, distance: 1.4964608263574493 entropy 10.771082707966375
epoch: 51, step: 13
	action: tensor([[  5644.7705,  10081.9674, -11617.9887,   -215.4229,  -7211.9286,
         -13269.7023,  22713.1152]], dtype=torch.float64)
	q_value: tensor([[-19.2024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.037205194895654
epoch: 51, step: 14
	action: tensor([[ -183.2000, -6317.8075,  2845.1306,  3526.3752, 13532.7575, 11798.6660,
         -5551.4401]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15171484123454437, distance: 1.2280871153031052 entropy 10.771082707966375
epoch: 51, step: 15
	action: tensor([[-22446.2136, -23965.9643,  29277.7909,  15344.4520,  21702.1851,
          22938.4162,  17719.3960]], dtype=torch.float64)
	q_value: tensor([[-20.2302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04246721769270967, distance: 1.1197820951318311 entropy 11.283204806567781
epoch: 51, step: 16
	action: tensor([[ -5382.1706,  -4878.5214, -23408.8386, -20256.1818,  25349.7894,
           8154.9707,  16616.3862]], dtype=torch.float64)
	q_value: tensor([[-20.3358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6311588408250466, distance: 1.4615201939824036 entropy 11.271544452447175
epoch: 51, step: 17
	action: tensor([[-32916.7455, -18936.4006,   5695.4665,  -9170.5612,  -7092.5542,
          -6426.6907,  10708.8155]], dtype=torch.float64)
	q_value: tensor([[-22.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1409446060548385, distance: 1.2223314132342662 entropy 11.380083125957395
epoch: 51, step: 18
	action: tensor([[  5758.7099,  13272.5266,  -3495.8628,  19442.3425,  -3846.5236,
         -57089.8927,  32014.1399]], dtype=torch.float64)
	q_value: tensor([[-22.8929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.377267940689306
epoch: 51, step: 19
	action: tensor([[-10960.1160, -17775.7493,  -8178.8247,  13686.0690,  -7412.0967,
          -7065.4476,  -9360.7596]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006855175994310403, distance: 1.1404151681468178 entropy 10.771082707966375
epoch: 51, step: 20
	action: tensor([[-20351.6181,  -1786.1953, -16936.5278,  31963.3856, -33565.2074,
          22947.6027,  24128.2734]], dtype=torch.float64)
	q_value: tensor([[-22.4524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19884173025485907, distance: 1.0242733029301863 entropy 11.307089106489551
epoch: 51, step: 21
	action: tensor([[ 11134.3511, -14391.5913,   2138.9365,   6165.0302,  -2790.9689,
           3747.8104,  -1731.2105]], dtype=torch.float64)
	q_value: tensor([[-21.5660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3367013945704822, distance: 0.9319899632081605 entropy 11.296616474730731
epoch: 51, step: 22
	action: tensor([[ -3694.2121,  17163.6032, -15617.9524,  35651.7910, -12964.5011,
         -33739.0250, -18354.8724]], dtype=torch.float64)
	q_value: tensor([[-25.2537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08765293963389476, distance: 1.19344349668073 entropy 11.318727321577043
epoch: 51, step: 23
	action: tensor([[-7483.6361, -5035.2870, 12940.4726, 20104.0894, 11499.3169, 11281.2062,
          9834.2227]], dtype=torch.float64)
	q_value: tensor([[-25.4655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26177076698972424, distance: 1.2854255380076547 entropy 11.36276564093379
epoch: 51, step: 24
	action: tensor([[-39817.0912, -16593.5079,  12356.1725, -11292.6027,  22363.4423,
          17049.7224,   4047.5080]], dtype=torch.float64)
	q_value: tensor([[-21.8106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9475693076581846, distance: 1.5969935207568269 entropy 11.30439946623615
epoch: 51, step: 25
	action: tensor([[ -6860.4482,  -4076.7986,   7228.1144,  19377.9809, -20854.3200,
          -9879.5607,  10523.5139]], dtype=torch.float64)
	q_value: tensor([[-22.7599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3406076751857643, distance: 1.3249745730123963 entropy 11.313994424396364
epoch: 51, step: 26
	action: tensor([[-36248.1890, -18522.8999,   9173.4328, -11612.1374, -15572.2243,
         -40878.3473, -28907.9477]], dtype=torch.float64)
	q_value: tensor([[-20.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6720336623440113, distance: 1.4797188340559573 entropy 11.217786984856852
epoch: 51, step: 27
	action: tensor([[-2.7904e+04, -4.6702e+04,  1.9580e+03, -3.8460e+01,  5.9396e+03,
         -3.2009e+04,  2.9155e+04]], dtype=torch.float64)
	q_value: tensor([[-21.0793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39163733206193596, distance: 1.3499563577644003 entropy 11.22613528892912
epoch: 51, step: 28
	action: tensor([[ -5792.6380,  12463.5691, -36651.0387,  16905.0055, -15858.3038,
          29026.8839, -22575.2095]], dtype=torch.float64)
	q_value: tensor([[-22.2700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3327188810104843, distance: 0.9347836582324999 entropy 11.23345459537447
epoch: 51, step: 29
	action: tensor([[-12953.7283,    707.7721,  -5945.9456,  28477.8542,   4456.0555,
         -11191.0486,  22972.7771]], dtype=torch.float64)
	q_value: tensor([[-22.6864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42079415371252393, distance: 1.364024823427383 entropy 11.023236462173559
epoch: 51, step: 30
	action: tensor([[-36484.5600, -15722.2056,  12162.5556,   5603.9155,  11079.9053,
          21995.0908,  -2203.4893]], dtype=torch.float64)
	q_value: tensor([[-24.5030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9291846852748793, distance: 1.5894380155562227 entropy 11.271020126850056
epoch: 51, step: 31
	action: tensor([[-15058.5338,  -1089.6489,  28449.1514, -25537.6441, -12268.9383,
          -9423.2112,   8372.9114]], dtype=torch.float64)
	q_value: tensor([[-22.1608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7130776372515355, distance: 1.4977703057160856 entropy 11.419305055897839
epoch: 51, step: 32
	action: tensor([[ -5763.7038, -19083.9605,  -4890.4958,   1924.3814,   6467.7592,
          34207.5003,  15816.4900]], dtype=torch.float64)
	q_value: tensor([[-24.4134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0075397544965217, distance: 1.6213947795251253 entropy 11.312933311705702
epoch: 51, step: 33
	action: tensor([[ 33565.9012,  11325.3821, -20354.8769,   5573.3719,   8606.4710,
          -4572.7202,  10985.5437]], dtype=torch.float64)
	q_value: tensor([[-21.4224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.833376044448789, distance: 0.4671167206628332 entropy 11.384356517254659
epoch: 51, step: 34
	action: tensor([[ 44984.2021,   2132.2597,  -2695.6948,   4969.2794,  14621.7769,
         -47749.4199,   2483.9985]], dtype=torch.float64)
	q_value: tensor([[-24.3323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24342426926354643, distance: 0.99536621625538 entropy 11.334898043466255
epoch: 51, step: 35
	action: tensor([[ 24624.9878, -10144.1338,  34006.5727,   2306.5820, -31188.8024,
         -37609.7647,  16584.5254]], dtype=torch.float64)
	q_value: tensor([[-24.0855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22044559516483875, distance: 1.264200347606949 entropy 11.102492908459608
epoch: 51, step: 36
	action: tensor([[-29764.0483, -39186.5180,  28409.2702, -21970.5903,   2948.7042,
          11329.5606,   9823.1822]], dtype=torch.float64)
	q_value: tensor([[-29.8105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5806938571525508, distance: 1.4387342289883331 entropy 11.306938869801202
epoch: 51, step: 37
	action: tensor([[-36109.3973, -29005.4369,   7642.8377,  18077.2635,  -5921.7035,
          -1303.1286,  20101.5689]], dtype=torch.float64)
	q_value: tensor([[-22.1339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3693714858125664, distance: 0.9087480566234218 entropy 11.322136247006751
epoch: 51, step: 38
	action: tensor([[-24459.4251, -48430.7701, -12526.6280,   3613.1288, -29186.8602,
         -33555.2952, -33957.6892]], dtype=torch.float64)
	q_value: tensor([[-22.0733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027805276578109828, distance: 1.1601445781968343 entropy 11.436236670133786
epoch: 51, step: 39
	action: tensor([[ 23633.0516,  10055.7910,  12389.3718,  -5719.0980, -30785.3337,
          15954.7843,   2127.6940]], dtype=torch.float64)
	q_value: tensor([[-22.7992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.439341988184093
epoch: 51, step: 40
	action: tensor([[ 6820.4317, -5906.5769,    94.5066,  3052.3633,  -381.5847,  9182.2424,
         31836.5813]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.771082707966375
epoch: 51, step: 41
	action: tensor([[-17116.8873, -13033.6993, -18771.5210,  17562.4884,  15113.3234,
           5612.5236,   1514.3307]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1310099385375494, distance: 1.2169981103115246 entropy 10.771082707966375
epoch: 51, step: 42
	action: tensor([[  9345.1631, -42714.9739,  -4900.4165,  47784.1191, -44248.9579,
          22915.2549,  15459.3513]], dtype=torch.float64)
	q_value: tensor([[-21.2724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35072493602151533, distance: 0.9220852078998565 entropy 11.30454463500305
epoch: 51, step: 43
	action: tensor([[-13650.3082, -37136.1195,   3796.1144,    123.7255,   9656.8258,
            766.8795,  -4380.9809]], dtype=torch.float64)
	q_value: tensor([[-23.2864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06037581054267782, distance: 1.109261111066177 entropy 11.375434052423177
epoch: 51, step: 44
	action: tensor([[-41913.7786, -19607.6920,  22428.2925,   7663.9137, -16725.5160,
         -35466.4505,  33750.2917]], dtype=torch.float64)
	q_value: tensor([[-20.1858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.174546295018853, distance: 1.2402001025018807 entropy 11.242709680620763
epoch: 51, step: 45
	action: tensor([[  4833.2325,  -8316.0659, -16933.7769,  10517.4819, -31124.4364,
          12404.0793,   3972.2832]], dtype=torch.float64)
	q_value: tensor([[-25.7808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17724860940809395, distance: 1.0379848284771507 entropy 11.424511461549411
epoch: 51, step: 46
	action: tensor([[-28767.6005, -12167.8563,  -9551.0707,   7275.5605,  -9875.9166,
         -30438.8267,  24495.3347]], dtype=torch.float64)
	q_value: tensor([[-21.0147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7197187084197665, distance: 1.5006706931398905 entropy 11.253729841448065
epoch: 51, step: 47
	action: tensor([[-46394.5063, -20688.5665, -15251.1055,   6190.3043, -26428.4139,
         -29398.5259, -25300.5143]], dtype=torch.float64)
	q_value: tensor([[-21.8203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6615814859779314, distance: 1.4750865921846512 entropy 11.313122026712056
epoch: 51, step: 48
	action: tensor([[-16631.6458, -62718.3062,  27109.6540,  -5964.3467,   6122.3157,
          39230.9412,  -1371.1960]], dtype=torch.float64)
	q_value: tensor([[-21.6369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4302524780323209, distance: 1.368557495855755 entropy 11.364900208807315
epoch: 51, step: 49
	action: tensor([[ -4359.1314, -24194.7568,  -2654.9874,  -9292.9189,   -374.0745,
         -16880.5610, -16447.9288]], dtype=torch.float64)
	q_value: tensor([[-22.6056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9212444157315918, distance: 1.586163684357626 entropy 11.30126115689481
epoch: 51, step: 50
	action: tensor([[-13532.3400,  19435.5123, -13117.2296, -28220.8467, -11936.6883,
           1741.4168, -14589.4015]], dtype=torch.float64)
	q_value: tensor([[-23.8156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2713814717121199, distance: 1.290311691049367 entropy 11.416209724990333
epoch: 51, step: 51
	action: tensor([[-18080.7546,   1458.5795, -23798.9895,  13237.9310,   5251.3209,
         -15908.6216,  21233.5291]], dtype=torch.float64)
	q_value: tensor([[-27.0753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13858159390199365, distance: 1.2210649709136694 entropy 11.109556218211027
epoch: 51, step: 52
	action: tensor([[-29084.0076,  -5964.3034,  -3840.3203,  40315.0565,   3283.1984,
          -4653.6449, -15091.2268]], dtype=torch.float64)
	q_value: tensor([[-22.5883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17389718957810374, distance: 1.0400967590012644 entropy 11.064499418101146
epoch: 51, step: 53
	action: tensor([[-17617.3759, -20317.6695,  -5908.1169,    990.6979, -13957.3606,
           2144.3656,  25369.7073]], dtype=torch.float64)
	q_value: tensor([[-21.5219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7613356282204244, distance: 1.5187201437497648 entropy 11.401124418319174
epoch: 51, step: 54
	action: tensor([[ -9443.7173, -39403.0743, -15398.7492,  24896.1555, -29320.0765,
         -19289.3610,  -4601.5689]], dtype=torch.float64)
	q_value: tensor([[-21.0803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2509010912771634, distance: 0.9904356760028532 entropy 11.350843161620743
epoch: 51, step: 55
	action: tensor([[-51278.7537, -14897.8705,  -1389.0615,  52554.9059, -20257.6767,
          48870.7399, -14642.2672]], dtype=torch.float64)
	q_value: tensor([[-24.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015932420623106514, distance: 1.135191564502101 entropy 11.44627589469748
epoch: 51, step: 56
	action: tensor([[ -4331.9912,  -4019.2104,   3853.2233, -12766.1314, -33694.3785,
           2262.7427, -36076.7803]], dtype=torch.float64)
	q_value: tensor([[-20.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2668741255964959, distance: 1.2880224311607138 entropy 11.292514186795445
epoch: 51, step: 57
	action: tensor([[-26001.4156, -11715.4666, -15265.4631,  29945.4905,  11682.1396,
         -13940.2071,  20005.8704]], dtype=torch.float64)
	q_value: tensor([[-19.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11457576773796108, distance: 1.076793425014027 entropy 11.308026349236643
epoch: 51, step: 58
	action: tensor([[-31880.8876, -28075.6972, -28334.4138,   4510.8565,  25560.5694,
         -25771.2559, -12513.0649]], dtype=torch.float64)
	q_value: tensor([[-23.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10999708129929431, distance: 1.2056398983269299 entropy 11.36783280186477
epoch: 51, step: 59
	action: tensor([[-16347.7950, -33587.1253,   3072.2836,   8322.2273, -18726.9675,
          33075.2089, -23024.1691]], dtype=torch.float64)
	q_value: tensor([[-26.0856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3659554965829661, distance: 1.3374420338126363 entropy 11.433804206977412
epoch: 51, step: 60
	action: tensor([[ 16088.5149,   1489.0217,  28621.0018,  28582.9054, -29097.1049,
         -32732.9326,   7440.5107]], dtype=torch.float64)
	q_value: tensor([[-22.0958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23172831863971766, distance: 1.0030304231859821 entropy 11.407537764377732
epoch: 51, step: 61
	action: tensor([[  9407.4902, -23563.0108, -33428.4266,  29334.5192,  -2038.6428,
          -2238.2508,   -140.2332]], dtype=torch.float64)
	q_value: tensor([[-24.5362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35783691713027865, distance: 0.9170211678593789 entropy 11.143897938687877
epoch: 51, step: 62
	action: tensor([[ -5169.8484, -19106.3151,  -8901.3281,   4305.2967,   -493.9819,
          -4606.9517,  40469.5498]], dtype=torch.float64)
	q_value: tensor([[-25.5788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06703931911056549, distance: 1.1053208537123695 entropy 11.393981813685473
epoch: 51, step: 63
	action: tensor([[ 30452.9374,  12188.8072, -38416.0284,  11750.8700,  -4149.3805,
          13329.9967,  -5012.6326]], dtype=torch.float64)
	q_value: tensor([[-21.7385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4517825273959557, distance: 0.8472920467087512 entropy 11.177819069671761
epoch: 51, step: 64
	action: tensor([[-18804.7791, -20713.8525, -25501.6607, -12518.3163,  -9509.6648,
          -4619.2570,  14236.5917]], dtype=torch.float64)
	q_value: tensor([[-22.1210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6901446842537433, distance: 1.4877112090657318 entropy 11.170067486989401
epoch: 51, step: 65
	action: tensor([[-10189.1494, -54849.9417, -26323.1905,   8081.1064,   4570.5905,
            892.1454, -24147.5335]], dtype=torch.float64)
	q_value: tensor([[-19.9379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5841168060025512, distance: 1.440291156173263 entropy 11.191804524954616
epoch: 51, step: 66
	action: tensor([[  2115.1942, -36210.5701, -48051.7119,  10549.3682,  33321.2268,
          -5070.5832,  48205.2309]], dtype=torch.float64)
	q_value: tensor([[-24.2847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43908827283663265, distance: 0.857045646698516 entropy 11.496437662608345
epoch: 51, step: 67
	action: tensor([[-14619.6174, -11067.8592, -36600.7991,   4985.9967, -11695.5513,
         -10539.3694,  10954.5243]], dtype=torch.float64)
	q_value: tensor([[-24.8883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.053864845739515665, distance: 1.174760004262855 entropy 11.131574806908194
epoch: 51, step: 68
	action: tensor([[ -6172.4138, -52450.3214, -30296.6889,  26514.3475,  -4965.3530,
         -19102.8829, -38984.6067]], dtype=torch.float64)
	q_value: tensor([[-25.1000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06936879057958689, distance: 1.1039400759947742 entropy 11.594104654234211
epoch: 51, step: 69
	action: tensor([[-29961.4019, -25219.2976, -43568.0778,   9926.2753,   2842.5222,
          11866.2972, -22424.9423]], dtype=torch.float64)
	q_value: tensor([[-22.2850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5435210830656079, distance: 1.421716409955639 entropy 11.412534879390366
epoch: 51, step: 70
	action: tensor([[-30885.0662, -13955.9344,   6711.0727,   2670.3794,  15943.4259,
           5083.1054,  -8029.7512]], dtype=torch.float64)
	q_value: tensor([[-20.6609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5556969782088019, distance: 1.4273129210052142 entropy 11.289201581090492
epoch: 51, step: 71
	action: tensor([[  9806.6558,  -9320.3455, -11141.2290,  40822.4215,  -6509.3862,
           9624.7168, -10328.0003]], dtype=torch.float64)
	q_value: tensor([[-20.8593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4835525620544959, distance: 0.8223747301589432 entropy 11.311514441360657
epoch: 51, step: 72
	action: tensor([[ -4037.9462,   3041.1254, -26075.3691,   8830.4204,  -6323.5229,
          25302.1832,  -1038.1617]], dtype=torch.float64)
	q_value: tensor([[-19.4490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22228478855237288, distance: 1.0091761773716428 entropy 11.113954320623517
epoch: 51, step: 73
	action: tensor([[ -6175.6526, -12802.2620,  -8790.6969,  24587.3350,   8643.4608,
          30078.2647, -22406.9059]], dtype=torch.float64)
	q_value: tensor([[-23.2041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7736413766701662, distance: 1.5240162548852116 entropy 11.23206711565994
epoch: 51, step: 74
	action: tensor([[  8586.7179,   1951.0882,    779.1520, -15795.9412, -11130.4370,
          17386.2772, -18191.4538]], dtype=torch.float64)
	q_value: tensor([[-21.3132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.323364039306453
epoch: 51, step: 75
	action: tensor([[  8767.2427, -15480.0133, -15107.1644, -12088.2521,   7048.1499,
          12348.4131,  -7042.9324]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21439926273772003, distance: 1.261064908260016 entropy 10.771082707966375
epoch: 51, step: 76
	action: tensor([[-30166.1966, -14604.3628,   3410.4268,  19134.1932, -12111.2647,
          23083.6909,  -5617.9092]], dtype=torch.float64)
	q_value: tensor([[-19.5157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6668905811192534, distance: 1.4774413154966624 entropy 11.124882374007342
epoch: 51, step: 77
	action: tensor([[ -6825.1757,   -152.0493,  -6787.6135, -15728.5146,   6456.8401,
           9354.6937,  20468.4576]], dtype=torch.float64)
	q_value: tensor([[-20.8032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12462566302118527, distance: 1.2135584204925192 entropy 11.313756233397937
epoch: 51, step: 78
	action: tensor([[ 15284.8306, -25143.6377,  -3161.9743,  15717.6578, -18422.7450,
          -7401.8384,  40247.4404]], dtype=torch.float64)
	q_value: tensor([[-23.4943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23082811795781732, distance: 1.0036178874595978 entropy 11.31215917161571
epoch: 51, step: 79
	action: tensor([[ 38039.3668,  17976.6489, -15868.7098, -24042.2752, -21095.7170,
           -106.4135,  -5406.4438]], dtype=torch.float64)
	q_value: tensor([[-22.2321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6733865782772217, distance: 0.6539939142607603 entropy 11.219965175264916
epoch: 51, step: 80
	action: tensor([[  6433.6535, -24496.3560,   7844.3402,  27333.6437,  29494.2114,
          16772.0521,   4229.1907]], dtype=torch.float64)
	q_value: tensor([[-30.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3653652683216766, distance: 1.3371530487831285 entropy 11.374658681833782
epoch: 51, step: 81
	action: tensor([[-31723.8937, -27884.7698,  10632.8578,  13714.1817,  14691.6849,
         -35305.0455,  35267.1794]], dtype=torch.float64)
	q_value: tensor([[-22.0681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2247605589876478, distance: 1.0075685965413317 entropy 11.005618759496231
epoch: 51, step: 82
	action: tensor([[ -9319.3512, -10494.7274, -11856.7675,  11390.3247,  31589.5486,
          17402.5945,  24086.9583]], dtype=torch.float64)
	q_value: tensor([[-20.3070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7747224306727372, distance: 1.5244806365018757 entropy 11.101012051726025
epoch: 51, step: 83
	action: tensor([[ -4046.9207, -14483.6616,  -1680.2957,  -3049.2353, -33413.4948,
          -6008.3339,  -6810.9663]], dtype=torch.float64)
	q_value: tensor([[-21.8835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12945281563356836, distance: 1.216160068018868 entropy 11.354717648130842
epoch: 51, step: 84
	action: tensor([[ 15440.6992, -13519.0032, -14308.1785,  -6463.2110,  -7474.5047,
           5219.9017,  14858.8022]], dtype=torch.float64)
	q_value: tensor([[-19.7664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5994906961194685, distance: 0.724207466271705 entropy 11.027944731501353
epoch: 51, step: 85
	action: tensor([[-38120.5524,  24214.0722,  17582.5369,  27727.6750, -37515.4959,
         -38025.0281, -17721.8443]], dtype=torch.float64)
	q_value: tensor([[-23.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.212442487258528
epoch: 51, step: 86
	action: tensor([[-23399.9949,  -2657.6106,  -5502.6492,   -224.9768,  -8107.3932,
          19296.7465,  -8240.8743]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.283836605416403, distance: 1.296616579303101 entropy 10.771082707966375
epoch: 51, step: 87
	action: tensor([[  7714.6007, -20959.7936,  -3222.3968,   8115.0933,  22889.0734,
          11766.3698,  -6485.8557]], dtype=torch.float64)
	q_value: tensor([[-21.9503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5094895594214097, distance: 0.801458097586184 entropy 11.138416320524504
epoch: 51, step: 88
	action: tensor([[-22632.0031,  -5743.8983, -14276.2964,   8809.8161,  13570.6838,
          21687.1134,   1145.3951]], dtype=torch.float64)
	q_value: tensor([[-18.0756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21403482679246033, distance: 1.260875674010438 entropy 10.828369658340188
epoch: 51, step: 89
	action: tensor([[   716.5291, -20542.2224, -21493.5817,  20963.7834, -24220.0580,
          26720.6062, -21387.9893]], dtype=torch.float64)
	q_value: tensor([[-19.1203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05647466511637389, distance: 1.1115614491319084 entropy 11.162632140254887
epoch: 51, step: 90
	action: tensor([[ -2841.9019,  -6647.6987,   5580.9779,   6899.4580, -11938.2000,
          -1749.9441,  10622.5985]], dtype=torch.float64)
	q_value: tensor([[-21.1507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20350834663120465, distance: 1.255397462681909 entropy 11.094205063550023
epoch: 51, step: 91
	action: tensor([[ -3335.0204, -16963.1242,   -989.4009,  -5007.8360, -55074.4708,
           6644.5216,  17647.3598]], dtype=torch.float64)
	q_value: tensor([[-21.7761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4544832403513772, distance: 1.3801015827558818 entropy 11.235071784371428
epoch: 51, step: 92
	action: tensor([[-23596.4668, -42138.1860,  29594.2229,  -4246.7480,  39384.0006,
          15220.7990,  -6752.1683]], dtype=torch.float64)
	q_value: tensor([[-24.6263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10406132150849623, distance: 1.2024119701204075 entropy 11.367685816336687
epoch: 51, step: 93
	action: tensor([[-26409.9724,  -7145.6279,  -5724.5856,  23234.4077,  -2693.6157,
         -26169.8726,  12734.5754]], dtype=torch.float64)
	q_value: tensor([[-21.8406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9589602405564299, distance: 1.6016569553905589 entropy 11.349689098108842
epoch: 51, step: 94
	action: tensor([[-20421.1577, -16977.9039,  -2428.9267,  29603.1154,   3046.5190,
          10271.2771, -28830.2235]], dtype=torch.float64)
	q_value: tensor([[-18.7802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1180993912920918, distance: 1.0746486920603 entropy 11.072248929955084
epoch: 51, step: 95
	action: tensor([[-50358.7534,  22587.7002,  -6503.6382,  43531.8657,  12128.0784,
          -3600.2590,  -1219.5555]], dtype=torch.float64)
	q_value: tensor([[-21.8805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.353559248818488
epoch: 51, step: 96
	action: tensor([[  5462.2124,  -5454.9510,   3009.1595, -20935.6680,  -1862.8679,
          -7599.8994,  13946.4215]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42295653050050364, distance: 1.3650624171565398 entropy 10.771082707966375
epoch: 51, step: 97
	action: tensor([[-38307.7674,  -8435.9094,  -7225.7453, -14457.1808,  26107.3102,
          -7014.8658,  25278.5996]], dtype=torch.float64)
	q_value: tensor([[-22.6231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24539735239843097, distance: 1.2770581185410783 entropy 11.183967120451616
epoch: 51, step: 98
	action: tensor([[-34306.2339,    960.9783, -19680.0265,   2936.7800,  18728.8013,
           4388.1294,  21453.4448]], dtype=torch.float64)
	q_value: tensor([[-21.4648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33878923568256747, distance: 1.3240756509250575 entropy 11.086100732465528
epoch: 51, step: 99
	action: tensor([[  2287.6340,  16042.3267, -14938.4750,  -4230.6598, -16750.4916,
          -1956.1662,  24363.4356]], dtype=torch.float64)
	q_value: tensor([[-22.5974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6009965650862094, distance: 0.7228447156856901 entropy 11.095958719420759
epoch: 51, step: 100
	action: tensor([[ 15673.6106, -15707.2890,  10365.2896,  10047.1618,  -8376.0059,
           9935.4984,   6921.7329]], dtype=torch.float64)
	q_value: tensor([[-27.2085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07876466925788217, distance: 1.0983531148508026 entropy 11.081206397802523
epoch: 51, step: 101
	action: tensor([[-40919.5443,  12704.1828,  -7743.5225,  31985.9781,  13511.7089,
           8144.5019,  21408.8092]], dtype=torch.float64)
	q_value: tensor([[-21.1016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.177090182029335
epoch: 51, step: 102
	action: tensor([[ -4897.1387, -12060.9016,  -4691.0518,   7514.3079,  -3083.9223,
           2254.4994,    871.7606]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18466765986620137, distance: 1.2455322003525042 entropy 10.771082707966375
epoch: 51, step: 103
	action: tensor([[  1114.4649,  -3492.0989,  34366.0931,  25122.8241, -18932.8580,
           6438.4609, -24997.1866]], dtype=torch.float64)
	q_value: tensor([[-21.6544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11939643120533217, distance: 1.2107337595763583 entropy 11.229614841472848
epoch: 51, step: 104
	action: tensor([[ -2777.3055, -12868.1429,   5736.0696,  43941.0619, -61412.2919,
          14618.9817,   2329.6417]], dtype=torch.float64)
	q_value: tensor([[-24.4032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23392664692266962, distance: 1.0015943623504509 entropy 11.470149600448437
epoch: 51, step: 105
	action: tensor([[  3625.5741, -27817.8056,   1016.0258,  25964.9826, -26425.3795,
          -7057.1299,  15491.2586]], dtype=torch.float64)
	q_value: tensor([[-19.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012817671732332503, distance: 1.1516548169888738 entropy 11.19627309863877
epoch: 51, step: 106
	action: tensor([[  9397.5487, -46992.0300,  -3497.0154,   -489.2374,  42136.2347,
         -66314.8908,  19690.5235]], dtype=torch.float64)
	q_value: tensor([[-27.1849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21578868500754833, distance: 1.261786108790258 entropy 11.365918986528923
epoch: 51, step: 107
	action: tensor([[-12594.9876,  -8432.9540, -14409.3433,  13671.6871,   8269.6214,
          14078.8053,  17426.6904]], dtype=torch.float64)
	q_value: tensor([[-27.1400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048388065750725406, distance: 1.1717035009029266 entropy 11.136598597133132
epoch: 51, step: 108
	action: tensor([[ 10720.3134, -28576.2962,  12163.3551, -24142.1849,  15693.9156,
          46891.8209,  16077.1084]], dtype=torch.float64)
	q_value: tensor([[-21.9949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16399597620734707, distance: 1.234617512010658 entropy 11.38806543335563
epoch: 51, step: 109
	action: tensor([[ 9.6152e+03, -1.7454e+03, -8.8255e+03,  2.3238e+04,  1.4298e+04,
         -1.3576e+04,  1.8676e+00]], dtype=torch.float64)
	q_value: tensor([[-20.8388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5328844408316173, distance: 0.7821118390898874 entropy 11.115846060875628
epoch: 51, step: 110
	action: tensor([[  4169.9607,   9030.9544, -20104.5617, -19686.7244,   7963.9672,
          -7923.7004,  12987.5205]], dtype=torch.float64)
	q_value: tensor([[-26.0809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4827831927996993, distance: 0.8229870619471107 entropy 11.351408145375164
epoch: 51, step: 111
	action: tensor([[-44025.6576,  20048.1138,  -9088.5886,  17651.1862,   8027.7044,
           -431.8331,  34770.6607]], dtype=torch.float64)
	q_value: tensor([[-26.1162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.196431499451434
epoch: 51, step: 112
	action: tensor([[  4117.3716, -13189.5413, -19115.1398,  -6757.9320, -15057.9638,
           8926.3989,   1270.2874]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4543605671866705, distance: 0.8452974671828085 entropy 10.771082707966375
epoch: 51, step: 113
	action: tensor([[ 21833.3541, -25339.0575,  21900.6736,   1657.2116,   4373.2799,
           8121.9056,   -700.0382]], dtype=torch.float64)
	q_value: tensor([[-22.9670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03731944782513297, distance: 1.1227880777735728 entropy 11.272426130890338
epoch: 51, step: 114
	action: tensor([[-11105.4773, -36242.7105,  -5531.5577,   8081.8547,  19636.5736,
         -29540.7765, -21545.8290]], dtype=torch.float64)
	q_value: tensor([[-21.0216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06808873567275897, distance: 1.1046990330931192 entropy 11.269081832726922
epoch: 51, step: 115
	action: tensor([[ -5266.9340, -25947.8610, -10688.3153,  -7595.1437,  11515.9826,
          20072.3943,  -7424.9093]], dtype=torch.float64)
	q_value: tensor([[-21.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8975986772925246, distance: 1.576372600938775 entropy 11.3024095376995
epoch: 51, step: 116
	action: tensor([[ -9908.6046,  -9421.7908,   3544.6124, -17757.1708,  -1254.1332,
          15555.0286, -17419.4484]], dtype=torch.float64)
	q_value: tensor([[-17.4454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18039527498938301, distance: 1.0359980071257708 entropy 10.962739811228548
epoch: 51, step: 117
	action: tensor([[  5400.4452, -15208.1071,  29541.8198,  12076.2939, -30081.6490,
          -8727.0516,  14499.6135]], dtype=torch.float64)
	q_value: tensor([[-23.9436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3132442614252918, distance: 0.9483264021366457 entropy 11.451689682388192
epoch: 51, step: 118
	action: tensor([[ 17382.9755, -13593.8729,   -756.7543,  19129.9440,  -5780.6704,
         -20528.1964,  39352.0831]], dtype=torch.float64)
	q_value: tensor([[-22.1138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031129549008294566, distance: 1.1263919776149474 entropy 11.036693624377545
epoch: 51, step: 119
	action: tensor([[ -9891.2422,  -5263.2771,  23254.9608, -17260.5559,  13488.4707,
          34172.0643,  51390.3503]], dtype=torch.float64)
	q_value: tensor([[-23.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13216434037899116, distance: 1.217619036133733 entropy 11.216909926859973
epoch: 51, step: 120
	action: tensor([[-23214.5421, -15556.4178,   4839.4541,   6146.3915,   -132.4620,
          -2620.8999,  12537.3300]], dtype=torch.float64)
	q_value: tensor([[-20.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2221166367392171, distance: 1.2650655269805655 entropy 11.094794584929556
epoch: 51, step: 121
	action: tensor([[  3624.8490, -13259.0707,  -6366.0287,   7324.4348, -11428.1365,
          -8281.1272,  -4905.5904]], dtype=torch.float64)
	q_value: tensor([[-22.5608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3946928988126108, distance: 0.8903168189814208 entropy 11.291233365006587
epoch: 51, step: 122
	action: tensor([[-3862.3857, 14906.6901, 13744.1407,  8992.3446, -3967.5282, 28459.0525,
          6539.0256]], dtype=torch.float64)
	q_value: tensor([[-22.0782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.171670120505453
epoch: 51, step: 123
	action: tensor([[-26511.3201,  -4044.4385,  -3358.8557,   5474.2544,  18676.1795,
          -4764.5770,   2807.6515]], dtype=torch.float64)
	q_value: tensor([[-25.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23654628564479518, distance: 0.9998803866025247 entropy 10.771082707966375
epoch: 51, step: 124
	action: tensor([[-28891.4170, -23585.1488, -16001.5231,  52051.4809,  18172.4742,
         -38653.2097,  -3364.3388]], dtype=torch.float64)
	q_value: tensor([[-23.7473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2817807291398391, distance: 1.2955779925839728 entropy 11.457240647624461
epoch: 51, step: 125
	action: tensor([[-28273.6150, -25681.7719,  30751.9187,  19479.8660,    738.8881,
         -10621.0793,  11021.5427]], dtype=torch.float64)
	q_value: tensor([[-24.2088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1357427914525543, distance: 1.0638446124429208 entropy 11.34696573400896
epoch: 51, step: 126
	action: tensor([[ 29066.0235, -22664.5889,   2361.5293,  13078.5959, -22201.6120,
         -26254.5998, -19117.4063]], dtype=torch.float64)
	q_value: tensor([[-22.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37614425108138183, distance: 0.9038550399801065 entropy 11.441853646460062
epoch: 51, step: 127
	action: tensor([[-39953.2484, -20367.9014, -13904.7043,  11620.1880,  32155.4200,
          14948.6933, -10776.6958]], dtype=torch.float64)
	q_value: tensor([[-24.4226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.4232230147217
LOSS epoch 51 actor 220.12291560328634 critic 352.985973686085
epoch: 52, step: 0
	action: tensor([[-12937.6100,   7458.7039,  29373.7625,  -9962.3235,  -4696.2387,
          -6290.8361, -27287.5634]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.813167454532193
epoch: 52, step: 1
	action: tensor([[-11207.3815, -11434.4772,  20454.1523,  24882.2248, -11438.6917,
          -3705.9751,  30834.9184]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2051690817909102, distance: 1.020220555491501 entropy 10.813167454532193
epoch: 52, step: 2
	action: tensor([[-5917.0080, -3331.2348,  -256.6601, 51195.3672, 34111.4452, 19705.5364,
         13881.3536]], dtype=torch.float64)
	q_value: tensor([[-29.4327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10761116299405249, distance: 1.081020071795997 entropy 11.457241957432286
epoch: 52, step: 3
	action: tensor([[-21494.3882,  13833.2738, -32557.3849,  18720.2686, -34826.6940,
          46199.2399,  12465.7725]], dtype=torch.float64)
	q_value: tensor([[-23.8662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10370675325608714, distance: 1.2022188778810756 entropy 11.367013808453763
epoch: 52, step: 4
	action: tensor([[ -9731.5702,   5668.4866, -43363.3877,  36766.2986, -17023.5555,
           7673.6847,   7615.4595]], dtype=torch.float64)
	q_value: tensor([[-25.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03725670264233427, distance: 1.1654665629809917 entropy 11.21309132794161
epoch: 52, step: 5
	action: tensor([[-27416.9568, -14678.9065, -20347.3857,  49030.6835, -20913.1934,
          -5356.6812,  -4297.5452]], dtype=torch.float64)
	q_value: tensor([[-32.9429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07856099473620315, distance: 1.1884448921996138 entropy 11.539291038155389
epoch: 52, step: 6
	action: tensor([[ 38503.1109, -45637.9151,  10602.6127,  33792.3075,    356.0331,
          -7281.2909,   1059.7417]], dtype=torch.float64)
	q_value: tensor([[-27.6704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17085302056370177, distance: 1.0420113625936394 entropy 11.53526798307254
epoch: 52, step: 7
	action: tensor([[-48247.9064, -62494.0035,  26551.8710, -20890.5990, -14464.4835,
          51016.0326,  20434.4003]], dtype=torch.float64)
	q_value: tensor([[-32.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7243888922304871, distance: 1.502706972755212 entropy 11.4295321815746
epoch: 52, step: 8
	action: tensor([[-17724.9757,   -649.8537,  11435.7623,   7503.5866,  -2051.4691,
         -10418.3144,  -7903.9073]], dtype=torch.float64)
	q_value: tensor([[-23.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14184407088411632, distance: 1.0600828217505651 entropy 11.245108450273888
epoch: 52, step: 9
	action: tensor([[  6194.9397, -34751.6397, -17917.0530,  19694.8549,  -7799.9346,
         -44633.7488, -11969.1199]], dtype=torch.float64)
	q_value: tensor([[-25.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5061889567412632, distance: 0.8041500480985692 entropy 11.453206179339722
epoch: 52, step: 10
	action: tensor([[ 11811.7317,  -2187.4681, -23899.0588, -38153.3610,  20842.9216,
          11165.5739, -12099.6011]], dtype=torch.float64)
	q_value: tensor([[-27.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16061301694534935, distance: 1.0484260622163104 entropy 11.42497544276362
epoch: 52, step: 11
	action: tensor([[ 38237.2269, -56337.9414, -25572.3028,  45263.6592,  15869.7339,
         -30002.1552,  15409.7919]], dtype=torch.float64)
	q_value: tensor([[-27.4976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.057838838576777185, distance: 1.1769728568257105 entropy 11.206545102860515
epoch: 52, step: 12
	action: tensor([[ -2326.9822, -29633.0836, -26934.7618,  -3016.4764,  -7620.1478,
         -10450.5406,   7665.2911]], dtype=torch.float64)
	q_value: tensor([[-31.5696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6453103586174571, distance: 1.4678463905915475 entropy 11.381124337403548
epoch: 52, step: 13
	action: tensor([[-14272.1786,  -1172.2221,  15048.4718,   7072.1700, -10890.6034,
           5426.6911,  27030.6978]], dtype=torch.float64)
	q_value: tensor([[-23.9270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.869369415969355, distance: 1.5646033645084962 entropy 11.12415117819137
epoch: 52, step: 14
	action: tensor([[ -2167.5771,  -4722.1257,  12418.2469,  -1949.3363, -21154.5159,
          25785.4150,  18672.4091]], dtype=torch.float64)
	q_value: tensor([[-22.9047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6398495451605277, distance: 1.4654084622855732 entropy 11.29731609954588
epoch: 52, step: 15
	action: tensor([[-28109.0313, -18845.7548,  31399.8147,   6133.4569,  22242.7665,
           1911.8756,  14017.9145]], dtype=torch.float64)
	q_value: tensor([[-21.4140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008257794995321754, distance: 1.1396095791168677 entropy 11.066389133917736
epoch: 52, step: 16
	action: tensor([[ -8566.2787,  -4942.0037,  -2080.4738,  18585.2675,  14293.1952,
         -11099.8319,  26788.5953]], dtype=torch.float64)
	q_value: tensor([[-24.8632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7708344844753501, distance: 1.5228098545454516 entropy 11.32076027399886
epoch: 52, step: 17
	action: tensor([[  9682.4964,  10142.9759,   9788.8294, -40310.1451,  17695.9517,
         -21696.0324,  -3601.8486]], dtype=torch.float64)
	q_value: tensor([[-25.6011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.413733059746553
epoch: 52, step: 18
	action: tensor([[  5675.5535,  -8994.0801,  -6536.2137,   4357.7303, -10371.8681,
          17555.8390, -12107.5557]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5193384648507301, distance: 0.7933711025755295 entropy 10.813167454532193
epoch: 52, step: 19
	action: tensor([[ -1428.8011, -23451.5724,   6444.8977,  35572.6304, -25387.3284,
           -209.0854,  15139.6611]], dtype=torch.float64)
	q_value: tensor([[-23.2019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2117247353456474, distance: 1.0160045264065867 entropy 11.218918934316118
epoch: 52, step: 20
	action: tensor([[  6285.4247, -38960.3108,  -9476.2943,  23512.5741,  14246.5383,
         -15600.1911, -64420.2000]], dtype=torch.float64)
	q_value: tensor([[-25.9423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2060136973310488, distance: 1.0196783501279507 entropy 11.485400088300338
epoch: 52, step: 21
	action: tensor([[-6176.4474, -1324.8452, -2769.8242, 22390.0454, 23066.7274, -4722.2894,
          8615.7689]], dtype=torch.float64)
	q_value: tensor([[-28.1456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5677970277524478, distance: 1.4328529153350453 entropy 11.30431981803754
epoch: 52, step: 22
	action: tensor([[-9032.8947, -4148.4570, -7197.4455,  6019.3651, 26889.8033,  1848.0045,
          7484.6308]], dtype=torch.float64)
	q_value: tensor([[-23.4257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2548556575010761, distance: 1.2818983241674171 entropy 11.2228837620557
epoch: 52, step: 23
	action: tensor([[-15754.1573,  35021.6790, -37745.6222,  -5828.6031, -11564.4848,
          18232.0114,   2089.9549]], dtype=torch.float64)
	q_value: tensor([[-26.0856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6546352954169233, distance: 0.6725052345854603 entropy 11.45240528607136
epoch: 52, step: 24
	action: tensor([[  8118.4620,   5186.4540, -40032.2060, -14944.2605, -10067.6982,
         -20207.5266,  14434.1100]], dtype=torch.float64)
	q_value: tensor([[-36.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24679599816675002, distance: 0.9931457825073842 entropy 11.438211237531059
epoch: 52, step: 25
	action: tensor([[ 27116.0955, -10248.8333,   7072.5445,  55272.1277,  30834.9786,
          19560.9180,  13243.8668]], dtype=torch.float64)
	q_value: tensor([[-34.1808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2368227413236883, distance: 1.2726542268411087 entropy 11.384637356315121
epoch: 52, step: 26
	action: tensor([[  4432.0695,   8481.2497, -16711.8587,   6433.7283,  10307.2471,
          21124.0178,   9454.7125]], dtype=torch.float64)
	q_value: tensor([[-26.4224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5895500928898139, distance: 0.7331397618718982 entropy 11.43014941390943
epoch: 52, step: 27
	action: tensor([[  4393.9179,   6253.2182, -26137.1785, -26332.5153,  38797.9714,
          -3362.2247,  -4639.8143]], dtype=torch.float64)
	q_value: tensor([[-31.8742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7833906372097548, distance: 0.5325928179562331 entropy 11.343537987354235
epoch: 52, step: 28
	action: tensor([[ -1700.3168, -11274.5299,   6123.7121,   -902.1167,   5652.5803,
          31648.5874, -14676.8012]], dtype=torch.float64)
	q_value: tensor([[-28.1993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26977056246126885, distance: 0.9778817961415052 entropy 11.002112317320265
epoch: 52, step: 29
	action: tensor([[-33664.6690, -11668.9752, -10773.9854,  31821.0675,  -6184.1316,
          -7876.3879,  10044.4084]], dtype=torch.float64)
	q_value: tensor([[-25.2906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4346447140912768, distance: 1.3706572718929162 entropy 11.264251913255093
epoch: 52, step: 30
	action: tensor([[-19508.7049,  -6702.9664,  38987.8526,  -2144.6445,   5778.2117,
         -10943.0364,  11559.0108]], dtype=torch.float64)
	q_value: tensor([[-25.1875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2864033357179041, distance: 1.2979120725491915 entropy 11.28895561977781
epoch: 52, step: 31
	action: tensor([[-46582.3387, -27421.3139,   8747.1934, -23114.6831,  19755.6922,
          28454.2766,  16517.2723]], dtype=torch.float64)
	q_value: tensor([[-27.6820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030555266991351804, distance: 1.1267257533171668 entropy 11.350182539268022
epoch: 52, step: 32
	action: tensor([[-14453.0349, -10754.3816,  32505.4656,  31926.4194, -10040.8553,
          -7950.2019,  23808.3173]], dtype=torch.float64)
	q_value: tensor([[-24.7323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0646103299259946, distance: 1.6442798746742295 entropy 11.19543081097373
epoch: 52, step: 33
	action: tensor([[-24697.5507,    926.6091,   2280.4665,  -4206.7337,  18436.9861,
         -13667.1783, -12261.9000]], dtype=torch.float64)
	q_value: tensor([[-20.7732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28152050559979436, distance: 0.9699824624029811 entropy 11.163602754595583
epoch: 52, step: 34
	action: tensor([[-29201.3418,  46388.0827, -10764.7278,  -9475.1179,  -4957.3007,
          33060.5305,  -6899.3629]], dtype=torch.float64)
	q_value: tensor([[-31.8472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.087283173852585, distance: 1.0932631799510482 entropy 11.311303341426505
epoch: 52, step: 35
	action: tensor([[-21354.1630,   4729.3525,  29010.1127,   3746.1763, -11072.3942,
            215.2688,   6814.5755]], dtype=torch.float64)
	q_value: tensor([[-33.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48068987293312726, distance: 0.824650808681655 entropy 11.050723029528696
epoch: 52, step: 36
	action: tensor([[ -3754.6164, -21783.7566,   9300.3198,  14369.2659, -51801.5583,
           6186.6854,  10507.8414]], dtype=torch.float64)
	q_value: tensor([[-25.0201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42931717683385884, distance: 1.3681099445256304 entropy 11.158951886652654
epoch: 52, step: 37
	action: tensor([[ 34188.5693, -16919.4796, -42504.3954, -31479.0846, -33903.1959,
         -23823.4824, -27670.9876]], dtype=torch.float64)
	q_value: tensor([[-27.0687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34604394524289517, distance: 0.925403155023398 entropy 11.526777071075681
epoch: 52, step: 38
	action: tensor([[ -7422.5011,   4376.8505, -37946.1429,  10984.1719,  -4567.8397,
          23695.3160,  16580.7380]], dtype=torch.float64)
	q_value: tensor([[-22.5871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.926341307701104
epoch: 52, step: 39
	action: tensor([[ -7100.0237, -21561.1847, -16822.6930,  -4790.7658, -19361.4271,
          -1828.6662,  -3117.9091]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7058547393994572, distance: 1.494609424648215 entropy 10.813167454532193
epoch: 52, step: 40
	action: tensor([[ -8835.5213,   1938.2038,  19228.4894,  43517.3044, -54086.6818,
          -7444.2611,  -3526.8345]], dtype=torch.float64)
	q_value: tensor([[-32.4342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0006798506136063143, distance: 1.1439551962918475 entropy 11.516300442246102
epoch: 52, step: 41
	action: tensor([[ -1912.7393,   5333.9475,   2377.4397,  32848.8394,  13183.5118,
          25594.9201, -17772.6842]], dtype=torch.float64)
	q_value: tensor([[-27.8218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1912453382948578, distance: 1.248985223443268 entropy 11.216404518190718
epoch: 52, step: 42
	action: tensor([[  9541.5856, -13145.9758,  25718.0841,  13653.2197,  14160.5748,
          25110.2326,   5694.0369]], dtype=torch.float64)
	q_value: tensor([[-28.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4931100794221771, distance: 0.8147296487872134 entropy 11.43165771760054
epoch: 52, step: 43
	action: tensor([[-25845.2303, -14282.6124,  30048.2519,  22690.1259, -22827.3472,
         -48544.4486,  -6749.0437]], dtype=torch.float64)
	q_value: tensor([[-25.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2512683190689744, distance: 1.280064689204962 entropy 11.320246711928746
epoch: 52, step: 44
	action: tensor([[ 18832.5372, -22075.4441,  -1936.4791,  10619.5066, -26522.1039,
         -14450.7621,  29284.9812]], dtype=torch.float64)
	q_value: tensor([[-24.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12515406014324437, distance: 1.2138434777764853 entropy 11.265281695183075
epoch: 52, step: 45
	action: tensor([[   603.0287,  -4963.8085,  -7122.1044,   6383.5959,  16009.2700,
         -20251.8985, -23876.4405]], dtype=torch.float64)
	q_value: tensor([[-25.4866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2743153045237574, distance: 1.2917995912166875 entropy 11.33307612431229
epoch: 52, step: 46
	action: tensor([[ 15783.8219,  22396.9703,   -554.0558,  11031.5421, -22472.8457,
         -36665.8539, -15142.3690]], dtype=torch.float64)
	q_value: tensor([[-32.2622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672910231536383, distance: 0.6544706466074648 entropy 11.267333831903144
epoch: 52, step: 47
	action: tensor([[-20631.4111, -29149.1144,  20181.8360,  28405.3363,   3679.1381,
         -10514.8392,   7313.5358]], dtype=torch.float64)
	q_value: tensor([[-28.6620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2711600239323383, distance: 0.9769510092111893 entropy 11.190986943479567
epoch: 52, step: 48
	action: tensor([[-46091.4259, -25838.4627,  -4910.3298,  -4877.3112, -24165.1144,
          14180.4492, -16604.0365]], dtype=torch.float64)
	q_value: tensor([[-24.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9421700529258861, distance: 1.5947783083921345 entropy 11.371026879296377
epoch: 52, step: 49
	action: tensor([[-14239.1620,  -6259.4639, -13590.7704,  21478.3473, -23329.1741,
           2785.7762, -27450.9754]], dtype=torch.float64)
	q_value: tensor([[-19.5763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1492148025452995, distance: 1.0555204596054846 entropy 11.080968308932865
epoch: 52, step: 50
	action: tensor([[ -9945.4897,  -5038.7563,  20331.5143, -22232.5068,  15066.1306,
           6591.8951,  -4411.7041]], dtype=torch.float64)
	q_value: tensor([[-21.8368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7893440733011119, distance: 1.5307477256791138 entropy 11.184036339401782
epoch: 52, step: 51
	action: tensor([[-22612.6459,    303.6549, -39872.1165,  -7121.3170, -20863.8183,
          -9916.4263,  11642.6759]], dtype=torch.float64)
	q_value: tensor([[-20.9608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3549467626459175, distance: 1.3320416641465855 entropy 11.056846296019812
epoch: 52, step: 52
	action: tensor([[ 35351.6369,  -8931.9020,   -557.4986,  20732.0458, -30261.2799,
           -315.6700,  12018.9493]], dtype=torch.float64)
	q_value: tensor([[-33.0783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39546992439350803, distance: 1.3518139807805702 entropy 11.270782602685543
epoch: 52, step: 53
	action: tensor([[ -2572.9207,  -7232.6227,  -6132.6238,    424.1801,  -3619.0136,
         -21916.9571,  21417.1328]], dtype=torch.float64)
	q_value: tensor([[-25.0927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5835544493199156, distance: 1.440035483974909 entropy 11.090610817388725
epoch: 52, step: 54
	action: tensor([[-36394.4620,   6069.8279, -14512.0430,  13782.8182,  10074.3304,
           9585.9321, -17855.2490]], dtype=torch.float64)
	q_value: tensor([[-24.5668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019705849861465063, distance: 1.1555643861322584 entropy 11.230116502031061
epoch: 52, step: 55
	action: tensor([[-25742.6393, -33588.7770,   8242.0440,   1742.1940,   7631.0341,
          39759.0110, -14959.6839]], dtype=torch.float64)
	q_value: tensor([[-28.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5864076552019315, distance: 1.4413322087689562 entropy 11.39899823621183
epoch: 52, step: 56
	action: tensor([[-72747.8046,   9735.8386,   2792.4320, -18658.6279,  10659.5396,
          52421.2247,  20618.7486]], dtype=torch.float64)
	q_value: tensor([[-26.8251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4712648820387224, distance: 0.8321004782375963 entropy 11.53302128501242
epoch: 52, step: 57
	action: tensor([[-29713.1515, -44665.4670,  24229.2531, -20149.8026,   4181.2081,
         -67694.0953,  27948.6547]], dtype=torch.float64)
	q_value: tensor([[-31.6369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9071466968984487, distance: 1.5803334887411387 entropy 11.426042365732103
epoch: 52, step: 58
	action: tensor([[ -3314.5726,  -9405.9301,  10156.6407,  -7763.6701,  11518.7119,
          33979.5968, -19600.2572]], dtype=torch.float64)
	q_value: tensor([[-27.2182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5300905739143564, distance: 1.4155175658881989 entropy 11.403622109515917
epoch: 52, step: 59
	action: tensor([[-35006.9662,  -7927.2471,  -5895.9742, -15373.3556,   -509.5903,
         -42329.6583,   1149.2401]], dtype=torch.float64)
	q_value: tensor([[-30.3520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15915973103166636, distance: 1.2320500082952166 entropy 11.460410324909622
epoch: 52, step: 60
	action: tensor([[-38299.3285,  -9696.1841,  21838.4690, -27111.1437,  -2821.9736,
          -1688.4430, -17278.9422]], dtype=torch.float64)
	q_value: tensor([[-29.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1853132388892631, distance: 1.2458715275629895 entropy 11.34198808451518
epoch: 52, step: 61
	action: tensor([[-25255.1225, -29205.3188,  -2327.2992,   4276.7637, -11013.9596,
         -19395.9464,   2893.8218]], dtype=torch.float64)
	q_value: tensor([[-28.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7171961972150607, distance: 1.499569685230437 entropy 11.309813920198994
epoch: 52, step: 62
	action: tensor([[-21586.8052, -19446.7723,   5058.5799,  44792.5262,  -4272.6731,
         -10286.8709,  11291.0346]], dtype=torch.float64)
	q_value: tensor([[-24.6514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3133976100950906, distance: 1.3114592605432516 entropy 11.302456954711511
epoch: 52, step: 63
	action: tensor([[ 15079.6135,   3603.1817,   4324.6182,  19594.3838,  -3604.0322,
         -10711.6191,  30044.7820]], dtype=torch.float64)
	q_value: tensor([[-24.1366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.236129062224455
epoch: 52, step: 64
	action: tensor([[ 15698.1054,   3347.9918,  11600.0368,   7147.3750,  -3649.0480,
         -16246.2800,  -7690.2577]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.813167454532193
epoch: 52, step: 65
	action: tensor([[ -5945.8423,  -9340.7430,  -6580.2135,  -9600.4097,  -7699.5983,
          16425.3068, -10853.7092]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1319022227695481, distance: 1.0662057378437468 entropy 10.813167454532193
epoch: 52, step: 66
	action: tensor([[  8966.6463, -33860.3487, -49369.7171,  29014.6791,  14768.1558,
           1861.4609, -11462.7532]], dtype=torch.float64)
	q_value: tensor([[-30.1261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04326507389172096, distance: 1.1193154733670305 entropy 11.42433998188304
epoch: 52, step: 67
	action: tensor([[   257.3312, -32238.6062,  12355.2497,  26009.2165,  -2297.7440,
           -605.8928,  26733.7835]], dtype=torch.float64)
	q_value: tensor([[-27.1651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1198958400355774, distance: 1.0735535939172973 entropy 11.39454617703523
epoch: 52, step: 68
	action: tensor([[-13819.5400, -35548.1348,  -4640.4982,   4515.8550,   5441.8479,
          37363.6681,   1601.3204]], dtype=torch.float64)
	q_value: tensor([[-32.8550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00933156225273879, distance: 1.1496711156933226 entropy 11.250170355105226
epoch: 52, step: 69
	action: tensor([[-14770.7210,   5355.9220, -13030.8464,  30016.5007,  21637.9786,
          -3532.7424,   4678.3403]], dtype=torch.float64)
	q_value: tensor([[-25.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1480513263499682, distance: 1.056241942161607 entropy 11.308830688437524
epoch: 52, step: 70
	action: tensor([[-26100.7138, -64697.5569,  35985.0714,  44492.4705, -26984.7254,
          23283.3549,  -1188.3257]], dtype=torch.float64)
	q_value: tensor([[-28.3358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3510278579626953, distance: 0.9218700814891371 entropy 11.363884694641259
epoch: 52, step: 71
	action: tensor([[-18115.1065,  -9224.7795,  26644.8997,  26986.5172,   -334.3419,
          -7255.6147,   7752.1162]], dtype=torch.float64)
	q_value: tensor([[-21.3561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08794203383016264, distance: 1.1936020926136943 entropy 11.204883206640185
epoch: 52, step: 72
	action: tensor([[-48959.4297, -17845.0462,  -7680.1165, -21263.9301,  -1978.8020,
          40505.5313,   4566.1429]], dtype=torch.float64)
	q_value: tensor([[-26.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2905512634557823, distance: 1.7319155088247364 entropy 11.374253887124482
epoch: 52, step: 73
	action: tensor([[-18136.9214, -16150.3031, -15395.9418,  16284.2954,   5869.5279,
          20441.5520,  24311.8958]], dtype=torch.float64)
	q_value: tensor([[-24.0755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3585892041928078, distance: 1.3338308958840885 entropy 11.25000266664861
epoch: 52, step: 74
	action: tensor([[-52611.9455, -12835.6963,   5975.8583,   8677.6476, -13018.5957,
           -519.9493, -65406.8035]], dtype=torch.float64)
	q_value: tensor([[-24.8737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0199320381131853, distance: 1.6263914107780262 entropy 11.454206796696818
epoch: 52, step: 75
	action: tensor([[ 19598.6292,  -5747.7137,  -5516.1491, -43696.8313,  15285.1896,
          32503.1815,   6634.1113]], dtype=torch.float64)
	q_value: tensor([[-21.1262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0581898939280493, distance: 1.1105506392297013 entropy 11.164750430297275
epoch: 52, step: 76
	action: tensor([[   566.6375, -12540.1473,  11357.1348,  14435.6444,  -2057.8482,
          -4843.6395,  13390.8023]], dtype=torch.float64)
	q_value: tensor([[-20.2816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23311668755071813, distance: 1.0021237088023902 entropy 10.957455049191847
epoch: 52, step: 77
	action: tensor([[ -6581.1774,  22463.6395,   2034.7145,  31416.5638,   6422.1316,
         -12943.3128,  57884.5850]], dtype=torch.float64)
	q_value: tensor([[-33.7358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.332079817195027
epoch: 52, step: 78
	action: tensor([[ 6805.6152, -9857.9879, -1040.3631,  8344.0531, 23098.1327, 10004.6931,
          4534.8774]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4134986619190202, distance: 0.8763774553975548 entropy 10.813167454532193
epoch: 52, step: 79
	action: tensor([[  -971.6059, -41904.2378,  12703.0139,  31095.3670,  -9134.1722,
           6624.6179,   3337.3395]], dtype=torch.float64)
	q_value: tensor([[-26.5220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17219842968254362, distance: 1.238959930254048 entropy 11.397048948106944
epoch: 52, step: 80
	action: tensor([[  6077.0552,  -8844.1652, -30895.2425,  24624.2353, -34019.4737,
          19589.1307,  15531.7129]], dtype=torch.float64)
	q_value: tensor([[-22.8303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2902683309764599, distance: 0.9640593820342458 entropy 11.272136754988024
epoch: 52, step: 81
	action: tensor([[-30995.0564,  21885.1759, -12661.7806,  -1724.3745,  -3977.4424,
          35294.9960,   6578.3268]], dtype=torch.float64)
	q_value: tensor([[-25.5420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.26567599023443
epoch: 52, step: 82
	action: tensor([[ 13169.9177,  -7855.0492,  -9273.7412,  10764.2787,  -3051.6564,
         -11758.4193,  10714.3949]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3978748508887012, distance: 0.887973646161452 entropy 10.813167454532193
epoch: 52, step: 83
	action: tensor([[  -538.2669, -30710.0258,  14755.7732,  23373.6688,   5185.3099,
         -12104.2027,  -1352.2181]], dtype=torch.float64)
	q_value: tensor([[-26.7908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07760797214167447, distance: 1.187919717848342 entropy 11.16439473746642
epoch: 52, step: 84
	action: tensor([[ 5.6467e+03, -1.9510e+04, -1.8628e+04, -1.5579e+01, -3.9287e+03,
         -1.3860e+04, -2.0043e+03]], dtype=torch.float64)
	q_value: tensor([[-25.0410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04860830878827205, distance: 1.171826568875013 entropy 11.232089153590914
epoch: 52, step: 85
	action: tensor([[-11135.3881,   2775.4150,   2016.9931,  -4567.5845,  24124.8956,
          -2680.2325,   8704.4253]], dtype=torch.float64)
	q_value: tensor([[-27.1951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.219085690244915
epoch: 52, step: 86
	action: tensor([[-14357.1223,   5149.8850,    109.5028,   9483.8295,   2923.4028,
           9582.5270,  -5312.8909]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19769301129722394, distance: 1.0250073530845991 entropy 10.813167454532193
epoch: 52, step: 87
	action: tensor([[  4513.3028,  -2007.4343,  20304.4411,  -4067.1890, -14377.4137,
           3283.1434, -42433.5428]], dtype=torch.float64)
	q_value: tensor([[-25.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.265391269524585
epoch: 52, step: 88
	action: tensor([[-21344.1730,   5901.0684,  -6973.2294,  25036.2517,  13930.6546,
           9566.1368,  -3431.6681]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2774108807381095, distance: 0.9727526041179965 entropy 10.813167454532193
epoch: 52, step: 89
	action: tensor([[-24523.6046, -41387.4285,   7504.7756,  18109.5583,   -974.7999,
          15862.9695,  24665.1474]], dtype=torch.float64)
	q_value: tensor([[-25.6688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1009989770709625, distance: 1.085017608278669 entropy 11.158493311120186
epoch: 52, step: 90
	action: tensor([[  7074.6666,  -6971.5512, -80964.2201,  27736.3696,   7246.6777,
          -5447.9895, -23866.2772]], dtype=torch.float64)
	q_value: tensor([[-26.6615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.602384482728928, distance: 0.7215864271509519 entropy 11.437412310582252
epoch: 52, step: 91
	action: tensor([[ 17285.8484, -15652.5159,  17378.3919,   2366.5846, -12173.9433,
          16654.2624, -15552.3133]], dtype=torch.float64)
	q_value: tensor([[-30.9006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1865242991048509, distance: 1.0321171290082676 entropy 11.29934865782234
epoch: 52, step: 92
	action: tensor([[ -4412.2428,   5905.4167,  11288.5092,  -3022.9460,  19138.6028,
         -25546.6146,  -4783.5089]], dtype=torch.float64)
	q_value: tensor([[-28.7401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.294949819310505
epoch: 52, step: 93
	action: tensor([[  1042.5434,  -3786.7114,    895.1805,   3977.0765,  -6284.4361,
         -11024.7875,  10698.7365]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02144407182039021, distance: 1.1320080608603327 entropy 10.813167454532193
epoch: 52, step: 94
	action: tensor([[-26830.0966,  -5365.2430,  -6591.0550,   9903.9388,  26936.0181,
           7152.0762,  -9955.8339]], dtype=torch.float64)
	q_value: tensor([[-33.1024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34518643903995305, distance: 0.9260096781807198 entropy 11.341142135248841
epoch: 52, step: 95
	action: tensor([[-32946.5095, -20006.5270,  38116.8138,  25572.5183,  -7177.7221,
         -18232.2975, -12990.1577]], dtype=torch.float64)
	q_value: tensor([[-22.8487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07371973340075089, distance: 1.185774647642521 entropy 11.330532345538543
epoch: 52, step: 96
	action: tensor([[ 14807.3158,   -152.3240, -32929.5130, -28482.9961,  32155.0226,
         -17221.3622, -18619.1794]], dtype=torch.float64)
	q_value: tensor([[-30.4050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1533404158571734, distance: 1.2289534943944977 entropy 11.548069472598417
epoch: 52, step: 97
	action: tensor([[ -8528.3147, -24258.7501,  -6794.8575,   5340.0352,   5818.4000,
          -9571.8319,   4250.3981]], dtype=torch.float64)
	q_value: tensor([[-26.3309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2980653410878151, distance: 1.3037819684771284 entropy 11.182283564277029
epoch: 52, step: 98
	action: tensor([[ -8830.6961,  18473.5674,  13512.6641,  27347.4433,  29097.5706,
           9086.2982, -22243.7869]], dtype=torch.float64)
	q_value: tensor([[-27.7015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.219582358149927, distance: 1.010928016145417 entropy 11.488152086933326
epoch: 52, step: 99
	action: tensor([[ -6898.0417, -11334.7574,  -5118.1375, -15094.4960,  11941.0946,
          10054.4330,  30141.7694]], dtype=torch.float64)
	q_value: tensor([[-28.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6973836436013565, distance: 1.490893769162519 entropy 11.353317168503951
epoch: 52, step: 100
	action: tensor([[18392.5161,  -851.3157, -3161.6226, 30666.5912, -8294.8757,  1057.1207,
          3999.1594]], dtype=torch.float64)
	q_value: tensor([[-25.1183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022177135854322905, distance: 1.1315839717290912 entropy 11.330948510671899
epoch: 52, step: 101
	action: tensor([[-37068.7528, -20619.7852, -12360.2745,  -7993.1602, -35646.4952,
          12980.0360,  -5849.6451]], dtype=torch.float64)
	q_value: tensor([[-23.9434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16433571249882206, distance: 1.2347976731890817 entropy 11.360894010922815
epoch: 52, step: 102
	action: tensor([[  716.9213,  9669.9031,  5045.6710,   423.4956, 34173.6078, 18107.2485,
          7719.1550]], dtype=torch.float64)
	q_value: tensor([[-22.9457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.190954369516557
epoch: 52, step: 103
	action: tensor([[ -4630.2893, -11502.9106,   3108.0017,  12570.4041,    906.4805,
           7036.2245,   9290.6273]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.650054744252752, distance: 1.4699611940945883 entropy 10.813167454532193
epoch: 52, step: 104
	action: tensor([[ -3257.8573, -27195.7305,  -4428.7362,  30197.7417,  -6997.2364,
         -18208.0355,  25624.4643]], dtype=torch.float64)
	q_value: tensor([[-25.2055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04778089733337754, distance: 1.1713641588132424 entropy 11.43263491709655
epoch: 52, step: 105
	action: tensor([[-17024.3477, -20337.2685,   1953.8400,  -2329.0258,  39924.0265,
          15472.8498,  13004.2965]], dtype=torch.float64)
	q_value: tensor([[-27.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.834392213527847, distance: 1.549896838556647 entropy 11.449583411420722
epoch: 52, step: 106
	action: tensor([[  2608.6633, -16557.3535,  16309.3619,   2293.0760,  18434.3920,
           8744.4297,  23189.8197]], dtype=torch.float64)
	q_value: tensor([[-21.6156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2477353295229643, distance: 0.9925263062366526 entropy 11.09460499107831
epoch: 52, step: 107
	action: tensor([[ 3672.4286,  -723.9131, -9759.6874, 13209.9832,  4350.1678, -8339.2412,
          4964.1564]], dtype=torch.float64)
	q_value: tensor([[-22.9119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002114161435390116, distance: 1.1455532795638936 entropy 11.234278441024122
epoch: 52, step: 108
	action: tensor([[ 21615.2777, -17541.4892,  34299.6403,  20628.1552,  13811.1571,
         -16330.1806,  34291.9680]], dtype=torch.float64)
	q_value: tensor([[-28.8910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3722093672880179, distance: 0.9067010296473589 entropy 11.251618496169996
epoch: 52, step: 109
	action: tensor([[-11777.6894,  -1055.4952, -14631.4438,  18532.4344, -18980.7872,
          -8095.4589, -22959.0672]], dtype=torch.float64)
	q_value: tensor([[-29.3376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05994729344069505, distance: 1.178145227665186 entropy 11.16181773027029
epoch: 52, step: 110
	action: tensor([[ 3350.8118, 13804.8710, 33267.0947,  7372.2376, 13534.1771, -8199.7066,
         -3310.5482]], dtype=torch.float64)
	q_value: tensor([[-23.1599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7936268442437305, distance: 0.5198562813854497 entropy 11.11717414024306
epoch: 52, step: 111
	action: tensor([[ -5517.6252, -18584.5335,  17042.0563,  18949.3467,  -4336.3600,
         -44285.2402,   7879.9657]], dtype=torch.float64)
	q_value: tensor([[-29.2045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08213803849180734, distance: 1.1904139975763026 entropy 11.216624099963715
epoch: 52, step: 112
	action: tensor([[ -2600.8420, -38459.0686, -27077.2167,  26919.8120,  -2503.1029,
          28312.4955,  -2701.7863]], dtype=torch.float64)
	q_value: tensor([[-23.6858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14288214967652513, distance: 1.2233688500245994 entropy 11.24337984538106
epoch: 52, step: 113
	action: tensor([[ 20224.5427, -12316.4176,   2160.2814,  -8541.6165,  43960.3796,
         -24386.7093,   3244.9378]], dtype=torch.float64)
	q_value: tensor([[-26.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07708400443068308, distance: 1.187630880315832 entropy 11.547386921657225
epoch: 52, step: 114
	action: tensor([[ -8672.9949, -40031.2371,  16671.3789,     49.0225,   7777.4199,
            598.9362, -27551.1415]], dtype=torch.float64)
	q_value: tensor([[-24.6655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.478403053705601, distance: 1.3914035873629131 entropy 11.338258832459333
epoch: 52, step: 115
	action: tensor([[-21939.2731, -33995.6983,  18771.8617,  35484.7884,   5787.6640,
          -9891.8525, -33472.8858]], dtype=torch.float64)
	q_value: tensor([[-21.9012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19381316639199353, distance: 1.2503306443954627 entropy 11.206200972561504
epoch: 52, step: 116
	action: tensor([[ -6092.6856, -59147.1893,  -2237.7472,   -724.5024,  39033.0417,
         -34753.5968,   2249.5146]], dtype=torch.float64)
	q_value: tensor([[-24.4366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5992447369854299, distance: 1.4471520306412415 entropy 11.385333665049686
epoch: 52, step: 117
	action: tensor([[ 10015.8103,   3719.9318,  -3374.4766,  18380.1120,   6121.0164,
          -4262.6272, -24061.1506]], dtype=torch.float64)
	q_value: tensor([[-22.4045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.139287494903643
epoch: 52, step: 118
	action: tensor([[ -6822.8782,    530.7585,   8517.1193,  -2075.2927,  -2129.5934,
           4258.5566, -13855.5309]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5576662695855796, distance: 0.7610824758105886 entropy 10.813167454532193
epoch: 52, step: 119
	action: tensor([[-22872.6186, -26747.3297, -13666.0059, -18246.3531,  -3166.3393,
         -14718.0527,  38956.0143]], dtype=torch.float64)
	q_value: tensor([[-40.0901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08807872876451228, distance: 1.0927866134647908 entropy 11.317809285510211
epoch: 52, step: 120
	action: tensor([[ 19678.0093,  -1794.0342,  23066.9670, -32407.3513,   7386.7274,
          32139.9783,  35297.9063]], dtype=torch.float64)
	q_value: tensor([[-26.1530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3587023816828324, distance: 0.9164030095416964 entropy 11.215876699331334
epoch: 52, step: 121
	action: tensor([[ -1132.0196,  -3242.7072,   9222.1507,  29385.4049,  14079.3567,
         -10647.4883,  -6423.1548]], dtype=torch.float64)
	q_value: tensor([[-22.8477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5869712070923347, distance: 1.4415881938444997 entropy 11.108121474953283
epoch: 52, step: 122
	action: tensor([[ -1649.2656, -16883.8949, -17042.7277,   5502.5992, -10640.9247,
         -10870.2368,  -5426.8587]], dtype=torch.float64)
	q_value: tensor([[-20.5220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1803160309216827, distance: 1.0360480890185482 entropy 11.108492007326475
epoch: 52, step: 123
	action: tensor([[-12884.0226, -15362.3617,    236.5656, -23574.9869,  15821.0012,
          -5921.7737, -30007.4189]], dtype=torch.float64)
	q_value: tensor([[-28.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22969663419262343, distance: 1.2689826533128792 entropy 11.565828776492774
epoch: 52, step: 124
	action: tensor([[-15419.3815,   5816.3727, -17819.8787, -21041.5684,   2230.4213,
          22366.3249,  22704.2765]], dtype=torch.float64)
	q_value: tensor([[-24.8597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11645028953128278, distance: 1.075652987400948 entropy 11.242906053296187
epoch: 52, step: 125
	action: tensor([[ 10574.1484,  -1059.1748, -28383.2742,   6982.4429,   6005.6825,
          11766.2632,   3320.8547]], dtype=torch.float64)
	q_value: tensor([[-35.6978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.3070154120938
epoch: 52, step: 126
	action: tensor([[-10286.4505, -19672.1025,  35925.2507,  20601.7993, -12255.1448,
           8988.4239,     93.0197]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.813167454532193
epoch: 52, step: 127
	action: tensor([[-10920.0027,    239.0938, -17614.8926,   6839.1465,   7490.0800,
          -9630.3496,  30247.3310]], dtype=torch.float64)
	q_value: tensor([[-28.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.813167454532193
LOSS epoch 52 actor 298.0632133887929 critic 277.9572834663942
epoch: 53, step: 0
	action: tensor([[ -9320.0201,   4103.9795,  14808.9261,   6593.7807,  18548.7405,
         -21681.1758, -16309.9127]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12951755660352404, distance: 1.216194923062424 entropy 10.8545236977184
epoch: 53, step: 1
	action: tensor([[ -7188.8292,  16997.7219,  10930.1760,  11831.6500,  14563.0958,
          -8832.9778, -17267.5181]], dtype=torch.float64)
	q_value: tensor([[-29.0936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2228477657920781, distance: 1.0088108469362045 entropy 11.097125974565312
epoch: 53, step: 2
	action: tensor([[  8775.2155,  19727.9517,  -4349.8123,  23649.4031,  17301.2954,
         -11789.1624,  44225.6805]], dtype=torch.float64)
	q_value: tensor([[-34.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37486795425097885, distance: 0.9047791301398096 entropy 11.389986386813188
epoch: 53, step: 3
	action: tensor([[-10719.6106, -30336.4338, -15485.6523,  12507.2387,   2117.4650,
          21273.6141,  11206.1611]], dtype=torch.float64)
	q_value: tensor([[-35.2126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12816883308149596, distance: 1.0684959700234316 entropy 11.440359361587157
epoch: 53, step: 4
	action: tensor([[ -5752.8106, -25916.5311, -10699.2035,  27951.8944,  16704.8430,
         -17640.3772,  14838.2364]], dtype=torch.float64)
	q_value: tensor([[-31.1501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21545201404729508, distance: 1.0135996436538701 entropy 11.582304297914801
epoch: 53, step: 5
	action: tensor([[ -8031.4182,  -3768.3178, -36094.9233,   4229.0650,  12090.8172,
          19746.7562,   5242.5817]], dtype=torch.float64)
	q_value: tensor([[-33.5374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011947559363573168, distance: 1.1511600169666978 entropy 11.520131034955314
epoch: 53, step: 6
	action: tensor([[-37228.2629,  32911.8815, -21566.7263, -16462.1843,  11362.7121,
         -24219.1487, -52043.0745]], dtype=torch.float64)
	q_value: tensor([[-29.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044689448160123, distance: 0.9543659940993048 entropy 11.426361543823617
epoch: 53, step: 7
	action: tensor([[ -8625.1104, -35245.1611,   9418.9687,  14832.5318,  -3729.2419,
          18707.7071,  -8636.7007]], dtype=torch.float64)
	q_value: tensor([[-42.4001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9481023515064202, distance: 1.597212051961132 entropy 11.440419944968406
epoch: 53, step: 8
	action: tensor([[-30853.6290, -14759.6837,    -62.9098,   -364.6461,  17386.1479,
          33847.7161,   5774.1415]], dtype=torch.float64)
	q_value: tensor([[-26.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7098014798464543, distance: 1.4963374227369954 entropy 11.312780230769604
epoch: 53, step: 9
	action: tensor([[  -238.7571, -36172.5985, -26057.9576,  11694.3594, -12454.6436,
           7225.4333,   9126.9502]], dtype=torch.float64)
	q_value: tensor([[-24.0763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2844195873527484, distance: 1.2969109385103395 entropy 11.060253094291296
epoch: 53, step: 10
	action: tensor([[-35286.4034,  13206.4849,    736.1176, -33070.4471, -13346.5999,
          -6139.0500, -11813.4568]], dtype=torch.float64)
	q_value: tensor([[-27.9358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2850709202263162, distance: 0.9675828775958476 entropy 11.422215679761681
epoch: 53, step: 11
	action: tensor([[22092.5138,  3300.2623, 17733.5057, 29967.6063, 21782.7351, 13153.8607,
           580.2064]], dtype=torch.float64)
	q_value: tensor([[-36.9887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.09869352566921
epoch: 53, step: 12
	action: tensor([[ -8098.1992, -17653.9495, -22818.5350,   -782.3115, -14898.6026,
         -13868.5632, -26885.4655]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3809002487718698, distance: 1.3447385255364184 entropy 10.8545236977184
epoch: 53, step: 13
	action: tensor([[-28627.8852, -29373.6058,   6088.4756,  -8235.6816,  42153.7194,
         -23777.0572,  13578.7527]], dtype=torch.float64)
	q_value: tensor([[-35.1172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03930683718248951, distance: 1.1216285187099455 entropy 11.44969435631236
epoch: 53, step: 14
	action: tensor([[-22637.4400, -12060.8149,  34488.9655,  44474.2181,   2307.0581,
          -2417.0488,  28037.9700]], dtype=torch.float64)
	q_value: tensor([[-30.8739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9296855836440081, distance: 1.589644345008299 entropy 11.303424272750684
epoch: 53, step: 15
	action: tensor([[-35115.6390, -29839.4179, -16580.8898,  19473.8147,  24209.5086,
          -6203.1920,  -7380.4312]], dtype=torch.float64)
	q_value: tensor([[-27.6162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7802980667282355, distance: 1.526873484944309 entropy 11.277300124332527
epoch: 53, step: 16
	action: tensor([[-16557.0423, -21319.5456,   9242.4682,  10614.7091,   3770.8162,
         -14512.6980,  -7487.1875]], dtype=torch.float64)
	q_value: tensor([[-29.0202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8339019811970441, distance: 1.54968972355019 entropy 11.349404248699102
epoch: 53, step: 17
	action: tensor([[ 22900.5932, -22318.6832, -23088.6954,  47644.3506, -20867.3054,
           1963.2306,  13506.1579]], dtype=torch.float64)
	q_value: tensor([[-29.4777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.366273287330582
epoch: 53, step: 18
	action: tensor([[ -9892.2288,  -6370.3661,  -8992.1699,  22942.2734,  23635.5382,
         -17597.2015,   9744.6968]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3016454460447293, distance: 1.305578666290248 entropy 10.8545236977184
epoch: 53, step: 19
	action: tensor([[ 12719.3678, -28167.4277,  29842.9473,   2566.3826, -39447.2112,
          23548.7362,  -2826.1133]], dtype=torch.float64)
	q_value: tensor([[-28.9260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013355054771190034, distance: 1.151960300252378 entropy 11.36230616822925
epoch: 53, step: 20
	action: tensor([[-15662.2478, -16702.4689, -11551.6293,  23247.7715, -18049.8635,
         -13341.5484,  14761.4298]], dtype=torch.float64)
	q_value: tensor([[-27.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05587495377667118, distance: 1.111914651154352 entropy 11.336142072799023
epoch: 53, step: 21
	action: tensor([[-19395.1934, -25104.6812,  -8215.1747,  73717.8061,  39736.7654,
         -15950.1999,  21838.1548]], dtype=torch.float64)
	q_value: tensor([[-32.9076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05366149784663421, distance: 1.1132173034091843 entropy 11.502627326461196
epoch: 53, step: 22
	action: tensor([[ 54785.1512, -10043.2460, -22015.2524,  16006.1497,   5867.4303,
          -9076.7138,  -2779.2518]], dtype=torch.float64)
	q_value: tensor([[-32.2762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020449210799069695, distance: 1.1325833496964763 entropy 11.583412518283307
epoch: 53, step: 23
	action: tensor([[-1045.3151,  4353.4312,  4572.0269, 32129.9101,  8175.9155, -8475.5706,
         -4225.6421]], dtype=torch.float64)
	q_value: tensor([[-34.3446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.273558053503745
epoch: 53, step: 24
	action: tensor([[-10100.2173, -18195.1394,    206.2902,  13720.9679,   6177.2477,
           7840.0759, -15144.4525]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0814246018356074, distance: 1.09676630143386 entropy 10.8545236977184
epoch: 53, step: 25
	action: tensor([[ -6356.4905,  17770.4423,  11731.0567,  36675.2660, -39656.9018,
         -23372.9525,  14104.5633]], dtype=torch.float64)
	q_value: tensor([[-26.8523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1577488895601895, distance: 1.0502132406470297 entropy 11.31059229111768
epoch: 53, step: 26
	action: tensor([[-13986.1421,  -5859.0050,   1055.8164,   3846.1495,   2531.2174,
          38431.0005,  12894.1043]], dtype=torch.float64)
	q_value: tensor([[-32.3095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3948880047672918, distance: 1.3515320939770852 entropy 11.314173033359067
epoch: 53, step: 27
	action: tensor([[-11609.3921,  -3054.0410, -27162.8601,  -3582.1950,   -539.8126,
           5150.4935,  17381.7457]], dtype=torch.float64)
	q_value: tensor([[-26.2981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8172742740398669, distance: 1.542648327223834 entropy 11.254509034313573
epoch: 53, step: 28
	action: tensor([[-23374.4031,  -6037.0705,    134.8831,  -8654.2175,  29460.4537,
          -1520.7590,  -8026.6766]], dtype=torch.float64)
	q_value: tensor([[-24.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3199683956328585, distance: 1.3147357118162029 entropy 11.24090841483419
epoch: 53, step: 29
	action: tensor([[ -4284.3731, -29306.9230, -17110.7609,  12261.2821, -47323.0316,
          36274.8208,   5865.7983]], dtype=torch.float64)
	q_value: tensor([[-35.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7991516457859094, distance: 1.534937083069833 entropy 11.509288008186063
epoch: 53, step: 30
	action: tensor([[ 17403.0295, -24162.7819,  19485.4075, -23009.8391,  17657.2273,
          13010.1361,  13809.7632]], dtype=torch.float64)
	q_value: tensor([[-29.3265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09205240979391027, distance: 1.0904031148152704 entropy 11.48936019578666
epoch: 53, step: 31
	action: tensor([[-15973.3927, -39508.4702,  13792.1221,   8349.2380, -21053.8194,
             96.5812,  13629.4387]], dtype=torch.float64)
	q_value: tensor([[-29.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30236278325511323, distance: 1.305938369143628 entropy 11.193247287755103
epoch: 53, step: 32
	action: tensor([[ 10071.5998, -13383.3173,  -3453.9340,   1083.8485,  -3397.7883,
          -8115.4547,  -3292.3944]], dtype=torch.float64)
	q_value: tensor([[-21.9694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5351790931546165, distance: 0.780188456130887 entropy 11.037725549229252
epoch: 53, step: 33
	action: tensor([[ 13419.8940, -13027.3410,  33553.9122,   3451.9993,  10937.6680,
          -9251.7683,  -6683.3840]], dtype=torch.float64)
	q_value: tensor([[-33.8978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27591728753734857, distance: 1.2926113177230418 entropy 11.187639540181262
epoch: 53, step: 34
	action: tensor([[-28723.4102, -14741.9794,  -1915.8526, -16342.0627,  15372.0150,
           5932.3736,  24237.5165]], dtype=torch.float64)
	q_value: tensor([[-35.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3947896150662429, distance: 1.3514844273604023 entropy 11.312470899886636
epoch: 53, step: 35
	action: tensor([[-45424.9888,   8514.8812,  31545.1242,    466.6005,  18918.6955,
           6752.6435,   3775.6716]], dtype=torch.float64)
	q_value: tensor([[-27.8481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5003787263585899, distance: 0.8088670685977266 entropy 11.356527586749015
epoch: 53, step: 36
	action: tensor([[-31903.5219, -25893.0252, -18887.1769, -30889.8618,  12875.6396,
          13435.6135,   8267.0706]], dtype=torch.float64)
	q_value: tensor([[-27.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20480895004143984, distance: 1.2560756188985533 entropy 11.147483872723772
epoch: 53, step: 37
	action: tensor([[ 15759.1398,  -2534.2545,  -3878.7114,  16401.3180,  -2433.7723,
         -14442.9141,  19896.0122]], dtype=torch.float64)
	q_value: tensor([[-25.4086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5936531325897243, distance: 0.7294661626247475 entropy 11.12731753952965
epoch: 53, step: 38
	action: tensor([[ -7314.5816,  -7021.6315,  23481.5778, -12877.4955,  39530.6024,
          12809.6284,   -420.8875]], dtype=torch.float64)
	q_value: tensor([[-27.5123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01392550282574101, distance: 1.1522844911936816 entropy 11.255918226670099
epoch: 53, step: 39
	action: tensor([[-35874.1883, -28031.4130,   5040.3058,  -1772.9965,  -3471.3731,
           4910.9203, -14226.0501]], dtype=torch.float64)
	q_value: tensor([[-26.3216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04000300322763761, distance: 1.1212220511559343 entropy 11.274960584477123
epoch: 53, step: 40
	action: tensor([[-11615.9254, -26524.0010,   7905.2315,    907.9323, -11569.8235,
           9354.9682,   8452.6994]], dtype=torch.float64)
	q_value: tensor([[-31.6678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9598675567045782, distance: 1.6020278258555212 entropy 11.404807787609158
epoch: 53, step: 41
	action: tensor([[-44368.1989,  19569.5229,  12415.1344,  45306.1702, -21849.5749,
           9983.7379,  -3879.5979]], dtype=torch.float64)
	q_value: tensor([[-26.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4033697044411736, distance: 0.8839126398737012 entropy 11.303377323237838
epoch: 53, step: 42
	action: tensor([[ 11319.1023, -30758.9964,  20332.9448,  22917.9763,  -8794.5195,
         -55675.9742,  -2203.7405]], dtype=torch.float64)
	q_value: tensor([[-34.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5042035185752344, distance: 0.8057650267494318 entropy 11.496948754589686
epoch: 53, step: 43
	action: tensor([[ -9419.9717, -27734.9111, -26774.8741, -13606.1564, -19036.3602,
          31614.2276,  -8027.8465]], dtype=torch.float64)
	q_value: tensor([[-32.9839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4096477873432953, distance: 1.3586637874021432 entropy 11.204030228003033
epoch: 53, step: 44
	action: tensor([[  2846.9692, -22442.9172,  13360.2819,  23824.6088,   6862.4012,
          34803.9291,  25607.1436]], dtype=torch.float64)
	q_value: tensor([[-24.5176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20930037450289185, distance: 1.0175657009910217 entropy 11.215279731569577
epoch: 53, step: 45
	action: tensor([[-22155.3711, -10806.2546,   2660.8443,  44415.9770, -22809.3828,
          25633.6585,  34608.2071]], dtype=torch.float64)
	q_value: tensor([[-32.8959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11301438075941639, distance: 1.2072774286828298 entropy 11.363394881940726
epoch: 53, step: 46
	action: tensor([[-32189.8324, -44668.2702,  13693.6913,  10016.7072,  15010.4702,
          17878.0948,  16035.6418]], dtype=torch.float64)
	q_value: tensor([[-29.4728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03508541642520635, distance: 1.1240901141880573 entropy 11.379013511036216
epoch: 53, step: 47
	action: tensor([[-28075.8364,  -2902.5061, -17540.4798,  18056.3128,  -6467.1433,
         -35944.9612,  -1330.5714]], dtype=torch.float64)
	q_value: tensor([[-29.6172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04356185556652381, distance: 1.1690034461304728 entropy 11.361144867293087
epoch: 53, step: 48
	action: tensor([[-18297.2156, -44336.5198,  15307.3451,  16589.2389,  19683.0094,
         -16288.2748, -22202.1377]], dtype=torch.float64)
	q_value: tensor([[-30.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15954492321540548, distance: 1.0490928948687643 entropy 11.443146718742568
epoch: 53, step: 49
	action: tensor([[-27911.2467, -50444.8882,  -5186.5618,  -4587.2610,  -5062.0228,
           2901.1351, -41884.2020]], dtype=torch.float64)
	q_value: tensor([[-31.8881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9771130955371707, distance: 1.609060781285711 entropy 11.533278820323039
epoch: 53, step: 50
	action: tensor([[-35598.5618,  12723.1963,  13475.1373,  -5096.1849,    474.5268,
          24361.4046,   1057.3652]], dtype=torch.float64)
	q_value: tensor([[-30.0380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14218565263507588, distance: 1.05987182222559 entropy 11.376923768111425
epoch: 53, step: 51
	action: tensor([[-19042.2058, -12831.5325, -18009.7137,   2905.5347,    695.7705,
         -47509.9498,  31915.3053]], dtype=torch.float64)
	q_value: tensor([[-49.0992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.601825809632299
epoch: 53, step: 52
	action: tensor([[  4599.7012,  -4057.1420, -16778.4953,  -9314.2496, -17490.0153,
           7822.7936,  -1939.8287]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5645880466111202, distance: 0.7551041672702385 entropy 10.8545236977184
epoch: 53, step: 53
	action: tensor([[ 11426.8468, -12886.4410,  13451.6888,   5291.3889,   -808.4875,
          29413.7669,  46183.2002]], dtype=torch.float64)
	q_value: tensor([[-31.6714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02963871918556793, distance: 1.1272582509468168 entropy 11.374141504330845
epoch: 53, step: 54
	action: tensor([[ -5979.6276,  -1916.6012,   9834.7208, -32118.1343, -22198.1450,
          17634.0339,   6241.6931]], dtype=torch.float64)
	q_value: tensor([[-28.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5661537447924347, distance: 1.4321017976681802 entropy 11.176686906753991
epoch: 53, step: 55
	action: tensor([[-21075.8902, -46454.8350,   9776.6405,  15337.1937,   9990.2358,
          27776.0586, -14896.8502]], dtype=torch.float64)
	q_value: tensor([[-26.1309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6436754289842086, distance: 1.4671169166610065 entropy 11.242133698195946
epoch: 53, step: 56
	action: tensor([[ 11233.9191,  13252.6487,  -4109.6826, -15303.9326, -14905.3801,
           8672.1078,  -9158.5605]], dtype=torch.float64)
	q_value: tensor([[-22.5430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.052588256142716
epoch: 53, step: 57
	action: tensor([[-14279.4618,  -7285.6875,   -556.6174,   9713.2655,  -8024.6556,
         -16084.4356,  -8732.4179]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41706749070630367, distance: 1.3622347687276528 entropy 10.8545236977184
epoch: 53, step: 58
	action: tensor([[ -5810.4285,   1425.6502,  21769.6565,  54003.8435,   8285.9214,
          -8153.2770, -23153.7609]], dtype=torch.float64)
	q_value: tensor([[-30.1069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3267098387800643, distance: 0.938983211428296 entropy 11.385183069144986
epoch: 53, step: 59
	action: tensor([[-21200.9534, -15218.0073, -11877.5465, -18519.1661,  -6484.4854,
           -702.7418, -14057.0984]], dtype=torch.float64)
	q_value: tensor([[-33.1609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3897101254571671, distance: 1.3490212915320037 entropy 11.35852158356906
epoch: 53, step: 60
	action: tensor([[-62177.8532,    303.6708,   -727.5814,   6295.3507,   2626.5874,
         -19130.7777,  44759.5947]], dtype=torch.float64)
	q_value: tensor([[-30.3052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.195236684322467, distance: 1.0265752249308946 entropy 11.326986477611399
epoch: 53, step: 61
	action: tensor([[-36696.2110, -44576.1661,  22990.1498, -14737.0465,  -5903.9993,
          26641.7204,  17316.5917]], dtype=torch.float64)
	q_value: tensor([[-31.3817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03689950669060171, distance: 1.165265872166859 entropy 11.287057218405872
epoch: 53, step: 62
	action: tensor([[-25736.3982,    916.3788, -11081.5337,  13298.1500,  17410.4981,
          -5885.5264,  15971.7090]], dtype=torch.float64)
	q_value: tensor([[-26.9492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.039242651986127974, distance: 1.166581740514458 entropy 11.238643695858443
epoch: 53, step: 63
	action: tensor([[ -6743.3493,  -7374.5673,   3853.2486,  23653.0180,  -7869.1543,
           1937.0496, -13502.3051]], dtype=torch.float64)
	q_value: tensor([[-25.8846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3523036827363226, distance: 1.3307418303526743 entropy 11.026635716035985
epoch: 53, step: 64
	action: tensor([[-32861.4127, -28979.7371,   6661.7296,  14405.8136,  27661.1418,
         -34404.0574,   2772.3449]], dtype=torch.float64)
	q_value: tensor([[-29.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06902283434343204, distance: 1.1831782680657574 entropy 11.420400296557421
epoch: 53, step: 65
	action: tensor([[  2026.7222, -28533.3914, -16712.1846,  10096.8837, -23183.5868,
          22983.6286,  -2823.7105]], dtype=torch.float64)
	q_value: tensor([[-31.9357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030813739427870246, distance: 1.1265755400274748 entropy 11.424430239123897
epoch: 53, step: 66
	action: tensor([[ 43863.9799, -21584.5431,  -4490.6198,  17869.4278, -11143.7767,
          19524.9943,   5747.0423]], dtype=torch.float64)
	q_value: tensor([[-33.8851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.396730506085494, distance: 1.352424415753276 entropy 11.380307928009548
epoch: 53, step: 67
	action: tensor([[ 11693.0323,  30440.4767, -35390.5744,  14518.0705,  -4979.3695,
          13509.9622,  43962.9163]], dtype=torch.float64)
	q_value: tensor([[-38.8634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5975501025333996, distance: 0.7259598526335849 entropy 11.419109761246927
epoch: 53, step: 68
	action: tensor([[ -5003.6342,  -5576.0718,  43777.2080,  24998.3250, -23910.4148,
          10148.5832, -14660.6070]], dtype=torch.float64)
	q_value: tensor([[-35.9709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03451834470034165, distance: 1.1639271302819456 entropy 11.42480596382516
epoch: 53, step: 69
	action: tensor([[ -2967.6531,  -4228.4309,    613.5675,   -162.6822, -24617.0568,
         -26048.8967, -18243.0614]], dtype=torch.float64)
	q_value: tensor([[-32.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6204770320904283, distance: 1.4567268772683588 entropy 11.50847210726565
epoch: 53, step: 70
	action: tensor([[  4845.9592, -24675.8198,  33685.0204,  65935.2037,  31276.0581,
          -5525.2662, -48995.3275]], dtype=torch.float64)
	q_value: tensor([[-30.7395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5972322953786162, distance: 0.726246434524652 entropy 11.482215459503774
epoch: 53, step: 71
	action: tensor([[ -6523.1840, -24619.7973,  22163.7811,  10563.8140,   5831.0037,
         -19081.9876,  15055.3241]], dtype=torch.float64)
	q_value: tensor([[-29.5638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6780501831657657, distance: 1.482378698207281 entropy 11.251087739039589
epoch: 53, step: 72
	action: tensor([[-36571.8100, -23713.4074,  27613.2167,  40979.4803, -10974.3518,
         -51531.0296,  14028.8622]], dtype=torch.float64)
	q_value: tensor([[-27.4540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10078391792447738, distance: 1.0851473795477733 entropy 11.30840780301945
epoch: 53, step: 73
	action: tensor([[-10893.6185, -15384.5436,  14969.3931,  -1255.4898,  -3106.1694,
           7791.6396, -21700.4211]], dtype=torch.float64)
	q_value: tensor([[-29.1841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5631815264242097, distance: 1.4307422438289399 entropy 11.418932477796336
epoch: 53, step: 74
	action: tensor([[ 28703.9881, -36625.2325,  38603.8948,  29710.9223, -23313.4469,
         -34013.6616,  21730.4456]], dtype=torch.float64)
	q_value: tensor([[-30.4198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5253182085089163, distance: 0.7884206300783019 entropy 11.419306674098568
epoch: 53, step: 75
	action: tensor([[  7715.9628, -53264.3075,  13135.7068, -27034.6705, -16931.2007,
          23469.3337,  50821.4488]], dtype=torch.float64)
	q_value: tensor([[-32.4216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2557120064846573, distance: 0.9872501306515798 entropy 11.475304020787291
epoch: 53, step: 76
	action: tensor([[-12690.9102, -52609.6543,  41959.0138,   3215.9107, -10829.1465,
          28430.0784,   8105.4782]], dtype=torch.float64)
	q_value: tensor([[-30.2195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49894490756525567, distance: 1.401036755003103 entropy 11.412633002539213
epoch: 53, step: 77
	action: tensor([[-21214.1277, -13029.9969,  23404.2057,  22990.4930,   6246.6943,
           6672.5675,  18688.0874]], dtype=torch.float64)
	q_value: tensor([[-29.8173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2128796713495098, distance: 1.2602756689703913 entropy 11.432294054073774
epoch: 53, step: 78
	action: tensor([[ -6119.6853,  19365.9214,  -1148.3174, -18299.5912,  -5471.3313,
          16940.7519,  10990.2524]], dtype=torch.float64)
	q_value: tensor([[-27.6413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015774342945859665, distance: 1.1533345779663338 entropy 11.351952655487915
epoch: 53, step: 79
	action: tensor([[ 11182.1279,  -4995.3357,   5958.2699, -14250.7588, -23784.4666,
           -130.0123,  14647.6991]], dtype=torch.float64)
	q_value: tensor([[-30.2624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10859883789393299, distance: 1.0804216824304085 entropy 11.058025813858222
epoch: 53, step: 80
	action: tensor([[ -5641.1674, -22132.3568, -20540.0934, -26789.5881,   4047.9057,
           2700.0099,  -5882.9133]], dtype=torch.float64)
	q_value: tensor([[-26.5630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.050825306651838265, distance: 1.1148842201772262 entropy 11.257307058778403
epoch: 53, step: 81
	action: tensor([[-10620.2338, -23828.0549,   1051.8229, -25177.3332, -20328.6301,
         -24249.2757, -44222.3776]], dtype=torch.float64)
	q_value: tensor([[-31.8292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6660151429724084, distance: 1.4770532941206267 entropy 11.378780848421167
epoch: 53, step: 82
	action: tensor([[ 16927.7164, -30501.4713, -16981.6535,   6507.6550,  -1715.1915,
          -2597.4903,  40975.3350]], dtype=torch.float64)
	q_value: tensor([[-32.4515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11030853365549564, distance: 1.2058090307764144 entropy 11.405072556843395
epoch: 53, step: 83
	action: tensor([[-12511.9742, -34482.2031,  37988.9842, -11177.3899,  32926.6622,
           8215.5268, -13614.7311]], dtype=torch.float64)
	q_value: tensor([[-32.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2604578762001951, distance: 0.9840975516999572 entropy 11.217769335170571
epoch: 53, step: 84
	action: tensor([[-21158.6272, -49369.6433,  50857.5257,  12050.2726, -28740.4422,
           4757.0430,  -5954.1843]], dtype=torch.float64)
	q_value: tensor([[-30.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6699930113213581, distance: 1.4788155891915968 entropy 11.389288111057112
epoch: 53, step: 85
	action: tensor([[-43476.7453,  -6793.8123, -20948.9080,  26658.2441,  14184.5596,
          -3810.1189, -12746.2700]], dtype=torch.float64)
	q_value: tensor([[-25.7536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14137773398109976, distance: 1.0603708163801682 entropy 11.28620588195013
epoch: 53, step: 86
	action: tensor([[ -8498.6906, -19931.6424,  32546.9900,  27919.8575,  26222.6607,
          14191.4812, -11023.3428]], dtype=torch.float64)
	q_value: tensor([[-31.7836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8106604682922307, distance: 1.5398386037630059 entropy 11.561917000392514
epoch: 53, step: 87
	action: tensor([[-21671.2382, -40879.6037, -35601.2793,  31872.5206,   8161.8142,
          48249.2302,  -1231.5988]], dtype=torch.float64)
	q_value: tensor([[-31.3027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06738707689968038, distance: 1.1051148322956375 entropy 11.532705501984035
epoch: 53, step: 88
	action: tensor([[-11878.5674, -10968.4891,  -1396.3029,   3579.5524,  25919.8457,
          10766.9330,  -8742.4327]], dtype=torch.float64)
	q_value: tensor([[-26.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6518880516650356, distance: 1.4707775738639541 entropy 11.282325229753031
epoch: 53, step: 89
	action: tensor([[-29888.0775,   9393.9477,  -8682.0292,  27137.0831,  39159.1295,
          -7551.4720,  14798.2114]], dtype=torch.float64)
	q_value: tensor([[-29.2615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1120022024867483, distance: 1.0783571880540452 entropy 11.447983880879915
epoch: 53, step: 90
	action: tensor([[  7074.1551, -34609.1626, -15080.5317,  28879.2454, -19674.7534,
         -17601.1722,  72238.4181]], dtype=torch.float64)
	q_value: tensor([[-34.4665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5781134182727967, distance: 0.7432835984449949 entropy 11.460960574275381
epoch: 53, step: 91
	action: tensor([[ 14319.4688,   2900.9211, -10342.1366,  67800.8830,   3502.1214,
          12732.1785,   1353.3057]], dtype=torch.float64)
	q_value: tensor([[-31.8496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16824109812177934, distance: 1.043651308772087 entropy 11.402501081149522
epoch: 53, step: 92
	action: tensor([[ -7041.2158, -36438.6500, -31699.7485,  33766.8640,  -4522.8254,
          13093.0425,  28147.0445]], dtype=torch.float64)
	q_value: tensor([[-33.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04991986510628588, distance: 1.1154158514329684 entropy 11.255387504996465
epoch: 53, step: 93
	action: tensor([[ 22677.7599,   8355.0821,   1952.4839,  20496.4625,  -2151.9116,
         -11866.1576,  12746.0085]], dtype=torch.float64)
	q_value: tensor([[-28.0361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3846283998575134, distance: 0.8976879963510549 entropy 11.375993271431339
epoch: 53, step: 94
	action: tensor([[-59377.5763, -16478.0366, -28387.1471,    973.1271,  25362.1045,
          41589.9409,  -1115.3219]], dtype=torch.float64)
	q_value: tensor([[-36.9512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.527920535379595
epoch: 53, step: 95
	action: tensor([[-15956.6123,  -8709.0496,  10772.7829,   -555.5908,   -383.9341,
          -9254.4363,   5378.8686]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.8545236977184
epoch: 53, step: 96
	action: tensor([[ -2894.6318,   -177.5853,  21480.9443,  16073.1626,  12604.9373,
          -2180.4828, -15090.0888]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8261155925788004, distance: 1.5463963845942184 entropy 10.8545236977184
epoch: 53, step: 97
	action: tensor([[ -7094.9912, -14561.3758,  15237.8062,  11094.3293,   -668.8649,
           8337.6781,  35011.1345]], dtype=torch.float64)
	q_value: tensor([[-27.5101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8005480327152408, distance: 1.53553262774235 entropy 11.25826255074048
epoch: 53, step: 98
	action: tensor([[-20677.9386, -24079.7237,  -9090.6187, -17283.9016, -34131.6586,
          16848.0921,  49257.6168]], dtype=torch.float64)
	q_value: tensor([[-30.4390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9493769525764801, distance: 1.5977344771051665 entropy 11.479972509317367
epoch: 53, step: 99
	action: tensor([[-20439.0069, -25938.3409,  16533.7668, -14427.5308,  28369.9362,
           -878.5602,  34954.7282]], dtype=torch.float64)
	q_value: tensor([[-25.8441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22606897432599182, distance: 1.2671094931291982 entropy 11.321711729744123
epoch: 53, step: 100
	action: tensor([[  6576.5001, -12003.6405,   4390.0501,  19256.0523,  -7232.5161,
          22968.0315, -25112.8316]], dtype=torch.float64)
	q_value: tensor([[-29.1687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27510285407034263, distance: 1.2921987071345555 entropy 11.389063264370984
epoch: 53, step: 101
	action: tensor([[  3695.9533, -11539.8613,  -3281.9953,   9884.4201,   3594.8424,
          14403.4995,   7863.0923]], dtype=torch.float64)
	q_value: tensor([[-25.1569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5542008102406396, distance: 0.7640580058987048 entropy 11.22121349475039
epoch: 53, step: 102
	action: tensor([[-21853.9009,  19655.0808, -16197.3066,  35827.8170, -26079.2867,
          20124.2058,  30337.3736]], dtype=torch.float64)
	q_value: tensor([[-31.2979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5999212664566163, distance: 0.7238180794314375 entropy 11.409375441876564
epoch: 53, step: 103
	action: tensor([[-18099.8710,  -4715.8389,   6417.7373,   9167.6416, -18135.1227,
          -4180.1580, -15690.3904]], dtype=torch.float64)
	q_value: tensor([[-29.4324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1273465159978504, distance: 1.0689997575889305 entropy 11.325258660220452
epoch: 53, step: 104
	action: tensor([[-20083.3346, -16489.1549, -11754.6746,  -7006.8970,  21059.5692,
           9663.9306,  38645.3428]], dtype=torch.float64)
	q_value: tensor([[-30.8833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0012687128604276, distance: 1.618860387116745 entropy 11.410915544560007
epoch: 53, step: 105
	action: tensor([[-27337.6037, -25599.4454,   2062.7999,   8036.0949,  -1473.1626,
          29915.0764,   3246.7290]], dtype=torch.float64)
	q_value: tensor([[-28.4651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4440313816007153, distance: 1.3751339648315128 entropy 11.299534355688113
epoch: 53, step: 106
	action: tensor([[-24112.7071, -28395.9011, -24188.0500, -10220.3771,  -3459.3637,
         -21705.9143,  27787.6730]], dtype=torch.float64)
	q_value: tensor([[-32.1581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8611662349714899, distance: 1.5611666879969446 entropy 11.51687512395981
epoch: 53, step: 107
	action: tensor([[-16996.8611,   7499.5675,   3214.4136,  10031.0308, -26051.0171,
           4380.5780, -19476.7704]], dtype=torch.float64)
	q_value: tensor([[-33.0779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041192404447581765, distance: 1.120527259296604 entropy 11.456366457087709
epoch: 53, step: 108
	action: tensor([[ 13413.3833, -31841.5286, -16544.5461,  -7212.6483, -35201.7155,
          19418.7244, -11016.7392]], dtype=torch.float64)
	q_value: tensor([[-35.1102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05693849701946607, distance: 1.1112881967757389 entropy 11.435476386691429
epoch: 53, step: 109
	action: tensor([[ 28608.2773, -38760.4490,  18072.9955,  -2029.2565,  19581.7185,
           3604.6772,  -5086.0341]], dtype=torch.float64)
	q_value: tensor([[-33.0454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33595620755767674, distance: 0.9325133412602479 entropy 11.368017937421905
epoch: 53, step: 110
	action: tensor([[-40114.2839,  -3334.8309,  -4216.9617,   4156.0983,  17973.6706,
         -20287.2366,  12015.9905]], dtype=torch.float64)
	q_value: tensor([[-37.2352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5313055255444779, distance: 1.4160794424675176 entropy 11.575543740387143
epoch: 53, step: 111
	action: tensor([[-12712.0677, -37815.2083,  -5434.6082, -13172.7993,  12197.5431,
          37368.2845, -22758.5024]], dtype=torch.float64)
	q_value: tensor([[-28.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20919169269736582, distance: 1.017635630882308 entropy 11.412435767321325
epoch: 53, step: 112
	action: tensor([[ 18501.2481, -14799.5674,   6097.2530,  -1615.9346,  12982.5737,
         -22711.3592,  11407.2165]], dtype=torch.float64)
	q_value: tensor([[-25.4858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5201984144812306, distance: 0.7926610763235056 entropy 11.149169127012227
epoch: 53, step: 113
	action: tensor([[-29266.6637,  34327.9351, -48399.6235,   6025.2403, -14439.5682,
           4958.5404,   7000.1878]], dtype=torch.float64)
	q_value: tensor([[-32.5262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.264575017315565
epoch: 53, step: 114
	action: tensor([[-15236.8057, -24478.6421,  -2883.1063,  18195.1906,  -7454.1238,
           1151.5141, -14791.6047]], dtype=torch.float64)
	q_value: tensor([[-32.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0761072825428801, distance: 1.6488516728354292 entropy 10.8545236977184
epoch: 53, step: 115
	action: tensor([[ -4361.7132, -22367.9853, -23734.6117,  20651.0128,  -9248.5096,
          -3137.6180,  23830.6568]], dtype=torch.float64)
	q_value: tensor([[-20.8760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7776660949185523, distance: 1.5257444115496057 entropy 11.052306988841979
epoch: 53, step: 116
	action: tensor([[-18860.5531, -37375.5149,  -1677.6913,  -6898.6840,  15748.5574,
          23717.2569,  25932.9933]], dtype=torch.float64)
	q_value: tensor([[-26.6613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06472534918927297, distance: 1.1807976774432383 entropy 11.299480380655195
epoch: 53, step: 117
	action: tensor([[11261.7480, 15544.8639, 14412.0217, -7445.4819, 27452.2621, -8829.1199,
         27583.6241]], dtype=torch.float64)
	q_value: tensor([[-28.7873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7047524158253768, distance: 0.6217987857841856 entropy 11.40395800901793
epoch: 53, step: 118
	action: tensor([[  6269.9586, -61613.2304, -22821.1710,  44644.8153,  15878.2552,
         -29167.4233,  -1481.3895]], dtype=torch.float64)
	q_value: tensor([[-35.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2140777712951507, distance: 1.2608979745238755 entropy 11.189369349456944
epoch: 53, step: 119
	action: tensor([[ 10983.3076, -10729.8402,   9985.7806, -10890.1752, -25078.2206,
          25137.4773,  16498.6023]], dtype=torch.float64)
	q_value: tensor([[-29.2919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4817755396584277, distance: 0.8237883525969661 entropy 11.061094127590767
epoch: 53, step: 120
	action: tensor([[  -343.9269, -13898.9843, -25915.5840, -10295.3371,   4826.8971,
         -14548.0071,  -2767.9416]], dtype=torch.float64)
	q_value: tensor([[-27.8861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24886113875390492, distance: 0.9917833405646111 entropy 11.122933836109171
epoch: 53, step: 121
	action: tensor([[ -1523.2561, -24684.1527, -10538.3670,   8516.5903,  -1872.2571,
          53416.8401,  38645.6472]], dtype=torch.float64)
	q_value: tensor([[-30.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21742402663850546, distance: 1.262634429705047 entropy 11.261666175693284
epoch: 53, step: 122
	action: tensor([[-13965.3962, -76136.3324,  17293.5582,  11397.1015, -44756.3880,
          11294.1917,   5446.1137]], dtype=torch.float64)
	q_value: tensor([[-29.3566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02373813706119443, distance: 1.1306803778637853 entropy 11.427831825243453
epoch: 53, step: 123
	action: tensor([[ -8451.0885, -23022.3141,  -4684.2301,  32286.2736, -25683.1051,
          11202.4443,  12856.2022]], dtype=torch.float64)
	q_value: tensor([[-30.1214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38526984242818085, distance: 1.3468644285254945 entropy 11.4071479166994
epoch: 53, step: 124
	action: tensor([[ 56665.1143, -17441.7071,  -6613.3607,  14935.5780,  12765.3757,
         -16810.3609,  10889.9195]], dtype=torch.float64)
	q_value: tensor([[-32.3006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22851953068710373, distance: 1.0051228849940805 entropy 11.484634236207139
epoch: 53, step: 125
	action: tensor([[-14643.0558, -19085.9824, -18315.0414,  15462.4881,   8885.7196,
           1925.1909, -10464.3290]], dtype=torch.float64)
	q_value: tensor([[-35.9015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4684387945140217, distance: 0.8343213018242023 entropy 11.423891355654394
epoch: 53, step: 126
	action: tensor([[-10248.3534, -38603.5738,   5907.2486,  -6466.2519,   5575.3621,
         -17422.1614, -28266.1077]], dtype=torch.float64)
	q_value: tensor([[-27.5274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8732733791251392, distance: 1.56623625956862 entropy 11.388629765503465
epoch: 53, step: 127
	action: tensor([[ 10037.7822,    864.8865, -30087.8037,  -1900.5130,   9651.1283,
            130.8877,  33651.7866]], dtype=torch.float64)
	q_value: tensor([[-28.2023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11418830880476183, distance: 1.077028999998146 entropy 11.317218676874601
LOSS epoch 53 actor 386.79097334085156 critic 126.07038674131945
epoch: 54, step: 0
	action: tensor([[-20944.9129,  -7865.4515,  27005.7719,  21291.4955,     52.5000,
          10274.5091,   1229.1469]], dtype=torch.float64)
	q_value: tensor([[-28.7371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.009578202214962
epoch: 54, step: 1
	action: tensor([[-17754.5605,  -7190.6081,  -8159.4395,  22546.0057,  -4903.0773,
         -16856.8507,  -3382.2638]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29615816324680644, distance: 1.3028238279239854 entropy 10.895060426209918
epoch: 54, step: 2
	action: tensor([[  -998.8272, -27064.6023,  -2352.3531,    917.9572,  16145.5967,
         -20200.7486, -27132.1860]], dtype=torch.float64)
	q_value: tensor([[-32.7480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29296283586139227, distance: 1.3012169570971097 entropy 11.567434755804438
epoch: 54, step: 3
	action: tensor([[-32757.2176, -23589.6503, -69710.6864,  26721.0120, -58718.3808,
          -4752.3120,   8591.1124]], dtype=torch.float64)
	q_value: tensor([[-33.2490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07139146096903137, distance: 1.1027397500907146 entropy 11.582297099213898
epoch: 54, step: 4
	action: tensor([[-15970.6485, -31660.1470,  -1470.4896,   4700.6096,   4307.9763,
          20864.2809,  28879.5583]], dtype=torch.float64)
	q_value: tensor([[-36.7497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8263486511325091, distance: 1.546495061098593 entropy 11.616572906073289
epoch: 54, step: 5
	action: tensor([[  2803.2130,  15153.9802, -23088.9773,  66653.7521,  26405.6255,
         -17615.6766,   -445.4692]], dtype=torch.float64)
	q_value: tensor([[-31.3261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5850572204869743, distance: 0.7371413932565107 entropy 11.49064568027967
epoch: 54, step: 6
	action: tensor([[ 13537.9822,  -8490.6870, -18122.0142, -44837.6621, -33008.8793,
         -24257.1751,   5826.4576]], dtype=torch.float64)
	q_value: tensor([[-32.8282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.059886401551959345, distance: 1.1781113861152739 entropy 11.351275479594742
epoch: 54, step: 7
	action: tensor([[-15754.3614, -68735.8493,  18523.3050,  36492.8967,   9793.0526,
         -30148.8327,  24550.6248]], dtype=torch.float64)
	q_value: tensor([[-35.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3495146161735253, distance: 0.9229442416744253 entropy 11.388551564021027
epoch: 54, step: 8
	action: tensor([[ -3886.2289, -35555.0089,  17892.6916,  60131.5206,  20351.1510,
         -57903.0530, -19175.1062]], dtype=torch.float64)
	q_value: tensor([[-31.4031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8501866991853086, distance: 1.556554998899018 entropy 11.492456341132655
epoch: 54, step: 9
	action: tensor([[  3358.2335, -15811.0299, -17233.4869,  11289.2925,   -330.9282,
          -4401.4282,  18365.6452]], dtype=torch.float64)
	q_value: tensor([[-28.9275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3477630317499515, distance: 0.9241860281787018 entropy 11.36801836873255
epoch: 54, step: 10
	action: tensor([[-12399.3238, -20242.5004,  -1400.7704,   4741.7964,  -6567.7400,
         -42187.8564,  25371.1507]], dtype=torch.float64)
	q_value: tensor([[-33.4905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1487859443803723, distance: 1.0557864559689223 entropy 11.223113674909465
epoch: 54, step: 11
	action: tensor([[-24733.6168,   2037.2437, -25865.4687, -13649.1064, -34585.8695,
          24307.4802,  24580.5781]], dtype=torch.float64)
	q_value: tensor([[-29.8274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3192854140412278, distance: 0.9441461391281669 entropy 11.371431485162317
epoch: 54, step: 12
	action: tensor([[-26945.4644,  44713.7804,  -5683.2879,  14004.3523,   3307.1229,
          13369.3666,  10780.8453]], dtype=torch.float64)
	q_value: tensor([[-34.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17804303867699978, distance: 1.242044831392744 entropy 11.393510225097113
epoch: 54, step: 13
	action: tensor([[-47377.6980,   6060.3750,  -5329.7084,  18021.0833, -15869.4044,
         -13245.2601, -12273.2466]], dtype=torch.float64)
	q_value: tensor([[-32.7661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21169395834829352, distance: 1.25965949464024 entropy 11.431224365305823
epoch: 54, step: 14
	action: tensor([[  5785.6609, -21845.1334, -17399.2576,   8703.6613,   -864.4757,
          11518.6318,  20162.4705]], dtype=torch.float64)
	q_value: tensor([[-34.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35010531713885285, distance: 0.922525087075652 entropy 11.431807647279024
epoch: 54, step: 15
	action: tensor([[-48770.6619, -39124.0925,  21722.4215,  15575.7718,   3516.6788,
          -3402.8233,  24695.8158]], dtype=torch.float64)
	q_value: tensor([[-30.2837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1224781352611013, distance: 1.0719774913819182 entropy 11.34097669953989
epoch: 54, step: 16
	action: tensor([[ -7046.4980,  25036.7239, -54715.6561,  31480.5089,  -9675.2940,
          17360.6904,   4221.1981]], dtype=torch.float64)
	q_value: tensor([[-31.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42615598290021817, distance: 0.8668692990403155 entropy 11.497497055106379
epoch: 54, step: 17
	action: tensor([[-21092.1163,  -7342.7595,   8820.5352,  -1263.9909, -18682.1997,
           4787.2233,  -1787.4280]], dtype=torch.float64)
	q_value: tensor([[-31.2152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2997790208653155, distance: 1.3046422980003947 entropy 11.290047923266892
epoch: 54, step: 18
	action: tensor([[-15958.1174, -29309.1656,  31497.6819,   1653.9377, -24187.0550,
           7497.4776,  26726.1112]], dtype=torch.float64)
	q_value: tensor([[-30.5345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8913471455654722, distance: 1.5737738235161363 entropy 11.462486319119261
epoch: 54, step: 19
	action: tensor([[-11195.2865, -18220.5935,  14960.3127, -12191.0282,   2467.8593,
         -58700.1411,  21673.9277]], dtype=torch.float64)
	q_value: tensor([[-32.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11621985404801416, distance: 1.2090146538722655 entropy 11.610591262997957
epoch: 54, step: 20
	action: tensor([[-12870.6317, -17815.3453, -28225.4730,   5376.5215,  18762.0033,
           2339.7925,  10969.4587]], dtype=torch.float64)
	q_value: tensor([[-30.3121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09268768465762967, distance: 1.196202532208643 entropy 11.368350128240204
epoch: 54, step: 21
	action: tensor([[-10463.4898,   3855.7548,  44610.1807,  21186.3344, -28058.9803,
          -1389.7387,  36042.7719]], dtype=torch.float64)
	q_value: tensor([[-27.1754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1104740745051922, distance: 1.2058989171484544 entropy 11.364569163661432
epoch: 54, step: 22
	action: tensor([[-30572.1225,   3681.5115,  16249.6102,  41683.0217,  -1590.2634,
           5577.7526,  13213.6121]], dtype=torch.float64)
	q_value: tensor([[-31.7500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.372947302181586, distance: 0.9061679832561476 entropy 11.361651349853563
epoch: 54, step: 23
	action: tensor([[-18507.4744,  -3921.5403,   6590.3437,   9574.7395, -30745.5209,
         -21827.3098,   4685.0595]], dtype=torch.float64)
	q_value: tensor([[-29.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13310082749451047, distance: 1.2181225181790498 entropy 11.261054153806764
epoch: 54, step: 24
	action: tensor([[  6831.4902, -15514.9843,  31793.0994, -13041.5387,  -4506.9391,
          20186.3697,   9348.8711]], dtype=torch.float64)
	q_value: tensor([[-32.2993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07167973726958565, distance: 1.1846476655798905 entropy 11.506550533157638
epoch: 54, step: 25
	action: tensor([[  -486.6491, -38337.0763,    -68.0322,  20567.5867,   8350.3231,
         -11810.8233, -14089.2622]], dtype=torch.float64)
	q_value: tensor([[-26.7031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1945687122572859, distance: 1.250726239264834 entropy 11.14309469275813
epoch: 54, step: 26
	action: tensor([[14829.5381, -7156.6322, -9010.6636, 40988.7213,  8521.1018, 18052.2889,
         19558.1163]], dtype=torch.float64)
	q_value: tensor([[-28.7495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.300203683614297
epoch: 54, step: 27
	action: tensor([[-19414.7156,  -2650.4454, -15569.6119,  27312.3274,  -6147.0514,
           4186.8110,  -7519.4150]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6955160727994636, distance: 1.4900733547605232 entropy 10.895060426209918
epoch: 54, step: 28
	action: tensor([[-38466.4197, -11507.2662,  -1384.8245,  91445.2425, -24911.8647,
         -46050.7688,  -2072.2549]], dtype=torch.float64)
	q_value: tensor([[-29.2338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4736118909804772, distance: 1.3891471489528853 entropy 11.428803555356138
epoch: 54, step: 29
	action: tensor([[  4915.4891,   6794.9994,  -7789.8702,  28989.5543, -39280.3793,
          -1678.7739,  22888.2281]], dtype=torch.float64)
	q_value: tensor([[-29.6246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32929416160375513, distance: 0.9371794060759775 entropy 11.438296781759323
epoch: 54, step: 30
	action: tensor([[-30737.8911, -35875.5702,  11755.6734,  14641.6397,  18651.4697,
          12740.5518, -12621.4864]], dtype=torch.float64)
	q_value: tensor([[-33.6107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41435471487746134, distance: 1.360930240936478 entropy 11.25072993884935
epoch: 54, step: 31
	action: tensor([[ 10399.6651,  -2632.4628,   9535.2972, -15415.0527,  11520.0525,
          56023.7445,   7130.3005]], dtype=torch.float64)
	q_value: tensor([[-30.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25904639382289574, distance: 0.9850362231858312 entropy 11.446003425619525
epoch: 54, step: 32
	action: tensor([[  2855.5706, -40855.1409, -14878.3709,  70893.5687,  12856.2360,
         -16124.0974,  44568.6038]], dtype=torch.float64)
	q_value: tensor([[-32.0999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36082191115444573, distance: 0.914887371029495 entropy 11.339953064192475
epoch: 54, step: 33
	action: tensor([[ -7243.2986, -45923.7935, -18439.2415,   3430.2026,  16869.4078,
           1671.5474, -28019.0380]], dtype=torch.float64)
	q_value: tensor([[-32.2779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20046248221945318, distance: 1.0232367203486727 entropy 11.1858156060396
epoch: 54, step: 34
	action: tensor([[ -7532.1996, -41938.4671, -16482.2847,  16960.3230,  10559.3340,
          32599.1089,   8253.4830]], dtype=torch.float64)
	q_value: tensor([[-32.0308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21309549882847634, distance: 1.2603877946937807 entropy 11.483769470026402
epoch: 54, step: 35
	action: tensor([[-31054.2579,   3941.2861, -18266.8151,  46098.2714,   7012.9689,
          30825.6510,  60966.2975]], dtype=torch.float64)
	q_value: tensor([[-37.6442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4366215547282568, distance: 0.8589280915919485 entropy 11.682534708548374
epoch: 54, step: 36
	action: tensor([[  6820.7666,  -5172.7348,   7780.5878,  -5441.3024,  14542.6536,
         -11017.7054, -28042.8903]], dtype=torch.float64)
	q_value: tensor([[-32.2496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24689585140757542, distance: 1.2778261846612655 entropy 11.350985822180343
epoch: 54, step: 37
	action: tensor([[  9498.2030,  -3939.8008,  86213.1184, -15781.1032,  14287.5201,
           1010.1664,  24435.6409]], dtype=torch.float64)
	q_value: tensor([[-34.8726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5683276448015555, distance: 0.7518545143191189 entropy 11.417698680929195
epoch: 54, step: 38
	action: tensor([[-24347.5721,  -6514.2184,  -5499.8399,  18224.8392, -10177.5053,
           7763.1960,  33729.7400]], dtype=torch.float64)
	q_value: tensor([[-28.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7187701187589586, distance: 1.5002567542052854 entropy 11.120025215845626
epoch: 54, step: 39
	action: tensor([[-30963.3237, -27920.2468, -46002.6603,  19203.1359, -25382.1127,
          29141.7332,  31962.6298]], dtype=torch.float64)
	q_value: tensor([[-27.7023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30675120292809344, distance: 1.308136752745415 entropy 11.346279083318743
epoch: 54, step: 40
	action: tensor([[ 22048.6029, -15284.5335, -23656.7629,  -5327.1518,  -6861.0505,
         -19269.6717,  39845.3745]], dtype=torch.float64)
	q_value: tensor([[-32.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12763009989066187, distance: 1.0688260484645267 entropy 11.484178252354948
epoch: 54, step: 41
	action: tensor([[  -516.7655, -22107.0305, -24224.2762,  -7834.6681,   7506.6005,
          48713.1843,   7276.8963]], dtype=torch.float64)
	q_value: tensor([[-28.8182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39343668698215406, distance: 0.8912401906419072 entropy 11.293194122239951
epoch: 54, step: 42
	action: tensor([[-10839.3246,   2162.0341,  15199.0933,  32975.3752, -17301.9655,
           -321.5800,  42626.4546]], dtype=torch.float64)
	q_value: tensor([[-32.9046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7402041100761654, distance: 1.509582276555364 entropy 11.4206503679205
epoch: 54, step: 43
	action: tensor([[-35859.8484,   -339.6190,  20764.0861,  -1508.9439,  -6873.6570,
          -3940.9688, -22328.5047]], dtype=torch.float64)
	q_value: tensor([[-26.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08515720962112716, distance: 1.0945356921249654 entropy 11.10830210987214
epoch: 54, step: 44
	action: tensor([[ 15862.8812, -46116.6266, -11896.5841,   3457.8427, -21014.0134,
          20199.3928,  18092.7479]], dtype=torch.float64)
	q_value: tensor([[-37.5435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10059319938873357, distance: 1.0852624501914974 entropy 11.499025137598013
epoch: 54, step: 45
	action: tensor([[-18423.1117,   1921.7179,  -6857.9393,  11565.7506,  23673.4103,
          30945.7851, -14488.7842]], dtype=torch.float64)
	q_value: tensor([[-33.7993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31589191185977405, distance: 0.9464965947146493 entropy 11.504050297905303
epoch: 54, step: 46
	action: tensor([[-23357.6971,  -6226.6305,  -8002.3474,  13165.9630, -13074.8179,
          -1425.2840, -18093.9293]], dtype=torch.float64)
	q_value: tensor([[-35.4594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0732593400361976, distance: 1.1855204000507737 entropy 11.514139310968833
epoch: 54, step: 47
	action: tensor([[  5281.6509, -12226.0533, -19799.5776,  42005.1718,    752.2408,
         -19963.6784,  16313.9743]], dtype=torch.float64)
	q_value: tensor([[-29.7947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4181305432071699, distance: 0.8729100100667543 entropy 11.33807300116563
epoch: 54, step: 48
	action: tensor([[-58406.1320,  30433.7789,  29780.0269,   6780.2437,  11152.4850,
          18767.2690,   3888.8732]], dtype=torch.float64)
	q_value: tensor([[-34.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.038927597790799795, distance: 1.1218498816606592 entropy 11.511475028962922
epoch: 54, step: 49
	action: tensor([[-30794.7789, -38772.4749, -28069.1660,  25354.6857, -21425.5274,
          13587.3490,  20123.1419]], dtype=torch.float64)
	q_value: tensor([[-31.1371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07504560372620972, distance: 1.1005679304906886 entropy 11.317658153922366
epoch: 54, step: 50
	action: tensor([[-18104.5629, -23058.0295,  -6887.4999,  10736.0185,   7150.9222,
         -38967.6971,    107.2912]], dtype=torch.float64)
	q_value: tensor([[-29.6554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7284220955975294, distance: 1.50446330020118 entropy 11.471728137707354
epoch: 54, step: 51
	action: tensor([[-46288.7250,  19551.9884,  -5350.1783,  -2218.4350,   4317.4459,
          35058.4514, -16725.7877]], dtype=torch.float64)
	q_value: tensor([[-28.0489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11868841767160865, distance: 1.0742897501609143 entropy 11.315514166593028
epoch: 54, step: 52
	action: tensor([[-17669.5389,  -4015.0015,   7874.4364,  22157.0956,  17188.7520,
          10795.7716,  11616.3764]], dtype=torch.float64)
	q_value: tensor([[-38.6656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2381907920974229, distance: 1.273357874325041 entropy 11.260841433569535
epoch: 54, step: 53
	action: tensor([[ -3240.8417, -49930.1949,  28798.5857,  -2696.6865, -10973.1193,
          36057.9441, -17421.0431]], dtype=torch.float64)
	q_value: tensor([[-30.2951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6066767668155377, distance: 1.4505107443667222 entropy 11.412553906090952
epoch: 54, step: 54
	action: tensor([[ 20751.9992,   2309.3214,  21981.6635, -35432.5215,   4385.8651,
          17462.5777, -32360.6970]], dtype=torch.float64)
	q_value: tensor([[-30.8669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23456929464284926, distance: 1.0011741627837916 entropy 11.444701762581376
epoch: 54, step: 55
	action: tensor([[24785.1036, -9565.9202,  8257.4843,  3857.9322,  8105.8572,  1051.8726,
         15600.1744]], dtype=torch.float64)
	q_value: tensor([[-36.8757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42074214698235024, distance: 1.3639998588257458 entropy 11.427171105263202
epoch: 54, step: 56
	action: tensor([[-22155.1153, -46198.6871, -31799.2853, -16157.6154,  -5107.6696,
          23865.8111,   5578.7425]], dtype=torch.float64)
	q_value: tensor([[-33.0699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13726526448004206, distance: 1.2203589220073803 entropy 11.220729305622354
epoch: 54, step: 57
	action: tensor([[ -7362.7731, -13595.1018,  12202.4931,   2316.5820,  -7406.9393,
           7246.0125,  -1049.6588]], dtype=torch.float64)
	q_value: tensor([[-26.8342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22334315258027138, distance: 1.2657001774530954 entropy 11.119998954127277
epoch: 54, step: 58
	action: tensor([[ 13111.5046, -16886.6304, -22580.1224, -31016.6843,   2730.8849,
           1235.9627,    230.9423]], dtype=torch.float64)
	q_value: tensor([[-23.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.496029162806473, distance: 0.8123803244353809 entropy 11.1034454117586
epoch: 54, step: 59
	action: tensor([[-30770.7899, -47429.0861, -29642.1499, -10609.7686,  -7870.9663,
           3509.4137, -23675.3106]], dtype=torch.float64)
	q_value: tensor([[-31.8889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17558611641618116, distance: 1.2407489532873925 entropy 11.363833185549897
epoch: 54, step: 60
	action: tensor([[ 10307.2913, -10705.9494,  10623.8354,  11024.4887,  -8287.7650,
           7942.1230, -35021.0411]], dtype=torch.float64)
	q_value: tensor([[-30.7012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5417905142136066, distance: 0.7746200449502179 entropy 11.352033417405277
epoch: 54, step: 61
	action: tensor([[  4720.3158, -15211.0565,   1514.7038,   2122.9473,  15779.0268,
           6223.0808,  -9308.8582]], dtype=torch.float64)
	q_value: tensor([[-25.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19490807076805106, distance: 1.250903882647471 entropy 11.017449209121791
epoch: 54, step: 62
	action: tensor([[-21506.0446, -26323.9874,  14178.6461,  29510.7156, -10597.1888,
          -1604.2224, -14884.7771]], dtype=torch.float64)
	q_value: tensor([[-37.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.412227347822082
epoch: 54, step: 63
	action: tensor([[  487.5351, -9276.6571, -1865.7381, -7542.9521,  1746.3694, -9317.9038,
           125.9696]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29398693591788416, distance: 0.9615304938066364 entropy 10.895060426209918
epoch: 54, step: 64
	action: tensor([[-4514.6500, 24864.4910,  7011.8315, 32966.7511,  3068.9701, 23510.7782,
         -7914.9569]], dtype=torch.float64)
	q_value: tensor([[-30.2630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.126109859295841
epoch: 54, step: 65
	action: tensor([[-17659.6761, -16024.9727,  -8891.0227,   6815.1550,   3677.0498,
          -8830.8694,  -7508.0361]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5924955708948605, distance: 1.444095151419936 entropy 10.895060426209918
epoch: 54, step: 66
	action: tensor([[ 12959.8618,  -9939.0193,  10956.2162,  -9234.0470,  -8251.9372,
          20463.5591, -17377.5642]], dtype=torch.float64)
	q_value: tensor([[-25.3309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02452821873484612, distance: 1.1582935971387576 entropy 11.039276924428835
epoch: 54, step: 67
	action: tensor([[-85705.4817,   5655.4169, -25579.1071, -42851.4045,   7244.0003,
           3191.1416,  13535.4506]], dtype=torch.float64)
	q_value: tensor([[-30.7889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.35934622638129
epoch: 54, step: 68
	action: tensor([[  8675.3276, -16914.7900,  -7689.1209,  -9251.1141,   8346.8734,
           7124.0511, -19902.8235]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19650697736517653, distance: 1.0257646969483356 entropy 10.895060426209918
epoch: 54, step: 69
	action: tensor([[25157.8912,  3053.4813, 17987.5381, 26176.7824, -8124.7138, 20376.3423,
          5128.6198]], dtype=torch.float64)
	q_value: tensor([[-33.4669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8537460000206052, distance: 0.4376335106842146 entropy 11.3538515858107
epoch: 54, step: 70
	action: tensor([[-2.7597e+04, -1.9498e+04,  1.3693e+01,  1.2173e+04,  1.8284e+04,
         -1.0195e+04,  3.7075e+04]], dtype=torch.float64)
	q_value: tensor([[-29.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.09438053641147
epoch: 54, step: 71
	action: tensor([[  3737.1370, -10393.9557, -15608.3683,   3859.9034, -15488.6035,
          -7966.6287,   2152.7812]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39117316883010367, distance: 0.892901566939561 entropy 10.895060426209918
epoch: 54, step: 72
	action: tensor([[-33625.2498,  -8100.3653,   7399.7290, -14787.0438,  27824.2404,
         -23585.7049, -10007.3669]], dtype=torch.float64)
	q_value: tensor([[-33.3465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2943238114240263, distance: 1.3019016089125102 entropy 11.257133081363877
epoch: 54, step: 73
	action: tensor([[  3596.0272,  -5660.7851, -12168.8077,  16308.2215,  13146.0732,
          23645.0370,  38717.6780]], dtype=torch.float64)
	q_value: tensor([[-30.5131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14354719974735386, distance: 1.2237247411438599 entropy 11.426311538147417
epoch: 54, step: 74
	action: tensor([[-48294.3823,  14589.9290,  -2937.7284,   3391.9214, -19671.4368,
          11430.0532,   4417.5480]], dtype=torch.float64)
	q_value: tensor([[-37.7022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007265444591107784, distance: 1.1484938154237363 entropy 11.298742946459884
epoch: 54, step: 75
	action: tensor([[ -5077.7274,    171.4729, -11675.4462,   2110.6555, -12395.2250,
           1476.4577,  -1628.9887]], dtype=torch.float64)
	q_value: tensor([[-31.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4837059758473423, distance: 0.8222525754264084 entropy 11.32016164527457
epoch: 54, step: 76
	action: tensor([[-10707.6529, -24238.7982,  10264.8891,  17383.8998,  35013.7583,
          11357.0145, -76093.7133]], dtype=torch.float64)
	q_value: tensor([[-33.2251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4563206993293547, distance: 1.3809730536605003 entropy 11.390077580217035
epoch: 54, step: 77
	action: tensor([[-17450.1830,  -8126.0648,  10596.9348,  47362.8058, -26430.5556,
          59575.7475,  17920.3621]], dtype=torch.float64)
	q_value: tensor([[-34.5167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49235362391344206, distance: 1.3979529842744365 entropy 11.652685703440186
epoch: 54, step: 78
	action: tensor([[-35170.9965,    516.5938, -23107.5057,  21733.6314,   6852.0195,
          19410.6153,  -5809.8179]], dtype=torch.float64)
	q_value: tensor([[-33.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1239524836726259, distance: 1.0710765835069245 entropy 11.614540229602596
epoch: 54, step: 79
	action: tensor([[ 24777.5363,   1991.1811,  -9318.1324,  -8414.2937,  -9068.0454,
         -26026.3505,  32675.2583]], dtype=torch.float64)
	q_value: tensor([[-33.3234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.534809722620306
epoch: 54, step: 80
	action: tensor([[-16076.4076,  -2945.2307,   7969.2489,    577.2912, -12377.3234,
          -4962.4253,  -3537.2227]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4772253462171496, distance: 1.3908492754336077 entropy 10.895060426209918
epoch: 54, step: 81
	action: tensor([[  -783.4263,  -5338.8101, -19472.9754, -13200.4494,  12605.3250,
          -6685.7038,  16495.5899]], dtype=torch.float64)
	q_value: tensor([[-31.1219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04369948718556649, distance: 1.1190613273560313 entropy 11.358792587895392
epoch: 54, step: 82
	action: tensor([[-31588.4200, -40577.3161,   3041.6339,   -337.5697,   7757.0722,
          29237.7019,   1352.4220]], dtype=torch.float64)
	q_value: tensor([[-31.4737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013483874593397394, distance: 1.1520335177315766 entropy 11.426040200104117
epoch: 54, step: 83
	action: tensor([[ 12340.0583,    -84.6090, -32160.0631,  58230.4481,  15525.9837,
          15911.0979,  17261.1051]], dtype=torch.float64)
	q_value: tensor([[-32.1672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03149881474238403, distance: 1.1622272662208417 entropy 11.404081549361292
epoch: 54, step: 84
	action: tensor([[-27037.0038,   2092.2109,   1806.3713,  22457.3310, -18260.5474,
          -3965.8048,  20741.3261]], dtype=torch.float64)
	q_value: tensor([[-24.6403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21824516776783875, distance: 1.2630601754886188 entropy 11.136697752725553
epoch: 54, step: 85
	action: tensor([[ 10721.1873, -49796.7583,  22552.8195,  43982.8507,  42493.2696,
          45768.2329,  20542.3463]], dtype=torch.float64)
	q_value: tensor([[-38.5467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3768240094442322, distance: 0.9033624818212791 entropy 11.55042097690673
epoch: 54, step: 86
	action: tensor([[ 17386.7509,  -4325.9075,   8613.8196, -47536.7744,   5604.6031,
           2495.9116,  35976.8314]], dtype=torch.float64)
	q_value: tensor([[-33.8445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2860733257347643, distance: 1.2977455806710074 entropy 11.290113172182359
epoch: 54, step: 87
	action: tensor([[-18246.6886,  -8637.5083,  -3912.7327, -18639.6784, -14643.9366,
           6494.4484,  10747.4207]], dtype=torch.float64)
	q_value: tensor([[-26.8225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05104806758221192, distance: 1.1147533869275572 entropy 10.949991714204113
epoch: 54, step: 88
	action: tensor([[ -4836.6841, -16532.8909,  16631.1370,  47146.4027,  40873.0485,
          32935.9443, -32336.5776]], dtype=torch.float64)
	q_value: tensor([[-32.0768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47826275353173053, distance: 1.3913375638260947 entropy 11.383641198638639
epoch: 54, step: 89
	action: tensor([[-30853.3927, -52110.5241,  -7602.7012, -26857.4114,  20337.0236,
          -1503.7251,  19793.9488]], dtype=torch.float64)
	q_value: tensor([[-26.6113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30201020627385866, distance: 0.956051372710906 entropy 11.376121243802237
epoch: 54, step: 90
	action: tensor([[-50886.4134,  -4645.7739,  -8413.1822, -21007.9411,   6732.1733,
         -25605.0585,  36400.1826]], dtype=torch.float64)
	q_value: tensor([[-30.1647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3771264066820048, distance: 1.3428997602705424 entropy 11.353008344946616
epoch: 54, step: 91
	action: tensor([[-6993.6346,  6864.1008, -6706.1588, -6665.6996, 39801.9499, 15891.9690,
          2779.5014]], dtype=torch.float64)
	q_value: tensor([[-33.8700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1904095209261325, distance: 1.029649444062938 entropy 11.503274610746486
epoch: 54, step: 92
	action: tensor([[ 15792.8800,   6933.9795,  -5900.6410,  13361.1926,  20300.5266,
         -23559.5859, -37270.3807]], dtype=torch.float64)
	q_value: tensor([[-47.7959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.515031700072651
epoch: 54, step: 93
	action: tensor([[ -4169.1113, -19422.1711,    573.2099,  31127.2982,   1147.1535,
           1042.0208, -24987.0752]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9149536076011393, distance: 1.5835647352682367 entropy 10.895060426209918
epoch: 54, step: 94
	action: tensor([[  1395.6981, -35347.5094, -40329.7202,  35442.7943, -20335.4903,
          20731.4126,  49249.3182]], dtype=torch.float64)
	q_value: tensor([[-27.9568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12281348391704527, distance: 1.0717726414649316 entropy 11.354516123142377
epoch: 54, step: 95
	action: tensor([[  2325.8593,  22711.3175,  15493.0654,  -4485.6861,  25231.5358,
         -67680.6134, -28133.7876]], dtype=torch.float64)
	q_value: tensor([[-34.5839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36672273857540727, distance: 0.9106545052263411 entropy 11.507067833572986
epoch: 54, step: 96
	action: tensor([[-14059.7197,  -3674.9203,  14520.4143,  -1425.7597,  34666.4703,
          -6261.9692,  11416.9678]], dtype=torch.float64)
	q_value: tensor([[-33.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5169634745157734, distance: 1.4094324143507648 entropy 11.236918321990379
epoch: 54, step: 97
	action: tensor([[-25960.3442,  -9673.5639,  10250.1659,  25809.8579, -14656.8260,
          17049.1607,  -4121.3779]], dtype=torch.float64)
	q_value: tensor([[-30.3797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3882509274169763, distance: 1.3483128681746983 entropy 11.286578022982889
epoch: 54, step: 98
	action: tensor([[-31997.9923,  -6351.4551,   6609.5725, -12580.9701,  -5246.2168,
         -13130.4354,   4287.4744]], dtype=torch.float64)
	q_value: tensor([[-25.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2471717533396598, distance: 0.9928980234550544 entropy 11.214189222944999
epoch: 54, step: 99
	action: tensor([[ 26447.1573,  13580.8940, -14238.3169,  21789.2332,  23767.3669,
          17737.5512, -23647.2649]], dtype=torch.float64)
	q_value: tensor([[-28.4447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.27543966473419
epoch: 54, step: 100
	action: tensor([[  8422.2738, -16221.1521, -16552.9472,  12951.6568,  -2912.8880,
          -9557.2530,  -3867.2720]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2573897369367787, distance: 0.9861368021537711 entropy 10.895060426209918
epoch: 54, step: 101
	action: tensor([[-21066.3833, -60572.3459, -12096.0360,   2587.2654, -29249.3350,
           3802.5425, -12231.6481]], dtype=torch.float64)
	q_value: tensor([[-34.9975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18143375977482634, distance: 1.2438310146760743 entropy 11.5130415782378
epoch: 54, step: 102
	action: tensor([[-23524.0628, -12137.0370, -35184.0766,  29386.6330,    708.3304,
          17774.9881,  15999.1417]], dtype=torch.float64)
	q_value: tensor([[-31.5579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5600281830504727, distance: 1.4292984258609347 entropy 11.54061471782398
epoch: 54, step: 103
	action: tensor([[-10906.7537, -30571.4407,   7886.4891,  19328.6282, -53306.8129,
          13962.0053,   1547.2740]], dtype=torch.float64)
	q_value: tensor([[-30.5132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003094689610694701, distance: 1.1461135813146701 entropy 11.450363133446414
epoch: 54, step: 104
	action: tensor([[-13778.7098,    199.4755,  -5091.5287,  35625.8638,  -2221.6757,
         -11474.4754, -26314.0936]], dtype=torch.float64)
	q_value: tensor([[-28.7915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05841537851002798, distance: 1.1104176893733517 entropy 11.345961864258197
epoch: 54, step: 105
	action: tensor([[-48166.1817,   3581.1471,   7436.2830,  -3206.2690,  -2993.6617,
          -8567.9354,  21913.7403]], dtype=torch.float64)
	q_value: tensor([[-33.4849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060882445261407514, distance: 1.108962020231551 entropy 11.482945607732855
epoch: 54, step: 106
	action: tensor([[ -7882.7349,   1041.9613,  12425.4602,  61612.3558, -21328.3430,
          30913.5300,   4485.9622]], dtype=torch.float64)
	q_value: tensor([[-45.8163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15238464690926112, distance: 1.2284441742411165 entropy 11.55317856429734
epoch: 54, step: 107
	action: tensor([[ -5801.3961, -10448.8752,   5099.5171,  -1231.5557,  16750.2171,
           4911.3419,   5745.6796]], dtype=torch.float64)
	q_value: tensor([[-35.0340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5252631832110399, distance: 1.4132828437566751 entropy 11.411233669615452
epoch: 54, step: 108
	action: tensor([[-29551.0035,  -9774.0385,  -9349.0450, -22583.1449,   4913.8959,
           9838.1302,   1358.2434]], dtype=torch.float64)
	q_value: tensor([[-33.2237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28218450987081156, distance: 1.2957820400567623 entropy 11.436127956921641
epoch: 54, step: 109
	action: tensor([[-15720.8594,  17791.0898,   2829.2110, -26263.4136, -26986.8012,
           9168.6377,   2682.7697]], dtype=torch.float64)
	q_value: tensor([[-30.1565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11876256191138446, distance: 1.0742445595190517 entropy 11.34378489661508
epoch: 54, step: 110
	action: tensor([[-21283.3931,   2014.5728,  22266.1587,  10403.5725, -15231.5517,
          27350.9822, -11987.9033]], dtype=torch.float64)
	q_value: tensor([[-36.9347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2603698384028935, distance: 0.9841561252414279 entropy 11.16848960164425
epoch: 54, step: 111
	action: tensor([[34924.2199, 16423.3030, -5989.7119, 21000.3030,  2426.6927, 24875.6383,
          3244.5536]], dtype=torch.float64)
	q_value: tensor([[-30.2014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2499307442873253, distance: 0.9910769499636157 entropy 11.077486868698404
epoch: 54, step: 112
	action: tensor([[-37502.9520, -24912.8590,   -655.7979,   1508.7837,  23390.3645,
          -1283.2366,   6283.9215]], dtype=torch.float64)
	q_value: tensor([[-31.7971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25435382900141423, distance: 1.2816419769786052 entropy 11.36526478492332
epoch: 54, step: 113
	action: tensor([[-21814.1570, -12680.1095, -16122.7957, -16110.2498, -13491.4964,
           2494.5226, -11225.4649]], dtype=torch.float64)
	q_value: tensor([[-33.2367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5415000270189305, distance: 1.4207853213628594 entropy 11.583956334722425
epoch: 54, step: 114
	action: tensor([[ -5381.0735,  -6310.1004, -16241.1307,  28836.6701, -20871.9703,
         -15165.3873, -24422.1703]], dtype=torch.float64)
	q_value: tensor([[-27.7042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7691100984783898, distance: 1.5220682404695913 entropy 11.280909907730974
epoch: 54, step: 115
	action: tensor([[  3530.7144, -34155.2704,  35350.0242,  24394.9315, -11802.7958,
         -17837.6016,   8651.8505]], dtype=torch.float64)
	q_value: tensor([[-27.0054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.344184569902903, distance: 0.9267178079304522 entropy 11.241999396216107
epoch: 54, step: 116
	action: tensor([[ -3518.0345,  -8909.9909,    -75.3127, -11843.4460,   8355.4779,
         -47671.5543, -28068.2287]], dtype=torch.float64)
	q_value: tensor([[-34.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20575514819269847, distance: 1.2565687523423303 entropy 11.454298593845285
epoch: 54, step: 117
	action: tensor([[-13813.8835,   2317.8753,   7216.7138,  29695.8230,  -2030.8081,
         -22459.4601,  -9912.9819]], dtype=torch.float64)
	q_value: tensor([[-27.2017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5187240913937641, distance: 0.7938779774300877 entropy 11.18077097185697
epoch: 54, step: 118
	action: tensor([[-20365.2189,  15326.0702,  -8824.9573,   1082.8810,  -4642.5962,
         -13513.1984,  42948.2318]], dtype=torch.float64)
	q_value: tensor([[-35.7534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11911550551421457, distance: 1.21058182610979 entropy 11.519996174409414
epoch: 54, step: 119
	action: tensor([[  4396.1341,  33957.4686, -34426.6818, -25728.1324, -20069.6032,
            187.1731,   5552.6375]], dtype=torch.float64)
	q_value: tensor([[-37.8833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7647592832928263, distance: 0.5550255044509596 entropy 11.550805292058993
epoch: 54, step: 120
	action: tensor([[-11087.9370,   5025.7938,  11294.9986, -26587.4603,  -7210.7926,
           8406.6770, -19599.0679]], dtype=torch.float64)
	q_value: tensor([[-29.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.117250075491613
epoch: 54, step: 121
	action: tensor([[  5284.8671,  -2952.5325, -34114.8425,  12211.9590,   2850.4581,
           4268.6507,   4132.0431]], dtype=torch.float64)
	q_value: tensor([[-33.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27119626343403835, distance: 1.2902177046591095 entropy 10.895060426209918
epoch: 54, step: 122
	action: tensor([[-31427.2569,  12850.8444,   7047.1103,  34270.9262, -18698.4020,
          35769.0822,  40460.5550]], dtype=torch.float64)
	q_value: tensor([[-40.5397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2805647665890456, distance: 0.9706273952055091 entropy 11.453478601016272
epoch: 54, step: 123
	action: tensor([[-26727.4643,   4891.5317,  23694.8145,  12806.5849, -43742.9067,
         -30313.8193,  -6912.8506]], dtype=torch.float64)
	q_value: tensor([[-33.2587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4297008223679586, distance: 0.8641876706300046 entropy 11.45704844749686
epoch: 54, step: 124
	action: tensor([[-15890.8523, -23443.7184, -14368.8754,  12508.4441,  42844.8882,
          18180.3922, -24622.5657]], dtype=torch.float64)
	q_value: tensor([[-31.6674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3199570310784654, distance: 0.9436802602337867 entropy 11.320766136298468
epoch: 54, step: 125
	action: tensor([[-13815.0684,  15123.3941, -10023.2582,  -1186.3305, -11061.7190,
         -25602.9123,  14298.0829]], dtype=torch.float64)
	q_value: tensor([[-26.0135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09072969785915541, distance: 1.0911970837206575 entropy 11.258662395572344
epoch: 54, step: 126
	action: tensor([[-38721.4698,  30441.4963,  -3623.6915,  65653.6111, -11481.3319,
          29869.5751,  25049.0446]], dtype=torch.float64)
	q_value: tensor([[-39.4267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6275695940613435, distance: 0.698359842679047 entropy 11.412154181190107
epoch: 54, step: 127
	action: tensor([[-13883.7594, -69354.3746,   4627.8473,  38178.5063, -15245.7411,
          29274.0004,  -7623.8058]], dtype=torch.float64)
	q_value: tensor([[-32.8377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31503541757174847, distance: 1.3122767009623932 entropy 11.451973359064995
LOSS epoch 54 actor 433.7723091066878 critic 124.40123769171917
epoch: 55, step: 0
	action: tensor([[-25116.3495, -19846.5437, -42197.0795,  -8169.3569,  15323.4265,
          47637.3112, -22534.6535]], dtype=torch.float64)
	q_value: tensor([[-29.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8647791227883679, distance: 1.5626812183525793 entropy 11.752121619326184
epoch: 55, step: 1
	action: tensor([[ 33248.5474, -17133.4677,   8753.1413,   8966.2044, -31511.8757,
         -11898.3090, -12420.2292]], dtype=torch.float64)
	q_value: tensor([[-26.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21404666147442908, distance: 1.0145070626747692 entropy 11.473006819426931
epoch: 55, step: 2
	action: tensor([[  5350.8734, -32554.7237, -32147.7884,  10258.7149, -18174.3442,
           -226.4522,  16624.7255]], dtype=torch.float64)
	q_value: tensor([[-26.9406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4036543495320839, distance: 0.8837017627092644 entropy 11.294140765146082
epoch: 55, step: 3
	action: tensor([[ -1474.2031,  25473.8940, -21382.0269,   2616.2959,   -139.0926,
          26240.2589,   2718.4507]], dtype=torch.float64)
	q_value: tensor([[-38.6915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2581159889889304, distance: 0.9856544771042002 entropy 11.464383438541146
epoch: 55, step: 4
	action: tensor([[ 15398.5995, -27046.2527,  23771.0294,  22712.0000,   4756.6343,
          -3346.5047, -14193.7726]], dtype=torch.float64)
	q_value: tensor([[-23.9479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23538586029354414, distance: 1.2719147578475984 entropy 11.206508259856973
epoch: 55, step: 5
	action: tensor([[ 20331.1141, -45793.2908,   9375.4165,  -4550.7728, -25638.4787,
          18333.1001,  28117.8874]], dtype=torch.float64)
	q_value: tensor([[-33.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.055880585487776635, distance: 1.1119113348608947 entropy 11.267145566091232
epoch: 55, step: 6
	action: tensor([[-20644.9416,  11995.7412, -16857.5848,  37302.0834,  33276.0548,
          -4142.3565, -32864.1285]], dtype=torch.float64)
	q_value: tensor([[-30.0324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.29049519512553
epoch: 55, step: 7
	action: tensor([[  9442.0880, -12083.4418,  -9442.7084,  17759.6627, -15299.3852,
         -35672.3563, -10864.1815]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93517352713569
epoch: 55, step: 8
	action: tensor([[ -2633.2277, -34269.5385,   7350.5923,  27070.9531, -18306.2958,
         -13994.0444,  12962.1711]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93517352713569
epoch: 55, step: 9
	action: tensor([[ 5110.6465,  3230.9165,  2570.8192, 19891.3668, 21783.0410,  6097.2742,
         -9998.8826]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93517352713569
epoch: 55, step: 10
	action: tensor([[ -2832.3741, -22263.8162, -10034.0385, -16765.7175,   1225.0980,
           1955.3048,   -843.9323]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5084498336645226, distance: 0.8023070657679136 entropy 10.93517352713569
epoch: 55, step: 11
	action: tensor([[ 9787.0148, 10523.3828, -4189.0782,  2280.4016, 10011.0546,  5569.5876,
           694.8725]], dtype=torch.float64)
	q_value: tensor([[-17.4054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.848612027234294
epoch: 55, step: 12
	action: tensor([[-10340.9896, -12164.7672,    373.6220,  17356.1792,  24422.1763,
         -24284.8965,  11495.0152]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5005275074731002, distance: 1.4017761736703829 entropy 10.93517352713569
epoch: 55, step: 13
	action: tensor([[-15673.1433, -35048.0118,  28312.8994, -12776.4493,   2049.9305,
         -28051.0351,   7822.1020]], dtype=torch.float64)
	q_value: tensor([[-25.7882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49160159143411764, distance: 1.3976007090127942 entropy 11.4116117318038
epoch: 55, step: 14
	action: tensor([[-26068.1888,    380.0761, -30828.3682,  14277.5322, -18201.5737,
          13021.8799,  19248.5002]], dtype=torch.float64)
	q_value: tensor([[-21.6963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3456531087206951, distance: 1.3274655303553906 entropy 11.124607192490206
epoch: 55, step: 15
	action: tensor([[-8758.8518, 13179.0563, -9026.1199,  8792.8826, 10518.8655, -8260.5703,
         -5824.5997]], dtype=torch.float64)
	q_value: tensor([[-23.0706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018269568661531732, distance: 1.1547502786850143 entropy 11.149226384559897
epoch: 55, step: 16
	action: tensor([[ -4834.2225,   -271.5333,   6324.7747,  37960.8887,  24083.7050,
          24652.3196, -37018.3566]], dtype=torch.float64)
	q_value: tensor([[-28.4884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.127233045579355, distance: 1.0690692559116983 entropy 11.399288169164674
epoch: 55, step: 17
	action: tensor([[-30127.6536,   6390.7215, -27223.9456,  16995.8500,   5349.5573,
          29836.9774,  48576.1896]], dtype=torch.float64)
	q_value: tensor([[-26.0725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16348886606535218, distance: 1.2343485440415902 entropy 11.46602755977716
epoch: 55, step: 18
	action: tensor([[-11350.9802,  -7506.5778, -18974.9665,  24750.2435,  21160.5802,
          27944.6089,   7967.4094]], dtype=torch.float64)
	q_value: tensor([[-29.2380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46548626681375294, distance: 1.3853119155764186 entropy 11.460830417951266
epoch: 55, step: 19
	action: tensor([[-34128.0454, -20545.0037,   6662.3598,  19913.2458, -22852.3771,
           5420.9386,  13990.3658]], dtype=torch.float64)
	q_value: tensor([[-25.1563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14295243538335556, distance: 1.2234064672112916 entropy 11.359291135483776
epoch: 55, step: 20
	action: tensor([[  3325.4104, -26422.2043, -28286.7821,    978.7886,  35617.4419,
          33696.5801,    225.1882]], dtype=torch.float64)
	q_value: tensor([[-26.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23890587488155202, distance: 0.9983340369413043 entropy 11.47634468716162
epoch: 55, step: 21
	action: tensor([[ 17509.7539,  -3371.4378, -24900.8549,  -2535.3666, -18342.9582,
         -14853.9647, -47197.9705]], dtype=torch.float64)
	q_value: tensor([[-27.8622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1066319806995456, distance: 1.0816129890664548 entropy 11.3388968336783
epoch: 55, step: 22
	action: tensor([[-23653.0686, -32481.4833,  -6798.4502,   7483.4606,  -3721.3967,
          32878.8878,  14424.7396]], dtype=torch.float64)
	q_value: tensor([[-27.1359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6197429625107618, distance: 1.456396894184891 entropy 11.30662826267278
epoch: 55, step: 23
	action: tensor([[-21898.9550,   1934.7598, -13312.8723,  19255.8933,  11689.8993,
          11136.0017, -21477.9880]], dtype=torch.float64)
	q_value: tensor([[-24.9381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3136326401101769, distance: 0.9480582122787455 entropy 11.35088268238246
epoch: 55, step: 24
	action: tensor([[-42760.6219,  16148.7736,  18139.1427,   9601.3099, -40414.1055,
          65272.1074,  -2371.5944]], dtype=torch.float64)
	q_value: tensor([[-29.6486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018328239511284594, distance: 1.1338088491143732 entropy 11.57281918262044
epoch: 55, step: 25
	action: tensor([[-17097.2289, -68105.3234,  20333.6559,  40903.8205, -11012.9815,
           -360.3518,  26361.1714]], dtype=torch.float64)
	q_value: tensor([[-28.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.669565536743036, distance: 1.4786263080014488 entropy 11.489540904204189
epoch: 55, step: 26
	action: tensor([[  2383.2199,  -9839.5682, -30285.6746,  26112.5128,   8251.8170,
           1787.7502,   3526.2318]], dtype=torch.float64)
	q_value: tensor([[-28.1611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.51483229956929
epoch: 55, step: 27
	action: tensor([[-12340.5470,  -2131.5716,  22630.9745,   -306.8974,  -3562.0194,
          -7009.3309,    114.5403]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11130213822457702, distance: 1.0787821725542335 entropy 10.93517352713569
epoch: 55, step: 28
	action: tensor([[ 26408.1405, -41531.2267,   -991.7102,  44460.6464,  19288.2076,
         -20344.0490,   3959.0633]], dtype=torch.float64)
	q_value: tensor([[-28.7577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1433780056819749, distance: 1.2236342094094006 entropy 11.394232161717051
epoch: 55, step: 29
	action: tensor([[-37500.1961,   -421.0991, -28068.3981,   3829.6841,  20812.7034,
           8873.8192,  -8674.3599]], dtype=torch.float64)
	q_value: tensor([[-24.4699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.059012704408763
epoch: 55, step: 30
	action: tensor([[ 16635.3554, -34613.8861, -29727.3243,   -192.8462, -12258.6023,
          24371.6870,   9506.2920]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4497794253085068, distance: 1.3778681435860058 entropy 10.93517352713569
epoch: 55, step: 31
	action: tensor([[19427.3404, 10478.5265, -8232.9586,  5586.9914, 31782.7202,  5605.0511,
         -3069.9023]], dtype=torch.float64)
	q_value: tensor([[-25.2678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44748199394635324, distance: 0.8506088779215688 entropy 11.184290950109448
epoch: 55, step: 32
	action: tensor([[-16995.2478, -15570.2729,  14590.2111,  55552.8270, -21926.4698,
          60591.9464,  51823.7837]], dtype=torch.float64)
	q_value: tensor([[-30.9928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.406184331444807
epoch: 55, step: 33
	action: tensor([[ 19353.8417, -22483.9989,   1626.9734, -12039.4242,  15412.4555,
           -214.8397,  12717.0834]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32329963529728045, distance: 0.9413581751231084 entropy 10.93517352713569
epoch: 55, step: 34
	action: tensor([[22826.2428, -9137.1555,  3789.8094, 17363.3110,  3128.7736, -6701.8977,
          6148.1257]], dtype=torch.float64)
	q_value: tensor([[-22.5281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2561900455270538, distance: 1.2825797153954497 entropy 11.022648592435099
epoch: 55, step: 35
	action: tensor([[  9544.4848, -41249.1205, -18398.5947,  -6882.1846, -15906.4570,
          -8428.1091, -10495.1603]], dtype=torch.float64)
	q_value: tensor([[-30.8472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17083384967721837, distance: 1.2382385710434827 entropy 11.26961296677433
epoch: 55, step: 36
	action: tensor([[-21603.0306, -35489.3234,  -2968.1806,  21295.7019, -33159.6773,
          14373.6493,   6903.4032]], dtype=torch.float64)
	q_value: tensor([[-31.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6532114978374406, distance: 1.4713666286838298 entropy 11.384230380416497
epoch: 55, step: 37
	action: tensor([[-55373.6295,  10869.5491,   2891.9152,  25691.0724,  -9255.8500,
         -27871.8938,  39855.9959]], dtype=torch.float64)
	q_value: tensor([[-27.9536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05353213263248113, distance: 1.1745745493125828 entropy 11.571498903881864
epoch: 55, step: 38
	action: tensor([[-25308.2104,  10249.4504, -66606.4693,  27263.2288,  12253.3794,
          24070.8609, -17548.6557]], dtype=torch.float64)
	q_value: tensor([[-30.6042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06856880606700266, distance: 1.1829269855752274 entropy 11.531284949198701
epoch: 55, step: 39
	action: tensor([[ 11971.3124,  15155.3143,  26809.9168, -11374.7080, -15332.9066,
          -9697.0437, -14493.3195]], dtype=torch.float64)
	q_value: tensor([[-26.8354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.331755639050995
epoch: 55, step: 40
	action: tensor([[-10658.0105,  -3246.4789,  16119.0924,  -6145.8367,   6299.3716,
          16116.4954,  -6124.4929]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05050860938146351, distance: 1.1728878873624173 entropy 10.93517352713569
epoch: 55, step: 41
	action: tensor([[ -2645.2884,   2876.6592, -10164.0727,  -8162.7737, -11857.7426,
          12609.8212,  -5597.8515]], dtype=torch.float64)
	q_value: tensor([[-30.1815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6444324314946315, distance: 0.6823666048321291 entropy 11.509792124075267
epoch: 55, step: 42
	action: tensor([[-22611.7026, -17597.7804, -38755.2921,  22516.9723, -36423.6814,
           9379.4357,   8034.6234]], dtype=torch.float64)
	q_value: tensor([[-32.7834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20336471185855776, distance: 1.2553225466623934 entropy 11.342402526319274
epoch: 55, step: 43
	action: tensor([[-7050.6991, 24074.8471,  1422.7929, 34865.3245, 12952.7883,  7930.6603,
         24597.4213]], dtype=torch.float64)
	q_value: tensor([[-22.1159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49558827889428037, distance: 0.8127355901480472 entropy 11.209237847264061
epoch: 55, step: 44
	action: tensor([[-33071.4334,  70158.7725,  16345.1255, -64637.0082,  -1303.0985,
         -25682.8023,  49408.4961]], dtype=torch.float64)
	q_value: tensor([[-29.4804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029711979120447207, distance: 1.1272156975048926 entropy 11.535575921689496
epoch: 55, step: 45
	action: tensor([[ 36879.2636,  -9422.5068,  14431.8816, -10782.5192, -16800.8555,
          24255.3075, -38302.6725]], dtype=torch.float64)
	q_value: tensor([[-32.0772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3045884650352322, distance: 1.3070537889470126 entropy 11.491934527588926
epoch: 55, step: 46
	action: tensor([[ 10969.9290, -29935.4731,  -6914.7159,   9559.2161,    539.4106,
          -9421.9579, -26418.7804]], dtype=torch.float64)
	q_value: tensor([[-25.1849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2580984791296028, distance: 1.2835536083518486 entropy 11.146987817784549
epoch: 55, step: 47
	action: tensor([[-16312.8404,   6028.0692, -25479.0027,  27950.9745,  -1397.7146,
          -3690.1471,   3083.6923]], dtype=torch.float64)
	q_value: tensor([[-31.6337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38653933679299435, distance: 0.8962931001907862 entropy 11.418293836582523
epoch: 55, step: 48
	action: tensor([[-18740.9738,  -5173.9452,   5517.3025,  -8610.3208,  30505.2036,
         -54675.5449,  11808.9716]], dtype=torch.float64)
	q_value: tensor([[-30.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6116756398452265, distance: 1.4527654878152922 entropy 11.508685528414615
epoch: 55, step: 49
	action: tensor([[-13637.7491,  -3547.1359,  13522.9161, -26511.6992,   8482.9505,
           6607.3997,   8235.9335]], dtype=torch.float64)
	q_value: tensor([[-26.8467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5871175795852084, distance: 1.4416546739392142 entropy 11.41489551984114
epoch: 55, step: 50
	action: tensor([[-13108.8009, -13874.9565,  -6017.8913, -20802.7951,   6580.0662,
         -14071.0166,   5410.5543]], dtype=torch.float64)
	q_value: tensor([[-26.9865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5156916831514533, distance: 1.4088414707121784 entropy 11.307601579845686
epoch: 55, step: 51
	action: tensor([[-30776.3557,  13199.3386, -16969.5073,   1914.9314,  15788.7161,
         -43533.0078, -22550.5876]], dtype=torch.float64)
	q_value: tensor([[-26.8985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00845611482095121, distance: 1.139495628902924 entropy 11.336669999022714
epoch: 55, step: 52
	action: tensor([[ -1298.9401, -14065.9473,   7735.3520,  -7900.2179, -14242.6110,
           2144.1518,  17975.3851]], dtype=torch.float64)
	q_value: tensor([[-26.3976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0010632128711550726, distance: 1.1437357514451059 entropy 11.126575732108607
epoch: 55, step: 53
	action: tensor([[-16320.7301, -35377.3742, -10516.1577, -13761.7518,  -6644.4471,
          10572.6648,   -575.8064]], dtype=torch.float64)
	q_value: tensor([[-26.1894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1234741412781225, distance: 1.071368960010258 entropy 11.28449940298591
epoch: 55, step: 54
	action: tensor([[ -7912.4698,  13936.6391, -21946.5376,  11663.0722,  36823.4261,
         -26333.5473, -50803.3283]], dtype=torch.float64)
	q_value: tensor([[-30.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5993386661344431, distance: 0.7243449047816095 entropy 11.547262950750522
epoch: 55, step: 55
	action: tensor([[-29008.5894,   6647.1019, -44145.1688,  26383.0210,  -6172.6169,
          19462.0150,   9405.9388]], dtype=torch.float64)
	q_value: tensor([[-31.4597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.625924231268005, distance: 0.6999007869388272 entropy 11.435548843492086
epoch: 55, step: 56
	action: tensor([[-46807.4825, -61494.8375,  -3404.2912,  28283.5870,  21463.1703,
          14080.1032,  -6556.6638]], dtype=torch.float64)
	q_value: tensor([[-27.6727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02991009254790833, distance: 1.1271006141683606 entropy 11.426349619064236
epoch: 55, step: 57
	action: tensor([[   224.2438, -16529.9151,  22028.8160,  12318.7817,  18705.7226,
          32193.5734,  10089.3321]], dtype=torch.float64)
	q_value: tensor([[-27.2439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4181806779833993, distance: 0.8728724036154893 entropy 11.488085389309807
epoch: 55, step: 58
	action: tensor([[-28746.4314,  41337.8932, -13437.9866,   2239.0214,  45817.9037,
         -13008.5125,  19028.0881]], dtype=torch.float64)
	q_value: tensor([[-25.5617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6837344607509617, distance: 0.6435504967012109 entropy 11.31782443435458
epoch: 55, step: 59
	action: tensor([[-46526.7914,   1737.6502,  41784.2606,  31361.0524,  29461.0691,
          12631.0362,  15966.7079]], dtype=torch.float64)
	q_value: tensor([[-30.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.466247216607924
epoch: 55, step: 60
	action: tensor([[-29249.7955,   2993.3895,   5496.9864,    511.0534, -13426.7094,
         -27504.9434,   8330.0053]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93517352713569
epoch: 55, step: 61
	action: tensor([[-3136.7466, -2923.5905,  1851.5327, 17546.4738, -7565.6945, 13961.3061,
          1567.0457]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93517352713569
epoch: 55, step: 62
	action: tensor([[  3840.9288, -14594.8331,   4561.3851,  23830.7354,  -2048.4837,
         -10253.2854, -12444.3338]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93517352713569
epoch: 55, step: 63
	action: tensor([[-16304.6926,   3285.9896,   2866.8537,  13911.0470,   2772.7168,
           5099.7560, -10679.8462]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93517352713569
epoch: 55, step: 64
	action: tensor([[-12277.2095, -52831.9909,   6669.8389,  -8929.9321,  -7902.9368,
            627.7719,  21325.0475]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35675228203360776, distance: 1.3329288674122048 entropy 10.93517352713569
epoch: 55, step: 65
	action: tensor([[-25087.8753, -30920.0755, -26693.9239,    619.3267,  49331.4542,
          17130.8625, -20498.5183]], dtype=torch.float64)
	q_value: tensor([[-28.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9289350368994349, distance: 1.589335170696572 entropy 11.457400811747538
epoch: 55, step: 66
	action: tensor([[-36530.3899, -34667.7383,   3628.6353, -11274.8394, -14849.6874,
         -22218.5462,  -6949.8078]], dtype=torch.float64)
	q_value: tensor([[-28.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.843567519404282, distance: 1.5537681588079213 entropy 11.591500022269193
epoch: 55, step: 67
	action: tensor([[ -4563.1345,  30999.4004, -12248.9267, -13715.2987,  -7499.9807,
           3326.1653,  -3685.6548]], dtype=torch.float64)
	q_value: tensor([[-34.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11150577706274245, distance: 1.0786585678153309 entropy 11.659118618379742
epoch: 55, step: 68
	action: tensor([[ 13408.3045, -40202.2435, -11340.5592,  54956.5827, -20411.7967,
         -34530.5202,   5132.0163]], dtype=torch.float64)
	q_value: tensor([[-42.3329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.564012628388172
epoch: 55, step: 69
	action: tensor([[ -1915.2815, -19446.2888, -20884.6625,   1622.6616, -21907.5134,
          13635.0114, -15214.6196]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7168558417755573, distance: 1.4994210673867656 entropy 10.93517352713569
epoch: 55, step: 70
	action: tensor([[-13179.5072, -22543.6183,    698.3799,  39898.1868,   2760.5075,
           5860.2508, -20870.3517]], dtype=torch.float64)
	q_value: tensor([[-25.5107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17835655923963656, distance: 1.2422100972939754 entropy 11.458559900338155
epoch: 55, step: 71
	action: tensor([[ 10735.8169, -21011.9032,   4206.5343,  20079.7026, -36174.4260,
          -4637.0456,  35020.2208]], dtype=torch.float64)
	q_value: tensor([[-27.9435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018346525045565665, distance: 1.133798289374367 entropy 11.524966013910776
epoch: 55, step: 72
	action: tensor([[-26490.7299, -53429.0483, -37004.3383,  -4481.6514,  22356.2397,
          -4606.8862,  27501.9989]], dtype=torch.float64)
	q_value: tensor([[-32.7503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2955609720612764, distance: 1.3025236621853853 entropy 11.505839295298667
epoch: 55, step: 73
	action: tensor([[-26804.0020, -12671.0782, -16008.9683,   3286.0046,  -8566.4458,
           8488.1255,  16098.9907]], dtype=torch.float64)
	q_value: tensor([[-26.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3323810743179172, distance: 1.3209029827102952 entropy 11.382187665668251
epoch: 55, step: 74
	action: tensor([[-17438.7417, -31333.8617,  19246.8269,   3883.9026,  26882.6592,
          -7700.6829, -15036.0690]], dtype=torch.float64)
	q_value: tensor([[-26.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25981479463210566, distance: 0.984525328197493 entropy 11.48866188442662
epoch: 55, step: 75
	action: tensor([[-40757.3756, -37704.5173,  -9224.1881, -34483.8817, -61704.2957,
          22895.8530,  25588.6038]], dtype=torch.float64)
	q_value: tensor([[-27.3827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027910268240602276, distance: 1.1282617656945848 entropy 11.571400168137773
epoch: 55, step: 76
	action: tensor([[-28298.5868, -15184.1933, -35630.4884,  50605.3726,  15357.9308,
         -15529.3534,  -4194.3584]], dtype=torch.float64)
	q_value: tensor([[-24.8758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2600964372314736, distance: 1.284572395446141 entropy 11.401960677145182
epoch: 55, step: 77
	action: tensor([[-17380.9062, -13293.8014,  -9200.4177,  -7349.2708,  17778.0488,
          18726.6522, -28896.2628]], dtype=torch.float64)
	q_value: tensor([[-28.8752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4738417882077406, distance: 1.3892555046959758 entropy 11.533110822155933
epoch: 55, step: 78
	action: tensor([[  -806.3175, -28501.3596, -28623.5304,  -2991.0993,  30078.5362,
          -1884.6545, -40573.9395]], dtype=torch.float64)
	q_value: tensor([[-31.9217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0781504854132078, distance: 1.0987191876213105 entropy 11.652320721998363
epoch: 55, step: 79
	action: tensor([[ -4116.9049, -48039.6406,   7064.6628,   3812.0758, -14657.3054,
         -33899.9341,   -711.1460]], dtype=torch.float64)
	q_value: tensor([[-29.4939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5469963250719259, distance: 1.4233160093094943 entropy 11.58561986695149
epoch: 55, step: 80
	action: tensor([[  3362.5766,  14084.0970,  -8817.0380,  35502.2322, -31496.1922,
          14674.0450,  -6159.0444]], dtype=torch.float64)
	q_value: tensor([[-26.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6244810667981864, distance: 0.7012495773526257 entropy 11.354502012195736
epoch: 55, step: 81
	action: tensor([[-28289.3417,  -2172.3236,  -9889.3445,  33411.7797,  33157.1349,
          28110.3539,   9446.2644]], dtype=torch.float64)
	q_value: tensor([[-29.8231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13479658134039652, distance: 1.064426814796906 entropy 11.381388509167008
epoch: 55, step: 82
	action: tensor([[  1279.5654, -18459.4925, -12812.1978,  24945.0252, -26641.3503,
         -16360.4037,  25983.4589]], dtype=torch.float64)
	q_value: tensor([[-26.1983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4556651961515876, distance: 0.8442863053389604 entropy 11.49124027277866
epoch: 55, step: 83
	action: tensor([[ -4457.7894,   5242.6185,  24094.7385,  15829.4652,  -9471.1080,
         -43655.0510,  32590.6526]], dtype=torch.float64)
	q_value: tensor([[-28.0028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.357936374796695
epoch: 55, step: 84
	action: tensor([[-32719.6957,  12062.4965,    801.8239,  29716.1429,    904.5681,
           2870.5328,  15926.5420]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2495515265448478, distance: 0.991327451146993 entropy 10.93517352713569
epoch: 55, step: 85
	action: tensor([[-22492.8817, -32742.7839, -11666.0009,   8492.9975, -14855.7884,
           1197.6316,   8742.3468]], dtype=torch.float64)
	q_value: tensor([[-29.7949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1266616255900619, distance: 1.2146564046389654 entropy 11.474782263209304
epoch: 55, step: 86
	action: tensor([[-29415.1388, -43689.6790,  -2080.5677,  15240.4723, -37268.6654,
         -24672.2630, -11562.9671]], dtype=torch.float64)
	q_value: tensor([[-30.3477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15958863376339127, distance: 1.0490656138023324 entropy 11.64854688013568
epoch: 55, step: 87
	action: tensor([[-10639.8100, -32306.1577, -14513.8245, -12583.1905,  -1159.9074,
          49372.2157, -18062.8328]], dtype=torch.float64)
	q_value: tensor([[-30.8159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09111899681148872, distance: 1.1953435757293922 entropy 11.66461299169852
epoch: 55, step: 88
	action: tensor([[ -6055.0633,  14362.6029, -15453.3820,  47446.3456,  -7027.0610,
          -7548.3552, -10434.0770]], dtype=torch.float64)
	q_value: tensor([[-26.3090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.483408300395544
epoch: 55, step: 89
	action: tensor([[  7256.5367,  -4887.1993,   6947.3452,   6158.9280,   -324.8919,
          16164.2766, -22227.3349]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3821022189892038, distance: 0.8995286725231958 entropy 10.93517352713569
epoch: 55, step: 90
	action: tensor([[ 14530.4816, -49527.1714,  16648.9096,  -5994.2110,  12475.3866,
          19960.6649,  29696.1759]], dtype=torch.float64)
	q_value: tensor([[-28.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007463979839181567, distance: 1.1400655738299426 entropy 11.372006547752575
epoch: 55, step: 91
	action: tensor([[  3151.8979, -33574.3269,   4518.3291,  24492.9479,  -3510.0744,
         -28525.6583,  25665.1668]], dtype=torch.float64)
	q_value: tensor([[-28.8735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5753868397486733, distance: 0.7456815856047306 entropy 11.462382528329142
epoch: 55, step: 92
	action: tensor([[-18554.8300, -20145.3405, -17224.3241,  41122.0841,  22114.5553,
          28176.9849,  34999.3635]], dtype=torch.float64)
	q_value: tensor([[-32.5345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.335146620280394
epoch: 55, step: 93
	action: tensor([[ -8119.0562,   -739.3791,  15056.6145,  -9062.8500, -12987.6896,
            781.5872,   3924.1001]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046554135608583125, distance: 1.1173898265216855 entropy 10.93517352713569
epoch: 55, step: 94
	action: tensor([[  7316.1692, -29978.7041,   2833.8483, -15164.8436,   3788.2509,
         -14172.5313,  34751.4414]], dtype=torch.float64)
	q_value: tensor([[-28.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20192047215830566, distance: 1.022303337266457 entropy 11.416418093185102
epoch: 55, step: 95
	action: tensor([[ 13236.0967, -10170.5261,   -149.4681,  14890.8652,   6625.2814,
         -21243.8487,  24489.8174]], dtype=torch.float64)
	q_value: tensor([[-29.4921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11476256197187285, distance: 1.2082251754792692 entropy 11.22567616256239
epoch: 55, step: 96
	action: tensor([[ 30378.6488,   3944.4741, -24819.1348,   2959.7993,  -6108.0234,
         -10108.8811,  -5298.5321]], dtype=torch.float64)
	q_value: tensor([[-32.7390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2587663713797089, distance: 0.9852223388830356 entropy 11.47582959143202
epoch: 55, step: 97
	action: tensor([[ -4626.6651,  -6270.0799,  -4058.5602,  -1553.2630,   2412.0173,
         -41108.4485,   5908.4607]], dtype=torch.float64)
	q_value: tensor([[-27.8665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.236492526657832
epoch: 55, step: 98
	action: tensor([[ -3043.0734,   9207.4459, -10430.0260,  -3307.8899,  -3653.0124,
          -2609.1205,   5728.4713]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.93517352713569
epoch: 55, step: 99
	action: tensor([[ -6304.3875,  10928.3922,  -7978.9622,    844.6539,    560.4063,
         -27885.7802,  10758.5970]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06996362316814586, distance: 1.1035872162962337 entropy 10.93517352713569
epoch: 55, step: 100
	action: tensor([[-22880.4420,    360.5877,  11832.5890, -16523.0472,  16070.2870,
           1502.9252,  -6701.8750]], dtype=torch.float64)
	q_value: tensor([[-26.6006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7087977297170449, distance: 0.6175243276970634 entropy 11.217992198686984
epoch: 55, step: 101
	action: tensor([[-10781.2177, -11482.6180,  -2256.5859,  11466.9229,   6864.8400,
         -23940.5685, -34139.7636]], dtype=torch.float64)
	q_value: tensor([[-30.3002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11114907967291743, distance: 1.0788750667397438 entropy 11.397350126916768
epoch: 55, step: 102
	action: tensor([[ 51944.4445,   -965.0017, -28880.7116,  32295.9272,   1930.9189,
          23007.9584,  24009.6525]], dtype=torch.float64)
	q_value: tensor([[-31.0259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0897431614109232, distance: 1.0917888845610397 entropy 11.705973595964169
epoch: 55, step: 103
	action: tensor([[-4386.8934, -8599.2129, 20543.9485, 36471.8790, 11786.7816, -4968.9123,
         20003.0855]], dtype=torch.float64)
	q_value: tensor([[-28.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8415184014271546, distance: 1.5529044151599827 entropy 11.507703354615705
epoch: 55, step: 104
	action: tensor([[  4607.8505, -28484.3962,  59194.9414,   5901.6621,   4532.1038,
         -28211.6881,  14770.7143]], dtype=torch.float64)
	q_value: tensor([[-25.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39029618353234263, distance: 0.8935444260572276 entropy 11.518507039140033
epoch: 55, step: 105
	action: tensor([[-22406.6586, -23392.6953,  23654.7567,  -2055.9488, -14916.8430,
          -5960.8628, -19902.2107]], dtype=torch.float64)
	q_value: tensor([[-30.8371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19478431707222754, distance: 1.0268637096078657 entropy 11.270271061869915
epoch: 55, step: 106
	action: tensor([[  8682.7096, -12627.6795,   2475.5576,  26633.7122,   2530.2765,
         -13570.9834, -22023.9792]], dtype=torch.float64)
	q_value: tensor([[-27.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13038939588765075, distance: 1.0671343674416938 entropy 11.46164606206952
epoch: 55, step: 107
	action: tensor([[ -2056.1521, -24488.1416,  35115.2867, -22457.0681,  -3291.5941,
         -26741.9213, -28954.6281]], dtype=torch.float64)
	q_value: tensor([[-30.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1476655016617513, distance: 1.0564810869022871 entropy 11.416602067600959
epoch: 55, step: 108
	action: tensor([[  9570.8691, -31261.5011,    113.9982,  32486.9451,  16199.4808,
          -8848.4069,  12820.7423]], dtype=torch.float64)
	q_value: tensor([[-25.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.510916603177684, distance: 0.8002914059668649 entropy 11.287634773429257
epoch: 55, step: 109
	action: tensor([[ 33999.5423, -20151.0297,  24125.1606,  26579.4207,  -9953.7931,
          14006.0705, -15643.8164]], dtype=torch.float64)
	q_value: tensor([[-32.3932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2213379961542662, distance: 1.009790277298505 entropy 11.41894990760395
epoch: 55, step: 110
	action: tensor([[-11037.4563,  -8698.9244, -20049.5926,  -5183.4881,  23191.7674,
          -5761.8473,  -9444.7490]], dtype=torch.float64)
	q_value: tensor([[-26.9517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35007783224604605, distance: 1.3296461991526924 entropy 11.31062727976015
epoch: 55, step: 111
	action: tensor([[  9640.3055, -16603.4178,  24016.2144,  25555.4814,   -683.5538,
         -19975.8067, -20496.2356]], dtype=torch.float64)
	q_value: tensor([[-22.2390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13030689212353097, distance: 1.0671849880951054 entropy 11.027598600766973
epoch: 55, step: 112
	action: tensor([[  8867.0275, -27702.6755,  37808.3633, -16774.4270,  35512.0232,
          15052.0709, -35318.3135]], dtype=torch.float64)
	q_value: tensor([[-33.8819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36054764190901034, distance: 1.3347919236943404 entropy 11.290633752877705
epoch: 55, step: 113
	action: tensor([[-32316.3789,   5400.0249, -31993.3853,  16256.6680,   9083.2956,
           8494.3357,  37250.9721]], dtype=torch.float64)
	q_value: tensor([[-27.7748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.199894922418581
epoch: 55, step: 114
	action: tensor([[ -1366.6819, -31043.6876,  13627.6335,  15027.9238,  16420.0084,
           6962.5496,  -2792.5180]], dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19612294551628817, distance: 1.026009801288811 entropy 10.93517352713569
epoch: 55, step: 115
	action: tensor([[-13099.4009,   4137.2947,  -9362.1275,  55135.3905,  -2894.2668,
          19433.6636,  35911.7976]], dtype=torch.float64)
	q_value: tensor([[-25.9146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7017973521247993, distance: 0.6249027573673936 entropy 11.414039673287023
epoch: 55, step: 116
	action: tensor([[-17458.6885,   -799.4998,   2295.4479,   8954.1282,  -4542.2132,
           -199.1441, -10530.2410]], dtype=torch.float64)
	q_value: tensor([[-29.6753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28959874387361206, distance: 1.2995230705951328 entropy 11.514407519704031
epoch: 55, step: 117
	action: tensor([[ -4175.5304, -16859.4376,  23747.8618,  38906.0081, -16191.4681,
           8082.0514,  -6717.7626]], dtype=torch.float64)
	q_value: tensor([[-25.6306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07514007689088142, distance: 1.1005117240491513 entropy 11.375705441722092
epoch: 55, step: 118
	action: tensor([[-14331.8960, -24350.3824,  53812.2079,  15542.7309,   1958.1773,
          16160.0177, -11068.5412]], dtype=torch.float64)
	q_value: tensor([[-25.9147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021575908830154367, distance: 1.1566235070977648 entropy 11.52176895936959
epoch: 55, step: 119
	action: tensor([[-40724.7507,  -9621.2678, -15483.4730,  10411.0017,   3472.6895,
           1728.9257, -13737.7291]], dtype=torch.float64)
	q_value: tensor([[-29.9048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14648123525673062, distance: 1.2252936102559413 entropy 11.594474432393614
epoch: 55, step: 120
	action: tensor([[  9179.2156, -17216.7121, -19820.9326,  -4110.0313,   2737.0000,
            954.8380,  43799.6549]], dtype=torch.float64)
	q_value: tensor([[-29.4775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5132906686137262, distance: 0.7983466911940205 entropy 11.645616763511429
epoch: 55, step: 121
	action: tensor([[-14552.7850, -25257.8131,    624.2678,  43572.3602,  12030.5196,
          -9738.5260,  -7803.0416]], dtype=torch.float64)
	q_value: tensor([[-28.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01884344982778674, distance: 1.133511281890566 entropy 11.47813783774897
epoch: 55, step: 122
	action: tensor([[ 16943.6574, -29374.0405, -26197.8476,  59947.3923,   6295.1368,
            -60.6443,   8056.5363]], dtype=torch.float64)
	q_value: tensor([[-25.5572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21186711627778143, distance: 1.2597494976630161 entropy 11.422969690632772
epoch: 55, step: 123
	action: tensor([[ 20774.5541, -23093.6217, -10547.5589, -11258.7883,  11913.7754,
         -11404.2928, -41275.4812]], dtype=torch.float64)
	q_value: tensor([[-33.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4090353595392199, distance: 0.879705771775746 entropy 11.447710694024988
epoch: 55, step: 124
	action: tensor([[  3543.3658,  -6292.3935,  11473.0580, -22843.9331,   1366.3339,
           -447.3920,  10966.6037]], dtype=torch.float64)
	q_value: tensor([[-29.9188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13767066761265268, distance: 1.2205764144707512 entropy 11.118293902846021
epoch: 55, step: 125
	action: tensor([[ 24384.1310, -18943.8816, -11763.6855,  24599.2720,  15100.8270,
         -33166.1318, -25880.8345]], dtype=torch.float64)
	q_value: tensor([[-31.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17305760933210979, distance: 1.0406251574776009 entropy 11.453993471332002
epoch: 55, step: 126
	action: tensor([[-18960.0267, -37980.1544, -12973.4838, -18889.6776,   5631.2396,
           6024.8460,  14645.5253]], dtype=torch.float64)
	q_value: tensor([[-29.5111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09983662426324202, distance: 1.0857188120814396 entropy 11.273948487646626
epoch: 55, step: 127
	action: tensor([[   938.1895, -20435.1486, -25565.5489,   2979.8831,   9899.9599,
         -33307.6033, -10232.9152]], dtype=torch.float64)
	q_value: tensor([[-24.2118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042049790188441016, distance: 1.1200261478276385 entropy 11.197068320298158
LOSS epoch 55 actor 323.9795596453379 critic 234.97347212239154
epoch: 56, step: 0
	action: tensor([[ -6773.2286,  -1190.7234,  -8827.3348,   3958.6266,  42795.2378,
         -15876.9653, -13876.0017]], dtype=torch.float64)
	q_value: tensor([[-22.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11310638830381081, distance: 1.20732732756017 entropy 11.218690697943277
epoch: 56, step: 1
	action: tensor([[-12835.3304, -25047.3510,   9365.7912, -13707.5993,  -9348.9939,
         -14106.0845,   6860.5115]], dtype=torch.float64)
	q_value: tensor([[-24.9239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1179271639239976, distance: 1.0747536216173286 entropy 11.448669964958059
epoch: 56, step: 2
	action: tensor([[ -1309.6830, -12275.5464, -30980.1371,  16846.8926,  25616.1699,
          -5475.2489,  26819.9431]], dtype=torch.float64)
	q_value: tensor([[-22.9677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.642661609252676, distance: 1.4666643876887022 entropy 11.295563146274123
epoch: 56, step: 3
	action: tensor([[-27196.8652, -37721.6889, -45548.7489,  13175.3781, -12696.4802,
          37730.9057,   9440.1428]], dtype=torch.float64)
	q_value: tensor([[-25.8168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6221015931943985, distance: 1.4574568934758214 entropy 11.686723352193756
epoch: 56, step: 4
	action: tensor([[-17380.3190,  26271.6200, -24606.3310,    453.0697,  51020.6996,
          -9901.3806,  20726.4398]], dtype=torch.float64)
	q_value: tensor([[-26.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35102211751257073, distance: 1.3301111152550174 entropy 11.655241352052428
epoch: 56, step: 5
	action: tensor([[ -6909.5055,    747.4550,   3344.5104, -41262.7352,   6597.4255,
         -15464.6956,  12592.0146]], dtype=torch.float64)
	q_value: tensor([[-24.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5819294953765624, distance: 0.7399143626362858 entropy 11.386790323629137
epoch: 56, step: 6
	action: tensor([[-27358.6496, -35165.8095,   9780.5286,  22994.8020,  21308.7577,
         -19511.5096, -12048.2886]], dtype=torch.float64)
	q_value: tensor([[-34.8190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23335671424291238, distance: 1.0019668692549648 entropy 11.446337719113696
epoch: 56, step: 7
	action: tensor([[-81382.7337,   4044.0023, -61113.4122,  10055.6942,  -3607.5399,
          31267.7614,   3537.4404]], dtype=torch.float64)
	q_value: tensor([[-26.5183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36371730090942167, distance: 0.9128128614107553 entropy 11.662484377146054
epoch: 56, step: 8
	action: tensor([[-27566.6320, -21804.7989,  -9403.9204, -18400.7723,  21804.8078,
         -38771.6509,  26625.1354]], dtype=torch.float64)
	q_value: tensor([[-25.3946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0050837004179596, distance: 1.6204026567094252 entropy 11.50731577659026
epoch: 56, step: 9
	action: tensor([[-11752.1475, -22139.6800,  10154.6085,  -5483.4674, -40489.4485,
          19910.4950,  10026.6624]], dtype=torch.float64)
	q_value: tensor([[-26.1870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6608725014642454, distance: 1.4747718543451103 entropy 11.592552835602545
epoch: 56, step: 10
	action: tensor([[ 20038.2775,  -5782.7353, -30142.0548,   2932.6810, -35280.4856,
         -16144.9280, -19466.7879]], dtype=torch.float64)
	q_value: tensor([[-23.5776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16096935289518433, distance: 1.23301134049458 entropy 11.271299853712515
epoch: 56, step: 11
	action: tensor([[-27341.3903,    -97.1061,  12283.1004,  29770.2292,   6875.7049,
          -2140.1126,  13564.9825]], dtype=torch.float64)
	q_value: tensor([[-21.7837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44671812585016957, distance: 1.37641264768102 entropy 11.102740535645026
epoch: 56, step: 12
	action: tensor([[-7371.3638, -1408.5115, -6916.1837, 13322.1232, -9605.7070,  7386.6918,
         36328.6077]], dtype=torch.float64)
	q_value: tensor([[-22.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2438947325856622, distance: 0.9950566926354227 entropy 11.390958591200917
epoch: 56, step: 13
	action: tensor([[  6838.0671, -13355.7584,  23549.4233,  25794.4174, -32280.3140,
          59232.7363,   2415.2704]], dtype=torch.float64)
	q_value: tensor([[-21.4193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3584856999043371, distance: 0.9165578136903881 entropy 11.36668970555956
epoch: 56, step: 14
	action: tensor([[-32379.5051, -10838.8507,  -1332.0143,  10675.7777,  24189.5928,
          -3392.7601,  -5297.7874]], dtype=torch.float64)
	q_value: tensor([[-27.9971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.527481869029831
epoch: 56, step: 15
	action: tensor([[ -4479.4339, -13283.9985,   6635.9189, -13541.7565,   -456.7817,
         -14069.0318,  15961.3897]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.975478667523642
epoch: 56, step: 16
	action: tensor([[-10051.0671,  14935.5367,  -1800.8902,  18046.6337,  -8691.4463,
           5201.2833,  15023.6296]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.975478667523642
epoch: 56, step: 17
	action: tensor([[ -8376.9238, -31972.8164,  -3079.3573,  -1536.4581,  13304.1753,
           9386.0695,  -1507.9049]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47981849827332024, distance: 0.8253423787073759 entropy 10.975478667523642
epoch: 56, step: 18
	action: tensor([[-14514.7626,  17704.2836,  21157.0543, -27620.5569,  -7836.3018,
          -5150.8806, -22768.3853]], dtype=torch.float64)
	q_value: tensor([[-25.3008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02705131353488821, distance: 1.1597189788010958 entropy 11.382722396537954
epoch: 56, step: 19
	action: tensor([[ 18700.2203, -40958.2181, -14248.0015,  -2899.2206,  13073.8702,
          18852.4410,   8047.7002]], dtype=torch.float64)
	q_value: tensor([[-32.4606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6878293057635017, distance: 0.6393707413693397 entropy 11.419136585945598
epoch: 56, step: 20
	action: tensor([[-46949.1506,   -550.7080,  -8208.4459,  -1472.0919,   8904.8667,
          50167.4738,   2458.8113]], dtype=torch.float64)
	q_value: tensor([[-24.0853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17635591158223818, distance: 1.2411551192953374 entropy 11.2700411653209
epoch: 56, step: 21
	action: tensor([[  7042.4696, -16666.1422, -64262.6781,   1356.5470,  42915.3578,
          53022.4391,  17437.7853]], dtype=torch.float64)
	q_value: tensor([[-27.5613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3310819131507359, distance: 1.3202588410237248 entropy 11.516814117570402
epoch: 56, step: 22
	action: tensor([[-9781.3837, -3189.5158,  8148.5482, 18168.4445, 26611.9300, 64671.4772,
         -3866.4385]], dtype=torch.float64)
	q_value: tensor([[-27.6243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15590000394072434, distance: 1.2303164360543681 entropy 11.319734858058892
epoch: 56, step: 23
	action: tensor([[-26697.4420,  38116.5999, -17848.2679,  41758.2905, -24902.4430,
          -7248.6596, -21621.0326]], dtype=torch.float64)
	q_value: tensor([[-24.5241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15608368364886405, distance: 1.051250910846659 entropy 11.575278991269004
epoch: 56, step: 24
	action: tensor([[-11999.3774, -15267.8561, -30089.0205,   3424.8855,   1505.6206,
          20097.0846,  -4040.1625]], dtype=torch.float64)
	q_value: tensor([[-25.4435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.46943612064034
epoch: 56, step: 25
	action: tensor([[ 11510.7402, -18035.5361,  -4452.1637,  23952.3925,  13914.9340,
          10929.7012,  12312.1327]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.975478667523642
epoch: 56, step: 26
	action: tensor([[-10896.3234,  -7426.0463,  -5146.6494,  12357.1195,  -5907.4054,
          12191.2898,  -5292.3653]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.975478667523642
epoch: 56, step: 27
	action: tensor([[ -3781.5497, -21703.8858,  17327.4969,   -978.6407,  -7829.8039,
          -5120.3596,  19474.7971]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22061590319084257, distance: 1.2642885514383524 entropy 10.975478667523642
epoch: 56, step: 28
	action: tensor([[ 38315.9069,   1120.3490, -30012.5245,  -5493.5517,  36922.1281,
         -45721.6625,  -6916.1886]], dtype=torch.float64)
	q_value: tensor([[-27.4459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5368639729103366, distance: 0.7787731614452236 entropy 11.457541899809815
epoch: 56, step: 29
	action: tensor([[-24974.1588,    679.1194,  -6831.4378,  22493.3256, -21654.5674,
         -25772.5637, -15372.9175]], dtype=torch.float64)
	q_value: tensor([[-32.2899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020668938856105212, distance: 1.1561099594891187 entropy 11.44636360194691
epoch: 56, step: 30
	action: tensor([[-82943.7496,   9566.7890, -33244.2371,  18352.0617,   7544.2289,
         -31462.4838, -32914.5606]], dtype=torch.float64)
	q_value: tensor([[-29.9065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.460791480770523, distance: 0.8403013589267287 entropy 11.571061628813272
epoch: 56, step: 31
	action: tensor([[ 19966.3295, -38178.1368, -35035.5119,   3322.2324,   5445.6383,
         -30335.3037,  -1786.4940]], dtype=torch.float64)
	q_value: tensor([[-29.4005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.531569234789347, distance: 0.7832121184082595 entropy 11.631381735559751
epoch: 56, step: 32
	action: tensor([[ -5379.3563, -25953.5324,  -4800.8072, -25714.3511, -27556.2835,
          17565.0314, -23995.8705]], dtype=torch.float64)
	q_value: tensor([[-26.2362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19277033479232286, distance: 1.2497844244287744 entropy 11.483261757282817
epoch: 56, step: 33
	action: tensor([[-32533.3761,  10663.9619,   6629.7306,   5146.3508,  -5132.8582,
         -28387.4358,   9713.3600]], dtype=torch.float64)
	q_value: tensor([[-21.1354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4391180824026525, distance: 0.8570228726174999 entropy 11.255478714210739
epoch: 56, step: 34
	action: tensor([[  6020.9553, -22506.4675,  -3504.6290,   5719.1111,   3759.0098,
           1579.7378,  -9019.7411]], dtype=torch.float64)
	q_value: tensor([[-20.2041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.058798167418336
epoch: 56, step: 35
	action: tensor([[-18146.4816, -12589.8979,   6308.9612,  19859.2044,   8512.6584,
         -17495.1640,  -3282.6522]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.975478667523642
epoch: 56, step: 36
	action: tensor([[ -4478.8814, -15784.5544,   4403.0723, -14374.6529, -10340.4798,
          14685.3879,  11316.2529]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4453548352006682, distance: 1.3757639751000414 entropy 10.975478667523642
epoch: 56, step: 37
	action: tensor([[-17432.7917,  -6785.6979,  11718.9193,  26855.9400,  12487.3668,
          10401.4742,   8005.4509]], dtype=torch.float64)
	q_value: tensor([[-25.8527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7693160228584, distance: 1.5221568222482813 entropy 11.424724672251156
epoch: 56, step: 38
	action: tensor([[  -311.8745, -11987.0468, -22380.0228,  -3051.3544, -14193.7726,
           -338.9583, -74466.6296]], dtype=torch.float64)
	q_value: tensor([[-22.0409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12924805800848682, distance: 1.0678344287666737 entropy 11.39437256283266
epoch: 56, step: 39
	action: tensor([[   241.3770, -35272.6546, -12406.8583,  14139.2274,  27056.6007,
          20488.9400, -20096.4484]], dtype=torch.float64)
	q_value: tensor([[-23.8100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4486440243533707, distance: 1.3773284962448236 entropy 11.36410552477416
epoch: 56, step: 40
	action: tensor([[-10540.9209, -29447.4125, -20430.6457,   5335.0335,  17219.4362,
          14781.7116, -11182.4638]], dtype=torch.float64)
	q_value: tensor([[-23.1273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37574466685263763, distance: 1.342225891935434 entropy 11.239653504231276
epoch: 56, step: 41
	action: tensor([[ 17169.7926, -28078.9264,  -9779.6303,   3786.1759,   2674.8294,
           4253.1858, -18661.6616]], dtype=torch.float64)
	q_value: tensor([[-26.3577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022059740519107418, distance: 1.1568973706726544 entropy 11.63953364263529
epoch: 56, step: 42
	action: tensor([[ -4775.8367,   5764.6978, -39508.7256,  51782.3167, -26608.2863,
           -634.0968, -12372.4968]], dtype=torch.float64)
	q_value: tensor([[-24.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23587514358587103, distance: 1.2721666083233338 entropy 11.563078055558504
epoch: 56, step: 43
	action: tensor([[-34556.3170, -19695.8232,   4620.7090,   4455.6337,  14852.2171,
          35744.0134,  26289.9562]], dtype=torch.float64)
	q_value: tensor([[-26.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0626627201657175, distance: 1.107910398005522 entropy 11.417924690332423
epoch: 56, step: 44
	action: tensor([[-32554.2279, -27837.7583,   3848.5454,  31513.3229, -12296.8544,
         -28624.5490, -25971.3184]], dtype=torch.float64)
	q_value: tensor([[-24.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3182863246111969, distance: 1.3138977433333108 entropy 11.540747855847231
epoch: 56, step: 45
	action: tensor([[22610.0191, -2428.8106,  4802.3122,  6716.2819, 20125.2591, 53526.2070,
         39451.0569]], dtype=torch.float64)
	q_value: tensor([[-27.4626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3321425242221423, distance: 0.9351872756907187 entropy 11.744050396379544
epoch: 56, step: 46
	action: tensor([[-15806.5637, -44866.2185, -32246.9702,   5903.6020,  -2507.4439,
         -10576.2455,  13111.4760]], dtype=torch.float64)
	q_value: tensor([[-27.0143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40562276942063913, distance: 1.3567226800443961 entropy 11.29873992739628
epoch: 56, step: 47
	action: tensor([[-36527.5058, -22416.7550, -26154.2707,  11236.8980,    282.8754,
          30948.1604, -40905.6972]], dtype=torch.float64)
	q_value: tensor([[-23.4886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03927232687965765, distance: 1.1216486642640788 entropy 11.581692261476253
epoch: 56, step: 48
	action: tensor([[  8480.1692, -12188.0540, -43235.0495, -21235.0412,  50140.0542,
          13769.4989, -21034.9300]], dtype=torch.float64)
	q_value: tensor([[-24.8325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029162254450634917, distance: 1.1609101761554175 entropy 11.561408798788717
epoch: 56, step: 49
	action: tensor([[ -2362.0007, -18238.5098,  -3220.1013, -25887.1388, -16527.4241,
         -18261.2140,   2203.5437]], dtype=torch.float64)
	q_value: tensor([[-22.3175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8785874644251757, distance: 1.5684562288481445 entropy 11.13282065309414
epoch: 56, step: 50
	action: tensor([[ 44444.3972,  -7018.0554, -29830.1740,  -8876.6092,   1506.1341,
          19211.7112,  44652.2396]], dtype=torch.float64)
	q_value: tensor([[-28.2023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7420715143822778, distance: 0.5811742280120463 entropy 11.590379337136753
epoch: 56, step: 51
	action: tensor([[-19726.0807, -36844.7744, -41832.0711,  14425.8422,  -1922.0806,
          28367.8332, -18905.2265]], dtype=torch.float64)
	q_value: tensor([[-26.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016211766098183533, distance: 1.153582881612338 entropy 11.431431623381368
epoch: 56, step: 52
	action: tensor([[-13089.6170,  33456.9048,  16542.9074,  19194.8707, -11992.3394,
         -19318.2150,   3615.1410]], dtype=torch.float64)
	q_value: tensor([[-20.6557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012944326727777788, distance: 1.1517268231751017 entropy 11.257371473017871
epoch: 56, step: 53
	action: tensor([[-40214.8268, -43377.1694,   4482.2137,  -7329.3863, -12804.6986,
          19513.3878, -26546.6324]], dtype=torch.float64)
	q_value: tensor([[-27.6609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9580163771024901, distance: 1.6012710548563651 entropy 11.54006008281985
epoch: 56, step: 54
	action: tensor([[-20048.4663, -11563.0644,   5277.6811,  14488.5325, -23979.6607,
          -4970.6366,  12642.8350]], dtype=torch.float64)
	q_value: tensor([[-20.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05413187818764276, distance: 1.1129406050510253 entropy 11.285239765640728
epoch: 56, step: 55
	action: tensor([[-11759.6459,   8161.1871,  32236.2789,  17830.5915,  12078.7011,
          13947.6162,  56914.3564]], dtype=torch.float64)
	q_value: tensor([[-26.1205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2860124133804993, distance: 0.9669455606963612 entropy 11.552858066965754
epoch: 56, step: 56
	action: tensor([[ 15825.7886, -67856.6471, -31676.5334, -36069.3764,  -8722.9734,
           7669.2727, -28632.5239]], dtype=torch.float64)
	q_value: tensor([[-26.8409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29952149924148297, distance: 0.9577542734344797 entropy 11.499819464240106
epoch: 56, step: 57
	action: tensor([[  4976.5536,  -4206.7169, -29677.2566,  17589.3294,   3032.3428,
         -34161.5354,  -4433.7552]], dtype=torch.float64)
	q_value: tensor([[-24.9561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5756500686456147, distance: 0.7454504159312354 entropy 11.449436771301821
epoch: 56, step: 58
	action: tensor([[-54876.7633,  -9368.4976,  -3388.0623,  34626.6966, -37909.9288,
         -25132.1226, -19699.2266]], dtype=torch.float64)
	q_value: tensor([[-27.0232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17697646476259588, distance: 1.0381564834028396 entropy 11.51713501682502
epoch: 56, step: 59
	action: tensor([[ -2303.3485, -13214.5735,  -1980.3936,  -9727.3331,  26106.4781,
         -17026.5962,   4594.7204]], dtype=torch.float64)
	q_value: tensor([[-24.6227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04680468314409536, distance: 1.1708183534883432 entropy 11.525990841695043
epoch: 56, step: 60
	action: tensor([[ 11634.0098,   5229.9649,  25873.5583,   4224.9158,  25837.9436,
         -27298.2014, -11834.0822]], dtype=torch.float64)
	q_value: tensor([[-22.1189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.241544557751128
epoch: 56, step: 61
	action: tensor([[  -669.3526, -11897.6269,  -8104.4135,   -431.5936,   -358.3168,
          28057.8823,   -860.4277]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.975478667523642
epoch: 56, step: 62
	action: tensor([[  5050.7768, -22333.6116,  18184.5895,  -5617.8137,  39038.6845,
          22354.2632,  21530.1827]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0550153481599438, distance: 1.1754010710751326 entropy 10.975478667523642
epoch: 56, step: 63
	action: tensor([[  5264.5226, -18496.9548,  -1313.3555,  32532.0683,  29424.4791,
         -22950.6982, -21215.5103]], dtype=torch.float64)
	q_value: tensor([[-26.4386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18474270258822667, distance: 1.2455716488196626 entropy 11.338846858936721
epoch: 56, step: 64
	action: tensor([[ -5606.5967,  -6005.2807,   3162.8249,  -7080.7695, -12524.6225,
          16485.0890, -30809.0091]], dtype=torch.float64)
	q_value: tensor([[-27.1124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11866158303003904, distance: 1.2103362902201165 entropy 11.3379774862136
epoch: 56, step: 65
	action: tensor([[ 7053.3347, 13042.5007, -8545.3576, 14584.0566, 10034.1110, -9162.6451,
         17954.3814]], dtype=torch.float64)
	q_value: tensor([[-26.7942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.542716363734666
epoch: 56, step: 66
	action: tensor([[  6400.7416, -14678.5882,  -2122.7920, -11220.3361,  -5244.4535,
           4991.0078,  -3989.2178]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0769829850991145, distance: 1.099414718230551 entropy 10.975478667523642
epoch: 56, step: 67
	action: tensor([[ -8634.7235, -29436.9167, -10330.0174,  10517.1163,  23561.0215,
          50086.8131,  39232.8637]], dtype=torch.float64)
	q_value: tensor([[-26.5550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5782603726575326, distance: 1.4376263338811146 entropy 11.293193905623687
epoch: 56, step: 68
	action: tensor([[  2552.0198, -19261.1033,   -279.4220,   5503.0687,  15563.1866,
         -19963.9316,   9619.5511]], dtype=torch.float64)
	q_value: tensor([[-18.8997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04495617473038083, distance: 1.1697841472490886 entropy 11.126525172177448
epoch: 56, step: 69
	action: tensor([[  9276.6168, -26615.4362,  -8353.9602,  43649.2099,  12185.6904,
            971.9123, -35788.4631]], dtype=torch.float64)
	q_value: tensor([[-23.9832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28154967979190326, distance: 1.2954612191240493 entropy 11.359650743963888
epoch: 56, step: 70
	action: tensor([[  2517.4097, -29589.1721,  16856.5556,  39813.1996,  -3476.7611,
           2082.0230,  14854.9814]], dtype=torch.float64)
	q_value: tensor([[-33.4899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10278349496793115, distance: 1.2017159405025415 entropy 11.471508676002008
epoch: 56, step: 71
	action: tensor([[-21383.9174, -24045.2420, -13390.1052,  41782.1103,  10941.5028,
          28854.5493,  28651.7611]], dtype=torch.float64)
	q_value: tensor([[-26.7178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3571478649457174, distance: 1.333123171656638 entropy 11.527060038007107
epoch: 56, step: 72
	action: tensor([[-29848.0992, -14657.8729,  17668.0174,  23359.2038,  11376.2917,
          -3618.3960, -11083.5078]], dtype=torch.float64)
	q_value: tensor([[-23.4995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09923747353410062, distance: 1.0860800803946973 entropy 11.445181486563675
epoch: 56, step: 73
	action: tensor([[ -6640.6734, -20484.8326,  28228.1391,  21717.4473,  16962.6173,
         -31671.0836, -17239.1905]], dtype=torch.float64)
	q_value: tensor([[-24.3505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3879891363602588, distance: 1.3481857323356827 entropy 11.558362866362074
epoch: 56, step: 74
	action: tensor([[-10618.3314, -38519.0956,   6341.3091, -14188.7949, -30185.0777,
          90867.0535,    905.2173]], dtype=torch.float64)
	q_value: tensor([[-26.0291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8074467419459053, distance: 1.5384714734413085 entropy 11.657599309872074
epoch: 56, step: 75
	action: tensor([[-22138.1149, -27230.4639, -10957.8359,   3490.7752,  -5887.3405,
         -29607.4037,   4114.5793]], dtype=torch.float64)
	q_value: tensor([[-21.9503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7228135491158678, distance: 1.502020404891363 entropy 11.275065825166271
epoch: 56, step: 76
	action: tensor([[-16386.9140,   3880.1891, -24664.2535,  24155.7383,  24885.9945,
          35750.3032, -12473.6545]], dtype=torch.float64)
	q_value: tensor([[-22.8044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6765396051103256, distance: 0.6508295293696118 entropy 11.353614883697421
epoch: 56, step: 77
	action: tensor([[ -3477.8602, -25049.0264,  11241.1606,  15414.2389,    723.4033,
          22624.8004, -22869.2179]], dtype=torch.float64)
	q_value: tensor([[-24.9587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19268836771802778, distance: 1.0281992869153105 entropy 11.48160771626512
epoch: 56, step: 78
	action: tensor([[ 2.0523e+04,  3.2496e+04, -5.8778e+04,  3.8604e+04, -2.6886e+04,
          8.4355e+00,  3.0691e+04]], dtype=torch.float64)
	q_value: tensor([[-24.6109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28589577071066363, distance: 0.9670245414152323 entropy 11.548481004250887
epoch: 56, step: 79
	action: tensor([[ 11600.5085, -64060.7322, -23678.1580,   4872.9337,  12131.4747,
           1033.0784, -25190.9083]], dtype=torch.float64)
	q_value: tensor([[-24.6171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07024429824413225, distance: 1.1034206782947076 entropy 11.395750762849387
epoch: 56, step: 80
	action: tensor([[-1.0689e+04, -1.3167e+04, -2.3934e+04,  2.4493e+04,  5.3518e+04,
          2.1423e+01,  5.4881e+03]], dtype=torch.float64)
	q_value: tensor([[-32.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20636727288793133, distance: 1.2568876724683038 entropy 11.501824484789749
epoch: 56, step: 81
	action: tensor([[  8810.2928, -10694.9132,  40645.0808,  -7684.9503,  42150.6713,
          16920.7733, -11370.1385]], dtype=torch.float64)
	q_value: tensor([[-25.0683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1615185270623083, distance: 1.233302932135845 entropy 11.60062317927037
epoch: 56, step: 82
	action: tensor([[-12493.9092, -56059.4914,  -6322.9019,  13975.8634,  13827.8875,
          46480.6745,  -1933.1045]], dtype=torch.float64)
	q_value: tensor([[-29.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027691608393033085, distance: 1.1283886529899891 entropy 11.54880063930246
epoch: 56, step: 83
	action: tensor([[ -4919.1494,   9033.7263,   4618.2973,  17323.2460, -11070.0926,
          13065.4520,  10075.0438]], dtype=torch.float64)
	q_value: tensor([[-24.8381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44943028493825055, distance: 0.849107843463352 entropy 11.577865106949217
epoch: 56, step: 84
	action: tensor([[ 20137.5359, -32814.1665,  -6884.4983,  20107.3393,  15390.8671,
          -7275.7636,  26467.1666]], dtype=torch.float64)
	q_value: tensor([[-26.2636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3667395464223766, distance: 0.9106424202802521 entropy 11.515317541289283
epoch: 56, step: 85
	action: tensor([[ -9182.9983,  15513.8651,  33833.6245, -17750.0541, -12427.0109,
         -13353.2330,   -969.0876]], dtype=torch.float64)
	q_value: tensor([[-28.2382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.613613577429664
epoch: 56, step: 86
	action: tensor([[  8022.5629, -30733.8575,  -5557.7030,  25041.0946,  -3802.9376,
           6468.8025, -15933.9175]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1204706993857646, distance: 1.0732029290997638 entropy 10.975478667523642
epoch: 56, step: 87
	action: tensor([[-37267.3452, -20017.4231,  25071.3052,  -1249.4314,  -1049.8865,
          15114.4847,  43484.4138]], dtype=torch.float64)
	q_value: tensor([[-28.9126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8168497599887079, distance: 1.5424681358929575 entropy 11.460799390558767
epoch: 56, step: 88
	action: tensor([[-25438.0708, -32493.9384,  -1172.6219,  -1928.9503,  11700.1102,
          11541.0830,  23803.5239]], dtype=torch.float64)
	q_value: tensor([[-20.6586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7754133958052125, distance: 1.5247773759915022 entropy 11.223448091356321
epoch: 56, step: 89
	action: tensor([[-27290.6881, -16877.8390,  -4942.7710,  83914.8338,  47734.7870,
          23275.8201,  19317.1101]], dtype=torch.float64)
	q_value: tensor([[-27.3075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2590060769883087, distance: 1.2840165055341526 entropy 11.595663618925297
epoch: 56, step: 90
	action: tensor([[ 67555.0146, -16917.3396, -40699.0963,  32228.4338,  -7967.8694,
          -7963.8074,  28106.6770]], dtype=torch.float64)
	q_value: tensor([[-24.2051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31105486177449304, distance: 1.3102890930812965 entropy 11.503010950911312
epoch: 56, step: 91
	action: tensor([[  6589.8324,  19874.7091,   1850.0895, -24536.5090,   -161.6453,
          27227.3795,  21691.9209]], dtype=torch.float64)
	q_value: tensor([[-26.2777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7415190512532521, distance: 0.5817963104951976 entropy 11.56193229810726
epoch: 56, step: 92
	action: tensor([[  2893.2242, -17857.1069,   2682.4073,   9440.3266,  17304.7951,
          32462.4335,  51959.2345]], dtype=torch.float64)
	q_value: tensor([[-30.9223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5425801933065577, distance: 0.7739522662893836 entropy 11.440572678828227
epoch: 56, step: 93
	action: tensor([[-19294.9954,  -4005.1990, -37243.7539,  14702.5597, -41611.9570,
           5178.5069,   9101.3424]], dtype=torch.float64)
	q_value: tensor([[-27.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3210724732236532, distance: 1.3152854472606939 entropy 11.533427724601479
epoch: 56, step: 94
	action: tensor([[-22542.2034,  -5477.4622,   8471.8195,  35630.6014,  37762.1544,
          -6392.3362, -39838.7731]], dtype=torch.float64)
	q_value: tensor([[-27.3831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29177663179349067, distance: 1.3006199318064064 entropy 11.641677882050763
epoch: 56, step: 95
	action: tensor([[ -9994.9514, -11564.5801,  19192.9662, -26416.6363, -50774.7570,
          22560.7999,  -3633.6740]], dtype=torch.float64)
	q_value: tensor([[-26.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6813995563893798, distance: 1.483857368063119 entropy 11.594937422299862
epoch: 56, step: 96
	action: tensor([[-10695.3750,   4621.2876,  13088.4817,   9997.2800, -21165.2901,
          16743.1565,  55907.3692]], dtype=torch.float64)
	q_value: tensor([[-24.3895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6980188139519525, distance: 0.6288493792036769 entropy 11.467773808629548
epoch: 56, step: 97
	action: tensor([[ 27561.6124, -27914.1260, -37966.1952,   1147.3624, -16697.5464,
           4725.7296, -43604.5388]], dtype=torch.float64)
	q_value: tensor([[-25.7831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.413652703288808
epoch: 56, step: 98
	action: tensor([[-14214.2567, -16319.4364,  -1624.0459,  -4368.7248, -12506.5891,
          -5634.9106,  -1768.2495]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.644820680501897, distance: 1.4676279437377324 entropy 10.975478667523642
epoch: 56, step: 99
	action: tensor([[ -2317.6677, -10703.3280,    912.5717,  47355.8776,   5271.4194,
         -17500.1057,    -81.0521]], dtype=torch.float64)
	q_value: tensor([[-30.6066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9390258811940226, distance: 1.593486895237301 entropy 11.66045679931633
epoch: 56, step: 100
	action: tensor([[ -3091.8957,   2640.3171,  -8647.8199,  19176.3250, -17532.0014,
          -2934.0464, -26407.8960]], dtype=torch.float64)
	q_value: tensor([[-21.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10097546322315953, distance: 1.0850317977909332 entropy 11.274285326175459
epoch: 56, step: 101
	action: tensor([[-12751.0760, -38169.1893,  -5723.3755,  22835.5302,  -2149.5011,
           2716.8318,  39594.0857]], dtype=torch.float64)
	q_value: tensor([[-28.9016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09355812050352175, distance: 1.0894985951488696 entropy 11.580849797910771
epoch: 56, step: 102
	action: tensor([[  6634.9175, -26658.2428, -11274.4063, -34461.7361,   2662.1321,
          36809.7185,  -9397.5168]], dtype=torch.float64)
	q_value: tensor([[-22.3323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07945380035090144, distance: 1.0979422258727276 entropy 11.44611234600423
epoch: 56, step: 103
	action: tensor([[-26887.5850, -46575.4382, -25323.3049,   9266.9527, -22270.7149,
         -20783.5901, -12057.3264]], dtype=torch.float64)
	q_value: tensor([[-26.4248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2147393775217643, distance: 1.0140598858479661 entropy 11.543873511457809
epoch: 56, step: 104
	action: tensor([[ -6485.9831, -31619.3321, -32424.2195,  27543.2647,  12790.7929,
         -16582.9902,  30688.3915]], dtype=torch.float64)
	q_value: tensor([[-26.0996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31343153609214625, distance: 1.3114761983979253 entropy 11.690761124990164
epoch: 56, step: 105
	action: tensor([[ -3408.9170, -40044.7414,   3455.3041,  34900.1299,  25589.9813,
          42298.7058, -10512.1972]], dtype=torch.float64)
	q_value: tensor([[-26.8232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08796897655184654, distance: 1.1936168722108063 entropy 11.703336506203055
epoch: 56, step: 106
	action: tensor([[-24422.1157,   -580.9179,   6571.7344, -45941.0448, -42127.6048,
         -30625.3062,  19123.1381]], dtype=torch.float64)
	q_value: tensor([[-23.2289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3599468445886511, distance: 1.3344971791602547 entropy 11.504102519566885
epoch: 56, step: 107
	action: tensor([[ -8472.0155,  29885.3896, -57586.6134,   3452.3673,  -1536.4031,
         -41316.6418, -19015.8657]], dtype=torch.float64)
	q_value: tensor([[-23.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2284054898564063, distance: 1.0051971712642063 entropy 11.34074953436488
epoch: 56, step: 108
	action: tensor([[-13441.8004, -40188.4980, -28381.5998,  68252.2378,  49872.2375,
          13416.6366,  26199.9778]], dtype=torch.float64)
	q_value: tensor([[-29.5863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7097201815386369, distance: 1.4963018480963362 entropy 11.615346130497542
epoch: 56, step: 109
	action: tensor([[ 25051.7797,  -6843.0788,   6536.9522, -24898.1278, -26603.0702,
          29666.2460, -35474.6407]], dtype=torch.float64)
	q_value: tensor([[-23.1295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025840845774062426, distance: 1.159035363344805 entropy 11.532559031662746
epoch: 56, step: 110
	action: tensor([[ -4135.8648, -10853.0714,  11792.9376,  29896.6606, -14639.9322,
          -3468.7788,   5035.4166]], dtype=torch.float64)
	q_value: tensor([[-21.6845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7621278091434873, distance: 1.519061636268099 entropy 11.02856219718405
epoch: 56, step: 111
	action: tensor([[-31219.7826, -39919.2867,  41568.2811,  44418.2710, -21518.8427,
          14918.8933,  18340.0210]], dtype=torch.float64)
	q_value: tensor([[-23.3974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13128939560431752, distance: 1.217148452808413 entropy 11.486962431852604
epoch: 56, step: 112
	action: tensor([[-13734.2666,  11911.9688,   3472.4048,  27423.4014,  29127.0187,
          10352.4187, -14044.9080]], dtype=torch.float64)
	q_value: tensor([[-23.2390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13888465201788558, distance: 1.0619091384512505 entropy 11.437071048434703
epoch: 56, step: 113
	action: tensor([[-30145.1428, -26786.9312, -36720.6166,  52738.4091, -14441.1073,
          -7029.0154, -46636.8238]], dtype=torch.float64)
	q_value: tensor([[-27.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06828737608608626, distance: 1.1827712010156592 entropy 11.554682355207717
epoch: 56, step: 114
	action: tensor([[  9632.9893, -14928.5377,  22871.3320,  17178.6319,  21051.0375,
          94910.6069,  -5508.7308]], dtype=torch.float64)
	q_value: tensor([[-26.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5325143946460476, distance: 0.7824215699488242 entropy 11.74169738500547
epoch: 56, step: 115
	action: tensor([[-17642.1298, -24473.3873, -23340.9231,  38119.4403,  21783.2793,
         -22709.1795, -21787.7794]], dtype=torch.float64)
	q_value: tensor([[-25.1061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17129812092333585, distance: 1.0417316402377996 entropy 11.361923045587057
epoch: 56, step: 116
	action: tensor([[ -5269.8228,   4839.1461,  -8437.7746,   8235.5130,  28081.9470,
         -42053.1031,  13131.7791]], dtype=torch.float64)
	q_value: tensor([[-22.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20843699794761794, distance: 1.0181210969017276 entropy 11.422638742286631
epoch: 56, step: 117
	action: tensor([[  9350.0845, -34820.0146, -14195.9318, -14708.1095, -20721.5408,
         -17738.7467,  21392.0896]], dtype=torch.float64)
	q_value: tensor([[-25.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.40843803553256
epoch: 56, step: 118
	action: tensor([[ -3790.9701,  -1333.3391, -20514.9227,  16306.9965,  17774.6017,
          -4813.2817,   -586.4599]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6264076035102242, distance: 1.459390084662011 entropy 10.975478667523642
epoch: 56, step: 119
	action: tensor([[-16235.0669,  20245.3315,   9648.0269,  32289.8679,  28578.2950,
          53554.1007,   -389.9762]], dtype=torch.float64)
	q_value: tensor([[-22.8237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41679120871551223, distance: 0.8739140552879182 entropy 11.409148363726304
epoch: 56, step: 120
	action: tensor([[   458.4357, -12133.9068,   2237.2966,  20738.2101,  16184.3784,
          -5936.0139,  -8461.3510]], dtype=torch.float64)
	q_value: tensor([[-26.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.431700083978217
epoch: 56, step: 121
	action: tensor([[-20116.3091,  10171.0659,  -2033.4573,  15599.3376,  19078.2478,
         -13154.9015,  12059.3644]], dtype=torch.float64)
	q_value: tensor([[-27.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5558943732866686, distance: 0.7626053207985901 entropy 10.975478667523642
epoch: 56, step: 122
	action: tensor([[-77602.9561,  -3439.3704, -36975.1534,  -2715.9293, -17444.6117,
          10610.2691, -33296.4711]], dtype=torch.float64)
	q_value: tensor([[-30.4251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7692372789132407, distance: 1.5221229498463404 entropy 11.662490668583475
epoch: 56, step: 123
	action: tensor([[  8460.9778, -30627.4299,  26935.1015,  13534.7823,  15397.4581,
          -1669.2067,   7898.4912]], dtype=torch.float64)
	q_value: tensor([[-24.7485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7500970078599402, distance: 0.5720611058419147 entropy 11.53325111323166
epoch: 56, step: 124
	action: tensor([[-12023.8055, -26936.6713,  25012.7889,  30242.3058,  20576.6124,
         -46081.1758,  47934.5298]], dtype=torch.float64)
	q_value: tensor([[-24.6837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1586634770924169, distance: 1.231786250373828 entropy 11.498239628147674
epoch: 56, step: 125
	action: tensor([[-43066.5682,  -3286.9067,  10765.6086, -32310.7067, -47529.7169,
          10820.4411,  -1683.7150]], dtype=torch.float64)
	q_value: tensor([[-22.2456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22785824431257162, distance: 1.2680337373900912 entropy 11.395626320244025
epoch: 56, step: 126
	action: tensor([[ -4055.1640,  -3223.1604,  18021.0758, -26207.9072,  35340.5716,
         -13739.7170,  38318.0961]], dtype=torch.float64)
	q_value: tensor([[-24.0739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27957877515048457, distance: 1.29446468618296 entropy 11.42556342849601
epoch: 56, step: 127
	action: tensor([[  6324.5812, -19975.0151,  10512.7196,  13338.7076,  14060.9791,
          25954.2377,  11450.1927]], dtype=torch.float64)
	q_value: tensor([[-27.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3719066770872368, distance: 0.906919586940845 entropy 11.541506168131926
LOSS epoch 56 actor 270.5060563706347 critic 253.33059922365305
epoch: 57, step: 0
	action: tensor([[-32368.2956,   2068.0448,  -3778.6241,   1829.9411, -15774.0755,
          -9772.8113,  16160.6206]], dtype=torch.float64)
	q_value: tensor([[-23.3685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4584148209768286, distance: 0.8421512134475049 entropy 11.349636974651615
epoch: 57, step: 1
	action: tensor([[  -883.9572, -11548.1976,  40953.6167,  -6921.4378,  12935.2272,
          -5134.4719,  -5546.7764]], dtype=torch.float64)
	q_value: tensor([[-27.1067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.501063858297835
epoch: 57, step: 2
	action: tensor([[-10919.0834,  -3448.7883, -17910.3502,  30897.1924,   3673.7194,
           9660.1625, -20460.7807]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14813407152816638, distance: 1.0561906473755023 entropy 11.015539327544072
epoch: 57, step: 3
	action: tensor([[-52830.7746, -65032.6043,   7379.1151, -15294.8848,   8455.5351,
         -40656.2254, -10691.4380]], dtype=torch.float64)
	q_value: tensor([[-26.7744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7075493355283085, distance: 1.4953516128862023 entropy 11.68699944833399
epoch: 57, step: 4
	action: tensor([[ 37722.2332, -12799.0978, -57841.4111,  15938.3048,  54898.3365,
          43022.0625,  27556.5072]], dtype=torch.float64)
	q_value: tensor([[-26.1032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23064382867865185, distance: 1.2694712868374918 entropy 11.542522939457438
epoch: 57, step: 5
	action: tensor([[-29931.7214,  42556.8188,  -4542.0490, -25030.5301,   -428.3857,
         -25069.5977,  25165.9581]], dtype=torch.float64)
	q_value: tensor([[-26.7949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.453928766833068
epoch: 57, step: 6
	action: tensor([[-16826.4781,  -2458.8122, -26683.9633,  32611.8269,  -2182.2898,
          -4408.5416, -10133.3942]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007211734336318942, distance: 1.1484631945391819 entropy 11.015539327544072
epoch: 57, step: 7
	action: tensor([[-37679.7682, -51215.3372,  38815.5155,  19289.3287,  -6254.7763,
         -20109.2748,  39400.8736]], dtype=torch.float64)
	q_value: tensor([[-23.1638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1257862328936462, distance: 1.0699550035121437 entropy 11.541169314696464
epoch: 57, step: 8
	action: tensor([[ -1390.2622, -69534.3082, -12495.5090,  19998.5323,  51936.2856,
          24370.6361,  -7120.0302]], dtype=torch.float64)
	q_value: tensor([[-26.1562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15104247920525427, distance: 1.0543861031959068 entropy 11.73637247987982
epoch: 57, step: 9
	action: tensor([[ 15983.6824,  15485.9861,  13941.9533, -18457.8145,   9492.9131,
          -2789.8921, -15584.0967]], dtype=torch.float64)
	q_value: tensor([[-25.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.584604202744973
epoch: 57, step: 10
	action: tensor([[-24141.4088, -38414.8701,   -639.0909,  21859.0709,  -1372.5118,
          -4953.7007,  -3052.9430]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009325510052378583, distance: 1.1496676688327312 entropy 11.015539327544072
epoch: 57, step: 11
	action: tensor([[ 14842.3497, -16534.0382,  -5910.3449,  24123.2012, -25375.8603,
          39838.1145, -24234.8467]], dtype=torch.float64)
	q_value: tensor([[-25.8007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22715329336190004, distance: 1.2676696766633342 entropy 11.685218989505385
epoch: 57, step: 12
	action: tensor([[  8943.5277, -28644.9498, -16044.9819,  -8966.1275, -13788.8911,
           7133.8170,  11353.0957]], dtype=torch.float64)
	q_value: tensor([[-29.1925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42950964833033234, distance: 0.8643325037366753 entropy 11.464313446701397
epoch: 57, step: 13
	action: tensor([[  9951.5821, -55331.2038, -35015.6908,  12147.5005,   2944.5686,
          31448.1366, -22839.4383]], dtype=torch.float64)
	q_value: tensor([[-30.8278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4301267436151617, distance: 0.86386490614612 entropy 11.441121882373684
epoch: 57, step: 14
	action: tensor([[  4476.2887, -19671.5665, -41829.6218,  23677.9877, -25498.7649,
          11296.2649, -19433.1359]], dtype=torch.float64)
	q_value: tensor([[-27.9252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14016227224137434, distance: 1.0611210789879078 entropy 11.439822434242028
epoch: 57, step: 15
	action: tensor([[ 14396.2539, -60172.1110,   5859.8264,  37052.3542,   2218.6326,
          22408.2519,  30810.0615]], dtype=torch.float64)
	q_value: tensor([[-25.9145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5285804779531871, distance: 0.7857067331684416 entropy 11.427586549377649
epoch: 57, step: 16
	action: tensor([[ 36248.9963, -12725.3669,  -1432.5745,  -4719.2275, -28078.2973,
          -6041.1049, -31508.6446]], dtype=torch.float64)
	q_value: tensor([[-28.6888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3480660598259271, distance: 1.32865516636706 entropy 11.623494841245574
epoch: 57, step: 17
	action: tensor([[   279.9926,  -7368.7451, -19756.1778,  49809.8559,   2534.1603,
          16519.3354,  41654.1042]], dtype=torch.float64)
	q_value: tensor([[-24.5288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5426561627452514, distance: 0.7738879936454524 entropy 11.285457738661794
epoch: 57, step: 18
	action: tensor([[-2264.7879, 30744.9360,   248.7609, 14636.4950, -4686.2701, -5700.0432,
         14460.5766]], dtype=torch.float64)
	q_value: tensor([[-28.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19751646573780468, distance: 1.025120121978072 entropy 11.569275884056022
epoch: 57, step: 19
	action: tensor([[ 14698.6120, -16190.5898, -21824.2900,    924.4779,  -1439.5249,
           9686.1427,  36045.9503]], dtype=torch.float64)
	q_value: tensor([[-27.6213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12338672093068392, distance: 1.0714223851948212 entropy 11.576483491377617
epoch: 57, step: 20
	action: tensor([[-16338.7612,  -5577.4231, -15013.7560,  10980.4505, -17951.7258,
          29622.4331,  58285.7166]], dtype=torch.float64)
	q_value: tensor([[-26.7518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295511512602992, distance: 0.9604917600423037 entropy 11.594628462812135
epoch: 57, step: 21
	action: tensor([[-43079.1058, -36708.3793,   7596.0913,  35590.8696, -57626.1755,
          47363.8541,  75616.4889]], dtype=torch.float64)
	q_value: tensor([[-28.5906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6796726533661177, distance: 1.483095164915215 entropy 11.677251573525087
epoch: 57, step: 22
	action: tensor([[-23378.1277,  -5117.2740,  20278.7317,  24120.5665,  -7075.8730,
           7695.6379, -46572.5898]], dtype=torch.float64)
	q_value: tensor([[-26.1237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09130510642675849, distance: 1.0908517609294 entropy 11.77073810626922
epoch: 57, step: 23
	action: tensor([[ 14205.4655,   4785.4269, -33652.5185,   4034.1861, -25112.6999,
           4820.2802,  -1417.1654]], dtype=torch.float64)
	q_value: tensor([[-26.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7454758403276717, distance: 0.5773261102300989 entropy 11.646708189070779
epoch: 57, step: 24
	action: tensor([[ -2868.0452,  24637.1935,  35125.1086, -13777.8471, -24961.4880,
         -42817.7407,  43483.0100]], dtype=torch.float64)
	q_value: tensor([[-28.2363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.509442387894408
epoch: 57, step: 25
	action: tensor([[-21610.7401, -12951.6001,   2631.2265,   3565.3778,   -430.7403,
          13971.5015,  30873.0190]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.711433176199183, distance: 1.4970512442282087 entropy 11.015539327544072
epoch: 57, step: 26
	action: tensor([[-58766.1397,   8634.0284, -17151.6084,  62695.7102, -20091.4188,
          55218.4016, -12769.8701]], dtype=torch.float64)
	q_value: tensor([[-25.1038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6855616260704176, distance: 0.6416888072763851 entropy 11.583820975486725
epoch: 57, step: 27
	action: tensor([[-7742.1916, 10338.1565, 30283.5209, 15467.6694,  6397.5714, 38074.6845,
         37837.4471]], dtype=torch.float64)
	q_value: tensor([[-26.8725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4910203084165876, distance: 0.8164073771237539 entropy 11.522723213738889
epoch: 57, step: 28
	action: tensor([[-43487.7631, -26269.6190, -17892.2664,   4851.8065,  20283.3846,
         -38054.7584,   -671.3921]], dtype=torch.float64)
	q_value: tensor([[-26.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03897310712285795, distance: 1.1218233200597465 entropy 11.435035606613686
epoch: 57, step: 29
	action: tensor([[ 39679.1209,  19557.1053, -22896.8402,  14958.4015, -16079.5140,
         -50436.6885,  26577.3976]], dtype=torch.float64)
	q_value: tensor([[-24.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.62655810985846
epoch: 57, step: 30
	action: tensor([[ 17600.9672,  -8841.2279,   5141.9153, -30371.2106,   5268.1743,
          14435.9128,  20334.2273]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.015539327544072
epoch: 57, step: 31
	action: tensor([[ 10862.1916, -18418.4479, -18417.3837,  21177.6841,  28575.6680,
          10051.7900,   1557.4486]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.015539327544072
epoch: 57, step: 32
	action: tensor([[ 2550.0033,  1042.8932, -7509.3808,   227.8330,  2213.2257, 11265.0142,
         16441.1888]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.015539327544072
epoch: 57, step: 33
	action: tensor([[ -1019.1664, -12763.6389, -10798.8055, -10936.8212,  -6962.5302,
           -765.3686,  26217.4239]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12556047934109804, distance: 1.2140626854249998 entropy 11.015539327544072
epoch: 57, step: 34
	action: tensor([[  8342.4608, -39592.0333, -31500.4420,    550.5500,  20973.8140,
         -40790.1415, -19199.9250]], dtype=torch.float64)
	q_value: tensor([[-27.1387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18587375911280124, distance: 1.032529740770829 entropy 11.533116008630389
epoch: 57, step: 35
	action: tensor([[  1215.7903,   2235.4295, -18346.4025,  25496.0175,  -5736.4378,
         -28073.8763, -15825.0222]], dtype=torch.float64)
	q_value: tensor([[-23.4495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2998975381364848, distance: 0.9574971626119493 entropy 11.411420123921955
epoch: 57, step: 36
	action: tensor([[-19059.7148, -41449.2940,   3486.2122,  25785.7438,  -7003.4235,
         -10556.0112,  -8359.6180]], dtype=torch.float64)
	q_value: tensor([[-27.9607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.438590152593752
epoch: 57, step: 37
	action: tensor([[-11532.1614, -24080.5341,   3900.6599,  -2375.1908, -21883.7041,
          23772.5809,   2540.4062]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01826360743851363, distance: 1.1547468985712273 entropy 11.015539327544072
epoch: 57, step: 38
	action: tensor([[ 15968.4310, -21087.6209, -14203.1931,  41006.4925, -15432.1039,
          14035.8258,  26280.9768]], dtype=torch.float64)
	q_value: tensor([[-25.6553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21000338632876403, distance: 1.2587804408196146 entropy 11.402693431944417
epoch: 57, step: 39
	action: tensor([[ 22783.3362,   6488.7019,   7315.7877, -19463.6152, -26000.9031,
         -33049.7861,  44113.3339]], dtype=torch.float64)
	q_value: tensor([[-25.7173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8041049074900835, distance: 0.5064871966731461 entropy 11.501293438909457
epoch: 57, step: 40
	action: tensor([[ -1453.6777, -18389.9128,  -1553.8595,  23169.6404,  -3783.3461,
          -5763.6048,  11216.6551]], dtype=torch.float64)
	q_value: tensor([[-25.9815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5468084892953027, distance: 1.4232295974093738 entropy 11.191560949548498
epoch: 57, step: 41
	action: tensor([[-10897.1829, -25359.5543,  -5736.1357,  15451.7033, -14654.9322,
          -7553.8029,  -4876.9178]], dtype=torch.float64)
	q_value: tensor([[-21.6688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14908182670048853, distance: 1.0556029441393247 entropy 11.408714122811455
epoch: 57, step: 42
	action: tensor([[  8124.6387,  20044.7877, -14017.8685,  85623.6205,  -3645.4979,
          63492.9482, -24376.3798]], dtype=torch.float64)
	q_value: tensor([[-24.5304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5257268049849166, distance: 0.7880812287668869 entropy 11.618218540150668
epoch: 57, step: 43
	action: tensor([[ 12830.7220,  -3359.0776, -13343.3754, -26846.6614, -10955.6405,
         -14536.5951, -12087.3305]], dtype=torch.float64)
	q_value: tensor([[-25.0718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4010375593732598, distance: 1.3545080247055528 entropy 11.466033886174355
epoch: 57, step: 44
	action: tensor([[ 9043.2814, -7626.2163, -8165.7228, 39366.2679, 25295.8425, -9349.9927,
          1211.6457]], dtype=torch.float64)
	q_value: tensor([[-31.8480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5191991755965266, distance: 1.4104706428473457 entropy 11.591514572961936
epoch: 57, step: 45
	action: tensor([[ 36794.5424, -32245.5618,   2749.5414,  50337.5866,   5421.3528,
          32557.4270, -12959.6449]], dtype=torch.float64)
	q_value: tensor([[-31.5606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0420557542206359, distance: 1.1681595702428447 entropy 11.46574670418378
epoch: 57, step: 46
	action: tensor([[-20598.1779, -32241.3998,  -3416.3878, -27405.1632, -17685.0066,
          48200.5839, -23272.2576]], dtype=torch.float64)
	q_value: tensor([[-28.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5448842154971691, distance: 1.4223440528438274 entropy 11.69850703326882
epoch: 57, step: 47
	action: tensor([[-13914.6851,   3196.0050,  -6431.3548, -13845.7628, -10118.4630,
          28316.8544, -27243.0585]], dtype=torch.float64)
	q_value: tensor([[-26.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42850431468062544, distance: 0.8650937436650743 entropy 11.469242111743778
epoch: 57, step: 48
	action: tensor([[-34274.7835,  -7976.0468, -27713.7008,  15493.2600,  13342.0524,
          -2967.9046, -13034.7315]], dtype=torch.float64)
	q_value: tensor([[-33.7981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08396691932076195, distance: 1.1914195098900504 entropy 11.550012152682024
epoch: 57, step: 49
	action: tensor([[ 28285.7611, -26848.7947, -21100.9372, -27411.4281, -58474.1371,
          -8233.1707,  60185.0921]], dtype=torch.float64)
	q_value: tensor([[-26.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3771673033665838, distance: 0.9031136263008736 entropy 11.617191958048377
epoch: 57, step: 50
	action: tensor([[-47520.0094, -10322.3742,   1396.0324, -18857.1318,   3039.9968,
          18547.3540,  19718.1144]], dtype=torch.float64)
	q_value: tensor([[-30.8557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.721586407082627
epoch: 57, step: 51
	action: tensor([[-11673.5179,  -9257.0234, -28206.4556,     93.8734,  10878.3422,
          21292.5899,   5319.7479]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8434232865997218, distance: 1.5537073775454413 entropy 11.015539327544072
epoch: 57, step: 52
	action: tensor([[ 17313.3825, -18557.5224,  23950.4192,  31603.8895,  19973.9904,
          49399.6317,   4132.0541]], dtype=torch.float64)
	q_value: tensor([[-20.8260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33197380134082854, distance: 0.9353053978448601 entropy 11.361117578921226
epoch: 57, step: 53
	action: tensor([[ -6159.8045,  14203.1097, -44792.5209, -34645.3470,  16487.7722,
         -16682.8738,  45189.4283]], dtype=torch.float64)
	q_value: tensor([[-26.6958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.475463253375912
epoch: 57, step: 54
	action: tensor([[ -2863.5991, -13798.9695,  -6802.0259,   9979.0640,  10734.9122,
         -19276.3165, -17709.9186]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006533649014095033, distance: 1.1480765394182024 entropy 11.015539327544072
epoch: 57, step: 55
	action: tensor([[-57753.6864, -10156.7989,  -2568.3845,  84738.2681,  11328.4317,
          50050.2870, -59104.1106]], dtype=torch.float64)
	q_value: tensor([[-27.0318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06102060995803227, distance: 1.10888044096801 entropy 11.701828157799659
epoch: 57, step: 56
	action: tensor([[  1356.2724,  -7353.0531,   3444.3681,  30452.2752,  -1043.0970,
          26128.4947, -10938.6316]], dtype=torch.float64)
	q_value: tensor([[-24.0459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3832940612483, distance: 0.8986607184699819 entropy 11.506774430949823
epoch: 57, step: 57
	action: tensor([[   380.3016,   1095.0120, -13389.0016, -31124.2703,  21232.1195,
          36164.2935,  19021.3361]], dtype=torch.float64)
	q_value: tensor([[-24.7145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5035748897611576, distance: 0.8062756865415813 entropy 11.280958952739905
epoch: 57, step: 58
	action: tensor([[ 1.6148e+04, -7.2005e+03, -1.1476e+04,  5.8119e+00,  1.7117e+03,
          3.0022e+04,  9.7384e+03]], dtype=torch.float64)
	q_value: tensor([[-26.9310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24284322570969352, distance: 0.9957483591795138 entropy 11.157601871611188
epoch: 57, step: 59
	action: tensor([[-14758.2076, -44440.4765, -16698.9595,   2606.3664, -13805.7408,
         -28049.9167, -29764.6204]], dtype=torch.float64)
	q_value: tensor([[-29.7986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16635587320240597, distance: 1.0448333843960675 entropy 11.388424105750204
epoch: 57, step: 60
	action: tensor([[  2243.1095,  -6902.7510, -16475.4086, -49963.7174,   2326.1663,
          35983.7768,   2326.0507]], dtype=torch.float64)
	q_value: tensor([[-23.1067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13655086494892954, distance: 1.219975563425169 entropy 11.471809262498216
epoch: 57, step: 61
	action: tensor([[  7098.1124, -15468.8495,  -8216.8182,   7647.1047, -23663.0591,
          -9908.8886,  -8516.2524]], dtype=torch.float64)
	q_value: tensor([[-22.8341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3652260434071517, distance: 1.3370848729736033 entropy 11.080377305320807
epoch: 57, step: 62
	action: tensor([[-22838.1968,   2097.8880,  -1051.1485,  19433.4130,   6910.5929,
         -13254.3520, -25757.3362]], dtype=torch.float64)
	q_value: tensor([[-29.1581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.386176614050829
epoch: 57, step: 63
	action: tensor([[-13791.0686,   8917.7566,   3121.2274,   3194.8861,  30891.6146,
           6027.0305,   4214.5921]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.015539327544072
epoch: 57, step: 64
	action: tensor([[ 7355.9510, -6148.5764,  9313.6289, 14768.3480, 10944.2986,  4448.2103,
         10698.9875]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05827644938164478, distance: 1.1772162789632945 entropy 11.015539327544072
epoch: 57, step: 65
	action: tensor([[-3920.4804, -9695.2422, -1430.3724, -2196.1870,  6265.9443, -6400.8909,
          9050.8638]], dtype=torch.float64)
	q_value: tensor([[-26.5330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4004288624512107, distance: 1.354213751924923 entropy 11.61105223088118
epoch: 57, step: 66
	action: tensor([[ -832.3110,  9866.0865, 35468.4723, 11851.0727, -5502.6026, 37976.6124,
         16818.8842]], dtype=torch.float64)
	q_value: tensor([[-23.7951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.403537201108602
epoch: 57, step: 67
	action: tensor([[-10450.9591,   6373.2233,  12143.9829, -13139.5086,    398.5212,
          15043.2346,  -5908.0439]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.015539327544072
epoch: 57, step: 68
	action: tensor([[-19233.1883, -30576.2619,  -4000.2370,  11569.6964,    400.5751,
          19608.0751,  -8285.8394]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03205878330467682, distance: 1.1625426919003352 entropy 11.015539327544072
epoch: 57, step: 69
	action: tensor([[  6823.7685,   4784.5088, -44009.2038,  -4217.7853,   4701.9589,
          30471.9782,  21045.1184]], dtype=torch.float64)
	q_value: tensor([[-25.1701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.584006064492188
epoch: 57, step: 70
	action: tensor([[   533.4743,  -1169.0165,  12222.4624, -19204.8461,   -246.6591,
          10311.9298,   5387.0962]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13941992381650536, distance: 1.061579044245733 entropy 11.015539327544072
epoch: 57, step: 71
	action: tensor([[-25522.9570,  45377.2878, -31195.4710,  17993.6482,  41313.0438,
          15795.1967, -26012.2361]], dtype=torch.float64)
	q_value: tensor([[-28.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.44298055192138
epoch: 57, step: 72
	action: tensor([[  1585.7894, -17774.7883, -12992.4033,  -5402.7005, -10788.9141,
           8983.7625,  31103.4369]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2886927909769388, distance: 1.2990665279941602 entropy 11.015539327544072
epoch: 57, step: 73
	action: tensor([[  4189.0645, -20614.7722, -26769.6766,  46172.7826, -21384.8442,
           5863.8116,  -2250.1622]], dtype=torch.float64)
	q_value: tensor([[-24.0336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2739717480938444, distance: 1.2916254443745379 entropy 11.198450810494474
epoch: 57, step: 74
	action: tensor([[  3515.1428, -12911.1238,   5296.5378,    545.4737,  13879.1924,
         -52299.1387, -60467.2028]], dtype=torch.float64)
	q_value: tensor([[-25.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18731901171949272, distance: 1.0316128503134085 entropy 11.535963966905099
epoch: 57, step: 75
	action: tensor([[-33888.8143,  16942.8831,  -3903.5308, -19168.7796,  25028.8785,
          13236.5649,  25661.9322]], dtype=torch.float64)
	q_value: tensor([[-26.8911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.344009622922966
epoch: 57, step: 76
	action: tensor([[  8090.8416, -21135.8931,  -1553.5653,  20499.0715, -15396.9050,
         -10240.0761,  20267.8028]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43701591798432904, distance: 0.8586274153946458 entropy 11.015539327544072
epoch: 57, step: 77
	action: tensor([[ 40730.9254, -22498.2156,  11482.8117,  39568.1487,   1859.2224,
           8797.4939, -17261.0505]], dtype=torch.float64)
	q_value: tensor([[-28.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15076808884705417, distance: 1.0545564827497043 entropy 11.632619654142824
epoch: 57, step: 78
	action: tensor([[ -5644.0993,  -2452.4746,  -8473.7940,  11007.5952, -22141.2841,
           4820.4055,  35272.3805]], dtype=torch.float64)
	q_value: tensor([[-26.2994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03226299778820607, distance: 1.1257329208626954 entropy 11.576315842501598
epoch: 57, step: 79
	action: tensor([[ 18300.3742,  17674.3038,    747.1956,   6482.3947, -10998.5812,
          41469.2780,  -5343.2788]], dtype=torch.float64)
	q_value: tensor([[-24.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30201220605614854, distance: 0.9560500031378908 entropy 11.472234001071028
epoch: 57, step: 80
	action: tensor([[-24375.9699, -40337.9641,   6458.0094,  62093.5017, -11179.0429,
          -8645.6601,  25022.1327]], dtype=torch.float64)
	q_value: tensor([[-29.3688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.63149721303083
epoch: 57, step: 81
	action: tensor([[  1037.6482, -19782.7373,   4366.2833,  27719.2060,  -1783.6210,
          12561.5489,   8832.1357]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6122855728159013, distance: 0.7125456189004387 entropy 11.015539327544072
epoch: 57, step: 82
	action: tensor([[ -5279.6878, -18993.5571,  -2787.0622,  17494.1513, -14774.1144,
          17335.4797, -14106.0861]], dtype=torch.float64)
	q_value: tensor([[-23.6441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.655833537598276, distance: 1.4725329807516974 entropy 11.34753809297242
epoch: 57, step: 83
	action: tensor([[-35142.3145,   3984.4207, -13733.8515,  21254.8820,  -7070.3805,
          18424.9101,  10787.6782]], dtype=torch.float64)
	q_value: tensor([[-25.3493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6005502323188779, distance: 0.7232488965519538 entropy 11.688249555716018
epoch: 57, step: 84
	action: tensor([[ -8389.6172, -15411.1842, -21170.8808,  61885.5832,   4796.3327,
         -43456.2639,  38344.6441]], dtype=torch.float64)
	q_value: tensor([[-24.6750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27430049458156425, distance: 0.9748439636318165 entropy 11.489386694562954
epoch: 57, step: 85
	action: tensor([[ -6613.5927, -20183.9014, -14297.6797, -23081.0225,  20282.9642,
          14131.9884,   5409.3666]], dtype=torch.float64)
	q_value: tensor([[-25.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0727623004090159, distance: 1.6475228391109211 entropy 11.695015153396978
epoch: 57, step: 86
	action: tensor([[ -2616.8423, -19966.3134,  39116.0487, -14520.0252, -29698.4839,
         -32891.7511, -26866.4599]], dtype=torch.float64)
	q_value: tensor([[-23.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4406293731886064, distance: 1.3735131635139808 entropy 11.376399578807124
epoch: 57, step: 87
	action: tensor([[54777.6701, 67555.2058, -1340.4152, 49465.6007,  4721.4558, 21643.7394,
         12577.2749]], dtype=torch.float64)
	q_value: tensor([[-31.3374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.681151411967383
epoch: 57, step: 88
	action: tensor([[ -1342.2981,   5783.9072,    252.3296, -10358.5805,  12298.5435,
         -25244.0939,   7256.6090]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.015539327544072
epoch: 57, step: 89
	action: tensor([[-19736.6789, -35236.3006, -17148.7352,   -126.9897,   1115.8824,
          -1653.9195,   8536.9802]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7217200405859583, distance: 1.5015436461507143 entropy 11.015539327544072
epoch: 57, step: 90
	action: tensor([[ 34731.7455, -62220.5537, -55045.0622,  11379.5316,  -5452.0890,
          30718.8121,   6353.8114]], dtype=torch.float64)
	q_value: tensor([[-28.6689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18579255288829843, distance: 1.246123403306416 entropy 11.63634211649889
epoch: 57, step: 91
	action: tensor([[  8237.2354,  -2194.0716,  27181.4411,   5385.7559,  26831.3188,
         -18600.7496,  -8223.1809]], dtype=torch.float64)
	q_value: tensor([[-31.0551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5481435738786115, distance: 0.769231260015193 entropy 11.561680790401889
epoch: 57, step: 92
	action: tensor([[  6708.3241,  -4071.8198,  14380.0187,  10129.6534,  32034.6723,
         -34513.9464, -25582.1754]], dtype=torch.float64)
	q_value: tensor([[-31.5583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41422917748609, distance: 0.8758315002490179 entropy 11.476616272427213
epoch: 57, step: 93
	action: tensor([[-36128.2243,  -6820.6124, -77220.0677, -13705.9602,  11130.1088,
          10952.5230,   3869.0678]], dtype=torch.float64)
	q_value: tensor([[-37.1981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3398438250266631, distance: 1.3245970478557056 entropy 11.720937837758417
epoch: 57, step: 94
	action: tensor([[ 16362.3788,  -2139.4818, -30262.9876, -30441.2904,  35442.6766,
          19470.4041, -17280.7162]], dtype=torch.float64)
	q_value: tensor([[-24.1656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4748388259421511, distance: 0.8292834505662748 entropy 11.418181358024926
epoch: 57, step: 95
	action: tensor([[ -1231.4465,  29844.5595,   2698.1574, -34197.1864,  32457.6274,
         -30990.4148,   3842.3317]], dtype=torch.float64)
	q_value: tensor([[-28.2782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06172547262115313, distance: 1.1791330482084448 entropy 11.45019657705602
epoch: 57, step: 96
	action: tensor([[ 34997.5584, -22727.8688, -10474.4491,   8386.5565,  22373.3693,
         -19197.1370,  16747.7568]], dtype=torch.float64)
	q_value: tensor([[-32.4169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.534281198339485
epoch: 57, step: 97
	action: tensor([[   197.4631, -13688.1139,  -2908.4970,  28595.7524,   2338.6818,
           7136.8514,  -6474.2539]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6020984535046983, distance: 0.7218459211750073 entropy 11.015539327544072
epoch: 57, step: 98
	action: tensor([[-32238.7808,  21073.0991,   5358.2845,  11118.6310,  -2186.9458,
            868.8361,  40525.9679]], dtype=torch.float64)
	q_value: tensor([[-24.2421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09957250090495107, distance: 1.0858780846503284 entropy 11.404354503988488
epoch: 57, step: 99
	action: tensor([[-3419.1410, -9296.5502, -9420.2799, 44787.8876,  5403.7253, -9887.7365,
         17049.1230]], dtype=torch.float64)
	q_value: tensor([[-24.8612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7175070551571634, distance: 1.4997054099812086 entropy 11.397595985572254
epoch: 57, step: 100
	action: tensor([[ -1357.9846, -26649.5548,  24494.2663,  40306.0226,   2063.1817,
         -12337.0943,  24723.1370]], dtype=torch.float64)
	q_value: tensor([[-23.9632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1885282884245535, distance: 1.0308450399259628 entropy 11.576138592478713
epoch: 57, step: 101
	action: tensor([[ 27255.6616,  12464.8178,   1959.9312,  -2236.4911, -25039.6301,
         -40884.6832, -23933.4314]], dtype=torch.float64)
	q_value: tensor([[-28.2141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.609461359671753
epoch: 57, step: 102
	action: tensor([[ 13911.2450, -37471.9597,  -8094.2259,  13073.2830,   -696.2058,
          10152.1481,   -881.4756]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5942278316342293, distance: 0.7289501357117257 entropy 11.015539327544072
epoch: 57, step: 103
	action: tensor([[-17986.1449,  12993.7979,   5543.4705,  36352.2609,  33423.5348,
         -15787.6707, -20971.4819]], dtype=torch.float64)
	q_value: tensor([[-24.7943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21367591190217827, distance: 1.0147463158815528 entropy 11.427787642414634
epoch: 57, step: 104
	action: tensor([[ 12940.2195,  -4251.1465,   -521.3821, -23346.2844, -18428.9956,
         -49336.2491,  11545.6798]], dtype=torch.float64)
	q_value: tensor([[-28.5754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.702586636736187
epoch: 57, step: 105
	action: tensor([[  1382.5023,  -6413.4600,  20668.5947, -22490.9295,   4993.7261,
         -33207.0851,  -1103.4876]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4175308736856084, distance: 0.8733597009070018 entropy 11.015539327544072
epoch: 57, step: 106
	action: tensor([[-21548.2712, -17195.7524,  47128.3550,  30765.4994,  37423.2824,
         -14071.8559,   4768.4032]], dtype=torch.float64)
	q_value: tensor([[-27.2327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6207826855837462, distance: 1.456864254304915 entropy 11.557664035034191
epoch: 57, step: 107
	action: tensor([[ -5151.2132, -10577.4304, -35174.5011,  -6719.3165,  -6563.1425,
          -9464.0521,  18844.4656]], dtype=torch.float64)
	q_value: tensor([[-21.8343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2951745040948721, distance: 1.3023293752211207 entropy 11.357859352099068
epoch: 57, step: 108
	action: tensor([[  3645.5597,  39987.0112, -45597.8768, -24598.5452,  -3556.5930,
          -3473.7280,  -1202.9361]], dtype=torch.float64)
	q_value: tensor([[-28.7024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48146267124394504, distance: 0.8240369885512366 entropy 11.602967014998805
epoch: 57, step: 109
	action: tensor([[-50541.7418, -18073.8255,   6415.8429,  -2219.7918,  17368.1040,
           9479.4982, -20673.7663]], dtype=torch.float64)
	q_value: tensor([[-32.8690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041293685871264474, distance: 1.1204680755803882 entropy 11.395198279729668
epoch: 57, step: 110
	action: tensor([[ -3478.9597, -18041.4331,  15483.9225,   -444.1367,  57246.3897,
          -5061.6174, -13714.1728]], dtype=torch.float64)
	q_value: tensor([[-26.7415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14326217672247532, distance: 1.0592065642095387 entropy 11.46015066129574
epoch: 57, step: 111
	action: tensor([[-29721.9548,  -5969.8316, -27039.3231,  45819.7465,  11251.6202,
          48208.8328, -14612.7473]], dtype=torch.float64)
	q_value: tensor([[-25.3834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5036610287550194, distance: 1.4032390607724914 entropy 11.54677405274939
epoch: 57, step: 112
	action: tensor([[ -955.5702, 28850.3723,  7157.4933, -7600.5255, 26367.3915, 13512.6470,
         -6257.9889]], dtype=torch.float64)
	q_value: tensor([[-24.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23244409894637796, distance: 1.0025630646048111 entropy 11.576088207356543
epoch: 57, step: 113
	action: tensor([[-28878.1041, -24387.8679,  10952.2651,  46660.1279, -19498.1663,
          -9163.2160,  53485.3750]], dtype=torch.float64)
	q_value: tensor([[-34.6921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.413806040557928, distance: 1.3606662407185277 entropy 11.560195266337379
epoch: 57, step: 114
	action: tensor([[-35124.5609, -37590.6694,   3742.0508,  37360.3411,    756.3866,
          35992.4854,  -6496.7444]], dtype=torch.float64)
	q_value: tensor([[-21.2681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08602157316653458, distance: 1.1925481401604825 entropy 11.4087565818868
epoch: 57, step: 115
	action: tensor([[-34018.2784, -30105.5086,  11505.8567,  -3098.2089,  24697.9425,
          43429.6966,  -2062.0031]], dtype=torch.float64)
	q_value: tensor([[-24.7869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9322289544719153, distance: 1.5906915942998388 entropy 11.569105697222847
epoch: 57, step: 116
	action: tensor([[ 11609.1337,  -8855.0012,  13009.9724,  30559.7909, -25274.5625,
           1367.9940,   9895.0640]], dtype=torch.float64)
	q_value: tensor([[-22.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.268021760194709, distance: 1.2886056953004277 entropy 11.324017124873658
epoch: 57, step: 117
	action: tensor([[-15647.5278,  -7471.4671, -42155.2260,   9737.8000,   3321.8625,
         -22571.2460,  10684.5766]], dtype=torch.float64)
	q_value: tensor([[-31.4811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4129981219104353, distance: 1.3602774091914596 entropy 11.604349507416396
epoch: 57, step: 118
	action: tensor([[-22722.6053,   1885.3688, -22244.6487, -13018.8390,  38485.2599,
          -1314.9846,  -4767.6574]], dtype=torch.float64)
	q_value: tensor([[-25.8026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5192273186655472, distance: 0.7934628251994784 entropy 11.746818159953195
epoch: 57, step: 119
	action: tensor([[ 15579.3310, -50901.9930,  27839.1496,  24684.1417,  29772.4373,
         -35603.4349,   6018.5405]], dtype=torch.float64)
	q_value: tensor([[-32.6074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.409560537096768
epoch: 57, step: 120
	action: tensor([[-7181.2521, 12227.9915, -2827.0481, -5519.8964, 18092.4574, 16568.3500,
         -7410.0165]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.015539327544072
epoch: 57, step: 121
	action: tensor([[-13092.2376,   8550.2799, -22007.3720,  13129.4094,   3340.7389,
         -17727.5251,   1519.6307]], dtype=torch.float64)
	q_value: tensor([[-27.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08499903103837747, distance: 1.1919865869110597 entropy 11.015539327544072
epoch: 57, step: 122
	action: tensor([[   303.8381,  -1931.0225,  -4733.7720, -22804.0757, -36226.6952,
         -27325.9547,  -3900.2259]], dtype=torch.float64)
	q_value: tensor([[-26.6599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03386314459741002, distance: 1.163558492082104 entropy 11.59201641431137
epoch: 57, step: 123
	action: tensor([[   114.6264, -37439.6493,   8665.8838, -11484.4754,  -5029.6767,
          32483.3002,  14342.6546]], dtype=torch.float64)
	q_value: tensor([[-25.1227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36516168432510476, distance: 0.9117762146155484 entropy 11.448196896102024
epoch: 57, step: 124
	action: tensor([[ -5372.8724, -63300.6890,  17515.8616,  23693.3521,   4612.8946,
          13495.5573,  -1793.3628]], dtype=torch.float64)
	q_value: tensor([[-28.9841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6276432748483403, distance: 1.4599443688719844 entropy 11.543482779293909
epoch: 57, step: 125
	action: tensor([[-19159.4360, -50321.6786, -23028.9526,   9074.1417, -15268.9113,
          30884.8198,  11643.6995]], dtype=torch.float64)
	q_value: tensor([[-24.0790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07175856046358331, distance: 1.1025217598376236 entropy 11.537966166397188
epoch: 57, step: 126
	action: tensor([[-28653.6928, -49654.9924, -42588.3197, -66997.8132, -33169.1984,
          -5052.7609,   2834.7191]], dtype=torch.float64)
	q_value: tensor([[-24.7225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003035415872701397, distance: 1.1426061536770364 entropy 11.60567786531274
epoch: 57, step: 127
	action: tensor([[-17278.3444, -14506.9337,  15179.2870, -20470.3662, -20437.7466,
           2499.8852,  10746.5270]], dtype=torch.float64)
	q_value: tensor([[-22.4984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0299898719818823, distance: 1.1270542672797157 entropy 11.317363968364102
LOSS epoch 57 actor 279.6246879135599 critic 333.08246823312606
epoch: 58, step: 0
	action: tensor([[-12011.4286,  13803.1500,  26127.2722, -11988.5792, -20649.2733,
          -7245.2396,  19833.7671]], dtype=torch.float64)
	q_value: tensor([[-30.3361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.55974768009527
epoch: 58, step: 1
	action: tensor([[ -9825.8447, -21835.0073,  -7296.4601, -38052.5419,  -1418.8119,
          10371.6998,   -296.9028]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45288049803650465, distance: 1.3793409837904893 entropy 11.055605692114154
epoch: 58, step: 2
	action: tensor([[-39011.9537,   5033.4080,  13739.2479,  19427.9208, -18314.3066,
         -12411.5836,  44999.9128]], dtype=torch.float64)
	q_value: tensor([[-31.4513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05473786048874785, distance: 1.1125840382383878 entropy 11.541301779382833
epoch: 58, step: 3
	action: tensor([[-25025.6901, -43564.7118, -24457.7443,  13207.8501,  20419.0571,
          24945.1478,  -5010.5817]], dtype=torch.float64)
	q_value: tensor([[-31.9478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1908259268428707, distance: 1.02938461432668 entropy 11.389649320548456
epoch: 58, step: 4
	action: tensor([[  6829.0247, -82025.1402, -18679.3728,   8581.6448,  -6578.8708,
         -68502.5167,  17716.9504]], dtype=torch.float64)
	q_value: tensor([[-29.3375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03471184027140117, distance: 1.1243076943920884 entropy 11.688789758933481
epoch: 58, step: 5
	action: tensor([[-51775.8877, -37274.8416,   8125.5398,  -5175.4937, -31270.6514,
           8327.3048,  -5267.3191]], dtype=torch.float64)
	q_value: tensor([[-30.2418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7460733025987858, distance: 1.512125820432583 entropy 11.403508339319258
epoch: 58, step: 6
	action: tensor([[-15195.1671, -25279.8716,  30311.2670,  39656.1807,  15550.3080,
          16950.0965,  21295.2546]], dtype=torch.float64)
	q_value: tensor([[-25.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08209717392544613, distance: 1.1903915206790263 entropy 11.341222256344016
epoch: 58, step: 7
	action: tensor([[-55129.5138, -35743.9348,  15663.2714,   3127.8681, -21358.6683,
         -27920.1434,  15646.9991]], dtype=torch.float64)
	q_value: tensor([[-29.2546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05891826742830064, distance: 1.1775732008861968 entropy 11.672044806825422
epoch: 58, step: 8
	action: tensor([[ 3203.6383, -3034.1263,  4280.3151, 49525.8139,  9898.0418, 15568.4991,
         50596.7633]], dtype=torch.float64)
	q_value: tensor([[-26.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.65664508589801
epoch: 58, step: 9
	action: tensor([[ -2776.2435, -12189.8960,  11152.0945,  -2966.5012,  -8618.9935,
          14608.0536,   4947.1030]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004174432801412387, distance: 1.1419532620366553 entropy 11.055605692114154
epoch: 58, step: 10
	action: tensor([[-10492.7480, -22756.5402,  -2105.9150,  -6302.2931,   2064.8244,
         -20017.0132, -28624.2702]], dtype=torch.float64)
	q_value: tensor([[-29.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2707701815581025, distance: 1.290001457778482 entropy 11.472318653098515
epoch: 58, step: 11
	action: tensor([[-40219.5516, -17748.8468, -57977.5401,  37868.3140,  -8962.3885,
          54756.2817,    353.8949]], dtype=torch.float64)
	q_value: tensor([[-32.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7929549006944923, distance: 1.5322914422454958 entropy 11.680406067506437
epoch: 58, step: 12
	action: tensor([[ -1376.1321,   7109.6450, -14445.9847,  18924.0868,  12641.8605,
         -40386.9835, -18400.5551]], dtype=torch.float64)
	q_value: tensor([[-30.4750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31735852793653097, distance: 0.9454814832613123 entropy 11.695689654104843
epoch: 58, step: 13
	action: tensor([[ 52862.4674, -40098.9395,  -3237.2088,   7570.6293,  19184.3986,
          -1662.5118,  16879.3190]], dtype=torch.float64)
	q_value: tensor([[-30.8556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0073972189275202105, distance: 1.1485689381541702 entropy 11.602336935792033
epoch: 58, step: 14
	action: tensor([[   936.1813, -37296.7010,  26870.0919, -16225.5590, -10998.5694,
          -4763.5973,   4461.9526]], dtype=torch.float64)
	q_value: tensor([[-31.6115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17112714003268747, distance: 1.238393648850111 entropy 11.615764215591396
epoch: 58, step: 15
	action: tensor([[ -8472.4370, -27667.4872, -21303.6542,  25551.3041,  19030.5964,
         -26475.8064, -12308.9691]], dtype=torch.float64)
	q_value: tensor([[-31.1129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3502129274563546, distance: 1.329712722850518 entropy 11.68739803936075
epoch: 58, step: 16
	action: tensor([[-20173.5211,  -9371.1073,   2052.5066,  15738.0312, -24680.1648,
         -40007.2630, -17427.6654]], dtype=torch.float64)
	q_value: tensor([[-27.1976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4976862051338198, distance: 1.4004483882365 entropy 11.524472700483082
epoch: 58, step: 17
	action: tensor([[ 21802.3832, -36900.0151,  12918.7579,   2039.7352,  12387.5442,
          19804.4909, -83241.1174]], dtype=torch.float64)
	q_value: tensor([[-27.3431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3865778427144311, distance: 0.8962649703241445 entropy 11.623674051838524
epoch: 58, step: 18
	action: tensor([[-29427.5900, -26199.4019,   8552.3513,   7945.8024,  20438.5403,
          -8044.2517,  38644.6159]], dtype=torch.float64)
	q_value: tensor([[-27.9849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20170581467938642, distance: 1.2544569864559276 entropy 11.464998385011857
epoch: 58, step: 19
	action: tensor([[-12657.7705, -80999.7318,  32907.4373,  34281.7356,  -1408.6793,
         -13156.8874, -35368.3390]], dtype=torch.float64)
	q_value: tensor([[-26.7257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14843031777270443, distance: 1.2263347019560518 entropy 11.55435619752608
epoch: 58, step: 20
	action: tensor([[  2393.7654,  10481.9393,  23278.2729,  39812.0267, -70467.7602,
           1278.1263,  26522.5638]], dtype=torch.float64)
	q_value: tensor([[-27.8052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.584903593915785
epoch: 58, step: 21
	action: tensor([[-2229.8060,  7017.3441, -4655.9277, 37426.6067, -7301.0816, -4860.1752,
         -6961.0164]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5429579456440697, distance: 0.7736326226502471 entropy 11.055605692114154
epoch: 58, step: 22
	action: tensor([[  8685.8727, -88752.2434, -10672.6478,  39045.1619, -16412.9072,
         -18412.6003,  -7272.5212]], dtype=torch.float64)
	q_value: tensor([[-27.8809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.33947222495173
epoch: 58, step: 23
	action: tensor([[-47334.2590,  26042.2994, -23067.4950,   8686.0475, -26171.7831,
         -17090.2293, -15966.6142]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.055605692114154
epoch: 58, step: 24
	action: tensor([[-16654.8522, -29376.0446,  -5994.9535,  19267.1174, -15650.7003,
          22455.8949,   2962.1187]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.055605692114154
epoch: 58, step: 25
	action: tensor([[ 1.0925e+03, -2.4028e+04, -8.9067e+03, -1.0330e+04, -6.2469e+03,
          2.2986e+01, -4.1154e+03]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.055605692114154
epoch: 58, step: 26
	action: tensor([[ 13549.7377, -18663.5152,   9668.7478,  28229.8743,  13002.6669,
          -8535.1634,  -4734.8359]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2826601423219989, distance: 0.9692128744453222 entropy 11.055605692114154
epoch: 58, step: 27
	action: tensor([[  4558.3201,  -7927.4875, -46600.3983,  38701.7965, -32772.9390,
          21428.1764,  40502.4303]], dtype=torch.float64)
	q_value: tensor([[-31.6392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27951677954891985, distance: 0.971334084784875 entropy 11.588025983156381
epoch: 58, step: 28
	action: tensor([[-17410.7938, -15435.1497, -15233.5259,  29485.8702,   9593.9145,
         -18255.4182, -30756.4446]], dtype=torch.float64)
	q_value: tensor([[-31.4697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08598122387518314, distance: 1.1925259864019866 entropy 11.328093231293
epoch: 58, step: 29
	action: tensor([[-18143.9650,  13156.4555, -18011.7753,   9727.7639,  22681.8557,
          27375.5411,   9066.3336]], dtype=torch.float64)
	q_value: tensor([[-26.8463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40171149910868087, distance: 0.8851401099428793 entropy 11.456703910964707
epoch: 58, step: 30
	action: tensor([[  1194.4386, -22887.4653,  27415.1038,  13727.7855,  22859.6887,
           2248.3199,  -2591.4323]], dtype=torch.float64)
	q_value: tensor([[-27.0135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03645565666067219, distance: 1.1650164465240584 entropy 11.413669153614032
epoch: 58, step: 31
	action: tensor([[  3889.5956,   8922.5318, -11439.2503,  23044.8083,  -4817.7358,
          -2184.3204,  -5316.2703]], dtype=torch.float64)
	q_value: tensor([[-31.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18932982282266786, distance: 1.030335803508654 entropy 11.508288574410122
epoch: 58, step: 32
	action: tensor([[ 10850.5285,   5316.0265,  -4266.4619,    729.4852,   5126.1276,
         -19630.5197,    506.4024]], dtype=torch.float64)
	q_value: tensor([[-32.9922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2588096697378588, distance: 0.9851935631158415 entropy 11.399529618118239
epoch: 58, step: 33
	action: tensor([[-46733.1727,  -9075.3113,  11927.1401,  31724.3199, -29327.3661,
          25389.3803,   8441.8548]], dtype=torch.float64)
	q_value: tensor([[-29.5890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002933998796306403, distance: 1.1460217767706729 entropy 11.373546923838216
epoch: 58, step: 34
	action: tensor([[ -7760.0671, -13939.5333, -80124.1411,  -7294.7472,  38617.6480,
           6031.6015,  28475.4730]], dtype=torch.float64)
	q_value: tensor([[-31.2780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49662385847024937, distance: 1.3999516134154486 entropy 11.729778118770472
epoch: 58, step: 35
	action: tensor([[-14042.7503,  -4540.1204,   5877.9362, -32584.4279,  26835.1851,
          14845.1473,  13751.1989]], dtype=torch.float64)
	q_value: tensor([[-25.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3374367828977758, distance: 1.323406687417318 entropy 11.350619597229302
epoch: 58, step: 36
	action: tensor([[-34321.4315,  22348.8148,  21360.0120,  16427.1090, -40896.7270,
          27133.5151,  -1284.4725]], dtype=torch.float64)
	q_value: tensor([[-33.9814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17336938543464786, distance: 1.2395785990279362 entropy 11.64732859352225
epoch: 58, step: 37
	action: tensor([[-22182.4909,  28219.2497, -22817.8991,  24055.1763, -19120.3101,
          11102.7937,   3531.7232]], dtype=torch.float64)
	q_value: tensor([[-36.8908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41786266134970507, distance: 0.8731109226983067 entropy 11.621922447296793
epoch: 58, step: 38
	action: tensor([[ 27038.8188, -42522.5159, -19938.6083,  33156.9792,  -3175.2789,
           6729.9715, -14754.5889]], dtype=torch.float64)
	q_value: tensor([[-29.6275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21298360388179716, distance: 1.260329664778405 entropy 11.554337386739865
epoch: 58, step: 39
	action: tensor([[-25865.2241,    932.1147, -27190.9280,  24358.0462, -14403.4703,
         -21563.7812,  27418.6663]], dtype=torch.float64)
	q_value: tensor([[-31.9100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011661634145840871, distance: 1.1509973756673948 entropy 11.843014981504458
epoch: 58, step: 40
	action: tensor([[ -5150.7428, -20642.9581,  20551.2614, -23263.1321,  42391.6466,
         -10393.3860, -48949.6546]], dtype=torch.float64)
	q_value: tensor([[-30.6267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41393234176523985, distance: 1.360727016364581 entropy 11.676937137357909
epoch: 58, step: 41
	action: tensor([[ -9750.9310, -56323.0160,  18639.5145,  20111.6527, -39747.8413,
          21719.4903, -20521.9550]], dtype=torch.float64)
	q_value: tensor([[-28.5905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26990421614159266, distance: 0.9777923013367317 entropy 11.544242098763034
epoch: 58, step: 42
	action: tensor([[-11921.9013,  -7067.4884, -11451.2205,  31934.4885, -69189.2289,
         -37477.1764,  31237.5752]], dtype=torch.float64)
	q_value: tensor([[-28.0461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17778908323892695, distance: 1.0376438410260538 entropy 11.67445757479019
epoch: 58, step: 43
	action: tensor([[  8413.0402,  20709.2571,  -8658.6891,  22133.7031, -13302.2150,
         -14958.1040,   4982.5358]], dtype=torch.float64)
	q_value: tensor([[-31.7750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.680530016628433
epoch: 58, step: 44
	action: tensor([[-20035.1564,  -3219.3551,  25146.3127,  -1728.7225,   6084.0281,
          -2651.3247,  15120.4457]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18072124368378528, distance: 1.2434558843445962 entropy 11.055605692114154
epoch: 58, step: 45
	action: tensor([[-5.0740e+01, -5.7712e+04, -7.9122e+03,  2.4669e+04, -8.1378e+03,
          5.8962e+04,  6.2789e+04]], dtype=torch.float64)
	q_value: tensor([[-28.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42929407752389204, distance: 1.3680988894134856 entropy 11.528890114570599
epoch: 58, step: 46
	action: tensor([[-18210.5381, -14977.3792,   4267.5055,  25686.7767,  25582.1265,
          -4740.2875,  -7553.0537]], dtype=torch.float64)
	q_value: tensor([[-23.7050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10607166779663779, distance: 1.0819521251807762 entropy 11.35969462283394
epoch: 58, step: 47
	action: tensor([[-16320.4944, -33233.3216, -14920.4941,  49695.0911,     56.6972,
          20561.0349,  12460.1758]], dtype=torch.float64)
	q_value: tensor([[-28.3521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2515987529803684, distance: 0.9899743550791117 entropy 11.703626602409797
epoch: 58, step: 48
	action: tensor([[  9317.6534,  -2675.6247,   1746.4184,  23097.6594, -19132.2731,
         -18124.4888, -33212.2870]], dtype=torch.float64)
	q_value: tensor([[-25.5942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4513027013196508, distance: 0.8476627608129839 entropy 11.452838833854186
epoch: 58, step: 49
	action: tensor([[ 15177.8668, -25663.2907,  20109.2099,  -2990.9137, -14407.4630,
          -3024.3012,  31776.4446]], dtype=torch.float64)
	q_value: tensor([[-26.6749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7719087396228517e-05, distance: 1.1443543923230137 entropy 11.27540430461726
epoch: 58, step: 50
	action: tensor([[ 18540.4504,  -4721.5107,  -8506.1095,  23277.2982, -24281.5547,
         -11184.3650, -22857.1571]], dtype=torch.float64)
	q_value: tensor([[-28.2918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44529136370982014, distance: 0.852293462117746 entropy 11.475815519959438
epoch: 58, step: 51
	action: tensor([[-18543.4721, -27065.5113,    968.5903, -10007.9263,  57610.7827,
          52192.2783,  -4848.9314]], dtype=torch.float64)
	q_value: tensor([[-34.8560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.532120188918765, distance: 1.4164560735983478 entropy 11.496191565067772
epoch: 58, step: 52
	action: tensor([[-21695.8541,  12379.2473, -18623.4762,   9002.3383,  -9752.6309,
          13278.6578,  12921.4941]], dtype=torch.float64)
	q_value: tensor([[-22.4917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052307707556967564, distance: 1.11401327872507 entropy 11.199565117896515
epoch: 58, step: 53
	action: tensor([[ -8649.1780, -39197.5448, -44855.0656,  21888.4140,  24164.7944,
          54640.7099,  -3399.1787]], dtype=torch.float64)
	q_value: tensor([[-30.9343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018941163795608285, distance: 1.155131021103721 entropy 11.625523243590113
epoch: 58, step: 54
	action: tensor([[-29393.6318, -30380.1779,   4317.6967,   -105.4916, -24189.1786,
           3733.9104,  15428.1606]], dtype=torch.float64)
	q_value: tensor([[-25.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11176116884263876, distance: 1.0785035300426973 entropy 11.47100155998704
epoch: 58, step: 55
	action: tensor([[ 11024.7977,  -9318.6518,  17390.5224,  -3212.5343,  12423.4364,
          -2068.5832, -12440.8822]], dtype=torch.float64)
	q_value: tensor([[-26.5748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27961273878086734, distance: 1.2944818654411918 entropy 11.520581103414212
epoch: 58, step: 56
	action: tensor([[-12827.4781,  -9293.9238,  -6823.7813,  28996.3581,  -3207.4976,
          -1382.9355,  38865.2213]], dtype=torch.float64)
	q_value: tensor([[-27.3893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07232740598200027, distance: 1.1850055819020235 entropy 11.260700562755613
epoch: 58, step: 57
	action: tensor([[ 17849.3308, -39349.0203,  25248.5427, -46194.9491,  -6564.5010,
         -26251.6047,  13343.6768]], dtype=torch.float64)
	q_value: tensor([[-28.9433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35992929968418075, distance: 0.9155259677411842 entropy 11.707897742308928
epoch: 58, step: 58
	action: tensor([[  7995.1353,   7977.6965,  -5880.7566,   4560.6245, -19869.5214,
           8933.2717,  32160.4430]], dtype=torch.float64)
	q_value: tensor([[-28.8057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5600633255186375, distance: 0.7590174788894064 entropy 11.297755346111947
epoch: 58, step: 59
	action: tensor([[-34751.3291, -15114.4722,  29964.9824,  28884.3886, -16867.6525,
         -25239.0812, -41608.4996]], dtype=torch.float64)
	q_value: tensor([[-32.7388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13294563085552114, distance: 1.0655647834452873 entropy 11.573359865470199
epoch: 58, step: 60
	action: tensor([[-11307.4078,  11567.1145, -30381.5280,  14705.4854, -19377.7596,
         -22261.3738, -15569.1445]], dtype=torch.float64)
	q_value: tensor([[-29.3707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1031341873860574, distance: 1.2019070021078944 entropy 11.55186357416763
epoch: 58, step: 61
	action: tensor([[-58911.8704,  47921.6182, -28446.2595,  45117.1682,    154.2364,
         -37583.7151,   1110.7128]], dtype=torch.float64)
	q_value: tensor([[-30.3620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994441012496174, distance: 0.9578071845586851 entropy 11.493988517370484
epoch: 58, step: 62
	action: tensor([[-19830.3719,  -9815.2846, -49045.1729,   4597.9870, -21886.8121,
         -12236.4674,  75652.1217]], dtype=torch.float64)
	q_value: tensor([[-34.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007939226829033252, distance: 1.1488778777241986 entropy 11.739930654945402
epoch: 58, step: 63
	action: tensor([[39399.0101,  7579.2271,  -209.7260, 60052.3829,  4607.4747, 34526.5038,
          3051.7297]], dtype=torch.float64)
	q_value: tensor([[-31.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.804736991800471
epoch: 58, step: 64
	action: tensor([[ -8771.0574,   8055.2070, -16243.9957,  20735.2051, -12631.3805,
          -1121.3944, -25014.2335]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2812676441594997, distance: 0.9701531350443359 entropy 11.055605692114154
epoch: 58, step: 65
	action: tensor([[-48743.3864, -33120.1525, -32890.2871,  29285.8568, -16225.6702,
          24224.8138,  31196.2625]], dtype=torch.float64)
	q_value: tensor([[-32.7898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15811936254237668, distance: 1.23149698971302 entropy 11.632891106063166
epoch: 58, step: 66
	action: tensor([[-33267.8578, -13117.5150,  32511.2895, -17593.6269, -39098.9498,
          59044.4015, -52776.6214]], dtype=torch.float64)
	q_value: tensor([[-30.0971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6472276285127974, distance: 1.4687013777066034 entropy 11.711944761646063
epoch: 58, step: 67
	action: tensor([[ -8554.1079, -47020.4468,  63154.7213,  21330.3539, -17196.8910,
          53723.3886, -15740.6581]], dtype=torch.float64)
	q_value: tensor([[-28.1012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08905267851570109, distance: 1.09220289883165 entropy 11.504392120699586
epoch: 58, step: 68
	action: tensor([[-44367.5126,   -323.9150, -27216.2809,    446.1877,  30025.1712,
          40405.2651,  15568.2009]], dtype=torch.float64)
	q_value: tensor([[-28.4941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1853797956496257, distance: 1.2459065056624419 entropy 11.608972599759765
epoch: 58, step: 69
	action: tensor([[  5354.0534, -51172.2982, -17446.5692,  13548.9422,  11739.5662,
          13978.2505, -22610.3603]], dtype=torch.float64)
	q_value: tensor([[-29.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12116823237333474, distance: 1.2116915664995207 entropy 11.692064468102087
epoch: 58, step: 70
	action: tensor([[-14674.6288, -18012.0759,  -2165.8245,  -8970.4271,   3845.0286,
         -22879.3162, -40885.0215]], dtype=torch.float64)
	q_value: tensor([[-36.3063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6245668645988345, distance: 1.4585639951157103 entropy 11.658634323262584
epoch: 58, step: 71
	action: tensor([[ 11557.2324, -24367.3854,  24794.2744,  20004.0191, -33310.6747,
          35227.1626,  25266.6576]], dtype=torch.float64)
	q_value: tensor([[-28.0548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2800872392160674, distance: 0.970949470245474 entropy 11.50501099137686
epoch: 58, step: 72
	action: tensor([[-10145.2492, -58345.3831,  14206.7595,  26697.3319,   6016.7045,
         -21392.7006,  -9586.9042]], dtype=torch.float64)
	q_value: tensor([[-28.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.412588503297258
epoch: 58, step: 73
	action: tensor([[-14336.7154, -19728.3486,  -4618.7273,  14478.8273,  23860.7354,
         -12174.3636,   5327.7664]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.055605692114154
epoch: 58, step: 74
	action: tensor([[ 15420.6619,  11429.6727,   6723.0946, -22699.3636,  -2696.2809,
           1595.8055,  21923.9385]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.055605692114154
epoch: 58, step: 75
	action: tensor([[-31448.1874,  -8442.2100, -22711.6666,  32232.7745,   4834.2529,
          31571.5519,  38736.7901]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2813509564182006, distance: 1.2953607749474736 entropy 11.055605692114154
epoch: 58, step: 76
	action: tensor([[ 27628.0628, -50018.4203,  -8562.8320,  17410.3681,   4875.7561,
         -16698.1854,  -8565.9488]], dtype=torch.float64)
	q_value: tensor([[-31.1523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21047719709892299, distance: 1.0168081818459256 entropy 11.68661686227666
epoch: 58, step: 77
	action: tensor([[  7037.5732, -26005.0338, -49557.4455, -32013.4627,  10361.3244,
         -31770.5263,  23892.9749]], dtype=torch.float64)
	q_value: tensor([[-33.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19765979575943216, distance: 1.2523433925692307 entropy 11.528623234618568
epoch: 58, step: 78
	action: tensor([[-20928.8665,   4874.1181, -24623.3832,  22244.0306,  13395.1722,
          -5335.8037,  11259.4103]], dtype=torch.float64)
	q_value: tensor([[-28.3390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.256287899830367
epoch: 58, step: 79
	action: tensor([[ 25251.0812,  -1606.8656, -17918.2202,   5322.3802,    402.7518,
           6605.2483, -20497.3031]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33940645234443, distance: 0.930087605584572 entropy 11.055605692114154
epoch: 58, step: 80
	action: tensor([[-20912.0510,  10043.6271,  -4395.1905,   7782.6416,   2050.4410,
          18071.6391,   -183.0286]], dtype=torch.float64)
	q_value: tensor([[-29.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6470194836056116, distance: 0.6798796784374924 entropy 11.303931789281537
epoch: 58, step: 81
	action: tensor([[-10135.5509, -11886.4943,  -7287.7903,    931.2946,  -4276.5524,
           4711.1123,  17490.4323]], dtype=torch.float64)
	q_value: tensor([[-29.0825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.446466722826315
epoch: 58, step: 82
	action: tensor([[  2534.2034,  -9810.6440,   1017.2426,  -6748.2225,  14948.2524,
         -21016.9746, -15510.7506]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.055605692114154
epoch: 58, step: 83
	action: tensor([[ -8967.8100,  -5124.9145,  -5030.4594,  -1642.2937,  -8035.7094,
          18978.8542, -15555.4334]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9797838329182442, distance: 1.6101471957433475 entropy 11.055605692114154
epoch: 58, step: 84
	action: tensor([[-22353.2573,  -7445.9563, -12503.8813,   9434.7364,   2634.3036,
         -19757.4763,  37749.2984]], dtype=torch.float64)
	q_value: tensor([[-30.2842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9347091640444831, distance: 1.5917121729666284 entropy 11.574335297780687
epoch: 58, step: 85
	action: tensor([[-23033.9686,  -6159.2904,   3447.1689,  23892.0111, -15799.5370,
          41552.6879,  18967.3889]], dtype=torch.float64)
	q_value: tensor([[-24.8541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21411091935328175, distance: 1.260915187603242 entropy 11.346623735401835
epoch: 58, step: 86
	action: tensor([[ -3217.5689, -16335.1244,  43827.9934,  -5874.0977, -17124.6488,
           1121.2578,  -1705.2605]], dtype=torch.float64)
	q_value: tensor([[-30.4022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7765214386289059, distance: 1.525253111700867 entropy 11.703582777293226
epoch: 58, step: 87
	action: tensor([[  9622.5714, -68796.4236,  45372.2013,  47124.6639,  -6833.3793,
          19555.7998, -46262.1354]], dtype=torch.float64)
	q_value: tensor([[-32.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017035351332280402, distance: 1.1345552318572099 entropy 11.725332754770523
epoch: 58, step: 88
	action: tensor([[-25415.7673, -56552.3677, -30586.6732,  10513.8318, -22275.7902,
          43581.3879,   -837.3923]], dtype=torch.float64)
	q_value: tensor([[-34.0815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7809980675276413, distance: 1.527173633452655 entropy 11.53302845101596
epoch: 58, step: 89
	action: tensor([[  9022.5579, -46809.2747,  10274.5627,  38389.7654,  -7743.1629,
         -25714.1245,  13889.6824]], dtype=torch.float64)
	q_value: tensor([[-26.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16152667473269333, distance: 1.0478553103089696 entropy 11.534587246316507
epoch: 58, step: 90
	action: tensor([[-19402.4088, -17237.8436, -21636.6461, -19102.6889, -28662.8736,
         -21107.2471,  36076.7032]], dtype=torch.float64)
	q_value: tensor([[-30.2799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6225717963609492, distance: 1.4576681167413594 entropy 11.575669962604382
epoch: 58, step: 91
	action: tensor([[ 27536.4798, -48314.6013,  -5596.1976,  40153.8165,  17427.6982,
           1494.0865,  -6762.3353]], dtype=torch.float64)
	q_value: tensor([[-27.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03694308189892892, distance: 1.1230075368035972 entropy 11.476189007935403
epoch: 58, step: 92
	action: tensor([[-47993.3684,  -4164.8414,  -7955.2565,  -5067.2974,  52813.8260,
           7749.1615, -31682.2865]], dtype=torch.float64)
	q_value: tensor([[-32.6389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4797610099151479, distance: 1.3920424630246244 entropy 11.613322532143497
epoch: 58, step: 93
	action: tensor([[ 32337.1383,   1778.8133,  15770.2237,  -8449.2048, -88936.4226,
          24199.9845, -26707.3795]], dtype=torch.float64)
	q_value: tensor([[-28.6505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4439257762354655, distance: 0.8533419096872051 entropy 11.589881949753138
epoch: 58, step: 94
	action: tensor([[-29335.0659,  15232.6474, -10670.7583,  15515.8442, -33058.9156,
          39251.9274,   1860.6999]], dtype=torch.float64)
	q_value: tensor([[-36.9498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34449174543692196, distance: 0.9265007511446235 entropy 11.48119229714933
epoch: 58, step: 95
	action: tensor([[-29423.8543,  -7397.7077, -43846.8399,  -7339.2813,  15315.4596,
          23767.7338,  -7065.2112]], dtype=torch.float64)
	q_value: tensor([[-37.1044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09808883308846295, distance: 1.08677233724401 entropy 11.6163003677234
epoch: 58, step: 96
	action: tensor([[  8713.5319,   2430.5388, -34088.2430,  17111.6577, -41396.4737,
           3814.2679,   6111.6494]], dtype=torch.float64)
	q_value: tensor([[-38.9182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.822367760185411
epoch: 58, step: 97
	action: tensor([[  6678.7644,  -8631.5559, -31038.4125,   7297.4165, -27446.4089,
         -15341.0270,   6279.7722]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31297556354193357, distance: 0.9485119035916671 entropy 11.055605692114154
epoch: 58, step: 98
	action: tensor([[-58199.1218, -16395.2605,  17543.5850,  20717.5995, -19196.2673,
           4170.8441,   3340.3129]], dtype=torch.float64)
	q_value: tensor([[-26.5606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.37612186737984
epoch: 58, step: 99
	action: tensor([[-24097.9858, -12865.5676, -29022.4449,  -7070.3055, -10960.0317,
           8305.2706, -24657.4008]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14221812635600495, distance: 1.2230134050317483 entropy 11.055605692114154
epoch: 58, step: 100
	action: tensor([[  5223.3033, -21556.9797, -29509.5314, -18892.4647, -26085.6601,
          13028.3322,  33939.1905]], dtype=torch.float64)
	q_value: tensor([[-29.3616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08692628364542976, distance: 1.0934769027495832 entropy 11.460458698188374
epoch: 58, step: 101
	action: tensor([[-45447.3500, -32343.5401, -17409.1958,  81073.8137,  14035.4174,
            795.0099, -34871.7044]], dtype=torch.float64)
	q_value: tensor([[-36.1362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22676205069785627, distance: 1.2674675804779656 entropy 11.571741298189364
epoch: 58, step: 102
	action: tensor([[  5502.4519, -14891.8219,  -5921.4605,  -8246.5519, -37910.4185,
           3633.1704,  -6807.9460]], dtype=torch.float64)
	q_value: tensor([[-23.7414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47296427179190514, distance: 0.8307621889591048 entropy 11.324037636998924
epoch: 58, step: 103
	action: tensor([[-28136.2177, -63567.9082,  17967.3271, -26675.2090,  44135.4176,
          39035.7135, -16409.0776]], dtype=torch.float64)
	q_value: tensor([[-36.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0249819749966258, distance: 1.158550068742816 entropy 11.737979387241518
epoch: 58, step: 104
	action: tensor([[-63033.3023, -24683.3072, -59495.9497, -54917.0734,  65014.5648,
          15517.5686,  -4335.4573]], dtype=torch.float64)
	q_value: tensor([[-35.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015652344044503996, distance: 1.135353097082266 entropy 11.735839079474077
epoch: 58, step: 105
	action: tensor([[  6168.5143, -38083.2967,  24413.9039,  12858.1210,   6000.8620,
           5575.8944,  -3676.1825]], dtype=torch.float64)
	q_value: tensor([[-31.8086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06284699376659986, distance: 1.107801489143208 entropy 11.577040709734748
epoch: 58, step: 106
	action: tensor([[-33073.6650,  19865.2983,  22854.1391,  44090.6942, -25461.1315,
           1377.7937, -34475.4396]], dtype=torch.float64)
	q_value: tensor([[-26.2153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6497585594855464, distance: 0.6772366589126029 entropy 11.376564033068636
epoch: 58, step: 107
	action: tensor([[-23827.9158,   8826.1947,   5629.8656,  10205.3773, -30140.9560,
         -18733.0254, -12237.3834]], dtype=torch.float64)
	q_value: tensor([[-28.2414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.531835349339351
epoch: 58, step: 108
	action: tensor([[-15891.2958,  -6421.5392,  -5697.2464,  11840.8108,   7679.3672,
          -1555.3217,    337.6955]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1547916810253427, distance: 1.2297264564207566 entropy 11.055605692114154
epoch: 58, step: 109
	action: tensor([[ 10024.3885, -58576.6777,  10183.3753,  -4969.2160,    -74.3408,
         -14373.5515,  -5925.1785]], dtype=torch.float64)
	q_value: tensor([[-26.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10999534905636565, distance: 1.2056389575758062 entropy 11.579742911759805
epoch: 58, step: 110
	action: tensor([[ 20269.9838,  12004.5958, -13944.4803,  15854.8783,   2407.6715,
          14110.0801,  -3622.3917]], dtype=torch.float64)
	q_value: tensor([[-24.9003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5180262502805074, distance: 0.7944533230940256 entropy 11.242351529206713
epoch: 58, step: 111
	action: tensor([[ -4793.5785, -46467.4364,  44063.8365,  -6946.5969, -17769.0743,
          12160.8605,   5156.2555]], dtype=torch.float64)
	q_value: tensor([[-30.6391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.456804893417157
epoch: 58, step: 112
	action: tensor([[  2166.4722, -20571.1764, -11307.5729,  25277.0269,   7817.5727,
          44364.9756,  11585.1612]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.055605692114154
epoch: 58, step: 113
	action: tensor([[-22342.7268, -20469.8720, -12147.0746,  -4812.1197,  -7838.4878,
          24446.3511,   4462.9804]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09124888767465755, distance: 1.195414722684379 entropy 11.055605692114154
epoch: 58, step: 114
	action: tensor([[-15374.5040,  -1506.2292,  -3291.9042,   7714.3899, -16967.2684,
         -25049.6280, -25530.3661]], dtype=torch.float64)
	q_value: tensor([[-33.5869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49208504832381994, distance: 1.3978271853568682 entropy 11.65863878215653
epoch: 58, step: 115
	action: tensor([[-46900.7638, -41331.5219,   8177.5789,  10212.0572, -51813.8011,
          25932.6269,  16489.7219]], dtype=torch.float64)
	q_value: tensor([[-28.2448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3135354261719838, distance: 1.3115280650803707 entropy 11.637394626460685
epoch: 58, step: 116
	action: tensor([[-56000.1375,  -8340.2661, -12832.5105,  95079.2723,  19502.0150,
          54353.8143,  49957.0360]], dtype=torch.float64)
	q_value: tensor([[-28.8350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14488094688454156, distance: 1.0582054279091992 entropy 11.73052739798934
epoch: 58, step: 117
	action: tensor([[-59431.9649, -13071.5403, -19054.7495,  26060.3491,   4093.7610,
         -58701.8859, -10516.2990]], dtype=torch.float64)
	q_value: tensor([[-28.3387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08939700208349921, distance: 1.0919964616788298 entropy 11.681689797384971
epoch: 58, step: 118
	action: tensor([[ 23074.3700, -75211.6372,   8079.7491, -14958.9151,  83216.5781,
          30871.4675,  13415.3449]], dtype=torch.float64)
	q_value: tensor([[-29.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30726082084473316, distance: 1.3083918070133365 entropy 11.764234678586803
epoch: 58, step: 119
	action: tensor([[-25866.1421,   7185.2802, -14506.9392,    984.2429, -12363.5577,
         -15776.6839,   8703.4943]], dtype=torch.float64)
	q_value: tensor([[-27.1273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.230515026179388
epoch: 58, step: 120
	action: tensor([[-23488.8938,  -4396.5333, -24467.1409,  11712.7858,   1587.8386,
          -4176.8237,  16793.7464]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24373276795602705, distance: 1.2762043822344489 entropy 11.055605692114154
epoch: 58, step: 121
	action: tensor([[-16094.3546, -21762.8415,  26400.0851, -22332.7220, -20023.3474,
         -48748.7522,   7610.2419]], dtype=torch.float64)
	q_value: tensor([[-26.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07246180288755533, distance: 1.1021040414788512 entropy 11.539443950666278
epoch: 58, step: 122
	action: tensor([[ 20416.6373, -46912.4519, -34608.6543,   3969.5071,  -8780.3730,
         -37957.7935, -38434.1038]], dtype=torch.float64)
	q_value: tensor([[-27.4310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5294682434707937, distance: 0.7849665728534282 entropy 11.427871149967194
epoch: 58, step: 123
	action: tensor([[ -1244.5166, -62657.9156,   -802.0712,  12106.9851, -22055.1159,
         -20327.9745,  43542.7687]], dtype=torch.float64)
	q_value: tensor([[-27.8662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10217003980927608, distance: 1.0843106915367169 entropy 11.551678754736118
epoch: 58, step: 124
	action: tensor([[ -9398.2873, -23724.3286,   1538.2386,  24517.6997,  -4256.8680,
         -10607.8084,  13375.0727]], dtype=torch.float64)
	q_value: tensor([[-25.3142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13380145457210435, distance: 1.0650387721663392 entropy 11.448368898410536
epoch: 58, step: 125
	action: tensor([[  9193.7197,  -6373.7852, -42464.0095, -13289.2946,  22621.0977,
            192.1659, -24995.1547]], dtype=torch.float64)
	q_value: tensor([[-29.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3310891054052648, distance: 0.9359245255874248 entropy 11.706842581492083
epoch: 58, step: 126
	action: tensor([[-33516.7083,   9666.9724, -22255.8565, -22706.8737,  63238.9125,
         -54366.5978,   9603.7644]], dtype=torch.float64)
	q_value: tensor([[-27.3868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.339271405944633
epoch: 58, step: 127
	action: tensor([[-40402.4482, -14538.5866,  -5331.3048,  -3202.6725,   1348.3071,
          16318.0649,   9337.8325]], dtype=torch.float64)
	q_value: tensor([[-31.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18110190407311555, distance: 1.0355513136420003 entropy 11.055605692114154
LOSS epoch 58 actor 354.4191853077192 critic 220.67565608718755
epoch: 59, step: 0
	action: tensor([[ 24518.8779, -58292.5463, -41438.3695,  -3107.1264, -28025.7658,
          38928.2315, -15813.3799]], dtype=torch.float64)
	q_value: tensor([[-37.8729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27980518107753893, distance: 0.9711396581374692 entropy 11.713268240857037
epoch: 59, step: 1
	action: tensor([[ 21161.7935, -47750.8573,  -3570.9785,  13070.0685,  16646.5395,
         -58948.9475,  15442.6989]], dtype=torch.float64)
	q_value: tensor([[-36.8958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19981193812254827, distance: 1.2534680907371807 entropy 11.584729959177752
epoch: 59, step: 2
	action: tensor([[  5804.1839, -53609.5148, -18616.9326,  31349.5603,  43180.2494,
          -9226.3369,  39929.6581]], dtype=torch.float64)
	q_value: tensor([[-34.9571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37905736925156064, distance: 0.9017422779286374 entropy 11.479731707842438
epoch: 59, step: 3
	action: tensor([[-33663.9974, -15399.8603,   6031.5754,  -5927.5044,   7221.4330,
           7803.6420,  13212.5361]], dtype=torch.float64)
	q_value: tensor([[-39.4473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07197201183449309, distance: 1.102394988772695 entropy 11.468724236598376
epoch: 59, step: 4
	action: tensor([[-13076.7290, -10175.9729, -22327.4798,   4656.6737, -35247.0759,
         -49602.2734,  -2442.4303]], dtype=torch.float64)
	q_value: tensor([[-28.5112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10202928550168644, distance: 1.2013049348242297 entropy 11.314918751610103
epoch: 59, step: 5
	action: tensor([[ -7762.6308, -45052.4660,  25024.5841,  27061.9774, -89706.4189,
           9747.4718,  -4697.1713]], dtype=torch.float64)
	q_value: tensor([[-30.5341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059876712861039016, distance: 1.1095556735929284 entropy 11.592765240107923
epoch: 59, step: 6
	action: tensor([[-36963.2685, -12162.7109,  -6671.2143, -16797.1701,  10464.9092,
          39537.5088, -22182.1021]], dtype=torch.float64)
	q_value: tensor([[-31.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20946780491471584, distance: 1.2585018242325539 entropy 11.676187567102048
epoch: 59, step: 7
	action: tensor([[   728.8217, -39904.4261,  33404.1770,  38585.4708, -33292.9334,
          15252.0955,  44928.1766]], dtype=torch.float64)
	q_value: tensor([[-34.0710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12834828533884357, distance: 1.2155652604307983 entropy 11.650907475872653
epoch: 59, step: 8
	action: tensor([[-17660.7307, -32527.4274, -23511.8271,  14232.0030,  15844.2715,
         -15320.2071,   7803.0073]], dtype=torch.float64)
	q_value: tensor([[-39.9512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21985030012401474, distance: 1.010754459521804 entropy 11.648343615410349
epoch: 59, step: 9
	action: tensor([[  2131.0933, -25200.2771, -44331.9476,   6907.4496,  -8845.4193,
          10617.7001,  45233.3783]], dtype=torch.float64)
	q_value: tensor([[-32.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.67117827806952
epoch: 59, step: 10
	action: tensor([[ -9454.6644, -21391.3874,   -159.0887,  31029.3069,   -154.1762,
          -2475.0420,  -1471.7540]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024275336041738038, distance: 1.1581506384208298 entropy 11.095625069521592
epoch: 59, step: 11
	action: tensor([[-27343.3255, -12621.7095, -34378.7625,  -9068.1885,  -1474.6282,
         -14520.8362,  28681.0352]], dtype=torch.float64)
	q_value: tensor([[-31.9062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5794795798265935, distance: 1.4381815103243822 entropy 11.64137237088705
epoch: 59, step: 12
	action: tensor([[-18553.1746, -30691.0898,   1219.9434, -25719.3171, -22354.9617,
           5774.8479, -14047.1092]], dtype=torch.float64)
	q_value: tensor([[-34.6627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4067653620719427, distance: 1.3572739895780284 entropy 11.591726444128621
epoch: 59, step: 13
	action: tensor([[-70950.2227, -46993.6544,   2930.1776, -25639.0703,   2342.3135,
         -24084.9991,  43597.1424]], dtype=torch.float64)
	q_value: tensor([[-35.1216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1779423986621771, distance: 1.2419917764210997 entropy 11.644596189478056
epoch: 59, step: 14
	action: tensor([[  -665.1863,  17591.8100,   6244.6010,   3904.6377,  25257.9570,
           9448.0182, -25927.4445]], dtype=torch.float64)
	q_value: tensor([[-35.5210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49514591508729144, distance: 1.3992602037903326 entropy 11.589442548124623
epoch: 59, step: 15
	action: tensor([[  8402.1902,   -116.1545, -12338.0207,   8953.9829,  -1908.6752,
          13494.7682,  -8249.6662]], dtype=torch.float64)
	q_value: tensor([[-30.9055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.298997578519703
epoch: 59, step: 16
	action: tensor([[-9695.9700, -2045.8943, 19176.0857, 28008.8134,  7226.3093,  6100.4432,
         26209.5085]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05999621331303295, distance: 1.1094851527260952 entropy 11.095625069521592
epoch: 59, step: 17
	action: tensor([[-34149.0049,  44772.8518, -44632.9292,  19452.4459,  45816.3436,
          39263.1824, -13688.9689]], dtype=torch.float64)
	q_value: tensor([[-32.0744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24116890889405385, distance: 0.9968487109286288 entropy 11.607495526364662
epoch: 59, step: 18
	action: tensor([[-71304.9645, -34766.5467,  11802.0180,  40379.9577,  67682.2892,
          -8020.7812,  -6295.0786]], dtype=torch.float64)
	q_value: tensor([[-40.1661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17729009481680458, distance: 1.2416478427097501 entropy 11.709074794725725
epoch: 59, step: 19
	action: tensor([[-25967.6480, -21258.8558,   4535.3505,  16109.2413,   -544.4295,
          10483.8715,  -2166.8035]], dtype=torch.float64)
	q_value: tensor([[-32.3236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5993996042365595, distance: 1.4472220984133275 entropy 11.557595431034212
epoch: 59, step: 20
	action: tensor([[-32641.6443, -50922.2069,  38512.0673,  20093.6630,  30302.1344,
           5697.0068, -23199.4182]], dtype=torch.float64)
	q_value: tensor([[-35.0283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09352746413610902, distance: 1.1966621115638252 entropy 11.734565165845753
epoch: 59, step: 21
	action: tensor([[-19784.0765, -22746.2820,  -2109.3198,    -62.0731, -52605.4136,
         -42154.6280, -37651.0043]], dtype=torch.float64)
	q_value: tensor([[-33.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3426605616502256, distance: 0.9277939537259952 entropy 11.64529945563522
epoch: 59, step: 22
	action: tensor([[  3817.1225, -36589.2559,  -7821.1379,  -1912.4564, -12945.2897,
          54910.7096,  20527.8291]], dtype=torch.float64)
	q_value: tensor([[-33.6490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44326770939625226, distance: 0.8538466893991561 entropy 11.700330234712926
epoch: 59, step: 23
	action: tensor([[ 23968.3108, -19956.0968, -22758.7388,  -1149.0828,  -2741.0800,
         -13270.8642,  16929.4800]], dtype=torch.float64)
	q_value: tensor([[-31.0048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2577416201560523, distance: 1.2833715557873546 entropy 11.320923242738592
epoch: 59, step: 24
	action: tensor([[-33041.0934,   5437.9722,  10537.8531,   8733.8985,   2039.4066,
         -11308.6774,  14256.7053]], dtype=torch.float64)
	q_value: tensor([[-33.2857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.483784322227795
epoch: 59, step: 25
	action: tensor([[ -3529.6072, -15348.3045, -23876.7286,  11168.9330,  14713.7969,
          25497.9253,  -8898.4923]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.095625069521592
epoch: 59, step: 26
	action: tensor([[-5155.0343,  2897.6049, -7774.9301, -1047.1380, 18314.7433, 19042.2486,
         14266.8952]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.095625069521592
epoch: 59, step: 27
	action: tensor([[ -4155.3689,  -2522.4275,   3884.7684,  34025.6722, -18387.2482,
          28314.6869,  12295.4968]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3689607848753407, distance: 1.3389125028426785 entropy 11.095625069521592
epoch: 59, step: 28
	action: tensor([[  7405.5275, -37454.5751,   1743.9163,  -9208.5866,   1615.0365,
           7556.5747,  18114.4671]], dtype=torch.float64)
	q_value: tensor([[-34.1416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15247197344121588, distance: 1.228490718482004 entropy 11.700486030144154
epoch: 59, step: 29
	action: tensor([[-30260.0754,  14985.8709,  34210.7827,  20256.8731,  20852.3943,
          17686.3992, -16674.5174]], dtype=torch.float64)
	q_value: tensor([[-31.9102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.427006868374688
epoch: 59, step: 30
	action: tensor([[-19254.2579, -19209.4476,  -7843.9373,  18115.2657,  -1607.0462,
            541.4015, -19351.4321]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49421148391504266, distance: 1.398822883026411 entropy 11.095625069521592
epoch: 59, step: 31
	action: tensor([[-26382.2792,   1271.2324, -10538.7860,  38964.1892,  60715.8084,
          30578.8389,  17507.7781]], dtype=torch.float64)
	q_value: tensor([[-32.5912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32489924688917693, distance: 0.9402449066419306 entropy 11.652130109698989
epoch: 59, step: 32
	action: tensor([[ -4643.8783, -24927.8865,  -8347.2831,  33892.4482, -10870.3626,
         -30540.0031, -20551.4820]], dtype=torch.float64)
	q_value: tensor([[-37.7367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3786621036680451, distance: 1.3436483162807042 entropy 11.674288805000442
epoch: 59, step: 33
	action: tensor([[-76183.1803, -19062.1355, -13992.0951,  52160.4398,  37948.3304,
         -13256.9733,  -1689.2922]], dtype=torch.float64)
	q_value: tensor([[-34.7529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14224438950392404, distance: 1.059835535459046 entropy 11.660326944998598
epoch: 59, step: 34
	action: tensor([[ 12648.2650, -47790.6010,  -8569.6931,  -9798.1693, -41353.9700,
          16197.6867,  93860.1815]], dtype=torch.float64)
	q_value: tensor([[-31.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26412628111572745, distance: 1.2866248153617887 entropy 11.657423332873693
epoch: 59, step: 35
	action: tensor([[-39174.8089,  20237.6578, -19729.7241,  19738.7669,  -2795.3348,
         -12928.2823,   7916.6866]], dtype=torch.float64)
	q_value: tensor([[-33.1045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.401008760405988
epoch: 59, step: 36
	action: tensor([[-21308.1955, -13709.1068, -21526.0999,  11616.3985,  -5585.3951,
           9703.1114,  -5295.1765]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25518116514976996, distance: 0.9876021313355787 entropy 11.095625069521592
epoch: 59, step: 37
	action: tensor([[-43000.0780,   1429.3300,    794.8202,  55902.3142,  18599.0344,
          -5350.2713,   8400.8924]], dtype=torch.float64)
	q_value: tensor([[-29.8382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14726535773184724, distance: 1.2257125512758278 entropy 11.478828131476837
epoch: 59, step: 38
	action: tensor([[-42158.2380, -21710.1960, -14217.9456,  25847.5706, -29442.4704,
          31811.1165, -45792.0710]], dtype=torch.float64)
	q_value: tensor([[-35.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12693776240077348, distance: 1.0692500896784236 entropy 11.54605424404699
epoch: 59, step: 39
	action: tensor([[-30928.9887,   8180.4352,  11529.4262,  16645.8113,  11325.4388,
           6568.4369, -35892.3341]], dtype=torch.float64)
	q_value: tensor([[-29.3497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5705456299778737, distance: 0.749920466708606 entropy 11.494526784046808
epoch: 59, step: 40
	action: tensor([[ -4274.0543, -52972.0016, -54818.9868,  15229.4228,  11464.4665,
          39001.0740, -59048.9252]], dtype=torch.float64)
	q_value: tensor([[-37.8831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.471676738896202, distance: 1.3882347329568543 entropy 11.639113665503581
epoch: 59, step: 41
	action: tensor([[ -9561.4071, -34821.7055,  25405.6917,  31267.2112, -16126.9714,
           2954.5443,  47442.4383]], dtype=torch.float64)
	q_value: tensor([[-32.4221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20722094055170337, distance: 1.2573323026557455 entropy 11.637050773831092
epoch: 59, step: 42
	action: tensor([[-19658.4833,  16620.8513, -54367.3460, -36678.7815,    824.4609,
          17402.9669, -26757.6567]], dtype=torch.float64)
	q_value: tensor([[-35.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10301248173456723, distance: 1.2018406987917383 entropy 11.709285070621538
epoch: 59, step: 43
	action: tensor([[-19061.6097, -19752.7121,  23930.9795,  18015.0518, -33047.7192,
          25162.2623,  -1239.6259]], dtype=torch.float64)
	q_value: tensor([[-37.3854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5096125797061286, distance: 1.4060133566671418 entropy 11.534587450930735
epoch: 59, step: 44
	action: tensor([[ -3297.3640, -21296.8160, -26429.0820,  95590.8913,   1521.4194,
         -11407.0070,  13688.9930]], dtype=torch.float64)
	q_value: tensor([[-37.9648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26957179341853355, distance: 1.2893930522979267 entropy 11.863151730328912
epoch: 59, step: 45
	action: tensor([[-38986.3027, -55867.4525, -38744.3477,    342.6057,  62979.7462,
          15204.8179,  29411.8304]], dtype=torch.float64)
	q_value: tensor([[-33.6732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5328560381909009, distance: 1.4167961817240582 entropy 11.698679970165827
epoch: 59, step: 46
	action: tensor([[-54388.5733, -35657.2981,  21596.5759,  31902.6722,  74566.0633,
          32974.3878,  57363.4555]], dtype=torch.float64)
	q_value: tensor([[-33.6402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13981643948778189, distance: 1.2217269428318762 entropy 11.801168295672449
epoch: 59, step: 47
	action: tensor([[ 17537.3330, -70626.2596,  58328.7972,  58509.9015,  26826.8087,
         -26510.9600,   3609.2163]], dtype=torch.float64)
	q_value: tensor([[-37.1140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1190382742868703, distance: 1.074076497157647 entropy 11.820987618935504
epoch: 59, step: 48
	action: tensor([[-56488.1089,   8552.0186, -33980.8002,  34729.7494,  12616.1600,
          -8114.2600, -22007.1067]], dtype=torch.float64)
	q_value: tensor([[-41.7697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03448973065362804, distance: 1.1639110334694787 entropy 11.764786913733307
epoch: 59, step: 49
	action: tensor([[-48482.4711,  17637.0326, -33091.7244,  33902.4071,   1721.2549,
          11110.7254, -47263.6395]], dtype=torch.float64)
	q_value: tensor([[-38.2348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037802353365209, distance: 0.9548383801085488 entropy 11.705870384016398
epoch: 59, step: 50
	action: tensor([[  9823.4291, -31967.9449,  13558.6134,  29947.5300,   -101.3709,
          15439.5742,  28033.1371]], dtype=torch.float64)
	q_value: tensor([[-34.3036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41455129296389104, distance: 0.8755906571869326 entropy 11.528857164678774
epoch: 59, step: 51
	action: tensor([[-62666.7174, -69640.3942,  54003.6190, -16106.1176,   5651.8329,
           -460.1191,  23221.4149]], dtype=torch.float64)
	q_value: tensor([[-34.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0116557881283308, distance: 1.6230560911309095 entropy 11.52764302566217
epoch: 59, step: 52
	action: tensor([[  2048.1905,   1140.4080,  30918.2888,  23191.2619,  33080.7320,
         -22649.9088,  -5572.1025]], dtype=torch.float64)
	q_value: tensor([[-32.3704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.62095490279677
epoch: 59, step: 53
	action: tensor([[  -832.7410,  -8400.4709, -23422.3720,  15722.6851,  -6784.2064,
          21677.0170,   6724.3818]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2761414051743385, distance: 1.2927248377280394 entropy 11.095625069521592
epoch: 59, step: 54
	action: tensor([[ 2246.2939, -8417.0800, 15322.4737, 28184.7583, -6533.3635,  4636.0674,
           489.1471]], dtype=torch.float64)
	q_value: tensor([[-31.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5752974503670435, distance: 0.7457600717591173 entropy 11.527438547135608
epoch: 59, step: 55
	action: tensor([[  3837.5437, -50258.4061,  32809.1692,  24661.6444, -20955.4188,
         -25302.4020,  -5049.1593]], dtype=torch.float64)
	q_value: tensor([[-32.0815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4671246160539636, distance: 0.8353520111972468 entropy 11.473312322511541
epoch: 59, step: 56
	action: tensor([[-39299.1045,  62800.2049, -19412.6083,   -832.0129,  18854.3550,
         -43230.1803, -10423.1071]], dtype=torch.float64)
	q_value: tensor([[-36.0010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.592551790045764
epoch: 59, step: 57
	action: tensor([[-30145.7366,  -8461.7489,  -2062.6607,  51793.4912,  39488.1519,
          24116.6507,  31966.9930]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1873162405450155, distance: 1.2469237512675042 entropy 11.095625069521592
epoch: 59, step: 58
	action: tensor([[ -6023.6809,  22669.7865, -13471.1969,  83647.6374,  33580.6500,
         -34817.0810,  -6769.3069]], dtype=torch.float64)
	q_value: tensor([[-31.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19486507584992419, distance: 1.026812213878693 entropy 11.652400300240837
epoch: 59, step: 59
	action: tensor([[-12079.0569,  -9454.1801, -20436.4960,  59213.1167,  16218.1947,
          31214.9180, -14168.5054]], dtype=torch.float64)
	q_value: tensor([[-38.8320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14053974916543854, distance: 1.0608881323057324 entropy 11.794529625169945
epoch: 59, step: 60
	action: tensor([[ -4446.1879, -35738.8084,   2033.9604,  27847.2669, -11724.6995,
         -49804.1278, -15250.8110]], dtype=torch.float64)
	q_value: tensor([[-31.4028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9024452486768431, distance: 1.5783843883634 entropy 11.528580833476038
epoch: 59, step: 61
	action: tensor([[-19869.9283,  17395.9625,  42394.7649,   9570.3561,  23101.5649,
         -15029.8220,  10609.5029]], dtype=torch.float64)
	q_value: tensor([[-30.1656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15873378190705711, distance: 1.049599023860819 entropy 11.503545587602748
epoch: 59, step: 62
	action: tensor([[ 23563.5115, -28398.2592,  66245.2349,  10050.1409, -36570.4675,
          49394.9641,  -7518.5974]], dtype=torch.float64)
	q_value: tensor([[-36.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3730898414305386, distance: 0.9060649840621894 entropy 11.689373641978445
epoch: 59, step: 63
	action: tensor([[-49944.1278, -29869.2699, -32696.1631,  31743.5671, -12887.0077,
         -43341.8919,   2456.2607]], dtype=torch.float64)
	q_value: tensor([[-36.4294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37124182610969814, distance: 1.3400275250863243 entropy 11.667746390737264
epoch: 59, step: 64
	action: tensor([[ 1597.0298, -9770.1640, 26800.2297, -9826.3408,   912.0916,  3741.2147,
         -9218.1662]], dtype=torch.float64)
	q_value: tensor([[-30.7222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24841864383607626, distance: 1.278606229880958 entropy 11.496267777016064
epoch: 59, step: 65
	action: tensor([[-27222.4871, -12516.0109,  38443.8951,  -5249.9742, -40163.7377,
          -7642.3416,  -3294.7957]], dtype=torch.float64)
	q_value: tensor([[-33.3324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5014189407676624, distance: 1.4021924954014195 entropy 11.32543298654456
epoch: 59, step: 66
	action: tensor([[-14253.9832, -39065.6828,   2615.3735,  47474.0257, -47156.7448,
          58141.9101,  41043.2480]], dtype=torch.float64)
	q_value: tensor([[-43.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5776013133016455, distance: 1.4373261362596115 entropy 11.813531672333934
epoch: 59, step: 67
	action: tensor([[ -1587.5060, -16455.6662,  -6025.4897,  17877.9804,  47853.9937,
          35878.0587,   6594.4480]], dtype=torch.float64)
	q_value: tensor([[-31.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1658762992263505, distance: 1.0451338740421994 entropy 11.66829819808426
epoch: 59, step: 68
	action: tensor([[-21895.0913, -26752.8304,  33738.0338,  -9604.9127, -15089.9407,
         -19353.1716, -59803.8327]], dtype=torch.float64)
	q_value: tensor([[-33.8221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8515551591330173, distance: 1.5571305325596871 entropy 11.671556183787999
epoch: 59, step: 69
	action: tensor([[ -9211.1115,  -4940.0849,  47650.0922,   1001.9990,  22605.3662,
           4374.7968, -72673.6522]], dtype=torch.float64)
	q_value: tensor([[-34.5355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09097546495509001, distance: 1.091049603668893 entropy 11.575720441058275
epoch: 59, step: 70
	action: tensor([[ -6036.8923,  26164.0264,  22413.6547,  34694.3148,  14971.2887,
          29970.0206, -11739.1665]], dtype=torch.float64)
	q_value: tensor([[-32.4435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6123278815602966, distance: 0.7125067401138564 entropy 11.735230530282971
epoch: 59, step: 71
	action: tensor([[-20947.4326,   3315.7305,  54816.9233, -13568.0537,  16371.9041,
           7860.6993, -31277.2633]], dtype=torch.float64)
	q_value: tensor([[-35.5408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6356810971673648, distance: 0.6907128663384313 entropy 11.63554148071729
epoch: 59, step: 72
	action: tensor([[-30151.3980, -24777.2870,  31181.9221,  21934.9306,   4895.9205,
         -16610.6633, -22753.7680]], dtype=torch.float64)
	q_value: tensor([[-38.0832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02947090089733284, distance: 1.1273557230818674 entropy 11.47961590532511
epoch: 59, step: 73
	action: tensor([[-11237.1183, -21353.2210, -13981.4634,   3480.1796,  43892.3811,
          33862.8579, -64864.2787]], dtype=torch.float64)
	q_value: tensor([[-29.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42156741464115854, distance: 1.3643959551735152 entropy 11.597958572337168
epoch: 59, step: 74
	action: tensor([[-59658.0274, -25778.0092,   6908.1982, -64865.1049, -54981.8163,
          -3745.4294,  14198.2380]], dtype=torch.float64)
	q_value: tensor([[-33.9285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.609869753563167, distance: 1.4519513461448794 entropy 11.801830891702
epoch: 59, step: 75
	action: tensor([[-46457.6669,   4183.5259, -13830.5158,  36328.6527, -51851.1453,
          -1342.9492,   9946.3429]], dtype=torch.float64)
	q_value: tensor([[-36.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44857627168288916, distance: 0.8497661328213753 entropy 11.704150268598069
epoch: 59, step: 76
	action: tensor([[ -9277.1652,  -3234.1856,  21950.9928,  -7146.5887, -36012.3906,
         -18172.6176, -42981.0907]], dtype=torch.float64)
	q_value: tensor([[-37.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7949233041749757, distance: 1.5331323279902158 entropy 11.65353559330371
epoch: 59, step: 77
	action: tensor([[ 22350.8530,   6665.6188, -26622.9982,  -9455.4755, -26666.6190,
          21523.3431,  20574.6228]], dtype=torch.float64)
	q_value: tensor([[-34.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.642835974716329
epoch: 59, step: 78
	action: tensor([[-22169.6882,   4910.3940, -16352.1937,   9937.1071,  -6240.6891,
           6442.7268,  -4468.6566]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2829866815730656, distance: 0.9689922523317258 entropy 11.095625069521592
epoch: 59, step: 79
	action: tensor([[19296.9451, 10266.1895, 21082.0790, 35191.0210,   386.9498,  8401.1757,
          4608.1038]], dtype=torch.float64)
	q_value: tensor([[-36.7054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.174482009827399
epoch: 59, step: 80
	action: tensor([[ -6099.5740,  -6384.4318,  16045.9206,  23346.5252,  -2896.7252,
          34895.1932, -10553.4787]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6981912915166506, distance: 1.4912484250022977 entropy 11.095625069521592
epoch: 59, step: 81
	action: tensor([[-43319.5959, -49238.2389,   7678.4664, -39858.4910,  13278.0456,
           2083.3669,  34859.1730]], dtype=torch.float64)
	q_value: tensor([[-28.3240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18124687362445813, distance: 1.2437326325274298 entropy 11.479543550786445
epoch: 59, step: 82
	action: tensor([[-10452.3236, -14582.4492,  13301.0821,  30188.3156,   4757.2462,
          11836.0058,  25984.6348]], dtype=torch.float64)
	q_value: tensor([[-32.6962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8268331224758885, distance: 1.5467001651073764 entropy 11.457641403200821
epoch: 59, step: 83
	action: tensor([[ 39884.6601, -15033.2530,  19049.7398, -27234.7105, -35816.7452,
         -33419.9889,  57024.9772]], dtype=torch.float64)
	q_value: tensor([[-28.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2546592778186958, distance: 1.2817980143692234 entropy 11.410967658410055
epoch: 59, step: 84
	action: tensor([[ 10542.9104,   8363.4526, -54938.8758,  -9303.0334, -41784.0113,
          23456.2179, -17083.0414]], dtype=torch.float64)
	q_value: tensor([[-30.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8519740695286858, distance: 0.44027658894687227 entropy 11.529398586706096
epoch: 59, step: 85
	action: tensor([[ -6612.7303,   3473.9119,  -7808.7812, -16586.0950,  36688.3586,
         -29142.5516, -35372.3732]], dtype=torch.float64)
	q_value: tensor([[-39.0174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.284785503046942
epoch: 59, step: 86
	action: tensor([[-11412.5237, -12532.4134,   4055.5557,  29328.1751,   5122.6972,
           4566.4880, -15364.2441]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3717714204158038, distance: 1.3402862695417581 entropy 11.095625069521592
epoch: 59, step: 87
	action: tensor([[-33449.3077,   7869.9767,  30797.3978,  59841.1962,  10666.5046,
          23540.0648,  34227.1549]], dtype=torch.float64)
	q_value: tensor([[-34.3746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0926709020479708, distance: 1.0900316612326277 entropy 11.696426885498031
epoch: 59, step: 88
	action: tensor([[ -2556.2880, -11821.5792, -68993.9063, -63035.5914,  89038.4982,
           7297.5736,  -1148.1953]], dtype=torch.float64)
	q_value: tensor([[-44.4549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6931609767434743, distance: 1.4890381285572953 entropy 11.871585375192163
epoch: 59, step: 89
	action: tensor([[-69104.4923,  12636.5095, -14982.1701,  21238.0209, -41521.0924,
           9060.3453, -14018.9647]], dtype=torch.float64)
	q_value: tensor([[-29.4446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3875649496987772, distance: 0.8955435539207258 entropy 11.432615249383641
epoch: 59, step: 90
	action: tensor([[-21954.8897, -68837.4882,   -252.5933,  59076.2152, -11148.0556,
          -8446.4283,  12387.1937]], dtype=torch.float64)
	q_value: tensor([[-34.8349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47699080250737724, distance: 1.3907388562957284 entropy 11.608484591615412
epoch: 59, step: 91
	action: tensor([[ -3874.8779, -22252.0383,   9687.7987,  27968.3623, -74528.0348,
         -20419.7630,  42030.1392]], dtype=torch.float64)
	q_value: tensor([[-31.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18624316307285038, distance: 1.0322954626991463 entropy 11.661535073654104
epoch: 59, step: 92
	action: tensor([[ 19347.2232, -33288.9193, -31497.0236,   -487.3477,  -2168.1988,
          70842.6910,  74148.1537]], dtype=torch.float64)
	q_value: tensor([[-35.9110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2261750831857262, distance: 1.0066489575186044 entropy 11.750106031433834
epoch: 59, step: 93
	action: tensor([[  1867.4420, -20567.1497,  11982.3845,   7856.1755, -24288.0965,
          11641.4537,  -4524.3568]], dtype=torch.float64)
	q_value: tensor([[-26.6932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04592526548300757, distance: 1.1177582676021198 entropy 11.076773074950236
epoch: 59, step: 94
	action: tensor([[-20626.6468,   1199.2179,  30194.2890,  -9653.6344,   4990.7226,
           3219.2143,  14053.3549]], dtype=torch.float64)
	q_value: tensor([[-40.8664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.57639431626349
epoch: 59, step: 95
	action: tensor([[ -7631.9467, -19116.8691, -39675.0732,  -2695.4497,    247.2734,
          16379.6805,  19922.4198]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20103217932098683, distance: 1.254105334233465 entropy 11.095625069521592
epoch: 59, step: 96
	action: tensor([[-58918.7635,  -3585.7580, -81498.5376, -24967.0220,    935.6864,
          -4339.3436,  28415.9208]], dtype=torch.float64)
	q_value: tensor([[-42.5089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16236558759975794, distance: 1.0473309768172294 entropy 11.801674927168195
epoch: 59, step: 97
	action: tensor([[-29388.2228,  12428.0425, -13225.2650,  79701.7871,  29361.1872,
          -7127.0919, -11483.5517]], dtype=torch.float64)
	q_value: tensor([[-37.5458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09317039853972109, distance: 1.0897315816106978 entropy 11.635668525349974
epoch: 59, step: 98
	action: tensor([[-51238.8904,  24411.7613,  24405.1485, -41652.7062,  33860.9503,
          11857.9844,   7282.9785]], dtype=torch.float64)
	q_value: tensor([[-34.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.414900195150308
epoch: 59, step: 99
	action: tensor([[-7666.7522, -3617.0640,  2039.1010,  4464.9041, 13148.4512, -5211.1737,
         -5243.3802]], dtype=torch.float64)
	q_value: tensor([[-36.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22493465231700527, distance: 1.2665232117662295 entropy 11.095625069521592
epoch: 59, step: 100
	action: tensor([[  6867.3377,   2310.4918,  14990.9257,   9610.6145, -24641.1957,
         -22483.2992,  -6486.4439]], dtype=torch.float64)
	q_value: tensor([[-33.8763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.610932746105135, distance: 0.7137876559249945 entropy 11.785632048055318
epoch: 59, step: 101
	action: tensor([[-13564.8138, -15258.1835,  54110.3106,  19285.4141,  49691.9261,
           9890.4241, -13821.8405]], dtype=torch.float64)
	q_value: tensor([[-36.6139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9151474366722376, distance: 1.5836448764042408 entropy 11.574804832512173
epoch: 59, step: 102
	action: tensor([[-57996.7979,  -9227.0196, -29491.1542,  40388.7719, -25175.5215,
          13856.4250,  26016.2680]], dtype=torch.float64)
	q_value: tensor([[-31.3794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14448112299046878, distance: 1.0584527890039586 entropy 11.658981119554413
epoch: 59, step: 103
	action: tensor([[  1895.5399, -22579.5554,  -5427.8268,  18596.0034, -30023.1083,
           1260.8174, -45688.2371]], dtype=torch.float64)
	q_value: tensor([[-33.7934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45184428358484074, distance: 0.8472443220337995 entropy 11.664853514889758
epoch: 59, step: 104
	action: tensor([[ 24988.5396, -34410.7509,  -8456.5682,   5222.4726,  30606.5041,
          19658.6638,  45561.1203]], dtype=torch.float64)
	q_value: tensor([[-36.8095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08367478911137072, distance: 1.1912589546748553 entropy 11.721476379486345
epoch: 59, step: 105
	action: tensor([[  6187.9424, -33944.1602,  10032.2218,  -5269.5900,  23172.9608,
         -41214.8171, -24277.3682]], dtype=torch.float64)
	q_value: tensor([[-39.6915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3233900719698277, distance: 1.31643866489368 entropy 11.485012777274047
epoch: 59, step: 106
	action: tensor([[-35182.4906,  -7957.1061,   1497.7799,  -6088.9096, -28331.2362,
           2941.4159,   1872.1957]], dtype=torch.float64)
	q_value: tensor([[-30.3263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.177634890230147, distance: 1.2418296513998646 entropy 11.28015714731306
epoch: 59, step: 107
	action: tensor([[ -9527.3369, -16250.9000,  16410.0580,  20846.6455, -13017.2176,
          68084.7078,  47214.8292]], dtype=torch.float64)
	q_value: tensor([[-35.7719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7951312366929606, distance: 1.5332211281168486 entropy 11.56897335571372
epoch: 59, step: 108
	action: tensor([[-16568.7485, -36400.2260, -43827.3645,  26904.9244, -15939.0866,
         -27156.2500,    751.3628]], dtype=torch.float64)
	q_value: tensor([[-30.4679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028334484439349605, distance: 1.1604432138600989 entropy 11.520568915287187
epoch: 59, step: 109
	action: tensor([[  2388.6794, -27597.7711,   8281.3552,   3693.0983,  -2976.5355,
          28087.2926,  38196.7275]], dtype=torch.float64)
	q_value: tensor([[-34.6997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6892463286665528, distance: 0.637917956905622 entropy 11.741491851802763
epoch: 59, step: 110
	action: tensor([[-24241.7544,  14531.1406, -10726.7941,   1175.8892, -31885.1992,
         -19673.2597,   7033.2290]], dtype=torch.float64)
	q_value: tensor([[-33.0046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08739269762235091, distance: 1.1933007109144893 entropy 11.504516983830642
epoch: 59, step: 111
	action: tensor([[ 18575.4668,   5160.0007, -10195.2373,  32551.2634, -10501.1636,
         -20124.1247,  26938.6941]], dtype=torch.float64)
	q_value: tensor([[-35.7645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6524571469889435, distance: 0.6746225816628839 entropy 11.605336461836234
epoch: 59, step: 112
	action: tensor([[-46421.4905,    222.4829,  49732.6786,   9654.8503,  17064.7445,
            523.2431,  31704.1071]], dtype=torch.float64)
	q_value: tensor([[-39.6877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3129881834218745, distance: 0.948503191993738 entropy 11.793098313819026
epoch: 59, step: 113
	action: tensor([[-22518.8993,  21018.1131,  20659.5792,  -1583.4723,  33674.7016,
          -3465.6087, -14977.0670]], dtype=torch.float64)
	q_value: tensor([[-32.4223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7690342800677064, distance: 0.549959180931235 entropy 11.47564167240497
epoch: 59, step: 114
	action: tensor([[ 13133.0970,    697.4706, -37722.1781,  -9388.2893,   8202.5932,
          -1650.6614,   9898.5458]], dtype=torch.float64)
	q_value: tensor([[-35.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5027713373192102, distance: 0.8069279730760399 entropy 11.55756009711514
epoch: 59, step: 115
	action: tensor([[  8463.6243, -35891.4963,  14844.1869,   6157.2302, -18112.7541,
          -7249.2725,  22344.7151]], dtype=torch.float64)
	q_value: tensor([[-41.9844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45108021779359664, distance: 1.3784861405869744 entropy 11.666619365472528
epoch: 59, step: 116
	action: tensor([[-22532.6757, -30004.1676, -33536.5022,  44319.4933,  38331.2594,
           -735.7821, -33409.3702]], dtype=torch.float64)
	q_value: tensor([[-34.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012838116612325479, distance: 1.1516664406632833 entropy 11.612401283720653
epoch: 59, step: 117
	action: tensor([[-57331.8044,  40613.8239,  22037.1881,  19109.5539,   9139.3581,
          -5270.8691, -10838.7026]], dtype=torch.float64)
	q_value: tensor([[-36.6243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06059650179723963, distance: 1.109130836321722 entropy 11.768745958485805
epoch: 59, step: 118
	action: tensor([[-31184.4602, -50697.8901,  -8111.1619, -35887.8908,  19607.4261,
         -16178.4486, -25605.2863]], dtype=torch.float64)
	q_value: tensor([[-36.8404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4876437669871674, distance: 1.3957452764153335 entropy 11.688302177411765
epoch: 59, step: 119
	action: tensor([[ 56551.5800, -13928.8823,    435.2325,  30430.6516,  44269.3176,
          -5500.4437,  25422.5576]], dtype=torch.float64)
	q_value: tensor([[-31.6587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032075674620236305, distance: 1.1258418686722818 entropy 11.524370834466595
epoch: 59, step: 120
	action: tensor([[ -7648.2256, -14542.4318,  -5339.4302,  66967.6887, -28538.9190,
          -8893.0705,  15180.9433]], dtype=torch.float64)
	q_value: tensor([[-34.8155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004237887714880539, distance: 1.1419168783053093 entropy 11.535569873910339
epoch: 59, step: 121
	action: tensor([[-46726.5673, -28862.5055,  24188.6794,  24072.8793, -21890.1068,
         -48613.2367,  -3539.1285]], dtype=torch.float64)
	q_value: tensor([[-30.2348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8479437733434492, distance: 1.5556112303173728 entropy 11.500208636073536
epoch: 59, step: 122
	action: tensor([[-17968.3307, -36968.7005, -14139.0464,  14255.6083,    999.5400,
         -28393.1491,   7561.4332]], dtype=torch.float64)
	q_value: tensor([[-30.8422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5057005730669017, distance: 1.404190404979835 entropy 11.502407959984058
epoch: 59, step: 123
	action: tensor([[-42663.7089, -40078.1997,  24558.6594,    840.6010, -15127.6881,
          -4762.5364, -13480.3049]], dtype=torch.float64)
	q_value: tensor([[-34.5776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2166576839757105, distance: 1.012820509361463 entropy 11.730200995879292
epoch: 59, step: 124
	action: tensor([[-35350.4194,  -7315.1184,  19066.1227, -24401.7236,  18326.2199,
          23644.6295, -10380.6709]], dtype=torch.float64)
	q_value: tensor([[-36.9073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4216627888999267, distance: 1.3644417236924815 entropy 11.825440840195805
epoch: 59, step: 125
	action: tensor([[  6632.9687, -29114.3775,  -2219.0062, -26475.0752,  11402.7432,
           4572.8839,  18184.3890]], dtype=torch.float64)
	q_value: tensor([[-34.3226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1653945602164535, distance: 1.2353590085355672 entropy 11.581806289163287
epoch: 59, step: 126
	action: tensor([[  1485.2987, -21085.6990,  -4938.6181,  -9504.7819,  12267.4765,
          31132.6166,  16623.8266]], dtype=torch.float64)
	q_value: tensor([[-34.1490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24431024272733448, distance: 0.9947832433000966 entropy 11.415078589226757
epoch: 59, step: 127
	action: tensor([[-20474.3075, -10511.6185, -42764.8045,  18575.0085,  17033.9736,
          35806.1359,  -6300.5590]], dtype=torch.float64)
	q_value: tensor([[-40.4322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3179860605576945, distance: 1.313748102575425 entropy 11.522923375270098
LOSS epoch 59 actor 496.94460116078784 critic 107.97134056380574
epoch: 60, step: 0
	action: tensor([[-8495.3951, -6009.2660, 18037.4546,  5601.5878, 18003.1081, 19110.4050,
         12819.1797]], dtype=torch.float64)
	q_value: tensor([[-26.5317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5921603508760895, distance: 1.4439431525414184 entropy 11.318320328823727
epoch: 60, step: 1
	action: tensor([[-25771.6684, -46737.8034,  18944.5380,  52849.8208,  22033.1752,
          66122.8334,  -8957.9556]], dtype=torch.float64)
	q_value: tensor([[-33.5663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.68166971843976, distance: 1.4839765740679267 entropy 11.6954765943624
epoch: 60, step: 2
	action: tensor([[-17106.6160, -59747.2847, -27799.4681,  19518.2126,  -8686.7085,
         -31549.2715,  -2510.3660]], dtype=torch.float64)
	q_value: tensor([[-32.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17597326377975275, distance: 1.0387890062790128 entropy 11.702978342470061
epoch: 60, step: 3
	action: tensor([[-20107.4186, -22944.9474, -31746.3987, -57157.3564, -18924.9784,
           7523.7955,  31235.5894]], dtype=torch.float64)
	q_value: tensor([[-37.4960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.987851231670066, distance: 1.6134244458297884 entropy 11.849829870670943
epoch: 60, step: 4
	action: tensor([[ -6606.1885, -18777.5151,   3730.3562,  28775.4513,  11200.3049,
          17543.0509,   9396.6552]], dtype=torch.float64)
	q_value: tensor([[-32.9648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2597841603471398, distance: 1.2844132143274067 entropy 11.628015791320765
epoch: 60, step: 5
	action: tensor([[-43839.9801, -71365.5494,  17372.2853, -22102.0533,    360.7480,
          13094.7437, -15681.9961]], dtype=torch.float64)
	q_value: tensor([[-34.2634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08538344017856092, distance: 1.192197725330405 entropy 11.875742356580187
epoch: 60, step: 6
	action: tensor([[-19420.1910, -89884.3785, -16927.1869,  16255.3516,   5877.0662,
          30040.6215,  -6272.9960]], dtype=torch.float64)
	q_value: tensor([[-33.3060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5966342597919831, distance: 1.4459704415543448 entropy 11.647269521614081
epoch: 60, step: 7
	action: tensor([[-63986.1839,   9294.7325, -16548.4817,  18913.6806, -35024.1990,
         -14332.8055, -42734.1621]], dtype=torch.float64)
	q_value: tensor([[-35.3647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21867945203926542, distance: 1.0115126449250151 entropy 11.78401837717403
epoch: 60, step: 8
	action: tensor([[  8377.9208, -27561.2577,  60153.3910, -49786.9205,   1178.1390,
           4285.2898,  11802.9485]], dtype=torch.float64)
	q_value: tensor([[-35.7060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.670244931571997
epoch: 60, step: 9
	action: tensor([[-15314.7801,   5460.1170,  15278.3479,  -2155.0733,    -92.1066,
          26102.5788,   8043.2054]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439569043599395, distance: 0.7727866941422831 entropy 11.13469950178291
epoch: 60, step: 10
	action: tensor([[25124.7860,  5849.1982, 11112.4013, -4795.2357, 22704.5325, 18584.5406,
         25915.8691]], dtype=torch.float64)
	q_value: tensor([[-43.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5302030995862559, distance: 0.7843533699457833 entropy 11.578161648683778
epoch: 60, step: 11
	action: tensor([[-31796.2156, -33315.8369,   5204.4919,  34535.2799, -20660.1088,
          18328.3045,   4909.1884]], dtype=torch.float64)
	q_value: tensor([[-49.4435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.710207317839165
epoch: 60, step: 12
	action: tensor([[-14911.9207, -13855.1597,  -7708.5945,   1390.1659,  11738.5815,
          -5590.7339,   -755.7358]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6470739447990563, distance: 1.4686328623590081 entropy 11.13469950178291
epoch: 60, step: 13
	action: tensor([[-43594.4077, -19330.8372,  51593.3805, -34445.6048,   7470.5466,
          54141.5364,  19551.9859]], dtype=torch.float64)
	q_value: tensor([[-30.4739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0045327741574681, distance: 1.6201800266749422 entropy 11.514170450997991
epoch: 60, step: 14
	action: tensor([[-40829.9278, -28601.3203, -10198.1890,  23080.4481, -19339.2529,
           5920.4152,   4795.9852]], dtype=torch.float64)
	q_value: tensor([[-31.7661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03682815574124931, distance: 1.1230745417171337 entropy 11.447747098934867
epoch: 60, step: 15
	action: tensor([[  7155.1603, -18754.5697,   -681.1644,  -3487.8620,  34355.7718,
         -10154.0997, -14550.8717]], dtype=torch.float64)
	q_value: tensor([[-30.3777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5273459467859725, distance: 0.7867348463469449 entropy 11.618378208555475
epoch: 60, step: 16
	action: tensor([[ 10029.7757, -20147.2281,  -6739.8865,  19873.5388, -13456.5756,
          41504.6497,  18250.5110]], dtype=torch.float64)
	q_value: tensor([[-27.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13756227750845085, distance: 1.2205182686646725 entropy 11.370105663022263
epoch: 60, step: 17
	action: tensor([[ 25769.3313,  -5132.6218, -21232.7886,  19875.5794, -44661.2557,
          -9024.6841,   8842.3276]], dtype=torch.float64)
	q_value: tensor([[-45.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18458019197380215, distance: 1.0333497096796593 entropy 11.64838635459835
epoch: 60, step: 18
	action: tensor([[-29847.3355,  16206.0671, -25886.3082,  26823.1164, -35764.0662,
         -28533.8730, -29907.7832]], dtype=torch.float64)
	q_value: tensor([[-38.3365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.730759100385045
epoch: 60, step: 19
	action: tensor([[-17445.9733, -12176.4514,  -5827.5289,  37067.0569, -17094.5660,
          -7872.8206,  25061.5290]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3335658095453813, distance: 0.9341902454260524 entropy 11.13469950178291
epoch: 60, step: 20
	action: tensor([[ 64811.6453,   -277.2599,  18962.5221,  27522.8637,   3355.3804,
          17818.6063, -35684.2706]], dtype=torch.float64)
	q_value: tensor([[-33.9282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5046005853897841, distance: 0.8054423069965478 entropy 11.739456830086613
epoch: 60, step: 21
	action: tensor([[ 13992.9437,  -2000.9407, -23766.0361,  56200.1907,   3139.3223,
         -11345.0009, -11083.8793]], dtype=torch.float64)
	q_value: tensor([[-33.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49107460488960797, distance: 0.8163638299802054 entropy 11.541373965816673
epoch: 60, step: 22
	action: tensor([[ -2454.2362, -51314.9354,  -1510.0747,  53102.2813, -12656.7425,
         -17957.7172,   7524.0412]], dtype=torch.float64)
	q_value: tensor([[-37.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061565660357554286, distance: 1.1085585577255177 entropy 11.797087521291813
epoch: 60, step: 23
	action: tensor([[ 17601.4879,   7889.3830,  28637.1739,  -6744.5670, -20363.9002,
         -11911.3624,  39907.7431]], dtype=torch.float64)
	q_value: tensor([[-35.9462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.868257914310638
epoch: 60, step: 24
	action: tensor([[ -4326.3236, -13540.2316,  17096.1345,  -7328.8030,   -799.9743,
          29403.5303,   1844.9642]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4116449720317794, distance: 1.3596259220917741 entropy 11.13469950178291
epoch: 60, step: 25
	action: tensor([[-15551.3706,  25148.3374, -17082.0994,   5445.9478,   1389.0748,
          -8784.0215,  -3830.2166]], dtype=torch.float64)
	q_value: tensor([[-30.0159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49240932519696134, distance: 0.8152926192042642 entropy 11.52482759867111
epoch: 60, step: 26
	action: tensor([[-22575.2917, -54421.4353,  41043.9124,  41207.7752,   4953.4579,
           2330.0661,  18593.2619]], dtype=torch.float64)
	q_value: tensor([[-34.8329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.587779584150239
epoch: 60, step: 27
	action: tensor([[ 1422.4460, -7551.0354, -3493.8541,  7575.5812, 16178.1736, -3570.7352,
         33823.5150]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.13469950178291
epoch: 60, step: 28
	action: tensor([[ -7329.6182, -25166.6099,   4868.7897, -21639.5671,   6934.9644,
            -38.2981,    549.4048]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20751206767604313, distance: 1.0187157533552402 entropy 11.13469950178291
epoch: 60, step: 29
	action: tensor([[ 18034.8698, -19569.5617,  -4657.6504,  40181.9927,  -5232.8118,
          39103.2460,   8173.3932]], dtype=torch.float64)
	q_value: tensor([[-33.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30475650707520874, distance: 0.9541686858616265 entropy 11.572099826248168
epoch: 60, step: 30
	action: tensor([[-48937.5339,   7352.6544, -26970.6447,   4051.6990, -10820.0500,
         -22186.2681, -18229.1190]], dtype=torch.float64)
	q_value: tensor([[-29.6706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21564655474769046, distance: 1.261712352868951 entropy 11.356295762147544
epoch: 60, step: 31
	action: tensor([[-22540.2743,  21030.5208, -17972.3351,  -3036.5946,  24267.6981,
          28668.3826,  29375.5900]], dtype=torch.float64)
	q_value: tensor([[-34.1949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2576677343376558, distance: 0.9859522038908157 entropy 11.606834565690828
epoch: 60, step: 32
	action: tensor([[ -8858.0739, -25627.5223,  15769.0444,  -4256.7721, -18631.1534,
          -8615.9162,  17334.6364]], dtype=torch.float64)
	q_value: tensor([[-44.4897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.632175284002278
epoch: 60, step: 33
	action: tensor([[-10477.9063, -26295.3847, -18726.1436,  -3237.5721,   5248.8303,
           9543.4057,  10819.4746]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23633446477888986, distance: 1.272402990940363 entropy 11.13469950178291
epoch: 60, step: 34
	action: tensor([[  7989.5435, -30641.8111,   7633.6054, -34362.7042,  55681.9601,
          20769.9142,    764.9721]], dtype=torch.float64)
	q_value: tensor([[-38.7597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017204954121436078, distance: 1.1541464673407813 entropy 11.715028051215796
epoch: 60, step: 35
	action: tensor([[-37964.1575,  -8883.5780, -10801.1005,   6865.6599, -32720.6011,
         -14795.2332,  12155.9152]], dtype=torch.float64)
	q_value: tensor([[-34.1053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7655044890984588, distance: 1.5205163917077145 entropy 11.401363546241212
epoch: 60, step: 36
	action: tensor([[ -7469.3091, -31470.6632,   1942.3215,  -1249.1909, -16949.5079,
           4729.5932,  16646.8274]], dtype=torch.float64)
	q_value: tensor([[-33.1181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21257702722386596, distance: 1.0154551202311948 entropy 11.59169353064535
epoch: 60, step: 37
	action: tensor([[-20483.0079, -11788.4047, -28678.8900,  -7029.4542, -41636.2026,
         -23376.8903,  20125.8455]], dtype=torch.float64)
	q_value: tensor([[-34.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8945177975995791, distance: 1.5750924073827963 entropy 11.701452384790958
epoch: 60, step: 38
	action: tensor([[ -1137.5675, -27346.0525, -25683.7908,   4373.3481,  -1274.6809,
         -41270.6121,    199.3808]], dtype=torch.float64)
	q_value: tensor([[-36.6899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9549089968490732, distance: 1.5999999383785524 entropy 11.695108064607485
epoch: 60, step: 39
	action: tensor([[  1356.0140, -28769.4667, -27168.8818,   6502.7577,   -359.3449,
          25148.1531,  -6838.7327]], dtype=torch.float64)
	q_value: tensor([[-30.4915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5045131535197254, distance: 0.805513379163974 entropy 11.436035260002892
epoch: 60, step: 40
	action: tensor([[-19846.2309,  16183.8719,  17835.0034,  45079.5284, -34410.7405,
          -2635.4480,  63797.5840]], dtype=torch.float64)
	q_value: tensor([[-36.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28575418952659337, distance: 0.9671203997566696 entropy 11.643246782558188
epoch: 60, step: 41
	action: tensor([[-40217.0073,  36792.9718,  -4472.0845,   7291.7490,  14878.2096,
           5093.6973,  41409.9521]], dtype=torch.float64)
	q_value: tensor([[-37.6413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05900890547014537, distance: 1.1100676588433054 entropy 11.657043984908347
epoch: 60, step: 42
	action: tensor([[ -7897.2799, -46342.2496,  40211.8827, -37910.1958,  25100.7999,
           8572.6635,  -7558.1549]], dtype=torch.float64)
	q_value: tensor([[-34.9876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9608282177850289, distance: 1.602420407800877 entropy 11.627131635609967
epoch: 60, step: 43
	action: tensor([[ 24376.6654,  14305.3332,  22337.2282,  13936.7503,  24934.6105,
         -12814.5293,  32254.8681]], dtype=torch.float64)
	q_value: tensor([[-27.6505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.377007428901374
epoch: 60, step: 44
	action: tensor([[  5874.9419,   -692.9326, -19776.9706,  17301.0266,   2043.9239,
         -18379.4828,   9514.2389]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05541441903152111, distance: 1.1756233540881524 entropy 11.13469950178291
epoch: 60, step: 45
	action: tensor([[  3791.1955, -45077.5438,  23156.4979,  33512.6552,    882.8486,
         -30506.8163,  43780.5243]], dtype=torch.float64)
	q_value: tensor([[-33.1912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44080997701658753, distance: 0.8557292959654094 entropy 11.667424476035062
epoch: 60, step: 46
	action: tensor([[ 16616.2636,  28610.4886, -10444.3036, -18010.3360, -16459.7153,
          -2367.5952,   2236.7159]], dtype=torch.float64)
	q_value: tensor([[-44.3488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7355503717549425, distance: 0.5884752115376631 entropy 11.81578577440131
epoch: 60, step: 47
	action: tensor([[ -5772.6615, -10872.0555, -13541.0667,   3688.5960,  -3263.6218,
           2787.2921,  -9544.2913]], dtype=torch.float64)
	q_value: tensor([[-37.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.435455339287818
epoch: 60, step: 48
	action: tensor([[-17339.5112, -20065.2182,   7340.1513, -33748.6642,   5929.8407,
         -13524.2075, -28172.4467]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23772622099474938, distance: 0.9991074187106329 entropy 11.13469950178291
epoch: 60, step: 49
	action: tensor([[-21511.7148,   3958.4835, -20930.0628,   1576.3207,  15257.8169,
          25886.3938,  -3944.3182]], dtype=torch.float64)
	q_value: tensor([[-31.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33439925521149383, distance: 0.9336059113657241 entropy 11.530218048845052
epoch: 60, step: 50
	action: tensor([[  4127.2712, -39556.7988,  21511.4041,  19643.0054,  -1810.4232,
          10962.1199,   7676.8772]], dtype=torch.float64)
	q_value: tensor([[-36.5037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07209283351590923, distance: 1.1848759643601643 entropy 11.430742572086583
epoch: 60, step: 51
	action: tensor([[-23863.8963, -35003.6165, -13510.8446,  23658.4711,  -3270.2902,
           2613.1809,  59559.8234]], dtype=torch.float64)
	q_value: tensor([[-31.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.612000433556297, distance: 1.4529118650739752 entropy 11.601308507841363
epoch: 60, step: 52
	action: tensor([[-40961.8389,  20535.9189,  -2709.2868,  22863.6991, -49013.7984,
         -53255.7523, -11923.0874]], dtype=torch.float64)
	q_value: tensor([[-35.1760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16725127233881687, distance: 1.2363427067894035 entropy 11.850815554796409
epoch: 60, step: 53
	action: tensor([[ -6326.2921,  -2708.0999, -17233.3353, -20844.3042,  14988.4091,
          -8557.9282, -19018.4672]], dtype=torch.float64)
	q_value: tensor([[-36.4731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9958991476841663, distance: 1.6166871619871206 entropy 11.659334105584659
epoch: 60, step: 54
	action: tensor([[  8825.3022, -27654.3479, -42780.8247,   3289.5671,  -5885.8060,
           3382.9156,  18686.3440]], dtype=torch.float64)
	q_value: tensor([[-33.0708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0819594904659251, distance: 1.1903157869831507 entropy 11.599992321165814
epoch: 60, step: 55
	action: tensor([[ 27791.1468,  14363.5133, -28624.0919,  29697.2540, -19693.6688,
          16785.2990, -63218.9117]], dtype=torch.float64)
	q_value: tensor([[-36.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26653630103874004, distance: 0.98004497317292 entropy 11.464787955634035
epoch: 60, step: 56
	action: tensor([[ 42492.5795, -23122.4695,   5564.6067,   8537.9486,  15585.7764,
         -41235.0041,  21285.4791]], dtype=torch.float64)
	q_value: tensor([[-38.2542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5053128844585826, distance: 0.8048630550204154 entropy 11.632635493506465
epoch: 60, step: 57
	action: tensor([[-14340.2541,  21953.1447,  -9179.3514,  31389.8006, -41310.2775,
           4835.1725,   8085.1808]], dtype=torch.float64)
	q_value: tensor([[-39.0233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06656451213852055, distance: 1.105602080618649 entropy 11.862711989781758
epoch: 60, step: 58
	action: tensor([[17840.2349,  8418.4373, 47145.8274, 17970.4190, 33872.6088,  5865.8270,
         37645.2073]], dtype=torch.float64)
	q_value: tensor([[-35.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43266170659642844, distance: 0.8619414027569337 entropy 11.6038285052135
epoch: 60, step: 59
	action: tensor([[-45387.5660, -80904.2222,  -2840.7939,   1023.4129,   8431.6659,
         -13679.9639,  43153.3638]], dtype=torch.float64)
	q_value: tensor([[-39.3076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8159111106375958, distance: 1.542069637405055 entropy 11.706867109740651
epoch: 60, step: 60
	action: tensor([[  4150.8779, -43322.1421,  -7701.9112,   4675.0671,  11315.3575,
          -1463.6756, -18395.6414]], dtype=torch.float64)
	q_value: tensor([[-28.8677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31763513208182514, distance: 0.9452899108138243 entropy 11.54064287260115
epoch: 60, step: 61
	action: tensor([[ -1577.7967,    575.1985,  -8617.4996,  18100.8803, -30146.5622,
           6753.2893,   9319.7079]], dtype=torch.float64)
	q_value: tensor([[-34.0064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3515552972487471, distance: 0.9214953894955712 entropy 11.469358263728319
epoch: 60, step: 62
	action: tensor([[  1352.3482, -24392.6961,  -1792.2882,  10788.6583, -16009.5987,
         -17839.7367,  11493.4351]], dtype=torch.float64)
	q_value: tensor([[-30.9047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.456399883751406
epoch: 60, step: 63
	action: tensor([[-11500.3283, -26260.3703,  -6490.0039, -12502.1676,  22741.1099,
           8421.5418,  -6210.6189]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.13469950178291
epoch: 60, step: 64
	action: tensor([[-32430.9597, -31077.3460,   6342.3284, -12671.8899,  11597.4095,
          27095.5602,  22152.6935]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4529406333036683, distance: 1.3793695292146992 entropy 11.13469950178291
epoch: 60, step: 65
	action: tensor([[-58942.4885,  10031.4348,   4811.5568,  -8513.2002,  20791.8752,
          22332.4301, -41304.2238]], dtype=torch.float64)
	q_value: tensor([[-38.8890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.518321744121804, distance: 0.7942097496037067 entropy 11.703940247310555
epoch: 60, step: 66
	action: tensor([[-10302.3263, -48904.8624,   3890.3938, -10830.0210,  17168.0782,
         -60263.6882,  20035.2473]], dtype=torch.float64)
	q_value: tensor([[-50.6727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9137508440050437, distance: 1.5830673464083527 entropy 11.672824612168323
epoch: 60, step: 67
	action: tensor([[-54275.7403, -18443.0718, -40636.7849, -57245.9028,  52441.4124,
         -42756.8537,  41514.5278]], dtype=torch.float64)
	q_value: tensor([[-39.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.643924599863972, distance: 1.4672281153055768 entropy 11.692834632757558
epoch: 60, step: 68
	action: tensor([[-36082.6008,    166.3752, -35913.0538,  10354.3360, -29658.6948,
         -29432.5643,  -7628.0392]], dtype=torch.float64)
	q_value: tensor([[-38.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32637233939140686, distance: 1.3179211313690467 entropy 11.810116284130189
epoch: 60, step: 69
	action: tensor([[ -3823.1340,  13849.1763, -12797.9719, -12019.3901, -21083.6021,
           6726.5438,   4571.9801]], dtype=torch.float64)
	q_value: tensor([[-33.1051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.018628506112666, distance: 1.625866542773648 entropy 11.489074571840604
epoch: 60, step: 70
	action: tensor([[-32508.9427,   -380.8995, -63889.1821,  79256.5860,  -5029.0094,
          35916.6764,  35829.8523]], dtype=torch.float64)
	q_value: tensor([[-42.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2120078378250825, distance: 1.701962645517349 entropy 11.520162488364496
epoch: 60, step: 71
	action: tensor([[ 18679.3792,  -2252.8513,  25013.7105,  16438.7308,  43245.7883,
         -48405.6140,  15098.2961]], dtype=torch.float64)
	q_value: tensor([[-31.9431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4305559773974317, distance: 0.8635395093621144 entropy 11.732774075014449
epoch: 60, step: 72
	action: tensor([[-29266.2558, -47107.1366, -20624.3182,  31882.1318,  -3441.5734,
            644.1876,  34376.1886]], dtype=torch.float64)
	q_value: tensor([[-32.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.401995022882935, distance: 1.354970778363193 entropy 11.458697598848843
epoch: 60, step: 73
	action: tensor([[16894.8170,  1988.8014, 13760.2065, 34101.1077, 30210.8527,  5882.0899,
         -4510.8336]], dtype=torch.float64)
	q_value: tensor([[-27.5631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47915833131578556, distance: 0.8258659373133799 entropy 11.495922842692764
epoch: 60, step: 74
	action: tensor([[-10721.6729, -19348.1799,   2778.0836, -14629.3583,   5209.9213,
          23586.8377,  12692.6844]], dtype=torch.float64)
	q_value: tensor([[-32.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5346988335185636, distance: 1.4176475601628757 entropy 11.334820684483933
epoch: 60, step: 75
	action: tensor([[  4152.8933, -34061.0646,  -5262.0955,   3646.9171, -20379.1396,
         -15048.7344, -24441.0670]], dtype=torch.float64)
	q_value: tensor([[-28.8451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0547676162926396, distance: 1.1125665266447202 entropy 11.35620137148395
epoch: 60, step: 76
	action: tensor([[-41069.2100, -43177.7389,  -9825.4736,  36239.1383,  -5279.3336,
         -38591.5972,  25756.7464]], dtype=torch.float64)
	q_value: tensor([[-36.4733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09766839295983254, distance: 1.1989257082838045 entropy 11.715858716378102
epoch: 60, step: 77
	action: tensor([[ 30629.6647,  -8316.6299,   9469.1638, -20670.0838,  54297.2017,
          28183.4836,  27392.5804]], dtype=torch.float64)
	q_value: tensor([[-32.5424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12832403247920765, distance: 1.2155521966044773 entropy 11.690080805486454
epoch: 60, step: 78
	action: tensor([[-11243.0445,  -4814.3526,  11545.5308,  -4132.8617,   7074.2619,
           7305.0259,   1958.3754]], dtype=torch.float64)
	q_value: tensor([[-33.7379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9959023543806733, distance: 1.6166884607057954 entropy 11.568716059167132
epoch: 60, step: 79
	action: tensor([[-33323.1588, -29023.9475, -85298.1447,  -3845.5035, -60640.8084,
          13065.3383, -19434.3074]], dtype=torch.float64)
	q_value: tensor([[-37.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5249495200256851, distance: 1.4131375188067992 entropy 11.617804802592826
epoch: 60, step: 80
	action: tensor([[  6918.8907, -16751.0672,  -9092.9504,  42489.8455,   5543.5206,
           5802.5129,  -8027.7876]], dtype=torch.float64)
	q_value: tensor([[-34.9118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20514066116073137, distance: 1.02023879525247 entropy 11.604688182793671
epoch: 60, step: 81
	action: tensor([[-17889.9058, -27077.4977,   9081.2164, -18409.3648, -45631.6778,
         -24189.2674,  14169.6244]], dtype=torch.float64)
	q_value: tensor([[-32.6698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1851835975739324, distance: 1.6916116180940406 entropy 11.505811930134323
epoch: 60, step: 82
	action: tensor([[ -1880.7986, -36968.9504, -56333.3929,  55212.8880, -77716.0122,
          31552.5556,   9643.4922]], dtype=torch.float64)
	q_value: tensor([[-37.7814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0851590225086103, distance: 1.1920744674765122 entropy 11.778248810531863
epoch: 60, step: 83
	action: tensor([[-6571.7361,  3624.5210, -4166.0327, 19289.3366, 24099.0014, -3301.0454,
          -755.4069]], dtype=torch.float64)
	q_value: tensor([[-28.6359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026345642315107387, distance: 1.1593204977719385 entropy 11.408620366427643
epoch: 60, step: 84
	action: tensor([[-18580.9840, -25233.3148,  -4507.6425,  25185.5380,   2224.3206,
          11168.8410, -60187.6795]], dtype=torch.float64)
	q_value: tensor([[-36.7393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11563164998129172, distance: 1.2086960602524988 entropy 11.630232828668793
epoch: 60, step: 85
	action: tensor([[ 14786.3105,  19289.5578,  -4018.3113, -40402.5786,   1010.3441,
         -11095.1754,   2551.1188]], dtype=torch.float64)
	q_value: tensor([[-30.7246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.639022588047911
epoch: 60, step: 86
	action: tensor([[ -4290.6911,   8469.6752,  -6547.8607,  36635.4644,  -4103.0304,
         -22435.7681,  -7189.2535]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3929626763470696, distance: 0.8915883610781868 entropy 11.13469950178291
epoch: 60, step: 87
	action: tensor([[-17334.0560, -53517.2783,    175.9831, -10505.6656,   3505.1909,
          42050.5612,  17956.3026]], dtype=torch.float64)
	q_value: tensor([[-33.7536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9396648573005213, distance: 1.5937494281391906 entropy 11.515845910271647
epoch: 60, step: 88
	action: tensor([[-22689.3626, -28235.5509,   2899.9423,   5601.0157,  33692.3484,
          19438.9402, -14323.1339]], dtype=torch.float64)
	q_value: tensor([[-27.9618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5727826853526157, distance: 1.4351293719135323 entropy 11.347102084985071
epoch: 60, step: 89
	action: tensor([[-18517.4722, -46060.8011,   -549.7827,  15020.5773,  24623.3128,
          29467.8307,  -4677.3822]], dtype=torch.float64)
	q_value: tensor([[-34.2763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09170411445750826, distance: 1.0906122380669592 entropy 11.761101057666838
epoch: 60, step: 90
	action: tensor([[-11385.4311, -25842.7450,  -8794.9411, -19584.5854, -18710.7300,
         -34139.6765,  11250.7719]], dtype=torch.float64)
	q_value: tensor([[-37.5182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7150583609599444, distance: 1.4986359442726958 entropy 11.811014960639325
epoch: 60, step: 91
	action: tensor([[-23071.6507, -16371.7898, -16126.8995,  31499.1709,  16504.9747,
          35110.2295,  11380.8468]], dtype=torch.float64)
	q_value: tensor([[-32.4555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02967528948104947, distance: 1.1611994956376903 entropy 11.5096637850534
epoch: 60, step: 92
	action: tensor([[-30504.3826, -50634.2573, -54976.0220,  57048.9981,  28609.5482,
           2979.5431,  52960.6811]], dtype=torch.float64)
	q_value: tensor([[-32.0512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2768312088213507, distance: 1.2930741743428829 entropy 11.677675149056537
epoch: 60, step: 93
	action: tensor([[  3635.2963, -33410.0110,   8294.3485,   2064.7635,  18274.4001,
         -17814.0476,  55301.4617]], dtype=torch.float64)
	q_value: tensor([[-35.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39045261485506, distance: 0.8934297906395383 entropy 11.723307140484112
epoch: 60, step: 94
	action: tensor([[  8654.5409, -57002.4850,  39217.7966,  -1841.4553,   9671.9712,
          12854.6787, -42911.7582]], dtype=torch.float64)
	q_value: tensor([[-39.4602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13375951050812174, distance: 1.218476520233026 entropy 11.695989888294026
epoch: 60, step: 95
	action: tensor([[ -3952.6787,  19596.5175, -14224.6643, -33260.8263,  24851.8055,
           9934.6100,  25691.2087]], dtype=torch.float64)
	q_value: tensor([[-35.2372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5236153849738455, distance: 0.7898335127298591 entropy 11.502762631094784
epoch: 60, step: 96
	action: tensor([[-14841.4366,  -6840.2144,   6799.0707,  12490.0773, -45061.3724,
           2624.5812, -13046.0827]], dtype=torch.float64)
	q_value: tensor([[-41.1835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45227513183763435, distance: 1.3790535914663118 entropy 11.377083709316903
epoch: 60, step: 97
	action: tensor([[ -8449.8362,  12252.7869, -25602.2786,   5913.6663,   5234.9242,
          24811.6537,   3437.3951]], dtype=torch.float64)
	q_value: tensor([[-30.7114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08518083598221682, distance: 1.192086448740879 entropy 11.536349150137664
epoch: 60, step: 98
	action: tensor([[ 20406.1686, -26940.6230,  49461.7629,  -1554.0971,  15219.6233,
         -14531.3293,  13463.5416]], dtype=torch.float64)
	q_value: tensor([[-33.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.48438792631485
epoch: 60, step: 99
	action: tensor([[ -6329.4592, -13475.1917,  18912.5185, -16775.6625,   6064.9655,
           8530.5420, -25526.3448]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21216448303651414, distance: 1.2599040462279991 entropy 11.13469950178291
epoch: 60, step: 100
	action: tensor([[ 17845.4323, -35606.3415, -11516.6508, -27945.6227,  29809.0005,
          -7488.5375,   -333.8680]], dtype=torch.float64)
	q_value: tensor([[-30.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17211747358984963, distance: 1.0412165233410202 entropy 11.543072148394845
epoch: 60, step: 101
	action: tensor([[ -1581.0790,  18316.1535,  -4338.4993,  19622.8590,  20430.6971,
         -17467.0513,  23954.6805]], dtype=torch.float64)
	q_value: tensor([[-32.3939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.373924685864383
epoch: 60, step: 102
	action: tensor([[-29320.7700, -11107.3438, -10764.2007,  -8833.5422,  28653.4200,
           5596.5914, -52143.9733]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13099680094727728, distance: 1.2169910420849688 entropy 11.13469950178291
epoch: 60, step: 103
	action: tensor([[-49646.0108, -10363.5381,  14691.6965,  30385.2733,  -3934.9760,
          -1995.3827,   8614.3496]], dtype=torch.float64)
	q_value: tensor([[-36.1598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3170132282698819, distance: 1.3132631609901335 entropy 11.602161070519255
epoch: 60, step: 104
	action: tensor([[-22901.9351,  -4263.7196,   7106.4222,  15210.0730,  -1495.2335,
         -16569.1126,  -9405.2362]], dtype=torch.float64)
	q_value: tensor([[-28.5752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7332681286275295, distance: 1.5065708802121704 entropy 11.462331288234646
epoch: 60, step: 105
	action: tensor([[ 42818.7695,   -880.2012,   4551.2982, -21337.8979,  54358.4636,
           3017.9899, -19859.1696]], dtype=torch.float64)
	q_value: tensor([[-29.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08665671621908988, distance: 1.0936383048337688 entropy 11.507973899102423
epoch: 60, step: 106
	action: tensor([[ -5883.6190, -12777.7566,  17523.9190,  24006.8977, -24792.6884,
          41314.7830,  33144.3232]], dtype=torch.float64)
	q_value: tensor([[-38.0333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22988189923072844, distance: 1.0042350123079664 entropy 11.649563338426137
epoch: 60, step: 107
	action: tensor([[-20169.8322,  17722.8279,  12869.3278,  70634.2795,  49780.4195,
          29051.7131,  10241.4054]], dtype=torch.float64)
	q_value: tensor([[-30.8295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5310556545521121, distance: 0.7836413516085461 entropy 11.550544560771025
epoch: 60, step: 108
	action: tensor([[ -9955.1457,  -5531.4262,  27358.9136,  67549.5101,   3712.6373,
         -17334.3678, -27045.4407]], dtype=torch.float64)
	q_value: tensor([[-32.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008706149836242072, distance: 1.1393519480356362 entropy 11.517818097576825
epoch: 60, step: 109
	action: tensor([[ -9055.8404,  38597.7922,  25014.1925,  17806.1846,  57635.7471,
         -12785.3548, -59094.7445]], dtype=torch.float64)
	q_value: tensor([[-33.0183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6902597544049738, distance: 0.6368769226839404 entropy 11.618684189099588
epoch: 60, step: 110
	action: tensor([[11786.8091, -1157.9917,  7349.5856,  -410.6844, 45550.9416,  3763.4584,
         19249.1303]], dtype=torch.float64)
	q_value: tensor([[-32.5570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.477111122364777
epoch: 60, step: 111
	action: tensor([[ -6069.0349,  -3352.8879,  31266.9923,  13440.7074, -25797.0598,
           -184.4197,  24989.5148]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2115249836848785, distance: 1.0161332477716092 entropy 11.13469950178291
epoch: 60, step: 112
	action: tensor([[ 38993.9300,  43166.1309, -14704.6450,  24286.0211,  -7043.2673,
           9868.3853,   2972.6747]], dtype=torch.float64)
	q_value: tensor([[-33.0892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3462473884027809, distance: 0.9252591991832378 entropy 11.773410772502165
epoch: 60, step: 113
	action: tensor([[ 5220.9975,  -217.1985, 15562.1861,  3008.2497,  5550.7207, 32681.7991,
         -9447.7265]], dtype=torch.float64)
	q_value: tensor([[-30.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7965872734068757, distance: 0.5161141355673621 entropy 11.402140863531075
epoch: 60, step: 114
	action: tensor([[-49707.9206, -33842.9745,  41393.6747, -23568.2433,  46737.6703,
          34523.5797, -19090.9534]], dtype=torch.float64)
	q_value: tensor([[-39.4028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21012664787141633, distance: 1.25884455438718 entropy 11.654234988895329
epoch: 60, step: 115
	action: tensor([[-17717.6699, -49935.8614, -53936.7245,  28917.1777, -17902.7121,
         -22256.2564,  -4248.4946]], dtype=torch.float64)
	q_value: tensor([[-34.2253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40512750971733746, distance: 1.3564836439088488 entropy 11.711590464976323
epoch: 60, step: 116
	action: tensor([[ -5589.3781, -19188.3121,   1405.9609, -19732.5111,  38924.1968,
          19389.7434,  22566.1820]], dtype=torch.float64)
	q_value: tensor([[-31.9155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9886754534030024, distance: 1.6137588978386954 entropy 11.725012228646671
epoch: 60, step: 117
	action: tensor([[ -8143.9523, -22063.6418, -11170.9675,  27900.7635, -10430.5527,
           7171.6276, -16964.1911]], dtype=torch.float64)
	q_value: tensor([[-33.1064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017289858326137164, distance: 1.15419463356471 entropy 11.619050439403605
epoch: 60, step: 118
	action: tensor([[-31476.2438, -18104.2176,  10531.9512,  17481.8086,   5204.5952,
         -46766.9731,  34576.4682]], dtype=torch.float64)
	q_value: tensor([[-30.5254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21928098750116898, distance: 1.263597023369909 entropy 11.592740836797914
epoch: 60, step: 119
	action: tensor([[-32580.4900, -11469.5563,  -2899.2345,   1431.5371,   3047.5031,
         -11626.1915,    794.0460]], dtype=torch.float64)
	q_value: tensor([[-32.8456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18590338384900007, distance: 1.2461816368594265 entropy 11.59047005043953
epoch: 60, step: 120
	action: tensor([[ 25792.4977, -53163.2624,  31396.3322,  29056.6624, -15613.9879,
           -132.4508, -18394.1022]], dtype=torch.float64)
	q_value: tensor([[-34.6687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0271821248131483, distance: 1.1597928307546739 entropy 11.670505265650522
epoch: 60, step: 121
	action: tensor([[-58074.3123, -47608.4344, -39166.6905, -64642.1222,  -7681.5221,
         -15586.1990, -11299.4059]], dtype=torch.float64)
	q_value: tensor([[-34.3380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5781060595376282, distance: 1.4375560507923368 entropy 11.702756156059028
epoch: 60, step: 122
	action: tensor([[ 18302.8558, -10871.0250, -69629.6506,  10643.9201,   7901.0376,
           3942.7564,  75247.0661]], dtype=torch.float64)
	q_value: tensor([[-38.7612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26404617168963085, distance: 0.9817072033038585 entropy 11.753649480848578
epoch: 60, step: 123
	action: tensor([[ -5862.7379,   3755.2131,  46963.7263,  -9107.8771, -24662.7028,
          12319.4634,   4786.7018]], dtype=torch.float64)
	q_value: tensor([[-38.7575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.64021471979317
epoch: 60, step: 124
	action: tensor([[-11260.6589,  -5830.0284,  19362.4329,  10194.4290,  -1493.5601,
          -2416.1088,  21808.9409]], dtype=torch.float64)
	q_value: tensor([[-37.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12011080285962861, distance: 1.0734224797742158 entropy 11.13469950178291
epoch: 60, step: 125
	action: tensor([[-13147.4168, -41760.6368,  27506.3446, -36859.3080, -48783.0968,
          -9203.8941,    921.7904]], dtype=torch.float64)
	q_value: tensor([[-32.4232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17396374820834093, distance: 1.2398925095947646 entropy 11.713097025907611
epoch: 60, step: 126
	action: tensor([[-21596.9563, -27309.4993, -21245.3451,    459.2262, -30973.6493,
         -16161.7697,  19449.9941]], dtype=torch.float64)
	q_value: tensor([[-31.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08125596878430219, distance: 1.189928734998593 entropy 11.490482952089044
epoch: 60, step: 127
	action: tensor([[ 38897.8181,  51051.0948,  12509.5648,  55330.1957, -16695.8228,
           2632.2655,  -8148.2432]], dtype=torch.float64)
	q_value: tensor([[-34.5676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42180564960340783, distance: 0.8701489794773521 entropy 11.746712018963754
LOSS epoch 60 actor 488.6517280244808 critic 127.40165970193581
epoch: 61, step: 0
	action: tensor([[-33774.7275,  14573.8883, -18914.3055,  53096.8048, -28800.6365,
          11332.0163,  18353.1812]], dtype=torch.float64)
	q_value: tensor([[-28.3177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.535994055264371
epoch: 61, step: 1
	action: tensor([[-10519.7595, -28248.7947, -24979.4669, -14817.4690,  -9144.2045,
           5515.8856,  31065.8684]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09820534596701358, distance: 1.0867021379451873 entropy 11.173171362744345
epoch: 61, step: 2
	action: tensor([[  6391.8601, -46283.6656, -24790.3976, -35153.2253, -30401.3428,
         -11516.0667,  36823.0640]], dtype=torch.float64)
	q_value: tensor([[-37.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.834562234446725
epoch: 61, step: 3
	action: tensor([[-14692.2882,  -2728.7560,  14847.6505,  26274.4498,   2501.8828,
         -28870.7476,  29557.8064]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7277633023831511, distance: 1.5041765575481447 entropy 11.173171362744345
epoch: 61, step: 4
	action: tensor([[-48084.7538, -61793.7533, -20646.7356,  29433.9977,  10448.7246,
         -35807.6077,  32899.4065]], dtype=torch.float64)
	q_value: tensor([[-28.0878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8416342947916453, distance: 1.5529532793162666 entropy 11.624183853010445
epoch: 61, step: 5
	action: tensor([[ -2126.4618, -24323.1543, -56151.5891,   7531.5304,   2332.6043,
         -17892.9592, -23940.0952]], dtype=torch.float64)
	q_value: tensor([[-26.1579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4567472733768374, distance: 1.381175290754928 entropy 11.558880072263634
epoch: 61, step: 6
	action: tensor([[-27947.5099,  -2499.8996,   -636.4991,   1924.4314, -11322.1451,
          16609.1992,  29056.8021]], dtype=torch.float64)
	q_value: tensor([[-29.4722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042924711809617544, distance: 1.119514556027007 entropy 11.632655554836552
epoch: 61, step: 7
	action: tensor([[-25903.0503,  25914.9369,  -3097.0800,  -6848.4525,  19388.2676,
          30478.1851,  -7592.0142]], dtype=torch.float64)
	q_value: tensor([[-32.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6213451207748488, distance: 0.7041715455778145 entropy 11.823870872932025
epoch: 61, step: 8
	action: tensor([[-19235.7448, -33030.3374,  -6456.5393,   3394.0550,  -1696.7334,
          29955.1209,  17431.1797]], dtype=torch.float64)
	q_value: tensor([[-34.1626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07057849729017251, distance: 1.1032223491631665 entropy 11.60582212211104
epoch: 61, step: 9
	action: tensor([[ -6184.9406, -39854.2425, -51974.4793,  25025.2810,  -4935.3033,
         -42504.5643,  39071.0001]], dtype=torch.float64)
	q_value: tensor([[-31.9993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1941981006693535, distance: 1.0272374323517077 entropy 11.837899850689402
epoch: 61, step: 10
	action: tensor([[ -32805.5330, -108042.7110,   -3870.9781,  -77953.6145,    6381.8983,
          -60919.6389,   55980.1778]], dtype=torch.float64)
	q_value: tensor([[-30.1900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5048017670655776, distance: 1.4037712369230915 entropy 11.84276052764612
epoch: 61, step: 11
	action: tensor([[  5219.6946, -30103.4106, -20444.3764,  24985.0328, -23642.3598,
         -32557.6482,  -7706.9862]], dtype=torch.float64)
	q_value: tensor([[-30.3459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4395025733983655, distance: 0.8567290727474323 entropy 11.726075815340774
epoch: 61, step: 12
	action: tensor([[-27701.6322, -13254.0437, -46004.1182,  20092.7189,  24120.0258,
          37470.8069,  19820.9411]], dtype=torch.float64)
	q_value: tensor([[-32.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24487356515290415, distance: 1.2767895387581965 entropy 11.617475790607472
epoch: 61, step: 13
	action: tensor([[ -3309.3581, -44981.6684, -34380.9937,  24132.5572,  12718.1731,
         -52236.9127,  10454.4122]], dtype=torch.float64)
	q_value: tensor([[-28.5990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.725779504477958, distance: 1.5033127704381288 entropy 11.738221287530157
epoch: 61, step: 14
	action: tensor([[  5459.0414, -19599.6379,  31565.1213,  -3363.9109,   3515.7425,
          18227.0469,  -1699.6012]], dtype=torch.float64)
	q_value: tensor([[-26.8945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0792595086904786, distance: 1.1888296692223463 entropy 11.551474878921173
epoch: 61, step: 15
	action: tensor([[-26356.6003, -65695.1011, -38654.9780,  12523.6791,    104.0128,
          35789.0236,  -6862.3875]], dtype=torch.float64)
	q_value: tensor([[-29.8777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8874500602659356, distance: 1.572151621741573 entropy 11.694005945590144
epoch: 61, step: 16
	action: tensor([[58543.9152, 34247.1071, 26681.3841, 33819.2781, 57432.1452, 49678.6490,
         24691.7639]], dtype=torch.float64)
	q_value: tensor([[-27.4559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.764638675132943
epoch: 61, step: 17
	action: tensor([[-24481.0874, -34975.4223,  -8140.7523,  -7369.6887,  -1985.6827,
         -26705.4647, -11963.4110]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05294849903322518, distance: 1.1136365895142555 entropy 11.173171362744345
epoch: 61, step: 18
	action: tensor([[ -5888.0340, -46225.6348,  53621.2060,  51934.8070, -30594.2009,
          28796.6223,  26565.0186]], dtype=torch.float64)
	q_value: tensor([[-36.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6149054396383742, distance: 1.4542204310880942 entropy 11.785261242443024
epoch: 61, step: 19
	action: tensor([[ -3393.7258,  11313.3349, -27231.6872, -13610.7499,  11097.6572,
          10704.5763,  -6147.6216]], dtype=torch.float64)
	q_value: tensor([[-24.6994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07577719363805346, distance: 1.100132598937005 entropy 11.497276139525367
epoch: 61, step: 20
	action: tensor([[ -9496.4976,  -9264.1993,  48270.4758, -41592.9110,  18287.9476,
          12687.7672,  13321.4835]], dtype=torch.float64)
	q_value: tensor([[-37.3646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07044383623082096, distance: 1.103302267536614 entropy 11.455062951652733
epoch: 61, step: 21
	action: tensor([[-10102.7510, -42965.1540,  32536.0348, -26374.5683,   4629.3678,
          22846.8429,  42129.6398]], dtype=torch.float64)
	q_value: tensor([[-31.5033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5779552573042297, distance: 1.43748736344366 entropy 11.709408128547562
epoch: 61, step: 22
	action: tensor([[-36698.4454, -52377.2393,  30506.3635, -20526.2400,  33877.3059,
          39081.4691, -12967.4543]], dtype=torch.float64)
	q_value: tensor([[-29.0094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4808001383719096, distance: 1.3925311423015299 entropy 11.563141661490562
epoch: 61, step: 23
	action: tensor([[-41473.0431,   9425.6217,  24961.0783,   3083.3536, -16832.4252,
          -1862.4004,  -4172.6284]], dtype=torch.float64)
	q_value: tensor([[-29.4143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2677236512067944, distance: 0.9792513906914581 entropy 11.587480175106467
epoch: 61, step: 24
	action: tensor([[-11764.9545,  25460.3208,   3053.2902, -31696.0949,    236.8808,
          16731.8707,  29663.0483]], dtype=torch.float64)
	q_value: tensor([[-28.2756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022381784411600325, distance: 1.1570796214617383 entropy 11.368014600110795
epoch: 61, step: 25
	action: tensor([[  6093.5312,  -8180.9396,  -9772.4149,  19272.0320,  15923.3544,
           9809.1591, -10434.8556]], dtype=torch.float64)
	q_value: tensor([[-40.0778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33145619954841143, distance: 0.9356676755606969 entropy 11.503382371616167
epoch: 61, step: 26
	action: tensor([[-21821.7044, -14537.3203, -11052.0517,  33084.4332,  13865.6962,
         -28422.9467,   1940.7293]], dtype=torch.float64)
	q_value: tensor([[-27.8174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.056407359114378997, distance: 1.176176240756383 entropy 11.44571853845045
epoch: 61, step: 27
	action: tensor([[ -6914.4404, -37108.9351, -49203.5275,  16582.5091, -19725.4802,
         -26547.7214,  18210.3078]], dtype=torch.float64)
	q_value: tensor([[-28.1062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19665605380783202, distance: 1.0256695346748184 entropy 11.668123663457214
epoch: 61, step: 28
	action: tensor([[-28157.7181, -24133.9416,  69169.4260,  13448.5927, -12954.2694,
          54928.6560, -58975.4549]], dtype=torch.float64)
	q_value: tensor([[-28.8609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6063796120556513, distance: 1.4503766022317681 entropy 11.775116835275627
epoch: 61, step: 29
	action: tensor([[-40093.4902, -10747.8067,  25208.7571, -19106.6915, -18983.0185,
          35805.1507,  32879.7221]], dtype=torch.float64)
	q_value: tensor([[-28.8230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44563178486434407, distance: 1.3758957763403465 entropy 11.701105417044745
epoch: 61, step: 30
	action: tensor([[-55090.6484, -46401.1158,  26305.6992,  37332.9920,  -8612.4600,
         -28863.1030, -19798.2714]], dtype=torch.float64)
	q_value: tensor([[-25.4147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013211392624783547, distance: 1.1367599302244478 entropy 11.499971241718713
epoch: 61, step: 31
	action: tensor([[-22180.3812, -23142.3794, -26406.7544, -21757.9110, -44471.5004,
         -54411.4383,  30082.2685]], dtype=torch.float64)
	q_value: tensor([[-26.2757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39769754936376045, distance: 1.3528925184532241 entropy 11.639413139608099
epoch: 61, step: 32
	action: tensor([[-60933.2456,   -604.2752,   6489.8401,  38370.5055,  -4180.4611,
            129.7122,  -9007.0272]], dtype=torch.float64)
	q_value: tensor([[-26.2700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47281196111325086, distance: 1.38877005809714 entropy 11.407998648609132
epoch: 61, step: 33
	action: tensor([[-30251.1667, -22745.7800,  25221.6927, -30381.9682,  -4357.8800,
          46722.0089,  -6989.9170]], dtype=torch.float64)
	q_value: tensor([[-25.6513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18061520470928172, distance: 1.2434000465471147 entropy 11.751976276613311
epoch: 61, step: 34
	action: tensor([[-40124.5034, -37889.4233, -12988.0404,  90456.6998,   6108.8403,
         -16618.7021,  38293.6343]], dtype=torch.float64)
	q_value: tensor([[-30.3274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35888246545033065, distance: 1.3339748466155512 entropy 11.689289460259241
epoch: 61, step: 35
	action: tensor([[-67772.2853, -17501.6103,   7372.2134,  -3102.2997, -31754.4793,
         -30145.2604, -54884.8742]], dtype=torch.float64)
	q_value: tensor([[-28.1463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6654378110964603, distance: 1.476797346915582 entropy 11.70187187958105
epoch: 61, step: 36
	action: tensor([[  8122.2997, -35757.9008, -15257.9401, -16705.6059,  -7899.7022,
           5222.6121,  24652.0708]], dtype=torch.float64)
	q_value: tensor([[-25.7809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6956379799437022, distance: 0.6313234515326013 entropy 11.38733437708244
epoch: 61, step: 37
	action: tensor([[-46942.9140, -28035.6454,  40112.0585,  35931.8855,   9136.6708,
          35633.9623,  21896.5727]], dtype=torch.float64)
	q_value: tensor([[-32.8641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11184596432823235, distance: 1.0784520492865897 entropy 11.677956897285103
epoch: 61, step: 38
	action: tensor([[  2406.4653, -19001.3899,  17482.6833,  21099.4958,  26129.7733,
          13374.1212, -16874.3141]], dtype=torch.float64)
	q_value: tensor([[-25.2929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2301844625093723, distance: 1.0040377214723384 entropy 11.442613255676122
epoch: 61, step: 39
	action: tensor([[-31532.7834,   6536.9081,  -8627.8768,  48820.5538,  21228.2304,
          46428.6482,  34747.7810]], dtype=torch.float64)
	q_value: tensor([[-35.4577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14945781069567543, distance: 1.22688317603164 entropy 11.787475823193391
epoch: 61, step: 40
	action: tensor([[  8354.3251, -28999.6424, -78133.7965,  10516.6239, -64090.0579,
          85137.0705,  10516.5921]], dtype=torch.float64)
	q_value: tensor([[-35.2916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10580058063241116, distance: 1.2033586942623082 entropy 11.84016584830915
epoch: 61, step: 41
	action: tensor([[-1313.6389, -1788.0991, -4472.3514, 19235.1179, 33268.7035, 15039.5442,
         18708.6824]], dtype=torch.float64)
	q_value: tensor([[-31.1951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07128592891299579, distance: 1.10280240894105 entropy 11.837529848932368
epoch: 61, step: 42
	action: tensor([[-52328.7742, -27385.0377, -40306.0383,  -7909.5152,  35256.0180,
         -61340.6850,  30216.0922]], dtype=torch.float64)
	q_value: tensor([[-30.3652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.810796989643374, distance: 1.5398966535431737 entropy 11.771536327882218
epoch: 61, step: 43
	action: tensor([[-26932.5472,   1497.8852,  -2021.0364,  26416.8527, -55679.7315,
          30991.9714,   3716.2163]], dtype=torch.float64)
	q_value: tensor([[-24.6634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.35343284406807
epoch: 61, step: 44
	action: tensor([[ -4112.6070,  -7992.6921,   8690.0763,  -8927.0729,  -8792.6269,
          21794.3639, -17004.4481]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06444835566041252, distance: 1.1806440722947396 entropy 11.173171362744345
epoch: 61, step: 45
	action: tensor([[  9330.1930, -10148.9069,  12867.8700,  19577.4957,  13745.0626,
         -39663.8328,  41461.5052]], dtype=torch.float64)
	q_value: tensor([[-32.5890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48756876181417375, distance: 0.8191708538192446 entropy 11.733939253964184
epoch: 61, step: 46
	action: tensor([[-51195.9516,   3044.8369,  -3048.9504,  17589.7222,  -1069.6866,
          24946.2627,  19031.3770]], dtype=torch.float64)
	q_value: tensor([[-28.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.523887571670667
epoch: 61, step: 47
	action: tensor([[ -2498.5882,  -4358.2861,  -5423.8294,   1299.5034, -11081.4583,
          -5764.5281,  16277.0539]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010064832686168756, distance: 1.1500886528103662 entropy 11.173171362744345
epoch: 61, step: 48
	action: tensor([[ -6685.9092, -10433.2844,   1571.2022,  10317.6170,   2815.9538,
         -29684.9384,  46854.6296]], dtype=torch.float64)
	q_value: tensor([[-28.6751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.665300575810789, distance: 1.4767365001881698 entropy 11.646616455882091
epoch: 61, step: 49
	action: tensor([[-21701.9358,  13084.1526,  27116.5323, -19844.2021,  13320.2012,
         -29064.9020,   7933.1086]], dtype=torch.float64)
	q_value: tensor([[-26.3603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6025079139053799, distance: 0.7214744179724932 entropy 11.54444173679499
epoch: 61, step: 50
	action: tensor([[ 16171.2860, -37489.5768, -41887.0036,  31767.7328,  16473.8422,
         -40341.2302,  19656.5909]], dtype=torch.float64)
	q_value: tensor([[-37.8582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.532596633330176
epoch: 61, step: 51
	action: tensor([[ -6331.9936, -54911.0917,    804.8628,  -9202.5535,  47802.7562,
          15608.2047,  17473.5426]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.173171362744345
epoch: 61, step: 52
	action: tensor([[ -7098.9208, -30879.1431, -22162.4053, -19168.5359,   6327.3910,
          13626.6564,   1098.3111]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49925011241392636, distance: 1.4011793824754153 entropy 11.173171362744345
epoch: 61, step: 53
	action: tensor([[ -4797.8159, -51906.3103, -31569.8894,  -9135.2647,  -6958.5984,
         -40613.0171,  34786.4705]], dtype=torch.float64)
	q_value: tensor([[-30.9133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7693043298908468, distance: 1.522151792462898 entropy 11.599307090531457
epoch: 61, step: 54
	action: tensor([[-39024.0686,   2308.2394, -11346.8217, -27614.7674,   3506.4956,
          62347.7088, -13911.2262]], dtype=torch.float64)
	q_value: tensor([[-38.7503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36064576294801554, distance: 1.3348400546105574 entropy 11.899632830362291
epoch: 61, step: 55
	action: tensor([[-21166.6546, -32013.2501, -45378.6268,  16663.0553,   5792.1885,
          20290.8125, -33042.7743]], dtype=torch.float64)
	q_value: tensor([[-36.1140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7157556320286418, distance: 1.4989405546938366 entropy 11.39970384408052
epoch: 61, step: 56
	action: tensor([[ -8752.3401,   4453.5934,  52536.1425,  24520.7930,  17406.4618,
         -29288.3478,  31120.6206]], dtype=torch.float64)
	q_value: tensor([[-23.4545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6224930145283267, distance: 0.7031033860279194 entropy 11.43579402031524
epoch: 61, step: 57
	action: tensor([[ 1.7637e+04, -1.6718e+01,  2.4900e+04,  1.3212e+04,  3.0798e+04,
         -5.6431e+03, -3.4666e+04]], dtype=torch.float64)
	q_value: tensor([[-29.7730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5924059853297656, distance: 0.7305847325246775 entropy 11.557522939189692
epoch: 61, step: 58
	action: tensor([[-39251.6551, -34749.7983,  18689.7911,  47220.1793,  36040.1813,
         -43539.4277,  -4297.5268]], dtype=torch.float64)
	q_value: tensor([[-30.1684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2827021248079633, distance: 1.2960435657794966 entropy 11.58902148192019
epoch: 61, step: 59
	action: tensor([[ -1481.6513, -22419.8951, -28787.8357,   9134.2080, -25582.9151,
           5967.5784,  23450.8524]], dtype=torch.float64)
	q_value: tensor([[-29.4797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0788833561978759, distance: 1.1886224807702739 entropy 11.629270877095033
epoch: 61, step: 60
	action: tensor([[-16539.2844, -22495.7501, -24780.7911, -13957.8517,    305.9936,
           -436.6349,  64064.4149]], dtype=torch.float64)
	q_value: tensor([[-29.8688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7489696202580916, distance: 1.5133794281818622 entropy 11.73559605761136
epoch: 61, step: 61
	action: tensor([[-39509.1284, -44867.8043,  24921.6807, -21241.5385, -17649.6439,
          24937.8440, -42862.9486]], dtype=torch.float64)
	q_value: tensor([[-24.3969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9313432867802127, distance: 1.5903269931935649 entropy 11.458427002260551
epoch: 61, step: 62
	action: tensor([[  5389.3220, -29876.6335, -34052.6948,  15337.1372,  -9746.8418,
           5656.8849,  -3830.3660]], dtype=torch.float64)
	q_value: tensor([[-27.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2774690294815604, distance: 0.97271346328531 entropy 11.521913423231954
epoch: 61, step: 63
	action: tensor([[-11074.7107,   2391.8063, -12364.7300,  98812.5752,  21818.5810,
           -499.3535,   -471.8924]], dtype=torch.float64)
	q_value: tensor([[-30.8614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12576433082730576, distance: 1.0699684064574957 entropy 11.588525107798768
epoch: 61, step: 64
	action: tensor([[-81277.5171, -38543.6782, -62796.3435,   4256.1561,  13982.8954,
          -5989.8960,  14792.9765]], dtype=torch.float64)
	q_value: tensor([[-33.6864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11759753879450674, distance: 1.0749544172300376 entropy 11.752048925594774
epoch: 61, step: 65
	action: tensor([[-18044.0285, -83393.5281,  32913.5254,  53342.6726,  29829.7690,
          14290.2220,  32891.6853]], dtype=torch.float64)
	q_value: tensor([[-30.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6120432264272442, distance: 1.4529311497639867 entropy 11.775849065510576
epoch: 61, step: 66
	action: tensor([[-15945.4602, -42431.4663, -25461.7206,  13178.8434,  22278.4330,
          35396.3665,   -560.1755]], dtype=torch.float64)
	q_value: tensor([[-29.1130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08118101331136551, distance: 1.1898874898080596 entropy 11.774232549424172
epoch: 61, step: 67
	action: tensor([[ 29803.6903, -35244.6062,  43665.5681, -29449.7568,  90595.5844,
         -30449.1539, -60236.4968]], dtype=torch.float64)
	q_value: tensor([[-30.8420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2300106223532652, distance: 1.0041510812403962 entropy 11.798273854476102
epoch: 61, step: 68
	action: tensor([[ -8083.5584,  41471.2236, -20083.2278,  85541.8779, -34381.3563,
          37327.5954,  15652.9171]], dtype=torch.float64)
	q_value: tensor([[-27.9403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22984957151825847, distance: 1.0042560897764794 entropy 11.609574015897442
epoch: 61, step: 69
	action: tensor([[-33160.7818,   8256.2799,  16260.9276,  17562.5304,  10629.7544,
         -10313.6500,  -1877.8950]], dtype=torch.float64)
	q_value: tensor([[-31.4681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6523561750183019, distance: 0.6747205739306615 entropy 11.649023186557107
epoch: 61, step: 70
	action: tensor([[  2540.8341,  -1454.4209,  18310.3552,  10744.8303,  11807.0068,
         -23067.0388, -33376.7763]], dtype=torch.float64)
	q_value: tensor([[-30.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.578933029150104
epoch: 61, step: 71
	action: tensor([[-24875.8554, -12380.5710,  27832.6571,  31106.9353,  -4221.3518,
         -31527.5726,   1770.1106]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3935309650718646, distance: 1.3508745039084247 entropy 11.173171362744345
epoch: 61, step: 72
	action: tensor([[-15847.6868,  25641.7004,  -9757.3780,   5593.9501, -38869.9809,
            663.4044,  31196.0794]], dtype=torch.float64)
	q_value: tensor([[-27.8509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6426572923862381, distance: 0.6840678111492869 entropy 11.683748493182549
epoch: 61, step: 73
	action: tensor([[-46989.0114, -32823.2521,   4141.0001, -18964.0805,  17242.8113,
          -7386.9159,  43889.5748]], dtype=torch.float64)
	q_value: tensor([[-29.6348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8965972329500891, distance: 1.5759565862732583 entropy 11.69210046534174
epoch: 61, step: 74
	action: tensor([[ -6670.4015,   3708.2005, -25351.3908,   3357.9787,  -3665.7058,
          -3088.0039,  34106.2820]], dtype=torch.float64)
	q_value: tensor([[-25.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.037744247174927636, distance: 1.122540325664843 entropy 11.446529752666711
epoch: 61, step: 75
	action: tensor([[ 30274.0541, -23951.4086,  -4980.5246,  80613.4431, -11221.5813,
          10078.0527,  23671.0414]], dtype=torch.float64)
	q_value: tensor([[-32.3973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46293113250412277, distance: 0.8386324874495851 entropy 11.720282728316059
epoch: 61, step: 76
	action: tensor([[ 18657.4607,  -8095.4463,  26149.6673,  24270.2063,  22962.0526,
         -41534.3114,  -1296.6838]], dtype=torch.float64)
	q_value: tensor([[-33.8907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15165590731425693, distance: 1.0540051024173005 entropy 11.630153451054223
epoch: 61, step: 77
	action: tensor([[-11324.1396, -61249.0941,  43685.5729, -32497.3587,  -9798.9496,
         -49378.8436,  40012.9288]], dtype=torch.float64)
	q_value: tensor([[-35.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6163212408347079, distance: 1.4548577550800592 entropy 11.762480660723762
epoch: 61, step: 78
	action: tensor([[-13890.1316, -52996.1108, -21092.5600,   1335.8628,  25591.1152,
          26613.9570,  14079.5251]], dtype=torch.float64)
	q_value: tensor([[-28.1455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1146506920485949, distance: 1.0767478650954403 entropy 11.64658179522074
epoch: 61, step: 79
	action: tensor([[-13102.1515, -12056.1217, -46542.3701,  50991.3786,  59796.2392,
          -9172.8207,  33656.6320]], dtype=torch.float64)
	q_value: tensor([[-25.6417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8259729712021142, distance: 1.5463359958957665 entropy 11.542409828647186
epoch: 61, step: 80
	action: tensor([[ -6146.8183,  37648.6043,  71809.8973, -37318.4738, -40819.7010,
         -10472.2798,  -5412.3056]], dtype=torch.float64)
	q_value: tensor([[-28.0931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.102819349140846, distance: 1.2017354756973986 entropy 11.73372950376813
epoch: 61, step: 81
	action: tensor([[ 29482.8471, -22102.8407, -48035.3684,  31315.7361, -16844.1474,
           6121.5194, -44514.2649]], dtype=torch.float64)
	q_value: tensor([[-36.9041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.598227765458876
epoch: 61, step: 82
	action: tensor([[-45792.3118, -49887.1058,  11644.1081,     53.2454, -13644.3897,
           5351.4164, -23959.4714]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.173171362744345
epoch: 61, step: 83
	action: tensor([[ 20608.7254, -41716.7117,  35330.5057,  18573.4794,  -6012.5511,
          51813.6521, -41750.9062]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5706247631167607, distance: 0.7498513717143915 entropy 11.173171362744345
epoch: 61, step: 84
	action: tensor([[26485.8849,  9009.6986, 39430.0345, -2500.5959, 13468.9803, 18370.9362,
          6642.2552]], dtype=torch.float64)
	q_value: tensor([[-26.6735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2251188130948566, distance: 1.0073357605419273 entropy 11.430183048623794
epoch: 61, step: 85
	action: tensor([[  3246.4739,    770.8738, -10357.6937,  76107.7204,   -225.1710,
           7198.6327,   1725.8795]], dtype=torch.float64)
	q_value: tensor([[-36.6623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5116613481282397, distance: 0.7996818575207535 entropy 11.604571889249504
epoch: 61, step: 86
	action: tensor([[-13380.8602,   4577.3744, -44403.0200,  -1699.9147, -17340.5200,
         -38612.8762,  40123.5832]], dtype=torch.float64)
	q_value: tensor([[-35.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.881474046276942, distance: 0.39397024501775674 entropy 11.750657292903782
epoch: 61, step: 87
	action: tensor([[ 24459.1592, -11649.9808, -26955.1824,   3242.4436,  18385.1267,
          13408.1082,   3217.0257]], dtype=torch.float64)
	q_value: tensor([[-27.3829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.391023359433165
epoch: 61, step: 88
	action: tensor([[ -4653.5579, -13853.6975, -17865.2209,  -9153.8497,   -517.6671,
          -3201.1361,  11616.7790]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.173171362744345
epoch: 61, step: 89
	action: tensor([[-2247.9335, -4916.4601,  8044.3334,   324.0557, 20080.1622, 16912.6077,
          2866.8435]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.173171362744345
epoch: 61, step: 90
	action: tensor([[ 27355.0800, -16977.3120,  22109.4153, -21174.8526,  -3876.6445,
         -27103.8098,  -9371.8223]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4158495396719053, distance: 0.8746192966242038 entropy 11.173171362744345
epoch: 61, step: 91
	action: tensor([[-27410.9240, -11937.7357,  25685.6089,   -923.8372,   5274.0963,
          37391.3848,   3337.5475]], dtype=torch.float64)
	q_value: tensor([[-28.6922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20750470489092143, distance: 1.0187204856467142 entropy 11.520019882955753
epoch: 61, step: 92
	action: tensor([[ 19373.1328, -46783.6579, -45117.1538, -16290.9307,  11829.2256,
         -17448.6903, -12769.7994]], dtype=torch.float64)
	q_value: tensor([[-31.7907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18845955308284434, distance: 1.2475239622635517 entropy 11.671527525578012
epoch: 61, step: 93
	action: tensor([[ -5405.8237, -14435.9766,  -2586.1815, -22839.4202,  -9444.4396,
          22283.8246,  74821.5823]], dtype=torch.float64)
	q_value: tensor([[-30.8460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7568008355621474, distance: 1.5167638103032821 entropy 11.68201976839089
epoch: 61, step: 94
	action: tensor([[-31574.9997, -14950.2132, -13561.2356,   9224.3601,  30819.7477,
           2864.3573,  52089.6280]], dtype=torch.float64)
	q_value: tensor([[-29.8849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6251686317683216, distance: 1.458834108547422 entropy 11.65290274528152
epoch: 61, step: 95
	action: tensor([[-32610.7521,  44108.2692,  34345.0384,  53260.0716,  45982.9055,
         -24793.3440,   8888.0993]], dtype=torch.float64)
	q_value: tensor([[-27.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40304269961957684, distance: 0.8841548368446698 entropy 11.663036696067735
epoch: 61, step: 96
	action: tensor([[ 24267.2084, -20830.5474, -10157.0619,  -3212.6674,  33054.7824,
          19516.0080, -43732.6455]], dtype=torch.float64)
	q_value: tensor([[-36.7895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02967429841400393, distance: 1.1272375847541023 entropy 11.813840678957325
epoch: 61, step: 97
	action: tensor([[-46575.0481,  -4717.6876, -25577.2005,  23092.2490,   8704.1258,
         -37066.4334,  24389.9051]], dtype=torch.float64)
	q_value: tensor([[-29.5584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14135308247327094, distance: 1.060386038179612 entropy 11.500617888589261
epoch: 61, step: 98
	action: tensor([[ -9227.3551, -37775.3064, -12904.8371,  15687.1530,  46762.9200,
         -19986.5617,   -775.6817]], dtype=torch.float64)
	q_value: tensor([[-30.5987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17743668055726547, distance: 1.0378661862244747 entropy 11.588166515820337
epoch: 61, step: 99
	action: tensor([[ -7316.2372,   3705.0591, -14541.8296,  33692.1933,  12651.3344,
         -15835.3595,  25403.4626]], dtype=torch.float64)
	q_value: tensor([[-34.0593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20193983092096057, distance: 1.022290938346931 entropy 11.923295327513415
epoch: 61, step: 100
	action: tensor([[ 20920.4277, -27393.8049,  43188.6597,  10131.1994, -48835.5637,
           -351.8074,  14653.5873]], dtype=torch.float64)
	q_value: tensor([[-35.4627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4978258281883048, distance: 0.8109309560637714 entropy 11.853311978282587
epoch: 61, step: 101
	action: tensor([[-58309.9524, -37396.6642,  25315.0359,  51731.1850, -13597.6056,
          -9983.4595,  39363.9489]], dtype=torch.float64)
	q_value: tensor([[-31.6097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5366232132867967, distance: 1.41853608550786 entropy 11.825376534047606
epoch: 61, step: 102
	action: tensor([[ -5980.2644, -16683.4515,  42463.8196,  29034.5605,   5861.0155,
          85347.0698, -10278.4755]], dtype=torch.float64)
	q_value: tensor([[-26.3712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31075974224397607, distance: 1.31014161120358 entropy 11.559049204138221
epoch: 61, step: 103
	action: tensor([[ -5384.1243, -31932.5239, -16907.9509, -29344.7074,  22322.4329,
          33976.2493,  26412.4890]], dtype=torch.float64)
	q_value: tensor([[-27.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6393435440051523, distance: 1.46518235752302 entropy 11.626863673808746
epoch: 61, step: 104
	action: tensor([[ 38564.3202, -10712.8791, -17696.9868, -14500.8537,  15989.5453,
          -9463.6092,  22599.7086]], dtype=torch.float64)
	q_value: tensor([[-25.5757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04622111033779086, distance: 1.1175849537833038 entropy 11.493791331466387
epoch: 61, step: 105
	action: tensor([[ 17797.4300,  16827.3094, -16956.0305,  40814.8505,  27024.3108,
          13909.0677, -12404.4353]], dtype=torch.float64)
	q_value: tensor([[-30.5502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49248467052313305, distance: 0.8152321070917433 entropy 11.457251044018964
epoch: 61, step: 106
	action: tensor([[-15017.2783,  -1318.9335,   9272.0045,  -8647.6819,  -8927.1423,
         -13817.6483, -38391.4916]], dtype=torch.float64)
	q_value: tensor([[-32.7030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.39039328334537
epoch: 61, step: 107
	action: tensor([[-18667.8140,    678.1376, -16041.2173,  -6841.2136,  -9919.3414,
           5665.1599,   6470.8231]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.173171362744345
epoch: 61, step: 108
	action: tensor([[-12179.3404,  -5921.0893,   -258.2309,  11716.4063, -28076.7317,
          10856.2161, -19871.1432]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.173171362744345
epoch: 61, step: 109
	action: tensor([[-31319.1846, -22273.8631, -17113.3541,   5692.0475,  18394.8427,
         -21724.6938,  32177.5992]], dtype=torch.float64)
	q_value: tensor([[-32.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11816059235148968, distance: 1.2100652363492026 entropy 11.173171362744345
epoch: 61, step: 110
	action: tensor([[-54746.8361, -44522.9072, -38045.6377,  -3612.3716, -56579.4712,
         -13093.6361, -26888.9527]], dtype=torch.float64)
	q_value: tensor([[-31.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19735043957464282, distance: 1.252181641629487 entropy 11.853145917014583
epoch: 61, step: 111
	action: tensor([[-33796.6690,  15844.8108,   8653.6347,  43434.4947,  54314.4093,
          42835.0029, -45186.7156]], dtype=torch.float64)
	q_value: tensor([[-30.5507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2204439306432744, distance: 1.2641994855081113 entropy 11.746612680187866
epoch: 61, step: 112
	action: tensor([[-33309.1946,  -4226.0477,   7277.8451,  32947.3588,  -6488.3524,
         -24106.2823,   8469.0996]], dtype=torch.float64)
	q_value: tensor([[-36.2084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0007839854440865324, distance: 1.1438955914274465 entropy 11.854741967354878
epoch: 61, step: 113
	action: tensor([[-28869.8934, -66127.0427,  -3905.2914,  46830.3178, -45781.3950,
          35632.9536,  24718.5701]], dtype=torch.float64)
	q_value: tensor([[-30.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10333605578795724, distance: 1.0836063630943717 entropy 11.660989309283721
epoch: 61, step: 114
	action: tensor([[   786.8336, -25230.4696,  38314.0844, -23213.5189,  49360.4139,
          12579.8048, -15417.1694]], dtype=torch.float64)
	q_value: tensor([[-28.8672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2281725695661636, distance: 1.2681960319883891 entropy 11.771290177626529
epoch: 61, step: 115
	action: tensor([[  2497.0212, -18083.7062, -17310.2435,  -7337.9454,  41815.2040,
          -4742.2261, -20272.3050]], dtype=torch.float64)
	q_value: tensor([[-26.1102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44186204056719836, distance: 0.8549239297999738 entropy 11.437240064676345
epoch: 61, step: 116
	action: tensor([[-37807.2467,  -7626.4312, -15133.3357,  -3193.3945,   2083.1144,
         -20099.0024,  22051.2349]], dtype=torch.float64)
	q_value: tensor([[-34.6989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7012293552241422, distance: 1.492581750429979 entropy 11.571410472041672
epoch: 61, step: 117
	action: tensor([[ -48617.0401,   -7237.2368,  -18769.7864,    1009.4755, -108401.5332,
           35549.6331,   45310.3482]], dtype=torch.float64)
	q_value: tensor([[-34.4779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0093373674938868, distance: 1.6221205405344046 entropy 11.815983043112656
epoch: 61, step: 118
	action: tensor([[-15170.4442, -10890.8927,   5564.5435, -15315.5605,  42917.5202,
         -22976.7673, -35635.0985]], dtype=torch.float64)
	q_value: tensor([[-32.1710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6661609240908015, distance: 1.4771179159134826 entropy 11.855310282689514
epoch: 61, step: 119
	action: tensor([[-15210.5842,  22682.6897,  18034.6540, -17737.7657,  -8865.3214,
          79615.0516,  21451.8034]], dtype=torch.float64)
	q_value: tensor([[-38.3987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2500529572216448, distance: 1.2794428722374225 entropy 11.948200476916984
epoch: 61, step: 120
	action: tensor([[   -37.3260,   2027.0355,  16272.6769, -11723.4025,  -7362.0521,
         -34282.2920,   6423.6423]], dtype=torch.float64)
	q_value: tensor([[-35.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10634987717905586, distance: 1.2036575359886261 entropy 11.521731441185704
epoch: 61, step: 121
	action: tensor([[  9175.4730, -19058.0962, -27992.8824,  18953.6950,   1464.4010,
          49863.2362,  36606.8808]], dtype=torch.float64)
	q_value: tensor([[-40.2823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5474757813982324, distance: 0.7697994683761191 entropy 11.706245904046893
epoch: 61, step: 122
	action: tensor([[-21204.8930,  -1269.1970,  -5125.4860,  30157.0357,  20284.5266,
          23378.1099, -13274.1618]], dtype=torch.float64)
	q_value: tensor([[-25.6378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01649939896527064, distance: 1.134864492571534 entropy 11.297743977638591
epoch: 61, step: 123
	action: tensor([[-62065.8788,  10319.6816,  -2774.1127, -23705.6172,   6263.2437,
          15761.8546, -17490.9953]], dtype=torch.float64)
	q_value: tensor([[-29.8409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6437850053195788, distance: 0.6829875572488529 entropy 11.705800510392416
epoch: 61, step: 124
	action: tensor([[  2131.6120, -50205.7542,   5905.4324,  18886.7526,  16463.5519,
          49012.4665, -13801.5566]], dtype=torch.float64)
	q_value: tensor([[-32.0015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33323514784879993, distance: 0.9344219732012082 entropy 11.555989336200634
epoch: 61, step: 125
	action: tensor([[-14617.4854,   3497.8767,  29388.9994,  31686.4596,  51603.3582,
            710.0620,   -760.6243]], dtype=torch.float64)
	q_value: tensor([[-30.0335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3777418084897701, distance: 0.9026970111253052 entropy 11.730748257382308
epoch: 61, step: 126
	action: tensor([[-10161.0282, -35092.7633, -24875.3306,  12137.3546,  -2780.0506,
         -26797.7222,  24395.3035]], dtype=torch.float64)
	q_value: tensor([[-29.5038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1617606681501601, distance: 1.0477090871993626 entropy 11.598754684059893
epoch: 61, step: 127
	action: tensor([[-32400.8046,  23908.0491,  14593.7793,  29838.8228,  -2647.4288,
          22420.5124,   6554.1502]], dtype=torch.float64)
	q_value: tensor([[-31.1691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12403689010771601, distance: 1.2132407130582121 entropy 11.838732125739265
LOSS epoch 61 actor 379.21195701251963 critic 158.5087576574287
epoch: 62, step: 0
	action: tensor([[-46976.2223, -59665.3900,  41363.6852,   4676.5654,  10892.7644,
          13490.1123,  53062.4032]], dtype=torch.float64)
	q_value: tensor([[-27.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20826867535797855, distance: 1.2578777972986148 entropy 11.743624498077702
epoch: 62, step: 1
	action: tensor([[ 15384.4346, -11701.4637,  -5329.9233,  -8455.5877,   6036.5960,
          -7574.5879,  -6789.8999]], dtype=torch.float64)
	q_value: tensor([[-25.1613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.340562632928067, distance: 0.9292733226039768 entropy 11.808355450767902
epoch: 62, step: 2
	action: tensor([[-12550.8471, -13517.1168,   3980.8823,  27036.8099, -21917.3208,
          24752.8217,   9562.6688]], dtype=torch.float64)
	q_value: tensor([[-24.8603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18151423201227646, distance: 1.035290572712618 entropy 11.735191603947424
epoch: 62, step: 3
	action: tensor([[-32066.6704, -24559.8018, -34949.6690,  52429.4878,  -2751.4272,
          57087.1965,  28529.2536]], dtype=torch.float64)
	q_value: tensor([[-24.9374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11749842020917578, distance: 1.20970688435652 entropy 11.874405968929894
epoch: 62, step: 4
	action: tensor([[ -8622.1861,   9226.3365, -12267.4318,  44981.3822,   9014.9656,
         -50004.3373,   -998.2173]], dtype=torch.float64)
	q_value: tensor([[-26.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008451947353752542, distance: 1.1491700473121451 entropy 11.924092469888476
epoch: 62, step: 5
	action: tensor([[-40934.3654, -68808.4165,  25787.1865, -35082.1013,   8515.7025,
          24693.5970,  50199.6426]], dtype=torch.float64)
	q_value: tensor([[-31.7090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7615642746352638, distance: 1.518818716288038 entropy 11.886949614811012
epoch: 62, step: 6
	action: tensor([[-17178.9905, -30795.1531,  25388.9854,  30547.3330,  75760.2753,
          26328.9200,   6961.4728]], dtype=torch.float64)
	q_value: tensor([[-25.9464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26029185289195467, distance: 1.2846719972798473 entropy 11.828222972237894
epoch: 62, step: 7
	action: tensor([[-45634.3080, -35378.3124, -84610.0994,  39387.7579,  -5698.0180,
         -65784.2758,  37327.7309]], dtype=torch.float64)
	q_value: tensor([[-25.7911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42796648006700977, distance: 1.3674633635989764 entropy 11.91655731402564
epoch: 62, step: 8
	action: tensor([[-54347.1501, -17946.4870, -57534.3125,  23917.5650,  15882.0704,
          -6566.0378, -43736.9019]], dtype=torch.float64)
	q_value: tensor([[-23.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24325776455290837, distance: 1.2759606565197872 entropy 11.66008288305561
epoch: 62, step: 9
	action: tensor([[-24180.6438, -34009.5709,  -1875.9182,   6678.2905,  15973.4377,
         -26543.8306,  49580.5031]], dtype=torch.float64)
	q_value: tensor([[-23.5118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24391135065016578, distance: 1.2762960015284828 entropy 11.639859316441816
epoch: 62, step: 10
	action: tensor([[ 24502.4030, -39176.6816,  -8325.4358, -15935.9224,  20124.9742,
          13547.5875,  64724.7842]], dtype=torch.float64)
	q_value: tensor([[-26.0626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.033166683092931804, distance: 1.1631665106940698 entropy 11.681244411905427
epoch: 62, step: 11
	action: tensor([[-37915.4622, -24479.5630,  11298.1295,  28043.0874,  -2399.6597,
          -4487.0164,  -3813.2010]], dtype=torch.float64)
	q_value: tensor([[-22.3036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.70119136509621, distance: 1.4925650849262835 entropy 11.335926732371474
epoch: 62, step: 12
	action: tensor([[  -944.7075,  20495.5973, -38144.0005,  43511.3932,  12516.8810,
         -55534.3870,    448.4263]], dtype=torch.float64)
	q_value: tensor([[-23.3321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6460033031316625, distance: 0.6808576133371822 entropy 11.586594900286302
epoch: 62, step: 13
	action: tensor([[  11111.2241,  -92917.9077,   30221.7232,   34297.0412,     354.5700,
          -31689.3001, -117982.1824]], dtype=torch.float64)
	q_value: tensor([[-27.3261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2960213674322324, distance: 0.9601441319355002 entropy 11.680888117271186
epoch: 62, step: 14
	action: tensor([[ 17935.6970,  -9554.2959, -44718.5326,  18353.8164, -13138.2555,
          38339.7915,  50018.8075]], dtype=torch.float64)
	q_value: tensor([[-28.9922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34583018908599816, distance: 0.9255543841914721 entropy 11.823879377244221
epoch: 62, step: 15
	action: tensor([[-10020.3243,  -8804.8241,  -5737.6149,  32518.8251, -13413.0302,
         -47449.3837,   -831.2208]], dtype=torch.float64)
	q_value: tensor([[-28.4386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3741107410241047, distance: 1.3414285967833968 entropy 11.6327084638629
epoch: 62, step: 16
	action: tensor([[-43031.6169, -44701.7767,   9048.7241, -25718.7095, -25104.1755,
          21260.0564,  -6448.5811]], dtype=torch.float64)
	q_value: tensor([[-24.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3629504929592533, distance: 1.3359700857914807 entropy 11.641229103682637
epoch: 62, step: 17
	action: tensor([[-21757.7098, -50619.4599,   5239.7007, -16113.5006,  34307.2859,
          28739.6077,  16321.8287]], dtype=torch.float64)
	q_value: tensor([[-23.9004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10943559071754305, distance: 1.2053349240660878 entropy 11.720129094907794
epoch: 62, step: 18
	action: tensor([[-20817.2616,  12393.7173, -27665.7384, -60645.2978, -30868.2065,
           -554.2959,  -7399.3986]], dtype=torch.float64)
	q_value: tensor([[-26.4419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.195673452199415, distance: 1.251304442892008 entropy 11.6333124234796
epoch: 62, step: 19
	action: tensor([[-19705.5458, -25412.8852,  55519.6765,  24822.3289,  45080.8034,
         -13438.3737, -15738.3714]], dtype=torch.float64)
	q_value: tensor([[-36.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48202508640970887, distance: 1.3931069883013845 entropy 11.732504727384692
epoch: 62, step: 20
	action: tensor([[ 20309.5959,  16119.9473,  -1049.2002, -14089.4291,   4975.2234,
          20126.2720,   7201.4730]], dtype=torch.float64)
	q_value: tensor([[-22.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.54657987446154
epoch: 62, step: 21
	action: tensor([[-19374.6376, -24567.1899, -19277.6350,   1033.9087, -26412.2397,
           2360.1371, -17699.9084]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.70603414025681, distance: 1.4946880149163664 entropy 11.211331666873718
epoch: 62, step: 22
	action: tensor([[-22817.5696, -14440.3354,  47048.0351,   9024.3359,   6186.3595,
          31796.2433, -28800.2295]], dtype=torch.float64)
	q_value: tensor([[-24.4497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8478035816111822, distance: 1.5555522220367712 entropy 11.725712475753681
epoch: 62, step: 23
	action: tensor([[-21279.3018,  -4616.1706,  20846.8772,  32267.2007,   6979.6133,
         -49206.9503,  13493.2534]], dtype=torch.float64)
	q_value: tensor([[-25.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6348649363664036, distance: 1.4631795848869076 entropy 11.83173052342856
epoch: 62, step: 24
	action: tensor([[-1664.6648, 23050.4929, -5616.5295, -5945.1467, -2573.1096,   106.6017,
         12965.3709]], dtype=torch.float64)
	q_value: tensor([[-25.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5400878832124345, distance: 1.4201343927658925 entropy 11.85841195479156
epoch: 62, step: 25
	action: tensor([[-41766.1686, -10009.5322,  34422.8475,    569.5174,  15828.8475,
         -23784.5123,  31762.3620]], dtype=torch.float64)
	q_value: tensor([[-27.9160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30306327027992674, distance: 1.3062895270238728 entropy 11.718458979518056
epoch: 62, step: 26
	action: tensor([[ 21449.2072, -62199.4889,  -2530.0934, -41064.3043, -33970.2589,
         -30931.1150,   4939.4644]], dtype=torch.float64)
	q_value: tensor([[-26.9558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.038604034228655326, distance: 1.1662232514262978 entropy 11.793866460816846
epoch: 62, step: 27
	action: tensor([[-29917.0529,  19774.2061, -23194.3786, -22109.3830,  11932.7243,
         -27459.6307,  11777.5767]], dtype=torch.float64)
	q_value: tensor([[-27.3966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.629865474340553
epoch: 62, step: 28
	action: tensor([[-26844.5382,    620.3126,  -3405.1478,   2194.2254,   2334.4242,
           -947.8717, -12387.9002]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48737619100494933, distance: 0.8193247608850582 entropy 11.211331666873718
epoch: 62, step: 29
	action: tensor([[  8910.6896, -41669.6121,  12017.3318,  43853.9772,  -3189.6601,
         -21050.1450,  35976.5360]], dtype=torch.float64)
	q_value: tensor([[-28.7802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039886349127355, distance: 1.1212901717734365 entropy 11.726469701365334
epoch: 62, step: 30
	action: tensor([[-27112.1288, -27024.2823, -12122.0426,  19400.1329,  21747.4130,
           4931.8002,  25529.0718]], dtype=torch.float64)
	q_value: tensor([[-26.8242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4411950764191681, distance: 1.3737828111365749 entropy 11.657604493221553
epoch: 62, step: 31
	action: tensor([[-24456.2905, -37176.0021,   -763.2994,  17849.4329, -62369.9183,
          41589.0628,  29531.6042]], dtype=torch.float64)
	q_value: tensor([[-24.2414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3479828234393332, distance: 1.3286141468037167 entropy 11.722443090785019
epoch: 62, step: 32
	action: tensor([[-36863.4276,  22397.9538, -11267.1105,  26476.1940, -10413.1362,
           5293.8427,  29960.0817]], dtype=torch.float64)
	q_value: tensor([[-28.0391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1858147393629641, distance: 1.2461350608918553 entropy 12.002100988814664
epoch: 62, step: 33
	action: tensor([[ 21390.3049, -63747.6161, -57115.7965,  23943.4231,  -3985.7945,
          16448.4497,  31585.0930]], dtype=torch.float64)
	q_value: tensor([[-28.3518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16490051490505353, distance: 1.2350971281200611 entropy 11.78594289509699
epoch: 62, step: 34
	action: tensor([[-70055.6634, -31233.6486, -34674.8715,  28502.8691, -10687.0710,
         -10860.8375, -24629.9066]], dtype=torch.float64)
	q_value: tensor([[-25.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2716887440775242, distance: 1.2904676053762643 entropy 11.71095654006093
epoch: 62, step: 35
	action: tensor([[-21612.2469,  23017.1319, -27579.7228,  68858.4588, -42369.0149,
          18611.1691,  75820.2024]], dtype=torch.float64)
	q_value: tensor([[-24.9410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010863076409862837, distance: 1.1381117321558158 entropy 11.815852201711042
epoch: 62, step: 36
	action: tensor([[-29464.5056, -18470.5601,    485.9760,  38058.1160,  39124.9252,
           5442.4967,   9250.8284]], dtype=torch.float64)
	q_value: tensor([[-28.4036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06482869518019185, distance: 1.1066295921574085 entropy 11.78123460525849
epoch: 62, step: 37
	action: tensor([[-34203.9730, -43105.5190,  -3854.1257,  33579.6346,  50779.6419,
          51081.5171,  18135.3014]], dtype=torch.float64)
	q_value: tensor([[-24.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17584752691255223, distance: 1.038868256798095 entropy 11.709248215534108
epoch: 62, step: 38
	action: tensor([[-39606.7997, -18564.8182, -27455.6881, -10011.1626,  18233.5091,
          10606.2462,  48605.5245]], dtype=torch.float64)
	q_value: tensor([[-25.3177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7341254317958714, distance: 1.5069434216259545 entropy 11.871589909157361
epoch: 62, step: 39
	action: tensor([[  3633.3197, -67949.1301,  15804.5518,  28994.9993, -63286.2305,
         -17603.6997,  36666.1972]], dtype=torch.float64)
	q_value: tensor([[-24.1367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02459221356947805, distance: 1.1583297716668846 entropy 11.618758637128368
epoch: 62, step: 40
	action: tensor([[  -6476.6763,  -33276.9658,   26183.2731,   14067.2580, -101209.6645,
           -9732.4594,   -9683.3849]], dtype=torch.float64)
	q_value: tensor([[-31.4853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25539508308787373, distance: 1.282173819781151 entropy 11.821306614872068
epoch: 62, step: 41
	action: tensor([[-32911.8910, -16301.1267, -53158.1778,  20931.5906, -19446.7408,
           3935.4203,  45829.3438]], dtype=torch.float64)
	q_value: tensor([[-24.6943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11712629714971279, distance: 1.075241415338167 entropy 11.608942669174013
epoch: 62, step: 42
	action: tensor([[-13094.4571,  -1029.6987,  15064.5935,  28426.5648,   7241.5231,
           8500.0038,  11161.2460]], dtype=torch.float64)
	q_value: tensor([[-24.8439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48816733796736833, distance: 1.3959908686235887 entropy 11.735003400398798
epoch: 62, step: 43
	action: tensor([[-36096.3009, -26244.5013, -12039.7592,  51030.8033,  -3300.3834,
          22644.9396, -30929.2738]], dtype=torch.float64)
	q_value: tensor([[-27.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27030563223874027, distance: 1.2897656464233314 entropy 11.948549672399011
epoch: 62, step: 44
	action: tensor([[ 24702.1231,   7261.3786,  -2951.1906,  31704.6167, -51685.6475,
         -48299.8575, -24042.2721]], dtype=torch.float64)
	q_value: tensor([[-25.4162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4788230844913923, distance: 0.8261316844791906 entropy 11.875686975387437
epoch: 62, step: 45
	action: tensor([[-17710.7743,  24148.8733,   3161.3684,  12962.7850, -29806.2314,
           8582.2502, -12609.3397]], dtype=torch.float64)
	q_value: tensor([[-28.6977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.260170096403163, distance: 0.984289005194647 entropy 11.571473720165837
epoch: 62, step: 46
	action: tensor([[-26057.4303, -75938.7373,  14090.6899, -17014.9318,  -5691.0098,
          31123.0750,  27616.0424]], dtype=torch.float64)
	q_value: tensor([[-25.5965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5640395970790357, distance: 1.4311348756133822 entropy 11.648807018640552
epoch: 62, step: 47
	action: tensor([[-18759.8070,  -6581.6352, -46696.4845, -24337.6438,  19068.2785,
         -29443.5561,  -2797.6136]], dtype=torch.float64)
	q_value: tensor([[-23.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2091050407644235, distance: 1.0176913824963252 entropy 11.59426584167031
epoch: 62, step: 48
	action: tensor([[-29812.3856, -30489.2928,  -3427.9661, -10556.6180,  19717.9575,
          20611.0877, -49528.6062]], dtype=torch.float64)
	q_value: tensor([[-26.0887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5761466046268395, distance: 1.4366633030687743 entropy 11.609249237265109
epoch: 62, step: 49
	action: tensor([[  1485.6725,  72728.4683, -35290.0863,  25607.2437, -57403.6608,
         -11104.6131,   3697.7978]], dtype=torch.float64)
	q_value: tensor([[-26.6896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.817764724092068
epoch: 62, step: 50
	action: tensor([[-31558.2487, -12301.2193,    188.9190,  52038.0857,  22073.8150,
          -7166.4562,  19267.9149]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8095284616621785, distance: 1.5393571827379993 entropy 11.211331666873718
epoch: 62, step: 51
	action: tensor([[ -3137.2894, -29436.8792, -46756.4385,  18608.0070, -17660.2510,
          42654.6146,   2093.0561]], dtype=torch.float64)
	q_value: tensor([[-24.2983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5988333279976257, distance: 1.446965877254769 entropy 11.635114239550889
epoch: 62, step: 52
	action: tensor([[-53452.3888, -17825.2320, -57547.8540, -44315.5397,   -183.2063,
         -52566.2304, -27954.0744]], dtype=torch.float64)
	q_value: tensor([[-24.9967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043481189070077475, distance: 1.168958263741802 entropy 11.766542296498026
epoch: 62, step: 53
	action: tensor([[-21406.5343, -28611.7900,  -1220.7599,  -4401.0041,  -9959.1056,
          21537.2883,  -8000.9188]], dtype=torch.float64)
	q_value: tensor([[-22.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23809609114549657, distance: 1.2733091780702126 entropy 11.439626145640023
epoch: 62, step: 54
	action: tensor([[ 10008.2233, -30089.3844, -30814.3105,  -5483.5951,  -7012.3392,
         -62760.9833,  -3751.0726]], dtype=torch.float64)
	q_value: tensor([[-27.4823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.573239844701803, distance: 0.747564424087653 entropy 11.720515451168975
epoch: 62, step: 55
	action: tensor([[ 19957.3785, -22158.2947, -18092.4609, -28120.0040,   4880.2434,
          68297.3485,  26854.2163]], dtype=torch.float64)
	q_value: tensor([[-24.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06953762087638526, distance: 1.103839935905706 entropy 11.460573544948094
epoch: 62, step: 56
	action: tensor([[-78847.8808, -40888.8354, -59236.6592, -21480.2461,  23134.5570,
          25478.2957,  43179.8142]], dtype=torch.float64)
	q_value: tensor([[-29.9631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6386596611749091, distance: 1.4648767115267687 entropy 11.745048800064634
epoch: 62, step: 57
	action: tensor([[   749.3860,  -2213.3532,  13029.0143, -20800.2319, -30445.4608,
           4498.1838,  16545.4192]], dtype=torch.float64)
	q_value: tensor([[-23.3080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40113057678705877, distance: 0.8855697295071384 entropy 11.57413284009245
epoch: 62, step: 58
	action: tensor([[-20307.3937, -49524.8151,  38915.5592, -24956.9662,  -2371.9144,
          42867.4861, -35269.9245]], dtype=torch.float64)
	q_value: tensor([[-32.8858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1139233708875127, distance: 1.0771900525615983 entropy 11.802970972648906
epoch: 62, step: 59
	action: tensor([[-46412.2026, -23153.0435, -21685.6782,   3641.4852,   -107.4415,
          34895.0735,  -3611.6794]], dtype=torch.float64)
	q_value: tensor([[-31.7041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17373112792136314, distance: 1.239769661491051 entropy 11.938455282115283
epoch: 62, step: 60
	action: tensor([[-11736.4200,   9063.8688,   6985.1270,  32291.9121,   6222.9645,
         -38185.9022, -49802.2847]], dtype=torch.float64)
	q_value: tensor([[-24.6032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22620003585586423, distance: 1.0066327272439441 entropy 11.802236141687123
epoch: 62, step: 61
	action: tensor([[ 32944.2621, -37248.8738,   8287.5353, -20578.0046, -27800.6668,
           9515.5670, -28160.5578]], dtype=torch.float64)
	q_value: tensor([[-30.8090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3361463830656881, distance: 1.3227681017645574 entropy 11.846793109091939
epoch: 62, step: 62
	action: tensor([[ -8647.6538, -30513.5825,  13553.3971,  18614.3807,  20873.1414,
          22759.5872, -37469.3750]], dtype=torch.float64)
	q_value: tensor([[-24.9704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22619095987368554, distance: 1.006638630677859 entropy 11.597235810195567
epoch: 62, step: 63
	action: tensor([[-19192.7807,  27882.4956,   9521.5355,  48474.0079,   6078.9531,
           6498.8107, -10193.1905]], dtype=torch.float64)
	q_value: tensor([[-24.1024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27399843521161316, distance: 0.9750468231601982 entropy 11.780045124694277
epoch: 62, step: 64
	action: tensor([[-16386.6186,  -6080.8621,  31962.1795, -42999.9478,  49633.0104,
         -12438.1194,   9665.0949]], dtype=torch.float64)
	q_value: tensor([[-29.4261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9637564768070221, distance: 1.6036164716463897 entropy 11.811995091955414
epoch: 62, step: 65
	action: tensor([[-30754.0182,  -9297.1035, -10520.7245,   3215.5417, -14565.4867,
          21305.9971, -29197.8466]], dtype=torch.float64)
	q_value: tensor([[-24.9953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2599355595390329, distance: 1.2844903915482029 entropy 11.729949529873002
epoch: 62, step: 66
	action: tensor([[ 17047.3843, -12362.4234,  -4120.7119,  20644.0124,   4576.4543,
           -373.6184,   5268.9255]], dtype=torch.float64)
	q_value: tensor([[-23.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.716111973698839
epoch: 62, step: 67
	action: tensor([[-12792.9298, -22549.5085, -10606.6657,  17061.8013,   2626.5734,
          -2332.2392,  18876.7761]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8916182797558891, distance: 1.573886623696974 entropy 11.211331666873718
epoch: 62, step: 68
	action: tensor([[-24759.5519,  -7461.6838,   7704.0409, -19931.3486, -28763.3010,
         -31596.9160,  15349.4379]], dtype=torch.float64)
	q_value: tensor([[-22.5772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7098816460701212, distance: 1.4963725011703368 entropy 11.582127986828919
epoch: 62, step: 69
	action: tensor([[ -4913.6853,  12024.1172,  28733.3370,  -3327.9263, -34827.4064,
          12382.2803, -14044.7940]], dtype=torch.float64)
	q_value: tensor([[-22.1142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16062489923670586, distance: 1.2328284128516132 entropy 11.436213545267307
epoch: 62, step: 70
	action: tensor([[ -6172.5946, -48754.9917,  12267.5756,  12355.2880,   2499.2868,
         -13780.9133, -45784.0812]], dtype=torch.float64)
	q_value: tensor([[-32.2759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43698411603606124, distance: 1.371774347132611 entropy 11.523277849376834
epoch: 62, step: 71
	action: tensor([[-42153.1414,  16995.2608,  -2964.8413,  -9939.2607,  -4228.6193,
         -24567.5457,  17663.0967]], dtype=torch.float64)
	q_value: tensor([[-22.0443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10520524965016564, distance: 1.082476325996187 entropy 11.66112332445866
epoch: 62, step: 72
	action: tensor([[-12802.2529,  -2092.6411, -14627.9497, -24639.9010,  -4045.4191,
          30005.4833, -19919.9414]], dtype=torch.float64)
	q_value: tensor([[-30.6657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11557173735647241, distance: 1.2086636045912398 entropy 11.491841510330234
epoch: 62, step: 73
	action: tensor([[ -2400.4211, -27337.0454,  30888.0283,  57234.2833,   4513.3017,
          10109.3142,  75743.0897]], dtype=torch.float64)
	q_value: tensor([[-25.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.464270091977828, distance: 1.3847369763171142 entropy 11.67325755267335
epoch: 62, step: 74
	action: tensor([[ -8766.6434,  -4653.3070, -47138.7012, -17649.8894, -37347.3987,
           9964.0762,  14296.0645]], dtype=torch.float64)
	q_value: tensor([[-23.3703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.550859765443585, distance: 1.4250921827600467 entropy 11.642888184899933
epoch: 62, step: 75
	action: tensor([[  5886.5384, -24629.3857, -16522.9914,  59008.2689,  21226.4499,
           3877.4658, -24254.3677]], dtype=torch.float64)
	q_value: tensor([[-27.4247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20719698575695444, distance: 1.257319828018484 entropy 11.781716141042642
epoch: 62, step: 76
	action: tensor([[-18814.5074,   8228.4291, -65642.4416,   5886.8803,  -2197.9169,
          20632.0716, -15100.6924]], dtype=torch.float64)
	q_value: tensor([[-29.5025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20975484455321447, distance: 1.0172732260798685 entropy 11.668084870713221
epoch: 62, step: 77
	action: tensor([[-15851.5537,  -8441.7662,   1762.7575, -16578.9508, -25786.5786,
         -15383.5300, -25846.0218]], dtype=torch.float64)
	q_value: tensor([[-29.2965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4505414537902708, distance: 1.3782302113655396 entropy 11.789827253111559
epoch: 62, step: 78
	action: tensor([[-82209.4116, -52283.9059,  15617.0465,  17745.7912, -25046.7855,
         -66918.3601,   -209.4582]], dtype=torch.float64)
	q_value: tensor([[-27.1211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3221972866320568, distance: 1.3158452711746746 entropy 11.829014272223755
epoch: 62, step: 79
	action: tensor([[ 30860.7920, -74594.6995, -36192.4450,   7207.3709, -34357.6520,
         -38511.7120, -36809.1208]], dtype=torch.float64)
	q_value: tensor([[-28.3585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.537745918230484, distance: 0.7780313031140512 entropy 11.953024661115949
epoch: 62, step: 80
	action: tensor([[ -6042.7975, -87528.6366, -13685.3470,  48161.4628,   1774.7224,
         -22453.0301, -31533.7628]], dtype=torch.float64)
	q_value: tensor([[-27.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13978966103743573, distance: 1.061350972817949 entropy 11.690749694180374
epoch: 62, step: 81
	action: tensor([[-35767.8953, -28096.5226, -16121.7392,  83099.1027,  33618.0833,
          62882.5110,   6572.1991]], dtype=torch.float64)
	q_value: tensor([[-26.8421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45585919613172465, distance: 1.380754223433125 entropy 11.825451516648457
epoch: 62, step: 82
	action: tensor([[  8922.2041, -47602.1177,  51395.4442,  29086.8210,  19527.1291,
          -1004.9184,  15185.5696]], dtype=torch.float64)
	q_value: tensor([[-25.6260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.786434831939914
epoch: 62, step: 83
	action: tensor([[  1322.4475,  18472.4786, -20462.1201,  -1408.0700,   5717.1730,
         -12508.6870,  42716.0260]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7518204221355218, distance: 0.5700851312346052 entropy 11.211331666873718
epoch: 62, step: 84
	action: tensor([[-36071.9160, -33051.4558,   6310.2002, -26848.3764, -17526.0888,
         -10272.9775,  -4795.5460]], dtype=torch.float64)
	q_value: tensor([[-29.5818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8752975127920952, distance: 1.5670822160759157 entropy 11.40582217603804
epoch: 62, step: 85
	action: tensor([[ 25079.8620,  31114.9996, -15862.2615, -66322.2284,  24228.2136,
          35223.7792,  41734.1172]], dtype=torch.float64)
	q_value: tensor([[-28.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1877670338169135, distance: 1.0313284526982431 entropy 11.800421836042556
epoch: 62, step: 86
	action: tensor([[-32867.9796, -26185.6031,  13517.2438,  15892.4180,  28876.0698,
          -2083.1988,   8943.8971]], dtype=torch.float64)
	q_value: tensor([[-28.8493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6668466122389443, distance: 1.4774218295539465 entropy 11.488227404437685
epoch: 62, step: 87
	action: tensor([[   451.6674,   5727.7752, -26859.5507,  22018.1785,  11202.9015,
         -29973.7768, -55467.3728]], dtype=torch.float64)
	q_value: tensor([[-23.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.674584339191911
epoch: 62, step: 88
	action: tensor([[-15592.0793, -35639.2670,   4045.9254, -16587.9974,  -5602.9278,
           9983.0300,   1545.1672]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02018910826766085, distance: 1.1558381759000664 entropy 11.211331666873718
epoch: 62, step: 89
	action: tensor([[-5611.2286,  -992.8774, 11263.2566, 48931.9280,  1507.1664,  8580.9223,
         30723.3456]], dtype=torch.float64)
	q_value: tensor([[-25.7957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33044295471403107, distance: 1.3199419214644785 entropy 11.607360761590895
epoch: 62, step: 90
	action: tensor([[-12290.4139, -19553.7358,  -2712.0682,   -577.0473,  35554.2766,
           6111.0125, -19012.6806]], dtype=torch.float64)
	q_value: tensor([[-22.3229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19703288235717142, distance: 1.252015580936905 entropy 11.64329791564819
epoch: 62, step: 91
	action: tensor([[  8198.1866, -25307.1668,  -8637.7284,  54299.8146,  34100.4316,
          56065.7783, -55682.7780]], dtype=torch.float64)
	q_value: tensor([[-25.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2149660753444258, distance: 1.2613591706676581 entropy 11.705027205229428
epoch: 62, step: 92
	action: tensor([[ 20618.4190, -10662.2410,   7457.1607,  25887.7905,    734.5436,
           2127.7474,  25644.6459]], dtype=torch.float64)
	q_value: tensor([[-26.6083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4529904793442626, distance: 0.8463580628933421 entropy 11.517084841101724
epoch: 62, step: 93
	action: tensor([[ 15848.2827,  -9540.4841,  37358.9176,  -1676.3835,  31411.5794,
          10992.1636, -47483.7823]], dtype=torch.float64)
	q_value: tensor([[-30.8877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.054374975038692375, distance: 1.175044294509597 entropy 11.72211020508855
epoch: 62, step: 94
	action: tensor([[  3162.5804,   5379.8595, -35572.7119,  -5967.1252,  35197.7235,
          40614.1158,   1649.2585]], dtype=torch.float64)
	q_value: tensor([[-28.0018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2334982846148569, distance: 1.0018743520607618 entropy 11.533918289068167
epoch: 62, step: 95
	action: tensor([[-59309.4751,   2804.6984,   6226.4675,  12743.2402,  20397.7238,
          -9821.8844, -14640.2009]], dtype=torch.float64)
	q_value: tensor([[-36.3741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5546716120724864, distance: 0.763654444226811 entropy 11.829370164454712
epoch: 62, step: 96
	action: tensor([[-27665.3455,  15496.9039,  15041.0787,  22631.8752,   2629.9660,
         -20626.8825,   8263.3341]], dtype=torch.float64)
	q_value: tensor([[-30.2463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3940069193107638, distance: 0.8908211630993833 entropy 11.662395708057673
epoch: 62, step: 97
	action: tensor([[ -7332.7355,   3279.5467,  20655.3304,  27766.6402, -26231.5479,
          27448.9988,  11595.0418]], dtype=torch.float64)
	q_value: tensor([[-29.7696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0015521410643577482, distance: 1.1452320015104218 entropy 11.796260514723826
epoch: 62, step: 98
	action: tensor([[ 21841.3010, -44718.4588,  15934.0041, -21947.4838, -23452.1187,
         -41268.3784, 115925.7434]], dtype=torch.float64)
	q_value: tensor([[-30.4385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26083328054061905, distance: 1.2849479183887391 entropy 11.867846221708225
epoch: 62, step: 99
	action: tensor([[ -9808.9408,  10439.9511, -33691.3590,  10746.1892,  15769.3251,
         -33190.1801,  15993.2475]], dtype=torch.float64)
	q_value: tensor([[-25.3506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.632269995100456
epoch: 62, step: 100
	action: tensor([[-28777.4266,  19592.8439, -16662.9397,   7075.3746,  25144.4950,
          -5153.3742, -30711.3243]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.211331666873718
epoch: 62, step: 101
	action: tensor([[-12353.2885,  18115.3366,  -7031.4419,   1540.9372,    900.1453,
           4865.4053, -22946.3599]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.211331666873718
epoch: 62, step: 102
	action: tensor([[-14848.7175,   2802.3047,    700.2737,  -4527.7889,   3989.3131,
          23026.0631, -20211.3399]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.211331666873718
epoch: 62, step: 103
	action: tensor([[-11021.9333, -20350.3248,  18773.1800,  11475.6137,   -887.5257,
           1661.9242,   5477.8189]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35713259369709216, distance: 1.3331156711796455 entropy 11.211331666873718
epoch: 62, step: 104
	action: tensor([[-37318.3868,  22510.5396,  18606.8307,  53788.3095,  46184.7469,
         -26819.3508,  35081.5990]], dtype=torch.float64)
	q_value: tensor([[-22.6916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28393361801660655, distance: 0.9683521824707374 entropy 11.704200333676074
epoch: 62, step: 105
	action: tensor([[ -5841.2471, -18315.9404,   2106.8746,  32024.4827, -13997.0191,
           5803.9170, -56123.1926]], dtype=torch.float64)
	q_value: tensor([[-34.0111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09527949702994065, distance: 1.1976203647131258 entropy 12.018491025722545
epoch: 62, step: 106
	action: tensor([[-24179.4136, -47441.3882,  24402.0997, -29398.0245,   9196.4075,
         -43276.9940,  45300.0652]], dtype=torch.float64)
	q_value: tensor([[-25.9112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.101241526564035, distance: 1.6588024985032634 entropy 11.799651594685015
epoch: 62, step: 107
	action: tensor([[ 29433.4680,   -217.4323,   9431.5460,  34142.6874, -33781.5530,
         -12299.5968,  -1272.7349]], dtype=torch.float64)
	q_value: tensor([[-28.2488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3811340324876834, distance: 0.9002331342105149 entropy 11.829098472893929
epoch: 62, step: 108
	action: tensor([[-70683.0546,   4637.3572, -39377.8894,  22356.1688, -29585.9489,
          71281.2240,  -6787.8299]], dtype=torch.float64)
	q_value: tensor([[-34.3094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6776961679014037, distance: 0.6496649365874172 entropy 11.831476394600868
epoch: 62, step: 109
	action: tensor([[-51389.2888,  33576.3986,   2714.0784, -26004.4221,   1212.1242,
           6339.0974,  15977.5454]], dtype=torch.float64)
	q_value: tensor([[-26.9145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21905200803923297, distance: 1.0112714570801287 entropy 11.634700316321995
epoch: 62, step: 110
	action: tensor([[ 28628.2219, -15096.0516, -40136.8176,  18536.1685, -16009.6994,
          48197.4504,  22711.0680]], dtype=torch.float64)
	q_value: tensor([[-28.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.326221755900038
epoch: 62, step: 111
	action: tensor([[-12860.6624, -20110.2189, -16767.1492,  24618.2738, -16426.0814,
          -1776.2999,    962.0209]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23895938496490277, distance: 1.2737530242304758 entropy 11.211331666873718
epoch: 62, step: 112
	action: tensor([[ -8417.3708, -33963.0495,  30682.9089,   6219.7267, -42215.4854,
          20027.6248,  19776.0694]], dtype=torch.float64)
	q_value: tensor([[-26.3145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0915935718803671, distance: 1.0906786015843388 entropy 11.8074491055837
epoch: 62, step: 113
	action: tensor([[-48160.9853, -39883.6222, -24735.7368,  40635.5126, -44220.3050,
           1420.0795,  58736.1837]], dtype=torch.float64)
	q_value: tensor([[-25.1351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18768260366998035, distance: 1.0313820536684775 entropy 11.751701079707257
epoch: 62, step: 114
	action: tensor([[ -6769.8092, -15831.6923,  15865.5645,   1053.9120,  -5909.6861,
         -16592.1464,  67518.9462]], dtype=torch.float64)
	q_value: tensor([[-23.0464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05739620629748221, distance: 1.1767265902547763 entropy 11.62933501616614
epoch: 62, step: 115
	action: tensor([[-13510.0203, -34829.7033,  33596.4381,  -2808.2222,   6829.3173,
         -10658.4188,  60028.9590]], dtype=torch.float64)
	q_value: tensor([[-25.3089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20356076097741305, distance: 1.2554247994764607 entropy 11.73277547323413
epoch: 62, step: 116
	action: tensor([[-12961.3410,  18561.9204,  50314.9475,  57064.5399, -23259.2850,
          14943.3058,  -9371.4011]], dtype=torch.float64)
	q_value: tensor([[-22.0761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5693404073943373, distance: 0.7509720194599392 entropy 11.437361321396864
epoch: 62, step: 117
	action: tensor([[-33586.3981,  -5949.3136, -24115.2500,  84517.7443,  10315.0419,
          27775.1675, -23921.5295]], dtype=torch.float64)
	q_value: tensor([[-26.6506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4111596314868573, distance: 1.3593921741142176 entropy 11.660403166930712
epoch: 62, step: 118
	action: tensor([[  1826.5314,   2010.9430, -14874.9492,  57494.1133,  12697.3028,
         -18522.2814,   -916.4050]], dtype=torch.float64)
	q_value: tensor([[-23.1322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24361881013064923, distance: 0.9952382370932705 entropy 11.709969289460158
epoch: 62, step: 119
	action: tensor([[-34834.2162, -16869.3251,  -1931.9648,  15000.3717,     60.9664,
         -28418.9951,  -5637.3160]], dtype=torch.float64)
	q_value: tensor([[-24.8515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30070683173496104, distance: 1.3051078561257528 entropy 11.529058414485545
epoch: 62, step: 120
	action: tensor([[-36003.7869, -48428.7402,  12326.1052,  40061.5059,  34956.8921,
          18892.8613, -24052.5195]], dtype=torch.float64)
	q_value: tensor([[-27.2544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.719608973777744, distance: 1.5006228137432325 entropy 11.911014170480879
epoch: 62, step: 121
	action: tensor([[ -8810.4921, -25006.2857, -22204.5262,  -2324.8807,  22881.0371,
         -28339.7722, -44550.4991]], dtype=torch.float64)
	q_value: tensor([[-25.0425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.700403623962641, distance: 1.4922194768199077 entropy 11.871051040933738
epoch: 62, step: 122
	action: tensor([[  3871.5687,  10856.1148, -63222.1613,   3982.0158,  46408.8601,
         -17520.1533, -18805.9357]], dtype=torch.float64)
	q_value: tensor([[-25.1983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.695160528021338
epoch: 62, step: 123
	action: tensor([[ -8723.6857,  -4462.7041,  -1881.7759,   4675.4470,  -9630.2015,
         -26176.0582, -10262.7113]], dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15381006122808027, distance: 1.0526660629866111 entropy 11.211331666873718
epoch: 62, step: 124
	action: tensor([[-47100.9573, -53052.3475, -10371.5482,  56138.7219,    977.6312,
          11443.0707,   8842.6504]], dtype=torch.float64)
	q_value: tensor([[-30.1607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33694898879373136, distance: 1.3231653269058843 entropy 11.85163571469285
epoch: 62, step: 125
	action: tensor([[-35246.7575, -90133.6870, -38921.6590,  10695.9939,  47779.1247,
          15821.2683,  34060.1041]], dtype=torch.float64)
	q_value: tensor([[-26.6339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19458856429353744, distance: 1.2507366318689725 entropy 11.919718597433775
epoch: 62, step: 126
	action: tensor([[-64709.9663,   2630.7166, -48265.3071,  20163.0961,  18039.0628,
          58974.1388, -50308.6957]], dtype=torch.float64)
	q_value: tensor([[-27.0755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16363052316320625, distance: 1.0465398758819344 entropy 11.850961029759247
epoch: 62, step: 127
	action: tensor([[-31861.5398,   4289.8844,  20812.6761,  60985.0723,  22519.1810,
          -3972.9673,  31637.2530]], dtype=torch.float64)
	q_value: tensor([[-25.1044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03722953618880387, distance: 1.1228405091621858 entropy 11.596663825219347
LOSS epoch 62 actor 295.75014875179005 critic 250.48756569460033
epoch: 63, step: 0
	action: tensor([[ 26602.8990,  -2603.7377, -10035.2077,  25602.2116,  -6054.6951,
          41396.3301,  12189.5157]], dtype=torch.float64)
	q_value: tensor([[-25.6585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3991206041440949, distance: 0.8870545940220925 entropy 11.734168672433368
epoch: 63, step: 1
	action: tensor([[-20507.3977, -15307.7085, -19816.8852,  37226.2290,  25066.5976,
          11400.6753,  13538.2716]], dtype=torch.float64)
	q_value: tensor([[-25.9552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3441071394202444, distance: 1.3267027740728206 entropy 11.703459338502665
epoch: 63, step: 2
	action: tensor([[-64976.3007, -71357.7957, -30485.9138,  16183.0489,  27911.4285,
          12529.8434,   2209.0961]], dtype=torch.float64)
	q_value: tensor([[-24.5987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1171545129103857, distance: 1.20952072792348 entropy 11.84050863495755
epoch: 63, step: 3
	action: tensor([[ -7240.2909, -74067.3221, -36051.4909,  -5401.4007, -45382.2400,
         -14612.5822,  -4855.2522]], dtype=torch.float64)
	q_value: tensor([[-23.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8903696107100427, distance: 1.5733670717457464 entropy 11.821700545391534
epoch: 63, step: 4
	action: tensor([[-36753.7547, -62485.8288,  15186.9326,  19891.8443,  -4499.9986,
         -16621.0913,    -92.3376]], dtype=torch.float64)
	q_value: tensor([[-21.5558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.168456229196567, distance: 1.2369806822583773 entropy 11.599756894100507
epoch: 63, step: 5
	action: tensor([[-22208.5073, -15646.4304,  39870.6703,  15518.7209,  12062.3592,
         -47730.9213, -19159.2916]], dtype=torch.float64)
	q_value: tensor([[-22.6053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5678101789184737, distance: 1.4328589249289947 entropy 11.770023345831406
epoch: 63, step: 6
	action: tensor([[ 10803.5288, -71495.0436,  11090.7275,  44494.4220,   8865.8354,
           7270.5780,  35431.6671]], dtype=torch.float64)
	q_value: tensor([[-24.8930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.853562650826737
epoch: 63, step: 7
	action: tensor([[-20988.6428,   7810.0002, -22146.1493,  44889.3938, -31100.9753,
            660.3089,  -7676.8574]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5509989130158508, distance: 1.4251561129986587 entropy 11.248905552976908
epoch: 63, step: 8
	action: tensor([[-24811.9828, -20857.8721,  10508.0498, -20272.5151, -53785.4282,
         -40287.7521, -39015.4783]], dtype=torch.float64)
	q_value: tensor([[-20.7901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8003513754442753, distance: 1.5354487694124237 entropy 11.582227086218252
epoch: 63, step: 9
	action: tensor([[-31826.0757,   1178.2960,  -7182.4134,  18491.5489,  24306.5693,
         -22558.4227,  75907.3563]], dtype=torch.float64)
	q_value: tensor([[-27.8390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24440438483280302, distance: 0.9947212774569738 entropy 11.921279000740583
epoch: 63, step: 10
	action: tensor([[-13159.1936,   7887.6741, -41979.9227,  16385.9465,   5870.0500,
           1887.8133,    693.3952]], dtype=torch.float64)
	q_value: tensor([[-28.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23018554951222847, distance: 1.004037012606251 entropy 11.873679014682414
epoch: 63, step: 11
	action: tensor([[-26551.5171,  -3875.8067, -56332.7458,  42625.6578, -83515.9783,
         -16684.2792, -22266.7354]], dtype=torch.float64)
	q_value: tensor([[-25.4438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14567257846227366, distance: 1.2248614109516964 entropy 11.804658774133454
epoch: 63, step: 12
	action: tensor([[-13037.1084, -68998.4869,  -1584.0038,  -9205.5202,  26924.1137,
          24848.4320,  19834.5925]], dtype=torch.float64)
	q_value: tensor([[-23.0187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5723663883538721, distance: 1.4349394284474182 entropy 11.719214703775803
epoch: 63, step: 13
	action: tensor([[ 15686.8210,   1387.5487,  68402.7308,  13829.1108,   6679.1474,
         -10421.4110, -10314.8552]], dtype=torch.float64)
	q_value: tensor([[-19.3909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.437607211547004
epoch: 63, step: 14
	action: tensor([[-22302.3911, -60937.6914, -17473.6411,  23385.9286, -28980.8725,
           5431.9076,  -7074.0953]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8469101660737821, distance: 1.5551761207655348 entropy 11.248905552976908
epoch: 63, step: 15
	action: tensor([[ 26169.2022,  14395.3936, -93150.6616,   3422.4136, -37875.3587,
          41704.4058, -54489.6840]], dtype=torch.float64)
	q_value: tensor([[-22.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5450173868541179, distance: 0.7718876521928562 entropy 11.824476178237477
epoch: 63, step: 16
	action: tensor([[ 13923.7493, -13686.3372,  16687.7533,  13761.0593, -22315.5126,
          24641.4213, -28979.5701]], dtype=torch.float64)
	q_value: tensor([[-22.5686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09830609768713583, distance: 1.0866414311271704 entropy 11.52333335915138
epoch: 63, step: 17
	action: tensor([[ 14708.1562, -63679.6074,   9821.8804, -11859.3079,   4421.5421,
           8447.5252,  26429.3652]], dtype=torch.float64)
	q_value: tensor([[-28.2142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3374326323410197, distance: 0.931476096460608 entropy 11.695127646417406
epoch: 63, step: 18
	action: tensor([[ 50349.3046, -13744.8184,  14934.6539,  -4663.9665,  11138.2259,
         -15049.9798, -39888.5217]], dtype=torch.float64)
	q_value: tensor([[-23.6329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46603740128538984, distance: 1.3855723824939632 entropy 11.621049284335372
epoch: 63, step: 19
	action: tensor([[-13982.5954,   9503.0655,  39481.2850,  42516.6719,  11138.1536,
          37065.5901,  72489.1571]], dtype=torch.float64)
	q_value: tensor([[-27.3010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.70204658808621
epoch: 63, step: 20
	action: tensor([[-57781.3599, -13396.1953, -37178.1952,  49056.8231,  -7370.3441,
         -22926.1268,   1332.9319]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8871475344054658, distance: 1.5720256222272047 entropy 11.248905552976908
epoch: 63, step: 21
	action: tensor([[  7317.5353, -42047.2444,   6757.6710,   3620.7566, -16973.3738,
         -16045.0779,   9768.1717]], dtype=torch.float64)
	q_value: tensor([[-21.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5154567204815831, distance: 0.7965682286714852 entropy 11.617938138010905
epoch: 63, step: 22
	action: tensor([[-17945.2792,   8859.9233, -15716.4447,  19851.5879,  45394.0006,
         -38900.8948,  16374.7756]], dtype=torch.float64)
	q_value: tensor([[-25.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20336093776476694, distance: 1.0213803353463446 entropy 11.848802059869225
epoch: 63, step: 23
	action: tensor([[ 22082.0413, -34724.4743, -80714.1807,  17708.4657,   8119.2247,
         -16314.3813,  15821.7369]], dtype=torch.float64)
	q_value: tensor([[-28.1477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1729024720542739, distance: 1.040722765355154 entropy 11.835538951332333
epoch: 63, step: 24
	action: tensor([[-27562.4020,    932.6371,  13642.6531,  -5892.1582,  -2689.8822,
          14974.7551,  16566.6477]], dtype=torch.float64)
	q_value: tensor([[-25.2463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.577052933131668
epoch: 63, step: 25
	action: tensor([[ 25073.2558,   9053.5825,   4086.0332,   6356.4487,   2250.2060,
         -17596.0572, -25861.3817]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.536349983380436, distance: 0.7792051838220655 entropy 11.248905552976908
epoch: 63, step: 26
	action: tensor([[-62122.3483, -15045.6968,  43934.8705,   6396.0075, -21778.4783,
         -16590.8917,  43456.8666]], dtype=torch.float64)
	q_value: tensor([[-26.9158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3678828573079618, distance: 1.3383852653458317 entropy 11.72482779513338
epoch: 63, step: 27
	action: tensor([[-66672.3068, -48731.5398, -21611.2315,  74819.1065, -59703.0729,
           1516.0170, -22006.4856]], dtype=torch.float64)
	q_value: tensor([[-25.0988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19738391887682472, distance: 1.0252047784465548 entropy 11.822349223975175
epoch: 63, step: 28
	action: tensor([[ 33416.1534, -14594.7479,  42294.3022,  12669.4389,  -9003.3368,
          11182.9749, -43082.1615]], dtype=torch.float64)
	q_value: tensor([[-25.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29671931026505804, distance: 0.9596680579030065 entropy 11.907086498406965
epoch: 63, step: 29
	action: tensor([[-39426.0746,  16218.6418, -19218.3631,  11885.6573,   3406.7456,
          63448.5239,  39868.0420]], dtype=torch.float64)
	q_value: tensor([[-26.8975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03291581170928204, distance: 1.1630252832848038 entropy 11.686217777988949
epoch: 63, step: 30
	action: tensor([[-38354.0765, -30300.4213,  10569.9413,   1108.3795, -29331.9217,
          14419.4056, -19785.0073]], dtype=torch.float64)
	q_value: tensor([[-23.6754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41811401195837083, distance: 1.362737689207873 entropy 11.642050623151732
epoch: 63, step: 31
	action: tensor([[-51487.9389, -38495.1798,  26101.0360,  27755.2544,   4185.6441,
          48937.9346,  64788.6225]], dtype=torch.float64)
	q_value: tensor([[-23.1855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08926618250433771, distance: 1.0920748983782178 entropy 11.83559175520551
epoch: 63, step: 32
	action: tensor([[-2.0715e+04, -7.8868e+04,  5.4916e+01,  1.8248e+04,  3.9422e+04,
          1.0004e+05, -3.4379e+04]], dtype=torch.float64)
	q_value: tensor([[-22.5951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4586267331283116, distance: 1.382065982872887 entropy 11.797072043704395
epoch: 63, step: 33
	action: tensor([[ 46639.1847,  20027.6190,  51840.8877,  50918.7093, -30725.3869,
          -1165.0421,  -5505.1779]], dtype=torch.float64)
	q_value: tensor([[-23.3282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3765188907954792, distance: 0.903583606361924 entropy 11.857727998798852
epoch: 63, step: 34
	action: tensor([[-10617.6714, -47270.9539,  26197.4809,  15389.5289, -61746.6116,
          16016.2449, -28132.1667]], dtype=torch.float64)
	q_value: tensor([[-25.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15640887690971106, distance: 1.051048347722628 entropy 11.602593058645878
epoch: 63, step: 35
	action: tensor([[  2039.2917, -15052.7966, -59520.2041, -45350.4489,  -4441.1614,
          85282.8949,  55344.5826]], dtype=torch.float64)
	q_value: tensor([[-24.2602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19527575292642219, distance: 1.251096324089125 entropy 11.899129361126947
epoch: 63, step: 36
	action: tensor([[  2552.6904, -30030.0608, -18719.2546,  12918.7208,  39910.3614,
          23194.6393, -32091.6260]], dtype=torch.float64)
	q_value: tensor([[-24.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3398683000587627, distance: 0.9297624177720004 entropy 11.632020242434022
epoch: 63, step: 37
	action: tensor([[-13451.0993,  39429.4119, -17541.9599, -11971.1820,  75242.3957,
         -16845.0639,  22568.1645]], dtype=torch.float64)
	q_value: tensor([[-25.5752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.730926306573304
epoch: 63, step: 38
	action: tensor([[-16053.9140, -20926.9663,   8997.6559, -16174.7710,  10288.9170,
          -3649.0395,  36778.0207]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3494762258526396, distance: 1.3293499152021078 entropy 11.248905552976908
epoch: 63, step: 39
	action: tensor([[-1708.5291, 20943.8712, 12357.1633, -8309.8978, 28524.3976, 18572.8079,
         60285.4831]], dtype=torch.float64)
	q_value: tensor([[-22.1185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20729095263125585, distance: 1.018857861304292 entropy 11.690047947997607
epoch: 63, step: 40
	action: tensor([[-13684.2682, -19751.7274, -21544.3176,  12337.3642, -44281.2020,
          40373.9221,  18341.3768]], dtype=torch.float64)
	q_value: tensor([[-34.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6651099108868197, distance: 1.4766519599207 entropy 11.732024929889791
epoch: 63, step: 41
	action: tensor([[-27000.7744, -85522.4846,  28509.3285,  30011.2846,  31476.3414,
         -10515.4121, -20076.7843]], dtype=torch.float64)
	q_value: tensor([[-21.3929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2605283358294278, distance: 0.9840506708002527 entropy 11.675735422752691
epoch: 63, step: 42
	action: tensor([[-35057.8823, -81726.7038,  -2443.6187,   6059.9751,   6384.6001,
         -82603.0864, -10689.9039]], dtype=torch.float64)
	q_value: tensor([[-25.3533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6318813525196385, distance: 1.4618438438006076 entropy 11.886767804685874
epoch: 63, step: 43
	action: tensor([[-14711.2032,  -2773.4399, -15474.3736, 117619.5920,  -7388.5011,
          14466.8535,   5818.5715]], dtype=torch.float64)
	q_value: tensor([[-24.4767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21288170959781572, distance: 1.015258643101479 entropy 11.750513571846565
epoch: 63, step: 44
	action: tensor([[ 43864.7718, -38092.8735,  -6136.6465, -17563.5689, -43421.7576,
          -2333.3889,   1970.1176]], dtype=torch.float64)
	q_value: tensor([[-20.9942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3862689393180797, distance: 0.8964906097504921 entropy 11.641540340314764
epoch: 63, step: 45
	action: tensor([[-21823.5955, -29712.6342, -42864.0677, -54620.9891,  10629.2602,
           3802.9639, -12479.2096]], dtype=torch.float64)
	q_value: tensor([[-24.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034001083467773974, distance: 1.1636361109583004 entropy 11.489857720988718
epoch: 63, step: 46
	action: tensor([[-23630.7934, -35970.4089,   6669.2345,  11033.7471, -20095.1648,
         -39331.2902,  83255.7221]], dtype=torch.float64)
	q_value: tensor([[-20.8316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47990051475938333, distance: 1.3921080790561668 entropy 11.61910934862773
epoch: 63, step: 47
	action: tensor([[ -9946.7304,  -2735.4886,   6903.5878, -37191.8536, -21471.6846,
          32439.4244,  -2315.3315]], dtype=torch.float64)
	q_value: tensor([[-23.3945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12258927222256644, distance: 1.0719096069802347 entropy 11.701648663201196
epoch: 63, step: 48
	action: tensor([[-88902.3698, -34815.3735, -39360.1306,   6461.6566, -22549.1292,
         -38608.5080,  30300.8888]], dtype=torch.float64)
	q_value: tensor([[-30.5169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4162178824036735, distance: 1.3618263409339626 entropy 11.994198551881778
epoch: 63, step: 49
	action: tensor([[-33004.4612, -17069.2392,   4127.5168,  71300.9724,  65739.5341,
         -11103.7504, -13440.6461]], dtype=torch.float64)
	q_value: tensor([[-24.1938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7412031349796329, distance: 1.5100155285789774 entropy 11.848304867415536
epoch: 63, step: 50
	action: tensor([[ -5579.6237, -25916.5587,  12747.4957,  37264.3239,  -8938.7111,
          46250.0545,  53011.5833]], dtype=torch.float64)
	q_value: tensor([[-24.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.840652447495321, distance: 1.5525392540494642 entropy 11.800675839256188
epoch: 63, step: 51
	action: tensor([[-99691.4796, -39082.0140, -45767.2704, -10094.2920,    704.0325,
          63773.0350,  -9173.2084]], dtype=torch.float64)
	q_value: tensor([[-23.5869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.890009056622399, distance: 1.5732170188200498 entropy 11.826597322622865
epoch: 63, step: 52
	action: tensor([[ -1439.4094, -18301.2001, -15103.3848,  -5205.5016, -35282.4104,
           2668.1738, -16585.8466]], dtype=torch.float64)
	q_value: tensor([[-18.8482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23062243114804404, distance: 1.2694602504725587 entropy 11.415020845936894
epoch: 63, step: 53
	action: tensor([[ 10502.4419, -27769.9681, -51509.5873,  36819.9290,  34378.5203,
          48753.2768,  17887.5295]], dtype=torch.float64)
	q_value: tensor([[-23.2970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24491411409083796, distance: 1.276810332853125 entropy 11.729634502351542
epoch: 63, step: 54
	action: tensor([[ -2083.5735, -14126.2015, -45639.8193,  30370.5562,   1892.4305,
          16716.7219,  35383.1873]], dtype=torch.float64)
	q_value: tensor([[-22.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23051627743313785, distance: 1.0038213120914008 entropy 11.64829760042845
epoch: 63, step: 55
	action: tensor([[-17414.5840,   -491.6027,   9873.2061,  50554.3380, -22324.2838,
         -57005.5628, -28792.4789]], dtype=torch.float64)
	q_value: tensor([[-22.3720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21526813571726988, distance: 1.261515958149606 entropy 11.722950557321264
epoch: 63, step: 56
	action: tensor([[ 40080.4186, -27013.7889,  24344.0589,   2619.8031,  -1553.8724,
         -12917.9676,  14712.5142]], dtype=torch.float64)
	q_value: tensor([[-27.0985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14751659831390806, distance: 1.056573366811751 entropy 11.878009246036955
epoch: 63, step: 57
	action: tensor([[  -137.9839, -73130.8671, -23992.1337, -18876.4842, -44559.1750,
          50140.5882, -15491.5543]], dtype=torch.float64)
	q_value: tensor([[-31.3098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6463295277482224, distance: 1.4683009407395733 entropy 11.909173981463653
epoch: 63, step: 58
	action: tensor([[-20877.1041, -16427.8375,  33324.3452, -21928.7129,  -3445.0322,
         -45619.0283, -13314.7986]], dtype=torch.float64)
	q_value: tensor([[-22.8584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3223890541524146, distance: 1.3159406908365188 entropy 11.798352509277434
epoch: 63, step: 59
	action: tensor([[ 25005.9932, -43061.0418, -18461.3830,   1156.7469,  32020.5356,
          -9838.3277, -46334.2469]], dtype=torch.float64)
	q_value: tensor([[-27.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23889033770485568, distance: 1.27371753058042 entropy 11.828397999309953
epoch: 63, step: 60
	action: tensor([[ -3489.4176, -20466.7061, -23969.1440,  16460.9168,   7118.7264,
           -391.7964,  19787.2801]], dtype=torch.float64)
	q_value: tensor([[-24.2912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5554743798552806, distance: 1.4272108031517163 entropy 11.57731006657934
epoch: 63, step: 61
	action: tensor([[ -3246.4060, -18979.1822,  25383.7883,  14598.3439,  53059.2857,
          25978.8457,  -3951.2953]], dtype=torch.float64)
	q_value: tensor([[-21.7215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7238543444873575, distance: 1.5024740407043367 entropy 11.672948505878193
epoch: 63, step: 62
	action: tensor([[ 19710.3200, -21131.2386, -22484.5565,  30083.9705,  51084.4615,
         -28649.1619,  27803.5434]], dtype=torch.float64)
	q_value: tensor([[-22.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10252578052873462, distance: 1.0840958559334841 entropy 11.787143226950503
epoch: 63, step: 63
	action: tensor([[-16395.7563,  -5312.9658, -22689.5414,   3042.7775, -14734.3508,
          -5043.4800,  23944.3296]], dtype=torch.float64)
	q_value: tensor([[-24.2163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8822729848959037, distance: 1.5699940186127816 entropy 11.614709359001953
epoch: 63, step: 64
	action: tensor([[-29914.9936, -22399.0174,   7171.6941,   -751.5771,  40695.4502,
           -294.7044,  -5277.0716]], dtype=torch.float64)
	q_value: tensor([[-21.7873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6561797420094126, distance: 1.4726869125200088 entropy 11.65670253903806
epoch: 63, step: 65
	action: tensor([[ -9817.8276, -36682.2559,  54464.5102, -15798.7498,  -1149.7684,
            124.5938, -55515.4818]], dtype=torch.float64)
	q_value: tensor([[-24.6513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6282402297967757, distance: 1.460212069151808 entropy 11.732046345531758
epoch: 63, step: 66
	action: tensor([[-26331.2586, -52086.3492,    731.9213,  20702.4843, -23777.2192,
          13594.8610,  23367.8048]], dtype=torch.float64)
	q_value: tensor([[-19.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31795683183547685, distance: 1.3137335351257384 entropy 11.65574186119458
epoch: 63, step: 67
	action: tensor([[ -3045.0180,   7730.6771, -30044.1863,  12091.7471,  36521.1275,
         -55079.0111,  30947.1836]], dtype=torch.float64)
	q_value: tensor([[-22.0361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15025014398310443, distance: 1.054878019688144 entropy 11.75562337318237
epoch: 63, step: 68
	action: tensor([[ -4034.5280,  35277.5820,  24737.6684,  51542.3755, -73784.8379,
         -30299.2779, -30593.4551]], dtype=torch.float64)
	q_value: tensor([[-28.8698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45383969044831585, distance: 0.8457008386519743 entropy 11.944332811843582
epoch: 63, step: 69
	action: tensor([[-40733.3792,    -74.8179, -25909.9987,   8119.7879, -10556.2905,
          17174.7704, -22559.7064]], dtype=torch.float64)
	q_value: tensor([[-24.7992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3328334078793598, distance: 0.9347034353425949 entropy 11.651440472493109
epoch: 63, step: 70
	action: tensor([[ 32847.6806, -16302.4995, -65958.3909,  37472.4923, -10860.6325,
         -19938.5464,  -9862.2417]], dtype=torch.float64)
	q_value: tensor([[-22.5111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4422613902245246, distance: 0.8546180245438054 entropy 11.688359365875879
epoch: 63, step: 71
	action: tensor([[ -1010.0409,  31189.1916, -21219.5151, -39092.7771, -43924.1939,
         -39790.4826,  28581.6081]], dtype=torch.float64)
	q_value: tensor([[-28.4723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.930572709059573
epoch: 63, step: 72
	action: tensor([[ 19250.2662,   2020.7984,  38914.3171, -10150.4085,   1051.3228,
         -30325.2192,  11562.8621]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6310219281992435, distance: 0.6951154984931642 entropy 11.248905552976908
epoch: 63, step: 73
	action: tensor([[-18790.9118, -13877.2900, -34852.4710,   5936.1779,  21229.9002,
         -16069.6703, -25087.7870]], dtype=torch.float64)
	q_value: tensor([[-24.4744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10871678927274897, distance: 1.2049443935694037 entropy 11.502959162115294
epoch: 63, step: 74
	action: tensor([[-80333.0807, -64431.3429,  -8430.7498,  59596.6509, -14859.6703,
          -7548.8530,   -383.4242]], dtype=torch.float64)
	q_value: tensor([[-23.4285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.425203194649995, distance: 1.3661396205527299 entropy 11.705031742914963
epoch: 63, step: 75
	action: tensor([[-26178.8333, -17813.4398, -16219.5977, -20878.2176, -20367.0286,
          22897.5368,  17699.8111]], dtype=torch.float64)
	q_value: tensor([[-22.6110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6808909746925287, distance: 1.4836329360503384 entropy 11.639428578784011
epoch: 63, step: 76
	action: tensor([[ -2495.7205, -30628.0283,  -3816.9202, -21123.1386, -33297.0196,
          45201.1104,  33689.1143]], dtype=torch.float64)
	q_value: tensor([[-24.5343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21135411387657477, distance: 1.0162433445305863 entropy 11.791000482643927
epoch: 63, step: 77
	action: tensor([[ 30641.4740, -81076.8192, -67982.6897, -15660.1888,  15780.3457,
           3562.1255, -54909.0351]], dtype=torch.float64)
	q_value: tensor([[-29.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4627045457898967, distance: 0.8388093762557143 entropy 11.930695053057761
epoch: 63, step: 78
	action: tensor([[-54548.7023, -62735.8858, -37214.2502,  23854.5228,  75452.9244,
          14065.6463, -40710.6360]], dtype=torch.float64)
	q_value: tensor([[-24.2468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8355128076762424, distance: 1.5503701670386976 entropy 11.694271241198981
epoch: 63, step: 79
	action: tensor([[-14692.4297, -38541.7053, -11983.3245, -64584.6310, -29014.7495,
           1616.7633,  53538.8313]], dtype=torch.float64)
	q_value: tensor([[-20.5012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24662035787164127, distance: 1.2776850131670645 entropy 11.66238629839314
epoch: 63, step: 80
	action: tensor([[-28312.7791, -32457.9786,   6474.7714,  18184.4469, -28541.8099,
          33204.1336, -21473.4281]], dtype=torch.float64)
	q_value: tensor([[-21.2309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9024051228458656, distance: 1.5783677428596232 entropy 11.51749611415666
epoch: 63, step: 81
	action: tensor([[-14299.6733,  22077.5524,  61917.5185,   9022.4914,  -6363.7852,
         -40317.0936, -19793.7045]], dtype=torch.float64)
	q_value: tensor([[-23.6724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.311714603323907, distance: 0.9493819508689403 entropy 11.866847259630097
epoch: 63, step: 82
	action: tensor([[-16554.4022,  12770.0430,   4764.3573,   7466.8202, -41717.7890,
          11349.9079, -36058.0044]], dtype=torch.float64)
	q_value: tensor([[-28.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2205763583807785, distance: 1.0102840129866306 entropy 11.857514120096557
epoch: 63, step: 83
	action: tensor([[ 61101.6430, -64081.3373,  78895.8572,  27472.5959,  -7519.8758,
         -13157.1424, -32040.5694]], dtype=torch.float64)
	q_value: tensor([[-25.1232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24820596151494145, distance: 1.2784973126819854 entropy 11.754615139778334
epoch: 63, step: 84
	action: tensor([[ 25096.1526, -17675.6798,   4385.4735,  38000.1434, -17633.8120,
          11471.7072, -20518.6513]], dtype=torch.float64)
	q_value: tensor([[-30.4338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3392483668268591, distance: 0.9301988877682327 entropy 11.795506819206528
epoch: 63, step: 85
	action: tensor([[-87918.4195,  30556.5615,  27850.1945,  -7309.2790,  36066.6483,
         -51053.7737, -42878.6530]], dtype=torch.float64)
	q_value: tensor([[-28.4048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.849685470321779
epoch: 63, step: 86
	action: tensor([[ -1685.0896, -22748.6867,  -3283.7207, -13238.8730, -10323.6511,
         -17082.1152,   1159.5193]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8986978332028528, distance: 1.5768290800793272 entropy 11.248905552976908
epoch: 63, step: 87
	action: tensor([[ -5334.7618, -22610.8663, -71541.7525, -11291.0995, -20543.8688,
          11844.1098,   8179.3857]], dtype=torch.float64)
	q_value: tensor([[-19.5374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2712991022236635, distance: 1.2902698924083613 entropy 11.46886192340212
epoch: 63, step: 88
	action: tensor([[-49185.4444,   4227.2097, -18620.3381, -19143.8160,   1026.7673,
          29383.9298, -37429.8497]], dtype=torch.float64)
	q_value: tensor([[-25.4010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05036984249350618, distance: 1.1151516782674267 entropy 11.79772381854225
epoch: 63, step: 89
	action: tensor([[ 13564.0082, -46839.7085, -14964.0219,  15326.2668,   5493.9527,
         -19089.5100, -14899.8623]], dtype=torch.float64)
	q_value: tensor([[-30.7120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.812643263218774
epoch: 63, step: 90
	action: tensor([[-33409.2420, -19430.4835,  27137.5439,  13554.5261, -14611.5389,
           6103.3223,  -5622.6612]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.898418453707259, distance: 1.576713066379873 entropy 11.248905552976908
epoch: 63, step: 91
	action: tensor([[-38056.8768, -68153.3625, -41989.0451, -17298.0562,  -9388.3694,
           3552.8310,  11927.6178]], dtype=torch.float64)
	q_value: tensor([[-22.1036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9039641330189527, distance: 1.5790143421076654 entropy 11.797702993585292
epoch: 63, step: 92
	action: tensor([[ 19308.5763, -26925.5709,  -1127.5120, -16788.0177,   2662.3117,
          37199.7181,  31269.4221]], dtype=torch.float64)
	q_value: tensor([[-20.2556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5623014389362497, distance: 0.7570843219454421 entropy 11.625026156947984
epoch: 63, step: 93
	action: tensor([[-22849.3627,  -6859.5195, -16629.0298,  14916.0555, -13009.3075,
          28517.6613,    -44.0195]], dtype=torch.float64)
	q_value: tensor([[-24.7270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8972865379055848, distance: 1.5762429454380464 entropy 11.7138623196462
epoch: 63, step: 94
	action: tensor([[-14416.5551, -39207.8604, -23498.1407,   -345.3828, -56987.3561,
         -55390.3433,  55001.5209]], dtype=torch.float64)
	q_value: tensor([[-23.3812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6349916019974182, distance: 1.4632362657085456 entropy 11.851220746660003
epoch: 63, step: 95
	action: tensor([[ -2793.3641, -38291.7824,   9372.8246,  26556.6329,  10203.4664,
          34135.6840,  26494.6445]], dtype=torch.float64)
	q_value: tensor([[-22.3710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39985613133112485, distance: 1.3539368083060794 entropy 11.55433756070345
epoch: 63, step: 96
	action: tensor([[-30074.4179, -13271.4132,  29059.9707,  31763.7727, -25786.3848,
          21998.4175, -39018.9660]], dtype=torch.float64)
	q_value: tensor([[-18.8796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05672584499147304, distance: 1.1114134825432083 entropy 11.60178216567763
epoch: 63, step: 97
	action: tensor([[-34502.1961, -26013.8482, -17350.7734,  21878.1772, -21734.8946,
          40100.6595,  22910.1066]], dtype=torch.float64)
	q_value: tensor([[-20.7842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023177057605691553, distance: 1.1575295588254617 entropy 11.631544308600834
epoch: 63, step: 98
	action: tensor([[ 15461.2027, -24351.3995, -25707.0072,  44825.2762,  24730.4384,
         -32509.8702, -26388.8813]], dtype=torch.float64)
	q_value: tensor([[-22.2469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25756106182398186, distance: 1.283279433629339 entropy 11.813654882975436
epoch: 63, step: 99
	action: tensor([[  5375.1382,  15649.8417, -27595.6504,  56138.9670,  59390.3536,
          58448.7406,  -1363.0652]], dtype=torch.float64)
	q_value: tensor([[-23.5230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12065092755242934, distance: 1.07309296613096 entropy 11.697677138107588
epoch: 63, step: 100
	action: tensor([[-11431.7968, -12623.8723, -13563.5691,   8063.3295,  -2511.8309,
         -30562.9596,  29301.1460]], dtype=torch.float64)
	q_value: tensor([[-25.1546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.501477050038218
epoch: 63, step: 101
	action: tensor([[-12699.7941,   9647.7374,  31645.1884,  28866.4423,  10546.9365,
            372.5854,   9702.1001]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.248905552976908
epoch: 63, step: 102
	action: tensor([[ -6553.7238, -12616.0409, -15419.3330,  21549.0035,  -8729.4575,
          43146.6206,  20104.0404]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04877690036417193, distance: 1.116086583168127 entropy 11.248905552976908
epoch: 63, step: 103
	action: tensor([[-10032.5672,  -3234.9096, -28192.7397, 105834.5838, -25055.6035,
         -28912.6688,  28065.2321]], dtype=torch.float64)
	q_value: tensor([[-21.6381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3264658271132608, distance: 1.317967576575082 entropy 11.70119978831656
epoch: 63, step: 104
	action: tensor([[-49205.1668,  25800.8362, -30534.7627, -12965.7197,  26739.9773,
         -55401.4124, -15448.8427]], dtype=torch.float64)
	q_value: tensor([[-23.9004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02105513963259098, distance: 1.132232999783286 entropy 11.759312039012741
epoch: 63, step: 105
	action: tensor([[-24360.4219,  -2939.2876,    808.3015,  38048.8514, -22695.3520,
          37237.2600,   -164.5913]], dtype=torch.float64)
	q_value: tensor([[-27.2657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24730419858442243, distance: 0.9928106792256485 entropy 11.657025631867159
epoch: 63, step: 106
	action: tensor([[-15338.0881, -26861.3388, -19862.2765,  30816.3687,  30500.6200,
           7295.4320, -32192.9209]], dtype=torch.float64)
	q_value: tensor([[-22.5689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38899756606828695, distance: 1.3486753988815154 entropy 11.864166951588647
epoch: 63, step: 107
	action: tensor([[  5725.7102,   4518.2564,  30650.9014,   1426.4130, -22526.0090,
           5059.6044,  20608.5870]], dtype=torch.float64)
	q_value: tensor([[-23.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.510675826732135, distance: 0.800488374021462 entropy 11.832505585847247
epoch: 63, step: 108
	action: tensor([[-15377.2048, -12321.1293,   9136.5874,  36043.1934, -16288.0150,
          -3028.2676,  33701.1531]], dtype=torch.float64)
	q_value: tensor([[-21.4995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3890393391500797, distance: 0.8944649288450579 entropy 11.420351700696173
epoch: 63, step: 109
	action: tensor([[-22996.5302, -33827.1090,  -3994.1169,  -8303.2990, -56385.6787,
          76164.6657,  11740.5504]], dtype=torch.float64)
	q_value: tensor([[-24.0776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4706764375450432, distance: 1.3877628599258662 entropy 11.804120516498248
epoch: 63, step: 110
	action: tensor([[-14133.7890, -27633.9755, -71880.7054,  18831.2283,  -2365.1436,
           2367.3989,  37644.6250]], dtype=torch.float64)
	q_value: tensor([[-22.6230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5072052193175978, distance: 1.4048918333703357 entropy 11.705437178607067
epoch: 63, step: 111
	action: tensor([[-21080.0094, -25465.8551,  19024.8494, -20925.8692,  14706.6289,
           3386.4950,  40937.5857]], dtype=torch.float64)
	q_value: tensor([[-23.7193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0940660158939215, distance: 1.1969567475817657 entropy 11.886740243351712
epoch: 63, step: 112
	action: tensor([[ -9629.1375, -16085.2680,  10482.2567,    249.9514,    375.2457,
         -10884.5813, -14916.4180]], dtype=torch.float64)
	q_value: tensor([[-23.5377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07401311792688492, distance: 1.1011820168903401 entropy 11.777287930487828
epoch: 63, step: 113
	action: tensor([[ 10770.7130,  -2959.8809,  15017.5778,  -1439.7809, -13063.9967,
           8781.5055,  24549.0702]], dtype=torch.float64)
	q_value: tensor([[-19.5475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2277262955969398, distance: 1.0056394851764276 entropy 11.569514434836583
epoch: 63, step: 114
	action: tensor([[-44974.0803, -22398.0493,  37408.3711, -10290.2981,  15670.3208,
         -56123.9847, -11126.3966]], dtype=torch.float64)
	q_value: tensor([[-19.0055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3472534395584701, distance: 0.9245469905685285 entropy 11.328521818159997
epoch: 63, step: 115
	action: tensor([[ -9631.8140, -52059.9785,  13114.9681,  39806.0265, -13381.3180,
         -28843.5651, -22148.6248]], dtype=torch.float64)
	q_value: tensor([[-20.2269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2782752170510434, distance: 1.2938051566815567 entropy 11.503781309621047
epoch: 63, step: 116
	action: tensor([[-75012.4304, -37203.2034, -11007.6049,  27214.6085,  21620.9578,
          29526.3623, -56111.1823]], dtype=torch.float64)
	q_value: tensor([[-21.4425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35050872314647685, distance: 0.9222387253457339 entropy 11.582261073602075
epoch: 63, step: 117
	action: tensor([[  3498.2432,  -9613.8886,  73524.6259,  22121.1583, -16565.5063,
         -17686.9250,  32661.2223]], dtype=torch.float64)
	q_value: tensor([[-21.5552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706425091836866
epoch: 63, step: 118
	action: tensor([[-12964.3173, -60976.1810,  31133.3304,  27262.3382,  18311.1213,
         -23802.2861,    480.0733]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33993746910816736, distance: 1.3246433362754126 entropy 11.248905552976908
epoch: 63, step: 119
	action: tensor([[  4595.5836, -20590.6868,  14705.5231,  46203.7412,  -8785.5697,
          25578.6333,  44261.1211]], dtype=torch.float64)
	q_value: tensor([[-22.9011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.72249346814821
epoch: 63, step: 120
	action: tensor([[-14182.6701, -18378.0293,  11538.3869, -14136.0396,   2167.4804,
          13763.0868,  11806.9427]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22219278203461146, distance: 1.2651049370028697 entropy 11.248905552976908
epoch: 63, step: 121
	action: tensor([[   631.9556, -15905.4169, -17882.3021,  33036.1801,  35574.3235,
            260.8420, -24672.8977]], dtype=torch.float64)
	q_value: tensor([[-22.9410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15042494262634654, distance: 1.0547695167393683 entropy 11.606306147446633
epoch: 63, step: 122
	action: tensor([[-31296.9674,   9156.3227,  15388.7672,  29444.4624,  -3194.5931,
         -43802.0622,  -6225.7176]], dtype=torch.float64)
	q_value: tensor([[-19.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.375495238038692
epoch: 63, step: 123
	action: tensor([[ 2962.8825,   870.8151,    47.8975, 14981.0023,  7715.2990,  2943.0094,
         30985.7731]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4114763355165457, distance: 0.8778870820482524 entropy 11.248905552976908
epoch: 63, step: 124
	action: tensor([[  -163.5177,  16194.6573,  50071.8120,  35864.3024,  30262.0389,
         -22724.3438, -20386.9476]], dtype=torch.float64)
	q_value: tensor([[-27.0401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6323956956893292, distance: 0.6938202757633963 entropy 11.753984599844188
epoch: 63, step: 125
	action: tensor([[-42689.7753,  30563.0933, -56192.7726,  -6535.7925, -33154.9478,
          -2234.2124, -40037.1703]], dtype=torch.float64)
	q_value: tensor([[-26.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.755090470307309
epoch: 63, step: 126
	action: tensor([[ -7554.2234, -19238.0848,  -5273.7055, -13021.4910, -12317.2970,
          17848.1901, -10658.3701]], dtype=torch.float64)
	q_value: tensor([[-25.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20136198797273686, distance: 1.254277513799238 entropy 11.248905552976908
epoch: 63, step: 127
	action: tensor([[-55905.6883, -55258.1649, -36385.4324, -20797.3095,  46308.0271,
          -9541.1774, -37660.0667]], dtype=torch.float64)
	q_value: tensor([[-23.9838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08618331647766231, distance: 1.0939216928165278 entropy 11.832474326063437
LOSS epoch 63 actor 236.25762836106324 critic 333.5105608837975
epoch: 64, step: 0
	action: tensor([[-44596.6520, -69229.0586,  29147.3277,  -7256.0505,  76941.6647,
         -19423.4680,  30765.4645]], dtype=torch.float64)
	q_value: tensor([[-27.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20478291861075393, distance: 1.2560620492690997 entropy 11.898416160678241
epoch: 64, step: 1
	action: tensor([[ 14808.0254, -16333.3953,  14301.8121, -16017.5564, -46766.5044,
           5407.6890,  24635.8491]], dtype=torch.float64)
	q_value: tensor([[-30.8272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018217827503327788, distance: 1.154720940247676 entropy 11.996343179251113
epoch: 64, step: 2
	action: tensor([[-26248.1118, -20814.7782,  23946.1118, -17324.5318,  17859.1060,
           5858.7906,   4137.3923]], dtype=torch.float64)
	q_value: tensor([[-22.7512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13913961432401745, distance: 1.0617519197654033 entropy 11.494717748845472
epoch: 64, step: 3
	action: tensor([[-49223.8946,   4788.6526,  25300.5959,  15673.5711, -42521.7165,
          -4454.0079, -37497.6437]], dtype=torch.float64)
	q_value: tensor([[-26.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7215884098265288, distance: 1.501486246253034 entropy 11.77238312045652
epoch: 64, step: 4
	action: tensor([[ -5388.9987, -16763.6642,  27519.1252,   2881.3163, -20054.0188,
          15366.4475,   7531.7500]], dtype=torch.float64)
	q_value: tensor([[-23.8484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6343544160212733, distance: 1.462951113025298 entropy 11.598945254274005
epoch: 64, step: 5
	action: tensor([[-55575.8727, -39372.4122,  -3615.1933, -38357.4039,  17735.5575,
         -16833.8758,  24622.3843]], dtype=torch.float64)
	q_value: tensor([[-25.1959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9932598066577416, distance: 1.6156178693980647 entropy 11.857684602973148
epoch: 64, step: 6
	action: tensor([[-22068.4139,  23851.2038, -60317.5125,  30348.4790,   9630.2687,
          34099.0682,  11517.0060]], dtype=torch.float64)
	q_value: tensor([[-27.2808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.832517082665143
epoch: 64, step: 7
	action: tensor([[ -3959.4436, -15021.8250, -14561.9535,  -4413.0793,   3850.3291,
          37772.7522,   1230.0113]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011355516372316687, distance: 1.137828393490759 entropy 11.285530007871317
epoch: 64, step: 8
	action: tensor([[-33445.6222,  70517.9583, -83639.6620,  70302.0227, -16343.5533,
         -20638.9147,  20512.4568]], dtype=torch.float64)
	q_value: tensor([[-27.4107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015929121306372362, distance: 1.1534224441527101 entropy 11.805275913436361
epoch: 64, step: 9
	action: tensor([[-19516.3833,  14167.5961,  -2294.1326,  -8431.1235, -28260.7982,
          52473.0686, -50131.3635]], dtype=torch.float64)
	q_value: tensor([[-26.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17998291662055776, distance: 1.0362585892792346 entropy 11.694299069110173
epoch: 64, step: 10
	action: tensor([[-32620.1470,  32659.1238, -27238.8367, -17270.4821,   9809.5354,
          19759.0721,  35591.8797]], dtype=torch.float64)
	q_value: tensor([[-30.3212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.689976734891015
epoch: 64, step: 11
	action: tensor([[-16432.3907, -32200.9252,  16712.5699, -20965.3463, -29039.7060,
           8872.0061, -18471.9793]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09037800506034177, distance: 1.1949376208508946 entropy 11.285530007871317
epoch: 64, step: 12
	action: tensor([[-12303.1718, -36400.1993,   9679.4281,  -5798.4955, -39542.4929,
         -40298.5404,  23263.9202]], dtype=torch.float64)
	q_value: tensor([[-23.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5484768164238472, distance: 1.4239969104516297 entropy 11.649216424370305
epoch: 64, step: 13
	action: tensor([[-60515.9693, -78422.8149, -18138.6979, 105944.0768,  -5728.4262,
          24422.3588,  14534.0193]], dtype=torch.float64)
	q_value: tensor([[-30.6757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6361032971657801, distance: 1.4637336371534617 entropy 11.941645347210118
epoch: 64, step: 14
	action: tensor([[ -5194.6181, -40593.7883, -10786.4963,   8204.6123,  -5221.4487,
          10332.0252,  -1711.9803]], dtype=torch.float64)
	q_value: tensor([[-21.3092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04410876769988914, distance: 1.1188218320273602 entropy 11.617766855052514
epoch: 64, step: 15
	action: tensor([[-37879.6961,  53730.5073,  15153.3435,   -760.3458,   -494.2296,
           8812.7972,  28818.0872]], dtype=torch.float64)
	q_value: tensor([[-23.0696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46075438402817737, distance: 0.8403302641700225 entropy 11.73296229004424
epoch: 64, step: 16
	action: tensor([[-10339.3722,  -5479.8034,  -2889.4664,  45964.7007, -58975.6945,
         -17539.2243,  34718.7570]], dtype=torch.float64)
	q_value: tensor([[-29.1760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14008864021089118, distance: 1.0611665124601015 entropy 11.75920946631992
epoch: 64, step: 17
	action: tensor([[ 17767.1603, -59663.8426, -37055.3556,   4317.3437,  -7910.0342,
          26040.4100,  44351.9484]], dtype=torch.float64)
	q_value: tensor([[-25.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2509003665871796, distance: 0.9904361550842082 entropy 11.815455128242956
epoch: 64, step: 18
	action: tensor([[-16696.0811, -10281.1984,  33053.4678, -10019.5463, -10476.8541,
          72211.1804,  -3716.7047]], dtype=torch.float64)
	q_value: tensor([[-27.4789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47082417267310306, distance: 1.387832561244952 entropy 11.603400746555335
epoch: 64, step: 19
	action: tensor([[-136326.9545,   23438.3213,  -37340.7327,   40958.6309,    6308.7551,
           54164.2098,   52447.1827]], dtype=torch.float64)
	q_value: tensor([[-28.7728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17004483321182884, distance: 1.042519074321179 entropy 11.964699540843
epoch: 64, step: 20
	action: tensor([[-24731.8801, -42072.8895,  30767.5123, -10074.8334,   6842.8323,
          -3073.0933,   5093.4667]], dtype=torch.float64)
	q_value: tensor([[-24.6834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.650277684229318
epoch: 64, step: 21
	action: tensor([[ 12573.2454,  -3698.5538, -34971.8705, -16069.3323,   -752.5200,
         -32730.4499,  10039.2919]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.285530007871317
epoch: 64, step: 22
	action: tensor([[ -1535.2560, -37336.8124,   4735.8829, -24418.6555,  20384.6196,
          -1959.1099,  16978.4065]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035391042983452525, distance: 1.1644179592197932 entropy 11.285530007871317
epoch: 64, step: 23
	action: tensor([[-22122.0290,  -2318.7721,  22490.1683,   4907.7459,  17547.0504,
          47772.6604, -20307.8358]], dtype=torch.float64)
	q_value: tensor([[-28.6629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6338059996764034, distance: 1.4627056419052953 entropy 11.847453171753637
epoch: 64, step: 24
	action: tensor([[-43835.8216,  31220.8205,  63753.9511,   7302.6863,  -8840.7408,
          10665.4027,  -1253.7817]], dtype=torch.float64)
	q_value: tensor([[-23.0089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20829683484168793, distance: 1.0182112329352404 entropy 11.750425070083681
epoch: 64, step: 25
	action: tensor([[ 40762.1587,  -3080.4070, -11954.0459,  30385.8730,  -9439.0431,
         -38335.5650, -28704.3184]], dtype=torch.float64)
	q_value: tensor([[-28.7073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035594489931721984, distance: 1.164532353508746 entropy 11.898661355923986
epoch: 64, step: 26
	action: tensor([[-52410.2587,  26921.3056,  13818.0968,  23953.3944, -12394.3665,
          35003.7641,   8700.9127]], dtype=torch.float64)
	q_value: tensor([[-32.3631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.91992373731972
epoch: 64, step: 27
	action: tensor([[-22352.4011, -28398.0309, -24104.3289,  38469.3894,   3863.9400,
         -19051.1805, -16322.8259]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.794464131966192, distance: 1.5329362146566992 entropy 11.285530007871317
epoch: 64, step: 28
	action: tensor([[ -8223.3804, -47267.5890,  -8150.9050, -23146.3322,  27380.1651,
         -13978.7596, 102791.4384]], dtype=torch.float64)
	q_value: tensor([[-24.3772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8043231537201578, distance: 1.5371415229438599 entropy 11.78203333897754
epoch: 64, step: 29
	action: tensor([[ -2794.5019, -20818.4112,  49439.3489, -12785.2356, -23485.9418,
          13084.9578, -14902.6328]], dtype=torch.float64)
	q_value: tensor([[-25.8463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8479852422807144, distance: 1.5556286846316074 entropy 11.750486979423467
epoch: 64, step: 30
	action: tensor([[-17596.0673,  -6597.0063,  -2396.9080,  66387.4496,  24185.8254,
         -31350.0853, -34259.9485]], dtype=torch.float64)
	q_value: tensor([[-26.9045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27036758047534293, distance: 1.2897970946556818 entropy 11.808226416891403
epoch: 64, step: 31
	action: tensor([[  9671.1420,  -5316.8119, -18205.0517, -16938.8519,  -1125.1653,
         -10402.3926,  -3630.7264]], dtype=torch.float64)
	q_value: tensor([[-27.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4287971450707627, distance: 0.8648720812764761 entropy 11.957978984984454
epoch: 64, step: 32
	action: tensor([[-30772.2594, -19831.6611,  17504.6641,  43486.8888,  11218.4865,
          12791.0543, -14351.4469]], dtype=torch.float64)
	q_value: tensor([[-26.4288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8558479447617529, distance: 1.558934572154039 entropy 11.752458390580058
epoch: 64, step: 33
	action: tensor([[-59876.2088,   1385.3752,   1870.4468,  15570.1650,  13403.8669,
          -3684.9452,   6490.9501]], dtype=torch.float64)
	q_value: tensor([[-21.9748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31758523961618956, distance: 1.3135483213302235 entropy 11.752841538394936
epoch: 64, step: 34
	action: tensor([[ 15100.1670,  -6083.8355,  44884.4628,  -8169.4405, -35108.0741,
         -21499.3278,  13802.3653]], dtype=torch.float64)
	q_value: tensor([[-24.7980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18232555797204353, distance: 1.2443003753149375 entropy 11.664372716271005
epoch: 64, step: 35
	action: tensor([[ 50685.0515, -14649.7819, -47869.5301,  17346.9606, -10523.5617,
         -53538.1790, -13709.0156]], dtype=torch.float64)
	q_value: tensor([[-27.8484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15000367095586253, distance: 1.2271744556566542 entropy 11.789056547672317
epoch: 64, step: 36
	action: tensor([[  502.9894, -6863.0617,  6391.4902, 23096.2339,  9504.0654,  1549.0438,
         31731.4157]], dtype=torch.float64)
	q_value: tensor([[-31.3365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1485538301663707, distance: 1.2264006456444971 entropy 11.624877588168976
epoch: 64, step: 37
	action: tensor([[ -8437.9124, -19228.1284,  34297.0189, -37247.4991,  -1665.4164,
          -3497.8776,  60192.0646]], dtype=torch.float64)
	q_value: tensor([[-29.6433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5039339953269142, distance: 1.4033664232453438 entropy 11.865102132516071
epoch: 64, step: 38
	action: tensor([[ -3304.9386, -30081.3781,   2513.7271,  -5066.4054, -25783.8123,
         -38035.6466,  -9499.6934]], dtype=torch.float64)
	q_value: tensor([[-23.2201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7282133271854629, distance: 1.5043724387444262 entropy 11.630065362719582
epoch: 64, step: 39
	action: tensor([[-28675.0504, -31498.3942,   4296.2171,  54047.7442,   3545.5091,
         -49913.7834,  58259.5730]], dtype=torch.float64)
	q_value: tensor([[-26.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7937902274494113, distance: 1.532648343300398 entropy 11.748960409154114
epoch: 64, step: 40
	action: tensor([[ 22374.4189, -45774.9721,  39858.4627,  -1898.3986,  20387.2122,
          12304.9967,  52553.7510]], dtype=torch.float64)
	q_value: tensor([[-23.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16725677782193715, distance: 1.0442686652070259 entropy 11.715663729918491
epoch: 64, step: 41
	action: tensor([[-21691.8442,   3742.4169,  67994.6127, -16837.2336, -38714.7753,
          17462.2360,  -6359.0296]], dtype=torch.float64)
	q_value: tensor([[-24.0291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38663283969464324, distance: 0.8962247916550747 entropy 11.669773916882647
epoch: 64, step: 42
	action: tensor([[ -2834.8304,  -7647.4912,   7528.5918,  11277.9988, -11078.1588,
         -21226.2939,  23438.9764]], dtype=torch.float64)
	q_value: tensor([[-35.1605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0490607247640438, distance: 1.6380762279757173 entropy 11.783387740876885
epoch: 64, step: 43
	action: tensor([[  8887.8611,  70527.3667,   2946.8669,  -4732.0270,  -5154.1499,
          31798.4071, -39678.6391]], dtype=torch.float64)
	q_value: tensor([[-23.2993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22621947871704662, distance: 1.006620080601253 entropy 11.696809561469612
epoch: 64, step: 44
	action: tensor([[-17522.2128, -12863.5849, -36174.8737,  25143.4154, -15525.4642,
           -653.1749,  -1336.2899]], dtype=torch.float64)
	q_value: tensor([[-26.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.505268469538596
epoch: 64, step: 45
	action: tensor([[ 10787.6621, -12781.0343,  -9087.2807, -14766.9458,  -4840.9974,
          16109.5722,  19375.3304]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21471461968222405, distance: 1.0140758714541054 entropy 11.285530007871317
epoch: 64, step: 46
	action: tensor([[  -550.4673, -53580.9015,  20332.4055, -21550.2233,  -7021.9268,
          58539.7900,  48997.5878]], dtype=torch.float64)
	q_value: tensor([[-29.1214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28665943853434317, distance: 0.9665073318555726 entropy 11.838940413494528
epoch: 64, step: 47
	action: tensor([[ 39981.5346,   1255.7518,  -1816.7861,   5528.1960, -10327.4037,
         -15398.0146,  35025.4755]], dtype=torch.float64)
	q_value: tensor([[-26.3401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6362626293069265, distance: 0.690161382261214 entropy 11.850501542400782
epoch: 64, step: 48
	action: tensor([[ 21914.4633, -12602.9918, -16648.4289,  17706.1961,  11187.6963,
         -31500.8264, -39766.5695]], dtype=torch.float64)
	q_value: tensor([[-29.3939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.252137190815352, distance: 0.9896181721097432 entropy 11.592553417284048
epoch: 64, step: 49
	action: tensor([[ -8081.2431,  -5892.8078,  -3850.2742,  -2228.5765,  -7179.4829,
         -31613.8221,  -8618.4997]], dtype=torch.float64)
	q_value: tensor([[-31.5897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5557222744588843, distance: 1.42732452529523 entropy 11.853305221823137
epoch: 64, step: 50
	action: tensor([[-34458.5921, -20104.0742,  33788.0192,  -8646.8174,  16260.7966,
         -10337.3699,   9662.7778]], dtype=torch.float64)
	q_value: tensor([[-24.1788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13567837849337328, distance: 1.0638842557934012 entropy 11.734885322011069
epoch: 64, step: 51
	action: tensor([[ 11868.3059, -36448.4484,  18463.4619, -10682.6471,  11666.0548,
         -15018.3934, -22864.5109]], dtype=torch.float64)
	q_value: tensor([[-25.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4622775089632901, distance: 0.8391426485230727 entropy 11.769548094560802
epoch: 64, step: 52
	action: tensor([[ 19653.7524,  21261.9846, -12373.3830,  86305.5206, -22843.0908,
          55692.3056, -39940.3689]], dtype=torch.float64)
	q_value: tensor([[-28.5004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45451702607396915, distance: 0.8451762664636683 entropy 11.825911089773156
epoch: 64, step: 53
	action: tensor([[  2771.4749, -58195.4027, -18939.8800, -18690.2848, -66473.4385,
         -31105.8082,   1336.8096]], dtype=torch.float64)
	q_value: tensor([[-28.3777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09337721763998297, distance: 1.1965799003385051 entropy 11.651445018338308
epoch: 64, step: 54
	action: tensor([[-68049.1195, -14973.2215,  13164.3316,  33232.2522,  46826.4794,
         -19833.7950,  30045.7254]], dtype=torch.float64)
	q_value: tensor([[-29.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2645635561945947, distance: 1.2868473249050525 entropy 11.815903652572251
epoch: 64, step: 55
	action: tensor([[-16883.7715, -15939.5497, -78514.9799, -20416.9348,  15464.8963,
          18402.0715,   -671.5593]], dtype=torch.float64)
	q_value: tensor([[-24.2315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2615554052943395, distance: 0.9833670478630938 entropy 11.719027453737393
epoch: 64, step: 56
	action: tensor([[ 15635.5416, -14623.0996,  -6759.6365,  21528.4352,  22958.7807,
         -16367.9799, -19049.1260]], dtype=torch.float64)
	q_value: tensor([[-24.3149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37692255808502906, distance: 1.342800365486665 entropy 11.67962831621609
epoch: 64, step: 57
	action: tensor([[-13480.3552, -29417.8006, -29885.7266,  36204.0575,   4315.7849,
         -41061.2961,   2672.3439]], dtype=torch.float64)
	q_value: tensor([[-21.2479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3220905671804495, distance: 1.3157921667246597 entropy 11.321912586176905
epoch: 64, step: 58
	action: tensor([[-45046.7014, -14411.9197, -22387.2974,  62131.8272,  45293.3132,
          29379.4617, -21043.4681]], dtype=torch.float64)
	q_value: tensor([[-26.9331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6049729697815249, distance: 1.4497414448118582 entropy 11.880076898958643
epoch: 64, step: 59
	action: tensor([[-21563.1173, -59246.7021, -65647.7868,   9252.4244,  11994.5008,
          17118.1249, -17902.9452]], dtype=torch.float64)
	q_value: tensor([[-23.9482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14710559055765804, distance: 1.05682803893683 entropy 11.805877839404697
epoch: 64, step: 60
	action: tensor([[-13798.3429, -34325.1644,   7890.9248,   7392.1250,  -3595.1567,
           8369.5652, -16417.7830]], dtype=torch.float64)
	q_value: tensor([[-23.2136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1549673150928783, distance: 1.229819968372486 entropy 11.75437138676629
epoch: 64, step: 61
	action: tensor([[ -6093.8184,  29317.7397,   -946.2010,  42338.2647, -37930.4727,
          51279.4268,  -3713.3941]], dtype=torch.float64)
	q_value: tensor([[-22.6427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2256808311324533, distance: 1.0069703861019186 entropy 11.732342236324344
epoch: 64, step: 62
	action: tensor([[   867.7684, -54878.3223,  -6861.3132,   5859.9315,  -2412.4747,
          -8076.8146,  31079.5052]], dtype=torch.float64)
	q_value: tensor([[-27.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19550453995863692, distance: 1.0264043692029854 entropy 11.790690015454642
epoch: 64, step: 63
	action: tensor([[ 16143.9360, -38566.2913, -23628.3365, -21349.7908, -14723.2043,
           7684.5088,  -3044.6144]], dtype=torch.float64)
	q_value: tensor([[-25.0350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3403894294808447, distance: 1.3248667182742295 entropy 11.599913488137167
epoch: 64, step: 64
	action: tensor([[ -3454.5966, -16965.9900, -16243.0102,  63302.5039,  28894.3161,
          10427.7715,  10704.7126]], dtype=torch.float64)
	q_value: tensor([[-25.5602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5639416144799352, distance: 1.4310900466617043 entropy 11.745066408123389
epoch: 64, step: 65
	action: tensor([[ 11861.6965,  45046.4348, -12988.0919, -58085.4220,  -8248.7671,
         -37346.7584,  34585.0035]], dtype=torch.float64)
	q_value: tensor([[-23.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.853364295811641
epoch: 64, step: 66
	action: tensor([[-22063.0986,  11027.6713,   5956.6960,    915.0591,   4463.8233,
          -5244.4686,   2177.6041]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15372376609476857, distance: 1.052719737485805 entropy 11.285530007871317
epoch: 64, step: 67
	action: tensor([[-33984.8240, -17556.6991,  27097.4682, -49329.4024,   6400.5909,
          36050.7162, -53881.3061]], dtype=torch.float64)
	q_value: tensor([[-26.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020613500639440563, distance: 1.1560785616762168 entropy 11.670588939659382
epoch: 64, step: 68
	action: tensor([[  8093.4698, -44306.9132,  39560.7059,  -4598.7442,  34401.1350,
          12776.8565, -42930.8616]], dtype=torch.float64)
	q_value: tensor([[-23.9102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4316585676783311, distance: 0.8627030868416544 entropy 11.791233903788468
epoch: 64, step: 69
	action: tensor([[  1643.5451, -43636.0687,  12400.2353,  20120.3904,   9998.4811,
          52839.3627,  -5228.0876]], dtype=torch.float64)
	q_value: tensor([[-27.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16339755830983993, distance: 1.0466856189058966 entropy 11.871607533247657
epoch: 64, step: 70
	action: tensor([[-23796.8151, -24682.9699, -13516.6686,  27163.3700,   5801.3368,
          10968.7595,  60305.5388]], dtype=torch.float64)
	q_value: tensor([[-27.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3490082613823495, distance: 1.329119402630822 entropy 11.745179574546045
epoch: 64, step: 71
	action: tensor([[-48626.6742, -33903.3237,  28555.4692,  15516.8423,  -5965.3896,
          29767.3245,  66259.1630]], dtype=torch.float64)
	q_value: tensor([[-24.1813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5808395599895466, distance: 1.4388005362076073 entropy 11.76132403564883
epoch: 64, step: 72
	action: tensor([[-1.2846e+05, -6.4489e+04, -5.5662e+04,  1.2465e+02, -7.6003e+04,
          2.1495e+03,  2.3539e+03]], dtype=torch.float64)
	q_value: tensor([[-26.3587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3988194792409432, distance: 1.3534353920047373 entropy 11.971913940855373
epoch: 64, step: 73
	action: tensor([[ -2207.2017, -80313.5544,  30824.5824,  27543.2750, -36656.1667,
            169.0713,  -5355.5234]], dtype=torch.float64)
	q_value: tensor([[-24.3626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15511009494844819, distance: 1.051857126753889 entropy 11.9010574415497
epoch: 64, step: 74
	action: tensor([[-17313.3359, -71719.2686, -42876.4378,  -8082.4461,   3745.3149,
          32973.0717, -25905.4791]], dtype=torch.float64)
	q_value: tensor([[-23.5906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7108693709698344, distance: 1.4968046337096927 entropy 11.808522644753953
epoch: 64, step: 75
	action: tensor([[-34420.1811, -46091.8647, -19639.9948,  49885.5998, -19996.5653,
         -50113.3167,  19696.7457]], dtype=torch.float64)
	q_value: tensor([[-20.0218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015251592434013972, distance: 1.153037768033151 entropy 11.443785618256499
epoch: 64, step: 76
	action: tensor([[-34607.1122, -24444.9569,    610.7571,  -6587.2928,   3720.5576,
          14606.7131,  28798.9540]], dtype=torch.float64)
	q_value: tensor([[-26.4442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5675838533997, distance: 1.4327554990652476 entropy 11.81921996523458
epoch: 64, step: 77
	action: tensor([[-23882.4511,   9988.4737, -21518.5854,  79061.1475,  -3318.2985,
         -52497.3565, -51947.8608]], dtype=torch.float64)
	q_value: tensor([[-21.6002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.565645051900722
epoch: 64, step: 78
	action: tensor([[-17732.0772, -22751.7900,  -1852.0815,   7297.8028, -11819.2362,
          -8695.9050,   5111.2635]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.285530007871317
epoch: 64, step: 79
	action: tensor([[   277.9293,  17587.2425,   3860.9823, -48814.8897,   8042.6612,
         -27942.1364,   4284.9582]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.285530007871317
epoch: 64, step: 80
	action: tensor([[-44185.8226, -11261.3575, -10194.7552,  31170.4256, -19382.8385,
           7591.0912,  -9260.2797]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09411196523194043, distance: 1.1969818826247733 entropy 11.285530007871317
epoch: 64, step: 81
	action: tensor([[ -8930.7162, -52392.7354, -11267.7219, -32689.7557,   5955.9614,
           -534.9389,  56332.5624]], dtype=torch.float64)
	q_value: tensor([[-26.2565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4323538408320029, distance: 1.3695624863444698 entropy 11.84732627313576
epoch: 64, step: 82
	action: tensor([[-83750.8235,   9998.6664,  10484.7631,  41575.0878,   5399.3650,
         -36814.5720, -74962.6293]], dtype=torch.float64)
	q_value: tensor([[-26.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06128429590274975, distance: 1.1788880413241476 entropy 11.85470839447844
epoch: 64, step: 83
	action: tensor([[-32357.3241,  27602.3581,    451.4916,  19170.2574, -27851.0211,
           -426.0142,  37514.0553]], dtype=torch.float64)
	q_value: tensor([[-28.6021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05155653436759078, distance: 1.1734727431865333 entropy 11.867622517346726
epoch: 64, step: 84
	action: tensor([[  2760.3855, -54112.4599,  -2958.8479, -20647.9087,  39693.1042,
          48953.1465,  75185.8312]], dtype=torch.float64)
	q_value: tensor([[-29.6334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.865325664390568
epoch: 64, step: 85
	action: tensor([[-21755.5914,  -8374.1728,  25175.2866,  13586.9033,  14427.1827,
           1496.7147, -11115.0020]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1847522360983318, distance: 1.2455766603069611 entropy 11.285530007871317
epoch: 64, step: 86
	action: tensor([[  8747.6285, -51241.4368, -16095.7567,  10059.6671,  31713.4156,
         -51563.4235,  33925.1786]], dtype=torch.float64)
	q_value: tensor([[-23.4536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2605553198571715, distance: 0.9840327161617349 entropy 11.735666609282333
epoch: 64, step: 87
	action: tensor([[-59166.6099, -23182.2068, -35617.8592, -23081.8875,  45168.7404,
         -10474.2863, -76645.0981]], dtype=torch.float64)
	q_value: tensor([[-27.6250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44880190383531593, distance: 1.3774035478038715 entropy 11.903520948935576
epoch: 64, step: 88
	action: tensor([[ 14714.9243,   3517.1230,   6172.2323,  17886.2506,  15506.4528,
          35228.3336, -10713.1047]], dtype=torch.float64)
	q_value: tensor([[-25.1163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.750633406530905
epoch: 64, step: 89
	action: tensor([[-38446.4707, -15811.6001,  -7218.8177,  -4974.4859,   3076.0977,
          -2173.8973,   4177.2210]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1846887321489583, distance: 1.2455432777588036 entropy 11.285530007871317
epoch: 64, step: 90
	action: tensor([[-51675.4497,  14108.5554,  37924.3052,   3576.7385,  -5937.5701,
          -1253.3800,  -5351.9260]], dtype=torch.float64)
	q_value: tensor([[-27.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33642471381219086, distance: 0.9321843227339176 entropy 11.816063929255426
epoch: 64, step: 91
	action: tensor([[ -6659.5048, -24591.6632, -24540.5421,  24774.4133,  -5256.1440,
         -39399.3311,    635.4013]], dtype=torch.float64)
	q_value: tensor([[-26.9203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09409936261337903, distance: 1.1969749888378436 entropy 11.47597101749227
epoch: 64, step: 92
	action: tensor([[-29558.5802, -57178.2790,  14100.9065,  45089.6398, -20373.9200,
          -9280.0635, -39531.3305]], dtype=torch.float64)
	q_value: tensor([[-25.8103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11011272825869622, distance: 1.2057027025267162 entropy 11.799743110752596
epoch: 64, step: 93
	action: tensor([[  7035.6612, -16777.2417,  37516.6573,   2751.2179,  33819.7390,
          -3334.2989,   3716.9961]], dtype=torch.float64)
	q_value: tensor([[-28.8355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41572284619621214, distance: 0.8747141373874963 entropy 11.912519977251028
epoch: 64, step: 94
	action: tensor([[ 29911.7791, -17442.3740, -26748.7465, -50080.9384, -18873.1316,
          -9943.6674, -59300.4983]], dtype=torch.float64)
	q_value: tensor([[-29.2434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3400651959145722, distance: 1.324706469233136 entropy 11.907770510769936
epoch: 64, step: 95
	action: tensor([[ 38939.9843, -38845.6520, -29293.1874,  -1869.0032,  15560.7044,
          34113.0462,  17962.9542]], dtype=torch.float64)
	q_value: tensor([[-27.0143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37017006094693816, distance: 0.9081724931393416 entropy 11.55055304776179
epoch: 64, step: 96
	action: tensor([[-61907.8211,  27067.8375,  29845.0856,  10652.1078, -27313.3810,
         -27240.4934,  27505.6257]], dtype=torch.float64)
	q_value: tensor([[-26.7554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.72873375553571
epoch: 64, step: 97
	action: tensor([[   607.0521, -22145.5810,  11015.7673,  15946.9089,  -8699.9696,
          30821.2535, -30675.7145]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34473264559162486, distance: 1.3270114415231173 entropy 11.285530007871317
epoch: 64, step: 98
	action: tensor([[ 37373.5122,  -2824.4486,  -3158.0218,  51473.8058, -37053.5165,
           8468.4974, -30955.6612]], dtype=torch.float64)
	q_value: tensor([[-27.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22456989321207654, distance: 1.0076924918355907 entropy 11.594739446590541
epoch: 64, step: 99
	action: tensor([[-40136.4261,  12011.3602, -28323.4350,  36670.8917,  36095.6974,
         -40066.5662, -43630.8769]], dtype=torch.float64)
	q_value: tensor([[-30.1824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.716508213951538
epoch: 64, step: 100
	action: tensor([[-14151.4482,  -6586.9954,  -6483.9934,  -2805.5371, -44447.7481,
          16219.9720,   2646.9717]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39462438343546413, distance: 1.351404374215704 entropy 11.285530007871317
epoch: 64, step: 101
	action: tensor([[-27673.1760,   8175.3494,  14432.8143, -15455.7608,  28860.7318,
           1091.3137, -26732.1275]], dtype=torch.float64)
	q_value: tensor([[-22.2550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19258115096395667, distance: 1.028267560755142 entropy 11.573702134649144
epoch: 64, step: 102
	action: tensor([[ 38018.3839,  -6416.3306, -22339.1510,  46567.9505, -36735.2097,
         -12710.5746,  66001.7137]], dtype=torch.float64)
	q_value: tensor([[-31.8866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2019938109474405, distance: 1.022256364371667 entropy 11.781461235051127
epoch: 64, step: 103
	action: tensor([[-16925.1201, -62145.1814,  -5279.7399,  13471.1848,  -4483.1833,
           3653.6768,  19680.1984]], dtype=torch.float64)
	q_value: tensor([[-28.9517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7948151623136055, distance: 1.5330861426516524 entropy 11.779526641607657
epoch: 64, step: 104
	action: tensor([[-79878.6479, -50627.3737, -10846.8752,  46192.2776,  65304.5632,
         -17586.2207,  53010.1384]], dtype=torch.float64)
	q_value: tensor([[-26.5723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08825484736777633, distance: 1.0926810838804888 entropy 11.944740422358029
epoch: 64, step: 105
	action: tensor([[-25296.7904,  -6246.8253,   9463.7452,  52223.5407,  24423.7075,
          -5516.7962,  13714.7156]], dtype=torch.float64)
	q_value: tensor([[-26.2553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1254270109235054, distance: 1.2139907018077851 entropy 11.968162949885306
epoch: 64, step: 106
	action: tensor([[-13413.2396, -45732.8230,  -7133.4439,  -2098.7393,    423.7265,
          21643.2923,  -7068.5656]], dtype=torch.float64)
	q_value: tensor([[-27.2393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4718064340404964, distance: 1.3882959024150114 entropy 11.806542028586673
epoch: 64, step: 107
	action: tensor([[-10530.2414,  34392.2791, -13126.3114, -14228.0648,   -931.7689,
         -32742.0198, -23036.8894]], dtype=torch.float64)
	q_value: tensor([[-24.6398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5722604037288744, distance: 0.7484217857588692 entropy 11.821105085061424
epoch: 64, step: 108
	action: tensor([[ -2857.8498,  -9368.3578, -32950.5211,  29992.2322,  16652.8900,
          25257.3029, -26737.6155]], dtype=torch.float64)
	q_value: tensor([[-35.4072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.769980882718913
epoch: 64, step: 109
	action: tensor([[  4207.9142,   9978.4138, -35892.6330,  34921.8605, -12866.3792,
           1904.0839,   4667.5630]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4691878912940406, distance: 0.8337332155053683 entropy 11.285530007871317
epoch: 64, step: 110
	action: tensor([[-17532.0651, -49184.2868, -48473.7197,  46586.4041,  21483.8912,
          -2934.8072,  -3816.3957]], dtype=torch.float64)
	q_value: tensor([[-25.3200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3566497487373347, distance: 0.9178684260468415 entropy 11.556610485277577
epoch: 64, step: 111
	action: tensor([[-48588.5536, -75010.1326,  61030.3306,  74357.9567,  21370.5471,
          35102.8057, -16560.9948]], dtype=torch.float64)
	q_value: tensor([[-30.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10244407878475748, distance: 1.0841452002513676 entropy 11.917414236710883
epoch: 64, step: 112
	action: tensor([[ -8477.0036, -85855.1834,  37314.7342,  58603.0590,  -1121.2764,
          62869.2614,  19905.2005]], dtype=torch.float64)
	q_value: tensor([[-25.1126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8542165678404279, distance: 1.558249233402051 entropy 11.800274563964226
epoch: 64, step: 113
	action: tensor([[-51688.9194, -52253.5909,   6580.2150,  10288.0029,   9173.9453,
          -2671.1367,  11417.4688]], dtype=torch.float64)
	q_value: tensor([[-24.9920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4872544175530322, distance: 1.3955626156845247 entropy 11.887596456686337
epoch: 64, step: 114
	action: tensor([[  6729.8083,  36572.5948,  36663.4194, -51054.6127, -10348.5419,
           4831.4999,  14592.4842]], dtype=torch.float64)
	q_value: tensor([[-24.4463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.69163996150456
epoch: 64, step: 115
	action: tensor([[ -6693.0565, -11691.5610,  -3622.2781,  11539.7644,  25534.2456,
         -16646.6424,  29080.0351]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6195901268169064, distance: 1.4563281812205429 entropy 11.285530007871317
epoch: 64, step: 116
	action: tensor([[-18910.9537, -19888.9649,  -8236.4274,  41662.0552,  -2188.0765,
           7284.9976,  21783.0939]], dtype=torch.float64)
	q_value: tensor([[-25.7044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07975625413108323, distance: 1.189103226127257 entropy 11.883494031372281
epoch: 64, step: 117
	action: tensor([[ 20412.2690, -43694.6700, -51249.4982,  67781.3416,   3073.0694,
         -35029.0308,  25863.3305]], dtype=torch.float64)
	q_value: tensor([[-23.5084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0673979923106518, distance: 1.1822787508688306 entropy 11.799176609514083
epoch: 64, step: 118
	action: tensor([[-13177.1056, -30518.8508,  19683.6855, -30337.9052,  -5827.3980,
          16882.6442,   5373.3515]], dtype=torch.float64)
	q_value: tensor([[-27.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31862786346064986, distance: 1.3140679332434344 entropy 11.633305209036308
epoch: 64, step: 119
	action: tensor([[-33224.2738,  22947.4876,  36578.8555,   6644.0516,  18993.0571,
          48062.3537,  35220.0072]], dtype=torch.float64)
	q_value: tensor([[-26.7038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07277486488409068, distance: 1.1852527946425955 entropy 11.92967303025026
epoch: 64, step: 120
	action: tensor([[-45990.1382, -52520.0426, -16357.3636,  18212.2021,  -4389.1371,
          -7233.2822,  37951.3886]], dtype=torch.float64)
	q_value: tensor([[-28.3569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917384062887862, distance: 0.9630604308768095 entropy 11.8724239390737
epoch: 64, step: 121
	action: tensor([[ 15983.9907,  -7297.2928,  16810.5130,  24347.3848,  38597.1002,
         -11156.4929, -15505.4656]], dtype=torch.float64)
	q_value: tensor([[-24.7536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2562596990676316, distance: 1.2826152733033547 entropy 11.90768663836217
epoch: 64, step: 122
	action: tensor([[ -42521.4816, -105029.5166,    8291.4447,   91926.7561,  -36224.7366,
           26018.8652,  -53673.9668]], dtype=torch.float64)
	q_value: tensor([[-29.5314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.169222821320411, distance: 1.2373863902959916 entropy 11.796805584910592
epoch: 64, step: 123
	action: tensor([[ 10046.7387, -92197.0860,  28481.8970, -46709.7761, -71005.9235,
          -3781.3789, -14382.1705]], dtype=torch.float64)
	q_value: tensor([[-26.1668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4878500790765783, distance: 1.3958420566887861 entropy 12.063974183068222
epoch: 64, step: 124
	action: tensor([[ -6513.2278,  16440.5561, -30604.3677,   7105.2225,  13923.7217,
          -2483.6366,  11923.8858]], dtype=torch.float64)
	q_value: tensor([[-20.5211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.334234370598425
epoch: 64, step: 125
	action: tensor([[ 29068.9998, -11785.4200,   -341.7039,  27488.2057,   8321.8117,
          -3251.8116,  -4179.6710]], dtype=torch.float64)
	q_value: tensor([[-26.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15313654884603167, distance: 1.05308490666163 entropy 11.285530007871317
epoch: 64, step: 126
	action: tensor([[  5277.8157,  -4959.9684, -27079.7483,  31971.2967,  34667.7578,
          75792.8156, -39984.3372]], dtype=torch.float64)
	q_value: tensor([[-31.3761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3547516565549653, distance: 1.3319457567627544 entropy 11.853426893979412
epoch: 64, step: 127
	action: tensor([[-77433.7035,  -2254.8949, -55522.7143,  -5986.6549,  -6970.0699,
           1907.5741,  70485.6464]], dtype=torch.float64)
	q_value: tensor([[-30.8780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5564494646302345, distance: 1.4276580729730617 entropy 11.84703116054545
LOSS epoch 64 actor 281.494157232614 critic 321.6557611861366
epoch: 65, step: 0
	action: tensor([[-31010.1006, -20858.7864,   -105.9352,  -1158.9529, -52002.6460,
          12789.9948,  -1020.6944]], dtype=torch.float64)
	q_value: tensor([[-25.0216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.64203859176113
epoch: 65, step: 1
	action: tensor([[-31475.3399, -24005.1917,  27820.4574,   -306.3664,  -9929.1917,
           6491.0567, -13458.2154]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1184253546086732, distance: 1.0744500707967588 entropy 11.322253775862496
epoch: 65, step: 2
	action: tensor([[-32082.6993, -50082.4812, -22590.6726,  14355.0404, -14494.0231,
          13985.7019,   9213.3093]], dtype=torch.float64)
	q_value: tensor([[-30.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5950668318392331, distance: 1.4452605072042797 entropy 11.76727122947985
epoch: 65, step: 3
	action: tensor([[  1549.1386, -11280.4598,  -9802.1727, -52737.8762,  -4605.6402,
          65562.1919, -27843.4528]], dtype=torch.float64)
	q_value: tensor([[-28.0000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06196457253713117, distance: 1.1792658107488383 entropy 11.812732338950434
epoch: 65, step: 4
	action: tensor([[-2.6032e+04,  5.3151e+04,  4.6576e+00, -1.7917e+03,  1.4461e+04,
         -4.1630e+04,  1.1142e+04]], dtype=torch.float64)
	q_value: tensor([[-28.4846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.631948852803593
epoch: 65, step: 5
	action: tensor([[-6558.3688, -7374.2574, 15025.4777, 16510.4666, -5330.5256, 32596.1875,
         21715.4627]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27238319436879577, distance: 1.2908199098597015 entropy 11.322253775862496
epoch: 65, step: 6
	action: tensor([[-19051.7898, -39343.8280, -32205.0465,  11185.3887, -41707.9255,
         -12130.8424,   1560.4738]], dtype=torch.float64)
	q_value: tensor([[-30.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2657423850812377, distance: 1.2874469861182911 entropy 11.920995320319694
epoch: 65, step: 7
	action: tensor([[ 14931.9087, -31085.8306,   -243.9387,  11487.8672,  36628.0389,
          53186.1508,   1943.0031]], dtype=torch.float64)
	q_value: tensor([[-32.2966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3786257271814881, distance: 0.9020556420507205 entropy 11.904409381109767
epoch: 65, step: 8
	action: tensor([[  4633.6048,  16630.2522, -44910.2988,  27683.4813,   2466.8376,
          43741.5913,  78600.4754]], dtype=torch.float64)
	q_value: tensor([[-31.0132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22319361294072326, distance: 1.0085863521948082 entropy 11.793958955111865
epoch: 65, step: 9
	action: tensor([[-34149.2134,  28369.6584,   8759.1669,  56943.7985,  28540.3088,
           2854.7972,   9992.2576]], dtype=torch.float64)
	q_value: tensor([[-34.5958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7463735610291253, distance: 0.5763070804306927 entropy 11.93631190719205
epoch: 65, step: 10
	action: tensor([[-66305.4791, -60925.2961,  -7009.3033,   3631.4878, -26070.6806,
          13446.0566,   8095.2690]], dtype=torch.float64)
	q_value: tensor([[-30.5579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14830083786872517, distance: 1.2262655683861572 entropy 11.747022594001525
epoch: 65, step: 11
	action: tensor([[ 15788.3461,  39891.4640, -17445.8472,  60702.0179,  95307.3308,
         -12122.0715,  31525.8295]], dtype=torch.float64)
	q_value: tensor([[-31.6857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5987905668764921, distance: 0.7248401824475492 entropy 11.970000003872451
epoch: 65, step: 12
	action: tensor([[-13807.5342, -43178.0069,   1889.5718,  62363.4974,  22176.7992,
          -2012.6408, -44435.5792]], dtype=torch.float64)
	q_value: tensor([[-35.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05768867920142828, distance: 1.1108461076552034 entropy 11.95898154538936
epoch: 65, step: 13
	action: tensor([[-19475.2298,  -3932.5266,  26869.9204,  69291.2513, -29804.2664,
          51706.7260, -28534.2546]], dtype=torch.float64)
	q_value: tensor([[-32.9441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3313791383459195, distance: 1.3204062370194785 entropy 12.044875368680128
epoch: 65, step: 14
	action: tensor([[-11013.7901, -21508.0046, -17185.7699,  73375.8477, -39344.7605,
          16454.5306,   7105.2156]], dtype=torch.float64)
	q_value: tensor([[-34.6312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0828427622285477, distance: 1.1908015527833806 entropy 12.128239831476094
epoch: 65, step: 15
	action: tensor([[  5829.6232,  12153.8355, -54375.1339, -59566.7032,  10790.8251,
          -3202.4989,  23796.5352]], dtype=torch.float64)
	q_value: tensor([[-28.5373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.81771148672379
epoch: 65, step: 16
	action: tensor([[ 20190.8148, -35815.8205, -13427.0379,  16965.1143,   -799.0467,
          -9928.4411, -36925.8026]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38795822073056396, distance: 0.8952559741236765 entropy 11.322253775862496
epoch: 65, step: 17
	action: tensor([[  1627.4070,  32215.1869, -31860.3761,  24316.7134,   6312.2854,
           -388.2005, -12177.8480]], dtype=torch.float64)
	q_value: tensor([[-28.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6152536548964016, distance: 0.7098129929590946 entropy 11.568615204169143
epoch: 65, step: 18
	action: tensor([[ 20743.4213, -27969.4884, -21859.1967,  22595.0041,  27752.0510,
         -18109.8547, -31064.9984]], dtype=torch.float64)
	q_value: tensor([[-32.5030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07498999428609732, distance: 1.1006010137685287 entropy 11.755408006729123
epoch: 65, step: 19
	action: tensor([[-35380.3116,   3693.7312,   3590.4717,  24451.7883, -35354.9326,
           1511.6662,  -1747.5309]], dtype=torch.float64)
	q_value: tensor([[-33.3488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39389031816140074, distance: 0.890906861912418 entropy 11.638641050812756
epoch: 65, step: 20
	action: tensor([[ 24309.2598,   4609.4132, -12523.9202,  42142.4727, -43904.5705,
          48396.8344,  -2003.8130]], dtype=torch.float64)
	q_value: tensor([[-26.6200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3222123960469052, distance: 0.9421141011124714 entropy 11.522413634638118
epoch: 65, step: 21
	action: tensor([[-5.7804e+03, -2.0773e+04, -1.0898e+04, -2.6140e+01,  1.7279e+04,
         -8.1551e+03,  5.0188e+04]], dtype=torch.float64)
	q_value: tensor([[-33.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.66749822735554
epoch: 65, step: 22
	action: tensor([[-18702.2211,   9946.3234,  24341.0707,  20454.3911,   2682.8539,
          -3043.6756,  -4253.5394]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5830471207054398, distance: 0.7389246965012628 entropy 11.322253775862496
epoch: 65, step: 23
	action: tensor([[-12356.2227, -38929.4842, -42674.0343,  -2539.6419, -28864.9132,
          -7414.4292,    508.6216]], dtype=torch.float64)
	q_value: tensor([[-28.8850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34570115145549085, distance: 1.3274892268432876 entropy 11.574381325425106
epoch: 65, step: 24
	action: tensor([[-32749.4122,  25617.8559,  -9427.9334,  24073.7204,  57517.1117,
         -48267.6872,   5493.7720]], dtype=torch.float64)
	q_value: tensor([[-25.7202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4768533150826957, distance: 0.8276913796614094 entropy 11.53010398907969
epoch: 65, step: 25
	action: tensor([[-40013.2241,  23367.2324,    799.8509,  56377.2348, -51992.4957,
         -44358.6077,  55908.1990]], dtype=torch.float64)
	q_value: tensor([[-32.2238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5914018998430515, distance: 0.7314840567037592 entropy 11.780822328645504
epoch: 65, step: 26
	action: tensor([[ -3771.1342,  -3445.8415, -28743.4113,   7668.7425, -56281.0580,
          11405.0602,  -5576.7305]], dtype=torch.float64)
	q_value: tensor([[-29.9842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.69451639813208
epoch: 65, step: 27
	action: tensor([[  4427.4440, -30110.1604,  10053.2480, -11845.2563, -14659.0817,
          -8279.6911,  -1476.2659]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.322253775862496
epoch: 65, step: 28
	action: tensor([[-23317.3745,   8655.6047,  11244.9505,  47346.8664, -31996.6904,
         -12297.6376,   7568.8111]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23846390063438194, distance: 0.9986238656764886 entropy 11.322253775862496
epoch: 65, step: 29
	action: tensor([[-78644.1024,  28301.2899,  43105.6526,   4187.3223,  20078.6451,
         -38933.3286, -14968.2612]], dtype=torch.float64)
	q_value: tensor([[-36.3184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09549096263573897, distance: 1.0883363846313099 entropy 11.966288824765162
epoch: 65, step: 30
	action: tensor([[ 10968.0810, -40837.8987, -61670.9003,    294.0121, -75489.4501,
          -7053.2729,  65012.2409]], dtype=torch.float64)
	q_value: tensor([[-33.5446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45125642425385215, distance: 0.8476985059482149 entropy 11.993066371533407
epoch: 65, step: 31
	action: tensor([[-30913.5612, -50422.2697,  20956.3610,  -3963.1644, -11653.6660,
          -5810.4795,  40297.9906]], dtype=torch.float64)
	q_value: tensor([[-33.4449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6017331880697863, distance: 1.448277489170723 entropy 11.890825409096546
epoch: 65, step: 32
	action: tensor([[ 15795.8969, -16517.0287,   5745.3320,  -2130.5309,  -8240.0653,
         -17213.7767,   3549.2148]], dtype=torch.float64)
	q_value: tensor([[-29.1606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2325201779009839, distance: 1.0025133771228247 entropy 11.742123025707716
epoch: 65, step: 33
	action: tensor([[41460.3251, -2145.9237, -5406.7249, 13546.3283, -6299.0092, -7301.3413,
         29448.3171]], dtype=torch.float64)
	q_value: tensor([[-33.0294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21986771804482608, distance: 1.2639010147411758 entropy 11.840932807640419
epoch: 65, step: 34
	action: tensor([[-19932.4228, -80816.4507, -33688.6697, -48012.8064, -23875.1424,
         -61605.7518,  -5081.9037]], dtype=torch.float64)
	q_value: tensor([[-41.4031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02059015739053771, distance: 1.1325018636177286 entropy 11.839371907118075
epoch: 65, step: 35
	action: tensor([[ 31576.2526,   7687.2647,  52811.0850,  71027.0260,  13702.8046,
          29610.7589, -21753.0252]], dtype=torch.float64)
	q_value: tensor([[-27.4923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.657775825554229
epoch: 65, step: 36
	action: tensor([[-10826.4809, -35333.5176,  -9776.1367, -18355.8472, -35570.0680,
           9559.6704,   4218.6564]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26362126609650627, distance: 1.2863677881275013 entropy 11.322253775862496
epoch: 65, step: 37
	action: tensor([[-67700.3879,  38586.3767, -38106.5603, -23001.6249,  23393.6184,
         -13087.8919,  -3616.3511]], dtype=torch.float64)
	q_value: tensor([[-28.9824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18878245047455833, distance: 1.2476934231741421 entropy 11.761413920748518
epoch: 65, step: 38
	action: tensor([[  12410.3941, -114241.4737,  -16452.1604,   22868.5426,   42053.4307,
          -14014.5336,  -13368.0411]], dtype=torch.float64)
	q_value: tensor([[-37.2937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015806377790368642, distance: 1.1352642618361795 entropy 11.820940207091365
epoch: 65, step: 39
	action: tensor([[ 12464.4025, -32842.6959,   -388.8755, -14768.9576,  20801.1213,
          33176.3149,  38156.5073]], dtype=torch.float64)
	q_value: tensor([[-32.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5549545215642764, distance: 0.7634118374001226 entropy 11.75933548394555
epoch: 65, step: 40
	action: tensor([[ 38963.6914, -21863.2082,  51616.8820, -37185.4793,   4682.6580,
         -50731.1196,  -1711.1588]], dtype=torch.float64)
	q_value: tensor([[-32.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38100691477490845, distance: 0.9003255853285327 entropy 11.81945743494914
epoch: 65, step: 41
	action: tensor([[-51266.8760, -34094.5354, -35975.6705,   5972.9545, -32717.1534,
          49837.1095, -45663.6717]], dtype=torch.float64)
	q_value: tensor([[-34.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18255395680561382, distance: 1.034632797476843 entropy 11.704623341907984
epoch: 65, step: 42
	action: tensor([[-17963.2929,    338.2505, -61884.3953,  23751.5141,   2166.8095,
          41334.1058,  25267.6960]], dtype=torch.float64)
	q_value: tensor([[-25.7515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14053455847749574, distance: 1.2221117447800078 entropy 11.67230663412237
epoch: 65, step: 43
	action: tensor([[-16022.0457, -40715.5582,  25345.1196,  47852.9822,  49668.2681,
          -1484.1341,  37100.7800]], dtype=torch.float64)
	q_value: tensor([[-30.1390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9913748294073699, distance: 1.6148537634720788 entropy 11.8451376707953
epoch: 65, step: 44
	action: tensor([[ 20748.9206, -24465.7706,  19343.3407,   -676.2193,  -1317.6780,
          16519.0921, -37655.6065]], dtype=torch.float64)
	q_value: tensor([[-28.3249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28653938359130193, distance: 0.9665886598405736 entropy 11.807781835501098
epoch: 65, step: 45
	action: tensor([[ 21560.0956,   4890.3232, -30894.8759, -19729.5124, -17936.8606,
          26607.0515,   8671.8838]], dtype=torch.float64)
	q_value: tensor([[-29.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14927081470270553, distance: 1.0554857134959943 entropy 11.684262881059093
epoch: 65, step: 46
	action: tensor([[-1077.6167,   -66.9685,  2740.2443,  5396.3339, -3595.2424,  9726.1007,
         15396.1532]], dtype=torch.float64)
	q_value: tensor([[-33.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.597493151753735
epoch: 65, step: 47
	action: tensor([[-14750.5621, -12803.7184,   1207.5266,  30957.7118, -12203.3204,
          19472.2884,  16148.1184]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41948865494437215, distance: 1.3633980113141329 entropy 11.322253775862496
epoch: 65, step: 48
	action: tensor([[-46745.3527,  38989.8462,  46277.7160, -52622.8151, -22055.4474,
           5604.9181,  42305.3802]], dtype=torch.float64)
	q_value: tensor([[-26.6928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24010384416104902, distance: 0.9975480339644295 entropy 11.737112284410827
epoch: 65, step: 49
	action: tensor([[ -5986.3185, -47324.5233, -64773.7421,   7460.8974,  18826.9798,
          25147.1386,   2318.5021]], dtype=torch.float64)
	q_value: tensor([[-31.2310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7897277103874325, distance: 1.530911813775048 entropy 11.532005627687582
epoch: 65, step: 50
	action: tensor([[ 46905.8167, -34882.3747,  12431.1998,  27794.1762,  15222.5207,
          15139.4641,   3917.5003]], dtype=torch.float64)
	q_value: tensor([[-28.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1356666706675136, distance: 1.2195009232978364 entropy 11.79334257033925
epoch: 65, step: 51
	action: tensor([[  1419.1785, -32543.4037,  -9411.5343,  18685.3187,  30107.3947,
           1640.4303, -12920.8452]], dtype=torch.float64)
	q_value: tensor([[-29.7177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5198479527374724, distance: 0.7929505154187416 entropy 11.855134936645472
epoch: 65, step: 52
	action: tensor([[ 24874.2692, -29318.7457,   7999.5523,   1773.5554, -15130.6703,
          11796.0450, -72538.7644]], dtype=torch.float64)
	q_value: tensor([[-35.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5159435622677904, distance: 0.7961679546658169 entropy 11.837000137971021
epoch: 65, step: 53
	action: tensor([[ 11134.4737, -31018.6001, -82476.8730,  61878.8965,  24068.9822,
          79413.8121,  22761.5588]], dtype=torch.float64)
	q_value: tensor([[-34.1114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1117318139780401, distance: 1.2065816334248038 entropy 11.617726698040439
epoch: 65, step: 54
	action: tensor([[  1291.9647,  13556.1465,  33370.7241,  34232.3270, -33234.7790,
          -3854.5672, -41612.0364]], dtype=torch.float64)
	q_value: tensor([[-39.8090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43753321536671597, distance: 0.8582328502498813 entropy 11.917196040255448
epoch: 65, step: 55
	action: tensor([[-52755.6517,  -4116.1379, -16624.7134,  57925.5000,  18189.7273,
         -17089.4786, -10497.1245]], dtype=torch.float64)
	q_value: tensor([[-35.1014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005168803008653411, distance: 1.147297887243627 entropy 11.916474990495056
epoch: 65, step: 56
	action: tensor([[-16464.1629, -37556.6280, -14201.7515,  22477.0937,  37018.0074,
          -9379.2850,  34310.4678]], dtype=torch.float64)
	q_value: tensor([[-28.4562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07802298209676084, distance: 1.1881484425165683 entropy 11.727979802587345
epoch: 65, step: 57
	action: tensor([[-38040.9202, -17799.5793,  15973.0946, -20965.7688,  52951.3497,
         -11290.5790,  70678.2076]], dtype=torch.float64)
	q_value: tensor([[-33.7488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9413085082097095, distance: 1.594424548115054 entropy 12.088865003172213
epoch: 65, step: 58
	action: tensor([[ -9984.4167, -30235.5496,  36476.2513,   9100.6818,  -3719.9009,
          36738.1783,   1676.5807]], dtype=torch.float64)
	q_value: tensor([[-30.5202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01683274077788055, distance: 1.153935286683655 entropy 11.81546033011619
epoch: 65, step: 59
	action: tensor([[-54145.0925, -17631.2675,  23167.8561, -13210.3804,  33430.1620,
         -14448.8390, -24018.5408]], dtype=torch.float64)
	q_value: tensor([[-28.9381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8732426595180209, distance: 1.566223417246634 entropy 11.827005339217589
epoch: 65, step: 60
	action: tensor([[-31238.3763, -67193.2478,  20351.6887,   5912.3657,  15684.4176,
          -3629.6834,   8944.7801]], dtype=torch.float64)
	q_value: tensor([[-29.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.617355016893901, distance: 1.4553229338674902 entropy 11.817756798132475
epoch: 65, step: 61
	action: tensor([[ 33325.2689, -20413.5984,  40682.1248,  48506.8005,  36759.3210,
         -60508.4757,  83194.6088]], dtype=torch.float64)
	q_value: tensor([[-33.1929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23844908170756807, distance: 0.9986335818685383 entropy 11.933007046019721
epoch: 65, step: 62
	action: tensor([[-45538.5822, -35016.7878, -28812.8593, -44933.5758,  17813.3125,
          27862.6438,  14449.3655]], dtype=torch.float64)
	q_value: tensor([[-34.0298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7786778773084138, distance: 1.5261785486849586 entropy 11.797821804729068
epoch: 65, step: 63
	action: tensor([[ 14329.9606, -60529.3819, -27290.0259,  12548.4399,   -541.5447,
           3520.9873, -22595.3927]], dtype=torch.float64)
	q_value: tensor([[-26.3708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01922276854567906, distance: 1.133292150901557 entropy 11.642689025733883
epoch: 65, step: 64
	action: tensor([[ 21034.0618, -15125.4173,   6030.5449,   9933.4829,  -1393.0004,
          16758.6429, -12777.4385]], dtype=torch.float64)
	q_value: tensor([[-28.7277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5291265483618194, distance: 0.7852515382761877 entropy 11.665113515231791
epoch: 65, step: 65
	action: tensor([[-33908.7390, -21935.2729, -53790.7461, -46488.8064,  44395.3828,
          -8070.4993,  -7307.1924]], dtype=torch.float64)
	q_value: tensor([[-36.6284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07145296269015344, distance: 1.1027032322725476 entropy 11.860917956007873
epoch: 65, step: 66
	action: tensor([[ 30613.3265,  -7326.3447, -42750.7853,  41877.7003,  24729.4113,
           9597.0987, -29498.1665]], dtype=torch.float64)
	q_value: tensor([[-25.5831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.674351464180247, distance: 0.6530271807996637 entropy 11.526257441805388
epoch: 65, step: 67
	action: tensor([[-20376.1642,   5809.9149,    813.4643,  22573.6198, -20741.4724,
         -20283.7416,  51592.7533]], dtype=torch.float64)
	q_value: tensor([[-28.5383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53674266278786
epoch: 65, step: 68
	action: tensor([[-18808.7051, -10206.2345,  -3676.7389,  -3664.8639,  33249.4611,
          15013.1510,  -2052.7970]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06486749254790114, distance: 1.180876494457593 entropy 11.322253775862496
epoch: 65, step: 69
	action: tensor([[-22163.3254,  -1969.1131, -36019.8823,  46501.1408, -33174.7981,
          12873.7467,  38790.8238]], dtype=torch.float64)
	q_value: tensor([[-29.0191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21545540216500636, distance: 1.2616131509028687 entropy 11.721101431902342
epoch: 65, step: 70
	action: tensor([[-11102.0661,  22277.8201,  16789.2806,  -6221.0557,  19276.5920,
          -5836.1149, -29001.3443]], dtype=torch.float64)
	q_value: tensor([[-23.9434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03262724715027354, distance: 1.1255210419692807 entropy 11.533466346277793
epoch: 65, step: 71
	action: tensor([[-19710.5098, -48278.4170, -41722.9528,  14648.7513,  47331.1532,
         -20208.6310, -10111.0021]], dtype=torch.float64)
	q_value: tensor([[-34.2385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8557603139807908, distance: 1.5588977662650934 entropy 11.587305790700956
epoch: 65, step: 72
	action: tensor([[-17046.7821, -96456.0537,  32734.3033, -36269.5831,  49549.7293,
          -5290.9068, -14211.9703]], dtype=torch.float64)
	q_value: tensor([[-29.2778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29036792006426393, distance: 0.9639917415675169 entropy 11.744798107083017
epoch: 65, step: 73
	action: tensor([[-28905.5477,  17552.1729, -13739.0000,  46735.1801,  -7135.1147,
          74711.3447,  12746.6240]], dtype=torch.float64)
	q_value: tensor([[-30.9085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08157023022250587, distance: 1.096679358849448 entropy 11.761627967523665
epoch: 65, step: 74
	action: tensor([[-32206.7519,   9526.9707,   9742.9832,  51272.6973, -24975.0647,
          17511.2752,  -4824.5168]], dtype=torch.float64)
	q_value: tensor([[-35.7125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2736753460519967, distance: 0.9752637593439424 entropy 11.915790115497527
epoch: 65, step: 75
	action: tensor([[ 33365.8275,  15969.4941, -40505.2744, -10410.9929, -60516.3157,
          -7578.4571,  17511.8403]], dtype=torch.float64)
	q_value: tensor([[-31.5983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.79490292556468
epoch: 65, step: 76
	action: tensor([[-14500.9633,  -4022.2896, -15361.4197,  32669.6015,   8822.6923,
          16959.2429,   1768.8033]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08007799864121568, distance: 1.1892803767281586 entropy 11.322253775862496
epoch: 65, step: 77
	action: tensor([[-39454.7050, -22825.0137, -38923.4622,  11620.8933,  10962.7614,
            541.1220,  28038.0109]], dtype=torch.float64)
	q_value: tensor([[-27.7636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5054940584256711, distance: 1.4040941056800806 entropy 11.76036953952398
epoch: 65, step: 78
	action: tensor([[-29211.2382, -14969.3898, -22240.1720, -46561.6509,  23225.0242,
          35732.0810,  15445.5539]], dtype=torch.float64)
	q_value: tensor([[-30.0786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.679111515176396, distance: 1.4828474110292873 entropy 11.917093764005354
epoch: 65, step: 79
	action: tensor([[-45066.1109, -30272.1673,  -6946.6883,  63706.1360,  54026.8902,
          -2358.3303,  41636.5415]], dtype=torch.float64)
	q_value: tensor([[-31.1280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20338060122399193, distance: 1.021367729892091 entropy 11.868998142354046
epoch: 65, step: 80
	action: tensor([[ 22653.1098, -23176.2497, -40427.5690,  67824.2093, -52907.3621,
         -30619.9752, -38846.9560]], dtype=torch.float64)
	q_value: tensor([[-33.4236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4575465115985998, distance: 0.8428260425989775 entropy 12.029283345762073
epoch: 65, step: 81
	action: tensor([[-12361.3663,  -6966.9169, -57154.6356,  40184.2574, -10098.7134,
         -23631.3748, -37659.8244]], dtype=torch.float64)
	q_value: tensor([[-34.7742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48600331242366046, distance: 1.394975506017389 entropy 12.04028520181322
epoch: 65, step: 82
	action: tensor([[-49920.6973, -42409.4076, -81300.9579, -10502.4897,  51497.1436,
         -40083.1032,  75448.2954]], dtype=torch.float64)
	q_value: tensor([[-31.6105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.600602737946454, distance: 1.4477663258721938 entropy 12.000342858833122
epoch: 65, step: 83
	action: tensor([[ 13775.7427,   3056.0048, -18140.1674,  31018.0622,  45986.0397,
         -34187.4726,   6915.2629]], dtype=torch.float64)
	q_value: tensor([[-30.9314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.80339353396622
epoch: 65, step: 84
	action: tensor([[ -1199.7553, -11248.0500, -25930.4317,   4686.6267,  23338.5463,
          20353.7482,  53107.1667]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6403448575480359, distance: 1.4656297570273364 entropy 11.322253775862496
epoch: 65, step: 85
	action: tensor([[-34533.8754, -86276.6809,  19676.8975,  42375.8743,  23895.3419,
         133896.0725,  -2953.4549]], dtype=torch.float64)
	q_value: tensor([[-30.2755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030118999980979333, distance: 1.1269792479052185 entropy 11.862920049004064
epoch: 65, step: 86
	action: tensor([[-47644.8298, -65558.6668,  15065.2798, -30683.5570, -20837.8486,
         -30172.6963,   1468.9924]], dtype=torch.float64)
	q_value: tensor([[-28.4794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8941756290167409, distance: 1.5749501623549205 entropy 11.809104008255245
epoch: 65, step: 87
	action: tensor([[-40244.4510, -36337.0614, -19364.2306, -27719.4572,  -7490.9184,
         -42330.9137,  42622.0014]], dtype=torch.float64)
	q_value: tensor([[-30.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.054463721109214824, distance: 1.1127453590903382 entropy 11.86376966007157
epoch: 65, step: 88
	action: tensor([[-40883.1417, -25503.1217,  -6781.1875,  -1507.9768, -28401.8021,
          -4327.8077,  13022.2976]], dtype=torch.float64)
	q_value: tensor([[-32.6243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47096784850266427, distance: 1.3879003440326496 entropy 11.881212676074437
epoch: 65, step: 89
	action: tensor([[-26149.2492,  14520.0371,   8544.1688,  13735.6410,   1849.1295,
         -19309.5208,  32425.3599]], dtype=torch.float64)
	q_value: tensor([[-35.3553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11617936153789987, distance: 1.2089927242846854 entropy 11.968521569096865
epoch: 65, step: 90
	action: tensor([[ 5674.7776,  -481.4544,  9193.4741, 25497.9476,  5579.6367, 20495.2038,
         -4279.7081]], dtype=torch.float64)
	q_value: tensor([[-32.9477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36983427377137423, distance: 0.9084145521641249 entropy 11.728086149507535
epoch: 65, step: 91
	action: tensor([[   303.3168,   3368.3972, -17569.0551,  36268.5072,  23413.2935,
           9823.1879,  21325.1988]], dtype=torch.float64)
	q_value: tensor([[-30.0418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28640522371365584, distance: 0.9666795347380521 entropy 11.572077046324575
epoch: 65, step: 92
	action: tensor([[-37350.1350,    914.9448,   2661.8186,  31820.7832,  -5609.6018,
          19491.5632, -19478.1260]], dtype=torch.float64)
	q_value: tensor([[-33.0818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08296469109395899, distance: 1.0958464931123404 entropy 11.628922612973275
epoch: 65, step: 93
	action: tensor([[  4806.9765, -17392.7493,  -6673.8383,  33900.5639,  14271.0312,
         -33223.5626,  -3718.1790]], dtype=torch.float64)
	q_value: tensor([[-30.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13243892206971186, distance: 1.0658760973580041 entropy 11.762109541320914
epoch: 65, step: 94
	action: tensor([[ -1069.8133,   1127.1465,  50218.3513,  22140.8223,  65240.3795,
         -15073.4660,  22084.6734]], dtype=torch.float64)
	q_value: tensor([[-32.9202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39905408994426894, distance: 0.887103688810685 entropy 11.761741227616884
epoch: 65, step: 95
	action: tensor([[-42236.7920, -15786.9332, -92572.3672,  30304.1219,  -9881.7169,
         -40273.0435,   2966.7101]], dtype=torch.float64)
	q_value: tensor([[-32.3005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09105169384877598, distance: 1.0910038561424424 entropy 11.841862783390136
epoch: 65, step: 96
	action: tensor([[-50153.5310, -34626.5320, -24493.7216,  -7028.6320, -29590.7824,
         -22655.9757,  -5787.8360]], dtype=torch.float64)
	q_value: tensor([[-30.0071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8789192734022229, distance: 1.5685947384699586 entropy 11.894176108186558
epoch: 65, step: 97
	action: tensor([[-29314.1725, -45254.5335, -88226.9243, -16333.2909,   8376.1803,
         -38586.3356,   3030.4341]], dtype=torch.float64)
	q_value: tensor([[-30.0220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049413763597846105, distance: 1.1157128992640133 entropy 11.85486980885884
epoch: 65, step: 98
	action: tensor([[  2157.9970,  -6674.1517,  35790.9041, -24585.6651,  47056.5175,
         -27647.4932,  37175.8965]], dtype=torch.float64)
	q_value: tensor([[-30.2879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5500353807460782, distance: 0.7676192840987118 entropy 11.823635438262297
epoch: 65, step: 99
	action: tensor([[-33002.4186, -25517.8357,  29891.0911, -17378.4037,   6235.5994,
           -488.9115, -19955.7487]], dtype=torch.float64)
	q_value: tensor([[-30.1737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8471845274035985, distance: 1.5552916283715916 entropy 11.646524590134485
epoch: 65, step: 100
	action: tensor([[-61527.7581, -76776.8420, -10775.5677,  60434.2325, -41366.4180,
          11226.6886,  32155.1098]], dtype=torch.float64)
	q_value: tensor([[-31.9587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5460251578898836, distance: 1.4228691773475035 entropy 11.857942608597892
epoch: 65, step: 101
	action: tensor([[-19966.8319, -50284.5585,  56058.1571,  22075.9550,    -71.2402,
         -30093.3246,   5190.8238]], dtype=torch.float64)
	q_value: tensor([[-28.3699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020131954356799264, distance: 1.132766745125649 entropy 11.814255708173578
epoch: 65, step: 102
	action: tensor([[  6956.9981, -12606.4292, -26057.4708,  27340.3025,  53913.1548,
         -27389.1041,  -8159.7339]], dtype=torch.float64)
	q_value: tensor([[-30.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45041787647710463, distance: 0.8483459525654389 entropy 11.914438621460231
epoch: 65, step: 103
	action: tensor([[-39437.3365,  37001.3437,  11866.1087,  -6283.3393, -35834.5229,
          13915.2351,   8458.6359]], dtype=torch.float64)
	q_value: tensor([[-33.3497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.957481688018031
epoch: 65, step: 104
	action: tensor([[-17267.2875, -48943.9222, -13347.1860,   9371.1554,  44184.0852,
          16043.5226,   1602.7746]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03994884414742739, distance: 1.1669780343795362 entropy 11.322253775862496
epoch: 65, step: 105
	action: tensor([[-21290.7438,  22566.7813,   7628.9589,   3899.8770, -29249.1240,
         -16003.2458,  52439.4360]], dtype=torch.float64)
	q_value: tensor([[-28.1370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5192780882285855, distance: 0.7934209292827248 entropy 11.780521891774614
epoch: 65, step: 106
	action: tensor([[ -8833.3337, -13946.7590,   7562.9858,   9749.8072,  -9331.9034,
          33785.6474, -20789.1797]], dtype=torch.float64)
	q_value: tensor([[-30.2198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07200422971773046, distance: 1.1023758529590433 entropy 11.69904188332263
epoch: 65, step: 107
	action: tensor([[-28958.9046, -80075.2791,  27373.8438,   1358.3386,   4380.2054,
          49411.0285,  24787.9773]], dtype=torch.float64)
	q_value: tensor([[-30.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1989336201161418, distance: 1.2530092083412014 entropy 11.949170329163826
epoch: 65, step: 108
	action: tensor([[-28414.3933, -24786.4923,  -9367.8252,  20077.2285,  40340.3003,
         -73308.1430, -16942.8623]], dtype=torch.float64)
	q_value: tensor([[-29.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16532753777538334, distance: 1.2353234849585606 entropy 11.839461534593704
epoch: 65, step: 109
	action: tensor([[-14913.0684,  -5213.4861,  22333.1220,  10778.0581,  33941.3996,
         -35158.9020,  -9222.3040]], dtype=torch.float64)
	q_value: tensor([[-33.4750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33013210214320177, distance: 1.3197877129327868 entropy 11.96225437117649
epoch: 65, step: 110
	action: tensor([[ -5314.5150, -23291.6875, -27043.1470,  64656.4646,  64971.0640,
          48932.7719,  28924.4929]], dtype=torch.float64)
	q_value: tensor([[-29.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8600935570790531, distance: 1.5607167361542151 entropy 11.768177101122273
epoch: 65, step: 111
	action: tensor([[-57892.8423, -40532.1507, -58299.7786,  76866.5219,  48072.8178,
           9554.0096,  73886.0072]], dtype=torch.float64)
	q_value: tensor([[-32.9268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0136042154050523, distance: 1.1365336458702306 entropy 12.048800142874029
epoch: 65, step: 112
	action: tensor([[-17479.9711,  27820.8709, -38956.9016,  82959.9259,  48670.8654,
          13038.2028, -26725.9346]], dtype=torch.float64)
	q_value: tensor([[-29.3980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5714712788108299, distance: 0.7491118389382759 entropy 11.828883268045969
epoch: 65, step: 113
	action: tensor([[   616.5270, -53805.1536,  -8270.2390, -24701.2965,  74037.6301,
         -18833.5740,  59752.6037]], dtype=torch.float64)
	q_value: tensor([[-31.0039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20828270759681744, distance: 1.2578851014648136 entropy 11.853584348697774
epoch: 65, step: 114
	action: tensor([[ -3607.1059, -89516.7452, -16270.2827,  56617.1199, -14324.8043,
          11536.4226,  61925.3508]], dtype=torch.float64)
	q_value: tensor([[-32.2610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13642322716417055, distance: 1.2199070581979818 entropy 11.929175758771155
epoch: 65, step: 115
	action: tensor([[-61525.6880, -22870.4351,   -143.3756,  96942.6497,  40834.4223,
         -58821.2756,  59200.6868]], dtype=torch.float64)
	q_value: tensor([[-32.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21566756144299637, distance: 1.2617232541841223 entropy 12.044644213291962
epoch: 65, step: 116
	action: tensor([[ 13344.2558, -11602.8889,  10863.5316,  11042.1286,  -1523.8826,
          11773.8711,  12177.3799]], dtype=torch.float64)
	q_value: tensor([[-28.1782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5903432060184521, distance: 0.7324310956957071 entropy 11.753926743425257
epoch: 65, step: 117
	action: tensor([[-57028.2541,  12758.6835,   2350.4432,  74926.8510,  37706.5054,
           1041.2738,  64219.5331]], dtype=torch.float64)
	q_value: tensor([[-30.6800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.687011765240905
epoch: 65, step: 118
	action: tensor([[20193.6804, -8750.7373, 24842.3571, 44161.7281, 25391.9840, -4775.1221,
         41400.3575]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07201491795891224, distance: 1.1848329074786703 entropy 11.322253775862496
epoch: 65, step: 119
	action: tensor([[ 36437.1573, -61444.0169, -32272.2704,  41519.3299,  10929.5405,
         -43722.1959,  12985.7639]], dtype=torch.float64)
	q_value: tensor([[-30.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3265152960124935, distance: 0.9391188581446627 entropy 11.643522790475611
epoch: 65, step: 120
	action: tensor([[ 47259.2341,  15227.6333,   8660.1116,   6166.5837, -19843.8808,
         -27170.5628, -38855.0859]], dtype=torch.float64)
	q_value: tensor([[-37.4844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7935111806780054, distance: 0.5200019398855549 entropy 11.876186397315058
epoch: 65, step: 121
	action: tensor([[ 12124.6709,  52153.3952, -64148.3840,  40634.8212, -45889.4687,
          21509.7608,  11733.0831]], dtype=torch.float64)
	q_value: tensor([[-34.5738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7473508477144871, distance: 0.5751956804499997 entropy 11.79327014151824
epoch: 65, step: 122
	action: tensor([[-50526.5533,  14741.2771,  27037.4649,  26864.2199,  62324.0129,
          23986.1644, -33758.8597]], dtype=torch.float64)
	q_value: tensor([[-40.3230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.906247155249991
epoch: 65, step: 123
	action: tensor([[-39624.9588, -33676.6575,   7090.8994,   6725.4377, -12644.2907,
           2702.3952,  26177.3399]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11362291004972747, distance: 1.2076074169109927 entropy 11.322253775862496
epoch: 65, step: 124
	action: tensor([[-31015.7038,  59588.1100,  45681.5613,   7128.9790,  29150.3700,
           8283.8880,  38255.5584]], dtype=torch.float64)
	q_value: tensor([[-30.8677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10242200114009847, distance: 1.2015189623543405 entropy 12.019503498713817
epoch: 65, step: 125
	action: tensor([[ -7920.2573, -26099.3521,   6545.3857,  25446.3169, -12923.0999,
          15688.5911,  36198.3931]], dtype=torch.float64)
	q_value: tensor([[-35.1145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5053777217438726, distance: 1.4040398541192534 entropy 11.996101138360343
epoch: 65, step: 126
	action: tensor([[ 24534.8475, -12352.0085,  12351.7656, -49884.5177,  -2761.3286,
          28567.4962, -17840.0825]], dtype=torch.float64)
	q_value: tensor([[-30.1515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003223749044289459, distance: 1.1461873091559638 entropy 11.895276742296407
epoch: 65, step: 127
	action: tensor([[-12208.6766, -59478.2136, -54702.7431,  24452.2143, -19046.1039,
          51970.4801, -21347.9172]], dtype=torch.float64)
	q_value: tensor([[-31.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3049485306970787, distance: 1.3072341495479258 entropy 11.925165942825288
LOSS epoch 65 actor 402.62879851287005 critic 172.90773065614903
epoch: 66, step: 0
	action: tensor([[-78878.0535, -28440.8552, -49056.7331,  59119.5673,  32322.1592,
         -27476.4774, -52849.3898]], dtype=torch.float64)
	q_value: tensor([[-33.6681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20644092742984876, distance: 1.0194039776993766 entropy 11.96893474661634
epoch: 66, step: 1
	action: tensor([[ -8862.4489,  -2084.7571,  24029.2958,   9607.2112, -11133.2115,
          -2055.7312,  -3693.9514]], dtype=torch.float64)
	q_value: tensor([[-35.9057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20307388721906428, distance: 1.255170846689866 entropy 12.084653770316294
epoch: 66, step: 2
	action: tensor([[-26229.9814, -37783.2968,  38333.8693,  24404.9788, -31272.9931,
          -6632.2111,  26085.9056]], dtype=torch.float64)
	q_value: tensor([[-34.0471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048160852069105475, distance: 1.1164479347532452 entropy 11.819093988061928
epoch: 66, step: 3
	action: tensor([[ 49670.1914, -33526.5411, -41265.5258, -28386.7271, -29379.1848,
          15537.9393,  93671.1207]], dtype=torch.float64)
	q_value: tensor([[-34.2842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3153555470972478, distance: 1.3124364202171452 entropy 11.905117308732017
epoch: 66, step: 4
	action: tensor([[ 9715.0415, 25880.1139,  5408.6878, 44110.0990, 15495.9843, 11337.7488,
         60259.8600]], dtype=torch.float64)
	q_value: tensor([[-34.2937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19562593648760684, distance: 1.0263269252428755 entropy 11.688330377057946
epoch: 66, step: 5
	action: tensor([[-41520.7109, -28537.8079, -42028.5006,   -920.0486,  50124.8849,
          22460.4530,  25018.6572]], dtype=torch.float64)
	q_value: tensor([[-38.7762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.807317956023903
epoch: 66, step: 6
	action: tensor([[ -3877.5252, -30084.5631,  12714.5766,  52163.2646,   2626.9121,
          17412.2741,  34798.3241]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.358531388846108
epoch: 66, step: 7
	action: tensor([[ -2185.2021, -28315.0067,   5364.1674,   3800.4368, -14773.0011,
         -17993.1528,  20377.4638]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7375878650591756, distance: 1.5084470871223268 entropy 11.358531388846108
epoch: 66, step: 8
	action: tensor([[-18654.7729, -45979.9096,  27813.2809,  44115.2802,  58331.7013,
         -23986.7119,  11028.5907]], dtype=torch.float64)
	q_value: tensor([[-32.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5819147670393294, distance: 1.4392897526844441 entropy 11.847196395094972
epoch: 66, step: 9
	action: tensor([[ 18499.7214,   4956.8189,  -2364.0648,   -557.3353, -41099.9865,
          18602.4248,  18367.8733]], dtype=torch.float64)
	q_value: tensor([[-34.8959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.01548167371277
epoch: 66, step: 10
	action: tensor([[ -5676.3772, -25244.5292,  13692.6915,   3747.8001,  -4916.0515,
         -28741.7448, -45453.5326]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04553640976247164, distance: 1.1179860288047794 entropy 11.358531388846108
epoch: 66, step: 11
	action: tensor([[-47763.8354, -43198.5523, -28169.0121,  -1853.3547, -51988.7109,
         -34980.2469,  75159.0773]], dtype=torch.float64)
	q_value: tensor([[-35.4489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8610849248832118, distance: 1.5611325857267448 entropy 12.011541931178874
epoch: 66, step: 12
	action: tensor([[-60116.0172, -29353.3353,  -5796.2551,  29777.2912, -14850.0609,
          12210.8628,  26529.5246]], dtype=torch.float64)
	q_value: tensor([[-33.8023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24000612105681052, distance: 1.2742909763987555 entropy 11.837130566714267
epoch: 66, step: 13
	action: tensor([[ 22977.2916, -56008.7221, -32543.6829,  40796.1517,  -4744.1763,
         -39931.4444,  75821.2384]], dtype=torch.float64)
	q_value: tensor([[-37.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3773727941734579, distance: 1.3430198868266168 entropy 12.087937818541972
epoch: 66, step: 14
	action: tensor([[  -222.2155, -15458.5309, -21721.4231,  49437.7745,   9435.7272,
         -50803.2876, -51687.7609]], dtype=torch.float64)
	q_value: tensor([[-35.1582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33239395077505063, distance: 1.320909365459896 entropy 11.95312269779372
epoch: 66, step: 15
	action: tensor([[ 19978.0313, -23726.5054, -52105.9627,  48751.1946, -11638.8303,
          42262.3000,   1981.5830]], dtype=torch.float64)
	q_value: tensor([[-34.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4474786577003155, distance: 0.8506114460154441 entropy 11.96816312445138
epoch: 66, step: 16
	action: tensor([[ 31274.8804, -26644.5947, -32878.7045,  51562.7899,  13884.2952,
         -29420.9949, -26245.5888]], dtype=torch.float64)
	q_value: tensor([[-37.2560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2231151833348024, distance: 1.265582240732955 entropy 11.89433191307501
epoch: 66, step: 17
	action: tensor([[  3247.3507, -39210.7024, -23448.1449, 101202.9332, -13429.7737,
          15950.0282,  37064.4338]], dtype=torch.float64)
	q_value: tensor([[-37.3086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2820182820257051, distance: 0.9696463929220008 entropy 11.747857704803318
epoch: 66, step: 18
	action: tensor([[-24884.1540,  32506.3669,  -9396.5348,  25167.7532, -25006.6311,
          43314.6136,  35093.0878]], dtype=torch.float64)
	q_value: tensor([[-38.0651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2731451201688432, distance: 0.9756196716385026 entropy 11.82151899184874
epoch: 66, step: 19
	action: tensor([[-31392.4143,  -5098.1485,  -2404.8444,  37411.5214,  44817.6053,
         -47271.5438, -26344.0503]], dtype=torch.float64)
	q_value: tensor([[-34.1314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2722084051120117, distance: 1.290731245846097 entropy 11.767162826583059
epoch: 66, step: 20
	action: tensor([[-68745.7285,  58131.3580, -39003.0005, -18679.8438, -51398.9569,
         -19173.5607,  29475.6282]], dtype=torch.float64)
	q_value: tensor([[-32.5637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10296784605120413, distance: 1.0838288285249877 entropy 11.804891465909808
epoch: 66, step: 21
	action: tensor([[-29615.6749,  -9016.1463,  39848.4236, -30999.0370, -19147.9296,
         -30091.4286, -90668.7106]], dtype=torch.float64)
	q_value: tensor([[-36.6559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6723976358178483, distance: 1.479879880184931 entropy 11.877703017561236
epoch: 66, step: 22
	action: tensor([[-10265.2803, -56561.7182, -28119.0698,  -1140.5730,  66581.1761,
         -33248.5715,  48480.3067]], dtype=torch.float64)
	q_value: tensor([[-38.1102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10568605786106322, distance: 1.2032963794365918 entropy 12.02145141858199
epoch: 66, step: 23
	action: tensor([[-19985.9522, -11399.7111, -32073.6096, -32129.0847,  34132.9667,
          -2701.6851,  22654.9371]], dtype=torch.float64)
	q_value: tensor([[-34.2913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7153941495374885, distance: 1.4987826448790569 entropy 11.818709831386235
epoch: 66, step: 24
	action: tensor([[-39439.8577,    713.0933, -23953.6908,  43516.7659,  10874.5191,
           1800.0193,  30824.2275]], dtype=torch.float64)
	q_value: tensor([[-36.7269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.033267117454244266, distance: 1.1632230451567955 entropy 11.925714746064981
epoch: 66, step: 25
	action: tensor([[ 23556.8402, -38824.2771,  14583.6971,  31110.9248,  18144.1366,
         -57967.9489, -29177.4468]], dtype=torch.float64)
	q_value: tensor([[-27.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14832125972466748, distance: 1.0560745978435375 entropy 11.626298469579748
epoch: 66, step: 26
	action: tensor([[-19036.7197, -48407.0673,  14387.0628,  61602.8896,  27759.5868,
          21886.3061,  23105.4729]], dtype=torch.float64)
	q_value: tensor([[-34.2215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.746891131148276
epoch: 66, step: 27
	action: tensor([[  -794.6585, -30432.1790,  -6652.1644,  11632.2381,   3673.6053,
         -12627.2532, -15271.9013]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.062146344215223914, distance: 1.1793667312310416 entropy 11.358531388846108
epoch: 66, step: 28
	action: tensor([[-30919.7707, -59684.9728, -57535.0986,  26879.5908, -20022.9959,
         -18139.5651,  -1816.0608]], dtype=torch.float64)
	q_value: tensor([[-35.2720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4758063739711125, distance: 1.390181113771395 entropy 11.916992732340864
epoch: 66, step: 29
	action: tensor([[ 39322.2301, -10868.6255, -53552.9784,   6577.0148, -38195.9902,
            150.6573, -34058.5708]], dtype=torch.float64)
	q_value: tensor([[-31.8452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.774557473502611
epoch: 66, step: 30
	action: tensor([[ 19928.9020,    989.4865,   4976.1164, -27942.8756,   6510.3834,
         -18967.6543, -17003.1801]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.358531388846108
epoch: 66, step: 31
	action: tensor([[  3846.9623,   8777.5727, -10289.6247,  -6574.7725,   5965.0490,
         -33981.0721, -32390.2993]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24914368130629239, distance: 0.9915967922662122 entropy 11.358531388846108
epoch: 66, step: 32
	action: tensor([[-23129.9427,  21287.6462,  -3485.9382,  27197.9676, -14403.1148,
          60682.6054, -16622.7766]], dtype=torch.float64)
	q_value: tensor([[-40.3778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.729260077100065
epoch: 66, step: 33
	action: tensor([[-13318.5108,   9160.3026,  -1884.2873, -30705.8580,  -1483.6302,
         -18621.3989,  -8895.3777]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35623569371365693, distance: 0.9181637448897602 entropy 11.358531388846108
epoch: 66, step: 34
	action: tensor([[-19685.2162, -48840.3324,   5343.8199,  15846.8724, -41866.2845,
          -7602.1054, -26880.5570]], dtype=torch.float64)
	q_value: tensor([[-40.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9171016355667667, distance: 1.584452638758365 entropy 11.669004658007239
epoch: 66, step: 35
	action: tensor([[-18377.9640, -17653.2231,   5176.3587, -12978.7884,  -4207.8297,
          38199.5686,  43058.5368]], dtype=torch.float64)
	q_value: tensor([[-32.1613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058580046626873994, distance: 1.1773851261442978 entropy 11.824793057480877
epoch: 66, step: 36
	action: tensor([[ -3723.3542, -19425.1346, -55046.5035,  20491.7339,  24144.1674,
         -27755.2528, -51903.3291]], dtype=torch.float64)
	q_value: tensor([[-34.0650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32795357171362216, distance: 1.3187064759921105 entropy 11.84897524032364
epoch: 66, step: 37
	action: tensor([[ -5567.8723, -27954.5495,  27583.3181, 110355.0252, -18049.2030,
         -41854.5103,  -1168.6469]], dtype=torch.float64)
	q_value: tensor([[-30.8364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09698962888368778, distance: 1.1985549617313176 entropy 11.808026758704822
epoch: 66, step: 38
	action: tensor([[  2728.6098,  48963.9709, -29324.2153,  28624.3175, -35399.3593,
          -7629.6793,  24930.6760]], dtype=torch.float64)
	q_value: tensor([[-32.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35502007973158234, distance: 0.9190302159541224 entropy 11.832944058047806
epoch: 66, step: 39
	action: tensor([[27711.0621,  5945.7349, 19048.6208, 25109.0510, 31252.6585, 35148.4899,
         17599.0332]], dtype=torch.float64)
	q_value: tensor([[-30.9146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38843739971281155, distance: 0.894905449159853 entropy 11.551207357704484
epoch: 66, step: 40
	action: tensor([[-25060.6581,  -8035.4046, -24405.9375,  26778.3578, -19221.6072,
          13573.9255, -38169.5862]], dtype=torch.float64)
	q_value: tensor([[-34.4354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08899490657546283, distance: 1.092237531833317 entropy 11.706207628728652
epoch: 66, step: 41
	action: tensor([[ 13287.5402, -59080.0091,   9409.4094,  -9329.1006,  -5601.3006,
         -14988.2623, -25735.3009]], dtype=torch.float64)
	q_value: tensor([[-31.0337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2278753283173609, distance: 1.26804255885592 entropy 11.862438275984875
epoch: 66, step: 42
	action: tensor([[ -1845.8297, -49279.9516, 113741.1667,  47678.0013,  55989.4682,
          46091.1116, -11303.8540]], dtype=torch.float64)
	q_value: tensor([[-34.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2549264403300867, distance: 0.9877709947276166 entropy 11.931994183379818
epoch: 66, step: 43
	action: tensor([[-11400.2956,  -8637.8408,  -8438.8184, -74311.2134,  15818.8682,
         -39635.3028,  -4673.3709]], dtype=torch.float64)
	q_value: tensor([[-34.7363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0377559161052825, distance: 1.633551288752945 entropy 11.865136690388303
epoch: 66, step: 44
	action: tensor([[-47373.3782,  22052.7504,  10587.9729,   8322.0376,  29676.2017,
          -3757.8905,  45362.5024]], dtype=torch.float64)
	q_value: tensor([[-34.2726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6557867094589147, distance: 0.6713832634835567 entropy 11.88206516031349
epoch: 66, step: 45
	action: tensor([[ 29388.1846, -14393.7248, -13727.2527, -18963.3460, -13418.6670,
           6937.9883, -18835.0085]], dtype=torch.float64)
	q_value: tensor([[-36.3229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.888125966738414
epoch: 66, step: 46
	action: tensor([[-24195.3174,   9556.4055,   3000.0716,  17680.0932,   -933.0340,
           4882.1077,   4621.4033]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19868054225918597, distance: 1.0243763364152618 entropy 11.358531388846108
epoch: 66, step: 47
	action: tensor([[  6400.5745,   6793.8476,   -395.6579, -21716.8887, -13621.8983,
          18812.0242,  49079.8121]], dtype=torch.float64)
	q_value: tensor([[-38.1716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.738440293589326
epoch: 66, step: 48
	action: tensor([[-45833.5483,  -6642.3140,  16766.1110,    251.2019,   7852.1958,
         -26997.5283, -37735.6776]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2735134501802716, distance: 1.2913930991543185 entropy 11.358531388846108
epoch: 66, step: 49
	action: tensor([[-56376.8304,  -3348.6252,  22799.3626,   -806.0296,  11009.3186,
         -16969.6136, -49991.0278]], dtype=torch.float64)
	q_value: tensor([[-34.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3028567678181644, distance: 1.3061860160519514 entropy 11.957290155538397
epoch: 66, step: 50
	action: tensor([[-44046.6004, -12047.0876,  39517.7073,   2717.9360,  -7387.4691,
           7663.2033, -17408.2261]], dtype=torch.float64)
	q_value: tensor([[-32.7041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1760935313882881, distance: 1.2410166949126156 entropy 11.782118980185349
epoch: 66, step: 51
	action: tensor([[-25939.1638, -14227.9016, -30845.0953,   2573.3639, -44475.6716,
         -52486.7127,  52069.8579]], dtype=torch.float64)
	q_value: tensor([[-35.1796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18793976161553594, distance: 1.0312187867696307 entropy 11.962478574298686
epoch: 66, step: 52
	action: tensor([[  11157.0917, -118651.3739,   21877.7562,   41465.9627,  -39855.8501,
           30702.2916,   90293.9785]], dtype=torch.float64)
	q_value: tensor([[-34.2848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27601215298254256, distance: 0.9736936356289605 entropy 12.021865721724238
epoch: 66, step: 53
	action: tensor([[-21996.7319,  50580.3105,  -8906.4442,  21068.5106,  -1843.4148,
          13841.8975, -10011.2401]], dtype=torch.float64)
	q_value: tensor([[-32.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2334252113563544, distance: 1.270905045563743 entropy 11.70351753513488
epoch: 66, step: 54
	action: tensor([[ -1748.4876, -34069.0750, -19108.3274, -22819.7108,  29263.6862,
          39195.1898,   7760.7396]], dtype=torch.float64)
	q_value: tensor([[-37.3767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.001050447818692568, distance: 1.144945133206079 entropy 12.044266384224139
epoch: 66, step: 55
	action: tensor([[-16195.5378, -29536.7743,  63013.9016,  53748.4612,  23940.0843,
          62606.9222, -43848.4911]], dtype=torch.float64)
	q_value: tensor([[-31.9673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05783792130324428, distance: 1.1769723465370705 entropy 11.839496308983623
epoch: 66, step: 56
	action: tensor([[-23012.5999, -50814.1443,  34065.3364,  17388.7318, -10877.3384,
          94824.8442,  89648.2588]], dtype=torch.float64)
	q_value: tensor([[-32.3497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12099908197036946, distance: 1.0728805140709938 entropy 11.828305861234593
epoch: 66, step: 57
	action: tensor([[-11279.9667,  -7395.0586, -15230.2508,    587.8009,  26505.8509,
          16713.6273,  36935.9459]], dtype=torch.float64)
	q_value: tensor([[-32.1246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8360265628530861, distance: 1.550587124111538 entropy 11.896315110320936
epoch: 66, step: 58
	action: tensor([[ -3698.7119, -17944.1963, -22538.8172, 105461.4729, -23535.6130,
         -19353.0232,  40974.6678]], dtype=torch.float64)
	q_value: tensor([[-34.4581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45995235379689214, distance: 1.3826938608354753 entropy 12.057346962312195
epoch: 66, step: 59
	action: tensor([[-52194.9513, -32244.8418, -30090.0069,  37041.7375,  24919.3192,
         -28102.8029,  28486.1356]], dtype=torch.float64)
	q_value: tensor([[-33.2602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8542453140913049, distance: 1.5582613122634308 entropy 11.884378781878805
epoch: 66, step: 60
	action: tensor([[-81936.5910,  51806.8618,   3251.2561,  39707.4932, -25214.7260,
          29128.0471, -11953.0942]], dtype=torch.float64)
	q_value: tensor([[-35.3435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20528051636942757, distance: 1.0201490359833867 entropy 11.946857980237663
epoch: 66, step: 61
	action: tensor([[-32376.6939,  -9215.0845, -25656.8330,  24846.2473,  42850.9095,
         -34499.6990,  13967.3030]], dtype=torch.float64)
	q_value: tensor([[-34.4709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25362246212967055, distance: 1.2812682837131624 entropy 11.860723146187652
epoch: 66, step: 62
	action: tensor([[-36287.1414, -25838.4510,  56639.1445,  69917.8324, -38646.7815,
         -42249.2327, -47652.2548]], dtype=torch.float64)
	q_value: tensor([[-34.5903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1630795586510756, distance: 1.234131407261269 entropy 11.861405414671157
epoch: 66, step: 63
	action: tensor([[-31753.7334, -14802.1728,  71776.3821, -27290.5071,  20331.1041,
           9556.7291,  78852.6460]], dtype=torch.float64)
	q_value: tensor([[-35.4101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47573878448764173, distance: 1.390149279412595 entropy 11.85253343917896
epoch: 66, step: 64
	action: tensor([[-101602.1792,  -15663.4575,   20468.1528,  -41515.8799,  -29161.5782,
           24401.0114,   16032.7347]], dtype=torch.float64)
	q_value: tensor([[-32.9214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09635512128051116, distance: 1.1982082846905622 entropy 11.896545721921727
epoch: 66, step: 65
	action: tensor([[-20734.7737, -16067.8453, -15923.3853,  19906.4508, -42619.1437,
          47289.1971, -13230.2931]], dtype=torch.float64)
	q_value: tensor([[-37.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4322325976680921, distance: 1.3695045210574346 entropy 11.950189567180317
epoch: 66, step: 66
	action: tensor([[  -566.2713, -60653.3867, -36837.9085,   4773.9743,  -8634.9171,
         -52075.2379, -49046.3684]], dtype=torch.float64)
	q_value: tensor([[-30.6890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10957739164873948, distance: 1.2054119506889116 entropy 11.861938110348646
epoch: 66, step: 67
	action: tensor([[-32799.5120,  30538.5590,  16825.1399,  43779.4319,   7918.9668,
         -71346.4456,  22224.1219]], dtype=torch.float64)
	q_value: tensor([[-31.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1398874842672464, distance: 1.2217650173699048 entropy 11.8426737760838
epoch: 66, step: 68
	action: tensor([[  5906.6459, -11041.2388,   5178.3775, -17863.8118,   3527.6883,
          26147.8710,  31858.4981]], dtype=torch.float64)
	q_value: tensor([[-34.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35481559874034097, distance: 1.3319771892829353 entropy 11.791847085943276
epoch: 66, step: 69
	action: tensor([[-39445.7939, -25680.0560,  -7955.0054, -41998.6603,  37270.6326,
          21288.7690,  11984.9436]], dtype=torch.float64)
	q_value: tensor([[-36.0787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2452316033237094, distance: 1.2769731343212356 entropy 11.847913853284258
epoch: 66, step: 70
	action: tensor([[-13804.8232, -17955.0613,  16917.9557, -24582.1130,  -9584.0320,
          21453.8397,  -9840.9377]], dtype=torch.float64)
	q_value: tensor([[-34.9182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.167374015826388, distance: 1.0441951537808882 entropy 11.87991013322644
epoch: 66, step: 71
	action: tensor([[ 11954.4696, -16356.8483, -38509.6411,  63398.8375,  43454.8917,
          16267.4261,  23817.4531]], dtype=torch.float64)
	q_value: tensor([[-37.6355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5409918124934026, distance: 0.775294868374448 entropy 11.963686501140355
epoch: 66, step: 72
	action: tensor([[-28741.5137, -15261.8477,  17782.8984,   -213.9490, -17031.7325,
         -47505.3866,  38831.1619]], dtype=torch.float64)
	q_value: tensor([[-31.4546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2509125774315093, distance: 1.7168644466239555 entropy 11.603343077898687
epoch: 66, step: 73
	action: tensor([[-25246.7214, -47929.6398,  11388.5422,   7143.3082,  11686.5729,
          43063.8836, -29690.0727]], dtype=torch.float64)
	q_value: tensor([[-29.3116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3815281308382691, distance: 0.8999464510580737 entropy 11.777043282590085
epoch: 66, step: 74
	action: tensor([[ 1.4373e+03,  9.1216e+03, -4.0280e+04,  8.3305e+03, -4.9505e+04,
         -3.9901e+01,  2.5535e+03]], dtype=torch.float64)
	q_value: tensor([[-31.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4506194005794655, distance: 0.8481903999878548 entropy 11.865797375198328
epoch: 66, step: 75
	action: tensor([[-24439.7982,  -3313.5491,  25985.2832,  46699.7093,  21273.2762,
          43548.5859,  29233.3133]], dtype=torch.float64)
	q_value: tensor([[-35.6105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1179430941412567, distance: 1.0747439165593706 entropy 11.7487054039486
epoch: 66, step: 76
	action: tensor([[-58300.5289, -71659.2430, -29399.1411, -14505.6229,   7342.6493,
          26294.4861,  73586.7180]], dtype=torch.float64)
	q_value: tensor([[-34.8768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05572420599740413, distance: 1.1757958768738104 entropy 12.097301781965447
epoch: 66, step: 77
	action: tensor([[-26535.4689,  42463.6015,   8205.8080, -12965.8324, -17694.4417,
         -74372.5884,  -6552.7157]], dtype=torch.float64)
	q_value: tensor([[-31.2720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3638808327958568, distance: 0.9126955522320569 entropy 11.880170455798151
epoch: 66, step: 78
	action: tensor([[-22724.9801, -34534.7108, -33637.5735,   7676.2935,  32113.9075,
          22845.0191,   4028.3471]], dtype=torch.float64)
	q_value: tensor([[-40.1839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08564053558390006, distance: 1.1923389152534898 entropy 11.640936572422262
epoch: 66, step: 79
	action: tensor([[ -8020.3816, -39573.7325, -51571.2004, -17845.9084,   4695.0083,
          33347.2999,  49293.1649]], dtype=torch.float64)
	q_value: tensor([[-28.2732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14368962028068966, distance: 1.2238009418045008 entropy 11.645049624190142
epoch: 66, step: 80
	action: tensor([[ 13158.0666, -11914.4481, -39786.7000,  22840.7548,   7784.8541,
          26509.4352,  38823.5564]], dtype=torch.float64)
	q_value: tensor([[-32.2870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6185964850044583, distance: 0.7067226963119392 entropy 11.781405311753735
epoch: 66, step: 81
	action: tensor([[-58090.7028, -32001.7810, -47521.2211,  44232.5717, -49984.4322,
          52875.1952, -14810.4913]], dtype=torch.float64)
	q_value: tensor([[-28.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22316746043654412, distance: 1.2656092865359718 entropy 11.41856172529958
epoch: 66, step: 82
	action: tensor([[-43152.0668,  -3557.5216,   7895.3002,  86581.2487, -16778.8101,
          12239.0541,  15997.1739]], dtype=torch.float64)
	q_value: tensor([[-32.0554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30318847554483597, distance: 1.3063522831468506 entropy 11.903617006238497
epoch: 66, step: 83
	action: tensor([[-75085.4347, -71042.7980,   4799.3165, -15306.8138, -59181.2528,
           9940.9600,  -6589.1623]], dtype=torch.float64)
	q_value: tensor([[-35.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8910882996209866, distance: 1.5736661280802535 entropy 12.084538773938972
epoch: 66, step: 84
	action: tensor([[-19147.4525, -27133.4803, -20456.1635,  26552.3173, -16484.1274,
         -15424.8270,  45341.3904]], dtype=torch.float64)
	q_value: tensor([[-29.3861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1891259614029268, distance: 1.2478736770837555 entropy 11.699472005852368
epoch: 66, step: 85
	action: tensor([[ 33747.8943, -96413.9155,  22229.0787,  43918.3286,  -7403.0825,
          51115.5916,  15275.7407]], dtype=torch.float64)
	q_value: tensor([[-37.8363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.985103605932503
epoch: 66, step: 86
	action: tensor([[  2115.1100, -20868.1022,   8520.0908,  35938.7651,   8952.9048,
         -10580.7478,   4714.4273]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47349445894582387, distance: 0.8303442189373251 entropy 11.358531388846108
epoch: 66, step: 87
	action: tensor([[ 23114.4064, -58951.1606,  27883.9062,  -4935.4914,  -1849.7598,
           -311.6398,   -827.1381]], dtype=torch.float64)
	q_value: tensor([[-31.9709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026955258786233838, distance: 1.1596647463015821 entropy 11.655881284196884
epoch: 66, step: 88
	action: tensor([[ -9917.6969, -46008.9145,  10667.3676, -13661.3783,   -262.3158,
          20583.9928, -17263.1672]], dtype=torch.float64)
	q_value: tensor([[-34.6458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07442941737204056, distance: 1.1009344577689972 entropy 11.863998561196084
epoch: 66, step: 89
	action: tensor([[ 26485.3514, -12616.0246,  43747.5298,  37489.7907,   5913.8198,
          40160.8064,   7338.1064]], dtype=torch.float64)
	q_value: tensor([[-35.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3148666310776762, distance: 0.9472055919193115 entropy 11.937807303115022
epoch: 66, step: 90
	action: tensor([[-23877.2737, -19011.9686, -40701.2046,  -9649.6688,  17407.8105,
          17734.1090, -14966.5647]], dtype=torch.float64)
	q_value: tensor([[-33.4018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19729402453301226, distance: 1.0252621891642206 entropy 11.708139896439093
epoch: 66, step: 91
	action: tensor([[-49849.5621,  63299.8143, -60672.5956,   2344.8971, -20321.2137,
          19568.6735,  25989.6502]], dtype=torch.float64)
	q_value: tensor([[-33.0771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09460284434306443, distance: 1.088870560777844 entropy 11.870771639674928
epoch: 66, step: 92
	action: tensor([[  8667.1372, -34346.7703, -43175.9827, -25465.0327, -17582.8275,
          64396.5680,   1993.5336]], dtype=torch.float64)
	q_value: tensor([[-40.5053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.908170740161166
epoch: 66, step: 93
	action: tensor([[ 12770.8728, -21527.2624,   1670.8091,   6498.3093,  16290.4013,
          12056.0290, -26818.9959]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.358531388846108
epoch: 66, step: 94
	action: tensor([[  5458.2144,   8886.3456,  38013.4320,  -3890.8276, -16946.9712,
          39437.6120,  19202.2467]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8618605031122457, distance: 0.4253198266952245 entropy 11.358531388846108
epoch: 66, step: 95
	action: tensor([[-32984.5214, -33616.0659,  21334.4474,   1831.8318,  17964.1669,
         -22053.0420, -16455.3251]], dtype=torch.float64)
	q_value: tensor([[-32.8195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.416172264279187
epoch: 66, step: 96
	action: tensor([[18991.4915, -7694.9506, 10618.6179, 18830.0245, -9734.7553, 21036.4339,
          1858.2470]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05495424124585602, distance: 1.1124566896727597 entropy 11.358531388846108
epoch: 66, step: 97
	action: tensor([[ 20143.7912, -44572.9578,   4502.2087,  25872.8963,   -289.4906,
           9365.9205,  38896.1200]], dtype=torch.float64)
	q_value: tensor([[-34.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16887129384428956, distance: 1.0432558641207164 entropy 11.827481387728056
epoch: 66, step: 98
	action: tensor([[  7538.5372,   5945.9521, -36050.7383,  19134.8701,  27857.9031,
          53015.1321,  27718.1952]], dtype=torch.float64)
	q_value: tensor([[-37.9699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23090478181672247, distance: 1.003567870591323 entropy 11.969356575087813
epoch: 66, step: 99
	action: tensor([[-82637.8570, -67708.0648,  32255.2303,  54388.3389,  -8026.1080,
         -14721.4061, -12247.1224]], dtype=torch.float64)
	q_value: tensor([[-40.0555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.039703965293077514, distance: 1.1668406309532493 entropy 11.948021442043713
epoch: 66, step: 100
	action: tensor([[ -1412.2505,   -944.4847,  -9070.4026,  80438.1798, 101511.2785,
          -4678.7124,  23269.7960]], dtype=torch.float64)
	q_value: tensor([[-35.6424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24852662681760207, distance: 1.2786615257258818 entropy 12.090994319222697
epoch: 66, step: 101
	action: tensor([[  5958.2681, -77208.2757,  14399.3841,  12462.0606,  32951.7716,
          22443.2629,  27830.9051]], dtype=torch.float64)
	q_value: tensor([[-37.9111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08987093903316001, distance: 1.1946597434869894 entropy 12.01419808924608
epoch: 66, step: 102
	action: tensor([[-21762.8804, -76010.0950,    663.1365, -12746.4208, -39624.0231,
         -15439.0531,  -6642.6570]], dtype=torch.float64)
	q_value: tensor([[-38.0320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2772693803763473, distance: 1.7268869065699348 entropy 11.994628171770882
epoch: 66, step: 103
	action: tensor([[-60825.9056, -12856.2061,  -6896.5789,  64518.9324,  13664.1483,
          33983.3135,  -3628.6120]], dtype=torch.float64)
	q_value: tensor([[-35.0978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18023941484115125, distance: 1.243202143654804 entropy 11.973626042470231
epoch: 66, step: 104
	action: tensor([[-16066.8280, -37632.5065,   4132.9541, -34266.3771,   1875.2146,
           1025.8660, -67190.7281]], dtype=torch.float64)
	q_value: tensor([[-36.1053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5477250673744205, distance: 1.4236512099967598 entropy 12.066672811891092
epoch: 66, step: 105
	action: tensor([[-20497.9135,  80562.6778,   -103.7143,  43330.0640, -55398.0983,
          23183.6621, -47257.4971]], dtype=torch.float64)
	q_value: tensor([[-36.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0035742290052848125, distance: 1.1422973491236987 entropy 12.04697451833937
epoch: 66, step: 106
	action: tensor([[ 16222.2585,  -5914.3004,  23502.9975,   3199.9134, -15777.7177,
         -10022.2914,  -1252.2903]], dtype=torch.float64)
	q_value: tensor([[-35.8484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11773298100627039, distance: 1.2098338352845213 entropy 11.936638117642435
epoch: 66, step: 107
	action: tensor([[-22471.0132, -27173.9650,  12450.2445,  47107.7201,  49016.5652,
         -72665.6680,  32761.6518]], dtype=torch.float64)
	q_value: tensor([[-32.9959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07923102550942884, distance: 1.0980750704323436 entropy 11.855110316195569
epoch: 66, step: 108
	action: tensor([[  1382.1289, -19360.4411,  -2580.3944,   9619.2317,  21184.6980,
          43230.7705, -31397.6697]], dtype=torch.float64)
	q_value: tensor([[-33.5133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13341331952357838, distance: 1.0652773621410734 entropy 11.975285678261837
epoch: 66, step: 109
	action: tensor([[-27444.8058,  37489.6713,  -9192.5636, -49942.1728,   1891.5821,
          24842.8067,  25906.1337]], dtype=torch.float64)
	q_value: tensor([[-36.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.857819935903454
epoch: 66, step: 110
	action: tensor([[-21543.9485, -17822.4266, -11639.7508,  -7023.1042, -18299.8225,
           6836.5078,  37288.2585]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5198320076377358, distance: 1.4107643825017835 entropy 11.358531388846108
epoch: 66, step: 111
	action: tensor([[ 18812.0249, -81135.3245, -55648.4875,  40771.7339, -11101.2705,
          33598.8680,  34183.1598]], dtype=torch.float64)
	q_value: tensor([[-33.7145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.053506309635071525, distance: 1.1133085768383082 entropy 11.932612455774445
epoch: 66, step: 112
	action: tensor([[-85894.4456,  11960.6383,  66536.6205, -23490.0409, -37009.6415,
          49254.0726,  15025.3777]], dtype=torch.float64)
	q_value: tensor([[-41.7745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.873992876915057
epoch: 66, step: 113
	action: tensor([[  2177.5789,  -7380.1179,   4596.4724, -25677.7469,  -4226.9570,
           5288.8677,  11340.1919]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.358531388846108
epoch: 66, step: 114
	action: tensor([[-37649.9401, -16430.1336,   5343.3799,  13641.0370,  -7759.5494,
          53053.9295, -19178.8113]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06138401645317515, distance: 1.1086658391585997 entropy 11.358531388846108
epoch: 66, step: 115
	action: tensor([[-29249.0961, -11383.2473,  26743.0420, 103939.2719,  -7000.2315,
         -14983.3425, -36051.7676]], dtype=torch.float64)
	q_value: tensor([[-34.2025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1200468244094055, distance: 1.2110854395704562 entropy 11.916553133623598
epoch: 66, step: 116
	action: tensor([[-37567.9837, -20413.9705, -41121.5874,  -5234.3298,  19629.7859,
          27966.8721, -34444.1169]], dtype=torch.float64)
	q_value: tensor([[-33.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.642356898919701, distance: 1.4665283497736326 entropy 11.813735935869838
epoch: 66, step: 117
	action: tensor([[-11794.5090, -82713.1559,  35410.4909,   4664.6985,    740.5182,
          44506.4545, -14463.2512]], dtype=torch.float64)
	q_value: tensor([[-29.1038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22168680992416334, distance: 1.0095640763235527 entropy 11.6382614917494
epoch: 66, step: 118
	action: tensor([[   3093.7680, -103684.4863,  -43445.4274,  -27605.9439,   66735.4865,
            -565.1239,  -31752.8905]], dtype=torch.float64)
	q_value: tensor([[-33.4866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19861852256891488, distance: 1.0244159774555817 entropy 11.98247390348328
epoch: 66, step: 119
	action: tensor([[ 14927.6508, -45295.5391,  16858.6578,   3088.9499,   2750.3527,
          -3836.9003,  -2666.6405]], dtype=torch.float64)
	q_value: tensor([[-35.1340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02566582545896301, distance: 1.1589364867069625 entropy 11.769417792767907
epoch: 66, step: 120
	action: tensor([[-60192.5030, -42315.3762,  14445.9809, -45832.2944, -51098.6778,
         -21351.6985, -10763.8154]], dtype=torch.float64)
	q_value: tensor([[-40.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6228542324429971, distance: 1.4577949771239158 entropy 11.971807493813959
epoch: 66, step: 121
	action: tensor([[ -6555.5380, -20201.0998, -27419.6438,  24740.8476, -37473.5510,
          12243.6266,  29695.5713]], dtype=torch.float64)
	q_value: tensor([[-32.9982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09200729914778671, distance: 1.0904302023799128 entropy 11.769982199324806
epoch: 66, step: 122
	action: tensor([[ 13664.2191,  32327.2645, -42275.9452, -19438.3192,  41329.5173,
          -9965.6696,  25093.1495]], dtype=torch.float64)
	q_value: tensor([[-33.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.939330689707097
epoch: 66, step: 123
	action: tensor([[ 1432.4045, -6216.4262,  1113.1904, 34670.4348, 21483.1389, 48085.0301,
          6883.1462]], dtype=torch.float64)
	q_value: tensor([[-34.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5952865320293079, distance: 0.7279985625121241 entropy 11.358531388846108
epoch: 66, step: 124
	action: tensor([[ -9574.6441, -57003.6211,  12119.2267,  13429.9575,  34986.7844,
          16062.0347,  -9804.6879]], dtype=torch.float64)
	q_value: tensor([[-32.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6004902342650109, distance: 1.4477154444448748 entropy 11.671557337550189
epoch: 66, step: 125
	action: tensor([[-17891.6268, -23784.6881,  27890.9150,   8211.6116, -29050.2722,
          71695.9363, -50204.0331]], dtype=torch.float64)
	q_value: tensor([[-35.5945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.803437838576951, distance: 1.536764367212074 entropy 12.03035841837528
epoch: 66, step: 126
	action: tensor([[   9205.6052, -106544.9586,  -10872.8526,   17443.7552,  -35900.7902,
          -54368.2909,   52702.5346]], dtype=torch.float64)
	q_value: tensor([[-34.1918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10783648924799571, distance: 1.2044659471258325 entropy 12.027977304225933
epoch: 66, step: 127
	action: tensor([[  1737.4330, -79623.3782,  28042.9228, -29363.8635,  21619.9521,
           6905.3211, -23176.7167]], dtype=torch.float64)
	q_value: tensor([[-33.2328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17216049219862506, distance: 1.2389398810023378 entropy 11.68499328588219
LOSS epoch 66 actor 461.44623132343065 critic 107.3647831508174
epoch: 67, step: 0
	action: tensor([[-27361.2146, -17610.9017, -22681.7583,  -7904.9689, -45866.0046,
          16921.7161, -20306.9725]], dtype=torch.float64)
	q_value: tensor([[-28.5481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04071132371182906, distance: 1.1208083357498355 entropy 11.435304820892823
epoch: 67, step: 1
	action: tensor([[ 18903.8671, -43162.7770, -45945.9983,  21510.5769, -22706.7633,
          -6640.7182,   -681.4787]], dtype=torch.float64)
	q_value: tensor([[-29.7356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09443817832373647, distance: 1.1971603113744964 entropy 11.669799559290112
epoch: 67, step: 2
	action: tensor([[-66316.8014, -86932.1000,  -2695.4921, -21271.9572,   6913.1576,
         -10748.0641,  -8087.8739]], dtype=torch.float64)
	q_value: tensor([[-34.5166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34775514487854653, distance: 1.3285019385003995 entropy 11.677697725121337
epoch: 67, step: 3
	action: tensor([[  3094.2400, -47030.1167,  -3735.6913,  51187.3610,  21670.6507,
         -19297.8046,   2754.1232]], dtype=torch.float64)
	q_value: tensor([[-32.3188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03457953892980159, distance: 1.1243847399449627 entropy 11.813463950704037
epoch: 67, step: 4
	action: tensor([[-48122.0644, -59362.2704, -43702.7441,  25185.3642,  63512.5143,
          51324.7189,   9129.0737]], dtype=torch.float64)
	q_value: tensor([[-35.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.432995204098247, distance: 1.3698690756584966 entropy 11.865151898755744
epoch: 67, step: 5
	action: tensor([[-72544.4484, -38198.9123,  24132.2620,  32538.8780,   -359.8775,
         -15782.5394,  19750.4934]], dtype=torch.float64)
	q_value: tensor([[-35.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42865397571740815, distance: 1.367792507174439 entropy 12.018617150881044
epoch: 67, step: 6
	action: tensor([[-65629.8917, -42724.3918, -64286.0030,  65284.9810, -16544.9185,
          58101.3585,  29024.1287]], dtype=torch.float64)
	q_value: tensor([[-33.1123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27676613855596877, distance: 0.9731864847152628 entropy 12.099561257117816
epoch: 67, step: 7
	action: tensor([[ -55314.2320,   -6360.1034,   48138.7601,   30769.1409,   80512.4652,
           -7265.5786, -100312.7874]], dtype=torch.float64)
	q_value: tensor([[-33.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40005876133946994, distance: 1.3540347963397519 entropy 12.082194310951095
epoch: 67, step: 8
	action: tensor([[-23375.4744, -90323.7166,  84940.6626, 116897.0827,  26979.6118,
           8833.4993, -32015.6771]], dtype=torch.float64)
	q_value: tensor([[-32.3001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28101500497907317, distance: 0.9703236276875749 entropy 12.09021911899795
epoch: 67, step: 9
	action: tensor([[-52640.8073,  14377.7542,  -5072.6915, -14182.2562,  69362.9066,
         104125.6633, -27154.4708]], dtype=torch.float64)
	q_value: tensor([[-30.3245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.670200650919246, distance: 0.6571758421458984 entropy 12.024986357038305
epoch: 67, step: 10
	action: tensor([[  7221.2770, -27103.3303, -24297.5615,  -3549.9300,  18879.1167,
          -5790.2499, -10857.7883]], dtype=torch.float64)
	q_value: tensor([[-43.0581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.97139785155295
epoch: 67, step: 11
	action: tensor([[  5017.0300, -25015.8351, -22335.1943,  17578.7468, -24599.8039,
         -30956.4961, -20556.3527]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.394279083852146
epoch: 67, step: 12
	action: tensor([[  6244.0611, -23049.3893,   6435.9120, -18456.4687, -27598.3476,
         -22473.9565, -40962.5989]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43508034304317145, distance: 0.8601021566445294 entropy 11.394279083852146
epoch: 67, step: 13
	action: tensor([[ 26115.7690, -25445.0549,  17258.9286,  -9394.1337,  32753.1392,
          -4239.3758, -35276.6124]], dtype=torch.float64)
	q_value: tensor([[-34.5611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15616459531436844, distance: 1.0512005145577827 entropy 11.967917817933174
epoch: 67, step: 14
	action: tensor([[-21266.1364, -58807.6652,  32173.4962,  52983.0779, -51891.9224,
           4790.5993, -10872.6334]], dtype=torch.float64)
	q_value: tensor([[-34.0724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46787777298624533, distance: 1.3864417902028787 entropy 11.80938423705677
epoch: 67, step: 15
	action: tensor([[ -4339.3890,  -5895.7905,   9157.8195,  19634.1090, -36689.8378,
          23663.4779,  18375.9567]], dtype=torch.float64)
	q_value: tensor([[-32.1029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23336786699083256, distance: 1.2708755017819635 entropy 11.99905161198315
epoch: 67, step: 16
	action: tensor([[ 32385.0975, -33784.7820,  71666.6413, -57131.9148,  51882.6912,
          18816.7328, -35773.4921]], dtype=torch.float64)
	q_value: tensor([[-34.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11388197767596941, distance: 1.0772152128206138 entropy 12.245627887997452
epoch: 67, step: 17
	action: tensor([[  1516.6199, -50592.8093,  49778.0833,  48461.2869, -16963.0640,
          18694.0787, -31869.0229]], dtype=torch.float64)
	q_value: tensor([[-36.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18869412283686382, distance: 1.0307397014934856 entropy 11.900142233187992
epoch: 67, step: 18
	action: tensor([[-34188.8326, -64330.3179, -60257.9144, -49154.5440,  34543.2654,
           6688.6392,  26198.0688]], dtype=torch.float64)
	q_value: tensor([[-44.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27268941849272943, distance: 0.9759254561008628 entropy 11.9429387331906
epoch: 67, step: 19
	action: tensor([[-87476.4601,   4224.0090, -25882.2852,   -914.6658, -16606.6809,
          36997.1485,  17446.5076]], dtype=torch.float64)
	q_value: tensor([[-31.8285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6184247408806661, distance: 0.7068817952778007 entropy 12.029574854890768
epoch: 67, step: 20
	action: tensor([[ -8002.6341, -50719.6970, -68579.5094,  -9176.6098, -63665.5530,
          35228.3645,  73594.6497]], dtype=torch.float64)
	q_value: tensor([[-43.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1875397379084116, distance: 1.2470411046093453 entropy 11.823881587677675
epoch: 67, step: 21
	action: tensor([[-35162.5927,    408.8795, -70905.4916, -12308.1662, -46511.5546,
         -26226.0231,   8099.8463]], dtype=torch.float64)
	q_value: tensor([[-34.1079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45395130064173395, distance: 1.379849191408402 entropy 11.891338509228694
epoch: 67, step: 22
	action: tensor([[16928.9941, -7990.7952,  3194.0909,  4262.2334, 10254.6263, 15339.1088,
           114.0420]], dtype=torch.float64)
	q_value: tensor([[-36.9056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17656683909993864, distance: 1.24126638750574 entropy 11.867806954676913
epoch: 67, step: 23
	action: tensor([[-14675.2032, -31201.5757,   8296.5975,   5913.5482,   3030.0546,
          47724.2222, -45211.5545]], dtype=torch.float64)
	q_value: tensor([[-33.5081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2616675045415582, distance: 1.2853729377622958 entropy 11.613857365396216
epoch: 67, step: 24
	action: tensor([[-12743.1057, -14120.1000,  -6295.0674,  69300.9425,  52613.4947,
          74264.1627,  14130.2579]], dtype=torch.float64)
	q_value: tensor([[-37.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24833566110364447, distance: 1.2785637345199652 entropy 12.021430170718494
epoch: 67, step: 25
	action: tensor([[ -6130.7481, -18500.7631,  65315.4847,  -5129.3031,  35410.2735,
           8509.4026,  46645.3247]], dtype=torch.float64)
	q_value: tensor([[-32.6149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6846369304413489, distance: 1.4852851938378786 entropy 11.962678087930538
epoch: 67, step: 26
	action: tensor([[-27542.8794, -55156.3991, -28485.3252,  54143.9134,  -9435.9829,
          23263.0317,   6868.7693]], dtype=torch.float64)
	q_value: tensor([[-28.5205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03552886853464243, distance: 1.1644954570925616 entropy 11.746058188329162
epoch: 67, step: 27
	action: tensor([[ 24347.6982, -65636.4909, -46625.9912,   3850.6562,  20444.7739,
          34390.9736,  62244.8962]], dtype=torch.float64)
	q_value: tensor([[-34.1485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5857630597843673, distance: 0.7365141684402547 entropy 11.918443719494736
epoch: 67, step: 28
	action: tensor([[ -6816.0898, -67210.2457, -28927.7106, -53537.6066, -25293.5323,
         -23306.8237,  -5648.0047]], dtype=torch.float64)
	q_value: tensor([[-37.0616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6107869538366477, distance: 1.452364901508373 entropy 11.895277442624698
epoch: 67, step: 29
	action: tensor([[-29902.0761,   2085.3877, -31518.3503,  14842.5541,  25442.6145,
         -26557.4054, -63268.0157]], dtype=torch.float64)
	q_value: tensor([[-32.4934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10868378957391234, distance: 1.2049264615337543 entropy 11.972539380101457
epoch: 67, step: 30
	action: tensor([[-51216.5956, -59825.9736, -20220.0649, -19871.4066,  32389.3985,
          17771.6051,  46553.3125]], dtype=torch.float64)
	q_value: tensor([[-34.3849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7692634281818389, distance: 1.5221341982682624 entropy 11.900852806118978
epoch: 67, step: 31
	action: tensor([[ -1346.9655, -19861.0778, -16645.3621, -30165.8990, -18767.9115,
           4678.3939,  24466.5917]], dtype=torch.float64)
	q_value: tensor([[-27.9094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1780184178198323, distance: 1.2420318520841929 entropy 11.730538188316428
epoch: 67, step: 32
	action: tensor([[  4585.8271,  22112.0720, -17186.7174,  -1966.9005, -24411.7840,
         -17538.0275, -31472.4678]], dtype=torch.float64)
	q_value: tensor([[-30.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.737143585109928
epoch: 67, step: 33
	action: tensor([[  7978.6320, -59587.9543, -19187.0524,  21622.6318, -30345.7713,
          29684.6050, -39351.0667]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.209407535264738, distance: 1.2584704672964362 entropy 11.394279083852146
epoch: 67, step: 34
	action: tensor([[-30372.1402, -32653.4375, -13234.6895,  74958.5493,  21113.3187,
          18092.2657,  22857.6405]], dtype=torch.float64)
	q_value: tensor([[-38.9661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5331890106898947, distance: 1.4169500541482092 entropy 11.79604983835459
epoch: 67, step: 35
	action: tensor([[-53740.2555,  -2345.9654, -41916.6611,  90643.5180,  13045.6616,
          53411.9341, -78605.8731]], dtype=torch.float64)
	q_value: tensor([[-33.4748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.653346657422736, distance: 1.4714267738132032 entropy 12.010294629373778
epoch: 67, step: 36
	action: tensor([[-41000.2198,  78682.8788,   7257.6673, -21858.3904,  14583.8726,
          82191.1967,   6946.9786]], dtype=torch.float64)
	q_value: tensor([[-33.2797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5391401630540057, distance: 0.7768570730098485 entropy 12.034804927480138
epoch: 67, step: 37
	action: tensor([[-20410.6945,  34359.3129,  51494.2315, -11619.2408, -51071.9492,
           -772.9135, -14849.8041]], dtype=torch.float64)
	q_value: tensor([[-34.4679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7734576143946251, distance: 0.5446674574815744 entropy 11.65815603973607
epoch: 67, step: 38
	action: tensor([[-107118.5951,  -79772.2086,   41150.6876,   -1548.5936,  -19328.5824,
          -53951.6421,  -70682.4082]], dtype=torch.float64)
	q_value: tensor([[-46.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2202696111210758, distance: 1.2641091976569072 entropy 11.963056588942726
epoch: 67, step: 39
	action: tensor([[-20299.2703,   2640.8600, -17098.8193, -56243.6184,  19280.8184,
          26306.4770,  32247.4245]], dtype=torch.float64)
	q_value: tensor([[-34.0847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17991317715392852, distance: 1.0363026533519846 entropy 11.961782270882935
epoch: 67, step: 40
	action: tensor([[-41443.2992,   6223.8234, -19749.9125,  40135.9299,   3389.6593,
         -43911.5288, -89111.6774]], dtype=torch.float64)
	q_value: tensor([[-48.1689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.013399454596168
epoch: 67, step: 41
	action: tensor([[-1.3202e+04, -5.6936e+04, -7.3900e+03,  7.6943e+03, -2.5484e+03,
          1.7478e+01,  9.6216e+03]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.394279083852146
epoch: 67, step: 42
	action: tensor([[ -9108.9724, -25689.6994,  30741.7250,  26078.8394,  -1339.6279,
         -12333.4586,  43087.5543]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6728253897778436, distance: 1.4800691246543867 entropy 11.394279083852146
epoch: 67, step: 43
	action: tensor([[ -1017.3685,  -4635.9049, -21959.5255,   5051.6039, -21855.1798,
         -16177.4557,   1871.0386]], dtype=torch.float64)
	q_value: tensor([[-32.8276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1847953722186948, distance: 1.0332133559188814 entropy 12.041296176183968
epoch: 67, step: 44
	action: tensor([[-101530.0830,  -26874.6296,  -16177.9653,   18635.5284,   32718.6124,
           55020.2610,  -42402.7037]], dtype=torch.float64)
	q_value: tensor([[-31.8024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016339079573981063, distance: 1.1536551411804288 entropy 11.965102126871736
epoch: 67, step: 45
	action: tensor([[ 51348.5587,   6618.1416,  27683.0721,  38134.4932, -24296.7768,
          28807.2828, -31227.8970]], dtype=torch.float64)
	q_value: tensor([[-35.8502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4257033250555101, distance: 0.8672111322087159 entropy 12.041274773010533
epoch: 67, step: 46
	action: tensor([[  1119.6726, -33321.6037,    748.7413, -20775.6667, -33839.2946,
          11863.0296, -33216.5218]], dtype=torch.float64)
	q_value: tensor([[-31.9918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38560946572186516, distance: 1.347029522172323 entropy 11.723042957863782
epoch: 67, step: 47
	action: tensor([[-23636.6014,   6945.1063,  31411.3437,  32433.5199,  -3920.9101,
          -9621.1841, -27050.7477]], dtype=torch.float64)
	q_value: tensor([[-35.7899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.723627856782755
epoch: 67, step: 48
	action: tensor([[   786.6604, -20724.4220, -29991.3935, -22714.8360, -16563.4440,
          13341.2018,  23893.7756]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28018157133825783, distance: 1.29476955467454 entropy 11.394279083852146
epoch: 67, step: 49
	action: tensor([[-50104.3247,  17481.5947,   6217.9001,  28429.5544, -55721.5614,
           5166.4098, -35652.8118]], dtype=torch.float64)
	q_value: tensor([[-32.8079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.746875872621603
epoch: 67, step: 50
	action: tensor([[ -4451.4852, -30584.4637,  17550.4129,  -7151.4102,  -9500.5756,
          18007.1182,  12807.3720]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4000962021210177, distance: 1.3540529012163076 entropy 11.394279083852146
epoch: 67, step: 51
	action: tensor([[-53781.7654, -29776.9236, -18868.3631,  -6319.5999,  40580.2682,
          10730.3743, -23359.1760]], dtype=torch.float64)
	q_value: tensor([[-29.9914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04679570807314026, distance: 1.1708133343092686 entropy 11.734256875952966
epoch: 67, step: 52
	action: tensor([[-15530.5542,  -4811.1560, -50579.0948,   3580.4718,  12942.2630,
          24839.8071,  27034.1218]], dtype=torch.float64)
	q_value: tensor([[-33.9270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8371014172347373, distance: 1.5510409333168285 entropy 11.938753593118937
epoch: 67, step: 53
	action: tensor([[-29518.2344,  -7201.2302,  16651.2937,  16888.9851,   1832.1911,
          24869.1454, -31481.0560]], dtype=torch.float64)
	q_value: tensor([[-28.0256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6897254095886209, distance: 1.487526669152183 entropy 11.820115703917038
epoch: 67, step: 54
	action: tensor([[-34876.3683, -35151.6122,  49103.0434,   5896.7839, -22598.6769,
           -513.8504,  37470.2359]], dtype=torch.float64)
	q_value: tensor([[-32.3151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2880893956519721, distance: 1.2987623661136198 entropy 12.046530091978385
epoch: 67, step: 55
	action: tensor([[ 27378.8276, -35323.1291,  -2084.9220,  61747.4315, -13787.2217,
         -13684.6573, -17581.5044]], dtype=torch.float64)
	q_value: tensor([[-33.5071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09753112190982316, distance: 1.0871082968338337 entropy 11.980116401274325
epoch: 67, step: 56
	action: tensor([[-3196.6098, 27061.3684, 39645.6362,  5894.5513, 21599.6128, 13331.8853,
         34892.9638]], dtype=torch.float64)
	q_value: tensor([[-33.1675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011480471565935346, distance: 1.1377564859130995 entropy 11.83266444540939
epoch: 67, step: 57
	action: tensor([[ 46502.5200, -41567.6124,  -9388.3554,  24865.8729,  13216.5231,
          41197.9452, -70322.4427]], dtype=torch.float64)
	q_value: tensor([[-33.8676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2753546823571117, distance: 0.9741356525030355 entropy 12.02803319550374
epoch: 67, step: 58
	action: tensor([[ -3221.9336, -54661.0490,  12608.1722,  31641.4183,  19326.9116,
         -76057.6157,   1608.6764]], dtype=torch.float64)
	q_value: tensor([[-38.9912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4417371158422254, distance: 1.374041129545976 entropy 11.959443984756167
epoch: 67, step: 59
	action: tensor([[ -8163.5680,   3215.5911, -31107.5911,   6926.4357, -37434.1757,
         -11588.6961,  55032.5027]], dtype=torch.float64)
	q_value: tensor([[-29.6835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2154194890172796, distance: 1.0136206538519388 entropy 11.857952790921427
epoch: 67, step: 60
	action: tensor([[ 28681.2399, -76479.1483,  -1848.2997,  34610.0913, -29922.1626,
           4255.1718,  40371.9397]], dtype=torch.float64)
	q_value: tensor([[-32.4365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011121682557825308, distance: 1.137962944903065 entropy 11.820569205017192
epoch: 67, step: 61
	action: tensor([[-48201.1815,  30202.9366,  27160.3934,  39924.5199,    412.4732,
          -7906.4115,  31251.5240]], dtype=torch.float64)
	q_value: tensor([[-32.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4145298335377009, distance: 0.8756067042809613 entropy 11.867845998461751
epoch: 67, step: 62
	action: tensor([[-71595.7010, -74480.1382,    772.7500,  47325.8765, -73927.4715,
           6845.8787,  -6461.8935]], dtype=torch.float64)
	q_value: tensor([[-37.2484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1679916976205268, distance: 1.043807764958062 entropy 12.079478498172147
epoch: 67, step: 63
	action: tensor([[ 34127.1852, -55428.0585,   6885.8908, -43018.7948,  23321.7059,
           4906.1021,   6454.9745]], dtype=torch.float64)
	q_value: tensor([[-30.3055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24662048552552807, distance: 1.277685078584516 entropy 11.906944573009579
epoch: 67, step: 64
	action: tensor([[-40028.5582,  -6270.1277,  51954.8814,  60175.3891, -21230.5777,
         -20280.1774,  18966.5075]], dtype=torch.float64)
	q_value: tensor([[-28.8747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9615126399288405, distance: 1.6027000438023027 entropy 11.657388051237152
epoch: 67, step: 65
	action: tensor([[-52959.8371, -52091.3975, -15995.6525, -30677.3697, -17772.3368,
          -7990.3376,  23861.6826]], dtype=torch.float64)
	q_value: tensor([[-31.0897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9432280413183387, distance: 1.5952126234042088 entropy 11.928124303365152
epoch: 67, step: 66
	action: tensor([[ 10820.8631, -61727.7757,  38657.1718,  36132.6742,  13363.7251,
          47331.2441,  -9387.5380]], dtype=torch.float64)
	q_value: tensor([[-31.9660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4228600278262199, distance: 0.8693552283953789 entropy 11.865062863933487
epoch: 67, step: 67
	action: tensor([[-41624.3991, -32121.2038,  28628.1226,  61517.2550,   -910.2874,
          57508.7380,  50721.6142]], dtype=torch.float64)
	q_value: tensor([[-36.7647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06535679154641072, distance: 1.1063170881321414 entropy 11.79581240546732
epoch: 67, step: 68
	action: tensor([[-45356.6798,   8812.0260,  -1185.2704,  51288.9716, -11579.7265,
          16869.1224,  24179.6953]], dtype=torch.float64)
	q_value: tensor([[-32.2040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13799514643453115, distance: 1.0624574565754181 entropy 11.92891072814463
epoch: 67, step: 69
	action: tensor([[-72707.2290, -44322.7184,  36060.0006,  36285.0647,  30748.7112,
         -14276.4353,  24793.3584]], dtype=torch.float64)
	q_value: tensor([[-34.4865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13254411017062417, distance: 1.217823236239621 entropy 11.869408882772797
epoch: 67, step: 70
	action: tensor([[-50221.8289, -16991.5333, -40077.9564, -15903.6388, -27023.5264,
          12786.3567,  16145.0462]], dtype=torch.float64)
	q_value: tensor([[-30.1577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9306072403626964, distance: 1.5900239227757864 entropy 11.85621283566733
epoch: 67, step: 71
	action: tensor([[-47402.3521, -47469.1426, -29905.9228,  12009.0636,  42385.4782,
         -35234.9386,  37255.9614]], dtype=torch.float64)
	q_value: tensor([[-30.3276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24762180397027356, distance: 1.278198110757452 entropy 11.8131188139014
epoch: 67, step: 72
	action: tensor([[ 15067.4414,  27643.4551, -64887.1349,  27308.9632,  -6612.4750,
         -72084.2863,  49683.9222]], dtype=torch.float64)
	q_value: tensor([[-34.7697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.555866831897118, distance: 0.7626289670702534 entropy 12.024223252891998
epoch: 67, step: 73
	action: tensor([[-11889.2437,  17934.7304, -10594.0443, -44433.2511, -51162.0970,
         101538.6239,   7815.6408]], dtype=torch.float64)
	q_value: tensor([[-35.0725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20007791385156437, distance: 1.0234827733232679 entropy 11.964706323192216
epoch: 67, step: 74
	action: tensor([[ 31824.0270, -33826.5931, -41094.8857,  44340.5779, -13626.1855,
          93311.3857,  -2589.6349]], dtype=torch.float64)
	q_value: tensor([[-30.8536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06460594878431225, distance: 1.1067613771445628 entropy 11.6781513258783
epoch: 67, step: 75
	action: tensor([[-61889.5644, -40057.9623, -30384.1585,  52169.7100, -73544.8089,
         -21979.2446, -18505.5122]], dtype=torch.float64)
	q_value: tensor([[-35.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24317566929050338, distance: 1.2759185284683134 entropy 11.947688565702283
epoch: 67, step: 76
	action: tensor([[   153.8045, -34473.9540, -12881.2161,  24880.5527,  32227.0832,
          18230.3107, -11210.7131]], dtype=torch.float64)
	q_value: tensor([[-31.3614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.848918146129819
epoch: 67, step: 77
	action: tensor([[-12080.2524,   4723.6108, -31645.2105,  14864.7249,  29415.1101,
          -6128.2161,  40200.5072]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.394279083852146
epoch: 67, step: 78
	action: tensor([[-20764.0799,   2614.1052, -30497.6842,   9926.3496,   7337.8241,
           7973.8379, -32082.0717]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25987768285274426, distance: 0.9844835032861127 entropy 11.394279083852146
epoch: 67, step: 79
	action: tensor([[-58970.3407,   1654.1029,  34359.3335,  17394.8275,  14483.5849,
         -13194.6997, -26348.1333]], dtype=torch.float64)
	q_value: tensor([[-27.5378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41811785185569306, distance: 0.8729195296825444 entropy 11.397284475929933
epoch: 67, step: 80
	action: tensor([[ 16324.6764, -59066.4999,  23905.5069,   7181.7575,   9008.0765,
          65279.4640,  24483.2714]], dtype=torch.float64)
	q_value: tensor([[-33.9739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.88442692860443
epoch: 67, step: 81
	action: tensor([[ -1808.1326, -18542.2526, -27022.7530,  29147.2048,   9891.8623,
          25195.1855, -30297.8275]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5824082493548381, distance: 1.439514230219869 entropy 11.394279083852146
epoch: 67, step: 82
	action: tensor([[ 58937.5254, -16442.7450, -25521.3288,   5974.2950,  65184.3781,
         -22631.6510,  27509.0560]], dtype=torch.float64)
	q_value: tensor([[-32.1083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.892061329887284
epoch: 67, step: 83
	action: tensor([[ -6712.9409, -25189.6615,  24521.7286,  40632.0805,  33327.2852,
          -7769.5877, -16869.4844]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22406098921590556, distance: 1.2660714684974663 entropy 11.394279083852146
epoch: 67, step: 84
	action: tensor([[-12874.8946, -76328.6096, -19459.6373,  45776.1726,  14831.8178,
         -39656.3843,  19633.5744]], dtype=torch.float64)
	q_value: tensor([[-30.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1507162995487249, distance: 1.0545886376339864 entropy 11.893936594723233
epoch: 67, step: 85
	action: tensor([[  8768.2512, -61126.2326, -45772.4514,   8149.9171,  26584.2129,
          91189.4581, -18341.5033]], dtype=torch.float64)
	q_value: tensor([[-32.3651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.02336215383282
epoch: 67, step: 86
	action: tensor([[  1190.3195, -18100.8898, -12805.1316,  -3363.2804, -24475.2659,
         -33567.3589,  12943.2762]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11777989796124144, distance: 1.0748433353091742 entropy 11.394279083852146
epoch: 67, step: 87
	action: tensor([[-28975.7968, -87431.8263, -37942.1016, -22240.0806,  39059.1699,
         -29297.5366,  25402.4628]], dtype=torch.float64)
	q_value: tensor([[-29.7672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7029037653317425, distance: 1.4933160956596012 entropy 11.759286524191145
epoch: 67, step: 88
	action: tensor([[-40952.3590, -95222.9206, -46676.9124,  52363.9914,  46922.3613,
          24564.2137, -36949.6278]], dtype=torch.float64)
	q_value: tensor([[-34.3179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5890288105104491, distance: 1.4425224439226316 entropy 11.925090540149553
epoch: 67, step: 89
	action: tensor([[ -4256.3463, -48701.2885,  25323.2792,  -2335.9275,  24279.4815,
           9957.9877,   1278.7657]], dtype=torch.float64)
	q_value: tensor([[-27.2556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.460215243465079, distance: 1.3828183442021327 entropy 11.688557907491518
epoch: 67, step: 90
	action: tensor([[-58591.0503,  -9269.4057, -23301.3474,  35181.2626,  41168.5823,
          19956.1989, -20700.7547]], dtype=torch.float64)
	q_value: tensor([[-35.0003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4666594907656947, distance: 1.3858663240343776 entropy 11.958437362405022
epoch: 67, step: 91
	action: tensor([[-45618.1479, -50068.8355,  33254.9290,  21496.0953, -45203.8345,
          14826.4105, -12175.4771]], dtype=torch.float64)
	q_value: tensor([[-34.5459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4792649978751, distance: 1.3918091389648903 entropy 11.939027623342794
epoch: 67, step: 92
	action: tensor([[ 5.3651e+04, -1.9497e+04, -1.5168e+04, -4.9306e+01, -3.1599e+04,
         -2.0258e+04,  4.8713e+03]], dtype=torch.float64)
	q_value: tensor([[-33.9262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36303411679611064, distance: 0.913302778816365 entropy 12.08839635006875
epoch: 67, step: 93
	action: tensor([[-34237.7463,  11581.0267,  18204.8527,  53796.3537,  10307.1994,
         -16927.9394, -13963.7295]], dtype=torch.float64)
	q_value: tensor([[-28.6403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009373712470237, distance: 0.95678583300189 entropy 11.677139007293798
epoch: 67, step: 94
	action: tensor([[-28230.6564,  46694.9366,  32580.1966,  19569.3406, -86385.9177,
          83337.5239,  42474.6621]], dtype=torch.float64)
	q_value: tensor([[-34.6904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021410583060278654, distance: 1.1565299127758921 entropy 12.038562382068324
epoch: 67, step: 95
	action: tensor([[ 51991.3697,  19325.4083, -27850.1655,  -5891.5959,  55762.7116,
          34699.4817,  33575.1052]], dtype=torch.float64)
	q_value: tensor([[-33.8998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.814128387661542
epoch: 67, step: 96
	action: tensor([[ -6792.8606, -53795.2245,    732.6883,  63835.4189,  -1915.3631,
          10671.6973, -17125.4220]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2765721706020272, distance: 1.2929430009271696 entropy 11.394279083852146
epoch: 67, step: 97
	action: tensor([[-43625.3966, -17422.6286, -75006.0044,  -1476.9935, -10564.5538,
         -49296.4776,  -8583.7957]], dtype=torch.float64)
	q_value: tensor([[-32.3545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7367153343924797, distance: 1.508068305813198 entropy 11.958438288490926
epoch: 67, step: 98
	action: tensor([[ 79653.8689,  -3666.3424, -45588.3346,  -1449.6961,  -5006.4784,
          16012.6798, -23202.2798]], dtype=torch.float64)
	q_value: tensor([[-31.6620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22221688543573936, distance: 1.0092202325160962 entropy 11.94204966047887
epoch: 67, step: 99
	action: tensor([[  5969.6313, -43231.0737,  18251.2924,  14688.3160,  27962.9201,
         -17129.0923,  35624.2698]], dtype=torch.float64)
	q_value: tensor([[-34.0196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08099487839294739, distance: 1.0970228133563644 entropy 11.7362332761088
epoch: 67, step: 100
	action: tensor([[-48299.8328,  31717.5827,  -4724.5778, -21515.8771, -28118.7787,
          29438.1533, -14005.2156]], dtype=torch.float64)
	q_value: tensor([[-34.4030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.806574567684462
epoch: 67, step: 101
	action: tensor([[ 22668.4917, -35558.9056,   2979.5972,  49533.6941,  -5956.8087,
          16526.3542, -17513.5977]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.394279083852146
epoch: 67, step: 102
	action: tensor([[ 14196.4235,  11241.8033,   1755.8485,  22557.8366,   1854.0347,
          10272.7665, -11621.3197]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.394279083852146
epoch: 67, step: 103
	action: tensor([[ 15961.9836,  10380.1271,   7808.3192, -11329.3263,   6314.0006,
          -3367.2464,  50594.0760]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3653745301781367, distance: 0.911623353603018 entropy 11.394279083852146
epoch: 67, step: 104
	action: tensor([[ 43354.7511, -44730.9352, -30721.3003, -17679.5477, -52057.4197,
         -14554.1282,  -7439.5420]], dtype=torch.float64)
	q_value: tensor([[-35.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5486169887845448, distance: 0.7688281884214329 entropy 11.72806282210908
epoch: 67, step: 105
	action: tensor([[   447.2306, -89585.9756,  43889.1234,  -7153.9184,  21937.4969,
          57542.9797,  26043.2585]], dtype=torch.float64)
	q_value: tensor([[-34.9426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3230080684500488, distance: 0.9415609526677428 entropy 11.893518451146857
epoch: 67, step: 106
	action: tensor([[12297.0025, -4285.8504,  5371.0064,  6039.2708, 21563.8762,  8882.1038,
         -1249.5588]], dtype=torch.float64)
	q_value: tensor([[-41.4695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2793963186770524, distance: 0.9714152822763993 entropy 11.89010804001641
epoch: 67, step: 107
	action: tensor([[-29245.8712, -27219.5288,  -3452.9057,   1334.6992,  19207.3670,
         -17713.3515,  39829.1532]], dtype=torch.float64)
	q_value: tensor([[-30.6099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31288511628107685, distance: 1.3112033668475576 entropy 11.652896067844697
epoch: 67, step: 108
	action: tensor([[-59947.8693, -16272.3249, -16832.8284,   -121.4025, -61732.8779,
         -55455.3331,  13283.4664]], dtype=torch.float64)
	q_value: tensor([[-32.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7191536918320292, distance: 1.500424148897882 entropy 11.86402310595646
epoch: 67, step: 109
	action: tensor([[ 13821.7014, -28149.9855, -14233.2298,  48647.2662,  36456.4798,
          54004.1266,   4021.8842]], dtype=torch.float64)
	q_value: tensor([[-27.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.649311522127848
epoch: 67, step: 110
	action: tensor([[14721.9357, -2045.2244, 17845.4762, -3226.4873, -2732.5973, 10283.0543,
         13668.0560]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2469119768018082, distance: 1.277834447333864 entropy 11.394279083852146
epoch: 67, step: 111
	action: tensor([[  7441.8750,   4266.8085, -32609.0236, -20169.7363, -43117.7770,
         -26538.9057, -22013.6879]], dtype=torch.float64)
	q_value: tensor([[-30.8755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.858598328667848, distance: 0.4303125026795126 entropy 11.626774663283097
epoch: 67, step: 112
	action: tensor([[ 12050.8377, -69253.6574,  29178.9447,  36379.7396,  10011.6516,
         -30689.0762, -31780.5793]], dtype=torch.float64)
	q_value: tensor([[-38.7720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5244253775343335, distance: 0.7891617535831039 entropy 11.978656869863258
epoch: 67, step: 113
	action: tensor([[-22015.8418, -15006.1614, -71101.9311,  87842.1793,  11634.2109,
          26221.7467,  43883.9152]], dtype=torch.float64)
	q_value: tensor([[-34.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.681625049183863
epoch: 67, step: 114
	action: tensor([[-25747.2854, -19607.1774,  24952.5887,   1009.2327,  -7253.4617,
          23456.0859,  17055.6947]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08731552824322553, distance: 1.0932438025404108 entropy 11.394279083852146
epoch: 67, step: 115
	action: tensor([[-31143.5691, -18329.3154,   5168.0682,  -5002.3987, -26467.7808,
          53343.5674, -34103.5800]], dtype=torch.float64)
	q_value: tensor([[-33.2773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44239307334809364, distance: 1.3743536726880448 entropy 11.989706511649429
epoch: 67, step: 116
	action: tensor([[  2895.6145,   9083.8674, -29980.9281,  31654.1654,  21687.9322,
          62621.9571,  57594.9727]], dtype=torch.float64)
	q_value: tensor([[-37.0587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.094892104175136
epoch: 67, step: 117
	action: tensor([[-23227.2287, -24867.2554, -19227.4175,  30342.0016,   6530.5111,
          39841.3471,  -2480.2278]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8480819147852563, distance: 1.5556693734182172 entropy 11.394279083852146
epoch: 67, step: 118
	action: tensor([[ 39059.2744, -35513.3391, -73772.4587,  -6503.9080, -72765.5397,
          44678.0993,  27883.7158]], dtype=torch.float64)
	q_value: tensor([[-32.2070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11910645514754492, distance: 1.2105769310692391 entropy 11.916545805453412
epoch: 67, step: 119
	action: tensor([[-21695.7722, -23967.4965,  54471.9979, -35164.0478, -16476.9512,
         -10757.1449,  -4973.3697]], dtype=torch.float64)
	q_value: tensor([[-27.8912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8051114619479636, distance: 1.5374772746033933 entropy 11.505358321740301
epoch: 67, step: 120
	action: tensor([[ -82873.9910,   60882.5322,  -40462.2482,   35672.8071,  -36461.5894,
           49987.2631, -119662.9099]], dtype=torch.float64)
	q_value: tensor([[-34.7892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0027346633545264343, distance: 1.1427784846559585 entropy 12.054025914838972
epoch: 67, step: 121
	action: tensor([[ 5803.1728, -3007.1239, 11915.0322, 17402.3735, 28050.9941, -4668.5515,
          7655.8249]], dtype=torch.float64)
	q_value: tensor([[-23.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.341131532742745
epoch: 67, step: 122
	action: tensor([[-39520.1338,    493.0117,  -5344.8961,  -5515.9328, -18132.7266,
          -3573.3574,  12219.5613]], dtype=torch.float64)
	q_value: tensor([[-32.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2622302822150697, distance: 0.9829175873145894 entropy 11.394279083852146
epoch: 67, step: 123
	action: tensor([[-81570.4497, -35325.5101,    588.0345,  36960.0685, -14188.6551,
          13231.2223,  -3951.7880]], dtype=torch.float64)
	q_value: tensor([[-30.4961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5421827913704149, distance: 1.4210999351026923 entropy 11.585295754627102
epoch: 67, step: 124
	action: tensor([[  6711.9533, -12541.9087,  30200.6606,  55284.9633, -42668.4324,
          -7157.8831,  -9591.7683]], dtype=torch.float64)
	q_value: tensor([[-27.8283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5458229691970449, distance: 1.422776133109467 entropy 11.807107806596234
epoch: 67, step: 125
	action: tensor([[ 16962.8078, -47610.5392, -25965.7895,  28858.6954,  41006.5622,
          10245.2232, -19880.2404]], dtype=torch.float64)
	q_value: tensor([[-29.3037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2507557515505314, distance: 1.2798024805412016 entropy 11.837952188989908
epoch: 67, step: 126
	action: tensor([[-29326.8623, -47575.1697,  12782.8946,   4868.1726,  41739.7844,
         -28253.4597, -43975.9657]], dtype=torch.float64)
	q_value: tensor([[-45.0888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016579219173119553, distance: 1.1537914253822588 entropy 11.925303788750568
epoch: 67, step: 127
	action: tensor([[-7260.2231, 32821.8578,  4990.8136, 37626.8520, 10655.5842, 42481.4354,
          1076.4219]], dtype=torch.float64)
	q_value: tensor([[-33.9518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4215268342977112, distance: 0.8703587546271885 entropy 11.854759446634656
LOSS epoch 67 actor 435.5467672198746 critic 151.4277495572737
epoch: 68, step: 0
	action: tensor([[-11900.6521, -45024.9085, -22417.4283,  28142.4752,  -9593.1756,
         -61285.8849, -42318.0381]], dtype=torch.float64)
	q_value: tensor([[-28.5219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6008229441581836, distance: 1.4478659121612052 entropy 11.86850994978993
epoch: 68, step: 1
	action: tensor([[28939.4308, 19216.8750, 22991.2113, 10410.7412, 10621.1239, -4356.8366,
         68967.3043]], dtype=torch.float64)
	q_value: tensor([[-27.7992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060155143436456, distance: 0.9533043486380354 entropy 11.833434040738045
epoch: 68, step: 2
	action: tensor([[-16602.4796, -99374.8454, -13091.4499,   4162.0755,  20588.4937,
          85679.7812, -39243.2652]], dtype=torch.float64)
	q_value: tensor([[-31.8485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03470592285995178, distance: 1.12431114050347 entropy 11.87172733896959
epoch: 68, step: 3
	action: tensor([[  9904.7094, -39666.9688,  52130.2801,  50049.8761,   8569.6366,
          42157.9063,   -472.1371]], dtype=torch.float64)
	q_value: tensor([[-29.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968511137626364, distance: 0.9595781268589134 entropy 12.012101518619376
epoch: 68, step: 4
	action: tensor([[-62822.0822, -17626.9113, -14040.9208, -47171.5141,    766.1094,
         -28089.6857, -16102.2598]], dtype=torch.float64)
	q_value: tensor([[-26.9694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.729923082645141, distance: 1.505116407415729 entropy 11.876696822448476
epoch: 68, step: 5
	action: tensor([[-13426.2160, -42629.5061,    314.2154,  55557.8436,  69473.5477,
          37916.7065, -10814.1594]], dtype=torch.float64)
	q_value: tensor([[-25.0829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04117065857205482, distance: 1.1205399660733646 entropy 11.751539477582767
epoch: 68, step: 6
	action: tensor([[-15083.8473,  -8523.4078,  18073.4385, -52989.9108,   6333.0598,
          33305.9656,  13302.5086]], dtype=torch.float64)
	q_value: tensor([[-22.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9121100419166119, distance: 1.5823885597175515 entropy 11.695530800038577
epoch: 68, step: 7
	action: tensor([[-14220.9179, -18209.2836,  -7858.7883,  25092.7407,  12078.4587,
           7893.9250,  61635.7409]], dtype=torch.float64)
	q_value: tensor([[-25.8451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24874565781142977, distance: 1.2787736796105822 entropy 11.744904601530708
epoch: 68, step: 8
	action: tensor([[-16547.5786, -48006.5991,   8495.6956,  59452.4280,  50400.9414,
          45316.6738, -53028.0040]], dtype=torch.float64)
	q_value: tensor([[-28.0215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7606004511979032, distance: 1.5184031557013882 entropy 11.932551007671188
epoch: 68, step: 9
	action: tensor([[-44800.2146, -45750.9294,  -7250.3914, -31112.2830,   5079.3089,
         103548.5181,  -9373.2580]], dtype=torch.float64)
	q_value: tensor([[-29.3111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07315981573289476, distance: 1.1854654315902735 entropy 12.175274606687713
epoch: 68, step: 10
	action: tensor([[ 33499.8836, -48220.8410, -34125.1830,   7073.5917, -11546.8307,
         -43935.7730, 101443.3993]], dtype=torch.float64)
	q_value: tensor([[-28.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1343667684544011, distance: 1.064691173181343 entropy 11.901493569877672
epoch: 68, step: 11
	action: tensor([[-33222.0677, -57595.4019, -20499.5442,   4079.9707, -15002.8345,
           3088.9664, -16387.1475]], dtype=torch.float64)
	q_value: tensor([[-28.4354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07959038176282818, distance: 1.097860772023791 entropy 11.767528468344816
epoch: 68, step: 12
	action: tensor([[-19323.5623, -82324.2141,  -9645.5676,  13895.5146, -30642.7279,
          -8853.6961, -12326.6128]], dtype=torch.float64)
	q_value: tensor([[-26.4313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9872179847047831, distance: 1.6131674403008476 entropy 11.819896393560454
epoch: 68, step: 13
	action: tensor([[-30109.7549,  -9528.4374, -50942.4674,   3345.9334,  20361.0951,
           7250.5167,  25269.1195]], dtype=torch.float64)
	q_value: tensor([[-25.3499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6936006503013947, distance: 1.4892314498671055 entropy 11.804247336246988
epoch: 68, step: 14
	action: tensor([[ -8706.3320, -28523.0642,    784.4709, -16874.4224,   1148.1670,
         -28289.0023,  -3018.2977]], dtype=torch.float64)
	q_value: tensor([[-27.3082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5843484477702381, distance: 1.4403964575602826 entropy 11.95783503149963
epoch: 68, step: 15
	action: tensor([[-63183.0682,  15640.9440, -19740.0678,  53496.3151, -24019.9206,
          -3791.8677,  23998.5658]], dtype=torch.float64)
	q_value: tensor([[-26.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6211153449598585, distance: 0.704385166292797 entropy 11.806983986127323
epoch: 68, step: 16
	action: tensor([[-29647.7130, -24022.8217, -27758.9830,  -2020.6598,  55583.2168,
         -13689.8879, -46982.2125]], dtype=torch.float64)
	q_value: tensor([[-29.8331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8959444135126469, distance: 1.5756853363683143 entropy 11.867457036565098
epoch: 68, step: 17
	action: tensor([[-35792.0427, -51603.8290,  31823.2279, -26345.3296, -25153.1557,
         -30197.3972, -54142.4692]], dtype=torch.float64)
	q_value: tensor([[-29.2714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36013668214799477, distance: 1.3345903184061358 entropy 11.923847365288779
epoch: 68, step: 18
	action: tensor([[ 17922.6882, -35181.5927,  -2857.8311, -17480.3842, -54519.4948,
          15958.2424,  -1733.3698]], dtype=torch.float64)
	q_value: tensor([[-29.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3635211806122909, distance: 1.3362497517969998 entropy 11.90487704667243
epoch: 68, step: 19
	action: tensor([[ 30419.3132,  -8554.9895,   7767.5043,  55946.0129, -17416.8338,
         -16079.2710, -21293.3764]], dtype=torch.float64)
	q_value: tensor([[-26.6193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13992504144246531, distance: 1.221785144648039 entropy 11.587233422245333
epoch: 68, step: 20
	action: tensor([[-68543.9286, -30578.8892, -21047.4108, -12438.2384,  19957.6435,
         -39904.7944, -16842.0974]], dtype=torch.float64)
	q_value: tensor([[-28.9803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.713576500114844, distance: 1.4979883716047862 entropy 11.540834052778663
epoch: 68, step: 21
	action: tensor([[ 22291.0639, -47731.6943,  25877.2603, -45134.1754, -34303.9013,
          -4255.9895,  58235.0064]], dtype=torch.float64)
	q_value: tensor([[-29.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30971695044267167, distance: 0.9507586773578368 entropy 11.98119242831714
epoch: 68, step: 22
	action: tensor([[-21897.8118, -51679.8868, -33968.7530, -18644.8011,  -8680.6809,
         -44548.4618,    437.2084]], dtype=torch.float64)
	q_value: tensor([[-35.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024639061925471717, distance: 1.1301585439043227 entropy 12.03556399450107
epoch: 68, step: 23
	action: tensor([[-41983.4541,   -689.8096,  16111.1252,  28136.9102,  14972.2389,
           4471.4631,  40035.2088]], dtype=torch.float64)
	q_value: tensor([[-32.9260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8194563003572339, distance: 1.5435741889586905 entropy 12.057829471336651
epoch: 68, step: 24
	action: tensor([[-10201.0122, -37299.0006, -38700.0559, -27888.3845,   5483.1746,
         -45424.8380,  -4242.9390]], dtype=torch.float64)
	q_value: tensor([[-22.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.571049976186552, distance: 1.4343386246931342 entropy 11.725553911395718
epoch: 68, step: 25
	action: tensor([[-78558.8047, -54902.7181,  15981.9000,   4074.3348,   8820.3328,
          42625.7052,  -2107.0115]], dtype=torch.float64)
	q_value: tensor([[-30.3817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3415946722721561, distance: 1.3254622270274525 entropy 11.982518039512396
epoch: 68, step: 26
	action: tensor([[-39894.7784,  -6147.9983, -55068.8935,  50758.4803, -24842.4274,
         -20095.0049, -47942.9726]], dtype=torch.float64)
	q_value: tensor([[-25.6863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17197095345003366, distance: 1.0413086574519956 entropy 11.936900824496337
epoch: 68, step: 27
	action: tensor([[-72854.8977, -29871.3561,  43893.2737,   9146.1463,  -7925.1975,
          16024.5429,  16991.3847]], dtype=torch.float64)
	q_value: tensor([[-28.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6521968398993903, distance: 1.4709150340228478 entropy 12.112842855321857
epoch: 68, step: 28
	action: tensor([[ 32015.7907, -69274.8983, -13014.9082,  -9706.7267,  53794.0654,
          32084.6328,  13937.7758]], dtype=torch.float64)
	q_value: tensor([[-29.7435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34688751005847884, distance: 0.9248061046231875 entropy 12.077391738090006
epoch: 68, step: 29
	action: tensor([[ 12018.5532, -10705.4014, -24557.2814,  15750.6961,  13407.1379,
          20176.2268,  70692.1682]], dtype=torch.float64)
	q_value: tensor([[-29.3562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0401564119708977, distance: 1.1670944896294353 entropy 11.938689891307858
epoch: 68, step: 30
	action: tensor([[-18978.7617, -79591.8939,  80090.6866,  10431.5416,  24729.1867,
         -34622.2969,  58337.5810]], dtype=torch.float64)
	q_value: tensor([[-29.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3414092286110504, distance: 1.32537061699513 entropy 11.944008908084863
epoch: 68, step: 31
	action: tensor([[-20748.5313,  10162.0037,  19274.9545, -28560.4615, -47992.1625,
         -79232.4066, -11820.5819]], dtype=torch.float64)
	q_value: tensor([[-27.6194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6748971501856373, distance: 0.6524798158312902 entropy 11.93519247116177
epoch: 68, step: 32
	action: tensor([[-22949.1970, -31120.1395,  12184.7394,  -6533.0327,  60663.1620,
         -45644.7860,   2525.1127]], dtype=torch.float64)
	q_value: tensor([[-35.5507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2501297922899899, distance: 1.279482192399582 entropy 11.843833299845171
epoch: 68, step: 33
	action: tensor([[ 16878.3597, -33188.7959, -11876.2747, -37545.6819,  23709.8427,
          93630.2086,   7429.1166]], dtype=torch.float64)
	q_value: tensor([[-29.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.946267640567106
epoch: 68, step: 34
	action: tensor([[  8651.8534, -12851.6325,   9755.5646,  37901.5463,  35882.3475,
           4540.7678,    758.1953]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04871185731289063, distance: 1.1161247405867047 entropy 11.430181749012052
epoch: 68, step: 35
	action: tensor([[-53806.2820,  24196.6126,  -2436.7061,  37204.0461,  -9131.7553,
          42409.5341,   3391.2979]], dtype=torch.float64)
	q_value: tensor([[-27.3426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.89825724596938
epoch: 68, step: 36
	action: tensor([[-38323.0859, -18892.7692, -12309.4424,  12999.1372,  10203.3797,
          33208.7779,  -4159.4552]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01837315811568807, distance: 1.1548090140693523 entropy 11.430181749012052
epoch: 68, step: 37
	action: tensor([[-96721.2487,  29280.4352, -25441.3728, -31267.3124,  44378.0163,
          -5325.7817, -67230.4340]], dtype=torch.float64)
	q_value: tensor([[-25.8450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18772694870463102, distance: 1.031353901312433 entropy 11.818887179756263
epoch: 68, step: 38
	action: tensor([[ -8374.4450,  -7593.5506, -25750.9641,   4623.5863,   2153.3445,
          50125.6585,   -352.7049]], dtype=torch.float64)
	q_value: tensor([[-31.8815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12077668926600249, distance: 1.2114799698382883 entropy 11.561163370640076
epoch: 68, step: 39
	action: tensor([[ -25152.6425, -122171.2700,   42631.8167,   21350.6896,   -2393.2784,
           25386.0275,   -6545.6435]], dtype=torch.float64)
	q_value: tensor([[-26.0792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0025822352642457247, distance: 1.1428658159207759 entropy 12.007775347927643
epoch: 68, step: 40
	action: tensor([[-19379.4788, -12837.6535, -51302.8890, -14070.7212,  62445.2432,
         -18537.9332,  38142.0692]], dtype=torch.float64)
	q_value: tensor([[-27.7751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0078550481726851, distance: 1.6215220984110332 entropy 11.876596859591471
epoch: 68, step: 41
	action: tensor([[-12203.3221,  48590.8491,  23239.4928,  39349.8070, -59111.8964,
         -37070.3741,  33036.3131]], dtype=torch.float64)
	q_value: tensor([[-28.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0900908466013628, distance: 1.1947802630176518 entropy 11.968854666130364
epoch: 68, step: 42
	action: tensor([[-32816.0773,  21779.7863,  14958.6285,   9557.6470, -13184.5630,
          -1977.1255,  37611.0350]], dtype=torch.float64)
	q_value: tensor([[-29.8419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4175679514053898, distance: 0.8733319031232711 entropy 11.81467472295994
epoch: 68, step: 43
	action: tensor([[-35191.4159, -85642.0307,  -3091.2685,  75921.1174,  -5199.5745,
          19467.1129,  52380.4656]], dtype=torch.float64)
	q_value: tensor([[-32.2876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7380841445381858, distance: 1.5086624885715774 entropy 11.995167711835714
epoch: 68, step: 44
	action: tensor([[  -228.1743, -34205.1198, -17894.1338,   3806.0889,  21890.0910,
          46606.1446,  54321.6060]], dtype=torch.float64)
	q_value: tensor([[-30.7621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2645071753907089, distance: 1.286818637420338 entropy 12.067035102366978
epoch: 68, step: 45
	action: tensor([[ 39790.5915, -16866.0735, -60446.5988,  49219.3040, -28885.1416,
          21730.6698, -49409.4093]], dtype=torch.float64)
	q_value: tensor([[-27.2360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01680600340945182, distance: 1.134687582857719 entropy 12.006486421904725
epoch: 68, step: 46
	action: tensor([[-30255.0233,   7499.6649,  20083.5159, -66318.0694,  -3185.1416,
          37385.1819,  57934.3546]], dtype=torch.float64)
	q_value: tensor([[-27.8898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05427167698239288, distance: 1.1128583560032999 entropy 11.857131547035678
epoch: 68, step: 47
	action: tensor([[-16188.2940,  12964.9071,  60381.1063,   8515.7220, -42689.1429,
          39011.4079, -12203.9494]], dtype=torch.float64)
	q_value: tensor([[-31.6182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45206202512862603, distance: 0.8470760317060371 entropy 11.89780546624405
epoch: 68, step: 48
	action: tensor([[-39335.3804,  -1445.6125,  54545.9826,   2006.9196,  25581.0541,
         -36529.7216,  63604.3892]], dtype=torch.float64)
	q_value: tensor([[-28.6773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022079675887434203, distance: 1.1569086533119293 entropy 11.869481408723473
epoch: 68, step: 49
	action: tensor([[-66021.3492, -16517.0586, -18081.1537, -43536.3729, -60640.8563,
          34448.7335,   5013.1894]], dtype=torch.float64)
	q_value: tensor([[-30.0027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9494732958375907, distance: 1.59777395870667 entropy 11.934833097982409
epoch: 68, step: 50
	action: tensor([[  5525.9953,  31640.9914, -28611.6127,  29897.2130,  31094.7769,
         -13660.2154, -12943.9980]], dtype=torch.float64)
	q_value: tensor([[-24.5895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.776731980293988
epoch: 68, step: 51
	action: tensor([[-20407.9115, -32992.1773, -17367.5396,  24744.4720, -30102.7901,
         -16875.8559,  28594.8245]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9560416206578366, distance: 1.6004633705940596 entropy 11.430181749012052
epoch: 68, step: 52
	action: tensor([[ -7679.5233,  40548.1409,  13712.2662, -12158.2272, -19084.2609,
         -17417.9446,  83654.6471]], dtype=torch.float64)
	q_value: tensor([[-25.7094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1317091546870639, distance: 1.066324295256603 entropy 11.809047088703522
epoch: 68, step: 53
	action: tensor([[-75935.6108, -30524.5561, -19100.3714,  25645.5928,  73192.9644,
          41877.1539,  52609.0114]], dtype=torch.float64)
	q_value: tensor([[-39.1413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041810357022683764, distance: 1.1201661105567793 entropy 12.015485037427583
epoch: 68, step: 54
	action: tensor([[-19641.2135,  17882.0002,  51784.7688,  18470.1503,  -6497.9530,
         -60282.0781,  13288.6431]], dtype=torch.float64)
	q_value: tensor([[-29.0571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16440149100427537, distance: 1.0460574129061497 entropy 11.930583822233217
epoch: 68, step: 55
	action: tensor([[-20443.4474, -52192.2035,   5190.2595,  16737.8184,  27322.2184,
          -8644.6756, -26642.7058]], dtype=torch.float64)
	q_value: tensor([[-31.6577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09451701779594557, distance: 1.0889221689507236 entropy 11.94515021280302
epoch: 68, step: 56
	action: tensor([[-23638.3753,  -2220.3181,  42860.0862, -61582.9044,  17874.4752,
          69609.8113, -18114.3878]], dtype=torch.float64)
	q_value: tensor([[-29.7225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9809939827727907, distance: 1.6106392246440446 entropy 12.039651154883947
epoch: 68, step: 57
	action: tensor([[-16452.9250, -60074.1271, -57660.8063, -13210.1692,  -3542.4410,
          59704.7699,  -8439.4950]], dtype=torch.float64)
	q_value: tensor([[-24.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009604590472556795, distance: 1.1388355158326935 entropy 11.70107261527998
epoch: 68, step: 58
	action: tensor([[-17996.7217, -30944.6215,  31886.0648,  -7273.0161, -15376.7990,
          39104.3288, -35827.7338]], dtype=torch.float64)
	q_value: tensor([[-28.9173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09450212331208885, distance: 1.197195284242553 entropy 11.80279962789138
epoch: 68, step: 59
	action: tensor([[-55873.2669,  17987.2501, -27995.0275,  -2926.0499,  33551.0233,
          30932.2917,  83280.1121]], dtype=torch.float64)
	q_value: tensor([[-28.2953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4199135053318116, distance: 0.8715716002650318 entropy 11.836338647446258
epoch: 68, step: 60
	action: tensor([[ 6.6125e+01, -9.2904e+04,  2.6644e+04, -3.5567e+04, -2.4365e+04,
         -4.7327e+04, -2.6707e+04]], dtype=torch.float64)
	q_value: tensor([[-42.3983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.0320374894016
epoch: 68, step: 61
	action: tensor([[-15737.9906, -21159.6833,  25418.1878,  22366.9569,   4739.9480,
         -59020.3936,  18349.9272]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1914259964558891, distance: 1.0290028565507423 entropy 11.430181749012052
epoch: 68, step: 62
	action: tensor([[ 15786.8379, -41869.6964,  -1887.7689, -23511.6387, -76083.2217,
         -61562.8512, -43006.6319]], dtype=torch.float64)
	q_value: tensor([[-29.8491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5643507298361374, distance: 0.7553099200905113 entropy 12.107284852855171
epoch: 68, step: 63
	action: tensor([[ 34951.7194, -50817.4706,  -4098.5306,   2315.6573,  -9982.9090,
          35195.0788,  22529.3279]], dtype=torch.float64)
	q_value: tensor([[-26.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6476177529732505, distance: 0.6793032674685785 entropy 11.826227141143079
epoch: 68, step: 64
	action: tensor([[-14183.3302, -15254.9876,  31624.4745, -13383.4055,  -7280.9148,
          83553.3602, -14539.5573]], dtype=torch.float64)
	q_value: tensor([[-30.9979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14062270391201537, distance: 1.222158968903433 entropy 11.887862955661987
epoch: 68, step: 65
	action: tensor([[ -4271.1732, -11162.0833, -31620.4171, -39907.9686, -17221.2723,
          63501.1784,  -5033.1074]], dtype=torch.float64)
	q_value: tensor([[-24.1215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5739699201246704, distance: 1.4356709324512633 entropy 11.617297164040652
epoch: 68, step: 66
	action: tensor([[-50695.9050, -16600.8874, -30225.3549, -26727.5128,  15495.1379,
          12107.9675,  74334.5298]], dtype=torch.float64)
	q_value: tensor([[-29.6737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8192240951419163, distance: 1.5434756877159148 entropy 11.888937127175145
epoch: 68, step: 67
	action: tensor([[  4627.9176, -77389.1804,  18644.3984,  12336.4501,  47554.4706,
          24905.8386, -50589.7019]], dtype=torch.float64)
	q_value: tensor([[-31.2933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03275971567589109, distance: 1.1254439769407991 entropy 11.96197045316333
epoch: 68, step: 68
	action: tensor([[-16426.4760, -53536.5766, -51917.0733, -44662.2536,  -7187.5918,
          32204.2883,   1383.3752]], dtype=torch.float64)
	q_value: tensor([[-29.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3092447989460161, distance: 1.3093842778748006 entropy 12.020830371865072
epoch: 68, step: 69
	action: tensor([[-21093.4252,  -2015.0452,  -8014.1507,   6098.8699, -53725.0497,
          43588.0489, -17163.0167]], dtype=torch.float64)
	q_value: tensor([[-28.7202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8825775645044738, distance: 1.5701210376011454 entropy 11.878010142997228
epoch: 68, step: 70
	action: tensor([[ -65405.6304, -112937.7427,    5447.4653,  -31919.3771,  -47773.0369,
           72618.5242,   35920.5752]], dtype=torch.float64)
	q_value: tensor([[-31.6772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29812943029003813, distance: 1.3038141538041048 entropy 12.030382911021388
epoch: 68, step: 71
	action: tensor([[  2443.5805, -79056.3906, -20324.9385,  48124.1704,  20087.7920,
          35954.1547,    670.2654]], dtype=torch.float64)
	q_value: tensor([[-27.7014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1923652675028944, distance: 1.0284050179892645 entropy 11.818417788955921
epoch: 68, step: 72
	action: tensor([[-36320.3244,  24580.3171,  33857.6816,   4105.9223,  20702.2839,
          30346.0074,  15549.4044]], dtype=torch.float64)
	q_value: tensor([[-26.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4922646674647254, distance: 0.8154087856210939 entropy 11.621332678041389
epoch: 68, step: 73
	action: tensor([[-20673.2538,   -868.8111,  11837.6900, -54016.3856, -33544.4310,
         -51767.6581,  12196.5463]], dtype=torch.float64)
	q_value: tensor([[-29.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7994579414001617, distance: 1.5350677347827186 entropy 11.924219008588862
epoch: 68, step: 74
	action: tensor([[ 15826.6026,  23284.5310, -31571.0596,  33894.6930, -36857.4415,
          22130.5172,   4202.0354]], dtype=torch.float64)
	q_value: tensor([[-24.8531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.635716300014682
epoch: 68, step: 75
	action: tensor([[-14979.5855,  22976.9912,  27863.5905,  38445.2740,  23553.0509,
          10886.1606,  -2207.6511]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36062920661735887, distance: 1.3348319334223877 entropy 11.430181749012052
epoch: 68, step: 76
	action: tensor([[ -2192.5693,  16135.5890,  -3164.8379, -14477.4409, -24295.0876,
          43691.9966, -11630.1550]], dtype=torch.float64)
	q_value: tensor([[-22.0183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05277825635942213, distance: 1.1741542287929614 entropy 11.658692753700722
epoch: 68, step: 77
	action: tensor([[ -4484.1730, -45423.6145, -14883.2400,  13544.7979,  -4395.6412,
          16807.4899, -48096.6034]], dtype=torch.float64)
	q_value: tensor([[-34.6516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6687761964248622, distance: 1.4782767328221784 entropy 11.811631893812022
epoch: 68, step: 78
	action: tensor([[-41442.9947, -11485.1254,  -7407.3355, -14120.4275,  19425.8390,
         -26116.8165,  38836.0463]], dtype=torch.float64)
	q_value: tensor([[-28.3651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035537441848718765, distance: 1.123826786821632 entropy 11.897134873605669
epoch: 68, step: 79
	action: tensor([[-29868.5308, -17445.1576,  12308.5880,  -8288.8918,   1394.9790,
         -24711.8413,  17566.0121]], dtype=torch.float64)
	q_value: tensor([[-28.3820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12619348204802883, distance: 1.0697057572491688 entropy 11.842615509040465
epoch: 68, step: 80
	action: tensor([[-63669.0788, -44030.3790,   9173.4324,  25676.2097,  46837.7728,
         -30635.8987, -36733.8849]], dtype=torch.float64)
	q_value: tensor([[-28.3753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5023306776147183, distance: 1.4026181715758501 entropy 11.816595490683934
epoch: 68, step: 81
	action: tensor([[ -2085.2632,  13476.4921, -11120.2536,  29973.6235,  39172.9508,
            282.5123,  42741.5321]], dtype=torch.float64)
	q_value: tensor([[-29.1812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8122574624134312, distance: 0.49583597683297614 entropy 11.954229492490573
epoch: 68, step: 82
	action: tensor([[-15731.0338, -34032.5035,  30521.8728,   9538.1947,  57422.8512,
         -34712.8745,  -4717.2751]], dtype=torch.float64)
	q_value: tensor([[-29.2872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6860444725951396, distance: 1.4859055545168514 entropy 12.072797003853298
epoch: 68, step: 83
	action: tensor([[-73957.3798,  10398.2150,   -541.0971, -19767.4808,  12065.1440,
         -45773.0807, -51718.3426]], dtype=torch.float64)
	q_value: tensor([[-27.8874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07747581809025306, distance: 1.1878468744670025 entropy 11.854827194511016
epoch: 68, step: 84
	action: tensor([[-41868.5406,  12987.4584,  45045.1662,  12742.7862,  46285.5828,
          17141.3367,  -6659.7867]], dtype=torch.float64)
	q_value: tensor([[-31.5772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6190234346593252, distance: 0.7063270267801353 entropy 11.644415010597413
epoch: 68, step: 85
	action: tensor([[-17420.6535, -86638.6471,  13895.7154,   3953.0375, -15215.9298,
         -44472.6282,  -5776.9048]], dtype=torch.float64)
	q_value: tensor([[-26.8313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05538344329093059, distance: 1.1756061020634545 entropy 11.864808013226916
epoch: 68, step: 86
	action: tensor([[-111992.8305,   -4087.7094,   17833.6812,   45330.1349,   55807.7207,
           15683.9532,    9108.7820]], dtype=torch.float64)
	q_value: tensor([[-27.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04472297075480558, distance: 1.1184623276266472 entropy 11.956617114053453
epoch: 68, step: 87
	action: tensor([[-97718.1556, -36073.6638,  69156.1397,  13867.2416, -54009.9514,
          45871.9375,  65783.8422]], dtype=torch.float64)
	q_value: tensor([[-28.8239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10160530480977681, distance: 1.0846516536139 entropy 12.088950230509521
epoch: 68, step: 88
	action: tensor([[ 20322.2728, -57840.2410,  -1355.4036,  31172.3378, -66251.1564,
          36055.6830, -19285.8808]], dtype=torch.float64)
	q_value: tensor([[-28.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5481678524438858, distance: 0.7692105940678359 entropy 12.065964738391642
epoch: 68, step: 89
	action: tensor([[17979.2096, -3344.5257, 24799.5864, 42746.8246, 75630.5708, -8768.1190,
          -354.6621]], dtype=torch.float64)
	q_value: tensor([[-32.4080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0722668053522475, distance: 1.184972097213143 entropy 11.981283873094636
epoch: 68, step: 90
	action: tensor([[-23544.3957, -20252.0076,  29096.7189,  12194.5925, -36116.4212,
          -4140.7726,  -6198.3996]], dtype=torch.float64)
	q_value: tensor([[-33.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09761283987009817, distance: 1.19889536903444 entropy 11.871202420441847
epoch: 68, step: 91
	action: tensor([[ 17256.8164, -31620.5351,  33526.4572, -10890.1216, -48582.1237,
         -46515.7935,   4307.2193]], dtype=torch.float64)
	q_value: tensor([[-29.2148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24051696765901998, distance: 1.2745534348549534 entropy 12.022103564973063
epoch: 68, step: 92
	action: tensor([[  4308.9937, -15375.0367, -24478.8065,  27350.9156, -45316.0046,
         -26315.7881,  40393.6874]], dtype=torch.float64)
	q_value: tensor([[-28.2328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13908065008717196, distance: 1.2213325464959517 entropy 11.800500177062688
epoch: 68, step: 93
	action: tensor([[-39869.1866,  -5983.5159,   6113.6761, -39908.5475, -46087.7477,
          75366.0404,  58547.4842]], dtype=torch.float64)
	q_value: tensor([[-31.7826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5152081413117955, distance: 1.408616725750773 entropy 11.884189955320474
epoch: 68, step: 94
	action: tensor([[ -9626.4564,  13594.3030, -60529.7807, -48899.7318, -19567.2906,
         -12869.6020, -11559.1083]], dtype=torch.float64)
	q_value: tensor([[-22.5354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1041523914477771, distance: 1.202461560358256 entropy 11.520561419668143
epoch: 68, step: 95
	action: tensor([[  4981.3630, -51201.9110,  -5264.6390, -12193.2881,  22938.4241,
          11885.0080, -68025.6066]], dtype=torch.float64)
	q_value: tensor([[-33.1310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.423160827424369, distance: 0.8691286492309976 entropy 11.868311825425023
epoch: 68, step: 96
	action: tensor([[ 20334.9888, -59281.9208,  35748.4019,  -3336.4832,  36346.3833,
         -10929.2950,  16714.2748]], dtype=torch.float64)
	q_value: tensor([[-28.9931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5270415329905123, distance: 0.7869881546491841 entropy 11.815789017112296
epoch: 68, step: 97
	action: tensor([[-61074.4653, -49542.6278, -30423.4721,  48611.1701,  33776.2285,
         -22023.3802,  27600.8285]], dtype=torch.float64)
	q_value: tensor([[-30.6310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.278109440920578, distance: 0.9722822880488279 entropy 11.86616364378427
epoch: 68, step: 98
	action: tensor([[ 12349.5616, -89748.0395, -46584.7705, -10569.2278,  16370.7232,
         -75158.2664,   -338.3759]], dtype=torch.float64)
	q_value: tensor([[-25.3198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45662058157364616, distance: 1.3811152297560136 entropy 11.797882272585598
epoch: 68, step: 99
	action: tensor([[-8564.7926, 52173.3688, -6142.0203, 18045.0643, 28728.7310, 61391.5988,
         40913.8385]], dtype=torch.float64)
	q_value: tensor([[-32.3935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.91367356254466
epoch: 68, step: 100
	action: tensor([[ 31470.3289,  23961.9532,  11717.3792,  24919.0778,  30831.8594,
          45987.7024, -30806.8041]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6175124971807483, distance: 0.7077262728666773 entropy 11.430181749012052
epoch: 68, step: 101
	action: tensor([[-10062.2142, -25583.7948,  -8858.8851,  13912.1872,  28449.2959,
          10666.7163,   1592.9341]], dtype=torch.float64)
	q_value: tensor([[-19.3655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10951783691714267, distance: 1.0798646029936931 entropy 10.986962385933964
epoch: 68, step: 102
	action: tensor([[-71208.5700,  21464.3843, -44083.9132, -26049.6734,  -4709.2354,
            482.8228,  28643.2024]], dtype=torch.float64)
	q_value: tensor([[-25.5956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47996339848812386, distance: 0.8252274182396906 entropy 11.805978955275881
epoch: 68, step: 103
	action: tensor([[-103258.3329,    6975.1209,  -14014.8656,  -55915.2209,   28267.7920,
          -11701.4013,   13185.6145]], dtype=torch.float64)
	q_value: tensor([[-35.8322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.938526586418707
epoch: 68, step: 104
	action: tensor([[-41312.0379,   9802.1311,  -3237.2904,   5974.4576,  37097.8367,
          19692.1003, -47221.5412]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.430181749012052
epoch: 68, step: 105
	action: tensor([[ 15673.4139, -37985.6299,  19738.6087,  -2628.1387,  -7067.1064,
          19289.8458,  12488.7837]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.430181749012052
epoch: 68, step: 106
	action: tensor([[-15599.0725, -28697.2580,  15580.5850, -17091.1789, -22599.8344,
         -16850.4222,   9979.5892]], dtype=torch.float64)
	q_value: tensor([[-30.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19906684987471823, distance: 1.0241293861626273 entropy 11.430181749012052
epoch: 68, step: 107
	action: tensor([[  2122.1312, -20877.3565,  42747.9810, -65268.4883, -14248.8468,
          -2917.2780,   7982.1608]], dtype=torch.float64)
	q_value: tensor([[-31.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3530979198879066, distance: 0.9203986374636318 entropy 11.944040162133101
epoch: 68, step: 108
	action: tensor([[ 15816.7465,  46594.1358, -29529.7223,  17461.8446, -22595.3937,
          61065.4397,   5794.2646]], dtype=torch.float64)
	q_value: tensor([[-31.7487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14247051746156325, distance: 1.0596958253601818 entropy 12.05941825064005
epoch: 68, step: 109
	action: tensor([[  6693.8825, -27747.0913,  39654.9993,  12393.2001,  73929.2749,
          25265.2862, -20516.8187]], dtype=torch.float64)
	q_value: tensor([[-27.3871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02679024827229637, distance: 1.1595715754640321 entropy 11.499329007916314
epoch: 68, step: 110
	action: tensor([[-10967.2680, -44739.6090,  23459.5233,  44703.2919, -27575.9573,
          16493.5453,  31346.3329]], dtype=torch.float64)
	q_value: tensor([[-38.4796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15273564880766188, distance: 1.2286312443352534 entropy 11.946627929495653
epoch: 68, step: 111
	action: tensor([[-99699.5552, -65359.4955, -80216.4132,  46560.1999,  22428.1175,
          44350.5793,  43475.8277]], dtype=torch.float64)
	q_value: tensor([[-28.5583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.301705242845403, distance: 1.3056086546910504 entropy 12.035046745605326
epoch: 68, step: 112
	action: tensor([[-46367.8177, -31094.6380, -43156.0712, 121853.5495,  15690.6626,
         -32927.3672,   6383.9607]], dtype=torch.float64)
	q_value: tensor([[-30.8087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11685435497846675, distance: 1.0754070001614222 entropy 12.195537895941781
epoch: 68, step: 113
	action: tensor([[   677.1164, -18620.8570,  39959.0438,  18316.9666, -29406.7465,
         -60359.5812, -43794.3121]], dtype=torch.float64)
	q_value: tensor([[-32.5509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03821366328154163, distance: 1.1222664884925144 entropy 12.031265164752687
epoch: 68, step: 114
	action: tensor([[-56413.2657, -41704.1110,  -1277.7653,  72197.2603, -44557.6832,
           6104.4240,  -2322.9402]], dtype=torch.float64)
	q_value: tensor([[-26.6232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.62837480267944, distance: 1.460272410643122 entropy 11.858933354240746
epoch: 68, step: 115
	action: tensor([[-27059.6598, -30247.4625, -74706.8872, -22696.1391,  70960.2577,
         -47361.3358, -89336.3909]], dtype=torch.float64)
	q_value: tensor([[-32.1110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8536631358188982, distance: 1.558016669041088 entropy 12.106079028865773
epoch: 68, step: 116
	action: tensor([[ 47697.2349, -11719.6245,   8714.4622, -18745.3143,   6660.1660,
         -33059.6405,  20570.8193]], dtype=torch.float64)
	q_value: tensor([[-29.2833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4615158098557812, distance: 0.8397367728392359 entropy 12.000369273455789
epoch: 68, step: 117
	action: tensor([[ 22781.8649, -28212.2950, -14876.8189, -23888.4869,   5043.8334,
          29474.5783, -13776.2288]], dtype=torch.float64)
	q_value: tensor([[-27.2999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5491473262733384, distance: 0.7683764010970164 entropy 11.683776811253134
epoch: 68, step: 118
	action: tensor([[  6658.9670,  22822.8076, -65467.5902, -49349.6612, -46660.1405,
          -1562.0733,    720.8481]], dtype=torch.float64)
	q_value: tensor([[-32.0977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44848139253943653, distance: 0.8498392359640624 entropy 11.871188107609923
epoch: 68, step: 119
	action: tensor([[ 61121.1279, -41489.6074, -52046.5404,  17911.1803,  35207.7996,
          -1641.5902,  42509.3751]], dtype=torch.float64)
	q_value: tensor([[-35.6979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13843834350759465, distance: 1.2209881544862071 entropy 11.946596646069617
epoch: 68, step: 120
	action: tensor([[-27948.3685, -78125.6185,  39773.2088, -25423.2458, -19786.8692,
         -17246.8040,  35459.0478]], dtype=torch.float64)
	q_value: tensor([[-34.0797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029055382784509476, distance: 1.1608498981840392 entropy 12.053173289238433
epoch: 68, step: 121
	action: tensor([[  6697.4020, -86154.0758,  11867.1951, -10090.0347,  -4274.6062,
           8482.5225, -15329.6096]], dtype=torch.float64)
	q_value: tensor([[-27.8655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5435134575757075, distance: 0.7731623236738698 entropy 11.88145304745538
epoch: 68, step: 122
	action: tensor([[  1817.0147,  22786.6046, -44131.8052,   4179.5614, -48327.0540,
         -52210.9175, -29697.9261]], dtype=torch.float64)
	q_value: tensor([[-31.5071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22439028436101016, distance: 1.0078091883694524 entropy 11.948001707005755
epoch: 68, step: 123
	action: tensor([[ 22272.6151, -36781.9106, -22868.2689,    676.0294,  65241.9387,
          15383.7861, -28317.3447]], dtype=torch.float64)
	q_value: tensor([[-31.6522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42200555635659653, distance: 1.3646061994163634 entropy 11.936251920757297
epoch: 68, step: 124
	action: tensor([[ 24350.7037, -56935.7642, -53049.9687,  26081.2770,  -6444.7097,
          59253.6881,  27901.6161]], dtype=torch.float64)
	q_value: tensor([[-35.2500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4769136014537759, distance: 0.8276436875429973 entropy 12.000038804975162
epoch: 68, step: 125
	action: tensor([[-60333.4031, -19990.2521, -18766.8708,  -5992.4680,   -387.2776,
          37857.7769,  74282.9145]], dtype=torch.float64)
	q_value: tensor([[-32.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11868942608990984, distance: 1.0742891355464146 entropy 11.929942819790226
epoch: 68, step: 126
	action: tensor([[-34302.2826, -23206.3401,  54950.8328,  45221.4415,  -9933.6097,
          49927.3367,  -5720.6800]], dtype=torch.float64)
	q_value: tensor([[-28.5674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06399322024302767, distance: 1.107123809033713 entropy 11.942255618725184
epoch: 68, step: 127
	action: tensor([[-74279.8732, -71189.6479,  43023.8512,   8565.2489, -30792.8655,
         -53424.0445, -28545.6381]], dtype=torch.float64)
	q_value: tensor([[-28.4600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1296519387285644, distance: 1.2162672681076485 entropy 12.067411587158503
LOSS epoch 68 actor 354.23370459787805 critic 148.54721435750318
epoch: 69, step: 0
	action: tensor([[-80101.4019,   6847.2268,   3744.6011,  65380.0182, -13903.7211,
          -8073.6519, -63028.1002]], dtype=torch.float64)
	q_value: tensor([[-26.5047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6887078503988584, distance: 0.6384704142203557 entropy 11.989495353102663
epoch: 69, step: 1
	action: tensor([[  2577.9841,  28287.1624,  -2402.8480,  10641.5013, -72142.7681,
           3651.8287,  62710.5602]], dtype=torch.float64)
	q_value: tensor([[-24.5742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5628214889724279, distance: 0.7566344247063839 entropy 11.813516210904998
epoch: 69, step: 2
	action: tensor([[  2788.6516, -11712.0784,  37313.3674,  -3194.9019,  50093.4646,
          -9069.0863,  -4321.6932]], dtype=torch.float64)
	q_value: tensor([[-26.0152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23121627566763314, distance: 1.0033646208037774 entropy 11.8695746967775
epoch: 69, step: 3
	action: tensor([[-15388.1032,   6820.1907, -53479.5133,   8643.5826, -25900.6149,
         -97225.2954,  57916.0996]], dtype=torch.float64)
	q_value: tensor([[-23.5420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.88245420456111
epoch: 69, step: 4
	action: tensor([[ 14559.6586,  10215.2688,    104.8688,  11603.7543, -20305.4934,
         -30363.2648, -22080.2368]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6392408613808743, distance: 0.6873301010914233 entropy 11.465478812298787
epoch: 69, step: 5
	action: tensor([[-22225.3831, -23732.0954,  14524.7371, -19109.0400,  42298.6485,
         -11696.4763,  -5462.9368]], dtype=torch.float64)
	q_value: tensor([[-26.3543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23437799782937008, distance: 1.001299261934473 entropy 11.821555903645091
epoch: 69, step: 6
	action: tensor([[ 12169.4831, -77769.3966,   1076.9113,   3577.8888, -37580.2143,
          32666.7110,  81668.0556]], dtype=torch.float64)
	q_value: tensor([[-23.1983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14772747347654136, distance: 1.0564426787183474 entropy 11.878382880392337
epoch: 69, step: 7
	action: tensor([[-34482.4469, -62115.4520,  13644.1718, -35489.1587,  32327.7531,
          68568.1648,  48035.5929]], dtype=torch.float64)
	q_value: tensor([[-27.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7846649463867614, distance: 1.5287449662212498 entropy 11.825855785438804
epoch: 69, step: 8
	action: tensor([[ -8304.2170, -76212.3967, -20372.2204,  84627.4244,  -5224.0535,
          33235.5677,  32151.2214]], dtype=torch.float64)
	q_value: tensor([[-24.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.747979821393788, distance: 1.5129511322253901 entropy 11.912508870453532
epoch: 69, step: 9
	action: tensor([[-43976.0020,  -4378.4873, -57913.6057,  10986.4320, -47041.3283,
          75138.1010,  28051.9784]], dtype=torch.float64)
	q_value: tensor([[-25.8551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17962244990292864, distance: 1.2428771619653387 entropy 12.147654771617367
epoch: 69, step: 10
	action: tensor([[-21544.0636, -53341.7430, -42636.3017,  52438.4916,  -2914.9978,
         -49738.5043,  20850.6924]], dtype=torch.float64)
	q_value: tensor([[-24.3255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8123556197508564, distance: 1.540559238319138 entropy 12.019629391749444
epoch: 69, step: 11
	action: tensor([[-29676.0933,   3625.2556,  33986.4859,   3937.8481,  67981.2235,
          44313.4483,  55972.1911]], dtype=torch.float64)
	q_value: tensor([[-26.3789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.031687019571395
epoch: 69, step: 12
	action: tensor([[-22279.7982, -54860.8771, -26771.4387,  13661.5230, -28201.9374,
           1749.1422,  17516.3764]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8366746927331692, distance: 1.5508607838699076 entropy 11.465478812298787
epoch: 69, step: 13
	action: tensor([[-58814.5082, -27162.4655, -49527.6670,  37823.3085, -23721.3595,
          14842.7923, -38019.3074]], dtype=torch.float64)
	q_value: tensor([[-20.6508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5995636057005558, distance: 1.4472962951488426 entropy 11.95330560916156
epoch: 69, step: 14
	action: tensor([[-13807.3688, -40585.5794,  21558.0064, -68878.5071,   6691.7015,
          46002.9486,  87282.9677]], dtype=torch.float64)
	q_value: tensor([[-24.2621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7892383187686296, distance: 1.5307024895771002 entropy 12.009541528614774
epoch: 69, step: 15
	action: tensor([[-16520.7429,  30903.3578,  -4667.4643, -21433.6087, -11305.5293,
          30337.5067,  25014.0003]], dtype=torch.float64)
	q_value: tensor([[-22.9260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2672414241491029, distance: 0.9795737715795277 entropy 11.901592712942454
epoch: 69, step: 16
	action: tensor([[ -4480.1867, -57499.1233, -26601.0519,  -6900.4978,  12548.3651,
         -71283.1622, -11062.0805]], dtype=torch.float64)
	q_value: tensor([[-32.2221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047853503529051156, distance: 1.1166281705358205 entropy 11.845067152284086
epoch: 69, step: 17
	action: tensor([[ 28193.3970, -62152.2612,  31031.5597,   1196.7582,  28594.8626,
          20915.0117, -40403.5184]], dtype=torch.float64)
	q_value: tensor([[-26.7808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5244449388386196, distance: 0.7891455235413758 entropy 12.04920629893067
epoch: 69, step: 18
	action: tensor([[-43452.4082, -40398.9496,  -7342.6586, -26305.3011,  12961.7883,
         -21437.7011,   1734.5072]], dtype=torch.float64)
	q_value: tensor([[-21.9988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3488785157018517, distance: 1.329055484694612 entropy 11.706707371846448
epoch: 69, step: 19
	action: tensor([[-15912.4229, -97679.8892, -31225.5545, -42324.7147,  14196.3517,
         -60956.6723,  52549.8524]], dtype=torch.float64)
	q_value: tensor([[-21.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7087705004031966, distance: 1.4958862224762122 entropy 11.760456785277615
epoch: 69, step: 20
	action: tensor([[-81476.6267, -39953.5253,  10789.4130,  45247.9409, -78695.6877,
          29968.0543,  27106.7939]], dtype=torch.float64)
	q_value: tensor([[-26.7817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07077475216056239, distance: 1.1841473692284323 entropy 11.99125801369152
epoch: 69, step: 21
	action: tensor([[ -4652.1275, -26602.0055,  19409.1414,   8517.8207, -71391.6995,
            539.2314, -43168.9359]], dtype=torch.float64)
	q_value: tensor([[-21.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7014390413859704, distance: 1.4926737321766343 entropy 11.746845822413459
epoch: 69, step: 22
	action: tensor([[-32105.8577, -19018.7135,  -3688.0417, -23827.1428,  24174.8890,
          88649.5266,  34639.0777]], dtype=torch.float64)
	q_value: tensor([[-24.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22488928293721178, distance: 1.2664997566763683 entropy 12.101635433656678
epoch: 69, step: 23
	action: tensor([[-18638.4981,  38511.1082,  86011.5878,   5052.5407, -37226.7329,
           8556.2221,   2850.8371]], dtype=torch.float64)
	q_value: tensor([[-22.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27880647293158356, distance: 0.9718127739772935 entropy 11.913338463827392
epoch: 69, step: 24
	action: tensor([[ 32099.7675,   1727.6475, -27188.3462,  38191.1611, -11261.1236,
         -48531.2150, -17027.6336]], dtype=torch.float64)
	q_value: tensor([[-26.2728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23902831041125616, distance: 0.9982537338166473 entropy 11.819390112992721
epoch: 69, step: 25
	action: tensor([[ 37503.3872, -53889.5752,  49785.4572,  34722.6502, -39352.8442,
         -41975.6440, -32515.1413]], dtype=torch.float64)
	q_value: tensor([[-23.3871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3827733459686592, distance: 0.8990400302311025 entropy 11.734845200652774
epoch: 69, step: 26
	action: tensor([[-24308.9291, -82658.2390, -42558.6335,  -9326.1170,   -814.4554,
         -28913.9690, -67714.4497]], dtype=torch.float64)
	q_value: tensor([[-26.0524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3878101593774683, distance: 1.3480988073060358 entropy 11.981731129238355
epoch: 69, step: 27
	action: tensor([[-12557.0103,   3424.8116,  22339.6019,  14305.5611,  13695.7113,
          29188.4479,   8890.2721]], dtype=torch.float64)
	q_value: tensor([[-24.6372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38802691577340287, distance: 0.8952057313336079 entropy 11.983484498971569
epoch: 69, step: 28
	action: tensor([[-30046.1534, -76402.1559,  40898.6317,  -2387.1687,  -2489.3215,
          13909.6120, -29089.9785]], dtype=torch.float64)
	q_value: tensor([[-25.2773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9049980225046184, distance: 1.5794430016434449 entropy 11.96810409568637
epoch: 69, step: 29
	action: tensor([[ 20325.0714,  19682.4761, -21041.2164,  57383.5097,  50373.3408,
         -19552.2419,  59918.3924]], dtype=torch.float64)
	q_value: tensor([[-21.1090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.76635948700414
epoch: 69, step: 30
	action: tensor([[ 27133.8498, -65280.1374,  34652.5055,  23558.1443,  -3403.9017,
          70081.7124,   2625.5363]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3276033897166636, distance: 0.9383599230314174 entropy 11.465478812298787
epoch: 69, step: 31
	action: tensor([[-45769.9615,  -8113.4335,  -5991.3570,  -8233.1900, -18635.5448,
         -14609.3649,  -4405.4064]], dtype=torch.float64)
	q_value: tensor([[-24.9025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7752016896973388, distance: 1.5246864635572337 entropy 11.815878753596719
epoch: 69, step: 32
	action: tensor([[-14724.9359, -57339.2006,    160.0889,  19370.1393, -55340.4836,
          22205.5553,   9621.7549]], dtype=torch.float64)
	q_value: tensor([[-20.9205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2889994639975375, distance: 1.2992210896400211 entropy 11.674084178937067
epoch: 69, step: 33
	action: tensor([[-16282.7952, -14647.5951, -43417.1046,  38680.8355, -15134.4974,
          62249.4431, -36247.7709]], dtype=torch.float64)
	q_value: tensor([[-26.5988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5919588032328715, distance: 1.443851757051718 entropy 12.121916272790662
epoch: 69, step: 34
	action: tensor([[-26702.7132, -43305.8708,   6103.8095, -31155.6400, -62472.8962,
           4747.4733,  39659.8156]], dtype=torch.float64)
	q_value: tensor([[-25.2258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9389649469433605, distance: 1.5934618572288646 entropy 11.990337855761654
epoch: 69, step: 35
	action: tensor([[-23526.4283,  40602.3272, -14837.4632,  17094.3152, -13303.3000,
          -9140.5972, -52163.8954]], dtype=torch.float64)
	q_value: tensor([[-25.7884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.044304620363355474, distance: 1.1694193966336999 entropy 11.972604542907606
epoch: 69, step: 36
	action: tensor([[52753.5860, 39246.5526, 59644.4119, 74159.4881, 73631.0743,  5031.3574,
          6906.6332]], dtype=torch.float64)
	q_value: tensor([[-29.9336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7728562610547587, distance: 0.5453898841500149 entropy 12.109369423347633
epoch: 69, step: 37
	action: tensor([[-31604.1908, -50415.5094,  -8833.1644,  17976.1870,  10916.4090,
          -6179.3800,  14141.8241]], dtype=torch.float64)
	q_value: tensor([[-22.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1218056840713071, distance: 1.07238814467887 entropy 11.809741397963537
epoch: 69, step: 38
	action: tensor([[  -2553.2900,  -55960.0776, -114765.3239,  -10872.8592,   -1709.3580,
          -30058.5365,    6055.7389]], dtype=torch.float64)
	q_value: tensor([[-22.3392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4179236546634779, distance: 1.362646224148015 entropy 11.96241078130494
epoch: 69, step: 39
	action: tensor([[ -5054.7393,  29638.4274,  44149.6357,  36315.6851, -65906.6096,
          59507.6111,  79850.0199]], dtype=torch.float64)
	q_value: tensor([[-25.0593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26806463795355606, distance: 1.2886274820153358 entropy 11.939805231049702
epoch: 69, step: 40
	action: tensor([[ -71298.3260,  -60316.3532, -105157.1789,  -75243.4870,   28138.3629,
            2936.4729,   11522.3289]], dtype=torch.float64)
	q_value: tensor([[-28.8745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4490543693749047, distance: 1.3775235544731383 entropy 12.164231423219645
epoch: 69, step: 41
	action: tensor([[-25093.1542, -41349.9194,  62128.6430,  49367.6999,  34713.4786,
          51149.2922, -38553.1372]], dtype=torch.float64)
	q_value: tensor([[-24.2906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5865705242218455, distance: 1.441406194272816 entropy 11.92154452254421
epoch: 69, step: 42
	action: tensor([[-34238.6003,  -4633.9383, -44570.4125,  93444.7743,  17763.2953,
          10497.0530, -54037.0838]], dtype=torch.float64)
	q_value: tensor([[-22.9642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18327120319190138, distance: 1.0341787922896966 entropy 11.90261479136659
epoch: 69, step: 43
	action: tensor([[-26597.0593,  11805.5548, -20444.5113,  28012.0199,  23935.1411,
           3055.2569,  14966.0953]], dtype=torch.float64)
	q_value: tensor([[-23.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5238651692329601, distance: 0.7896264176217073 entropy 11.931401752280081
epoch: 69, step: 44
	action: tensor([[-116256.9468,  -49563.7452,  -54556.0499,   14828.0649,  -36899.6953,
           19183.9249,   12237.9672]], dtype=torch.float64)
	q_value: tensor([[-27.7989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08961897404512476, distance: 1.0918633590146216 entropy 11.906629205509676
epoch: 69, step: 45
	action: tensor([[-28465.1066, -83698.8436,  53377.6104,  13952.9472, -31683.5195,
         -78957.1388,  33409.4325]], dtype=torch.float64)
	q_value: tensor([[-23.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02749262449715051, distance: 1.1599681103320187 entropy 11.950844018315506
epoch: 69, step: 46
	action: tensor([[-16738.2716, -88401.5967, -10360.6324, -13053.0328,  17545.1281,
         -30453.7817,  52160.5996]], dtype=torch.float64)
	q_value: tensor([[-30.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.001364414028215, distance: 1.618899093807261 entropy 12.146593148942445
epoch: 69, step: 47
	action: tensor([[ 29664.1065,  -8203.8536,  15350.0812, -49282.8246,  67939.2339,
           2965.3172,  -5475.9788]], dtype=torch.float64)
	q_value: tensor([[-24.8290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03560668887477214, distance: 1.1237864415230772 entropy 11.92873513517796
epoch: 69, step: 48
	action: tensor([[-59200.0586, -35347.9480,  74318.2190, -53614.1690,  35813.1394,
         -25206.3568,  50245.8724]], dtype=torch.float64)
	q_value: tensor([[-26.1923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3632541252680084, distance: 0.9131450374774955 entropy 11.934354081634563
epoch: 69, step: 49
	action: tensor([[-61473.9788, -17301.0064,   1461.2304, -22542.9009, -57123.6406,
           1898.2872,  17640.5647]], dtype=torch.float64)
	q_value: tensor([[-22.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2041753690520406, distance: 1.020858105860526 entropy 11.779779840960414
epoch: 69, step: 50
	action: tensor([[-30186.4082,  15969.5417, -49784.9080,  85816.8085,  62806.0134,
          40181.1571,   -537.9964]], dtype=torch.float64)
	q_value: tensor([[-28.5836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22808871894490546, distance: 1.00540348724099 entropy 12.095471487622612
epoch: 69, step: 51
	action: tensor([[-24026.9092,  11181.6753, -49581.4819, -36013.9864, -85147.4872,
          -2504.1231,  33290.5402]], dtype=torch.float64)
	q_value: tensor([[-27.2638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5309041279256647, distance: 0.7837679475905107 entropy 11.991276810263477
epoch: 69, step: 52
	action: tensor([[  2216.1958,  32762.3921,  -2717.9560, -17406.4901, -42740.1354,
          -9565.4141, -48886.4956]], dtype=torch.float64)
	q_value: tensor([[-34.5322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5725017887210488, distance: 0.7482105786562495 entropy 11.998007711500106
epoch: 69, step: 53
	action: tensor([[ 44433.2049, -35158.5349,  80594.6429,   5997.5432, -18273.9119,
         -35357.1886,  -9672.9513]], dtype=torch.float64)
	q_value: tensor([[-29.8745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27309536645496113, distance: 1.29118110502228 entropy 12.02140111196554
epoch: 69, step: 54
	action: tensor([[-131241.7905,  -83629.8859,  -10089.1821,   88419.0860,  -62593.7880,
            7436.8853,    -862.0194]], dtype=torch.float64)
	q_value: tensor([[-28.3389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966284230880962, distance: 0.9597300663614218 entropy 12.026518138292195
epoch: 69, step: 55
	action: tensor([[-44325.3614, -22795.8820,   7300.1226,  77278.2282,  -8388.2510,
         -26016.7013,  23497.7296]], dtype=torch.float64)
	q_value: tensor([[-21.9200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7795046300189761, distance: 1.5265332013401787 entropy 11.850894534741002
epoch: 69, step: 56
	action: tensor([[ 20570.1355,   1992.5668,   8395.5677,  12098.9665, -33833.5968,
          31584.1759, -35795.6948]], dtype=torch.float64)
	q_value: tensor([[-22.4636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7007634933254241, distance: 0.6259850787676143 entropy 11.862958534526843
epoch: 69, step: 57
	action: tensor([[-17350.7971,  15334.9282,  10960.3781,  -5800.5512, -28704.1473,
          36654.1893,  30697.4200]], dtype=torch.float64)
	q_value: tensor([[-24.4472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5896234377988019, distance: 0.7330742551309065 entropy 11.674374903212769
epoch: 69, step: 58
	action: tensor([[-56099.5305,   3850.4105, -13115.3240, -47447.8004,  51541.0098,
         -22783.0850, -51148.2509]], dtype=torch.float64)
	q_value: tensor([[-27.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17294128902242734, distance: 1.0406983437028898 entropy 11.940882492715135
epoch: 69, step: 59
	action: tensor([[  5931.4652, -49841.9495,  -3306.2624,  27452.2946,  86635.0065,
           1092.8667, -11586.7868]], dtype=torch.float64)
	q_value: tensor([[-36.5331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.010827340566236
epoch: 69, step: 60
	action: tensor([[   259.9001, -40374.9530, -28005.4535,   -705.9989, -19883.8180,
          13944.8647, -20855.5188]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6272583597636889, distance: 0.6986515859697441 entropy 11.465478812298787
epoch: 69, step: 61
	action: tensor([[-120043.2023,  -62887.5543,  -11047.7554,   10342.3770,  -79182.8708,
          -66403.4017,   -2834.2278]], dtype=torch.float64)
	q_value: tensor([[-26.5468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0840357045224747, distance: 1.6519970629714347 entropy 11.866018349446394
epoch: 69, step: 62
	action: tensor([[ -3719.7128,  15723.9854, -13047.0708,  26855.4289, -12677.9036,
         -30299.2190,  -7108.3993]], dtype=torch.float64)
	q_value: tensor([[-23.1626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33883688555677116, distance: 0.9304884820942338 entropy 11.953821240663917
epoch: 69, step: 63
	action: tensor([[-68138.7312,  -4621.0673,  21727.5621,  35251.1134,  45667.0154,
         -88907.1136,  62696.1821]], dtype=torch.float64)
	q_value: tensor([[-29.0408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36293665847388557, distance: 1.335963305462481 entropy 12.038590032508276
epoch: 69, step: 64
	action: tensor([[ 76578.7324, -22775.5609, -88096.2362,  12385.0482,  27137.6452,
          -1629.6920, -50006.8744]], dtype=torch.float64)
	q_value: tensor([[-27.2728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4499342687905765, distance: 0.8487191237363685 entropy 12.043040915157366
epoch: 69, step: 65
	action: tensor([[ 11653.2887, -12319.6230, -31482.1770,   9512.8021,  21372.5922,
         -12426.9213, -32026.7935]], dtype=torch.float64)
	q_value: tensor([[-24.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41955006343037005, distance: 0.8718445906227236 entropy 11.734960741513216
epoch: 69, step: 66
	action: tensor([[-87529.9797, -46053.4590,   9893.2256, -18263.2495, -42854.1120,
         -14794.4232, -12668.2300]], dtype=torch.float64)
	q_value: tensor([[-33.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6863856830618598, distance: 1.4860559007525058 entropy 11.981272570080218
epoch: 69, step: 67
	action: tensor([[ 92717.6723, -58411.7417,  35623.9679, -44233.6078, -21326.2649,
         -75224.5202,  43559.6867]], dtype=torch.float64)
	q_value: tensor([[-25.5859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47471332411669875, distance: 0.8293825347853209 entropy 12.091267487394783
epoch: 69, step: 68
	action: tensor([[  8584.0459, -42806.4025,  -2107.8009,  14276.2931,  67300.4458,
           5545.6134,   5822.8357]], dtype=torch.float64)
	q_value: tensor([[-26.2581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13733380276008622, distance: 1.0628649454637362 entropy 11.884127385161921
epoch: 69, step: 69
	action: tensor([[-56448.0447,  17676.2756,  38769.1609,   6244.7922,   9258.8735,
          51521.7588,  -5011.3046]], dtype=torch.float64)
	q_value: tensor([[-26.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.935934196973337
epoch: 69, step: 70
	action: tensor([[-48888.0048,   -206.2412,  17906.8278, -31880.2641,  13713.1763,
           6268.5569, -45044.1711]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0150115659335186, distance: 1.1357225758181397 entropy 11.465478812298787
epoch: 69, step: 71
	action: tensor([[ 14549.0253,  -6606.1498, -30814.1281,   4160.4955,  40813.1399,
         -34927.4327,  51529.1528]], dtype=torch.float64)
	q_value: tensor([[-26.0677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46123953056766653, distance: 0.8399521664677453 entropy 11.899624022573116
epoch: 69, step: 72
	action: tensor([[ -7993.5631, -38928.4566, -33791.6919,  84842.3039, -36995.4415,
           4011.6323,   5908.0378]], dtype=torch.float64)
	q_value: tensor([[-23.9914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2606373851018081, distance: 0.9839781095573025 entropy 11.874565398428256
epoch: 69, step: 73
	action: tensor([[-10433.3339, -50358.7962,  64315.0071,  72323.1709,  13746.8187,
          19951.4462,  31503.7111]], dtype=torch.float64)
	q_value: tensor([[-22.2615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14221386335441832, distance: 1.223011122756222 entropy 11.875976210603522
epoch: 69, step: 74
	action: tensor([[ 23283.7005, -10592.6312, -11885.4250, -38232.5659,   4622.5919,
          59829.8222, -44358.3088]], dtype=torch.float64)
	q_value: tensor([[-22.7063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45113446887786457, distance: 0.8477926989841171 entropy 11.86881483847772
epoch: 69, step: 75
	action: tensor([[  -819.9176,   -428.7634, -25799.7568, -15663.9793,  11027.6141,
          66290.8595,  12776.6004]], dtype=torch.float64)
	q_value: tensor([[-22.9888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43075881072899014, distance: 1.368799720253344 entropy 11.720983443185668
epoch: 69, step: 76
	action: tensor([[-89143.6593,  32015.7376,   -979.8143,  26628.8628, -51247.1618,
          19879.0720, -10252.9573]], dtype=torch.float64)
	q_value: tensor([[-25.5202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3205661130636279, distance: 0.9432575609113587 entropy 11.948605327849917
epoch: 69, step: 77
	action: tensor([[-14904.9116,  35602.2726,  -9078.5792,  21853.4647,  -3421.5539,
          40012.0250, -34996.8090]], dtype=torch.float64)
	q_value: tensor([[-26.6553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7020744729136367, distance: 0.624612327691959 entropy 11.865493977529734
epoch: 69, step: 78
	action: tensor([[ 25420.0540, -40200.9156, -56226.9858,  24783.7968, -27437.9939,
         -22551.1084,  10484.3059]], dtype=torch.float64)
	q_value: tensor([[-28.6527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937067451294928, distance: 0.9617212730401847 entropy 11.938217753874492
epoch: 69, step: 79
	action: tensor([[-11558.7694,   4021.1016, -52339.1831, -60736.5626, -38648.7761,
         -32832.3843,  31715.4899]], dtype=torch.float64)
	q_value: tensor([[-26.5352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.781418283023303
epoch: 69, step: 80
	action: tensor([[-17118.5175, -17502.2143,  -2908.6507, -31094.4613,  -8447.2888,
         -17158.3156, -14539.8123]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6532973677067799, distance: 1.471404840497539 entropy 11.465478812298787
epoch: 69, step: 81
	action: tensor([[-35913.4731,  11149.4745,  12895.6872,   7958.6111, -14737.7733,
           7115.3998,  17418.6594]], dtype=torch.float64)
	q_value: tensor([[-23.2048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29094835420195797, distance: 0.9635974187954114 entropy 11.874407909307962
epoch: 69, step: 82
	action: tensor([[-34905.3084,  13031.6063, -84244.1873,  15260.3146, -10455.0299,
         -16097.4566, -63179.0907]], dtype=torch.float64)
	q_value: tensor([[-28.9074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6947448608322458, distance: 0.6322490499497926 entropy 11.961984195262412
epoch: 69, step: 83
	action: tensor([[-14796.8073,  55199.7070,   -128.8248,  40866.3644,   3928.4628,
          36106.5808, -38669.4727]], dtype=torch.float64)
	q_value: tensor([[-23.8108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2750725015685094, distance: 1.2921833273175818 entropy 11.734090962908292
epoch: 69, step: 84
	action: tensor([[-97901.9748,   -660.3537,  22876.3695, -11583.0754,   5654.0273,
         -60381.5281,  25117.6739]], dtype=torch.float64)
	q_value: tensor([[-25.9241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7899776513683354, distance: 1.5310187083154838 entropy 11.934166845504828
epoch: 69, step: 85
	action: tensor([[ 35809.1308,   8557.9979, -14856.8961,  -4137.6669,  -4076.5633,
         -22757.6212, -34246.2510]], dtype=torch.float64)
	q_value: tensor([[-20.7933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24045047349044335, distance: 0.9973204904741008 entropy 11.762818807873265
epoch: 69, step: 86
	action: tensor([[ -3018.3380,    817.4837, -19158.4419,  68519.0630,  11472.8156,
         -54092.3436, -26080.9365]], dtype=torch.float64)
	q_value: tensor([[-30.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15305171599799727, distance: 1.0531376506751873 entropy 11.850941503917051
epoch: 69, step: 87
	action: tensor([[ -5530.7441, -32831.2067,   1072.1472,  13233.8916,  30973.5683,
          57905.4998, -27503.2162]], dtype=torch.float64)
	q_value: tensor([[-24.6233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6447345512112257, distance: 1.4675895178442349 entropy 11.610094113837812
epoch: 69, step: 88
	action: tensor([[-59126.8184, -22684.2121,  -9637.3996, -25420.7008,    912.7991,
         -10997.1345,  42515.9999]], dtype=torch.float64)
	q_value: tensor([[-24.1795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8296415975804861, distance: 1.547888615390261 entropy 12.101125126838758
epoch: 69, step: 89
	action: tensor([[ -2147.7656, -59978.2023, -10900.5559,  21411.1735, 107337.1372,
         -14201.9875,  -1456.8734]], dtype=torch.float64)
	q_value: tensor([[-23.9573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5620498213453866, distance: 1.4302242389127096 entropy 11.871487663882776
epoch: 69, step: 90
	action: tensor([[-55154.6359, -10383.0802,  -4890.1923, -62863.0419,  22314.2862,
         -53542.9125,  -1474.4661]], dtype=torch.float64)
	q_value: tensor([[-26.4052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8780441857001746, distance: 1.5682294173390305 entropy 11.982260413292545
epoch: 69, step: 91
	action: tensor([[-22024.3744,  -2176.7845, -76278.2930, -19844.7610,  31940.8035,
          66551.9212,  19184.1643]], dtype=torch.float64)
	q_value: tensor([[-25.5422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07679607657821563, distance: 1.0995260268921354 entropy 12.020441140546865
epoch: 69, step: 92
	action: tensor([[-90277.3661,  50654.2722,  -9874.7358, 107304.9001,  40297.5546,
          90461.2253, -32136.0996]], dtype=torch.float64)
	q_value: tensor([[-27.5021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.250781758031964, distance: 0.9905145622455871 entropy 12.014927909640006
epoch: 69, step: 93
	action: tensor([[ 38234.1629,  23527.5507,  14567.3971, -45951.8968,  -8069.3855,
           6498.6570,  53954.2894]], dtype=torch.float64)
	q_value: tensor([[-27.9470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3351634981847136, distance: 0.9330697741306738 entropy 11.987899014428894
epoch: 69, step: 94
	action: tensor([[-59413.0186, -16007.8489,  38162.9856,  12076.1264, -30029.7057,
         -31407.5874,  36987.3593]], dtype=torch.float64)
	q_value: tensor([[-27.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.823460118377122
epoch: 69, step: 95
	action: tensor([[-16188.1427, -13070.6381,   3207.7555, -43616.2531, -21310.1964,
          22756.9662, -10140.0332]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.465478812298787
epoch: 69, step: 96
	action: tensor([[ -1798.3810,  29638.6350,   6592.5795,  46336.2813, -21146.7510,
           3217.9208,   6428.2132]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.465478812298787
epoch: 69, step: 97
	action: tensor([[-15189.2538, -22708.1154,   3007.3031,  -6998.8785,  42745.0503,
         -14526.9094,  38608.9955]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.248030122674175, distance: 0.9923318144313119 entropy 11.465478812298787
epoch: 69, step: 98
	action: tensor([[-35303.6636,  -6701.6720,  39632.5137, -74839.4483, -24778.9227,
         -27076.7287,  -4750.3246]], dtype=torch.float64)
	q_value: tensor([[-27.9434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02614463685713786, distance: 1.1292859461610698 entropy 12.001943266436708
epoch: 69, step: 99
	action: tensor([[ -7059.9777, -59854.1142,  36069.6566, 159155.7184,  19459.2346,
          49299.7444, -34036.6001]], dtype=torch.float64)
	q_value: tensor([[-27.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42323367906735765, distance: 1.3651953469631488 entropy 11.988408398596826
epoch: 69, step: 100
	action: tensor([[-68713.9380,   6954.5091, -10710.3797,  25627.5840, -26824.5317,
          11739.5695,  35932.0595]], dtype=torch.float64)
	q_value: tensor([[-22.2785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43692912065495726, distance: 0.8586936017191574 entropy 11.847919155283261
epoch: 69, step: 101
	action: tensor([[-28909.4325,  27361.3330,    165.0249,  -2816.8685,  15111.5374,
          -9975.8653,  -4832.3322]], dtype=torch.float64)
	q_value: tensor([[-25.5578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09778520145239178, distance: 1.0869552547571 entropy 12.016113310590503
epoch: 69, step: 102
	action: tensor([[-11334.1569,  13737.0054,  14827.1467, -15327.9380, -40412.9314,
          51056.6075, -25268.2531]], dtype=torch.float64)
	q_value: tensor([[-21.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5989947043707481, distance: 0.7246557577147791 entropy 11.496891992391046
epoch: 69, step: 103
	action: tensor([[ 52793.7374, -46232.1150, -39965.4083,  69660.8636,  17420.4577,
          -9973.4027,  59878.9588]], dtype=torch.float64)
	q_value: tensor([[-38.2149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.982869309577733
epoch: 69, step: 104
	action: tensor([[ 26024.4657, -10437.8844, -29893.4729,  30925.8232, -27007.8042,
         -28602.4668, -10521.8105]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2936991833312531, distance: 0.961726421271967 entropy 11.465478812298787
epoch: 69, step: 105
	action: tensor([[-33006.7101, -61106.5473,  -3192.4621,   7646.6896,  24663.6104,
          98017.9999,  11918.3619]], dtype=torch.float64)
	q_value: tensor([[-26.1232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7330910496404961, distance: 1.5064939189921385 entropy 12.121511654938208
epoch: 69, step: 106
	action: tensor([[-20323.4555, -85410.4003, -56670.2516,  -7648.0857,  -4442.3259,
           7169.0160, -28248.4134]], dtype=torch.float64)
	q_value: tensor([[-25.7242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8617476507175967, distance: 1.5614105179138813 entropy 12.043781364683648
epoch: 69, step: 107
	action: tensor([[-45079.7618, -16026.2118,  11525.4128,  -9888.5249, -29262.8933,
          -7796.0270, -19389.6388]], dtype=torch.float64)
	q_value: tensor([[-24.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6023758268426669, distance: 1.4485679950862966 entropy 11.851788295306976
epoch: 69, step: 108
	action: tensor([[-30743.6305, -57502.4115,  25554.4997,  43439.4212,   8838.1130,
          27621.6092,  25210.6226]], dtype=torch.float64)
	q_value: tensor([[-25.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1134792285748945, distance: 1.6636259467603935 entropy 11.991501671167669
epoch: 69, step: 109
	action: tensor([[-73848.8568, -18613.4818,   -190.6252, -91459.8590, -11890.2687,
          15650.4722, -39232.8351]], dtype=torch.float64)
	q_value: tensor([[-23.5540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10353728832336473, distance: 1.0834847628600337 entropy 12.004751270098287
epoch: 69, step: 110
	action: tensor([[   553.6081, -51136.0028, -10689.7104,  15816.6354,  19573.7108,
         -51989.4356,  -7218.1349]], dtype=torch.float64)
	q_value: tensor([[-26.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4288757872225277, distance: 1.3678986841580485 entropy 12.087053040256365
epoch: 69, step: 111
	action: tensor([[ -1880.9711, -34724.2666, -19544.3460,  12451.3037, -39496.9453,
         -62995.5535, -30040.6436]], dtype=torch.float64)
	q_value: tensor([[-21.2211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2708722401831525, distance: 1.2900532583061113 entropy 11.697577750225651
epoch: 69, step: 112
	action: tensor([[-24096.8661, -21741.7432,  17737.8544,  44029.5064, -41472.4778,
          -2546.0721, -30677.5227]], dtype=torch.float64)
	q_value: tensor([[-25.2930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018075029399408526, distance: 1.1339550656841684 entropy 12.001429672779404
epoch: 69, step: 113
	action: tensor([[ -2005.5716, -91929.9939, -19815.7019, 103503.1804,   9643.1756,
          38573.6922, -26916.6185]], dtype=torch.float64)
	q_value: tensor([[-28.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35528489785592066, distance: 1.332207863329871 entropy 12.150199338957295
epoch: 69, step: 114
	action: tensor([[ 28630.3571, -33633.8992, -19709.0622, -15184.7352,  54536.6134,
         -26735.7137,  33609.1623]], dtype=torch.float64)
	q_value: tensor([[-23.9875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3083660095444163, distance: 1.3089447625984958 entropy 11.960458884650219
epoch: 69, step: 115
	action: tensor([[  1250.4801, -40171.8293,   8906.8013, -21352.1879, -29530.2836,
          42009.2887,  24852.3543]], dtype=torch.float64)
	q_value: tensor([[-22.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4262322341978545, distance: 0.866811703158058 entropy 11.619443602751932
epoch: 69, step: 116
	action: tensor([[ 54090.2451,  -5089.7100,  34899.2866, -19390.1521, -61647.6907,
         -30292.7713, -46814.5480]], dtype=torch.float64)
	q_value: tensor([[-28.7423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47878366552944096, distance: 0.8261629259220934 entropy 11.982552126271939
epoch: 69, step: 117
	action: tensor([[ 34029.5389,   -111.1653,  39468.2552, -10827.3007,  55105.0717,
          -3802.2670, -15592.4422]], dtype=torch.float64)
	q_value: tensor([[-29.8940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043524528730455536, distance: 1.1689825390850666 entropy 12.032261237234723
epoch: 69, step: 118
	action: tensor([[-53208.9880,  -2258.1141,  44461.9426, -40229.9444,  11449.7504,
          14247.2706, -23319.1758]], dtype=torch.float64)
	q_value: tensor([[-28.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.095272659614656
epoch: 69, step: 119
	action: tensor([[ 31141.6598, -55509.8207, -15024.0503,  17404.9092,  13081.1294,
          16730.7849, -12869.4650]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.254124350896631, distance: 0.9883025312146555 entropy 11.465478812298787
epoch: 69, step: 120
	action: tensor([[-14747.8676, -21621.2168,  21949.7961, -24359.5943,  -3769.2927,
         -20149.2401,  16736.9720]], dtype=torch.float64)
	q_value: tensor([[-25.3039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31789926760514464, distance: 1.3137048449274868 entropy 11.887134757403285
epoch: 69, step: 121
	action: tensor([[-32860.2933, -26410.0789,  12288.1454,  20727.9351, -19726.4287,
          -9723.6750,  -1137.4010]], dtype=torch.float64)
	q_value: tensor([[-22.3467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16740357231059944, distance: 1.0441766202423857 entropy 11.803827180659683
epoch: 69, step: 122
	action: tensor([[-51520.4325, -25001.6025, -14867.4389,  81058.9352,  45715.1497,
         -37864.5407, -45952.1390]], dtype=torch.float64)
	q_value: tensor([[-25.4053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6718721708853765, distance: 1.4796473738549731 entropy 12.075005370984671
epoch: 69, step: 123
	action: tensor([[-77107.3802, -62982.4465, -17143.6077,  14590.8350, -58542.2353,
         -69681.3599, -34806.5486]], dtype=torch.float64)
	q_value: tensor([[-24.5796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2817557031882012, distance: 1.2955653448550322 entropy 12.054908568951408
epoch: 69, step: 124
	action: tensor([[-58432.5448, -22844.6353,  48742.6214, -44637.7076,  29628.2072,
         -21702.4101,  19423.7076]], dtype=torch.float64)
	q_value: tensor([[-28.6567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8095997539236897, distance: 1.5393875064317804 entropy 12.27030664915639
epoch: 69, step: 125
	action: tensor([[-11719.1969, -64278.2903, -11711.5336, -31286.2342,  35488.4883,
          24174.3160,  44669.4179]], dtype=torch.float64)
	q_value: tensor([[-28.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10405004731268508, distance: 1.2024058308497254 entropy 12.157488414274864
epoch: 69, step: 126
	action: tensor([[ 20495.6971,  20733.8367, -31010.5410,  41559.7141, -87213.1560,
          29727.7134, -12896.7144]], dtype=torch.float64)
	q_value: tensor([[-25.7947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.889875591272629
epoch: 69, step: 127
	action: tensor([[  3289.4241,  -7890.3894, -10106.6055,  10479.4003,  -4531.1012,
          -6980.4674, -18022.9895]], dtype=torch.float64)
	q_value: tensor([[-26.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13442761007660442, distance: 1.2188354780797734 entropy 11.465478812298787
LOSS epoch 69 actor 263.1358697204036 critic 270.3907444405005
epoch: 70, step: 0
	action: tensor([[-43696.1009,  -5934.7862,  -8083.9821,  51751.6587,  44754.8460,
           4219.0826,  15862.5002]], dtype=torch.float64)
	q_value: tensor([[-23.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2816530594399097, distance: 0.969892981191582 entropy 12.157915100178444
epoch: 70, step: 1
	action: tensor([[ 15607.3889, -38046.2586, -72125.9643,  46003.6057,  -8495.9735,
          -8906.5610, -42728.5101]], dtype=torch.float64)
	q_value: tensor([[-23.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32352367325449116, distance: 0.9412023325727505 entropy 12.135681091368328
epoch: 70, step: 2
	action: tensor([[  3015.7684, -24147.1116, -62938.5104,  85042.9710,  12852.9392,
          48161.1820,  17850.0771]], dtype=torch.float64)
	q_value: tensor([[-26.0037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6169253027146633, distance: 0.7082693148920726 entropy 12.121271940406668
epoch: 70, step: 3
	action: tensor([[-72834.0270, -10439.0775,  47200.1171,  61806.8199,  47026.8967,
         -15454.8946,  -6851.5410]], dtype=torch.float64)
	q_value: tensor([[-22.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.94481414102649
epoch: 70, step: 4
	action: tensor([[ 26781.8980,   9288.1647, -18633.7839,  41971.8050, -53275.4276,
          16308.0485,  18219.3232]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39199699164839885, distance: 0.8922972557835912 entropy 11.500287151164553
epoch: 70, step: 5
	action: tensor([[-42003.3038, -47691.7669,  15916.2490,  13121.7661, -43683.6028,
         -60978.8447, -47393.0643]], dtype=torch.float64)
	q_value: tensor([[-26.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15144762910630682, distance: 1.0541344796496006 entropy 11.960825879638735
epoch: 70, step: 6
	action: tensor([[ 18240.3770, -56985.5878, -85673.4690,  -3351.8689,   4595.1274,
         -32122.5733, -44218.0745]], dtype=torch.float64)
	q_value: tensor([[-24.2095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.376750319882323, distance: 1.342716377918393 entropy 12.168613088897837
epoch: 70, step: 7
	action: tensor([[ -6269.0567, -27180.2887,  23646.3030,  34164.7638,    186.5872,
          27392.2428,  42415.9431]], dtype=torch.float64)
	q_value: tensor([[-25.5716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21598233096537223, distance: 1.0132570133014207 entropy 11.973166942237844
epoch: 70, step: 8
	action: tensor([[ 48152.7491, -42102.4601,  19645.0222,  52002.9217, -57674.7225,
          27465.5601, -67450.4915]], dtype=torch.float64)
	q_value: tensor([[-23.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07709246150731475, distance: 1.1876355428424352 entropy 12.125513030136698
epoch: 70, step: 9
	action: tensor([[-76074.7898,  34245.7920, -11201.7460, 102326.9835, -39291.3594,
         -26719.9906, -14408.0304]], dtype=torch.float64)
	q_value: tensor([[-23.3872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1365001748439637, distance: 1.063378365337811 entropy 12.017768310873635
epoch: 70, step: 10
	action: tensor([[ -5969.1339, -27515.8860, -44559.7551,  14151.8175, -10593.7551,
         -32709.3768,  27295.2656]], dtype=torch.float64)
	q_value: tensor([[-24.1098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.579415848773642, distance: 1.438152495151362 entropy 12.03418684932636
epoch: 70, step: 11
	action: tensor([[ 56828.7843, -12563.4019, -76235.8275,  51787.3855,  22198.8016,
          46225.0269,  40891.2475]], dtype=torch.float64)
	q_value: tensor([[-21.6729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.889352149243908
epoch: 70, step: 12
	action: tensor([[-22998.7322, -12043.7818, -13134.3908, -18386.2497,  17382.2006,
         -15067.0739,   1064.1743]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44422806477825505, distance: 1.375227611171951 entropy 11.500287151164553
epoch: 70, step: 13
	action: tensor([[ 17290.3436,  24331.9073,  48298.5559,  43353.6367,  49868.3387,
          42671.9399, -27808.5199]], dtype=torch.float64)
	q_value: tensor([[-28.8670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.190592491872037
epoch: 70, step: 14
	action: tensor([[ 25822.7719, -33024.6004, -31413.5166,   7319.7105, -18501.9887,
          -6564.8845,  -2190.9120]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21803398992660483, distance: 1.0119303725175148 entropy 11.500287151164553
epoch: 70, step: 15
	action: tensor([[-20143.2354, -58018.9812, -10769.9329,  76651.7902, -27202.6897,
          32089.6191,  57145.9322]], dtype=torch.float64)
	q_value: tensor([[-28.5314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7022700338918066, distance: 1.4930382028503453 entropy 12.023150682380656
epoch: 70, step: 16
	action: tensor([[ -58412.2908, -105085.3076,   -6647.9515,  -12634.9030,     262.7429,
            2297.4296,  -41755.7121]], dtype=torch.float64)
	q_value: tensor([[-24.0769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7865366719294398, distance: 1.5295464166997272 entropy 12.137776199720705
epoch: 70, step: 17
	action: tensor([[ -8274.3176,  -1903.2549,  38119.5940,  19966.3722,  44473.2928,
         -25113.1779, -30361.6010]], dtype=torch.float64)
	q_value: tensor([[-21.7974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7796824498032442, distance: 1.526609470044145 entropy 11.859854096457743
epoch: 70, step: 18
	action: tensor([[-64786.6228,  -9516.9040,  11427.4330, -34156.1872,  32041.6261,
         -27239.9653, -36290.2432]], dtype=torch.float64)
	q_value: tensor([[-22.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9862184860531777, distance: 1.612761706887526 entropy 12.036035778170318
epoch: 70, step: 19
	action: tensor([[-91251.0268,  37053.4554, -78907.5164,  94219.4553,  40233.7189,
          75275.9174,  14583.9014]], dtype=torch.float64)
	q_value: tensor([[-26.1050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14392540603956006, distance: 1.2239270861525384 entropy 12.150197329870664
epoch: 70, step: 20
	action: tensor([[   373.9415,  -6550.5106, -15473.0916,   5354.6647, -36899.7742,
          35470.2154,  63409.7176]], dtype=torch.float64)
	q_value: tensor([[-30.6891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03534340414149195, distance: 1.1643911711953103 entropy 12.157677865634046
epoch: 70, step: 21
	action: tensor([[-60720.2449, -62913.2791,  35773.3803,  43969.7134,   3246.0292,
          25121.0981,  73520.8589]], dtype=torch.float64)
	q_value: tensor([[-24.9092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16606241351301376, distance: 1.2357129317676532 entropy 11.902623077029077
epoch: 70, step: 22
	action: tensor([[-49005.9539, -67276.0894, -33688.4053, -22933.2318, -33520.4688,
         -15239.6584,   9066.0640]], dtype=torch.float64)
	q_value: tensor([[-26.6028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6139745819305473, distance: 1.4538012525436919 entropy 12.266113672132706
epoch: 70, step: 23
	action: tensor([[-32576.0179, -39744.2460,  37657.5609,  12628.8633,  -7345.7236,
         -34820.6930,   4488.0076]], dtype=torch.float64)
	q_value: tensor([[-20.7146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46499540330945965, distance: 1.3850798915623939 entropy 11.79251607616583
epoch: 70, step: 24
	action: tensor([[  1808.4970,  -6559.5960,  12812.4417,  50174.8894, -41834.6940,
         -27913.7572,  18678.4880]], dtype=torch.float64)
	q_value: tensor([[-23.3758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0780861829631172, distance: 1.098757506838612 entropy 12.018019011910537
epoch: 70, step: 25
	action: tensor([[-20701.0783, -42278.6640,  -3860.3891,  20007.6317, -27905.8453,
           2200.5337, -28144.2888]], dtype=torch.float64)
	q_value: tensor([[-23.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015225989407304352, distance: 1.153023229054284 entropy 11.952418290180884
epoch: 70, step: 26
	action: tensor([[-6.8424e+01, -5.6763e+04, -3.1404e+04,  4.2800e+04, -7.1990e+04,
         -2.7234e+03,  5.7756e+04]], dtype=torch.float64)
	q_value: tensor([[-23.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23666874719363984, distance: 0.9998001906396535 entropy 12.163730608485464
epoch: 70, step: 27
	action: tensor([[-71206.9937,  12641.3397,  36189.9489,  35042.4945,  12180.1138,
          23496.8550, -44786.0541]], dtype=torch.float64)
	q_value: tensor([[-25.8488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05379195279887261, distance: 1.1131405709740925 entropy 12.183612125494975
epoch: 70, step: 28
	action: tensor([[ 41548.6939, -43520.9499,  32235.5326,  48339.4887, -46427.3306,
         -40640.9826,  30805.6112]], dtype=torch.float64)
	q_value: tensor([[-27.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3991073768903022, distance: 1.3535746633069998 entropy 12.117003363399443
epoch: 70, step: 29
	action: tensor([[-71388.3803, -43098.3107,  28331.0101,  62569.0254,   4864.1363,
         -45497.4128, -15695.9180]], dtype=torch.float64)
	q_value: tensor([[-24.4797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4783894142581562, distance: 1.3913971689441238 entropy 11.927184103682814
epoch: 70, step: 30
	action: tensor([[ 28217.0378,  15226.3168,  23941.1741,   3235.4127,   2965.8363,
          -7816.5566, -14474.6118]], dtype=torch.float64)
	q_value: tensor([[-22.1591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.88605256264128
epoch: 70, step: 31
	action: tensor([[-41678.4397, -30669.6381,   7821.3470,  14873.7012, -11938.1132,
         -37250.7340, -37909.3017]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8900787493776152, distance: 1.5732460241884336 entropy 11.500287151164553
epoch: 70, step: 32
	action: tensor([[-12438.6880, -55457.5541, -51503.8824,  71879.8955, -13659.0778,
          86687.3507,  92484.0087]], dtype=torch.float64)
	q_value: tensor([[-23.0732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10151602080879263, distance: 1.0847055495428635 entropy 12.060919778555268
epoch: 70, step: 33
	action: tensor([[-36138.5709,   4399.3320,   8096.0752,  10257.3441,  64203.5184,
           9958.0055,  27347.3854]], dtype=torch.float64)
	q_value: tensor([[-22.5010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16307569142369183, distance: 1.0468869457411063 entropy 12.003478834532356
epoch: 70, step: 34
	action: tensor([[-17895.6020, -22518.0406,  29345.5583,  20519.8951, -46396.6538,
          61953.7397, -84112.5049]], dtype=torch.float64)
	q_value: tensor([[-28.9351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022929424307106427, distance: 1.131148595659349 entropy 12.141271547465932
epoch: 70, step: 35
	action: tensor([[-38482.8615, -40238.1855,  12960.9574, -25015.7784, -64776.5591,
          35317.6809, -85236.4510]], dtype=torch.float64)
	q_value: tensor([[-26.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15666761301594012, distance: 1.230724881993072 entropy 12.244427223198553
epoch: 70, step: 36
	action: tensor([[ 42385.7622,   7505.6103, -37701.4254,  11848.1880, -21955.9566,
          -9150.8396,  20155.6227]], dtype=torch.float64)
	q_value: tensor([[-25.3196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.08665523969734
epoch: 70, step: 37
	action: tensor([[  2014.4458, -21055.3451, -29623.5432,  25835.3711, -30774.7256,
         -17878.3296,  46589.5055]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2615481046000896, distance: 1.2853121146515294 entropy 11.500287151164553
epoch: 70, step: 38
	action: tensor([[-8748.7722, 11707.3393, 93722.0520, 87092.9191, -1116.4770, 58383.2518,
          6893.6708]], dtype=torch.float64)
	q_value: tensor([[-23.2669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1143871622713275, distance: 1.2080217215887188 entropy 12.031744299604584
epoch: 70, step: 39
	action: tensor([[ -5512.1630,   3846.5680, -88262.5542, -21135.8848, -40604.9483,
           6691.0546,  70546.5638]], dtype=torch.float64)
	q_value: tensor([[-26.1687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6382691737079109, distance: 0.6882551241892236 entropy 11.967892824077973
epoch: 70, step: 40
	action: tensor([[ 25928.4452, -19874.1300, -42809.5596,  27668.0532,  17819.7183,
          -8031.1071,  39359.5470]], dtype=torch.float64)
	q_value: tensor([[-35.1000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3366523352871187, distance: 1.3230185212260255 entropy 12.114612837843834
epoch: 70, step: 41
	action: tensor([[-30702.4689, -93886.8496,   9476.0833, -46275.6650,  57101.7389,
          10754.6504, -61292.2201]], dtype=torch.float64)
	q_value: tensor([[-22.3874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.776060743924196, distance: 1.5250553314833795 entropy 11.942936920791626
epoch: 70, step: 42
	action: tensor([[  2296.4114,  -8585.9285, -11267.9392,  27470.9968, -18319.5423,
           7960.0195,  26519.4697]], dtype=torch.float64)
	q_value: tensor([[-20.6227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24491487398330114, distance: 1.276810722533983 entropy 11.77836733838163
epoch: 70, step: 43
	action: tensor([[ 42265.5559, -16161.8876, -18527.8401,    994.0474, -13641.3582,
          10375.0338,  43477.3452]], dtype=torch.float64)
	q_value: tensor([[-25.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4146752066667794, distance: 0.8754979902966525 entropy 12.12123325451255
epoch: 70, step: 44
	action: tensor([[-17318.2139, -33429.0887,  43102.6035,  25482.1449,  78332.8570,
          22377.3605, -13643.6031]], dtype=torch.float64)
	q_value: tensor([[-27.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2500033142071445, distance: 0.9910290049812227 entropy 11.976326653606517
epoch: 70, step: 45
	action: tensor([[-28979.1284, -82676.9097,  31362.0579, -38126.1087,  27104.4470,
         -54255.7027,  20569.6573]], dtype=torch.float64)
	q_value: tensor([[-21.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.575017100429233, distance: 1.4361484372402187 entropy 12.006264672845628
epoch: 70, step: 46
	action: tensor([[ -2864.8021, -26949.7395,  71118.8093,  42116.9250,  -7630.2566,
           2923.1109,  14287.0562]], dtype=torch.float64)
	q_value: tensor([[-20.7570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8986745681303296, distance: 1.5768194194701872 entropy 11.846511779596733
epoch: 70, step: 47
	action: tensor([[  5230.5465,   3638.3853,  36335.6688,  46729.5856, -32364.6616,
         -14275.8622, -32790.1807]], dtype=torch.float64)
	q_value: tensor([[-20.8819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35809081393271536, distance: 0.9168398651912116 entropy 12.034175196150153
epoch: 70, step: 48
	action: tensor([[-13924.9632, -53910.8112,   -346.6292,  48562.9929,  27095.0758,
          35508.0490,  23333.3226]], dtype=torch.float64)
	q_value: tensor([[-21.6906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7081397531167759, distance: 1.4956101136421458 entropy 11.638867661861482
epoch: 70, step: 49
	action: tensor([[-14370.1164,  10664.8293, -41872.8115, -30220.5419, -19745.7073,
         -45432.7445,  -9344.3331]], dtype=torch.float64)
	q_value: tensor([[-23.1727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0023466670274117085, distance: 1.1430007678939869 entropy 12.019643287871165
epoch: 70, step: 50
	action: tensor([[ 51679.9362,  -7410.0587, -45641.7019,  22317.2877,  52788.4785,
          34601.1310, -17116.2476]], dtype=torch.float64)
	q_value: tensor([[-28.7106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26591614530650387, distance: 0.9804592077771547 entropy 11.916675578081149
epoch: 70, step: 51
	action: tensor([[ 11594.9507, -72813.5844,  16610.5057, -20420.6005, -23608.9501,
          17818.9168, -17935.9896]], dtype=torch.float64)
	q_value: tensor([[-23.2703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10555645909022748, distance: 1.0822638676550234 entropy 12.031957323466811
epoch: 70, step: 52
	action: tensor([[ -9318.1245, -55392.6220,   8216.2144,  14587.3221,  14674.6736,
         -21503.2395,  48827.5274]], dtype=torch.float64)
	q_value: tensor([[-26.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13786567297471186, distance: 1.0625372443383523 entropy 12.152840319945124
epoch: 70, step: 53
	action: tensor([[ -8045.8214, -16792.6685,  27288.3827, -33164.7873, -72615.3669,
         -62525.3474, -22522.9488]], dtype=torch.float64)
	q_value: tensor([[-23.5855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17627726275004152, distance: 1.2411136280122173 entropy 12.17488070044642
epoch: 70, step: 54
	action: tensor([[ 71053.3099,  13478.9393,  30486.9783,  35002.5751, -19796.7841,
         -13488.1795,  25226.7221]], dtype=torch.float64)
	q_value: tensor([[-23.5027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.940701041932881
epoch: 70, step: 55
	action: tensor([[-32807.4870,  18627.9435, -41049.7422,  -7247.7186, -13009.5926,
          41910.8624,   3567.1806]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3331947190495437, distance: 0.9344503017683313 entropy 11.500287151164553
epoch: 70, step: 56
	action: tensor([[-83100.3691,  22295.2182,  42022.2862,  -7781.6496, -28963.7162,
          64468.3485, -58189.7273]], dtype=torch.float64)
	q_value: tensor([[-35.1843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30937314388976367, distance: 0.9509954181935419 entropy 12.059409760832382
epoch: 70, step: 57
	action: tensor([[-40626.3231, -17571.3197, -12359.4060,  17580.0051,  -7474.4321,
         -29752.1930,  27569.3443]], dtype=torch.float64)
	q_value: tensor([[-31.2758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7704198340847337, distance: 1.5226315570607678 entropy 11.856626176534743
epoch: 70, step: 58
	action: tensor([[ 33500.5348,   8497.5073, -23317.0167,   7039.4836,  47099.6792,
          19218.7973,  67263.9643]], dtype=torch.float64)
	q_value: tensor([[-21.6022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47951717395260374, distance: 0.825581391173098 entropy 11.921961194890368
epoch: 70, step: 59
	action: tensor([[ 36026.7567, -25207.5171,  88018.5482, -15386.8653, -27148.2528,
          -7205.7247,  -1958.0637]], dtype=torch.float64)
	q_value: tensor([[-24.8623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5642250138179428, distance: 1.431219703548059 entropy 11.94419897363993
epoch: 70, step: 60
	action: tensor([[-36866.9182,  -7109.8040,   4092.2457,  31775.4524,  31694.8136,
          18049.2376,  -7040.8981]], dtype=torch.float64)
	q_value: tensor([[-22.3206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1866120938019482, distance: 1.246553947719969 entropy 11.891738016633843
epoch: 70, step: 61
	action: tensor([[-50762.2433, -50918.7157,  14654.6707,  63191.6233, -28690.4550,
         -20484.1300,  26418.1162]], dtype=torch.float64)
	q_value: tensor([[-25.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11605035423957322, distance: 1.2089228549619815 entropy 12.13477725746302
epoch: 70, step: 62
	action: tensor([[-23657.4299, -68187.2211, -62383.7213, 120998.5399,   7526.4580,
         -15051.7571,  62679.9587]], dtype=torch.float64)
	q_value: tensor([[-24.1506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15932325233819755, distance: 1.2321369071569601 entropy 12.121837792051078
epoch: 70, step: 63
	action: tensor([[-26371.6288,   7735.6395, -22876.6008,   2636.3437, -58426.2709,
           9874.9448, -26733.0239]], dtype=torch.float64)
	q_value: tensor([[-23.0341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2951320484309736, distance: 0.960750403831773 entropy 11.998055416016541
epoch: 70, step: 64
	action: tensor([[ 17211.9403, -44521.9601, -20168.5961,  44724.0728, -24481.3944,
         -23170.5401,  56377.2310]], dtype=torch.float64)
	q_value: tensor([[-24.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4160102380857982, distance: 0.8744989854964703 entropy 11.808609304089734
epoch: 70, step: 65
	action: tensor([[ 14879.0973, -34862.3050, -54670.3632,   3232.4007, -25155.8931,
         -44981.6790,   -851.7435]], dtype=torch.float64)
	q_value: tensor([[-22.6008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43155332284403813, distance: 0.862782960343139 entropy 11.985065035122947
epoch: 70, step: 66
	action: tensor([[ -4362.5204, -57175.2463, -57561.4673, -65735.8186,  75757.7779,
         -47200.9285,  24464.4033]], dtype=torch.float64)
	q_value: tensor([[-28.2325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2821827453633725, distance: 1.2957811484464605 entropy 12.022873039042462
epoch: 70, step: 67
	action: tensor([[-36797.5484,  -4000.3045,   4071.0120,   -295.3790,   4094.5340,
           -452.1821, -13647.2329]], dtype=torch.float64)
	q_value: tensor([[-22.0177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21923856670543507, distance: 1.263575041866872 entropy 11.849948905563013
epoch: 70, step: 68
	action: tensor([[ 15345.5656, -57153.3741, -62301.4474,  17243.9752, -24855.1928,
         -19821.6804,  13508.6341]], dtype=torch.float64)
	q_value: tensor([[-23.7716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38917661996719877, distance: 0.8943644315604148 entropy 11.964692164001164
epoch: 70, step: 69
	action: tensor([[ 30871.1069,  22587.4254, -40894.6102,   8364.1095,   5671.7529,
          -5980.0444,  16540.1331]], dtype=torch.float64)
	q_value: tensor([[-22.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6472232743496757, distance: 0.6796833882981629 entropy 11.618814561561408
epoch: 70, step: 70
	action: tensor([[  50859.6705, -109761.7606,  -55285.8263,   51353.9490,   13933.7696,
           81990.2686,  -20714.1604]], dtype=torch.float64)
	q_value: tensor([[-26.2275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3370668629668725, distance: 0.931733171014043 entropy 12.05706248946639
epoch: 70, step: 71
	action: tensor([[-59907.3567,  20451.5501,  -2208.4085,  51611.9762, -30290.9158,
          16213.9056,   5847.0053]], dtype=torch.float64)
	q_value: tensor([[-24.5509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1817586315084997, distance: 1.035135992513802 entropy 11.980492493174255
epoch: 70, step: 72
	action: tensor([[-53233.5604, -18955.9221,  -8224.3562,  -3271.2315,  24914.2991,
          15362.8263, -60418.9616]], dtype=torch.float64)
	q_value: tensor([[-23.5810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27422632904386113, distance: 1.2917544922939428 entropy 12.00764492444891
epoch: 70, step: 73
	action: tensor([[ 29815.1614, -68341.9857, -14413.9924,  51881.1404,  15666.9115,
           5562.2168,  27134.6928]], dtype=torch.float64)
	q_value: tensor([[-20.5960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40186971496087853, distance: 0.8850230656938755 entropy 11.730894849033376
epoch: 70, step: 74
	action: tensor([[-51361.3034,  15290.4661,  43867.5402,  39655.6377,  28098.3235,
          -9525.4173,   3827.1443]], dtype=torch.float64)
	q_value: tensor([[-25.6052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.045508793845931
epoch: 70, step: 75
	action: tensor([[ 26583.7414,  32307.7649,  13704.9348,  -8454.1711, -12065.7739,
         -39403.1706, -19598.9219]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.500287151164553
epoch: 70, step: 76
	action: tensor([[-13649.2680, -32613.9202, -13846.7165, -11810.7421,  40623.0667,
         -26170.0276, -27756.3210]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49906337269408163, distance: 1.401092117518308 entropy 11.500287151164553
epoch: 70, step: 77
	action: tensor([[-37297.3750, -15868.9623,  48347.3770,  22573.6957,  75975.9488,
         -61291.5972, -14859.7943]], dtype=torch.float64)
	q_value: tensor([[-25.2921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.850071056019988, distance: 1.5565063530623653 entropy 12.042881049959627
epoch: 70, step: 78
	action: tensor([[ 45092.3693,  31800.3820, -31840.2300, -23352.7804,   5796.4315,
         -23241.0135,  22998.7858]], dtype=torch.float64)
	q_value: tensor([[-20.0837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.734433898229856
epoch: 70, step: 79
	action: tensor([[-19739.0864,   6094.6527,   1157.4229,  -3603.3338,  15225.6055,
            430.7600, -15491.5678]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2719759340792749, distance: 0.9764040253224107 entropy 11.500287151164553
epoch: 70, step: 80
	action: tensor([[-10731.1805, -38891.5391,  26644.5808, -49879.0483, -40314.1450,
         -42758.0694, -64841.0845]], dtype=torch.float64)
	q_value: tensor([[-31.2090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2493510645109258, distance: 1.2790836243691426 entropy 11.834379907881255
epoch: 70, step: 81
	action: tensor([[-38378.3350, -24949.8849,  30123.0097, -42090.7731,  10646.5046,
          -8454.8537, -28666.2775]], dtype=torch.float64)
	q_value: tensor([[-25.2284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4077564724370186, distance: 1.3577520264645038 entropy 12.001470498755962
epoch: 70, step: 82
	action: tensor([[  7493.3213, -59743.2570,  65007.4338, -14496.5130, -53168.9872,
         -54696.3571,  62572.2540]], dtype=torch.float64)
	q_value: tensor([[-26.9552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03750616654204986, distance: 1.1226791861400378 entropy 12.17338978033987
epoch: 70, step: 83
	action: tensor([[-51818.4746,  -7031.9418,  45826.6746,  33393.7990,  -9173.7092,
          21950.1487, -16333.0510]], dtype=torch.float64)
	q_value: tensor([[-23.2352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.060313752804124654, distance: 1.1783488722437054 entropy 11.83124279245062
epoch: 70, step: 84
	action: tensor([[-5.4065e+04, -5.3211e+04, -9.7768e+00,  3.7109e+04, -6.4875e+04,
         -5.0938e+04, -6.0647e+03]], dtype=torch.float64)
	q_value: tensor([[-23.6400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25603573258356427, distance: 1.2825009356238684 entropy 12.14952024195938
epoch: 70, step: 85
	action: tensor([[-21819.0372, -59210.8531, -33929.3562,  60404.1854, -31551.1186,
         -26493.4400, -38663.5950]], dtype=torch.float64)
	q_value: tensor([[-23.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011000895008129397, distance: 1.1380324416026566 entropy 12.059958392796016
epoch: 70, step: 86
	action: tensor([[-13812.1905, -24372.3336,  41610.9769,  13927.0760, -25826.1156,
         -23608.4259,  13944.1732]], dtype=torch.float64)
	q_value: tensor([[-28.5195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06446533360347839, distance: 1.1806534878891204 entropy 12.203778651368603
epoch: 70, step: 87
	action: tensor([[-94664.3054, -21451.3159, -11723.2267, -33476.9413,  58347.6288,
          52524.1949,   3034.0176]], dtype=torch.float64)
	q_value: tensor([[-24.9293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3367101348046795, distance: 1.3230471258988985 entropy 12.177039418104574
epoch: 70, step: 88
	action: tensor([[-33344.9814, -60552.2256,  44314.2049,  -3192.2167, -44223.6575,
          21727.5849,  25915.8567]], dtype=torch.float64)
	q_value: tensor([[-20.9781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019319890779093685, distance: 1.1332360369409649 entropy 11.787768331264028
epoch: 70, step: 89
	action: tensor([[ -1887.7096, -79606.2504,  47499.4129,   5666.9023,   8756.4873,
            588.2027, 105464.1389]], dtype=torch.float64)
	q_value: tensor([[-26.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5876001972587763, distance: 1.4418738496161667 entropy 12.08634938435232
epoch: 70, step: 90
	action: tensor([[-36240.0746,  40624.1830,  61157.4582,  10668.6862,   2934.1293,
         -79800.9663,  37795.8798]], dtype=torch.float64)
	q_value: tensor([[-24.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31507452146287207, distance: 1.3122962117463168 entropy 12.051262691360833
epoch: 70, step: 91
	action: tensor([[-58846.2595,  14837.9467, -54188.3166, -72097.2077,  24477.6343,
         -19326.6278, -15984.9972]], dtype=torch.float64)
	q_value: tensor([[-29.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5832307107726994, distance: 0.7387619992542699 entropy 12.17054530119181
epoch: 70, step: 92
	action: tensor([[  -700.4934, -26693.9477,   9031.0594, -18049.9466,  64729.1804,
          34142.2367,  17032.6991]], dtype=torch.float64)
	q_value: tensor([[-30.5628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9635527794774597, distance: 1.6035332991990934 entropy 11.958481816323822
epoch: 70, step: 93
	action: tensor([[ -5231.8313,  56714.7652,  34295.0262,   8830.7297,  41042.1667,
         -19860.1396,  27632.7156]], dtype=torch.float64)
	q_value: tensor([[-23.6346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3239076466021501, distance: 0.9409351776896973 entropy 11.92659201342427
epoch: 70, step: 94
	action: tensor([[-27040.5150, -23801.6144, -33119.1655,  34856.8895,  45298.5267,
          25740.6266,  -3657.1772]], dtype=torch.float64)
	q_value: tensor([[-22.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9244594803328068, distance: 1.5874902951192114 entropy 11.653462783530031
epoch: 70, step: 95
	action: tensor([[-59777.7873, -25986.7199,  -5128.1384,   2868.4714, -19413.1165,
          42164.0051,    304.2186]], dtype=torch.float64)
	q_value: tensor([[-22.8557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.717958987288879, distance: 1.4999027077102587 entropy 12.033469918055951
epoch: 70, step: 96
	action: tensor([[-30931.6895, -59779.6094,  23541.9149, -28817.0426, -83892.1903,
          16471.0960,   5291.8233]], dtype=torch.float64)
	q_value: tensor([[-24.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9383757614360992, distance: 1.5932197394005587 entropy 12.052170104958103
epoch: 70, step: 97
	action: tensor([[ 24602.2867, -45326.5332, -15996.3158, -18402.4178,   3331.6683,
          29347.3088, -46131.5128]], dtype=torch.float64)
	q_value: tensor([[-21.6543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04204641083936722, distance: 1.1200281233770712 entropy 11.812203952819672
epoch: 70, step: 98
	action: tensor([[-19101.9371, -14670.9538, -25726.3471, -52304.6462,   5774.1344,
          -2135.6687,  64047.8131]], dtype=torch.float64)
	q_value: tensor([[-23.8663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12359725058100623, distance: 1.0712937197215444 entropy 11.746209934581008
epoch: 70, step: 99
	action: tensor([[  5331.4337, -59050.0906,  36650.9913,  14616.5594, -18622.5930,
          39675.3284,  52124.1249]], dtype=torch.float64)
	q_value: tensor([[-23.6581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06063629003383553, distance: 1.1091073475719415 entropy 11.968745841307946
epoch: 70, step: 100
	action: tensor([[ 36069.3645, -35039.8500,  23274.7683,  22107.9983, -22204.3411,
          32504.1874,  49682.4013]], dtype=torch.float64)
	q_value: tensor([[-19.8785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27109169824435597, distance: 0.9769968006658923 entropy 11.569915201407877
epoch: 70, step: 101
	action: tensor([[-38572.0855, -64246.9899, -25350.1194,  30283.3262,  -3486.9069,
          55437.6280,  26198.8921]], dtype=torch.float64)
	q_value: tensor([[-26.2881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0971283742539304, distance: 1.0873508434145494 entropy 11.846283968178666
epoch: 70, step: 102
	action: tensor([[-56666.2296, -22509.1604,   2106.4012, -14763.6525, -35333.3535,
          58696.0854,  10059.8068]], dtype=torch.float64)
	q_value: tensor([[-22.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48941760301630843, distance: 1.3965771575831059 entropy 11.964718808972743
epoch: 70, step: 103
	action: tensor([[ -28558.3516,  -57530.8795,  -30819.1048,   41472.7213,   26960.2178,
         -129445.4815,    -406.6422]], dtype=torch.float64)
	q_value: tensor([[-23.3547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5960054534192241, distance: 1.4456856784778516 entropy 11.952361560788317
epoch: 70, step: 104
	action: tensor([[  1053.4270, -41943.5268, -32035.4638, -19053.9800,  46442.8068,
         -10428.1165, -25137.0723]], dtype=torch.float64)
	q_value: tensor([[-21.7053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25804492940479884, distance: 1.2835262914624903 entropy 11.972843393458271
epoch: 70, step: 105
	action: tensor([[-94289.9283,  -3359.8726, -35242.6700,  35543.7839,  -5004.7012,
          10407.5504,  20167.4737]], dtype=torch.float64)
	q_value: tensor([[-22.1088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9373490971228537, distance: 1.5927977576239463 entropy 11.942264104177767
epoch: 70, step: 106
	action: tensor([[-62619.3167,  25244.8760,  71894.6502, -56600.1863,  54000.4731,
          53388.7728,   7686.2867]], dtype=torch.float64)
	q_value: tensor([[-24.1011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07103033069220044, distance: 1.1842886802690145 entropy 12.11673072599179
epoch: 70, step: 107
	action: tensor([[-79949.1436, -19426.3741,  13118.1423,  14131.9250,   3277.4261,
          12027.9391,  41378.0019]], dtype=torch.float64)
	q_value: tensor([[-28.9145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9210579077828309, distance: 1.5860866927701183 entropy 12.048520981937331
epoch: 70, step: 108
	action: tensor([[ 30314.2775,    158.7367, -20152.8204, -24055.5773,  40121.3687,
         -24983.8143,  65335.8289]], dtype=torch.float64)
	q_value: tensor([[-22.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.1824401518988
epoch: 70, step: 109
	action: tensor([[  1788.8110, -25079.9505,  25698.7238,  21805.9762, -49008.1988,
          -2918.9848, -27833.0720]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10859334981288349, distance: 1.0804250083362332 entropy 11.500287151164553
epoch: 70, step: 110
	action: tensor([[ -5629.5178, -11396.6639,  -2319.8677,   8223.5726,   4335.7706,
          -4452.8767, -16905.4888]], dtype=torch.float64)
	q_value: tensor([[-22.0539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7516209829995488, distance: 1.5145261028392014 entropy 11.770788034922372
epoch: 70, step: 111
	action: tensor([[-78373.1133,  44982.0984,   5248.0396,  42678.4124, 123832.4076,
         -33325.7462,  16172.2012]], dtype=torch.float64)
	q_value: tensor([[-23.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16341769952515695, distance: 1.0466730193447102 entropy 12.046153105638053
epoch: 70, step: 112
	action: tensor([[ -9776.9059, -29266.4697,  -6183.2217, -43073.6009,  24265.3687,
         -27972.9958,  32917.1916]], dtype=torch.float64)
	q_value: tensor([[-26.2693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06229462505094374, distance: 1.1794490510462285 entropy 11.990389425287313
epoch: 70, step: 113
	action: tensor([[ -9405.6425, -57861.9564,  30544.7190,  -6649.1357,  32716.9887,
         -21352.5472,  -5510.8963]], dtype=torch.float64)
	q_value: tensor([[-21.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08676610852437872, distance: 1.1929568530966097 entropy 11.75858784527203
epoch: 70, step: 114
	action: tensor([[  2559.0147, -60593.3953,   1577.2942, -10999.8448,  -5630.5455,
          12960.5452,  10517.6200]], dtype=torch.float64)
	q_value: tensor([[-22.5313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07267955799129244, distance: 1.1852001436746284 entropy 11.83537974302896
epoch: 70, step: 115
	action: tensor([[ -1285.0743, -11045.4412,  21956.7987,   2259.6792,   1236.5354,
          21230.5075,  32919.8514]], dtype=torch.float64)
	q_value: tensor([[-22.0421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3008864898356023, distance: 1.3051979860062304 entropy 11.67204499180838
epoch: 70, step: 116
	action: tensor([[ -13650.5327, -120763.5580,  -33584.9355,   46390.7588,   15730.6012,
          -25436.7909,   11641.6963]], dtype=torch.float64)
	q_value: tensor([[-20.3753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1782499529561441, distance: 1.2421539044565557 entropy 11.866243614571028
epoch: 70, step: 117
	action: tensor([[-27086.3024, -72073.1287, -26490.1839,  51794.7246, -23437.2904,
         -21122.5217,   4846.5755]], dtype=torch.float64)
	q_value: tensor([[-25.3375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7321165180225362, distance: 1.5060703023565454 entropy 12.237925102723276
epoch: 70, step: 118
	action: tensor([[-22508.6802, -32521.7268, -69960.7346,  46658.8216, -40368.2474,
          22629.5271, -26901.5056]], dtype=torch.float64)
	q_value: tensor([[-22.6454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25716031143243545, distance: 1.2830749442693532 entropy 12.00469465863537
epoch: 70, step: 119
	action: tensor([[-56216.9355, -88794.4623, -43187.9296,  53336.3384,  17741.8838,
          17962.4381, -13168.0708]], dtype=torch.float64)
	q_value: tensor([[-26.6962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0002693217611877685, distance: 1.1444983420308297 entropy 12.224213151760868
epoch: 70, step: 120
	action: tensor([[-10920.1876,  34662.8928, -76411.8239,  46071.5518, -13740.5684,
          30755.7143,  76685.9937]], dtype=torch.float64)
	q_value: tensor([[-23.6372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17713890629725637, distance: 1.038054027004475 entropy 12.088975217931944
epoch: 70, step: 121
	action: tensor([[ 24864.0501, -81605.0122, -43001.1115, 128696.0492, -41013.7081,
          10498.1241,  23041.5598]], dtype=torch.float64)
	q_value: tensor([[-27.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15473188075492872, distance: 1.2296946156446698 entropy 11.996415700553717
epoch: 70, step: 122
	action: tensor([[ 26339.5843, -39600.7835,  49314.8685,  20893.8588,  24118.2031,
         -14563.4171,  11642.3031]], dtype=torch.float64)
	q_value: tensor([[-27.2490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46684678344402686, distance: 0.835569752335818 entropy 12.016725953953612
epoch: 70, step: 123
	action: tensor([[ 28543.0694, -15292.2419,  13774.7937, -65760.4659, -14556.2155,
         -10819.7154,  -9602.7164]], dtype=torch.float64)
	q_value: tensor([[-25.8265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06319687727768553, distance: 1.1075946725101333 entropy 12.174816025076343
epoch: 70, step: 124
	action: tensor([[ 23226.0496,  15404.2964, -72995.9979, -18450.9140,   4392.3373,
          57128.3871, -19918.4161]], dtype=torch.float64)
	q_value: tensor([[-26.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.846546752905251
epoch: 70, step: 125
	action: tensor([[ -3553.7258, -45164.4333, -26195.9446,   9812.5955,  -5100.4935,
         -11833.9821,  15804.2389]], dtype=torch.float64)
	q_value: tensor([[-25.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2256175745233251, distance: 1.2668762168444152 entropy 11.500287151164553
epoch: 70, step: 126
	action: tensor([[-52711.9972, -66278.8926,  46593.7932,  87054.3621,  47650.5959,
          22943.2328,  10243.2879]], dtype=torch.float64)
	q_value: tensor([[-24.1247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23263368236208026, distance: 1.0024392423085877 entropy 12.040270883880094
epoch: 70, step: 127
	action: tensor([[  33685.9316, -102346.3058,    7835.8787,   57605.5954,   35488.4337,
          -10147.2696,   15089.9918]], dtype=torch.float64)
	q_value: tensor([[-23.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1500385737669896, distance: 1.0550093329675139 entropy 12.116742765655465
LOSS epoch 70 actor 248.59899982543027 critic 284.9110849867059
epoch: 71, step: 0
	action: tensor([[ 65485.9648, -28519.9267, -22104.3723,  31392.9994,   1565.1671,
          28652.9679,  13347.1946]], dtype=torch.float64)
	q_value: tensor([[-25.6028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19230890949254942, distance: 1.0284408992157839 entropy 12.023073695022177
epoch: 71, step: 1
	action: tensor([[  8682.6671, -74739.1169, -73199.4903,   4247.6230,  -5487.1689,
          10518.5382,  18174.3503]], dtype=torch.float64)
	q_value: tensor([[-25.7664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42334260005652546, distance: 0.8689916992303505 entropy 11.877850432698446
epoch: 71, step: 2
	action: tensor([[-38869.0880,  -5009.7505,  22275.7073,  -8943.8330, -12035.8790,
         -10753.0888,   3331.3097]], dtype=torch.float64)
	q_value: tensor([[-27.4764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47240046033884764, distance: 1.3885760347280214 entropy 12.131042799313274
epoch: 71, step: 3
	action: tensor([[-58138.4543, -69005.7757, -11218.9479,  13819.2038,  16361.1859,
         -16457.9508,  19909.4951]], dtype=torch.float64)
	q_value: tensor([[-24.4046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2992796331145251, distance: 1.3043916457877263 entropy 11.991248243284785
epoch: 71, step: 4
	action: tensor([[   1661.3153,  -99732.5043,  -69513.2227, -109636.4128,   19100.8523,
          -15556.5497,  -63054.6950]], dtype=torch.float64)
	q_value: tensor([[-26.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48327585414295215, distance: 0.8225950111640409 entropy 12.163845864939471
epoch: 71, step: 5
	action: tensor([[-34842.0602,  32452.9968, -24770.6611,   -488.6492,   5997.6423,
         -16506.7593,   5495.4958]], dtype=torch.float64)
	q_value: tensor([[-24.7547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6397853267080315, distance: 0.6868112386803145 entropy 12.017655406149828
epoch: 71, step: 6
	action: tensor([[-58346.9846, -63018.8336,  29258.5926, 120309.1867, -14372.9739,
         -46646.5731,  -8758.3607]], dtype=torch.float64)
	q_value: tensor([[-36.7197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29016951580312744, distance: 1.2998106209824956 entropy 12.21108440239756
epoch: 71, step: 7
	action: tensor([[-40497.5121, -58418.9463,   -329.0725, -62975.8598,  -9534.6540,
         -65433.9168, -59182.1542]], dtype=torch.float64)
	q_value: tensor([[-24.5381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6489997376847725, distance: 1.4694911894815696 entropy 12.102431973867656
epoch: 71, step: 8
	action: tensor([[ 21611.4081, -20082.9511, -18849.0358,  17409.2795,  19871.7609,
         -50391.7600, -73448.0706]], dtype=torch.float64)
	q_value: tensor([[-29.5405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4982750496397338, distance: 0.8105681645198726 entropy 12.238665702098473
epoch: 71, step: 9
	action: tensor([[-13984.0453,   4924.6518, -69522.9218,   6636.9274,  -2112.6955,
          41104.7994,  14737.2528]], dtype=torch.float64)
	q_value: tensor([[-23.0372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.902771294697887
epoch: 71, step: 10
	action: tensor([[-1.5782e+04,  1.2531e+04, -2.5261e+00, -5.6228e+04, -1.4389e+04,
         -9.7401e+03, -4.0617e+03]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53440131446303
epoch: 71, step: 11
	action: tensor([[ 29163.9804, -20944.1301, -38445.0060,   3300.4928, -48476.3947,
          14986.3765,  19826.5302]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53440131446303
epoch: 71, step: 12
	action: tensor([[ 30207.3973, -46758.9694,  28408.8772,  -5021.7114,  51777.7627,
          -7984.9872, -25150.7050]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33445249803757626, distance: 0.9335685700528906 entropy 11.53440131446303
epoch: 71, step: 13
	action: tensor([[ -6545.9728, -47718.8960, -38490.2310, -44413.3541,  73416.1955,
          44445.0327, -55855.4138]], dtype=torch.float64)
	q_value: tensor([[-26.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0631498599917879, distance: 1.107622466739634 entropy 12.085787110187685
epoch: 71, step: 14
	action: tensor([[-35179.0031,  -8851.6323,   4511.1412, -39602.9956, -59913.4729,
         -43550.3432, -30281.8925]], dtype=torch.float64)
	q_value: tensor([[-26.7338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3692676860605024, distance: 1.3390625768297635 entropy 12.080575583234989
epoch: 71, step: 15
	action: tensor([[-56720.5545, -13516.7287, -80485.3266, -63731.1721, -30392.5077,
         -13234.3976,  14437.6424]], dtype=torch.float64)
	q_value: tensor([[-28.4410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13420446607490955, distance: 1.0647909809321294 entropy 12.192223581028355
epoch: 71, step: 16
	action: tensor([[-34302.4372, -17326.6590,   4325.0089,  58480.1122, -55696.0749,
         -25701.0205,  31105.3636]], dtype=torch.float64)
	q_value: tensor([[-24.3265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15786702804044062, distance: 1.231362821285235 entropy 11.934230404511954
epoch: 71, step: 17
	action: tensor([[-19311.0573, -17014.7934, -26273.2139,  -8211.7804, -22821.3432,
          83974.6054,  94298.5811]], dtype=torch.float64)
	q_value: tensor([[-25.4191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7613699286830515, distance: 1.5187349315474692 entropy 12.133040642155752
epoch: 71, step: 18
	action: tensor([[-90283.1178, -67628.6993,  66062.8728,  23636.2069,  49631.8764,
         104052.5323,  49056.0992]], dtype=torch.float64)
	q_value: tensor([[-24.4826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013560762547294547, distance: 1.13655867897142 entropy 12.01954322245386
epoch: 71, step: 19
	action: tensor([[-14630.2131,  -1714.2885,   3416.5803,  25513.4129,  10009.7855,
         -83401.1494, -23552.7902]], dtype=torch.float64)
	q_value: tensor([[-22.5405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8551126098686372, distance: 1.5586256964821377 entropy 11.974545059781201
epoch: 71, step: 20
	action: tensor([[ -8817.9081, -27117.4696,   4973.2838,  22429.8064, -26304.6210,
         -93565.1982,  -6668.0204]], dtype=torch.float64)
	q_value: tensor([[-26.6568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11337411646484274, distance: 1.0775238610169318 entropy 12.111754900544412
epoch: 71, step: 21
	action: tensor([[-15493.4478,  47127.5957, -53502.8709, -90720.5429,  17025.7157,
         -54569.8369, -14807.5787]], dtype=torch.float64)
	q_value: tensor([[-25.7255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6765133120265441, distance: 0.6508559807804739 entropy 12.214245868486273
epoch: 71, step: 22
	action: tensor([[-61543.2656,  46470.7913,  32338.4890,  31574.5926,   8758.6212,
         -11719.5698, -19547.8277]], dtype=torch.float64)
	q_value: tensor([[-30.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3102832412277968, distance: 0.9503686081339326 entropy 12.004116007551975
epoch: 71, step: 23
	action: tensor([[-30585.1700,   3527.5777,  38129.6397, -43658.8842,  73407.5774,
          36957.5083,  28424.1510]], dtype=torch.float64)
	q_value: tensor([[-24.6570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49131059560462453, distance: 0.8161745324676234 entropy 11.902201572413118
epoch: 71, step: 24
	action: tensor([[-29950.3529, -24669.0785, -27860.0224, -35092.2422, -58494.1324,
          -2360.1567, -20651.3254]], dtype=torch.float64)
	q_value: tensor([[-31.4653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.866650966893832
epoch: 71, step: 25
	action: tensor([[-35609.3783,   4550.4869,   2721.6446, -12133.2355,  25147.2385,
          13676.0492,  39177.9776]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53440131446303
epoch: 71, step: 26
	action: tensor([[-37838.6396,  -8185.9965,  20160.4312,  70411.9711, -25419.8778,
          30088.9594,  28634.7173]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53440131446303
epoch: 71, step: 27
	action: tensor([[-38867.3558,  -6990.7425, -28118.6677,  -8582.7888, -49618.8589,
         -10434.8599,  21469.2959]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029931988688413957, distance: 1.1613442307875117 entropy 11.53440131446303
epoch: 71, step: 28
	action: tensor([[ 9.6335e+03,  8.0251e+03, -6.8863e+03,  1.9067e+04, -5.5456e+04,
         -2.6839e+04,  3.8999e+00]], dtype=torch.float64)
	q_value: tensor([[-25.6636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.966437058184463
epoch: 71, step: 29
	action: tensor([[ 25828.8688, -36177.9750,   4549.2603,  72489.7129,  20079.2603,
         -10492.6415,   3891.5443]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3582729406648453, distance: 0.9167097900200804 entropy 11.53440131446303
epoch: 71, step: 30
	action: tensor([[ 21469.1939, -50505.2948,  68387.7483,  20256.5787, -36176.2675,
           3576.0352,  42659.8999]], dtype=torch.float64)
	q_value: tensor([[-22.0741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41101619248850296, distance: 1.3593230838416785 entropy 11.817741837421915
epoch: 71, step: 31
	action: tensor([[  6198.8885, -64126.7565, -19111.0780,  46138.5763, 103770.5482,
          41873.7708,  88777.8590]], dtype=torch.float64)
	q_value: tensor([[-30.5462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3346314623869098, distance: 1.3220180128896755 entropy 12.083359279615204
epoch: 71, step: 32
	action: tensor([[ -5651.8476, -66685.8477, -37843.1216,  20007.1984,  10808.9006,
          -1346.0713,  47632.5847]], dtype=torch.float64)
	q_value: tensor([[-29.8025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0026022259324528507, distance: 1.142854362963642 entropy 12.201118591309383
epoch: 71, step: 33
	action: tensor([[-17962.4957, -16962.0619,   8960.0057, -30800.3284, -31765.1685,
          -3568.2058, -32470.7620]], dtype=torch.float64)
	q_value: tensor([[-22.9507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0077965769166934, distance: 1.6214984878610403 entropy 11.863892493228391
epoch: 71, step: 34
	action: tensor([[-31480.3727, -80980.0434,   8560.5242,   3627.1728, -77272.7268,
         -64807.2505,  56755.9128]], dtype=torch.float64)
	q_value: tensor([[-23.4776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12324801723885048, distance: 1.0715071456963006 entropy 11.930553384530514
epoch: 71, step: 35
	action: tensor([[ 23482.2068,  13452.5203, -36917.3745, -13050.1457,  29594.5360,
         -10872.3727, -11669.6923]], dtype=torch.float64)
	q_value: tensor([[-24.5879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.095705842727364
epoch: 71, step: 36
	action: tensor([[ -3495.3560, -51520.9071,  58379.2075, -17864.5450, -14939.8139,
         -16223.6734,   2702.4455]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.209172597827755, distance: 1.2583482272005342 entropy 11.53440131446303
epoch: 71, step: 37
	action: tensor([[-22561.3985, -42864.8365,  22692.6419, -11466.7907, -48519.4974,
          22137.6386, -47527.5808]], dtype=torch.float64)
	q_value: tensor([[-24.1050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26795806100852926, distance: 1.288573328282935 entropy 11.936203733480438
epoch: 71, step: 38
	action: tensor([[ 32190.4160, -32859.3558, -43686.9651,  -4093.7202,  34393.3524,
         -37809.6924,   8372.2383]], dtype=torch.float64)
	q_value: tensor([[-27.3678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5187842254008531, distance: 0.7938283795233819 entropy 12.002693680094328
epoch: 71, step: 39
	action: tensor([[-83379.4548,  -5269.4205,  54976.4926,   4174.1248,  14639.6017,
           3258.6172,  71070.4134]], dtype=torch.float64)
	q_value: tensor([[-23.0360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8464025296919901, distance: 1.5549623804742094 entropy 11.806145338509909
epoch: 71, step: 40
	action: tensor([[ -6962.4526, -41364.8225,  17145.5143,  28123.7942, -23347.0054,
          24092.0632, -39458.7605]], dtype=torch.float64)
	q_value: tensor([[-19.6083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34043842345447506, distance: 1.3248909313463744 entropy 11.752563332744794
epoch: 71, step: 41
	action: tensor([[ -3223.4258, -36689.6500, -27833.2931,  99240.2306, -25942.2435,
          21149.2206, -16123.7798]], dtype=torch.float64)
	q_value: tensor([[-23.9552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16750477768943728, distance: 1.04411315642099 entropy 12.010249216067669
epoch: 71, step: 42
	action: tensor([[-32842.3589, -54795.7746,   5174.4351,  39243.9815, -30398.4132,
          26848.4109, 108262.7968]], dtype=torch.float64)
	q_value: tensor([[-22.6539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16597416885994865, distance: 1.0450725582746565 entropy 11.96595148825392
epoch: 71, step: 43
	action: tensor([[-25231.0371,  -4412.6788, -58224.5161,   8732.3798,  -4182.2495,
          17194.5239, -40704.2773]], dtype=torch.float64)
	q_value: tensor([[-24.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3811705680403632, distance: 1.3448701392959437 entropy 12.171652018910777
epoch: 71, step: 44
	action: tensor([[-14385.3726, -16168.4637,  72556.7839,  23258.9087, -32878.9032,
           6332.7939,  70522.8548]], dtype=torch.float64)
	q_value: tensor([[-26.0151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2569989144466973, distance: 1.282992579644288 entropy 12.195079523264265
epoch: 71, step: 45
	action: tensor([[ -8171.9215, -49090.4906, -62990.1187,   6753.7686,  35487.4540,
          21380.7129,  14664.8363]], dtype=torch.float64)
	q_value: tensor([[-24.1032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10838861576903902, distance: 1.0805490746540078 entropy 12.026119709467661
epoch: 71, step: 46
	action: tensor([[-63101.5180, -14172.1129,  31470.5437,  64313.8776,   2346.0172,
          57637.9484,  49150.2911]], dtype=torch.float64)
	q_value: tensor([[-23.1923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7744350236176378, distance: 1.5243571906539384 entropy 11.978897219382201
epoch: 71, step: 47
	action: tensor([[ -28961.2099,    -849.5545, -114563.5898,    -849.2081,   62455.1993,
            3713.2423,  -40000.7770]], dtype=torch.float64)
	q_value: tensor([[-26.0819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9232736818482046, distance: 1.5870011360404543 entropy 12.207383237688905
epoch: 71, step: 48
	action: tensor([[-20228.8739, -19826.8894,    336.0765,  35595.1125,  14757.0426,
         -36171.6750,  24587.4213]], dtype=torch.float64)
	q_value: tensor([[-23.8742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23015140586821903, distance: 1.2692172819217624 entropy 11.991581700495683
epoch: 71, step: 49
	action: tensor([[ -1794.1750,  17584.0173,   -695.1379, -30076.3434,  82432.9406,
         -52714.5446,  39938.2741]], dtype=torch.float64)
	q_value: tensor([[-28.4831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016210477239008036, distance: 1.135031174183483 entropy 12.291739009046793
epoch: 71, step: 50
	action: tensor([[-34956.0217, 121611.3664, -37329.0061,  -2999.1882,  39765.1718,
          54663.1971, -20119.5205]], dtype=torch.float64)
	q_value: tensor([[-29.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.045169340692838844, distance: 1.16990345629476 entropy 12.082365771932682
epoch: 71, step: 51
	action: tensor([[ -4505.7642,  16603.6753, -26454.4712,   1233.4640, -21861.7305,
          18603.5920,  -2910.4844]], dtype=torch.float64)
	q_value: tensor([[-31.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10842901387430337, distance: 1.0805245950261655 entropy 11.785915055520151
epoch: 71, step: 52
	action: tensor([[ 36436.9741, -56607.8377,  52777.1634,  48757.9821,  47313.5158,
          18527.6929,  54983.4066]], dtype=torch.float64)
	q_value: tensor([[-27.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6105204552660158, distance: 0.7141657527532131 entropy 11.975274087329058
epoch: 71, step: 53
	action: tensor([[-53134.3874, -58122.2942, -69675.5909, -29666.5466,  16479.1655,
         -26206.8738, -82224.2438]], dtype=torch.float64)
	q_value: tensor([[-23.8310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29501025998806374, distance: 1.3022467968866784 entropy 11.927184234260906
epoch: 71, step: 54
	action: tensor([[ 37623.9402, -20746.0388,  10341.3121, -27606.2566,  -6715.1617,
         -85001.0710, -23382.8679]], dtype=torch.float64)
	q_value: tensor([[-23.3078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027414015529983793, distance: 1.1285497182887287 entropy 12.012924357329988
epoch: 71, step: 55
	action: tensor([[-59416.4897,  -2771.0835, -14302.9359,  26552.2601,  22304.2415,
          13435.6357, -21216.2147]], dtype=torch.float64)
	q_value: tensor([[-26.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3678257745912663, distance: 1.338357339173798 entropy 12.009328113112952
epoch: 71, step: 56
	action: tensor([[  -961.6027, -46387.7777,  27417.2085,  -9082.3555,  27767.9772,
          29169.9868,  20049.6316]], dtype=torch.float64)
	q_value: tensor([[-23.5990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006207901939770499, distance: 1.1478907469028128 entropy 12.008353308354353
epoch: 71, step: 57
	action: tensor([[-89048.1281,   2300.8919, -30658.1185,   7369.1779, -23920.9398,
          57026.7147, -43184.4249]], dtype=torch.float64)
	q_value: tensor([[-29.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1099534526285193, distance: 1.2056162041281064 entropy 12.183722170950444
epoch: 71, step: 58
	action: tensor([[-22266.9009,  43926.3060,  35506.5958,  23724.4306,  48486.3485,
          27921.6979, -57975.1146]], dtype=torch.float64)
	q_value: tensor([[-30.0717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0767891064746209, distance: 1.1874682867327782 entropy 11.904744457076655
epoch: 71, step: 59
	action: tensor([[ 29577.6451,   -123.5216,  -7568.5893, -20942.9523, -18324.5270,
          45408.4444, -36357.2477]], dtype=torch.float64)
	q_value: tensor([[-25.3185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.918541972699812
epoch: 71, step: 60
	action: tensor([[ -7208.6477,  -8159.5589, -15368.5863,  -9298.9924,  -7372.1595,
          14848.4758,  31001.5872]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3793823029683463, distance: 1.343999224682831 entropy 11.53440131446303
epoch: 71, step: 61
	action: tensor([[-10418.4720, -18611.6553,  -4354.4930, -37187.5807,  10649.2367,
          59505.0663, -19048.7983]], dtype=torch.float64)
	q_value: tensor([[-23.1777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.348243969942176, distance: 1.3287428376628516 entropy 11.863508680694485
epoch: 71, step: 62
	action: tensor([[ 29496.6627, -42264.2525,   9705.5157, -10647.5096,    149.3099,
           8465.7626, -10601.1271]], dtype=torch.float64)
	q_value: tensor([[-25.6308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005010380243641421, distance: 1.1414738541152392 entropy 11.910879976275465
epoch: 71, step: 63
	action: tensor([[-99065.8395, -44471.4204,  -8215.7343,  19304.0008,  10752.1823,
          -6035.5845,  21684.9205]], dtype=torch.float64)
	q_value: tensor([[-28.0771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.584141108130716, distance: 1.4403022039716 entropy 12.049533783696676
epoch: 71, step: 64
	action: tensor([[ 26953.2828, -96655.9846, 157669.0131, -28676.5095,  10812.9026,
         -75288.8688, -12137.0013]], dtype=torch.float64)
	q_value: tensor([[-24.8901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5109832186591635, distance: 0.8002369023657075 entropy 12.064126353008376
epoch: 71, step: 65
	action: tensor([[-15084.5746, -80372.0153, -29944.9503, -27025.2332,  50683.0863,
          -8435.5590,  58148.8970]], dtype=torch.float64)
	q_value: tensor([[-25.1531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1254776036096954, distance: 1.214017988499757 entropy 12.000266269229941
epoch: 71, step: 66
	action: tensor([[ 66270.9048, -16215.5607, -18481.1277,  61551.7266, -39258.4655,
          15809.0030,  28268.8061]], dtype=torch.float64)
	q_value: tensor([[-27.1263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32508684914274144, distance: 1.3172823267888323 entropy 12.19786161800438
epoch: 71, step: 67
	action: tensor([[-13900.2717, -56618.0771,   3743.0181,  12493.4203,  25405.6793,
          22915.4561,  25647.8257]], dtype=torch.float64)
	q_value: tensor([[-23.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7193661818089214, distance: 1.5005168733712775 entropy 11.956708024415585
epoch: 71, step: 68
	action: tensor([[-15230.4095, -63677.5590,   2360.8897,  24948.8075,  -1544.9921,
          35983.4226,   9814.1112]], dtype=torch.float64)
	q_value: tensor([[-25.3473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12897580302119316, distance: 1.21590322463935 entropy 12.110884237415764
epoch: 71, step: 69
	action: tensor([[-30795.0296,  52054.8764, -79229.2647, 103237.3838,   3643.1657,
          31295.8869, -33664.0285]], dtype=torch.float64)
	q_value: tensor([[-23.8795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31966088194605435, distance: 0.9438857175970813 entropy 12.015845380499721
epoch: 71, step: 70
	action: tensor([[ 34287.7522, -84355.3367,  46751.6406,  59093.7129,  52002.3339,
           5189.6409,  42782.0049]], dtype=torch.float64)
	q_value: tensor([[-30.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0023786579905407113, distance: 1.1457044474235427 entropy 12.164410419724984
epoch: 71, step: 71
	action: tensor([[-25005.4220,   8320.3256,  28856.9332,  31805.8661,  21951.9180,
          20858.3836,  37301.6643]], dtype=torch.float64)
	q_value: tensor([[-26.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1274963009538277, distance: 1.2151062540209931 entropy 12.083987229789361
epoch: 71, step: 72
	action: tensor([[-28253.6919, -45502.5864,  44410.9101,  44447.3938,  24355.7947,
         -57461.4839, -49262.0866]], dtype=torch.float64)
	q_value: tensor([[-26.9337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15091016946160374, distance: 1.2276580248417923 entropy 12.097060586501717
epoch: 71, step: 73
	action: tensor([[-47362.8059,  12073.8112,   1030.3695,   7496.4388, -56434.5904,
          33885.4271, -53974.6455]], dtype=torch.float64)
	q_value: tensor([[-25.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46861808939296057, distance: 0.834180582231164 entropy 12.108278591546322
epoch: 71, step: 74
	action: tensor([[-12322.4535, -17573.4634, -73827.3153,  36565.8022, -15551.6983,
         -44480.4604,  60970.8247]], dtype=torch.float64)
	q_value: tensor([[-26.4356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4088263239099512, distance: 1.358267853298612 entropy 11.939837443131902
epoch: 71, step: 75
	action: tensor([[-20141.6589,  19243.6562,  26761.8473,  29628.6815,  37785.7565,
          37288.4046, -47770.7359]], dtype=torch.float64)
	q_value: tensor([[-23.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6316625830041591, distance: 0.6945117735135475 entropy 11.962283767703152
epoch: 71, step: 76
	action: tensor([[-34993.2041, -19841.2239, -36461.0878, -21869.8675,  39505.8214,
          46013.6844,   5288.0243]], dtype=torch.float64)
	q_value: tensor([[-23.6220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1712116560090149, distance: 1.2384383332214723 entropy 11.888057407804114
epoch: 71, step: 77
	action: tensor([[ -1510.8463,   1787.8194, -54684.9644, -19372.7931, -14970.8199,
          69066.9584, -24651.4539]], dtype=torch.float64)
	q_value: tensor([[-23.3465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5779913674332072, distance: 0.743391105807121 entropy 12.008991476757307
epoch: 71, step: 78
	action: tensor([[ 37786.2187, -43168.4149,  25096.0090,   2203.6406,  27937.1072,
           2668.9837,   5946.7397]], dtype=torch.float64)
	q_value: tensor([[-35.6249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.073438801787338
epoch: 71, step: 79
	action: tensor([[ 13081.5801, -15422.4526,   2170.7638,   1164.9517,   6511.2986,
         -32617.5156, -19904.7314]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53440131446303
epoch: 71, step: 80
	action: tensor([[  9320.2939, -16451.6281,  20400.5714,  -2673.8387,  24667.8664,
           9656.4279, -31621.4662]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0686720102671996, distance: 1.1829841087502349 entropy 11.53440131446303
epoch: 71, step: 81
	action: tensor([[-26567.6388, -66324.1298, -47400.4475,  19366.4560,  26762.9716,
         -39948.3234,  55287.1000]], dtype=torch.float64)
	q_value: tensor([[-23.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6743065652017854, distance: 1.4807242309703914 entropy 11.745381263833437
epoch: 71, step: 82
	action: tensor([[-53066.1026,  36255.3002, -73628.0683,  70833.0984,  40670.8744,
          57900.7299,  10201.2686]], dtype=torch.float64)
	q_value: tensor([[-24.0588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28978731819551706, distance: 0.9643860169866474 entropy 11.978768701460124
epoch: 71, step: 83
	action: tensor([[-51643.9897, -32438.7060, -30252.7504, 132205.4022,  -8741.0090,
          54115.6895, -28342.3730]], dtype=torch.float64)
	q_value: tensor([[-28.5842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2804340019628162, distance: 1.2948972019491203 entropy 12.077525698786712
epoch: 71, step: 84
	action: tensor([[-15142.5175,   9223.4412, -20731.5414, -62082.6311, -43487.4476,
          25371.3651,  50522.9559]], dtype=torch.float64)
	q_value: tensor([[-22.9572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2256651736144808, distance: 1.006980567031081 entropy 11.974844666413672
epoch: 71, step: 85
	action: tensor([[ 26214.2232, -51471.9161,  -1176.9532, -42232.1385,  12199.7373,
         -30451.4658, -23869.5561]], dtype=torch.float64)
	q_value: tensor([[-32.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.887198164987096
epoch: 71, step: 86
	action: tensor([[  8276.0375, -20902.9396,  12948.1014, -11599.5571,   4138.4457,
          16238.7048,    659.1697]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17636915579770873, distance: 1.241162106160198 entropy 11.53440131446303
epoch: 71, step: 87
	action: tensor([[  8202.7505,   8130.4714,  32002.2981,  17823.6427,  67383.1309,
         -27617.5702,  24727.7769]], dtype=torch.float64)
	q_value: tensor([[-26.4780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6868152973952544, distance: 0.6404083173898228 entropy 11.921671772651196
epoch: 71, step: 88
	action: tensor([[  2935.2208, -52219.8038,  33852.7446,  71754.2163,  -9760.3904,
          10363.9808, -21672.7761]], dtype=torch.float64)
	q_value: tensor([[-25.5942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295181990402128, distance: 0.9607163672304561 entropy 11.862717844587122
epoch: 71, step: 89
	action: tensor([[ 26342.7507, -52618.5457, -91032.1746,  12832.2848,  10686.0749,
          24810.8148, -15717.2271]], dtype=torch.float64)
	q_value: tensor([[-25.7833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05022040274820405, distance: 1.1727269856552591 entropy 12.088412146934603
epoch: 71, step: 90
	action: tensor([[  9519.3471, -54723.9129,  94986.0624,  14962.1314, -56114.7578,
          17610.0600,  23248.5713]], dtype=torch.float64)
	q_value: tensor([[-26.6453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2803164366426537, distance: 0.9707948981589508 entropy 12.124321359586654
epoch: 71, step: 91
	action: tensor([[  5853.9374,   7084.5566,  34760.5720, -42549.5514, -26897.5207,
           2586.1010,  21100.6976]], dtype=torch.float64)
	q_value: tensor([[-25.6961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5447889723647421, distance: 0.7720813828939468 entropy 12.075579695732827
epoch: 71, step: 92
	action: tensor([[ -6326.4546, -13364.4840,  -6605.8771,   1523.3126,  -5523.3945,
          79078.9860,  35312.2132]], dtype=torch.float64)
	q_value: tensor([[-26.4862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11092629812266397, distance: 1.0790102628652942 entropy 11.722729208841326
epoch: 71, step: 93
	action: tensor([[-49433.0959,  -3428.2883, -29481.0120,  67584.0162,   -476.2664,
          -6881.6360, -61793.3432]], dtype=torch.float64)
	q_value: tensor([[-22.6202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9267369509844741, distance: 1.5884293623121248 entropy 12.001279608711526
epoch: 71, step: 94
	action: tensor([[ -1394.3775,  -3509.2967, -13397.7033,  40846.3755,  -9180.4413,
         -59931.2617, -20259.0990]], dtype=torch.float64)
	q_value: tensor([[-24.8729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0608963197137977, distance: 1.6428002683216332 entropy 12.019917150729585
epoch: 71, step: 95
	action: tensor([[ 40879.4474,  -3080.8494,  34101.8959,  -5642.7585, -35127.5153,
           2651.1976,   5227.4311]], dtype=torch.float64)
	q_value: tensor([[-25.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1661697973313323, distance: 1.0449499853701345 entropy 12.162842256707266
epoch: 71, step: 96
	action: tensor([[ 7.2380e+04, -5.2646e+04,  8.8787e+02,  3.1532e+04, -3.1184e+01,
         -7.5152e+03, -4.3444e+04]], dtype=torch.float64)
	q_value: tensor([[-25.8375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7799498954091026, distance: 1.5267241729949008 entropy 11.916259127256819
epoch: 71, step: 97
	action: tensor([[-100875.5200,    3965.8813,   38733.8107,  -38064.8034,   -3475.9547,
           10203.8900,   49754.8733]], dtype=torch.float64)
	q_value: tensor([[-28.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.929693064474536
epoch: 71, step: 98
	action: tensor([[-41618.1489,  10211.9907,  56306.0762, -33646.0212, -61053.4961,
           6407.5361,  53789.2689]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53440131446303
epoch: 71, step: 99
	action: tensor([[ 11013.4842, -20443.9781,  -4815.1365,  10591.0280,   9532.4841,
           8310.6765,  11363.2665]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5637658786543519, distance: 0.7558167449273453 entropy 11.53440131446303
epoch: 71, step: 100
	action: tensor([[ 14648.3368, -92521.2760, -29683.3933,  81702.6638,  35087.7161,
          -7315.9764,  16021.9103]], dtype=torch.float64)
	q_value: tensor([[-25.9469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1109895866301801, distance: 1.0789718576327958 entropy 11.99005438261739
epoch: 71, step: 101
	action: tensor([[  6850.5896, -48980.0536, -61773.1765, -10044.5788,  -6375.3513,
          -9205.3679,  -4336.0394]], dtype=torch.float64)
	q_value: tensor([[-28.3715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3139437771608522, distance: 1.3117319127095164 entropy 11.985832991021699
epoch: 71, step: 102
	action: tensor([[ 55409.8824,  -7745.0117,  18977.7630, -46502.1241, -60277.3845,
          37220.0116,  -2278.0044]], dtype=torch.float64)
	q_value: tensor([[-28.8257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3137751188313086, distance: 0.9479598064247728 entropy 11.935119223359374
epoch: 71, step: 103
	action: tensor([[ 83720.5489,  17365.5487, -66499.1095,  42657.1841,  89591.5260,
          25333.8743, -93715.5431]], dtype=torch.float64)
	q_value: tensor([[-27.3612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28821334831659395, distance: 0.9654540593584457 entropy 12.110893840607227
epoch: 71, step: 104
	action: tensor([[-45874.1435, -49770.7352,  22278.0743, -43241.1755,  -9078.8837,
          46798.9321,  33493.5976]], dtype=torch.float64)
	q_value: tensor([[-25.5989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.779383025733035
epoch: 71, step: 105
	action: tensor([[ 17527.2740,  28992.1388,  -8512.5316,  27380.7109,    108.7392,
          10172.8101, -25637.5997]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17580244282743673, distance: 1.2408631067372975 entropy 11.53440131446303
epoch: 71, step: 106
	action: tensor([[ 11110.1183, -31885.2013, -16699.4326,  12775.5247,   9005.5255,
          22402.8422,   3196.4111]], dtype=torch.float64)
	q_value: tensor([[-18.6671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24709680228087683, distance: 1.2779291483300204 entropy 11.408253455993082
epoch: 71, step: 107
	action: tensor([[  4167.6997,  22854.0282,  36865.7283,  52719.1305,  49250.4041,
         -85402.7678,  74690.2162]], dtype=torch.float64)
	q_value: tensor([[-28.1466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3263762009449309, distance: 0.9392158314362196 entropy 11.960153991619807
epoch: 71, step: 108
	action: tensor([[-61458.0315,  14013.2268, -41682.0644,  34743.0676, -13484.3657,
          -4766.0227, -48339.7938]], dtype=torch.float64)
	q_value: tensor([[-30.6909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7263994037109579, distance: 0.5985703674436464 entropy 12.142508363823646
epoch: 71, step: 109
	action: tensor([[-11042.7547,  12702.2525, -15659.1555,  25978.0496,   8664.7035,
         -16259.8613,  -2904.5265]], dtype=torch.float64)
	q_value: tensor([[-27.4485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2469844234511237, distance: 0.9930215495764101 entropy 12.063204045238448
epoch: 71, step: 110
	action: tensor([[ -1994.1079, -26546.3080, -22882.6378,  -1319.6009, -20501.0270,
           9247.0545, -48328.0023]], dtype=torch.float64)
	q_value: tensor([[-25.4846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4685444309942217, distance: 1.3867565908009876 entropy 11.950412580323194
epoch: 71, step: 111
	action: tensor([[-82528.1606, -39361.5855, -29079.2995, -31997.6010,  44096.6579,
           4458.9635, -26274.5215]], dtype=torch.float64)
	q_value: tensor([[-22.3206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12550878516831565, distance: 1.2140348056761994 entropy 11.90794607503093
epoch: 71, step: 112
	action: tensor([[-78367.5215, -18081.8742,  12279.0653, -13059.2300, -31578.4557,
         -41143.9532,  25873.1661]], dtype=torch.float64)
	q_value: tensor([[-26.5635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.252495193979726, distance: 1.2806920903987349 entropy 11.9863973285304
epoch: 71, step: 113
	action: tensor([[ 13245.2056, -26414.2532,   2255.8452,  54583.6338,  97319.3119,
          45989.5433,  11467.0608]], dtype=torch.float64)
	q_value: tensor([[-23.9486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49312995160700446, distance: 0.8147136782419391 entropy 11.877273813054812
epoch: 71, step: 114
	action: tensor([[-52313.9993,  11805.8439, -15661.5433,  22198.7050, -41910.3546,
          35527.3804,  15198.5642]], dtype=torch.float64)
	q_value: tensor([[-19.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.524065978547394
epoch: 71, step: 115
	action: tensor([[ 17762.3661,   3485.6415,  13242.2075, -14721.9490, -27514.2436,
         -57616.6286,  28678.6396]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4865674110898083, distance: 0.8199708411427071 entropy 11.53440131446303
epoch: 71, step: 116
	action: tensor([[-9.8593e+03, -1.2756e+05,  5.6514e+04,  1.4310e+04, -9.0132e+01,
         -6.7057e+04, -9.6378e+03]], dtype=torch.float64)
	q_value: tensor([[-32.1890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.087637708045053
epoch: 71, step: 117
	action: tensor([[-15313.5575, -20730.3272,  25183.7145,   1077.0249,   1093.8721,
         -16737.5549,  28258.7939]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53440131446303
epoch: 71, step: 118
	action: tensor([[-27170.0774,  30285.6049, -11160.7831,  35591.5777,  19448.7764,
          50434.5216, -33787.8999]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5734067997824854, distance: 0.7474181804816837 entropy 11.53440131446303
epoch: 71, step: 119
	action: tensor([[-20560.4028,   1778.6291,   8237.8212,  -5954.8986, -25190.6495,
         -57680.2011,  -4068.8523]], dtype=torch.float64)
	q_value: tensor([[-28.3717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.844033684796328
epoch: 71, step: 120
	action: tensor([[22611.0653, 33620.4943,   637.9647, 30923.8429, -2699.6766, 56440.4285,
          3563.3229]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.53440131446303
epoch: 71, step: 121
	action: tensor([[ -1918.5265, -28651.7685,  19716.5716, -24885.5870,   5221.2939,
         -15313.4310,  15655.8886]], dtype=torch.float64)
	q_value: tensor([[-26.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1037156188374202, distance: 1.0833769903881543 entropy 11.53440131446303
epoch: 71, step: 122
	action: tensor([[-10081.5869,  -3606.0088,  11245.8517,  18321.0233, -67327.7821,
         -17785.1471, -21624.9460]], dtype=torch.float64)
	q_value: tensor([[-24.7226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.623937588389212, distance: 1.4582814802679416 entropy 11.936502245417582
epoch: 71, step: 123
	action: tensor([[-51522.6278, -12583.4478, -25554.8767,   3489.0591,  15894.0624,
          -3784.4672,  43559.5569]], dtype=torch.float64)
	q_value: tensor([[-26.5014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0534200819868027, distance: 1.174512085467285 entropy 12.127205351917471
epoch: 71, step: 124
	action: tensor([[-26463.7951,  -5300.4670, -45129.1842,  44888.8152, -10042.8319,
         -56116.8207, -46377.0290]], dtype=torch.float64)
	q_value: tensor([[-25.8200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23595829301774507, distance: 1.2722094031607116 entropy 12.221662429448648
epoch: 71, step: 125
	action: tensor([[-55683.6019,   7421.7198,  25633.1755, -23097.4348,  59246.8821,
          67125.8302, -21854.7376]], dtype=torch.float64)
	q_value: tensor([[-26.6964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3515645068484188, distance: 1.3303780856033882 entropy 12.245611294378637
epoch: 71, step: 126
	action: tensor([[ -44817.3753,   20932.6254,   77627.4607, -107705.6272,   37137.4105,
           38148.0704,   65146.0125]], dtype=torch.float64)
	q_value: tensor([[-27.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14733325959219679, distance: 1.2257488231507165 entropy 12.012649404355258
epoch: 71, step: 127
	action: tensor([[-21184.9759, -25263.5194, -40961.1332,  10337.1308, -13374.0901,
          15613.3442,  42175.0255]], dtype=torch.float64)
	q_value: tensor([[-38.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0237777797476544, distance: 1.6279389150525732 entropy 12.169910028210179
LOSS epoch 71 actor 270.2086534858934 critic 292.63662928654367
epoch: 72, step: 0
	action: tensor([[-12323.0335, -24363.5451,  67762.8239,  -9118.2177, -37407.0615,
           7930.6760,  -1421.1543]], dtype=torch.float64)
	q_value: tensor([[-28.7186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04740285606166439, distance: 1.1711528245793872 entropy 12.144441837416831
epoch: 72, step: 1
	action: tensor([[-34110.8749,   9485.6825, -63599.8482, -44177.7481, -22683.0657,
         -42116.7567,  10980.0215]], dtype=torch.float64)
	q_value: tensor([[-30.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38561196914165397, distance: 0.896970306885413 entropy 12.148882581462463
epoch: 72, step: 2
	action: tensor([[-74629.5090,   5365.1807, -13472.8567, -22392.0591, -59169.1785,
          16624.7920,   3061.2810]], dtype=torch.float64)
	q_value: tensor([[-39.0071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1231114302185099, distance: 1.2127411579134943 entropy 12.062114466154881
epoch: 72, step: 3
	action: tensor([[  3797.6380, -14717.4660,  44582.5346,  65316.5619,  25882.7450,
         -20603.6635,  41371.5476]], dtype=torch.float64)
	q_value: tensor([[-40.6932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06454467791097596, distance: 1.1806974895042164 entropy 11.962073253224771
epoch: 72, step: 4
	action: tensor([[-49882.2261, -79681.5979,  10937.0667,  13234.7103,  31265.7472,
          39071.4696,  11466.9926]], dtype=torch.float64)
	q_value: tensor([[-31.2269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21185887252405644, distance: 1.2597452129183704 entropy 12.182562748358631
epoch: 72, step: 5
	action: tensor([[-1.6904e+01, -7.6828e+04,  4.1366e+04, -4.4652e+04,  6.6800e+04,
         -3.5810e+04, -1.2720e+04]], dtype=torch.float64)
	q_value: tensor([[-27.5668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.129958961468622
epoch: 72, step: 6
	action: tensor([[-40118.3984, -18507.0758, -44659.9462,  37923.6336,  -4643.5756,
          54106.6638,  28410.5158]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20641689221574722, distance: 1.0194194153701086 entropy 11.569125439409438
epoch: 72, step: 7
	action: tensor([[-23820.5737, -42035.4209,  71800.7625, -12848.2919, -16312.1623,
          15209.1025,   1445.6508]], dtype=torch.float64)
	q_value: tensor([[-26.3183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.467970720071506, distance: 1.386485684757913 entropy 12.047359114237894
epoch: 72, step: 8
	action: tensor([[ -5649.5198,  16117.5696, -25881.0700,  68257.9211,  15696.2543,
         -11134.7966,  -5592.2523]], dtype=torch.float64)
	q_value: tensor([[-24.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.823189976954463
epoch: 72, step: 9
	action: tensor([[10972.6997, -6821.7452, -6877.1589,  9775.6806, 24271.3726, -1986.6922,
         34530.4971]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3018237715081453, distance: 1.305668095384757 entropy 11.569125439409438
epoch: 72, step: 10
	action: tensor([[-68243.0593,   5749.5480,  15598.0910,  10769.1574, -41224.6959,
          39847.2528,  25886.9177]], dtype=torch.float64)
	q_value: tensor([[-32.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3359303661430406, distance: 0.932531485567705 entropy 12.067232560014407
epoch: 72, step: 11
	action: tensor([[-61974.8553, -35155.5092,  -8651.6702, -28357.3656,  28648.1598,
         -37155.7877, -17571.9835]], dtype=torch.float64)
	q_value: tensor([[-33.2017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8600076064491406, distance: 1.560680677178649 entropy 12.160278841693259
epoch: 72, step: 12
	action: tensor([[-14225.0928, -19837.4063, -35779.0835, -61902.0662,  41256.5562,
         -76265.0467, -67515.5131]], dtype=torch.float64)
	q_value: tensor([[-29.6994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04812305944945827, distance: 1.116470098725715 entropy 12.144090192747809
epoch: 72, step: 13
	action: tensor([[ 22520.1149, -84131.2175, -14569.1808, -23339.4611,  17842.7470,
          15824.2112, -69995.0304]], dtype=torch.float64)
	q_value: tensor([[-31.8867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5421986440369968, distance: 0.7742749888334399 entropy 12.253880789013461
epoch: 72, step: 14
	action: tensor([[-47280.3152,   2216.8569, -34157.6195,  13206.4045, -60733.9027,
           3846.2130, -37774.7073]], dtype=torch.float64)
	q_value: tensor([[-27.4965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.873308237273521
epoch: 72, step: 15
	action: tensor([[ -8311.0966,   3680.7064,  33917.7071,   1750.0277, -32195.1289,
           -858.4850, -17714.9909]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.569125439409438
epoch: 72, step: 16
	action: tensor([[-50853.1723, -23034.7069,  32115.5971,  51604.5134,  13637.3879,
          38813.4352,  26914.9501]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6340424373458646, distance: 1.4628114764419433 entropy 11.569125439409438
epoch: 72, step: 17
	action: tensor([[-66386.3021,  -5765.2281, -42435.3350, -43528.2288, -13704.6180,
          30000.9661,  -2130.3102]], dtype=torch.float64)
	q_value: tensor([[-27.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7277746313198215, distance: 1.5041814889797256 entropy 12.104651212286585
epoch: 72, step: 18
	action: tensor([[ -2422.4022, -57197.0108,  52075.2571,  22813.8363, -22690.9405,
          54396.0271,  44231.4909]], dtype=torch.float64)
	q_value: tensor([[-26.7813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8422713685615959, distance: 1.5532218614818682 entropy 11.88346871459145
epoch: 72, step: 19
	action: tensor([[-27783.2555, -94620.4593,  70533.7126,  42990.2303, -21775.0911,
          58446.5938,  75536.7509]], dtype=torch.float64)
	q_value: tensor([[-25.7133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.818673440250961, distance: 1.5432420752754339 entropy 12.048130764026936
epoch: 72, step: 20
	action: tensor([[  5124.6765, -29802.9332,  20939.5280,  11064.1691,  -9246.3275,
            337.6932,  30990.9202]], dtype=torch.float64)
	q_value: tensor([[-28.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030011022516016506, distance: 1.127041979815386 entropy 12.124669622713196
epoch: 72, step: 21
	action: tensor([[-25663.8519,  12949.0006, -26725.4834,  -9650.6865,   5754.6717,
         -53129.8308,  74019.7683]], dtype=torch.float64)
	q_value: tensor([[-30.6193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.164679280883874
epoch: 72, step: 22
	action: tensor([[ -7643.8043, -10024.3622, -15091.0839,  25201.2979, -21096.6524,
          19748.7503, -38730.5845]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.453942733268893, distance: 1.3798451260384352 entropy 11.569125439409438
epoch: 72, step: 23
	action: tensor([[  4731.2856, -71593.1593,  13365.6479,  41790.0480,  17806.0735,
          58505.3294,  16372.7567]], dtype=torch.float64)
	q_value: tensor([[-27.0151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4341363526167048, distance: 0.8608204794080184 entropy 12.067669249222096
epoch: 72, step: 24
	action: tensor([[-34779.5819,  17011.8139,  -2429.9492, -24313.8672, -10910.3322,
          37519.9573,  81327.1156]], dtype=torch.float64)
	q_value: tensor([[-27.5804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.886338328174743
epoch: 72, step: 25
	action: tensor([[-42018.3502,  19170.4461,  -4886.5867,  22740.6053,  51936.6544,
          10950.1532, -23537.8795]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.569125439409438
epoch: 72, step: 26
	action: tensor([[-10931.0880, -22733.3891, -34404.5757, -12113.2580, -48726.5434,
          17213.3471,   8756.5483]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4592640217349593, distance: 1.382367869126816 entropy 11.569125439409438
epoch: 72, step: 27
	action: tensor([[  49934.1761, -146015.3196,  -37149.6017,   21871.0189,   20586.3472,
            7129.6275,   63459.9943]], dtype=torch.float64)
	q_value: tensor([[-33.1206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18486755291369317, distance: 1.245637277385528 entropy 12.103904973705797
epoch: 72, step: 28
	action: tensor([[-14266.8048, -39853.5350,  35919.3996,  17436.9502, -37151.8440,
           4916.5318,   9053.3201]], dtype=torch.float64)
	q_value: tensor([[-28.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07517833169869914, distance: 1.186579784142733 entropy 11.966494086995386
epoch: 72, step: 29
	action: tensor([[-17329.4626, -64022.2747,  49312.3496, -77322.6629, -34605.6742,
         -67856.7242,   8487.4322]], dtype=torch.float64)
	q_value: tensor([[-27.2596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6462317259549617, distance: 1.4682573271751547 entropy 12.066804712941684
epoch: 72, step: 30
	action: tensor([[-49951.3617,  -4851.4959,  -9326.1669,  21461.5758,  58080.0725,
          80122.9422,  36294.9931]], dtype=torch.float64)
	q_value: tensor([[-27.4297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10259533041363311, distance: 1.2016134135823675 entropy 12.012015682945698
epoch: 72, step: 31
	action: tensor([[-31481.2146, -46490.1987,   2957.1866, 105914.0597, -65542.7437,
         -26496.0275,  17909.6636]], dtype=torch.float64)
	q_value: tensor([[-28.7581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0953881728895285, distance: 1.0883982229535734 entropy 12.279289085632573
epoch: 72, step: 32
	action: tensor([[ -58907.8641, -110348.7738,  -23780.3633,  -48489.9859,  -42612.6533,
          -71374.3934,   27717.1141]], dtype=torch.float64)
	q_value: tensor([[-32.9787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6864788007107787, distance: 1.4860969281766792 entropy 12.359633541402117
epoch: 72, step: 33
	action: tensor([[-25507.6708,  13752.2358,  23433.1749,  17346.8846, -17933.2259,
          14333.3724,  10742.1370]], dtype=torch.float64)
	q_value: tensor([[-26.8122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2266461527829734, distance: 1.0063425097041216 entropy 11.9863053366078
epoch: 72, step: 34
	action: tensor([[-74110.1137, -48949.0193,  41153.3812,  17259.9232,  39868.9704,
         113828.3725, -23006.8287]], dtype=torch.float64)
	q_value: tensor([[-33.5362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6858340207034177, distance: 1.4858128163362403 entropy 12.223714156350656
epoch: 72, step: 35
	action: tensor([[ -1555.4774,  38697.2946,  12956.0096, -37159.2372,  71411.0057,
          -3447.8216, -30048.1223]], dtype=torch.float64)
	q_value: tensor([[-31.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14926044589919618, distance: 1.0554921456784163 entropy 12.268256176628103
epoch: 72, step: 36
	action: tensor([[ 29722.1649, -21283.6738, -44198.7028,  44182.7573, -34349.7668,
         -13355.1210,  31522.2181]], dtype=torch.float64)
	q_value: tensor([[-34.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28105828254684795, distance: 1.295212829569194 entropy 12.050106712606519
epoch: 72, step: 37
	action: tensor([[-48360.1622, -68700.4878,  51651.5243,  16513.5079,  67367.2379,
         -25831.1470,  69119.0587]], dtype=torch.float64)
	q_value: tensor([[-27.2688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5887742357099051, distance: 1.442406887624828 entropy 11.95289445829184
epoch: 72, step: 38
	action: tensor([[ 52253.0003, -41634.0956,  30525.0556,   2708.2320,  33966.4142,
          51859.8719,  77157.6021]], dtype=torch.float64)
	q_value: tensor([[-29.7488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5607134551320243, distance: 0.7584564411198831 entropy 12.248758225420247
epoch: 72, step: 39
	action: tensor([[ 22583.4262, -38802.5382, -25274.0578,  -5826.9705,  13757.4660,
          11121.9289,  25898.7570]], dtype=torch.float64)
	q_value: tensor([[-28.3889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29738696499691475, distance: 0.9592124225721322 entropy 11.892879102039142
epoch: 72, step: 40
	action: tensor([[ -6758.9769,  26578.5861, -14436.0181,  86215.0745, -13295.7945,
          80797.5887, -52217.5634]], dtype=torch.float64)
	q_value: tensor([[-32.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.14258483044243
epoch: 72, step: 41
	action: tensor([[-13322.1082,  -1127.5401,   4056.8412, -15065.6863,   9416.5117,
          -7715.7474,  20079.7122]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014613698854073931, distance: 1.1526754780527553 entropy 11.569125439409438
epoch: 72, step: 42
	action: tensor([[ 94374.7659,  12185.7090,  55366.3100, -21176.7301, -24019.8181,
          -2711.4678,  -1423.3685]], dtype=torch.float64)
	q_value: tensor([[-30.1515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5724416328025776, distance: 0.7482632194745135 entropy 12.096031325546548
epoch: 72, step: 43
	action: tensor([[-16279.4378,   5728.1887, -32554.3771,  28031.7980, -16791.7669,
          40423.4559,  34823.5524]], dtype=torch.float64)
	q_value: tensor([[-34.3642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6867338219426795, distance: 0.6404916135469829 entropy 11.960898980864872
epoch: 72, step: 44
	action: tensor([[  4446.7797, -38731.5471,   3267.7125,  16210.6950, -11087.5759,
           6675.2118,  21440.7891]], dtype=torch.float64)
	q_value: tensor([[-32.9224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.017611605721978
epoch: 72, step: 45
	action: tensor([[  6577.0294, -25639.4315, -20459.6015,  -5036.7870,  14266.3623,
          46505.9421,   8179.3269]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.569125439409438
epoch: 72, step: 46
	action: tensor([[-16912.4863, -55081.6540, -26472.6221, -13133.4501, -20338.2507,
          39672.3309,  -3513.2909]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6300936752637487, distance: 1.461042921270654 entropy 11.569125439409438
epoch: 72, step: 47
	action: tensor([[-13535.3233,  10148.3732,  12595.0167,   9389.7692,   2900.1469,
           2914.0060,  24831.6717]], dtype=torch.float64)
	q_value: tensor([[-27.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13140917073463942, distance: 1.0665084804029026 entropy 11.914239304095535
epoch: 72, step: 48
	action: tensor([[-57261.8632, -55384.9948,   7192.0187, -16987.2730, -57543.1693,
         -53029.0983,  19509.9094]], dtype=torch.float64)
	q_value: tensor([[-35.0604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6492198262488273, distance: 1.4695892511754634 entropy 12.108871179059408
epoch: 72, step: 49
	action: tensor([[  4035.1539, -75918.5013, -22970.5645, -23062.7891, -31672.4425,
          23883.9619,  -4208.7005]], dtype=torch.float64)
	q_value: tensor([[-32.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023065212082179043, distance: 1.157466291164833 entropy 12.140241749898044
epoch: 72, step: 50
	action: tensor([[-10443.4346,  -2910.8488, -36610.0378, -48072.1057, -21383.6936,
          38762.6061,  28695.0094]], dtype=torch.float64)
	q_value: tensor([[-27.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.488507945166613, distance: 1.3961506145486782 entropy 11.783931365061347
epoch: 72, step: 51
	action: tensor([[-36981.6751,   7200.9503, -43609.2996,  18919.7435,  10411.4479,
          49330.2914, -12906.3012]], dtype=torch.float64)
	q_value: tensor([[-27.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17139261335107203, distance: 1.0416722470106294 entropy 11.940864185539814
epoch: 72, step: 52
	action: tensor([[  2026.2461,  14935.2115,  23489.6807,  -3189.2145, -51410.4682,
           3820.6318, -62214.9569]], dtype=torch.float64)
	q_value: tensor([[-27.3426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4574348040651285, distance: 0.842912819782454 entropy 11.787015836974438
epoch: 72, step: 53
	action: tensor([[ -1017.4106, -84474.4223,   7402.9588,   2132.3331,  16640.0829,
          -3811.6854,   5376.0214]], dtype=torch.float64)
	q_value: tensor([[-38.7261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.031019776387508
epoch: 72, step: 54
	action: tensor([[-27919.6119,   1938.1828,  29167.6402,  -1594.0216,  42259.7614,
         -26123.5004,  20790.6692]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3553993800207742, distance: 0.9187599442136044 entropy 11.569125439409438
epoch: 72, step: 55
	action: tensor([[ -7565.1311, -52885.7516, -47418.7884,   8925.8047,  38851.2807,
           1103.7879, -10039.8211]], dtype=torch.float64)
	q_value: tensor([[-34.8210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.790385749832921, distance: 1.5311932274708395 entropy 11.867157852622134
epoch: 72, step: 56
	action: tensor([[-67135.4201,  10565.1859,  12421.5053, -30466.1072, -34667.1406,
          27092.7087,  -5972.7357]], dtype=torch.float64)
	q_value: tensor([[-25.2795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11294816424352072, distance: 1.0777826615882489 entropy 11.986388325315179
epoch: 72, step: 57
	action: tensor([[  69636.5073, -133438.5309,  -82996.6681,   35050.3864,    9112.1702,
           55005.4119,   38005.5384]], dtype=torch.float64)
	q_value: tensor([[-40.9832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23452953624758155, distance: 1.0012001641977253 entropy 12.103798806903882
epoch: 72, step: 58
	action: tensor([[-47390.8379,   2178.3470,  55069.3059, -52671.8210, -51652.3460,
          18746.2461,  -5642.8006]], dtype=torch.float64)
	q_value: tensor([[-29.1984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.021665749480773
epoch: 72, step: 59
	action: tensor([[-52144.8126, -22905.6497,  -4907.8036, -10711.6706,  -1172.5563,
          29854.9049, -14133.1771]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1881984712397733, distance: 1.0310545086384775 entropy 11.569125439409438
epoch: 72, step: 60
	action: tensor([[-66088.9605,   6178.1043,  66908.3252, -20458.8039,  47152.0290,
           4804.5272, -20832.0409]], dtype=torch.float64)
	q_value: tensor([[-29.6189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2810856115081861, distance: 0.9702759821414046 entropy 12.028564295246062
epoch: 72, step: 61
	action: tensor([[  48654.8454,   74984.5830,   28156.1824,  -27006.6985, -108847.7629,
          -15316.1356,    8992.7311]], dtype=torch.float64)
	q_value: tensor([[-41.1936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.177145831947852
epoch: 72, step: 62
	action: tensor([[  3778.9543,  34320.7964,   2497.0645,   5314.2959, -15998.2559,
          25717.5237,  -7253.6141]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7503447314662751, distance: 0.5717774994399278 entropy 11.569125439409438
epoch: 72, step: 63
	action: tensor([[-29339.2114, -21565.3806, -13453.2254,  83689.3196,  72164.5317,
          55124.2713, -60624.0686]], dtype=torch.float64)
	q_value: tensor([[-29.3631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8258387815387926, distance: 1.5462791751973253 entropy 12.098248257017772
epoch: 72, step: 64
	action: tensor([[ -5417.3092,  76780.1195,   7138.3594, -11196.0581,  31811.3315,
         -62557.2070, -14797.7885]], dtype=torch.float64)
	q_value: tensor([[-30.6577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23773391129531496, distance: 1.2731229240864412 entropy 12.220751834957222
epoch: 72, step: 65
	action: tensor([[-69361.8333,   5314.3310, -36964.9240,  28719.2835, -43939.7665,
         -55165.4021, -35139.7435]], dtype=torch.float64)
	q_value: tensor([[-29.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5925499578453552, distance: 0.7304556906250536 entropy 11.803586537832183
epoch: 72, step: 66
	action: tensor([[ -51518.2274, -102058.3449,   22844.7388,  -48093.1251,  -77886.2376,
          -58380.3977,  -12034.9898]], dtype=torch.float64)
	q_value: tensor([[-32.4738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02319206215716163, distance: 1.1309965583299233 entropy 12.116540968766833
epoch: 72, step: 67
	action: tensor([[  1234.2508,  -3371.6016,  -2855.2508,  10198.4804, -85631.5097,
          17763.2317,  10116.4436]], dtype=torch.float64)
	q_value: tensor([[-29.6573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1975866755460498, distance: 1.2523051625918487 entropy 12.07182994503385
epoch: 72, step: 68
	action: tensor([[-40158.3351,  -8354.2789,  55901.9814,  38665.6258, -37305.8872,
          -7772.4470, -32538.7341]], dtype=torch.float64)
	q_value: tensor([[-27.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09028899657742284, distance: 1.0914614900788293 entropy 12.038123656755127
epoch: 72, step: 69
	action: tensor([[ 19119.6221, -77555.5461, -32445.0511, -43024.6505, -28775.2433,
         -12985.8482,   5430.9761]], dtype=torch.float64)
	q_value: tensor([[-30.2836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1032023046687538, distance: 1.08368717829856 entropy 12.240652765482906
epoch: 72, step: 70
	action: tensor([[-34441.8723, -29126.7161, -35359.3373,  -3184.0158,   4951.4224,
          29547.1303, -54055.0061]], dtype=torch.float64)
	q_value: tensor([[-27.1223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9150107316304017, distance: 1.5835883543583582 entropy 11.902149566776975
epoch: 72, step: 71
	action: tensor([[-32650.4749, -27982.6081, -10974.3767,   8203.8993,  14403.6073,
           4499.6123,   1181.9839]], dtype=torch.float64)
	q_value: tensor([[-26.5561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8907219471185426, distance: 1.5735136908703202 entropy 11.94312353002854
epoch: 72, step: 72
	action: tensor([[-13497.0563, -28183.1669, -45475.4382,  -3213.9462,  41600.0835,
         -35460.9184, -19296.5024]], dtype=torch.float64)
	q_value: tensor([[-27.4093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1518198849639838, distance: 1.2281431187098137 entropy 12.056113486913233
epoch: 72, step: 73
	action: tensor([[-47430.2165, -96872.8214, -39010.2317, -10860.3333,  46855.3068,
         -18704.6214,  51813.8183]], dtype=torch.float64)
	q_value: tensor([[-28.8659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3671800962199585, distance: 1.3380414179479994 entropy 12.013519103442745
epoch: 72, step: 74
	action: tensor([[-70039.0475, -29159.0753,  -5633.1698, -38544.0447,  -5696.5643,
          49003.3703,  21225.1179]], dtype=torch.float64)
	q_value: tensor([[-29.2722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07737207106774835, distance: 1.1877896859245822 entropy 12.07542404781079
epoch: 72, step: 75
	action: tensor([[ 92502.6538, -57734.3120, -37935.3078,  59253.2227,    303.1607,
          26310.9174, -11531.0228]], dtype=torch.float64)
	q_value: tensor([[-30.8120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3037014733966772, distance: 1.3066093794875298 entropy 12.132263113478988
epoch: 72, step: 76
	action: tensor([[ -3891.8422, -47692.8102,  62758.7334,    350.3883,  -3610.4554,
         -36421.0971, -72098.5074]], dtype=torch.float64)
	q_value: tensor([[-28.1760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16929837004177817, distance: 1.0429877907192107 entropy 11.91963895005767
epoch: 72, step: 77
	action: tensor([[ -9660.7353,  -7395.1567,   4683.3614, -26722.4136, -26281.1927,
          -8763.3482,   4469.8902]], dtype=torch.float64)
	q_value: tensor([[-30.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.696534335379283, distance: 1.490520728343403 entropy 12.076336070018957
epoch: 72, step: 78
	action: tensor([[-37290.8266, -67003.8020,  36399.0621,   3321.1842,  21593.4984,
         -37792.0130,  58044.6895]], dtype=torch.float64)
	q_value: tensor([[-28.6350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22926592186925565, distance: 1.0046365496749738 entropy 12.076427119091743
epoch: 72, step: 79
	action: tensor([[  9844.7365,  45260.1918,  46623.5439,  77733.8434, -40873.2384,
         -12149.7160, -43885.3067]], dtype=torch.float64)
	q_value: tensor([[-28.7679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.148281429504735
epoch: 72, step: 80
	action: tensor([[-55525.4445,   5352.1488,  43105.6833,  50850.2542,  24594.8982,
         -20623.4046,  39081.6201]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08256133230432605, distance: 1.096087471140031 entropy 11.569125439409438
epoch: 72, step: 81
	action: tensor([[ 36372.5257, -29046.0834,  -5501.9222,  -1798.0154,  13272.3231,
          53779.5320,  32461.6860]], dtype=torch.float64)
	q_value: tensor([[-31.2540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1368805848068776, distance: 1.0631441068731473 entropy 12.11773577887202
epoch: 72, step: 82
	action: tensor([[ 59361.1673,   1845.8133,  -4228.2526, -11314.8795, -20941.4537,
         -45987.1089, -33939.0986]], dtype=torch.float64)
	q_value: tensor([[-28.2246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.997886231059836
epoch: 72, step: 83
	action: tensor([[-12595.9350,  -4116.8179,   3949.6926,  47262.5117,  -8580.4579,
          -4138.6008,  12905.2185]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04043588231645601, distance: 1.1209692335391088 entropy 11.569125439409438
epoch: 72, step: 84
	action: tensor([[-45335.5107, -23213.0311,  40424.4783,  11185.9579, -38032.3730,
          -9692.0719, -42106.0210]], dtype=torch.float64)
	q_value: tensor([[-27.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.83747562154073, distance: 1.5511988931979337 entropy 12.075999709169396
epoch: 72, step: 85
	action: tensor([[-39634.2610, -97330.3320,  27960.2135, -26842.9520, -93763.2197,
          -5006.2485,  60179.1895]], dtype=torch.float64)
	q_value: tensor([[-28.6916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2843414008235585, distance: 1.2968714644517287 entropy 12.14842651888675
epoch: 72, step: 86
	action: tensor([[-21354.7722, -16649.7810,  26171.0108,   1811.9022,  -5237.7785,
         -67416.4166, -55660.7948]], dtype=torch.float64)
	q_value: tensor([[-25.0657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07342462002775552, distance: 1.1856116805322356 entropy 11.792836993370617
epoch: 72, step: 87
	action: tensor([[-40105.8039, -80834.6953, -76652.1360,   -331.9002,  36821.9733,
          36655.2875,  21189.3711]], dtype=torch.float64)
	q_value: tensor([[-29.3535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0435287163667324, distance: 1.6358635126922532 entropy 12.058524155973062
epoch: 72, step: 88
	action: tensor([[ 28886.1082,   6387.9505,  11440.3178, -41704.3946,  14586.2647,
           1868.3927,  88854.6130]], dtype=torch.float64)
	q_value: tensor([[-29.6374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.178959054981751
epoch: 72, step: 89
	action: tensor([[10498.7010, -3263.1611,  4906.8659, 30693.9027, -8312.5115, 54474.4413,
         -9537.7086]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.374194482000064, distance: 0.9052663708868671 entropy 11.569125439409438
epoch: 72, step: 90
	action: tensor([[  6578.4369, -28096.0603,  -2757.2669, -33693.8827, -40350.1465,
           6451.7065,  13756.4173]], dtype=torch.float64)
	q_value: tensor([[-26.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29131545699718053, distance: 1.3003877451181403 entropy 11.816057747992172
epoch: 72, step: 91
	action: tensor([[  5779.3474, -17205.5621, -59503.4297,  21535.6525,  34377.5043,
          -1576.7793,  32180.0303]], dtype=torch.float64)
	q_value: tensor([[-27.2275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5325246353631494, distance: 0.7824130000573221 entropy 11.777722697557207
epoch: 72, step: 92
	action: tensor([[-69827.1795,  58608.2678,  12757.7979,  23639.3906,  14355.8106,
         -41078.7514, -32474.0003]], dtype=torch.float64)
	q_value: tensor([[-33.3323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3419875632783278, distance: 0.9282687800076034 entropy 12.009518709118494
epoch: 72, step: 93
	action: tensor([[-33280.1470,  22562.9055,   9536.5669,  29263.4303, -13535.2940,
          27652.7343,  -7078.5847]], dtype=torch.float64)
	q_value: tensor([[-28.5458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15685926210867984, distance: 1.0507677379544975 entropy 12.081050182460897
epoch: 72, step: 94
	action: tensor([[ 53679.3679, -72656.4843, -26060.9515, -20930.9450,  24792.7662,
          60872.6841,  50657.8067]], dtype=torch.float64)
	q_value: tensor([[-30.4353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.056716621631066
epoch: 72, step: 95
	action: tensor([[32731.1343, 13680.9929, 18311.9067, 48559.3158,  7295.6096, 16851.9629,
          1388.5102]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10678581271870602, distance: 1.0815198617831008 entropy 11.569125439409438
epoch: 72, step: 96
	action: tensor([[ 14296.0195, -50702.7960,  21282.6265,  46760.2662,   8379.9726,
          39947.2376, -13629.3071]], dtype=torch.float64)
	q_value: tensor([[-20.7178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.446111944476618
epoch: 72, step: 97
	action: tensor([[-18411.5667, -39895.5816,   4001.9503,  -1830.7770,   7926.8533,
          36066.3012, -21259.3050]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0043423386404009, distance: 1.1468261277704792 entropy 11.569125439409438
epoch: 72, step: 98
	action: tensor([[ -1767.8771, -57557.4135,  40805.7403,  -3118.4498, -30499.9097,
           2187.6744,  42794.6817]], dtype=torch.float64)
	q_value: tensor([[-30.2401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7365223616926955, distance: 1.5079845200334916 entropy 12.024999877631899
epoch: 72, step: 99
	action: tensor([[ 16631.5326,   2041.9306,  26650.3163,  45289.8906,  31531.9108,
         -20714.6159, -24920.7492]], dtype=torch.float64)
	q_value: tensor([[-27.3064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.907243184874426
epoch: 72, step: 100
	action: tensor([[  7115.0838, -38433.4443,  13650.4438,  13022.2897, -39086.4979,
          34414.2619,  41959.8830]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06310452024699365, distance: 1.107649268631547 entropy 11.569125439409438
epoch: 72, step: 101
	action: tensor([[ -8427.5643, -41791.8228, -12753.2177,  28261.1665,   -922.5735,
          63676.5607,  73476.1179]], dtype=torch.float64)
	q_value: tensor([[-28.1806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09866736487195826, distance: 1.0864237257514484 entropy 11.985761121836104
epoch: 72, step: 102
	action: tensor([[-104255.0732,  -25123.3587,  -47145.7173,   -5297.8339,   -5760.3871,
           53065.6491,   43270.2524]], dtype=torch.float64)
	q_value: tensor([[-26.2113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8578932563813073, distance: 1.559793378766355 entropy 11.98910382212201
epoch: 72, step: 103
	action: tensor([[-42655.9075, -60433.5993,  27810.5315,  50296.7796,   9896.8645,
           7539.7848,  31244.5719]], dtype=torch.float64)
	q_value: tensor([[-30.0937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22426271428395017, distance: 1.007892065754244 entropy 12.12608833810537
epoch: 72, step: 104
	action: tensor([[-93756.8923, -19261.2681, -40592.4129,  74822.6360, -31337.3371,
          16474.6182,  28812.9901]], dtype=torch.float64)
	q_value: tensor([[-26.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2933881784408763, distance: 1.3014309684558747 entropy 12.04010748787445
epoch: 72, step: 105
	action: tensor([[-80540.6400, -12217.5375, -60534.2401,  94064.6362,  59349.2838,
         -71062.5493,  89342.6395]], dtype=torch.float64)
	q_value: tensor([[-31.8553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46883367265963605, distance: 1.3868931505226323 entropy 12.411247341118154
epoch: 72, step: 106
	action: tensor([[-45481.9279, -42826.0627, -11448.0221,  25238.7673, -89704.9601,
          10768.0702,  -4510.9189]], dtype=torch.float64)
	q_value: tensor([[-31.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19100192367561253, distance: 1.0292726617258785 entropy 12.280300241410945
epoch: 72, step: 107
	action: tensor([[-34026.4480, -20257.3289,   -894.9585, -80521.9374,   9080.3297,
         -12973.5762,  35339.3253]], dtype=torch.float64)
	q_value: tensor([[-26.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3359998994775324, distance: 1.322695591322872 entropy 11.975442731760255
epoch: 72, step: 108
	action: tensor([[-118874.7576,    3643.3440,   52746.3469,   37715.6363,   74063.1239,
            4735.4328,   -4626.9235]], dtype=torch.float64)
	q_value: tensor([[-29.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19718874458821245, distance: 1.0253294217568345 entropy 12.093513179986852
epoch: 72, step: 109
	action: tensor([[ 84523.0542, -80908.9826,  -6772.4261, -18372.5585,  42605.7267,
          24553.7680,  49149.8813]], dtype=torch.float64)
	q_value: tensor([[-31.6329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05600953304811429, distance: 1.1759547553494802 entropy 12.304809342523438
epoch: 72, step: 110
	action: tensor([[-40819.5857, -89795.2241, -28787.9855, 100865.8271,  38196.3947,
          -6895.9196, -34915.6664]], dtype=torch.float64)
	q_value: tensor([[-25.7780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10818667898913881, distance: 1.2046562993552759 entropy 11.89212329400703
epoch: 72, step: 111
	action: tensor([[-27348.6823, -58302.9186,  -7517.4258,  28786.0723, -21519.5422,
          32542.1850,  -9513.5649]], dtype=torch.float64)
	q_value: tensor([[-28.3571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0020709640720446565, distance: 1.1431586919497285 entropy 11.978983296278711
epoch: 72, step: 112
	action: tensor([[ 45809.2970, -23419.1607, -17507.9003, -80134.3330,  18712.4303,
          12767.7125, -17382.3637]], dtype=torch.float64)
	q_value: tensor([[-27.6238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.054557480647369094, distance: 1.1126901877131175 entropy 12.106236659389266
epoch: 72, step: 113
	action: tensor([[ 44734.3396, -56226.7033, -36455.1405,  35915.7361, -39762.2972,
         -35291.2240,  14299.4298]], dtype=torch.float64)
	q_value: tensor([[-30.8152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3115969119349251, distance: 0.9494631158224668 entropy 12.066009976473508
epoch: 72, step: 114
	action: tensor([[ 36590.2657, -44489.4887, 105756.8987, -43661.0790,  72491.0886,
         -36991.1292,  -6060.8687]], dtype=torch.float64)
	q_value: tensor([[-31.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3324703844620449, distance: 1.3209472523495176 entropy 12.12493648409158
epoch: 72, step: 115
	action: tensor([[-38255.8498, -33853.4362,  -3389.8248,  48332.1476, -76728.9066,
          51120.0150,  12477.7399]], dtype=torch.float64)
	q_value: tensor([[-29.8343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25939460650229806, distance: 1.2842146141197344 entropy 12.036564474033266
epoch: 72, step: 116
	action: tensor([[ 23805.8785, -62694.7387,  47250.3834,  42961.5729,  -7985.0125,
         -10246.5771,  -6799.4926]], dtype=torch.float64)
	q_value: tensor([[-29.0750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03706288271142444, distance: 1.122937685584492 entropy 12.10771500001662
epoch: 72, step: 117
	action: tensor([[-28547.6909,  37952.5775, -12700.5285,  12218.5244, -14763.8661,
         -31721.6043, -16461.9517]], dtype=torch.float64)
	q_value: tensor([[-32.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7259353283613816, distance: 0.5990777933488562 entropy 11.88804947578537
epoch: 72, step: 118
	action: tensor([[ 19950.8789, -49020.8675, -57651.6287,  -9282.0107, -75674.1045,
          -6660.5877,  40653.6391]], dtype=torch.float64)
	q_value: tensor([[-29.4049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.990847488103563
epoch: 72, step: 119
	action: tensor([[ -5699.1077, -35401.3824, -24238.1975, -12286.1791, -21647.4890,
         -15204.9843, -15155.8496]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0631780911625166, distance: 1.17993939839655 entropy 11.569125439409438
epoch: 72, step: 120
	action: tensor([[-43371.9589, -14809.1192, -16227.7706,  88036.2861,  60560.9387,
         -40553.7452,  10430.8187]], dtype=torch.float64)
	q_value: tensor([[-31.5839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11343478037783372, distance: 1.2075054091394128 entropy 12.125107614786357
epoch: 72, step: 121
	action: tensor([[-48340.7662, -24342.4720,  50131.8018,  75326.2798, -55567.3202,
         -90017.6046,   3076.5498]], dtype=torch.float64)
	q_value: tensor([[-28.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18450976725994006, distance: 1.2454491952679891 entropy 12.175085580731363
epoch: 72, step: 122
	action: tensor([[ 48573.3260,   1171.6345,   9041.7111,  57348.7688, -26345.4690,
          -8527.2064,  54092.8408]], dtype=torch.float64)
	q_value: tensor([[-27.6171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.085467773285332
epoch: 72, step: 123
	action: tensor([[-16470.7155,   2706.4504, -16307.8930, -19366.9173, -11640.6189,
           8596.7225,  30110.1655]], dtype=torch.float64)
	q_value: tensor([[-30.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.131218819410033, distance: 1.0666253364494245 entropy 11.569125439409438
epoch: 72, step: 124
	action: tensor([[-11005.6875,  41949.5868, -23811.4894, -88722.7415,  -7132.7052,
         -15423.3238,   6726.7751]], dtype=torch.float64)
	q_value: tensor([[-44.8342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22555108404264512, distance: 1.2668418519850764 entropy 12.18410233985521
epoch: 72, step: 125
	action: tensor([[46939.6054, 89965.5458, 31871.7289,   725.2406, 39031.4681, 68099.5616,
         48797.8290]], dtype=torch.float64)
	q_value: tensor([[-35.1163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07036217079248486, distance: 1.183919214702327 entropy 12.089780932958886
epoch: 72, step: 126
	action: tensor([[-27771.6735,  32648.8507, -81553.1345,  17426.4366, -10018.0321,
          12336.6277,  90453.7396]], dtype=torch.float64)
	q_value: tensor([[-24.7663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7284330140096142, distance: 1.5044680520278255 entropy 11.87053589732767
epoch: 72, step: 127
	action: tensor([[ -2337.9385, -47102.1986,  17124.5737, -74068.6377,  -5005.8787,
          56505.6588, -28148.6670]], dtype=torch.float64)
	q_value: tensor([[-33.8183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.332063499235683, distance: 1.3207455536981683 entropy 12.086645571744185
LOSS epoch 72 actor 355.5634530223924 critic 243.23212136678578
epoch: 73, step: 0
	action: tensor([[-73657.8754, -80449.9137,   -891.2319,  90092.3066,  -4749.0413,
          49497.9250, -14796.9350]], dtype=torch.float64)
	q_value: tensor([[-34.7101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20459372879838833, distance: 1.2559634242514555 entropy 12.116896063904276
epoch: 73, step: 1
	action: tensor([[ 44284.3619, -67198.6396,  51833.6798,  59264.4872,  25117.6267,
          -3014.8625,  -6254.6236]], dtype=torch.float64)
	q_value: tensor([[-27.8154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30550799247106075, distance: 0.9536528682329737 entropy 11.845117510652189
epoch: 73, step: 2
	action: tensor([[  -8928.1293,   20689.4173,    4481.9781,   60072.1507,  -24186.3652,
         -104114.3044,    1353.3272]], dtype=torch.float64)
	q_value: tensor([[-37.4694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022305980988548635, distance: 1.1570367254402858 entropy 12.208428171376912
epoch: 73, step: 3
	action: tensor([[-119661.6778, -124666.3125,   41018.8790,   95632.2581,   59149.4701,
          -47661.7336,  -52778.4675]], dtype=torch.float64)
	q_value: tensor([[-35.3249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24992974923770683, distance: 1.2793798183242224 entropy 12.259801523358846
epoch: 73, step: 4
	action: tensor([[-70947.8831, -63891.6542,  80168.2928,  35680.8182, -47724.4492,
          -5397.3399,   5565.3706]], dtype=torch.float64)
	q_value: tensor([[-33.6177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2874264165555569, distance: 1.298428086859714 entropy 12.09945321613739
epoch: 73, step: 5
	action: tensor([[ 42498.2802,  -5614.8509,  25638.1189,  40954.0766, -35653.8895,
          47935.7916,  -7440.2632]], dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30194582578849505, distance: 1.3057293013052935 entropy 12.118956891482387
epoch: 73, step: 6
	action: tensor([[ -56346.3082,  -57921.1163,   45026.5103, -115558.4706,  -29495.1388,
          -38086.2015,   27260.3426]], dtype=torch.float64)
	q_value: tensor([[-37.1726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45628580189720225, distance: 1.3809565076132677 entropy 12.247129201162469
epoch: 73, step: 7
	action: tensor([[-27217.5484, -26674.9084,  20460.8025,  10954.9636,  68496.9776,
          66830.3772, -41429.6906]], dtype=torch.float64)
	q_value: tensor([[-31.7861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2854151766597155, distance: 1.2974134769964647 entropy 12.042818752088042
epoch: 73, step: 8
	action: tensor([[ 12033.9639,  35222.8607,   -475.2634,  39381.6895,  28954.7065,
         -76044.6605, -12784.1230]], dtype=torch.float64)
	q_value: tensor([[-36.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47528981617183097, distance: 0.8289272941317312 entropy 12.346735411350007
epoch: 73, step: 9
	action: tensor([[ 22971.1640,  -8706.8834,   9368.9213,  63357.8293, -18881.9661,
         -77143.9849,  -7617.6105]], dtype=torch.float64)
	q_value: tensor([[-32.8003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048606100195590196, distance: 1.171825334816058 entropy 12.031491220386176
epoch: 73, step: 10
	action: tensor([[-125850.4509,  -58979.0742,  -12356.6716,   43998.2454,   14461.8214,
          -54270.5907,    8225.0385]], dtype=torch.float64)
	q_value: tensor([[-38.5042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0825498725069087, distance: 1.1906404965640753 entropy 12.050201308193126
epoch: 73, step: 11
	action: tensor([[-40602.6177, -84043.6185,   3484.6126,  18440.0459, -87626.8241,
          23056.0092,  25025.5438]], dtype=torch.float64)
	q_value: tensor([[-33.7805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09787340749438467, distance: 1.0869021197583884 entropy 12.17480379286812
epoch: 73, step: 12
	action: tensor([[-133718.3774,  -27352.9034,  -56669.4027,  129349.4684,    2677.5229,
          -61810.4261,  -20720.3284]], dtype=torch.float64)
	q_value: tensor([[-33.1400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4673736436429172, distance: 1.3862036893118856 entropy 12.22540783476664
epoch: 73, step: 13
	action: tensor([[-46018.6141, -71475.1045, -58897.3669, -47475.0254, 101944.0610,
          44422.7848,  36777.0668]], dtype=torch.float64)
	q_value: tensor([[-31.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1778915090256867, distance: 1.688786754795211 entropy 12.045774714814241
epoch: 73, step: 14
	action: tensor([[-52908.3867,  10341.0541,  45052.3204,  32262.7687,  52026.0565,
         -58262.3761, -41390.9493]], dtype=torch.float64)
	q_value: tensor([[-27.5738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5941425269478751, distance: 0.7290267545630671 entropy 11.840689635776087
epoch: 73, step: 15
	action: tensor([[ 14118.4386, -24415.2152,  20567.1801,  34996.8873,    331.1252,
          35285.3423,  -5426.5112]], dtype=torch.float64)
	q_value: tensor([[-37.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06795480432564183, distance: 1.1047784121712794 entropy 12.149754179189902
epoch: 73, step: 16
	action: tensor([[-16288.6892, -14755.5597,  30829.1794,  58524.2349,   9548.1527,
         -59574.5066,   9000.2425]], dtype=torch.float64)
	q_value: tensor([[-35.5521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2071066238531334, distance: 1.2572727702719582 entropy 12.19894587732871
epoch: 73, step: 17
	action: tensor([[  15901.3375, -164964.5365,  -53976.6194,  -23311.8547, -101328.1939,
          -45814.4681,   -3200.0508]], dtype=torch.float64)
	q_value: tensor([[-38.2101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40548967070272324, distance: 0.8823408687768423 entropy 12.396202407243834
epoch: 73, step: 18
	action: tensor([[ -5533.8665, -54769.6251, -82286.6792, -42598.6780, -24503.4320,
         -46673.9148,  15274.3705]], dtype=torch.float64)
	q_value: tensor([[-33.0592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1679038397102015, distance: 1.0438628751942682 entropy 11.890549307848195
epoch: 73, step: 19
	action: tensor([[  54415.7748,   -1010.2216, -116756.5258,   15549.0981,  -39734.5545,
          -75280.2792,   53023.3983]], dtype=torch.float64)
	q_value: tensor([[-37.8229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0019834520510715326, distance: 1.1432088147193342 entropy 12.150609262743648
epoch: 73, step: 20
	action: tensor([[  1878.8665,   -107.2308,  -8924.3092,  30879.2081,  29244.4139,
         -65885.4790,  23385.7533]], dtype=torch.float64)
	q_value: tensor([[-28.4522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12174573218368201, distance: 1.0724247485347351 entropy 11.859916664336211
epoch: 73, step: 21
	action: tensor([[-39549.2696, -18052.4503, -57446.0845,  32210.5206,  49237.4658,
          -4234.8625,  86054.4843]], dtype=torch.float64)
	q_value: tensor([[-37.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1886747593534044, distance: 1.0307520017815983 entropy 12.088129240409687
epoch: 73, step: 22
	action: tensor([[-74035.5994,  29508.7680, -50326.7551,  93956.9222, -50259.9192,
          89167.8190,  -6169.8698]], dtype=torch.float64)
	q_value: tensor([[-36.2765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.10375467008585
epoch: 73, step: 23
	action: tensor([[-41013.1755, -22556.8306,  -7962.0579,  60266.2233,  20703.6314,
          -9775.0454,   9950.7979]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.750217885055187, distance: 1.513919392196687 entropy 11.604792306147383
epoch: 73, step: 24
	action: tensor([[-35806.7916, -10356.5415,  13553.2419,  46991.2353,  20225.6857,
         -20249.0631,  59645.6278]], dtype=torch.float64)
	q_value: tensor([[-32.1507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7671053816743265, distance: 1.5212056088299912 entropy 12.072815265822054
epoch: 73, step: 25
	action: tensor([[ 19581.8383,  -9445.4274, -33596.3941,  14603.2134,  77189.0016,
          -3009.7159,  -3903.0331]], dtype=torch.float64)
	q_value: tensor([[-31.5080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.389504332930382, distance: 0.8941244815599299 entropy 11.977583203110992
epoch: 73, step: 26
	action: tensor([[-47906.4155, -29571.5652, -20107.0847,  48860.2980,  30252.0092,
          61120.4886,  28655.9421]], dtype=torch.float64)
	q_value: tensor([[-32.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33746729526217756, distance: 1.3234217834716915 entropy 11.86907024781333
epoch: 73, step: 27
	action: tensor([[-23733.7108, -62266.4338,  50378.2399,  91525.6186, -52477.2214,
          81774.1603,   4133.6578]], dtype=torch.float64)
	q_value: tensor([[-31.2920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13993576362105486, distance: 1.0612608363146434 entropy 12.143421720492382
epoch: 73, step: 28
	action: tensor([[-11624.3992, -77432.6161,  61357.2236,  -9865.3833, -17599.0184,
          16138.8552,  16505.1780]], dtype=torch.float64)
	q_value: tensor([[-30.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6371218496321485, distance: 1.4641891883229767 entropy 12.012229629591888
epoch: 73, step: 29
	action: tensor([[ 24241.9936,  12212.7912,   6790.0071,  37966.1053,  11888.9277,
         -19996.7090,  21275.8089]], dtype=torch.float64)
	q_value: tensor([[-28.3730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7736228583007901, distance: 0.5444687763464469 entropy 11.80475419276839
epoch: 73, step: 30
	action: tensor([[ -4629.8702, -20967.9645,  11468.1140, -31257.7838, 101986.6712,
         -24584.3048,  -7834.3640]], dtype=torch.float64)
	q_value: tensor([[-35.0099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.077545158587013
epoch: 73, step: 31
	action: tensor([[-11241.8471,  21657.6473,   3534.3430,  18176.5183,  15787.9771,
          18226.0058, -20284.3189]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15392410277026258, distance: 1.0525951263678202 entropy 11.604792306147383
epoch: 73, step: 32
	action: tensor([[  8572.2849,  16748.9244, -31517.4612,   9731.2412, -57849.6682,
         -89824.7068,  85200.5241]], dtype=torch.float64)
	q_value: tensor([[-39.3917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4195009032362469, distance: 0.8718815095188842 entropy 12.279770058314483
epoch: 73, step: 33
	action: tensor([[-17664.8731,  -6988.4851, -38094.1397, -26582.9360,  76205.2974,
          10441.2252,  38817.3263]], dtype=torch.float64)
	q_value: tensor([[-31.0157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.841822787326924
epoch: 73, step: 34
	action: tensor([[-42254.0708,     91.2814, -20576.5664, -37476.8811, -47734.1551,
           5996.3448, -22792.7250]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1969563944614532, distance: 1.0254777867582237 entropy 11.604792306147383
epoch: 73, step: 35
	action: tensor([[ -8640.9418, -26607.8874,  20525.5185,  -9234.7724, -55324.6551,
          26557.6405,   5622.8065]], dtype=torch.float64)
	q_value: tensor([[-36.2291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01455238956919247, distance: 1.1359872673527027 entropy 11.750889977391816
epoch: 73, step: 36
	action: tensor([[-59254.5512, -21628.4139,  -4433.1570,  39881.0147,  -7711.2180,
         -26151.0444,  80505.1801]], dtype=torch.float64)
	q_value: tensor([[-34.6300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5475943738650968, distance: 1.4235911005158668 entropy 11.999220278171013
epoch: 73, step: 37
	action: tensor([[ 53009.0254, -61561.1225,  23159.0462, -18835.1665, -19220.1426,
          -8029.3909,  25447.3787]], dtype=torch.float64)
	q_value: tensor([[-33.3797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33450805014940965, distance: 0.9335296075538188 entropy 12.132198952773816
epoch: 73, step: 38
	action: tensor([[ 44259.4079,  13594.6003, -55854.4131,   4468.1603,  -2306.8965,
          -6505.4511,  -2685.4262]], dtype=torch.float64)
	q_value: tensor([[-31.5146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6370128285030013, distance: 0.689449294643172 entropy 11.906832412585995
epoch: 73, step: 39
	action: tensor([[-53328.3025, -69660.8338, -15953.8308,  65641.5377,   -572.2230,
          68729.6190,  46557.2761]], dtype=torch.float64)
	q_value: tensor([[-36.3330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1516979777067018, distance: 1.0539789674100306 entropy 12.110737910343698
epoch: 73, step: 40
	action: tensor([[-79461.2537,  15423.1422, -10717.0525,  25048.2175,  16590.3776,
         -22389.1191,  10840.5516]], dtype=torch.float64)
	q_value: tensor([[-28.8414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13802771422828508, distance: 1.062437385793909 entropy 11.998397306864572
epoch: 73, step: 41
	action: tensor([[101250.7988, -67638.2817,  37929.3786,  48858.5327,  53889.0891,
          50498.0703,  -3502.2626]], dtype=torch.float64)
	q_value: tensor([[-37.2607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27870194265052683, distance: 1.2940210936349872 entropy 12.36162494055431
epoch: 73, step: 42
	action: tensor([[ -1834.9014, -11876.3473, -53490.0398,  80034.8532, -55904.6221,
         111834.9011,  15125.6321]], dtype=torch.float64)
	q_value: tensor([[-33.1619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7829646708333513, distance: 1.5280165642070906 entropy 12.219962298594371
epoch: 73, step: 43
	action: tensor([[  41682.0550,  -60805.8027,  -13471.2841,  -30050.8229,  -28128.0142,
         -105470.6761,    6191.3493]], dtype=torch.float64)
	q_value: tensor([[-36.2529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.036873154184168255, distance: 1.1652510646222012 entropy 12.310747093833276
epoch: 73, step: 44
	action: tensor([[-1477.9088, 16203.7190,  -793.8995, 19075.3959, 47980.4148, -7932.4349,
         -9476.5267]], dtype=torch.float64)
	q_value: tensor([[-32.5301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.986632625623363
epoch: 73, step: 45
	action: tensor([[  5940.6219, -29992.7521,  15688.9583, -24348.0425,  23230.5873,
           7392.1275,  32699.9976]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2269357618983625, distance: 1.0061540621676672 entropy 11.604792306147383
epoch: 73, step: 46
	action: tensor([[ 35674.9770, -42490.8163, -14505.1306, -14654.9079,  22885.8965,
           -638.8627,  20461.0753]], dtype=torch.float64)
	q_value: tensor([[-32.8384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42796249104190165, distance: 0.8655037355571673 entropy 11.924598592209962
epoch: 73, step: 47
	action: tensor([[ 30384.6860, -87527.3754,  23817.7969,  16912.4140,  10878.4779,
          29127.2132, -41659.1520]], dtype=torch.float64)
	q_value: tensor([[-37.4215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06473532227660894, distance: 1.180803207588076 entropy 12.188694623547534
epoch: 73, step: 48
	action: tensor([[  -4118.9154,  -15532.7097, -103277.2278,  -28024.4516,  -10288.2298,
            4514.8883,    8323.9158]], dtype=torch.float64)
	q_value: tensor([[-39.0816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1717113072762293, distance: 1.0414719069285046 entropy 12.19544844784946
epoch: 73, step: 49
	action: tensor([[-42659.1259, -20595.0991,  22466.4944, -51400.5922,   5971.6342,
         -24348.7626,  63535.9985]], dtype=torch.float64)
	q_value: tensor([[-36.9699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3391529540033247, distance: 1.3242554991993938 entropy 12.113403633320715
epoch: 73, step: 50
	action: tensor([[ -2880.7271, -89693.7357,  -2412.9199, -26861.4302, -22129.1239,
          18623.7057,  -3101.5454]], dtype=torch.float64)
	q_value: tensor([[-38.8076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20876529793160792, distance: 1.2581362771983515 entropy 12.21346828849951
epoch: 73, step: 51
	action: tensor([[-23386.1618,  37254.3667,  11314.9097,  42157.6187,  29219.6065,
         -36659.9069,  30657.2667]], dtype=torch.float64)
	q_value: tensor([[-34.0994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0708229549513818, distance: 1.103077254128004 entropy 11.9984056466265
epoch: 73, step: 52
	action: tensor([[ 87826.9803, -24651.2606, -71399.1185,  98761.6761,  26294.0293,
         -28435.5412,  42391.4981]], dtype=torch.float64)
	q_value: tensor([[-37.9512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07416722580621182, distance: 1.1860217185761543 entropy 12.134356469140316
epoch: 73, step: 53
	action: tensor([[-61890.0691, -32678.6157,   3820.9878,  20016.1201,  65781.4200,
         -64555.5792, -19069.8186]], dtype=torch.float64)
	q_value: tensor([[-39.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03894132902778247, distance: 1.1664126061239397 entropy 12.198480385257286
epoch: 73, step: 54
	action: tensor([[  5530.7227, -22591.8625, -33913.0334,  79460.9312,  33920.7387,
          -9329.6954, -50924.8055]], dtype=torch.float64)
	q_value: tensor([[-34.7341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14951585554529534, distance: 1.055333693476025 entropy 12.1529893102508
epoch: 73, step: 55
	action: tensor([[-25946.5281, -54869.5316,  12051.5027,  -7226.1743,  12616.5752,
         -49814.5022,  12130.2442]], dtype=torch.float64)
	q_value: tensor([[-36.0559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41904462764696704, distance: 1.3631847537858224 entropy 12.153704842014562
epoch: 73, step: 56
	action: tensor([[  3905.4908,  -2996.2075, -20910.3800,  18119.8474,  85533.0518,
          62616.4968,  52048.3967]], dtype=torch.float64)
	q_value: tensor([[-28.7331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4808487907014891, distance: 0.8245246204109966 entropy 11.845955875301032
epoch: 73, step: 57
	action: tensor([[ 10605.2862,  11918.4828,  28382.7345,  44684.9488,  10183.0446,
         -31707.2962,  24717.8184]], dtype=torch.float64)
	q_value: tensor([[-29.3917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5284247971006311, distance: 0.7858364577464192 entropy 11.848695080446344
epoch: 73, step: 58
	action: tensor([[-49665.5649,   5975.3476, -87697.5585, -29475.4824,  29807.8749,
          11269.0018, -76419.9513]], dtype=torch.float64)
	q_value: tensor([[-32.5049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37157750345314966, distance: 0.9071572068146704 entropy 11.968062363798145
epoch: 73, step: 59
	action: tensor([[  -911.5122, -25175.1610,  23472.2066,  15350.1745, -28244.0317,
          21180.6562, -23081.9018]], dtype=torch.float64)
	q_value: tensor([[-28.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1866088428053886, distance: 1.0320634941740117 entropy 11.66717425136882
epoch: 73, step: 60
	action: tensor([[ -25381.5399, -131248.0425,  118587.2525,  -25460.2383,   64185.5491,
         -108703.4548,   99317.8847]], dtype=torch.float64)
	q_value: tensor([[-36.0648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8873646317845074, distance: 1.5721160425100467 entropy 12.302728379138951
epoch: 73, step: 61
	action: tensor([[-16853.3461,  24576.0044,  17327.7342,  -8818.8028, -47619.2614,
          -1615.0445, -31507.1887]], dtype=torch.float64)
	q_value: tensor([[-33.3883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08634918746335773, distance: 1.1927280013942207 entropy 12.12908194365077
epoch: 73, step: 62
	action: tensor([[ 41101.5795, -68787.8296, -80781.6928,  19598.8816, -18007.2030,
         -38959.5404,  57314.8353]], dtype=torch.float64)
	q_value: tensor([[-47.1020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.188309387501377
epoch: 73, step: 63
	action: tensor([[-37674.2953,  14625.7498,   4375.0533,  41114.7041,  -5016.9301,
          -7716.5496, -48655.5204]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.604792306147383
epoch: 73, step: 64
	action: tensor([[ 20303.2940,  -3425.1872, -19442.8432,  32624.3999,  15536.9536,
         -16638.0758, -28049.6022]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2415085692538136, distance: 0.9966255862482519 entropy 11.604792306147383
epoch: 73, step: 65
	action: tensor([[-31407.3876,  55976.9495, -65551.3309,  15136.7728, -36688.6775,
          52293.0365,   7187.1275]], dtype=torch.float64)
	q_value: tensor([[-32.5722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6578586209635875, distance: 0.6693595962692102 entropy 12.080056226675397
epoch: 73, step: 66
	action: tensor([[-17073.1861,  -6281.5671, -26493.1447,   9574.4830,  38464.5626,
          30928.9503,  27248.5226]], dtype=torch.float64)
	q_value: tensor([[-31.7112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12211563496252753, distance: 1.0721988830378848 entropy 12.021387235788954
epoch: 73, step: 67
	action: tensor([[-10795.4080,  -8908.0972, -49168.9717,  14803.0821, -66991.3493,
         -24418.6322,  21776.8266]], dtype=torch.float64)
	q_value: tensor([[-34.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40548188159285026, distance: 1.356654685093897 entropy 12.23970767272682
epoch: 73, step: 68
	action: tensor([[ 14304.1804, -17462.2332, -77055.1392, -22055.5777, -63861.7173,
          25000.3708,  27646.5758]], dtype=torch.float64)
	q_value: tensor([[-34.0575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31744343231234295, distance: 1.313477633016286 entropy 12.219594435741056
epoch: 73, step: 69
	action: tensor([[-28886.9951,  15117.2834, -51472.4785,  -7450.6188,   8566.3733,
          36633.2608,    240.1040]], dtype=torch.float64)
	q_value: tensor([[-29.3539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3096833472488647, distance: 0.9507818186904488 entropy 11.714822537839268
epoch: 73, step: 70
	action: tensor([[ -7960.3784, -17000.6375,   -552.1216,  28505.2351, -62118.9958,
          50404.0556, 102072.3468]], dtype=torch.float64)
	q_value: tensor([[-41.4974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7235281248282093, distance: 1.5023318709898883 entropy 11.981764684688565
epoch: 73, step: 71
	action: tensor([[ 15140.5705,  -5105.4368, -59561.3012,     80.4232,   6937.6199,
         -51589.5505,   6467.5576]], dtype=torch.float64)
	q_value: tensor([[-31.7904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1600000303778042, distance: 1.0488088140439653 entropy 12.110896630875526
epoch: 73, step: 72
	action: tensor([[ 10002.0182,   7764.6479, -16528.6025,  30615.4520,  27309.9783,
          20204.5824, -51043.7537]], dtype=torch.float64)
	q_value: tensor([[-36.5285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4411974092924542, distance: 0.8554328005157965 entropy 12.030568294168345
epoch: 73, step: 73
	action: tensor([[-23589.2131,  48674.3872, 121699.8605,  61063.5921,  22512.8258,
          22058.8156,  81499.7581]], dtype=torch.float64)
	q_value: tensor([[-35.2162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3854061985235706, distance: 0.8971205007968561 entropy 12.140334408630286
epoch: 73, step: 74
	action: tensor([[ 19503.1996,   3915.6443, -21512.4980,  15546.0180,  23227.1493,
          16931.8244,  26644.5363]], dtype=torch.float64)
	q_value: tensor([[-31.3525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2686441009462278, distance: 0.9786357521349532 entropy 11.85216180882242
epoch: 73, step: 75
	action: tensor([[-38809.3802, -17824.3677,  39640.5294, -22687.0341,  60370.5295,
         -14379.9577,  29424.3800]], dtype=torch.float64)
	q_value: tensor([[-33.8757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.878155840045185, distance: 1.5682760341937627 entropy 11.996521725352745
epoch: 73, step: 76
	action: tensor([[ -2708.1277, -69302.0839,  80543.0669, -96611.8480,  47251.6573,
          97616.7065, -32686.7648]], dtype=torch.float64)
	q_value: tensor([[-28.6825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04519895616124847, distance: 1.1181836450759401 entropy 11.82386861039076
epoch: 73, step: 77
	action: tensor([[-35225.5911, -70556.8856,  62191.8193, -68692.0780, -73712.0778,
         -22688.0081, -94838.4835]], dtype=torch.float64)
	q_value: tensor([[-34.5930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5250820836748675, distance: 1.4131989393957596 entropy 12.085925136584157
epoch: 73, step: 78
	action: tensor([[-32080.1166, -44939.7334, -16411.4787,  30835.8073, -30217.1714,
          -8428.1395,   5569.6295]], dtype=torch.float64)
	q_value: tensor([[-34.1933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41345844200318727, distance: 1.3604989636382168 entropy 12.048521266532676
epoch: 73, step: 79
	action: tensor([[   695.3849, -17940.1736,  21613.5624,  32002.7774, -35846.6445,
         -34432.2068,  39686.0376]], dtype=torch.float64)
	q_value: tensor([[-30.1479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5356086341929481, distance: 0.7798278867027629 entropy 11.94401164620803
epoch: 73, step: 80
	action: tensor([[-50424.6659, -46802.6084,  26919.2097,   2671.5896,  56253.3995,
         -24066.5582,  17877.3590]], dtype=torch.float64)
	q_value: tensor([[-32.0080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5607416622335246, distance: 1.4296252334508295 entropy 11.96783267188056
epoch: 73, step: 81
	action: tensor([[  2560.0786, -75429.7016,  43535.1712,  59848.1949, -45934.8969,
          33676.4411, -12532.8076]], dtype=torch.float64)
	q_value: tensor([[-32.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.033752696130282
epoch: 73, step: 82
	action: tensor([[-10754.5441, -25881.6731,  29858.3327,   7097.8795, -20841.3713,
         -39465.1839, -29967.2281]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8359704278595264, distance: 1.5505634199709477 entropy 11.604792306147383
epoch: 73, step: 83
	action: tensor([[-31246.2709, -10875.4837, -13546.6883, -16348.2645,  26299.6646,
         -32478.8290,  86937.9912]], dtype=torch.float64)
	q_value: tensor([[-33.2610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4052276267401813, distance: 1.3565319685925543 entropy 12.094081617411883
epoch: 73, step: 84
	action: tensor([[-24237.7394, -81834.8726,  36500.6334,   -190.0665,  -1141.5917,
         -66356.3621,  39040.1140]], dtype=torch.float64)
	q_value: tensor([[-31.5624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.831080122663689, distance: 1.5484969965897146 entropy 12.0715047774127
epoch: 73, step: 85
	action: tensor([[ 23507.9665,  67202.7634, -22637.3461,  40785.1108, -13631.4305,
         102795.7857, -85629.8706]], dtype=torch.float64)
	q_value: tensor([[-35.7464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.225591925782267
epoch: 73, step: 86
	action: tensor([[ 25887.7654,   5047.3768,  28177.4562, -13548.8115, -32955.6511,
         -55492.9804, -56299.4298]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4266850948568405, distance: 0.8664695591499911 entropy 11.604792306147383
epoch: 73, step: 87
	action: tensor([[ -4134.7185, -19986.5805,   1883.6012,  13996.0098,  38001.2683,
           4343.4238,  24545.4942]], dtype=torch.float64)
	q_value: tensor([[-39.4574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.012105382644775
epoch: 73, step: 88
	action: tensor([[ 3.5404e+03, -1.0365e+04,  1.9898e+01, -1.1444e+04,  1.1202e+04,
         -2.2857e+04,  6.4521e+03]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0957751484826217, distance: 1.0881654001071195 entropy 11.604792306147383
epoch: 73, step: 89
	action: tensor([[ 55537.8178, -38565.5506, -10039.0777,   3236.6908,   1632.3708,
          10041.1649,  42211.1135]], dtype=torch.float64)
	q_value: tensor([[-31.9941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22187618403512277, distance: 1.0094412485642685 entropy 11.877963847569564
epoch: 73, step: 90
	action: tensor([[ 27719.5527, -25143.6277,  -9605.4587,  98070.5233,  -6108.6151,
          -5921.1524,  32533.6961]], dtype=torch.float64)
	q_value: tensor([[-31.3294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23472147436328106, distance: 1.2715726963336689 entropy 11.79798906185513
epoch: 73, step: 91
	action: tensor([[-11954.7174, -47930.5032,  15433.0899,  42069.1994, -57058.1417,
         -35909.1136,  28988.1558]], dtype=torch.float64)
	q_value: tensor([[-36.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03799016480267314, distance: 1.1223968762270893 entropy 11.989880255062394
epoch: 73, step: 92
	action: tensor([[ 26462.6944, -36609.7305, -75368.3813,  59556.9091,  18224.2549,
         -33576.2222, -61925.9609]], dtype=torch.float64)
	q_value: tensor([[-33.9005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4498443484455906, distance: 0.8487884918076731 entropy 12.07934408562932
epoch: 73, step: 93
	action: tensor([[ -5948.8383,  13990.8758,  65692.6777,  75222.6301, -28950.6112,
          86308.7751, -24586.6937]], dtype=torch.float64)
	q_value: tensor([[-41.2738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.34427623175296
epoch: 73, step: 94
	action: tensor([[  1848.0360, -60889.6052, -33941.1068, -18377.9643,   2055.4088,
         -51051.7979,  -1665.8262]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49002185098366047, distance: 0.8172077515329249 entropy 11.604792306147383
epoch: 73, step: 95
	action: tensor([[-32889.2667, -36223.2170,  54839.6714,   7161.1877,  -3189.7293,
         -46117.4999,  14442.5129]], dtype=torch.float64)
	q_value: tensor([[-33.7811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5651139290696567, distance: 1.4316263113945993 entropy 11.834674676769579
epoch: 73, step: 96
	action: tensor([[-49224.7210, -56660.6400,  -2861.9417, -15157.8325,  71858.5605,
         -32293.2790,  49789.2916]], dtype=torch.float64)
	q_value: tensor([[-32.7365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44786466617018017, distance: 1.3769579508832137 entropy 12.04255248794087
epoch: 73, step: 97
	action: tensor([[ 16116.3521, -76559.4447,   5332.6512,  44184.9268, -44249.4423,
          94365.7790, -11818.7071]], dtype=torch.float64)
	q_value: tensor([[-40.8516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027717349275165004, distance: 1.1600949527629736 entropy 12.342460654982062
epoch: 73, step: 98
	action: tensor([[-21759.7236, -76781.8808,  -3892.4856, -13499.1459,  34858.6628,
         -47586.6278, -23249.4549]], dtype=torch.float64)
	q_value: tensor([[-34.7718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13682659429071942, distance: 1.220123538614913 entropy 12.093602907692913
epoch: 73, step: 99
	action: tensor([[ -50570.3409, -118219.3435,  -18689.4529,  117207.2996,  -65242.7315,
          -28642.2216,   -1891.8605]], dtype=torch.float64)
	q_value: tensor([[-41.1220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17695525528689426, distance: 1.2414712582181138 entropy 12.323651338085964
epoch: 73, step: 100
	action: tensor([[-33020.3496, -33271.9800,  99927.6560,  26250.2525, -99102.3136,
          -1074.1852,  44716.0956]], dtype=torch.float64)
	q_value: tensor([[-33.2326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14511832736954589, distance: 1.0580585391501027 entropy 12.073500346919086
epoch: 73, step: 101
	action: tensor([[-19540.7334, -51145.9081, -96585.2094, -21286.2275,  49386.8098,
         -34647.0939, -12537.0478]], dtype=torch.float64)
	q_value: tensor([[-33.0384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.389737505579951, distance: 1.349034580701448 entropy 12.177612417278207
epoch: 73, step: 102
	action: tensor([[-36485.7785,  15371.7842, -47089.0994, -62347.8255, -12416.4826,
          49319.0929, -35636.1830]], dtype=torch.float64)
	q_value: tensor([[-30.6897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1473911629718232, distance: 1.056651096634166 entropy 11.928887105338314
epoch: 73, step: 103
	action: tensor([[-127562.9400,   15051.3344,  -10851.0222,  -34970.4506,   -7579.5959,
          -17643.8747,  -20621.8986]], dtype=torch.float64)
	q_value: tensor([[-48.4809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023469623508300064, distance: 1.157695038237148 entropy 12.075910646317478
epoch: 73, step: 104
	action: tensor([[-31718.3348,  21921.8707, -63678.3984, -53113.7616,  16113.6996,
          25146.0832,  27800.4897]], dtype=torch.float64)
	q_value: tensor([[-40.2867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3367089436009626, distance: 0.9319846596837746 entropy 11.979632406038093
epoch: 73, step: 105
	action: tensor([[-46669.5165,  12983.4840,  93654.4678,  58751.9264, -55759.7734,
          31867.7027, -19088.4032]], dtype=torch.float64)
	q_value: tensor([[-44.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47384586972606113, distance: 0.8300670702734192 entropy 12.20394024300965
epoch: 73, step: 106
	action: tensor([[-46610.6571, -59704.6633,  -2402.2087, -15102.0354, -20610.7502,
          60721.6879,  18618.7380]], dtype=torch.float64)
	q_value: tensor([[-45.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20208317031869671, distance: 1.0221991275076971 entropy 12.237821646558176
epoch: 73, step: 107
	action: tensor([[-62827.6119, -27245.3776,  99020.2284, -41885.5790, -46429.5372,
          62083.8246, -95166.8764]], dtype=torch.float64)
	q_value: tensor([[-39.7275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1313334337516484, distance: 1.0665549766190814 entropy 12.174563372842206
epoch: 73, step: 108
	action: tensor([[  23743.9064, -110308.1423,     943.8844,    2691.4575,  -28793.9780,
          -31398.6557,   -8765.0088]], dtype=torch.float64)
	q_value: tensor([[-43.8288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30027752640786387, distance: 0.9572372813130554 entropy 12.39507331394421
epoch: 73, step: 109
	action: tensor([[-25407.9025, -11588.4120,  -1203.8523,  20651.2786,  27505.9738,
          57696.0302, -22162.5807]], dtype=torch.float64)
	q_value: tensor([[-32.8855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043912376603690295, distance: 1.1189367589949282 entropy 11.794750699102845
epoch: 73, step: 110
	action: tensor([[-20767.7007, -79785.3756, -63380.2672,  51498.8231,  16072.4716,
         -52542.3883,  63672.2092]], dtype=torch.float64)
	q_value: tensor([[-32.9897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.507986631509391, distance: 1.4052559700322513 entropy 12.223674618261153
epoch: 73, step: 111
	action: tensor([[ 39934.0462, -51246.8507, -23607.5881,  39655.3830,  -1797.1145,
         -60404.1927,  73602.0951]], dtype=torch.float64)
	q_value: tensor([[-31.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07338065233062785, distance: 1.1015580162005698 entropy 12.152702283001801
epoch: 73, step: 112
	action: tensor([[   202.0658,  -8625.8077,  -6975.7547,  43081.6605,  -8121.5566,
         -27559.6152,  79680.6534]], dtype=torch.float64)
	q_value: tensor([[-31.7507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15747865077837686, distance: 1.0503817091606007 entropy 11.954083404337753
epoch: 73, step: 113
	action: tensor([[-106657.5710,   19151.4087,  -54036.5974,   65350.5452,  -40072.2407,
           93814.5601,   29059.5936]], dtype=torch.float64)
	q_value: tensor([[-38.1543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46508509242746454, distance: 0.8369490947979175 entropy 12.224424587466856
epoch: 73, step: 114
	action: tensor([[-6.7533e+04, -6.5740e+01,  3.3625e+04,  1.2048e+05, -3.9423e+04,
          6.9660e+03, -1.4320e+04]], dtype=torch.float64)
	q_value: tensor([[-35.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7996290233499728, distance: 1.5351407056850284 entropy 12.04872668074552
epoch: 73, step: 115
	action: tensor([[-27849.4564, -75699.3265,  46588.7547,  42865.3950, -17595.6911,
         -35714.4898, -34996.3637]], dtype=torch.float64)
	q_value: tensor([[-35.2295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07592234458335845, distance: 1.1000462065888563 entropy 12.238835548607613
epoch: 73, step: 116
	action: tensor([[-47242.9618, -19778.6736,  27761.0934,  50429.6591,  14419.1370,
         -56269.7262,  45189.9149]], dtype=torch.float64)
	q_value: tensor([[-32.5346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3400798277646113, distance: 1.3247137012892902 entropy 12.22192736145566
epoch: 73, step: 117
	action: tensor([[  8475.4706,  12401.2477,    628.3832,  -1146.8098,  22351.8284,
          58457.8141, -47945.1448]], dtype=torch.float64)
	q_value: tensor([[-31.4432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.030138898120738
epoch: 73, step: 118
	action: tensor([[-30708.7079, -18521.9206,   9435.1026,   5954.6205,  30597.3464,
          29964.6639,  -5529.5260]], dtype=torch.float64)
	q_value: tensor([[-35.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.044885266444248284, distance: 1.1697444571653157 entropy 11.604792306147383
epoch: 73, step: 119
	action: tensor([[   9617.0544, -110727.1435,  -52195.5976,   46867.7091,   40153.1058,
          -64099.2691,   36215.3048]], dtype=torch.float64)
	q_value: tensor([[-33.1423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5441294740168667, distance: 0.7726404665660118 entropy 12.217270641742406
epoch: 73, step: 120
	action: tensor([[-39663.8875,   2406.4349,   5600.9047, -23959.4560,  52507.5771,
         -43162.5950,  83074.9353]], dtype=torch.float64)
	q_value: tensor([[-33.2984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27323505106213275, distance: 0.9755593149806167 entropy 12.144230764510505
epoch: 73, step: 121
	action: tensor([[   9829.2835,  -17734.1175,   44075.1997,   22744.5618,   23258.0163,
         -117175.1551,    3160.9973]], dtype=torch.float64)
	q_value: tensor([[-44.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39982325541827457, distance: 0.8865357941047681 entropy 12.134718252085397
epoch: 73, step: 122
	action: tensor([[ -1512.5048, -49514.9850, -13315.1153, -60125.3607,  24772.7465,
         -56837.9367,  94423.7150]], dtype=torch.float64)
	q_value: tensor([[-34.4471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6172534534339011, distance: 1.4552772389051776 entropy 12.204123072914914
epoch: 73, step: 123
	action: tensor([[  9668.4829, -68913.7457, -22793.5735,  87333.7935,  19471.5715,
         -51674.0810,   9882.1105]], dtype=torch.float64)
	q_value: tensor([[-32.0829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29230727331440354, distance: 0.9626735940222236 entropy 11.998175570978313
epoch: 73, step: 124
	action: tensor([[-37350.0344, -27613.7982,  41507.8254,  31535.3162,  30939.4874,
         125468.0915, -19028.4722]], dtype=torch.float64)
	q_value: tensor([[-33.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19675099116198058, distance: 1.0256089272417543 entropy 11.89267753681493
epoch: 73, step: 125
	action: tensor([[-31720.8782, -84767.0241, -28338.8933,  49123.0507,  -1449.4190,
          23070.5292,  44733.0816]], dtype=torch.float64)
	q_value: tensor([[-32.5230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4169271229067575, distance: 1.362167298888136 entropy 12.183096871133753
epoch: 73, step: 126
	action: tensor([[  4619.4898, -11743.3573, -71678.4308, -27673.0116,  10354.6697,
         -12480.4785, 111427.5712]], dtype=torch.float64)
	q_value: tensor([[-35.1100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18457122114831148, distance: 1.2454815026010484 entropy 12.246958271000647
epoch: 73, step: 127
	action: tensor([[  2251.3093, -12997.0664,   2760.6917,  66418.5663, -27169.8580,
         -12971.7722,  64124.4120]], dtype=torch.float64)
	q_value: tensor([[-28.2466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2585553975613535, distance: 1.2837866688282518 entropy 11.747522627135131
LOSS epoch 73 actor 500.3544448536979 critic 104.35995495071118
epoch: 74, step: 0
	action: tensor([[ 75425.0245, -17427.0617, -15471.0138, -40367.5005,  69297.5458,
          37079.6876,  23345.0208]], dtype=torch.float64)
	q_value: tensor([[-36.5391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03544752635338577, distance: 1.164449719854498 entropy 12.167201881990424
epoch: 74, step: 1
	action: tensor([[ 25132.8911, -24136.7936, -28638.5435, -33215.1613, -27166.3005,
          -2296.1546,  32688.7536]], dtype=torch.float64)
	q_value: tensor([[-37.2762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23202322147599408, distance: 1.2701825442681132 entropy 12.170197458290962
epoch: 74, step: 2
	action: tensor([[ 17529.0493, -37946.7418, -70988.7158,  15868.3926, -42044.7340,
         -10142.5543,  46429.2956]], dtype=torch.float64)
	q_value: tensor([[-37.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2429109312671568, distance: 0.9957038378813864 entropy 12.098586115148999
epoch: 74, step: 3
	action: tensor([[-40737.0187, -12434.9325, -20041.3065, -19065.5643, -14798.8610,
         -27858.7542,   9750.2169]], dtype=torch.float64)
	q_value: tensor([[-35.5883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5022648520288906, distance: 1.4025874429301568 entropy 12.156684460948542
epoch: 74, step: 4
	action: tensor([[  6373.1735, -36244.6518, -18620.9570,  29810.3129, -20552.3686,
         -58429.7631,  30345.1525]], dtype=torch.float64)
	q_value: tensor([[-35.4104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010616127915625828, distance: 1.13825379410494 entropy 12.26954510608327
epoch: 74, step: 5
	action: tensor([[ -2146.2446,  19805.3363, -15787.3336, -33434.9608, -12138.6120,
         -23244.6894,  20545.1350]], dtype=torch.float64)
	q_value: tensor([[-35.3141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.965863757916129
epoch: 74, step: 6
	action: tensor([[-22373.6172, -25410.5473,   8262.8150,  23890.2253, -44188.8721,
         -14176.2176,  -1120.9982]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6634134645620453, distance: 1.475899547377125 entropy 11.639535653744764
epoch: 74, step: 7
	action: tensor([[  7465.7316, -18152.5683, -15990.0022,   6674.4575, -82116.3703,
         -14170.5027,  22607.2978]], dtype=torch.float64)
	q_value: tensor([[-30.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14750781496347298, distance: 1.2258420623651012 entropy 11.992055632626728
epoch: 74, step: 8
	action: tensor([[  9190.0632, -45118.5778, -36494.8166,  -3745.7402,  14567.0085,
         -60830.3465, -10677.5656]], dtype=torch.float64)
	q_value: tensor([[-34.1015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048719565626241135, distance: 1.1718887323442724 entropy 11.987212857451718
epoch: 74, step: 9
	action: tensor([[ 30889.4610, -46989.8287,  65366.2664,  20151.8985,  -7622.3998,
           7995.3919, -26884.9120]], dtype=torch.float64)
	q_value: tensor([[-35.7508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4653969531557347, distance: 0.8367050843911106 entropy 12.015098530325526
epoch: 74, step: 10
	action: tensor([[ 27060.3416, -48750.2025,  48294.5137,   -242.0706,  28871.5915,
         -12303.7757,  18526.7646]], dtype=torch.float64)
	q_value: tensor([[-35.5823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1954110722561555, distance: 1.0264639922068677 entropy 11.970382908879158
epoch: 74, step: 11
	action: tensor([[-16061.0302, -77615.9055,  -3199.1722, -23030.8923,  47882.8968,
          29627.3793,  56364.8398]], dtype=torch.float64)
	q_value: tensor([[-35.5519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4528792789669873, distance: 1.3793404051080367 entropy 11.920796836705142
epoch: 74, step: 12
	action: tensor([[-40344.2370,  -8167.9330, -43938.2164, -54040.3728,  30247.3524,
         -46366.9255,  38075.1455]], dtype=torch.float64)
	q_value: tensor([[-31.8446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09700961986064083, distance: 1.0874223505038574 entropy 11.989437421390965
epoch: 74, step: 13
	action: tensor([[ 56706.4590, -21311.8678,  81514.9482,  80662.5171,  61672.1345,
         111037.4760,  38571.6188]], dtype=torch.float64)
	q_value: tensor([[-30.5396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4067490463522705, distance: 1.3572661186968071 entropy 11.961359130811276
epoch: 74, step: 14
	action: tensor([[-26177.4947, -93734.9750,  43675.5127, -70914.5114,   1019.6262,
         -23544.9066,  42279.9748]], dtype=torch.float64)
	q_value: tensor([[-32.9145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3597262234331102, distance: 1.3343889285133341 entropy 11.951651698604062
epoch: 74, step: 15
	action: tensor([[ 40281.4583, -35831.6563, -39692.8846,  -1760.4407,  19274.8348,
          -2011.1833, -11964.3935]], dtype=torch.float64)
	q_value: tensor([[-30.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4612688517362439, distance: 0.8399293096365872 entropy 11.978696307516785
epoch: 74, step: 16
	action: tensor([[ -1286.7601,   3859.4036,  -4140.1186,  13823.2927,  -2260.2887,
         -17021.5960,  42198.7704]], dtype=torch.float64)
	q_value: tensor([[-34.9189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.103567968117186
epoch: 74, step: 17
	action: tensor([[-38492.8805, -22674.9968, -54784.7979,  32970.8023, -24190.1339,
         -12100.2530, -26305.8544]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7399396374377665, distance: 1.509467560593184 entropy 11.639535653744764
epoch: 74, step: 18
	action: tensor([[-34794.2244, -21606.1086, -71051.7910, -57959.2644,  -2901.1839,
          66436.8572,  40786.9371]], dtype=torch.float64)
	q_value: tensor([[-32.5761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6700451914153585, distance: 1.4788386923223202 entropy 12.160619224956509
epoch: 74, step: 19
	action: tensor([[ -18870.6038,  -14703.5138,  -72344.9147, -128928.1186,   -5931.1041,
           15794.1596,  -31674.9023]], dtype=torch.float64)
	q_value: tensor([[-34.4760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2574300346765239, distance: 1.2832125784965278 entropy 12.21367516094594
epoch: 74, step: 20
	action: tensor([[-31471.1621, -20275.8043,  50342.4127,  56604.0411, -18081.2147,
         112532.0020,   2138.7130]], dtype=torch.float64)
	q_value: tensor([[-33.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3599008528032577, distance: 1.3344746134154828 entropy 12.02031165501957
epoch: 74, step: 21
	action: tensor([[ 26658.5702, -38413.0662, -51363.6983,  67042.9614, -11664.9253,
          62994.3944,   -188.3250]], dtype=torch.float64)
	q_value: tensor([[-31.7440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08822500861080917, distance: 1.0926989638650735 entropy 12.154940468861716
epoch: 74, step: 22
	action: tensor([[ 10464.2633, -49268.3625,  41520.1548,  -6026.0927, -58716.1600,
           4559.7508,  14546.4992]], dtype=torch.float64)
	q_value: tensor([[-37.7830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36480293457142543, distance: 1.3368776632349173 entropy 12.136736214667605
epoch: 74, step: 23
	action: tensor([[ 11574.7121, -12330.7094,   5781.8627,  -9957.4341, -15339.9813,
          17822.6954, -27199.2739]], dtype=torch.float64)
	q_value: tensor([[-30.3334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5724934207601405, distance: 0.7482179014544155 entropy 11.692140368127188
epoch: 74, step: 24
	action: tensor([[-57052.3312, -27155.9400, -87049.9023,  22030.1966,  27133.9878,
         -21385.0388, -55035.3687]], dtype=torch.float64)
	q_value: tensor([[-33.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10904335880730276, distance: 1.2051218370616879 entropy 11.986836806852196
epoch: 74, step: 25
	action: tensor([[ -4303.7284, -83354.8599,  14336.0529, -91283.1623,  10681.5958,
         -62054.8390, -40449.9063]], dtype=torch.float64)
	q_value: tensor([[-32.6937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21428487251381745, distance: 1.0143533098473314 entropy 12.250628622783745
epoch: 74, step: 26
	action: tensor([[-22354.3162, -53538.4695, -28786.6393,   7079.7850,  43876.6425,
          24278.4184, -23965.2403]], dtype=torch.float64)
	q_value: tensor([[-34.5006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4865460274941529, distance: 1.3952302178044023 entropy 12.12728679654924
epoch: 74, step: 27
	action: tensor([[-54066.9165,  51094.7421,  14061.0148, -16063.9318, -24128.4287,
          69586.9140,  17709.3412]], dtype=torch.float64)
	q_value: tensor([[-30.9017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30061417633879106, distance: 0.957006980982026 entropy 12.05511234664338
epoch: 74, step: 28
	action: tensor([[ 25119.1987, -38308.9618, -65446.6677,  45664.4068, -85779.3396,
          73250.4645,  58155.9849]], dtype=torch.float64)
	q_value: tensor([[-45.9021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.19710430551144
epoch: 74, step: 29
	action: tensor([[ -3657.0981, -22041.8282,  23326.4109,  21847.0355,  -7282.4932,
          -9753.3751,  63162.3556]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.603122532536571, distance: 1.4489054714662846 entropy 11.639535653744764
epoch: 74, step: 30
	action: tensor([[ -3592.6555, -10323.2758, -37892.2353,  35889.6832,  10290.3624,
          36233.4813,   2515.7984]], dtype=torch.float64)
	q_value: tensor([[-32.8059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2276189362076848, distance: 1.2679101622383906 entropy 12.280947120783793
epoch: 74, step: 31
	action: tensor([[   221.2787, -90658.5739, -36187.2224,  37007.8163, -32209.6887,
           3572.3442, -47494.8893]], dtype=torch.float64)
	q_value: tensor([[-33.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019079962409221052, distance: 1.1552096935189404 entropy 12.253767090782976
epoch: 74, step: 32
	action: tensor([[ 15277.2103,   -387.5747, -87828.3790, -41948.3772, -69726.2507,
           3776.3834,  52383.9042]], dtype=torch.float64)
	q_value: tensor([[-32.6170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32390576294667106, distance: 1.3166951310079578 entropy 12.260900185428449
epoch: 74, step: 33
	action: tensor([[-35167.0111, -52562.2648, -40471.8719,  -8377.8681,  18512.3105,
         -15497.7477, -25456.9999]], dtype=torch.float64)
	q_value: tensor([[-33.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.226279861172077, distance: 1.006580803715936 entropy 11.981718792909245
epoch: 74, step: 34
	action: tensor([[-62894.5534,  47099.3494,  40446.2109,  -3356.6368,  89710.1596,
          -9035.0404, -20652.1179]], dtype=torch.float64)
	q_value: tensor([[-35.4402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16675187170020533, distance: 1.0445851959998904 entropy 12.249011848079096
epoch: 74, step: 35
	action: tensor([[-50922.9150, -33671.2591,   5208.3972, 140516.9939, -36210.4862,
          61589.9409,  53906.0388]], dtype=torch.float64)
	q_value: tensor([[-35.7537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5944710246265503, distance: 1.4449905570542791 entropy 12.152321769504413
epoch: 74, step: 36
	action: tensor([[-11187.2929, -88065.0665,  20685.6942,   6772.0777, -74161.0077,
          31730.9657,  10197.8851]], dtype=torch.float64)
	q_value: tensor([[-28.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009115192760501767, distance: 1.149547882111042 entropy 12.031286311775004
epoch: 74, step: 37
	action: tensor([[-19050.1124, -31690.3202,  87934.9753,  22674.5931, -12134.2048,
          18712.9184, -10155.4011]], dtype=torch.float64)
	q_value: tensor([[-29.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28853903669737146, distance: 1.2989890296960611 entropy 12.028516489326053
epoch: 74, step: 38
	action: tensor([[  50448.6864, -101336.8002,  -35964.5631,    2858.3751,  -64412.8256,
          -81696.2641,   27869.0089]], dtype=torch.float64)
	q_value: tensor([[-31.5614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4334764215695158, distance: 0.8613222934315604 entropy 12.189948099745667
epoch: 74, step: 39
	action: tensor([[-62166.6813,  10124.1815,  13066.9419,   2089.2454,  40686.6258,
           3182.2211, -21354.2540]], dtype=torch.float64)
	q_value: tensor([[-36.9308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3522845650520391, distance: 0.9209770677586253 entropy 12.160187933002678
epoch: 74, step: 40
	action: tensor([[-93047.5172, -17924.8188,  25668.3350, -12708.8282,  53962.0716,
          17952.1666,  40904.9202]], dtype=torch.float64)
	q_value: tensor([[-38.1477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2135125237943578, distance: 1.2606044173805082 entropy 12.222104058095724
epoch: 74, step: 41
	action: tensor([[-37451.3277,  66436.7164,  29351.5953,  54585.3780,  -1956.9133,
           3396.7921,   4072.4736]], dtype=torch.float64)
	q_value: tensor([[-31.0672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4069816640276813, distance: 0.8812330043681285 entropy 12.03598036690374
epoch: 74, step: 42
	action: tensor([[ -97110.8684,  -39395.4478,  -31606.7773,    7936.9629,   37402.9294,
         -118740.6054,   -3223.0540]], dtype=torch.float64)
	q_value: tensor([[-33.7562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7023654260639942, distance: 1.4930800358613396 entropy 12.204404406479375
epoch: 74, step: 43
	action: tensor([[ -6266.7427, -57477.2341, -32421.2929, -22661.4235, -27435.1555,
           -486.1525, -65904.6102]], dtype=torch.float64)
	q_value: tensor([[-32.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8935144076310924, distance: 1.5746752454963377 entropy 12.19988133168091
epoch: 74, step: 44
	action: tensor([[-59681.0005, -56030.8108,  66576.9912,  26070.9161,  46645.3081,
           6332.4697,  -9142.9790]], dtype=torch.float64)
	q_value: tensor([[-31.1710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3699390006115919, distance: 1.3393907895117396 entropy 12.117913102787883
epoch: 74, step: 45
	action: tensor([[-6.2761e+03, -4.0880e+04, -2.0008e+04, -6.6730e+04,  1.0290e+04,
          8.6356e+03, -1.8805e+01]], dtype=torch.float64)
	q_value: tensor([[-29.9282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028942038098297873, distance: 1.1607859658666229 entropy 12.051849104448289
epoch: 74, step: 46
	action: tensor([[  6481.0307,  22334.1946, -13325.6567,   -962.3240, -24546.6864,
          59654.1985,  37329.1184]], dtype=torch.float64)
	q_value: tensor([[-30.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.619110915841605, distance: 0.7062459274730702 entropy 12.033244540391008
epoch: 74, step: 47
	action: tensor([[-43531.4400,  -6673.1017,  -6315.9868,  32000.0284,  43980.5517,
          44576.7940,  -4883.5177]], dtype=torch.float64)
	q_value: tensor([[-36.8998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.821440942439825
epoch: 74, step: 48
	action: tensor([[-43355.9905, -20406.7114, -12887.1212,  25685.2507,   3949.7289,
          23616.4841,   7689.6938]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.639535653744764
epoch: 74, step: 49
	action: tensor([[ -5831.5328,  -7798.4910,  23209.7703, -36551.5000, -57476.8329,
          37879.2466, -24162.5002]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04020938043677791, distance: 1.1211015262397166 entropy 11.639535653744764
epoch: 74, step: 50
	action: tensor([[-90163.7922, -39158.7596, -38946.8283,  19935.3435,  49113.5188,
          17874.1850, -98841.9566]], dtype=torch.float64)
	q_value: tensor([[-34.3095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9626182301635724, distance: 1.6031516544288638 entropy 12.081804232563991
epoch: 74, step: 51
	action: tensor([[-82584.0655, -38747.7335,  41501.3715,  23580.7492, -10624.5034,
          10333.4410, -28198.6689]], dtype=torch.float64)
	q_value: tensor([[-28.4667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14445796513524545, distance: 1.0584671144216877 entropy 12.01571483990745
epoch: 74, step: 52
	action: tensor([[-52903.2811, -16132.8234,   4707.1808,  10146.2421, -44903.7996,
         -28334.9288,  46248.2072]], dtype=torch.float64)
	q_value: tensor([[-30.2190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2809395785919606, distance: 1.2951528204153835 entropy 12.019015255112603
epoch: 74, step: 53
	action: tensor([[-31233.6231, -68083.2555, -46428.7266,  -5650.6917,  43180.9239,
          33421.5329,   5944.6714]], dtype=torch.float64)
	q_value: tensor([[-33.4632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7767466402861638, distance: 1.5253497833863465 entropy 12.152147500450022
epoch: 74, step: 54
	action: tensor([[ 20502.5561, -37983.0741, -54382.8927, -70018.7397,  17070.7489,
          75193.1523, -25900.1539]], dtype=torch.float64)
	q_value: tensor([[-34.6647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019026114773880165, distance: 1.1551791727873202 entropy 12.178164967200559
epoch: 74, step: 55
	action: tensor([[  8304.2530, -20674.2024,  43885.2009,  30674.1173,  70522.4871,
           2755.1012,  12067.1706]], dtype=torch.float64)
	q_value: tensor([[-34.1974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08580991030549656, distance: 1.1924319221876436 entropy 11.97939893411866
epoch: 74, step: 56
	action: tensor([[ 12292.0440, -32761.3026, -96485.2736, -13433.4870,  35666.6396,
         -19072.5877, -78973.4307]], dtype=torch.float64)
	q_value: tensor([[-37.0778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2679089096792655, distance: 1.288548352809502 entropy 12.17885474672371
epoch: 74, step: 57
	action: tensor([[   -454.0191, -104659.2254,  -39103.5891,   31236.4139,   12976.5941,
          -26430.5402,   34768.3553]], dtype=torch.float64)
	q_value: tensor([[-31.2163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011480312651979685, distance: 1.1377565773657068 entropy 12.056600543483503
epoch: 74, step: 58
	action: tensor([[ 51264.5238,  -3438.1591,  -2441.3850, -29570.2070,    980.0467,
         101798.2129, -90426.0988]], dtype=torch.float64)
	q_value: tensor([[-34.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37083618449557954, distance: 0.9076921133694602 entropy 12.262710357158483
epoch: 74, step: 59
	action: tensor([[-22480.0856, -26408.8347, -34221.6059, -13298.8026, -34644.3136,
           8243.1011, -32141.9659]], dtype=torch.float64)
	q_value: tensor([[-34.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5322022492782199, distance: 1.416494005785081 entropy 12.04570872662536
epoch: 74, step: 60
	action: tensor([[-104875.4563,   -3979.5544,   36325.7738,   49682.4725,   26321.6343,
           32146.4701,    5753.6674]], dtype=torch.float64)
	q_value: tensor([[-36.1613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4502010586759606, distance: 1.37806848887374 entropy 12.235928336014883
epoch: 74, step: 61
	action: tensor([[-3152.0051, 15042.1815,  4619.3865, 38699.6275, 39202.4924, 51590.4120,
         25107.3078]], dtype=torch.float64)
	q_value: tensor([[-32.2090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1280749726739936, distance: 1.0685534850400125 entropy 12.121771128732044
epoch: 74, step: 62
	action: tensor([[-32111.9944, -35578.6260, -33090.3148,  61851.8492, -22264.2789,
         -17174.2491,  19587.3563]], dtype=torch.float64)
	q_value: tensor([[-34.7867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07941994154293464, distance: 1.0979624175117617 entropy 12.015627708619572
epoch: 74, step: 63
	action: tensor([[-74493.1543, -59873.4779, -56457.7267,  54592.7990,  25121.0732,
         -61522.3418,  25413.4353]], dtype=torch.float64)
	q_value: tensor([[-38.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08281201994679876, distance: 1.1907846490287675 entropy 12.484599380364232
epoch: 74, step: 64
	action: tensor([[ 67315.7442,  29935.9244,  47564.1258,  -1288.4387,  26316.9689,
         100869.3865,  98496.8996]], dtype=torch.float64)
	q_value: tensor([[-37.1328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.285433324621975
epoch: 74, step: 65
	action: tensor([[ 59051.0126, -94338.7396,  12232.9179,  16738.1261,  -7445.7780,
          -8587.5378,   6501.4808]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16337710578200948, distance: 1.0466984130568864 entropy 11.639535653744764
epoch: 74, step: 66
	action: tensor([[-14801.5123, -78284.5321, -32024.8040,  35917.4583, -36046.8178,
          40090.9156,  73155.4055]], dtype=torch.float64)
	q_value: tensor([[-31.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07361339078514129, distance: 1.1014196686626625 entropy 12.017480240978983
epoch: 74, step: 67
	action: tensor([[-50028.7436, -37689.7473,  -4638.9545,  36258.5644,  -9720.5305,
         -11420.8546, -30807.8442]], dtype=torch.float64)
	q_value: tensor([[-28.4251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9134818049774198, distance: 1.5829560670663618 entropy 11.961974530725817
epoch: 74, step: 68
	action: tensor([[ -7170.2155, -46395.2569, -22429.3243,  73512.4185,  32163.8487,
         -14525.3235,  14570.6709]], dtype=torch.float64)
	q_value: tensor([[-30.6587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.757005013023285, distance: 1.5168519477802467 entropy 12.113012244366356
epoch: 74, step: 69
	action: tensor([[ 14148.5285, -13260.4048,  21110.4766,  51221.0554, -18602.7746,
          24406.8977, -31685.9249]], dtype=torch.float64)
	q_value: tensor([[-30.3100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.98399237629127
epoch: 74, step: 70
	action: tensor([[-74129.9833, -81756.7107, -16601.4847,  44888.6562, -85732.3375,
         -13618.2568,  30211.5192]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15113125353012413, distance: 1.2277759326529063 entropy 11.639535653744764
epoch: 74, step: 71
	action: tensor([[-15185.8020, -39440.2449, -92555.2716,  39971.3839,  82846.8130,
         -51678.8239, -23497.2117]], dtype=torch.float64)
	q_value: tensor([[-33.4586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12695281114643675, distance: 1.06924087444651 entropy 12.160481238395487
epoch: 74, step: 72
	action: tensor([[-86679.3525, -76549.7437,  21191.5587,  10279.5023, -49029.4500,
          59521.2484, -24080.8596]], dtype=torch.float64)
	q_value: tensor([[-37.5790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22653368834575605, distance: 1.2673496051342896 entropy 12.408941724218556
epoch: 74, step: 73
	action: tensor([[ -76519.1294, -114687.0746,    7652.9037,   26531.7674,   74933.5974,
          -57849.5750,   45142.2697]], dtype=torch.float64)
	q_value: tensor([[-34.3152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8534881185325867, distance: 1.5579431156858745 entropy 12.308114883185217
epoch: 74, step: 74
	action: tensor([[ 40968.0264, -54916.9092, -50156.5908,  44214.5639, -96343.4186,
         -43967.6430, -43661.2427]], dtype=torch.float64)
	q_value: tensor([[-32.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48430160799548316, distance: 0.8217781351059449 entropy 12.123029105933691
epoch: 74, step: 75
	action: tensor([[ -5249.8625, -10707.9178, -15892.3170,  25356.6794, -47395.7118,
         -12054.4089, -60655.5025]], dtype=torch.float64)
	q_value: tensor([[-39.0563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43850246268631077, distance: 1.3724988781355079 entropy 12.102333480236334
epoch: 74, step: 76
	action: tensor([[ 54821.6738,  31524.7578,  26361.7670,  46458.5878,  21652.2656,
          62307.6639, -27882.4054]], dtype=torch.float64)
	q_value: tensor([[-36.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39108291333106093, distance: 0.8929677485579438 entropy 12.39440157675701
epoch: 74, step: 77
	action: tensor([[-42066.2838, -47305.9209, -17785.6629,  57659.7969, -12576.7913,
          42093.9475,   7327.1263]], dtype=torch.float64)
	q_value: tensor([[-35.6257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4122817544049231, distance: 1.359932546003262 entropy 12.130683633682748
epoch: 74, step: 78
	action: tensor([[ -26843.5134,  -47710.6213,  105741.4066,  -51122.4233,   64497.6838,
           68126.5213, -158143.1824]], dtype=torch.float64)
	q_value: tensor([[-36.0283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5653473582749253, distance: 1.4317330674991782 entropy 12.257825909299001
epoch: 74, step: 79
	action: tensor([[    93.8573, -44558.7204,  10648.0391,  32712.7822,  35831.1944,
          27701.8874,  12903.7169]], dtype=torch.float64)
	q_value: tensor([[-29.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20394583762055474, distance: 1.0210053129964678 entropy 11.918970836144
epoch: 74, step: 80
	action: tensor([[ 60265.1258, -41088.5561,  77445.0215,  58109.9446, -48642.3111,
          36770.3124,  72521.0998]], dtype=torch.float64)
	q_value: tensor([[-32.4529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4870052309264242, distance: 0.8196211593416342 entropy 12.316929543016753
epoch: 74, step: 81
	action: tensor([[ 22286.4396,  24078.3814,  11386.5215, -30289.6322, -68326.2297,
           1109.1578,  -3690.7159]], dtype=torch.float64)
	q_value: tensor([[-37.3717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7841288910362695, distance: 0.5316844447632629 entropy 12.099334406782031
epoch: 74, step: 82
	action: tensor([[ -7759.5158,  11354.3684,  48549.3476, -12356.3042,  20531.5782,
           9252.5431, -26737.6118]], dtype=torch.float64)
	q_value: tensor([[-35.1134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04719491776131579, distance: 1.117014281445274 entropy 11.991821701647838
epoch: 74, step: 83
	action: tensor([[-52635.5821,  21616.6317, -11403.3804, -11599.5692, -26470.0783,
         -44849.7081,  -4934.3885]], dtype=torch.float64)
	q_value: tensor([[-41.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17639065824196043, distance: 1.0385258839315068 entropy 12.106204339738834
epoch: 74, step: 84
	action: tensor([[-24579.7107, -29030.8747, -12731.9627,  -1676.5878,  14483.4373,
          16266.1432,  84456.7941]], dtype=torch.float64)
	q_value: tensor([[-37.6269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27632137801398304, distance: 1.29281599030595 entropy 11.981094242288547
epoch: 74, step: 85
	action: tensor([[ -3989.5364, -96012.7176,  49973.2892,   -757.1592,  39235.8413,
          47906.4853,  -9005.2473]], dtype=torch.float64)
	q_value: tensor([[-33.3634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18984532686460354, distance: 1.0300081568582458 entropy 12.059823586685367
epoch: 74, step: 86
	action: tensor([[-21458.4299, -82842.6142, -37046.8033, -99529.5180,  -1924.0351,
         -32387.0838,  14670.9031]], dtype=torch.float64)
	q_value: tensor([[-40.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24462903637302102, distance: 1.2766641336045497 entropy 12.358279711949015
epoch: 74, step: 87
	action: tensor([[  -8794.1382,   -3374.7873,   17106.4287,  -45853.3460,   -5474.5346,
         -102497.4332,   61020.4662]], dtype=torch.float64)
	q_value: tensor([[-32.3875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6830581322757576, distance: 1.4845890452598407 entropy 12.098082527289003
epoch: 74, step: 88
	action: tensor([[-106888.7702,   23204.5119, -111798.3920,   43436.9015,   -7119.9526,
          139608.7476,   65938.1128]], dtype=torch.float64)
	q_value: tensor([[-37.5827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20999681154654248, distance: 1.258777020904183 entropy 12.324109316935727
epoch: 74, step: 89
	action: tensor([[ 53048.8225, -15509.9189, -91527.7203,  29334.3428,   1397.6178,
          39159.0642, -34944.4277]], dtype=torch.float64)
	q_value: tensor([[-38.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43862374697473805, distance: 0.8574004596357414 entropy 12.146247608228805
epoch: 74, step: 90
	action: tensor([[ -9367.8454, -73004.2816,  28341.2147,  -1701.9012, -26165.9689,
          92575.7143, -31466.6896]], dtype=torch.float64)
	q_value: tensor([[-36.3760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04313226067284859, distance: 1.168762804139261 entropy 12.204778400887534
epoch: 74, step: 91
	action: tensor([[ -90031.0470, -201094.1335,    -833.0929,   47268.1549,  -63928.6740,
          -32129.6016,    8994.7912]], dtype=torch.float64)
	q_value: tensor([[-32.1880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10604083920541285, distance: 1.0819707814706707 entropy 12.269222630727127
epoch: 74, step: 92
	action: tensor([[-19013.4389,  -2547.6459, -25583.2254, -22851.4882,   5533.6311,
          66301.5492,  33321.8069]], dtype=torch.float64)
	q_value: tensor([[-39.8141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.043561670592879, distance: 1.6358767027194785 entropy 12.376239524433618
epoch: 74, step: 93
	action: tensor([[  7643.7563,   8585.5712,  49042.9898,   5081.1619, -74749.4455,
          57588.7788, -64329.2217]], dtype=torch.float64)
	q_value: tensor([[-28.9437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.07807586131533
epoch: 74, step: 94
	action: tensor([[-40950.2648,   5187.4289,  -1166.7160,   6503.2954, -25261.6602,
          -3646.0946, -12530.9925]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24392321844438392, distance: 0.9950379483463904 entropy 11.639535653744764
epoch: 74, step: 95
	action: tensor([[ -37466.1309, -139908.7476,   -2012.3002,  -17258.5743,  -31775.7215,
           17846.5003,   61595.1886]], dtype=torch.float64)
	q_value: tensor([[-37.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1456715451225068, distance: 1.2248608585694913 entropy 12.170041573928447
epoch: 74, step: 96
	action: tensor([[-57650.7882,  57736.6935,  36531.2628,  21636.7606,  -9907.7047,
           1906.1130, -15484.6788]], dtype=torch.float64)
	q_value: tensor([[-30.0445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3852743035263959, distance: 0.8972167589741988 entropy 11.970353303339621
epoch: 74, step: 97
	action: tensor([[-2.8964e+04, -1.0151e+05, -2.2832e+04,  4.8896e+04,  4.2437e+04,
         -2.8719e+01,  3.6076e+04]], dtype=torch.float64)
	q_value: tensor([[-33.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03513164164876692, distance: 1.124063188520208 entropy 11.984106448060851
epoch: 74, step: 98
	action: tensor([[ -16116.5809, -123988.9153,   54970.0997,   59155.2113,   27304.4070,
           66993.9003,    6022.0179]], dtype=torch.float64)
	q_value: tensor([[-28.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6543780233321441, distance: 1.4718856439513175 entropy 12.035692827026349
epoch: 74, step: 99
	action: tensor([[-15447.7256, -96112.1648,  88020.6168,  13296.5203, -69987.8399,
         -21261.4416, -85724.1637]], dtype=torch.float64)
	q_value: tensor([[-33.9910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.907531262934105, distance: 1.5804928136509209 entropy 12.281121472441667
epoch: 74, step: 100
	action: tensor([[-42720.0304,  69981.6853,  -8230.4221, 108276.5632, -38760.3699,
          51037.7339,  33691.3959]], dtype=torch.float64)
	q_value: tensor([[-31.5429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21606135841018692, distance: 1.0132059448537187 entropy 12.128345228234098
epoch: 74, step: 101
	action: tensor([[-43229.7140, -31893.3888,  25392.1210, -12873.5546,  -4690.3739,
          33525.6906,  14740.6666]], dtype=torch.float64)
	q_value: tensor([[-32.1135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3597010392793709, distance: 1.334376571021454 entropy 11.981226233802337
epoch: 74, step: 102
	action: tensor([[-53436.8380, -66919.3321, -58136.8208, -22561.7411, -80093.9220,
          15856.3761,  12243.7738]], dtype=torch.float64)
	q_value: tensor([[-32.7939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22950438068282075, distance: 1.2688834516486227 entropy 12.10247804575458
epoch: 74, step: 103
	action: tensor([[ 39972.8000, -37172.9478, -23450.1327,   8239.7321,   7901.4740,
          21791.4133,  47144.2646]], dtype=torch.float64)
	q_value: tensor([[-33.1892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17885517605271017, distance: 1.2424728875870001 entropy 12.12142027900514
epoch: 74, step: 104
	action: tensor([[ 78127.0627,  34008.5767, -34677.2067,  11093.9020,  22676.0391,
         -83103.6577,  56676.3054]], dtype=torch.float64)
	q_value: tensor([[-29.8629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25411975382239405, distance: 0.9883055768249457 entropy 11.976418362324491
epoch: 74, step: 105
	action: tensor([[-39121.2177,  -8100.7748,  43043.4605,  16362.9877, -18288.5982,
          -3447.5803, -59199.3172]], dtype=torch.float64)
	q_value: tensor([[-30.8046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.869547288192363
epoch: 74, step: 106
	action: tensor([[-69804.8124, -39108.5383, -61339.8038,  76757.8731,  35462.4482,
          30746.0009, -10096.9067]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.639535653744764
epoch: 74, step: 107
	action: tensor([[-39416.1293, -40379.1756,  35772.0153, -38906.2378,   3358.3844,
          14305.6623, -17647.8555]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.639535653744764
epoch: 74, step: 108
	action: tensor([[-36417.9295, -44602.6940,  28736.3372, -56733.9858,  -3573.8590,
         -24647.9268, -16474.9505]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1648817682824144, distance: 1.045756748284798 entropy 11.639535653744764
epoch: 74, step: 109
	action: tensor([[-20873.1039, -31522.3010, -57057.0982,  66028.7909,    996.7056,
          99061.2547,  80025.0955]], dtype=torch.float64)
	q_value: tensor([[-36.2758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6889020296370825, distance: 1.487164199346978 entropy 12.181384200827793
epoch: 74, step: 110
	action: tensor([[-34179.7410,  48948.0063, -45346.6739, -42067.7694,  27669.1387,
         -26773.7557,  40308.2586]], dtype=torch.float64)
	q_value: tensor([[-30.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.125894478669606
epoch: 74, step: 111
	action: tensor([[-32735.8150,  18790.1503, -19289.6335,  -2116.6747,   6146.7016,
          34760.6503,  30868.8334]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.639535653744764
epoch: 74, step: 112
	action: tensor([[-43795.4794, -21670.6854,  39405.2047,  -9063.1733,  29200.5519,
          -9068.8384, -13426.0907]], dtype=torch.float64)
	q_value: tensor([[-33.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2510811094734089, distance: 1.2799689266281895 entropy 11.639535653744764
epoch: 74, step: 113
	action: tensor([[-75777.0132, -61734.4206,  54872.2841, 102643.9199,  51180.5595,
         -35464.4799, -20373.4017]], dtype=torch.float64)
	q_value: tensor([[-38.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11431122957667061, distance: 1.207980564480022 entropy 12.276246168094474
epoch: 74, step: 114
	action: tensor([[ -46783.4856, -111369.1760,  -14471.1436,   -2586.3838,   17154.2062,
           61554.6712,  -37182.2184]], dtype=torch.float64)
	q_value: tensor([[-33.0844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5761961419517245, distance: 1.436685879616349 entropy 12.252981115204461
epoch: 74, step: 115
	action: tensor([[  9216.5998, -10581.6518,  18027.6235,  67912.1202, -52140.4318,
          12295.7036,  20071.7505]], dtype=torch.float64)
	q_value: tensor([[-36.1219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.158944876049407, distance: 1.0494673306454652 entropy 12.16806459008559
epoch: 74, step: 116
	action: tensor([[ 35896.1892, -51986.9472,  53667.1416, -54185.0954,  40387.8162,
         -57494.8673, -27810.3421]], dtype=torch.float64)
	q_value: tensor([[-34.3348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3552916908774597, distance: 0.9188366865686773 entropy 12.20074844399666
epoch: 74, step: 117
	action: tensor([[ 15014.6568,  31742.8187, -28404.6989,  46436.9193,   4538.7595,
          90316.8381, -50614.6616]], dtype=torch.float64)
	q_value: tensor([[-34.3789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6788058880383566, distance: 0.6485455457652147 entropy 11.970319240633756
epoch: 74, step: 118
	action: tensor([[  8700.7164,  60524.3004,  -8431.7313, -32234.3561, -13548.3355,
          33667.5868,  55722.4965]], dtype=torch.float64)
	q_value: tensor([[-37.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6979736717326195, distance: 0.6288963798068581 entropy 12.011006660141094
epoch: 74, step: 119
	action: tensor([[-66602.4031, -86684.3567,  33446.5359,  65743.5723,  -2039.7644,
         -34174.8864,   1771.0465]], dtype=torch.float64)
	q_value: tensor([[-40.0219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05216412480821431, distance: 1.1738117111049622 entropy 12.04827542448832
epoch: 74, step: 120
	action: tensor([[-53874.4948,  -4096.3598, -60648.4116,  -5278.1417,    952.9121,
          -3778.0525, -15540.0564]], dtype=torch.float64)
	q_value: tensor([[-30.9019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9112138165824657, distance: 1.5820176754844293 entropy 11.942224993803325
epoch: 74, step: 121
	action: tensor([[-115563.8849,  -12525.3135,   39703.5602,  -19815.3316,   70540.2962,
           95709.9041,   50197.9400]], dtype=torch.float64)
	q_value: tensor([[-33.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7225553586111955, distance: 1.5019078500715441 entropy 12.192457474594429
epoch: 74, step: 122
	action: tensor([[-32110.3448, -20010.7724,  14032.5873,  14272.9144,  44483.2338,
         -38566.2661, -87452.2764]], dtype=torch.float64)
	q_value: tensor([[-32.7029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31273877731994126, distance: 1.3111302890575154 entropy 12.093304512904297
epoch: 74, step: 123
	action: tensor([[-88598.6247,  54204.9810,  73328.3464,  79481.6098, -70678.2541,
           -734.5875, -20802.1602]], dtype=torch.float64)
	q_value: tensor([[-33.8736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09613109556273525, distance: 1.1980858594016575 entropy 12.24569432694297
epoch: 74, step: 124
	action: tensor([[-112507.4660,   50922.7899,  -61952.4158,   40841.1124,   30426.3653,
           49773.6884, -106371.8892]], dtype=torch.float64)
	q_value: tensor([[-38.1880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2874762916528578, distance: 0.9659537949373801 entropy 12.25777233968893
epoch: 74, step: 125
	action: tensor([[-104166.6189, -155845.5516,  -74035.5531,    -763.7876,   32023.6379,
           -7074.3431,  -26429.1846]], dtype=torch.float64)
	q_value: tensor([[-38.2695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5823384629308297, distance: 1.4394824875712533 entropy 12.301122768880791
epoch: 74, step: 126
	action: tensor([[ 86944.7428, -21754.3452, -90652.0604,  38651.2966,  52420.0834,
          66167.8139, -11516.9448]], dtype=torch.float64)
	q_value: tensor([[-29.5473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.568208706796562, distance: 0.7519580858312059 entropy 12.077148979559807
epoch: 74, step: 127
	action: tensor([[ 40102.2583, -57516.4987,  -7952.9139,  26717.2918, -10913.3590,
          19472.0693, -88830.3707]], dtype=torch.float64)
	q_value: tensor([[-30.9095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22140379737912685, distance: 1.009747609979064 entropy 11.857043897052364
LOSS epoch 74 actor 470.96256649902375 critic 114.7682987049021
epoch: 75, step: 0
	action: tensor([[ 31269.1884, -20220.8550,  15556.1483, -34011.8329, -50238.6996,
          67850.7880,  15396.4379]], dtype=torch.float64)
	q_value: tensor([[-31.4625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2912602069095638, distance: 1.3003599256937752 entropy 12.012132685319253
epoch: 75, step: 1
	action: tensor([[ 53713.8921, -27995.7358, 110262.0694,  21644.9584,  -5913.6681,
          34057.1467,  43400.3994]], dtype=torch.float64)
	q_value: tensor([[-30.2592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21114326485468204, distance: 1.0163791847086954 entropy 12.088033574115359
epoch: 75, step: 2
	action: tensor([[-110020.8616,  -69655.2819,    8935.2918,  164077.3915,  -12527.7229,
          -44972.7855,  -34255.3618]], dtype=torch.float64)
	q_value: tensor([[-26.7788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.056067122719346
epoch: 75, step: 3
	action: tensor([[-20468.6949, -14452.5553,  54766.5841,  41833.8167, -19348.9058,
         -45525.2954, -17562.5938]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022615013741407286, distance: 1.1313305766096673 entropy 11.673203972238463
epoch: 75, step: 4
	action: tensor([[-104597.0502,  -12966.8725,   24007.7035,  -22170.4976,   17451.1996,
          -48592.5945,  -35560.8617]], dtype=torch.float64)
	q_value: tensor([[-28.8125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7464175828814221, distance: 1.5122748890440318 entropy 12.329475666066045
epoch: 75, step: 5
	action: tensor([[-39599.1665, -21896.5957, -29725.0551,  59937.0941,  33996.8447,
          56381.4925,  45928.0300]], dtype=torch.float64)
	q_value: tensor([[-27.6280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5473809456946763, distance: 1.423492933668892 entropy 12.205183088409518
epoch: 75, step: 6
	action: tensor([[-25969.7131, -86347.3157, -17867.7368,  38877.2218, -69764.3530,
         100469.0962, -34344.0333]], dtype=torch.float64)
	q_value: tensor([[-25.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3747985200373838, distance: 1.3417642651508546 entropy 12.106266696639585
epoch: 75, step: 7
	action: tensor([[-48354.0786, -64952.5516,  13511.7359,   5815.4010,   7239.7972,
         -38070.9050,  15709.0004]], dtype=torch.float64)
	q_value: tensor([[-26.8740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06010135532215044, distance: 1.1094231015078544 entropy 12.208407883695177
epoch: 75, step: 8
	action: tensor([[-25227.3811, -54054.4599,  -5904.4665,  10517.5262, -36466.0557,
         -46102.5872, -37059.8318]], dtype=torch.float64)
	q_value: tensor([[-30.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31707956326161524, distance: 1.3132962336283331 entropy 12.22819730330266
epoch: 75, step: 9
	action: tensor([[ -82915.9568,  -85964.6288,  -25607.8691,   23623.8110,   17615.8125,
          -59075.1413, -102360.9406]], dtype=torch.float64)
	q_value: tensor([[-27.9960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0018910586850078293, distance: 1.143261730909064 entropy 12.128673910496532
epoch: 75, step: 10
	action: tensor([[ 73789.8770, -64051.2819, -27601.1435, -53673.8181,  62801.9728,
          49725.0946,   9839.8910]], dtype=torch.float64)
	q_value: tensor([[-28.6640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2601036049222376, distance: 1.2845760488984994 entropy 12.336084794576806
epoch: 75, step: 11
	action: tensor([[-32708.4468,  15420.3893, -29865.6019,  48374.0978, -82853.0830,
          59578.4210,  57832.5466]], dtype=torch.float64)
	q_value: tensor([[-26.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.790401015466642
epoch: 75, step: 12
	action: tensor([[-19691.9490, -37690.6427,  30470.2998,   7403.3966,  18686.1533,
           8168.0415, -23320.6125]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17626063076914567, distance: 1.0386078594264236 entropy 11.673203972238463
epoch: 75, step: 13
	action: tensor([[ 42824.1108,  -6731.6527, -24674.8305,    589.6231, -21190.5585,
         -24209.0774,  24428.2978]], dtype=torch.float64)
	q_value: tensor([[-28.4306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22142676514031578, distance: 1.0097327166281052 entropy 12.223324001566926
epoch: 75, step: 14
	action: tensor([[-14459.0663,   1924.8096, -35617.5814,  29181.8165,  -2261.4600,
          -5309.2220,  56797.1067]], dtype=torch.float64)
	q_value: tensor([[-29.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49682646470088077, distance: 0.8117374610955292 entropy 12.078652466746433
epoch: 75, step: 15
	action: tensor([[-17270.3992, -16322.7869, -40573.5828, -30706.1163,  61622.9440,
          50404.2543,  -3787.7091]], dtype=torch.float64)
	q_value: tensor([[-26.8195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8769181174878302, distance: 1.567759194619804 entropy 12.064782934372536
epoch: 75, step: 16
	action: tensor([[-59448.7725, -29250.7102,  14610.7616,  38446.8184, -50040.4642,
           7791.7676,  60156.6131]], dtype=torch.float64)
	q_value: tensor([[-25.5982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.057045729850405236, distance: 1.1765315597015564 entropy 12.176653038948839
epoch: 75, step: 17
	action: tensor([[-16726.3183,   4161.9136, -60714.8989,   7435.2406, -23297.1770,
           2730.2208,  23339.0238]], dtype=torch.float64)
	q_value: tensor([[-27.2341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22886671318722795, distance: 1.0048966962836556 entropy 12.202724197855758
epoch: 75, step: 18
	action: tensor([[ -6851.4893, -40930.3192, -17598.9555,  18967.3420, -78857.2259,
         -28787.3183,  11610.2915]], dtype=torch.float64)
	q_value: tensor([[-29.9550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1384368060297384, distance: 1.062185239632805 entropy 12.183881325588185
epoch: 75, step: 19
	action: tensor([[-27504.7924,   6001.3804, -17091.9890,  -6727.7898, -33997.1889,
          24035.4608,  11659.1347]], dtype=torch.float64)
	q_value: tensor([[-28.5242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5745825908609576, distance: 1.4359503240637717 entropy 12.253198939161097
epoch: 75, step: 20
	action: tensor([[  4131.0044,  -3872.2976, -20413.1715,   9665.6209, -15813.6464,
          66291.2451,  -3880.4911]], dtype=torch.float64)
	q_value: tensor([[-27.4457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5114514004396469, distance: 0.7998537395904707 entropy 11.920249443729242
epoch: 75, step: 21
	action: tensor([[-5527.6820, 14762.3901, 36731.9065, -9040.1781, 61813.3864, 32045.2494,
          4410.0011]], dtype=torch.float64)
	q_value: tensor([[-28.1143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.074864599075218
epoch: 75, step: 22
	action: tensor([[-34933.5006,  -4548.8015,   7259.0776,  -7419.0540, -29059.4200,
           3508.0417, -28935.8987]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7548305225188321, distance: 1.5159130200898374 entropy 11.673203972238463
epoch: 75, step: 23
	action: tensor([[-77975.7509,  -6579.9036,  35526.4830,  77984.7119,  -1864.3477,
         -19271.9087, -68778.4918]], dtype=torch.float64)
	q_value: tensor([[-26.4466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24435979428107246, distance: 1.2765260401231604 entropy 12.159357032673167
epoch: 75, step: 24
	action: tensor([[-38876.8574, -99616.7464,  -2242.5344, -29161.9773, -19400.8345,
          29571.6403, 109514.4058]], dtype=torch.float64)
	q_value: tensor([[-29.4831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26877998467215103, distance: 1.288990904133216 entropy 12.244464377937977
epoch: 75, step: 25
	action: tensor([[ 8610.0659,  5672.1313, 47739.4037, 26257.0750, 37282.6964, 20876.7356,
         30097.8008]], dtype=torch.float64)
	q_value: tensor([[-25.6978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.991959638206152
epoch: 75, step: 26
	action: tensor([[-4.8252e+04, -1.4955e+04,  7.0352e+04, -2.9485e+04, -6.6644e+04,
          3.7914e+02,  4.3549e+01]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012146182862190047, distance: 1.1512729853756745 entropy 11.673203972238463
epoch: 75, step: 27
	action: tensor([[-45903.1016,  40193.7910,   2178.1699,  27558.3437,  27998.0895,
          42501.8564,  20834.7749]], dtype=torch.float64)
	q_value: tensor([[-26.8647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002821620932034752, distance: 1.1427286606967095 entropy 12.046292176320474
epoch: 75, step: 28
	action: tensor([[ -6430.1556,  -6117.4440,  40814.4709,  89980.0898,  -8717.6113,
           6805.8495, -39331.7025]], dtype=torch.float64)
	q_value: tensor([[-29.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6633372660315024, distance: 1.4758657425934416 entropy 11.922136547615711
epoch: 75, step: 29
	action: tensor([[ -98152.3646, -116813.9077,  -78196.6548,   70832.7919,   28223.6576,
           41482.9579,  -28197.4913]], dtype=torch.float64)
	q_value: tensor([[-27.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22447818402308217, distance: 1.0077520793652028 entropy 12.269603525638814
epoch: 75, step: 30
	action: tensor([[-134054.8043,  -25930.6896,  -57692.8679,    3285.6821,  -15044.4171,
           43622.8097,   -9775.6071]], dtype=torch.float64)
	q_value: tensor([[-27.7763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.802488359570686, distance: 1.5363597739472767 entropy 12.274108610554492
epoch: 75, step: 31
	action: tensor([[ -54482.0932, -129974.2509,  -30243.0848,    2054.9662,   12365.6837,
          -31637.5954,  -29734.9130]], dtype=torch.float64)
	q_value: tensor([[-27.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7327866109485033, distance: 1.5063615961169858 entropy 12.253643019846493
epoch: 75, step: 32
	action: tensor([[-88583.8042, -52604.6554, -68003.3163,  71868.4832,  16487.6139,
         -70166.5592,  25432.1232]], dtype=torch.float64)
	q_value: tensor([[-28.0927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8119078499132255, distance: 1.5403689173453974 entropy 12.282713894881066
epoch: 75, step: 33
	action: tensor([[48382.9692, 12154.7096, 36366.2943, -1419.0460,  9959.7644, 20475.7270,
         45989.1892]], dtype=torch.float64)
	q_value: tensor([[-27.1765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.193167716229732
epoch: 75, step: 34
	action: tensor([[-36933.4109, -36788.6907, -16775.4721, -38659.1408, -72508.4083,
           6801.4984, -22605.8186]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9616209179595208, distance: 1.6027442787478692 entropy 11.673203972238463
epoch: 75, step: 35
	action: tensor([[11164.2575,  9278.7684, 78236.5595, 37521.9104, -4955.7231, 19297.5109,
          2053.5681]], dtype=torch.float64)
	q_value: tensor([[-22.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.843298032608901
epoch: 75, step: 36
	action: tensor([[-32316.7726,  36143.7477,  21682.2137,  47553.1906, -45632.9792,
          -8618.6500,    546.4641]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11590719795425941, distance: 1.2088453179339145 entropy 11.673203972238463
epoch: 75, step: 37
	action: tensor([[  7773.0145,   1167.8357, -82183.8466,  74768.7743,  61151.2371,
         -47033.2968,  45328.5846]], dtype=torch.float64)
	q_value: tensor([[-28.4106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5500482872757078, distance: 0.7676082750419378 entropy 12.00794774480129
epoch: 75, step: 38
	action: tensor([[-48361.9505, -48765.1141, -91595.1713, -25255.7652,  70206.7024,
          61754.3892,  12280.3257]], dtype=torch.float64)
	q_value: tensor([[-27.3865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.175523502523182
epoch: 75, step: 39
	action: tensor([[ 36437.7152,   3364.1704,  41986.0824, -16637.2517, -12853.6264,
          17348.1715,   8536.0887]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2687267560810146, distance: 0.9785804496784577 entropy 11.673203972238463
epoch: 75, step: 40
	action: tensor([[ 10217.6778,  43073.6884, -49207.7833,   7047.7155, 113102.3101,
          21959.0330, -30797.2811]], dtype=torch.float64)
	q_value: tensor([[-33.1017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2705728898935813, distance: 0.977344432827888 entropy 11.994797157361807
epoch: 75, step: 41
	action: tensor([[ -35666.0734,  -83625.7739,   70936.3825,   28038.7027, -139557.8068,
          -26442.7270,   43742.3692]], dtype=torch.float64)
	q_value: tensor([[-31.4650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29100584754110004, distance: 0.9635583514322631 entropy 12.190089583890423
epoch: 75, step: 42
	action: tensor([[-45450.2731, -45305.0892, -81716.4247,  35352.3820,  41896.9560,
         -53171.4206, 132740.2492]], dtype=torch.float64)
	q_value: tensor([[-25.5656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5169130099578276, distance: 1.4094089704852775 entropy 12.15048923596332
epoch: 75, step: 43
	action: tensor([[-77875.5793, -24473.1706, -43582.1546, -43527.6734,  38501.6606,
          11479.6100, -30778.0741]], dtype=torch.float64)
	q_value: tensor([[-27.6369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4282863085837152, distance: 1.3676164936965727 entropy 12.092973487934659
epoch: 75, step: 44
	action: tensor([[ 54788.2748,  -7606.9120,  55461.7118, -67848.0912, -18054.1689,
         -82644.9054, -13115.0700]], dtype=torch.float64)
	q_value: tensor([[-30.3419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.182367491042416, distance: 1.034750794618814 entropy 12.311202718051069
epoch: 75, step: 45
	action: tensor([[-10022.0439,   5425.5389,  38692.6855,  76743.7389,  -8159.8367,
         -39692.9943,   9132.1642]], dtype=torch.float64)
	q_value: tensor([[-26.5415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4240406995794649, distance: 0.8684655407159091 entropy 12.24320760832755
epoch: 75, step: 46
	action: tensor([[ -9635.9851, -64165.9625, -24167.6165,  93080.8397,   3494.9700,
          33672.1874,  -6079.9571]], dtype=torch.float64)
	q_value: tensor([[-30.2121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5359464193632519, distance: 1.4182236594263133 entropy 12.109546076721253
epoch: 75, step: 47
	action: tensor([[-13039.6003, -27116.1253, -41126.3322, -20616.6372, -47121.3668,
         -69391.7762, -68272.7698]], dtype=torch.float64)
	q_value: tensor([[-27.7415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8701533153070253, distance: 1.564931379661286 entropy 12.227619335618757
epoch: 75, step: 48
	action: tensor([[-62562.4905, -44540.6577,  61096.3133,  47338.2205, -27354.2790,
         -68470.8787,  21667.0560]], dtype=torch.float64)
	q_value: tensor([[-27.8753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13206153794831754, distance: 1.0661078971647961 entropy 12.252821481274745
epoch: 75, step: 49
	action: tensor([[ 17636.1048,   8287.7830,  81671.1859,  92593.0296, -29586.2429,
         -37710.4960, -19048.8525]], dtype=torch.float64)
	q_value: tensor([[-29.9488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8470904196051188, distance: 0.4474804246468034 entropy 12.2899146282722
epoch: 75, step: 50
	action: tensor([[-103700.2722,    4906.0217,  -33267.8500,  -36624.1245,   97708.8615,
          104043.9077,    5212.8643]], dtype=torch.float64)
	q_value: tensor([[-30.2296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.178296645311605
epoch: 75, step: 51
	action: tensor([[ -5262.3534,  17736.9341,  18793.4665, -26438.0123,   4570.1445,
         -15687.6776, -52347.4530]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.673203972238463
epoch: 75, step: 52
	action: tensor([[ -3215.2270,  -6295.2440,   -690.4997,  22621.7621,  38117.0059,
           8944.6631, -21627.6603]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7843932327017789, distance: 1.5286285867460543 entropy 11.673203972238463
epoch: 75, step: 53
	action: tensor([[ -3739.8774, -70707.5115,   9324.3535, -26964.0862,  87332.2019,
          38930.5425,  49410.1330]], dtype=torch.float64)
	q_value: tensor([[-25.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04701350807771543, distance: 1.1709351297488033 entropy 12.23528084145859
epoch: 75, step: 54
	action: tensor([[   -859.3126,   58587.8180,   10110.8617,  -24432.9892,  -38908.7137,
          -22284.9296, -112728.0729]], dtype=torch.float64)
	q_value: tensor([[-25.8028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0817613849922767, distance: 1.1902068093042815 entropy 12.00348850066089
epoch: 75, step: 55
	action: tensor([[  4756.3625,  79464.7716,  14580.1326, -52641.6053, -18653.7784,
         -34848.3133,  32573.1024]], dtype=torch.float64)
	q_value: tensor([[-34.5302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4003646309909368, distance: 0.8861358643273463 entropy 12.170911023402207
epoch: 75, step: 56
	action: tensor([[  8586.2028, -32188.8048, -72073.8033, -22056.7130,  -6308.7037,
         -48863.4360, -56562.4662]], dtype=torch.float64)
	q_value: tensor([[-29.1207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22743461220059624, distance: 1.0058293794114495 entropy 11.983738021815565
epoch: 75, step: 57
	action: tensor([[-87680.2765, -60348.6814,  -5255.4746,  28286.7059,  18700.3242,
         -36272.3882,  26997.9507]], dtype=torch.float64)
	q_value: tensor([[-27.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0017245948730392202, distance: 1.1433570630735106 entropy 11.931887430836072
epoch: 75, step: 58
	action: tensor([[-76367.5953, -41404.4344,  19983.7688,   2722.5726, -19899.9453,
           1545.7008,  54684.8054]], dtype=torch.float64)
	q_value: tensor([[-26.6730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20281751471523612, distance: 1.021728640507665 entropy 12.029861008273286
epoch: 75, step: 59
	action: tensor([[-67069.4487,  -5868.4559, -43209.4804,  40510.7059,  16536.6110,
          71811.8810,  -4959.6696]], dtype=torch.float64)
	q_value: tensor([[-30.7616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1432081606839174, distance: 1.0592399543781938 entropy 12.439281594202447
epoch: 75, step: 60
	action: tensor([[-100437.5580,   21399.6369,  110003.8205,  -87295.8158,   73599.0779,
          -35245.7454,  -64431.2643]], dtype=torch.float64)
	q_value: tensor([[-26.1668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13580742503724774, distance: 1.0638048318221838 entropy 12.250370979952638
epoch: 75, step: 61
	action: tensor([[  3250.6010, -25476.1514, -26812.1622,  66871.9042,  73666.2379,
           -143.4661, -25087.4586]], dtype=torch.float64)
	q_value: tensor([[-32.1785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09396176112750043, distance: 1.0892559900404732 entropy 12.255496168634666
epoch: 75, step: 62
	action: tensor([[ -13251.8725,  -16715.5949,   28681.2062, -103998.2573,   65715.2817,
           42671.6690,   44351.1063]], dtype=torch.float64)
	q_value: tensor([[-30.6740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9205246678729455, distance: 1.5858665475519866 entropy 12.252404860963908
epoch: 75, step: 63
	action: tensor([[  9751.2994, -77285.6663, -38668.7289,  49627.1290,  -2711.3129,
          41710.0139,  -6197.3050]], dtype=torch.float64)
	q_value: tensor([[-25.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22142456359390406, distance: 1.264707278906506 entropy 12.089417670193512
epoch: 75, step: 64
	action: tensor([[-81641.7023,   5621.4392,  21317.1633, -78865.6698,  38764.4678,
          21994.5876,  21277.1663]], dtype=torch.float64)
	q_value: tensor([[-28.2797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.33672887882487
epoch: 75, step: 65
	action: tensor([[-31478.3866,  20105.6524,  -3209.3077,  -2818.9542, -28163.2386,
          12007.4552,  23409.1190]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.673203972238463
epoch: 75, step: 66
	action: tensor([[ 22569.8546, -41943.4868, -35594.4179,  -9773.0465, -71658.4660,
          16009.8142,  -3636.4606]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3381447154555044, distance: 1.3237568941516258 entropy 11.673203972238463
epoch: 75, step: 67
	action: tensor([[-52355.4772, -28782.8735,  41693.8151,   4762.0949, -30968.5602,
          45296.3820,  39335.0540]], dtype=torch.float64)
	q_value: tensor([[-27.6653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11854860752840768, distance: 1.2102751717352511 entropy 11.898006629784385
epoch: 75, step: 68
	action: tensor([[-80593.4782, -15397.8167, -51605.7607,   4187.7547, -13393.2331,
          40364.2719,   -113.2233]], dtype=torch.float64)
	q_value: tensor([[-27.3583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7731972822617745, distance: 1.5238254470147474 entropy 12.209404570379789
epoch: 75, step: 69
	action: tensor([[ -11806.9990,   -8681.3354,   13200.9122,   42378.9068,  -72864.3868,
           28663.3164, -105715.9935]], dtype=torch.float64)
	q_value: tensor([[-29.8943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2940502549372599, distance: 1.3017640226118825 entropy 12.295067123858127
epoch: 75, step: 70
	action: tensor([[ -72550.2665, -113395.0905,    6390.7800,   48152.2397,  -11190.9367,
           13625.5463,   36096.9658]], dtype=torch.float64)
	q_value: tensor([[-26.8829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07621684338203782, distance: 1.1871527028452138 entropy 12.135339495471085
epoch: 75, step: 71
	action: tensor([[-40355.3486,  42419.3681,   8593.0919,  24804.0116, -11318.9881,
           1023.2588,   6727.4258]], dtype=torch.float64)
	q_value: tensor([[-25.4159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2683528024615637, distance: 0.978830627660916 entropy 12.064806153975107
epoch: 75, step: 72
	action: tensor([[-45276.1645,  -4066.0304, -73394.9126,  28042.2557,  26305.7871,
         -40953.0994, -19677.4887]], dtype=torch.float64)
	q_value: tensor([[-29.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21325449476282743, distance: 1.2604703891677433 entropy 12.338874513718537
epoch: 75, step: 73
	action: tensor([[-74943.8651, -62756.1982,   5564.0436, 127982.9893,   7061.6882,
         -21115.4030,  -8207.0195]], dtype=torch.float64)
	q_value: tensor([[-29.3111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5397620237895508, distance: 1.4199841452833422 entropy 12.356092412817363
epoch: 75, step: 74
	action: tensor([[ 15562.4841, -92590.7852,   8569.4977, -15861.9305, -22122.7529,
          45551.7609, -43700.9512]], dtype=torch.float64)
	q_value: tensor([[-27.4258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029713117304542958, distance: 1.1272150363716373 entropy 12.292919989394761
epoch: 75, step: 75
	action: tensor([[ 21002.8889, -59157.9922, -19760.4011,  34095.6665,  -1476.2137,
          29919.4574,  39350.6610]], dtype=torch.float64)
	q_value: tensor([[-25.3131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24783855283614797, distance: 0.9924582082835376 entropy 11.802726972424788
epoch: 75, step: 76
	action: tensor([[ 33797.7207, -16147.0249, -15316.4051,  65823.9174,  -2786.1147,
         -32456.1938,  26077.6342]], dtype=torch.float64)
	q_value: tensor([[-29.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02577010473536301, distance: 1.1295030796388374 entropy 11.951987760709796
epoch: 75, step: 77
	action: tensor([[  -1854.1028,  -56162.4143,   65696.9482,   35873.9660,    -620.8222,
           80533.7163, -103963.9589]], dtype=torch.float64)
	q_value: tensor([[-32.3543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5342325302949953, distance: 0.7809824413173092 entropy 12.280297135225394
epoch: 75, step: 78
	action: tensor([[   -92.8477, -43702.7943,  75427.2945,  44514.1658, -12684.3885,
          24984.0678, -19815.3087]], dtype=torch.float64)
	q_value: tensor([[-25.9035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38817766587434055, distance: 1.3482772907506273 entropy 12.125566914490387
epoch: 75, step: 79
	action: tensor([[ 24525.9722, -72110.9025,  11091.9338, -76228.1815,  -8971.8952,
         -47775.7056,  75912.0150]], dtype=torch.float64)
	q_value: tensor([[-28.1748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5379352272777269, distance: 0.7778719714244711 entropy 12.239064356562825
epoch: 75, step: 80
	action: tensor([[  4380.6307,   -388.5806, -16541.1788,  83106.6101,  71069.7065,
         -57470.5317,  53658.9759]], dtype=torch.float64)
	q_value: tensor([[-29.9955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3195819854928359, distance: 1.3145432585734433 entropy 12.14238845320421
epoch: 75, step: 81
	action: tensor([[-48678.2100,  -1237.7697, -10625.0115, -25459.4897,  17235.8106,
         -31376.3253,  16195.0857]], dtype=torch.float64)
	q_value: tensor([[-31.1986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5138390508237289, distance: 1.4079801928738727 entropy 12.066842390637799
epoch: 75, step: 82
	action: tensor([[-40193.8242,  16417.3347, -12241.1341,  43668.7484,  55614.5334,
          44142.4629, -67536.7952]], dtype=torch.float64)
	q_value: tensor([[-24.8410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43063949353145836, distance: 1.3687426439835564 entropy 12.025104606775102
epoch: 75, step: 83
	action: tensor([[  3300.3010, -35177.5653,   3406.2616,  31120.8008, -34090.5734,
         -59406.0083,  13896.1141]], dtype=torch.float64)
	q_value: tensor([[-24.5183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40169866967059986, distance: 0.8851496001715398 entropy 11.951149101173163
epoch: 75, step: 84
	action: tensor([[ -7625.3306, -60293.7625,  36733.1619,   2957.7482, -74230.6484,
         -46453.3720, -21326.1433]], dtype=torch.float64)
	q_value: tensor([[-32.7971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16449275658689122, distance: 1.046000285203564 entropy 12.207620916908649
epoch: 75, step: 85
	action: tensor([[ -36875.8324,  -53822.4130, -108076.5498,   42562.1234,   36564.9008,
           31797.9367,  -10347.3559]], dtype=torch.float64)
	q_value: tensor([[-26.2232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5699052493841816, distance: 1.4338159726153241 entropy 12.078165005659157
epoch: 75, step: 86
	action: tensor([[-44255.6625,  27719.6146,  36805.8762, 120674.2668,  30627.1563,
         -40823.9930, -81681.3841]], dtype=torch.float64)
	q_value: tensor([[-27.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.134632139487192, distance: 1.0645279632656506 entropy 12.301616155114287
epoch: 75, step: 87
	action: tensor([[-25206.4422,  58196.2650,  19379.9543,  96568.6130,  75172.9892,
         -28315.5095,  22389.1640]], dtype=torch.float64)
	q_value: tensor([[-29.8657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09277579171005301, distance: 1.089968654128945 entropy 12.146281623808676
epoch: 75, step: 88
	action: tensor([[-106181.5726,  -21917.8341,   54333.4176,   14165.7249,   28733.8678,
           55582.9419,   83779.0993]], dtype=torch.float64)
	q_value: tensor([[-32.6759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04021070546522765, distance: 1.1671249489000155 entropy 12.318677604728034
epoch: 75, step: 89
	action: tensor([[ 72966.3097,  67027.9405,  62807.0418,  30293.9698,  24651.2915,
         -29234.1694, -97382.6305]], dtype=torch.float64)
	q_value: tensor([[-28.6208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16044044734911445, distance: 1.0485338296511633 entropy 12.314496224049297
epoch: 75, step: 90
	action: tensor([[ -40522.7502, -107354.1777,   30465.7787,  -61712.0717,   64765.0950,
          -23212.2667,  -13739.2621]], dtype=torch.float64)
	q_value: tensor([[-28.3919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.050688714900058
epoch: 75, step: 91
	action: tensor([[  9672.1437, -50994.9721, -30543.3382,  22398.1999,  21335.9360,
         -34078.8133,  24998.1970]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4154112761037836, distance: 0.8749473301794378 entropy 11.673203972238463
epoch: 75, step: 92
	action: tensor([[   4532.1733,   -9778.1569,   59910.8442,  -54685.2227, -104196.9436,
          -40202.1896,  -29444.3201]], dtype=torch.float64)
	q_value: tensor([[-29.0418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17356616964941174, distance: 1.2396825387070602 entropy 12.149921590851282
epoch: 75, step: 93
	action: tensor([[ 47648.5049, -58742.6050,  -5713.3796,    994.4401,  76893.1957,
          15324.8255,  -7179.4056]], dtype=torch.float64)
	q_value: tensor([[-26.3071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3858280965863614, distance: 0.8968125263440285 entropy 12.06659055733299
epoch: 75, step: 94
	action: tensor([[-88732.0943, -53658.9850, -27379.1762, -15196.5934,   9393.7956,
         -47021.6947,  63147.8220]], dtype=torch.float64)
	q_value: tensor([[-28.9826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0839298952076849, distance: 1.0952696374569981 entropy 12.144276237115223
epoch: 75, step: 95
	action: tensor([[-71526.8657, -46301.3480,  85465.9929, -50541.7120,  12702.9359,
          83009.7453,  20373.7000]], dtype=torch.float64)
	q_value: tensor([[-25.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4924747822889284, distance: 1.3980097303004146 entropy 12.143445024884096
epoch: 75, step: 96
	action: tensor([[-34698.5854,   9084.3930, -27615.5775, -16898.2950, -49510.8195,
           4824.6869,  -7494.5549]], dtype=torch.float64)
	q_value: tensor([[-28.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1824457199959899, distance: 1.2443636040304713 entropy 12.22472274482793
epoch: 75, step: 97
	action: tensor([[  -5482.2670,  -23089.5998,   71197.9153,  -73250.0742,   38541.5616,
          -20984.4760, -135831.8597]], dtype=torch.float64)
	q_value: tensor([[-34.3940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011451994321102, distance: 1.1377728740246824 entropy 12.125635064597637
epoch: 75, step: 98
	action: tensor([[-28512.0547, -39041.3421, -69199.5003,  30440.8905,  24375.0643,
          63927.0600, -37392.1540]], dtype=torch.float64)
	q_value: tensor([[-25.9501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1907009413279446, distance: 1.6937458366806553 entropy 12.102816671936045
epoch: 75, step: 99
	action: tensor([[-43021.5620, -87464.6631,   -159.7461,  84765.1470,   9259.9058,
         -18875.2083, -31791.6490]], dtype=torch.float64)
	q_value: tensor([[-28.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32127751428898366, distance: 1.3153875147366196 entropy 12.237764083249033
epoch: 75, step: 100
	action: tensor([[ -42072.4730,   37169.6473,   -2499.5404,  -17009.7833,  -24958.0572,
          -42874.4912, -100041.8024]], dtype=torch.float64)
	q_value: tensor([[-25.7882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06856601194416156, distance: 1.182925438999353 entropy 12.267904493748366
epoch: 75, step: 101
	action: tensor([[ 33993.0084, -50977.1034, -52323.5127,   9294.7106,  38930.3968,
         -16678.2740,  27298.3438]], dtype=torch.float64)
	q_value: tensor([[-25.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33144783981825965, distance: 1.3204403042058042 entropy 11.944271922914902
epoch: 75, step: 102
	action: tensor([[ 29269.7656, -51904.8121,  99475.8555,  25568.6404,  59837.1243,
         -69497.6135,  75419.5899]], dtype=torch.float64)
	q_value: tensor([[-30.7653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5041475644003219, distance: 0.8058104936364903 entropy 12.14118400030151
epoch: 75, step: 103
	action: tensor([[-67607.3483,   -605.5679,  11458.9237,  77792.2792, -16641.0649,
          42524.1026,  46662.2189]], dtype=torch.float64)
	q_value: tensor([[-35.5926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4503830739947745, distance: 1.3781549671258115 entropy 12.222247109736646
epoch: 75, step: 104
	action: tensor([[ -90692.1725,  -71632.3268, -117164.5951,   13241.6802,    8102.1705,
          -83566.4913,   25044.7270]], dtype=torch.float64)
	q_value: tensor([[-28.2444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17124284414761493, distance: 1.0417663828922317 entropy 12.265914699409302
epoch: 75, step: 105
	action: tensor([[-67418.0874, -70704.7623,  -6617.2401, -86431.8940,  27318.7696,
         104869.6733, -40938.1620]], dtype=torch.float64)
	q_value: tensor([[-28.3524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9370384082476853, distance: 1.592670035576813 entropy 12.400743687220517
epoch: 75, step: 106
	action: tensor([[-21980.1261,  -4831.8075,  33761.1285,  42786.8713,  40288.0754,
         -24120.3068, -15517.9100]], dtype=torch.float64)
	q_value: tensor([[-23.8061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7133909546958657, distance: 1.4979072686005246 entropy 12.079670732458775
epoch: 75, step: 107
	action: tensor([[-30568.2069,   2710.8055, -55245.2903,  71058.3275, -21731.4932,
         -20989.9698, -43726.2824]], dtype=torch.float64)
	q_value: tensor([[-26.5761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5788227099167634, distance: 0.7426585173877185 entropy 12.078208364211104
epoch: 75, step: 108
	action: tensor([[ -32083.9038, -100258.1063,    5245.3517,   39526.5882,  -38264.5876,
           76466.0057,   16565.1951]], dtype=torch.float64)
	q_value: tensor([[-29.7273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13800740290656188, distance: 1.0624499032383719 entropy 12.104252964388426
epoch: 75, step: 109
	action: tensor([[ 88563.9791, -30462.8980,  40795.8249,  14320.2707,  35169.2051,
         107628.2520, -61692.8631]], dtype=torch.float64)
	q_value: tensor([[-27.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5154076598007298, distance: 0.7966085544711775 entropy 12.27024740334842
epoch: 75, step: 110
	action: tensor([[-15378.6234, -38635.4222,   2293.8356, -18448.6427, -33842.4924,
         -80192.2594, -37037.5893]], dtype=torch.float64)
	q_value: tensor([[-28.9093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1741907254663646, distance: 1.2400123658513298 entropy 12.122420443526883
epoch: 75, step: 111
	action: tensor([[ -7076.9684,  -2774.7060, -49510.9404,  44077.7801,  35282.2543,
         -20937.5716, -13475.6181]], dtype=torch.float64)
	q_value: tensor([[-24.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25705963941483945, distance: 1.2830235696243544 entropy 12.08813912547771
epoch: 75, step: 112
	action: tensor([[   9346.1127,  -38183.6380,    1025.4837, -101024.1335,   57977.2372,
           24430.9632,   -5781.0993]], dtype=torch.float64)
	q_value: tensor([[-30.1548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18078201013148942, distance: 1.2434878814926142 entropy 12.211023985912307
epoch: 75, step: 113
	action: tensor([[-32907.0040,  23300.5647, -37500.5752,  15961.0617,  -7991.3252,
         -12759.8446,  47253.5997]], dtype=torch.float64)
	q_value: tensor([[-27.3342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.900082840282511
epoch: 75, step: 114
	action: tensor([[ -6850.8398,  10244.0723,   2536.6611,  29615.3774, -17279.4037,
         -13114.5176,  24175.2262]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22211720281395353, distance: 1.0092849025338153 entropy 11.673203972238463
epoch: 75, step: 115
	action: tensor([[ -70935.4646, -163215.7256,   85598.2831,  -10343.3583,   61499.2882,
           56428.1461,  -81279.0869]], dtype=torch.float64)
	q_value: tensor([[-28.7056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6029017243984023, distance: 1.4488056846033661 entropy 12.19122262621615
epoch: 75, step: 116
	action: tensor([[-103172.9511,  -68151.3476,    2371.3582,   81091.3786,   -7229.1078,
            3726.3258,  -40858.9264]], dtype=torch.float64)
	q_value: tensor([[-26.7864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2522031618627558, distance: 1.2805427784383543 entropy 12.078700814228881
epoch: 75, step: 117
	action: tensor([[-59638.6541, -54402.7608,   2527.0693,  -2243.0077, -18558.8063,
          94905.4435, -51990.4751]], dtype=torch.float64)
	q_value: tensor([[-28.7487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33271513676306985, distance: 1.3210685646271738 entropy 12.35128318423533
epoch: 75, step: 118
	action: tensor([[ 10788.5865,  10562.4793,  57465.7826, -20344.1803, -12271.3642,
         -27251.1660, -29876.0054]], dtype=torch.float64)
	q_value: tensor([[-26.2833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47252079512040535, distance: 0.8311116398220099 entropy 12.106560013301747
epoch: 75, step: 119
	action: tensor([[-35069.4606, -43137.4659, -68401.2109,  41683.5147,  26050.1882,
         -34107.5981,  20786.5239]], dtype=torch.float64)
	q_value: tensor([[-25.7201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6686375535971096, distance: 1.4782153235319617 entropy 11.79215365209708
epoch: 75, step: 120
	action: tensor([[ 37982.1040, -85346.3645,   -808.6531,    910.2184, -36812.2609,
          37024.4917,  78787.7174]], dtype=torch.float64)
	q_value: tensor([[-25.5866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.05185909918317
epoch: 75, step: 121
	action: tensor([[-20285.0094, -31831.7342,  13650.6945,  -9448.3180, -27569.6849,
         -23112.5257,  21325.7935]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03245800079715533, distance: 1.1256194952284235 entropy 11.673203972238463
epoch: 75, step: 122
	action: tensor([[-41742.3203, -54409.4061, 127303.6581, -11246.6446,  -8603.5835,
          -8045.0954,   5635.5454]], dtype=torch.float64)
	q_value: tensor([[-27.8677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0396239130727114, distance: 1.1214434072068906 entropy 12.126522794951397
epoch: 75, step: 123
	action: tensor([[ 58901.7232,  32169.7345, -74241.2623, -58725.6912, -72393.0189,
         -29043.5110,  53322.5024]], dtype=torch.float64)
	q_value: tensor([[-25.3833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.091115824745213
epoch: 75, step: 124
	action: tensor([[ -3010.5923, -42506.7570, -31317.1457, -39934.6755,   7751.1119,
          16463.4928, -65134.8469]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19982335652997985, distance: 1.0236456106257446 entropy 11.673203972238463
epoch: 75, step: 125
	action: tensor([[ 68477.9411,  53143.1178, -13522.0988,  63283.0690, -29336.2332,
         -51876.5700, -80864.0024]], dtype=torch.float64)
	q_value: tensor([[-29.4734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.204867858893534
epoch: 75, step: 126
	action: tensor([[ 19043.2674,   6519.9464,  17935.7960,  -2121.3189,  -3717.5463,
          28391.6347, -62505.3663]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.673203972238463
epoch: 75, step: 127
	action: tensor([[ 36410.8151, -22040.6159, -61853.9275, -28228.7997,  28291.4358,
          -1272.9961,  -3026.5201]], dtype=torch.float64)
	q_value: tensor([[-27.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5750032528146959, distance: 0.7460183263958922 entropy 11.673203972238463
LOSS epoch 75 actor 319.54908367880046 critic 252.71288956289914
epoch: 76, step: 0
	action: tensor([[-54956.7009, -53485.7704, -50679.9351, -20390.5789,  30740.8846,
          73899.0196,  35961.7280]], dtype=torch.float64)
	q_value: tensor([[-22.8492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.196020590812983
epoch: 76, step: 1
	action: tensor([[-11527.4998, -32141.5998,   5029.6716,  34200.9260,  13163.7187,
          50646.1531,  63041.8054]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 2
	action: tensor([[-41507.5454, -31624.5156,  42055.8743,  35487.6648, -13685.8526,
          25343.0401, -28687.3107]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11108936016765647, distance: 1.078911309489551 entropy 11.706406437789237
epoch: 76, step: 3
	action: tensor([[-69641.4931,  -7848.5419,  48417.5616,  44973.1638,  55948.6944,
           5071.2575, -34807.8121]], dtype=torch.float64)
	q_value: tensor([[-24.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.645843438588076, distance: 1.4680841621876983 entropy 12.235110576169033
epoch: 76, step: 4
	action: tensor([[-81632.8770, -77426.4744,  67504.9781, -15158.0559, -18549.8413,
          69412.5356, 169025.4796]], dtype=torch.float64)
	q_value: tensor([[-26.2000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16820351534456757, distance: 1.2368469078603783 entropy 12.425521621924082
epoch: 76, step: 5
	action: tensor([[ 32791.6737, -13130.1976, -78608.9106,  63820.0696, -25809.1304,
          48330.3001,  82778.1542]], dtype=torch.float64)
	q_value: tensor([[-23.8533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3846423884406136, distance: 0.8976777932034801 entropy 12.147227988272968
epoch: 76, step: 6
	action: tensor([[ 14199.4135, -74124.0548,  11040.4180,  44150.4307,  13481.8175,
         -13965.6159, -12271.4577]], dtype=torch.float64)
	q_value: tensor([[-26.1107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13048020608945166, distance: 1.2167130735175062 entropy 12.104152386677203
epoch: 76, step: 7
	action: tensor([[-52003.3502, -67506.2658, -26982.1621,  31270.6596,  50051.6564,
          29278.4803,  -4265.9790]], dtype=torch.float64)
	q_value: tensor([[-26.1028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.126434163471718
epoch: 76, step: 8
	action: tensor([[-25337.0991, -20133.7854,   3274.9462,  59052.8971, -28829.5447,
          30681.9467, -10669.7343]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3295536190205115, distance: 1.3195006895821189 entropy 11.706406437789237
epoch: 76, step: 9
	action: tensor([[ 24589.8961, -27414.8756,  61514.8496,  82784.5685,   7936.2538,
          77275.9662, -11573.6643]], dtype=torch.float64)
	q_value: tensor([[-20.8453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2627968390531742, distance: 0.9825401079878814 entropy 12.144911770682464
epoch: 76, step: 10
	action: tensor([[-10318.0616,   7074.7025, -39942.9378,  41879.2022, -26767.0701,
         125815.0185,  26266.3235]], dtype=torch.float64)
	q_value: tensor([[-27.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1585759132910466, distance: 1.049697500994953 entropy 11.976592401451793
epoch: 76, step: 11
	action: tensor([[  6817.0866,  22886.1186, -27248.4207,  26777.4436, -22395.2527,
         -26585.8157,  40934.3441]], dtype=torch.float64)
	q_value: tensor([[-23.6486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3729556327902336, distance: 0.9061619638610182 entropy 12.053389922358592
epoch: 76, step: 12
	action: tensor([[ -5293.3157, -68487.5930, -23505.8491, -61263.1529,  -6580.1094,
          57024.2892,  18591.8313]], dtype=torch.float64)
	q_value: tensor([[-24.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5887071651841984, distance: 1.4423764415088567 entropy 12.049327615563621
epoch: 76, step: 13
	action: tensor([[-14766.9910,  26269.2487, -65067.5352,  -7140.2592, -51554.2357,
         -28766.4384,  -6720.4839]], dtype=torch.float64)
	q_value: tensor([[-19.6934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028410401019287934, distance: 1.1279714873016895 entropy 11.915945301748271
epoch: 76, step: 14
	action: tensor([[-61103.5533, -40648.8184, -39644.7747,  31199.9620, 108931.0083,
          10795.2537,  16390.7749]], dtype=torch.float64)
	q_value: tensor([[-26.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07389234261841393, distance: 1.1858699552925198 entropy 12.010643703544888
epoch: 76, step: 15
	action: tensor([[   863.1693,  18805.8004,  -7842.3284, -15094.4933,  -7494.5659,
         -42035.8219,  80730.1751]], dtype=torch.float64)
	q_value: tensor([[-23.1166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.2218425640034
epoch: 76, step: 16
	action: tensor([[ -5294.7665, -86813.7647,  12531.4894,  27790.2417,  20002.8161,
           6156.9593,  -7690.5882]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2030768871116151, distance: 1.2551724115876524 entropy 11.706406437789237
epoch: 76, step: 17
	action: tensor([[-16335.6325,  -3543.4265,  61377.1781,  29900.3010, -32123.9517,
          44695.0580,  45501.1213]], dtype=torch.float64)
	q_value: tensor([[-25.8141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38498515284580637, distance: 1.3467260230097444 entropy 12.356972509142079
epoch: 76, step: 18
	action: tensor([[-27452.7472, -13041.8310, -32059.0292,  -7988.8380,  29413.6721,
          11053.7396, -33410.9940]], dtype=torch.float64)
	q_value: tensor([[-26.8877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6608876958306564, distance: 1.4747786002490428 entropy 12.332337366330814
epoch: 76, step: 19
	action: tensor([[ -4481.5645,  15793.7053, -38016.7328,  73015.0956,  53430.6328,
         -19881.8071,   7045.4173]], dtype=torch.float64)
	q_value: tensor([[-27.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1947391820400337, distance: 1.2508154779256664 entropy 12.333236088739698
epoch: 76, step: 20
	action: tensor([[ 29571.9279,  16458.1275, -40391.5962,  68795.5800, -66272.7272,
          59418.1982,  -2380.1649]], dtype=torch.float64)
	q_value: tensor([[-27.4992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.28020402659809
epoch: 76, step: 21
	action: tensor([[ 10661.0381,  24837.1559, -21256.4849,   6589.4364,  24410.5176,
          -8301.6545,    458.7866]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 22
	action: tensor([[-21966.0245,  -7145.1103, -46017.8246, -31445.2415,  52449.6110,
           4676.3263, -46757.2941]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07781124274865947, distance: 1.1880317519935282 entropy 11.706406437789237
epoch: 76, step: 23
	action: tensor([[-31952.5947, -66908.2701,    965.2859, -42360.0854, -37678.4455,
          59471.2210,  35593.8214]], dtype=torch.float64)
	q_value: tensor([[-24.8334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3401563515298123, distance: 1.3247515238991798 entropy 12.191316004649028
epoch: 76, step: 24
	action: tensor([[-70908.3418,  14347.1598, -10088.3785,  24359.5397,  37476.7068,
          92489.6457, -32709.3542]], dtype=torch.float64)
	q_value: tensor([[-26.2756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4579545574915129, distance: 0.8425089864721648 entropy 12.190196719958143
epoch: 76, step: 25
	action: tensor([[ 17517.8841, -58532.3366, -57463.3303,  27036.6607,   9969.4015,
           9424.0726,  35267.6415]], dtype=torch.float64)
	q_value: tensor([[-30.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11136251464787483, distance: 1.0787455267389412 entropy 12.289349181122187
epoch: 76, step: 26
	action: tensor([[-65109.9410,  26883.3906, -67792.6419,  32110.3309, -17601.8411,
         -72175.6600, -21140.8550]], dtype=torch.float64)
	q_value: tensor([[-26.9101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09112691823019159, distance: 1.090958709577043 entropy 12.214166034288294
epoch: 76, step: 27
	action: tensor([[116706.1185, -42126.0210,  91355.6550, -23775.9159, -38225.5398,
          37881.6020,  56318.8660]], dtype=torch.float64)
	q_value: tensor([[-28.8170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33072374433081564, distance: 0.93618009304208 entropy 12.390203260695444
epoch: 76, step: 28
	action: tensor([[ 16861.4345, -32882.2038, -12614.2995,  30435.8251, -29153.0004,
         108277.5730,  65200.3159]], dtype=torch.float64)
	q_value: tensor([[-25.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34815593456736027, distance: 0.9239076247663367 entropy 12.136829686170184
epoch: 76, step: 29
	action: tensor([[ 17523.9047,  -3412.0650,  37428.9692,  40204.6104, 104700.2646,
          13192.4065, -30693.4075]], dtype=torch.float64)
	q_value: tensor([[-31.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008358447166636696, distance: 1.1491167725352 entropy 12.153853480226568
epoch: 76, step: 30
	action: tensor([[-61596.6689, -35345.6111,  33181.6688,  24370.8568,  31280.9527,
         -41667.3505, -30903.2203]], dtype=torch.float64)
	q_value: tensor([[-27.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2256831513943648, distance: 1.0069688773979648 entropy 12.219080147828171
epoch: 76, step: 31
	action: tensor([[-57030.6842,  39808.8304,  57187.6575,  52514.6674, -15516.2491,
         -31610.3121,  58895.3665]], dtype=torch.float64)
	q_value: tensor([[-24.7261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1880668284157201, distance: 1.0311381038402174 entropy 12.3915227098321
epoch: 76, step: 32
	action: tensor([[ -90206.6146, -150111.1683,  -69577.3750,  164981.8408,  -21118.6362,
          109133.1208,   86888.3129]], dtype=torch.float64)
	q_value: tensor([[-27.5043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22082261101719824, distance: 1.0101244048210813 entropy 12.489104849693417
epoch: 76, step: 33
	action: tensor([[  2027.2965,  21726.3553,  10423.8010, -15266.1238,  38142.0405,
          78470.1187,  17990.8856]], dtype=torch.float64)
	q_value: tensor([[-24.2656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.371518542024576
epoch: 76, step: 34
	action: tensor([[-29044.1044, -27494.7214,  -1198.7049, -12781.8748, -30335.2090,
          -2619.8572,  34778.6373]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10686230533111485, distance: 1.0814735514736509 entropy 11.706406437789237
epoch: 76, step: 35
	action: tensor([[ 27762.9940,  86618.1356, -25389.1488,   8923.5508,  52454.3308,
           6185.2232,  16738.6922]], dtype=torch.float64)
	q_value: tensor([[-23.3362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.096020276257935
epoch: 76, step: 36
	action: tensor([[-21923.1601, -10103.4075, -30154.3026,   8968.4177,  18478.7384,
         -20193.5545,  25721.6775]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8854025335928069, distance: 1.5712986466273946 entropy 11.706406437789237
epoch: 76, step: 37
	action: tensor([[-43202.1682,   3944.5511,   9559.4574, -28357.2172,  52722.2461,
          33401.3939,  28503.7248]], dtype=torch.float64)
	q_value: tensor([[-23.1045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21927503335901877, distance: 1.0111270458983006 entropy 12.181052473605543
epoch: 76, step: 38
	action: tensor([[ -5398.9519, -55468.4492, -86407.0796,   9909.2014, -25608.0143,
         -46059.5745,  38537.2470]], dtype=torch.float64)
	q_value: tensor([[-34.0623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11072844656360625, distance: 1.2060370245714493 entropy 12.303443191813193
epoch: 76, step: 39
	action: tensor([[ -44682.8504,   19619.7066,  123411.8457,   -2309.3246,   33544.5606,
         -100274.1931,    6171.6858]], dtype=torch.float64)
	q_value: tensor([[-25.8951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49029871039338546, distance: 0.8169858965655744 entropy 12.359803442432517
epoch: 76, step: 40
	action: tensor([[ 14569.4066,   5958.8367, -56515.1547,   7308.0760,  28145.8951,
          14300.8459,   3100.1385]], dtype=torch.float64)
	q_value: tensor([[-32.0346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3733113377595617, distance: 0.90590490706002 entropy 12.37429549381026
epoch: 76, step: 41
	action: tensor([[-44547.1879,  43313.6479,  20825.2221, -35791.1634,  20164.0005,
           2832.2671,   1626.0398]], dtype=torch.float64)
	q_value: tensor([[-21.4643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5464009067223783, distance: 0.770713173269891 entropy 11.995104691441911
epoch: 76, step: 42
	action: tensor([[-23290.9731, -48094.6575,  23624.0557,   6146.9925,  64903.2161,
         -19941.8301,  38090.3335]], dtype=torch.float64)
	q_value: tensor([[-34.8196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7304106115001734, distance: 1.505328479268584 entropy 12.24702116424123
epoch: 76, step: 43
	action: tensor([[-2.7116e+04,  1.7264e+02,  4.1229e+04, -5.8133e+01, -7.6139e+04,
          7.2129e+04,  3.6196e+03]], dtype=torch.float64)
	q_value: tensor([[-22.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12867108850067743, distance: 1.215739125297933 entropy 12.226600400721512
epoch: 76, step: 44
	action: tensor([[-21158.7394, -57107.3164, -21575.3679, -35705.6763,  29257.3936,
          58967.0447, 115352.0395]], dtype=torch.float64)
	q_value: tensor([[-27.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1385955794347168, distance: 1.0620873625769551 entropy 11.964227545675632
epoch: 76, step: 45
	action: tensor([[ -38201.2937,    7818.9561,    -517.9305,   26107.6702, -116595.1304,
           43590.5109,   14889.8992]], dtype=torch.float64)
	q_value: tensor([[-27.1831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0582693186648402, distance: 1.1772123128871106 entropy 12.211302295944318
epoch: 76, step: 46
	action: tensor([[-28802.9478,  52351.2102,  51600.7454, -24455.3889, -38230.4957,
          67038.5660,  29472.8925]], dtype=torch.float64)
	q_value: tensor([[-25.6470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1413191109783145, distance: 1.060407014521527 entropy 12.009324183392641
epoch: 76, step: 47
	action: tensor([[ -64219.9364, -151429.6743,   20640.5092,  -18035.3209,   23329.7438,
          -65996.7461,  -69116.3114]], dtype=torch.float64)
	q_value: tensor([[-37.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11019494444980438, distance: 1.0794539695553182 entropy 12.366656025352617
epoch: 76, step: 48
	action: tensor([[ -4961.7134,  25957.1003,  19362.4340, -46404.4273,  -5110.5749,
          31003.0263, -13065.5127]], dtype=torch.float64)
	q_value: tensor([[-21.8948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5021189644101953, distance: 0.8074571515349591 entropy 12.045771603890586
epoch: 76, step: 49
	action: tensor([[ 22208.3021,  -3447.4569,  -9871.5561,   3808.2309, -34801.6906,
           5328.6734, -39795.4979]], dtype=torch.float64)
	q_value: tensor([[-26.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1658760403727091, distance: 1.2356141750887208 entropy 12.190888280540394
epoch: 76, step: 50
	action: tensor([[ -52413.9204,  -68783.7531,  -48165.4834,  -27067.2485,  -26148.3086,
         -120114.5330,   14832.8886]], dtype=torch.float64)
	q_value: tensor([[-24.6622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8229560305068311, distance: 1.545058010770075 entropy 12.009355582044554
epoch: 76, step: 51
	action: tensor([[ 15191.4379, -46019.5636, -29466.8738, -43954.5564,  -2826.3590,
          -2196.1121,  72557.1149]], dtype=torch.float64)
	q_value: tensor([[-26.6793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5948398279127749, distance: 0.7284002173799914 entropy 12.363156050574176
epoch: 76, step: 52
	action: tensor([[-101362.5820,    2863.7404,  -62049.2851,    7034.4296,  -81748.4435,
          -26079.1887,    7871.9599]], dtype=torch.float64)
	q_value: tensor([[-23.3227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.961704623336002
epoch: 76, step: 53
	action: tensor([[-26186.3093,  35836.1396, -34733.9164, -21954.1754,  70853.3736,
          -7155.2478,   9748.2433]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6745705442061818, distance: 0.6528074818515016 entropy 11.706406437789237
epoch: 76, step: 54
	action: tensor([[-18584.2197, -24274.5057,  33212.7261,  12668.7656, -23567.7977,
          22382.1870, -18151.1041]], dtype=torch.float64)
	q_value: tensor([[-30.5822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.24633514778509
epoch: 76, step: 55
	action: tensor([[-52566.0142, -13430.1426,  -7073.2526,  18670.9810,  34913.9820,
         -52949.3231,  -8276.6257]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 56
	action: tensor([[-21538.0920, -19118.1461,  35606.7477,  25950.0231,  37998.6158,
          -4075.3236,  10835.6079]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 57
	action: tensor([[-30554.3181,   8106.1185,  25578.2031,  21594.9598,  32397.4379,
          16719.0553, -26400.9797]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6290402836863536, distance: 0.696979602888072 entropy 11.706406437789237
epoch: 76, step: 58
	action: tensor([[  -6443.5191,  -11056.1598,   40980.7709,  -61036.3846,   -3284.5402,
         -109245.3996,  -47543.1549]], dtype=torch.float64)
	q_value: tensor([[-26.0073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4423013053033831, distance: 1.374309952374314 entropy 11.92762995811557
epoch: 76, step: 59
	action: tensor([[ 63537.0732, -13457.7663,  48903.6737,  10214.8654,   2027.2803,
           2483.7711,  -2070.1475]], dtype=torch.float64)
	q_value: tensor([[-23.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12459401853388807, distance: 1.0706843337724208 entropy 12.092145732199933
epoch: 76, step: 60
	action: tensor([[-71175.3441, -28367.8382,   3600.7467,  48131.2606, -10131.2877,
          -7738.3241, -13181.5284]], dtype=torch.float64)
	q_value: tensor([[-21.4678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23671113973069358, distance: 0.9997724276894591 entropy 12.028870001036779
epoch: 76, step: 61
	action: tensor([[ 23205.7936, -98918.1164,  86918.7148,  75088.0855, -30841.9273,
           3325.3731, -88833.4937]], dtype=torch.float64)
	q_value: tensor([[-23.8502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.295900710141735
epoch: 76, step: 62
	action: tensor([[  2147.8286,  16894.2947, -19551.3565, -24591.9335, -12344.6847,
          -8388.1964, -65027.2771]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5043773198257333, distance: 0.8056237840725902 entropy 11.706406437789237
epoch: 76, step: 63
	action: tensor([[-52898.7648, -41612.3710,  44174.4789,  20624.2752,  72750.4500,
         -15103.1764, -14744.3126]], dtype=torch.float64)
	q_value: tensor([[-28.2708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.112694606324343
epoch: 76, step: 64
	action: tensor([[ 19896.7705, -15315.3425, -53489.2811,  10014.8891,  15215.0845,
         -20581.2844, -25827.4781]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3927782743900884, distance: 0.8917237713307272 entropy 11.706406437789237
epoch: 76, step: 65
	action: tensor([[ 22388.8434, -33484.7070, -13802.8110, -14605.1688, -44542.0539,
          16587.2335, -74260.2683]], dtype=torch.float64)
	q_value: tensor([[-23.6033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017972575111207423, distance: 1.1340142227135843 entropy 12.064582108058252
epoch: 76, step: 66
	action: tensor([[ 13332.9514, -37650.8132,  11176.8309, -29434.7263,  25733.3413,
          15070.7623, -24133.4132]], dtype=torch.float64)
	q_value: tensor([[-23.4878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35762160325625514, distance: 0.9171748911404966 entropy 11.849394803780113
epoch: 76, step: 67
	action: tensor([[  9446.3083, -54608.7249, -49295.6626,  59879.1435, -28112.3016,
         -25508.3076,  -6029.7836]], dtype=torch.float64)
	q_value: tensor([[-25.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4577330222697402, distance: 1.381642517223651 entropy 12.101116964501639
epoch: 76, step: 68
	action: tensor([[-69564.2316, -34447.5080,   3313.5855,  12949.3997,  23451.3110,
          45416.8539,  55467.4216]], dtype=torch.float64)
	q_value: tensor([[-23.1125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039034971840427146, distance: 1.1217872116010186 entropy 11.901692428290144
epoch: 76, step: 69
	action: tensor([[-104363.0380,   11047.7167,  -14029.3920,    8607.2370,   -5682.6430,
           38777.0444,   77311.7893]], dtype=torch.float64)
	q_value: tensor([[-24.9158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6883278912407933, distance: 0.6388599497420853 entropy 12.165293293620765
epoch: 76, step: 70
	action: tensor([[ -42419.3774,  -69769.8166, -102672.4750,  116229.0700,   37409.9034,
          -23054.2267,   36539.5235]], dtype=torch.float64)
	q_value: tensor([[-25.0984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.303639651283422
epoch: 76, step: 71
	action: tensor([[-26111.9234, -11933.6440,  54796.1196,  -7093.5450,   8191.3293,
          80751.1784,  24386.9367]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 72
	action: tensor([[-67611.7130,  27419.6304, -25497.0540,    399.3983,  18511.9249,
          12918.1202,  19377.6115]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.046036392691997996, distance: 1.17038862018331 entropy 11.706406437789237
epoch: 76, step: 73
	action: tensor([[ 16474.3621, -13500.8177,  16873.2860, -18030.0150,   6085.6177,
         -15463.1066,  46079.0312]], dtype=torch.float64)
	q_value: tensor([[-24.9794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27540467400347524, distance: 0.974102050212511 entropy 11.73858154640802
epoch: 76, step: 74
	action: tensor([[ -8015.1846, -75590.1167, -46842.5715,  57115.5342, -28700.9017,
          49819.4279,  -2691.7439]], dtype=torch.float64)
	q_value: tensor([[-22.7265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7466343287109387, distance: 1.5123687294602794 entropy 12.23498678154672
epoch: 76, step: 75
	action: tensor([[-93690.9820, -10585.3910,  62101.6799,  51425.8718,  57563.0526,
          39014.4810,  79479.4044]], dtype=torch.float64)
	q_value: tensor([[-22.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8210915943382258, distance: 1.5442677012620722 entropy 12.202321744172293
epoch: 76, step: 76
	action: tensor([[-105956.9904,  -31889.1788,  -68505.3280,   -6776.1912,   42233.6781,
          144985.9228,  -11625.4584]], dtype=torch.float64)
	q_value: tensor([[-27.8261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.612335470683939, distance: 1.4530628433608004 entropy 12.43848863529353
epoch: 76, step: 77
	action: tensor([[  -799.3536,   8265.4717,  61119.1528,  21320.5976, -14055.9764,
         -27200.3789,  63688.0020]], dtype=torch.float64)
	q_value: tensor([[-21.5742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5138514634792508, distance: 0.7978866242349285 entropy 12.080382713266003
epoch: 76, step: 78
	action: tensor([[-17580.6523, -40054.8778, -17421.2111,  20626.7076,  59131.6317,
           2515.8032, -65079.3628]], dtype=torch.float64)
	q_value: tensor([[-25.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10864095307643107, distance: 1.080396159301358 entropy 12.158255974085337
epoch: 76, step: 79
	action: tensor([[-175658.1149,  -33066.9537,   -9010.2152,   44682.3150,    7569.4428,
          -52927.6188,  -39164.8951]], dtype=torch.float64)
	q_value: tensor([[-24.8599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.038790300861252014, distance: 1.1663278238773944 entropy 12.199156079530443
epoch: 76, step: 80
	action: tensor([[-95299.6638, -41704.2971,  -6940.4155,  66464.1894, -88299.7207,
          16111.4245, 167190.0080]], dtype=torch.float64)
	q_value: tensor([[-23.8479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3744131014624066, distance: 1.341576173182067 entropy 12.31317007290647
epoch: 76, step: 81
	action: tensor([[ 88079.8464, -18160.4291,  18238.6895,  17234.5177,  43315.3256,
         -50557.3054,  51723.8924]], dtype=torch.float64)
	q_value: tensor([[-24.3443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4037440274362648, distance: 0.8836353150865708 entropy 12.289720195787543
epoch: 76, step: 82
	action: tensor([[ 28928.7534, -42683.2643, -80011.2778,  52320.6776,  30431.5369,
         -34231.3075, -37155.7135]], dtype=torch.float64)
	q_value: tensor([[-23.3652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14628312888664496, distance: 1.0573374754507376 entropy 11.986831179080157
epoch: 76, step: 83
	action: tensor([[  -1072.5222, -116293.3869,  -33287.6699,   17971.2328,   22107.6260,
           84179.8781, -108449.0026]], dtype=torch.float64)
	q_value: tensor([[-29.1592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2721101929119878, distance: 1.2906814238195339 entropy 12.280817322288973
epoch: 76, step: 84
	action: tensor([[-12749.3588, -16016.0866, -38408.6546, -41999.9203,  67874.6761,
           8121.8779,   2415.4802]], dtype=torch.float64)
	q_value: tensor([[-23.7538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33353119135822284, distance: 1.3214729642854708 entropy 12.320015919523
epoch: 76, step: 85
	action: tensor([[-131334.8121, -101491.2112,   62005.1512,  -49221.9617,   47497.8624,
           49495.2916,   35813.9061]], dtype=torch.float64)
	q_value: tensor([[-24.2029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1189053109056224, distance: 1.0741575491774686 entropy 12.232845420440198
epoch: 76, step: 86
	action: tensor([[-22839.1398,   1853.5164, -18149.3307, -17114.7698,  61055.0521,
         -34725.5875,  20581.7417]], dtype=torch.float64)
	q_value: tensor([[-25.5794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5475905643975705, distance: 0.769701832170055 entropy 12.282830609079781
epoch: 76, step: 87
	action: tensor([[-70986.1513, -46134.9586,  45935.3282,  94132.0662,   1578.4383,
           1113.8691,  57608.0129]], dtype=torch.float64)
	q_value: tensor([[-27.4926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1278312119143084, distance: 1.6692649741990893 entropy 12.137184277667803
epoch: 76, step: 88
	action: tensor([[ 93527.1376, -75343.3749,  58935.4405, -15520.1070, -66254.9226,
           -849.7220,  53870.0062]], dtype=torch.float64)
	q_value: tensor([[-23.1202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20605126172016663, distance: 1.2567230391245683 entropy 12.275877513734574
epoch: 76, step: 89
	action: tensor([[-79401.1446,  11026.2643,  34389.0351, -61500.6232,   4269.6964,
          42065.1726, 116412.5135]], dtype=torch.float64)
	q_value: tensor([[-21.1737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17068458353377902, distance: 1.042117196897242 entropy 12.003551860694397
epoch: 76, step: 90
	action: tensor([[ 63937.0255, -16737.0024,  36366.5197,   4142.6520,  89257.2237,
          20462.6729,   8217.0229]], dtype=torch.float64)
	q_value: tensor([[-28.9292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08178860597745052, distance: 1.0965489719498083 entropy 12.19231920594583
epoch: 76, step: 91
	action: tensor([[-70220.4872,   2630.1048, -15628.2002,  35366.7378,  36269.1713,
          11008.6957, -41927.5888]], dtype=torch.float64)
	q_value: tensor([[-23.0744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.978624329150247
epoch: 76, step: 92
	action: tensor([[  1984.7652, -59706.0769, -46282.9772, -17661.0877,   7923.0982,
          11700.6638,    879.1845]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 93
	action: tensor([[-43025.3088, -32415.8578,  12169.1003, -10056.4436, -23032.5357,
           8752.9695,  19837.5546]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32842308454988234, distance: 1.3189395771011325 entropy 11.706406437789237
epoch: 76, step: 94
	action: tensor([[-64119.0651, -13658.2893, -12912.5467,  46156.8090, -33062.0233,
          -4158.6055,  48845.2391]], dtype=torch.float64)
	q_value: tensor([[-26.5278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7629212084085457, distance: 1.5194035770589487 entropy 12.188221568707048
epoch: 76, step: 95
	action: tensor([[-42501.0862, -20759.2900, -61525.2803,  52000.7717, -57701.4944,
         -17860.4298,  -6749.9596]], dtype=torch.float64)
	q_value: tensor([[-20.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058956843281424254, distance: 1.1100983667601678 entropy 11.916334820289586
epoch: 76, step: 96
	action: tensor([[ -10467.2866, -117323.1958,  -61241.0432,   73883.8327,   49509.6431,
            1981.6313,   -3214.9727]], dtype=torch.float64)
	q_value: tensor([[-24.2961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5387888380761743, distance: 1.4195353334912642 entropy 12.328287628733202
epoch: 76, step: 97
	action: tensor([[ 10984.7036,  -7092.6372,   1308.5461, -21091.1335,  33525.0205,
         -12646.7707,  24982.9725]], dtype=torch.float64)
	q_value: tensor([[-24.9028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37876237888755115, distance: 1.3436971796605153 entropy 12.299989595088993
epoch: 76, step: 98
	action: tensor([[ -3226.1712,  -8265.5269,  -6128.1934,  82562.2191, -75647.7817,
         -72983.2946, -32466.2266]], dtype=torch.float64)
	q_value: tensor([[-21.6914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13145413656878246, distance: 1.0664808741569562 entropy 12.020884017294177
epoch: 76, step: 99
	action: tensor([[-33872.2938,  51454.5479,  15039.8266,  12903.3577,  28533.5325,
         -79127.4452,  13046.4938]], dtype=torch.float64)
	q_value: tensor([[-25.3961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1250273792326404, distance: 1.2137751426341952 entropy 12.332746632730435
epoch: 76, step: 100
	action: tensor([[ 20571.8302,  24753.6178,  21168.6859, 117973.3565,  23886.5968,
         -42718.7926,  28235.2521]], dtype=torch.float64)
	q_value: tensor([[-27.8208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42256315252067433, distance: 0.8695787936743933 entropy 12.234910688406504
epoch: 76, step: 101
	action: tensor([[-37523.9326, -30414.1513, -39554.1518,  -7754.3601, -42278.2519,
          56771.3163, -48886.8161]], dtype=torch.float64)
	q_value: tensor([[-23.7075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.076230126875588
epoch: 76, step: 102
	action: tensor([[ -8017.5406,  47811.8308,   2157.2837, -14940.1079,   3521.0241,
          -3892.3646, -11452.0404]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5704561140335103, distance: 0.7499986197624235 entropy 11.706406437789237
epoch: 76, step: 103
	action: tensor([[ 23574.8908, -15496.7744,  83364.4297,  48875.0711, -51240.5183,
           8640.3917,  81987.2830]], dtype=torch.float64)
	q_value: tensor([[-32.4953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.248724822392647
epoch: 76, step: 104
	action: tensor([[ 19576.3471,  20053.7451, -38102.7245,   1225.8926,   6015.8904,
         -32831.1164, -30565.7283]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 105
	action: tensor([[-44948.5924, -29630.1012,  12365.3368, -13952.8649,  -7638.3435,
          13363.1898,  -2789.9241]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1317901683327397, distance: 1.0662745487723364 entropy 11.706406437789237
epoch: 76, step: 106
	action: tensor([[ 71811.2521, -84554.7021,  21861.0062,  41124.3424,  24178.9882,
          49515.1976,   3045.9015]], dtype=torch.float64)
	q_value: tensor([[-25.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29459446220555696, distance: 1.302037719469454 entropy 12.11150860869694
epoch: 76, step: 107
	action: tensor([[ -25209.4263, -141309.7123,   25572.1781,  -37043.5377,   40859.4745,
          -26503.1032,  -86952.5200]], dtype=torch.float64)
	q_value: tensor([[-24.7562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30575799417438443, distance: 0.9534812058533294 entropy 11.965455620473374
epoch: 76, step: 108
	action: tensor([[-44483.7775,  14255.1216,   8673.5164,   3746.3100,  38480.3088,
         -15521.5993,  29229.2829]], dtype=torch.float64)
	q_value: tensor([[-18.8809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6629924852894691, distance: 0.6643187124734984 entropy 11.853591592876894
epoch: 76, step: 109
	action: tensor([[-33840.5810, -28428.0881, -27552.2717,  15030.0898, -49565.8647,
         -13280.8862,  20266.4829]], dtype=torch.float64)
	q_value: tensor([[-25.8043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2651007926974256, distance: 1.2871206476421515 entropy 11.965023458857841
epoch: 76, step: 110
	action: tensor([[  31066.3756,  -47082.6128, -132088.4060,    2288.2822,  -39388.6871,
           75581.1326,   74743.1953]], dtype=torch.float64)
	q_value: tensor([[-25.7596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20473833126004992, distance: 1.020496967122982 entropy 12.300932187715533
epoch: 76, step: 111
	action: tensor([[ 31069.2795, -42274.8622, -38659.0841, -74304.5839,  28677.1365,
          36283.2597,  -8717.9139]], dtype=torch.float64)
	q_value: tensor([[-25.4224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3775726257271077, distance: 1.3431173070814073 entropy 12.041683949853029
epoch: 76, step: 112
	action: tensor([[-74003.0776,  -7237.2149, -36054.1651,  53033.6996,  38272.2553,
         -42691.2874, 107857.7502]], dtype=torch.float64)
	q_value: tensor([[-26.8507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5109628687393999, distance: 1.4066420279211367 entropy 12.095493409384115
epoch: 76, step: 113
	action: tensor([[  1868.8547, -35003.7469,  27362.6999,  38404.8754, -48974.4751,
          64751.6216, -36719.2017]], dtype=torch.float64)
	q_value: tensor([[-24.9099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41972637369715904, distance: 0.8717121702216598 entropy 12.134362438954678
epoch: 76, step: 114
	action: tensor([[-31921.4680, -46461.3398, -47582.6662,  13048.3815,  32414.3584,
         -16551.2009, -11923.7927]], dtype=torch.float64)
	q_value: tensor([[-26.5951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.14328007603992
epoch: 76, step: 115
	action: tensor([[-15084.0074, -34267.2453,  30071.3991,   8884.2679,  -1544.1851,
           8494.5989, -21179.5450]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 116
	action: tensor([[-53929.8121, -44962.1057,  24167.9853,     55.7540,  17298.8587,
          41999.8366,  33539.0105]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 117
	action: tensor([[-45011.0067, -16800.5321,  24206.6723,  36675.7957, -17288.5798,
          57580.7727,   5140.3520]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.706406437789237
epoch: 76, step: 118
	action: tensor([[  6798.2599, -67545.7778, -13704.4804,  43473.6752, -18307.7324,
          21803.6967,  14998.5842]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5865158868260318, distance: 0.7358446000387784 entropy 11.706406437789237
epoch: 76, step: 119
	action: tensor([[ 19332.9128,   5677.9638,  17492.9753, -36207.3172, -29254.2840,
          38352.0781, -20968.7477]], dtype=torch.float64)
	q_value: tensor([[-23.7981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5468675303158651, distance: 0.7703166496731426 entropy 12.008563756045914
epoch: 76, step: 120
	action: tensor([[ 10002.4638, -14721.3925,  -6435.8072,    560.5378,  52648.0298,
          48538.7095,  -4583.8391]], dtype=torch.float64)
	q_value: tensor([[-25.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4875920940461964, distance: 0.8191522041931344 entropy 11.712758635855993
epoch: 76, step: 121
	action: tensor([[ -8230.1681, -20278.8305,  39063.5958, -60251.2908,  51932.9507,
          -9618.4529, -36374.8949]], dtype=torch.float64)
	q_value: tensor([[-26.3630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6911522130828296, distance: 1.4881545701076033 entropy 12.149355378910457
epoch: 76, step: 122
	action: tensor([[ 25656.7724, -48684.2146,   5700.0474, -45114.8609, -19808.6407,
         -14790.4540,  19829.0485]], dtype=torch.float64)
	q_value: tensor([[-23.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40860063926506585, distance: 0.8800292730520897 entropy 12.135918671862575
epoch: 76, step: 123
	action: tensor([[  53491.9206, -126778.0089,   30698.0000,  -34140.0431,   84490.0672,
           29370.9985,  -30276.9575]], dtype=torch.float64)
	q_value: tensor([[-26.9460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.509886329885435, distance: 1.4061408327382083 entropy 12.065630411821866
epoch: 76, step: 124
	action: tensor([[-42271.4585, -19210.4868,  15243.5633,  15872.6999,  -2303.4883,
           2016.6888, -24717.4376]], dtype=torch.float64)
	q_value: tensor([[-24.7027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.328161234591251, distance: 1.3188095803785616 entropy 11.973231151580139
epoch: 76, step: 125
	action: tensor([[-41073.7738,  52572.7534, -37169.6122,  12160.5884,  22204.2112,
          40904.6030,  13950.8633]], dtype=torch.float64)
	q_value: tensor([[-22.4559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18451118606293393, distance: 1.033393433003957 entropy 12.019481821780124
epoch: 76, step: 126
	action: tensor([[ 10925.1544, -81007.0184,  42724.3357, -92402.8468,  25176.0109,
         -25042.1508,   5538.1111]], dtype=torch.float64)
	q_value: tensor([[-25.3198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.131562187049013
epoch: 76, step: 127
	action: tensor([[ -6078.8351, -34943.1475, -28762.2936,  13651.4536,  13677.0191,
         -57273.8182, -51621.9189]], dtype=torch.float64)
	q_value: tensor([[-24.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44245238086573546, distance: 1.3743819273532765 entropy 11.706406437789237
LOSS epoch 76 actor 261.01351932386854 critic 368.9379002413315
epoch: 77, step: 0
	action: tensor([[-15646.5210, -27378.7424, -42199.9723,  -5041.7119,   3006.5310,
          79387.7817,  54963.3051]], dtype=torch.float64)
	q_value: tensor([[-27.9223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5360096176767399, distance: 1.4182528363625455 entropy 12.39918695484022
epoch: 77, step: 1
	action: tensor([[ 42381.1799, -95066.1561,  52732.4306,  26781.2913, -48528.0956,
          32690.0688,  53331.7571]], dtype=torch.float64)
	q_value: tensor([[-27.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18652826819787371, distance: 1.2465099169100218 entropy 12.426223058143725
epoch: 77, step: 2
	action: tensor([[-27656.1174,  53430.5955,  -6828.7112, -34198.1220, -65819.7108,
         -42996.4255, -54187.8237]], dtype=torch.float64)
	q_value: tensor([[-26.9988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.346055482407802
epoch: 77, step: 3
	action: tensor([[ 49144.5082,  30793.3288,  15373.9661,  -5610.3739,  21788.8493,
          18994.8052, -21357.7977]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.74048627719508
epoch: 77, step: 4
	action: tensor([[  4910.5032, -35290.0057, -41128.7395,   4842.7262,  -7195.5220,
          35091.4364,  36364.9399]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1472378041134318, distance: 1.0567461223438883 entropy 11.74048627719508
epoch: 77, step: 5
	action: tensor([[  -7559.3354,  -71966.5496, -103032.3191,   -7138.8943,   32920.6030,
             685.9862,  -70614.1377]], dtype=torch.float64)
	q_value: tensor([[-25.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1430377424225321, distance: 1.059345292015558 entropy 12.107627274612039
epoch: 77, step: 6
	action: tensor([[-94374.1639,  -6589.7295, -33486.6512,  -1437.7104,  19503.7421,
         -65822.3486,    941.1480]], dtype=torch.float64)
	q_value: tensor([[-28.2426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22834749411491706, distance: 1.0052349476215485 entropy 12.349096806122818
epoch: 77, step: 7
	action: tensor([[ -98731.6290,   -2161.3071,    5681.0016, -100193.3455,   80079.2928,
            1621.1540,  -23112.4087]], dtype=torch.float64)
	q_value: tensor([[-29.2522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8945717901321915, distance: 1.575114851780619 entropy 12.308883597852203
epoch: 77, step: 8
	action: tensor([[-17634.6936,  -8336.2863,  28896.7613,  61100.3954, -66342.1973,
         -15602.0846, -62741.0476]], dtype=torch.float64)
	q_value: tensor([[-27.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7897210423845566, distance: 1.530908961907184 entropy 12.21901514224892
epoch: 77, step: 9
	action: tensor([[-6.2505e+04, -4.3116e+04,  4.7954e+04, -1.2609e+05, -1.6483e+01,
          8.1717e+04, -5.8076e+04]], dtype=torch.float64)
	q_value: tensor([[-28.7612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.279068268366539, distance: 0.9716363724768928 entropy 12.275638668468114
epoch: 77, step: 10
	action: tensor([[-36420.9455, -22420.6612,  74733.5203, -10756.7924,  54282.6301,
          29795.1479, 105844.9287]], dtype=torch.float64)
	q_value: tensor([[-27.3802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1203969665981589, distance: 1.0732479125842422 entropy 12.180950495555749
epoch: 77, step: 11
	action: tensor([[21735.2765, 31941.2055, 28938.3665, 33412.5479,  9079.2474, -2074.5817,
         41462.5335]], dtype=torch.float64)
	q_value: tensor([[-28.7083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.224328075181903
epoch: 77, step: 12
	action: tensor([[-18581.4512, -32605.1275, -19052.0510,   5942.4677,   -211.2738,
         -55293.8169,  31629.1588]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34544798737687765, distance: 1.327364352017152 entropy 11.74048627719508
epoch: 77, step: 13
	action: tensor([[-64846.7653,  20190.6286, -91289.4967,  17699.2733,  14214.6179,
         -29624.0090,   2235.0648]], dtype=torch.float64)
	q_value: tensor([[-24.9266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13708425737943686, distance: 1.2202618020006744 entropy 12.184419813900307
epoch: 77, step: 14
	action: tensor([[ 46341.4833,  18445.9773, -45149.6905,  48590.4632,  29651.6731,
         -51583.6535, -37512.5040]], dtype=torch.float64)
	q_value: tensor([[-28.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2331546928529925, distance: 1.0020988768066155 entropy 12.20513648898689
epoch: 77, step: 15
	action: tensor([[-22585.4065, -14369.0434,  34487.5583,  23483.9133,  20912.1348,
         -12455.0838,  67166.9072]], dtype=torch.float64)
	q_value: tensor([[-27.0354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4226275375769193, distance: 1.3649046042413047 entropy 11.988573124851744
epoch: 77, step: 16
	action: tensor([[   879.3804, -15828.1935, -48687.5349, -18810.5761, -69263.3680,
          12174.3786,   8905.4124]], dtype=torch.float64)
	q_value: tensor([[-25.0757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.345364977283374, distance: 1.3273234041917528 entropy 12.150411571743579
epoch: 77, step: 17
	action: tensor([[  1539.6230, -31948.0953, -34739.2003, -20276.8807, -31650.9255,
         -15340.3283, -16564.2320]], dtype=torch.float64)
	q_value: tensor([[-26.3192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35173739388036784, distance: 1.3304631713549553 entropy 12.053732914033588
epoch: 77, step: 18
	action: tensor([[-2.8321e+04, -1.7379e+05,  1.1656e+05, -3.9512e+04, -8.8229e+04,
          1.6885e+02,  6.7877e+04]], dtype=torch.float64)
	q_value: tensor([[-27.5820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5643410342278314, distance: 1.4312727801886476 entropy 12.30450715500301
epoch: 77, step: 19
	action: tensor([[ 22241.1155, -29250.3151, -64260.1289,  49708.0925,  30346.3109,
         -34386.7337, -25673.3279]], dtype=torch.float64)
	q_value: tensor([[-23.6372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29584356607879747, distance: 0.9602653743570743 entropy 12.129717339037796
epoch: 77, step: 20
	action: tensor([[ 26787.6667, -27580.8568, -51000.5626,   -775.8733,  -7089.6518,
          33029.4921, -20862.3542]], dtype=torch.float64)
	q_value: tensor([[-23.7133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.042137404955586266, distance: 1.1682053351737183 entropy 12.103651568712873
epoch: 77, step: 21
	action: tensor([[ -3556.0030,  -6901.6310, -38922.2769,  12513.4425,  14651.6491,
          87739.2830,  26781.1761]], dtype=torch.float64)
	q_value: tensor([[-26.6484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1848731863629265, distance: 1.2456402385713616 entropy 12.172980339547207
epoch: 77, step: 22
	action: tensor([[-61394.6498,  21712.3540,   4416.0527,   6872.5167,  23517.5746,
         -39148.2380,  -4833.4700]], dtype=torch.float64)
	q_value: tensor([[-22.6392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39113335850111497, distance: 0.8929307592502858 entropy 12.01686642534555
epoch: 77, step: 23
	action: tensor([[-11045.3581, -83020.0390,   4559.6852,  28296.2294,  18817.0292,
         -26594.6243,  87408.9306]], dtype=torch.float64)
	q_value: tensor([[-27.8066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6873413416498813, distance: 1.4864769079269107 entropy 12.254520265701256
epoch: 77, step: 24
	action: tensor([[  -6216.1381, -118570.5999,  -50525.1242,   18241.1492,   56711.4849,
           48302.8414,  112724.4901]], dtype=torch.float64)
	q_value: tensor([[-26.7871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21886800365051362, distance: 1.0113905862850834 entropy 12.409632403560673
epoch: 77, step: 25
	action: tensor([[-34940.9872, -52374.6421,  53353.7399, -38800.9965,   3340.5674,
          30483.9481, -66995.8685]], dtype=torch.float64)
	q_value: tensor([[-25.9191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45972714816667715, distance: 1.382587212679849 entropy 12.358506449573243
epoch: 77, step: 26
	action: tensor([[-157805.3788, -142105.7124,  -20588.7007,   35694.4726,  -21821.8640,
          -78722.1561,   39680.2776]], dtype=torch.float64)
	q_value: tensor([[-29.7218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25062599706916, distance: 1.2797360949123755 entropy 12.37712814972683
epoch: 77, step: 27
	action: tensor([[-89217.1814,  28052.0929,  17444.5834,  62184.0079, -61674.9303,
          94959.1306,  35926.8945]], dtype=torch.float64)
	q_value: tensor([[-26.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10253824639668796, distance: 1.2015823079689125 entropy 12.207502274430379
epoch: 77, step: 28
	action: tensor([[-45176.3514,  -3612.7558,  -4889.1425, -89715.7129,  16362.3239,
           9017.9713,  43400.2818]], dtype=torch.float64)
	q_value: tensor([[-28.2305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6677806780557987, distance: 1.477835729651303 entropy 12.144771609975825
epoch: 77, step: 29
	action: tensor([[ 41516.1439,  12870.7803,  70906.2531,  59140.4559,  14785.7375,
         -30083.5108,  64422.8401]], dtype=torch.float64)
	q_value: tensor([[-24.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.095142102376155
epoch: 77, step: 30
	action: tensor([[-18770.3001,  18097.1126,  18904.9396, -15804.3223, -37165.5590,
          23423.7867,  22386.3114]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07612101290830842, distance: 1.0999279502478034 entropy 11.74048627719508
epoch: 77, step: 31
	action: tensor([[  9833.3662, -50446.4138,  84896.9285,  37545.0056,  -7889.0036,
         -28859.8333, -39231.2983]], dtype=torch.float64)
	q_value: tensor([[-33.4499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.04264633784357
epoch: 77, step: 32
	action: tensor([[ -8783.9965, -10451.2821, -84893.7021,   3235.6705, -29091.4322,
         -37265.4021,  -6852.4668]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9099672115337698, distance: 1.581501649256179 entropy 11.74048627719508
epoch: 77, step: 33
	action: tensor([[  51087.9010, -132070.1128,   36187.5346,  -23359.1805,  -16335.8151,
           17859.6084,  -32621.6770]], dtype=torch.float64)
	q_value: tensor([[-25.8060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.051810919412355494, distance: 1.1736146736748831 entropy 12.203242625653987
epoch: 77, step: 34
	action: tensor([[-75858.1673,  -4216.4752, -57372.4949, -36558.5453,  48860.1747,
         -68086.5728,  -3934.0263]], dtype=torch.float64)
	q_value: tensor([[-27.4264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015065497499659686, distance: 1.1356914829983913 entropy 12.037185723191712
epoch: 77, step: 35
	action: tensor([[-40970.6399, -25933.9987, -45072.0824,  79566.6852, -36773.5889,
           5271.5927, -20223.1525]], dtype=torch.float64)
	q_value: tensor([[-28.8934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0837176466764729, distance: 1.6518709973106684 entropy 12.254018217251266
epoch: 77, step: 36
	action: tensor([[-13114.8874, -80797.5179,  -3350.0937,  72825.5616, -72179.3757,
          40621.4595,  26562.2296]], dtype=torch.float64)
	q_value: tensor([[-26.4921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5565331342031676, distance: 1.4276964455443169 entropy 12.274385395276076
epoch: 77, step: 37
	action: tensor([[ -80265.1915, -100662.1914,  -81760.7981,   12610.7160, -135108.2899,
           34647.0484,   61829.4081]], dtype=torch.float64)
	q_value: tensor([[-29.2902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17231443575842365, distance: 1.0410926576614088 entropy 12.387043314971649
epoch: 77, step: 38
	action: tensor([[-66631.3111, -18130.7106, -26819.4611, -12983.6358,  56995.1160,
          -4671.6170, -27479.1577]], dtype=torch.float64)
	q_value: tensor([[-29.9897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3767917309117097, distance: 1.3427365714307775 entropy 12.485678123823067
epoch: 77, step: 39
	action: tensor([[ 53882.4113,  60005.4705, -45538.7265,  57038.0155, -82135.5286,
          -8659.1479,    966.2754]], dtype=torch.float64)
	q_value: tensor([[-26.7909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.359031788041246
epoch: 77, step: 40
	action: tensor([[-33922.8755,  -5070.5154, -33994.9743, -10044.2039,  59850.1290,
         -20774.7740,   6716.7180]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40154381424061225, distance: 1.3547527234794756 entropy 11.74048627719508
epoch: 77, step: 41
	action: tensor([[  29569.4587,  -28258.3479,   11333.8413,  -95311.2799, -120269.6924,
           36335.4896,  -45717.8086]], dtype=torch.float64)
	q_value: tensor([[-31.3843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46615732173037705, distance: 0.8361098477605775 entropy 12.384224400528648
epoch: 77, step: 42
	action: tensor([[ 21428.1253, -69783.1620,   1527.8399,  -4539.9860, -31043.0505,
          -7739.3945,   4646.1308]], dtype=torch.float64)
	q_value: tensor([[-31.6441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18838388626800695, distance: 1.0309367557695461 entropy 12.234876408600583
epoch: 77, step: 43
	action: tensor([[-45008.3647, -55972.4471,  57862.2357, -60703.0855,  39412.3262,
          65675.3944,  -3314.2721]], dtype=torch.float64)
	q_value: tensor([[-30.5255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1702848181065657, distance: 1.2379482173849772 entropy 12.293356095326475
epoch: 77, step: 44
	action: tensor([[-36150.7712, -11237.6180,  10289.9881, -48843.8769, -57054.3098,
         -39584.0189,  92399.7647]], dtype=torch.float64)
	q_value: tensor([[-25.1060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3301713855258246, distance: 1.3198072017274842 entropy 12.228584306371147
epoch: 77, step: 45
	action: tensor([[  7754.1294,  -4248.7657, 112003.8417, -58319.2944,  39410.6644,
          20730.9007,  22632.5521]], dtype=torch.float64)
	q_value: tensor([[-27.6849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4730050885202651, distance: 0.8307300187955498 entropy 12.364612081484873
epoch: 77, step: 46
	action: tensor([[-51921.3481,   5698.2805, -87267.6512, -17962.9225, -38442.7072,
          56657.4246, -41516.8852]], dtype=torch.float64)
	q_value: tensor([[-27.4449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07450573360732693, distance: 1.1862085823946766 entropy 12.070858499534706
epoch: 77, step: 47
	action: tensor([[ 2.5413e+03, -1.2090e+04, -2.2139e+01, -2.3428e+04,  4.4420e+03,
         -5.0205e+03,  1.0030e+04]], dtype=torch.float64)
	q_value: tensor([[-35.0312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.505154592246311, distance: 0.8049918165743271 entropy 12.248344343655706
epoch: 77, step: 48
	action: tensor([[ 16763.8608, -52572.8292,   7052.0957,  -4257.7010,   7712.5221,
         -45642.1792, -42629.9772]], dtype=torch.float64)
	q_value: tensor([[-26.1045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3760797911706566, distance: 0.9039017341961757 entropy 12.257824503219496
epoch: 77, step: 49
	action: tensor([[-53220.5413, -38739.3572, -32919.7252,  -3835.6435,   7866.1079,
         -27241.5765,  36460.8568]], dtype=torch.float64)
	q_value: tensor([[-29.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051089514061175234, distance: 1.1147290426453442 entropy 12.313402966501783
epoch: 77, step: 50
	action: tensor([[ 21900.7730, -58506.6737,   6462.0605,  37237.4038,  63806.0085,
          21684.2051,  -7494.3700]], dtype=torch.float64)
	q_value: tensor([[-26.2398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3873926392936027, distance: 1.3478960055527236 entropy 12.199830075952791
epoch: 77, step: 51
	action: tensor([[ 42151.1147, -50395.3976,  23115.9403,  84476.0251, -34570.7309,
          40657.5044,  16795.6693]], dtype=torch.float64)
	q_value: tensor([[-28.4474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5164630437328568, distance: 0.795740622759268 entropy 12.020780268983794
epoch: 77, step: 52
	action: tensor([[ 78566.2072, -38438.1485,  10254.9057,  67863.6221,  25275.6521,
         -18604.8575, -63221.5665]], dtype=torch.float64)
	q_value: tensor([[-29.5361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15154600226529458, distance: 1.2279970945119074 entropy 12.050757142768877
epoch: 77, step: 53
	action: tensor([[-85619.0007, -49459.3176, -43579.2546, -12474.8563, -27487.1471,
          67232.5682,  -9560.5312]], dtype=torch.float64)
	q_value: tensor([[-27.5521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4476446060521937, distance: 1.376853305383257 entropy 12.099747588040715
epoch: 77, step: 54
	action: tensor([[ 19304.2706,  -4225.9685,  -9273.4534, -36542.1999,  12504.5435,
          -2178.8634, -35300.5507]], dtype=torch.float64)
	q_value: tensor([[-24.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38290100371356384, distance: 0.898947053561194 entropy 12.042030363138482
epoch: 77, step: 55
	action: tensor([[-19252.4750, -41519.7869,   5686.3220,  -7563.4354,  23818.3935,
           3081.6845,  25295.4789]], dtype=torch.float64)
	q_value: tensor([[-24.4958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4502498978439138, distance: 1.3780916936394965 entropy 12.019910712200756
epoch: 77, step: 56
	action: tensor([[ 12948.8884, -16235.9922,  24878.0329, -34565.7443,  31279.4887,
         -52302.4198,  63702.9577]], dtype=torch.float64)
	q_value: tensor([[-24.5601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6342523939761058, distance: 0.6920658816305394 entropy 11.996297075830073
epoch: 77, step: 57
	action: tensor([[-66691.6380, -23036.9913,   8073.7952,  35291.3096,  -4685.4988,
          80701.5801,  25305.8314]], dtype=torch.float64)
	q_value: tensor([[-26.7470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22862396370899196, distance: 1.2684290626249142 entropy 12.032464992827673
epoch: 77, step: 58
	action: tensor([[-40791.1800,  45190.1810,  53370.1477,  -7607.7082, -17720.9552,
          71793.3238,  82850.1983]], dtype=torch.float64)
	q_value: tensor([[-23.5752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22130293645495058, distance: 1.009813010233218 entropy 12.162255242546282
epoch: 77, step: 59
	action: tensor([[-59140.7396, -18663.4282, -58891.9090,  -4089.3070, -14313.6488,
          -1865.4950, -23350.4107]], dtype=torch.float64)
	q_value: tensor([[-32.3884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26122278553601463, distance: 0.9835884933768936 entropy 12.1408843906214
epoch: 77, step: 60
	action: tensor([[-97177.4232, -88741.2772, -98748.3254, -57897.8794,  24396.7209,
         -35464.6142,  36474.2385]], dtype=torch.float64)
	q_value: tensor([[-24.1395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16921870796470806, distance: 1.2373842137156388 entropy 12.076574273883283
epoch: 77, step: 61
	action: tensor([[  13701.0069, -100863.8443,  -20060.6391,   79140.4902,  -15170.3665,
           34412.4849,   31225.0479]], dtype=torch.float64)
	q_value: tensor([[-26.5178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27040177000878196, distance: 1.2898144507598828 entropy 12.199680571380526
epoch: 77, step: 62
	action: tensor([[ 62310.9760, -66166.7122, -79532.0131, -32631.6500,   4846.0077,
         -37750.8050,  21256.6046]], dtype=torch.float64)
	q_value: tensor([[-23.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38637371117969166, distance: 0.8964140851902245 entropy 12.106485536222035
epoch: 77, step: 63
	action: tensor([[ 38980.7792,  -3827.7657, -42053.9183, -44442.7630,  42439.5469,
          -6779.5329,  21160.0045]], dtype=torch.float64)
	q_value: tensor([[-27.6275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41239706890976335, distance: 1.3599880649344758 entropy 12.031652217056916
epoch: 77, step: 64
	action: tensor([[-21411.1340,  31837.0803, 106181.4144, -24150.7671,  37298.9920,
          12053.4873,   3126.3785]], dtype=torch.float64)
	q_value: tensor([[-29.5288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11560107705344724, distance: 1.0761697882915606 entropy 12.191510096418925
epoch: 77, step: 65
	action: tensor([[-69970.7768, -78893.4886,  40976.1028,  36633.3167,  20087.8388,
          -6708.7817, -32815.6716]], dtype=torch.float64)
	q_value: tensor([[-32.2860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8704734371086422, distance: 1.5650653117865787 entropy 12.209072150593
epoch: 77, step: 66
	action: tensor([[ -40727.3073,   -2292.8096, -170179.8219,    1176.5469,  -42264.9584,
           45987.8022,   71106.7127]], dtype=torch.float64)
	q_value: tensor([[-27.7662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.917323003921759, distance: 1.5845441147508337 entropy 12.33525422371674
epoch: 77, step: 67
	action: tensor([[-57744.1199,   5387.3232,  76831.2004,  75857.3336,  91220.2679,
         -11731.2243, -56762.1570]], dtype=torch.float64)
	q_value: tensor([[-25.7538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4550036869315076, distance: 0.844799163962525 entropy 12.199623723201475
epoch: 77, step: 68
	action: tensor([[-66128.6192, -47302.8650, -24945.6365,   4843.9648,  16006.7540,
           4212.6664,  14671.8674]], dtype=torch.float64)
	q_value: tensor([[-25.9190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7057598336682678, distance: 1.4945678475519968 entropy 11.990712834355191
epoch: 77, step: 69
	action: tensor([[ 24498.5987, -54298.5601,   9127.3283,  -7264.2760, -69107.7924,
          90565.6782, -83551.8806]], dtype=torch.float64)
	q_value: tensor([[-27.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23865833628608846, distance: 0.9984963730483376 entropy 12.381235929000832
epoch: 77, step: 70
	action: tensor([[ 26466.7752,  -4257.1299,   6150.7548,   1006.2407, -11371.7627,
          19965.5719, -55281.1978]], dtype=torch.float64)
	q_value: tensor([[-26.4841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3981461265154215, distance: 0.8877735941079802 entropy 12.0785151923586
epoch: 77, step: 71
	action: tensor([[ 40611.1776, -37803.8588,  24064.6456,    160.2606, -63724.2560,
          23736.4082,  60209.0003]], dtype=torch.float64)
	q_value: tensor([[-28.6328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41674941115982, distance: 0.8739453706740204 entropy 12.104774135467759
epoch: 77, step: 72
	action: tensor([[-26012.2589,  46870.4392, -48423.4428, -41649.4115, -52558.2357,
          37690.7921, -63425.6000]], dtype=torch.float64)
	q_value: tensor([[-29.0692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.189907236800776
epoch: 77, step: 73
	action: tensor([[-13561.5292, -44022.0195,  54434.7243, -13221.9547, -17060.3529,
         -32636.3965, -50997.8892]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5978809892818906, distance: 1.4465348733275438 entropy 11.74048627719508
epoch: 77, step: 74
	action: tensor([[ -5862.1349, -91437.7292,  -8694.2422,    138.9069, -16012.4552,
         -43441.5940,  14728.6183]], dtype=torch.float64)
	q_value: tensor([[-25.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0005750501067234293, distance: 1.144673234354446 entropy 12.20438576583783
epoch: 77, step: 75
	action: tensor([[-51418.3223, -98155.5281, -15218.0679,  31216.6697, -70346.5936,
         -58035.8114, -18091.7281]], dtype=torch.float64)
	q_value: tensor([[-25.4181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6001552927094274, distance: 1.4475639516632994 entropy 12.292839177519976
epoch: 77, step: 76
	action: tensor([[ -33067.9649,  -78373.4898,  -28625.9365,   48691.8581, -105908.7077,
          -26124.0124,  -14103.3548]], dtype=torch.float64)
	q_value: tensor([[-26.3887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8234931145813675, distance: 1.5452855985246767 entropy 12.304855130375495
epoch: 77, step: 77
	action: tensor([[ 11782.6355, -96539.9991,  21227.4539,  65314.8058,  47588.2234,
          90028.6800,   1045.5840]], dtype=torch.float64)
	q_value: tensor([[-28.1512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5450153847627244, distance: 0.771889350486242 entropy 12.23219485830501
epoch: 77, step: 78
	action: tensor([[ 64149.7280,  27651.6467,  64171.1814,  -9337.3313, -34628.9938,
         -27843.5038,  -7011.4843]], dtype=torch.float64)
	q_value: tensor([[-29.6605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7342169798687825, distance: 0.5899569331452645 entropy 12.203069503193095
epoch: 77, step: 79
	action: tensor([[-46325.6790, -21180.4277, -24854.2560, -71927.2575,  90543.8051,
          54286.9036,  35480.4631]], dtype=torch.float64)
	q_value: tensor([[-24.8503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.024706442169059
epoch: 77, step: 80
	action: tensor([[ -3206.7154, -52736.6493,  12207.1635,  -3889.2664,  25026.5283,
           7674.9689,  18637.0602]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08584751879696506, distance: 1.0941226645399136 entropy 11.74048627719508
epoch: 77, step: 81
	action: tensor([[-37534.4358, -10269.3729,  79058.4420,  38592.5371, -21985.5235,
            723.4209,  36409.6725]], dtype=torch.float64)
	q_value: tensor([[-25.9767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006486703375678138, distance: 1.1480497654426798 entropy 12.040354645664479
epoch: 77, step: 82
	action: tensor([[-27220.0825,   -447.5804,  24939.2838, -79583.1168, -29389.0274,
         122424.8402, -74180.7108]], dtype=torch.float64)
	q_value: tensor([[-24.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07226128435782808, distance: 1.1849690465578746 entropy 12.256791162260523
epoch: 77, step: 83
	action: tensor([[-149215.0943,  -54420.5047,  -54893.5319,  -68345.0602,  -57958.7224,
          -67613.7403,  -16894.6257]], dtype=torch.float64)
	q_value: tensor([[-26.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8247433451862993, distance: 1.545815250185153 entropy 12.251114946376374
epoch: 77, step: 84
	action: tensor([[  7945.2585,   9878.8122, -36771.4303,  21992.2195,  44260.5183,
          85687.1591, 112470.4820]], dtype=torch.float64)
	q_value: tensor([[-27.6560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.329688677711545
epoch: 77, step: 85
	action: tensor([[-13038.7552, -16210.1014,  24204.2401,  -9987.5674,   3613.7067,
          23745.6360,  49783.9282]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030847679168569453, distance: 1.126555814192509 entropy 11.74048627719508
epoch: 77, step: 86
	action: tensor([[-43647.3670, -42610.9418, -45202.9025, -33904.7208,  40722.8381,
          34617.2009,  37526.2303]], dtype=torch.float64)
	q_value: tensor([[-25.2857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3893696627468981, distance: 1.3488560341986884 entropy 12.100473559924321
epoch: 77, step: 87
	action: tensor([[ 18434.9696,  62439.5172,  69048.4325,  51299.9917, -24951.2516,
          14185.1838,  15535.0574]], dtype=torch.float64)
	q_value: tensor([[-31.4610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.374669988820967
epoch: 77, step: 88
	action: tensor([[-16262.3859,   2645.6596,  -9709.5584,  -6901.1941,  -1148.1491,
          44783.1490,  18699.2951]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.74048627719508
epoch: 77, step: 89
	action: tensor([[-47829.1997, -58598.3208,  26885.5177, -39265.8594,  20534.9610,
          -8036.1447,  -7878.6205]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24323418264661423, distance: 1.2759485553577514 entropy 11.74048627719508
epoch: 77, step: 90
	action: tensor([[-111840.3281,  -43790.0812,  -12003.2066,   22622.0735,  -52801.7488,
           34480.3577,   18426.5525]], dtype=torch.float64)
	q_value: tensor([[-26.8520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9908479410769064, distance: 1.614640116126867 entropy 12.146547011955645
epoch: 77, step: 91
	action: tensor([[   2363.7013, -100126.2973, -108007.6464, -110151.7593,  -21147.9839,
           41498.6924, -117708.6210]], dtype=torch.float64)
	q_value: tensor([[-27.3964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28053972454690557, distance: 0.970644287817584 entropy 12.336744826891847
epoch: 77, step: 92
	action: tensor([[-86961.0679, -22842.2461,   5220.8843,  30980.9384,  -3778.8000,
         -24197.4548, -25509.6748]], dtype=torch.float64)
	q_value: tensor([[-27.1035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5333657199929436, distance: 1.417031707825774 entropy 11.985683745482975
epoch: 77, step: 93
	action: tensor([[-136699.8132, -108257.3936,   21084.7071,    6388.1001,  -63005.5664,
          116569.3930,  -79505.5177]], dtype=torch.float64)
	q_value: tensor([[-25.0710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.815307731879398, distance: 1.5418134218443496 entropy 12.212764338468162
epoch: 77, step: 94
	action: tensor([[-57106.1658,  54124.9727,  71005.7040,  23266.7440, -40874.8392,
         107070.5278,  23191.1694]], dtype=torch.float64)
	q_value: tensor([[-28.2372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30613438838741003, distance: 0.9532226984005604 entropy 12.337111956099195
epoch: 77, step: 95
	action: tensor([[-14592.7131,  17387.0801, -49837.1984,  46166.7263,  52674.4069,
          76475.7679,  57861.6230]], dtype=torch.float64)
	q_value: tensor([[-27.3262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4957435580397048, distance: 0.8126104834181735 entropy 12.262682347749086
epoch: 77, step: 96
	action: tensor([[ -37723.5910, -120174.4976,   -8496.7263,   33162.9922,   40348.0295,
          -27934.8015,   43565.7159]], dtype=torch.float64)
	q_value: tensor([[-30.1236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.171256379151846
epoch: 77, step: 97
	action: tensor([[  5867.1490,  -1385.2512, -19759.7056,  64067.4546,  31111.8961,
          -1424.5264, -25540.3070]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.74048627719508
epoch: 77, step: 98
	action: tensor([[-15890.4386,  10352.6697,   6724.6716,  -2576.8977,  -4572.2921,
          70873.1341,  20091.9370]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.74048627719508
epoch: 77, step: 99
	action: tensor([[ 15259.3734, -52821.8254, -60614.5960, -19790.6145, -31298.5330,
          52909.6668, -11439.6582]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1093798974644683, distance: 1.0799482375517142 entropy 11.74048627719508
epoch: 77, step: 100
	action: tensor([[-42886.2942, -97126.6074,  33880.3490,  52415.0896, -16984.2348,
          -9617.6787, -75297.0009]], dtype=torch.float64)
	q_value: tensor([[-27.2793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39956621232995393, distance: 1.3537965966356111 entropy 12.054527514152
epoch: 77, step: 101
	action: tensor([[-13906.2125, -23114.7098,  17023.0313,  12847.5652,  33688.4882,
          12782.8061,  25737.4522]], dtype=torch.float64)
	q_value: tensor([[-20.9774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21371361671594635, distance: 1.2607088613476085 entropy 11.837950992280886
epoch: 77, step: 102
	action: tensor([[  11509.9542, -126800.5384,   10475.1548,  -17988.9922,   -6778.2081,
          -14808.0908,  107348.8134]], dtype=torch.float64)
	q_value: tensor([[-25.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2656875863312069, distance: 0.9806118301990774 entropy 12.257580949009533
epoch: 77, step: 103
	action: tensor([[-24980.8447, -36357.0806,  -5362.2546,   1559.9574,   3008.8342,
          37331.5459,  -7241.6426]], dtype=torch.float64)
	q_value: tensor([[-24.3012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2695942882442267, distance: 0.9779998172302917 entropy 11.876744481847956
epoch: 77, step: 104
	action: tensor([[-65611.2024,  45929.6879,  -6146.0896,  17630.6054,   3462.9588,
          16457.7710,  81769.8164]], dtype=torch.float64)
	q_value: tensor([[-28.3248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21962560099385298, distance: 1.010900008059963 entropy 12.338361982428678
epoch: 77, step: 105
	action: tensor([[  2320.2859, -73168.9858, -65877.6544,   4630.5817, -40184.9379,
          24903.5262, -31338.8758]], dtype=torch.float64)
	q_value: tensor([[-29.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25642118793871704, distance: 1.282697709059537 entropy 12.214760979283524
epoch: 77, step: 106
	action: tensor([[-17496.3205, -10752.0619,  44005.8351,  29825.9347, -30773.7628,
          -9659.5165,  23668.4811]], dtype=torch.float64)
	q_value: tensor([[-31.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010442397185672525, distance: 1.1383537254803682 entropy 12.207689393718239
epoch: 77, step: 107
	action: tensor([[ 12629.4102, -69335.8981,  59773.0571,  35061.4733,  11289.4260,
          73169.7723,  21729.7216]], dtype=torch.float64)
	q_value: tensor([[-26.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3694310027126779, distance: 0.9087051731014489 entropy 12.283842624032122
epoch: 77, step: 108
	action: tensor([[-42099.1355, -25416.5247, -35573.5665,  19122.5937,  32764.6782,
         -97820.0564,    549.6029]], dtype=torch.float64)
	q_value: tensor([[-28.4104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11527813685361743, distance: 1.076366253233538 entropy 12.204594477104447
epoch: 77, step: 109
	action: tensor([[-98114.5101,  22622.7565, -19700.7868,  20428.8471,  47147.2892,
           6998.1517,  -9556.2644]], dtype=torch.float64)
	q_value: tensor([[-30.6642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13483583123534526, distance: 1.0644026706976153 entropy 12.447592944153927
epoch: 77, step: 110
	action: tensor([[-42403.2863,  55545.7914,  -3065.6034,  62649.6291, -24681.5789,
         -26816.4294, -56534.2860]], dtype=torch.float64)
	q_value: tensor([[-29.8732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1915716215564115, distance: 1.0289101900840047 entropy 12.340240044388304
epoch: 77, step: 111
	action: tensor([[-46860.1728, -85579.7508,   7278.2689,  85795.1465,   8427.9163,
         -53741.6929,  -1780.4764]], dtype=torch.float64)
	q_value: tensor([[-28.7010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10263800069580975, distance: 1.0840280761473426 entropy 12.205530944173722
epoch: 77, step: 112
	action: tensor([[ 88751.9014,   9729.6248,  39450.0523,  44265.5200,  26092.9732,
         -30747.2099, -34871.4004]], dtype=torch.float64)
	q_value: tensor([[-30.1857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.544526190823208, distance: 0.7723042020388227 entropy 12.402612380358162
epoch: 77, step: 113
	action: tensor([[-18185.4662, -49192.4753, -51477.4690,   3718.7611,  -4830.0226,
         -65525.7943, -34309.9083]], dtype=torch.float64)
	q_value: tensor([[-26.9094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.177000906841124
epoch: 77, step: 114
	action: tensor([[-26409.7512, -43794.9500,  -6977.1542,  -4869.9177, -39506.0314,
          45642.5181,  32079.0102]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.098483877029097, distance: 1.1993709808720305 entropy 11.74048627719508
epoch: 77, step: 115
	action: tensor([[ 12317.4819,  90965.3535, -67391.5497,   5808.8044, -15852.8858,
         -50593.6389, -36796.6105]], dtype=torch.float64)
	q_value: tensor([[-32.2041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.45188907271918
epoch: 77, step: 116
	action: tensor([[  5355.1744,  -7754.7434, -11709.8985,  37559.5616,  44295.2749,
          -5483.0511,  15167.6394]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2256669836039591, distance: 1.2669017527699786 entropy 11.74048627719508
epoch: 77, step: 117
	action: tensor([[-53991.8976, -32705.7207,  35396.6696,  79787.5232,  -9204.0479,
           4980.5782,  28964.2988]], dtype=torch.float64)
	q_value: tensor([[-26.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.033977624969523834, distance: 1.163622911112123 entropy 12.089351760187313
epoch: 77, step: 118
	action: tensor([[ -30171.8577,   -4087.2672,   93084.2385, -105668.8711,   11337.5461,
           18236.1683,  117243.6010]], dtype=torch.float64)
	q_value: tensor([[-25.4521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2121954802473056, distance: 1.2599201551233596 entropy 12.324817586877257
epoch: 77, step: 119
	action: tensor([[-120341.1351,   42545.1607,   23513.7455,  -51265.6695,   55327.4652,
          -12770.5318,   -9169.3431]], dtype=torch.float64)
	q_value: tensor([[-26.6240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.195366546001013
epoch: 77, step: 120
	action: tensor([[  3133.3357,  28026.9220,   6022.8564,  44515.6709, -23161.9132,
         -16040.8172, -17218.8429]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6283303464506271, distance: 0.6976462187445632 entropy 11.74048627719508
epoch: 77, step: 121
	action: tensor([[ -53812.8252,  -85682.3025,  -75218.2031,   -1942.2862,   24516.7500,
          -51161.0026, -146792.0844]], dtype=torch.float64)
	q_value: tensor([[-28.3328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.285755586986754
epoch: 77, step: 122
	action: tensor([[-32765.0460, -32296.5068, -33773.7741, -12236.1258, -17214.5957,
         -12887.2149,  20598.4233]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.74048627719508
epoch: 77, step: 123
	action: tensor([[-39791.6468,  -6377.9479,  27815.0687, -27205.6683, -36754.7345,
          -5255.9663,   3201.7274]], dtype=torch.float64)
	q_value: tensor([[-27.1820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2173162794958885, distance: 1.2625785542420729 entropy 11.74048627719508
epoch: 77, step: 124
	action: tensor([[-38754.7457, -32461.1052,  -5250.8855,  12842.1476, -35050.0642,
         -94549.6942, -43118.4222]], dtype=torch.float64)
	q_value: tensor([[-27.4861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10479189712094139, distance: 1.2028097322603992 entropy 12.24193935433109
epoch: 77, step: 125
	action: tensor([[ -82166.5134,  -51258.6854,  -36015.7299,   -7978.9613,   14868.0312,
           10362.3805, -146023.8178]], dtype=torch.float64)
	q_value: tensor([[-28.3976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31392517292246713, distance: 0.9478561574402972 entropy 12.379150701112952
epoch: 77, step: 126
	action: tensor([[ -48398.3516, -111236.8147,   44366.7013,  -55188.5149,  -10622.0322,
          -31884.9957,   38304.3358]], dtype=torch.float64)
	q_value: tensor([[-27.1558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7339387695590494, distance: 1.5068623153245784 entropy 12.244472081047787
epoch: 77, step: 127
	action: tensor([[ -31471.8619,  -34125.4880, -117659.8692,   20383.9057,   11275.6451,
          -41217.0954,   32924.8598]], dtype=torch.float64)
	q_value: tensor([[-29.0470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6509787172420896, distance: 1.4703726999430788 entropy 12.322948139429375
LOSS epoch 77 actor 292.0946950599151 critic 257.53108264950225
epoch: 78, step: 0
	action: tensor([[ 37344.3404,  35361.5882, -10191.1349, -67636.0059,  97683.7123,
          63875.3435,  39761.4990]], dtype=torch.float64)
	q_value: tensor([[-30.6686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8233889146302356, distance: 0.4809120653829321 entropy 12.280128784693346
epoch: 78, step: 1
	action: tensor([[-24649.7270,   2072.0299,  32338.2824, -25343.0900, -69414.6013,
          44762.9538, -48795.7502]], dtype=torch.float64)
	q_value: tensor([[-32.9540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.687703019992902, distance: 0.639500054056137 entropy 11.906124289455855
epoch: 78, step: 2
	action: tensor([[-63724.5128,  62087.8456,  51542.0344,  35464.0177,  55167.7806,
          67345.0651, 112693.4653]], dtype=torch.float64)
	q_value: tensor([[-38.8474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5538242110036651, distance: 0.7643806656575723 entropy 12.074612747504137
epoch: 78, step: 3
	action: tensor([[ -9035.1589, -47017.7555,  66054.9617,  62765.2466,    500.7120,
           7269.8514,  -2747.8488]], dtype=torch.float64)
	q_value: tensor([[-32.6406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7580600106451287, distance: 1.5173072780256411 entropy 11.985755153469356
epoch: 78, step: 4
	action: tensor([[   9775.3320, -129163.9796,   17551.2908,   48665.4307, -113195.4886,
          -27406.7910,  -55523.1622]], dtype=torch.float64)
	q_value: tensor([[-30.6791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.415431961493093
epoch: 78, step: 5
	action: tensor([[ 27898.1718,   7309.7314,  16078.6683, -55777.8245, -14737.4626,
           5948.4570,  68097.4140]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.774952933136987
epoch: 78, step: 6
	action: tensor([[ 16452.9239, -36762.6528,   5995.9809, -32032.5567,  12785.9855,
          -2113.9200,  17080.7544]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4155011718165009, distance: 0.8748800546448365 entropy 11.774952933136987
epoch: 78, step: 7
	action: tensor([[-10365.4672,   4690.0625,  47498.7950,  61802.6183,   7068.3121,
         -56793.3885, 137679.6981]], dtype=torch.float64)
	q_value: tensor([[-30.3875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.235548022024128
epoch: 78, step: 8
	action: tensor([[ 33311.7394, -64649.0226, -30763.6645,  16516.3944, -27010.3555,
           6953.7107,  54491.7311]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4821901810888062, distance: 0.8234587221376835 entropy 11.774952933136987
epoch: 78, step: 9
	action: tensor([[-60735.6406,  32953.6727, -45740.7825, -45837.9448, -71049.8214,
         -40727.2321,  53910.5113]], dtype=torch.float64)
	q_value: tensor([[-31.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.031424158897549
epoch: 78, step: 10
	action: tensor([[-22591.6709, -34784.9119, -17435.1822,  13414.1903,  20386.4514,
           -981.2072,  13073.1044]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49351338795792976, distance: 0.8144054632049404 entropy 11.774952933136987
epoch: 78, step: 11
	action: tensor([[ 42002.5062, -10594.5076, -18573.5045,  -5258.8078,  12389.5573,
         -61224.4506,  46057.0039]], dtype=torch.float64)
	q_value: tensor([[-27.2585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2471189135456202, distance: 1.2779404772438716 entropy 12.27166178258807
epoch: 78, step: 12
	action: tensor([[-25646.6095, -64381.4302, -42273.0236, -62068.1089, -51900.0306,
         -23481.6955,  43812.0949]], dtype=torch.float64)
	q_value: tensor([[-27.5435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3598439434125701, distance: 0.9155870104338345 entropy 11.915368944064989
epoch: 78, step: 13
	action: tensor([[ 16651.4939, -21767.4507,  27155.9617,  30485.3842, -17554.2316,
         -50237.1857,  21682.0834]], dtype=torch.float64)
	q_value: tensor([[-27.2607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06891974068609685, distance: 1.104206381478361 entropy 12.059849653240251
epoch: 78, step: 14
	action: tensor([[-14236.7790,  13495.8655,  51124.7825,    643.7454,  -9574.6412,
         -36358.5845,  -8260.9742]], dtype=torch.float64)
	q_value: tensor([[-28.5221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.880409014606553
epoch: 78, step: 15
	action: tensor([[ 35341.8957, -31564.3935,  -6865.9940,  13778.2068, -10363.4339,
           3060.6013,  -1547.3189]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36537500882428287, distance: 0.911623009821443 entropy 11.774952933136987
epoch: 78, step: 16
	action: tensor([[-19079.3704, -32947.4395,  50702.2093, -33690.3203,  11949.0890,
          71131.6215, -73805.5801]], dtype=torch.float64)
	q_value: tensor([[-34.0976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4489292144718362, distance: 1.3774640647971912 entropy 12.294798487425615
epoch: 78, step: 17
	action: tensor([[ -3967.5083,  18018.0026, -52098.9021,  39537.5374,  35237.2274,
          -6382.8331,   3744.3500]], dtype=torch.float64)
	q_value: tensor([[-29.5704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6782549895803205, distance: 0.6491014863320456 entropy 12.227563242946298
epoch: 78, step: 18
	action: tensor([[-11050.0018, -52044.6711,    168.7054, -37889.2477, -46527.2970,
          16134.6460,  22278.6984]], dtype=torch.float64)
	q_value: tensor([[-30.9847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9424041231308258, distance: 1.594874406784335 entropy 12.140309515015431
epoch: 78, step: 19
	action: tensor([[-25368.4164, -28410.0136, -10968.1753,  48088.2463,  17035.0176,
         -65879.5477,  27017.0234]], dtype=torch.float64)
	q_value: tensor([[-27.4363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.362941614126536, distance: 1.3359657342488946 entropy 12.065321906995385
epoch: 78, step: 20
	action: tensor([[  49323.4096,  -68349.1669,  -21220.4020,   75162.2579,  -41592.2183,
         -102863.2351,   65095.6922]], dtype=torch.float64)
	q_value: tensor([[-30.6025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016655839612461, distance: 0.9562873618180988 entropy 12.19323616330364
epoch: 78, step: 21
	action: tensor([[  21306.4266,  -20201.5188,  -17145.8674,  -34870.2544,   63821.6926,
         -154915.4826,  -23251.0926]], dtype=torch.float64)
	q_value: tensor([[-33.9980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3292564775265123, distance: 1.3193532339588774 entropy 12.370583601742755
epoch: 78, step: 22
	action: tensor([[  9763.1177, -24532.3848, 112462.8184,  37754.6525,  41560.6136,
          76435.6251, -45709.4344]], dtype=torch.float64)
	q_value: tensor([[-34.0424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18487679066789264, distance: 1.0331617586101804 entropy 12.387721899466426
epoch: 78, step: 23
	action: tensor([[-60537.8250, -34530.7394,  -4687.1478,  83482.6759, -24479.5602,
          99906.6290,  42131.3809]], dtype=torch.float64)
	q_value: tensor([[-34.7197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06440331086970064, distance: 1.1068812515826407 entropy 12.302852079271771
epoch: 78, step: 24
	action: tensor([[  6024.1695, -89187.1391,  -8930.7762, -30343.2787,  52619.8108,
         -12700.8956, -16624.0877]], dtype=torch.float64)
	q_value: tensor([[-30.4067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19829747270833342, distance: 1.024621157936237 entropy 12.248359762710548
epoch: 78, step: 25
	action: tensor([[-123913.0828,  -85529.7084,   62623.0732,  -34905.9044,   63432.5184,
          -25836.2413,  -47046.4579]], dtype=torch.float64)
	q_value: tensor([[-27.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19456345381051876, distance: 1.2507234864367547 entropy 12.103125867559248
epoch: 78, step: 26
	action: tensor([[104211.8494, -30525.6970, -29447.2545, -21347.3509,  24824.3096,
         102095.1724, -13900.9207]], dtype=torch.float64)
	q_value: tensor([[-34.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42364054309643673, distance: 0.8687671781541281 entropy 12.401882257850875
epoch: 78, step: 27
	action: tensor([[ -46405.9999, -131265.9312,  -34435.8345,  -56472.0885,  -83518.0117,
           19643.1578,   -5488.0496]], dtype=torch.float64)
	q_value: tensor([[-35.3847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12821695311420012, distance: 1.2154945165224063 entropy 12.359176407739444
epoch: 78, step: 28
	action: tensor([[-139368.2874,   14170.8132,   26677.0924,   33645.5651,  -40500.3979,
          -25933.6071,   -3039.1111]], dtype=torch.float64)
	q_value: tensor([[-32.9888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.385545556789876, distance: 1.3469984570659816 entropy 12.37200309256279
epoch: 78, step: 29
	action: tensor([[ -6638.8005,  16363.9618,  38397.5070, 107409.6323, -12447.5668,
         -30969.2842,   2693.3836]], dtype=torch.float64)
	q_value: tensor([[-30.2975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32614615054181917, distance: 0.9393761943396185 entropy 12.127637345476293
epoch: 78, step: 30
	action: tensor([[ 26383.9688, -51270.6912, -27365.6390,  -2221.1457,  63926.9939,
          -6567.4143,   1585.1326]], dtype=torch.float64)
	q_value: tensor([[-30.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.214417078669445
epoch: 78, step: 31
	action: tensor([[-48778.9752, -36340.0606,   9256.4355,  16252.4368,   4666.9413,
          -2213.5104,  49262.5530]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4338530361329611, distance: 1.3702790357759314 entropy 11.774952933136987
epoch: 78, step: 32
	action: tensor([[-105891.4602,   11972.4196,  -45563.6064,  -72320.8396,   74411.4086,
           80967.8205,  -43770.3721]], dtype=torch.float64)
	q_value: tensor([[-30.1732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0018547989309189994, distance: 1.1454050265973645 entropy 12.257060715116399
epoch: 78, step: 33
	action: tensor([[ -5485.1045, -12202.4005,  34172.9191,  59280.4131,  58005.1415,
          14426.3565, -26144.2514]], dtype=torch.float64)
	q_value: tensor([[-32.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09012979513762032, distance: 1.1948016073518775 entropy 11.828682756681646
epoch: 78, step: 34
	action: tensor([[ 75518.6240, -71207.2107, -41629.4843,  49353.9801,   2117.2753,
          -6879.5405, -20121.4255]], dtype=torch.float64)
	q_value: tensor([[-29.3989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31196936503489425, distance: 1.3107459979746514 entropy 12.238125257836913
epoch: 78, step: 35
	action: tensor([[-43115.0460, -56034.9868,  26240.6333,  45573.4189,  93888.7017,
         -54458.9391,  56325.8663]], dtype=torch.float64)
	q_value: tensor([[-33.8526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0953717829802363, distance: 1.197670818138817 entropy 12.371704048677445
epoch: 78, step: 36
	action: tensor([[ -49847.1756, -103787.1225,  -85435.2232,   -6662.7740,   31165.5276,
           17098.4976,   12252.3467]], dtype=torch.float64)
	q_value: tensor([[-31.8953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8616791582289831, distance: 1.5613817960095715 entropy 12.394709708766616
epoch: 78, step: 37
	action: tensor([[   9304.4304,  -74539.0088,  -16264.4843, -132861.6153,   46178.2595,
          -44435.8876,  -36014.2968]], dtype=torch.float64)
	q_value: tensor([[-32.2183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.31490413609353
epoch: 78, step: 38
	action: tensor([[-40925.4932, -24147.1489, -34678.5866, -10484.1578, -43762.2087,
         -52883.7978,  20787.6010]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2808030910092205, distance: 0.9704666139697156 entropy 11.774952933136987
epoch: 78, step: 39
	action: tensor([[-85949.6801, -86809.8397, -41118.9413, -26404.5464,   2434.0375,
          -1916.1252, -56259.5892]], dtype=torch.float64)
	q_value: tensor([[-32.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25365061330890026, distance: 1.2812826696274613 entropy 12.278799726843284
epoch: 78, step: 40
	action: tensor([[ -43455.2297,  -19598.6312,   75267.4400,   16158.1027, -107102.7571,
            6280.3990,    4086.2813]], dtype=torch.float64)
	q_value: tensor([[-35.2486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6060614649125764, distance: 1.4502329699240137 entropy 12.352939048723282
epoch: 78, step: 41
	action: tensor([[ -4090.2329, -41661.9782,  28358.3002,  61523.6400,   1276.1632,
         -16625.6225, -52304.8583]], dtype=torch.float64)
	q_value: tensor([[-28.0502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4318724411332282, distance: 1.3693323189251136 entropy 12.125304638977642
epoch: 78, step: 42
	action: tensor([[-85240.2027,  23111.5135,  68222.8559,  47191.5892,  23405.4523,
         105704.6365,  49588.5763]], dtype=torch.float64)
	q_value: tensor([[-33.5902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24431424024356108, distance: 0.9947806121492984 entropy 12.316109369604586
epoch: 78, step: 43
	action: tensor([[-66294.9701, -60682.2650,  21109.9956,     68.8291, -53232.0839,
          37000.2200,  29733.5591]], dtype=torch.float64)
	q_value: tensor([[-32.5678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026816515043196487, distance: 1.1288963228483815 entropy 12.13531805426918
epoch: 78, step: 44
	action: tensor([[  9803.3433, -83124.7692, 118438.4684, -14268.9416,  17606.1541,
           6378.2147,  14968.6877]], dtype=torch.float64)
	q_value: tensor([[-29.9545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.054587605891465385, distance: 1.1751627713707797 entropy 12.256857566470526
epoch: 78, step: 45
	action: tensor([[-43626.3636, -36745.4755,  -2077.5954, -46343.8531, -10172.4726,
          26652.7533, -23447.9339]], dtype=torch.float64)
	q_value: tensor([[-28.4578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7836680757265269, distance: 1.5283179466066474 entropy 11.954324714947887
epoch: 78, step: 46
	action: tensor([[ 20076.4483,  22611.7496,   9103.1020,  54455.2895, -16629.0471,
          25805.4054,  -9713.1803]], dtype=torch.float64)
	q_value: tensor([[-30.1364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.106690401501611
epoch: 78, step: 47
	action: tensor([[ -9281.1573, -42354.2405, -18936.7591, -10628.4175,  19106.1814,
          47203.5148,  19887.7196]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37551641213731624, distance: 1.342114540568786 entropy 11.774952933136987
epoch: 78, step: 48
	action: tensor([[-60653.1089, -48737.6562,  53731.6342, -64985.0943,  72648.6191,
         -54403.0261,  -4230.6780]], dtype=torch.float64)
	q_value: tensor([[-31.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2328365717097043, distance: 1.2706017461144732 entropy 12.12438910424864
epoch: 78, step: 49
	action: tensor([[-39353.1694, -80530.0175, -75736.4486,  -4882.4863,  -5394.8648,
          18507.7301,   -310.7285]], dtype=torch.float64)
	q_value: tensor([[-35.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22974548471179757, distance: 1.0043239507253232 entropy 12.432821240949924
epoch: 78, step: 50
	action: tensor([[ 82473.9856,  -1344.1453,  30474.4650,  71869.5997,  51922.1357,
         -28595.9172,  74860.6926]], dtype=torch.float64)
	q_value: tensor([[-34.1732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04094086340971481, distance: 1.1206742335734299 entropy 12.355392559661471
epoch: 78, step: 51
	action: tensor([[-92833.2917,   4971.5170,   5022.9419,  -7727.8526, -11577.4929,
         -30277.3894,  59656.3468]], dtype=torch.float64)
	q_value: tensor([[-28.7087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.243670915717344
epoch: 78, step: 52
	action: tensor([[-20016.0556, -14840.2067,  61453.5308,  27337.2817, -15776.1057,
          15902.1454,  -7608.0120]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2613429121025277, distance: 1.2852075815278488 entropy 11.774952933136987
epoch: 78, step: 53
	action: tensor([[-119864.8951,  -18235.7830,  -83054.4674,  -15222.5683,   68402.8030,
         -101187.1775,  -87752.4017]], dtype=torch.float64)
	q_value: tensor([[-31.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8906762288788796, distance: 1.5734946667322074 entropy 12.308910120003777
epoch: 78, step: 54
	action: tensor([[-182888.1301,   -1665.0775,  -77684.4453,  -25495.9013,   48430.6439,
          -13444.3802,  -20620.7764]], dtype=torch.float64)
	q_value: tensor([[-31.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16460178171051332, distance: 1.045932036824021 entropy 12.34985363689521
epoch: 78, step: 55
	action: tensor([[ -17447.4764, -101167.9959,  -22718.9355,   -8131.2766,   71883.8415,
            -490.1215,   13141.8149]], dtype=torch.float64)
	q_value: tensor([[-29.9043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20394484511012134, distance: 1.2556251010722155 entropy 12.141482575290553
epoch: 78, step: 56
	action: tensor([[-118023.8888,   47422.7187,    2820.6705,  -25194.2781,  -71759.8882,
          -36894.7178,   60233.0714]], dtype=torch.float64)
	q_value: tensor([[-29.9827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15368575719669086, distance: 1.052743377679361 entropy 12.255768815618072
epoch: 78, step: 57
	action: tensor([[-85127.4640, -50355.6811, -11901.4478,  -6549.3225, -37485.2446,
          29909.6420,  -8933.2365]], dtype=torch.float64)
	q_value: tensor([[-38.8092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25007814410896634, distance: 1.2794557616998046 entropy 12.25157714213652
epoch: 78, step: 58
	action: tensor([[ -8653.4924, -22256.5097, -19954.8961, -17826.5899, -69450.4215,
          17493.2339, -16843.4657]], dtype=torch.float64)
	q_value: tensor([[-33.7355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1510646512214613, distance: 1.2277404137256671 entropy 12.23387548616644
epoch: 78, step: 59
	action: tensor([[-28970.7397,  17782.7155,  36040.6832, -62282.4386, -10636.9665,
          23539.8674, 124626.6985]], dtype=torch.float64)
	q_value: tensor([[-32.6280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09262670141131824, distance: 1.1961691515214943 entropy 12.255118158332959
epoch: 78, step: 60
	action: tensor([[  4499.2695, -54324.1040,  15219.5493,  21162.3865, -85785.3098,
          13611.5889,  32980.2164]], dtype=torch.float64)
	q_value: tensor([[-37.4982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3603213247157484, distance: 0.9152455580392113 entropy 11.98124678670482
epoch: 78, step: 61
	action: tensor([[ 23711.1798, -28933.5930,  26458.0008, -26645.0629, -16867.2539,
          22183.8565,   3532.1521]], dtype=torch.float64)
	q_value: tensor([[-27.8403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3580029751743974, distance: 0.9169025931613074 entropy 11.922832455644217
epoch: 78, step: 62
	action: tensor([[ -15680.3946, -114240.7308,  -37189.5266,  -12428.5909,  -29462.8826,
            7907.0558,   44328.4967]], dtype=torch.float64)
	q_value: tensor([[-37.2889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5293203815795238, distance: 1.415161260834551 entropy 12.214713380551776
epoch: 78, step: 63
	action: tensor([[-12885.8351, -79056.2852,   4304.6602,  75698.7123, -21697.5101,
          60192.1535,  40509.9985]], dtype=torch.float64)
	q_value: tensor([[-28.7169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8429591535936833, distance: 1.5535117707421826 entropy 12.006400284951763
epoch: 78, step: 64
	action: tensor([[-20826.5855,  73590.3985,  12611.9711,  21716.8496,  34812.4814,
         -10097.6655,  80315.5515]], dtype=torch.float64)
	q_value: tensor([[-29.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08874516376661634, distance: 1.0923872344962307 entropy 12.221809045789081
epoch: 78, step: 65
	action: tensor([[-117130.8064,  -62587.3630,  -10831.5637,   86875.2363,  -48100.4061,
         -124588.4878,    3654.3789]], dtype=torch.float64)
	q_value: tensor([[-32.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24001159887675594, distance: 0.997608579282929 entropy 12.467644587607916
epoch: 78, step: 66
	action: tensor([[    362.1880, -128094.4229, -107110.7189,   62135.0335,  -47301.8653,
           23617.3100,   46336.4010]], dtype=torch.float64)
	q_value: tensor([[-31.9354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.452535898342774
epoch: 78, step: 67
	action: tensor([[  5570.4457, -29004.1722,  20536.1677,  32035.7800, -12111.6369,
         -59130.5551, -20824.1225]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4473291205463358, distance: 0.8507265451073971 entropy 11.774952933136987
epoch: 78, step: 68
	action: tensor([[-88278.5377,  33821.5157, -60433.0888,  -5325.6662, -88866.1402,
          67913.3783, -54787.9568]], dtype=torch.float64)
	q_value: tensor([[-36.1552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.242358182314552
epoch: 78, step: 69
	action: tensor([[-27539.5031, -52862.2629, -90210.2183,  56284.5196, -10576.3449,
           -928.1689,  65805.5551]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39643159214888013, distance: 1.3522796920089841 entropy 11.774952933136987
epoch: 78, step: 70
	action: tensor([[  40964.8053,  -18709.3895,  -60080.0717,   11180.3896, -130823.8759,
         -131485.6792,  110886.3162]], dtype=torch.float64)
	q_value: tensor([[-32.9490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.536667832334246, distance: 0.7789380512815938 entropy 12.438189342421028
epoch: 78, step: 71
	action: tensor([[-22121.0536, -12280.4495, -37641.7246, -25379.0951, -11327.0701,
         -13414.1727,  17697.8847]], dtype=torch.float64)
	q_value: tensor([[-30.5039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7070794151177957, distance: 1.4951458371052004 entropy 12.163476708500472
epoch: 78, step: 72
	action: tensor([[-18782.2408,   2486.8026, -17823.2400,  92909.8478, -40355.7849,
          -9182.0865, -15797.4804]], dtype=torch.float64)
	q_value: tensor([[-28.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6495180804283716, distance: 0.6774691175375175 entropy 12.131408579056556
epoch: 78, step: 73
	action: tensor([[ 52205.1147, -12828.0330,  27256.8487, -40884.0746,  75929.7250,
           2373.6195,  10958.1675]], dtype=torch.float64)
	q_value: tensor([[-31.7271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.159816776812765
epoch: 78, step: 74
	action: tensor([[ 35613.6917,   2690.6440, -32114.5902,  33445.1294, -45743.9657,
         -56747.9155,  29133.7621]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37182628287942143, distance: 0.9069776266828147 entropy 11.774952933136987
epoch: 78, step: 75
	action: tensor([[-18494.5923, -25534.4847,  29796.0586,  26449.9436,  35014.2004,
          26045.5481,  53028.0106]], dtype=torch.float64)
	q_value: tensor([[-31.6608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5459242419187313, distance: 1.4228227380747038 entropy 12.256481298576007
epoch: 78, step: 76
	action: tensor([[-25830.7353, -59592.0872,  33245.7368,  -9629.3892,  40761.1407,
          71010.2956,   1059.3846]], dtype=torch.float64)
	q_value: tensor([[-30.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3700377026272572, distance: 1.3394390391744992 entropy 12.359176898468577
epoch: 78, step: 77
	action: tensor([[-129590.4636,    3179.3076,  -39287.8238,   27824.1755,  -43816.5596,
          -49869.5592,  -13267.4069]], dtype=torch.float64)
	q_value: tensor([[-30.7025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.231536601270028
epoch: 78, step: 78
	action: tensor([[-45432.0425, -13785.2096, -47950.3272,  41080.2335, -32442.6292,
          33137.1719, -83011.5723]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20469737486209216, distance: 1.2560174561054878 entropy 11.774952933136987
epoch: 78, step: 79
	action: tensor([[ -39616.6281, -100849.3762,  -36526.2433,  -46770.1259,   16882.8730,
           16267.3512,  -76465.3628]], dtype=torch.float64)
	q_value: tensor([[-35.0148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7384066392313804, distance: 1.5088024452823365 entropy 12.519372191880262
epoch: 78, step: 80
	action: tensor([[-57610.9290, -21990.0304,   2295.7294,  65852.4989,   -233.4049,
         -21907.6545, -17171.1565]], dtype=torch.float64)
	q_value: tensor([[-29.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16813859863594205, distance: 1.2368125417825677 entropy 12.126198776389142
epoch: 78, step: 81
	action: tensor([[-13534.2805,  50460.4939,  71200.5309, -39511.6881, -83792.1473,
          67597.0407,  13412.5907]], dtype=torch.float64)
	q_value: tensor([[-31.7935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48693616206538015, distance: 0.819676333778122 entropy 12.280989526720509
epoch: 78, step: 82
	action: tensor([[  8952.4839,  12605.8620,  43211.0956,  60179.5122,  20500.8759,
         -13030.1543, -12068.7709]], dtype=torch.float64)
	q_value: tensor([[-32.3635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.89746876338435
epoch: 78, step: 83
	action: tensor([[ 27846.0688, -25429.8855, -33823.2118,   -352.7468,   7341.4618,
         -20647.4554, -54566.0009]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.774952933136987
epoch: 78, step: 84
	action: tensor([[ -4498.0876,  -8071.5025, -47560.5426, -40373.0363,  62635.4560,
         -21524.7793, -33027.6149]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5207022113285931, distance: 1.411168202361081 entropy 11.774952933136987
epoch: 78, step: 85
	action: tensor([[-43367.3868, 113552.7558, 145715.7092,  32934.8878,   9391.9727,
         -35637.3646, -66432.6004]], dtype=torch.float64)
	q_value: tensor([[-33.7192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04183940431878863, distance: 1.168038298263581 entropy 12.35285835771308
epoch: 78, step: 86
	action: tensor([[ -5222.6981, -22594.6495, -31094.6468,   3825.5102,  29688.3418,
          -7376.8223, -33468.0179]], dtype=torch.float64)
	q_value: tensor([[-29.9939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4948037227689104, distance: 1.3991000710942596 entropy 12.137730929283858
epoch: 78, step: 87
	action: tensor([[-22820.3718,  29791.3101,  49906.5300, -31238.2413,  71813.9882,
          47883.3247,  23957.1931]], dtype=torch.float64)
	q_value: tensor([[-31.6954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3469376032989877, distance: 1.3280989460377433 entropy 12.229063722529256
epoch: 78, step: 88
	action: tensor([[-22754.2041,   1594.6919,   7486.8383,  44022.2702, -41185.9944,
          12873.3495, -32330.5600]], dtype=torch.float64)
	q_value: tensor([[-33.2621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2508534074570353, distance: 0.9904671985458228 entropy 12.002408781144121
epoch: 78, step: 89
	action: tensor([[-65246.0471, -56923.4889, -34168.4541, -35938.3924, -34779.9654,
          26852.3885, -29947.0454]], dtype=torch.float64)
	q_value: tensor([[-34.2375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11160730883900305, distance: 1.2065140677500634 entropy 12.237401378477227
epoch: 78, step: 90
	action: tensor([[ 43862.6029, -21395.1879,  95077.3680,  51254.6006,  19417.4290,
         -23153.4537, -36138.7489]], dtype=torch.float64)
	q_value: tensor([[-32.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5415328891304367, distance: 0.7748377767050759 entropy 12.301419684272057
epoch: 78, step: 91
	action: tensor([[ -46427.4232, -107097.0761,   33179.5940,   -5478.4102,   69998.6837,
           62549.8820,    8386.6790]], dtype=torch.float64)
	q_value: tensor([[-33.2941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3261774473252985, distance: 1.3178243026730045 entropy 12.23509928270298
epoch: 78, step: 92
	action: tensor([[ -6081.7369,  -9489.1090,   1789.3901,  77046.4054, -51717.9251,
         -85552.5894,  93119.9362]], dtype=torch.float64)
	q_value: tensor([[-31.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7965515560739658, distance: 1.5338275553975034 entropy 12.306878620346039
epoch: 78, step: 93
	action: tensor([[ 13045.9455,  43880.6614,  -1064.2441, -14941.4352, -16226.4667,
          42548.0462,  28900.5254]], dtype=torch.float64)
	q_value: tensor([[-29.5952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.408206736937071
epoch: 78, step: 94
	action: tensor([[ -9094.9483,  28837.1467, -33064.4755, -49206.1727,  69582.8767,
          32894.0711,  21968.5558]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5542163941054259, distance: 0.7640446511405686 entropy 11.774952933136987
epoch: 78, step: 95
	action: tensor([[ 49810.1378,   1964.0963, -12895.6774,  39765.4594,  13875.0349,
         -81811.8019,  82377.8693]], dtype=torch.float64)
	q_value: tensor([[-43.5802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.324049095734713
epoch: 78, step: 96
	action: tensor([[-45795.0860, -49601.1402, -34820.1813, -15932.3580,  43715.7543,
         -24757.0144, -31185.9779]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23613865753486984, distance: 1.2723022271116533 entropy 11.774952933136987
epoch: 78, step: 97
	action: tensor([[-59720.8838,  87272.6925, -31410.2239,  88481.8353,   8701.7509,
         -28983.5167,   9081.2073]], dtype=torch.float64)
	q_value: tensor([[-30.5244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4179240519594768, distance: 0.8730648835408715 entropy 12.259150782049451
epoch: 78, step: 98
	action: tensor([[  8781.0469, -34498.6507,  16321.2242,  23026.8206,  72784.2014,
           1024.8161,  11467.7551]], dtype=torch.float64)
	q_value: tensor([[-31.8318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36920669994560384, distance: 0.9088667787012943 entropy 11.947449862680674
epoch: 78, step: 99
	action: tensor([[  -328.9048, -28480.3156, -74140.4684,   5579.4356,  39031.1778,
          -7097.3794,  66460.9779]], dtype=torch.float64)
	q_value: tensor([[-29.1009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12550113264708962, distance: 1.2140306784570103 entropy 12.171995006555534
epoch: 78, step: 100
	action: tensor([[-41179.2449,  73161.5914, -14553.7838, -52128.1770, -34339.0084,
          61518.6877,  19358.2001]], dtype=torch.float64)
	q_value: tensor([[-31.4593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5692740764682029, distance: 0.7510298502298751 entropy 12.324482108088477
epoch: 78, step: 101
	action: tensor([[  23123.2411, -116842.1161,   12146.5382,     701.2870,  -27052.2924,
           -7368.8237,   85194.1658]], dtype=torch.float64)
	q_value: tensor([[-41.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5292573609071198, distance: 0.7851424560087273 entropy 12.308220823854814
epoch: 78, step: 102
	action: tensor([[ 19497.6621, -78330.3436, -76876.1617,  25407.7300,  61026.1748,
         -69182.5219, -67889.9617]], dtype=torch.float64)
	q_value: tensor([[-31.6698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22263864899792418, distance: 1.0089465636539632 entropy 12.136085650386615
epoch: 78, step: 103
	action: tensor([[ -88009.8521,  -84584.6943,  -10893.9518,  -34369.1377,   31083.6520,
         -116748.0347,   31414.9364]], dtype=torch.float64)
	q_value: tensor([[-32.0183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5399821670051321, distance: 1.4200856508077384 entropy 12.239944372435415
epoch: 78, step: 104
	action: tensor([[-31988.8480,   9971.5314, -59132.3663,  42240.4814, -20700.8880,
         -75694.7186,  49614.8219]], dtype=torch.float64)
	q_value: tensor([[-25.3030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010196844283226092, distance: 1.1501638064413455 entropy 11.917927364959109
epoch: 78, step: 105
	action: tensor([[ 71643.9919,   6998.5684,  26990.0984,  32214.2533, -15437.5827,
          29941.1174,  13843.5502]], dtype=torch.float64)
	q_value: tensor([[-33.2679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41061415740875506, distance: 0.8785298922374549 entropy 12.247483593948584
epoch: 78, step: 106
	action: tensor([[ 20172.9338, -45802.2165,  40551.1552, -25776.7227, -95610.4201,
         -63583.6136,  -5876.2033]], dtype=torch.float64)
	q_value: tensor([[-32.3808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31393143865211237, distance: 1.3117257538258766 entropy 12.162499726774273
epoch: 78, step: 107
	action: tensor([[  8670.9855, -33403.0379,  -5615.3082,  36849.8111,  50628.4500,
         -42962.7702,  94347.0265]], dtype=torch.float64)
	q_value: tensor([[-28.7928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22149090489704348, distance: 1.2647416245354632 entropy 12.110828399153894
epoch: 78, step: 108
	action: tensor([[ 38888.0912, -69181.3087,   2771.0362,  80205.6434, -54358.0017,
          17356.8913, -25467.3990]], dtype=torch.float64)
	q_value: tensor([[-32.3673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46448648711609264, distance: 0.83741726467045 entropy 12.211880974549912
epoch: 78, step: 109
	action: tensor([[-67036.3295,  10910.4512, -23158.4557,   4646.6639,  83714.7684,
         -31437.4835,   4319.7369]], dtype=torch.float64)
	q_value: tensor([[-33.5478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4416478255316567, distance: 0.8550879752551311 entropy 12.245967654742758
epoch: 78, step: 110
	action: tensor([[-108654.2969,  -53511.4100,  -29167.3547,  -79053.2884,   50519.8338,
           54637.8666, -105048.0034]], dtype=torch.float64)
	q_value: tensor([[-30.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.367148999772928
epoch: 78, step: 111
	action: tensor([[-36280.0610, -18321.6707, -21271.3398,  56609.4266,  -3068.5344,
           3789.2168,   9331.7167]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08132339475378436, distance: 1.1899658357638745 entropy 11.774952933136987
epoch: 78, step: 112
	action: tensor([[  93742.9962,  -26605.2545,  -59559.4582,  -40301.5765,  -11361.6482,
         -139716.5625,  -90324.1556]], dtype=torch.float64)
	q_value: tensor([[-32.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2731853306466665, distance: 0.9755926850440404 entropy 12.469008365363875
epoch: 78, step: 113
	action: tensor([[-34866.6146, -82267.7424,  19179.0757,  64405.8077,   9384.0886,
          29316.0573,  43138.6007]], dtype=torch.float64)
	q_value: tensor([[-30.7050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48945806802472447, distance: 1.3965961287979245 entropy 12.256324681895075
epoch: 78, step: 114
	action: tensor([[  34294.2579, -162586.9135,   14844.0441,   -2282.6400,   13469.2695,
          131102.9109,   21606.5417]], dtype=torch.float64)
	q_value: tensor([[-31.4571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2750524258348833, distance: 0.9743387919910163 entropy 12.528892726962152
epoch: 78, step: 115
	action: tensor([[-68337.4529,  14408.2276,  -8632.0286,   4862.0046, -17371.2318,
          10738.6188,   1049.3207]], dtype=torch.float64)
	q_value: tensor([[-32.1237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.286799028941257
epoch: 78, step: 116
	action: tensor([[  -196.5195, -61910.5443, -50993.6121,  20272.5097, -50594.2731,
         -17399.7940,   7062.6992]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5703173458691164, distance: 0.7501197570092152 entropy 11.774952933136987
epoch: 78, step: 117
	action: tensor([[-45031.3806, -84753.4096, -14284.2162,  32603.6001,  36373.2140,
          12803.8632,  43853.6080]], dtype=torch.float64)
	q_value: tensor([[-27.4807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21207334610581974, distance: 1.0157798401234244 entropy 12.12748130602546
epoch: 78, step: 118
	action: tensor([[ -7776.5957, -40019.1300, -55272.8527,  -8901.7439,  -7238.2329,
          53752.0227, -21759.8108]], dtype=torch.float64)
	q_value: tensor([[-32.1745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9117211123502629, distance: 1.582227619975189 entropy 12.350599007594598
epoch: 78, step: 119
	action: tensor([[  1138.6423,  58399.3075, -34226.3356,   3266.9844, -23317.6186,
         -28994.1039, -11048.2421]], dtype=torch.float64)
	q_value: tensor([[-27.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.960298735108367
epoch: 78, step: 120
	action: tensor([[-19250.9334,  -4824.1877, -15079.3215,  11489.6475,  -9567.8480,
          62341.1412,  12546.3624]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06703005143474328, distance: 1.1820749626958496 entropy 11.774952933136987
epoch: 78, step: 121
	action: tensor([[-71986.5443, -11115.2696,   3718.4586,  59194.2437,   9725.3043,
         -11340.3239, -11947.2767]], dtype=torch.float64)
	q_value: tensor([[-29.6877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8222161942767676, distance: 1.5447444524950968 entropy 12.227472976519763
epoch: 78, step: 122
	action: tensor([[  7861.9739, -53159.0327, -18704.1311,  61783.8454,  16290.4020,
         -29780.8277, -16537.0092]], dtype=torch.float64)
	q_value: tensor([[-30.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2509625585101566, distance: 0.9903950401210851 entropy 12.42264138078871
epoch: 78, step: 123
	action: tensor([[ -2419.7612,   8914.9078,  -8957.7461, -10510.9560,  32369.0444,
         -63552.2969,  -2120.2148]], dtype=torch.float64)
	q_value: tensor([[-30.7864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.187285189028414
epoch: 78, step: 124
	action: tensor([[-37234.9570,   9849.7268, -37147.8482,   5343.0536, -13882.4004,
          40827.7809,  22956.6728]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060574786277996284, distance: 1.1091436557400425 entropy 11.774952933136987
epoch: 78, step: 125
	action: tensor([[ 52123.9097, -85201.1918,  18436.5584,  13757.9573, -63621.6155,
          68963.9491,   -215.8940]], dtype=torch.float64)
	q_value: tensor([[-32.1770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.357015819182893
epoch: 78, step: 126
	action: tensor([[ -1717.6734, -70245.6168,  -6665.7492,   -171.9098,   8597.9526,
          35533.1430,   3116.6041]], dtype=torch.float64)
	q_value: tensor([[-32.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0228695534644473, distance: 1.1311832511816151 entropy 11.774952933136987
epoch: 78, step: 127
	action: tensor([[-29625.0764, -43559.7204,   8630.5465,   9551.9765,  26968.7765,
         -11828.0752, -25120.1165]], dtype=torch.float64)
	q_value: tensor([[-30.3879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16868607340989827, distance: 1.2371023380228239 entropy 12.141983843854605
LOSS epoch 78 actor 393.447018847547 critic 204.14861894784121
epoch: 79, step: 0
	action: tensor([[-98312.5888, -46152.5736, -28468.9155,  68164.8940,  -4218.5919,
          29106.5507, -65234.0668]], dtype=torch.float64)
	q_value: tensor([[-33.8898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.236607363451101
epoch: 79, step: 1
	action: tensor([[-10293.6038,   6602.8987,  11909.3903,  28439.0506,  27341.5548,
         -35303.2343,   2208.1033]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026816781958699965, distance: 1.1595865578374185 entropy 11.809297244595934
epoch: 79, step: 2
	action: tensor([[  16398.6890, -151967.0736,  -45637.8594, -106675.8763,   71257.3479,
           -3241.5845,   -9958.9753]], dtype=torch.float64)
	q_value: tensor([[-45.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6019935878776859, distance: 0.721941034952577 entropy 12.507163685482434
epoch: 79, step: 3
	action: tensor([[ -50436.9948,   19368.0785, -124300.0780,   87851.9642, -119663.3779,
           29985.1546,    3214.7706]], dtype=torch.float64)
	q_value: tensor([[-32.5080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.23639102837725
epoch: 79, step: 4
	action: tensor([[-52031.6811,  28628.8530, -43150.9187,  23739.4702,  31903.4568,
          -8406.2754,  11963.8229]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.809297244595934
epoch: 79, step: 5
	action: tensor([[ 40923.9424,  -1237.5320,  18713.4766,  40973.1222,   3573.9173,
         -13061.2400,  -6825.3310]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1608613653979134, distance: 1.048270952441262 entropy 11.809297244595934
epoch: 79, step: 6
	action: tensor([[  17854.2039,  -34742.6545,   41609.4149,   30384.1948,    3196.3354,
           -6329.8323, -110244.5784]], dtype=torch.float64)
	q_value: tensor([[-34.4231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0013424594765878428, distance: 1.1435758781409 entropy 12.354419820386514
epoch: 79, step: 7
	action: tensor([[ -38196.8002,  -85696.3653, -106967.5043,   46147.5265,   50506.6610,
           17340.4028,   31945.9680]], dtype=torch.float64)
	q_value: tensor([[-38.7106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5421043000793739, distance: 1.4210637703274929 entropy 12.271043867333976
epoch: 79, step: 8
	action: tensor([[-45852.0654,  42672.0104,  16472.1037,  54649.4919,   6831.4615,
         -37324.0138,  46868.8645]], dtype=torch.float64)
	q_value: tensor([[-39.6427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1821682070295958, distance: 1.2442175730993617 entropy 12.500394364683787
epoch: 79, step: 9
	action: tensor([[  58161.6593,  -54481.1761,  -21639.3814,   96326.1434, -103508.8677,
           74065.4212,   22110.2702]], dtype=torch.float64)
	q_value: tensor([[-41.5830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5495288391431985, distance: 0.7680512310975915 entropy 12.415837121907781
epoch: 79, step: 10
	action: tensor([[ 16982.7348, -41624.2856, -10277.0206,  42715.7368,   2840.1756,
         103866.6901, -79476.2781]], dtype=torch.float64)
	q_value: tensor([[-31.9509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4215363554168102, distance: 0.870351591958329 entropy 12.143343516523627
epoch: 79, step: 11
	action: tensor([[ 44483.4604,  18294.3070, -33823.9856, 110787.7351,  11356.4941,
          54190.5393, 123611.2051]], dtype=torch.float64)
	q_value: tensor([[-42.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7348862989197538, distance: 0.5892136231945867 entropy 12.248557526170952
epoch: 79, step: 12
	action: tensor([[-1.3357e+05, -1.1429e+05, -1.0281e+02, -1.2090e+04,  1.6150e+04,
          1.7686e+04,  3.5075e+04]], dtype=torch.float64)
	q_value: tensor([[-42.3363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.260585585825089
epoch: 79, step: 13
	action: tensor([[ 18436.8235, -26573.9021,  -6130.9132,  47912.8776,  46959.2699,
          22482.9355, -12891.5238]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.51663642528607, distance: 0.7955979458519279 entropy 11.809297244595934
epoch: 79, step: 14
	action: tensor([[-46427.8718,  46933.0003,  50633.2188,  55804.1738, -45346.1460,
         -35903.5406,  99825.1874]], dtype=torch.float64)
	q_value: tensor([[-36.7285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17762464383591414, distance: 1.037747598468214 entropy 12.151203692985904
epoch: 79, step: 15
	action: tensor([[ 11313.7369, -43222.3591, -47022.3019,  94590.6425, -66466.5712,
          99618.3407, -43562.7441]], dtype=torch.float64)
	q_value: tensor([[-39.5598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.307771477921705
epoch: 79, step: 16
	action: tensor([[ 19033.6705, -44645.3434, -45547.1647, -16599.8425,  42051.7661,
         -38919.4247,  -5978.6839]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.809297244595934
epoch: 79, step: 17
	action: tensor([[ 14653.3707, -31245.9976,  14982.8853, -11183.1515,  -4331.8827,
         -41622.6227, -27465.5423]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4551455621333683, distance: 0.8446891963807369 entropy 11.809297244595934
epoch: 79, step: 18
	action: tensor([[ 20175.6540,  40603.2426, -30578.0983,   5961.7759, -11281.2503,
          18680.1282,  63377.6091]], dtype=torch.float64)
	q_value: tensor([[-37.0354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.673108150003268, distance: 0.6542726101049594 entropy 12.430821070920109
epoch: 79, step: 19
	action: tensor([[-55330.9639, -37637.8615,  51730.6073,  -1279.4029, -41214.3357,
         -59960.4928,  64401.5403]], dtype=torch.float64)
	q_value: tensor([[-31.2193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6556760591456943, distance: 1.4724629562825333 entropy 12.159011708658738
epoch: 79, step: 20
	action: tensor([[ -60280.4799,   11323.7465,   37829.5379,  -18826.4808, -104432.8084,
           24155.7730,   73126.2905]], dtype=torch.float64)
	q_value: tensor([[-33.8932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07735076962608889, distance: 1.1877779435755633 entropy 12.30498309861244
epoch: 79, step: 21
	action: tensor([[21151.5713, 48559.1481, 10517.1622, 44831.4525, -4053.4363, 75811.1889,
         18323.8768]], dtype=torch.float64)
	q_value: tensor([[-44.6178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5411842240695552, distance: 0.7751323534692883 entropy 12.24211143011048
epoch: 79, step: 22
	action: tensor([[ 36743.3767,   1973.0887,  16740.7967,   5194.0576,  38563.6040,
         -14411.6556, -44088.1700]], dtype=torch.float64)
	q_value: tensor([[-33.1201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.988604735605264
epoch: 79, step: 23
	action: tensor([[ 40468.8620, -13490.3963, -21769.9522,  26919.3806, -38394.0792,
          -4797.9504,  82956.0561]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5500301010765822, distance: 0.7676237875241558 entropy 11.809297244595934
epoch: 79, step: 24
	action: tensor([[ 83697.3017,  78179.7783, -75950.3684,  86203.1213,  83517.7998,
         -37929.9430, -19120.9733]], dtype=torch.float64)
	q_value: tensor([[-39.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41821783303858584, distance: 0.8728445322996974 entropy 12.177854575927658
epoch: 79, step: 25
	action: tensor([[-29833.7023, -27847.0035, -13029.9647, -77785.8760,  18783.1060,
         -55209.3032,   8326.5207]], dtype=torch.float64)
	q_value: tensor([[-36.7528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.04699588881709
epoch: 79, step: 26
	action: tensor([[ 26784.7984, -43804.6674, -35690.0006, -15893.6460,  20919.6143,
          22978.6125, -51543.2329]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.809297244595934
epoch: 79, step: 27
	action: tensor([[-20071.4832, -40382.1553,  -8397.0936,  55228.2092,  78279.3623,
         -70405.7304, -29990.0193]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12554520120186918, distance: 1.2140544456715503 entropy 11.809297244595934
epoch: 79, step: 28
	action: tensor([[-45298.2140, 104390.7911,  15670.2671, -43283.7798,  44594.2579,
         161989.0819,  37300.3801]], dtype=torch.float64)
	q_value: tensor([[-38.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37542690564078285, distance: 1.3420708733599855 entropy 12.50857898662677
epoch: 79, step: 29
	action: tensor([[-43899.5671, -58599.4873, -27642.9193,  56850.1178, -52168.5962,
         -39031.1988,  21539.0893]], dtype=torch.float64)
	q_value: tensor([[-37.4882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2893229463309759, distance: 1.299384103169942 entropy 12.116625881955487
epoch: 79, step: 30
	action: tensor([[  3516.7734,  -9156.1252,  24748.2670,  31190.1860, -14190.7912,
          41635.7572, -87709.0502]], dtype=torch.float64)
	q_value: tensor([[-37.4389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20492524072889462, distance: 1.2561362369627862 entropy 12.332872473554005
epoch: 79, step: 31
	action: tensor([[ 14977.2142,  68553.8898, -25040.3832,  53602.6182, -28328.9880,
         -59824.8187,   2584.7868]], dtype=torch.float64)
	q_value: tensor([[-38.4608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.375206368772472
epoch: 79, step: 32
	action: tensor([[-17421.4385,   5945.4146, -29566.5929,  11223.1949,  -9663.9806,
          21324.7521,  58774.2121]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2663354688551193, distance: 0.9801791387088771 entropy 11.809297244595934
epoch: 79, step: 33
	action: tensor([[-102135.5162,  -35019.1408,  -93417.5681,   88115.8117,  -77075.6521,
           21435.8984,  -57337.1174]], dtype=torch.float64)
	q_value: tensor([[-41.2202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12686713961678775, distance: 1.0692933351011626 entropy 12.342082874545278
epoch: 79, step: 34
	action: tensor([[ 12158.5791, -31925.5135, -27715.6935,  31617.9027,  65559.5339,
          40096.6517,  10695.9089]], dtype=torch.float64)
	q_value: tensor([[-34.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42075215635070173, distance: 0.8709413418498301 entropy 12.260014891849641
epoch: 79, step: 35
	action: tensor([[ 17824.1430, -52875.8012, -70745.1030, -40131.0774,  52086.7974,
           -885.1364, -10472.7135]], dtype=torch.float64)
	q_value: tensor([[-34.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02345653536326142, distance: 1.1576876359026458 entropy 12.130565713577804
epoch: 79, step: 36
	action: tensor([[-73714.5441, -40621.8733, -27653.3837,   4747.2734,  19925.3206,
         -47312.7193,  -6712.3910]], dtype=torch.float64)
	q_value: tensor([[-30.0845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49301949740505613, distance: 1.3982648259210382 entropy 12.026492153606494
epoch: 79, step: 37
	action: tensor([[-57835.3141, -25918.8685,  46110.9520,  -5317.6866,  35169.1623,
         -45658.2043, -10202.8098]], dtype=torch.float64)
	q_value: tensor([[-32.0707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6087218436936639, distance: 1.4514336003793238 entropy 12.152656539939327
epoch: 79, step: 38
	action: tensor([[ -10106.3202,  -47480.5080,   -3428.5910,   19359.9986, -115627.3930,
          -56859.9128,  -58171.1755]], dtype=torch.float64)
	q_value: tensor([[-33.5737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1369902893589201, distance: 1.06307654064058 entropy 12.247537009021688
epoch: 79, step: 39
	action: tensor([[-24101.4142, -20228.2447,  51987.0389,  84638.9191, -42209.1763,
          64056.8524,   -237.8222]], dtype=torch.float64)
	q_value: tensor([[-36.5871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4384206154527861, distance: 1.3724598316837118 entropy 12.457097875090161
epoch: 79, step: 40
	action: tensor([[-31163.6529,  21376.4676,   4721.7641,  72407.2301,  22684.2688,
          33434.2083, 119635.0201]], dtype=torch.float64)
	q_value: tensor([[-36.1316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10675664087425796, distance: 1.2038787857049926 entropy 12.30667779517969
epoch: 79, step: 41
	action: tensor([[-79509.9951, -69757.3572,  94257.5536, -71425.2253,  55522.4786,
         -42938.5290, 107676.0855]], dtype=torch.float64)
	q_value: tensor([[-44.6284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0163734888103741, distance: 1.624958158276044 entropy 12.511660233780342
epoch: 79, step: 42
	action: tensor([[-43721.5928, -26444.9288,  32109.6612,  24077.6476, -54107.6110,
         -38026.4908,  79194.0406]], dtype=torch.float64)
	q_value: tensor([[-38.7105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08168941843823474, distance: 1.0966081963603214 entropy 12.427287735102952
epoch: 79, step: 43
	action: tensor([[ -44996.0232, -120503.5922,  -11445.1548,  131347.4561,  -90142.0072,
          -12332.3961,  -51707.1626]], dtype=torch.float64)
	q_value: tensor([[-38.1635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08117215051696625, distance: 1.0969170027690596 entropy 12.508048140639023
epoch: 79, step: 44
	action: tensor([[   1083.6682,   14132.9930,  -31554.3302,   25958.1689, -122478.3071,
          -11727.2106,   -6539.3491]], dtype=torch.float64)
	q_value: tensor([[-43.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.376459482969304, distance: 0.9036266539044872 entropy 12.541556800382752
epoch: 79, step: 45
	action: tensor([[ 10564.5409,  58985.6708, -11733.5823, -13419.0128,  -2687.7982,
          12357.4103, -36641.3768]], dtype=torch.float64)
	q_value: tensor([[-33.5078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6908053012932507, distance: 0.6363158084069659 entropy 12.030652530163605
epoch: 79, step: 46
	action: tensor([[ -8516.2589, -22666.5500, 112971.2324,  35416.7845, -75577.6309,
         -23626.0607, -49163.8070]], dtype=torch.float64)
	q_value: tensor([[-37.8922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1950209278876348, distance: 1.0267128277284836 entropy 12.09657410968332
epoch: 79, step: 47
	action: tensor([[ 51663.7540, -26007.4778, -52652.8769, -69822.2994,   7868.6185,
         -19402.1328, -94770.2131]], dtype=torch.float64)
	q_value: tensor([[-36.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1449551644957633, distance: 1.058159504956753 entropy 12.32297980741635
epoch: 79, step: 48
	action: tensor([[ 19353.9618, -45247.2180,  19369.7317,  65917.6063,   7331.4039,
         -18107.3938, -51089.2382]], dtype=torch.float64)
	q_value: tensor([[-32.1510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41154034302703746, distance: 0.8778393414917339 entropy 11.986997913204542
epoch: 79, step: 49
	action: tensor([[  4862.8946, -47151.8564,  11701.9934,   1545.0771, -11402.2538,
          35195.9881,  26570.0751]], dtype=torch.float64)
	q_value: tensor([[-35.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0009219519360206263, distance: 1.1448716476703256 entropy 12.221451608523543
epoch: 79, step: 50
	action: tensor([[-35078.0160, -71612.5210,  72504.5315,  10335.6372,  77122.4358,
         -29753.8271, -79160.1757]], dtype=torch.float64)
	q_value: tensor([[-44.4168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4192667969393232, distance: 0.8720572999533033 entropy 12.378480585680952
epoch: 79, step: 51
	action: tensor([[-13041.3789,  16561.5710,  87847.6497, -55298.5900, -40330.8297,
          67828.1777,  32255.5380]], dtype=torch.float64)
	q_value: tensor([[-40.6037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07429777381077551, distance: 1.1860937874593451 entropy 12.452715700126081
epoch: 79, step: 52
	action: tensor([[ 35836.2218, -17879.0875,  -4887.2197, -63060.5543, -28144.2444,
          30513.1490, -45620.2645]], dtype=torch.float64)
	q_value: tensor([[-35.3289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5022891133905225, distance: 0.8073191670154296 entropy 12.131583561741573
epoch: 79, step: 53
	action: tensor([[-56785.7908, -34644.8429, -20244.0493,  30397.7732,  68777.8817,
         -34218.7542,  -9375.1493]], dtype=torch.float64)
	q_value: tensor([[-34.4851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5802581627350643, distance: 1.4385359326293432 entropy 12.119558930539256
epoch: 79, step: 54
	action: tensor([[  9716.5486, -40111.2937, -25645.7384,  27660.2666,  28996.1027,
          73971.1674,  -4293.1448]], dtype=torch.float64)
	q_value: tensor([[-30.3276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.071826195639636
epoch: 79, step: 55
	action: tensor([[  4818.9472, -13298.9705,   9834.7910, -29000.2049,  54171.1598,
         -25404.6225,  13069.4439]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2492168010048701, distance: 1.2790148931409888 entropy 11.809297244595934
epoch: 79, step: 56
	action: tensor([[-14073.7704, -12031.7302,  65981.3537, -29548.3902, -38896.2738,
         -50020.8685, -45982.7638]], dtype=torch.float64)
	q_value: tensor([[-31.3008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8011153460542322, distance: 1.5357745150785376 entropy 12.173800885077517
epoch: 79, step: 57
	action: tensor([[ 13875.0421,  65423.2110,  51482.4586,  58653.4439,  -9111.4748,
           9668.7658, 119971.3660]], dtype=torch.float64)
	q_value: tensor([[-37.6243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.389689247360797
epoch: 79, step: 58
	action: tensor([[-47258.7290, -54597.5268,  11176.0387,  10158.2182, -60654.9456,
          -8629.7804,   1759.7651]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20776237125962638, distance: 1.2576142237126346 entropy 11.809297244595934
epoch: 79, step: 59
	action: tensor([[ -1659.0355, -21572.1955,   8402.5523, -32954.8840, -17100.1180,
          74264.0205, -12196.1750]], dtype=torch.float64)
	q_value: tensor([[-33.9960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09681199260644813, distance: 1.0875413398867317 entropy 12.323538469281624
epoch: 79, step: 60
	action: tensor([[-42351.9002, -48616.9681,  38136.4763,  16224.0912, -65228.6588,
          27273.5157,  66303.8964]], dtype=torch.float64)
	q_value: tensor([[-38.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7709148728594477, distance: 1.5228444187114718 entropy 12.349147882824672
epoch: 79, step: 61
	action: tensor([[-101192.1182,  -85991.9086, -109175.3408,  198846.2663,   48897.3188,
          -84621.0822,  -45535.4819]], dtype=torch.float64)
	q_value: tensor([[-36.4181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10357421623209029, distance: 1.2021466923591662 entropy 12.491298436065255
epoch: 79, step: 62
	action: tensor([[-42852.5036, -60458.0361,  52438.1120,  21415.3107, -60890.9435,
         -60630.3434,  53341.0130]], dtype=torch.float64)
	q_value: tensor([[-36.2802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6250490493544665, distance: 1.4587804359284968 entropy 12.31718343697648
epoch: 79, step: 63
	action: tensor([[  4127.3196,  18846.1662, -83938.5470,  24089.8708,  98954.6855,
         124036.7906,  11502.1481]], dtype=torch.float64)
	q_value: tensor([[-39.9923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.266680464042205, distance: 0.9799486540434034 entropy 12.531189497499167
epoch: 79, step: 64
	action: tensor([[  9534.4380, -25213.4736,  25934.1546,  29234.6862, -63596.4127,
          59908.5489,  36246.3634]], dtype=torch.float64)
	q_value: tensor([[-37.1824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18557674748626884, distance: 1.032718068672026 entropy 12.2051796635655
epoch: 79, step: 65
	action: tensor([[-86739.0911,  -7174.8002, -59350.8760,  56195.9291,  40778.5362,
         -92996.7577,  38253.0421]], dtype=torch.float64)
	q_value: tensor([[-41.4118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6858462567097627, distance: 1.4858182084394473 entropy 12.412654091240197
epoch: 79, step: 66
	action: tensor([[  3142.7895,  45438.9269,  24748.4315, -38374.1911, -13254.2390,
         -13378.6862,  -8603.2254]], dtype=torch.float64)
	q_value: tensor([[-31.0795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.14153236719868
epoch: 79, step: 67
	action: tensor([[ -22826.1525, -111738.6578,  -14198.9041,   68574.5174,    3082.4966,
           -3624.8479,  -27701.5092]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1688566451590573, distance: 1.043265057798735 entropy 11.809297244595934
epoch: 79, step: 68
	action: tensor([[ -27636.0460,  -44566.9207, -101074.4948, -109473.2506,   59639.2906,
           72167.6339,    4446.4054]], dtype=torch.float64)
	q_value: tensor([[-35.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1433718555225743, distance: 1.6753496340525014 entropy 12.353531132942345
epoch: 79, step: 69
	action: tensor([[ 14242.3770, -49495.7788, -19804.6331,  32342.7758,  21327.5908,
         -68118.4544, -10387.0812]], dtype=torch.float64)
	q_value: tensor([[-30.0850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4128346034935133, distance: 0.876873448465095 entropy 12.011467877379962
epoch: 79, step: 70
	action: tensor([[ 19405.0408, -65563.7758,  46895.7137, -32109.4331, -10727.2817,
          30717.8596,  71063.7409]], dtype=torch.float64)
	q_value: tensor([[-37.0420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5955885467313784, distance: 1.4454968459798405 entropy 12.225420056000916
epoch: 79, step: 71
	action: tensor([[ 42011.9582,   9992.2153,  58707.2355,  -8300.7009, -18590.5644,
         109166.7573, -75431.2143]], dtype=torch.float64)
	q_value: tensor([[-32.4445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5780575015869678, distance: 0.7433328540745834 entropy 12.062385089831809
epoch: 79, step: 72
	action: tensor([[-47448.4272,  -1394.6106, -62552.8303,  -1339.2276,  99904.9325,
         -59658.5725, -18304.0050]], dtype=torch.float64)
	q_value: tensor([[-46.1090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.275249588891569
epoch: 79, step: 73
	action: tensor([[-26782.8375, -82776.5274,  20328.4759, -18961.4147,   -117.5700,
          -6045.0307, -24879.1802]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07818245523209466, distance: 1.1882363213251008 entropy 11.809297244595934
epoch: 79, step: 74
	action: tensor([[-24311.0547,  14265.6267, -46143.2698, -80336.4427, -34996.4330,
          40747.7233, -11846.2732]], dtype=torch.float64)
	q_value: tensor([[-40.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.435109631807732
epoch: 79, step: 75
	action: tensor([[-70784.7549, -48207.6988,  31563.9038,  37524.5822, -10739.4055,
         -48912.9726, -31514.9891]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4817532865211007, distance: 1.3929792361836861 entropy 11.809297244595934
epoch: 79, step: 76
	action: tensor([[ 29734.2665,  77701.9144,  13310.6252,  -7694.5328, 143643.7871,
         -54513.2330,  -7864.1618]], dtype=torch.float64)
	q_value: tensor([[-38.2005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.47924607576957
epoch: 79, step: 77
	action: tensor([[-19093.8051,  23990.6338,  32081.7086,  16372.1850,   1996.9324,
         -39770.5443, -27622.9500]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7316362470750006, distance: 0.5928142322076397 entropy 11.809297244595934
epoch: 79, step: 78
	action: tensor([[  2102.0770, -63047.7460, -12110.6745,  12700.9276,  36269.9503,
          29085.6489,  57105.8700]], dtype=torch.float64)
	q_value: tensor([[-35.2306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07750801912227612, distance: 1.1878646241040824 entropy 12.191808533079541
epoch: 79, step: 79
	action: tensor([[  6361.5562,  20132.9225, -66358.2476,  49039.3921, -35298.7001,
          18682.5377, 131696.9403]], dtype=torch.float64)
	q_value: tensor([[-34.3647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5032918986579148, distance: 0.8065054657502104 entropy 12.398075086393083
epoch: 79, step: 80
	action: tensor([[ 89637.2771, -61306.1781, 155430.3057,  73969.8156,  52660.8984,
          10094.0869, -24031.6743]], dtype=torch.float64)
	q_value: tensor([[-37.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3262872902239058, distance: 0.9392778123234719 entropy 12.21376363064812
epoch: 79, step: 81
	action: tensor([[-54077.7203, -76169.1095, -31589.1850,  -5637.5353, -65881.9279,
         -33500.3286, -28533.8017]], dtype=torch.float64)
	q_value: tensor([[-39.8949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5981923256017643, distance: 1.4466757902393301 entropy 12.375606894328127
epoch: 79, step: 82
	action: tensor([[  -2338.2288,  -78812.0929, -128665.2203,   10113.4686,   88115.6480,
          -84640.5414,   15203.6642]], dtype=torch.float64)
	q_value: tensor([[-36.9348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22849222165378635, distance: 1.2683610556771676 entropy 12.449282705323336
epoch: 79, step: 83
	action: tensor([[  4287.3849, -11404.3043, -27470.5240,  -5889.6971, -15729.4005,
           6259.2068, 105112.6840]], dtype=torch.float64)
	q_value: tensor([[-34.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06447909185751, distance: 1.1806611178595896 entropy 12.269766633543894
epoch: 79, step: 84
	action: tensor([[  16292.9106, -105082.1412,    2720.3584,  -31571.2487,  -51564.9342,
           76606.6310,  -21623.6203]], dtype=torch.float64)
	q_value: tensor([[-31.5297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26265011011625905, distance: 1.2858733742052186 entropy 12.066858291458828
epoch: 79, step: 85
	action: tensor([[ -73104.3416, -111231.6902,    3958.6129,  -29206.6748,  -50789.6929,
           -6571.7733,  -18849.1282]], dtype=torch.float64)
	q_value: tensor([[-36.5911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1823405657413415, distance: 1.2443082725110968 entropy 12.145344950846212
epoch: 79, step: 86
	action: tensor([[-79344.2123, -59543.5259,  74775.6637,  94846.6977, -18509.3209,
         -20639.7539, 116134.3355]], dtype=torch.float64)
	q_value: tensor([[-34.3238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7495257521231693, distance: 1.5136200188757938 entropy 12.281117513025647
epoch: 79, step: 87
	action: tensor([[ 13429.2490,   -985.6307, -62379.1787,  11713.0390, -38080.9759,
           1671.7691,  18947.1575]], dtype=torch.float64)
	q_value: tensor([[-34.0152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.214862659671777
epoch: 79, step: 88
	action: tensor([[ 58530.5330,  47163.8549,  46661.5835,   2332.9146,  16472.9332,
           6894.6012, -56681.6438]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19063661101245244, distance: 1.2486660664616702 entropy 11.809297244595934
epoch: 79, step: 89
	action: tensor([[  5027.7224, -18735.6898, -38213.1464,  20206.9771,  45225.0343,
          28030.8227,  -2921.8708]], dtype=torch.float64)
	q_value: tensor([[-24.4837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5220474938136653, distance: 0.7911322067624504 entropy 11.877514610362638
epoch: 79, step: 90
	action: tensor([[-91967.7736, -34718.1893,  -4383.7196,   2931.9105, -31381.7639,
           6502.6530, -15618.4229]], dtype=torch.float64)
	q_value: tensor([[-37.8729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06671761685632704, distance: 1.1819018897204914 entropy 12.144463906637759
epoch: 79, step: 91
	action: tensor([[  59329.2314, -145876.6033,  -56710.4778,  140804.8283,  -28895.0606,
           95168.4634,   27081.2890]], dtype=torch.float64)
	q_value: tensor([[-37.8393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13110948511256626, distance: 1.2170516665730318 entropy 12.388733859885084
epoch: 79, step: 92
	action: tensor([[-40390.8048, -37890.1375, -15171.8713,  56949.3322,  23334.2515,
         -13454.2833, -25145.0050]], dtype=torch.float64)
	q_value: tensor([[-38.8440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02096329361130045, distance: 1.156276655035101 entropy 12.138528415314934
epoch: 79, step: 93
	action: tensor([[ 11487.5590, -14794.9268,  -7032.3499,  -9622.8525, -33097.7593,
          53541.8902,  63336.9874]], dtype=torch.float64)
	q_value: tensor([[-32.6861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924844649470115, distance: 0.9625530696915495 entropy 12.21427363316697
epoch: 79, step: 94
	action: tensor([[  7951.8956, -61586.2224,  34113.2228, -25968.2372,  47649.3763,
         130548.2170,  50676.0851]], dtype=torch.float64)
	q_value: tensor([[-39.8915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3700396781373916, distance: 0.9082664899045019 entropy 12.322392205507468
epoch: 79, step: 95
	action: tensor([[ 41636.0310, -15233.1787, -34036.2679,   5891.3711,  23601.2644,
         112935.4581, -12036.3429]], dtype=torch.float64)
	q_value: tensor([[-38.9815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3429875490786878, distance: 1.3261501123800898 entropy 12.30558411201568
epoch: 79, step: 96
	action: tensor([[ 70085.6140, -90185.4188, -16835.6206,  35076.4996,  11540.2704,
          69945.9774,  36066.0204]], dtype=torch.float64)
	q_value: tensor([[-36.9228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21720416157524025, distance: 1.2625204095576772 entropy 12.10845736870573
epoch: 79, step: 97
	action: tensor([[  27799.7174,  -10950.1233,   -6298.8705,    8813.6632,  -82400.8480,
         -100311.8185,   59648.6870]], dtype=torch.float64)
	q_value: tensor([[-41.7162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3852264718267028, distance: 0.8972516644353205 entropy 12.32548957068494
epoch: 79, step: 98
	action: tensor([[155755.6278, -56815.5528, 103212.8603, -45086.8549,  91872.1395,
          25258.5703,   5627.2199]], dtype=torch.float64)
	q_value: tensor([[-42.9993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3152199351516405, distance: 1.3123687629800935 entropy 12.544267477785757
epoch: 79, step: 99
	action: tensor([[ -6789.8753, -23890.9254, -79988.3789,  33515.0304,  39647.2284,
         110145.2891,  31505.3833]], dtype=torch.float64)
	q_value: tensor([[-36.8614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13326114382955312, distance: 1.065370891276385 entropy 12.23185338051644
epoch: 79, step: 100
	action: tensor([[ -7651.3773,  31933.9700,  53149.4125,  19417.8919, -61947.2664,
          41938.5529, -33072.1174]], dtype=torch.float64)
	q_value: tensor([[-36.7072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1479735952058443, distance: 1.22609082541355 entropy 12.388641740290268
epoch: 79, step: 101
	action: tensor([[-95909.3547,  65947.6026,  99571.1313, -12413.5976,  10957.3899,
         -14075.9028,  47150.8020]], dtype=torch.float64)
	q_value: tensor([[-41.4926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2738624766944203, distance: 0.9751381175325979 entropy 12.387208496932336
epoch: 79, step: 102
	action: tensor([[-34128.4506, -11194.8171,  44308.9137, 158429.1667,  -3667.1693,
          41160.5058,  43188.0197]], dtype=torch.float64)
	q_value: tensor([[-34.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3737543120580089, distance: 1.3412546097084854 entropy 11.893081912111194
epoch: 79, step: 103
	action: tensor([[ 18668.1221, -27415.3145,  88552.9617,  92464.4559,  42631.9151,
          98549.5448,  47984.8329]], dtype=torch.float64)
	q_value: tensor([[-36.1530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5292025804950209, distance: 0.7851881382696007 entropy 12.39435922618286
epoch: 79, step: 104
	action: tensor([[ -66562.9222,  -19284.0822, -118058.0554,   76061.4966,   15794.5253,
            8282.7379,   10523.1299]], dtype=torch.float64)
	q_value: tensor([[-33.5310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06856739997253236, distance: 1.104415289389672 entropy 12.163750367929238
epoch: 79, step: 105
	action: tensor([[-64724.8046, -31635.8438,  25797.4974, 121453.1055,  43479.2473,
         -47010.6761,  23556.1986]], dtype=torch.float64)
	q_value: tensor([[-36.9499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8229834256220263, distance: 1.5450696201787664 entropy 12.423422557562905
epoch: 79, step: 106
	action: tensor([[-15390.5445,  39434.4490,  -3872.1974,  98327.8946, -41827.9192,
         -23633.4818,  22674.0429]], dtype=torch.float64)
	q_value: tensor([[-34.3601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21532899027185948, distance: 1.0136791110473102 entropy 12.28330520521296
epoch: 79, step: 107
	action: tensor([[-34957.1352, 105716.3067,  49162.2674,  63454.4704,  41154.5599,
          15693.6562,  -8024.8673]], dtype=torch.float64)
	q_value: tensor([[-38.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25549732575799766, distance: 0.9873925004710336 entropy 12.280540255951621
epoch: 79, step: 108
	action: tensor([[ 3.0664e+04, -9.8657e+04,  4.8459e+04, -2.6767e+04,  9.0851e+04,
          5.5053e+04,  3.6944e+01]], dtype=torch.float64)
	q_value: tensor([[-35.5955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3102571638558751, distance: 0.950386574118586 entropy 12.26600771571943
epoch: 79, step: 109
	action: tensor([[ 62667.8891, -65017.4518,  36203.2449,  73978.5937,  65273.1879,
         -30521.6949,  49214.1313]], dtype=torch.float64)
	q_value: tensor([[-37.4890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5606609572173917, distance: 0.7585017602770693 entropy 12.217629174090279
epoch: 79, step: 110
	action: tensor([[    920.0502,   54486.9310, -109836.1625,   21425.3005,  -94205.6192,
          -34188.8609,  -42694.4120]], dtype=torch.float64)
	q_value: tensor([[-36.1918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24731099060910944, distance: 0.9928061998530772 entropy 12.364975237438902
epoch: 79, step: 111
	action: tensor([[ 16801.0551,  48907.7144, -15564.5001,   1637.8142,  -6269.6702,
         -47497.7952,  -6217.7532]], dtype=torch.float64)
	q_value: tensor([[-35.5559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7802588880539343, distance: 0.5364291283151887 entropy 12.113098286778476
epoch: 79, step: 112
	action: tensor([[-32865.3475,  27615.0834, -31271.9905, -50252.2964,  25002.9747,
           8153.9704,  64470.6290]], dtype=torch.float64)
	q_value: tensor([[-37.7032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.13697190301892
epoch: 79, step: 113
	action: tensor([[ 66443.9119,  22884.1029,  -6175.3932,   3450.7439,  -8413.7864,
         -44812.7065,  17523.9555]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26188778723175077, distance: 0.9831457108560586 entropy 11.809297244595934
epoch: 79, step: 114
	action: tensor([[  20011.0577,   24843.0835, -102593.2094,   31267.8225,  -12211.3798,
           67999.9930,   45260.7496]], dtype=torch.float64)
	q_value: tensor([[-33.5319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.949743015175473
epoch: 79, step: 115
	action: tensor([[-39578.8855, -40058.6101,  -4706.3701,  42513.0664,  -3427.8230,
         -24796.7959,   3166.0931]], dtype=torch.float64)
	q_value: tensor([[-36.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07230414039168154, distance: 1.1849927266845854 entropy 11.809297244595934
epoch: 79, step: 116
	action: tensor([[  19505.5149,  -73304.1751, -100779.4496,   37400.3350,  -23437.7904,
            7410.8016,  -32690.0675]], dtype=torch.float64)
	q_value: tensor([[-33.4396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10126069135903504, distance: 1.2008859455137628 entropy 12.289891348238795
epoch: 79, step: 117
	action: tensor([[-1.0063e+05,  4.0496e+04, -3.6877e+03,  8.6892e+04,  3.1861e+03,
         -7.3622e+04, -1.0552e+01]], dtype=torch.float64)
	q_value: tensor([[-34.7529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38228523378877877, distance: 0.8993954472092537 entropy 12.243581702239117
epoch: 79, step: 118
	action: tensor([[   964.3521,  -4596.8101, -56135.3822, -51070.5556,  21581.4021,
          14440.5818,  30309.7578]], dtype=torch.float64)
	q_value: tensor([[-37.5007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29950505517243964, distance: 1.3045047953703772 entropy 12.161651123645365
epoch: 79, step: 119
	action: tensor([[-70466.5352,  -7434.8917, -31313.1372, -15728.1250,  13523.7226,
         108863.9781,  65658.7924]], dtype=torch.float64)
	q_value: tensor([[-30.9799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1908776804723853, distance: 1.2487924694364811 entropy 11.986916627896987
epoch: 79, step: 120
	action: tensor([[-17802.5549, -55944.3165, -36763.7621,  20654.2424,  55852.2143,
         -25490.1986,  55193.4317]], dtype=torch.float64)
	q_value: tensor([[-39.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6540604737111106, distance: 1.471744376613194 entropy 12.373475657649534
epoch: 79, step: 121
	action: tensor([[-35784.8886, -26751.9784, -23653.0238,  59478.0518, -71344.2941,
           7924.3287, -82075.3636]], dtype=torch.float64)
	q_value: tensor([[-34.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26211063822875746, distance: 1.2855986478015162 entropy 12.248274171419768
epoch: 79, step: 122
	action: tensor([[  13565.2014, -119899.4253,  -29671.4581,  193746.6988,  -15034.4453,
           46130.1788,   -2296.6347]], dtype=torch.float64)
	q_value: tensor([[-39.4855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4493737007819141, distance: 0.8491514753733909 entropy 12.481355150571105
epoch: 79, step: 123
	action: tensor([[ 39897.9232, 104113.9898,  35953.2907, -27414.0122,  -4171.7682,
         -17779.0211, -12530.5817]], dtype=torch.float64)
	q_value: tensor([[-38.2886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2598451404240586, distance: 0.9845051464194836 entropy 12.38787121782878
epoch: 79, step: 124
	action: tensor([[ 24843.5035, -14288.7352,  37953.4721, -11490.2658, -31831.4551,
           -652.8587,  29989.5652]], dtype=torch.float64)
	q_value: tensor([[-37.9668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5364811460772412, distance: 0.7790949607241451 entropy 12.186233851706392
epoch: 79, step: 125
	action: tensor([[110815.3802, -66026.4950,  12242.8100,  -4164.1423, -56311.4633,
         -81402.8908, -85545.0571]], dtype=torch.float64)
	q_value: tensor([[-36.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5162579927572233, distance: 0.7959093276475044 entropy 12.30647983933699
epoch: 79, step: 126
	action: tensor([[-67575.4797, -49593.3055,  58159.4947,   9244.5025, -11268.0811,
         -40182.4715,  14441.0013]], dtype=torch.float64)
	q_value: tensor([[-37.9883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09716166476025778, distance: 1.1986489399989921 entropy 12.37149724480243
epoch: 79, step: 127
	action: tensor([[ 36195.0006, -45446.9693, -14288.9288,   1545.7193, -35816.3854,
          38902.6294, 178080.2830]], dtype=torch.float64)
	q_value: tensor([[-34.3359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24118303612704017, distance: 1.2748955607393837 entropy 12.345177857802131
LOSS epoch 79 actor 534.4843297027661 critic 86.9020373334761
epoch: 80, step: 0
	action: tensor([[-73156.3619, -38144.5158, -99982.2405,   5061.3311,  19248.8965,
          49905.6185, -11048.2194]], dtype=torch.float64)
	q_value: tensor([[-35.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27713450311026766, distance: 1.2932277415187452 entropy 12.415687149229658
epoch: 80, step: 1
	action: tensor([[-52036.0549,   9056.9008, 148522.3549, -11385.1975,  89548.6036,
          65449.7203, 114081.4993]], dtype=torch.float64)
	q_value: tensor([[-34.6934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32613147954194643, distance: 0.9393864202313508 entropy 12.50958111093927
epoch: 80, step: 2
	action: tensor([[-27719.6128, 128310.2107,   6069.5720,  -2886.4828,  31805.0612,
          10272.4207,   9899.2580]], dtype=torch.float64)
	q_value: tensor([[-37.3308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14228403652392752, distance: 1.2230486906983875 entropy 12.096739386163154
epoch: 80, step: 3
	action: tensor([[ 76709.4255,  35648.5975,  -1027.0450,  -9728.8777, -22325.5838,
         -21049.6698, -54752.7738]], dtype=torch.float64)
	q_value: tensor([[-43.3268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23136136439653976, distance: 1.003269936328805 entropy 12.294044206248257
epoch: 80, step: 4
	action: tensor([[ -43392.7917,  -44093.7319,   -9634.8045,  -23273.2955,   -9683.8589,
           81975.7715, -127848.6378]], dtype=torch.float64)
	q_value: tensor([[-38.1639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2348346271184123, distance: 1.0010006218987488 entropy 12.19177339965191
epoch: 80, step: 5
	action: tensor([[ -2180.8134, -52340.5847,   2153.5938, -39881.6566,  -3687.0966,
          50402.8821,  85300.6423]], dtype=torch.float64)
	q_value: tensor([[-39.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3839528248950437, distance: 1.346224024098503 entropy 12.514189318699541
epoch: 80, step: 6
	action: tensor([[-47113.3046, -58241.3106,  49696.0392, -90108.4938,  12683.3107,
         -56399.9029,  39773.6365]], dtype=torch.float64)
	q_value: tensor([[-36.1550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8606975143348545, distance: 1.56097009160823 entropy 12.27361890950956
epoch: 80, step: 7
	action: tensor([[  26563.0072,  -85956.1542, -268522.9113,  101522.0320,    7003.4965,
           13012.6516,   12924.2028]], dtype=torch.float64)
	q_value: tensor([[-41.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18458732047827386, distance: 1.033345192831986 entropy 12.60322162678081
epoch: 80, step: 8
	action: tensor([[ 23705.4011, -61037.8166,  35872.8346, -34014.8094,  36879.9586,
         -25116.9198, -22658.8900]], dtype=torch.float64)
	q_value: tensor([[-30.9461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2664130063005594, distance: 0.9801273421510484 entropy 12.180492340192163
epoch: 80, step: 9
	action: tensor([[ -95022.9151,  -56924.9933, -101609.8471,   35477.0524,   33265.2306,
          -55399.7517,   -1646.2947]], dtype=torch.float64)
	q_value: tensor([[-33.7996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023150212422221372, distance: 1.1310207859172499 entropy 12.252002548002283
epoch: 80, step: 10
	action: tensor([[ 14543.6922, -75820.5360, -57014.9082, 151038.5672, -16070.4843,
         -11660.7984,  18771.8756]], dtype=torch.float64)
	q_value: tensor([[-30.6171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5092974071447591, distance: 0.8016150635813416 entropy 12.12752407644659
epoch: 80, step: 11
	action: tensor([[-22102.5732, -54482.1611, -18558.7137,  23537.0581,  27707.5770,
         -43454.3419, -60263.3245]], dtype=torch.float64)
	q_value: tensor([[-37.4043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03887028322057273, distance: 1.1218833325146649 entropy 12.279018324963031
epoch: 80, step: 12
	action: tensor([[-75354.5005, -60447.3877,    663.1133,  15917.0232,   6780.3599,
          39796.1965,  39043.5482]], dtype=torch.float64)
	q_value: tensor([[-33.7195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05240334736798058, distance: 1.173945143844418 entropy 12.338827208611203
epoch: 80, step: 13
	action: tensor([[-75003.0766, -50456.8284, -69329.0069,  27758.6443, -83746.8890,
          12506.3353,  61485.0796]], dtype=torch.float64)
	q_value: tensor([[-33.9482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6196878387099651, distance: 1.4563721116060513 entropy 12.34934587624282
epoch: 80, step: 14
	action: tensor([[ -9490.7970, -95869.4113, -34045.7524,    139.6163,  21343.4630,
         -26442.1606,  20158.4681]], dtype=torch.float64)
	q_value: tensor([[-34.8866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4878859858949185, distance: 1.3958588997650736 entropy 12.417279689062498
epoch: 80, step: 15
	action: tensor([[ -1619.8092, -57493.1876, -35727.5760, 125894.0000,  85572.8644,
         -65073.8324,  67086.4634]], dtype=torch.float64)
	q_value: tensor([[-36.5216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12413534949625749, distance: 1.0709647896143903 entropy 12.476027205751658
epoch: 80, step: 16
	action: tensor([[ 82478.2423,  32866.1469, -22065.8571, -50354.6550, 108506.2404,
            735.2460,  59044.3292]], dtype=torch.float64)
	q_value: tensor([[-34.6451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.411831364056635
epoch: 80, step: 17
	action: tensor([[-3390.2818,  -620.3038, 12375.9130, 22325.6821, 27863.5146, 12342.5354,
         40503.7719]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5195011789417414, distance: 1.41061083041847 entropy 11.842795516613451
epoch: 80, step: 18
	action: tensor([[-129140.5304,   16616.4215,   51550.5780,   15535.3481,   67034.4556,
          -56755.9262,  114999.5003]], dtype=torch.float64)
	q_value: tensor([[-35.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1403503459183446, distance: 1.222013046576831 entropy 12.475351155580885
epoch: 80, step: 19
	action: tensor([[ 51247.5232, -94666.2063, -63756.7303, -33381.2928,  42282.3398,
         -64818.0836,  49399.9244]], dtype=torch.float64)
	q_value: tensor([[-40.3502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1705902568613542, distance: 1.2381097561350631 entropy 12.411694617593982
epoch: 80, step: 20
	action: tensor([[  7657.5340, -62095.4760,  29449.2793, -56081.3465, -67947.9188,
          71766.2400,  20859.0403]], dtype=torch.float64)
	q_value: tensor([[-32.0868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3274903205570219, distance: 0.9384388162815519 entropy 12.23003890737142
epoch: 80, step: 21
	action: tensor([[  1404.0807,  13716.0605, -52823.0615,  42630.8522,   -399.2054,
          11913.1899,  34764.2308]], dtype=torch.float64)
	q_value: tensor([[-32.5736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5641914612031755, distance: 0.7554479740181175 entropy 12.085517421937103
epoch: 80, step: 22
	action: tensor([[  3042.5566, -93507.0921, -98504.3536, -24635.7614,  42179.1984,
         -12999.0089, -46006.2129]], dtype=torch.float64)
	q_value: tensor([[-35.9245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07022042512963345, distance: 1.10343484433803 entropy 12.086316982851114
epoch: 80, step: 23
	action: tensor([[-12501.2169,  56945.9846,   3848.2869, -25284.4857,  41273.2904,
          45844.5073,  38908.9065]], dtype=torch.float64)
	q_value: tensor([[-29.3712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6752698580634868, distance: 0.6521056971391253 entropy 12.062472788851029
epoch: 80, step: 24
	action: tensor([[-71016.5012,  10075.2779, -23180.6820,  47373.2506, -26271.1455,
           4577.4671, -46765.6134]], dtype=torch.float64)
	q_value: tensor([[-38.9963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13474312505663577, distance: 1.2190049618408518 entropy 12.357467371695773
epoch: 80, step: 25
	action: tensor([[-72248.6382,  -5500.1603, -10431.4909,  88330.3942, -18655.2539,
         -20718.4238,  53273.5218]], dtype=torch.float64)
	q_value: tensor([[-38.0984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9061404974281468, distance: 1.579916546375581 entropy 12.17712328495549
epoch: 80, step: 26
	action: tensor([[-49180.2239, -88009.9132, -50484.8896, -21590.6293, -75966.9862,
         -69810.8756, -42840.3842]], dtype=torch.float64)
	q_value: tensor([[-32.9294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8122788030930186, distance: 1.5405265896864437 entropy 12.333558546477983
epoch: 80, step: 27
	action: tensor([[ 49777.1369,   9343.0877, -62444.1345, -76653.4491,  19193.4250,
          14482.4445,  95945.1753]], dtype=torch.float64)
	q_value: tensor([[-34.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.770774879238163, distance: 0.5478829662436179 entropy 12.372925781020369
epoch: 80, step: 28
	action: tensor([[-34345.3832, -47029.6273,   3851.0046,  87586.5459,  84657.9073,
         -27009.7650, -11627.8373]], dtype=torch.float64)
	q_value: tensor([[-40.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.133036446541203
epoch: 80, step: 29
	action: tensor([[ 22532.9091, -31556.5005, -60015.0307,  45738.2994,  12985.2050,
           3532.7834,  31378.6422]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14725911456409602, distance: 1.2257092162411598 entropy 11.842795516613451
epoch: 80, step: 30
	action: tensor([[ -82183.8292,  -64229.5602,  -55567.7530,   -7107.2362, -121748.2381,
           17928.9660,  -58987.4332]], dtype=torch.float64)
	q_value: tensor([[-33.4631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042106303365344155, distance: 1.119993110009634 entropy 12.196126812688155
epoch: 80, step: 31
	action: tensor([[ -2241.7525, -51138.4304, -33781.9935, -18401.4116,  62627.1166,
         -79066.9644, -67386.9882]], dtype=torch.float64)
	q_value: tensor([[-31.6166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8512083230559948, distance: 1.5569846837210326 entropy 12.089315476986505
epoch: 80, step: 32
	action: tensor([[  55709.4311,  -23445.3479,  -81142.4842,   55945.8945,  -32683.9880,
          -15839.8520, -105349.6986]], dtype=torch.float64)
	q_value: tensor([[-34.4786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2199603986643417, distance: 1.2639490268749771 entropy 12.37483202270125
epoch: 80, step: 33
	action: tensor([[ 30019.2755, -92925.9401,  59739.9814,  -2197.0295,  -6599.6022,
          96967.9557, -13338.5157]], dtype=torch.float64)
	q_value: tensor([[-29.4648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.340686079865557, distance: 0.9291863383650519 entropy 12.131874220849426
epoch: 80, step: 34
	action: tensor([[ -78663.2785,  -42995.3644,  -46661.1026,    2550.5276, -181294.2940,
           33603.9585,   67039.8477]], dtype=torch.float64)
	q_value: tensor([[-34.3397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5343256638850482, distance: 1.4174751956665073 entropy 12.226320444055839
epoch: 80, step: 35
	action: tensor([[-50160.6263, -36478.5453, -26303.7319,  -3899.2649,  59782.1381,
          76880.5852,  68879.8684]], dtype=torch.float64)
	q_value: tensor([[-31.4449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2143933644283499, distance: 1.0142832762385088 entropy 12.206339798849928
epoch: 80, step: 36
	action: tensor([[ 11445.3410, -27980.9055, -58691.0203, -69750.2102, 135204.0775,
          17242.5100,  57650.7274]], dtype=torch.float64)
	q_value: tensor([[-41.6412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5339865252208358, distance: 0.781188660400614 entropy 12.555006042699253
epoch: 80, step: 37
	action: tensor([[-124423.7495,   19059.0271, -137629.1362,   33743.6316,  133837.2468,
           45459.8629,   86368.0639]], dtype=torch.float64)
	q_value: tensor([[-35.6359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.361377476553317
epoch: 80, step: 38
	action: tensor([[-17877.5355, -10954.3990,   1085.1730,  46618.4954,   9600.6269,
          -3436.8972, -10854.6716]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.624281775016333, distance: 1.4584360103420932 entropy 11.842795516613451
epoch: 80, step: 39
	action: tensor([[ 11005.8733, -42056.6785,   4536.8479,  56553.5903,  57012.2909,
          90315.2972,  87238.1562]], dtype=torch.float64)
	q_value: tensor([[-30.5228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4502152522383073, distance: 0.8485023255663959 entropy 12.193898596817393
epoch: 80, step: 40
	action: tensor([[-31748.5005, -21569.2771, -27476.3027,  27364.5262,  58696.6834,
           1950.1269,  70152.1361]], dtype=torch.float64)
	q_value: tensor([[-31.8855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19620411743837418, distance: 1.2515820898153227 entropy 12.215377493422029
epoch: 80, step: 41
	action: tensor([[ 44645.5725, -45526.0941, -52109.0401,  15191.6963,   9878.2225,
          70405.3313, -11280.0298]], dtype=torch.float64)
	q_value: tensor([[-30.2694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6212892572750843, distance: 0.7042234873833434 entropy 12.186361883406477
epoch: 80, step: 42
	action: tensor([[ 101077.7041, -113908.2389,  -65480.5332,   27586.9191,  -28095.5103,
          -14966.9599,  -44293.5420]], dtype=torch.float64)
	q_value: tensor([[-34.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3526029423349957, distance: 0.9207506919448316 entropy 12.262848689894895
epoch: 80, step: 43
	action: tensor([[-22357.9063,  -9069.1137, -26182.6657,  -9304.7392, -86397.3762,
         -32278.5288,  86600.8203]], dtype=torch.float64)
	q_value: tensor([[-43.8291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6875559487036909, distance: 1.4865714348252759 entropy 12.511554971135917
epoch: 80, step: 44
	action: tensor([[-41648.6341,  37907.1989,   9530.3318,  11478.8610, -27336.8778,
          60996.8514,  -6556.7314]], dtype=torch.float64)
	q_value: tensor([[-31.8174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5661896187619104, distance: 0.7537141411870865 entropy 12.289566283986602
epoch: 80, step: 45
	action: tensor([[  35810.7542,  -69613.8758,  -14703.0910, -113120.8078,  -33343.2375,
           39839.3440,  -88435.6705]], dtype=torch.float64)
	q_value: tensor([[-38.7624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.336586062334044
epoch: 80, step: 46
	action: tensor([[-37012.9712,  17972.9393,    199.8352,  59674.4407, -43716.7258,
          12419.5204, -36875.4102]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38915136427533925, distance: 1.348750063561218 entropy 11.842795516613451
epoch: 80, step: 47
	action: tensor([[ 11730.7915, -39580.9707, -63442.0936,  47271.5059, -10397.5091,
          29949.7555,  -8030.8943]], dtype=torch.float64)
	q_value: tensor([[-31.6628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.023472869531899
epoch: 80, step: 48
	action: tensor([[-27866.3694, -39384.2717, -16844.5124, -21246.1366,  47406.5736,
         -37857.2564,  30946.0787]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1420887558883288, distance: 1.0599316808837516 entropy 11.842795516613451
epoch: 80, step: 49
	action: tensor([[ -97213.5867,  -26795.7457, -133831.0356,   -3960.2379,   87923.3868,
           29534.5736,  -11404.6288]], dtype=torch.float64)
	q_value: tensor([[-32.6984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23765676980745554, distance: 0.9991529322898249 entropy 12.226634345654926
epoch: 80, step: 50
	action: tensor([[  31166.6805,  -37428.2236, -105685.5145,   40454.7105,    6957.6606,
           96973.1593,   25020.5525]], dtype=torch.float64)
	q_value: tensor([[-36.2859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14014655734706627, distance: 1.0611307757765425 entropy 12.303333275477387
epoch: 80, step: 51
	action: tensor([[-68755.9791,  -9970.9845,  83311.2561, 120714.8480, -50227.8714,
          17175.4300, -14505.0954]], dtype=torch.float64)
	q_value: tensor([[-30.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16580968285201414, distance: 1.045175607444142 entropy 12.083404204406516
epoch: 80, step: 52
	action: tensor([[ -18410.4375,  -50200.7567,  -59420.5499,  137800.0499,   14896.4483,
         -138958.6074, -138036.1014]], dtype=torch.float64)
	q_value: tensor([[-35.3234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7280446027586536, distance: 1.5042990014641655 entropy 12.48039551012785
epoch: 80, step: 53
	action: tensor([[-21480.9525, -18646.6750, -17351.1021,  96916.5332,  44844.7866,
           4854.0732, -14157.6007]], dtype=torch.float64)
	q_value: tensor([[-34.7833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41499926747506577, distance: 1.3612403085583642 entropy 12.409523470860078
epoch: 80, step: 54
	action: tensor([[  -458.4841, -87562.7569, -16830.6189,  88961.2630, -24493.8625,
         -61472.3204,   8002.7758]], dtype=torch.float64)
	q_value: tensor([[-35.6121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1741830916523548, distance: 1.0399167623047216 entropy 12.463707611414614
epoch: 80, step: 55
	action: tensor([[ -2076.2344,  48322.8802, 108925.8970,  43786.1531, -16531.3388,
          87285.0902,  71114.6962]], dtype=torch.float64)
	q_value: tensor([[-35.2545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1572121073257513, distance: 1.0505478475188739 entropy 12.449195996112326
epoch: 80, step: 56
	action: tensor([[  22683.3429,  -55279.8612,  -26932.3670,  -77408.6635,   47081.3207,
         -102964.4974,  -42701.5260]], dtype=torch.float64)
	q_value: tensor([[-33.6674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.237497810065046
epoch: 80, step: 57
	action: tensor([[-68217.1400, -40919.3275,  30764.5286,  33884.0925,   9063.6644,
           7784.5525,  -1329.7956]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11322286721641528, distance: 1.2073904951499888 entropy 11.842795516613451
epoch: 80, step: 58
	action: tensor([[  8835.5056, -46771.9698, -72686.9043,   2582.5779,  51581.2448,
         -15276.2588, 118119.4561]], dtype=torch.float64)
	q_value: tensor([[-35.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3204530478446246, distance: 0.943336041826283 entropy 12.481335860833743
epoch: 80, step: 59
	action: tensor([[ -29810.8897, -134996.5764,  -41992.7355,  155949.0468,   77854.3716,
          -75952.5617,  -41603.4091]], dtype=torch.float64)
	q_value: tensor([[-35.4540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6187236228947974, distance: 1.455938550878056 entropy 12.469914465211234
epoch: 80, step: 60
	action: tensor([[ -50617.3402,  -54070.4124,    -549.0102,  -37601.7449,  -84347.7973,
          -88533.4996, -117422.0489]], dtype=torch.float64)
	q_value: tensor([[-35.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8995922902702589, distance: 1.5772004503474872 entropy 12.480425765897191
epoch: 80, step: 61
	action: tensor([[-72790.5860,  39984.0467,  23712.7719, -48764.1011, -19468.3477,
         142324.5782, -83317.6657]], dtype=torch.float64)
	q_value: tensor([[-36.6550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.467406728405205
epoch: 80, step: 62
	action: tensor([[  21227.3172,   21428.3032,   12743.7315, -101729.4119,   21246.2323,
          -42520.3575,   47936.2269]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7693744576662938, distance: 0.5495540283162499 entropy 11.842795516613451
epoch: 80, step: 63
	action: tensor([[ 67207.5103, -51419.9397, -20098.5303, -15984.8213,  -4739.7626,
          82525.4450, -20322.2774]], dtype=torch.float64)
	q_value: tensor([[-39.2530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5867528032626145, distance: 0.7356337592176062 entropy 12.282210084311766
epoch: 80, step: 64
	action: tensor([[ 4.1789e+04, -7.6296e+04, -4.1136e+01,  1.1585e+05,  3.8025e+04,
         -1.0398e+04,  4.0179e+04]], dtype=torch.float64)
	q_value: tensor([[-34.1565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3744558099592563, distance: 1.3415970170814022 entropy 12.28232486588342
epoch: 80, step: 65
	action: tensor([[ 22757.7941, -40634.2305,  41726.2130, -57525.2967, 113298.0511,
          65737.7462, -16115.4711]], dtype=torch.float64)
	q_value: tensor([[-34.8341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010360316714829843, distance: 1.1502568637842774 entropy 12.19850887573335
epoch: 80, step: 66
	action: tensor([[-49559.2152,  39821.8003,  10321.8982,  50691.7385, -24291.8610,
         -19962.9200,  -5050.6250]], dtype=torch.float64)
	q_value: tensor([[-31.7456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.015699599876019
epoch: 80, step: 67
	action: tensor([[-17064.4735, -39083.4164,  -7953.0093,   5152.5252,  15957.2327,
         -47829.9707, -10403.2871]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06752499528797029, distance: 1.1050331149571038 entropy 11.842795516613451
epoch: 80, step: 68
	action: tensor([[ -68423.3472, -116028.3157,   -9762.0360,   36109.2593,   85158.7921,
           73675.6561,   30469.2705]], dtype=torch.float64)
	q_value: tensor([[-29.8714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3175180517226026, distance: 1.3135148298841655 entropy 12.161641632497512
epoch: 80, step: 69
	action: tensor([[ -38945.7489, -115321.0007, -105900.3835,  -11675.8690,   48836.2942,
         -119282.7357,  -80926.3275]], dtype=torch.float64)
	q_value: tensor([[-34.6059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9087328277781201, distance: 1.5809905160147664 entropy 12.433752827484765
epoch: 80, step: 70
	action: tensor([[ 89783.1343, -72656.5826,  56526.9659,  11406.6878, -49273.7315,
          57472.9719,  39322.7941]], dtype=torch.float64)
	q_value: tensor([[-34.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11767042124037352, distance: 1.0749100231307596 entropy 12.41229626344827
epoch: 80, step: 71
	action: tensor([[  12843.9474, -127433.5710,  -55054.5745,  109462.8198,   -4246.4350,
           68296.2343,   14533.7931]], dtype=torch.float64)
	q_value: tensor([[-34.1925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4748684351887431, distance: 0.8292600722154865 entropy 12.438972966216369
epoch: 80, step: 72
	action: tensor([[-53895.5124, -82337.7518, -29592.8415,  48443.4282,  50148.7381,
         141502.6589,   1269.4879]], dtype=torch.float64)
	q_value: tensor([[-33.2323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02761424022356196, distance: 1.1600367561728284 entropy 12.184772643475076
epoch: 80, step: 73
	action: tensor([[ -35631.5900,  -23331.0635,  -79341.8615,  -48203.2543,   -9521.6420,
         -120663.1088,  -11634.3852]], dtype=torch.float64)
	q_value: tensor([[-32.2747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6162268831961937, distance: 1.4548152886005554 entropy 12.295840665454424
epoch: 80, step: 74
	action: tensor([[ 23403.4888, -28596.9668,  53074.4286, -52251.4709, -12226.6006,
         -40064.8301,  28026.6900]], dtype=torch.float64)
	q_value: tensor([[-33.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43582292142688484, distance: 0.8595366750865113 entropy 12.352743578209529
epoch: 80, step: 75
	action: tensor([[  -3977.5202, -111713.8855,  -76577.8070,   -1732.2359,   80867.8444,
           36925.4126,   49934.6924]], dtype=torch.float64)
	q_value: tensor([[-32.9312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024036766361412498, distance: 1.1580157549044794 entropy 12.234673445550701
epoch: 80, step: 76
	action: tensor([[-48241.0922, -56956.2515, -40258.8067,  55060.6835,  -7266.0857,
         -37442.7404, -32480.4566]], dtype=torch.float64)
	q_value: tensor([[-35.8303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20625256036124773, distance: 1.2568279128115314 entropy 12.381165410316296
epoch: 80, step: 77
	action: tensor([[ -9615.5330,   5956.4646,  74968.8465, -17542.4243,  16497.3567,
         -76238.4777, -28131.9974]], dtype=torch.float64)
	q_value: tensor([[-37.5641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6041122196376281, distance: 0.7200169854214232 entropy 12.542021385785905
epoch: 80, step: 78
	action: tensor([[-45139.2994, -10972.6679,  18600.8876,  92643.1189,  26051.5378,
          26416.8061,  61709.2157]], dtype=torch.float64)
	q_value: tensor([[-46.3186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8535252149144614, distance: 1.5579587062264897 entropy 12.430879233887262
epoch: 80, step: 79
	action: tensor([[-154065.0801, -175715.8216,  -19499.8377,   80978.4836,  -65041.9506,
          -51320.4159,   49404.2107]], dtype=torch.float64)
	q_value: tensor([[-36.5716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24067211132942157, distance: 0.9971749699529461 entropy 12.553193693068861
epoch: 80, step: 80
	action: tensor([[-74661.6827,  -8109.8478, -55933.1054,  44340.9140, -35777.2337,
          94893.5504,  19930.7430]], dtype=torch.float64)
	q_value: tensor([[-33.7359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12847406707998077, distance: 1.0683089098099754 entropy 12.480743918470925
epoch: 80, step: 81
	action: tensor([[ 24783.5142, -29954.4275,  29746.2475, -27133.9580,  61989.6380,
          40592.0496, -81093.3888]], dtype=torch.float64)
	q_value: tensor([[-32.3095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0606676474159179, distance: 1.10908883557667 entropy 12.293293046627282
epoch: 80, step: 82
	action: tensor([[-51414.2468, -74863.5827,  49345.5848,  52338.0864,  51288.0913,
         -15003.4521,  51564.1317]], dtype=torch.float64)
	q_value: tensor([[-28.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3022939565511813, distance: 1.3059038608552296 entropy 11.878708580850395
epoch: 80, step: 83
	action: tensor([[-7.3617e+04,  9.9248e+01, -7.8632e+04,  8.2496e+03, -5.3351e+04,
         -2.7234e+04, -1.2062e+05]], dtype=torch.float64)
	q_value: tensor([[-34.8800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16563754237748984, distance: 1.2354877866194982 entropy 12.417652174648358
epoch: 80, step: 84
	action: tensor([[ -15843.3792,   36564.1411,   43883.3851,   28476.0178, -120706.3486,
           96235.0084,  -24441.8644]], dtype=torch.float64)
	q_value: tensor([[-35.0091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18817669440694562, distance: 1.0310683377274232 entropy 12.364319330822415
epoch: 80, step: 85
	action: tensor([[-34833.7211, -16226.2054,   5884.7301,   7504.9212, -44872.1782,
          -2625.2741,  37575.9919]], dtype=torch.float64)
	q_value: tensor([[-31.6773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3663841867838179, distance: 0.9108978919220624 entropy 12.138857494646711
epoch: 80, step: 86
	action: tensor([[ -17983.3291,  -63226.6674,   46855.3180,  140365.2508,  -52207.2233,
          -79349.8849, -127567.3889]], dtype=torch.float64)
	q_value: tensor([[-37.8069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.273358198385909, distance: 1.29131438101962 entropy 12.520958277683674
epoch: 80, step: 87
	action: tensor([[ -2411.4792,  34581.2545,  65371.8452, -35012.2663, -21058.5278,
         -54612.5595,  95718.3273]], dtype=torch.float64)
	q_value: tensor([[-33.1602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5376048854102018, distance: 0.7781499819803973 entropy 12.253161312041955
epoch: 80, step: 88
	action: tensor([[   -498.2975,   11174.8884, -142950.8507,   -1504.0670,   -9085.1536,
            8242.5773,   27605.0336]], dtype=torch.float64)
	q_value: tensor([[-36.5976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.51488604772648, distance: 0.7970371713210592 entropy 12.1454066166342
epoch: 80, step: 89
	action: tensor([[  44166.4528, -160150.4505,  -17206.2663,   65958.9085,   24141.9621,
           18552.7697,   50652.1751]], dtype=torch.float64)
	q_value: tensor([[-38.9407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.35461619073261
epoch: 80, step: 90
	action: tensor([[-56864.9340, -43549.0353,  22512.8485,  12078.9866,  37177.7362,
          -2479.5223,  62848.8006]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8801769668396833, distance: 1.569119636295925 entropy 11.842795516613451
epoch: 80, step: 91
	action: tensor([[-47283.8512,   3912.4344,  58227.3292,  25870.9618,  45243.2369,
          95681.6157,  -9771.7648]], dtype=torch.float64)
	q_value: tensor([[-32.7591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6585184668255317, distance: 0.668713829134579 entropy 12.264646931093335
epoch: 80, step: 92
	action: tensor([[ 17483.9089, -62853.8394, -36839.6718,  -4618.6871, 166886.7798,
          25360.0589, 109387.4485]], dtype=torch.float64)
	q_value: tensor([[-34.8908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.309971539729421
epoch: 80, step: 93
	action: tensor([[-35690.5062, -31349.0036,  50794.1237,  23629.6425,   4249.9501,
         -26231.9939,  61272.5086]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06409915807503208, distance: 1.1804503983249968 entropy 11.842795516613451
epoch: 80, step: 94
	action: tensor([[-58230.9937,   4478.5138,  96646.6629,  43963.9422, -44141.2440,
         -20358.4597, -14146.1004]], dtype=torch.float64)
	q_value: tensor([[-32.3974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5531772649173069, distance: 0.7649346333577685 entropy 12.328651503487464
epoch: 80, step: 95
	action: tensor([[-14259.3210,  33573.5087,  45871.5778,  66823.6318,  80311.2925,
           -871.8020,  39546.9918]], dtype=torch.float64)
	q_value: tensor([[-38.6599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02303290163831262, distance: 1.131088696538461 entropy 12.446647126362176
epoch: 80, step: 96
	action: tensor([[ 16416.9037, -29762.1964,  65406.8755,  34927.4338, -54943.7160,
          52033.1992,  43929.7091]], dtype=torch.float64)
	q_value: tensor([[-37.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4494920909114025, distance: 0.8490601824605315 entropy 12.283665402955894
epoch: 80, step: 97
	action: tensor([[-64964.9224, -17404.1051,  19051.2389,  27486.7689,   4919.7910,
          21270.5567, -31268.3430]], dtype=torch.float64)
	q_value: tensor([[-34.5504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2560382213302852, distance: 0.9870337554392669 entropy 12.33550993170606
epoch: 80, step: 98
	action: tensor([[ 67263.8690,  67712.1921, -10838.8266,  -9278.5090,  31405.3544,
           3107.6693, -16559.8723]], dtype=torch.float64)
	q_value: tensor([[-33.1392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.399724275203525
epoch: 80, step: 99
	action: tensor([[-28067.8566, -29993.9941,    429.2417,  64743.9200,   3249.0240,
         -11275.3174, -11196.1909]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18055533699113058, distance: 1.2433685204127103 entropy 11.842795516613451
epoch: 80, step: 100
	action: tensor([[ 61335.2394, -33393.4103,  13261.2977,   7295.9588,  14952.3544,
          51982.2596, 108076.3904]], dtype=torch.float64)
	q_value: tensor([[-35.7037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1521600001821728, distance: 1.22832443149096 entropy 12.350896708101866
epoch: 80, step: 101
	action: tensor([[  42042.0144,   -2517.9724,   68761.6518,  -23277.3449,   47184.9261,
         -120373.5397,   -4618.7990]], dtype=torch.float64)
	q_value: tensor([[-32.1799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14312489685723107, distance: 1.0592914220890697 entropy 12.32978082985485
epoch: 80, step: 102
	action: tensor([[-38187.3577, -34097.3645, -10584.8135, -22158.6118, -58674.2485,
         -15920.8255,  56730.0314]], dtype=torch.float64)
	q_value: tensor([[-30.7796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5365800075453064, distance: 1.4185161426442805 entropy 12.119746842600886
epoch: 80, step: 103
	action: tensor([[ -34301.3324, -150010.8539,  -37544.2689,   60966.7777,   44634.3567,
           65574.5496,  -36218.4336]], dtype=torch.float64)
	q_value: tensor([[-33.6033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29202257793229025, distance: 1.300743740837298 entropy 12.335466563657851
epoch: 80, step: 104
	action: tensor([[-38084.1608, -64456.7057, -68295.5143, -38624.2032,  45859.0770,
         -36244.4459,  -5510.6412]], dtype=torch.float64)
	q_value: tensor([[-32.5911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7419973612206701, distance: 1.5103598758879837 entropy 12.334607472256135
epoch: 80, step: 105
	action: tensor([[-38712.1744, -61308.1567,  -2007.4284, -10910.3324, -76198.3451,
          55815.2383,  70436.0206]], dtype=torch.float64)
	q_value: tensor([[-33.2241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1590002600723628, distance: 1.0494327760259288 entropy 12.328834055002009
epoch: 80, step: 106
	action: tensor([[ -24366.7972,  -24140.3291,   11837.4755,  122996.3870,   40601.8820,
         -102997.0156,   44038.6025]], dtype=torch.float64)
	q_value: tensor([[-41.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0060749458320766, distance: 1.620803143291714 entropy 12.513222619639752
epoch: 80, step: 107
	action: tensor([[-11887.1410,  76160.6501, -12018.4177,  -6421.9818,  -8397.7937,
          -8718.0922, -59397.8572]], dtype=torch.float64)
	q_value: tensor([[-29.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16424189757735497, distance: 1.2347479260332337 entropy 12.11312783892581
epoch: 80, step: 108
	action: tensor([[ 66773.5768,  25700.4896,  53856.7921, -37821.8369,  23824.7133,
         -75909.4881, -65005.8909]], dtype=torch.float64)
	q_value: tensor([[-42.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2493915064402047, distance: 0.9914331371951253 entropy 12.292574311927433
epoch: 80, step: 109
	action: tensor([[-116005.7735,   52006.2763,  -30722.5495,   -3371.2793,    1428.3458,
           22993.7116,  -14446.3961]], dtype=torch.float64)
	q_value: tensor([[-37.2883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013937350602686749, distance: 1.152291223428797 entropy 12.180440105185763
epoch: 80, step: 110
	action: tensor([[-68688.9959, -48272.9142, -57650.4048,  31182.8839,    927.2532,
         120621.5649,  39490.1608]], dtype=torch.float64)
	q_value: tensor([[-40.2755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6955050915028411, distance: 1.4900685293966547 entropy 12.31195245459384
epoch: 80, step: 111
	action: tensor([[ -5971.8612, -63584.3757,   1221.4569, 119746.8047, -55420.1342,
         -11564.2216, 206039.2339]], dtype=torch.float64)
	q_value: tensor([[-34.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0791215602576576, distance: 1.0981403406005819 entropy 12.411793189534578
epoch: 80, step: 112
	action: tensor([[-15541.6558, -53595.6056, -40395.2626, -16101.3620,  18778.9357,
         -23780.4314,  10469.9655]], dtype=torch.float64)
	q_value: tensor([[-35.6897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.844538742038349, distance: 1.554177380626885 entropy 12.481061777383237
epoch: 80, step: 113
	action: tensor([[-96805.0183, -35982.5525,  92711.3400, -11842.5187,   7181.8139,
          16225.9321,  38702.3473]], dtype=torch.float64)
	q_value: tensor([[-35.3798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9053174530541126, distance: 1.579575416793517 entropy 12.390756724355048
epoch: 80, step: 114
	action: tensor([[ -23732.8183, -119858.2539,   98365.9612,  -26135.6742,   76314.9551,
           60957.3453, -137561.7549]], dtype=torch.float64)
	q_value: tensor([[-36.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.721878761523485, distance: 1.5016128562769877 entropy 12.376258364936445
epoch: 80, step: 115
	action: tensor([[ -25917.6490, -146294.7507,  -56072.9349,  -56878.0493,  -37112.3652,
           28763.9031,  -12966.3351]], dtype=torch.float64)
	q_value: tensor([[-35.4351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3158273307631543, distance: 1.3126717674414963 entropy 12.38656876568562
epoch: 80, step: 116
	action: tensor([[   3699.2181,  -82210.5381,  -98148.0152,   38588.9239,   82847.6094,
           18742.4943, -147551.8335]], dtype=torch.float64)
	q_value: tensor([[-38.9441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12214241434460771, distance: 1.072182529493119 entropy 12.434919029230828
epoch: 80, step: 117
	action: tensor([[  27686.3838,  -60729.1998,  -66665.4608, -115952.6406,     373.5322,
          110021.4644,    1230.4944]], dtype=torch.float64)
	q_value: tensor([[-34.3157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46644815321495303, distance: 0.8358820651142217 entropy 12.363478025081738
epoch: 80, step: 118
	action: tensor([[ 33266.4306, -85262.0548, -23257.1223,  51322.4042,  20919.7359,
         -73863.8285,   9588.8406]], dtype=torch.float64)
	q_value: tensor([[-41.0179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26247213833601535, distance: 1.2857827484505304 entropy 12.372686021540984
epoch: 80, step: 119
	action: tensor([[ 66411.9278,  28029.0676,  85685.0349, 130594.2253, -45822.6127,
          -7735.8523, -86830.1308]], dtype=torch.float64)
	q_value: tensor([[-35.1760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19129017075160737, distance: 1.029089279789731 entropy 12.399972310931233
epoch: 80, step: 120
	action: tensor([[ 36497.5628, -32679.6344, -23374.3305,  19858.6125,  85773.0491,
          52973.2519,  42621.6433]], dtype=torch.float64)
	q_value: tensor([[-35.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0639809853231954, distance: 1.107131044839626 entropy 12.22415713124332
epoch: 80, step: 121
	action: tensor([[ -49329.2318,  -12798.0877,  -18275.8142,   13884.8365, -123553.7853,
           26842.5467,  -19800.2074]], dtype=torch.float64)
	q_value: tensor([[-36.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17667642212894408, distance: 1.038345702062964 entropy 12.302000817661057
epoch: 80, step: 122
	action: tensor([[ 60452.5520,  23150.1126,  19133.8975, -11660.4228,  29950.6202,
         -69756.9574,  -5834.3841]], dtype=torch.float64)
	q_value: tensor([[-32.9172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8656648155518849, distance: 0.41942236159450996 entropy 12.34089749338624
epoch: 80, step: 123
	action: tensor([[-44066.2692,  11098.7682, -55947.6820,  40058.8837, -37241.5279,
         -70965.1373,  36134.7304]], dtype=torch.float64)
	q_value: tensor([[-34.1476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.148046784718234
epoch: 80, step: 124
	action: tensor([[ 32384.0810,  16111.3987,  24718.8989,    947.7214,  51460.4528,
          37175.2329, -63379.0304]], dtype=torch.float64)
	q_value: tensor([[-35.4328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19364627676163204, distance: 1.0275891051142292 entropy 11.842795516613451
epoch: 80, step: 125
	action: tensor([[  8827.2117,  -1541.2881, -43710.0723, -67105.6450,  57313.5941,
         -19633.0321,  -8584.3703]], dtype=torch.float64)
	q_value: tensor([[-33.2227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14595947665120268, distance: 1.2250147656404107 entropy 11.967583505212314
epoch: 80, step: 126
	action: tensor([[ 18974.9089,  -9171.5906, -19437.6313,  31057.1575, -13136.7904,
          60125.9813,   4169.9747]], dtype=torch.float64)
	q_value: tensor([[-30.1427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3636767652371682, distance: 1.3363259860312249 entropy 12.00799593575523
epoch: 80, step: 127
	action: tensor([[-15322.9522, -53442.6884,  -9072.4504,  65142.9836,  36151.8929,
          59441.5817, -46186.7299]], dtype=torch.float64)
	q_value: tensor([[-36.1032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05415565373372455, distance: 1.112926617405013 entropy 12.236372257845423
LOSS epoch 80 actor 509.1742064700024 critic 98.34856458911449
epoch: 81, step: 0
	action: tensor([[ 68502.8731, -13588.6845, -16936.7327,  55929.5450, -57827.0952,
          27388.6570, -34517.5293]], dtype=torch.float64)
	q_value: tensor([[-24.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3555756934061054, distance: 1.3323507776871677 entropy 12.233840018045948
epoch: 81, step: 1
	action: tensor([[  42671.4763,    -300.0153, -160217.5349,  137306.4285,  -12914.4775,
           46034.3856, -104304.9326]], dtype=torch.float64)
	q_value: tensor([[-30.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2498937973269324, distance: 1.279361418698221 entropy 12.515432004435384
epoch: 81, step: 2
	action: tensor([[-28464.0693, -66786.9331, -62949.5317, -59891.7826,  22593.2869,
          22314.8814, -83821.6634]], dtype=torch.float64)
	q_value: tensor([[-28.5447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29900137095034185, distance: 1.304251959815304 entropy 12.515165722816175
epoch: 81, step: 3
	action: tensor([[-66034.2033, -96280.9911,  28804.6528,  46304.3240,  61819.3943,
          55339.4154,   4228.1031]], dtype=torch.float64)
	q_value: tensor([[-24.0219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06597864409545362, distance: 1.1059489901427368 entropy 12.232826234444259
epoch: 81, step: 4
	action: tensor([[ 67859.9571,  91574.9604,  41178.0648, -50035.9076,   7509.9711,
         -25865.6054,  15620.6277]], dtype=torch.float64)
	q_value: tensor([[-27.2306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.401123389613618
epoch: 81, step: 5
	action: tensor([[ -7627.2347, -67183.7693,  -9862.3347,   2289.6061,  -5491.9093,
          23481.5481,  17997.1799]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14443858917850572, distance: 1.0584791002131897 entropy 11.875201075344103
epoch: 81, step: 6
	action: tensor([[-68827.7524, -18202.4799,  -5106.3582,  54413.0470,   1603.6607,
          21042.3653, -81037.0219]], dtype=torch.float64)
	q_value: tensor([[-27.7227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.062466214127902764, distance: 1.1080265246538767 entropy 12.431362834603751
epoch: 81, step: 7
	action: tensor([[-77901.9996, -90362.8570,   4244.6985,  -8707.7144, -17233.8344,
           4881.5187, -49221.1720]], dtype=torch.float64)
	q_value: tensor([[-28.8122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8895383074818595, distance: 1.573021084129205 entropy 12.486468346983111
epoch: 81, step: 8
	action: tensor([[-13490.4418, -64785.1138, -38890.3733, 129900.1556,  72227.2241,
          73491.3500, 103209.8893]], dtype=torch.float64)
	q_value: tensor([[-28.7933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20166433374141657, distance: 1.2544353353576043 entropy 12.414344304743247
epoch: 81, step: 9
	action: tensor([[-13609.4132, -82846.8973, -43161.1957,  17616.0878,  47095.6308,
          74373.9543,  22583.1392]], dtype=torch.float64)
	q_value: tensor([[-28.4702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6228816207193091, distance: 1.4578072783899159 entropy 12.495104180149651
epoch: 81, step: 10
	action: tensor([[ -39044.8272, -126156.1875, -100379.1033,   92172.3476,    5941.2675,
           22962.9867,  -18704.4862]], dtype=torch.float64)
	q_value: tensor([[-30.9262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12431032298155009, distance: 1.2133882703694803 entropy 12.60663123521984
epoch: 81, step: 11
	action: tensor([[-108377.1846, -133052.5395,   38805.4837,  108822.7567,  -24749.8376,
          -32098.9913, -120340.0373]], dtype=torch.float64)
	q_value: tensor([[-28.9389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40277711058402255, distance: 1.3553486535405777 entropy 12.495608685658642
epoch: 81, step: 12
	action: tensor([[-15467.4346,  21481.5631, -75714.9262, 112116.9243, -47251.9782,
          61040.1313,  26435.2732]], dtype=torch.float64)
	q_value: tensor([[-27.0895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24742247288172647, distance: 1.2780959985638933 entropy 12.336504855236717
epoch: 81, step: 13
	action: tensor([[-98645.5241, -36081.8733, -15226.9230,  62808.4175, -59360.5232,
          70552.1999, -77425.6162]], dtype=torch.float64)
	q_value: tensor([[-31.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45254561173802155, distance: 1.3791820068415361 entropy 12.466483780711641
epoch: 81, step: 14
	action: tensor([[  86161.9607, -112577.1254,  -25018.7969,   86471.1800,  140107.4185,
            1484.3340,   15380.6619]], dtype=torch.float64)
	q_value: tensor([[-31.2290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5513254029785326, distance: 0.7665181345153199 entropy 12.62466087312706
epoch: 81, step: 15
	action: tensor([[ 11373.0651,  19899.2502, -54216.3214,  48243.8206,  -5309.8785,
          83359.8697,  12867.4338]], dtype=torch.float64)
	q_value: tensor([[-26.3062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3933845935664684, distance: 0.8912784609658255 entropy 12.22936754743216
epoch: 81, step: 16
	action: tensor([[ -8218.3700,  61333.7889,  17195.2190,  68284.4728,  62214.6229,
          87808.5504, -17026.0316]], dtype=torch.float64)
	q_value: tensor([[-26.5753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0014028159382364347, distance: 1.1451466248831215 entropy 12.106150829282594
epoch: 81, step: 17
	action: tensor([[ 26052.6087, -67084.7131,  39812.6148, -39065.3159,  26261.6374,
         -15523.7936, -57027.2997]], dtype=torch.float64)
	q_value: tensor([[-25.9116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.196562798266458
epoch: 81, step: 18
	action: tensor([[  17595.5900, -127383.5670,   73516.7951,  -78866.2945,  -47495.0109,
          -44964.6753,  -27670.7659]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5175001987568915, distance: 0.7948867589477351 entropy 11.875201075344103
epoch: 81, step: 19
	action: tensor([[  -532.6476, -53699.3608,  -6873.2349,  27472.7305, 113111.6042,
         -16712.4926, -40744.5380]], dtype=torch.float64)
	q_value: tensor([[-25.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5821261038284606, distance: 1.4393858908338801 entropy 12.233295381013734
epoch: 81, step: 20
	action: tensor([[-34863.9222, -71112.0604,  13182.7756,  71308.4074, -66694.4018,
          46560.2623, -54321.3508]], dtype=torch.float64)
	q_value: tensor([[-25.4733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21777154155413947, distance: 1.0121001735035984 entropy 12.255345392198754
epoch: 81, step: 21
	action: tensor([[ 96454.7610, -66505.4244, -19625.7312,  41265.9294, -84946.9339,
         135656.5541, -79270.7785]], dtype=torch.float64)
	q_value: tensor([[-27.0056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10482965023313429, distance: 1.2028302833809117 entropy 12.469935522597032
epoch: 81, step: 22
	action: tensor([[-29315.2922, -17864.5836, -42078.1129,  23492.5085,   5295.8851,
         -27604.1651, -10995.7525]], dtype=torch.float64)
	q_value: tensor([[-29.9837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21943137576102123, distance: 1.0110258002915076 entropy 12.379272016479517
epoch: 81, step: 23
	action: tensor([[ -28353.1778, -104953.7963,    4121.4510,   -2424.5407,   20527.4767,
           43807.0880,   35021.2274]], dtype=torch.float64)
	q_value: tensor([[-23.9200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7885016670486016, distance: 1.53038735248914 entropy 12.161443175217647
epoch: 81, step: 24
	action: tensor([[-40025.2277, -54197.9158, -52377.6111,  17766.0083,  -8139.9982,
          33391.1676, -78705.1110]], dtype=torch.float64)
	q_value: tensor([[-24.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6696442825098414, distance: 1.4786611776088343 entropy 12.069585856685524
epoch: 81, step: 25
	action: tensor([[ -69105.6535,  -42352.4199,   37917.8851, -162101.0984,   31624.7629,
          -42350.8484,  -25375.9353]], dtype=torch.float64)
	q_value: tensor([[-29.6621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24874642678609793, distance: 1.2787740733434487 entropy 12.499114856739181
epoch: 81, step: 26
	action: tensor([[-120154.4244,  -93439.4736,  -28751.4078,  -21631.3064,  104859.5889,
         -101685.4137,  -64981.8083]], dtype=torch.float64)
	q_value: tensor([[-26.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30425076748029345, distance: 0.9545156672892887 entropy 12.337205964938558
epoch: 81, step: 27
	action: tensor([[-156008.3185,  -65344.6945,   96167.5489,  -11044.0942,   20190.7793,
           25748.5832,  -71258.2053]], dtype=torch.float64)
	q_value: tensor([[-27.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23459296063430524, distance: 1.0011586852479653 entropy 12.298452521847805
epoch: 81, step: 28
	action: tensor([[ 45790.4670, -21764.7578, -38978.7810, -45741.7055,  24170.8577,
         -34917.0621,  67786.5501]], dtype=torch.float64)
	q_value: tensor([[-29.7377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308493695987352, distance: 0.9516007268593573 entropy 12.42508506835985
epoch: 81, step: 29
	action: tensor([[ -18426.0133,   75143.7038,  -16714.1329,  -11209.6497,  -70843.2672,
         -158641.9307,   16439.0442]], dtype=torch.float64)
	q_value: tensor([[-28.2529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3865055084786243, distance: 0.8963178121801628 entropy 12.296066522188594
epoch: 81, step: 30
	action: tensor([[  8180.1369,   -835.7855,  30311.8761,  50584.9571,  -8427.3126,
          36169.3115, -12874.6443]], dtype=torch.float64)
	q_value: tensor([[-34.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5736227394649445, distance: 0.7472289865677891 entropy 12.298480098993164
epoch: 81, step: 31
	action: tensor([[-21821.5215, -21393.6197, -41146.9781,   9711.3860,  -4861.2998,
          19821.9288, -25010.3806]], dtype=torch.float64)
	q_value: tensor([[-22.5695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019810410239405662, distance: 1.1329525888261707 entropy 12.072954975894072
epoch: 81, step: 32
	action: tensor([[-33770.9606, -51952.6803,  77650.9932,  28123.1739,  87258.4562,
          27000.6489,  35274.3962]], dtype=torch.float64)
	q_value: tensor([[-27.8850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07043201759051199, distance: 1.1839578425703194 entropy 12.485120512570816
epoch: 81, step: 33
	action: tensor([[  76907.7997,   13391.1667, -188687.7426,   40979.4181,   63540.1075,
         -194023.3394,  -53696.7578]], dtype=torch.float64)
	q_value: tensor([[-30.3485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7858392355555808, distance: 0.5295739910507764 entropy 12.60114003619915
epoch: 81, step: 34
	action: tensor([[ 36297.2196, -27714.7713, -24412.9351,   5597.9658,  79023.0778,
         -43170.4611, -59292.4161]], dtype=torch.float64)
	q_value: tensor([[-28.6298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045939616056511756, distance: 1.1177498612729735 entropy 12.270062180040663
epoch: 81, step: 35
	action: tensor([[ 84257.4736,  -4221.3794,  27965.3756,  27159.9497,   1502.7221,
         -66725.4364,  -9972.4614]], dtype=torch.float64)
	q_value: tensor([[-29.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2611196367061396, distance: 0.9836571557957418 entropy 12.399214576483473
epoch: 81, step: 36
	action: tensor([[ -4047.9637, -12767.1143,  60882.4762,  -4032.5703,  27462.1897,
         -91104.6083, -16744.9443]], dtype=torch.float64)
	q_value: tensor([[-28.4403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11913947403729264, distance: 1.0740148035607222 entropy 12.166804694783892
epoch: 81, step: 37
	action: tensor([[-3.2782e+04,  1.9899e+04, -5.3725e+04,  8.3791e+04,  2.4667e+04,
         -6.3188e+01,  2.3260e+04]], dtype=torch.float64)
	q_value: tensor([[-24.6160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27983568662718983, distance: 1.2945946297821214 entropy 12.17150394927568
epoch: 81, step: 38
	action: tensor([[ -5933.1993,  42543.6094,  53610.5383,  43335.7133, -83728.2278,
          49479.1993, -77993.6143]], dtype=torch.float64)
	q_value: tensor([[-31.4796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00759453688655376, distance: 1.1486814172130273 entropy 12.418070080537577
epoch: 81, step: 39
	action: tensor([[ 26607.1880,  14727.9580,  75528.0430,   -172.2162, -10867.2956,
         -26928.3572, -19728.7042]], dtype=torch.float64)
	q_value: tensor([[-29.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.282003508155045
epoch: 81, step: 40
	action: tensor([[  9921.9337,   1721.1670, -25849.1548,  22800.4528,  11923.7661,
          -4371.0908,  -7693.6177]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32556515382098217, distance: 0.9397810721168941 entropy 11.875201075344103
epoch: 81, step: 41
	action: tensor([[-61282.8403, -79554.4313,  51266.3127, -15646.6493, -18729.8349,
         -49843.8121,  36003.6850]], dtype=torch.float64)
	q_value: tensor([[-28.6021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.26385971049279
epoch: 81, step: 42
	action: tensor([[-12268.0550, -24924.9821,  -5703.1129, -17771.1501, -17996.8227,
          16696.3495,  27800.7923]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.875201075344103
epoch: 81, step: 43
	action: tensor([[  6988.1036, -23199.4486,  13669.1773,  66072.8387, -15905.9370,
          -1734.4085, -51486.9353]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2195622333265137, distance: 1.0109410505872085 entropy 11.875201075344103
epoch: 81, step: 44
	action: tensor([[  6525.1587, -23246.6136,  18252.2145, -93641.2051,  -5800.6343,
         -37303.8103,  47573.2438]], dtype=torch.float64)
	q_value: tensor([[-30.8035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17315758058709285, distance: 1.04056225361119 entropy 12.445100665331049
epoch: 81, step: 45
	action: tensor([[ 10190.2142, -56943.1292,  39777.0953, -19369.1087,  -9814.0322,
         -60300.7532,  23570.5222]], dtype=torch.float64)
	q_value: tensor([[-25.7923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3342038390605053, distance: 0.9337429516907266 entropy 12.154646105698733
epoch: 81, step: 46
	action: tensor([[ -2305.6447, -29295.1662,  -3662.8795,  37211.4869,  55517.6827,
         -29136.0115,  40587.4423]], dtype=torch.float64)
	q_value: tensor([[-27.1982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3694831639059313, distance: 1.3391679349606687 entropy 12.386081066830018
epoch: 81, step: 47
	action: tensor([[-35601.7517, -48618.6912, -30871.5177,   7843.0821,  20243.2773,
          15499.3437,  12647.9511]], dtype=torch.float64)
	q_value: tensor([[-25.1133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13647773434868893, distance: 1.0633921827014365 entropy 12.17099501549051
epoch: 81, step: 48
	action: tensor([[  35712.7077, -201217.5577,   59443.6401,   55754.0571,  -66655.8772,
          -22223.4102,   16165.0533]], dtype=torch.float64)
	q_value: tensor([[-27.8032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23344635585334705, distance: 1.0019082888492346 entropy 12.428748065432924
epoch: 81, step: 49
	action: tensor([[ 50746.1966, -43547.6478, -56031.3320,  30046.2344, 139779.0994,
          70250.7491, -70346.8154]], dtype=torch.float64)
	q_value: tensor([[-28.9473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35843984747841495, distance: 1.3337575764159673 entropy 12.365189245192667
epoch: 81, step: 50
	action: tensor([[-116568.8521,   55066.1947,  -49870.6390,  -21113.5234,  -75992.5775,
          114098.7564, -108135.1174]], dtype=torch.float64)
	q_value: tensor([[-29.6600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.539865851869772
epoch: 81, step: 51
	action: tensor([[ 46361.1445,   2137.0250, -65756.2719,  17622.0091,   5083.9543,
          11633.5883,   3379.9320]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5733921858929386, distance: 0.7474309825999379 entropy 11.875201075344103
epoch: 81, step: 52
	action: tensor([[ 39746.1094, -53104.5546,  31627.5696,  40678.5065,  -8234.6187,
         -16959.6169,  15588.4193]], dtype=torch.float64)
	q_value: tensor([[-29.6780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16090508161413464, distance: 1.0482436464428304 entropy 12.264354592987448
epoch: 81, step: 53
	action: tensor([[-56478.3270, -70563.0648,  54672.8516,  53037.6530, -24886.7645,
          19998.1446, -46288.4103]], dtype=torch.float64)
	q_value: tensor([[-30.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07784156360712235, distance: 1.1880484626604026 entropy 12.3523499579714
epoch: 81, step: 54
	action: tensor([[-99381.5835,  59666.7993, -44726.7851,  43671.0634, -71415.0717,
         -54831.8516, -64754.5461]], dtype=torch.float64)
	q_value: tensor([[-26.7518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2143104442763819, distance: 1.2610187916753492 entropy 12.330376373572218
epoch: 81, step: 55
	action: tensor([[  -7259.4343,   11224.1126,   99051.1112, -106288.4243,   11886.7508,
           58322.5619,   96968.4050]], dtype=torch.float64)
	q_value: tensor([[-32.4917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19232563437078065, distance: 1.0284302511858292 entropy 12.42613856523894
epoch: 81, step: 56
	action: tensor([[ -20180.9839, -115097.7568,  -28002.7559,   49643.6257,   11434.2711,
          -45205.0161,   35329.3963]], dtype=torch.float64)
	q_value: tensor([[-27.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20117057991263487, distance: 1.254177590382481 entropy 12.17722997303947
epoch: 81, step: 57
	action: tensor([[-85763.6245, -52800.3132, -50395.4095,  36508.4362,  -9185.6187,
          87867.6505,  38761.3204]], dtype=torch.float64)
	q_value: tensor([[-28.4982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13553006021014635, distance: 1.0639755335733698 entropy 12.345075838897099
epoch: 81, step: 58
	action: tensor([[ -2183.8101, -43543.6876,  10426.1115,  38246.0638,  23882.6232,
          98938.8912, -35256.5908]], dtype=torch.float64)
	q_value: tensor([[-25.2779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030989053069611305, distance: 1.1619400472136214 entropy 12.284993864977233
epoch: 81, step: 59
	action: tensor([[   5452.1456,   -9753.7589,  -80056.9539, -128031.6967,   45998.8702,
            2735.7019,   36226.7648]], dtype=torch.float64)
	q_value: tensor([[-27.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21276576721934948, distance: 1.260216489985999 entropy 12.37745717289324
epoch: 81, step: 60
	action: tensor([[ 91480.0383, -85254.8450, -30426.7691, -57457.1256, -58642.3523,
         -61078.9732,  86141.2507]], dtype=torch.float64)
	q_value: tensor([[-28.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3653907877795036, distance: 0.911611676712707 entropy 12.323246492777324
epoch: 81, step: 61
	action: tensor([[ -48397.8696,  -27703.5582,   92976.6654,  138489.6210,   -5928.4946,
          108344.3040, -104176.4653]], dtype=torch.float64)
	q_value: tensor([[-28.8766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9052559594717875, distance: 1.5795499264127593 entropy 12.274613435572551
epoch: 81, step: 62
	action: tensor([[ -23930.9718,   71110.9444,  -43768.0853, -104901.3885,  -17761.0218,
           -3624.5172,  -73485.7385]], dtype=torch.float64)
	q_value: tensor([[-27.5285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27285366621081875, distance: 1.2910585322715755 entropy 12.392228150472741
epoch: 81, step: 63
	action: tensor([[-106450.9401,   80599.3022,  -34327.0657,   13477.5733,  -13212.6930,
           44087.7264,   35030.4144]], dtype=torch.float64)
	q_value: tensor([[-37.0501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3265441336202337, distance: 1.3180064784118293 entropy 12.395250028908636
epoch: 81, step: 64
	action: tensor([[-59822.8289,  19397.4786, -13769.2038,  92410.8445, -54859.3375,
         -87928.2694,  29263.8192]], dtype=torch.float64)
	q_value: tensor([[-29.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16527034786547268, distance: 1.2352931720622582 entropy 12.035945150983517
epoch: 81, step: 65
	action: tensor([[  -410.3362, -31073.0721, -43349.9139, 219795.7405, -25336.5173,
          -2047.0675, 110368.2459]], dtype=torch.float64)
	q_value: tensor([[-35.0820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15925545706468358, distance: 1.2321008799834723 entropy 12.593500921926532
epoch: 81, step: 66
	action: tensor([[ -19508.6860, -136759.9066,  -13599.7237,  103838.1447,  102059.1606,
           85897.9847,   60918.4077]], dtype=torch.float64)
	q_value: tensor([[-31.7946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35824904149695214, distance: 1.333663903567347 entropy 12.624135334332333
epoch: 81, step: 67
	action: tensor([[ -79436.0548, -137999.6206,   -1474.8738,   67941.7390,   20587.4088,
           31680.9775,   69270.5962]], dtype=torch.float64)
	q_value: tensor([[-29.7019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6646653461163041, distance: 1.4764548224048748 entropy 12.589177240679843
epoch: 81, step: 68
	action: tensor([[-79714.1311, -39296.2319, -39205.4217,  49440.3544,   1661.5700,
          62043.5493,  21197.0162]], dtype=torch.float64)
	q_value: tensor([[-29.2121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4765943674429334, distance: 1.3905522015623653 entropy 12.49802507760061
epoch: 81, step: 69
	action: tensor([[-27268.6260, -30861.5471, -86657.0217, -82839.7626, -23799.9478,
          -3314.7192, -28854.8288]], dtype=torch.float64)
	q_value: tensor([[-29.3521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5930797996733876, distance: 1.4443600201570723 entropy 12.52510842891635
epoch: 81, step: 70
	action: tensor([[ 14139.9701,  48169.9248,  94584.3051,  90664.2199, -32715.0327,
         -32030.3188,  30491.0066]], dtype=torch.float64)
	q_value: tensor([[-26.2757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.240628561666009
epoch: 81, step: 71
	action: tensor([[-36752.9690, -37579.2448, -86849.5016,  27787.0338,  -5887.6479,
           7925.0422, -23600.7674]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23629030433906228, distance: 1.0000479994067148 entropy 11.875201075344103
epoch: 81, step: 72
	action: tensor([[ 20225.7770, -96946.0686, -26184.2225,  72195.8360,  18617.1921,
         -43070.8964, -89418.8521]], dtype=torch.float64)
	q_value: tensor([[-27.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25001592192419686, distance: 1.279423919081063 entropy 12.402538319718483
epoch: 81, step: 73
	action: tensor([[-52036.6525, -42968.4211,  50170.8921,  -6071.2656,  36559.3139,
         -39105.0763, -61807.6546]], dtype=torch.float64)
	q_value: tensor([[-29.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7817218427440102, distance: 1.5274839140115877 entropy 12.455054451720954
epoch: 81, step: 74
	action: tensor([[-32625.6069, -79299.5057,  -7892.0101,  42692.9003, -49407.1477,
         -59996.9170,  -4452.4130]], dtype=torch.float64)
	q_value: tensor([[-27.5242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07765270116320977, distance: 1.0990157931330033 entropy 12.337683085216808
epoch: 81, step: 75
	action: tensor([[ -75888.2047,  -65773.6704,   62349.4099,  103745.8988,  153771.9872,
         -111429.7768,  -31632.6276]], dtype=torch.float64)
	q_value: tensor([[-30.0439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17584374915028145, distance: 1.038870637785191 entropy 12.570295179902514
epoch: 81, step: 76
	action: tensor([[-134229.9145,  -83107.3961,   63419.1300, -112437.7465,  -40965.8717,
          -30140.8352,  -22886.6052]], dtype=torch.float64)
	q_value: tensor([[-32.8074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5289100622536693, distance: 1.4149714029815785 entropy 12.616946647383488
epoch: 81, step: 77
	action: tensor([[ 10401.7715, -46450.2474, -18755.0497, 116036.9771, -98812.2912,
          12294.8007,  21603.6020]], dtype=torch.float64)
	q_value: tensor([[-28.5635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34736488346319616, distance: 0.9244680629025954 entropy 12.47488344540712
epoch: 81, step: 78
	action: tensor([[-133266.5503,   43126.9535, -106150.1398,   -3342.2030,  -16421.4963,
           23702.7231,   62997.8809]], dtype=torch.float64)
	q_value: tensor([[-26.4740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.432020714699757
epoch: 81, step: 79
	action: tensor([[ -1207.6923, -25895.4320, -18220.3669,  41770.3072,   7991.8004,
          -6605.2947,  61653.2006]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6284191042473537, distance: 1.4602922745949798 entropy 11.875201075344103
epoch: 81, step: 80
	action: tensor([[-23050.4412, -10048.8515,  18802.9456,  74631.7769,  -9841.0140,
         -80479.0093, -50653.9269]], dtype=torch.float64)
	q_value: tensor([[-25.7094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5146229395093562, distance: 1.4083446823818784 entropy 12.228478193410055
epoch: 81, step: 81
	action: tensor([[-86075.4964, -22568.8105,  75861.5956,  13194.7563, -88428.8435,
          76124.7294, -46018.2225]], dtype=torch.float64)
	q_value: tensor([[-28.1093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08946428563002118, distance: 1.1944368465557988 entropy 12.419765404951688
epoch: 81, step: 82
	action: tensor([[ -9218.6968, -84240.6170, -31913.3430,   8388.8682, -31723.4358,
          30687.5756, -29588.5941]], dtype=torch.float64)
	q_value: tensor([[-27.0586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06427439584712114, distance: 1.1069575070397895 entropy 12.335047638195988
epoch: 81, step: 83
	action: tensor([[-110799.4731, -139049.8174,    1997.8573,  -59272.4476,   59420.1140,
           43477.2349,   26863.1652]], dtype=torch.float64)
	q_value: tensor([[-26.7063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9407068777408425, distance: 1.5941774650995277 entropy 12.317935417414816
epoch: 81, step: 84
	action: tensor([[-59082.7250,  24021.6332,  84833.2665,  72812.2758, -46674.8091,
           -573.2306,  -3426.7968]], dtype=torch.float64)
	q_value: tensor([[-25.7650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11247171333494399, distance: 1.2069830793820482 entropy 12.32784141058295
epoch: 81, step: 85
	action: tensor([[   6147.3019,  -69040.4559,   20826.2801,   62623.0049,  -20503.4708,
           92056.8053, -130337.5682]], dtype=torch.float64)
	q_value: tensor([[-30.3413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21371025728584359, distance: 1.2607071165924324 entropy 12.346413203081875
epoch: 81, step: 86
	action: tensor([[-144779.4102,   -9512.6877, -141102.6177,   25323.9920,   36021.9403,
          -57334.0246,  -43087.8464]], dtype=torch.float64)
	q_value: tensor([[-23.6265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8066368132226969, distance: 1.5381267352452714 entropy 12.312923293546547
epoch: 81, step: 87
	action: tensor([[-75626.5875, -41720.3779,  37124.5520, -26894.2627,  -3141.6118,
         -82157.7267,  84138.3028]], dtype=torch.float64)
	q_value: tensor([[-28.2054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5146234448831284, distance: 1.408344917338179 entropy 12.429715707254042
epoch: 81, step: 88
	action: tensor([[ 48981.4620,  51254.6588, -11558.2096,  -4965.3688, -74867.3434,
          37027.9750,  36377.5390]], dtype=torch.float64)
	q_value: tensor([[-28.3684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.364412637100328
epoch: 81, step: 89
	action: tensor([[-51931.7943,  27313.6167, -65993.4837,   5613.1480, -43064.7450,
         -12276.3585,  24821.1585]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11766336292560509, distance: 1.0749143225661975 entropy 11.875201075344103
epoch: 81, step: 90
	action: tensor([[-86869.0889,  22190.0845,  -8750.1158,  38848.3006,  96160.9913,
          34573.3507,  79985.4960]], dtype=torch.float64)
	q_value: tensor([[-30.4667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17368450088471388, distance: 1.2397450360218187 entropy 12.303456177947933
epoch: 81, step: 91
	action: tensor([[ -48496.9296,  -54431.9087,  -87117.2108,    6454.3713, -100220.3261,
          -68313.8198,   69383.0466]], dtype=torch.float64)
	q_value: tensor([[-34.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26665707372025127, distance: 0.9799642823818138 entropy 12.498217447122695
epoch: 81, step: 92
	action: tensor([[-139857.9296,  -60009.9810,  -36705.0583,  -10116.1489,  -23037.4266,
           -8350.2639,   76059.3212]], dtype=torch.float64)
	q_value: tensor([[-26.4210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6827186649805246, distance: 1.4844393193434064 entropy 12.434397985525877
epoch: 81, step: 93
	action: tensor([[ -29597.7414,   11612.5523, -104414.2847,  -64647.5761,  -77444.3868,
           37303.7583,  -60716.2893]], dtype=torch.float64)
	q_value: tensor([[-28.2401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1331609109790015, distance: 1.2181548136612583 entropy 12.426621228071564
epoch: 81, step: 94
	action: tensor([[ 26583.0749, -36490.1789,  48783.6359,  44223.3745, -31455.0458,
         -47566.8229, -21392.6091]], dtype=torch.float64)
	q_value: tensor([[-33.3516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24433088059439068, distance: 1.2765112094895525 entropy 12.238383271044539
epoch: 81, step: 95
	action: tensor([[-16703.6744, -67450.4985,   1967.0928,  18834.5680,  -7870.9608,
          37635.3647,  62362.5350]], dtype=torch.float64)
	q_value: tensor([[-23.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18031684129413394, distance: 1.036047576879072 entropy 12.01966852801611
epoch: 81, step: 96
	action: tensor([[  44955.8880, -139374.6347,  -17418.7146,  -42119.3974,   32973.9019,
         -121214.2561,   26909.4132]], dtype=torch.float64)
	q_value: tensor([[-24.5306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02078503960624456, distance: 1.1561757111798132 entropy 12.323279714672063
epoch: 81, step: 97
	action: tensor([[-66070.2079,  76714.3930, -85814.3683,  49574.1882,  31721.9248,
           3574.6006,   6318.7903]], dtype=torch.float64)
	q_value: tensor([[-26.8292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.241478189900572
epoch: 81, step: 98
	action: tensor([[ 34761.3080, -38707.2265, -46377.4793, -21034.1084, -18121.3697,
          -3303.9075,  13550.3436]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.875201075344103
epoch: 81, step: 99
	action: tensor([[-13415.7673, -17293.3865,  26419.4556,  63056.1607, -27119.6288,
         -25007.6002, -55036.5954]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6559919247713948, distance: 1.4726034059344708 entropy 11.875201075344103
epoch: 81, step: 100
	action: tensor([[-16822.4887, -90885.0060,   7770.8346,  33447.3377, -47386.1809,
         -14367.5497,  -2238.0634]], dtype=torch.float64)
	q_value: tensor([[-28.7674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3427663465793771, distance: 1.326040893273856 entropy 12.437416523403416
epoch: 81, step: 101
	action: tensor([[-15499.9474,  65783.1112,  17587.8941,  11355.4709, -28668.5390,
         -55550.2129,  -7665.2835]], dtype=torch.float64)
	q_value: tensor([[-27.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18656486517053128, distance: 1.0320913940949887 entropy 12.354850119098048
epoch: 81, step: 102
	action: tensor([[-27545.9303, -10885.5842,  28117.2169, -30384.8590, -75402.6902,
          48839.3826,   9653.2555]], dtype=torch.float64)
	q_value: tensor([[-29.3190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4602448081827588, distance: 1.3828323429695055 entropy 12.177727369254454
epoch: 81, step: 103
	action: tensor([[ -92505.6314,  -21745.4460,  -32480.7005,    -424.9886,  -16359.7275,
           88883.1213, -100565.0524]], dtype=torch.float64)
	q_value: tensor([[-24.9013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36105543748862723, distance: 1.3350409918584119 entropy 12.28074069105472
epoch: 81, step: 104
	action: tensor([[-135154.5925,  -47398.9396,   19167.4891,   -5644.7058,   46385.8368,
           18280.5999,  -22275.0373]], dtype=torch.float64)
	q_value: tensor([[-32.2181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16747346466235713, distance: 1.0441327925914055 entropy 12.464267230899923
epoch: 81, step: 105
	action: tensor([[  14410.3330, -122745.9203,  151661.4000, -123973.5416,  106558.9092,
          152122.5799,   40766.5445]], dtype=torch.float64)
	q_value: tensor([[-33.7900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5465682890736252, distance: 0.7705709599276566 entropy 12.535883252584727
epoch: 81, step: 106
	action: tensor([[-97277.8308, -48270.4730,  11433.0132, 114329.4912,  36219.8302,
          43394.4628, -36366.9710]], dtype=torch.float64)
	q_value: tensor([[-28.9534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6995477026520136, distance: 1.4918438650534913 entropy 12.366192385531109
epoch: 81, step: 107
	action: tensor([[-115700.0425,  -24969.3159,   45964.4384,    4468.8679,  -65046.4779,
           20316.4261,  -49004.7717]], dtype=torch.float64)
	q_value: tensor([[-28.3880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3442189695491624, distance: 1.3267579639650646 entropy 12.46278547109013
epoch: 81, step: 108
	action: tensor([[ -19532.8555,  -36934.1919,  176945.8782,  -29468.3458, -259614.4656,
            -560.4130,  -13625.7455]], dtype=torch.float64)
	q_value: tensor([[-31.5026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08857815022998028, distance: 1.1939509893789173 entropy 12.617091069426575
epoch: 81, step: 109
	action: tensor([[-21979.4575, -24786.5938, -29597.4108, -11328.2584,  56239.9077,
          -8968.4704,  63809.7424]], dtype=torch.float64)
	q_value: tensor([[-25.5192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20753789036195092, distance: 1.2574973449723632 entropy 12.328805613599783
epoch: 81, step: 110
	action: tensor([[-26008.6347, -89859.1034,  95605.0131, -26803.0666,  15289.7597,
          26690.3793, -18420.0165]], dtype=torch.float64)
	q_value: tensor([[-29.5333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19104929155897388, distance: 1.0292425286622195 entropy 12.42923292608566
epoch: 81, step: 111
	action: tensor([[-65901.0150,  74927.3046, -38005.8111,  17490.4241, 100234.1472,
          37417.4907,   5536.3506]], dtype=torch.float64)
	q_value: tensor([[-30.5178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5790283049902399, distance: 0.7424772331947087 entropy 12.48086724214345
epoch: 81, step: 112
	action: tensor([[ 39150.0739,  22431.8326, -12432.3771,  -9481.1597,  28584.4926,
          20556.0012,  10046.7163]], dtype=torch.float64)
	q_value: tensor([[-27.2486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6304010055785039, distance: 0.6957001287750505 entropy 12.171531930880986
epoch: 81, step: 113
	action: tensor([[  10531.2583, -111352.0057,  -53240.7293,  -24366.9929,   87129.0404,
          -12992.8980,  -64059.9376]], dtype=torch.float64)
	q_value: tensor([[-30.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.190733721878669
epoch: 81, step: 114
	action: tensor([[-92038.9575, -29964.5879, -29281.5783,  33394.5632, -26701.2337,
         -14710.9792, -76219.0183]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.875201075344103
epoch: 81, step: 115
	action: tensor([[-45474.1460,  38655.8589, -32775.3370, -38144.0248, -12184.3155,
          28170.4349,  21508.3324]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.875201075344103
epoch: 81, step: 116
	action: tensor([[ 16881.1988,   8578.0938, -49443.6677,  22390.6480,  16116.0270,
         -19281.3695,  40891.1437]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.875201075344103
epoch: 81, step: 117
	action: tensor([[ -6524.1968,  24844.7344,   5927.0787,  23306.1625, -43961.3149,
          51483.4016,  12230.8385]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06869527216093729, distance: 1.1043394766860515 entropy 11.875201075344103
epoch: 81, step: 118
	action: tensor([[ 15736.6454, -47034.4828,  31656.0393,  17586.2739,  35857.2186,
          69426.8694,   9714.9153]], dtype=torch.float64)
	q_value: tensor([[-30.6106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.050892687934379
epoch: 81, step: 119
	action: tensor([[ 64187.5077, -14779.0011,  -1044.1019,  21217.4477, -26658.4952,
         -43468.9482, -31333.0747]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06684408606740067, distance: 1.1054364983853573 entropy 11.875201075344103
epoch: 81, step: 120
	action: tensor([[-15275.1816,  18830.9503,  28497.4572, -12822.3554,  63336.1898,
          -2243.2488,  22817.7694]], dtype=torch.float64)
	q_value: tensor([[-26.3538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.197517551833954
epoch: 81, step: 121
	action: tensor([[ 26532.4841, -40383.1864,  32515.4349,  52267.5035,   9432.7689,
          43881.3015,  -4943.7345]], dtype=torch.float64)
	q_value: tensor([[-29.1616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5610211296007729, distance: 0.7581907847273526 entropy 11.875201075344103
epoch: 81, step: 122
	action: tensor([[ -2583.8103, -24575.7630, -95759.2878, -21895.3920,  33986.9612,
         -56938.2177,  98678.2377]], dtype=torch.float64)
	q_value: tensor([[-24.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06199003049853391, distance: 1.1792799456476606 entropy 12.182486509776046
epoch: 81, step: 123
	action: tensor([[ 21687.1824, -23319.9307,  29529.3967, -34774.0990, -36661.8316,
          66566.1590,  67097.3573]], dtype=torch.float64)
	q_value: tensor([[-24.8042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17844053836098284, distance: 1.0372326859730168 entropy 12.202209328615137
epoch: 81, step: 124
	action: tensor([[ -4724.5956, -15276.2573,  24673.3375,   3430.5310,  41206.6050,
          30312.2114, 121850.6061]], dtype=torch.float64)
	q_value: tensor([[-26.8368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44099968024476, distance: 1.373689679745404 entropy 12.142603250559576
epoch: 81, step: 125
	action: tensor([[-28559.5988,  38526.3600, -10447.1862, -50354.9385,  11950.5225,
         -59189.3194,  21123.9526]], dtype=torch.float64)
	q_value: tensor([[-23.4787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25782060829685705, distance: 0.9858506764584151 entropy 12.100589855803344
epoch: 81, step: 126
	action: tensor([[-68232.9310,  -6321.6323,  74308.6928,   2485.7013,    600.0565,
          12642.4395, -86768.5186]], dtype=torch.float64)
	q_value: tensor([[-31.3759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7598598350321386, distance: 1.5180837555185116 entropy 12.138392437759
epoch: 81, step: 127
	action: tensor([[  22777.2966,   91699.2096, -124249.4203,   36566.8087,   80216.1927,
           64930.2578,   44582.6155]], dtype=torch.float64)
	q_value: tensor([[-28.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5977833221807085, distance: 0.7257494753450439 entropy 12.449613849188978
LOSS epoch 81 actor 322.2871951145241 critic 230.3753064716854
epoch: 82, step: 0
	action: tensor([[ 14224.8899, -78694.6125, -47264.8131,  19262.8731,  49887.1062,
          53405.9503,  71548.6484]], dtype=torch.float64)
	q_value: tensor([[-25.4178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15833338400286023, distance: 1.0498487707585573 entropy 12.36516758985152
epoch: 82, step: 1
	action: tensor([[-95063.4066,  -7930.7022, -43384.9835, 104298.1886,  23493.8612,
         -21684.3841, -90960.5701]], dtype=torch.float64)
	q_value: tensor([[-25.9095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23150187650557075, distance: 1.2699137695652452 entropy 12.401696869672827
epoch: 82, step: 2
	action: tensor([[-48421.1087, -60739.9835,  -7392.0907,  -3627.0020,  19215.7574,
          29851.6187, -45801.1129]], dtype=torch.float64)
	q_value: tensor([[-23.2526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7796715353523807, distance: 1.5266047888355405 entropy 12.291034054821022
epoch: 82, step: 3
	action: tensor([[ -57130.4208, -130776.4023,  -11090.6472,   62063.3915,   92668.6438,
           16077.2887,   33616.5162]], dtype=torch.float64)
	q_value: tensor([[-25.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1788344212667221, distance: 1.2424619501228806 entropy 12.428137077490504
epoch: 82, step: 4
	action: tensor([[-127591.0056,  -67157.9321,   60116.3082, -110690.6464,  -92468.1884,
           60790.3069,    7717.6543]], dtype=torch.float64)
	q_value: tensor([[-25.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6189003001945363, distance: 1.4560180036856738 entropy 12.579662400705647
epoch: 82, step: 5
	action: tensor([[ -5591.7291, -54135.1336,  12465.6903,   1545.1146, -20266.1996,
          75058.1552, -74078.0579]], dtype=torch.float64)
	q_value: tensor([[-22.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8904210437181295, distance: 1.573388475615772 entropy 12.353015101742034
epoch: 82, step: 6
	action: tensor([[-27585.1244,  83802.7567, 119382.8843, 156998.9249,  73086.0231,
         -28129.6291, -15507.3135]], dtype=torch.float64)
	q_value: tensor([[-24.6369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.492028405107506
epoch: 82, step: 7
	action: tensor([[  8642.8361,   4028.7301, -19016.8464, -15111.3254, -30765.1412,
          14487.6397,  41182.9557]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32391549703870637, distance: 0.9409297148462545 entropy 11.9070930096095
epoch: 82, step: 8
	action: tensor([[-51981.2198, -21340.6378, -70140.8327,  -8659.6021, -16296.8958,
          11434.3422, -45177.8662]], dtype=torch.float64)
	q_value: tensor([[-24.6578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029364620633900662, distance: 1.1610243065624293 entropy 12.330636172693149
epoch: 82, step: 9
	action: tensor([[-69420.0870,  11504.7789,  32246.0294,  15766.2892, 118767.3934,
         -36784.7473,  71216.2072]], dtype=torch.float64)
	q_value: tensor([[-25.3789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23749315631392487, distance: 1.272999098767605 entropy 12.444732796977027
epoch: 82, step: 10
	action: tensor([[ 10147.4994, -76321.3734,  34448.1726, -73657.0482,   6659.8464,
          -7946.5842, -41276.5265]], dtype=torch.float64)
	q_value: tensor([[-27.7713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3695300075804504, distance: 0.9086338329566404 entropy 12.351860755869591
epoch: 82, step: 11
	action: tensor([[ -65816.2861, -156458.7699,    3650.1634,   88880.6838,    7131.6282,
            4012.7733,  -48128.7366]], dtype=torch.float64)
	q_value: tensor([[-24.6780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7881545881152792, distance: 1.5302388508154177 entropy 12.376515478399437
epoch: 82, step: 12
	action: tensor([[ 16764.4889,   3991.0133,  40560.1993,   5752.1080, -23829.4098,
          -4533.4607, 130898.1139]], dtype=torch.float64)
	q_value: tensor([[-19.5078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29657417643815076, distance: 0.9597670746362034 entropy 12.074342167302222
epoch: 82, step: 13
	action: tensor([[ -7474.6253, -49905.7325, -17481.2830,  39835.9887,  59658.0106,
         -14219.4337, -50413.5170]], dtype=torch.float64)
	q_value: tensor([[-24.6158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.050120710164571536, distance: 1.1726713235497597 entropy 12.213839987283563
epoch: 82, step: 14
	action: tensor([[   3287.1553,    8946.6699,  -21962.9271, -115182.8292,   83167.5005,
          -17916.5706,  101647.8387]], dtype=torch.float64)
	q_value: tensor([[-24.4004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.460423063095535
epoch: 82, step: 15
	action: tensor([[-24549.1435,   1121.0678, -70567.5562,  23579.8165,  39306.0197,
          11225.1273,  95508.4955]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02768734282531804, distance: 1.1283911281372627 entropy 11.9070930096095
epoch: 82, step: 16
	action: tensor([[ 11305.1736,  -2140.0482, -22956.9734,  26938.2421, -19629.2752,
         -12652.1652,  77869.3743]], dtype=torch.float64)
	q_value: tensor([[-26.8517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.07460228187921
epoch: 82, step: 17
	action: tensor([[ 13452.8107, -84375.5877,  -7977.9696,  28394.9280, -22870.6526,
          34293.8495, -26533.7829]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.9070930096095
epoch: 82, step: 18
	action: tensor([[  9642.4290, -63129.0080, -47376.0985,    903.6830,   4081.6181,
          53819.0336,  43757.8384]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05009539970399879, distance: 1.172657191350547 entropy 11.9070930096095
epoch: 82, step: 19
	action: tensor([[ 19039.7741,   7250.0812, -31616.8905,  54180.9249, -60168.2747,
          -5877.5385, -50042.9161]], dtype=torch.float64)
	q_value: tensor([[-21.7338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10035485740372174, distance: 1.0854062374479017 entropy 12.316515234309076
epoch: 82, step: 20
	action: tensor([[  8512.4050,  28797.1863, -23613.8490,  14422.8224,  34253.6801,
          23311.5648, 104472.8942]], dtype=torch.float64)
	q_value: tensor([[-23.9767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.154073997309881
epoch: 82, step: 21
	action: tensor([[ 49752.1065,  22087.0482,  67176.6692,  17855.8963,  52722.2584,
         -30318.3908,  -1211.5800]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.9070930096095
epoch: 82, step: 22
	action: tensor([[ 15881.5085, -20946.5567, -27763.3613, -26089.0494, -55030.6746,
          12491.4203,  60913.2667]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5026092843052292, distance: 0.8070594563021264 entropy 11.9070930096095
epoch: 82, step: 23
	action: tensor([[ 16246.1709, -36954.9266, 100706.5667, -94790.9547,  41699.4719,
         -10327.0636, -25825.0935]], dtype=torch.float64)
	q_value: tensor([[-24.3729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18526327962385558, distance: 1.2458452714307948 entropy 12.442247253164632
epoch: 82, step: 24
	action: tensor([[-138120.4213,    2422.7812,   99287.3873,   14188.8813,   41780.6692,
          -91356.4036,   -9302.5781]], dtype=torch.float64)
	q_value: tensor([[-23.4500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.253871603959876
epoch: 82, step: 25
	action: tensor([[-41443.4925, -32303.5592, -35596.9554, -11293.1239,  25530.1722,
          75512.8470,  16161.6027]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.9070930096095
epoch: 82, step: 26
	action: tensor([[ 10470.5729, -18265.5209,  13833.9948,  35326.0392,  11598.0245,
          84436.0985, -29371.4089]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14266502534704462, distance: 1.059575636557383 entropy 11.9070930096095
epoch: 82, step: 27
	action: tensor([[-34919.9139, -88568.5148, -99718.0724,  50397.1045, -10220.3182,
         -50336.0226,  12286.7779]], dtype=torch.float64)
	q_value: tensor([[-20.3047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1481904919817948, distance: 1.05615567019178 entropy 12.259112517168083
epoch: 82, step: 28
	action: tensor([[  18643.8034, -199116.1452,   92033.3803,   37691.2482,   88308.7481,
           69337.0934,   90479.9551]], dtype=torch.float64)
	q_value: tensor([[-27.2253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00848916087885443, distance: 1.1394766402566077 entropy 12.704549178528653
epoch: 82, step: 29
	action: tensor([[  -4750.3726,  -26723.7808,  -94584.8001,   42814.5538,   66264.2061,
           89712.0903, -133973.4002]], dtype=torch.float64)
	q_value: tensor([[-22.7724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17458868482408096, distance: 1.2402224819375343 entropy 12.484095136348662
epoch: 82, step: 30
	action: tensor([[ -80672.4205, -127111.3705,   29176.4947,  -90354.0801,  -70837.1280,
          -66298.1854,   69742.7230]], dtype=torch.float64)
	q_value: tensor([[-24.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39693046667122545, distance: 1.3525212210778805 entropy 12.572648442877746
epoch: 82, step: 31
	action: tensor([[ 28306.4246,  60653.4149, -32291.8190,  37044.2061, -86442.6261,
          -3943.7180,  50068.4917]], dtype=torch.float64)
	q_value: tensor([[-25.8002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.452517730896066
epoch: 82, step: 32
	action: tensor([[-36380.5561,  10997.8259,   1206.5772,  57684.9522,  12779.0894,
          20141.0023,  16041.8037]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10496797874979003, distance: 1.0826198355039358 entropy 11.9070930096095
epoch: 82, step: 33
	action: tensor([[  3177.9598,  13579.7164, -32028.1817,  62617.5694, -12509.3631,
         -24502.3251, -17507.7364]], dtype=torch.float64)
	q_value: tensor([[-23.2226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.841027777739336
epoch: 82, step: 34
	action: tensor([[-55822.4285,  -2888.9702, -11196.9430,  -1334.0640, -80440.1360,
         -26585.5352,  -7429.6120]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19382313669680884, distance: 1.2503358655439503 entropy 11.9070930096095
epoch: 82, step: 35
	action: tensor([[   616.9521,   8416.3220,  22203.0651,  82549.7340, -48344.0503,
          -9231.4398, -68762.5247]], dtype=torch.float64)
	q_value: tensor([[-23.6362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.372633794867692
epoch: 82, step: 36
	action: tensor([[-53985.5363,  -2571.6254, -56701.8479,  24318.5798,  31799.5628,
         118680.7189,  13364.1156]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2912323902255407, distance: 1.300345919262003 entropy 11.9070930096095
epoch: 82, step: 37
	action: tensor([[-35934.9176,  -8569.8557,  16032.8554, 112789.2266,  77517.9272,
         -31369.0005,   5070.8646]], dtype=torch.float64)
	q_value: tensor([[-22.9302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20603225391556113, distance: 1.0196664343946602 entropy 12.326807983045459
epoch: 82, step: 38
	action: tensor([[ 12718.0201, -39695.5780,   7551.3131,  37065.0950, -35098.6793,
         -27695.8161, -46232.5485]], dtype=torch.float64)
	q_value: tensor([[-23.6777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23598870862530175, distance: 1.0002454438156536 entropy 12.43645758602373
epoch: 82, step: 39
	action: tensor([[ 22459.4844,  38126.9868, -43450.1382,  75602.1920, -10582.4900,
          32351.4044, -20350.0314]], dtype=torch.float64)
	q_value: tensor([[-23.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.203611404774646
epoch: 82, step: 40
	action: tensor([[-38228.8343, -89637.9808,   6582.1661, -18029.8169, -61464.6534,
          40341.7938,  38257.7343]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3281478972133547, distance: 0.9379799036444021 entropy 11.9070930096095
epoch: 82, step: 41
	action: tensor([[   380.0905, -90025.9020, -98905.2445, 169906.5110,  50068.1240,
          16821.7909, -45198.7411]], dtype=torch.float64)
	q_value: tensor([[-27.5708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1798754244427916, distance: 1.243010424698277 entropy 12.510790760335771
epoch: 82, step: 42
	action: tensor([[ -16934.9405,  -90523.6385, -149310.9616,   21582.4802,   48993.4304,
          -52111.7966,   58624.7144]], dtype=torch.float64)
	q_value: tensor([[-24.0806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060056106202692194, distance: 1.1094498064161733 entropy 12.440261298638429
epoch: 82, step: 43
	action: tensor([[ -56118.2686,   15027.5067,   89616.2032, -132597.6457,   36907.7372,
           -3971.8999, -154186.6878]], dtype=torch.float64)
	q_value: tensor([[-28.3015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10459256880890999, distance: 1.0828468573479215 entropy 12.760229369576324
epoch: 82, step: 44
	action: tensor([[ -63074.8301,  -63411.3176,  -18464.3157,   14257.3017,   95356.2551,
         -148996.4517,  -27865.5974]], dtype=torch.float64)
	q_value: tensor([[-29.0658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20193431164247055, distance: 1.0222944733552235 entropy 12.359154300708495
epoch: 82, step: 45
	action: tensor([[  41381.4710, -136693.9006,   -5865.5893,   26297.7180,  -33467.6169,
          -26518.5050,  -46987.3311]], dtype=torch.float64)
	q_value: tensor([[-27.7708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4672948294891014, distance: 0.8352185846062787 entropy 12.549410076777667
epoch: 82, step: 46
	action: tensor([[-49020.4536,  27614.1531, -36826.6256,  13132.8830,  34677.3615,
           7246.9022,  29088.8202]], dtype=torch.float64)
	q_value: tensor([[-28.9561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09254106656488104, distance: 1.0901096482194228 entropy 12.43307194507223
epoch: 82, step: 47
	action: tensor([[-13697.2126, -49167.2120,  28532.1591, -68942.2648, -34413.4110,
          16955.9010, -52659.0655]], dtype=torch.float64)
	q_value: tensor([[-27.7146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8962003698436625, distance: 1.575791693132725 entropy 12.40253678584254
epoch: 82, step: 48
	action: tensor([[-72783.4043, -62878.0251, -32310.4167,    676.8106, -14171.5536,
         113181.2529, -57344.9580]], dtype=torch.float64)
	q_value: tensor([[-21.2911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7029911485514082, distance: 1.4933544093614992 entropy 12.08061709492979
epoch: 82, step: 49
	action: tensor([[ -43315.7857, -159393.2247,  -32121.2615,  -55376.9325,    1344.3122,
           98698.2749,  -34890.9248]], dtype=torch.float64)
	q_value: tensor([[-26.1948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6652049884372564, distance: 1.4766941176323864 entropy 12.592295628811511
epoch: 82, step: 50
	action: tensor([[-46899.6645, -14552.1748,   8770.0738,  13701.3497,  41711.2812,
         -45556.8685,  43962.6468]], dtype=torch.float64)
	q_value: tensor([[-22.9824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0745204520381515, distance: 1.186216706627349 entropy 12.207170108389208
epoch: 82, step: 51
	action: tensor([[-47062.8922,  52951.9932, -80606.1919, 103823.4098, -21959.0916,
          15068.7602,  66206.1462]], dtype=torch.float64)
	q_value: tensor([[-25.6370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5219724441044596, distance: 0.7911943174436263 entropy 12.504067274483292
epoch: 82, step: 52
	action: tensor([[  20523.8094, -160824.2939,   23851.2348,   15564.7431,   -6018.5262,
            5502.5038,   32347.3758]], dtype=torch.float64)
	q_value: tensor([[-25.4229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39832793739251326, distance: 0.8876394925474543 entropy 12.182420004227211
epoch: 82, step: 53
	action: tensor([[ 10327.2503, -91629.8720,   1755.0667,  -3312.0607, -94091.1924,
          57471.9876,  91480.7562]], dtype=torch.float64)
	q_value: tensor([[-20.3989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24184067638539197, distance: 1.2752332674238471 entropy 12.162764373182005
epoch: 82, step: 54
	action: tensor([[-101967.6101,  -15716.7310,  -12658.8887,   74630.0194,  -43621.6126,
           43529.0104,  119969.1826]], dtype=torch.float64)
	q_value: tensor([[-22.6263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019223656364376285, distance: 1.133291637962351 entropy 12.244034498504002
epoch: 82, step: 55
	action: tensor([[ 64496.6061, -29595.3635,   8151.2540, 199696.5811, -66132.2342,
         -21094.0378, -36724.1634]], dtype=torch.float64)
	q_value: tensor([[-24.0965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03195745346798162, distance: 1.1624856199802847 entropy 12.446301834274395
epoch: 82, step: 56
	action: tensor([[-62610.2538, -58710.5026, -75019.1989,  30457.3835, -66812.5167,
         -68148.5762, -41566.8619]], dtype=torch.float64)
	q_value: tensor([[-26.9568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21441198729138344, distance: 1.01427125433746 entropy 12.436661386223488
epoch: 82, step: 57
	action: tensor([[  26211.2554,  -16640.6681,  -10857.5709, -122442.4658,  -95855.1419,
           -2399.4064, -108452.0513]], dtype=torch.float64)
	q_value: tensor([[-26.9862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16143577790341412, distance: 1.0479121064771686 entropy 12.543637659301842
epoch: 82, step: 58
	action: tensor([[-59942.1439,  21130.4721,   6386.1284, -29499.0011,  -4358.2899,
           6841.7793,  -4002.0359]], dtype=torch.float64)
	q_value: tensor([[-21.6326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13531441685407297, distance: 1.0641082304651703 entropy 12.144049500379712
epoch: 82, step: 59
	action: tensor([[ -81056.5961,  -39687.1208, -125352.5798,  -65652.9467,   59650.4616,
          -60933.6757,   37746.1526]], dtype=torch.float64)
	q_value: tensor([[-27.3810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7816716220833908, distance: 1.5274623865795354 entropy 12.265438982754153
epoch: 82, step: 60
	action: tensor([[ 19048.7873, -60279.9670,  54432.1084,  81320.0884, -80243.9190,
          21089.3969,  76540.0834]], dtype=torch.float64)
	q_value: tensor([[-28.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22948173560935547, distance: 1.0044958853673438 entropy 12.632322093652903
epoch: 82, step: 61
	action: tensor([[-36644.6692, -85309.6793, -60486.4058,  35502.8244, -10308.0549,
          33345.7069,  53173.4547]], dtype=torch.float64)
	q_value: tensor([[-20.9338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14245438114239284, distance: 1.2231398816759655 entropy 12.179877819528075
epoch: 82, step: 62
	action: tensor([[ 21571.1723,   1288.7986,   8305.9663,  27887.4424, -37946.3101,
         -31166.4601,  -6419.3125]], dtype=torch.float64)
	q_value: tensor([[-24.3073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5135774939424321, distance: 0.7981114175064691 entropy 12.466882236551852
epoch: 82, step: 63
	action: tensor([[ -80017.0863,   33185.0560,  -79164.0850,  112854.9226, -105599.7481,
           42411.1258,   12055.0436]], dtype=torch.float64)
	q_value: tensor([[-27.1515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15139067838687736, distance: 1.054169853252805 entropy 12.423227695363948
epoch: 82, step: 64
	action: tensor([[-80244.3761, -47937.0371, -97116.8594,  -7972.1908, -52526.1708,
          29331.6009,  92081.6181]], dtype=torch.float64)
	q_value: tensor([[-30.2753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7604333824538465, distance: 1.5183311110400428 entropy 12.506889876661393
epoch: 82, step: 65
	action: tensor([[  11373.4994, -105125.6374,   -3276.9034,   72528.5430,  -20620.8672,
           -2384.0890,   37487.3802]], dtype=torch.float64)
	q_value: tensor([[-23.5232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27091026793603956, distance: 1.2900725590107724 entropy 12.37068072035474
epoch: 82, step: 66
	action: tensor([[ -39358.8646,  -57240.2616, -108016.4644,   34481.3661,  -36517.8285,
           71716.8794,   39427.7473]], dtype=torch.float64)
	q_value: tensor([[-25.2309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0901912765665065, distance: 1.0915201102143564 entropy 12.348640007308317
epoch: 82, step: 67
	action: tensor([[-32973.0695, -39748.6870, -44424.5238,  41346.6431, -88429.5123,
         -45831.1461, -76848.0854]], dtype=torch.float64)
	q_value: tensor([[-23.7150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5418792405583766, distance: 1.4209600692998394 entropy 12.443230576369617
epoch: 82, step: 68
	action: tensor([[ 32223.8084, -56953.4486, -16295.7944,  63035.2412, -66454.3475,
          59362.1746, -45592.6844]], dtype=torch.float64)
	q_value: tensor([[-24.6627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24908113307773316, distance: 1.2789454392196788 entropy 12.481871811200508
epoch: 82, step: 69
	action: tensor([[-96608.3241, -60074.1751, -12306.7256,  17101.2769,  55692.5503,
          25592.7708,  29074.8716]], dtype=torch.float64)
	q_value: tensor([[-27.9015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05319932373256053, distance: 1.174389011668191 entropy 12.537535680601858
epoch: 82, step: 70
	action: tensor([[-65600.6872, -62754.3948,  76492.8146, -42871.8709,  23379.4988,
         -74265.2632,  42486.2854]], dtype=torch.float64)
	q_value: tensor([[-24.2820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5240970246013339, distance: 1.4127424691131838 entropy 12.524519612437297
epoch: 82, step: 71
	action: tensor([[-31855.7795,  77575.6358,  56891.5506,  80841.1864,  10696.3131,
         100200.0361, -14326.3168]], dtype=torch.float64)
	q_value: tensor([[-22.0603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1089588944372688, distance: 1.080203457346399 entropy 12.21226830569616
epoch: 82, step: 72
	action: tensor([[  3319.2699,  17914.6445,  -2667.6600,  21555.7525, -10537.3231,
         120642.4016, -79960.9857]], dtype=torch.float64)
	q_value: tensor([[-25.3993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43418740816736767, distance: 0.8607816443854867 entropy 12.270333072417557
epoch: 82, step: 73
	action: tensor([[ 54604.9580, -15521.7247, -28325.3017,   9503.1664, -39257.4364,
          85022.4283, -61657.8491]], dtype=torch.float64)
	q_value: tensor([[-23.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3346223294463482, distance: 0.9334494505454685 entropy 12.235597794587685
epoch: 82, step: 74
	action: tensor([[-78348.9560, -50533.0364,  13151.9901, -88199.0127,  40951.6231,
         -16925.7556, -23714.7112]], dtype=torch.float64)
	q_value: tensor([[-23.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.888711164611802, distance: 1.5726767525032337 entropy 12.330677703672285
epoch: 82, step: 75
	action: tensor([[-37574.0470, -53989.0124, -78536.8248, -42972.4265,  21133.4696,
          52793.7600, -40126.3021]], dtype=torch.float64)
	q_value: tensor([[-21.9272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16742049751061638, distance: 1.0441660070645131 entropy 12.295890803650318
epoch: 82, step: 76
	action: tensor([[-42517.8649,  51713.6167,  67425.3064,  62142.2684, -65098.4589,
           3393.7142, -50439.1866]], dtype=torch.float64)
	q_value: tensor([[-28.6976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1697188558794045, distance: 1.042723786718861 entropy 12.569212464754717
epoch: 82, step: 77
	action: tensor([[-49742.5602,  -8186.1229,  -3357.0032, -11025.1280, -39486.5610,
          52838.2216,   1239.1623]], dtype=torch.float64)
	q_value: tensor([[-24.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10198804659345473, distance: 1.0844205827183624 entropy 12.230403635570312
epoch: 82, step: 78
	action: tensor([[ 46820.9531, -14376.1117,  28818.1258, -55653.5268,  85751.8536,
         -20395.0268, -20338.7111]], dtype=torch.float64)
	q_value: tensor([[-25.5521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6525677970010646, distance: 0.6745151806062538 entropy 12.42209439333936
epoch: 82, step: 79
	action: tensor([[-107177.0716,  -24707.3352,   39280.3894,   62443.7892,   18533.6518,
          -78235.3995,   16580.9484]], dtype=torch.float64)
	q_value: tensor([[-21.9844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8508341075610479, distance: 1.5568273061760982 entropy 12.240696631074949
epoch: 82, step: 80
	action: tensor([[-77130.3629, -54364.4419, -25920.7973, -18669.1054,  69677.9447,
         -31390.2919,  31462.7098]], dtype=torch.float64)
	q_value: tensor([[-24.2207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8181521725390686, distance: 1.5430208976032387 entropy 12.38215762291902
epoch: 82, step: 81
	action: tensor([[ -88503.0725, -105374.6582,    8295.3814,   22435.8207,   26116.3691,
          -60654.1633,   85912.8551]], dtype=torch.float64)
	q_value: tensor([[-28.3631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023724919253232613, distance: 1.1306880320937016 entropy 12.615080180641302
epoch: 82, step: 82
	action: tensor([[ -91836.9240, -128027.8506,   -8481.6442,   45359.4182,   58102.4658,
           10878.4477,   -5069.3023]], dtype=torch.float64)
	q_value: tensor([[-25.2968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.136429778771183, distance: 1.2199105746432317 entropy 12.569349920245134
epoch: 82, step: 83
	action: tensor([[-180916.1820,    -618.6175,   33245.0550,   28604.4456,   61056.9908,
           64920.5852,  -21639.7504]], dtype=torch.float64)
	q_value: tensor([[-22.6554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6850894316001401, distance: 1.4854846576313627 entropy 12.357862025444765
epoch: 82, step: 84
	action: tensor([[  15793.2137,  -50140.5401,   71315.2072,    3076.8978,  108974.1049,
          -39764.2262, -133484.6848]], dtype=torch.float64)
	q_value: tensor([[-25.1378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08995110649453508, distance: 1.091664169706439 entropy 12.505324163116057
epoch: 82, step: 85
	action: tensor([[ -18139.3213, -111270.0344,   49829.6201,  106440.3945,   83639.4228,
           51922.1470,   88880.8876]], dtype=torch.float64)
	q_value: tensor([[-23.1075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006965275923600411, distance: 1.1403519532428008 entropy 12.251120545381808
epoch: 82, step: 86
	action: tensor([[ -4689.6830,  28501.6652, -34829.9929,  94560.5566,  18887.0841,
          47086.6746,   -143.1693]], dtype=torch.float64)
	q_value: tensor([[-24.8595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23343823707854394, distance: 1.0019135945746034 entropy 12.505905386367667
epoch: 82, step: 87
	action: tensor([[ 57763.7192,  62222.1490, -83905.6561, -29526.8206,  97739.8488,
         -10307.0289,  16325.8399]], dtype=torch.float64)
	q_value: tensor([[-27.1130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14103699546344206, distance: 1.060581196052211 entropy 12.39550428562617
epoch: 82, step: 88
	action: tensor([[-33429.1444, -10805.8709, -12729.0052, -44198.6318,  19860.8952,
         -28578.9040,  -6293.3541]], dtype=torch.float64)
	q_value: tensor([[-19.6584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.098624202948011
epoch: 82, step: 89
	action: tensor([[-12210.2697,  34846.2791,  67849.6142,  25414.9573,  42793.4039,
          -3368.3907, -10219.5483]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.9070930096095
epoch: 82, step: 90
	action: tensor([[-35893.6304,  10984.6038,  -8659.5970, -21442.1991,  10690.8731,
          -8881.2398, -21235.3577]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.9070930096095
epoch: 82, step: 91
	action: tensor([[ 26405.0502, -37230.5229,  36269.7598, -18776.8116, -29807.9907,
          18990.7669,  -7330.6697]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13498050018829488, distance: 1.0643136746069304 entropy 11.9070930096095
epoch: 82, step: 92
	action: tensor([[ 55259.4658, -28362.9420,  10189.2885, -47005.1135,  -7256.0434,
         -83204.1032,   9428.0023]], dtype=torch.float64)
	q_value: tensor([[-25.7841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013409216490215092, distance: 1.136645980026106 entropy 12.442463180038777
epoch: 82, step: 93
	action: tensor([[  22138.7277, -135395.1782,  -29594.5263,    4616.3158,   67638.2481,
           77217.8978, -134871.9162]], dtype=torch.float64)
	q_value: tensor([[-26.7401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39924778925690396, distance: 0.8869607098666064 entropy 12.427572945721499
epoch: 82, step: 94
	action: tensor([[-110172.9631,  -29516.8117,   61255.1423,   51945.6791, -129657.5234,
            7186.0757,   27021.4202]], dtype=torch.float64)
	q_value: tensor([[-20.2250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14640677520635004, distance: 1.05726090403344 entropy 12.24350318752302
epoch: 82, step: 95
	action: tensor([[-17491.2472, -21531.5597,  27858.2904,  13185.5782,  96715.6279,
         197911.7058, -26956.9693]], dtype=torch.float64)
	q_value: tensor([[-24.4511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1385307346149378, distance: 1.221037698741112 entropy 12.458626149640889
epoch: 82, step: 96
	action: tensor([[ 68751.9656, -81398.4170, -81212.5072,  19881.6537,  78188.6997,
          25972.9578,  22087.1238]], dtype=torch.float64)
	q_value: tensor([[-24.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5142506748804473, distance: 0.7975589560109173 entropy 12.498142843117353
epoch: 82, step: 97
	action: tensor([[ -15757.0794,  -26338.5599,   44123.3824,  -31813.3732,   21615.5996,
           19104.4305, -133218.7471]], dtype=torch.float64)
	q_value: tensor([[-22.3172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8343763147649259, distance: 1.549890122027979 entropy 12.459235363253724
epoch: 82, step: 98
	action: tensor([[   585.3431,  11516.1699, -33173.3306,  21462.1098, -57195.1060,
          -9570.6279, -29425.7478]], dtype=torch.float64)
	q_value: tensor([[-24.9880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.400501952684921
epoch: 82, step: 99
	action: tensor([[-59117.2986, -54521.2708,  39244.3725, -57956.2272,   1772.9514,
          -6519.3960,  27288.3541]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1943859997647579, distance: 1.027117658349044 entropy 11.9070930096095
epoch: 82, step: 100
	action: tensor([[  10090.7970, -104181.0179,   79315.4393,  -10206.4717,   -6476.9514,
           74221.6132,    -943.4995]], dtype=torch.float64)
	q_value: tensor([[-24.2135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5451024820515227, distance: 0.7718154659139345 entropy 12.321823784800825
epoch: 82, step: 101
	action: tensor([[-62586.7309, 102572.5794,  36753.2301, -32520.0020,  63898.8591,
         -52786.5957, -34198.1025]], dtype=torch.float64)
	q_value: tensor([[-25.4366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.36147774212744
epoch: 82, step: 102
	action: tensor([[-25723.0668,  -7617.0875, -53487.7400,   3883.4637, -17238.6559,
          -5838.2725,  63542.4744]], dtype=torch.float64)
	q_value: tensor([[-25.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12268208444530426, distance: 1.212509330933617 entropy 11.9070930096095
epoch: 82, step: 103
	action: tensor([[-97770.5016,  -9531.1313,  -4696.2650,   3237.7721,  13621.2534,
         -46037.9467, -80175.7163]], dtype=torch.float64)
	q_value: tensor([[-22.3924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8504639591840351, distance: 1.5566716234186275 entropy 12.321695481674766
epoch: 82, step: 104
	action: tensor([[ -54834.0565,  -34866.7193, -119090.1077,  114152.3444,    7971.8281,
          164046.8287,  -20514.4748]], dtype=torch.float64)
	q_value: tensor([[-23.8853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5019212069970516, distance: 1.402427011907268 entropy 12.351676466878084
epoch: 82, step: 105
	action: tensor([[-25835.8606, -66023.5725, -34819.8833, -38602.3208,  76794.0058,
          61408.2057, -20783.8862]], dtype=torch.float64)
	q_value: tensor([[-23.1055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5545897740578571, distance: 1.426804914595086 entropy 12.3365232251164
epoch: 82, step: 106
	action: tensor([[-51302.1790,  44374.3749,  41110.9525,  89457.1896,  95623.7040,
          10213.0166, -80308.2300]], dtype=torch.float64)
	q_value: tensor([[-21.3419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14993792771517656, distance: 1.0550717942613819 entropy 12.14781477449523
epoch: 82, step: 107
	action: tensor([[  -8059.3815,  -21455.9833, -126614.4375,   -5904.6004,  -52627.8093,
          131551.4689,   -2029.0626]], dtype=torch.float64)
	q_value: tensor([[-24.9175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0080326348744104, distance: 1.6215938053787577 entropy 12.288361891367064
epoch: 82, step: 108
	action: tensor([[-24393.6431,   5990.5915, -21285.9841, -66989.7693,  56460.7073,
           1655.2199, -26123.1739]], dtype=torch.float64)
	q_value: tensor([[-21.3545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07658480928360878, distance: 1.1873556333163056 entropy 12.121279989916388
epoch: 82, step: 109
	action: tensor([[-19525.3974, -11849.2688,  -8346.5734,  21113.0508,  23250.8016,
          12535.6704, -67792.1297]], dtype=torch.float64)
	q_value: tensor([[-29.8057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9000331621976647, distance: 1.5773834641093185 entropy 12.167301178970549
epoch: 82, step: 110
	action: tensor([[ -4846.9719, -39913.8793, 173497.0504,  66899.5435,  21161.8443,
          36989.4858,   3599.6728]], dtype=torch.float64)
	q_value: tensor([[-23.9989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17434974063123498, distance: 1.0398118299591363 entropy 12.516537760970674
epoch: 82, step: 111
	action: tensor([[-135944.7936,  -16487.4715,  -35021.9100,   86524.5894,   29975.7424,
           72846.1126,  -74120.8823]], dtype=torch.float64)
	q_value: tensor([[-23.5847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3969751421428571, distance: 1.3525428485109496 entropy 12.364602972452726
epoch: 82, step: 112
	action: tensor([[-103523.4672,   36566.0373,    5805.5328,  102040.6021,   76436.7677,
           -3814.9958,   34248.8912]], dtype=torch.float64)
	q_value: tensor([[-25.2325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.040638507272270696, distance: 1.1208508733747258 entropy 12.5304827893213
epoch: 82, step: 113
	action: tensor([[ -7848.0407,  55986.4979, -10854.7358, -71251.4613,  31287.8440,
          13699.9204, -60861.9670]], dtype=torch.float64)
	q_value: tensor([[-27.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.753883576962475, distance: 0.5677105834527494 entropy 12.33209531947688
epoch: 82, step: 114
	action: tensor([[-80216.4128, -46057.4074,   9233.9726, -35722.8547, -62698.4730,
         -18026.4300, -82221.4578]], dtype=torch.float64)
	q_value: tensor([[-24.4480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12737022118210173, distance: 1.0689852380752989 entropy 12.29492068468045
epoch: 82, step: 115
	action: tensor([[-119195.3651,  -94253.7277,  -34299.8834,    6120.2462,  -49988.7519,
          -13270.3960,   28376.6361]], dtype=torch.float64)
	q_value: tensor([[-26.2425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7802432904699734, distance: 1.5268499953216303 entropy 12.430198952082835
epoch: 82, step: 116
	action: tensor([[116947.5431, -28860.0636,   6682.7251,  44767.5749,  19002.8930,
         -22310.7534, -36791.6241]], dtype=torch.float64)
	q_value: tensor([[-24.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1605756951742796, distance: 1.0484493701084545 entropy 12.358132384087536
epoch: 82, step: 117
	action: tensor([[ 66721.5073, -17483.2536,  21229.3413,  40145.5972, -15511.9025,
         -54509.1134, -26832.6141]], dtype=torch.float64)
	q_value: tensor([[-24.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025132320893089033, distance: 1.1298727362036214 entropy 12.494474614618353
epoch: 82, step: 118
	action: tensor([[-41361.0364, -37235.9610, -63576.6358,  27789.6831,  -2834.4043,
         -37825.6585,   -788.4808]], dtype=torch.float64)
	q_value: tensor([[-25.7529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44489548728827777, distance: 1.3755453420997539 entropy 12.453000533602596
epoch: 82, step: 119
	action: tensor([[  -9407.8960,   89565.2937, -123133.0787,   15666.5144,  176679.6809,
           25663.0726,  -78411.1017]], dtype=torch.float64)
	q_value: tensor([[-28.5297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09640864747214317, distance: 1.087784149546308 entropy 12.564366660679488
epoch: 82, step: 120
	action: tensor([[  20107.3967,  -20284.5366, -168682.6385,  -45635.2221,   -5653.8315,
          -16597.2784,  -50139.1275]], dtype=torch.float64)
	q_value: tensor([[-29.2205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43328613622472545, distance: 0.8614669328213388 entropy 12.561940650324809
epoch: 82, step: 121
	action: tensor([[-72096.1095, -25239.0448,  -6815.9478,   6727.5433, -66836.3071,
           -236.5364, -86331.3055]], dtype=torch.float64)
	q_value: tensor([[-22.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006473309469986477, distance: 1.1480421265332965 entropy 12.230058419801981
epoch: 82, step: 122
	action: tensor([[ 70598.7749, -27747.1743, -31685.1275,  10541.8083,  22016.9562,
         -52438.6671, -37943.6380]], dtype=torch.float64)
	q_value: tensor([[-24.2309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41789808155029184, distance: 0.8730843600365449 entropy 12.360617746203596
epoch: 82, step: 123
	action: tensor([[ 50414.9765,  13041.6413, -80209.2696,  -3734.9497,   1091.8971,
          47600.1235,  20028.3125]], dtype=torch.float64)
	q_value: tensor([[-23.8611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36970766973624647, distance: 0.9085058005192571 entropy 12.137303012581748
epoch: 82, step: 124
	action: tensor([[ 36598.4357,  -8443.6653, -42822.3220,  50091.3289,  41531.0053,
         -17507.7110,  26024.4369]], dtype=torch.float64)
	q_value: tensor([[-23.2490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027721178295924798, distance: 1.128371494546809 entropy 12.061736329800409
epoch: 82, step: 125
	action: tensor([[-111797.1027,  -63601.8334,  -18940.8211,  140795.5904,  -36621.3531,
           15500.4364,   54057.5811]], dtype=torch.float64)
	q_value: tensor([[-26.6384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15008881936582408, distance: 1.2272198859549048 entropy 12.511138347018889
epoch: 82, step: 126
	action: tensor([[   1393.9595, -108882.4795, -115820.6838,   63798.4815,    3850.2359,
           39777.5318,   41511.8898]], dtype=torch.float64)
	q_value: tensor([[-27.1349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07489931376002845, distance: 1.1864258103702925 entropy 12.643428175452831
epoch: 82, step: 127
	action: tensor([[  9083.3884, -10897.3783, -40896.6825,  12449.0118,  36380.8657,
          93183.2748, -99896.8317]], dtype=torch.float64)
	q_value: tensor([[-24.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14255522960306022, distance: 1.0596434823669507 entropy 12.463709229096668
LOSS epoch 82 actor 250.82654697777033 critic 254.39281015204404
epoch: 83, step: 0
	action: tensor([[-127599.3215,  -46034.7718,   53082.6960,  -90973.0039,  -23639.1577,
           67559.7953,  -38020.6455]], dtype=torch.float64)
	q_value: tensor([[-23.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1228131068971432, distance: 1.2125800817333228 entropy 12.298126807267385
epoch: 83, step: 1
	action: tensor([[-61065.3030, -51129.9852, -17877.8978,  45482.3934,  31164.0764,
         -69903.2094, -26165.1514]], dtype=torch.float64)
	q_value: tensor([[-21.1975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6453508021539904, distance: 1.4678644311192823 entropy 12.247299160993181
epoch: 83, step: 2
	action: tensor([[ -72060.1164, -113377.9948,  -32909.0865,   74394.9096,   56776.5819,
           22845.9892,   41762.0673]], dtype=torch.float64)
	q_value: tensor([[-24.9276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08330695490544793, distance: 1.095641973397008 entropy 12.557371427132503
epoch: 83, step: 3
	action: tensor([[ -51452.9602, -133273.7207,  -19738.8386,  -25294.2260,  -78515.2525,
         -143012.4287,  -69365.6008]], dtype=torch.float64)
	q_value: tensor([[-22.2354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6240727869450269, distance: 1.4583421825535985 entropy 12.463814351132289
epoch: 83, step: 4
	action: tensor([[-20553.8531,  23015.9545,  37569.6025, -13283.6027, -18460.3069,
         -22798.6121, -34219.5336]], dtype=torch.float64)
	q_value: tensor([[-21.1658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09496816662983643, distance: 1.0886508622647622 entropy 12.323444219721107
epoch: 83, step: 5
	action: tensor([[  65687.8037, -113595.1698,   27468.2507,  -28338.7547,   27324.4226,
          -29593.1299,   -1166.6061]], dtype=torch.float64)
	q_value: tensor([[-26.4549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5413919618119434, distance: 0.7749568554977166 entropy 12.304860848157157
epoch: 83, step: 6
	action: tensor([[-64425.2873, -56886.9718,  16850.0396,  77144.2162, -41178.1583,
         -42878.4529, -22555.9497]], dtype=torch.float64)
	q_value: tensor([[-22.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46212467043131666, distance: 1.383722158912139 entropy 12.526983554428904
epoch: 83, step: 7
	action: tensor([[  -9362.0803, -130396.0689,  -26265.6784,  -93909.7102,  -32552.8077,
          114379.4725,   -5473.5176]], dtype=torch.float64)
	q_value: tensor([[-20.7713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43207748049382255, distance: 1.3694303573273283 entropy 12.315706586417491
epoch: 83, step: 8
	action: tensor([[   8091.5182,  -76043.1794,   65261.7494,   28933.1765, -122666.8570,
          -13158.8899,   44242.1408]], dtype=torch.float64)
	q_value: tensor([[-24.4071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03104758381631556, distance: 1.161973029257984 entropy 12.36341836517442
epoch: 83, step: 9
	action: tensor([[ -3088.7061,  23111.3764,  20832.6374,  47138.4274,  -3424.5945,
         -82467.3774, 123732.2827]], dtype=torch.float64)
	q_value: tensor([[-20.2293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09739036347101226, distance: 1.198773860141682 entropy 12.231881491047004
epoch: 83, step: 10
	action: tensor([[-81077.1611, -76618.0714,  74182.9687, -66966.7800, -12780.6693,
         -25538.8573, -36370.1442]], dtype=torch.float64)
	q_value: tensor([[-28.8240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4701810974488132, distance: 1.3875291333008286 entropy 12.517781837887355
epoch: 83, step: 11
	action: tensor([[  39598.7635,   10804.4928, -146480.5543,   -3201.4334,   12791.7960,
           20396.0649,  -24285.6190]], dtype=torch.float64)
	q_value: tensor([[-26.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.57432460005477
epoch: 83, step: 12
	action: tensor([[-55119.9700,  41883.1188,   -901.8478, -22889.9728,   2874.7100,
          -8284.5313,  15530.2350]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3717276520478693, distance: 0.907048827088006 entropy 11.938663150365915
epoch: 83, step: 13
	action: tensor([[-17355.2350, -63649.5378, -73520.4900,   3665.4523, -52975.7468,
          26266.0121, -91842.3186]], dtype=torch.float64)
	q_value: tensor([[-26.8057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49244598268915074, distance: 1.3979962418597967 entropy 12.478156377456997
epoch: 83, step: 14
	action: tensor([[-35332.9376, -80187.6346, -27283.4667, -22790.0956,   6924.9690,
          79102.4379, -19567.2311]], dtype=torch.float64)
	q_value: tensor([[-22.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5366925080081666, distance: 1.418568069911644 entropy 12.422123092682215
epoch: 83, step: 15
	action: tensor([[  14258.4118,   34942.9006, -119795.9742,  -82114.8217,   18568.6599,
            1405.1272,   41074.7274]], dtype=torch.float64)
	q_value: tensor([[-24.3531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.380637727390933
epoch: 83, step: 16
	action: tensor([[-18815.1510,   6866.1741,  13801.3890,  65540.9465, -41761.6480,
           4388.5049, -27280.4751]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08184017122039677, distance: 1.0965181813233886 entropy 11.938663150365915
epoch: 83, step: 17
	action: tensor([[-102783.3987,  -80415.5017,   47458.6358,   33230.4995,  -31408.3859,
            5393.8818, -106502.2686]], dtype=torch.float64)
	q_value: tensor([[-27.7233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026155547584820305, distance: 1.159213130958236 entropy 12.49286298045829
epoch: 83, step: 18
	action: tensor([[  39178.1186, -142663.8592,  -16939.5509,  -40856.6126,  -66265.7468,
            2428.4093,   20674.0208]], dtype=torch.float64)
	q_value: tensor([[-22.9518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16345772671875114, distance: 1.234332026021761 entropy 12.55185330390995
epoch: 83, step: 19
	action: tensor([[-39354.2610, -40213.1276, -84334.8974,  46743.1904, -67091.9599,
          55231.5972,   3863.9317]], dtype=torch.float64)
	q_value: tensor([[-21.3066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05961560858421966, distance: 1.1097097436084964 entropy 12.37594871053457
epoch: 83, step: 20
	action: tensor([[-76885.1807,  52458.3794, -21792.2717,   6821.4366,  13404.0189,
          39648.5614,  63832.7089]], dtype=torch.float64)
	q_value: tensor([[-21.8302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15455668353152496, distance: 1.0522015588797222 entropy 12.34963315914091
epoch: 83, step: 21
	action: tensor([[ 46333.0572,  41876.4399,  -8317.9681, -54205.9399,  43204.8124,
          53348.6139,  -3613.8528]], dtype=torch.float64)
	q_value: tensor([[-27.3646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5660588359490515, distance: 0.7538277454616213 entropy 12.49370252167311
epoch: 83, step: 22
	action: tensor([[-23766.6302, -55770.8402,  67822.3811,  68823.3130,  37606.4389,
         -21587.0039,  10643.2375]], dtype=torch.float64)
	q_value: tensor([[-22.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.948777554339884
epoch: 83, step: 23
	action: tensor([[ 45805.6507, -79481.3352,  36794.9616,  26841.9744, -38859.7657,
          67867.1480,  80124.1360]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5392310235932909, distance: 0.7767804888443897 entropy 11.938663150365915
epoch: 83, step: 24
	action: tensor([[-31460.4655, -67761.0624,   8319.6812,  90283.2239, -67394.8043,
         -46170.6727, -12143.7732]], dtype=torch.float64)
	q_value: tensor([[-19.8469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.25236877124452
epoch: 83, step: 25
	action: tensor([[-26532.3376,   9230.3394,  19405.4955,  89134.6063,  48108.2155,
          -2664.0744, -12876.3827]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.938663150365915
epoch: 83, step: 26
	action: tensor([[  1651.8059, -22245.5706, -44897.1719, -16305.9847,   3437.2485,
          77220.1763, -11349.0936]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23727631349970868, distance: 0.9994022206711516 entropy 11.938663150365915
epoch: 83, step: 27
	action: tensor([[  47558.1159,  -41220.6492,  -33623.6466,  -23500.6076,  -26437.2229,
            9071.5712, -110554.5345]], dtype=torch.float64)
	q_value: tensor([[-24.6022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5352237465580963, distance: 0.7801509805073341 entropy 12.3713337113111
epoch: 83, step: 28
	action: tensor([[ 22942.1514, -75150.7021, -42607.1312, 103356.7650,  -9551.6834,
         -37302.2013, -60913.1709]], dtype=torch.float64)
	q_value: tensor([[-27.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12420834304004358, distance: 1.0709201622030606 entropy 12.49562843192958
epoch: 83, step: 29
	action: tensor([[ -35165.6026, -102306.0229,  -33992.6123,   -7532.6554,   74753.0513,
           -6628.3013,  -43349.6102]], dtype=torch.float64)
	q_value: tensor([[-22.0257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32714069815094926, distance: 1.3183028076166168 entropy 12.31232181131239
epoch: 83, step: 30
	action: tensor([[ -7396.0637,  -3837.7463, 132640.7630,  -4530.1374,  44156.8316,
          25983.8085,  48811.5512]], dtype=torch.float64)
	q_value: tensor([[-23.9653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020798896210647122, distance: 1.1323811737443477 entropy 12.434468101265967
epoch: 83, step: 31
	action: tensor([[-120609.1376, -138487.8163,   28191.5086,   44125.6367,    5905.7352,
          -98507.4725,  -48862.7916]], dtype=torch.float64)
	q_value: tensor([[-26.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9895882040788531, distance: 1.614129192184191 entropy 12.520961281957897
epoch: 83, step: 32
	action: tensor([[-44888.1742,  14675.6031, -19162.3223,  45207.8936, -27355.3006,
         -48059.6314,  -6679.3472]], dtype=torch.float64)
	q_value: tensor([[-21.0329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5489552434493612, distance: 0.7685400645294488 entropy 12.31499479610964
epoch: 83, step: 33
	action: tensor([[-26928.7825, -72196.7259, -71886.0675, 132685.8994, -72397.7932,
          68969.5699, -87148.0712]], dtype=torch.float64)
	q_value: tensor([[-25.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8175095601910216, distance: 1.5427481888806653 entropy 12.305485239976536
epoch: 83, step: 34
	action: tensor([[  9486.8059, -58969.5414,  34201.6782,  -2594.2192, -16773.1579,
         -37753.2246,  83961.1835]], dtype=torch.float64)
	q_value: tensor([[-24.2582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016863408910542166, distance: 1.1539526881559927 entropy 12.527376629448039
epoch: 83, step: 35
	action: tensor([[  7268.7198,   6739.6687,  -9647.3382,   8198.5157, -11839.4004,
          33252.4020,  -6941.9755]], dtype=torch.float64)
	q_value: tensor([[-22.0664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39646231721103575, distance: 0.8890145908287675 entropy 12.267133984515509
epoch: 83, step: 36
	action: tensor([[ -64256.7744, -106616.7442,  -28772.9648,    4298.4520,  -31803.8868,
           -9488.4020,  165360.7314]], dtype=torch.float64)
	q_value: tensor([[-25.1587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15250393867995415, distance: 1.0534781624168619 entropy 12.468778664116352
epoch: 83, step: 37
	action: tensor([[-17050.6064,  16723.7177,  40265.5094, -47735.1241,  91459.8414,
          -3927.7929,  71502.9063]], dtype=torch.float64)
	q_value: tensor([[-21.0490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6451801327614504, distance: 0.6816487737883238 entropy 12.394437183015851
epoch: 83, step: 38
	action: tensor([[-67065.3156,   5088.1292,  40546.6079,  72582.5189,  55702.7690,
          89893.2038,  -9563.7439]], dtype=torch.float64)
	q_value: tensor([[-25.5069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.299072696791525
epoch: 83, step: 39
	action: tensor([[ -58276.2342,  -55554.1737, -120114.3527,   80420.0802,   71670.3918,
           50631.8062,  -24114.6539]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9274049948081795, distance: 1.5887047108644696 entropy 11.938663150365915
epoch: 83, step: 40
	action: tensor([[ -9380.8098, -77983.2269, -72676.1434,  14242.5227,   4376.8031,
          16449.3943,  -7833.8819]], dtype=torch.float64)
	q_value: tensor([[-21.0624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4122097180444986, distance: 1.3598978624707498 entropy 12.32018172946397
epoch: 83, step: 41
	action: tensor([[-116548.6185,  -90230.5843,    7378.0577,  101597.2823,  -32872.6038,
            3945.7585,  139014.2027]], dtype=torch.float64)
	q_value: tensor([[-23.9835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1309646991409943, distance: 1.0667813200881497 entropy 12.524599664313726
epoch: 83, step: 42
	action: tensor([[ -7173.0961, -33598.2774, -90339.7721,   9355.1902,  67349.1102,
         -57555.9152,  11428.4428]], dtype=torch.float64)
	q_value: tensor([[-21.6301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7285009965624578, distance: 1.5044976385296875 entropy 12.36293515748129
epoch: 83, step: 43
	action: tensor([[-18921.2599, -74408.1723,  16057.9507, 100278.3956, -22514.2419,
         -19022.2150, -14004.1644]], dtype=torch.float64)
	q_value: tensor([[-22.7826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5106194649078857, distance: 1.4064821716728015 entropy 12.311621632038156
epoch: 83, step: 44
	action: tensor([[ -74758.6645, -142420.8275,   29558.0181,   19636.2050,   71580.9161,
          -18778.3401,   48906.0166]], dtype=torch.float64)
	q_value: tensor([[-24.0303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10390902285808556, distance: 1.0832600962556953 entropy 12.511849951896023
epoch: 83, step: 45
	action: tensor([[  -8300.8081, -112610.3500,   40529.9621, -113870.9226,  -39946.0595,
          -27067.0403,   80185.2621]], dtype=torch.float64)
	q_value: tensor([[-26.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5285452293505775, distance: 1.4148025706424106 entropy 12.688384567610697
epoch: 83, step: 46
	action: tensor([[-118737.6957,  137315.0689,   53004.1712,  -33572.2417,     732.4871,
          -42712.0142,   57195.3031]], dtype=torch.float64)
	q_value: tensor([[-25.3879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5761944841951587, distance: 0.7449720783413705 entropy 12.547055575531
epoch: 83, step: 47
	action: tensor([[  12892.1708, -103332.7029,    2419.7152,   69568.2545,  -53304.5652,
           64277.9014,  -99666.0636]], dtype=torch.float64)
	q_value: tensor([[-30.9520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.460319870829196
epoch: 83, step: 48
	action: tensor([[  6243.4322, -14499.6894, -23315.4319,  -1527.8042,  55521.3599,
           9424.4715,   5514.2438]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39979872188603593, distance: 0.8865539134607896 entropy 11.938663150365915
epoch: 83, step: 49
	action: tensor([[-50873.2327,  -6641.2843, -25159.9353,  -2881.0399, -19358.8269,
         -11412.9030, -21325.5065]], dtype=torch.float64)
	q_value: tensor([[-23.9455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08581486862112297, distance: 1.09414220339101 entropy 12.326878219000154
epoch: 83, step: 50
	action: tensor([[-77530.2546,  16813.9676, -57956.8010,  83565.5317,  11687.5798,
          81990.5277, -54015.7160]], dtype=torch.float64)
	q_value: tensor([[-24.1601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6057697500620645, distance: 1.4501012583689914 entropy 12.373644407515979
epoch: 83, step: 51
	action: tensor([[-23538.2400, -28568.3263, -41710.0405,   8813.5837,   8814.2484,
         -30626.5718,  37389.6504]], dtype=torch.float64)
	q_value: tensor([[-19.3882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6458080813228637, distance: 1.4680683928506395 entropy 11.751554099433989
epoch: 83, step: 52
	action: tensor([[ -63408.3347,   52134.9339, -105686.5698,   14884.5670,  -25358.2976,
           71473.6730,   36990.4061]], dtype=torch.float64)
	q_value: tensor([[-24.5434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03650074135871906, distance: 1.1232654108314106 entropy 12.502073410606569
epoch: 83, step: 53
	action: tensor([[-124112.1202,  -99195.9702,  131353.8282, -111408.9667,   62107.6879,
          -31389.6212,  -26972.5536]], dtype=torch.float64)
	q_value: tensor([[-24.3515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1131595353988812, distance: 1.6635001187017655 entropy 12.359131923308512
epoch: 83, step: 54
	action: tensor([[-98530.8786,  28793.3358,   8478.1979,  15534.0791, -66557.2249,
          39240.8548,  72040.4076]], dtype=torch.float64)
	q_value: tensor([[-23.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02873118563397825, distance: 1.1606670247054356 entropy 12.455507457577571
epoch: 83, step: 55
	action: tensor([[-148531.2299,   73202.3007,  -83906.4823,  -56551.8087,  -77636.2566,
           -4873.3497,  143307.3115]], dtype=torch.float64)
	q_value: tensor([[-24.6345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.42225016430027
epoch: 83, step: 56
	action: tensor([[ -4084.7854, -17108.9032, -46784.5845,  56911.7523, -27871.7547,
          -5209.1948,  -1336.6143]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8124961828788935, distance: 1.5406189786991547 entropy 11.938663150365915
epoch: 83, step: 57
	action: tensor([[-99765.0900,   2631.5497,  35464.7481,  94238.0858, -33664.5131,
          -2848.9831,  94571.4573]], dtype=torch.float64)
	q_value: tensor([[-22.0245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7057870320565682, distance: 0.6207083658638685 entropy 12.33787401163012
epoch: 83, step: 58
	action: tensor([[ -10497.0487,   12683.6958, -102377.8525,   24482.8858,    -124.0728,
           67402.8734,   32866.7313]], dtype=torch.float64)
	q_value: tensor([[-26.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4988996960294292, distance: 0.8100634296380289 entropy 12.470417742351968
epoch: 83, step: 59
	action: tensor([[-72655.7360, -44186.9442, -48391.1218, -62909.6983, -10243.2454,
           9551.7224,   8427.0201]], dtype=torch.float64)
	q_value: tensor([[-26.2593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8014647612593155, distance: 1.535923477482506 entropy 12.316973883799658
epoch: 83, step: 60
	action: tensor([[-53902.6441,  -4608.4044,  -1449.8689, -74741.8393,  28555.5213,
          62940.7491, -63378.7651]], dtype=torch.float64)
	q_value: tensor([[-21.5752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4911642240805483, distance: 1.3973957917807742 entropy 12.275986961129906
epoch: 83, step: 61
	action: tensor([[ 32660.1458, -89311.4390,  20166.0146,  50592.8635,  70580.5975,
         102889.6787, -13281.0413]], dtype=torch.float64)
	q_value: tensor([[-22.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10185555834637494, distance: 1.2012102424610704 entropy 12.306400722156624
epoch: 83, step: 62
	action: tensor([[ 54781.4304,  25442.1424,  64263.3267,  12245.4131, -14722.6324,
         -55628.3473, -23970.4744]], dtype=torch.float64)
	q_value: tensor([[-21.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.29505706913904
epoch: 83, step: 63
	action: tensor([[ 27983.4018, -31933.1671,  71439.5344,   1004.1675,  49043.7512,
          57079.2968,  -1622.9562]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3140455177750435, distance: 0.9477730217474221 entropy 11.938663150365915
epoch: 83, step: 64
	action: tensor([[ 58939.0928, -71249.2193, -13566.6984,  30578.9919, -68477.2337,
         -42668.4407, -33192.0050]], dtype=torch.float64)
	q_value: tensor([[-25.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3410288726647561, distance: 0.9289447540542691 entropy 12.452955181893685
epoch: 83, step: 65
	action: tensor([[ -95976.9305, -100015.2535,   27537.9480, -122410.5192,   90092.9716,
           85376.8379,   30082.0416]], dtype=torch.float64)
	q_value: tensor([[-26.8818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36052081570459604, distance: 1.3347787644427274 entropy 12.597512909273783
epoch: 83, step: 66
	action: tensor([[ 25269.0709, -47161.9107,  95310.2436,  85283.2637,  55066.4051,
         -23175.5019, -30905.8108]], dtype=torch.float64)
	q_value: tensor([[-22.2745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3333008307258639, distance: 0.934375947203444 entropy 12.395407412237331
epoch: 83, step: 67
	action: tensor([[  4239.8265,  39451.8766, -80761.3236,  24032.3165,  39747.0642,
         -14687.4007, -27671.0136]], dtype=torch.float64)
	q_value: tensor([[-22.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5868021679129638, distance: 0.7355898201562571 entropy 12.243380527536756
epoch: 83, step: 68
	action: tensor([[-109272.1428,  -91926.8003,    5973.3000,   15863.5591,  -25368.6122,
          -65816.1344,   44960.0477]], dtype=torch.float64)
	q_value: tensor([[-24.3486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1424145487187336, distance: 1.059730406605851 entropy 12.191199794279857
epoch: 83, step: 69
	action: tensor([[ -22425.1519, -107534.8878,  -77800.9759,  110586.2453,   15072.3145,
          130391.9203,   44148.5881]], dtype=torch.float64)
	q_value: tensor([[-26.8878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17609536866927145, distance: 1.0387120391671014 entropy 12.684769882551105
epoch: 83, step: 70
	action: tensor([[ 66441.8357, -63873.0937, -41193.1607,   6560.5429,  23052.0842,
          45388.9933, -46571.7623]], dtype=torch.float64)
	q_value: tensor([[-24.7132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13717433494780307, distance: 1.06296317865984 entropy 12.674669842316687
epoch: 83, step: 71
	action: tensor([[   829.7813, -32304.8409, -53921.5068, -39228.8664,  26819.0739,
           9564.0341, -24530.8224]], dtype=torch.float64)
	q_value: tensor([[-21.5376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07245151339372002, distance: 1.1850741541122394 entropy 12.2952614144328
epoch: 83, step: 72
	action: tensor([[ -29950.5858,  -51828.2415, -131929.0112,    6677.6027,    7921.8301,
          -22283.8591,   61468.7307]], dtype=torch.float64)
	q_value: tensor([[-22.8285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5261364859776625, distance: 1.4136873795821003 entropy 12.281677169548074
epoch: 83, step: 73
	action: tensor([[-78091.7326, -61191.3275,  83370.4361, -49824.5274,   2199.2929,
          76558.4179,  72187.0146]], dtype=torch.float64)
	q_value: tensor([[-20.4281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0051408992835111, distance: 1.6204257690943806 entropy 12.259811459040709
epoch: 83, step: 74
	action: tensor([[ 34598.6973, -74359.0490,  40149.1993, -26598.3047,  27182.0172,
         -18938.4601, 132182.8841]], dtype=torch.float64)
	q_value: tensor([[-20.9868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059850623106048495, distance: 1.1095710693581615 entropy 12.160512026628865
epoch: 83, step: 75
	action: tensor([[-57772.0813,  37881.1273,   4073.7017,  31256.3894,  32525.2713,
          84207.8810, -47425.7922]], dtype=torch.float64)
	q_value: tensor([[-20.7776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.184316686039713
epoch: 83, step: 76
	action: tensor([[-63190.3499, -52138.7303, -13556.3772,  41074.3636,  48200.6544,
          35855.9337,  18289.0407]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0162578523196402, distance: 1.6249115629517765 entropy 11.938663150365915
epoch: 83, step: 77
	action: tensor([[-45203.7036,  15683.3347, -50412.9381,  39111.3054, 134923.9565,
          13221.9807,  10910.0101]], dtype=torch.float64)
	q_value: tensor([[-21.6778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24548789672459326, distance: 0.994007814479572 entropy 12.406881255442112
epoch: 83, step: 78
	action: tensor([[ -18097.6588,  -61591.0361, -131405.8850,   41741.9923,   48633.3321,
          -12938.3139,   31615.5823]], dtype=torch.float64)
	q_value: tensor([[-29.4749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04072330143080227, distance: 1.1208013384978581 entropy 12.646705139369766
epoch: 83, step: 79
	action: tensor([[  -7583.8867, -110292.3660,   21867.0502, -102602.2261,  -90326.7471,
          -14972.2523,   54517.1686]], dtype=torch.float64)
	q_value: tensor([[-24.1162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6339048939903662, distance: 1.462749910038144 entropy 12.432441968950826
epoch: 83, step: 80
	action: tensor([[-17799.1620, -29576.6105,  31880.5831,  37105.7631,  97221.6409,
          42221.2418,  22465.2936]], dtype=torch.float64)
	q_value: tensor([[-21.3293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41521346376037105, distance: 1.3613433339064323 entropy 12.258959003845932
epoch: 83, step: 81
	action: tensor([[-83629.6944,   9195.5890,  89909.8023, -34278.5420, -51427.7815,
          71485.1908,  36011.6865]], dtype=torch.float64)
	q_value: tensor([[-21.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17951376334962288, distance: 1.0365549822444693 entropy 12.510912118322352
epoch: 83, step: 82
	action: tensor([[-60486.5381, -80745.9661, -42806.8226, 105632.7684, -62810.6180,
         -46259.1374,   6200.6455]], dtype=torch.float64)
	q_value: tensor([[-30.2544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6034423011340226, distance: 1.4490499680220592 entropy 12.441866160653321
epoch: 83, step: 83
	action: tensor([[  14975.9982,   46257.6807,   12357.9741,    5187.9956,  -40667.9290,
          104671.7067, -103202.9297]], dtype=torch.float64)
	q_value: tensor([[-20.8831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5213327681042759, distance: 0.7917235116399466 entropy 12.377361369879107
epoch: 83, step: 84
	action: tensor([[-145735.6374,  -57568.2086,   16495.0355,   47943.7561,   85570.5020,
           50811.3201,   23917.7394]], dtype=torch.float64)
	q_value: tensor([[-26.0464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.402882550995695
epoch: 83, step: 85
	action: tensor([[-12654.9306,  32570.2670, -19975.1027,  28475.3822, -62357.1521,
          30653.7024, -14680.7194]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.938663150365915
epoch: 83, step: 86
	action: tensor([[-19451.4828, -39265.1858,   4272.8126,  84023.7088,  12863.4636,
          50209.3752,  62553.5857]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.938663150365915
epoch: 83, step: 87
	action: tensor([[-56035.8227, -47718.5375, -49715.9389,  38604.3777, -25848.7361,
         -29278.7913, -32585.0418]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36552358963123055, distance: 1.3372305715127952 entropy 11.938663150365915
epoch: 83, step: 88
	action: tensor([[-111179.7640,  -65007.2305,  -18874.9790,  106071.4962,   83007.6582,
         -177790.1944,  -69858.9126]], dtype=torch.float64)
	q_value: tensor([[-24.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6978258082589741, distance: 1.4910879437705078 entropy 12.427419017503954
epoch: 83, step: 89
	action: tensor([[  3041.9180, -57748.8717,  38791.2800, 134241.9546, -11020.4501,
          19011.9561,  86115.4284]], dtype=torch.float64)
	q_value: tensor([[-24.6568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48650082693516516, distance: 0.8200240080982993 entropy 12.485118651441551
epoch: 83, step: 90
	action: tensor([[-20474.7496, -44613.7502,  32940.2800, -23394.1264,  24650.8692,
          50548.5480,   2005.3720]], dtype=torch.float64)
	q_value: tensor([[-21.6031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6545976850974999, distance: 1.4719833562827547 entropy 12.317030285836532
epoch: 83, step: 91
	action: tensor([[-2.3834e+04, -1.1864e+05,  1.3610e+05, -2.0765e+04, -3.0268e+03,
         -7.6082e+01,  4.9011e+02]], dtype=torch.float64)
	q_value: tensor([[-23.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5331878066400524, distance: 0.7818578285768523 entropy 12.41584692648021
epoch: 83, step: 92
	action: tensor([[ -31070.6800,   72820.5116,   72730.8728,    -758.6015,  -10810.8593,
         -106742.3892,  -77394.5347]], dtype=torch.float64)
	q_value: tensor([[-21.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672781713272673, distance: 0.6545992094344556 entropy 12.301349485483696
epoch: 83, step: 93
	action: tensor([[ 106050.6173,  -51761.8001,  -96056.7558,  -58858.4607,   29158.0711,
           36312.0041, -103223.8797]], dtype=torch.float64)
	q_value: tensor([[-35.5121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49835571772710574, distance: 0.8105029997199806 entropy 12.644366461379146
epoch: 83, step: 94
	action: tensor([[  -6895.8859,  -51695.6397,   73078.4663,   13740.5973,   93692.6983,
         -106790.2591, -110385.6755]], dtype=torch.float64)
	q_value: tensor([[-22.1861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7394091471894106, distance: 1.5092374322459272 entropy 12.381804364721225
epoch: 83, step: 95
	action: tensor([[ 45197.1261,  -6985.6104,   7527.5676, -23630.8731, -25978.3046,
          92521.5868,  11904.7495]], dtype=torch.float64)
	q_value: tensor([[-23.4131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38775555768613235, distance: 0.8954041831924026 entropy 12.539075711803793
epoch: 83, step: 96
	action: tensor([[ -33717.6541,    4968.8120,  -32442.8782,   28894.8195, -131747.6175,
            3295.9894,   80865.3421]], dtype=torch.float64)
	q_value: tensor([[-24.1392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.35928324521613
epoch: 83, step: 97
	action: tensor([[-48858.7122, -39306.7123,  56686.0408,  11379.9138,  -3599.8149,
         -14460.5854, -25942.4830]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9344730237910392, distance: 1.5916150320648568 entropy 11.938663150365915
epoch: 83, step: 98
	action: tensor([[-138049.7987,    7062.1667,   16287.7740,  -30331.2305,   24207.2621,
           24291.7875,   61041.2753]], dtype=torch.float64)
	q_value: tensor([[-21.3248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2558864216166611, distance: 0.987134448649253 entropy 12.287221427633886
epoch: 83, step: 99
	action: tensor([[ 21956.3112,  37775.7103,  20265.0191,   2315.1781, -44410.2071,
         -83643.1442,  37013.6199]], dtype=torch.float64)
	q_value: tensor([[-25.7736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15491362736643177, distance: 1.0519794171332244 entropy 12.1948398080178
epoch: 83, step: 100
	action: tensor([[-15841.4126,   4091.6840,  35549.2112,  27531.4558,  15254.2726,
         -53808.3679, -24247.7127]], dtype=torch.float64)
	q_value: tensor([[-15.7767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5677143348994225, distance: 0.7523884333230259 entropy 11.682050886536183
epoch: 83, step: 101
	action: tensor([[-55360.4995, -23666.7562,  69362.9430,  45994.0831, -51479.9510,
          10678.8894,  30605.5045]], dtype=torch.float64)
	q_value: tensor([[-25.5855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.450678724192886, distance: 1.3782954234764297 entropy 12.316533196634918
epoch: 83, step: 102
	action: tensor([[-80052.9580,   9702.1632,  48746.0807, -15485.9057,  12098.7919,
          89241.0489,  85004.8477]], dtype=torch.float64)
	q_value: tensor([[-23.0771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10031974922360742, distance: 1.2003728037732206 entropy 12.42696224496935
epoch: 83, step: 103
	action: tensor([[-29215.3205,  23846.1929,  42038.9356,  40134.5614, -22512.0445,
          44788.1751,  11059.1497]], dtype=torch.float64)
	q_value: tensor([[-26.2879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4919855249539007, distance: 0.8156329023658092 entropy 12.283436160625758
epoch: 83, step: 104
	action: tensor([[  51102.1989, -127192.6142,  -57955.3534,  -97186.1894,  -14792.0410,
           19020.3731,   55424.6488]], dtype=torch.float64)
	q_value: tensor([[-24.9778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.251284825069519
epoch: 83, step: 105
	action: tensor([[ -94655.4848,  -61343.7011, -100363.2864,   96650.4144,  -50047.8469,
           54174.9449,   40756.0156]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4070369871492878, distance: 1.357405017775027 entropy 11.938663150365915
epoch: 83, step: 106
	action: tensor([[  -8239.1136, -108105.3381,   18687.6492,  -73720.0998,   70717.7134,
         -102482.5891,  126027.9921]], dtype=torch.float64)
	q_value: tensor([[-24.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5429744492358326, distance: 1.421464639146069 entropy 12.59565319784574
epoch: 83, step: 107
	action: tensor([[ 30633.0091,  24760.1496, -42129.6681, -21786.5012,  32627.7397,
         -25594.5558, -73215.3215]], dtype=torch.float64)
	q_value: tensor([[-20.7675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2542251893989179, distance: 0.9882357222795842 entropy 12.277283536739906
epoch: 83, step: 108
	action: tensor([[-125790.5300,  -26803.3907,   41292.0774,   25324.4411,   50174.8655,
          -19365.7650,  -31485.7965]], dtype=torch.float64)
	q_value: tensor([[-26.4829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.374185508873678
epoch: 83, step: 109
	action: tensor([[-25347.4202, -17843.9220, -23541.3391,  19155.9265,  12823.7336,
          59883.4639, -11353.7074]], dtype=torch.float64)
	q_value: tensor([[-24.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1272006693820219, distance: 1.0690890848454286 entropy 11.938663150365915
epoch: 83, step: 110
	action: tensor([[ -7301.9829, -61039.9325, -49051.6587,  18054.4459, -68127.5908,
          30741.9787, 127813.2888]], dtype=torch.float64)
	q_value: tensor([[-21.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18885861283518546, distance: 1.0306352064003426 entropy 12.372347953092873
epoch: 83, step: 111
	action: tensor([[-49556.6472, -20635.7460, -63062.6851, -17235.7895, -16199.8776,
         -14851.7718,  34062.7080]], dtype=torch.float64)
	q_value: tensor([[-21.3033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6047157277977311, distance: 1.4496252592695555 entropy 12.34105941868915
epoch: 83, step: 112
	action: tensor([[ 30056.5953, -54547.3824, -97640.6237, -42352.1686,    542.4600,
          41346.4350,  -9869.2479]], dtype=torch.float64)
	q_value: tensor([[-20.4625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07088739863648452, distance: 1.1030390011404863 entropy 12.312903755012275
epoch: 83, step: 113
	action: tensor([[-143278.3686,  -24724.0327,  -22068.2682,   25684.0895,   21269.6324,
          -20075.9128,  -16447.1090]], dtype=torch.float64)
	q_value: tensor([[-21.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8107741530416166, distance: 1.5398869434216915 entropy 12.28359524203212
epoch: 83, step: 114
	action: tensor([[27882.5604, 92845.0654, 28369.3214,  6634.2595, 54631.4607, 28509.6821,
         51480.3989]], dtype=torch.float64)
	q_value: tensor([[-20.9745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10533337586698921, distance: 1.082398822992477 entropy 12.324402064458749
epoch: 83, step: 115
	action: tensor([[ -9273.5563, -79176.7984, -29667.4508,  20744.4709,  -7286.2622,
          -9364.5546,  37086.5722]], dtype=torch.float64)
	q_value: tensor([[-19.6108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17530204971138796, distance: 1.2405990379575085 entropy 12.15285912504036
epoch: 83, step: 116
	action: tensor([[-20220.2356, -53626.3786,  76991.2868,  87610.7865,  45857.8116,
         -26174.4902,  72131.4487]], dtype=torch.float64)
	q_value: tensor([[-24.3490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0740424443501102, distance: 1.1011645792860965 entropy 12.513853549415284
epoch: 83, step: 117
	action: tensor([[   5878.0169,    3514.6891, -111376.5065,  139669.8283,  -27886.0244,
          -91401.9800,   -9769.9900]], dtype=torch.float64)
	q_value: tensor([[-26.5475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023671892750417, distance: 0.9558068578621306 entropy 12.613457098347407
epoch: 83, step: 118
	action: tensor([[-56097.9141, -54969.0871,   5618.9973,  40733.2493, -87918.0076,
         -17788.5261, -50881.8532]], dtype=torch.float64)
	q_value: tensor([[-25.2047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16200597116792714, distance: 1.0475557747762623 entropy 12.270005148009558
epoch: 83, step: 119
	action: tensor([[-52385.1336,   7638.7212, -14886.1529,  81806.7601,  46710.2180,
           6782.3952,  44673.8782]], dtype=torch.float64)
	q_value: tensor([[-25.7858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31824753785175164, distance: 0.9448656283158922 entropy 12.525258449637857
epoch: 83, step: 120
	action: tensor([[    430.5225, -123751.7143,   -2706.2639,   69291.3782,  -66684.3191,
          -25351.0802,  -29307.0880]], dtype=torch.float64)
	q_value: tensor([[-25.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5351816556839537, distance: 1.4178705413185007 entropy 12.346187831984546
epoch: 83, step: 121
	action: tensor([[ -27631.5565, -139735.9487,   53095.0027,   42360.0919,   10263.7023,
           97241.6080,  -17356.7020]], dtype=torch.float64)
	q_value: tensor([[-23.5709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10639183541198016, distance: 1.2036803600813808 entropy 12.524588728914832
epoch: 83, step: 122
	action: tensor([[-103626.4167,  -96689.6254,  -21623.7722,  -44905.0422,  -51119.2921,
          -52250.7830,  -11614.2566]], dtype=torch.float64)
	q_value: tensor([[-25.0751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.529905854806453, distance: 1.4154321196281974 entropy 12.666823131236086
epoch: 83, step: 123
	action: tensor([[-102866.3322, -117582.5140,   75647.2192,   75251.3541,   72302.5135,
           57581.4397, -126614.4384]], dtype=torch.float64)
	q_value: tensor([[-23.8011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6798757565133158, distance: 1.4831848288713867 entropy 12.485135206449982
epoch: 83, step: 124
	action: tensor([[-46288.6689, -65650.6415, -49594.2094,  64233.3260,  44541.5858,
         -65506.3639, -52043.0419]], dtype=torch.float64)
	q_value: tensor([[-24.6140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9018672562677361, distance: 1.5781446013055134 entropy 12.593165587207949
epoch: 83, step: 125
	action: tensor([[  1660.4005, -60842.6928,  65484.0350, -82345.3419,  -3747.8935,
         -89355.3312,  66129.5976]], dtype=torch.float64)
	q_value: tensor([[-22.5980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011875959676582726, distance: 1.151119291459669 entropy 12.343252811942994
epoch: 83, step: 126
	action: tensor([[ 10113.4301,  74503.8000, -12596.9517,  31522.3632,  30282.7083,
         117205.1740,  54950.5188]], dtype=torch.float64)
	q_value: tensor([[-21.5338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31220191682160436, distance: 0.9490458050200887 entropy 12.182588975352951
epoch: 83, step: 127
	action: tensor([[-151047.4990,  -62324.7667,   11718.3859,   83457.2319,   26061.0406,
            1703.1258,   67144.6516]], dtype=torch.float64)
	q_value: tensor([[-24.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.303476382627759
LOSS epoch 83 actor 227.57811100970827 critic 405.33165467577135
epoch: 84, step: 0
	action: tensor([[ 68203.4604,   9663.8827,  12065.5226,  17379.9730,  -8557.2849,
          52701.3160, -31650.9786]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.970125030473024
epoch: 84, step: 1
	action: tensor([[  25927.3143, -116339.0426,  -53406.6747,  -40537.3402,   26927.4793,
           -7455.7891,   45767.3250]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002522329199741602, distance: 1.1457865515486894 entropy 11.970125030473024
epoch: 84, step: 2
	action: tensor([[-38071.1736,  65707.0334,  41249.0544,  51911.6504,  38924.2420,
         -30500.0008,  23417.5988]], dtype=torch.float64)
	q_value: tensor([[-23.7364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.318926466086676
epoch: 84, step: 3
	action: tensor([[ 25595.7249, -61538.1056,   4870.5867,  34373.0086, -14063.8513,
          -9687.4113,  27665.5524]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3192369424039513, distance: 0.9441797534325171 entropy 11.970125030473024
epoch: 84, step: 4
	action: tensor([[-80527.2088, -65826.1720, -78240.9640, -29349.8951, -13677.7052,
         150620.9078,  54899.0069]], dtype=torch.float64)
	q_value: tensor([[-24.8399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7688063943287791, distance: 1.521937587696266 entropy 12.339696383088585
epoch: 84, step: 5
	action: tensor([[   -201.2289,  -14152.7413, -132423.6701,   38002.6312,   -5634.9362,
          -51198.0510,   81206.0295]], dtype=torch.float64)
	q_value: tensor([[-19.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5939164563998154, distance: 1.4447392462656081 entropy 12.159333677239443
epoch: 84, step: 6
	action: tensor([[  1735.4882, -85939.0318,  41632.2571, -38519.7406, -26409.8215,
          18757.6263,  20274.7959]], dtype=torch.float64)
	q_value: tensor([[-24.8364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14009829003342178, distance: 1.2218779860652593 entropy 12.420248330884059
epoch: 84, step: 7
	action: tensor([[-11276.8614,  -2452.0326,  13517.7001,  30599.7775,  49308.4930,
             80.7350,  39574.2220]], dtype=torch.float64)
	q_value: tensor([[-20.5874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4395371704447779, distance: 1.3729924052556388 entropy 12.050285627793182
epoch: 84, step: 8
	action: tensor([[ -6308.6968,  -6328.1305, -19419.2312,  42656.9660,   1769.1287,
          11202.2859,  44733.3151]], dtype=torch.float64)
	q_value: tensor([[-25.6446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22951741768307943, distance: 1.2688901789081166 entropy 12.550938871328134
epoch: 84, step: 9
	action: tensor([[  -3374.7199,  112024.1998,   40523.1937,   67215.5795,  -20491.7125,
         -226794.3986,   69558.9049]], dtype=torch.float64)
	q_value: tensor([[-28.9436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.046620228169486966, distance: 1.1707151953789205 entropy 12.701685350491292
epoch: 84, step: 10
	action: tensor([[175436.2862,  28392.9735, -74701.0793,  39378.4720,  -4549.3794,
         145845.1614,  51927.9900]], dtype=torch.float64)
	q_value: tensor([[-29.9827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5116740099369391, distance: 0.7996714902435615 entropy 12.622264314607333
epoch: 84, step: 11
	action: tensor([[ -2175.9719,  96237.1795, -70334.5506, -23022.3026, 112271.8206,
          70419.0168,  31131.7604]], dtype=torch.float64)
	q_value: tensor([[-27.5595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002862737132886539, distance: 1.1427051016492749 entropy 12.40436992885785
epoch: 84, step: 12
	action: tensor([[ 10651.7354,   3124.7249, -62690.2389,  31330.4707, -66472.9669,
         -46171.3702,  -8948.6051]], dtype=torch.float64)
	q_value: tensor([[-30.7161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.298738414378805
epoch: 84, step: 13
	action: tensor([[ -24651.7985,   20312.1479,    7968.5891,   26439.9175, -118078.4818,
            6434.8623,   -3489.9802]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02402801145623845, distance: 1.130512503015816 entropy 11.970125030473024
epoch: 84, step: 14
	action: tensor([[ -7816.8300, -77440.5227, -40673.6012,  10193.2275, -58794.2887,
         -14259.1308,  11847.0667]], dtype=torch.float64)
	q_value: tensor([[-26.9554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02110120260039894, distance: 1.1563547457760148 entropy 12.00502666845188
epoch: 84, step: 15
	action: tensor([[  32550.9623, -127367.4052, -153878.6921,    -415.0122,  122163.3124,
           10414.1950,   94598.2502]], dtype=torch.float64)
	q_value: tensor([[-30.8199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2728884365567579, distance: 1.2910761659738712 entropy 12.735787246536477
epoch: 84, step: 16
	action: tensor([[-10415.5100,  10509.3499,  68421.5127, -14603.8480,  71103.0363,
           9560.9450,  34433.7337]], dtype=torch.float64)
	q_value: tensor([[-22.4877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.266348430204287
epoch: 84, step: 17
	action: tensor([[-15726.7623, -30519.1796, -49041.6149,  42975.5705, -13223.0645,
          18482.3268,  44803.3748]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8934601195684504, distance: 1.5746526719471836 entropy 11.970125030473024
epoch: 84, step: 18
	action: tensor([[ -87805.3436, -115902.3796,   28511.3946,   15345.9929, -117261.1475,
          -68170.8502,   39702.7855]], dtype=torch.float64)
	q_value: tensor([[-26.3198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08323734630143331, distance: 1.0956835711057091 entropy 12.53952537282437
epoch: 84, step: 19
	action: tensor([[ -1804.2034, -39408.2357, -16218.7211,  -2141.4769, -66320.2911,
         -90378.9793,  79348.2800]], dtype=torch.float64)
	q_value: tensor([[-24.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8530365282388448, distance: 1.5577533127836074 entropy 12.476040556227087
epoch: 84, step: 20
	action: tensor([[-56386.2579, -24105.5992,   2492.1164, -51595.3299,  -6914.6219,
          25613.9019,  23000.5072]], dtype=torch.float64)
	q_value: tensor([[-27.1361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13850651173349238, distance: 1.0621422701397931 entropy 12.47374856082313
epoch: 84, step: 21
	action: tensor([[-33061.3507,  98937.3297,  10461.7468,  35063.4633, -16433.2992,
         -48484.5337, -58539.2102]], dtype=torch.float64)
	q_value: tensor([[-32.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5819586003553723, distance: 1.4393096932042795 entropy 12.685655167424008
epoch: 84, step: 22
	action: tensor([[ 47866.0327, -56388.6041,   5398.2267, -71139.5214, -82002.4235,
          52421.5118, -78284.3835]], dtype=torch.float64)
	q_value: tensor([[-25.8095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029804802293911914, distance: 1.1612725213215045 entropy 12.292299072022988
epoch: 84, step: 23
	action: tensor([[-35297.9296, -79787.5320, -43875.2956, 133939.7422,  96373.6058,
         -39929.2791, -42701.3604]], dtype=torch.float64)
	q_value: tensor([[-23.2267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8391869735736757, distance: 1.5519210876933531 entropy 12.274651879146163
epoch: 84, step: 24
	action: tensor([[-27211.4784, -27268.0944,  21960.7900,  30978.7703,  44035.0151,
          18343.5563,  33215.4899]], dtype=torch.float64)
	q_value: tensor([[-22.7036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27863654191757625, distance: 1.2939880010872609 entropy 12.310426731087489
epoch: 84, step: 25
	action: tensor([[ -36060.8971, -151067.3277,  -30480.8358,   83075.4361,  -61465.0550,
          117505.1808,  -21858.8545]], dtype=torch.float64)
	q_value: tensor([[-24.6805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9459862327302537, distance: 1.596344333497268 entropy 12.571290217311349
epoch: 84, step: 26
	action: tensor([[-62765.5243, -50333.1503,  33292.2064, -24307.2318, -53098.5316,
          51151.4163, -56692.6363]], dtype=torch.float64)
	q_value: tensor([[-27.3094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0853970358036027, distance: 1.6525365326491246 entropy 12.588069066914693
epoch: 84, step: 27
	action: tensor([[ -287.8376, 11016.2857, -5725.8722, 73185.2975, 70735.0284, -5906.2139,
         -1851.0016]], dtype=torch.float64)
	q_value: tensor([[-24.5606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3102441075658644, distance: 0.9503955691116205 entropy 12.325843511386635
epoch: 84, step: 28
	action: tensor([[ -51819.6035,  -72175.7112,   48403.0656,    8457.8576, -101264.9086,
           39109.3197,  -27249.4312]], dtype=torch.float64)
	q_value: tensor([[-30.5538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40841961437630836, distance: 1.3580717821609993 entropy 12.505852156163199
epoch: 84, step: 29
	action: tensor([[ -42452.5776,  -11418.0958, -139781.5047,    3596.1003,  -51790.4479,
           25466.8014,   57172.4933]], dtype=torch.float64)
	q_value: tensor([[-26.0095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.691792550766531, distance: 1.4884362807829044 entropy 12.55715091330948
epoch: 84, step: 30
	action: tensor([[ 81045.3230, -26528.7701, -56542.6128,  49704.1614, 137777.3229,
         -59683.4799, 195039.8112]], dtype=torch.float64)
	q_value: tensor([[-24.3100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3877997405645489, distance: 0.8953718739992153 entropy 12.489322960870208
epoch: 84, step: 31
	action: tensor([[-87896.9729,  52975.6131,  45853.5102, -65757.5559,  45223.1528,
          -3753.2026,   7119.7489]], dtype=torch.float64)
	q_value: tensor([[-24.7352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.380646370793915
epoch: 84, step: 32
	action: tensor([[-67299.2129, -63511.1314,  10095.1225,  67495.3134,   6316.1693,
          52541.0506,  42974.9437]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5376943960666121, distance: 1.4190304313868418 entropy 11.970125030473024
epoch: 84, step: 33
	action: tensor([[ 22617.7595, 144116.3547,  71263.2508, -81915.6610, -44399.4864,
          58445.9821,  28825.4495]], dtype=torch.float64)
	q_value: tensor([[-26.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.531257388609175
epoch: 84, step: 34
	action: tensor([[ 14331.3685, -34167.2963,  75977.7800, -54994.1731,  43304.7718,
          -9751.6748,  29111.0736]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13836678467793329, distance: 1.220949780051219 entropy 11.970125030473024
epoch: 84, step: 35
	action: tensor([[-39270.2917, -91723.7634, -59344.2996, 135752.9933,  -3456.4159,
           3478.8182,  13396.8501]], dtype=torch.float64)
	q_value: tensor([[-20.7564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1145473330918918, distance: 1.2081085329254269 entropy 12.07381855650714
epoch: 84, step: 36
	action: tensor([[  6734.8636,  65484.1870, 134579.9813, 104230.2149,  10492.6544,
           6753.5214, -29181.7730]], dtype=torch.float64)
	q_value: tensor([[-20.8819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4507784308549748, distance: 0.848067627436258 entropy 12.236930374477495
epoch: 84, step: 37
	action: tensor([[-16899.9744, -39543.0837,   -908.5237,  -5000.8251,   1201.1031,
           -828.5557, -76274.0536]], dtype=torch.float64)
	q_value: tensor([[-24.1011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.280469107275364
epoch: 84, step: 38
	action: tensor([[-60140.1225,  -7337.8466,  37036.6850, -24232.5680,  48309.1615,
         -43062.0089,  -4567.5874]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8965884731607268, distance: 1.5759529468438 entropy 11.970125030473024
epoch: 84, step: 39
	action: tensor([[-41004.5678,  30585.2161, -16538.7562,  87985.8971,   5072.9290,
         -19615.4603,  47807.5348]], dtype=torch.float64)
	q_value: tensor([[-25.9899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05592431213961935, distance: 1.1759073040948165 entropy 12.401210957364754
epoch: 84, step: 40
	action: tensor([[ -51875.5629, -131232.2200,    -979.5152,  -23071.0667,   18155.4002,
           30688.4527,  -35397.7350]], dtype=torch.float64)
	q_value: tensor([[-29.5170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3584735301719042, distance: 1.3337741116601465 entropy 12.417537121495979
epoch: 84, step: 41
	action: tensor([[ 14913.0336,   8745.6043,    732.2582, -21601.9481,  20861.6222,
         -62703.7692, -19501.4510]], dtype=torch.float64)
	q_value: tensor([[-23.5680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.37799320396367
epoch: 84, step: 42
	action: tensor([[ -1268.7079,  -3991.2457,  11601.2635, 112955.0752,  10122.9764,
         -61594.5219, -24601.0136]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035102714846753535, distance: 1.1642558186260188 entropy 11.970125030473024
epoch: 84, step: 43
	action: tensor([[ 50179.0727, -68262.2595,  21725.0095,  76230.7437,  22675.8365,
         -73724.4392, -35113.5058]], dtype=torch.float64)
	q_value: tensor([[-27.1920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17290461888023545, distance: 1.2393330788439336 entropy 12.611015347726141
epoch: 84, step: 44
	action: tensor([[   5169.8984,   45110.7871,   48969.7605,  -68246.6908,   89640.6112,
          -74253.9511, -144845.0217]], dtype=torch.float64)
	q_value: tensor([[-28.7215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2535387355652341, distance: 0.9886904320376574 entropy 12.565340984003365
epoch: 84, step: 45
	action: tensor([[-78707.0077, -34084.9291, -74957.3926, -66852.3508, -10592.5194,
         -87995.9180,  13250.4387]], dtype=torch.float64)
	q_value: tensor([[-27.5064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05388247766515164, distance: 1.17476983151655 entropy 12.28190532239201
epoch: 84, step: 46
	action: tensor([[-101475.0902, -107464.9123,  -39990.9506,  -43905.6392,  -76597.0175,
          -30928.0291,  -47061.3120]], dtype=torch.float64)
	q_value: tensor([[-23.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3831121348459856, distance: 1.3458150762478942 entropy 12.241574956386655
epoch: 84, step: 47
	action: tensor([[ -57702.8296,  -60344.7789,  -57809.0193, -164115.3923,  -19467.9640,
          -11771.3009,    2452.3581]], dtype=torch.float64)
	q_value: tensor([[-24.8709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8044209900482184, distance: 1.5371831968101577 entropy 12.352162067539414
epoch: 84, step: 48
	action: tensor([[ -26365.4287,    1840.3305,   34512.6411,  -65118.9577,  -62842.8483,
          -95036.2081, -103790.9501]], dtype=torch.float64)
	q_value: tensor([[-27.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28468565012639946, distance: 0.967843554181673 entropy 12.61172471952835
epoch: 84, step: 49
	action: tensor([[ 10609.2238, -98478.2201, -83779.9186,  39961.8789, -11567.5662,
         -27052.6791, 108650.2474]], dtype=torch.float64)
	q_value: tensor([[-36.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.504980004403862
epoch: 84, step: 50
	action: tensor([[ -1053.8389, -73516.1032, -19215.8910,  35264.5292,  -8642.9654,
          -3905.0851, -66046.4306]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.970125030473024
epoch: 84, step: 51
	action: tensor([[-17217.8100, -87845.6125,  31100.1671, -19427.1679,  11671.9941,
          11427.8362, -72245.1340]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10921966445777087, distance: 1.0800453808624566 entropy 11.970125030473024
epoch: 84, step: 52
	action: tensor([[ 37390.2357, -25748.5178, -38585.5923, 158110.0497,  16731.6979,
           1098.3589, -56445.8089]], dtype=torch.float64)
	q_value: tensor([[-28.7766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4392077435577315, distance: 0.8569543691326749 entropy 12.476071486322954
epoch: 84, step: 53
	action: tensor([[ 51743.5594, -59782.1001,   4913.2017,   3711.3942,  21779.8530,
         -56382.4098, -92837.9451]], dtype=torch.float64)
	q_value: tensor([[-23.4179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47278964948563207, distance: 0.8308998054309861 entropy 12.251040543337037
epoch: 84, step: 54
	action: tensor([[ -35181.3803, -112089.5669,   62404.7107,  148201.9639,  -97264.7812,
          -17312.5833,   57812.5352]], dtype=torch.float64)
	q_value: tensor([[-26.2765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21337091207222214, distance: 1.0149430969876414 entropy 12.43541422234486
epoch: 84, step: 55
	action: tensor([[ 61681.1382,  98732.1208,  27641.7933,  71932.4522,  47654.8430,
         -20314.0842,  19884.7657]], dtype=torch.float64)
	q_value: tensor([[-23.9502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5037277366540869, distance: 0.8061515527916464 entropy 12.368326187924946
epoch: 84, step: 56
	action: tensor([[-55483.5247, -55868.9160, -63588.9951, -18935.4784, -87204.9693,
          45733.1422,  60868.4670]], dtype=torch.float64)
	q_value: tensor([[-29.0753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7904950134705646, distance: 1.5312399495779356 entropy 12.475796434771945
epoch: 84, step: 57
	action: tensor([[ 83758.4676, -66489.6940, -58597.0225,  64596.8690,   6813.7133,
         -35340.4755, -30766.7977]], dtype=torch.float64)
	q_value: tensor([[-22.9606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35347590266837503, distance: 0.9201297052093294 entropy 12.266222040766413
epoch: 84, step: 58
	action: tensor([[ 23312.3127, -44163.4630,  41598.3910, -39641.9771,  -6667.4036,
         -57658.5516,    267.1336]], dtype=torch.float64)
	q_value: tensor([[-26.2158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11876590958132993, distance: 1.074242519081659 entropy 12.480455380623917
epoch: 84, step: 59
	action: tensor([[ -41243.7129,  -76816.3641,   -1650.0414,   93836.2876,   25019.5351,
          -73037.3412, -102008.1637]], dtype=torch.float64)
	q_value: tensor([[-27.3973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24906977948468967, distance: 1.2789396266833055 entropy 12.438684139744202
epoch: 84, step: 60
	action: tensor([[-76237.7071, -23185.6720, -33678.0389, -61970.9146,  54281.3882,
         160426.6354, -37029.2123]], dtype=torch.float64)
	q_value: tensor([[-25.5964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5041349396199633, distance: 1.403460173720992 entropy 12.413135853217105
epoch: 84, step: 61
	action: tensor([[-72680.7045, -91188.9104,  23852.7403,   2641.4516, -38399.1948,
         -34814.7710, -24943.2494]], dtype=torch.float64)
	q_value: tensor([[-25.1536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15778713458286597, distance: 1.0501893962824007 entropy 12.34112695468944
epoch: 84, step: 62
	action: tensor([[  -5612.6049, -132696.6556, -101887.6772,  -10217.2593,  182046.0375,
           31881.4586,   80299.2974]], dtype=torch.float64)
	q_value: tensor([[-27.5226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3833463412877358, distance: 1.3459290168352391 entropy 12.652721560167702
epoch: 84, step: 63
	action: tensor([[-7.2979e+04, -7.4879e+04,  6.6546e+04,  5.6039e+01,  3.6069e+04,
         -7.8233e+03, -2.6848e+04]], dtype=torch.float64)
	q_value: tensor([[-23.8217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1393220369893307, distance: 1.673766131417304 entropy 12.388530734499946
epoch: 84, step: 64
	action: tensor([[  67412.8861,  -60015.0259, -138169.0913,  -15335.8518,  -51452.5905,
           -9831.1726,    6018.6464]], dtype=torch.float64)
	q_value: tensor([[-23.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10280122224333532, distance: 1.0839294845049545 entropy 12.380098686740086
epoch: 84, step: 65
	action: tensor([[-21273.0078, -92455.1665,  15610.0255, -55910.8772, -12586.2945,
          68686.3656,  42307.0373]], dtype=torch.float64)
	q_value: tensor([[-24.6141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4443755030316421, distance: 1.3752978064504333 entropy 12.2962875535016
epoch: 84, step: 66
	action: tensor([[  -7829.3934, -141402.9253,  -81886.5248, -100443.1229,  -72068.2255,
         -102770.8148,   26582.8616]], dtype=torch.float64)
	q_value: tensor([[-24.9967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16404171695425784, distance: 1.0462825831326987 entropy 12.431297951539351
epoch: 84, step: 67
	action: tensor([[-54984.4532,  -7449.8715,  10068.5752,  17975.6258,  15333.8264,
          -3799.1621,  50141.9217]], dtype=torch.float64)
	q_value: tensor([[-24.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07664914120029676, distance: 1.1873911083304207 entropy 12.35522047670313
epoch: 84, step: 68
	action: tensor([[-34097.1224,  -8694.1418,  28776.2847, -31686.3233, 145778.3666,
          21776.1153,  58648.4854]], dtype=torch.float64)
	q_value: tensor([[-25.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1332918956788982, distance: 1.2182252162980103 entropy 12.53562347526947
epoch: 84, step: 69
	action: tensor([[ -16040.2964,  160531.7165, -128655.7773,   -5412.7568,   60425.7585,
          190166.4370,  -50609.0556]], dtype=torch.float64)
	q_value: tensor([[-27.6947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5037100932883882, distance: 0.8061658827283483 entropy 12.493085896864951
epoch: 84, step: 70
	action: tensor([[   5053.9760, -106213.0176,    3214.7612,   47236.0697,   50638.3571,
           46208.9099,  -13609.9976]], dtype=torch.float64)
	q_value: tensor([[-35.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.40362565711757
epoch: 84, step: 71
	action: tensor([[-13477.1047, -40316.4892,  34628.1474,  85330.1415, -47785.7544,
          93511.7917,  66746.1554]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6722943940699566, distance: 1.479834200933697 entropy 11.970125030473024
epoch: 84, step: 72
	action: tensor([[ -61072.0332, -107848.2972,   54990.8332,   -1846.4644,   76365.8181,
           54175.5552,   10910.8351]], dtype=torch.float64)
	q_value: tensor([[-24.0341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5678130305202613, distance: 1.4328602280016718 entropy 12.36540677915219
epoch: 84, step: 73
	action: tensor([[-44803.8509, -42471.9256, -23329.8865, -26079.2508,  32525.4203,
         -95531.5538,  50964.4289]], dtype=torch.float64)
	q_value: tensor([[-25.9857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17482370318581775, distance: 1.24034655110367 entropy 12.29657177153874
epoch: 84, step: 74
	action: tensor([[-21320.1066,  83348.3895, -62188.4053, -80038.4678,  34539.3844,
          77730.4506, -45519.0062]], dtype=torch.float64)
	q_value: tensor([[-28.4923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2323658580300505, distance: 1.270359156489217 entropy 12.624374638341022
epoch: 84, step: 75
	action: tensor([[ 20120.1633, -56765.8548, -70738.7502,  37275.5679, -62854.7735,
          58174.8192,   7452.2733]], dtype=torch.float64)
	q_value: tensor([[-30.6100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11545628102004657, distance: 1.208601057523263 entropy 12.364567305373615
epoch: 84, step: 76
	action: tensor([[-82079.4326,  37784.9064,   -272.6882,  59418.2776,  86182.6216,
          46659.2157,  35520.9996]], dtype=torch.float64)
	q_value: tensor([[-23.7436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1241680544385515, distance: 1.2133114977742618 entropy 12.350618007281081
epoch: 84, step: 77
	action: tensor([[-83238.3033, -43581.8133, -43127.7771, -24488.3164,  17832.3499,
          49462.0875, -90455.1371]], dtype=torch.float64)
	q_value: tensor([[-31.1224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8338903283045289, distance: 1.5496848000600985 entropy 12.570203684121676
epoch: 84, step: 78
	action: tensor([[  3700.3181,  15959.4133, -44615.8292,  -3970.8230,  82152.2045,
         -15192.7000,  17615.4790]], dtype=torch.float64)
	q_value: tensor([[-25.0613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5247784582591752, distance: 0.7888687506270377 entropy 12.33304265326792
epoch: 84, step: 79
	action: tensor([[  1902.2621, -24169.0450, -24182.7226, -39795.2139,  25434.2591,
         -23334.1088,  35824.4022]], dtype=torch.float64)
	q_value: tensor([[-29.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3451285010598021, distance: 0.9260506439966715 entropy 12.275941515522032
epoch: 84, step: 80
	action: tensor([[-67128.4531, -56543.3280,   6669.2711,  16749.9061, -48171.5446,
          13956.3818,  62308.9133]], dtype=torch.float64)
	q_value: tensor([[-27.0558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.76370414908817, distance: 1.519740934965989 entropy 12.32341451269185
epoch: 84, step: 81
	action: tensor([[  9253.9963, -58920.9992,  28048.3450,  39975.7473, 114674.9601,
          73884.3773, -19876.8584]], dtype=torch.float64)
	q_value: tensor([[-22.9113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13380584099076076, distance: 1.06503607549138 entropy 12.35515300740751
epoch: 84, step: 82
	action: tensor([[  58818.3425, -128963.8515,   21375.2540,  -10552.5111,   14372.4640,
           -3813.6125,   -2126.0641]], dtype=torch.float64)
	q_value: tensor([[-28.0656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08538697416162666, distance: 1.1921996662127636 entropy 12.457627209117788
epoch: 84, step: 83
	action: tensor([[-76242.7721, -26632.5201,  13210.7684, -28326.9047,  17774.8326,
          35491.5071, -21527.7633]], dtype=torch.float64)
	q_value: tensor([[-26.7789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12471992850048863, distance: 1.0706073325413663 entropy 12.336029553564373
epoch: 84, step: 84
	action: tensor([[ 70543.7128,  38200.6392, -90461.5314,   9201.5072,   4246.1220,
         -55439.9180, -67819.6552]], dtype=torch.float64)
	q_value: tensor([[-24.9187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.308427123817001
epoch: 84, step: 85
	action: tensor([[ 11691.3675, -15739.2680, -25062.8876,  28447.7119,  -8391.2400,
          20665.6199,  11863.4967]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0712256297611571, distance: 1.1843966510365969 entropy 11.970125030473024
epoch: 84, step: 86
	action: tensor([[   2253.0135, -128519.9722,  110331.5642,   79513.7409,   35220.2551,
          -96979.6963,   24652.7732]], dtype=torch.float64)
	q_value: tensor([[-22.5197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29119279415434085, distance: 0.9634313081796698 entropy 12.261064538687625
epoch: 84, step: 87
	action: tensor([[-10184.1806,  -7668.1837,  14195.3330,  96855.6191, 167632.0938,
          47540.2589, 129269.4279]], dtype=torch.float64)
	q_value: tensor([[-31.4448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.592207780053714
epoch: 84, step: 88
	action: tensor([[ -6003.4661, -14926.6405,  18893.3879,  96402.3463,  23923.1358,
         -12853.6970,  29282.6339]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.970125030473024
epoch: 84, step: 89
	action: tensor([[ 13159.8408, -75958.1173,  37986.1576,  45489.7153,  -8372.5099,
          -4673.9072,  -6806.0232]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2580471562450479, distance: 0.9857002010499628 entropy 11.970125030473024
epoch: 84, step: 90
	action: tensor([[   1447.5597, -141473.2293,   -7646.9227,   32470.2796,  -38097.4775,
          -63050.3250,   11361.1015]], dtype=torch.float64)
	q_value: tensor([[-26.9134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31804883914516935, distance: 0.9450033101806861 entropy 12.459216332646813
epoch: 84, step: 91
	action: tensor([[ 43298.5068, -80106.4638, -78612.9267,  21060.9657,  -7093.1504,
         -42678.1540, 105750.3081]], dtype=torch.float64)
	q_value: tensor([[-27.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21471378828294974, distance: 1.2612282035869307 entropy 12.44837675043203
epoch: 84, step: 92
	action: tensor([[133681.6269, -11453.4173,  35624.3214,  35196.2848, -96290.2728,
          15533.3234,  -7949.9427]], dtype=torch.float64)
	q_value: tensor([[-28.3882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.510343110199766, distance: 0.8007604742692327 entropy 12.33156638950275
epoch: 84, step: 93
	action: tensor([[-18007.5527, -95878.3505,  55660.5194,  -4012.2486, -69392.0667,
          13159.9578,  17886.0114]], dtype=torch.float64)
	q_value: tensor([[-24.8912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6279299931592095, distance: 1.4600729518229119 entropy 12.282783282037027
epoch: 84, step: 94
	action: tensor([[-68494.1437, -43803.3837,  -2116.7823,  35203.8158, -37018.5640,
         -95396.9603, -77104.1666]], dtype=torch.float64)
	q_value: tensor([[-24.0154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38669246827980963, distance: 1.3475558434433048 entropy 12.313097649752674
epoch: 84, step: 95
	action: tensor([[ -71796.9279,  -84295.1666,  134934.3035, -137367.2770,  -66916.4547,
          -49536.0526,  -15129.5528]], dtype=torch.float64)
	q_value: tensor([[-26.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6296866018375251, distance: 1.4608604811776635 entropy 12.504980987242543
epoch: 84, step: 96
	action: tensor([[ 12141.2112,   6013.8243,  60436.3797,  -1255.3461, -56197.5052,
          47093.6275,  51424.0468]], dtype=torch.float64)
	q_value: tensor([[-23.0995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6329582227153323, distance: 0.6932892127731698 entropy 12.290677747870335
epoch: 84, step: 97
	action: tensor([[ -63695.7333, -115468.2305,  -34703.1326,  -27549.2329,   86524.2366,
           55880.6193,   36784.5294]], dtype=torch.float64)
	q_value: tensor([[-33.2439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.366758014609214
epoch: 84, step: 98
	action: tensor([[-19413.6612, -79671.7861,  32277.1690,  -8671.8232,  19448.5591,
         -36227.0952,  10774.8670]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.970125030473024
epoch: 84, step: 99
	action: tensor([[  6643.5666, -66615.5583,  19343.6168,  37159.3904,   8352.8801,
          -1004.6000,  39383.5370]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.970125030473024
epoch: 84, step: 100
	action: tensor([[-42416.4704,  17222.6350,  12048.4922,  23851.8517,  43605.2877,
          40499.1598,   -428.2829]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5825879197012983, distance: 0.7393314829833626 entropy 11.970125030473024
epoch: 84, step: 101
	action: tensor([[ 24012.3443,  19156.5809,  59934.2447,   6240.4477,  32803.9093,
         -16121.4690, -18737.0937]], dtype=torch.float64)
	q_value: tensor([[-31.6731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43063032147170277, distance: 0.8634831375868444 entropy 12.311673007811732
epoch: 84, step: 102
	action: tensor([[  11085.2721,   76202.7674,  -80587.6910,  -31502.5546,  -86743.1453,
         -110200.4757,    2476.0290]], dtype=torch.float64)
	q_value: tensor([[-26.8245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5135832614891903, distance: 0.7981066858601057 entropy 12.175189188728831
epoch: 84, step: 103
	action: tensor([[ 81998.7129, -16246.6904, 107360.4029,   6088.4186,  22710.1443,
         -29064.6360,  16809.7340]], dtype=torch.float64)
	q_value: tensor([[-27.6410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45380985935149243, distance: 0.8457239342877071 entropy 12.269435181571055
epoch: 84, step: 104
	action: tensor([[  49852.7680, -159750.3296,  -41553.6439,   86270.2840,  101205.3978,
          -41653.5001,    9668.1715]], dtype=torch.float64)
	q_value: tensor([[-27.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006122186530794105, distance: 1.1478418534185992 entropy 12.439111788306553
epoch: 84, step: 105
	action: tensor([[   1760.0684, -158593.5363,   37953.9858,  -27505.7754,  -52707.4647,
          -15030.0367,  -22608.4057]], dtype=torch.float64)
	q_value: tensor([[-30.1356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39537488249322617, distance: 1.3517679456941154 entropy 12.544021186788544
epoch: 84, step: 106
	action: tensor([[-67092.2327,  22678.4862,  49557.6084,  10637.9690, -65539.9931,
          17949.8607,   8778.5274]], dtype=torch.float64)
	q_value: tensor([[-26.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.430037505581465
epoch: 84, step: 107
	action: tensor([[-84404.9102, -96631.3937,  33115.4468,  57919.3479, -46954.9290,
          78537.5894,  58516.2127]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19981344207623397, distance: 1.0236519522602479 entropy 11.970125030473024
epoch: 84, step: 108
	action: tensor([[ 20058.7817,  22043.4548, -35913.5650, 165033.2789,  79707.2711,
          38228.9727, -71669.5423]], dtype=torch.float64)
	q_value: tensor([[-21.7787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3400860911663146, distance: 0.9296090311892627 entropy 12.387841738714204
epoch: 84, step: 109
	action: tensor([[   3248.2078,  -18017.2494,   -4070.1844, -113545.8543,   67313.0701,
           80114.7541,  -32370.2391]], dtype=torch.float64)
	q_value: tensor([[-27.7521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2941995713104453, distance: 1.3018391236657678 entropy 12.507851347929913
epoch: 84, step: 110
	action: tensor([[ -4088.0590,  -9274.0581, -51051.1910,  18763.6390, -52246.7444,
         -64063.0213, -22114.8216]], dtype=torch.float64)
	q_value: tensor([[-22.6162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33418258391103894, distance: 0.933757856168079 entropy 12.213419765466353
epoch: 84, step: 111
	action: tensor([[ 19583.4855, -16500.7928, -19673.9477,  55451.5320,  77800.9669,
         -63504.2759,  38905.7998]], dtype=torch.float64)
	q_value: tensor([[-25.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19786970532584058, distance: 1.0248944769324848 entropy 12.54309566386101
epoch: 84, step: 112
	action: tensor([[122417.3611,  26258.5607,  -9883.6574,  79314.5175,  -1757.3895,
          26859.0330,  26452.4904]], dtype=torch.float64)
	q_value: tensor([[-25.9849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23309428045065128, distance: 1.002138348918975 entropy 12.392435778738152
epoch: 84, step: 113
	action: tensor([[ -2591.9830, -22515.9120, -64075.2604, -85860.7040,  89231.8139,
         -16486.3289,  -3186.5970]], dtype=torch.float64)
	q_value: tensor([[-26.6552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 12.212860080963267
epoch: 84, step: 114
	action: tensor([[-50045.8545, -12611.0737, -24529.5002,  11837.4918, -27572.2914,
          34803.0930,  39703.1127]], dtype=torch.float64)
	q_value: tensor([[-27.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46751151718625694, distance: 1.386268811213889 entropy 11.970125030473024
epoch: 84, step: 115
	action: tensor([[-57540.3185, -83217.9499, -85954.8267, -48900.3668, -25744.7906,
         -74438.0310,  14541.0738]], dtype=torch.float64)
	q_value: tensor([[-24.2692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0631273453716452, distance: 1.6436892354492594 entropy 12.393839585530278
epoch: 84, step: 116
	action: tensor([[ -57082.7654,   45638.2265, -141512.0339,  150827.7391,  116201.9846,
           92319.1372,  -10755.6640]], dtype=torch.float64)
	q_value: tensor([[-26.3976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3574590993945175, distance: 0.9172908936692407 entropy 12.505097533292465
epoch: 84, step: 117
	action: tensor([[-19055.5759,  11237.3628,  -5810.6405,  16204.7974,  28034.3488,
         -29287.3190, -43456.8026]], dtype=torch.float64)
	q_value: tensor([[-29.8105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26442937880661654, distance: 1.286779052182663 entropy 12.430403602770207
epoch: 84, step: 118
	action: tensor([[  8998.2406, -81031.1617, -15637.4184,  55310.8111,  -8735.8210,
           7143.4862,  57712.2517]], dtype=torch.float64)
	q_value: tensor([[-27.5531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5931789795909533, distance: 0.7298916337633761 entropy 12.417733017247249
epoch: 84, step: 119
	action: tensor([[  31310.7249,  -82246.9687, -150274.7857,  -48011.4391,   42364.3981,
           50155.0110,   46708.7258]], dtype=torch.float64)
	q_value: tensor([[-23.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07692501489793768, distance: 1.1875432233427052 entropy 12.343322773959654
epoch: 84, step: 120
	action: tensor([[-84022.3934, -57934.8180, -28983.5521, -27310.2099,  21988.8346,
          54746.5195, -98860.6465]], dtype=torch.float64)
	q_value: tensor([[-23.9903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11634045607726584, distance: 1.2090799661321372 entropy 12.344723431159222
epoch: 84, step: 121
	action: tensor([[-77418.3094, -32316.2181,   8325.6901, -17068.4348, 133413.3780,
         -32863.5841, -85956.5395]], dtype=torch.float64)
	q_value: tensor([[-25.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5155078554859158, distance: 1.4087560338470757 entropy 12.3497253394407
epoch: 84, step: 122
	action: tensor([[  65600.1950,  -26480.5677,  -44354.0416,   23018.5245, -114614.9718,
         -125018.7985,   16314.6668]], dtype=torch.float64)
	q_value: tensor([[-28.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3182688276525534, distance: 0.9448508750445976 entropy 12.524188130174267
epoch: 84, step: 123
	action: tensor([[  1917.2418, -16575.8023, -34711.6932,  74056.4707,  65699.4631,
           5698.9207, -71350.4125]], dtype=torch.float64)
	q_value: tensor([[-27.4313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1260854298115276, distance: 1.0697718934727616 entropy 12.443686005117055
epoch: 84, step: 124
	action: tensor([[ 126303.7872,  -62327.4307,   31671.5942,  -10789.8856,   -2124.8803,
           15393.2386, -134245.6320]], dtype=torch.float64)
	q_value: tensor([[-24.9590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3105651905389102, distance: 1.310044377616465 entropy 12.503957608489438
epoch: 84, step: 125
	action: tensor([[-83081.7140,  -8484.6208,  32290.5617,  33117.8069, -13822.2862,
         151436.0086, -42091.0079]], dtype=torch.float64)
	q_value: tensor([[-23.6847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00024351689400514687, distance: 1.144483579097716 entropy 12.24245840872186
epoch: 84, step: 126
	action: tensor([[ 12958.7355,  -3951.7062,  67700.0366, -45249.9873,  40387.7400,
          14405.0846, 125366.0886]], dtype=torch.float64)
	q_value: tensor([[-23.9113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2774970241884436, distance: 1.2934112731081613 entropy 12.389017874029188
