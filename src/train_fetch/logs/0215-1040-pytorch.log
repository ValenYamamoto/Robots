epoch: 0, step: 0
	action: tensor([[-0.0459,  0.2569, -0.0313, -0.1947,  0.1770, -0.1334,  0.0733]],
       dtype=torch.float64)
	q_value: tensor([[-30.1117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995499640774607, distance: 0.9577348134544545 entropy -3.367712109680099
epoch: 0, step: 1
	action: tensor([[-0.2348, -0.1194,  0.2271, -0.3242,  0.1998,  0.1332, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[-22.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23056955899370246, distance: 1.2694329797932942 entropy -3.1493370598789108
epoch: 0, step: 2
	action: tensor([[-0.1418, -0.8703,  0.1892,  0.0842,  0.2928,  0.1568, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-27.4801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4920262595527789, distance: 1.3977996475995833 entropy -2.784530838886171
epoch: 0, step: 3
	action: tensor([[-0.7546, -0.2196,  0.1408, -0.5242,  0.5364, -0.3573,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-37.6793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0793978380957507, distance: 1.650157840853612 entropy -2.4930042849949325
epoch: 0, step: 4
	action: tensor([[-0.1124, -0.0357, -0.0271, -0.0165,  0.1871,  0.0133, -0.0499]],
       dtype=torch.float64)
	q_value: tensor([[-26.3101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12595469262147418, distance: 1.06985190915496 entropy -3.320278256108246
epoch: 0, step: 5
	action: tensor([[ 0.3435, -0.0918,  0.2904,  0.0458,  0.0868,  0.1457,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[-25.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5777971502973593, distance: 0.7435621481418963 entropy -3.0463078142011155
epoch: 0, step: 6
	action: tensor([[ 0.0774,  0.1883, -0.3652,  0.5414,  0.6311, -0.0269, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-32.6948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7435621481418963 entropy -2.4999572890820914
epoch: 0, step: 7
	action: tensor([[ 0.1567,  0.1209,  0.0662,  0.0819,  0.0320, -0.0549,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5246550500190952, distance: 0.7889711729460508 entropy -3.3657999214414027
epoch: 0, step: 8
	action: tensor([[-0.1666,  0.4639, -0.4675,  0.1393, -0.4958,  0.2267,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-25.2170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7889711729460508 entropy -2.8440704253669766
epoch: 0, step: 9
	action: tensor([[ 0.1916,  0.0009, -0.0966,  0.3826, -0.0368, -0.0864,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7889711729460508 entropy -3.3657999214414027
epoch: 0, step: 10
	action: tensor([[-0.0789, -0.1310, -0.3325,  0.0105,  0.0124,  0.1039,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11081610625688398, distance: 1.0790771271027453 entropy -3.3657999214414027
epoch: 0, step: 11
	action: tensor([[-0.0467,  0.0994, -0.2283, -0.1659,  0.1782,  0.2689,  0.0343]],
       dtype=torch.float64)
	q_value: tensor([[-25.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3095240073092411, distance: 0.9508915428118129 entropy -3.0694251929381644
epoch: 0, step: 12
	action: tensor([[ 0.6760, -0.1110, -0.1640,  0.0285,  0.2275, -0.2405, -0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-25.5789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5958398707678993, distance: 0.727500719441398 entropy -3.0523098864346068
epoch: 0, step: 13
	action: tensor([[ 0.1203,  0.0482,  0.1065, -0.2492, -0.5175,  0.0279, -0.1014]],
       dtype=torch.float64)
	q_value: tensor([[-30.6417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30522738960620877, distance: 0.9538455059446985 entropy -2.78439483040185
epoch: 0, step: 14
	action: tensor([[ 0.1675, -0.1260,  0.6282, -1.0829, -0.6814,  0.6561,  0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-27.0838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17918105034522758, distance: 1.2426446058794944 entropy -2.606434672626815
epoch: 0, step: 15
	action: tensor([[ 0.4136, -0.1376, -0.0193, -0.7569, -0.0980,  0.7996,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-41.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3359415377172592, distance: 0.9325236415947346 entropy -1.948435958193181
epoch: 0, step: 16
	action: tensor([[-0.5525, -0.6958, -0.3135,  1.2458, -0.4136,  0.0628, -0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-38.4781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32824154208440603, distance: 1.3188494507977804 entropy -2.233164730618284
epoch: 0, step: 17
	action: tensor([[-0.0999, -0.1832, -0.1731,  0.0692,  0.3302,  0.0507,  0.2631]],
       dtype=torch.float64)
	q_value: tensor([[-38.8889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06982963477485904, distance: 1.1036667091828973 entropy -2.541878007007419
epoch: 0, step: 18
	action: tensor([[-0.4331, -0.2711,  0.3792, -0.5655,  0.0088,  0.3641, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-26.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6757197759420903, distance: 1.4813490072368685 entropy -3.0103543851137284
epoch: 0, step: 19
	action: tensor([[ 0.1649, -0.5173,  0.6122, -0.4607,  0.3032,  0.3986,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-31.7542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22764887152917268, distance: 1.2679256210529395 entropy -2.4884473341743525
epoch: 0, step: 20
	action: tensor([[ 1.3057, -0.1201,  0.1509, -0.6175,  2.0172,  0.0489, -0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-39.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19427339223711015, distance: 1.0271894403306268 entropy -2.213409622161898
epoch: 0, step: 21
	action: tensor([[ 1.4153, -0.2413,  0.4909, -1.3735,  0.3232,  0.3199, -0.5917]],
       dtype=torch.float64)
	q_value: tensor([[-53.1846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04055844220980398, distance: 1.1673200143397555 entropy -2.1168285781775484
epoch: 0, step: 22
	action: tensor([[-0.1356,  1.5679,  0.6762, -1.2581, -0.4846, -0.5749, -0.5143]],
       dtype=torch.float64)
	q_value: tensor([[-52.0635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1673200143397555 entropy -1.878582170316195
epoch: 0, step: 23
	action: tensor([[-0.1604,  0.1493,  0.0474, -0.1882,  0.1363, -0.0611,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0822422435153557, distance: 1.096278066342922 entropy -3.3657999214414027
epoch: 0, step: 24
	action: tensor([[ 0.0509,  0.1525, -0.1151,  0.2836, -0.1528, -0.0589, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[-23.2622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4874874437410066, distance: 0.8192358486399557 entropy -3.0377517658647717
epoch: 0, step: 25
	action: tensor([[-0.2035,  0.1893,  0.3661, -0.1698,  0.1201,  0.2055,  0.0793]],
       dtype=torch.float64)
	q_value: tensor([[-23.9500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11324749839815074, distance: 1.077600798268585 entropy -2.921541643483278
epoch: 0, step: 26
	action: tensor([[ 0.2269,  0.0550, -0.4948, -0.4984,  0.4940,  0.5132,  0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-26.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5411231633562399, distance: 0.7751839303355293 entropy -2.668533374161516
epoch: 0, step: 27
	action: tensor([[-0.1378, -0.0502,  0.1083,  0.3746,  0.0950,  0.0249, -0.1740]],
       dtype=torch.float64)
	q_value: tensor([[-30.6800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27373133410077677, distance: 0.975226169977443 entropy -2.9173806719706183
epoch: 0, step: 28
	action: tensor([[-0.1174,  0.0965,  0.3761,  0.1962,  0.1010,  0.5218,  0.1037]],
       dtype=torch.float64)
	q_value: tensor([[-27.7495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.975226169977443 entropy -2.8489063117306226
epoch: 0, step: 29
	action: tensor([[-0.4461,  0.2564, -0.2569,  0.0391,  0.1821,  0.0513,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08188972920825033, distance: 1.19027741250351 entropy -3.3657999214414027
epoch: 0, step: 30
	action: tensor([[-0.3044, -0.0871,  0.1366,  0.0347, -0.0407,  0.0583,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-21.1069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12788464394512045, distance: 1.215315495227304 entropy -3.483188727745324
epoch: 0, step: 31
	action: tensor([[-0.7641, -0.2130, -0.2664, -0.3220,  0.1416,  0.5147,  0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-25.3345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8142708062886712, distance: 1.5413730077657646 entropy -2.8262437427401084
epoch: 0, step: 32
	action: tensor([[ 0.1513, -0.0659,  0.3393, -0.2072, -0.1471,  0.1793,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-29.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27767013634791016, distance: 0.9725780829498332 entropy -3.090254356463767
epoch: 0, step: 33
	action: tensor([[-0.4455, -0.4594,  0.0723,  0.4738,  0.6186,  0.5523,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-30.5375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.054101116364484, distance: 1.1748916842056565 entropy -2.462244614571212
epoch: 0, step: 34
	action: tensor([[-0.3328, -0.6714,  0.0601, -0.4939,  0.1854,  0.4199,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[-37.0175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7278007519438947, distance: 1.5041928590999956 entropy -2.698175705048896
epoch: 0, step: 35
	action: tensor([[ 0.2437, -0.7131,  0.0969,  0.2814,  0.2099, -0.1131, -0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-35.3530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02396330695511173, distance: 1.1579742189566173 entropy -2.533839207113005
epoch: 0, step: 36
	action: tensor([[ 0.5417, -0.1755, -0.8281, -1.0093,  0.9287,  0.5287,  0.0307]],
       dtype=torch.float64)
	q_value: tensor([[-36.5893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4557520319936925, distance: 0.844218959620929 entropy -2.5672954944119715
epoch: 0, step: 37
	action: tensor([[ 0.5237, -0.4335,  0.1854,  0.2857, -0.3998, -0.5031, -0.3066]],
       dtype=torch.float64)
	q_value: tensor([[-37.6006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3419147078731192, distance: 0.928320167734074 entropy -2.6330116793060827
epoch: 0, step: 38
	action: tensor([[-1.5027, -0.1078,  0.5000, -1.2961,  0.6797,  0.6001,  0.0677]],
       dtype=torch.float64)
	q_value: tensor([[-33.6479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5273723192793285, distance: 1.8192454842430874 entropy -2.4909817611773746
epoch: 0, step: 39
	action: tensor([[-0.2305, -0.1606,  0.3291, -0.2317, -0.4641, -0.0496, -0.2150]],
       dtype=torch.float64)
	q_value: tensor([[-39.7312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2946348449099303, distance: 1.3020580267532462 entropy -2.6027638428449564
epoch: 0, step: 40
	action: tensor([[-0.2192,  0.0502, -0.2647, -0.2576, -0.0126,  0.3677,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-27.5786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03446537411087791, distance: 1.1244512194690215 entropy -2.5856779767407936
epoch: 0, step: 41
	action: tensor([[ 0.1807, -0.1309,  0.2602,  0.0965,  0.0542,  0.1678,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[-26.2562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44880898065665376, distance: 0.8495868069580478 entropy -2.9565597186128465
epoch: 0, step: 42
	action: tensor([[-0.3044, -0.8307, -0.3150, -0.7174, -0.5313,  0.2873,  0.0434]],
       dtype=torch.float64)
	q_value: tensor([[-31.6220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8064276406160784, distance: 1.5380376904274906 entropy -2.5406669278504275
epoch: 0, step: 43
	action: tensor([[ 0.6752, -0.7589, -0.1096, -0.8398, -0.4801, -0.0634,  0.0616]],
       dtype=torch.float64)
	q_value: tensor([[-36.4906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.587330304328709, distance: 1.4417512847225826 entropy -2.5496458764123937
epoch: 0, step: 44
	action: tensor([[-1.3325,  1.1089,  0.2476,  0.7210, -0.3446,  1.0108, -0.1678]],
       dtype=torch.float64)
	q_value: tensor([[-40.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4417512847225826 entropy -2.309674341510106
epoch: 0, step: 45
	action: tensor([[-0.0249, -0.1000, -0.0280,  0.4823,  0.3044,  0.0955,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4417512847225826 entropy -3.3657999214414027
epoch: 0, step: 46
	action: tensor([[-0.2435,  0.2341,  0.3597,  0.2370, -0.1153, -0.2641,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21264974001177261, distance: 1.015408234199173 entropy -3.3657999214414027
epoch: 0, step: 47
	action: tensor([[ 0.7722, -0.0015,  0.3116,  0.5650, -0.4493, -0.0624,  0.1060]],
       dtype=torch.float64)
	q_value: tensor([[-22.3432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9438036962672157, distance: 0.2712754976360101 entropy -2.8131797433392323
epoch: 0, step: 48
	action: tensor([[-0.2334, -0.3227, -0.0408,  0.4816, -0.2485,  0.1421,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023324836570973728, distance: 1.1309196893250095 entropy -3.3657999214414027
epoch: 0, step: 49
	action: tensor([[-0.8423, -0.0200,  0.2690,  0.7542,  1.1873,  0.4522,  0.1867]],
       dtype=torch.float64)
	q_value: tensor([[-30.3213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025674783314903138, distance: 1.1589415475967604 entropy -2.6413379602420552
epoch: 0, step: 50
	action: tensor([[-0.5847, -0.2626,  0.0777, -0.4911,  0.0034, -0.1030, -0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-38.0706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8540892947025192, distance: 1.5581957534913558 entropy -2.7907665645072455
epoch: 0, step: 51
	action: tensor([[-0.4266, -0.1299,  0.3005,  0.0079, -0.0122,  0.1613,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-25.6316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2914648505285107, distance: 1.3004629644992012 entropy -3.1089799701749783
epoch: 0, step: 52
	action: tensor([[ 0.4466, -0.5909, -0.6165,  0.1437,  0.5875,  0.4630,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-27.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2942973198006503, distance: 0.9613191121824276 entropy -2.672925020979786
epoch: 0, step: 53
	action: tensor([[ 0.0015,  0.1835, -0.0899, -0.2904, -0.3723,  0.1672, -0.1632]],
       dtype=torch.float64)
	q_value: tensor([[-39.9734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29441751486422074, distance: 0.9612372429084449 entropy -2.7100627178223937
epoch: 0, step: 54
	action: tensor([[-0.0279,  0.3323,  1.1526, -0.2302,  0.1905,  0.2071,  0.0655]],
       dtype=torch.float64)
	q_value: tensor([[-24.8531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9612372429084449 entropy -2.8206707155476494
epoch: 0, step: 55
	action: tensor([[-3.2591e-01, -2.2018e-04,  2.8060e-01, -2.2879e-02, -9.7251e-02,
         -5.3706e-02,  7.3557e-02]], dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15639588924412573, distance: 1.230580312868016 entropy -3.3657999214414027
epoch: 0, step: 56
	action: tensor([[ 0.2248,  0.0026,  0.1353, -0.1316, -0.3566,  0.0656,  0.1078]],
       dtype=torch.float64)
	q_value: tensor([[-24.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41594468464697887, distance: 0.8745480658150264 entropy -2.804206826842041
epoch: 0, step: 57
	action: tensor([[-0.0405, -1.0436, -0.2718,  0.2074, -0.2244,  0.6837,  0.0657]],
       dtype=torch.float64)
	q_value: tensor([[-28.5344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4459026711357639, distance: 1.3760246797797802 entropy -2.536497420162938
epoch: 0, step: 58
	action: tensor([[-0.5493, -0.4573,  0.1653,  0.3309, -0.2042,  0.4587,  0.1302]],
       dtype=torch.float64)
	q_value: tensor([[-43.6109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3943287246971743, distance: 1.3512611185493528 entropy -2.2531989894586553
epoch: 0, step: 59
	action: tensor([[ 0.0349, -0.3656, -0.4605, -0.2204,  0.8248,  0.1033,  0.2407]],
       dtype=torch.float64)
	q_value: tensor([[-33.5789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028864237542555626, distance: 1.1607420802554353 entropy -2.4330969052949163
epoch: 0, step: 60
	action: tensor([[-0.0628,  0.0543, -0.5672, -0.1265,  0.1968, -0.1020, -0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-30.9343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.215845244977102, distance: 1.0133455937462388 entropy -3.061781229364421
epoch: 0, step: 61
	action: tensor([[-0.2537, -0.1826, -0.0096, -0.0010,  0.1217,  0.0972,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[-21.8425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14704980196579553, distance: 1.225597398396119 entropy -3.656141939858928
epoch: 0, step: 62
	action: tensor([[ 0.1878,  0.2720,  0.2286, -0.4486,  0.4550,  0.3113,  0.0531]],
       dtype=torch.float64)
	q_value: tensor([[-26.3326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.499860170714983, distance: 0.8092867202692212 entropy -2.9431801580748203
epoch: 0, step: 63
	action: tensor([[ 0.0488, -0.4535,  0.3699, -0.1520, -0.7309, -0.1300, -0.1750]],
       dtype=torch.float64)
	q_value: tensor([[-29.9262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21598487735152316, distance: 1.2618879121674451 entropy -2.648310721865234
LOSS epoch 0 actor 459.910805136757 critic 326.7005252470899
epoch: 1, step: 0
	action: tensor([[ 1.1850,  0.7477, -0.1297, -1.0171, -0.9617,  0.5550,  0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-32.0855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8665224103514357, distance: 0.4180814234441056 entropy -2.3465581955856143
epoch: 1, step: 1
	action: tensor([[-1.2674,  0.6292,  0.9804,  0.6959, -0.1801, -0.3190, -0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-39.3706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4180814234441056 entropy -2.0339436860034494
epoch: 1, step: 2
	action: tensor([[ 0.3570,  0.0110,  0.0508,  0.0898, -0.0551, -0.1569,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6191296001162886, distance: 0.7062286050355386 entropy -3.364541563998618
epoch: 1, step: 3
	action: tensor([[-0.4416, -0.2730, -0.3706,  0.6660,  0.0532, -0.5367,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-27.1180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2975927378025949, distance: 1.303544604564621 entropy -2.7315643406335934
epoch: 1, step: 4
	action: tensor([[ 0.0395, -0.1317, -0.0582,  0.0984, -0.1198,  0.0085,  0.0905]],
       dtype=torch.float64)
	q_value: tensor([[-24.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943184729022569, distance: 0.9613047045332778 entropy -3.330086385919381
epoch: 1, step: 5
	action: tensor([[ 0.6896, -0.5240,  0.0948, -0.3049, -0.3454,  0.0497,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-26.1071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02226170713173592, distance: 1.1315350356820653 entropy -2.7922090961457378
epoch: 1, step: 6
	action: tensor([[-0.2541, -0.1395,  1.2662,  1.0729, -0.2260,  0.5260, -0.0728]],
       dtype=torch.float64)
	q_value: tensor([[-36.6646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8217155839804746, distance: 0.48318493446566757 entropy -2.294489974772994
epoch: 1, step: 7
	action: tensor([[ 0.0233, -1.2735, -0.1207, -1.4873, -0.7661,  0.8244,  0.3593]],
       dtype=torch.float64)
	q_value: tensor([[-44.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.48318493446566757 entropy -1.8680650025153092
epoch: 1, step: 8
	action: tensor([[ 0.2673,  0.0840, -0.1806,  0.1791, -0.0821, -0.0038,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6345724707611736, distance: 0.6917629914632988 entropy -3.364541563998618
epoch: 1, step: 9
	action: tensor([[ 0.6106, -0.2207,  0.2099, -0.8389, -0.7100,  0.1117,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-25.7535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6917629914632988 entropy -2.8584285778040246
epoch: 1, step: 10
	action: tensor([[0.0487, 0.0199, 0.0295, 0.2839, 0.3936, 0.0561, 0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5086923909343027, distance: 0.8021090906294183 entropy -3.364541563998618
epoch: 1, step: 11
	action: tensor([[-0.3485, -0.5547,  0.0932, -0.1626, -0.0282,  0.2303, -0.0116]],
       dtype=torch.float64)
	q_value: tensor([[-26.9859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.569453541531082, distance: 1.4336096822898996 entropy -2.8786470782235343
epoch: 1, step: 12
	action: tensor([[ 0.2206,  0.1061, -0.0921, -0.3801, -0.4873,  0.2867,  0.1069]],
       dtype=torch.float64)
	q_value: tensor([[-30.2565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4377051213425386, distance: 0.8581016899448483 entropy -2.6259430998165305
epoch: 1, step: 13
	action: tensor([[-0.8469, -0.2928,  0.6443, -0.4190, -0.0864,  0.0865,  0.0462]],
       dtype=torch.float64)
	q_value: tensor([[-28.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2265069691667874, distance: 1.707531494262064 entropy -2.520283052158906
epoch: 1, step: 14
	action: tensor([[-0.6692,  0.3428,  0.2046, -0.0354, -0.5948,  0.2000,  0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-29.3962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29183610822106854, distance: 1.3006498732586982 entropy -2.461558876545912
epoch: 1, step: 15
	action: tensor([[-0.2736, -0.4885, -0.4565, -0.2576,  0.5178,  0.3073,  0.2147]],
       dtype=torch.float64)
	q_value: tensor([[-24.5987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35221662184656544, distance: 1.3306989932909947 entropy -2.5989201537391105
epoch: 1, step: 16
	action: tensor([[-0.2439,  0.0436, -0.1256, -0.2585, -0.3899, -0.2121, -0.0979]],
       dtype=torch.float64)
	q_value: tensor([[-29.7867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04352912172630519, distance: 1.1689851116772265 entropy -3.0732142665383635
epoch: 1, step: 17
	action: tensor([[-0.2984, -0.1010,  0.0763, -0.3039,  0.0625, -0.0472,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-22.2268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.295360052954162, distance: 1.3024226588192789 entropy -3.2047698890135314
epoch: 1, step: 18
	action: tensor([[-0.3824, -0.1093,  0.3260, -0.1631, -0.2469,  0.1894,  0.0193]],
       dtype=torch.float64)
	q_value: tensor([[-24.2141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29963642240071886, distance: 1.3045707300305946 entropy -2.9819860696788223
epoch: 1, step: 19
	action: tensor([[-0.1370,  0.0300,  0.0724, -0.0506, -0.1669,  0.4518,  0.1606]],
       dtype=torch.float64)
	q_value: tensor([[-27.5444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23967226776870665, distance: 0.9978312681066671 entropy -2.568560455505587
epoch: 1, step: 20
	action: tensor([[ 0.3013, -0.5531,  0.5239,  0.2507,  0.0139,  0.1982,  0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-28.2013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3190222352670966, distance: 0.9443286350056032 entropy -2.533428494481921
epoch: 1, step: 21
	action: tensor([[ 1.6253, -0.5786, -0.2615, -0.2065,  0.1302, -0.4527,  0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-38.1018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4004455096107955, distance: 1.3542218007969684 entropy -2.2484502103707893
epoch: 1, step: 22
	action: tensor([[ 0.5747, -0.2091,  0.6563,  0.3294, -0.5895, -0.4423, -0.3560]],
       dtype=torch.float64)
	q_value: tensor([[-43.6186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5987268950478343, distance: 0.7248976961363646 entropy -2.2758632641556846
epoch: 1, step: 23
	action: tensor([[-0.8287, -0.0824, -0.1360, -0.7797, -0.3294, -0.0128,  0.1081]],
       dtype=torch.float64)
	q_value: tensor([[-35.5236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8965584116772525, distance: 1.5759404571361306 entropy -2.224283097179938
epoch: 1, step: 24
	action: tensor([[-0.3139, -0.1628,  0.0107,  0.0372,  0.3213,  0.1382,  0.0524]],
       dtype=torch.float64)
	q_value: tensor([[-26.0864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1252170040226126, distance: 1.2138774299931712 entropy -3.2421013434607047
epoch: 1, step: 25
	action: tensor([[ 0.2204, -0.2850,  0.0176, -0.0740,  0.7685,  0.0202,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[-25.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21484242308937518, distance: 1.0139933488271209 entropy -2.994492922559726
epoch: 1, step: 26
	action: tensor([[ 0.1498, -0.3788,  0.0269,  0.1680, -0.2871,  0.1106, -0.1681]],
       dtype=torch.float64)
	q_value: tensor([[-30.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23925362234035397, distance: 0.9981059391425767 entropy -2.815794634780974
epoch: 1, step: 27
	action: tensor([[-1.0264, -0.2079,  0.2534,  0.6670, -1.0213,  0.0755,  0.1328]],
       dtype=torch.float64)
	q_value: tensor([[-31.3972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6504293990155632, distance: 1.4701280663473015 entropy -2.531969262929073
epoch: 1, step: 28
	action: tensor([[-0.7509,  0.7486,  0.8635, -0.1979,  0.0224,  0.6527,  0.3979]],
       dtype=torch.float64)
	q_value: tensor([[-33.1776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043587268422503156, distance: 1.1191269845382608 entropy -2.2807388665884414
epoch: 1, step: 29
	action: tensor([[-0.4966, -1.2453,  0.1714, -0.3272, -0.5865,  0.6304,  0.0929]],
       dtype=torch.float64)
	q_value: tensor([[-30.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1191269845382608 entropy -2.186687643463958
epoch: 1, step: 30
	action: tensor([[-0.0561, -0.0164,  0.1746, -0.2343,  0.0083, -0.0612,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06387837913201999, distance: 1.1071917249032104 entropy -3.364541563998618
epoch: 1, step: 31
	action: tensor([[-0.2149,  0.0428, -0.3435, -0.0732,  0.0242,  0.1530,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-24.6794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07682848273243892, distance: 1.099506729030751 entropy -2.8345615547580088
epoch: 1, step: 32
	action: tensor([[-0.0876, -0.0691, -0.1696, -0.2982, -0.0409,  0.2857,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-23.2551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07996733153273738, distance: 1.0976359369409583 entropy -3.2566416012503656
epoch: 1, step: 33
	action: tensor([[-0.5712,  0.3861,  0.1388,  0.0317,  0.4256,  0.4172,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[-26.4460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05077187328520938, distance: 1.1149156006909144 entropy -2.8593639879739343
epoch: 1, step: 34
	action: tensor([[ 0.3287, -0.3146, -0.3242,  0.4156,  0.2043,  0.2277, -0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-26.0517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1149156006909144 entropy -2.999271617197858
epoch: 1, step: 35
	action: tensor([[-0.2018, -0.1801,  0.2892, -0.0436,  0.2976,  0.0036,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13647412606272646, distance: 1.2199343769067517 entropy -3.364541563998618
epoch: 1, step: 36
	action: tensor([[ 0.1772,  0.0509, -0.6955,  0.1015,  0.4686,  0.3826,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[-26.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5309403870658238, distance: 0.7837376560203441 entropy -2.793878066527931
epoch: 1, step: 37
	action: tensor([[ 0.0312,  0.1626, -0.2472,  0.5258, -0.0212, -0.1877, -0.0723]],
       dtype=torch.float64)
	q_value: tensor([[-27.7158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7837376560203441 entropy -3.178338866104587
epoch: 1, step: 38
	action: tensor([[ 0.1314,  0.4441, -0.2954, -0.2856, -0.0206,  0.0699,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6182061967576449, distance: 0.7070841966975504 entropy -3.364541563998618
epoch: 1, step: 39
	action: tensor([[-0.0056, -0.0388, -0.0937,  0.0943, -0.2624,  0.0013,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7070841966975504 entropy -3.364541563998618
epoch: 1, step: 40
	action: tensor([[ 0.1084, -0.1800,  0.1532, -0.0226,  0.0245,  0.1279,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28396873054855976, distance: 0.9683284404583831 entropy -3.364541563998618
epoch: 1, step: 41
	action: tensor([[-0.2616, -0.5118, -0.0206, -0.3833, -0.0153,  0.3075,  0.0353]],
       dtype=torch.float64)
	q_value: tensor([[-28.8061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5068939705567908, distance: 1.4047467657266508 entropy -2.6115786626807873
epoch: 1, step: 42
	action: tensor([[-0.1848,  0.3601, -0.4446, -0.6571,  0.0558,  0.4629,  0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-30.7195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.331159610543455, distance: 0.9358751997149843 entropy -2.6400969984088745
epoch: 1, step: 43
	action: tensor([[ 0.2300,  0.2180, -0.0273, -0.2265, -0.1858,  0.2893, -0.0661]],
       dtype=torch.float64)
	q_value: tensor([[-25.2280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6079974260406374, distance: 0.7164751839057628 entropy -3.1999911131573513
epoch: 1, step: 44
	action: tensor([[ 1.0406, -0.2525, -0.5277, -0.2074, -0.6391,  0.0557,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[-26.9806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38988554819996457, distance: 0.8938452763578124 entropy -2.656012900492519
epoch: 1, step: 45
	action: tensor([[-0.5548,  0.4997,  0.8872, -0.7676,  0.1347,  0.0636, -0.1354]],
       dtype=torch.float64)
	q_value: tensor([[-36.7986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6530976148082321, distance: 1.471315949573878 entropy -2.3859516212865866
epoch: 1, step: 46
	action: tensor([[ 0.4677, -0.7537, -0.1231,  0.4118, -0.1389,  0.4795, -0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-30.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3302812131280235, distance: 0.936489547052201 entropy -2.468210925558972
epoch: 1, step: 47
	action: tensor([[ 0.1340,  0.3704,  0.9489,  0.7689, -0.2404,  0.4195,  0.0689]],
       dtype=torch.float64)
	q_value: tensor([[-41.1905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.936489547052201 entropy -2.244325740247533
epoch: 1, step: 48
	action: tensor([[ 0.1598,  0.0285,  0.2676, -0.3656,  0.1696, -0.1619,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23029269943106645, distance: 1.0039671345803283 entropy -3.364541563998618
epoch: 1, step: 49
	action: tensor([[-0.3086, -0.7574, -0.0627, -0.2271,  0.5343, -0.0896, -0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-26.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.740727051337335, distance: 1.5098090785601868 entropy -2.745022822036247
epoch: 1, step: 50
	action: tensor([[ 0.0985,  0.0751,  0.4514, -0.4410, -0.3078,  0.0377, -0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-30.2651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20246364100792735, distance: 1.021955390839311 entropy -2.9286773469189677
epoch: 1, step: 51
	action: tensor([[ 0.1091, -0.2045, -0.4446, -0.4947, -0.9345,  0.0845,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[-28.5468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10425792905831388, distance: 1.0830491840985887 entropy -2.4768050239837116
epoch: 1, step: 52
	action: tensor([[-0.2231, -0.2499, -0.5994, -0.2214,  0.2759,  0.0708,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[-30.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11247354449170821, distance: 1.2069840727440788 entropy -2.6438300710505183
epoch: 1, step: 53
	action: tensor([[ 0.0301, -0.1788,  0.0885,  0.0837,  0.1083, -0.0770, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-25.1906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23854792958679916, distance: 0.9985687693834387 entropy -3.5072154198581336
epoch: 1, step: 54
	action: tensor([[-0.6483, -0.5734,  0.1229, -0.9161,  0.0435,  0.2288,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-26.6776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9983771242748904, distance: 1.6176904366983034 entropy -2.8416709098471142
epoch: 1, step: 55
	action: tensor([[-0.7190, -0.1589, -0.0518,  0.5958, -0.4644,  0.0277, -0.0482]],
       dtype=torch.float64)
	q_value: tensor([[-32.3286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3536041118286677, distance: 1.3313815237790307 entropy -2.654159573051511
epoch: 1, step: 56
	action: tensor([[-0.0895, -0.3566,  0.4097,  0.0374, -1.1589,  0.4547,  0.2348]],
       dtype=torch.float64)
	q_value: tensor([[-27.3803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008250026225080198, distance: 1.149054993079705 entropy -2.742092496539624
epoch: 1, step: 57
	action: tensor([[-1.0075,  0.3135, -0.1034,  1.0641,  0.0850,  0.8217,  0.3273]],
       dtype=torch.float64)
	q_value: tensor([[-38.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2316299865683672, distance: 1.0030946107030985 entropy -1.9848758172632224
epoch: 1, step: 58
	action: tensor([[-0.4225, -0.5219, -0.1355,  0.8101,  0.9399,  0.0875,  0.1479]],
       dtype=torch.float64)
	q_value: tensor([[-35.4098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10537623053090339, distance: 1.2031277781313403 entropy -2.5862607053700217
epoch: 1, step: 59
	action: tensor([[-0.2194,  0.2296, -0.0079,  0.7403,  0.3578,  0.2294, -0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-34.3299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2031277781313403 entropy -2.9695251830571303
epoch: 1, step: 60
	action: tensor([[ 0.2775,  0.1997,  0.0853, -0.2046, -0.0330, -0.0512,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-32.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5846742890355685, distance: 0.7374814516248412 entropy -3.364541563998618
epoch: 1, step: 61
	action: tensor([[ 0.2318, -0.2608, -0.5986, -0.0167, -0.0501, -0.1209, -0.0341]],
       dtype=torch.float64)
	q_value: tensor([[-25.6291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2717131641442051, distance: 0.9765802190000875 entropy -2.764970654298016
epoch: 1, step: 62
	action: tensor([[-0.0108, -0.4880,  0.1135, -0.1414,  0.0674, -0.0389, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-25.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2419190840631087, distance: 1.2752735248025626 entropy -3.174981873113357
epoch: 1, step: 63
	action: tensor([[ 1.1004, -0.5666,  0.5684, -0.3682, -0.5544,  0.0862,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-29.0854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -9.38550540865446e-05, distance: 1.1443979539859521 entropy -2.6938375877023697
LOSS epoch 1 actor 403.91291075650474 critic 279.57545001737947
epoch: 2, step: 0
	action: tensor([[ 0.3135,  0.1153, -1.3213,  0.9203,  0.1370,  0.0253, -0.1483]],
       dtype=torch.float64)
	q_value: tensor([[-44.8044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1443979539859521 entropy -1.9755401519083862
epoch: 2, step: 1
	action: tensor([[ 0.1898,  0.0020, -0.0085, -0.1462,  0.0462, -0.1266,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35889325887576085, distance: 0.9162666193174727 entropy -3.3627630285575227
epoch: 2, step: 2
	action: tensor([[-0.2581, -0.4760,  0.4338, -0.2129, -0.4988,  0.4465, -0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-26.0873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40640427125643597, distance: 1.3570997847536572 entropy -2.8897238025591983
epoch: 2, step: 3
	action: tensor([[ 0.9261, -1.8471, -0.6969, -2.2517, -0.1580,  0.5139,  0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-36.1031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3570997847536572 entropy -2.1857427327355965
epoch: 2, step: 4
	action: tensor([[ 0.0410,  0.0143, -0.0177,  0.1080,  0.0328,  0.1645,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4280939788699889, distance: 0.865404258036004 entropy -3.3627630285575227
epoch: 2, step: 5
	action: tensor([[ 0.1850, -0.6202,  0.6783, -0.3398,  0.5074,  0.0572,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-27.6945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36108155487354576, distance: 1.3350538008929438 entropy -2.77163794552654
epoch: 2, step: 6
	action: tensor([[-0.3463, -0.9623,  1.1609,  0.8545, -0.8136,  0.2900, -0.1740]],
       dtype=torch.float64)
	q_value: tensor([[-37.3498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11514937981453954, distance: 1.0764445742830637 entropy -2.288541269760309
epoch: 2, step: 7
	action: tensor([[-1.5099, -0.3548, -0.8585,  1.0084,  0.7175, -0.5252,  0.4792]],
       dtype=torch.float64)
	q_value: tensor([[-49.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6393527504488632, distance: 1.8591113921754188 entropy -1.7930318780730903
epoch: 2, step: 8
	action: tensor([[0.0885, 0.0683, 0.1314, 0.2590, 0.3513, 0.0088, 0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-33.5109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8591113921754188 entropy -3.472037832696084
epoch: 2, step: 9
	action: tensor([[ 0.3639, -0.2221,  0.1913,  0.1549,  0.0806,  0.0034,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5072585654154834, distance: 0.8032786701211873 entropy -3.3627630285575227
epoch: 2, step: 10
	action: tensor([[ 1.0185, -0.3797,  0.0783, -0.2642, -0.1861,  0.5097,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-31.6960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3917474319599835, distance: 0.8924803622620254 entropy -2.5714573853340683
epoch: 2, step: 11
	action: tensor([[-0.8673, -1.1049, -0.3503,  0.5854, -1.1045,  0.1120, -0.1494]],
       dtype=torch.float64)
	q_value: tensor([[-42.5580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2839396639093996, distance: 1.7294141444237532 entropy -2.1012810057616664
epoch: 2, step: 12
	action: tensor([[-0.2844,  0.2884, -0.4017,  0.1018,  0.5190, -0.2721,  0.3572]],
       dtype=torch.float64)
	q_value: tensor([[-42.5058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7294141444237532 entropy -2.3381127077958124
epoch: 2, step: 13
	action: tensor([[-0.1937,  0.0490,  0.0520,  0.0787,  0.0197, -0.0721,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0976324085023117, distance: 1.087047290528036 entropy -3.3627630285575227
epoch: 2, step: 14
	action: tensor([[ 0.5214, -0.0842, -0.2734, -0.1435, -0.0300,  0.2222,  0.0673]],
       dtype=torch.float64)
	q_value: tensor([[-24.6024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5907446724105763, distance: 0.7320721140420551 entropy -2.9587052750199545
epoch: 2, step: 15
	action: tensor([[-0.3888,  0.2416,  0.2519, -0.6005,  0.5608,  0.2339, -0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-31.2480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2701625481835963, distance: 1.2896930063864953 entropy -2.6404177920997163
epoch: 2, step: 16
	action: tensor([[ 0.1295, -0.1814,  0.3672,  0.3039,  0.4438,  0.2296, -0.1517]],
       dtype=torch.float64)
	q_value: tensor([[-29.3010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5354891050193271, distance: 0.779928239758993 entropy -2.9290700092310877
epoch: 2, step: 17
	action: tensor([[-0.2271, -0.0799, -0.1378, -0.0652, -0.2420,  0.0954,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[-34.7190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07626305810057787, distance: 1.187178191825147 entropy -2.5445593832983517
epoch: 2, step: 18
	action: tensor([[-0.5861, -0.0843, -0.5394,  0.1329, -0.1432,  0.2616,  0.1059]],
       dtype=torch.float64)
	q_value: tensor([[-26.2173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5281847916464644, distance: 1.4146357524658542 entropy -2.9267005672101605
epoch: 2, step: 19
	action: tensor([[ 0.0195, -0.1473, -0.0527,  0.1087, -0.2816,  0.1882,  0.0760]],
       dtype=torch.float64)
	q_value: tensor([[-27.3957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2497119010073625, distance: 0.9912215197555807 entropy -3.3242713308840885
epoch: 2, step: 20
	action: tensor([[ 0.2328,  0.1526, -0.3470, -0.0482, -1.0142,  0.2180,  0.1243]],
       dtype=torch.float64)
	q_value: tensor([[-29.4525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5350408481134592, distance: 0.7803044676583668 entropy -2.625552558548493
epoch: 2, step: 21
	action: tensor([[-1.1214, -0.3097, -0.1870,  0.0124,  0.3089,  0.1436,  0.1121]],
       dtype=torch.float64)
	q_value: tensor([[-30.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3482604658185116, distance: 1.7535971321958015 entropy -2.4715362831259067
epoch: 2, step: 22
	action: tensor([[-0.1152, -0.0079, -0.0289,  0.5203, -0.1231,  0.2693,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[-30.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7535971321958015 entropy -3.3916203343688567
epoch: 2, step: 23
	action: tensor([[-0.2961,  0.0571,  0.2063,  0.2717, -0.0180, -0.2310,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052557546290050605, distance: 1.1138664262000746 entropy -3.3627630285575227
epoch: 2, step: 24
	action: tensor([[ 0.2156, -0.2848,  0.0451, -0.2459,  0.0133,  0.5122,  0.1048]],
       dtype=torch.float64)
	q_value: tensor([[-24.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24745487269612065, distance: 0.9927113041833715 entropy -2.9348457493883013
epoch: 2, step: 25
	action: tensor([[ 0.2568,  0.7910,  1.0806, -0.8255,  0.6329,  0.3158, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[-34.2823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9927113041833715 entropy -2.4148607237004716
epoch: 2, step: 26
	action: tensor([[-0.3141,  0.0040, -0.1132,  0.1705, -0.1991, -0.1018,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05146452322609574, distance: 1.1734214026617555 entropy -3.3627630285575227
epoch: 2, step: 27
	action: tensor([[-0.4554, -0.2393, -0.1114,  0.1710,  0.0107,  0.1556,  0.1121]],
       dtype=torch.float64)
	q_value: tensor([[-24.3141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36552805460677296, distance: 1.3372327577426637 entropy -3.0440216255710006
epoch: 2, step: 28
	action: tensor([[ 0.0958,  0.1938, -0.1234,  0.2923,  0.6130,  0.4859,  0.1103]],
       dtype=torch.float64)
	q_value: tensor([[-27.6956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3372327577426637 entropy -2.944825609428289
epoch: 2, step: 29
	action: tensor([[-0.0090, -0.0529, -0.2459,  0.4309,  0.2893,  0.0185,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3646968690325719, distance: 0.9121099452443563 entropy -3.3627630285575227
epoch: 2, step: 30
	action: tensor([[-0.3206,  0.1092,  0.0986,  0.2891, -0.1416,  0.2069,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[-27.6488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9121099452443563 entropy -3.0675291727741434
epoch: 2, step: 31
	action: tensor([[ 0.1472,  0.0245,  0.0425, -0.1162, -0.2306, -0.1264,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33176645266797256, distance: 0.9354505413189196 entropy -3.3627630285575227
epoch: 2, step: 32
	action: tensor([[ 0.0501, -0.0828, -0.1654,  0.6084,  0.1010,  0.6471,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-26.2180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9354505413189196 entropy -2.785801655107964
epoch: 2, step: 33
	action: tensor([[-0.3954,  0.1000,  0.3967, -0.1253, -0.1393, -0.0243,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23717251091617486, distance: 1.2728341654326012 entropy -3.3627630285575227
epoch: 2, step: 34
	action: tensor([[ 0.0955, -0.3265,  0.3878,  0.3324,  0.0944,  0.2620,  0.1201]],
       dtype=torch.float64)
	q_value: tensor([[-26.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4326842731647361, distance: 0.8619242602044258 entropy -2.701430840182713
epoch: 2, step: 35
	action: tensor([[-0.8971,  0.3477,  0.3768,  0.6212,  0.0159,  0.8762,  0.0916]],
       dtype=torch.float64)
	q_value: tensor([[-34.9046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8619242602044258 entropy -2.396883703859285
epoch: 2, step: 36
	action: tensor([[-0.1091, -0.2748, -0.0047,  0.2608,  0.1170, -0.2248,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012316874622376961, distance: 1.1372750465706276 entropy -3.3627630285575227
epoch: 2, step: 37
	action: tensor([[ 0.0978, -0.2748, -0.0035,  0.4672,  0.0969,  0.1875,  0.0693]],
       dtype=torch.float64)
	q_value: tensor([[-26.7637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45879573548389796, distance: 0.8418550051577329 entropy -2.956078757816967
epoch: 2, step: 38
	action: tensor([[-0.4481, -0.2722,  0.1151,  0.0301, -0.7586, -0.1032,  0.0904]],
       dtype=torch.float64)
	q_value: tensor([[-32.6394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.451966152125606, distance: 1.3789068829541284 entropy -2.6345350924688655
epoch: 2, step: 39
	action: tensor([[ 1.3044,  0.1822,  0.3255,  0.4068, -0.7964,  0.4996,  0.2377]],
       dtype=torch.float64)
	q_value: tensor([[-29.3647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8758551608940515, distance: 0.4032004686734569 entropy -2.5748362726549248
epoch: 2, step: 40
	action: tensor([[ 1.1330, -1.7608,  0.4708, -1.8948,  0.0123,  0.1066, -0.0472]],
       dtype=torch.float64)
	q_value: tensor([[-46.8111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4032004686734569 entropy -1.9242632522441565
epoch: 2, step: 41
	action: tensor([[ 0.3171, -0.0213,  0.1542, -0.1534, -0.1729, -0.0120,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45112008030812734, distance: 0.8478038114003055 entropy -3.3627630285575227
epoch: 2, step: 42
	action: tensor([[-0.2242,  0.2247,  0.0078,  0.3206,  0.1904, -0.0307,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-28.7434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8478038114003055 entropy -2.59529177669837
epoch: 2, step: 43
	action: tensor([[ 0.0684, -0.1983, -0.0604,  0.1822,  0.0533, -0.1159,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2694290226879642, distance: 0.9781104547427386 entropy -3.3627630285575227
epoch: 2, step: 44
	action: tensor([[ 0.0582, -0.3252,  0.0338,  0.0437,  0.1024,  0.1969,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-27.2176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17615977754751866, distance: 1.0386714376324724 entropy -2.880325127506714
epoch: 2, step: 45
	action: tensor([[-0.3481,  0.0847,  0.5946, -0.0246, -0.3629,  0.3911,  0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-30.9501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06756057174703534, distance: 1.10501203474575 entropy -2.650347964329009
epoch: 2, step: 46
	action: tensor([[-0.7112, -0.0765, -0.8904, -0.2165,  0.2463,  0.2092,  0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-31.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5779662565661423, distance: 1.437492373494604 entropy -2.2841772585501277
epoch: 2, step: 47
	action: tensor([[-0.0798,  0.1046, -0.0832, -0.0470, -0.0633,  0.0804,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-28.5669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28395131853798494, distance: 0.968340213996261 entropy -3.745565353283552
epoch: 2, step: 48
	action: tensor([[ 0.3356,  0.2770, -0.0498, -0.3156, -0.0929, -0.0047,  0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-25.1801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6466702594186795, distance: 0.6802159175781579 entropy -2.9603988031442436
epoch: 2, step: 49
	action: tensor([[-0.1061,  0.0138, -0.2142, -0.1925, -0.3183,  0.1782, -0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-26.4493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1422128735374112, distance: 1.0598550057102616 entropy -2.8131738391950134
epoch: 2, step: 50
	action: tensor([[-0.4031,  0.0614, -0.0631,  0.1509, -0.1930,  0.2024,  0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-26.6379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03356008095540086, distance: 1.1633879385023906 entropy -2.8970676719973523
epoch: 2, step: 51
	action: tensor([[ 0.1075,  0.3874, -0.1126,  0.0041, -0.3451,  0.4226,  0.1342]],
       dtype=torch.float64)
	q_value: tensor([[-26.2182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1633879385023906 entropy -2.868678515135342
epoch: 2, step: 52
	action: tensor([[-0.2463, -0.0846, -0.1183,  0.2575,  0.3133,  0.2658,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10815656389510186, distance: 1.0806896780208517 entropy -3.3627630285575227
epoch: 2, step: 53
	action: tensor([[-0.4037,  0.0261, -0.1942, -0.0712, -0.7720,  0.0434,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[-28.3737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2376939902216615, distance: 1.2731023926768308 entropy -2.9584678477478734
epoch: 2, step: 54
	action: tensor([[-0.7621, -0.0468,  0.2949,  0.9046,  0.2923,  0.1032,  0.1721]],
       dtype=torch.float64)
	q_value: tensor([[-27.4293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029248300047954534, distance: 1.1274850009952653 entropy -2.821233981388123
epoch: 2, step: 55
	action: tensor([[-0.5236,  0.1291,  1.0181, -0.1242, -0.3137, -0.0277,  0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-31.8008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41480495733330636, distance: 1.3611468414151213 entropy -2.7017435435347488
epoch: 2, step: 56
	action: tensor([[ 0.8572, -0.9369, -0.5234,  0.0444,  0.3535,  0.1474,  0.2042]],
       dtype=torch.float64)
	q_value: tensor([[-30.7257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17780692762147865, distance: 1.2419203557996272 entropy -2.2341971566785053
epoch: 2, step: 57
	action: tensor([[ 0.0827,  1.0844,  0.1504,  0.3298,  0.4390,  0.4147, -0.2138]],
       dtype=torch.float64)
	q_value: tensor([[-41.5375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2419203557996272 entropy -2.461269430379708
epoch: 2, step: 58
	action: tensor([[-0.1275, -0.0772, -0.3292,  0.1303, -0.1671, -0.0887,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-36.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6322379884897402, distance: 0.6939690889277559 entropy -3.3627630285575227
epoch: 2, step: 59
	action: tensor([[-0.4643,  0.1347,  0.0238,  0.1941,  0.0191, -0.0222,  0.0771]],
       dtype=torch.float64)
	q_value: tensor([[-25.0306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5531438530726656, distance: 0.7649632323942089 entropy -3.1162674616249086
epoch: 2, step: 60
	action: tensor([[-0.2967, -0.1151, -0.1643, -0.2686, -0.2121,  0.2040,  0.0881]],
       dtype=torch.float64)
	q_value: tensor([[-24.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3572554828190583, distance: 0.9174362235896197 entropy -3.082067816383546
epoch: 2, step: 61
	action: tensor([[-0.0967, -0.3197,  0.1185, -0.3605,  0.3918,  0.1692,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-27.3877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19630782019673, distance: 1.0258918142532811 entropy -2.8975932027940874
epoch: 2, step: 62
	action: tensor([[-0.1880,  0.1724, -0.3673, -0.6859, -0.5771,  0.1195, -0.0962]],
       dtype=torch.float64)
	q_value: tensor([[-29.6714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5233529191245601, distance: 0.7900510636029027 entropy -2.7357070267512564
epoch: 2, step: 63
	action: tensor([[-0.4062, -0.1270,  0.2003,  0.1400,  0.0094,  0.1659,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[-27.1534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47992972655625943, distance: 0.8252541341973146 entropy -3.0928123206118654
LOSS epoch 2 actor 429.29130773401425 critic 231.25172457976964
epoch: 3, step: 0
	action: tensor([[-0.5518, -0.4209, -0.4305, -0.4325,  0.0826,  0.3680,  0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-30.6867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11062640701394022, distance: 1.2059816256565508 entropy -2.732225369942855
epoch: 3, step: 1
	action: tensor([[ 0.3829, -0.0284,  0.0667, -0.1111,  0.0867,  0.2263, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[-33.4215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7420696029436206, distance: 0.5811763814710279 entropy -3.1259849676913327
epoch: 3, step: 2
	action: tensor([[ 0.3266,  0.2610,  0.0417, -0.7055,  0.2011,  0.7353, -0.0428]],
       dtype=torch.float64)
	q_value: tensor([[-33.2680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8104866264996117, distance: 0.4981689147736665 entropy -2.5823790756111293
epoch: 3, step: 3
	action: tensor([[-0.5603,  0.1421, -0.6957,  0.2799, -0.6961,  0.9545, -0.1862]],
       dtype=torch.float64)
	q_value: tensor([[-35.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4981689147736665 entropy -2.434146597736138
epoch: 3, step: 4
	action: tensor([[ 0.2062,  0.0219,  0.0004,  0.1223,  0.1417, -0.0162,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5435223159936413, distance: 0.7731548217788686 entropy -3.3606126923765856
epoch: 3, step: 5
	action: tensor([[ 0.2474,  0.1931,  0.3691, -0.4254, -0.0537,  0.2278,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-29.7081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48342797887082223, distance: 0.8224739153581497 entropy -2.848299489676795
epoch: 3, step: 6
	action: tensor([[-0.2130, -0.5619, -0.7564,  1.1324,  0.7599,  0.0208, -0.0484]],
       dtype=torch.float64)
	q_value: tensor([[-32.3652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24044903154516528, distance: 1.2745185343275762 entropy -2.4696640917042205
epoch: 3, step: 7
	action: tensor([[ 0.1480,  0.0649, -0.0353,  0.1013, -0.0368,  0.0277,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[-41.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5348106822802565, distance: 0.7804975783931751 entropy -3.1033544688546857
epoch: 3, step: 8
	action: tensor([[-0.1165, -0.4645, -0.2116, -0.6740, -0.2122,  0.2197,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-29.3379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4130430218127106, distance: 1.3602990213353843 entropy -2.826798801280277
epoch: 3, step: 9
	action: tensor([[ 0.6352,  0.2396, -0.1693, -0.4274, -0.1041, -0.0313, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[-34.0394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7149136066966107, distance: 0.6110052446652184 entropy -2.715079867438663
epoch: 3, step: 10
	action: tensor([[-6.2418e-01,  5.3121e-05,  4.9178e-02, -9.3707e-02,  6.3708e-01,
          3.1389e-01, -1.2286e-01]], dtype=torch.float64)
	q_value: tensor([[-30.0048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36786381704024174, distance: 1.338375950475663 entropy -2.7157221893139325
epoch: 3, step: 11
	action: tensor([[ 0.0409, -0.0785, -0.2814, -0.0121,  0.1201,  0.1241, -0.0559]],
       dtype=torch.float64)
	q_value: tensor([[-32.1276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038101125965158, distance: 0.954817892137617 entropy -3.144713717273515
epoch: 3, step: 12
	action: tensor([[ 0.1221,  0.2403, -0.5103,  0.4059, -0.2473,  0.2844,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-29.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.954817892137617 entropy -3.0607434454289666
epoch: 3, step: 13
	action: tensor([[-0.2420, -0.1869,  0.2900,  0.6254, -0.1661, -0.1871,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2195339472387534, distance: 1.010959370631035 entropy -3.3606126923765856
epoch: 3, step: 14
	action: tensor([[-0.1477,  0.4052,  0.1973, -0.2442,  1.1337,  0.3008,  0.1872]],
       dtype=torch.float64)
	q_value: tensor([[-32.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.010959370631035 entropy -2.63894168135891
epoch: 3, step: 15
	action: tensor([[-0.3755,  0.1996,  0.0533, -0.2710, -0.0164, -0.0905,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1976799829429312, distance: 1.252353946976846 entropy -3.3606126923765856
epoch: 3, step: 16
	action: tensor([[ 0.2132, -0.3105,  0.0384,  0.1446,  0.0381, -0.1893,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[-25.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22682148289244775, distance: 1.0062284272924042 entropy -3.169392479976089
epoch: 3, step: 17
	action: tensor([[ 0.2204, -0.2014,  0.6480, -0.3457,  0.6968, -0.1353,  0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-31.4667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007118118420995412, distance: 1.1402641914841543 entropy -2.7704880188756857
epoch: 3, step: 18
	action: tensor([[ 0.8252, -0.1419, -0.3659, -0.3610, -0.0281, -0.0415, -0.2305]],
       dtype=torch.float64)
	q_value: tensor([[-36.0507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4301233102911963, distance: 0.8638675084121367 entropy -2.470761313927873
epoch: 3, step: 19
	action: tensor([[-0.6509, -0.5442, -0.1090, -0.1697, -0.8138,  0.1211, -0.1538]],
       dtype=torch.float64)
	q_value: tensor([[-33.3532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0105661391183491, distance: 1.6226164530399068 entropy -2.651498563591297
epoch: 3, step: 20
	action: tensor([[ 0.5786,  0.3817, -0.1353, -0.0338, -0.4929,  0.1651,  0.2477]],
       dtype=torch.float64)
	q_value: tensor([[-35.6528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8879467452148102, distance: 0.38306187598532465 entropy -2.5878898331621687
epoch: 3, step: 21
	action: tensor([[-8.0612e-01, -2.6766e-01, -4.7560e-01,  7.9866e-01,  2.1579e-01,
          1.0963e-01, -8.6173e-05]], dtype=torch.float64)
	q_value: tensor([[-32.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.38306187598532465 entropy -2.467199745362958
epoch: 3, step: 22
	action: tensor([[-0.3438,  0.0197,  0.0535,  0.0181, -0.3010, -0.0755,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13555098325361503, distance: 1.2194388080201868 entropy -3.3606126923765856
epoch: 3, step: 23
	action: tensor([[-0.1714, -0.1153, -0.0002,  0.0554, -0.0496,  0.0233,  0.1348]],
       dtype=torch.float64)
	q_value: tensor([[-27.5120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010808319633762609, distance: 1.1381432335917454 entropy -2.886361730426571
epoch: 3, step: 24
	action: tensor([[ 0.0754, -0.1021, -0.0943,  0.1603,  0.0120,  0.2683,  0.0860]],
       dtype=torch.float64)
	q_value: tensor([[-28.7845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39266235961787066, distance: 0.8918088794725029 entropy -2.8490057853027184
epoch: 3, step: 25
	action: tensor([[ 1.0101, -0.2080,  0.4073, -0.3568, -0.2192,  0.3512,  0.0568]],
       dtype=torch.float64)
	q_value: tensor([[-32.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4878153117850922, distance: 0.8189737631323301 entropy -2.7103820721735437
epoch: 3, step: 26
	action: tensor([[-1.0465,  0.0391,  0.9598, -0.1526,  0.6088, -0.3538, -0.1604]],
       dtype=torch.float64)
	q_value: tensor([[-43.3889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2976428847830292, distance: 1.734594470305756 entropy -2.0725350623299836
epoch: 3, step: 27
	action: tensor([[ 0.1267,  0.3503,  0.7036, -1.1298,  0.6472, -0.0624,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-34.4708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15160169851715888, distance: 1.2280267911147116 entropy -2.650823594307565
epoch: 3, step: 28
	action: tensor([[-0.3744,  0.0718,  0.0759, -0.0142, -0.6831,  0.2282, -0.3533]],
       dtype=torch.float64)
	q_value: tensor([[-36.5691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2311727946315092, distance: 1.2697440850795503 entropy -2.4247257669673283
epoch: 3, step: 29
	action: tensor([[-0.5117, -0.1609,  0.3800, -0.4948,  0.2511,  0.0234,  0.2203]],
       dtype=torch.float64)
	q_value: tensor([[-31.3830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8643188371991026, distance: 1.5624883472432698 entropy -2.587751986547577
epoch: 3, step: 30
	action: tensor([[-0.2382, -0.1339, -0.0770,  0.2673, -0.7529,  0.4084, -0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-31.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09526120603976329, distance: 1.197610364638331 entropy -2.7330204598460974
epoch: 3, step: 31
	action: tensor([[-1.6256, -0.3048, -1.1433, -0.6662, -0.5155,  0.4171,  0.2621]],
       dtype=torch.float64)
	q_value: tensor([[-35.2932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6442441943218724, distance: 1.8608333161639932 entropy -2.393335516473244
epoch: 3, step: 32
	action: tensor([[-0.1015, -0.1770,  0.0299,  0.0063,  0.0560, -0.0148,  0.0698]],
       dtype=torch.float64)
	q_value: tensor([[-43.5837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.042423326499586356, distance: 1.1683655789978742 entropy -3.4962679751751957
epoch: 3, step: 33
	action: tensor([[-0.1509, -0.0683,  0.3147, -0.6145, -0.1699,  0.1654,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[-29.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3494103583032684, distance: 1.329317472211273 entropy -2.8685651154918266
epoch: 3, step: 34
	action: tensor([[-0.5500, -0.0008, -0.6724, -0.3514, -0.5956,  0.1909,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[-31.9158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48010590251368, distance: 1.3922046774523174 entropy -2.5514251774215775
epoch: 3, step: 35
	action: tensor([[ 0.0109, -0.2888,  0.1765,  0.2594, -0.2193,  0.1138,  0.0684]],
       dtype=torch.float64)
	q_value: tensor([[-30.2575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19199419872546608, distance: 1.0286412418382143 entropy -3.38654828237969
epoch: 3, step: 36
	action: tensor([[ 0.0848,  0.3308,  0.1702,  0.1921, -0.5161,  0.2581,  0.1539]],
       dtype=torch.float64)
	q_value: tensor([[-33.9858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0286412418382143 entropy -2.513203470376961
epoch: 3, step: 37
	action: tensor([[-0.1896, -0.1462,  0.0715, -0.0584,  0.0561,  0.1225,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06191511260613969, distance: 1.1792383488700393 entropy -3.3606126923765856
epoch: 3, step: 38
	action: tensor([[-0.2593, -0.5657,  0.1406, -0.0099, -0.1515,  0.2757,  0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-29.7179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41652920810713145, distance: 1.36197601714237 entropy -2.8006722248859677
epoch: 3, step: 39
	action: tensor([[ 0.0289, -0.0347,  0.1475, -0.1324, -0.4240,  0.1740,  0.1621]],
       dtype=torch.float64)
	q_value: tensor([[-35.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.218648035424433, distance: 1.0115329809982212 entropy -2.4838003746611386
epoch: 3, step: 40
	action: tensor([[-0.9320,  0.1057,  0.0123, -0.8539,  0.0283,  0.2197,  0.1266]],
       dtype=torch.float64)
	q_value: tensor([[-31.9142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0472853071284445, distance: 1.637366415027714 entropy -2.4823708604734813
epoch: 3, step: 41
	action: tensor([[ 0.0044, -0.5771, -0.0058, -0.2680, -0.1786, -0.0399, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-32.3741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39774436729213747, distance: 1.352915176822574 entropy -3.148781468045932
epoch: 3, step: 42
	action: tensor([[ 0.2468, -0.4485,  0.7649, -0.4614,  1.3348, -0.3533,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[-32.8979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2402661624079636, distance: 1.2744245850038534 entropy -2.655499094662676
epoch: 3, step: 43
	action: tensor([[-0.5213, -0.4550, -0.2652, -0.1502,  0.0843,  0.2714, -0.4107]],
       dtype=torch.float64)
	q_value: tensor([[-42.2938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6970123226059914, distance: 1.4907306856898312 entropy -2.34248983269051
epoch: 3, step: 44
	action: tensor([[-0.5110,  0.3621,  0.2492,  0.0694, -0.1907,  0.2765,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-33.2993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004738761675881054, distance: 1.1416296468645186 entropy -3.0844865862526953
epoch: 3, step: 45
	action: tensor([[ 0.2603, -0.1014, -0.1507, -0.1989, -0.0589,  0.0834,  0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-28.7514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1416296468645186 entropy -2.6895331195197296
epoch: 3, step: 46
	action: tensor([[ 0.3973, -0.4141,  0.1629, -0.0291,  0.1372, -0.2310,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14722578938722863, distance: 1.0567535666646304 entropy -3.3606126923765856
epoch: 3, step: 47
	action: tensor([[ 0.4080, -0.4779, -0.3607,  0.1853,  0.0329,  0.3976, -0.0510]],
       dtype=torch.float64)
	q_value: tensor([[-33.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3960835316862391, distance: 0.8892935237341628 entropy -2.621777573261589
epoch: 3, step: 48
	action: tensor([[ 0.6119,  0.0599, -0.6451, -0.0091,  0.4935,  0.0479, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[-38.4077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7689162579907965, distance: 0.5500996758823343 entropy -2.5458339889244317
epoch: 3, step: 49
	action: tensor([[-0.0509,  0.3538,  0.6397,  0.2610,  0.1459,  0.2095, -0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-30.9940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5500996758823343 entropy -3.0214858300298153
epoch: 3, step: 50
	action: tensor([[-0.1541, -0.0479,  0.0378, -0.1860,  0.1300, -0.2738,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10500420226464224, distance: 1.2029252972038023 entropy -3.3606126923765856
epoch: 3, step: 51
	action: tensor([[-6.6178e-01, -8.4453e-02, -6.3611e-02,  1.7838e-01,  4.7858e-01,
          9.1655e-02,  1.9558e-04]], dtype=torch.float64)
	q_value: tensor([[-26.0662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.513718897116868, distance: 1.4079243159309949 entropy -3.122862205106684
epoch: 3, step: 52
	action: tensor([[-0.3256,  0.1710, -0.0812,  0.4093,  0.3213,  0.0924,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[-29.9989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4079243159309949 entropy -3.3040538122901597
epoch: 3, step: 53
	action: tensor([[0.2400, 0.1728, 0.1017, 0.3497, 0.2316, 0.0560, 0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7889996046347652, distance: 0.5256520080443219 entropy -3.3606126923765856
epoch: 3, step: 54
	action: tensor([[ 0.1380, -0.0015,  0.2032,  0.1034,  0.0867, -0.0531,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47465899234733866, distance: 0.8294254262743924 entropy -3.3606126923765856
epoch: 3, step: 55
	action: tensor([[ 0.7386, -0.6011,  0.4189,  0.0185,  0.2350,  0.0009,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-30.4118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15897382777550806, distance: 1.0494492675296687 entropy -2.707003875599132
epoch: 3, step: 56
	action: tensor([[-0.6287, -0.7862, -1.0274, -0.5670,  0.6891,  1.0079, -0.1286]],
       dtype=torch.float64)
	q_value: tensor([[-42.2262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.547097340510641, distance: 1.4233624782447396 entropy -2.264987701521828
epoch: 3, step: 57
	action: tensor([[-0.5851, -0.1143, -0.0033,  0.2055,  0.0376,  0.0153, -0.1322]],
       dtype=torch.float64)
	q_value: tensor([[-44.7656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39984767886098127, distance: 1.3539327206975504 entropy -2.9343090009698742
epoch: 3, step: 58
	action: tensor([[ 0.1384, -0.1747,  0.0095, -0.4055,  0.3291, -0.0117,  0.1115]],
       dtype=torch.float64)
	q_value: tensor([[-28.6041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05769384545363976, distance: 1.1108430625262213 entropy -3.110325860364455
epoch: 3, step: 59
	action: tensor([[-0.1949, -0.5183,  0.3265, -0.9070,  0.2399, -0.0122, -0.1160]],
       dtype=torch.float64)
	q_value: tensor([[-30.0678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8070120275454429, distance: 1.5382864511369074 entropy -2.8303715677791272
epoch: 3, step: 60
	action: tensor([[ 0.0941, -0.2345,  0.0353,  0.4127,  0.3710,  0.4449, -0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-34.7256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.575402315881359, distance: 0.7456679963270114 entropy -2.5567463916120636
epoch: 3, step: 61
	action: tensor([[-0.2906, -0.8354, -0.3430,  0.1414,  0.2291,  0.7803,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[-38.2173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39068337236595907, distance: 1.3494935846344123 entropy -2.6119585687902256
epoch: 3, step: 62
	action: tensor([[ 0.2405,  0.1455, -0.3066,  0.2171,  0.7274,  0.4569,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-41.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3494935846344123 entropy -2.5232398403669696
epoch: 3, step: 63
	action: tensor([[-0.0698, -0.2751, -0.0167, -0.2870,  0.0495, -0.1779,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-37.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3494935846344123 entropy -3.3606126923765856
LOSS epoch 3 actor 515.578367353798 critic 219.67246268227882
epoch: 4, step: 0
	action: tensor([[-0.1946, -0.2526,  0.3170, -0.0574,  0.3200,  0.0598,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17438794481689435, distance: 1.2401164989193405 entropy -3.3592666152301307
epoch: 4, step: 1
	action: tensor([[-0.3461,  0.0068,  0.3794,  0.0826,  0.1215,  0.1788, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[-32.8435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015102602860038727, distance: 1.1529531599861373 entropy -2.721215639879781
epoch: 4, step: 2
	action: tensor([[-0.4286,  0.2166,  0.1658, -0.2617, -0.3526,  0.2959,  0.1005]],
       dtype=torch.float64)
	q_value: tensor([[-32.3741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19727799861416706, distance: 1.2521437619036133 entropy -2.6712483860955385
epoch: 4, step: 3
	action: tensor([[0.6574, 0.4578, 0.1864, 0.0210, 0.4245, 0.0089, 0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-31.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2521437619036133 entropy -2.6503825497580875
epoch: 4, step: 4
	action: tensor([[-0.0758, -0.1694,  0.2440,  0.1916, -0.0009,  0.2188,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2611625847411092, distance: 0.9836285674356093 entropy -3.3592666152301307
epoch: 4, step: 5
	action: tensor([[ 0.3565, -0.1989, -0.2624,  0.3719, -0.3351,  0.2740,  0.1143]],
       dtype=torch.float64)
	q_value: tensor([[-34.8595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6038821900899514, distance: 0.7202261370265741 entropy -2.5709196107197676
epoch: 4, step: 6
	action: tensor([[-0.2951,  0.1120, -0.8844,  0.3729,  0.5630,  0.2266,  0.0846]],
       dtype=torch.float64)
	q_value: tensor([[-38.1517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7202261370265741 entropy -2.5079870323832125
epoch: 4, step: 7
	action: tensor([[ 0.0864, -0.0514,  0.0430,  0.2923, -0.1185,  0.1949,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5238301346725425, distance: 0.7896554679072308 entropy -3.3592666152301307
epoch: 4, step: 8
	action: tensor([[ 0.0067,  0.1508,  0.5710, -0.4868,  0.8179,  0.3685,  0.1087]],
       dtype=torch.float64)
	q_value: tensor([[-34.5088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2127354006249309, distance: 1.0153529964846002 entropy -2.611634693824483
epoch: 4, step: 9
	action: tensor([[-0.8018,  0.4060,  0.1536, -0.1557,  0.3877,  0.3847, -0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-37.0689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32375861805958084, distance: 1.3166219572422981 entropy -2.4616287208270085
epoch: 4, step: 10
	action: tensor([[ 0.0267, -0.2691, -0.0082,  0.3882, -0.3074,  0.1474,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-32.0024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34712077438378985, distance: 0.9246409389429165 entropy -3.1650925107298336
epoch: 4, step: 11
	action: tensor([[ 0.0360, -0.5195, -0.0685,  0.7223,  0.1794,  0.5534,  0.1721]],
       dtype=torch.float64)
	q_value: tensor([[-36.0205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4903252598820882, distance: 0.816964618573212 entropy -2.5542720627191042
epoch: 4, step: 12
	action: tensor([[-0.1440,  1.0325, -0.2458,  0.2015, -0.6420,  0.4312,  0.0931]],
       dtype=torch.float64)
	q_value: tensor([[-44.6832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.816964618573212 entropy -2.4430329741345393
epoch: 4, step: 13
	action: tensor([[ 0.0081, -0.0454,  0.0099, -0.3792, -0.0464,  0.0261,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07458565504848791, distance: 1.1008415341691005 entropy -3.3592666152301307
epoch: 4, step: 14
	action: tensor([[-0.0219, -0.2425, -0.3293, -0.3599,  0.7100, -0.0133, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-29.8626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04480496423370939, distance: 1.1696995073160292 entropy -2.8274277848838762
epoch: 4, step: 15
	action: tensor([[-0.0604,  0.0898, -0.0580,  0.1951,  0.4355,  0.1351, -0.1297]],
       dtype=torch.float64)
	q_value: tensor([[-30.4476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1696995073160292 entropy -3.1685617211321135
epoch: 4, step: 16
	action: tensor([[ 0.1340,  0.2773,  0.1713, -0.0186, -0.0786,  0.0591,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5728952340218616, distance: 0.7478661939403708 entropy -3.3592666152301307
epoch: 4, step: 17
	action: tensor([[ 0.0669,  0.3156, -0.1724, -0.2124,  0.2305, -0.0956,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4600161333642019, distance: 0.840905291738601 entropy -3.3592666152301307
epoch: 4, step: 18
	action: tensor([[-0.6682, -0.2945, -0.0418, -0.0051, -0.1014,  0.0166, -0.0571]],
       dtype=torch.float64)
	q_value: tensor([[-26.7135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.840905291738601 entropy -3.2200208232052625
epoch: 4, step: 19
	action: tensor([[-0.1582, -0.2253, -0.0173,  0.1648,  0.0356,  0.1648,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0449330591249516, distance: 1.1183393324949102 entropy -3.3592666152301307
epoch: 4, step: 20
	action: tensor([[ 0.0793, -0.2859,  0.2327, -0.7661, -0.5895,  0.3601,  0.0923]],
       dtype=torch.float64)
	q_value: tensor([[-32.9566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3057535634348747, distance: 1.3076373087562927 entropy -2.768545070943328
epoch: 4, step: 21
	action: tensor([[-0.5789, -0.1529,  0.2691,  0.6439, -1.0629, -0.3859,  0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-38.3102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04906151167032102, distance: 1.1720797700962227 entropy -2.2365001708868455
epoch: 4, step: 22
	action: tensor([[ 0.5583, -0.4653, -0.9484, -0.6412, -0.5678, -0.2990,  0.3108]],
       dtype=torch.float64)
	q_value: tensor([[-36.7164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1604311994197638, distance: 1.0485396045482773 entropy -2.404460256393284
epoch: 4, step: 23
	action: tensor([[-0.1342,  0.1871,  0.5928, -0.0815,  0.5686,  0.3281, -0.1302]],
       dtype=torch.float64)
	q_value: tensor([[-35.8759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3159446215906143, distance: 0.9464601307825353 entropy -2.884511152350129
epoch: 4, step: 24
	action: tensor([[ 0.7996, -0.2116, -0.4173,  0.3163,  0.1032,  0.0117, -0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-35.4772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.730079250431573, distance: 0.5945314441010677 entropy -2.552097626072558
epoch: 4, step: 25
	action: tensor([[-0.6315,  0.1080, -0.1241,  0.5230,  0.4243, -0.0701, -0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-38.3148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5945314441010677 entropy -2.6690266659071518
epoch: 4, step: 26
	action: tensor([[ 0.1447, -0.1886,  0.0457,  0.0655,  0.0658, -0.2652,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2554019327429491, distance: 0.9874557556707524 entropy -3.3592666152301307
epoch: 4, step: 27
	action: tensor([[ 0.1383, -0.3215,  0.0866, -0.2079,  0.1656,  0.0096,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[-30.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029710000240632972, distance: 1.127216846969384 entropy -2.8649025744666026
epoch: 4, step: 28
	action: tensor([[ 0.3134, -0.7686, -0.5688, -0.0738,  0.5572,  0.1698, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-32.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16003126155182268, distance: 1.2325130883175717 entropy -2.7127886098083853
epoch: 4, step: 29
	action: tensor([[-0.7636, -0.3361,  0.1952, -0.5046, -0.0369,  0.0107, -0.1429]],
       dtype=torch.float64)
	q_value: tensor([[-38.1221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0746477388875477, distance: 1.648271983593653 entropy -2.8272757597646168
epoch: 4, step: 30
	action: tensor([[-0.3912,  0.5238,  0.3084, -0.3397,  0.0217,  0.4988,  0.0849]],
       dtype=torch.float64)
	q_value: tensor([[-31.9983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.648271983593653 entropy -2.909868626149526
epoch: 4, step: 31
	action: tensor([[-0.1977, -0.1146,  0.4510,  0.1521, -0.1875,  0.0032,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06284700373398022, distance: 1.1078014832520253 entropy -3.3592666152301307
epoch: 4, step: 32
	action: tensor([[-0.1702,  0.0313, -0.4543, -0.7891, -0.1841,  0.6055,  0.1638]],
       dtype=torch.float64)
	q_value: tensor([[-33.2735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029651726335123252, distance: 1.1272506957887105 entropy -2.541692516575343
epoch: 4, step: 33
	action: tensor([[-0.4938, -0.6038, -0.4683,  0.0983,  0.7248,  0.1934, -0.0487]],
       dtype=torch.float64)
	q_value: tensor([[-34.3319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.706398653139038, distance: 1.494847684621742 entropy -2.816958250274895
epoch: 4, step: 34
	action: tensor([[-0.5680, -0.0182,  0.1562, -0.3277,  0.3054, -0.0125, -0.0354]],
       dtype=torch.float64)
	q_value: tensor([[-36.0629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6355701855294169, distance: 1.4634951445678712 entropy -3.2550143022099296
epoch: 4, step: 35
	action: tensor([[ 0.3441,  0.1258, -0.0609,  0.0086,  0.3144, -0.2190, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-29.3744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6042284489093783, distance: 0.71991128224319 entropy -3.151882655148268
epoch: 4, step: 36
	action: tensor([[-0.4370, -0.3763, -0.0603, -0.2630,  0.1235,  0.1311, -0.0654]],
       dtype=torch.float64)
	q_value: tensor([[-29.2322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6198194178450556, distance: 1.4564312663044299 entropy -2.9937386280230602
epoch: 4, step: 37
	action: tensor([[ 0.1797,  0.0890, -0.3310,  0.5259, -0.1635,  0.2098,  0.0392]],
       dtype=torch.float64)
	q_value: tensor([[-31.7462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4564312663044299 entropy -2.998065439986997
epoch: 4, step: 38
	action: tensor([[ 0.0573, -0.0713,  0.0952, -0.3453,  0.0234,  0.1628,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18004016485462093, distance: 1.036222416246434 entropy -3.3592666152301307
epoch: 4, step: 39
	action: tensor([[ 0.4234,  0.2706, -0.3581, -0.8894, -0.2953,  0.3194, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[-31.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5744724177164049, distance: 0.7464840818788924 entropy -2.6721816246381485
epoch: 4, step: 40
	action: tensor([[-0.2415, -0.1688, -0.8291, -0.8137,  0.3643,  0.1019, -0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-32.4291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06519229727738485, distance: 1.106414438015327 entropy -2.671411651848565
epoch: 4, step: 41
	action: tensor([[-0.2683, -0.0036, -0.0322, -0.1722, -0.0395, -0.0211, -0.0410]],
       dtype=torch.float64)
	q_value: tensor([[-30.8263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10231415073083627, distance: 1.201460188363451 entropy -3.462179039451437
epoch: 4, step: 42
	action: tensor([[ 0.0377, -0.3853, -0.0613, -0.3480, -0.4574, -0.0453,  0.0549]],
       dtype=torch.float64)
	q_value: tensor([[-27.8414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19104845343649068, distance: 1.2488820052260012 entropy -3.0949342004203153
epoch: 4, step: 43
	action: tensor([[ 0.2727,  0.1580, -0.2120, -0.5026,  0.4879,  0.0294,  0.0791]],
       dtype=torch.float64)
	q_value: tensor([[-33.2393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4735386198807573, distance: 0.8303093954235146 entropy -2.646107324135124
epoch: 4, step: 44
	action: tensor([[-0.1906, -0.3941,  0.3891, -0.3603, -0.2426, -0.0009, -0.1644]],
       dtype=torch.float64)
	q_value: tensor([[-29.5485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4961360297929529, distance: 1.399723435774046 entropy -2.992107126215717
epoch: 4, step: 45
	action: tensor([[0.2361, 0.1147, 0.2038, 0.2707, 1.1825, 0.2315, 0.1150]],
       dtype=torch.float64)
	q_value: tensor([[-34.0091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.399723435774046 entropy -2.501138900318954
epoch: 4, step: 46
	action: tensor([[0.1196, 0.0968, 0.0240, 0.0738, 0.0420, 0.0742, 0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5032920273865004, distance: 0.8065053612418338 entropy -3.3592666152301307
epoch: 4, step: 47
	action: tensor([[-0.1110, -0.4793, -0.4580, -0.6122,  0.0613,  0.3666,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[-30.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27646065049004065, distance: 1.2928865245677557 entropy -2.815564111660023
epoch: 4, step: 48
	action: tensor([[ 0.1852,  0.1399, -0.5520, -0.8069, -0.0196,  0.0428, -0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-35.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43270559612989523, distance: 0.8619080620339902 entropy -2.8830373210978157
epoch: 4, step: 49
	action: tensor([[ 0.2399, -0.2139,  0.2034,  0.1268,  0.4120, -0.1330, -0.0964]],
       dtype=torch.float64)
	q_value: tensor([[-28.4500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37180790498762506, distance: 0.906990893884482 entropy -3.2237404494518667
epoch: 4, step: 50
	action: tensor([[ 0.1778, -0.3200, -0.0589, -0.8917,  0.0204,  0.1590, -0.0371]],
       dtype=torch.float64)
	q_value: tensor([[-34.2788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22574855131303861, distance: 1.266943908002909 entropy -2.7507720462135548
epoch: 4, step: 51
	action: tensor([[-0.3522,  0.2477,  0.1432, -1.2934, -0.0586,  0.4442, -0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-34.1180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35078236068114466, distance: 1.3299930870702958 entropy -2.617521880441288
epoch: 4, step: 52
	action: tensor([[-0.4767,  0.2182, -0.0782, -0.2941,  0.1386, -0.0963, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-35.8930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24273368218246216, distance: 1.2756916942051468 entropy -2.656664950525765
epoch: 4, step: 53
	action: tensor([[ 0.0384, -0.2955,  0.1709,  0.0709, -0.0399,  0.2625,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[-26.0970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22343111462149035, distance: 1.0084321572306465 entropy -3.5449393484565297
epoch: 4, step: 54
	action: tensor([[-1.1771, -0.5535,  0.5612, -1.0678,  0.0493,  0.2987,  0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-36.0994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4980388781612652, distance: 1.8086573178714898 entropy -2.5149547037750777
epoch: 4, step: 55
	action: tensor([[ 0.2940, -0.3621,  0.8460, -1.3206,  0.7169,  0.0932, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-40.5616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4806617672295448, distance: 1.3924660792921904 entropy -2.4004447656566237
epoch: 4, step: 56
	action: tensor([[-0.1782, -0.1088,  0.1276, -0.0822, -0.0824,  0.2482, -0.4604]],
       dtype=torch.float64)
	q_value: tensor([[-43.8871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03979274836033897, distance: 1.1213448273145759 entropy -2.1153992456216977
epoch: 4, step: 57
	action: tensor([[ 0.0195,  0.0196,  0.2513, -0.3688,  0.4660,  0.3615,  0.1123]],
       dtype=torch.float64)
	q_value: tensor([[-32.9286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24860624224677652, distance: 0.9919516055014468 entropy -2.7158476050043956
epoch: 4, step: 58
	action: tensor([[ 0.6826, -0.4480, -0.6399, -1.0656, -0.8596, -0.0944, -0.1451]],
       dtype=torch.float64)
	q_value: tensor([[-33.9164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1368530845674023, distance: 1.2201377541610665 entropy -2.6157420398317384
epoch: 4, step: 59
	action: tensor([[-0.4967,  0.1146,  0.6600, -0.1670,  0.7040,  0.3456, -0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-39.6241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1996186051970148, distance: 1.2533670972362734 entropy -2.523571168011791
epoch: 4, step: 60
	action: tensor([[-0.2334, -0.5717,  0.5771, -0.0829,  0.2624,  0.8265, -0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-36.8434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07806355575545787, distance: 1.1881708015370156 entropy -2.5879562941395746
epoch: 4, step: 61
	action: tensor([[-1.1366,  1.1898, -0.9389,  0.4417, -0.2203,  1.1127,  0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-44.8368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1553788455463283, distance: 1.2300390495912146 entropy -2.160215286572719
epoch: 4, step: 62
	action: tensor([[0.1906, 0.0599, 0.3154, 0.2104, 0.3006, 0.1294, 0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-40.5427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2300390495912146 entropy -3.1090159721696105
epoch: 4, step: 63
	action: tensor([[-0.2427,  0.2741,  0.1027, -0.1001,  0.0569, -0.0431,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-37.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10864268446538206, distance: 1.0803951100121363 entropy -3.3592666152301307
LOSS epoch 4 actor 525.7337166947468 critic 210.63047328266663
epoch: 5, step: 0
	action: tensor([[ 0.4561, -0.3860, -0.2916, -0.0960,  0.0180,  0.0728,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-27.1575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2551113533263345, distance: 0.9876484141944993 entropy -3.020277862775425
epoch: 5, step: 1
	action: tensor([[-0.3154,  0.4807,  0.3407,  0.6653,  0.1214, -0.2030, -0.0520]],
       dtype=torch.float64)
	q_value: tensor([[-34.8915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48761073837664104, distance: 0.8191373013363185 entropy -2.684907584217158
epoch: 5, step: 2
	action: tensor([[-0.0576, -0.0408,  0.2132,  0.2362,  0.3153,  0.0354,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.338486791974804, distance: 0.9307348017442584 entropy -3.3592266971931273
epoch: 5, step: 3
	action: tensor([[-0.3304, -0.0460,  0.2032,  0.0750, -0.0424,  0.2198,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[-32.3622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024343071641325098, distance: 1.1581889321921628 entropy -2.813724553797286
epoch: 5, step: 4
	action: tensor([[ 0.3280, -0.3709,  0.4421,  0.1086,  0.0514,  0.6585,  0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-31.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5654750821882688, distance: 0.7543346135862343 entropy -2.692548907044445
epoch: 5, step: 5
	action: tensor([[0.2906, 0.3874, 0.6125, 0.1487, 0.3397, 1.5437, 0.0300]],
       dtype=torch.float64)
	q_value: tensor([[-44.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7543346135862343 entropy -2.1577477036168813
epoch: 5, step: 6
	action: tensor([[-0.1380, -0.0183,  0.1089,  0.1930, -0.0780, -0.1995,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16137602021663788, distance: 1.0479494439202637 entropy -3.3592266971931273
epoch: 5, step: 7
	action: tensor([[-0.1696,  0.1236, -0.1134, -0.0068,  0.6043,  0.1211,  0.0986]],
       dtype=torch.float64)
	q_value: tensor([[-28.9400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2049167537453238, distance: 1.0203824829059638 entropy -2.8966504402556965
epoch: 5, step: 8
	action: tensor([[ 0.0757,  0.0573,  0.2838,  0.0650,  0.2242,  0.1826, -0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-30.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48246330103092394, distance: 0.8232415259253557 entropy -3.1924514968679203
epoch: 5, step: 9
	action: tensor([[-0.4490,  0.6685, -0.1520,  0.1761, -0.1108,  0.2669,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[-33.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8232415259253557 entropy -2.6635031941123506
epoch: 5, step: 10
	action: tensor([[ 0.0875, -0.0467,  0.0935, -0.0095,  0.0204,  0.1990,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40313829614801344, distance: 0.8840840398906363 entropy -3.3592266971931273
epoch: 5, step: 11
	action: tensor([[-1.2631, -0.3210, -0.4169,  0.1613, -0.1727,  0.4174,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-32.6793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4394643693260378, distance: 1.7873266578208042 entropy -2.639332440839881
epoch: 5, step: 12
	action: tensor([[-0.2113, -0.1989,  0.3045,  0.2930, -0.2041, -0.0280,  0.1243]],
       dtype=torch.float64)
	q_value: tensor([[-36.9884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10558840946038528, distance: 1.0822445377373706 entropy -3.1610533129452163
epoch: 5, step: 13
	action: tensor([[-0.6288, -0.5790,  0.3105, -0.1837, -0.9770,  0.2442,  0.1783]],
       dtype=torch.float64)
	q_value: tensor([[-33.4473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.990747302490135, distance: 1.614599305086082 entropy -2.5784827570525217
epoch: 5, step: 14
	action: tensor([[ 1.3896,  0.2287, -0.4227, -1.6821, -0.6550,  0.9970,  0.3511]],
       dtype=torch.float64)
	q_value: tensor([[-39.9832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2039648489318867, distance: 1.020993121133975 entropy -2.1851282156583682
epoch: 5, step: 15
	action: tensor([[ 0.1862,  0.4355, -0.6434, -1.6808,  0.6397,  1.9639, -0.3945]],
       dtype=torch.float64)
	q_value: tensor([[-49.0473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8736170774443559, distance: 0.40681868371405544 entropy -1.8911622166850248
epoch: 5, step: 16
	action: tensor([[ 0.7734, -0.5895, -0.3953, -0.1502,  0.3476, -0.5623, -0.4617]],
       dtype=torch.float64)
	q_value: tensor([[-47.8867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09995522053739836, distance: 1.2001739494999626 entropy -2.1769900439630896
epoch: 5, step: 17
	action: tensor([[-0.3166, -0.4503, -0.1938, -0.0156, -0.5221, -0.0911, -0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-34.3363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4251886499924413, distance: 1.3661326495875645 entropy -2.7509922830941167
epoch: 5, step: 18
	action: tensor([[ 0.3911, -0.0614, -0.0291, -0.7161,  0.3066, -0.0841,  0.1709]],
       dtype=torch.float64)
	q_value: tensor([[-31.5844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16060714386257335, distance: 1.0484297300610599 entropy -2.839156675285453
epoch: 5, step: 19
	action: tensor([[-0.0204, -0.6457,  0.0999, -0.2204, -0.1323,  0.1805, -0.2071]],
       dtype=torch.float64)
	q_value: tensor([[-30.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3872255493584973, distance: 1.3478148365198246 entropy -2.727669190947686
epoch: 5, step: 20
	action: tensor([[-0.2333, -0.9531, -0.2283,  0.7839, -0.7429,  0.1073,  0.0940]],
       dtype=torch.float64)
	q_value: tensor([[-35.8961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3538828967962224, distance: 1.331518620749394 entropy -2.482765420452677
epoch: 5, step: 21
	action: tensor([[-0.0460, -1.2404, -0.1703, -0.5076,  1.8877, -0.0541,  0.2586]],
       dtype=torch.float64)
	q_value: tensor([[-45.7129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9730144681335573, distance: 1.607392095244829 entropy -2.344945244603959
epoch: 5, step: 22
	action: tensor([[ 0.1373,  0.3277,  0.2159, -0.3382, -0.7546,  0.9142, -0.3988]],
       dtype=torch.float64)
	q_value: tensor([[-45.6343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.607392095244829 entropy -2.4517478895790785
epoch: 5, step: 23
	action: tensor([[ 0.1286,  0.3970,  0.1154,  0.2245,  0.0290, -0.0313,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6949407471074471, distance: 0.6320461560902786 entropy -3.3592266971931273
epoch: 5, step: 24
	action: tensor([[-0.2983, -0.6046,  0.6946, -0.4543,  0.4893,  0.2343,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-29.8406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6320461560902786 entropy -2.8053528255058247
epoch: 5, step: 25
	action: tensor([[-9.8369e-02,  3.9138e-06, -4.3185e-01,  1.6788e-01,  1.8305e-01,
         -2.0409e-01,  7.6613e-02]], dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16413789364051767, distance: 1.0462223941925042 entropy -3.3592266971931273
epoch: 5, step: 26
	action: tensor([[-0.1199,  0.1103, -0.0593, -0.1063,  0.2108,  0.1522,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-26.8831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21685947620816204, distance: 1.01269004757521 entropy -3.4965142401317357
epoch: 5, step: 27
	action: tensor([[ 0.2788, -0.0093,  0.3467,  0.0143,  0.0366, -0.0763, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[-28.8527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.504727281295226, distance: 0.8053393065037697 entropy -3.0371547603991162
epoch: 5, step: 28
	action: tensor([[-0.1363, -0.3088,  0.0947, -0.6601, -0.0416, -0.1482,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-32.9255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48116696011736804, distance: 1.392703609557771 entropy -2.5932221378040703
epoch: 5, step: 29
	action: tensor([[ 0.2811,  0.2737,  0.2378, -0.4924,  0.0725,  0.3147, -0.0346]],
       dtype=torch.float64)
	q_value: tensor([[-29.6294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5525410380693601, distance: 0.7654790313860598 entropy -2.831567731513608
epoch: 5, step: 30
	action: tensor([[-0.3531, -0.6776, -0.7516,  0.0912, -0.0165,  0.2538, -0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-32.2120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6818382017806612, distance: 1.4840509106234936 entropy -2.544290146112784
epoch: 5, step: 31
	action: tensor([[ 0.2789, -0.0165,  0.2160,  0.2750,  0.3575,  0.1960,  0.0599]],
       dtype=torch.float64)
	q_value: tensor([[-34.6499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7131118705035264, distance: 0.6129329697312036 entropy -3.143748190398555
epoch: 5, step: 32
	action: tensor([[ 0.7233, -0.1692, -0.5025, -0.3154, -0.3134,  0.2440, -0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-36.3041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4958421383710109, distance: 0.8125310483137578 entropy -2.59824800346582
epoch: 5, step: 33
	action: tensor([[-0.2553,  0.0025,  0.2045, -0.0786, -0.3347, -0.0992, -0.0877]],
       dtype=torch.float64)
	q_value: tensor([[-35.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08490065683922898, distance: 1.191932548437022 entropy -2.5370073964638586
epoch: 5, step: 34
	action: tensor([[-0.2104, -0.6195, -0.4378,  1.1521,  0.1640,  0.2282,  0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-28.9435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007084939346889918, distance: 1.140283243394936 entropy -2.7869118322135478
epoch: 5, step: 35
	action: tensor([[-0.5039, -0.5607, -0.0419, -0.2500,  0.1355,  0.1834,  0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-44.7036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7918115367476604, distance: 1.5318027946543065 entropy -2.727268609759771
epoch: 5, step: 36
	action: tensor([[-0.2067, -0.4968,  0.2471,  0.3077, -0.5939,  0.3563,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[-33.1844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07013257362620529, distance: 1.1837922300784987 entropy -2.8342513830948097
epoch: 5, step: 37
	action: tensor([[ 0.0095,  0.1114, -1.1252,  2.0587, -0.1632,  0.6939,  0.3053]],
       dtype=torch.float64)
	q_value: tensor([[-40.4268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1837922300784987 entropy -2.231271742824912
epoch: 5, step: 38
	action: tensor([[0.2943, 0.1095, 0.1004, 0.3231, 0.1638, 0.0526, 0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7828186610798464, distance: 0.533295533524799 entropy -3.3592266971931273
epoch: 5, step: 39
	action: tensor([[ 0.3899,  0.0047,  0.1187, -0.0601, -0.1780, -0.0292,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5978321931260471, distance: 0.7257053832644085 entropy -3.3592266971931273
epoch: 5, step: 40
	action: tensor([[-0.6515,  0.2347, -0.1570, -0.5554,  0.0987, -0.0590,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[-32.7894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4680602952262627, distance: 1.3865279855931951 entropy -2.5786828848893344
epoch: 5, step: 41
	action: tensor([[-0.1449,  0.0559,  0.2502, -0.1972,  0.2642,  0.0386,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-26.3973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08575790331471889, distance: 1.0941762923150138 entropy -3.632001804108317
epoch: 5, step: 42
	action: tensor([[-0.3081, -0.2449,  0.1086, -0.1557, -0.2671,  0.1773, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-29.6032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28668799581962134, distance: 1.2980556680046433 entropy -2.8501355227305916
epoch: 5, step: 43
	action: tensor([[-0.3951,  0.6568,  0.2095,  0.3707, -0.7781,  0.2234,  0.1559]],
       dtype=torch.float64)
	q_value: tensor([[-32.1153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2980556680046433 entropy -2.6485987896838443
epoch: 5, step: 44
	action: tensor([[-0.1490, -0.0304,  0.1494,  0.0502, -0.2166, -0.0596,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08954830378352951, distance: 1.0919057372988756 entropy -3.3592266971931273
epoch: 5, step: 45
	action: tensor([[ 0.1855,  0.2843,  0.1867, -0.2815, -0.0147, -0.2463,  0.1222]],
       dtype=torch.float64)
	q_value: tensor([[-29.8331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44399219735708806, distance: 0.8532909438116056 entropy -2.760335285038592
epoch: 5, step: 46
	action: tensor([[ 0.2202,  0.1369,  0.0184,  0.0012, -0.4688, -0.1472, -0.0467]],
       dtype=torch.float64)
	q_value: tensor([[-28.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5561972309737828, distance: 0.7623452472270118 entropy -2.8381951334159248
epoch: 5, step: 47
	action: tensor([[ 0.4481, -0.0076, -0.0977, -0.9164,  0.2120,  0.0891,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-29.8905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1958916297980945, distance: 1.0261574078923952 entropy -2.73043127673708
epoch: 5, step: 48
	action: tensor([[-0.3217, -0.7404, -0.2446, -1.2228, -0.4590,  0.2395, -0.2299]],
       dtype=torch.float64)
	q_value: tensor([[-31.2252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6765738506733927, distance: 1.4817264634220018 entropy -2.6511133319539297
epoch: 5, step: 49
	action: tensor([[ 0.4381, -0.2475, -0.5403, -0.0172, -1.2132,  0.5674, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-37.2447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3121442419708482, distance: 0.9490855949883132 entropy -2.584633509858341
epoch: 5, step: 50
	action: tensor([[-2.0038, -1.6867, -1.1854,  1.1367,  1.2791,  0.0362,  0.1177]],
       dtype=torch.float64)
	q_value: tensor([[-42.8697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9490855949883132 entropy -2.1924741016826714
epoch: 5, step: 51
	action: tensor([[-0.3703, -0.0483, -0.1259, -0.4642, -0.0621, -0.2455,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3843049482945242, distance: 1.3463952751781867 entropy -3.3592266971931273
epoch: 5, step: 52
	action: tensor([[ 0.1749, -0.0126,  0.4751,  0.0634, -0.5395,  0.0814,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-26.2520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48001239703677545, distance: 0.8251885403047651 entropy -3.361818519657749
epoch: 5, step: 53
	action: tensor([[ 0.6043, -0.6512, -0.1255,  0.5076,  0.1369,  0.4489,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-35.7634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5785438286985682, distance: 0.7429043512008937 entropy -2.3094791161914894
epoch: 5, step: 54
	action: tensor([[-0.7444, -1.1993,  0.1249, -0.8500,  0.5960,  0.7245, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-47.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9309550067631835, distance: 1.5901671243513127 entropy -2.327200167359997
epoch: 5, step: 55
	action: tensor([[-0.3420, -0.6465, -0.6281,  0.9761, -0.9023,  0.2646, -0.2067]],
       dtype=torch.float64)
	q_value: tensor([[-45.4221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5172479522857119, distance: 1.409564564333564 entropy -2.3196496348611015
epoch: 5, step: 56
	action: tensor([[-0.3641, -0.6190, -0.1529, -1.1262,  0.2626,  0.2023,  0.2573]],
       dtype=torch.float64)
	q_value: tensor([[-45.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6890132525436898, distance: 1.4872131672559565 entropy -2.440361952355253
epoch: 5, step: 57
	action: tensor([[-0.2142,  0.4896, -0.5790,  0.3066, -0.1026,  0.2539, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-35.5190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4872131672559565 entropy -2.7231035860563138
epoch: 5, step: 58
	action: tensor([[ 0.3188,  0.1384,  0.0457,  0.3454,  0.0279, -0.1089,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7840075492036315, distance: 0.5318338545237186 entropy -3.3592266971931273
epoch: 5, step: 59
	action: tensor([[ 0.0922, -0.3884, -0.0439, -0.3770, -0.2316, -0.2176,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1981331764268499, distance: 1.2525908654260731 entropy -3.3592266971931273
epoch: 5, step: 60
	action: tensor([[ 0.2573, -0.0414, -0.2879,  0.2368, -0.7715,  0.3394,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[-30.5057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.535980068738393, distance: 0.7795159591133592 entropy -2.782896066936454
epoch: 5, step: 61
	action: tensor([[ 0.4651,  0.0956, -0.4076,  0.6466,  0.5157,  0.1651,  0.1480]],
       dtype=torch.float64)
	q_value: tensor([[-37.1840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7795159591133592 entropy -2.4121886272411226
epoch: 5, step: 62
	action: tensor([[ 0.3152, -0.1228,  0.0023, -0.1177,  0.2349,  0.0126,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-34.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.413485187383001, distance: 0.8763875224775411 entropy -3.3592266971931273
epoch: 5, step: 63
	action: tensor([[ 0.2078, -0.0980,  0.2028, -0.3440, -0.4511,  0.0239, -0.0648]],
       dtype=torch.float64)
	q_value: tensor([[-32.0373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.208285961074133, distance: 1.0182182252997651 entropy -2.7666186728195865
LOSS epoch 5 actor 508.5078944285508 critic 383.4276380786082
epoch: 6, step: 0
	action: tensor([[-0.0335,  0.3375, -0.5636,  0.3223, -0.7031,  0.1596,  0.0687]],
       dtype=torch.float64)
	q_value: tensor([[-31.8889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0182182252997651 entropy -2.500165272069354
epoch: 6, step: 1
	action: tensor([[0.1574, 0.0436, 0.3325, 0.1305, 0.1930, 0.0612, 0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5832952599990933, distance: 0.7387047873107097 entropy -3.3595368998916535
epoch: 6, step: 2
	action: tensor([[ 0.0229,  0.5467,  0.5291, -0.3409,  0.2515,  0.2605,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[-33.5546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7387047873107097 entropy -2.600502778569143
epoch: 6, step: 3
	action: tensor([[-0.1258,  0.0966, -0.0381, -0.0823, -0.1516, -0.1980,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13775132584060534, distance: 1.062607705551053 entropy -3.3595368998916535
epoch: 6, step: 4
	action: tensor([[-0.1470, -0.1114,  0.2300,  0.0777, -0.0687,  0.3541,  0.0571]],
       dtype=torch.float64)
	q_value: tensor([[-26.5239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1849194803398425, distance: 1.0331347038572742 entropy -3.063007850751261
epoch: 6, step: 5
	action: tensor([[ 0.6311,  0.6201,  0.0409, -0.7163,  0.1903,  0.1807,  0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-34.2120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.863518272349635, distance: 0.42276005813672046 entropy -2.521091844868992
epoch: 6, step: 6
	action: tensor([[ 0.2453, -0.0713,  0.0469, -0.1065,  0.0467, -0.0554, -0.2263]],
       dtype=torch.float64)
	q_value: tensor([[-31.6010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37067604832386103, distance: 0.9078076199265379 entropy -2.5614231153054496
epoch: 6, step: 7
	action: tensor([[-2.3968e-01, -5.4514e-01, -2.0015e-01,  3.2561e-01,  6.3351e-02,
          3.0548e-01,  2.3069e-04]], dtype=torch.float64)
	q_value: tensor([[-30.6923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2129716920690241, distance: 1.2603234763815427 entropy -2.8116196561362266
epoch: 6, step: 8
	action: tensor([[-0.1101,  0.5109,  0.5443, -0.0344,  0.3334,  0.2004,  0.1255]],
       dtype=torch.float64)
	q_value: tensor([[-36.3698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2603234763815427 entropy -2.731982172618825
epoch: 6, step: 9
	action: tensor([[ 0.0378,  0.0505,  0.0463,  0.0166,  0.1154, -0.3371,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979098948124902, distance: 0.9588554023377516 entropy -3.3595368998916535
epoch: 6, step: 10
	action: tensor([[-0.3900,  0.1984, -0.1295, -0.4672,  0.3381,  0.1134,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[-27.6024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13792752283916676, distance: 1.220714193243968 entropy -3.0315049748881764
epoch: 6, step: 11
	action: tensor([[ 0.2941,  0.2170, -0.0840, -0.0135, -0.0431,  0.0121, -0.0545]],
       dtype=torch.float64)
	q_value: tensor([[-26.9076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6861356006354198, distance: 0.6411028716567955 entropy -3.3581231913461225
epoch: 6, step: 12
	action: tensor([[-0.4474, -0.2585, -0.5317, -0.0797,  0.6001,  0.4035,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-29.7038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3162378611022343, distance: 1.3128765243830038 entropy -2.858812564878997
epoch: 6, step: 13
	action: tensor([[-0.1112,  0.0410, -0.1675, -0.0270, -0.1912,  0.0092, -0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-32.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19266958124311206, distance: 1.0282112501566207 entropy -3.3444179680317916
epoch: 6, step: 14
	action: tensor([[ 0.0594, -0.0349, -0.1177,  0.0973, -0.0552,  0.3296,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-27.7921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42883697799908227, distance: 0.8648419247498204 entropy -3.034476896306319
epoch: 6, step: 15
	action: tensor([[ 0.4579, -0.1798,  0.6522, -0.3560,  0.1300,  0.3858,  0.0674]],
       dtype=torch.float64)
	q_value: tensor([[-33.0870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4001177335335532, distance: 0.8863182769957867 entropy -2.690786241317577
epoch: 6, step: 16
	action: tensor([[ 0.5169,  0.2205, -0.5524, -0.2340, -0.4917,  0.0226, -0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-39.0251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7751296976266251, distance: 0.5426536708610197 entropy -2.176691533441851
epoch: 6, step: 17
	action: tensor([[-0.0203, -0.6398, -0.3824,  0.2100, -0.0053, -0.0047, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[-29.3332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21569171702239887, distance: 1.2617357894798804 entropy -2.866654826387783
epoch: 6, step: 18
	action: tensor([[-0.2369, -0.4560, -0.1022,  0.6245, -0.0871, -0.0904,  0.0781]],
       dtype=torch.float64)
	q_value: tensor([[-33.9320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03214421769288078, distance: 1.1625908088631822 entropy -2.890217989156404
epoch: 6, step: 19
	action: tensor([[ 0.5252,  0.0080, -0.3019,  0.0155, -0.6349,  0.1854,  0.1642]],
       dtype=torch.float64)
	q_value: tensor([[-35.5431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7021865074243294, distance: 0.6244948743182512 entropy -2.769954033697114
epoch: 6, step: 20
	action: tensor([[-0.1728, -0.5304, -0.1331, -0.5847, -0.1252,  0.1944,  0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-35.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5230698392837247, distance: 1.4122663206508548 entropy -2.4702657767358276
epoch: 6, step: 21
	action: tensor([[ 0.1874, -0.7563, -1.0006,  0.6911, -0.1078,  0.4413,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-32.6163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16884359964783013, distance: 1.2371857092186158 entropy -2.695598595731311
epoch: 6, step: 22
	action: tensor([[-0.5928,  0.1187, -0.3844, -0.0914,  0.7998,  0.3010,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-43.3776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2437708475058269, distance: 1.2762239189539146 entropy -2.6929079421570363
epoch: 6, step: 23
	action: tensor([[-0.2827, -0.1835,  0.1766, -0.4182, -0.1553, -0.1183, -0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-31.1639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4279158997806449, distance: 1.3674391447878236 entropy -3.43599558595583
epoch: 6, step: 24
	action: tensor([[-0.4009,  0.2970, -0.0916,  0.1719, -0.0038,  0.1457,  0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-28.2758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3674391447878236 entropy -2.8514712176931383
epoch: 6, step: 25
	action: tensor([[ 0.1605,  0.0475,  0.0843,  0.2498,  0.2380, -0.0083,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5736284464889001, distance: 0.7472239857523244 entropy -3.3595368998916535
epoch: 6, step: 26
	action: tensor([[ 0.6223,  0.1351,  0.0846,  0.1750, -0.7859,  0.4596,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-32.0315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7472239857523244 entropy -2.8291565800951495
epoch: 6, step: 27
	action: tensor([[-0.0551, -0.1999,  0.0796,  0.2521,  0.2768, -0.0281,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20339771424527442, distance: 1.021356759294591 entropy -3.3595368998916535
epoch: 6, step: 28
	action: tensor([[ 0.0540, -0.1343,  0.0793, -0.4952, -0.1054,  0.1948,  0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-32.6437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004379218364852333, distance: 1.1418358380751323 entropy -2.8458864688747165
epoch: 6, step: 29
	action: tensor([[ 0.3026, -0.0754,  0.6038,  0.1864,  1.3600,  0.0417, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-31.0453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6141661421496224, distance: 0.7108154533825509 entropy -2.619905263312481
epoch: 6, step: 30
	action: tensor([[-0.3833,  0.6725,  0.3997,  0.5359, -0.8504, -0.0094, -0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-43.1196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7108154533825509 entropy -2.436606210213354
epoch: 6, step: 31
	action: tensor([[-0.0684, -0.0502,  0.0407,  0.1139, -0.0059,  0.2489,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29264878363952584, distance: 0.9624412879436589 entropy -3.3595368998916535
epoch: 6, step: 32
	action: tensor([[ 0.1965,  0.4340, -0.0096,  0.0523,  0.7174,  0.1059,  0.0852]],
       dtype=torch.float64)
	q_value: tensor([[-32.5012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9624412879436589 entropy -2.6865764504368994
epoch: 6, step: 33
	action: tensor([[ 0.0401,  0.0607,  0.2909, -0.1855, -0.0539,  0.0256,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2576566109148434, distance: 0.9859595908270034 entropy -3.3595368998916535
epoch: 6, step: 34
	action: tensor([[-0.1715, -0.9467,  0.1190, -0.3395,  0.1390,  0.3944,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[-30.4101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7319143037318305, distance: 1.5059823874201477 entropy -2.641161064940029
epoch: 6, step: 35
	action: tensor([[-7.3140e-01, -1.1118e+00, -1.0786e+00,  4.9869e-01, -5.6397e-01,
          9.1185e-01, -8.3244e-04]], dtype=torch.float64)
	q_value: tensor([[-38.6927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2925482541179218, distance: 1.7326703196082935 entropy -2.3519315424242393
epoch: 6, step: 36
	action: tensor([[ 0.1467,  0.2700,  0.5215, -0.4399,  0.2499,  0.2944,  0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-49.1630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4531916111640194, distance: 0.8462024484278932 entropy -2.5071376998067265
epoch: 6, step: 37
	action: tensor([[-1.3931,  0.7287, -0.7267,  0.1464,  0.4690,  1.0205, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-33.0456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8462024484278932 entropy -2.4442246258613136
epoch: 6, step: 38
	action: tensor([[-0.0965, -0.3205,  0.0278,  0.6651, -0.3081, -0.0639,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28271637455541987, distance: 0.9691748854278909 entropy -3.3595368998916535
epoch: 6, step: 39
	action: tensor([[ 0.0281,  0.1858,  0.1414, -0.5680, -0.5827,  0.2481,  0.1947]],
       dtype=torch.float64)
	q_value: tensor([[-36.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16287328916743982, distance: 1.0470135279589954 entropy -2.5935483943946744
epoch: 6, step: 40
	action: tensor([[-0.3717, -1.0886, -0.2932, -0.3867, -0.2674,  0.3090,  0.0932]],
       dtype=torch.float64)
	q_value: tensor([[-31.7886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9761645194126007, distance: 1.608674738686297 entropy -2.4349575794754412
epoch: 6, step: 41
	action: tensor([[-0.4840, -0.2935, -0.5360, -0.7128, -0.3252,  0.2927,  0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-38.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5129434299831948, distance: 1.4075636350602667 entropy -2.5030293247810884
epoch: 6, step: 42
	action: tensor([[ 0.5401,  0.1120, -0.1017,  0.1242,  0.3460,  0.1440,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-30.7934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.829019873072216, distance: 0.4731834112623336 entropy -3.1509650754462077
epoch: 6, step: 43
	action: tensor([[-0.1240, -0.4711,  0.0269, -0.2112,  0.3114, -0.3924, -0.0882]],
       dtype=torch.float64)
	q_value: tensor([[-34.1467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4805927042037963, distance: 1.3924336042718173 entropy -2.7276656985785963
epoch: 6, step: 44
	action: tensor([[ 0.2548,  0.1425,  0.3012,  0.2134,  0.0692,  0.2235, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-29.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3924336042718173 entropy -3.0703423399887773
epoch: 6, step: 45
	action: tensor([[ 0.4930,  0.0971, -0.0221, -0.0738,  0.1845, -0.0233,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7211241844851954, distance: 0.6043132546606029 entropy -3.3595368998916535
epoch: 6, step: 46
	action: tensor([[ 0.1801,  0.0029,  0.0984,  0.4823,  0.1584, -0.3051, -0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-31.8367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6430937391004389, distance: 0.6836499344717781 entropy -2.7364463501912972
epoch: 6, step: 47
	action: tensor([[0.7099, 0.2335, 0.2483, 0.0851, 0.4687, 0.2000, 0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-32.7389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6836499344717781 entropy -2.8628148365523947
epoch: 6, step: 48
	action: tensor([[-0.0420,  0.0376, -0.2651,  0.1767, -0.2618, -0.0303,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31386282836406787, distance: 0.9478992229550266 entropy -3.3595368998916535
epoch: 6, step: 49
	action: tensor([[-0.2380, -0.0775,  0.0096, -0.2587, -0.0031,  0.3257,  0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-29.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08532379246343536, distance: 1.1921649660102034 entropy -2.9709250882585247
epoch: 6, step: 50
	action: tensor([[ 0.2171,  0.1554, -0.3752,  0.5776,  0.2592,  0.1138,  0.0481]],
       dtype=torch.float64)
	q_value: tensor([[-30.5330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1921649660102034 entropy -2.7509921892867566
epoch: 6, step: 51
	action: tensor([[ 0.0044,  0.0746, -0.0221,  0.2876,  0.0999,  0.1974,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1921649660102034 entropy -3.3595368998916535
epoch: 6, step: 52
	action: tensor([[ 0.0450,  0.1393,  0.0431, -0.0416,  0.0849, -0.2093,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3684395350614209, distance: 0.909419288381047 entropy -3.3595368998916535
epoch: 6, step: 53
	action: tensor([[-1.0063e-01, -1.7341e-01, -5.5094e-01,  2.2940e-01,  2.4355e-01,
          2.1078e-01, -5.4413e-04]], dtype=torch.float64)
	q_value: tensor([[-27.6753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10659774004312306, distance: 1.0816337166854653 entropy -3.0014354579378626
epoch: 6, step: 54
	action: tensor([[-0.4314,  0.1098, -0.1357, -0.6401, -0.0537,  0.0566,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[-31.4231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3196729541498875, distance: 1.3145885684425267 entropy -3.2365255973661635
epoch: 6, step: 55
	action: tensor([[-0.2071, -0.2486,  0.1173, -0.3264, -0.0950, -0.0605, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[-26.9659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3362997739221505, distance: 1.3228440270941633 entropy -3.2753405683057286
epoch: 6, step: 56
	action: tensor([[ 0.0862, -0.4878,  0.2082,  0.0806, -0.7279, -0.0818,  0.0574]],
       dtype=torch.float64)
	q_value: tensor([[-29.2441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009153375192344049, distance: 1.1495696299346725 entropy -2.8159370741780023
epoch: 6, step: 57
	action: tensor([[-0.7640, -0.7415, -0.4839,  1.1908,  1.5672,  0.1064,  0.1865]],
       dtype=torch.float64)
	q_value: tensor([[-36.3754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7237897752928066, distance: 1.5024459018863991 entropy -2.354524140965992
epoch: 6, step: 58
	action: tensor([[-0.7851, -0.2825, -0.1599,  0.6281, -0.2399, -0.3019, -0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-48.7809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6595986687394935, distance: 1.4742061959354025 entropy -2.9783121200646763
epoch: 6, step: 59
	action: tensor([[-0.1921,  0.3821,  0.3883,  0.4124, -0.2327,  0.1836,  0.1709]],
       dtype=torch.float64)
	q_value: tensor([[-31.5881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4742061959354025 entropy -3.0491416821180946
epoch: 6, step: 60
	action: tensor([[-0.2178,  0.0675,  0.0443,  0.1691,  0.4814,  0.0486,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15967571705307315, distance: 1.0490112603897592 entropy -3.3595368998916535
epoch: 6, step: 61
	action: tensor([[ 0.3331,  0.0455,  0.1298, -0.0732, -0.4067,  0.0978, -0.0294]],
       dtype=torch.float64)
	q_value: tensor([[-30.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5695086592373722, distance: 0.7508253087261105 entropy -3.0547731875780095
epoch: 6, step: 62
	action: tensor([[ 0.2901, -0.7820, -0.1976, -1.2805, -0.7743,  0.1752,  0.0761]],
       dtype=torch.float64)
	q_value: tensor([[-33.0596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.666875713833371, distance: 1.4774347267044408 entropy -2.4968411297145297
epoch: 6, step: 63
	action: tensor([[-0.6326, -0.7375,  0.0579, -1.0273,  1.4535,  0.0813, -0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-39.7681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9438373383548502, distance: 1.5954626923941073 entropy -2.267629592836676
LOSS epoch 6 actor 482.85656221870045 critic 189.31465997116226
epoch: 7, step: 0
	action: tensor([[-0.3884,  0.1472,  0.0723,  0.4304, -0.1937, -0.2271, -0.3398]],
       dtype=torch.float64)
	q_value: tensor([[-39.9765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5954626923941073 entropy -2.6131260079847105
epoch: 7, step: 1
	action: tensor([[ 0.1362, -0.1112, -0.1246,  0.4078, -0.2776, -0.0997,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5088659538715404, distance: 0.8019673986379746 entropy -3.358877649233461
epoch: 7, step: 2
	action: tensor([[-0.2611, -0.3361,  0.1056, -0.0732, -0.9722,  0.4630,  0.1094]],
       dtype=torch.float64)
	q_value: tensor([[-33.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3155804772713926, distance: 1.3125486309171184 entropy -2.750204071024311
epoch: 7, step: 3
	action: tensor([[ 0.8312,  0.7604, -0.5399,  0.5610, -0.8904,  0.9136,  0.3048]],
       dtype=torch.float64)
	q_value: tensor([[-38.3580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3125486309171184 entropy -2.1786958352256263
epoch: 7, step: 4
	action: tensor([[ 0.0212, -0.0504,  0.0884,  0.0196,  0.1500, -0.0477,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27322631482977044, distance: 0.9755651784219255 entropy -3.358877649233461
epoch: 7, step: 5
	action: tensor([[-0.1652, -0.0487, -0.1641,  0.1669, -0.5167,  0.2511,  0.0131]],
       dtype=torch.float64)
	q_value: tensor([[-30.9828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13850690721686354, distance: 1.0621420263424461 entropy -2.849860204171404
epoch: 7, step: 6
	action: tensor([[-0.3852, -0.1181, -0.3259, -0.4605, -0.7134, -0.0959,  0.1782]],
       dtype=torch.float64)
	q_value: tensor([[-32.5591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2979348691855619, distance: 1.3037164435829447 entropy -2.6435721000107244
epoch: 7, step: 7
	action: tensor([[ 0.2449,  0.1909,  0.4996, -0.0878, -0.0222,  0.1515,  0.0996]],
       dtype=torch.float64)
	q_value: tensor([[-28.7249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.633332564927106, distance: 0.6929355832417521 entropy -3.0314058086548945
epoch: 7, step: 8
	action: tensor([[ 0.3202, -0.1089, -0.2252, -0.5220,  0.4580,  0.6658,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-34.8930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47551180177917995, distance: 0.828751931235616 entropy -2.422658184220053
epoch: 7, step: 9
	action: tensor([[ 0.0845,  0.1800,  0.4352,  0.3884, -0.4740,  0.5576, -0.2098]],
       dtype=torch.float64)
	q_value: tensor([[-34.8218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.828751931235616 entropy -2.586128440030899
epoch: 7, step: 10
	action: tensor([[ 0.1475,  0.1879,  0.0631,  0.1922, -0.0241,  0.0571,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6678879576531571, distance: 0.6594760149608768 entropy -3.358877649233461
epoch: 7, step: 11
	action: tensor([[ 1.1203,  0.4083, -0.3601,  0.2048,  0.3115,  0.5964,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-32.5452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9694545345211426, distance: 0.20000003290267163 entropy -2.7214001484729917
epoch: 7, step: 12
	action: tensor([[ 0.0078, -0.1094,  0.3123,  0.3376, -0.1025, -0.0991,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40150351335085754, distance: 0.8852939492189753 entropy -3.358877649233461
epoch: 7, step: 13
	action: tensor([[-0.5957,  0.0594, -0.3028,  0.1380,  0.0091,  0.3084,  0.1254]],
       dtype=torch.float64)
	q_value: tensor([[-34.3831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3117548226073623, distance: 1.3106388224059602 entropy -2.623127640986642
epoch: 7, step: 14
	action: tensor([[0.5251, 0.0414, 0.1854, 0.3083, 0.0991, 0.1174, 0.0738]],
       dtype=torch.float64)
	q_value: tensor([[-29.9759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8107705325866015, distance: 0.4977956266149187 entropy -3.1951704608167906
epoch: 7, step: 15
	action: tensor([[-1.1476, -0.4350, -0.1352,  0.5061,  1.0069,  0.2538, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[-38.0031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3283259894169115, distance: 1.746138090564945 entropy -2.5098840954035966
epoch: 7, step: 16
	action: tensor([[-0.4121, -0.0518,  0.1522,  0.1990,  0.2333, -0.0033, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-39.3166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3497095038126494, distance: 1.3294648096440496 entropy -3.1882261989972567
epoch: 7, step: 17
	action: tensor([[-0.3796, -0.0500, -0.2185, -0.4186, -0.3513,  0.3350,  0.0709]],
       dtype=torch.float64)
	q_value: tensor([[-31.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5234105031479541, distance: 1.412424252083296 entropy -2.9858540020634683
epoch: 7, step: 18
	action: tensor([[ 0.0422, -0.4273, -0.2927, -0.6629,  0.8226,  0.2901,  0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-30.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2678400762916806, distance: 1.2885133753945879 entropy -2.857733657562629
epoch: 7, step: 19
	action: tensor([[ 0.0467,  0.0840,  0.0474, -0.0408,  0.8697,  0.2852, -0.2505]],
       dtype=torch.float64)
	q_value: tensor([[-33.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2885133753945879 entropy -2.7814116313390413
epoch: 7, step: 20
	action: tensor([[ 0.1567,  0.2524,  0.0984,  0.0525, -0.0741,  0.0521,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6444393829466575, distance: 0.6823599345641943 entropy -3.358877649233461
epoch: 7, step: 21
	action: tensor([[-0.1556,  0.4767,  0.6980, -1.1370, -0.1434,  0.2519,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-31.3064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6823599345641943 entropy -2.718190140273687
epoch: 7, step: 22
	action: tensor([[-0.1046, -0.0584,  0.2361,  0.0204, -0.2292, -0.3260,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06511542206966281, distance: 1.106459930842837 entropy -3.358877649233461
epoch: 7, step: 23
	action: tensor([[-0.4094, -0.5886,  0.6171, -0.4587, -0.3315,  0.3908,  0.1016]],
       dtype=torch.float64)
	q_value: tensor([[-29.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8553617289254283, distance: 1.5587303452044778 entropy -2.822228307791228
epoch: 7, step: 24
	action: tensor([[ 2.6707, -1.2635, -0.4020, -0.5564,  0.1274,  0.6364,  0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-38.2801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5587303452044778 entropy -2.1486316388211155
epoch: 7, step: 25
	action: tensor([[-0.2113,  0.0911,  0.0663,  0.1951, -0.0147,  0.0020,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1804138649635708, distance: 1.0359862579954693 entropy -3.358877649233461
epoch: 7, step: 26
	action: tensor([[-0.0303, -0.1654, -0.0484, -0.3652, -0.7097,  0.3538,  0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-30.2224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.073568345772139, distance: 1.185691051367658 entropy -2.902579748415037
epoch: 7, step: 27
	action: tensor([[ 0.2877, -0.3593, -0.5249, -0.3575, -0.1150,  0.3602,  0.1602]],
       dtype=torch.float64)
	q_value: tensor([[-34.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14727288902627544, distance: 1.0567243834422715 entropy -2.390476446031599
epoch: 7, step: 28
	action: tensor([[-0.3982, -0.6656,  0.1999, -0.0326, -0.1120,  0.3367, -0.0545]],
       dtype=torch.float64)
	q_value: tensor([[-33.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6306794460192293, distance: 1.461305409051829 entropy -2.6786042492497346
epoch: 7, step: 29
	action: tensor([[-0.1691, -0.1260,  0.1614, -0.4104,  0.5015,  0.1543,  0.1803]],
       dtype=torch.float64)
	q_value: tensor([[-36.5809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20114149299881223, distance: 1.254162405038722 entropy -2.4459211459026626
epoch: 7, step: 30
	action: tensor([[-0.0109, -0.1654, -0.6189, -0.5420,  0.0561,  0.2546, -0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-31.5420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10146092010580876, distance: 1.084738809524891 entropy -2.7992680711320337
epoch: 7, step: 31
	action: tensor([[-0.2015, -0.2436, -0.1194,  0.3952, -0.1488,  0.1772, -0.0529]],
       dtype=torch.float64)
	q_value: tensor([[-28.8552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05780847385147203, distance: 1.1107754952963111 entropy -3.239784962355666
epoch: 7, step: 32
	action: tensor([[-0.7297, -0.2682,  0.4294,  0.6221,  0.3286,  0.2965,  0.1613]],
       dtype=torch.float64)
	q_value: tensor([[-34.2783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17513298278151068, distance: 1.2405098048050323 entropy -2.7549952995750955
epoch: 7, step: 33
	action: tensor([[-0.0418, -0.0065,  0.1692,  0.1728,  0.3055,  0.3860,  0.1550]],
       dtype=torch.float64)
	q_value: tensor([[-39.2716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44350166783899414, distance: 0.8536672623777647 entropy -2.5598248622789397
epoch: 7, step: 34
	action: tensor([[-0.1594,  0.2525,  0.0674, -0.2918, -0.7813,  0.2289,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-35.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11359721435971826, distance: 1.0773882861179105 entropy -2.628771207068229
epoch: 7, step: 35
	action: tensor([[ 0.3173, -0.1007,  0.1328, -0.3396, -0.0999, -0.2078,  0.1824]],
       dtype=torch.float64)
	q_value: tensor([[-30.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2126820800638417, distance: 1.0153873802704667 entropy -2.5064839579813665
epoch: 7, step: 36
	action: tensor([[ 0.0812,  0.3880,  0.4865,  0.0227, -0.5214,  0.1561, -0.0534]],
       dtype=torch.float64)
	q_value: tensor([[-31.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0153873802704667 entropy -2.672825805294953
epoch: 7, step: 37
	action: tensor([[-0.0683,  0.0689, -0.1995,  0.0338,  0.0174, -0.0465,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.258724272499952, distance: 0.9852503166799598 entropy -3.358877649233461
epoch: 7, step: 38
	action: tensor([[ 0.4391, -0.1776,  0.3700, -0.3007,  0.3258,  0.1538,  0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-28.3571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3416906872386901, distance: 0.9284781603074717 entropy -3.123923471572342
epoch: 7, step: 39
	action: tensor([[-0.2560, -0.0956,  0.6450, -0.0104, -0.0933,  0.9064, -0.1470]],
       dtype=torch.float64)
	q_value: tensor([[-36.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934683364202517, distance: 0.9618835734589484 entropy -2.434011839718145
epoch: 7, step: 40
	action: tensor([[-2.4563, -0.2149,  0.2672, -0.8321,  0.6069,  1.2713,  0.1999]],
       dtype=torch.float64)
	q_value: tensor([[-41.1628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9618835734589484 entropy -2.1108894011139574
epoch: 7, step: 41
	action: tensor([[-0.3728,  0.1367, -0.0065, -0.2985, -0.0402,  0.0203,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21160757107774342, distance: 1.2596145903620481 entropy -3.358877649233461
epoch: 7, step: 42
	action: tensor([[-0.3624, -0.3960,  0.3774, -0.3297,  0.4657,  0.3554,  0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-27.5680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5413136801077392, distance: 1.420699441715306 entropy -3.0917517922858546
epoch: 7, step: 43
	action: tensor([[ 0.2563, -0.2738,  0.0633,  0.1159,  0.9928,  0.1063, -0.0849]],
       dtype=torch.float64)
	q_value: tensor([[-34.9426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3764051502241412, distance: 0.9036660221940349 entropy -2.5526658012966066
epoch: 7, step: 44
	action: tensor([[-0.0275,  0.0730, -0.2012, -0.2123,  0.4749,  0.2760, -0.1876]],
       dtype=torch.float64)
	q_value: tensor([[-37.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29970811037276623, distance: 0.9576266895662088 entropy -2.740550299121091
epoch: 7, step: 45
	action: tensor([[ 0.0107,  0.0617,  0.1141, -0.0332,  0.1599,  0.1347, -0.0851]],
       dtype=torch.float64)
	q_value: tensor([[-30.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33769369210808564, distance: 0.9312925718090391 entropy -3.127956278062108
epoch: 7, step: 46
	action: tensor([[-0.5905, -0.0869, -0.1063,  0.3866, -0.3602,  0.3409,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-31.3491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3246620084579277, distance: 1.3170711406352609 entropy -2.8305635807333203
epoch: 7, step: 47
	action: tensor([[ 0.7568, -0.3092, -0.3340,  0.2295,  0.2204, -0.1664,  0.2129]],
       dtype=torch.float64)
	q_value: tensor([[-33.7054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5536742256713635, distance: 0.7645091310044394 entropy -2.701898181084714
epoch: 7, step: 48
	action: tensor([[-0.4423, -0.0765,  0.2597, -0.0979,  0.0732, -0.3329, -0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-37.6997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48973591222006685, distance: 1.3967263835669348 entropy -2.710515324690459
epoch: 7, step: 49
	action: tensor([[ 0.3743, -0.3375,  0.0968,  0.0446,  0.2132,  0.2279,  0.0690]],
       dtype=torch.float64)
	q_value: tensor([[-28.2641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4016878975761885, distance: 0.8851575684574691 entropy -3.099155909221161
epoch: 7, step: 50
	action: tensor([[ 0.0639,  0.2100,  0.1677,  0.7825, -0.6128,  0.1612, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-37.5692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8851575684574691 entropy -2.50655870962463
epoch: 7, step: 51
	action: tensor([[-0.0838, -0.0473, -0.0840,  0.3587, -0.0907,  0.0948,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3190184263793242, distance: 0.9443312759411431 entropy -3.358877649233461
epoch: 7, step: 52
	action: tensor([[-0.4236,  0.0337,  0.1070, -0.9075,  0.0666,  0.1307,  0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-32.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5424787235447488, distance: 1.4212362772480747 entropy -2.7960688031365484
epoch: 7, step: 53
	action: tensor([[ 0.0040, -0.5608, -0.1647,  0.2883,  0.0828, -0.1592, -0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-30.4363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1009955627225747, distance: 1.2007413800983504 entropy -2.8894652809935386
epoch: 7, step: 54
	action: tensor([[-0.0353,  0.4435, -0.0447, -0.4911, -0.4336,  0.2893,  0.0846]],
       dtype=torch.float64)
	q_value: tensor([[-34.0595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3678841586515532, distance: 0.9098190593136816 entropy -2.883418245027537
epoch: 7, step: 55
	action: tensor([[ 0.4174,  0.1137,  0.1022,  0.4975,  0.2282, -0.1477,  0.0527]],
       dtype=torch.float64)
	q_value: tensor([[-29.3215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9098190593136816 entropy -2.6974581251862846
epoch: 7, step: 56
	action: tensor([[-0.1002, -0.2086,  0.0621,  0.2458,  0.2712, -0.0548,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12362442672924978, distance: 1.071277109856584 entropy -3.358877649233461
epoch: 7, step: 57
	action: tensor([[-0.3644, -0.2812,  0.2004, -0.2114, -0.3112,  0.0834,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-32.8550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48414739929806117, distance: 1.3941041209437541 entropy -2.8868246983792596
epoch: 7, step: 58
	action: tensor([[ 0.0986,  0.3242,  0.1379,  0.0855, -0.3348,  0.0013,  0.1649]],
       dtype=torch.float64)
	q_value: tensor([[-31.9849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3941041209437541 entropy -2.6110011909328454
epoch: 7, step: 59
	action: tensor([[-0.1492,  0.3823,  0.1997,  0.0153, -0.1586,  0.0014,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3454469999974137, distance: 0.9258254226994409 entropy -3.358877649233461
epoch: 7, step: 60
	action: tensor([[-0.1237, -0.6985,  0.0136,  0.2423, -0.7903,  0.3045,  0.0828]],
       dtype=torch.float64)
	q_value: tensor([[-29.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9258254226994409 entropy -2.8292426366522707
epoch: 7, step: 61
	action: tensor([[ 0.2651,  0.0218,  0.1217,  0.2038, -0.1006,  0.0022,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6366798627543925, distance: 0.6897654358163505 entropy -3.358877649233461
epoch: 7, step: 62
	action: tensor([[-0.2156, -0.2497, -0.1762,  0.1371,  0.4409, -0.0394,  0.0617]],
       dtype=torch.float64)
	q_value: tensor([[-33.8876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0783755004497475, distance: 1.1883426915741648 entropy -2.6471466862344846
epoch: 7, step: 63
	action: tensor([[ 0.0499,  0.0853, -0.1815,  0.0991,  0.0142, -0.0121, -0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-31.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4503992082065035, distance: 0.8483603608009334 entropy -3.1998859419090664
LOSS epoch 7 actor 460.5416045466172 critic 280.21193709094456
epoch: 8, step: 0
	action: tensor([[ 0.1926, -0.1381, -0.0173, -0.0881, -0.0307,  0.3621,  0.0388]],
       dtype=torch.float64)
	q_value: tensor([[-31.1498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42829413600591326, distance: 0.8652528065827267 entropy -3.0374287854754547
epoch: 8, step: 1
	action: tensor([[ 0.3899,  0.0425, -0.3123,  1.0098, -0.8267,  0.1142,  0.0238]],
       dtype=torch.float64)
	q_value: tensor([[-36.4830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8652528065827267 entropy -2.5577472722527648
epoch: 8, step: 2
	action: tensor([[ 0.1026,  0.1925,  0.0964,  0.1067,  0.0599, -0.0572,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5295774252647928, distance: 0.7848754960772961 entropy -3.357482864455475
epoch: 8, step: 3
	action: tensor([[ 0.0037, -0.1668, -0.0089, -0.0271, -0.0408, -0.0636,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13657560305943883, distance: 1.0633319203462748 entropy -3.357482864455475
epoch: 8, step: 4
	action: tensor([[-0.1494, -0.2769, -0.0767, -0.1265, -0.2980,  0.3670,  0.0510]],
       dtype=torch.float64)
	q_value: tensor([[-32.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09046530596635449, distance: 1.1949854561192397 entropy -2.836065548463035
epoch: 8, step: 5
	action: tensor([[-0.2596, -0.7472, -0.2755, -0.6127, -0.3533, -0.1647,  0.1395]],
       dtype=torch.float64)
	q_value: tensor([[-35.1220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6576382352538861, distance: 1.4733352211503545 entropy -2.561478392059218
epoch: 8, step: 6
	action: tensor([[-0.2633,  0.4323,  0.1846, -0.6612, -0.1277,  0.0020,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-33.3898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022803602002115397, distance: 1.1573182926767054 entropy -2.851316962079095
epoch: 8, step: 7
	action: tensor([[-0.0570, -0.1690, -0.1754,  0.0331, -0.0921,  0.2690, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-30.0338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16651899703137818, distance: 1.0447311551862406 entropy -2.962981624943918
epoch: 8, step: 8
	action: tensor([[ 0.3212, -0.0190,  0.4137, -0.0298, -0.4674,  0.4786,  0.0861]],
       dtype=torch.float64)
	q_value: tensor([[-33.7476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.650556000125444, distance: 0.6764652427883264 entropy -2.7876948316409114
epoch: 8, step: 9
	action: tensor([[-1.0639, -0.0378, -0.6972, -1.2237, -0.1521,  0.4519,  0.1380]],
       dtype=torch.float64)
	q_value: tensor([[-41.7708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.665799926904616, distance: 1.4769578880644418 entropy -2.1613621611259104
epoch: 8, step: 10
	action: tensor([[-0.0852,  0.1138,  0.1127,  0.1087, -0.1368, -0.0791,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[-34.1777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3226630994125407, distance: 0.941800813730638 entropy -3.426244312067342
epoch: 8, step: 11
	action: tensor([[ 0.5043, -0.0950,  0.1344,  0.2201, -0.6145,  0.1154,  0.0953]],
       dtype=torch.float64)
	q_value: tensor([[-31.9031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7348716641260762, distance: 0.5892298858410923 entropy -2.8522930921297536
epoch: 8, step: 12
	action: tensor([[ 0.0401,  0.3514,  0.0256, -1.0177,  0.3648, -0.4438,  0.1049]],
       dtype=torch.float64)
	q_value: tensor([[-40.9078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08276976811446901, distance: 1.0959629522670464 entropy -2.3151831452346703
epoch: 8, step: 13
	action: tensor([[-0.2856, -0.2366, -0.3678,  0.3751, -0.0349,  0.3417, -0.1701]],
       dtype=torch.float64)
	q_value: tensor([[-30.3176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06162219994783702, distance: 1.1790757004351622 entropy -3.026605448539771
epoch: 8, step: 14
	action: tensor([[-0.0670,  0.0349, -0.3579, -0.1959,  0.2062,  0.1195,  0.1180]],
       dtype=torch.float64)
	q_value: tensor([[-34.7566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22908968259843887, distance: 1.004751405302561 entropy -2.9414873141281683
epoch: 8, step: 15
	action: tensor([[ 0.0897, -0.1401,  0.0692, -0.1769, -0.0740,  0.0977, -0.0378]],
       dtype=torch.float64)
	q_value: tensor([[-29.8537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1973302147205941, distance: 1.0252390768103168 entropy -3.241257125038007
epoch: 8, step: 16
	action: tensor([[-0.2952,  0.1149,  0.7135,  0.0354,  0.3745,  1.0634,  0.0362]],
       dtype=torch.float64)
	q_value: tensor([[-33.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0252390768103168 entropy -2.6906974761797433
epoch: 8, step: 17
	action: tensor([[ 0.1590, -0.0486,  0.0660,  0.0404,  0.0111, -0.0898,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39293521352287797, distance: 0.8916085289134482 entropy -3.357482864455475
epoch: 8, step: 18
	action: tensor([[-0.1651,  0.1348,  0.0150,  0.4661,  0.3387,  0.1913,  0.0292]],
       dtype=torch.float64)
	q_value: tensor([[-33.5148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8916085289134482 entropy -2.7886015719113715
epoch: 8, step: 19
	action: tensor([[-0.1157, -0.1030, -0.1001,  0.4216, -0.1431,  0.1569,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27230300012686826, distance: 0.9761846751007616 entropy -3.357482864455475
epoch: 8, step: 20
	action: tensor([[0.4680, 0.1720, 0.0436, 0.3665, 0.3091, 0.2811, 0.1396]],
       dtype=torch.float64)
	q_value: tensor([[-35.7515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9761846751007616 entropy -2.734716587341351
epoch: 8, step: 21
	action: tensor([[ 0.0137, -0.1381, -0.2225,  0.2337, -0.1634, -0.0759,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9761846751007616 entropy -3.357482864455475
epoch: 8, step: 22
	action: tensor([[-0.1466, -0.1252,  0.1889, -0.1143, -0.2827, -0.0908,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07928630438087803, distance: 1.1888444271715581 entropy -3.357482864455475
epoch: 8, step: 23
	action: tensor([[-0.6572,  0.2108,  0.1767,  0.1209, -0.4170,  0.5156,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[-32.5130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21799659613507827, distance: 1.2629313110550706 entropy -2.727090429951201
epoch: 8, step: 24
	action: tensor([[-0.9045, -0.2319, -0.2479, -0.2699,  0.9290,  0.4377,  0.2371]],
       dtype=torch.float64)
	q_value: tensor([[-35.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8982018188840368, distance: 1.5766231018357693 entropy -2.4894389630599325
epoch: 8, step: 25
	action: tensor([[-0.1002, -0.1708, -0.1339,  0.0800,  0.2495,  0.0138, -0.1126]],
       dtype=torch.float64)
	q_value: tensor([[-36.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06263398662386732, distance: 1.107927379057333 entropy -3.1572085814826765
epoch: 8, step: 26
	action: tensor([[-0.4078, -0.3928,  0.2679,  0.2264,  0.2145,  0.0458,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[-32.3439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3416767579850357, distance: 1.3255027757303655 entropy -3.0951501721315844
epoch: 8, step: 27
	action: tensor([[-0.4190, -0.1738, -0.8730, -0.0639, -0.9580,  0.2744,  0.1055]],
       dtype=torch.float64)
	q_value: tensor([[-36.4199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45471193448864367, distance: 1.3802100778991873 entropy -2.7355151802356383
epoch: 8, step: 28
	action: tensor([[-0.3519,  0.1995, -0.4442, -0.4363,  0.2942, -0.1446,  0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-34.6007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048509162222424296, distance: 1.1717711691010873 entropy -2.9535819320691954
epoch: 8, step: 29
	action: tensor([[-0.0927, -0.2962, -0.0180, -0.2380,  0.1921, -0.0401, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-26.3254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20833968853584883, distance: 1.2579147611747536 entropy -3.708324968775348
epoch: 8, step: 30
	action: tensor([[ 0.5353, -0.0850,  0.1096,  0.4074,  0.0922,  0.0861, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-31.7495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8283256547343459, distance: 0.47414305447831234 entropy -2.9511273330651053
epoch: 8, step: 31
	action: tensor([[-0.1372, -0.1900,  0.0341,  0.7360,  0.4142,  0.4341,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[-41.0193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.47414305447831234 entropy -2.527702304383778
epoch: 8, step: 32
	action: tensor([[-0.1244, -0.0444,  0.0344,  0.3399, -0.2539,  0.0755,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822425069059964, distance: 0.9694949713617433 entropy -3.357482864455475
epoch: 8, step: 33
	action: tensor([[ 0.3530, -0.5741, -0.5752, -0.6204,  0.0023, -0.0310,  0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-35.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17427232892372113, distance: 1.240055454054967 entropy -2.679634605681982
epoch: 8, step: 34
	action: tensor([[-0.4544,  0.0539, -0.1448, -0.5055, -0.3285,  0.3983, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-33.9982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3757494763134426, distance: 1.342228238074704 entropy -2.837426243204452
epoch: 8, step: 35
	action: tensor([[ 0.1180, -0.3722, -0.2023, -0.1837, -0.5464,  0.1110,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-31.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016609163763294577, distance: 1.1538084184292865 entropy -2.88736423746818
epoch: 8, step: 36
	action: tensor([[ 0.1186, -0.3528,  0.5722, -0.2344,  0.1414,  0.2766,  0.1009]],
       dtype=torch.float64)
	q_value: tensor([[-35.2901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014738858980756109, distance: 1.1358797847749251 entropy -2.562236303494415
epoch: 8, step: 37
	action: tensor([[-0.6090, -0.6053,  0.3963,  1.2901, -1.2654, -0.0942, -0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-40.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18800083589721317, distance: 1.031180007546178 entropy -2.3034794346435175
epoch: 8, step: 38
	action: tensor([[ 1.8664, -1.4154,  0.0609, -0.2299, -0.5385,  1.4321,  0.4701]],
       dtype=torch.float64)
	q_value: tensor([[-51.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.031180007546178 entropy -2.030910745851742
epoch: 8, step: 39
	action: tensor([[-0.3534,  0.0115, -0.4233,  0.3115,  0.1740,  0.0898,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09314615873579712, distance: 1.1964534595070637 entropy -3.357482864455475
epoch: 8, step: 40
	action: tensor([[ 0.2232, -0.1982, -0.2715, -0.1205, -0.0865,  0.0698,  0.0472]],
       dtype=torch.float64)
	q_value: tensor([[-30.7919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2731914946109377, distance: 0.9755885481357903 entropy -3.381954947812947
epoch: 8, step: 41
	action: tensor([[-7.5393e-01,  5.8844e-02,  2.9614e-01, -3.6468e-01,  1.7683e-01,
          4.1632e-01, -2.1560e-04]], dtype=torch.float64)
	q_value: tensor([[-33.1884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7216017836704862, distance: 1.5014920782520322 entropy -2.840133702259554
epoch: 8, step: 42
	action: tensor([[-0.0977,  0.4113, -0.0899,  0.1076, -0.0307,  0.0973,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[-34.2631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5014920782520322 entropy -2.7508299933391354
epoch: 8, step: 43
	action: tensor([[-0.4164,  0.1118, -0.1692,  0.1397, -0.0007, -0.0184,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1191063420452807, distance: 1.2105768698958832 entropy -3.357482864455475
epoch: 8, step: 44
	action: tensor([[ 0.5726, -0.2228, -0.0703, -0.0275,  0.5936,  0.1741,  0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-29.5679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439011923601687, distance: 0.7728338960161009 entropy -3.242989991100452
epoch: 8, step: 45
	action: tensor([[-0.2621, -0.3506, -0.1757, -0.2600,  0.4531, -0.3838, -0.1713]],
       dtype=torch.float64)
	q_value: tensor([[-38.7146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49780922548138307, distance: 1.4005059036590108 entropy -2.634583494173943
epoch: 8, step: 46
	action: tensor([[-0.2330,  0.1751, -0.0046, -0.1037,  0.2954,  0.0440, -0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-29.4230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08316635273006479, distance: 1.095725994836438 entropy -3.436189443380234
epoch: 8, step: 47
	action: tensor([[-0.1985, -0.3009,  0.3618,  0.1221,  0.2724, -0.0149, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-30.4624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11943919909107747, distance: 1.2107568881245112 entropy -3.190397175859563
epoch: 8, step: 48
	action: tensor([[ 0.5327, -0.0927, -0.3170, -0.1637,  0.6022,  0.1803,  0.0523]],
       dtype=torch.float64)
	q_value: tensor([[-36.5732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5852787945625513, distance: 0.7369445549924347 entropy -2.698896833347331
epoch: 8, step: 49
	action: tensor([[-0.1781,  0.0692,  0.0325,  0.7491,  0.0901,  0.1443, -0.1760]],
       dtype=torch.float64)
	q_value: tensor([[-35.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7369445549924347 entropy -2.815977923374574
epoch: 8, step: 50
	action: tensor([[0.1980, 0.0208, 0.2516, 0.6664, 0.0492, 0.1531, 0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8010295742996096, distance: 0.5104473550842744 entropy -3.357482864455475
epoch: 8, step: 51
	action: tensor([[ 0.3849,  0.1063, -0.1473, -0.1719,  0.8178,  0.6973,  0.1059]],
       dtype=torch.float64)
	q_value: tensor([[-41.6430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8370434156159657, distance: 0.46194753057016175 entropy -2.4995438165331447
epoch: 8, step: 52
	action: tensor([[-0.0669,  0.0964, -0.2144, -0.2542,  0.1438,  0.0737,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21211919418720204, distance: 1.0157502864625312 entropy -3.357482864455475
epoch: 8, step: 53
	action: tensor([[ 0.1221, -0.3605,  0.2611,  0.3569, -0.2272, -0.0042, -0.0312]],
       dtype=torch.float64)
	q_value: tensor([[-29.7322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3453652896541295, distance: 0.9258832080524626 entropy -3.1482371978325645
epoch: 8, step: 54
	action: tensor([[-0.4028,  0.0511, -0.1013,  0.6973,  0.8913,  0.5358,  0.1553]],
       dtype=torch.float64)
	q_value: tensor([[-39.5588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9258832080524626 entropy -2.4671198775428107
epoch: 8, step: 55
	action: tensor([[ 0.1849, -0.1206, -0.0379, -0.0152,  0.0699, -0.1492,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32838394923970227, distance: 0.9378151117895843 entropy -3.357482864455475
epoch: 8, step: 56
	action: tensor([[-0.3714, -0.0513, -0.3216,  0.1896,  0.1833,  0.1627, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-32.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08493964240941909, distance: 1.1919539641087191 entropy -2.8806519946109206
epoch: 8, step: 57
	action: tensor([[ 0.0871, -0.2744, -0.0215, -0.4728,  0.2357,  0.0483,  0.0485]],
       dtype=torch.float64)
	q_value: tensor([[-31.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08218195841443365, distance: 1.1904381545499634 entropy -3.2848288672742703
epoch: 8, step: 58
	action: tensor([[ 0.3322, -0.1468,  0.1308, -0.0229, -0.2298,  0.3273, -0.0974]],
       dtype=torch.float64)
	q_value: tensor([[-32.5704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5373005931427965, distance: 0.7784059817433272 entropy -2.7994502536758072
epoch: 8, step: 59
	action: tensor([[-0.1406, -0.5201,  0.3275,  0.8668, -0.8556, -0.0763,  0.0671]],
       dtype=torch.float64)
	q_value: tensor([[-38.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38584342414655903, distance: 0.8968013356385676 entropy -2.410469907322199
epoch: 8, step: 60
	action: tensor([[ 0.5631, -0.7341, -0.9478, -0.5520,  0.1771,  0.5172,  0.3135]],
       dtype=torch.float64)
	q_value: tensor([[-45.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06703899617965758, distance: 1.1820799172596117 entropy -2.211363055149145
epoch: 8, step: 61
	action: tensor([[-0.5671, -0.3638,  0.3224, -0.1503, -0.0456,  0.2473, -0.2246]],
       dtype=torch.float64)
	q_value: tensor([[-40.5380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6655546962582772, distance: 1.4768491689251328 entropy -2.5745028691975342
epoch: 8, step: 62
	action: tensor([[-0.7855,  0.0464,  0.1467, -0.0069, -0.3883,  0.5645,  0.1654]],
       dtype=torch.float64)
	q_value: tensor([[-35.4313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5196517345124376, distance: 1.410680711923385 entropy -2.6010623755938647
epoch: 8, step: 63
	action: tensor([[-0.8632, -0.3989,  0.6393, -0.1059, -0.2083,  0.1802,  0.2366]],
       dtype=torch.float64)
	q_value: tensor([[-35.4435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0664378637127432, distance: 1.6450074483709685 entropy -2.489529716458073
LOSS epoch 8 actor 490.08645025877007 critic 276.1476340056695
epoch: 9, step: 0
	action: tensor([[-0.6631, -0.2176, -0.4014,  1.1902,  1.1854,  0.5112,  0.2467]],
       dtype=torch.float64)
	q_value: tensor([[-38.8042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0023948207019911516, distance: 1.142973182985799 entropy -2.3239729130851607
epoch: 9, step: 1
	action: tensor([[-0.5051, -0.1739, -0.4879, -0.3175,  0.4648, -0.0903, -0.0407]],
       dtype=torch.float64)
	q_value: tensor([[-48.5597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4489818793793936, distance: 1.3774890982348933 entropy -2.9762028915642946
epoch: 9, step: 2
	action: tensor([[-0.0491,  0.0552, -0.0210, -0.0282,  0.0297, -0.0117, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[-29.3336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2767712352112529, distance: 0.9731830556689733 entropy -3.713297708742975
epoch: 9, step: 3
	action: tensor([[ 0.5638, -0.1200,  0.3347, -0.2511, -0.3488,  0.0671,  0.0387]],
       dtype=torch.float64)
	q_value: tensor([[-32.3033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.473335058870589, distance: 0.8304699031935844 entropy -2.9786723756608935
epoch: 9, step: 4
	action: tensor([[ 0.8134,  0.7406,  0.4876,  0.1539, -0.5079,  0.2729, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-40.3752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9395044158096459, distance: 0.28146119728640734 entropy -2.312908156123837
epoch: 9, step: 5
	action: tensor([[-0.2881, -0.4206,  1.5183,  0.5761, -1.8185,  0.5642,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[-45.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18107278917748093, distance: 1.0355697223430278 entropy -2.1328005337632745
epoch: 9, step: 6
	action: tensor([[-1.8068, -0.9011, -3.4602,  1.2420,  0.6438,  2.1653,  0.6397]],
       dtype=torch.float64)
	q_value: tensor([[-63.4505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0355697223430278 entropy -1.5230827203650203
epoch: 9, step: 7
	action: tensor([[ 0.1007,  0.1303, -0.0648, -0.0058,  0.0700,  0.1130,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5285561318385513, distance: 0.7857270215297981 entropy -3.3562039111402986
epoch: 9, step: 8
	action: tensor([[-0.2624,  0.2127, -0.2340,  0.4976,  0.1219,  0.1605,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-33.9057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7857270215297981 entropy -2.8642420550822503
epoch: 9, step: 9
	action: tensor([[ 0.1524, -0.0408, -0.0350,  0.1052,  0.0235, -0.0761,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7857270215297981 entropy -3.3562039111402986
epoch: 9, step: 10
	action: tensor([[ 0.0547, -0.0480, -0.2601,  0.2786,  0.0851, -0.0449,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41125698296712654, distance: 0.8780506683586284 entropy -3.3562039111402986
epoch: 9, step: 11
	action: tensor([[-0.0780, -0.0389,  0.0465,  0.3187,  0.5845,  0.1327,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-33.7551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39193609534598994, distance: 0.8923419399728663 entropy -3.0181121170303613
epoch: 9, step: 12
	action: tensor([[-0.1478,  0.0460,  0.2927, -0.0270, -0.0656, -0.1497, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-37.4990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1081518738463273, distance: 1.0806925195961952 entropy -2.9273522755741936
epoch: 9, step: 13
	action: tensor([[-0.0774, -0.3950, -0.4768,  0.2010,  0.1488,  0.4164,  0.0863]],
       dtype=torch.float64)
	q_value: tensor([[-33.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02466303382691548, distance: 1.1301446556017272 entropy -2.805200212525151
epoch: 9, step: 14
	action: tensor([[-0.4800,  0.4159, -0.0465, -0.1587, -0.5818,  0.0671,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-36.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1301446556017272 entropy -2.8761977433834707
epoch: 9, step: 15
	action: tensor([[-0.1830, -0.2037,  0.0595,  0.3304, -0.2577, -0.1618,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05952574486259343, distance: 1.1097627646210313 entropy -3.3562039111402986
epoch: 9, step: 16
	action: tensor([[-0.6385,  0.0785, -0.1408, -0.2249,  0.0686,  0.0801,  0.1596]],
       dtype=torch.float64)
	q_value: tensor([[-34.6839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5416560708968423, distance: 1.420857231592151 entropy -2.7637683150061365
epoch: 9, step: 17
	action: tensor([[-0.1432,  0.1626,  0.2111, -0.2199, -0.3043,  0.1934,  0.0384]],
       dtype=torch.float64)
	q_value: tensor([[-30.4468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13131546809860306, distance: 1.0665660057402286 entropy -3.332922053219933
epoch: 9, step: 18
	action: tensor([[ 0.3244, -0.4120,  0.5087,  0.3741, -0.6257,  0.5246,  0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-34.1571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6085835945418234, distance: 0.7159393043871063 entropy -2.615465711823932
epoch: 9, step: 19
	action: tensor([[ 0.3361, -0.0498, -0.1115,  1.7481, -1.7253,  1.3896,  0.2250]],
       dtype=torch.float64)
	q_value: tensor([[-49.6586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7159393043871063 entropy -2.0103761054701357
epoch: 9, step: 20
	action: tensor([[-0.1543,  0.0110, -0.0739, -0.3144,  0.0170, -0.0010,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029350309019092302, distance: 1.1610162354717386 entropy -3.3562039111402986
epoch: 9, step: 21
	action: tensor([[ 0.4767,  0.2531, -0.3256,  0.2476, -0.1764, -0.0021,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-31.0064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1610162354717386 entropy -3.0338050795669464
epoch: 9, step: 22
	action: tensor([[-0.4448, -0.2320,  0.1937, -0.1599,  0.2582,  0.1058,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5098930408184439, distance: 1.4061439576444539 entropy -3.3562039111402986
epoch: 9, step: 23
	action: tensor([[-0.1913,  0.4340, -0.0135,  0.0228, -0.2487, -0.0406,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-33.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4061439576444539 entropy -2.884946963640765
epoch: 9, step: 24
	action: tensor([[-0.1349,  0.2946,  0.0823, -0.1194, -0.1176,  0.0092,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26027212507160813, distance: 0.9842211320533347 entropy -3.3562039111402986
epoch: 9, step: 25
	action: tensor([[-0.1113, -0.0483, -0.0274, -0.2439, -0.3522,  0.0729,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012150017668800861, distance: 1.137371106850969 entropy -3.3562039111402986
epoch: 9, step: 26
	action: tensor([[-0.6608, -0.3597, -0.0274,  0.2926,  0.4138,  0.4274,  0.0995]],
       dtype=torch.float64)
	q_value: tensor([[-32.6757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47401553751475634, distance: 1.3893373910522662 entropy -2.770416949629266
epoch: 9, step: 27
	action: tensor([[ 0.2659, -0.7670, -0.0496, -0.0253,  0.5743,  0.3733,  0.0526]],
       dtype=torch.float64)
	q_value: tensor([[-37.8279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08173239487360662, distance: 1.190190861023868 entropy -2.88109554974073
epoch: 9, step: 28
	action: tensor([[-0.3903,  0.0886, -0.4886, -0.9073,  0.1913, -0.2491, -0.1402]],
       dtype=torch.float64)
	q_value: tensor([[-42.8138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11063234561747537, distance: 1.2059848498897974 entropy -2.482824841231332
epoch: 9, step: 29
	action: tensor([[-0.0248,  0.0479, -0.0466, -0.1663, -0.1271,  0.0138, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-28.4385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21207698659760765, distance: 1.0157774934948267 entropy -3.641992463762265
epoch: 9, step: 30
	action: tensor([[-0.5956, -0.8107, -0.1901, -0.2720,  0.0318,  0.1045,  0.0480]],
       dtype=torch.float64)
	q_value: tensor([[-31.8837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.007282004633176, distance: 1.6212906900053843 entropy -2.920754581217778
epoch: 9, step: 31
	action: tensor([[ 0.2428,  0.2285, -0.5217,  0.1721, -0.2136, -0.0591,  0.0671]],
       dtype=torch.float64)
	q_value: tensor([[-35.3633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6212906900053843 entropy -2.888223086498141
epoch: 9, step: 32
	action: tensor([[-0.0026, -0.3264,  0.0877,  0.0992,  0.1516,  0.0940,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1071030828450319, distance: 1.0813277665254302 entropy -3.3562039111402986
epoch: 9, step: 33
	action: tensor([[ 0.0129, -0.3519, -0.2087, -0.7449,  0.0811,  0.1673,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-36.9805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24022312809758306, distance: 1.2744024750485239 entropy -2.69844275006589
epoch: 9, step: 34
	action: tensor([[-0.4748, -0.1340, -0.5735,  1.1465, -0.4428,  0.0944, -0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-33.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20956198548129223, distance: 1.2585508226875173 entropy -2.8064856933896154
epoch: 9, step: 35
	action: tensor([[-0.2627, -0.0081,  0.1041,  0.7427, -0.4791,  0.4889,  0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-39.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2585508226875173 entropy -2.792586121387035
epoch: 9, step: 36
	action: tensor([[ 0.0733, -0.2419,  0.0776,  0.3259, -0.0143,  0.1987,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4269984376782474, distance: 0.8662327441630614 entropy -3.3562039111402986
epoch: 9, step: 37
	action: tensor([[-0.0096, -0.2918,  0.0106, -0.0951, -0.5232,  0.1599,  0.1092]],
       dtype=torch.float64)
	q_value: tensor([[-39.3733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013925551665948888, distance: 1.1363485075991158 entropy -2.5692069660464694
epoch: 9, step: 38
	action: tensor([[-0.0045, -0.2592, -0.6656, -0.4810, -0.9213,  0.5446,  0.1601]],
       dtype=torch.float64)
	q_value: tensor([[-36.6445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07353675977656349, distance: 1.185673608832247 entropy -2.4790327235731873
epoch: 9, step: 39
	action: tensor([[0.7355, 0.0425, 0.0095, 0.6640, 0.6107, 0.4074, 0.0895]],
       dtype=torch.float64)
	q_value: tensor([[-37.3181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05064890291652174 entropy -2.4843277662877337
epoch: 9, step: 40
	action: tensor([[-0.4033, -0.1434, -0.0204, -0.2104,  0.0934,  0.1965,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3761917929018126, distance: 1.3424439903296506 entropy -3.3562039111402986
epoch: 9, step: 41
	action: tensor([[-0.3591,  0.0516,  0.1421,  0.4430,  0.3693,  0.4747,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-32.7475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3424439903296506 entropy -2.9298333090148803
epoch: 9, step: 42
	action: tensor([[-0.1858, -0.0857, -0.2246, -0.0051,  0.2109,  0.1591,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02445218519668968, distance: 1.130266806501301 entropy -3.3562039111402986
epoch: 9, step: 43
	action: tensor([[-0.1639, -0.1002, -0.3589, -0.1206,  0.0761,  0.0109,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[-32.5667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024720129869800767, distance: 1.1584020758639517 entropy -3.0961812611694888
epoch: 9, step: 44
	action: tensor([[ 0.3773, -0.0442, -0.0711, -0.3286, -0.0867, -0.0087,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[-30.0410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38953667869691677, distance: 0.8941007946392976 entropy -3.336814351233087
epoch: 9, step: 45
	action: tensor([[-0.5198, -0.0108, -0.5044, -0.0085,  0.6565,  0.0188, -0.0546]],
       dtype=torch.float64)
	q_value: tensor([[-34.3035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3840308962674339, distance: 1.3462619950730237 entropy -2.7269813773342415
epoch: 9, step: 46
	action: tensor([[-0.1548, -0.0150, -0.2818,  0.1311, -0.3211, -0.0866, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[-31.0868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11214182047904053, distance: 1.078272410825747 entropy -3.6681954533465624
epoch: 9, step: 47
	action: tensor([[ 0.4415,  0.1523,  0.0597, -0.6866, -0.0610,  0.2701,  0.1016]],
       dtype=torch.float64)
	q_value: tensor([[-30.6302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4738479910487521, distance: 0.8300653969596649 entropy -3.0815257357725594
epoch: 9, step: 48
	action: tensor([[-0.3365, -0.4662,  0.1065, -0.4944, -0.1146,  0.4170, -0.1355]],
       dtype=torch.float64)
	q_value: tensor([[-36.2193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6366837390559461, distance: 1.463993259201607 entropy -2.5056852766661244
epoch: 9, step: 49
	action: tensor([[ 1.0927,  0.3675,  0.6719, -0.5522,  0.3513,  0.0093,  0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-36.7169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8263201748066231, distance: 0.4769044555767639 entropy -2.5268432013438487
epoch: 9, step: 50
	action: tensor([[-1.8502,  0.3418,  0.5652, -0.2825,  1.2500,  1.1647, -0.3375]],
       dtype=torch.float64)
	q_value: tensor([[-47.2574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4769044555767639 entropy -2.1553396385073014
epoch: 9, step: 51
	action: tensor([[ 0.1197,  0.1415,  0.1530,  0.1276, -0.0635,  0.1559,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5723452556776562, distance: 0.7483475487729568 entropy -3.3562039111402986
epoch: 9, step: 52
	action: tensor([[ 0.2313, -0.0702, -0.0881,  0.0782,  0.4104,  0.0405,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5206289181406548, distance: 0.7923053875511225 entropy -3.3562039111402986
epoch: 9, step: 53
	action: tensor([[-0.0606, -0.3420,  0.2278,  0.3340,  0.2894,  0.3516, -0.0630]],
       dtype=torch.float64)
	q_value: tensor([[-35.7785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3356743464594567, distance: 0.9327112283213802 entropy -2.889347215957474
epoch: 9, step: 54
	action: tensor([[ 0.5178,  0.0795, -0.1514, -0.5850, -1.3594,  0.0782,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-41.7796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4686652304038016, distance: 0.8341435796655197 entropy -2.550044973724097
epoch: 9, step: 55
	action: tensor([[-1.5869, -1.8734,  0.0387,  0.8222,  1.1153,  0.8714,  0.0514]],
       dtype=torch.float64)
	q_value: tensor([[-40.3527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8341435796655197 entropy -2.252798458043307
epoch: 9, step: 56
	action: tensor([[ 0.1342, -0.0528, -0.1427, -0.1884,  0.1747, -0.0209,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.291187796569348, distance: 0.9634347046049042 entropy -3.3562039111402986
epoch: 9, step: 57
	action: tensor([[-0.2234,  0.2371,  0.2363,  0.0395, -0.2629, -0.0821, -0.0452]],
       dtype=torch.float64)
	q_value: tensor([[-32.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9634347046049042 entropy -2.9802883929432604
epoch: 9, step: 58
	action: tensor([[ 0.2046, -0.3749,  0.0482, -0.0384,  0.0986,  0.0804,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-34.4646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16333971822230675, distance: 1.0467218005726258 entropy -3.3562039111402986
epoch: 9, step: 59
	action: tensor([[-1.0512e+00,  3.3654e-01, -1.5567e-01,  4.8088e-01, -5.1261e-01,
          3.0533e-01, -7.0825e-04]], dtype=torch.float64)
	q_value: tensor([[-37.7373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4677186395329094, distance: 1.3863666356995732 entropy -2.6208646516294567
epoch: 9, step: 60
	action: tensor([[-0.1137, -0.3201, -0.3101, -0.2545,  0.3907,  0.2117,  0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-34.8976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09621866709086757, distance: 1.1981337168680373 entropy -2.837756353989088
epoch: 9, step: 61
	action: tensor([[-0.6482, -0.0272,  0.1208, -0.2947,  0.3403, -0.2055, -0.0803]],
       dtype=torch.float64)
	q_value: tensor([[-33.6733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7192007178905122, distance: 1.500444670201939 entropy -3.014971566861619
epoch: 9, step: 62
	action: tensor([[-0.1553, -0.0144,  0.1667, -0.0011, -0.0373,  0.0139,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[-31.1523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12549424985300828, distance: 1.0701336684140488 entropy -3.394675565529183
epoch: 9, step: 63
	action: tensor([[-0.2303,  0.0427,  0.3051,  0.2150, -0.1520,  0.4497,  0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-33.7709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0701336684140488 entropy -2.805489233212974
LOSS epoch 9 actor 634.0636124379188 critic 1255.8461157054578
epoch: 10, step: 0
	action: tensor([[-0.0264,  0.0866, -0.1507,  0.1027,  0.1087, -0.1552,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3235063340095571, distance: 0.941214394808011 entropy -3.355396238074402
epoch: 10, step: 1
	action: tensor([[-0.2718, -0.1586, -0.1930,  0.0876, -0.1559,  0.0905,  0.0238]],
       dtype=torch.float64)
	q_value: tensor([[-28.4273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10497274844611515, distance: 1.2029081765163063 entropy -3.149378031862956
epoch: 10, step: 2
	action: tensor([[ 0.0125,  0.4628,  0.7226, -0.2094,  0.4852,  0.2400,  0.1157]],
       dtype=torch.float64)
	q_value: tensor([[-28.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2029081765163063 entropy -2.968007601405558
epoch: 10, step: 3
	action: tensor([[ 0.0370,  0.2158,  0.1979,  0.2517, -0.2148, -0.0591,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.593783414495654, distance: 0.7293492134510341 entropy -3.355396238074402
epoch: 10, step: 4
	action: tensor([[-0.1477, -0.1175,  0.0412, -0.2143, -0.6146,  0.2073,  0.1109]],
       dtype=torch.float64)
	q_value: tensor([[-32.1755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7293492134510341 entropy -2.6853424414677347
epoch: 10, step: 5
	action: tensor([[ 0.0242,  0.0993,  0.2115,  0.0452,  0.1624, -0.0569,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4103956753028075, distance: 0.8786927102612012 entropy -3.355396238074402
epoch: 10, step: 6
	action: tensor([[-0.2416, -0.2789,  0.4108,  0.2061, -0.6048,  0.3716,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-31.6052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04278292380454407, distance: 1.1195974794181733 entropy -2.821696418878299
epoch: 10, step: 7
	action: tensor([[ 2.7787e-04, -7.0615e-01, -9.8728e-01, -2.4862e-01, -2.7010e-01,
          9.7905e-01,  3.1894e-01]], dtype=torch.float64)
	q_value: tensor([[-36.4282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34430697836079927, distance: 1.3268013960558218 entropy -2.2043351234869717
epoch: 10, step: 8
	action: tensor([[ 0.3644, -0.0375, -0.0719, -0.3407, -0.9335,  0.5083, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[-36.5827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41191522353616017, distance: 0.8775596814646921 entropy -2.482550807890246
epoch: 10, step: 9
	action: tensor([[ 1.3707, -0.4470, -0.1301, -0.4708,  0.4063,  0.4403,  0.1344]],
       dtype=torch.float64)
	q_value: tensor([[-34.5263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03239903957034351, distance: 1.1627343136586386 entropy -2.184237582247224
epoch: 10, step: 10
	action: tensor([[ 0.2396, -0.3492,  0.2844,  0.1895, -0.5124, -0.1410, -0.3685]],
       dtype=torch.float64)
	q_value: tensor([[-44.5069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090156820778429, distance: 0.9512414993918641 entropy -2.126161364004184
epoch: 10, step: 11
	action: tensor([[ 0.0163,  0.5152, -0.5341,  0.1340, -0.3178,  0.1616,  0.1747]],
       dtype=torch.float64)
	q_value: tensor([[-35.4076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9512414993918641 entropy -2.3893038884230555
epoch: 10, step: 12
	action: tensor([[-0.1189, -0.2046,  0.1085, -0.1827,  0.1985, -0.0789,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1487538535056112, distance: 1.226507431267688 entropy -3.355396238074402
epoch: 10, step: 13
	action: tensor([[-0.2365, -0.2119, -0.2548, -0.1715,  0.3158, -0.2299, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-29.7786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26542473613947215, distance: 1.287285428032488 entropy -2.9229528834597667
epoch: 10, step: 14
	action: tensor([[ 0.1788,  0.2360, -0.1340,  0.1986,  0.2104,  0.0538, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[-26.9264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.287285428032488 entropy -3.4783348980280784
epoch: 10, step: 15
	action: tensor([[-0.0945,  0.1109, -0.2287,  0.0149, -0.2205,  0.0345,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2545162791341745, distance: 0.9880428400942167 entropy -3.355396238074402
epoch: 10, step: 16
	action: tensor([[ 0.0469,  0.1149,  0.2023, -0.1129, -0.1878,  0.1101,  0.0753]],
       dtype=torch.float64)
	q_value: tensor([[-27.6604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3651113413426579, distance: 0.9118123660440831 entropy -3.0066408154908677
epoch: 10, step: 17
	action: tensor([[-0.5195, -0.4162,  0.3497,  0.3347, -0.6528,  0.6250,  0.0781]],
       dtype=torch.float64)
	q_value: tensor([[-31.3716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2811612197156437, distance: 1.2952648657900685 entropy -2.6179421979795996
epoch: 10, step: 18
	action: tensor([[-1.2662, -0.1690,  0.7297, -0.1853, -1.4084,  0.6034,  0.3975]],
       dtype=torch.float64)
	q_value: tensor([[-38.0892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5072635435618995, distance: 1.8119937119421405 entropy -2.113566424833164
epoch: 10, step: 19
	action: tensor([[ 1.7275,  1.5146, -0.4848, -0.5604, -1.8139,  0.4398,  0.5782]],
       dtype=torch.float64)
	q_value: tensor([[-41.3200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8119937119421405 entropy -1.8584315467406785
epoch: 10, step: 20
	action: tensor([[ 0.0102, -0.0505,  0.1669, -0.0346, -0.0087, -0.1267,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20867029931601588, distance: 1.017971047853768 entropy -3.355396238074402
epoch: 10, step: 21
	action: tensor([[ 0.1812, -0.1811, -0.3243, -0.5356, -0.5351,  0.0728,  0.0490]],
       dtype=torch.float64)
	q_value: tensor([[-30.3914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11771645752700721, distance: 1.0748819805984582 entropy -2.8282925555078413
epoch: 10, step: 22
	action: tensor([[-0.3246, -0.7006,  0.4918, -0.4624,  0.5751,  0.4475,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-29.0599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7374524713397221, distance: 1.50838831649999 entropy -2.719707827098567
epoch: 10, step: 23
	action: tensor([[ 0.1807,  0.1792, -0.7420, -0.6000, -0.6013, -0.1421, -0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-37.5131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5882607145292218, distance: 0.7342903935443025 entropy -2.3433576444672672
epoch: 10, step: 24
	action: tensor([[-0.2207, -0.4881,  0.1395, -0.1223,  0.1327, -0.1083, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[-25.0942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45394528596055816, distance: 1.379846337337039 entropy -3.3337962856283276
epoch: 10, step: 25
	action: tensor([[-0.3917, -0.7009,  0.4275,  0.1673,  0.1237,  0.3011,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-30.9747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42749037005249213, distance: 1.3672353760064464 entropy -2.8110240732106506
epoch: 10, step: 26
	action: tensor([[-1.5465, -0.1799, -0.0854, -0.2931, -0.5615,  0.2167,  0.1564]],
       dtype=torch.float64)
	q_value: tensor([[-37.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7891394267852196, distance: 1.9111369343553712 entropy -2.380646387269415
epoch: 10, step: 27
	action: tensor([[-0.0429,  0.1566,  0.1041,  0.3604,  0.1206,  0.1412,  0.1743]],
       dtype=torch.float64)
	q_value: tensor([[-32.1022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9111369343553712 entropy -2.939131191310738
epoch: 10, step: 28
	action: tensor([[-0.3578, -0.2053,  0.0150, -0.0547, -0.0863, -0.0554,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36232070072522693, distance: 1.335661387563491 entropy -3.355396238074402
epoch: 10, step: 29
	action: tensor([[0.0109, 0.2625, 0.3902, 0.2913, 0.0714, 0.0460, 0.1049]],
       dtype=torch.float64)
	q_value: tensor([[-28.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.335661387563491 entropy -2.9449816425452897
epoch: 10, step: 30
	action: tensor([[-0.0138,  0.1570,  0.0452, -0.2094,  0.4743, -0.1179,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2443105710093595, distance: 0.9947830272263111 entropy -3.355396238074402
epoch: 10, step: 31
	action: tensor([[-0.3760, -0.3547,  0.1084, -0.1424,  0.4862,  0.1809, -0.0925]],
       dtype=torch.float64)
	q_value: tensor([[-30.2850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4377524207603416, distance: 1.3721410178080922 entropy -3.0815321699960307
epoch: 10, step: 32
	action: tensor([[ 0.3756,  0.0250, -0.0949, -0.3845, -0.0412,  0.2126, -0.0307]],
       dtype=torch.float64)
	q_value: tensor([[-32.4083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49131240762511996, distance: 0.8161730788042634 entropy -2.8933489723687416
epoch: 10, step: 33
	action: tensor([[ 0.1095, -0.0027, -0.3853,  0.1511, -0.1466,  0.3796, -0.0648]],
       dtype=torch.float64)
	q_value: tensor([[-31.3726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44924944945192036, distance: 0.8492472774163564 entropy -2.667054303783458
epoch: 10, step: 34
	action: tensor([[ 0.6393, -0.1763, -0.1711,  0.2252,  0.2357,  0.3296,  0.0661]],
       dtype=torch.float64)
	q_value: tensor([[-31.1671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7755754342440692, distance: 0.5421155815898454 entropy -2.8021011234066044
epoch: 10, step: 35
	action: tensor([[ 0.1844, -0.4490, -0.6391,  0.0383, -0.0614,  0.3449, -0.0698]],
       dtype=torch.float64)
	q_value: tensor([[-38.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10628891503643367, distance: 1.0818206462988018 entropy -2.5285781173260404
epoch: 10, step: 36
	action: tensor([[-0.1390, -0.3889,  0.1715,  0.3897,  0.0911,  0.1251,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[-31.5677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12670515946124894, distance: 1.0693925160370177 entropy -2.8165459232414762
epoch: 10, step: 37
	action: tensor([[ 0.1262, -0.0305,  0.9969, -0.8983, -0.4164,  0.7666,  0.1395]],
       dtype=torch.float64)
	q_value: tensor([[-35.3161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07384439561715939, distance: 1.1012823344620002 entropy -2.619010897478088
epoch: 10, step: 38
	action: tensor([[ 0.6631, -1.1406, -0.7213,  0.0563,  1.9367, -0.0065,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-42.1244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5152571609180729, distance: 1.4086395111612218 entropy -1.862357466816364
epoch: 10, step: 39
	action: tensor([[-0.5513,  0.1408, -0.0366, -0.5489,  0.8005, -0.0498, -0.3428]],
       dtype=torch.float64)
	q_value: tensor([[-47.5235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5000664923440183, distance: 1.4015608195149258 entropy -2.4609448373393614
epoch: 10, step: 40
	action: tensor([[ 0.1016, -0.1374, -0.0881,  0.2221, -0.0425,  0.1841, -0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-31.5787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4189512368909927, distance: 0.8722941979553238 entropy -3.3421101766094026
epoch: 10, step: 41
	action: tensor([[-0.1074, -0.1288,  0.3841, -0.3047, -0.3126,  0.4686,  0.0909]],
       dtype=torch.float64)
	q_value: tensor([[-32.8167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0038594887759191288, distance: 1.1465504192844502 entropy -2.729249104901807
epoch: 10, step: 42
	action: tensor([[-0.6836,  0.2159,  0.6381,  0.3424, -1.3915,  0.3519,  0.1515]],
       dtype=torch.float64)
	q_value: tensor([[-34.5569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12760468004224368, distance: 1.2151646528572013 entropy -2.284460724999184
epoch: 10, step: 43
	action: tensor([[-2.4853, -0.2138, -1.2619, -0.5014,  1.4054,  1.2799,  0.5071]],
       dtype=torch.float64)
	q_value: tensor([[-39.6722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2151646528572013 entropy -1.9480472666363566
epoch: 10, step: 44
	action: tensor([[-0.3763, -0.0507,  0.0915, -0.0952, -0.1898, -0.0283,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2839700356524544, distance: 1.2966839567873287 entropy -3.355396238074402
epoch: 10, step: 45
	action: tensor([[-0.1580, -0.4083, -0.4652,  0.2503,  0.0281,  0.2438,  0.1197]],
       dtype=torch.float64)
	q_value: tensor([[-28.3358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1565373800939016, distance: 1.230655594408768 entropy -2.91303578402555
epoch: 10, step: 46
	action: tensor([[ 0.1450, -0.1917,  0.4352, -0.4343,  0.4985,  0.2521,  0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-30.7745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.034039255249055156, distance: 1.1246993188156915 entropy -2.941729432511448
epoch: 10, step: 47
	action: tensor([[ 0.2244, -0.5871, -0.0177,  0.9954, -0.5010, -0.4435, -0.1791]],
       dtype=torch.float64)
	q_value: tensor([[-36.0718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5173845995029228, distance: 0.7949819743374645 entropy -2.4581110892609668
epoch: 10, step: 48
	action: tensor([[ 0.0339, -0.9664, -0.1871,  0.1118, -1.2464,  0.0849,  0.1963]],
       dtype=torch.float64)
	q_value: tensor([[-38.9066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5542488664561744, distance: 1.426648463253771 entropy -2.4590863570078776
epoch: 10, step: 49
	action: tensor([[-0.9292,  0.1173,  0.2640,  1.5491,  1.2092,  0.5345,  0.2096]],
       dtype=torch.float64)
	q_value: tensor([[-38.9534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.426648463253771 entropy -2.177130838159245
epoch: 10, step: 50
	action: tensor([[-0.1268, -0.1379,  0.0589,  0.1046,  0.0510,  0.0372,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09195861237643821, distance: 1.0904594365434892 entropy -3.355396238074402
epoch: 10, step: 51
	action: tensor([[ 0.2653, -0.1622, -0.2978, -0.7767, -0.3592,  0.1154,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-30.9317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07954426127489245, distance: 1.0978882778425074 entropy -2.8115751577353802
epoch: 10, step: 52
	action: tensor([[ 0.3472,  0.0762,  0.1084,  0.1117,  0.6681,  0.4947, -0.0680]],
       dtype=torch.float64)
	q_value: tensor([[-29.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8055872730924312, distance: 0.5045672278292087 entropy -2.69848708972658
epoch: 10, step: 53
	action: tensor([[-0.2157, -0.1480,  0.2258, -0.9495,  0.3974,  0.1768, -0.1457]],
       dtype=torch.float64)
	q_value: tensor([[-38.0567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.508677456115626, distance: 1.4055778144788704 entropy -2.6027703145360617
epoch: 10, step: 54
	action: tensor([[-0.0719, -0.5778, -0.6524,  0.2476, -0.3213,  0.0968, -0.1836]],
       dtype=torch.float64)
	q_value: tensor([[-32.6797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2780556211583216, distance: 1.2936940200117293 entropy -2.7046263508618735
epoch: 10, step: 55
	action: tensor([[ 0.6408, -0.3135, -0.1876, -0.0394, -0.0504,  0.0579,  0.1075]],
       dtype=torch.float64)
	q_value: tensor([[-29.9663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4355654406558531, distance: 0.8597327916330672 entropy -2.9107037965782787
epoch: 10, step: 56
	action: tensor([[ 0.2071, -0.4992, -0.8169,  0.0806,  0.4343,  0.2899, -0.0638]],
       dtype=torch.float64)
	q_value: tensor([[-35.4557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0922525286124446, distance: 1.090282941464653 entropy -2.5497887957424856
epoch: 10, step: 57
	action: tensor([[ 0.0910, -0.2591, -0.1506,  0.3728,  0.2257, -0.1272, -0.0707]],
       dtype=torch.float64)
	q_value: tensor([[-32.1380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29011357077444944, distance: 0.964164485060351 entropy -3.076776980614853
epoch: 10, step: 58
	action: tensor([[-0.0034, -0.1259,  0.1112, -0.0943,  0.2264,  0.2903,  0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-32.7501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2165439526935813, distance: 1.012894030982878 entropy -2.9664664126549796
epoch: 10, step: 59
	action: tensor([[ 0.5243,  0.3428,  0.2798,  0.3221,  0.6611,  0.3155, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[-32.8216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.012894030982878 entropy -2.682646687495794
epoch: 10, step: 60
	action: tensor([[ 0.0087,  0.1075, -0.1726,  0.0017,  0.1637, -0.1377,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-31.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3256611250330276, distance: 0.9397142049185008 entropy -3.355396238074402
epoch: 10, step: 61
	action: tensor([[-0.1343, -0.1882, -0.1967, -0.1261, -0.0580,  0.1702, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-28.2377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.046706840201030664, distance: 1.1707636350712822 entropy -3.1835469607068942
epoch: 10, step: 62
	action: tensor([[ 0.2451, -0.3607, -0.3202, -0.2671,  0.0217, -0.0263,  0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-28.9149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05619750919128652, distance: 1.111724694998086 entropy -2.931370794545688
epoch: 10, step: 63
	action: tensor([[ 0.2162, -0.0988, -0.4367,  0.6241,  0.0346,  0.2595, -0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-29.9782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.111724694998086 entropy -2.8853644441768247
LOSS epoch 10 actor 481.03444770406344 critic 206.10370329831107
epoch: 11, step: 0
	action: tensor([[ 0.0125,  0.0530,  0.1754, -0.0722, -0.1853,  0.0201,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058672150921532, distance: 0.9534062001724858 entropy -3.354116011491917
epoch: 11, step: 1
	action: tensor([[ 0.6829,  0.3374, -0.3589, -0.1869, -0.2118,  0.0990,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-27.8932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8939843618654405, distance: 0.37259897786388074 entropy -2.697559794787774
epoch: 11, step: 2
	action: tensor([[-0.1661, -0.4800,  0.4203, -1.0053, -0.3346,  0.4992, -0.0813]],
       dtype=torch.float64)
	q_value: tensor([[-28.5711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7354072031909522, distance: 1.5075002441437397 entropy -2.683061065506991
epoch: 11, step: 3
	action: tensor([[-0.5634,  0.8461,  1.3190, -0.4250,  0.0662,  0.8029,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-32.2060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5075002441437397 entropy -2.1698019949937017
epoch: 11, step: 4
	action: tensor([[-0.2459, -0.0838,  0.1087,  0.4669,  0.1350, -0.1580,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13653441558264057, distance: 1.0633572818235921 entropy -3.354116011491917
epoch: 11, step: 5
	action: tensor([[-0.2003, -0.3466, -0.5405, -0.2627,  0.1022,  0.2509,  0.1054]],
       dtype=torch.float64)
	q_value: tensor([[-29.5563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23928498615748905, distance: 1.2739203857533863 entropy -2.905455722426379
epoch: 11, step: 6
	action: tensor([[-0.3975, -0.0507,  0.2234, -0.0276,  0.5572, -0.0377, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[-25.4456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2908553234249951, distance: 1.3001560413283184 entropy -3.1738657547585083
epoch: 11, step: 7
	action: tensor([[ 0.0865,  0.0082,  0.2860, -0.1018,  0.4214,  0.0474, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-29.6289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30803788269211974, distance: 0.9519143037610074 entropy -3.04845248574284
epoch: 11, step: 8
	action: tensor([[-0.5007, -0.1241,  0.0141,  0.6422, -0.2226,  0.3809, -0.0711]],
       dtype=torch.float64)
	q_value: tensor([[-30.9626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9519143037610074 entropy -2.7469768562641
epoch: 11, step: 9
	action: tensor([[ 0.0190,  0.0870,  0.2204,  0.0586, -0.2522,  0.0398,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41020402687371305, distance: 0.8788355063604772 entropy -3.354116011491917
epoch: 11, step: 10
	action: tensor([[ 0.8850,  0.5944,  0.1365, -0.4171,  0.2693,  0.1802,  0.1160]],
       dtype=torch.float64)
	q_value: tensor([[-28.8034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9573597113939648, distance: 0.23630165373980408 entropy -2.6359019550749676
epoch: 11, step: 11
	action: tensor([[-0.2516,  0.2536,  0.2977, -0.3135,  0.0413, -0.1000, -0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-33.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.23630165373980408 entropy -2.4250902507367806
epoch: 11, step: 12
	action: tensor([[-0.3515, -0.3088,  0.0547, -0.0192,  0.0735,  0.0095,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40336210114341764, distance: 1.3556312299803304 entropy -3.354116011491917
epoch: 11, step: 13
	action: tensor([[-0.0616,  0.1497, -0.4306, -0.4927, -0.7676, -0.0506,  0.0803]],
       dtype=torch.float64)
	q_value: tensor([[-27.2546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24427478308323103, distance: 0.9948065824056188 entropy -2.8910924635729915
epoch: 11, step: 14
	action: tensor([[ 0.0126,  0.5054,  0.2970,  0.5390, -0.3021,  0.2897,  0.0519]],
       dtype=torch.float64)
	q_value: tensor([[-23.6130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9948065824056188 entropy -3.02255768076618
epoch: 11, step: 15
	action: tensor([[-0.0035,  0.0716,  0.0387, -0.1061,  0.4631,  0.1067,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32164417196438966, distance: 0.9425089296439824 entropy -3.354116011491917
epoch: 11, step: 16
	action: tensor([[ 0.1632,  0.1227,  0.0048,  0.2602,  0.0637, -0.0168, -0.0832]],
       dtype=torch.float64)
	q_value: tensor([[-28.9074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9425089296439824 entropy -2.947090456749957
epoch: 11, step: 17
	action: tensor([[ 0.2895,  0.2531, -0.2859,  0.0670, -0.0891,  0.1020,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7094609946093604, distance: 0.6168206661212915 entropy -3.354116011491917
epoch: 11, step: 18
	action: tensor([[ 0.1799, -0.0186, -0.2147, -0.1792,  0.1489,  0.1161,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6168206661212915 entropy -3.354116011491917
epoch: 11, step: 19
	action: tensor([[-0.0860,  0.0547,  0.2539,  0.0175, -0.2646,  0.0038,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23514544115600866, distance: 1.000797295495388 entropy -3.354116011491917
epoch: 11, step: 20
	action: tensor([[-0.0730, -0.5713,  0.4020,  0.3307, -0.5802,  0.0623,  0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-28.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0010262983300237671, distance: 1.1449313227101443 entropy -2.6520898670923567
epoch: 11, step: 21
	action: tensor([[-0.0567,  0.6379,  0.8121, -1.0320,  0.6085,  0.8093,  0.2568]],
       dtype=torch.float64)
	q_value: tensor([[-33.8386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1449313227101443 entropy -2.2438730907773863
epoch: 11, step: 22
	action: tensor([[-0.3117, -0.0461,  0.1947, -0.1146, -0.2280, -0.0496,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21903640858544815, distance: 1.2634702828183615 entropy -3.354116011491917
epoch: 11, step: 23
	action: tensor([[-0.6907, -0.1497, -0.2949, -0.4345,  1.0240,  0.2036,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-26.6060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6829159827738172, distance: 1.4845263504409763 entropy -2.8086802267008397
epoch: 11, step: 24
	action: tensor([[ 0.1967, -0.3565,  0.2310,  0.5704, -0.2280,  0.1084, -0.1170]],
       dtype=torch.float64)
	q_value: tensor([[-29.7802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5614257783891616, distance: 0.7578412556979918 entropy -3.1939840073488948
epoch: 11, step: 25
	action: tensor([[-0.3113, -0.6968,  0.1749, -0.3960, -0.6270,  0.1037,  0.1859]],
       dtype=torch.float64)
	q_value: tensor([[-35.0936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.843777043787753, distance: 1.553856450411767 entropy -2.386655102300595
epoch: 11, step: 26
	action: tensor([[ 0.5239, -1.5791, -0.8981, -0.5579, -0.1980, -0.0201,  0.1891]],
       dtype=torch.float64)
	q_value: tensor([[-29.6608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.553856450411767 entropy -2.3465675716226166
epoch: 11, step: 27
	action: tensor([[-0.0577, -0.0874,  0.2511,  0.0774, -0.0049,  0.1829,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2601033072144854, distance: 0.9843334332315099 entropy -3.354116011491917
epoch: 11, step: 28
	action: tensor([[-0.1249, -0.2418, -0.0197, -0.6512, -0.6714, -0.1028,  0.0914]],
       dtype=torch.float64)
	q_value: tensor([[-30.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32766426125794146, distance: 1.318562820234374 entropy -2.610726836154299
epoch: 11, step: 29
	action: tensor([[-0.8785,  0.4152, -0.2093, -0.3949, -0.4041,  0.0497,  0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-26.2744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.626057331488244, distance: 1.4592329250912823 entropy -2.6692830780434904
epoch: 11, step: 30
	action: tensor([[ 0.0722, -0.3160, -0.0872,  0.3008, -0.0562, -0.0322,  0.0684]],
       dtype=torch.float64)
	q_value: tensor([[-23.8604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24467395030147132, distance: 0.9945438237887726 entropy -3.4532412425147436
epoch: 11, step: 31
	action: tensor([[ 0.1804,  0.1756,  0.0227, -0.0632,  0.1598,  0.4585,  0.0973]],
       dtype=torch.float64)
	q_value: tensor([[-29.4112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9945438237887726 entropy -2.758497096114658
epoch: 11, step: 32
	action: tensor([[ 0.0368,  0.0019, -0.0145,  0.4799,  0.2985, -0.1229,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5003821521990386, distance: 0.8088642954429217 entropy -3.354116011491917
epoch: 11, step: 33
	action: tensor([[-0.0191,  0.3013, -0.2470, -0.0543, -0.0722,  0.1356,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-30.5707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8088642954429217 entropy -2.9317384675723264
epoch: 11, step: 34
	action: tensor([[ 0.2079, -0.0355, -0.0464,  0.0742, -0.1223, -0.0412,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5065616062369636, distance: 0.8038465689942412 entropy -3.354116011491917
epoch: 11, step: 35
	action: tensor([[ 0.2388, -0.0404,  0.0428, -0.1188, -0.0921,  0.4340,  0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-28.5292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5325421323420043, distance: 0.7823983575832429 entropy -2.7601354730665255
epoch: 11, step: 36
	action: tensor([[-0.2538,  0.5661, -1.3612,  1.6371, -1.2918, -0.1046,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[-30.5139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7823983575832429 entropy -2.479134835024125
epoch: 11, step: 37
	action: tensor([[ 0.0897, -0.2287, -0.0820,  0.2229,  0.3216,  0.2639,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39774893707282344, distance: 0.8880664859188958 entropy -3.354116011491917
epoch: 11, step: 38
	action: tensor([[ 0.3362, -0.7462,  0.2064,  0.0307, -0.2791, -0.0673,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[-30.7937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14565355155496884, distance: 1.2248512398863642 entropy -2.7636998192675706
epoch: 11, step: 39
	action: tensor([[-0.4179,  0.6734, -0.5073,  0.4818,  0.4830, -0.2938,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[-33.1248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2248512398863642 entropy -2.348342406428741
epoch: 11, step: 40
	action: tensor([[ 0.3560, -0.1487, -0.0342,  0.3857,  0.1866, -0.0336,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6660371712265618, distance: 0.6613110184554436 entropy -3.354116011491917
epoch: 11, step: 41
	action: tensor([[ 0.3437,  0.3902, -0.1553, -0.0864,  0.4394,  0.1723,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-32.3670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6613110184554436 entropy -2.705514499016228
epoch: 11, step: 42
	action: tensor([[-0.0240,  0.0348, -0.1660,  0.1800, -0.1028, -0.1397,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3318166877366059, distance: 0.9354153789826812 entropy -3.354116011491917
epoch: 11, step: 43
	action: tensor([[-0.1749, -0.1264, -0.0312, -0.0886, -0.2694,  0.0555,  0.0700]],
       dtype=torch.float64)
	q_value: tensor([[-26.1098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.039551301197340294, distance: 1.1667549617554362 entropy -3.0190938738331363
epoch: 11, step: 44
	action: tensor([[-0.1137, -0.3493, -0.2463, -0.1351,  0.4712,  0.7301,  0.1180]],
       dtype=torch.float64)
	q_value: tensor([[-26.3684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11132369256025132, distance: 1.0787690901698492 entropy -2.7939089251118263
epoch: 11, step: 45
	action: tensor([[ 1.0345,  0.5307, -0.7959, -0.4767, -0.3773, -0.1209, -0.1069]],
       dtype=torch.float64)
	q_value: tensor([[-30.3123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8214258679913234, distance: 0.48357736802854595 entropy -2.6885192436660037
epoch: 11, step: 46
	action: tensor([[ 0.5374,  0.1256, -0.0919,  0.5851,  0.5636,  0.2579, -0.1619]],
       dtype=torch.float64)
	q_value: tensor([[-28.4030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9434803358444195, distance: 0.2720548543550205 entropy -2.7502805557851357
epoch: 11, step: 47
	action: tensor([[-0.4546,  0.4614, -0.1969, -0.3455,  0.2361,  0.3744, -0.0537]],
       dtype=torch.float64)
	q_value: tensor([[-37.0188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.2720548543550205 entropy -2.674051238918724
epoch: 11, step: 48
	action: tensor([[-0.3480, -0.1831,  0.0387,  0.0177,  0.1466,  0.0313,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27476141800910914, distance: 1.292025688630659 entropy -3.354116011491917
epoch: 11, step: 49
	action: tensor([[ 0.1788, -0.1746,  0.0684, -0.2744,  0.2623,  0.2403,  0.0599]],
       dtype=torch.float64)
	q_value: tensor([[-27.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21710390543389224, distance: 1.0125319978036735 entropy -2.9546232068948197
epoch: 11, step: 50
	action: tensor([[-0.4204, -0.0316,  0.1045,  0.2013,  0.5298, -0.1957, -0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-29.7846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2362750649825207, distance: 1.272372424217298 entropy -2.655719634191642
epoch: 11, step: 51
	action: tensor([[-0.5532, -0.2369,  0.3614,  0.4867, -0.0282,  0.0265,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-29.1393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20984011997618146, distance: 1.2586955140224356 entropy -3.2516040227430656
epoch: 11, step: 52
	action: tensor([[-0.5430, -0.1548, -0.2387, -0.3495, -0.3106,  0.1353,  0.2134]],
       dtype=torch.float64)
	q_value: tensor([[-31.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6050612474077275, distance: 1.449781314010907 entropy -2.6173757715091734
epoch: 11, step: 53
	action: tensor([[-0.4178, -0.0184,  0.1438,  0.3955, -0.3210,  0.0636,  0.0947]],
       dtype=torch.float64)
	q_value: tensor([[-24.9250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006312080273789711, distance: 1.1479501691406242 entropy -3.031401884598447
epoch: 11, step: 54
	action: tensor([[ 0.5890, -0.4372,  0.5517, -0.3130, -0.4850,  0.5967,  0.2113]],
       dtype=torch.float64)
	q_value: tensor([[-28.6822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005957628883187, distance: 0.9570195789530346 entropy -2.6600463852961793
epoch: 11, step: 55
	action: tensor([[-0.5305, -1.1799, -0.0929,  0.8709, -0.9149,  0.3325,  0.0346]],
       dtype=torch.float64)
	q_value: tensor([[-38.2120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7486491678131202, distance: 1.5132407784452817 entropy -1.9491203532123236
epoch: 11, step: 56
	action: tensor([[ 0.4038,  1.8078,  0.4414, -0.7575,  0.4757,  0.7333,  0.3824]],
       dtype=torch.float64)
	q_value: tensor([[-38.6932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5132407784452817 entropy -2.1340033838237917
epoch: 11, step: 57
	action: tensor([[ 0.0066,  0.0673, -0.1697, -0.0401,  0.1612, -0.4282,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-28.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40501301309936233, distance: 0.8826945129411521 entropy -3.354116011491917
epoch: 11, step: 58
	action: tensor([[-0.2830, -0.1793, -0.2952,  0.4473, -0.1339,  0.1093, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-25.0064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16336170568123043, distance: 1.046708046541088 entropy -3.3341570352674217
epoch: 11, step: 59
	action: tensor([[-0.7763, -0.0873, -0.4416, -0.1796,  0.0332,  0.4386,  0.1375]],
       dtype=torch.float64)
	q_value: tensor([[-27.5034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5367613459482037, distance: 1.4185998427645894 entropy -2.9486063815240313
epoch: 11, step: 60
	action: tensor([[-0.1744,  0.0579,  0.3149,  0.0411,  0.0225, -0.0057,  0.0443]],
       dtype=torch.float64)
	q_value: tensor([[-25.6768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3407312074986455, distance: 0.9291545381129904 entropy -3.3421806741680986
epoch: 11, step: 61
	action: tensor([[-0.3218, -0.1351, -0.0911,  0.2770, -0.1542,  0.4053,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-29.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20057928857971374, distance: 1.0231619740610307 entropy -2.7366045413468645
epoch: 11, step: 62
	action: tensor([[ 0.0890, -0.1751, -0.2566, -0.3724, -0.0018,  0.2173,  0.1661]],
       dtype=torch.float64)
	q_value: tensor([[-28.8482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26781317204930355, distance: 0.9791915320813541 entropy -2.666775192868751
epoch: 11, step: 63
	action: tensor([[-0.2087, -0.0668, -0.4058, -0.3013, -0.1576,  0.2542, -0.0369]],
       dtype=torch.float64)
	q_value: tensor([[-26.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1392448556685334, distance: 1.0616870174788398 entropy -2.810530656994524
LOSS epoch 11 actor 352.86633112567415 critic 368.29359905644105
epoch: 12, step: 0
	action: tensor([[-0.2723,  0.0964,  0.3472,  0.2084,  0.1095,  0.2415,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-25.7857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0616870174788398 entropy -3.1359495800635933
epoch: 12, step: 1
	action: tensor([[ 0.1413, -0.0065,  0.3200,  0.5121,  0.2569,  0.1628,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7675947536464968, distance: 0.5516703678457504 entropy -3.35446511269484
epoch: 12, step: 2
	action: tensor([[-0.7508, -0.9573, -0.4245, -0.0486,  0.1371,  0.2960,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-36.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5516703678457504 entropy -2.52961228994996
epoch: 12, step: 3
	action: tensor([[ 0.0932,  0.0017,  0.0433,  0.2118, -0.3357, -0.0410,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4770514379869889, distance: 0.8275346357180009 entropy -3.35446511269484
epoch: 12, step: 4
	action: tensor([[-0.3266, -0.5207,  0.1639,  0.2725,  0.0929,  0.1926,  0.1189]],
       dtype=torch.float64)
	q_value: tensor([[-29.9300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22964476164138548, distance: 1.2689558881498015 entropy -2.666483551751405
epoch: 12, step: 5
	action: tensor([[ 0.2922, -0.8238,  0.1963, -0.3884, -0.2198,  0.1087,  0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-32.3950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5154326780450815, distance: 1.4087210924291145 entropy -2.598312681987526
epoch: 12, step: 6
	action: tensor([[-1.6063, -0.2713,  1.1810,  0.1700, -0.2884,  0.1972, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[-33.6291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6886482717339657, distance: 1.876392503042947 entropy -2.2750046261719015
epoch: 12, step: 7
	action: tensor([[1.0560, 0.4241, 1.5256, 0.3910, 0.9228, 1.7034, 0.4251]],
       dtype=torch.float64)
	q_value: tensor([[-39.2044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.876392503042947 entropy -2.059224764549719
epoch: 12, step: 8
	action: tensor([[ 0.1246,  0.2123, -0.0978, -0.0217,  0.3344, -0.0664,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5252014648593497, distance: 0.7885175765430753 entropy -3.35446511269484
epoch: 12, step: 9
	action: tensor([[-0.0586, -0.0053,  0.1893,  0.0857,  0.1635,  0.2330,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3400560398742448, distance: 0.9296301973096476 entropy -3.35446511269484
epoch: 12, step: 10
	action: tensor([[ 0.3999,  0.0743, -0.3141, -0.3417, -0.3311,  0.4027,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-31.1139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5950045097985976, distance: 0.7282521691247967 entropy -2.699134458102194
epoch: 12, step: 11
	action: tensor([[ 0.6150, -0.2408,  0.1326,  0.0194,  0.2895, -0.2968, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[-28.7273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4340236424675653, distance: 0.8609062053370603 entropy -2.555387954842449
epoch: 12, step: 12
	action: tensor([[-0.6211, -0.0382,  0.9903, -0.1884,  0.3959, -0.2115, -0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-33.6947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7531171922473476, distance: 1.5151728078865043 entropy -2.641338223478218
epoch: 12, step: 13
	action: tensor([[-0.7019,  0.2858, -0.3168,  0.4814, -0.8581,  0.3639,  0.0125]],
       dtype=torch.float64)
	q_value: tensor([[-36.1368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.320385761619995, distance: 1.3149435511102734 entropy -2.4750130534286066
epoch: 12, step: 14
	action: tensor([[-0.1711,  0.5552, -0.3804, -0.7693,  0.0968,  0.0403,  0.2308]],
       dtype=torch.float64)
	q_value: tensor([[-29.9607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38351428255146314, distance: 0.8985002514687824 entropy -2.6336387979494233
epoch: 12, step: 15
	action: tensor([[-0.1861, -0.0328, -0.0890, -0.0629,  0.1194,  0.0630, -0.0700]],
       dtype=torch.float64)
	q_value: tensor([[-25.8947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03855608014363909, distance: 1.1220666950643283 entropy -3.3540088877019274
epoch: 12, step: 16
	action: tensor([[-0.1549, -0.2665,  0.0928,  0.0520, -0.1119,  0.1126,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06400521259515157, distance: 1.1803982883190316 entropy -3.35446511269484
epoch: 12, step: 17
	action: tensor([[ 0.0407, -0.3575,  0.3929, -0.5091,  0.4525, -0.1241,  0.1211]],
       dtype=torch.float64)
	q_value: tensor([[-29.8374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39798523395306784, distance: 1.3530317425305092 entropy -2.659107146421898
epoch: 12, step: 18
	action: tensor([[ 0.6382, -0.6476,  0.9224,  0.0511, -0.1325,  0.6732, -0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-32.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34321730090382907, distance: 0.9274009689898444 entropy -2.59132584022169
epoch: 12, step: 19
	action: tensor([[ 1.3240, -0.3598, -1.1536, -1.0172,  0.6603,  0.1822,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[-45.6136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1729582754943615, distance: 1.2393614262710007 entropy -1.8647515651440751
epoch: 12, step: 20
	action: tensor([[-0.1389, -1.1704, -1.0188, -0.4848,  0.2268,  0.3885, -0.3782]],
       dtype=torch.float64)
	q_value: tensor([[-33.1809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6885375505887932, distance: 1.487003719511599 entropy -2.4251544169096855
epoch: 12, step: 21
	action: tensor([[-0.4356,  0.4591, -0.1079, -0.1146, -0.1303, -0.5530, -0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-31.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.487003719511599 entropy -2.8520590429107813
epoch: 12, step: 22
	action: tensor([[-0.0712, -0.0315, -0.1485,  0.3248, -0.0998, -0.1472,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2817210574485145, distance: 0.9698470755516051 entropy -3.35446511269484
epoch: 12, step: 23
	action: tensor([[-0.6519, -0.1437,  0.2420, -0.3363,  0.1559,  0.3896,  0.0932]],
       dtype=torch.float64)
	q_value: tensor([[-28.2380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6915479712215178, distance: 1.4883286865550187 entropy -2.975277319607516
epoch: 12, step: 24
	action: tensor([[ 0.5397,  0.1372,  0.1928, -0.6138,  0.6433,  0.0813,  0.0401]],
       dtype=torch.float64)
	q_value: tensor([[-30.6244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49104180308955947, distance: 0.8163901381305794 entropy -2.693472230437668
epoch: 12, step: 25
	action: tensor([[ 0.1582, -0.9287, -0.2441,  0.1606,  1.3900,  0.3163, -0.2888]],
       dtype=torch.float64)
	q_value: tensor([[-33.8111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24793628420215663, distance: 1.278359194312494 entropy -2.5614082866371715
epoch: 12, step: 26
	action: tensor([[ 0.1421,  0.4056, -0.6138, -0.2701,  0.3509,  0.7048, -0.2497]],
       dtype=torch.float64)
	q_value: tensor([[-40.9979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.278359194312494 entropy -2.580341997231084
epoch: 12, step: 27
	action: tensor([[-0.4777,  0.0433,  0.1952,  0.2861, -0.0040, -0.2122,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1789746238844041, distance: 1.2425358329496399 entropy -3.35446511269484
epoch: 12, step: 28
	action: tensor([[-0.3473, -0.2651,  0.3557, -0.6129,  0.1633,  0.1045,  0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-28.9150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.689684628601591, distance: 1.4875087185516058 entropy -2.983499854128076
epoch: 12, step: 29
	action: tensor([[-0.5567, -0.5984, -0.0444, -0.1748, -0.6664,  0.0638, -0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-30.7385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9108694700086819, distance: 1.581875151685839 entropy -2.615863300302095
epoch: 12, step: 30
	action: tensor([[-0.1451,  0.2893,  0.1595, -1.0928, -0.9426,  0.1513,  0.2286]],
       dtype=torch.float64)
	q_value: tensor([[-29.4460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19534473860957902, distance: 1.251132427259928 entropy -2.5868543816619503
epoch: 12, step: 31
	action: tensor([[-0.5504, -0.1063,  0.1083, -0.2991,  0.2756, -0.0921,  0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-30.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6750317391950806, distance: 1.4810448611590632 entropy -2.4065358651676334
epoch: 12, step: 32
	action: tensor([[-0.3873,  0.0609,  0.1572, -0.5247,  0.2032,  0.0926, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-28.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4009710148087713, distance: 1.3544758570394178 entropy -3.1671759388559693
epoch: 12, step: 33
	action: tensor([[ 0.6181, -0.2371, -0.2970, -0.1389, -0.0854,  0.0837, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[-28.7340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4546921782292055, distance: 0.8450405644006244 entropy -2.9819870606051957
epoch: 12, step: 34
	action: tensor([[ 0.3411, -0.6211, -0.4713, -0.6031, -1.0837,  0.0635, -0.0734]],
       dtype=torch.float64)
	q_value: tensor([[-30.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27684748580463237, distance: 1.293082416340531 entropy -2.624882121688968
epoch: 12, step: 35
	action: tensor([[ 0.4505, -0.2400,  0.3673, -0.4019,  0.2969,  0.1751,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-30.8311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21306389800103953, distance: 1.0151411391266307 entropy -2.401728375308772
epoch: 12, step: 36
	action: tensor([[-0.5325, -0.0732, -0.0560,  0.2970,  0.5256,  0.2905, -0.1668]],
       dtype=torch.float64)
	q_value: tensor([[-34.4824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18545478094545098, distance: 1.2459459121002825 entropy -2.3959406940127264
epoch: 12, step: 37
	action: tensor([[-0.3709, -0.1839,  0.0088,  0.2883,  0.1333,  0.3070,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-31.4833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0644166020163297, distance: 1.1806264622195044 entropy -3.1293315194977813
epoch: 12, step: 38
	action: tensor([[ 0.3728,  0.0035,  0.3812,  0.3220, -0.2030, -0.0596,  0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-30.7120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7457458207230415, distance: 0.577019836580855 entropy -2.799984200031133
epoch: 12, step: 39
	action: tensor([[ 1.3643,  0.0055,  0.7357, -0.0887, -0.3276,  0.6303,  0.0796]],
       dtype=torch.float64)
	q_value: tensor([[-35.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6837801045316917, distance: 0.6435040560777225 entropy -2.4284307513894463
epoch: 12, step: 40
	action: tensor([[ 0.8890, -2.7124,  0.8992, -0.1942, -1.5435,  0.8934, -0.1846]],
       dtype=torch.float64)
	q_value: tensor([[-47.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6435040560777225 entropy -1.8298244790867582
epoch: 12, step: 41
	action: tensor([[-0.2797, -0.0882, -0.0391, -0.1217, -0.0256, -0.1453,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21659209499739585, distance: 1.2622029428012587 entropy -3.35446511269484
epoch: 12, step: 42
	action: tensor([[-0.9902, -0.0727,  0.1501, -0.0572, -0.0899,  0.1599,  0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-26.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0665214069527758, distance: 1.645040700728936 entropy -3.0952946402425456
epoch: 12, step: 43
	action: tensor([[ 0.3018,  0.1467, -0.4401,  0.1452,  0.1909,  0.3245,  0.1409]],
       dtype=torch.float64)
	q_value: tensor([[-29.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.645040700728936 entropy -2.925616318697301
epoch: 12, step: 44
	action: tensor([[-0.1574, -0.0815, -0.0105, -0.3115,  0.1837,  0.1475,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07769661229291458, distance: 1.1879685738427836 entropy -3.35446511269484
epoch: 12, step: 45
	action: tensor([[-0.2322, -0.0966, -0.0068, -0.0195,  0.1451, -0.1650, -0.0321]],
       dtype=torch.float64)
	q_value: tensor([[-28.1612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12908044290040643, distance: 1.2159595717365208 entropy -2.901643437432038
epoch: 12, step: 46
	action: tensor([[ 0.0044,  0.0185, -0.4465, -0.2795, -0.1887,  0.0857,  0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-27.6644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2375391347365403, distance: 0.9992300175760835 entropy -3.164934044884242
epoch: 12, step: 47
	action: tensor([[ 0.3950,  0.0054, -0.0433,  0.3116, -0.3073,  0.0303,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-25.1117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7368094280202481, distance: 0.5870726620146937 entropy -3.1541935497372635
epoch: 12, step: 48
	action: tensor([[ 0.7094, -0.2014, -0.1255,  0.5433,  0.0603,  0.2771,  0.0768]],
       dtype=torch.float64)
	q_value: tensor([[-32.2032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8842119995437732, distance: 0.3893932961296766 entropy -2.5848647831881366
epoch: 12, step: 49
	action: tensor([[-0.8507, -0.0889,  0.8576, -0.1610,  0.4233,  0.2905, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[-37.3716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9811810460383159, distance: 1.6107152683683668 entropy -2.44635994288786
epoch: 12, step: 50
	action: tensor([[ 0.1178,  0.1645, -1.0616, -0.7887,  0.5377, -0.2136,  0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-36.3255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5641849979398279, distance: 0.755453575837109 entropy -2.4186538991758697
epoch: 12, step: 51
	action: tensor([[-0.0434, -0.2372, -0.0634,  0.0221,  0.1211, -0.1839, -0.0489]],
       dtype=torch.float64)
	q_value: tensor([[-25.0815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07922903566579498, distance: 1.1888128857280678 entropy -3.353297196265557
epoch: 12, step: 52
	action: tensor([[ 0.0081, -0.1254, -0.2789,  0.2441,  0.2210,  0.0250,  0.0346]],
       dtype=torch.float64)
	q_value: tensor([[-28.3735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19190905984362228, distance: 1.0286954339356869 entropy -3.0461636539047383
epoch: 12, step: 53
	action: tensor([[-0.5702,  0.0387,  0.0702,  0.0507, -0.1998,  0.0607,  0.0238]],
       dtype=torch.float64)
	q_value: tensor([[-28.8627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4627529529019443, distance: 1.3840194232362044 entropy -3.0834016445403134
epoch: 12, step: 54
	action: tensor([[ 0.6721,  0.2456, -0.3513, -0.2872,  0.9295,  0.1162,  0.1410]],
       dtype=torch.float64)
	q_value: tensor([[-27.9286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8039985835031478, distance: 0.5066246284797182 entropy -2.926410002874649
epoch: 12, step: 55
	action: tensor([[-0.1909, -0.3888,  0.2116, -0.1886, -0.0042, -0.3376, -0.2392]],
       dtype=torch.float64)
	q_value: tensor([[-32.7709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5295178686810798, distance: 1.4152526305295885 entropy -2.765470281390397
epoch: 12, step: 56
	action: tensor([[ 0.0060, -0.1245, -0.6072,  0.2336, -0.0689,  0.4396,  0.0694]],
       dtype=torch.float64)
	q_value: tensor([[-29.2782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14029046182418858, distance: 1.0610419770049093 entropy -2.8806581097527086
epoch: 12, step: 57
	action: tensor([[-0.5443, -0.3240, -0.2199, -0.4988, -0.0127,  0.2814,  0.0376]],
       dtype=torch.float64)
	q_value: tensor([[-28.4919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7736778770323129, distance: 1.5240319364259567 entropy -2.906130135774551
epoch: 12, step: 58
	action: tensor([[ 0.0431,  0.1190, -0.2284,  0.1128,  0.0045,  0.4150,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[-27.7915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5240319364259567 entropy -3.0382139057782527
epoch: 12, step: 59
	action: tensor([[ 0.4168,  0.2179, -0.1278,  0.0290, -0.2793,  0.1179,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7768720300556362, distance: 0.5405472974356738 entropy -3.35446511269484
epoch: 12, step: 60
	action: tensor([[-0.2685,  0.1657,  0.1011, -0.0328,  0.0240, -0.0633,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0309560413909592, distance: 1.1264928315727434 entropy -3.35446511269484
epoch: 12, step: 61
	action: tensor([[ 0.5844, -0.3763,  0.3231,  0.1661,  0.4997, -0.0200,  0.0555]],
       dtype=torch.float64)
	q_value: tensor([[-27.8875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46662049466922806, distance: 0.835747055961551 entropy -3.001081597863803
epoch: 12, step: 62
	action: tensor([[-0.1233,  0.9365, -0.3991,  0.1508,  0.4920,  0.1058, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-37.7513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.835747055961551 entropy -2.4488634383063936
epoch: 12, step: 63
	action: tensor([[-0.3503, -0.1564,  0.0284,  0.1782,  0.4339, -0.0804,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21189977518954106, distance: 1.2597664722017041 entropy -3.35446511269484
LOSS epoch 12 actor 422.4145082757338 critic 668.2175872633517
epoch: 13, step: 0
	action: tensor([[ 0.3573, -0.3245,  0.0978, -0.0423,  0.2147,  0.1641,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-33.9193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3228101466765767, distance: 0.9416985774948791 entropy -3.109546596132209
epoch: 13, step: 1
	action: tensor([[ 1.5102, -0.2854, -0.3480,  0.9689,  0.2122, -0.0166, -0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-36.7774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8287568611073615, distance: 0.4735472110719981 entropy -2.547946557868704
epoch: 13, step: 2
	action: tensor([[-0.6715, -0.0702,  0.7675,  0.3055,  0.5069,  0.3042, -0.1620]],
       dtype=torch.float64)
	q_value: tensor([[-50.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16471994078208407, distance: 1.2350013966728326 entropy -2.2735393242443456
epoch: 13, step: 3
	action: tensor([[ 0.8240, -0.1055,  0.4631, -0.3916, -0.4396,  0.6995,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-41.9179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6547713476783401, distance: 0.6723727588728824 entropy -2.4747164456878217
epoch: 13, step: 4
	action: tensor([[-0.7295,  0.4572, -1.4585,  0.8042,  2.1655,  0.4927, -0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-42.6437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6723727588728824 entropy -1.9508894779793902
epoch: 13, step: 5
	action: tensor([[ 0.0618,  0.2807, -0.0327, -0.1138, -0.1524, -0.2950,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4367764296097263, distance: 0.8588100221742112 entropy -3.354857000422242
epoch: 13, step: 6
	action: tensor([[ 0.5693, -0.5050,  0.0042, -0.1769, -0.5981,  0.1649,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-30.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13040995155383717, distance: 1.0671217550222047 entropy -3.067828565928653
epoch: 13, step: 7
	action: tensor([[-0.1803, -0.0108,  0.0462,  0.0198,  0.0721,  0.0507,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07431938989530606, distance: 1.10099989277903 entropy -3.354857000422242
epoch: 13, step: 8
	action: tensor([[-0.2052, -0.2649, -0.1536, -0.3092,  0.0469, -0.1238,  0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-32.5963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31491688852387734, distance: 1.3122175594324756 entropy -2.8876031072886392
epoch: 13, step: 9
	action: tensor([[ 0.1729, -0.4857, -0.2656,  0.0710, -0.1290,  0.1494,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-30.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06425951719659861, distance: 1.1069663076798204 entropy -3.14870992603874
epoch: 13, step: 10
	action: tensor([[ 0.3852, -0.0897, -0.4160, -0.4088,  0.1182,  0.1483,  0.0591]],
       dtype=torch.float64)
	q_value: tensor([[-33.6232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4103679529863681, distance: 0.8787133674290828 entropy -2.6667343079618466
epoch: 13, step: 11
	action: tensor([[ 0.1869, -0.0777,  0.7963,  0.0441,  0.1571,  0.2166, -0.1157]],
       dtype=torch.float64)
	q_value: tensor([[-31.4912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48963780991826333, distance: 0.8175153943932377 entropy -2.8794306457969867
epoch: 13, step: 12
	action: tensor([[-0.7350, -0.5285,  0.3764,  0.8137, -0.4462,  0.0269,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-41.6924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35372409505872326, distance: 1.331440529183385 entropy -2.2707160574462235
epoch: 13, step: 13
	action: tensor([[-0.7792, -0.3921,  1.0704,  0.0330, -0.3736,  0.5003,  0.3580]],
       dtype=torch.float64)
	q_value: tensor([[-39.9607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7222866603373044, distance: 1.5017907055676776 entropy -2.340701318633901
epoch: 13, step: 14
	action: tensor([[ 0.1518, -0.9590,  0.3289, -0.9629,  0.4439,  1.0896,  0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-43.5149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5704978497178466, distance: 1.4340865620843317 entropy -1.94829488767006
epoch: 13, step: 15
	action: tensor([[-1.6951, -0.3638, -0.6566, -0.4025,  1.4253, -1.2548, -0.3474]],
       dtype=torch.float64)
	q_value: tensor([[-44.2430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4340865620843317 entropy -1.9659639777708438
epoch: 13, step: 16
	action: tensor([[-0.4750,  0.0835, -0.0552,  0.3582, -0.2875, -0.2041,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11684478777397445, distance: 1.2093530497120073 entropy -3.354857000422242
epoch: 13, step: 17
	action: tensor([[-0.1304, -0.0552, -0.2748, -0.2173,  0.0517,  0.5304,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-31.3846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11616866916091317, distance: 1.0758243991814684 entropy -3.0157405074176395
epoch: 13, step: 18
	action: tensor([[-0.1191, -0.0409, -0.1336,  0.1373,  0.1070,  0.5820, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-32.5770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30076475768121136, distance: 0.9569039511888658 entropy -2.8000705622090303
epoch: 13, step: 19
	action: tensor([[-0.1923, -0.2320,  0.2200, -0.5296, -0.3503, -0.0659,  0.0510]],
       dtype=torch.float64)
	q_value: tensor([[-34.7861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4570963913214876, distance: 1.3813407841802086 entropy -2.7086640023235167
epoch: 13, step: 20
	action: tensor([[-0.8593,  0.0737,  0.5666,  0.0333,  0.5405,  0.5092,  0.0731]],
       dtype=torch.float64)
	q_value: tensor([[-32.9487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4947091125446581, distance: 1.3990557939547164 entropy -2.6318308207206824
epoch: 13, step: 21
	action: tensor([[ 0.1936,  0.2511, -0.1344,  0.2761,  0.7955,  0.2185,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-40.1443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3990557939547164 entropy -2.5762930873242444
epoch: 13, step: 22
	action: tensor([[ 0.0619, -0.0036, -0.0309, -0.4676,  0.1133,  0.0361,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14007782035367555, distance: 1.061173188516825 entropy -3.354857000422242
epoch: 13, step: 23
	action: tensor([[ 0.1090,  0.3741,  0.1931,  0.2046,  0.1383,  0.3140, -0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-32.0628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.061173188516825 entropy -2.8848863425139823
epoch: 13, step: 24
	action: tensor([[-0.2877, -0.1729, -0.0264, -0.1400, -0.1243, -0.0303,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28064169367243585, distance: 1.2950022165411863 entropy -3.354857000422242
epoch: 13, step: 25
	action: tensor([[-0.0501,  0.2118, -0.4545,  0.1109,  0.2149,  0.1358,  0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-31.2659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2950022165411863 entropy -2.937813965852064
epoch: 13, step: 26
	action: tensor([[-0.0048,  0.1030, -0.0903, -0.0192, -0.1189, -0.0454,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33914763320645125, distance: 0.9302697909078779 entropy -3.354857000422242
epoch: 13, step: 27
	action: tensor([[-0.2391, -0.1229,  0.1973, -0.0766,  0.0128,  0.2320,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-31.1817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04863702483777388, distance: 1.1718426139507114 entropy -2.9427983097092487
epoch: 13, step: 28
	action: tensor([[-0.8464, -0.5221,  0.2648, -0.5870, -0.2877,  0.7167,  0.0831]],
       dtype=torch.float64)
	q_value: tensor([[-34.0919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1834379193285383, distance: 1.6909357939646081 entropy -2.6622668020927773
epoch: 13, step: 29
	action: tensor([[ 0.6074, -0.2356,  0.1316,  0.1694, -0.0605,  0.4508,  0.1603]],
       dtype=torch.float64)
	q_value: tensor([[-38.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7458764243058406, distance: 0.5768716177031566 entropy -2.284504110087932
epoch: 13, step: 30
	action: tensor([[ 0.6848,  0.5370, -0.1272,  0.4322, -0.2325,  0.6457, -0.0149]],
       dtype=torch.float64)
	q_value: tensor([[-40.5744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8837848144693247, distance: 0.3901109432977033 entropy -2.2728701167993166
epoch: 13, step: 31
	action: tensor([[ 0.3832, -1.1989, -0.5894,  0.2744,  1.2035,  0.1265,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[-40.0956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47614695393827944, distance: 1.3903415143948747 entropy -2.3332862700469077
epoch: 13, step: 32
	action: tensor([[-0.4347, -0.1710,  0.0291, -0.5333,  0.0013,  0.1498, -0.2313]],
       dtype=torch.float64)
	q_value: tensor([[-43.4014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5328812973657551, distance: 1.4168078550169725 entropy -2.6255206116825747
epoch: 13, step: 33
	action: tensor([[-0.6118,  0.2543,  0.1399, -0.2190,  0.0285,  0.1970,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[-32.8278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3240288550445569, distance: 1.3167563404245861 entropy -2.9847687240228846
epoch: 13, step: 34
	action: tensor([[ 0.0197, -0.1488, -0.0927, -0.7281,  0.3693, -0.1804,  0.0615]],
       dtype=torch.float64)
	q_value: tensor([[-32.9380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16055851108567554, distance: 1.2327931532387322 entropy -2.995438292345249
epoch: 13, step: 35
	action: tensor([[ 0.1691, -0.0838, -0.0842, -0.1212,  0.0588,  0.3520, -0.1524]],
       dtype=torch.float64)
	q_value: tensor([[-32.2681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4298508030208297, distance: 0.8640740285639241 entropy -3.0159369435140233
epoch: 13, step: 36
	action: tensor([[ 0.0158, -0.0077, -0.3082, -1.3953,  0.4413,  0.1893, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-34.0188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011101879816916127, distance: 1.137974338960427 entropy -2.694403881367087
epoch: 13, step: 37
	action: tensor([[-0.6786, -0.4757, -0.0174, -0.6426,  0.1201,  0.1961, -0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-36.0895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9583257058966843, distance: 1.6013975348207807 entropy -2.8027149587044464
epoch: 13, step: 38
	action: tensor([[-0.3380, -0.0916, -0.1291, -0.1787, -0.4045,  0.0681,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-34.7942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2224348390850499, distance: 1.2652302087330523 entropy -2.951537293459633
epoch: 13, step: 39
	action: tensor([[ 0.6096, -0.0372, -0.1682,  0.5560, -0.1358,  0.1937,  0.1264]],
       dtype=torch.float64)
	q_value: tensor([[-30.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9063015558117106, distance: 0.35028608312690174 entropy -2.9197181163071373
epoch: 13, step: 40
	action: tensor([[ 0.7513, -0.1901,  0.4815,  0.2616, -0.0784,  0.3805,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[-38.6817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8132743793138423, distance: 0.4944912932165922 entropy -2.5077032980150373
epoch: 13, step: 41
	action: tensor([[-0.1520, -0.6548, -0.2774, -1.5292, -0.0973,  0.5834, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[-44.2386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4871828869764627, distance: 1.3955290549844577 entropy -2.1538001906592363
epoch: 13, step: 42
	action: tensor([[-0.1235, -0.8780,  0.5882,  0.1808, -0.0355,  0.4135, -0.2234]],
       dtype=torch.float64)
	q_value: tensor([[-38.7510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23428376698456477, distance: 1.2713472908075967 entropy -2.427332073152918
epoch: 13, step: 43
	action: tensor([[-1.1247, -0.4950, -0.0457,  1.5205,  0.5372,  0.0386,  0.1896]],
       dtype=torch.float64)
	q_value: tensor([[-42.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32608479452287553, distance: 1.3177782672621017 entropy -2.129325506361484
epoch: 13, step: 44
	action: tensor([[ 0.2720,  0.1020,  0.3076,  0.3721, -0.5110,  0.3100,  0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-46.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3177782672621017 entropy -2.7357202671946075
epoch: 13, step: 45
	action: tensor([[ 0.2205,  0.2841,  0.1452, -0.0024,  0.0366, -0.0728,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6549262665976947, distance: 0.6722218806813387 entropy -3.354857000422242
epoch: 13, step: 46
	action: tensor([[-2.4591e-01,  1.1429e-01,  8.3284e-02, -5.0140e-01, -3.5263e-04,
          5.0115e-02,  7.5423e-02]], dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16540407411668068, distance: 1.235364051058832 entropy -3.354857000422242
epoch: 13, step: 47
	action: tensor([[-0.1450, -0.0471, -0.3403,  0.2115, -0.0170,  0.1496, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[-32.2207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13241035160046755, distance: 1.065893647897281 entropy -2.949603364070783
epoch: 13, step: 48
	action: tensor([[ 0.0193,  0.2814,  0.1911,  0.1095, -0.2525, -0.1066,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[-31.2394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.065893647897281 entropy -3.077464681979891
epoch: 13, step: 49
	action: tensor([[-0.3121, -0.1928,  0.1268,  0.3910,  0.1654, -0.0244,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.065893647897281 entropy -3.354857000422242
epoch: 13, step: 50
	action: tensor([[-0.0609, -0.1958, -0.1449, -0.3408,  0.1111,  0.1702,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04013034109624014, distance: 1.167079863288404 entropy -3.354857000422242
epoch: 13, step: 51
	action: tensor([[-0.3378, -0.2218,  0.1118, -0.6839,  0.0874, -0.0134, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-31.8731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5914243059359499, distance: 1.4436093513915476 entropy -2.8872414075014032
epoch: 13, step: 52
	action: tensor([[-0.2193,  0.4096,  0.1884, -0.5296, -0.3489, -0.0539, -0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-33.2144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0482710824714917, distance: 1.1163832861891572 entropy -2.9115997689966853
epoch: 13, step: 53
	action: tensor([[-0.0835,  0.4022,  0.1434, -0.2515,  0.0415,  0.2905,  0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-32.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1163832861891572 entropy -2.894604569935597
epoch: 13, step: 54
	action: tensor([[ 0.3535, -0.1016, -0.0556,  0.1670, -0.0344, -0.1138,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6021751369996909, distance: 0.7217763608330546 entropy -3.354857000422242
epoch: 13, step: 55
	action: tensor([[ 0.1581, -0.1155, -0.1951, -0.3875, -0.3696,  0.1302,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-34.5993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21825079019739446, distance: 1.0117900838193337 entropy -2.7303747161046443
epoch: 13, step: 56
	action: tensor([[ 0.0093,  0.1234,  0.3269,  0.6676, -0.2373,  0.2172,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[-31.5158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0117900838193337 entropy -2.7100567187612072
epoch: 13, step: 57
	action: tensor([[-0.3133, -0.3286,  0.0247,  0.2721, -0.2662, -0.1993,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-38.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2423028220020702, distance: 1.275470531613926 entropy -3.354857000422242
epoch: 13, step: 58
	action: tensor([[ 0.4016, -0.1109,  0.2992,  0.0054,  0.3045,  0.0152,  0.1641]],
       dtype=torch.float64)
	q_value: tensor([[-32.5172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5240120994600463, distance: 0.7895045729955878 entropy -2.8162305385083184
epoch: 13, step: 59
	action: tensor([[-0.4706,  0.2228, -0.5030, -0.2596,  0.6902,  0.3687, -0.0897]],
       dtype=torch.float64)
	q_value: tensor([[-37.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.044447986791041894, distance: 1.1694996652279301 entropy -2.5447612125974195
epoch: 13, step: 60
	action: tensor([[-0.2226,  0.0432,  0.1176,  0.2975, -0.1206, -0.0987, -0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-33.4966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17539187626685504, distance: 1.0391553975873034 entropy -3.4686436480164673
epoch: 13, step: 61
	action: tensor([[ 0.4674, -0.1804,  0.4967, -0.5426, -0.0460,  0.1679,  0.1303]],
       dtype=torch.float64)
	q_value: tensor([[-33.0750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18601760962916192, distance: 1.0324385162834322 entropy -2.865481011130052
epoch: 13, step: 62
	action: tensor([[-0.2550, -0.8076,  0.7013, -0.6464, -0.3201,  0.3643, -0.1253]],
       dtype=torch.float64)
	q_value: tensor([[-38.2082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9915340659572438, distance: 1.6149183265562734 entropy -2.265593756656798
epoch: 13, step: 63
	action: tensor([[-0.7534, -0.4158, -1.8578, -0.6910,  0.1702,  1.1822,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-40.4808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6694445835366891, distance: 1.4785727468119467 entropy -2.0580221682523248
LOSS epoch 13 actor 572.8411210439986 critic 253.63656965531746
epoch: 14, step: 0
	action: tensor([[-0.0457, -0.2913, -0.2720,  0.2570, -0.0603, -0.0340, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[-45.8049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06649544871469004, distance: 1.1056429807417691 entropy -3.1286524520524592
epoch: 14, step: 1
	action: tensor([[-0.2723, -0.5504,  0.3618, -0.6389,  0.3270,  0.1784,  0.0894]],
       dtype=torch.float64)
	q_value: tensor([[-33.7928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8027482721151113, distance: 1.5364705388190187 entropy -2.9544347102992243
epoch: 14, step: 2
	action: tensor([[-0.5441, -0.4678, -0.5880, -1.3094,  0.2792,  0.2091, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-38.8078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.294684282845703, distance: 1.3020828872175048 entropy -2.4904816157672376
epoch: 14, step: 3
	action: tensor([[ 0.0473, -0.2798,  0.0071,  0.2243, -0.4498,  0.1354, -0.1162]],
       dtype=torch.float64)
	q_value: tensor([[-39.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22707969595866417, distance: 1.0060603916977255 entropy -3.1312006436587057
epoch: 14, step: 4
	action: tensor([[ 0.0220, -0.7507,  0.2032, -0.3550,  0.0071, -0.0087,  0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-36.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6265796863085826, distance: 1.4594672883367192 entropy -2.511821880854086
epoch: 14, step: 5
	action: tensor([[-0.6334,  0.0919, -0.2017, -0.4031, -1.3154,  0.4961, -0.0176]],
       dtype=torch.float64)
	q_value: tensor([[-37.3338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7051450775173089, distance: 1.494298502021183 entropy -2.456856505005313
epoch: 14, step: 6
	action: tensor([[-0.7138,  0.7422, -0.1321, -0.2484,  0.5603,  0.3995,  0.2601]],
       dtype=torch.float64)
	q_value: tensor([[-38.6766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02484452895334055, distance: 1.158472387639421 entropy -2.3848726105477716
epoch: 14, step: 7
	action: tensor([[-0.3101, -0.0443, -0.1319,  0.0515,  0.0816,  0.2370, -0.0721]],
       dtype=torch.float64)
	q_value: tensor([[-39.1519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.158472387639421 entropy -3.246724167856375
epoch: 14, step: 8
	action: tensor([[0.0886, 0.0884, 0.1730, 0.1903, 0.2647, 0.0797, 0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5687818633608964, distance: 0.7514588483084423 entropy -3.354457626078163
epoch: 14, step: 9
	action: tensor([[ 0.5804, -0.3649, -0.1115,  0.3394, -0.2941,  0.2221,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-37.3302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7514588483084423 entropy -2.738231473637871
epoch: 14, step: 10
	action: tensor([[ 0.0491,  0.2180,  0.0491,  0.0269, -0.0021,  0.3069,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.527948024516459, distance: 0.7862336060968713 entropy -3.354457626078163
epoch: 14, step: 11
	action: tensor([[-0.3896, -0.4287,  0.3978,  0.1696, -0.5393,  0.4089,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[-36.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7862336060968713 entropy -2.6974525112190086
epoch: 14, step: 12
	action: tensor([[-0.1378,  0.2102,  0.1014,  0.1032,  0.0717, -0.0232,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983131145915252, distance: 0.9585800210240355 entropy -3.354457626078163
epoch: 14, step: 13
	action: tensor([[ 0.1900,  0.1397, -0.0777, -0.0695,  0.2905,  0.0893,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5670524304064142, distance: 0.7529642317311244 entropy -3.354457626078163
epoch: 14, step: 14
	action: tensor([[ 0.2244, -0.1659,  0.3018,  0.3064,  0.3174, -0.0604, -0.0655]],
       dtype=torch.float64)
	q_value: tensor([[-35.1876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5459052917909271, distance: 0.7711341095983701 entropy -2.925419365669676
epoch: 14, step: 15
	action: tensor([[-0.3831, -0.3949,  0.0437,  0.5438, -0.1911,  0.4475,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-39.6054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0008303667585218832, distance: 1.1448192681258904 entropy -2.6359343077441157
epoch: 14, step: 16
	action: tensor([[-0.1419, -0.0933,  0.6972,  0.1347, -0.2912,  0.4912,  0.2437]],
       dtype=torch.float64)
	q_value: tensor([[-38.9818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36413630090805915, distance: 0.9125122626564252 entropy -2.4688861353776836
epoch: 14, step: 17
	action: tensor([[-0.0371,  0.8886, -0.2035, -0.0272, -0.0164,  0.4302,  0.2166]],
       dtype=torch.float64)
	q_value: tensor([[-41.9939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9125122626564252 entropy -2.1267886142171766
epoch: 14, step: 18
	action: tensor([[ 0.0491, -0.4186, -0.0487, -0.0321,  0.0074,  0.0361,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02527733503243823, distance: 1.1587169813082399 entropy -3.354457626078163
epoch: 14, step: 19
	action: tensor([[-0.3347, -0.2134, -0.4782, -0.1573, -0.3158,  0.2312,  0.0394]],
       dtype=torch.float64)
	q_value: tensor([[-35.1741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28768431915208326, distance: 1.2985581335955283 entropy -2.716100197385855
epoch: 14, step: 20
	action: tensor([[ 0.4119, -0.2424, -0.2702, -0.2590,  0.0571, -0.0338,  0.0793]],
       dtype=torch.float64)
	q_value: tensor([[-32.9157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28323494255688697, distance: 0.9688244843205712 entropy -3.0819933697803816
epoch: 14, step: 21
	action: tensor([[-0.5497, -0.5208, -0.0648,  0.3697,  0.2281,  0.1647, -0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-34.2777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5169359713799171, distance: 1.4094196375148274 entropy -2.8065059509570136
epoch: 14, step: 22
	action: tensor([[-0.6686, -0.1423, -0.1978,  0.0542,  0.3493,  0.3440,  0.1219]],
       dtype=torch.float64)
	q_value: tensor([[-37.2480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5049725156179017, distance: 1.4038508770167974 entropy -2.9074037849803114
epoch: 14, step: 23
	action: tensor([[ 0.0108, -0.3546,  0.3800,  0.5022,  0.1780,  0.1005,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-36.1740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42550611672864647, distance: 0.8673600156905555 entropy -3.1857944239675153
epoch: 14, step: 24
	action: tensor([[ 0.6760, -0.7500, -0.1853,  0.6970,  0.0009, -0.1283,  0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-41.1049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4385731813102277, distance: 0.8574390737091977 entropy -2.4850395704534662
epoch: 14, step: 25
	action: tensor([[ 0.5407, -1.4359,  0.7962, -0.1391, -0.5536,  0.3270, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-42.4469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8574390737091977 entropy -2.4574279609814256
epoch: 14, step: 26
	action: tensor([[-0.0655,  0.2957,  0.0004,  0.0672, -0.0986,  0.0971,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43221178198253474, distance: 0.8622831140431434 entropy -3.354457626078163
epoch: 14, step: 27
	action: tensor([[-0.0157, -0.1089,  0.1889, -0.2580,  0.1295,  0.2108,  0.0642]],
       dtype=torch.float64)
	q_value: tensor([[-34.4947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8622831140431434 entropy -2.8561859377654346
epoch: 14, step: 28
	action: tensor([[-1.9768e-01, -1.0402e-05, -1.4937e-02,  2.3033e-01,  1.9179e-01,
         -1.9731e-01,  7.5378e-02]], dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08918439622675312, distance: 1.0921239328677343 entropy -3.354457626078163
epoch: 14, step: 29
	action: tensor([[-0.2640,  0.0709,  0.0857, -0.3833,  0.0164,  0.3128,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-34.3518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09969390854536209, distance: 1.2000313807534146 entropy -3.1124950780903364
epoch: 14, step: 30
	action: tensor([[ 0.3384,  0.2220, -0.6716, -0.0564, -0.4713, -0.3878,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[-35.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7411861280196236, distance: 0.5821708664940215 entropy -2.7860450319697656
epoch: 14, step: 31
	action: tensor([[-0.1572,  0.0092,  0.2433,  0.0368,  0.2671,  0.2776, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-30.8354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21898427336502868, distance: 1.0113153118938736 entropy -3.253902740804579
epoch: 14, step: 32
	action: tensor([[ 0.5373,  0.1143,  0.1556,  0.0585, -0.0521,  0.2626,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[-37.4128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8305620422744758, distance: 0.4710446193977523 entropy -2.7146653473031157
epoch: 14, step: 33
	action: tensor([[-0.1712,  0.0071, -0.4649,  0.1649, -0.6717,  0.2356, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[-38.8630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04906829874348273, distance: 1.1159156186841248 entropy -2.435027567069523
epoch: 14, step: 34
	action: tensor([[-0.5361, -0.4010,  0.1847,  0.0829,  0.4774,  0.2505,  0.1347]],
       dtype=torch.float64)
	q_value: tensor([[-33.2625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5292746352141025, distance: 1.4151400949071613 entropy -2.7953527960387072
epoch: 14, step: 35
	action: tensor([[-0.4011,  0.1280,  0.0868, -0.3120,  1.0091,  0.4373,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[-38.5533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08006602616627334, distance: 1.189273785227829 entropy -2.781750756189822
epoch: 14, step: 36
	action: tensor([[-0.0297, -0.0405,  0.5740, -0.1238, -0.1488, -0.2309, -0.2086]],
       dtype=torch.float64)
	q_value: tensor([[-41.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03270918196938699, distance: 1.1254733761054765 entropy -2.8968358133757874
epoch: 14, step: 37
	action: tensor([[-0.2604, -0.0305,  0.5068, -0.5269,  0.6427,  0.1161,  0.0868]],
       dtype=torch.float64)
	q_value: tensor([[-37.5975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38988670402487524, distance: 1.3491069931027173 entropy -2.5558500867226486
epoch: 14, step: 38
	action: tensor([[-0.4373, -0.2644,  0.4673, -0.6222,  0.4361,  0.2261, -0.1997]],
       dtype=torch.float64)
	q_value: tensor([[-40.3653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.774614294509979, distance: 1.524434191493177 entropy -2.6410326103662882
epoch: 14, step: 39
	action: tensor([[-1.0062,  0.2885,  0.3525,  0.8204,  0.5345,  0.4071, -0.1124]],
       dtype=torch.float64)
	q_value: tensor([[-39.9737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13965663933461436, distance: 1.0614330325477903 entropy -2.5971328165560164
epoch: 14, step: 40
	action: tensor([[-0.3392,  0.3910, -0.2729, -0.6552, -0.3177,  0.3056,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[-44.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012830488217274838, distance: 1.1369793060299098 entropy -2.730868602303978
epoch: 14, step: 41
	action: tensor([[-0.0171, -0.2591, -0.2638, -0.1257, -0.4305,  0.1866,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-34.4134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007674054401799468, distance: 1.1487267422355094 entropy -3.0859382016635744
epoch: 14, step: 42
	action: tensor([[ 0.9017, -0.8383, -0.2085, -0.2024,  0.3354,  0.5192,  0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-33.7481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07546636128973372, distance: 1.1867387099663587 entropy -2.705993507113771
epoch: 14, step: 43
	action: tensor([[ 0.0577, -0.9922, -0.7363,  1.5143, -0.6864,  0.5185, -0.2539]],
       dtype=torch.float64)
	q_value: tensor([[-43.2251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07744785819192357, distance: 1.1878314623840838 entropy -2.2094657026071887
epoch: 14, step: 44
	action: tensor([[ 0.7320,  0.5379,  0.6926, -0.1158, -0.2195,  1.0065,  0.2184]],
       dtype=torch.float64)
	q_value: tensor([[-48.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1878314623840838 entropy -2.2027361131937933
epoch: 14, step: 45
	action: tensor([[ 0.1179, -0.3925, -0.0180,  0.1664,  0.2388, -0.0206,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17022547420037348, distance: 1.042405615226075 entropy -3.354457626078163
epoch: 14, step: 46
	action: tensor([[ 0.5932, -0.5831,  0.3674, -0.2972, -0.3662,  0.1391,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-36.4378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034227972245559624, distance: 1.1637637711211808 entropy -2.7664822942226417
epoch: 14, step: 47
	action: tensor([[-0.6784,  1.3337,  0.3849, -0.6717,  0.0388,  0.4696, -0.0327]],
       dtype=torch.float64)
	q_value: tensor([[-40.5549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1637637711211808 entropy -2.1659594932077253
epoch: 14, step: 48
	action: tensor([[-0.1605, -0.0141, -0.1365, -0.0244, -0.3699,  0.0152,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1637637711211808 entropy -3.354457626078163
epoch: 14, step: 49
	action: tensor([[-0.1378, -0.1548,  0.0304,  0.3753,  0.2545,  0.0128,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20263838607138085, distance: 1.021843426387309 entropy -3.354457626078163
epoch: 14, step: 50
	action: tensor([[ 0.8431, -0.0967, -0.3453, -0.2082, -0.2492,  0.3493,  0.0616]],
       dtype=torch.float64)
	q_value: tensor([[-36.6209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6539407755850508, distance: 0.6731810906246352 entropy -2.8793800503895097
epoch: 14, step: 51
	action: tensor([[ 0.0144, -0.4578, -0.1325, -0.0921,  1.1801,  0.3993, -0.1080]],
       dtype=torch.float64)
	q_value: tensor([[-37.7530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0157657042878504, distance: 1.1352877199727256 entropy -2.3948791343520606
epoch: 14, step: 52
	action: tensor([[-0.0014,  0.0552,  0.3285,  0.5456, -0.5220,  0.3529, -0.2525]],
       dtype=torch.float64)
	q_value: tensor([[-41.9970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1352877199727256 entropy -2.7218754679527417
epoch: 14, step: 53
	action: tensor([[-0.2171, -0.2599, -0.0707, -0.1289,  0.1925, -0.1539,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2916652866083007, distance: 1.300563876880038 entropy -3.354457626078163
epoch: 14, step: 54
	action: tensor([[-0.0442,  0.0416,  0.3578,  0.3539, -0.2051,  0.0176,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-33.6038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.300563876880038 entropy -3.1102351803806454
epoch: 14, step: 55
	action: tensor([[ 0.1433, -0.0998,  0.0938,  0.1663, -0.0444,  0.0259,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4422843292919717, distance: 0.8546004496962512 entropy -3.354457626078163
epoch: 14, step: 56
	action: tensor([[ 0.2996,  0.2521,  0.0545, -0.9067,  0.5858,  0.3140,  0.0674]],
       dtype=torch.float64)
	q_value: tensor([[-36.1465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42350481081908087, distance: 0.8688694691849094 entropy -2.6682585834609505
epoch: 14, step: 57
	action: tensor([[-0.2923, -0.3280,  0.0441,  0.2109,  0.4265, -0.3983, -0.2934]],
       dtype=torch.float64)
	q_value: tensor([[-39.2447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30432703128909644, distance: 1.3069228184921486 entropy -2.6133982838603593
epoch: 14, step: 58
	action: tensor([[ 0.6380,  0.0951, -0.2327, -0.6112,  0.3182,  0.0605,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-36.7107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5349782728929056, distance: 0.7803569734335942 entropy -3.188691765138923
epoch: 14, step: 59
	action: tensor([[-0.1524, -0.5808,  0.1811, -0.0070,  0.3697,  0.0148, -0.2203]],
       dtype=torch.float64)
	q_value: tensor([[-35.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3500575766450291, distance: 1.329636224585567 entropy -2.7197000945925582
epoch: 14, step: 60
	action: tensor([[ 0.0412,  0.0839, -0.1384, -0.0157, -0.4386, -0.0583,  0.0193]],
       dtype=torch.float64)
	q_value: tensor([[-38.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39433994181933973, distance: 0.8905763548030337 entropy -2.709181444703869
epoch: 14, step: 61
	action: tensor([[ 0.6792,  0.1634, -0.1800,  0.1832, -0.5267,  0.0802,  0.0924]],
       dtype=torch.float64)
	q_value: tensor([[-32.8002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9014244894294957, distance: 0.3592867578910675 entropy -2.8453384918161797
epoch: 14, step: 62
	action: tensor([[-0.9888,  0.7590,  1.2117, -0.7217, -0.0241,  0.0567,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-37.3524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3592867578910675 entropy -2.4704353320911747
epoch: 14, step: 63
	action: tensor([[-0.1931,  0.0463,  0.0062,  0.1146,  0.2923,  0.1233,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-44.1715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1686738117103953, distance: 1.0433797992922211 entropy -3.354457626078163
LOSS epoch 14 actor 621.3582361601774 critic 117.07832398142415
epoch: 15, step: 0
	action: tensor([[-0.1474, -0.2993, -0.5352,  0.5294, -0.2339,  0.3180,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-33.7766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04652449446383389, distance: 1.1706616518566764 entropy -2.9713694009022826
epoch: 15, step: 1
	action: tensor([[ 0.0964, -0.1379, -0.0516,  0.0898,  0.2894,  0.2065,  0.1200]],
       dtype=torch.float64)
	q_value: tensor([[-33.5964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36128471698018905, distance: 0.9145560925600934 entropy -2.8073798493441067
epoch: 15, step: 2
	action: tensor([[ 0.1138, -0.3460,  0.0200,  0.1637, -0.3645,  0.0639, -0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-34.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19451347856316437, distance: 1.0270363905799673 entropy -2.7734288112800733
epoch: 15, step: 3
	action: tensor([[-0.4466, -0.0616,  0.3003, -0.2604,  0.2658, -0.1471,  0.1446]],
       dtype=torch.float64)
	q_value: tensor([[-34.2161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5430656081547209, distance: 1.421506628586004 entropy -2.5278373002066843
epoch: 15, step: 4
	action: tensor([[-0.1910,  0.0388, -0.1232, -0.1185,  0.1330, -0.2175, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-34.3961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015516663579821177, distance: 1.153188281018103 entropy -2.936348987788836
epoch: 15, step: 5
	action: tensor([[-0.0689, -0.2835,  0.0586,  0.4244,  0.1441,  0.2096,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-31.0775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28157670103909893, distance: 0.9699445283652681 entropy -3.3605884731045848
epoch: 15, step: 6
	action: tensor([[ 0.3514,  0.0247, -0.4236, -0.6937, -0.4922,  0.4347,  0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-36.0562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39211461832200867, distance: 0.8922109379253619 entropy -2.658350643565192
epoch: 15, step: 7
	action: tensor([[ 0.6860, -0.2104, -0.4997,  0.0905, -0.6047,  0.0489, -0.0531]],
       dtype=torch.float64)
	q_value: tensor([[-33.3875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.597553524308998, distance: 0.7259567664394959 entropy -2.524952087540368
epoch: 15, step: 8
	action: tensor([[-0.5772,  0.4608,  0.1117, -0.8147, -0.1060, -0.0846, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[-34.7518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.466457811709295, distance: 1.385771036128969 entropy -2.528964441021808
epoch: 15, step: 9
	action: tensor([[-0.2707,  0.2304,  0.0729,  0.0569,  0.0045, -0.0192, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-35.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.385771036128969 entropy -3.2600442229126734
epoch: 15, step: 10
	action: tensor([[-0.3042,  0.2671,  0.2987,  0.0347, -0.1420,  0.1607,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14686556060196398, distance: 1.0569767399597583 entropy -3.3537353235228173
epoch: 15, step: 11
	action: tensor([[ 0.5272,  0.0386, -0.1875, -0.2935, -0.9496,  0.0739,  0.1257]],
       dtype=torch.float64)
	q_value: tensor([[-34.4887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0569767399597583 entropy -2.689176397963916
epoch: 15, step: 12
	action: tensor([[ 0.0783, -0.1357, -0.1399,  0.0703,  0.0262,  0.0262,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29464620195101765, distance: 0.9610814564737975 entropy -3.3537353235228173
epoch: 15, step: 13
	action: tensor([[ 0.1927, -0.0952, -0.4890,  0.2888,  0.6431,  0.3475,  0.0346]],
       dtype=torch.float64)
	q_value: tensor([[-32.6514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5574488351820197, distance: 0.7612695123976884 entropy -2.872552034149378
epoch: 15, step: 14
	action: tensor([[-0.2755,  0.0608, -0.1425,  0.0858, -0.4649,  0.2820, -0.0908]],
       dtype=torch.float64)
	q_value: tensor([[-35.0596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06312194234000168, distance: 1.1076389699062843 entropy -3.0327279842951236
epoch: 15, step: 15
	action: tensor([[ 0.1229,  0.2797, -0.5541, -0.1899,  0.0813,  0.3859,  0.1646]],
       dtype=torch.float64)
	q_value: tensor([[-32.4670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1076389699062843 entropy -2.7381610895153154
epoch: 15, step: 16
	action: tensor([[-0.1895, -0.0918,  0.0592,  0.1271,  0.2459,  0.0234,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.053701629864292055, distance: 1.1131936986789353 entropy -3.3537353235228173
epoch: 15, step: 17
	action: tensor([[ 0.2696,  0.4174,  0.2793, -0.3984,  0.0946, -0.1264,  0.0331]],
       dtype=torch.float64)
	q_value: tensor([[-33.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5553228873023015, distance: 0.7630958326396635 entropy -2.9218878312083425
epoch: 15, step: 18
	action: tensor([[-0.2338, -0.4619, -0.0956,  0.4089, -0.0715,  0.2095, -0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-35.7677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1113175069088741, distance: 1.2063567851851404 entropy -2.737185671264766
epoch: 15, step: 19
	action: tensor([[ 0.1040,  0.0401,  0.3623,  0.2325, -0.2320,  0.2462,  0.1761]],
       dtype=torch.float64)
	q_value: tensor([[-34.6839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2063567851851404 entropy -2.686193044283585
epoch: 15, step: 20
	action: tensor([[ 0.2034, -0.2784,  0.0669,  0.0753, -0.1962,  0.0141,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31454864738578325, distance: 0.9474253746690281 entropy -3.3537353235228173
epoch: 15, step: 21
	action: tensor([[-0.6810, -0.4173,  0.6605, -0.1493,  0.4059,  0.5976,  0.0723]],
       dtype=torch.float64)
	q_value: tensor([[-34.4833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6419527758703703, distance: 1.4663479095869432 entropy -2.562139144464356
epoch: 15, step: 22
	action: tensor([[ 0.4846, -0.2920,  0.0272, -0.2321, -0.2244,  0.3396,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-39.9252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3676795209186088, distance: 0.9099663173251538 entropy -2.3137115511045367
epoch: 15, step: 23
	action: tensor([[-1.6648, -1.5259,  0.0025,  0.0149, -0.2695, -0.3693, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-35.9025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9099663173251538 entropy -2.347271911811134
epoch: 15, step: 24
	action: tensor([[-0.3025,  0.0320,  0.0613,  0.1617,  0.0409, -0.1336,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023544136217027445, distance: 1.1577371798997738 entropy -3.3537353235228173
epoch: 15, step: 25
	action: tensor([[-0.2332, -0.0523, -0.2165, -0.2111,  0.2178,  0.3517,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-32.7425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022683427708284798, distance: 1.1572503011317865 entropy -3.01415921559042
epoch: 15, step: 26
	action: tensor([[ 0.2191,  0.0629, -0.3817, -0.5475,  0.9401,  0.0667, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[-32.6507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3862469332926747, distance: 0.8965066819517837 entropy -3.0188632085830798
epoch: 15, step: 27
	action: tensor([[ 0.4160,  0.1829,  0.0214, -0.3953,  0.1090, -0.0635, -0.1930]],
       dtype=torch.float64)
	q_value: tensor([[-34.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5499258584714389, distance: 0.7677126984376287 entropy -2.960165321662988
epoch: 15, step: 28
	action: tensor([[ 0.5539,  0.0021,  0.4233, -0.2685, -0.1996,  0.1549, -0.1089]],
       dtype=torch.float64)
	q_value: tensor([[-33.7658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5784420453310137, distance: 0.7429940529860205 entropy -2.818836155412026
epoch: 15, step: 29
	action: tensor([[ 0.2777,  0.1485, -0.5316,  0.2835,  0.1358,  0.0772, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[-37.5899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7429940529860205 entropy -2.307092732548206
epoch: 15, step: 30
	action: tensor([[-0.2044, -0.0740, -0.1599, -0.0898,  0.0065, -0.1711,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09054866572715459, distance: 1.1950311301070258 entropy -3.3537353235228173
epoch: 15, step: 31
	action: tensor([[-0.2425,  0.1058, -0.0413, -0.2203, -0.1834,  0.0794,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[-31.0002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023111534843448522, distance: 1.1574924949821168 entropy -3.2148095617497954
epoch: 15, step: 32
	action: tensor([[ 0.3543, -0.2749, -0.3675, -0.5143, -0.4376,  0.1402,  0.0709]],
       dtype=torch.float64)
	q_value: tensor([[-31.9059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12722347480616802, distance: 1.0690751176089517 entropy -2.961177985838878
epoch: 15, step: 33
	action: tensor([[ 0.1158,  0.3619, -0.1097,  0.8477, -1.0217,  0.0852, -0.0368]],
       dtype=torch.float64)
	q_value: tensor([[-32.7636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0690751176089517 entropy -2.6182306438740355
epoch: 15, step: 34
	action: tensor([[ 0.3449,  0.3192,  0.1232, -0.2317,  0.0402,  0.1065,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7268607893438899, distance: 0.5980654557215237 entropy -3.3537353235228173
epoch: 15, step: 35
	action: tensor([[-0.2207,  0.1702, -0.1396,  0.3618,  0.4301,  0.3114,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33496096063999103, distance: 0.9332118897258731 entropy -3.3537353235228173
epoch: 15, step: 36
	action: tensor([[ 0.1619, -0.3060, -0.1934, -0.4581,  0.3134,  0.0592, -0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-35.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9332118897258731 entropy -3.009628107526484
epoch: 15, step: 37
	action: tensor([[ 0.3571, -0.1199,  0.1487, -0.0857, -0.1711,  0.1081,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4812782851102282, distance: 0.8241834847446642 entropy -3.3537353235228173
epoch: 15, step: 38
	action: tensor([[ 0.7704,  0.3715,  0.5174, -1.3085,  0.2611, -0.1026,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-35.0714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39682841838250016, distance: 0.8887449153269878 entropy -2.491351449144703
epoch: 15, step: 39
	action: tensor([[-0.8021, -1.8282, -0.2305, -0.6939, -0.7900, -0.4478, -0.4050]],
       dtype=torch.float64)
	q_value: tensor([[-42.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8887449153269878 entropy -2.249362512953787
epoch: 15, step: 40
	action: tensor([[-0.2711, -0.1375, -0.0452,  0.4632, -0.0955, -0.1457,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039276186003718405, distance: 1.1216464114999418 entropy -3.3537353235228173
epoch: 15, step: 41
	action: tensor([[ 0.1280,  0.4666,  0.1399, -0.1421, -0.4755,  0.4950,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-33.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1216464114999418 entropy -2.9029179502782094
epoch: 15, step: 42
	action: tensor([[ 0.1394,  0.1204,  0.0867,  0.0601, -0.2782, -0.1264,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1216464114999418 entropy -3.3537353235228173
epoch: 15, step: 43
	action: tensor([[-0.4087, -0.0392, -0.1605,  0.0214, -0.1575, -0.0849,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26492920922513896, distance: 1.287033359688461 entropy -3.3537353235228173
epoch: 15, step: 44
	action: tensor([[ 0.2347,  0.0680, -0.4900, -0.4482, -0.0975,  0.2720,  0.0945]],
       dtype=torch.float64)
	q_value: tensor([[-31.3543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46478502583287773, distance: 0.8371838099376664 entropy -3.162075337278511
epoch: 15, step: 45
	action: tensor([[ 0.2112, -0.7505, -0.1277, -0.2421,  0.9952,  0.0984, -0.0690]],
       dtype=torch.float64)
	q_value: tensor([[-31.4536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35936897941203383, distance: 1.3342136234505626 entropy -2.909991780738405
epoch: 15, step: 46
	action: tensor([[-0.0235, -0.4626, -0.1055, -0.1179, -0.1165,  0.0816, -0.2552]],
       dtype=torch.float64)
	q_value: tensor([[-37.7937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2014214626661084, distance: 1.2543085605811595 entropy -2.6513810280083905
epoch: 15, step: 47
	action: tensor([[-0.1967,  0.1214, -0.2696,  0.3299, -0.2249,  0.2107,  0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-32.6869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2543085605811595 entropy -2.7525607098117
epoch: 15, step: 48
	action: tensor([[ 0.0726, -0.1304, -0.0092,  0.0008, -0.2121, -0.0156,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.265203028755588, distance: 0.9809353195753513 entropy -3.3537353235228173
epoch: 15, step: 49
	action: tensor([[ 0.1805, -0.1856, -0.3337, -0.4542, -0.2538,  0.1637,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-32.9539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16499692362820695, distance: 1.045684645541114 entropy -2.721125527971401
epoch: 15, step: 50
	action: tensor([[-0.0145, -0.0084,  0.3076,  0.5060,  0.6136, -0.0520, -0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-32.0811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.045684645541114 entropy -2.7729774791595303
epoch: 15, step: 51
	action: tensor([[ 0.0601,  0.0698, -0.2269, -0.3856, -0.2260, -0.0990,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26521194039516616, distance: 0.9809293711509124 entropy -3.3537353235228173
epoch: 15, step: 52
	action: tensor([[-0.1691, -0.0107,  0.1824,  0.1359,  0.0429,  0.3978, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-31.1169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29850379534126736, distance: 0.9584497669319214 entropy -3.046763708245009
epoch: 15, step: 53
	action: tensor([[ 0.6255, -0.3778,  0.0408, -0.2649,  0.1339,  0.2242,  0.1050]],
       dtype=torch.float64)
	q_value: tensor([[-35.4287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27223488292447096, distance: 0.9762303626692718 entropy -2.6018493796505275
epoch: 15, step: 54
	action: tensor([[-0.1730,  0.6459,  0.1911, -1.4762,  0.2531,  0.4409, -0.1417]],
       dtype=torch.float64)
	q_value: tensor([[-36.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060852134284599124, distance: 1.1089799165272183 entropy -2.4032228702975176
epoch: 15, step: 55
	action: tensor([[-0.3507, -0.3309,  0.5572, -0.5492,  0.4202,  0.2675, -0.2581]],
       dtype=torch.float64)
	q_value: tensor([[-40.9399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6583185431765577, distance: 1.4736375243861541 entropy -2.6262607907445856
epoch: 15, step: 56
	action: tensor([[-0.2043, -0.6451,  0.1210, -0.0337, -0.8414, -0.0347, -0.1001]],
       dtype=torch.float64)
	q_value: tensor([[-38.0588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4930324259278742, distance: 1.3982708799141725 entropy -2.471390415440928
epoch: 15, step: 57
	action: tensor([[ 0.2069, -1.1439, -0.1288,  0.0693,  0.2452,  0.7274,  0.2482]],
       dtype=torch.float64)
	q_value: tensor([[-35.4955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2783378867606823, distance: 1.2938368718390751 entropy -2.3588270270417397
epoch: 15, step: 58
	action: tensor([[ 0.5100, -0.8441, -1.0806,  0.2788,  0.3384,  0.1775, -0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-40.4822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11477784708550054, distance: 1.2082334587659618 entropy -2.199745821791292
epoch: 15, step: 59
	action: tensor([[ 0.4024, -0.2897, -0.4766, -0.3845,  0.5365,  0.0866, -0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-35.5898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23979674905516202, distance: 0.9977495820132518 entropy -2.8517267606925367
epoch: 15, step: 60
	action: tensor([[ 0.0679, -0.2384,  0.4345, -0.1696,  0.5447, -0.1000, -0.1732]],
       dtype=torch.float64)
	q_value: tensor([[-32.7106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0020962737431398093, distance: 1.1455430554812729 entropy -2.935799679604737
epoch: 15, step: 61
	action: tensor([[-0.0438,  0.6597,  0.3153,  1.1944,  0.0300,  0.2245, -0.1073]],
       dtype=torch.float64)
	q_value: tensor([[-37.4561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1455430554812729 entropy -2.653800018486978
epoch: 15, step: 62
	action: tensor([[-0.0037, -0.2654,  0.0236, -0.0315, -0.1184,  0.0902,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-41.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07686715898597407, distance: 1.0994836968825406 entropy -3.3537353235228173
epoch: 15, step: 63
	action: tensor([[ 0.3126, -0.5333, -0.7633,  0.1541,  0.4786,  0.2972,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-33.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1895912440228219, distance: 1.0301696611315077 entropy -2.658648753060405
LOSS epoch 15 actor 539.8151283429336 critic 182.42962604390237
epoch: 16, step: 0
	action: tensor([[ 0.4424,  0.0171, -0.1536,  0.1922,  0.1694, -0.0837, -0.1073]],
       dtype=torch.float64)
	q_value: tensor([[-31.8051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7135191684249133, distance: 0.6124977216882588 entropy -2.9459495792775288
epoch: 16, step: 1
	action: tensor([[ 0.3905, -0.2269, -0.0372, -0.2682,  0.2331,  0.5288, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[-32.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45111749591828565, distance: 0.8478058073313828 entropy -2.8576264079552276
epoch: 16, step: 2
	action: tensor([[ 0.0700, -0.3597, -0.7396, -0.6800,  0.9922,  0.7665, -0.1151]],
       dtype=torch.float64)
	q_value: tensor([[-33.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3163340313890135, distance: 0.9461906984181211 entropy -2.461017332161398
epoch: 16, step: 3
	action: tensor([[ 0.3840,  0.1521, -0.5616, -0.0974, -0.1548,  0.1660, -0.2586]],
       dtype=torch.float64)
	q_value: tensor([[-33.6329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7045507752262303, distance: 0.6220110796124151 entropy -2.7441505174781726
epoch: 16, step: 4
	action: tensor([[ 0.3693, -0.0388, -0.1920,  0.6320, -0.0313,  0.1646, -0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-29.3008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6220110796124151 entropy -3.001441141778993
epoch: 16, step: 5
	action: tensor([[ 0.1262,  0.2279,  0.0189,  0.3223, -0.3129, -0.0555,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6724839309191617, distance: 0.6548969980561209 entropy -3.3520167025297463
epoch: 16, step: 6
	action: tensor([[ 0.4170,  0.5886, -0.5209,  0.3672, -0.1824,  0.5162,  0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-32.7532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6548969980561209 entropy -2.697968607246184
epoch: 16, step: 7
	action: tensor([[-0.2798,  0.1809,  0.0857, -0.0790, -0.1178, -0.0409,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012761674744789286, distance: 1.1370189335355352 entropy -3.3520167025297463
epoch: 16, step: 8
	action: tensor([[-0.3455,  0.0715, -0.2651, -0.5364,  0.2890, -0.2046,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-30.3878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23905463613039424, distance: 1.2738019863392318 entropy -2.951719817808098
epoch: 16, step: 9
	action: tensor([[-0.2552, -0.2368, -0.0452,  0.1457, -0.1728,  0.0110, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-29.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14605663749258624, distance: 1.2250666963347328 entropy -3.5632800821756816
epoch: 16, step: 10
	action: tensor([[-0.0107, -0.2359, -0.1740, -0.3026,  0.2434,  0.2790,  0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-30.4537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014848237265210118, distance: 1.135816733455153 entropy -2.8545717247144315
epoch: 16, step: 11
	action: tensor([[-0.0575, -0.2253,  0.4225, -0.3067, -0.3224,  0.1966, -0.0676]],
       dtype=torch.float64)
	q_value: tensor([[-30.5830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14854312047778273, distance: 1.226394927845613 entropy -2.824554413043073
epoch: 16, step: 12
	action: tensor([[0.5375, 0.2494, 0.3939, 0.3304, 1.6413, 0.5889, 0.1260]],
       dtype=torch.float64)
	q_value: tensor([[-33.8184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9462535789087481, distance: 0.2652964682403334 entropy -2.3721776659408804
epoch: 16, step: 13
	action: tensor([[ 0.0984, -0.7137, -0.2360,  0.7648,  0.0657,  0.3183, -0.4101]],
       dtype=torch.float64)
	q_value: tensor([[-46.3418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2368903022970935, distance: 0.9996550852811092 entropy -2.3199834876958194
epoch: 16, step: 14
	action: tensor([[ 0.1959,  0.2085, -0.2883,  0.3585, -0.4064,  0.5021,  0.1753]],
       dtype=torch.float64)
	q_value: tensor([[-36.8934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9996550852811092 entropy -2.4472720028208266
epoch: 16, step: 15
	action: tensor([[-0.2519, -0.0356,  0.2628, -0.0566,  0.0319, -0.0078,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09773266189737528, distance: 1.1989608065676352 entropy -3.3520167025297463
epoch: 16, step: 16
	action: tensor([[ 0.1112, -0.1492,  0.0313, -0.5460, -0.0521,  0.3408,  0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-31.5763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05130163867925086, distance: 1.1146044393806624 entropy -2.786103486246392
epoch: 16, step: 17
	action: tensor([[-0.1671, -0.5705,  0.4138, -0.0771,  0.2079, -0.0070, -0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-31.8167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.461488367026079, distance: 1.3834210344755213 entropy -2.54513754794717
epoch: 16, step: 18
	action: tensor([[-0.1586,  0.2338,  0.7944,  0.5479,  0.4672,  0.4900,  0.0429]],
       dtype=torch.float64)
	q_value: tensor([[-34.1368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3834210344755213 entropy -2.526643488257386
epoch: 16, step: 19
	action: tensor([[ 0.0161, -0.1310,  0.0989, -0.0447, -0.0524,  0.0770,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21073876709712613, distance: 1.0166397329133934 entropy -3.3520167025297463
epoch: 16, step: 20
	action: tensor([[0.2308, 0.1645, 0.0254, 0.0917, 0.2695, 0.5942, 0.0603]],
       dtype=torch.float64)
	q_value: tensor([[-31.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0166397329133934 entropy -2.687132953849852
epoch: 16, step: 21
	action: tensor([[ 0.2233,  0.0263,  0.2314,  0.0358, -0.0544,  0.1112,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.550000477342246, distance: 0.7676490553353459 entropy -3.3520167025297463
epoch: 16, step: 22
	action: tensor([[0.0852, 0.2058, 0.2019, 0.1607, 0.3446, 0.2768, 0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-33.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7676490553353459 entropy -2.56508728995529
epoch: 16, step: 23
	action: tensor([[-0.0720, -0.0873,  0.2918,  0.2377,  0.0053,  0.1171,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3254169462032229, distance: 0.9398843252803912 entropy -3.3520167025297463
epoch: 16, step: 24
	action: tensor([[-0.3336,  0.5447, -0.6848, -0.8391, -0.0987,  0.0387,  0.1139]],
       dtype=torch.float64)
	q_value: tensor([[-33.5855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37104749499573897, distance: 0.9075396721631271 entropy -2.6056841863730105
epoch: 16, step: 25
	action: tensor([[ 0.1340, -0.1026,  0.2298, -0.1287, -0.1556, -0.0471,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[-28.8825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23554088612354662, distance: 1.0005385460268423 entropy -3.633661316103116
epoch: 16, step: 26
	action: tensor([[ 0.1214, -0.7075,  0.1393, -0.1609,  0.0711,  0.0847,  0.0531]],
       dtype=torch.float64)
	q_value: tensor([[-32.4221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3429759009008526, distance: 1.3261443612970472 entropy -2.617095386025354
epoch: 16, step: 27
	action: tensor([[ 0.9884, -0.7643, -0.0591, -0.3807, -0.2637,  0.3640,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[-33.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20424338978401435, distance: 1.255780771388851 entropy -2.4698351710502213
epoch: 16, step: 28
	action: tensor([[-0.7611, -1.0116,  0.7247,  0.6319, -1.8910,  0.0060, -0.1717]],
       dtype=torch.float64)
	q_value: tensor([[-38.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7057980838719966, distance: 1.494584604660192 entropy -2.0950347529549527
epoch: 16, step: 29
	action: tensor([[-0.4930, -0.0527,  1.4811, -1.6182, -0.7696,  1.5898,  0.6058]],
       dtype=torch.float64)
	q_value: tensor([[-49.4634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3067345693652712, distance: 1.3081284271192168 entropy -1.769790597593168
epoch: 16, step: 30
	action: tensor([[-1.0780,  0.4111,  1.3036,  0.0942,  3.1482,  0.3364,  0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-51.3681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8325496121231553, distance: 1.5491182265491377 entropy -1.4842409523116296
epoch: 16, step: 31
	action: tensor([[-0.6105, -1.6207,  0.8484,  2.1930, -0.7352,  1.5887,  0.8191]],
       dtype=torch.float64)
	q_value: tensor([[-59.1234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5491182265491377 entropy -1.460904095127017
epoch: 16, step: 32
	action: tensor([[ 0.3155, -0.0719,  0.2756, -0.1873, -0.4040, -0.1266,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36223352772042794, distance: 0.9138765541470216 entropy -3.3520167025297463
epoch: 16, step: 33
	action: tensor([[ 0.1526,  0.3353, -0.5591, -0.4965,  0.1766,  0.0206,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-33.5042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6244051728858897, distance: 0.7013204364764464 entropy -2.4802847224164473
epoch: 16, step: 34
	action: tensor([[-0.3177, -0.1190, -0.2434, -0.1541, -0.2771,  0.1796, -0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-28.1339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18546640195055053, distance: 1.2459520190849378 entropy -3.341701765589774
epoch: 16, step: 35
	action: tensor([[-0.4864, -0.1255,  0.3823, -0.0216, -0.1528,  0.3168,  0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-29.1803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2733090268197138, distance: 1.2912894483009074 entropy -2.998550724741858
epoch: 16, step: 36
	action: tensor([[ 0.4413, -0.7600,  0.2555, -0.4099, -0.7756,  0.6170,  0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-33.5799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2412513571641195, distance: 1.2749306486296215 entropy -2.4743293424324895
epoch: 16, step: 37
	action: tensor([[-1.4134, -0.0042, -0.2973,  0.1941, -0.5266,  1.2155,  0.0861]],
       dtype=torch.float64)
	q_value: tensor([[-39.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0433383646615106, distance: 1.6357873217734855 entropy -1.932483391962238
epoch: 16, step: 38
	action: tensor([[ 0.0536, -1.1165, -0.2129, -0.1123, -1.7154,  0.8796,  0.2388]],
       dtype=torch.float64)
	q_value: tensor([[-39.6855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8663605440915889, distance: 1.5633436918929515 entropy -2.468819655835027
epoch: 16, step: 39
	action: tensor([[ 0.7758, -0.6661,  2.0871, -0.7390,  2.5588,  0.2252,  0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-44.8459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2783253145980451, distance: 0.9721369018513516 entropy -1.7949450657328867
epoch: 16, step: 40
	action: tensor([[ 2.0366, -1.1174, -4.1340,  0.1085,  1.4161,  1.6686, -1.0039]],
       dtype=torch.float64)
	q_value: tensor([[-66.5369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9721369018513516 entropy -1.603931878402481
epoch: 16, step: 41
	action: tensor([[ 0.2381,  0.0494,  0.0955,  0.2365,  0.1222, -0.0700,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6402632265100114, distance: 0.6863554883778683 entropy -3.3520167025297463
epoch: 16, step: 42
	action: tensor([[-2.2476e-05, -6.3714e-01, -2.8427e-01,  4.6181e-02,  1.1574e-01,
          2.2849e-01,  1.9537e-02]], dtype=torch.float64)
	q_value: tensor([[-33.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17330595658225634, distance: 1.239545094612554 entropy -2.748367703190055
epoch: 16, step: 43
	action: tensor([[ 0.1695, -0.3736, -0.3975, -0.6084, -1.0621,  0.3394,  0.0394]],
       dtype=torch.float64)
	q_value: tensor([[-31.2232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1332639229868513, distance: 1.2182101816678192 entropy -2.7174240803681093
epoch: 16, step: 44
	action: tensor([[-0.1218, -1.5792, -0.2470, -0.2973,  0.7889,  0.4125,  0.0808]],
       dtype=torch.float64)
	q_value: tensor([[-33.0879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2182101816678192 entropy -2.34422133641023
epoch: 16, step: 45
	action: tensor([[ 0.1417,  0.0649, -0.0120,  0.2642,  0.1471, -0.0146,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5738054504003165, distance: 0.7470688683474321 entropy -3.3520167025297463
epoch: 16, step: 46
	action: tensor([[ 0.1059, -0.4236, -0.0041,  0.3284, -0.0313, -0.1666,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-32.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18202453749729908, distance: 1.0349677834523032 entropy -2.844565305519346
epoch: 16, step: 47
	action: tensor([[-0.1431,  0.1294,  0.2648, -0.6418,  0.3135,  0.5923,  0.0954]],
       dtype=torch.float64)
	q_value: tensor([[-32.5954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08788093316952184, distance: 1.0929051196702375 entropy -2.726722903696284
epoch: 16, step: 48
	action: tensor([[ 0.4277, -0.0830,  0.5474, -0.1274,  1.2371,  0.3100, -0.1452]],
       dtype=torch.float64)
	q_value: tensor([[-33.7167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5249882370691114, distance: 0.7886946147765058 entropy -2.527137612682695
epoch: 16, step: 49
	action: tensor([[ 0.1764, -0.3081, -0.0862, -0.3003, -0.0283, -0.0483,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032384117977182214, distance: 1.125662471324048 entropy -3.3520167025297463
epoch: 16, step: 50
	action: tensor([[ 0.2265,  0.6817, -0.1542,  0.4357, -0.1346,  0.1284, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-30.8148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.125662471324048 entropy -2.7615798659975446
epoch: 16, step: 51
	action: tensor([[-0.0006,  0.0772, -0.1657, -0.2759,  0.0390,  0.2132,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286405549928325, distance: 0.9666793137827628 entropy -3.3520167025297463
epoch: 16, step: 52
	action: tensor([[-0.1224, -0.6538,  0.3400, -0.0792, -0.0835, -0.1172, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[-30.0309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5008437276188074, distance: 1.4019238705678214 entropy -2.908774750580154
epoch: 16, step: 53
	action: tensor([[-0.4320, -0.7200,  0.3093, -0.1013,  0.3177,  0.4964,  0.1052]],
       dtype=torch.float64)
	q_value: tensor([[-33.2345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5997435259580028, distance: 1.4473776894113355 entropy -2.4989335657452725
epoch: 16, step: 54
	action: tensor([[-1.2152, -0.2165, -0.8056, -0.4657, -0.3411, -0.0220,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-34.8344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1954158571311084, distance: 1.6955675314179273 entropy -2.4025337735106223
epoch: 16, step: 55
	action: tensor([[-0.1969, -0.1227, -0.1320, -0.0556,  0.2519,  0.0713,  0.0592]],
       dtype=torch.float64)
	q_value: tensor([[-31.6296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03147852452027111, distance: 1.1622158352986687 entropy -3.714993445602579
epoch: 16, step: 56
	action: tensor([[ 0.5923,  0.2876, -0.1149, -0.3174, -0.1154,  0.0820, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-29.9979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7926384561358745, distance: 0.5210996747444525 entropy -3.101689900718943
epoch: 16, step: 57
	action: tensor([[ 0.8442,  0.0509,  0.6407, -0.1076,  0.0082,  0.2175, -0.0956]],
       dtype=torch.float64)
	q_value: tensor([[-32.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7917073197801092, distance: 0.5222683373624217 entropy -2.656575576572265
epoch: 16, step: 58
	action: tensor([[ 0.1922,  0.0235,  0.4098,  0.9335,  0.9449,  0.1230, -0.1237]],
       dtype=torch.float64)
	q_value: tensor([[-40.1146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5222683373624217 entropy -2.152126042816778
epoch: 16, step: 59
	action: tensor([[-0.0993, -0.0936,  0.1062,  0.0505,  0.0871, -0.1220,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-36.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08892534624959203, distance: 1.0922792302546216 entropy -3.3520167025297463
epoch: 16, step: 60
	action: tensor([[-0.3123, -0.3143,  0.1807,  0.2416,  0.2031,  0.1547,  0.0516]],
       dtype=torch.float64)
	q_value: tensor([[-31.0852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11028063055124226, distance: 1.2057938791201557 entropy -2.8987240051276566
epoch: 16, step: 61
	action: tensor([[ 0.8702, -0.0468,  0.0812, -0.5959, -0.3649,  0.3802,  0.1005]],
       dtype=torch.float64)
	q_value: tensor([[-32.9906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4979182986057967, distance: 0.8108562901603594 entropy -2.7150985909270937
epoch: 16, step: 62
	action: tensor([[-0.5510,  0.0720, -1.1144,  0.4220,  0.5914,  0.1001, -0.1439]],
       dtype=torch.float64)
	q_value: tensor([[-35.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5417254142446704, distance: 1.420889186154254 entropy -2.1776406129809938
epoch: 16, step: 63
	action: tensor([[-0.0467,  0.0101,  0.3139, -0.1186, -0.0113,  0.0113,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[-30.4837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13830516110849023, distance: 1.0622663862928134 entropy -3.620884146744001
LOSS epoch 16 actor 519.0425569976655 critic 369.9265296321287
epoch: 17, step: 0
	action: tensor([[-0.4319, -0.2712, -0.7460,  0.0009,  0.5235,  0.1658,  0.0512]],
       dtype=torch.float64)
	q_value: tensor([[-31.4291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.445557898334465, distance: 1.375860614736799 entropy -2.6569824914657856
epoch: 17, step: 1
	action: tensor([[ 0.0095, -0.1155, -0.1069,  0.0662, -0.1483,  0.0513,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[-27.3417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21353245459065284, distance: 1.0148388770414964 entropy -3.6048853204762494
epoch: 17, step: 2
	action: tensor([[-0.6826,  0.2014, -0.1280,  0.2296,  0.4233, -0.1301,  0.0871]],
       dtype=torch.float64)
	q_value: tensor([[-29.5378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4093188347235379, distance: 1.3585052506054633 entropy -2.8235159292974266
epoch: 17, step: 3
	action: tensor([[-0.2071,  0.0369, -0.2187, -0.0395, -0.3072,  0.0049,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-29.2715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032592040428404134, distance: 1.125541522981194 entropy -3.558025885550856
epoch: 17, step: 4
	action: tensor([[ 0.3098, -0.0177, -0.0034, -0.1235, -0.5771, -0.0603,  0.0985]],
       dtype=torch.float64)
	q_value: tensor([[-27.7396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45653872884852886, distance: 0.8436085902543566 entropy -3.036328844729387
epoch: 17, step: 5
	action: tensor([[-0.6445,  0.1805,  0.3055, -0.4594,  0.0656,  0.2251,  0.0728]],
       dtype=torch.float64)
	q_value: tensor([[-31.3987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6306850277333715, distance: 1.4613079100283721 entropy -2.555682876834657
epoch: 17, step: 6
	action: tensor([[ 0.4246,  0.2943, -0.0255, -0.5829,  0.0295,  0.1739,  0.0420]],
       dtype=torch.float64)
	q_value: tensor([[-30.9311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6163982971196647, distance: 0.7087563394934369 entropy -2.804845432917794
epoch: 17, step: 7
	action: tensor([[ 0.6075, -0.0920, -0.7306, -0.1198, -0.5210, -0.2605, -0.1385]],
       dtype=torch.float64)
	q_value: tensor([[-31.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.602554893288974, distance: 0.7214317813689338 entropy -2.6720032781191714
epoch: 17, step: 8
	action: tensor([[-0.1761,  0.4192,  0.0163, -0.2310, -0.4225,  0.2361, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[-29.3560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7214317813689338 entropy -2.937183405496395
epoch: 17, step: 9
	action: tensor([[-0.0079,  0.0602, -0.0547,  0.0600,  0.2395, -0.0824,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3236980902422609, distance: 0.9410809888892242 entropy -3.3502483254022932
epoch: 17, step: 10
	action: tensor([[ 0.0131,  0.4505, -0.4656,  0.0030, -0.2274,  0.2375, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-29.6230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9410809888892242 entropy -3.0444754233741778
epoch: 17, step: 11
	action: tensor([[-0.6230, -0.0586, -0.2018,  0.1243,  0.3684, -0.1566,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.572203532317316, distance: 1.4348651154243643 entropy -3.3502483254022932
epoch: 17, step: 12
	action: tensor([[ 0.0463, -0.3423, -0.1022, -0.0871, -0.0922,  0.0634,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[-28.2276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009958080334503627, distance: 1.150027875566926 entropy -3.5718017787653524
epoch: 17, step: 13
	action: tensor([[ 0.4584, -0.3913, -0.2939,  0.3452,  0.0289,  0.1475,  0.0569]],
       dtype=torch.float64)
	q_value: tensor([[-29.7823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5035931939554725, distance: 0.806260821899788 entropy -2.7476551292030726
epoch: 17, step: 14
	action: tensor([[-0.2445, -0.1622,  0.4824, -0.6349,  0.5322,  0.0511,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-33.0946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5561852759723553, distance: 1.427536903900372 entropy -2.6255544662299903
epoch: 17, step: 15
	action: tensor([[ 0.2967,  0.0195, -0.3608,  1.1714,  0.7053,  0.5444, -0.1772]],
       dtype=torch.float64)
	q_value: tensor([[-32.9445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.427536903900372 entropy -2.6304147643262863
epoch: 17, step: 16
	action: tensor([[ 0.3250,  0.1796, -0.1448,  0.2002, -0.0345,  0.1558,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.747917358292666, distance: 0.5745504431634479 entropy -3.3502483254022932
epoch: 17, step: 17
	action: tensor([[ 0.4038,  0.1162, -0.2145,  0.1693,  0.0721,  0.1783,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7658590192034932, distance: 0.5537266295506047 entropy -3.3502483254022932
epoch: 17, step: 18
	action: tensor([[-0.0862, -0.0516,  0.1689, -0.1317,  0.1016,  0.0326,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08026627759081606, distance: 1.097457595123728 entropy -3.3502483254022932
epoch: 17, step: 19
	action: tensor([[ 0.0492, -0.0365, -0.1207,  0.0668,  0.0614,  0.2134,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-30.0846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3768224769274139, distance: 0.9033635925968595 entropy -2.8050212387042843
epoch: 17, step: 20
	action: tensor([[ 0.0680, -0.2330,  0.2829, -0.3267, -0.1951,  0.2580,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-29.9729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01413830512480918, distance: 1.1362259128407768 entropy -2.819406316864163
epoch: 17, step: 21
	action: tensor([[-1.3071, -0.6470, -0.4391, -0.1027, -0.1304,  0.7348,  0.0615]],
       dtype=torch.float64)
	q_value: tensor([[-32.0548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6034538552954238, distance: 1.846424846002827 entropy -2.404976191310122
epoch: 17, step: 22
	action: tensor([[ 0.2481, -0.0900, -0.1460, -0.1602, -0.6289,  0.0333,  0.1216]],
       dtype=torch.float64)
	q_value: tensor([[-33.6610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3909123548714447, distance: 0.8930928005095375 entropy -2.880706114995892
epoch: 17, step: 23
	action: tensor([[ 0.1533, -0.6009, -0.1307, -0.1299, -0.1826,  0.5007,  0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-30.5913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028110305715220996, distance: 1.1603167176360114 entropy -2.562490775660421
epoch: 17, step: 24
	action: tensor([[-0.6263,  0.0633, -0.1494,  0.2165, -0.6168,  0.0342,  0.0638]],
       dtype=torch.float64)
	q_value: tensor([[-32.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3154620343459167, distance: 1.3124895446044156 entropy -2.35924877566016
epoch: 17, step: 25
	action: tensor([[-0.5535,  0.0157,  0.5766, -0.2435, -0.0013,  0.4113,  0.1949]],
       dtype=torch.float64)
	q_value: tensor([[-29.5214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34927370046070516, distance: 1.3292501590117443 entropy -2.856445393148185
epoch: 17, step: 26
	action: tensor([[-0.8733,  0.3542,  0.1424,  0.0499, -0.6749,  0.5588,  0.1250]],
       dtype=torch.float64)
	q_value: tensor([[-33.1839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40929300783669453, distance: 1.358492802704653 entropy -2.3734352881122383
epoch: 17, step: 27
	action: tensor([[-1.0401, -0.3394, -0.0428,  0.6387,  0.6217,  0.5563,  0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-32.0771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5564478923477869, distance: 1.427657351882417 entropy -2.413056149798467
epoch: 17, step: 28
	action: tensor([[ 0.0588, -0.1816, -0.4075, -0.9486,  0.0547, -0.1111,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[-34.1429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024814619230516977, distance: 1.1584554827139193 entropy -2.8679594992993924
epoch: 17, step: 29
	action: tensor([[ 0.0459, -0.0675,  0.2277, -0.2465, -0.0127, -0.2472, -0.1225]],
       dtype=torch.float64)
	q_value: tensor([[-28.2216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0937912442574762, distance: 1.0893584844915918 entropy -3.113551963810558
epoch: 17, step: 30
	action: tensor([[ 0.6513,  0.2006, -0.3828, -0.3795, -0.6299,  0.1916,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[-30.5437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7525130186793956, distance: 0.56928910512709 entropy -2.8299144620604904
epoch: 17, step: 31
	action: tensor([[ 0.3507, -0.3026,  0.0113, -0.2724,  0.4404,  0.2714, -0.0472]],
       dtype=torch.float64)
	q_value: tensor([[-31.3212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27187824410284545, distance: 0.9764695325570554 entropy -2.5086955578031787
epoch: 17, step: 32
	action: tensor([[-0.0848, -0.2230,  0.5043, -0.6483, -0.1082,  0.3066, -0.1527]],
       dtype=torch.float64)
	q_value: tensor([[-31.9837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30012008924999733, distance: 1.3048134590389746 entropy -2.5928861652423913
epoch: 17, step: 33
	action: tensor([[ 0.6386, -1.1250, -0.7234, -0.3240,  0.0888,  0.3025,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-32.8188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5808976717273235, distance: 1.4388269811528174 entropy -2.338560073638941
epoch: 17, step: 34
	action: tensor([[ 0.2296, -1.0622,  1.0643, -0.9168, -0.1863,  0.8079, -0.1937]],
       dtype=torch.float64)
	q_value: tensor([[-33.5033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.719606338852878, distance: 1.5006216640545704 entropy -2.4187712689735075
epoch: 17, step: 35
	action: tensor([[-1.5802, -1.5671, -1.6676,  0.2590, -2.6041,  0.5304, -0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-41.4345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5006216640545704 entropy -1.719904632553648
epoch: 17, step: 36
	action: tensor([[ 0.0181,  0.1943, -0.1921,  0.3668, -0.2346,  0.0694,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5266398974510429, distance: 0.7873222382510277 entropy -3.3502483254022932
epoch: 17, step: 37
	action: tensor([[-0.4654,  0.1508,  0.0557,  0.0406, -0.0252,  0.1709,  0.1029]],
       dtype=torch.float64)
	q_value: tensor([[-30.4962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10364033382407234, distance: 1.2021827034642067 entropy -2.8240217786535298
epoch: 17, step: 38
	action: tensor([[-0.2065, -0.2553, -0.0355, -0.3038,  0.1129, -0.1231,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3439130757002751, distance: 1.3266069951865926 entropy -3.3502483254022932
epoch: 17, step: 39
	action: tensor([[-0.1914,  0.1739,  0.2967,  0.4441, -0.0898,  0.1789, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[-28.3687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3266069951865926 entropy -3.02202315743002
epoch: 17, step: 40
	action: tensor([[ 0.0312,  0.0118, -0.4672,  0.1291,  0.1446,  0.2709,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3936786395647085, distance: 0.8910624191138087 entropy -3.3502483254022932
epoch: 17, step: 41
	action: tensor([[ 0.0658, -0.4805,  0.3000, -0.0080,  0.1258, -0.0041,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[-28.7414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07673843104801525, distance: 1.1874403443206318 entropy -3.062850404991972
epoch: 17, step: 42
	action: tensor([[ 0.6372, -0.7448,  0.3569, -0.8556,  0.6032, -0.0084,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-32.8415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5943030650200367, distance: 1.4449144482904954 entropy -2.5316536896823556
epoch: 17, step: 43
	action: tensor([[-0.3806,  0.3724, -0.0682, -0.9168, -0.2277, -0.2381, -0.3749]],
       dtype=torch.float64)
	q_value: tensor([[-36.4298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15734920250193896, distance: 1.2310874431945489 entropy -2.228589118650537
epoch: 17, step: 44
	action: tensor([[ 0.0343,  0.0767, -0.0476,  0.0019,  0.2313, -0.0296, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[-28.8960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3680366059018704, distance: 0.9097093422584037 entropy -3.436749340209777
epoch: 17, step: 45
	action: tensor([[-0.5088,  0.1573, -0.5694,  0.4654,  0.2666,  0.1949, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[-29.5651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9097093422584037 entropy -3.032005459341353
epoch: 17, step: 46
	action: tensor([[ 0.0654, -0.0996, -0.2760, -0.1002,  0.0547,  0.0263,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9097093422584037 entropy -3.3502483254022932
epoch: 17, step: 47
	action: tensor([[ 0.1942,  0.3739, -0.1874,  0.1205, -0.2302,  0.0150,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.686092248379375, distance: 0.6411471460253374 entropy -3.3502483254022932
epoch: 17, step: 48
	action: tensor([[ 0.0226, -0.0139,  0.2179,  0.1167,  0.2754,  0.0232,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3958691544138875, distance: 0.8894513497037219 entropy -3.3502483254022932
epoch: 17, step: 49
	action: tensor([[ 0.5337, -0.5283,  0.2285,  0.0016, -0.0804, -0.1180,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[-31.7339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1390432396881719, distance: 1.0618113504771667 entropy -2.7753421874374196
epoch: 17, step: 50
	action: tensor([[ 0.4282,  0.1123, -0.5293,  0.4876,  0.3411,  0.5589, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-34.5944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8058112758540865, distance: 0.5042764623279049 entropy -2.404847464899496
epoch: 17, step: 51
	action: tensor([[-0.2542, -0.5957, -0.3336,  0.7440,  0.3964,  0.1934, -0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-33.6055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1109944785040069, distance: 1.2061814456320952 entropy -2.759200834629503
epoch: 17, step: 52
	action: tensor([[-0.6108, -0.5710,  0.2483, -0.2138,  0.0475,  0.3126,  0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-33.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2061814456320952 entropy -2.8870159250584995
epoch: 17, step: 53
	action: tensor([[ 0.0395,  0.1327, -0.4759,  0.1524, -0.1577, -0.0020,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46213536838441005, distance: 0.8392535499104419 entropy -3.3502483254022932
epoch: 17, step: 54
	action: tensor([[-0.1001,  0.2804,  0.0828, -0.2717,  0.3508,  0.1994,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[-28.0583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3118641715137914, distance: 0.9492787923131838 entropy -3.141091993607521
epoch: 17, step: 55
	action: tensor([[ 0.2123, -0.0889,  0.2261, -0.0943, -0.0679,  0.1709,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.411880870124156, distance: 0.8775853127429089 entropy -3.3502483254022932
epoch: 17, step: 56
	action: tensor([[ 0.9141, -0.5944,  0.1066,  0.0423,  0.0984,  0.9610,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[-32.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.550415717800609, distance: 0.7672947966108375 entropy -2.49547391303824
epoch: 17, step: 57
	action: tensor([[ 0.1134, -0.9899, -1.6313,  0.7892,  0.4316, -0.0627, -0.1424]],
       dtype=torch.float64)
	q_value: tensor([[-40.5935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7887151180963001, distance: 1.5304786727948112 entropy -1.9969208262399654
epoch: 17, step: 58
	action: tensor([[-0.2005,  0.1803, -0.0661, -0.3165,  0.0199, -0.0947, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-33.4469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0754733675529966, distance: 1.1003134111355375 entropy -3.0765659806474517
epoch: 17, step: 59
	action: tensor([[ 0.7916,  0.3976, -0.0269, -0.0755,  0.0514,  0.1868,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-27.9439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9600698513983722, distance: 0.22866892836557481 entropy -3.244775769513546
epoch: 17, step: 60
	action: tensor([[-0.5087,  0.6980, -0.5164,  0.9005, -0.8598,  0.4549, -0.1203]],
       dtype=torch.float64)
	q_value: tensor([[-34.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22899863036460244, distance: 1.0048107391554606 entropy -2.5146420646421968
epoch: 17, step: 61
	action: tensor([[-1.1458,  0.7218,  0.3207, -1.2988,  0.0453,  0.3582,  0.1907]],
       dtype=torch.float64)
	q_value: tensor([[-34.4055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0048107391554606 entropy -2.576470683890908
epoch: 17, step: 62
	action: tensor([[ 0.0998, -0.1725,  0.1291,  0.1722, -0.3177,  0.0008,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3638191913347939, distance: 0.9127397723462921 entropy -3.3502483254022932
epoch: 17, step: 63
	action: tensor([[-0.0100, -0.0467,  0.0163, -0.3808,  0.1694,  0.9568,  0.1343]],
       dtype=torch.float64)
	q_value: tensor([[-32.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3859679859297477, distance: 0.8967103874646443 entropy -2.549198563108918
LOSS epoch 17 actor 431.7993817228248 critic 667.7710046892693
epoch: 18, step: 0
	action: tensor([[ 0.0022, -1.4813,  0.6317, -0.3745,  0.3430, -0.3444, -0.0715]],
       dtype=torch.float64)
	q_value: tensor([[-32.9179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8967103874646443 entropy -2.3796989811954607
epoch: 18, step: 1
	action: tensor([[-0.3228, -0.3064,  0.1746, -0.0742,  0.0809,  0.0246,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3949155646654203, distance: 1.3515454455518054 entropy -3.3491207555117546
epoch: 18, step: 2
	action: tensor([[-0.3212, -0.0872, -0.0328,  0.4165,  0.7448, -0.0244,  0.0797]],
       dtype=torch.float64)
	q_value: tensor([[-30.8529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008299389386399292, distance: 1.1395856808380143 entropy -2.784840318902483
epoch: 18, step: 3
	action: tensor([[-0.1368, -0.1456, -0.0468, -0.2044,  0.3544,  0.1113, -0.0349]],
       dtype=torch.float64)
	q_value: tensor([[-33.4834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06439998387831558, distance: 1.180617245954917 entropy -3.141972426648734
epoch: 18, step: 4
	action: tensor([[-0.1598,  0.1443, -0.1547,  0.2251, -0.0561,  0.0998, -0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-30.2785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2604378178714769, distance: 0.9841108972672371 entropy -3.006065337165292
epoch: 18, step: 5
	action: tensor([[-0.2123,  0.0552, -0.5100,  0.0707, -0.5561,  0.2919,  0.0907]],
       dtype=torch.float64)
	q_value: tensor([[-30.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9841108972672371 entropy -2.9946425241101196
epoch: 18, step: 6
	action: tensor([[-0.3259, -0.0451, -0.1310, -0.2800,  0.1198,  0.1156,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2342396727695104, distance: 1.2713245814586491 entropy -3.3491207555117546
epoch: 18, step: 7
	action: tensor([[-0.0649, -0.1586, -0.0768, -0.3390,  0.2581,  0.3744,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-29.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03276708842752363, distance: 1.125439687606047 entropy -3.104675342013717
epoch: 18, step: 8
	action: tensor([[-0.5800, -0.2793,  0.2404, -0.0928,  0.3019,  0.3162, -0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-30.4447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5918004516590805, distance: 1.4437799455565152 entropy -2.7885393103780993
epoch: 18, step: 9
	action: tensor([[ 0.0700, -0.0351,  0.1193, -0.0116, -0.1927,  0.4380,  0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-31.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4021902070294673, distance: 0.8847859259887328 entropy -2.7491531884908
epoch: 18, step: 10
	action: tensor([[-0.3071,  1.3066,  0.5467,  0.1626, -0.3383, -0.0089,  0.1108]],
       dtype=torch.float64)
	q_value: tensor([[-33.2283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8847859259887328 entropy -2.4435251606849238
epoch: 18, step: 11
	action: tensor([[-0.2189, -0.0992,  0.1351,  0.2289, -0.1754, -0.1487,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16268861736844076, distance: 1.233923978000973 entropy -3.3491207555117546
epoch: 18, step: 12
	action: tensor([[ 0.0041, -0.3046,  0.1698, -0.4054,  0.2144,  0.2136,  0.1470]],
       dtype=torch.float64)
	q_value: tensor([[-31.7380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32146180760935983, distance: 1.3154792474261887 entropy -2.790822386321674
epoch: 18, step: 13
	action: tensor([[-0.9534,  0.2424,  0.7729,  0.3025,  0.1640, -0.2210, -0.0695]],
       dtype=torch.float64)
	q_value: tensor([[-31.7195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8324952494544857, distance: 1.5490952490469498 entropy -2.575106859776936
epoch: 18, step: 14
	action: tensor([[-0.4211, -0.9890, -0.0984,  0.4392, -0.4417,  0.1404,  0.1713]],
       dtype=torch.float64)
	q_value: tensor([[-35.5444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9402931421200482, distance: 1.5940075262057398 entropy -2.6007188413954343
epoch: 18, step: 15
	action: tensor([[-0.4569, -0.5660, -0.7918,  1.0070, -0.2577,  0.4351,  0.2591]],
       dtype=torch.float64)
	q_value: tensor([[-34.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8288318678451949, distance: 1.5475460591910644 entropy -2.4052405665864995
epoch: 18, step: 16
	action: tensor([[-0.8570, -0.3417, -0.1159,  0.9333, -0.0746,  0.4520,  0.1257]],
       dtype=torch.float64)
	q_value: tensor([[-36.5886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6003625991248742, distance: 1.4476577174298189 entropy -2.7370438443421414
epoch: 18, step: 17
	action: tensor([[-0.2563, -0.1292, -0.1018,  0.7034,  1.0046,  0.2429,  0.2409]],
       dtype=torch.float64)
	q_value: tensor([[-36.3213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09938513509240943, distance: 1.0859910564592077 entropy -2.615694763623775
epoch: 18, step: 18
	action: tensor([[-0.3398, -0.5562,  0.7853, -0.1844,  0.6743,  0.2578, -0.0813]],
       dtype=torch.float64)
	q_value: tensor([[-36.7185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8090030027959465, distance: 1.5391336638416784 entropy -2.9226629298294378
epoch: 18, step: 19
	action: tensor([[-0.7068, -0.7160, -0.2273, -0.1243,  0.3024, -0.6002, -0.1051]],
       dtype=torch.float64)
	q_value: tensor([[-37.1200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2258373192426282, distance: 1.7072746941844066 entropy -2.317148152962109
epoch: 18, step: 20
	action: tensor([[ 0.2506,  0.0237,  0.4626, -0.0453,  0.1739,  0.1453,  0.0516]],
       dtype=torch.float64)
	q_value: tensor([[-30.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4085865804794129, distance: 0.8800397330476635 entropy -3.4799860969205816
epoch: 18, step: 21
	action: tensor([[-0.3906,  0.0990, -0.0108, -0.4344,  0.5854,  0.4279, -0.0236]],
       dtype=torch.float64)
	q_value: tensor([[-35.3340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3541805953319255, distance: 1.331665003186485 entropy -2.46306811350373
epoch: 18, step: 22
	action: tensor([[-0.0154,  0.0808, -0.1238,  0.2479, -0.3393,  0.4129, -0.1290]],
       dtype=torch.float64)
	q_value: tensor([[-31.3334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.331665003186485 entropy -3.0062687283701517
epoch: 18, step: 23
	action: tensor([[-0.1290, -0.0675, -0.1927,  0.2425,  0.1798,  0.0836,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1825112887835033, distance: 1.0346597993562916 entropy -3.3491207555117546
epoch: 18, step: 24
	action: tensor([[-0.5214, -0.3349, -0.2939,  0.5150, -0.1828,  0.1042,  0.0488]],
       dtype=torch.float64)
	q_value: tensor([[-30.8289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44112675499743603, distance: 1.3737502479156967 entropy -3.018844263259255
epoch: 18, step: 25
	action: tensor([[-0.6367, -0.1746,  0.2368,  0.3576,  0.0186,  0.1787,  0.1722]],
       dtype=torch.float64)
	q_value: tensor([[-31.4425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33820256618110034, distance: 1.3237855081977346 entropy -2.9063513237389533
epoch: 18, step: 26
	action: tensor([[-1.6924, -0.0238,  0.3947,  0.2931, -0.2779,  0.4182,  0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-32.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3237855081977346 entropy -2.660451484411297
epoch: 18, step: 27
	action: tensor([[ 0.2109, -0.0584,  0.3076, -0.0196, -0.2103, -0.0696,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42907142646644125, distance: 0.8646644082901398 entropy -3.3491207555117546
epoch: 18, step: 28
	action: tensor([[-0.6020, -0.1610, -0.1472,  0.6372, -0.5985, -0.2852,  0.0662]],
       dtype=torch.float64)
	q_value: tensor([[-34.0264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2527267818545953, distance: 1.2808104856836753 entropy -2.516519519065499
epoch: 18, step: 29
	action: tensor([[-0.2183, -0.1859, -0.0808, -0.0863, -0.3926,  0.3252,  0.2135]],
       dtype=torch.float64)
	q_value: tensor([[-32.7412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0915030551796403, distance: 1.195553929189449 entropy -2.8062812416311322
epoch: 18, step: 30
	action: tensor([[-0.9719,  0.3798,  0.9193,  0.0730, -0.4185,  0.2080,  0.1643]],
       dtype=torch.float64)
	q_value: tensor([[-31.3323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46620997044171797, distance: 1.385653928851813 entropy -2.5423391024734903
epoch: 18, step: 31
	action: tensor([[ 0.2812, -0.3156,  0.8980,  0.6872, -1.6080,  0.9264,  0.3285]],
       dtype=torch.float64)
	q_value: tensor([[-36.9269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.385653928851813 entropy -2.1898780950693566
epoch: 18, step: 32
	action: tensor([[ 0.4467, -0.2479,  0.0110, -0.0732, -0.2943, -0.1461,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34615794113120635, distance: 0.9253224945939896 entropy -3.3491207555117546
epoch: 18, step: 33
	action: tensor([[ 0.3528, -0.5210,  0.7369, -0.3711,  0.2831,  0.2975,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[-33.2597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06171562712120515, distance: 1.1791275810789574 entropy -2.5798473069619385
epoch: 18, step: 34
	action: tensor([[-0.6581,  0.4227, -0.6563, -0.9038,  0.1840,  0.6795, -0.1420]],
       dtype=torch.float64)
	q_value: tensor([[-37.9718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008649327292656217, distance: 1.1492825028518627 entropy -2.138882374639923
epoch: 18, step: 35
	action: tensor([[-0.0956,  0.0184, -0.2934,  0.2476,  0.0546, -0.1366, -0.0162]],
       dtype=torch.float64)
	q_value: tensor([[-31.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2649042377799967, distance: 0.9811347384919624 entropy -3.3970463622668174
epoch: 18, step: 36
	action: tensor([[-0.0587, -0.1379, -0.0751,  0.0257, -0.1067,  0.1232,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-29.9667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.175766168500463, distance: 1.0389195328596763 entropy -3.2439240304566432
epoch: 18, step: 37
	action: tensor([[-0.0425, -0.0145, -0.0237, -0.0214,  0.1714,  0.4253,  0.0919]],
       dtype=torch.float64)
	q_value: tensor([[-30.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3670178624510715, distance: 0.9104422860117298 entropy -2.7813287084356806
epoch: 18, step: 38
	action: tensor([[ 0.2615, -0.3922, -0.1413, -0.3258, -0.3855,  0.0319,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[-31.7139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008674255932551223, distance: 1.1393702766518237 entropy -2.706414831325872
epoch: 18, step: 39
	action: tensor([[-0.8785, -0.4443,  0.8036,  0.3829,  0.4802,  0.0877,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-31.6589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.696725050075037, distance: 1.4906045039006972 entropy -2.5777554130644424
epoch: 18, step: 40
	action: tensor([[ 2.0265e-01, -5.9521e-01,  3.1558e-01,  4.9840e-04, -1.0319e-01,
          6.3192e-01,  1.4667e-01]], dtype=torch.float64)
	q_value: tensor([[-37.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2017682087415822, distance: 1.0224008538483764 entropy -2.4273149020221583
epoch: 18, step: 41
	action: tensor([[ 0.1058, -0.2680,  1.4253,  0.3214, -0.0662,  0.2560,  0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-36.5855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45096311735538763, distance: 0.8479250257792654 entropy -2.1389387634361716
epoch: 18, step: 42
	action: tensor([[ 0.4545, -0.1351,  1.2390, -0.0105,  1.1208,  0.2478,  0.1382]],
       dtype=torch.float64)
	q_value: tensor([[-44.5293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5230137232021888, distance: 0.790332125263597 entropy -1.9186958336332656
epoch: 18, step: 43
	action: tensor([[-0.1811,  0.5408, -0.1693,  0.6622,  2.7431,  0.3822, -0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-45.9790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.790332125263597 entropy -2.0449386929612703
epoch: 18, step: 44
	action: tensor([[-0.3206,  0.0492, -0.2203,  0.3129, -0.0175,  0.0211,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017504354565996305, distance: 1.1342845336382885 entropy -3.3491207555117546
epoch: 18, step: 45
	action: tensor([[-0.5184, -0.0889, -0.0537, -0.4761, -0.2552,  0.1239,  0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-30.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6025354730488262, distance: 1.4486401543828455 entropy -3.1058659533838116
epoch: 18, step: 46
	action: tensor([[ 0.2884,  0.0993,  0.3572,  0.3971, -0.0701,  0.0360,  0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-30.0691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4486401543828455 entropy -2.942766407643967
epoch: 18, step: 47
	action: tensor([[-0.0842, -0.1472,  0.2934,  0.0085,  0.2312,  0.2495,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17449161776895372, distance: 1.03972248712744 entropy -3.3491207555117546
epoch: 18, step: 48
	action: tensor([[-0.3163,  0.2289, -0.6875,  0.3777,  0.1369,  0.1262,  0.0258]],
       dtype=torch.float64)
	q_value: tensor([[-32.9954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.03972248712744 entropy -2.601658461451916
epoch: 18, step: 49
	action: tensor([[-0.2753, -0.2069, -0.1809,  0.1046, -0.0844, -0.0246,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18976348263356413, distance: 1.2482081409624728 entropy -3.3491207555117546
epoch: 18, step: 50
	action: tensor([[-0.3249, -0.3490,  0.0128,  0.3449,  0.3729,  0.2414,  0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-29.5964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10109638117206088, distance: 1.2007963549398022 entropy -3.041484500931602
epoch: 18, step: 51
	action: tensor([[0.3145, 0.5276, 0.2077, 0.0432, 0.3612, 0.1491, 0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-32.6816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2007963549398022 entropy -2.811729776403987
epoch: 18, step: 52
	action: tensor([[ 0.1197,  0.0291, -0.0652,  0.1336, -0.1109, -0.1531,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4410887389735654, distance: 0.8555159744646255 entropy -3.3491207555117546
epoch: 18, step: 53
	action: tensor([[ 0.3895, -0.0941, -0.3582,  0.3489,  0.1368,  0.3169,  0.0652]],
       dtype=torch.float64)
	q_value: tensor([[-31.4031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7101171071372077, distance: 0.6161238018468463 entropy -2.9019412077635165
epoch: 18, step: 54
	action: tensor([[-0.5100, -0.3514, -0.0854, -0.2661, -0.2059, -0.1062, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[-33.9032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6691023814805537, distance: 1.478421200432341 entropy -2.7177695780717395
epoch: 18, step: 55
	action: tensor([[ 0.2317,  0.1550, -0.1639, -0.2309, -0.2288,  0.3275,  0.1085]],
       dtype=torch.float64)
	q_value: tensor([[-29.5715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5740776680619679, distance: 0.7468302474825949 entropy -3.0618705948266274
epoch: 18, step: 56
	action: tensor([[ 0.0013,  0.2441,  0.0085, -0.5991, -0.7847,  0.0242,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-31.5711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21964855648799897, distance: 1.010885139632767 entropy -2.624152179924608
epoch: 18, step: 57
	action: tensor([[-0.5648,  0.4962,  0.1036, -0.7202, -0.2234,  0.0852,  0.0932]],
       dtype=torch.float64)
	q_value: tensor([[-31.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33142939158018314, distance: 1.3204311563140314 entropy -2.618239821146088
epoch: 18, step: 58
	action: tensor([[-0.1132, -0.0080, -0.2519, -0.6200,  0.3900,  0.1946,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-31.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046321055353085394, distance: 1.1175263972494975 entropy -3.054250202326428
epoch: 18, step: 59
	action: tensor([[-0.2532, -0.3534, -0.2460, -0.0121, -0.1073,  0.3355, -0.1253]],
       dtype=torch.float64)
	q_value: tensor([[-29.5226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21626256684821166, distance: 1.2620319900348678 entropy -3.1102050820184437
epoch: 18, step: 60
	action: tensor([[-0.3756,  0.3753,  0.8278, -0.3289, -0.2518,  0.1639,  0.1146]],
       dtype=torch.float64)
	q_value: tensor([[-30.1677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08694525416155541, distance: 1.1930551742566806 entropy -2.8180463744334974
epoch: 18, step: 61
	action: tensor([[ 0.4760, -1.3193, -0.0888, -0.8665, -0.7706,  0.0631,  0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-36.4234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1930551742566806 entropy -2.300861306776347
epoch: 18, step: 62
	action: tensor([[ 0.1136, -0.0282,  0.0813, -0.1436,  0.0656, -0.2387,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-34.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24898415420561903, distance: 0.9917021241094515 entropy -3.3491207555117546
epoch: 18, step: 63
	action: tensor([[-0.1965,  0.2115,  0.0871, -0.4655,  0.5276,  0.4085, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[-31.5120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16155821374479773, distance: 1.0478356026861377 entropy -2.885035529468315
LOSS epoch 18 actor 477.9066820574506 critic 185.18068137204858
epoch: 19, step: 0
	action: tensor([[-0.1156,  0.1584, -0.3461, -0.2314,  0.0543,  0.3108, -0.1439]],
       dtype=torch.float64)
	q_value: tensor([[-33.7776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27590238807043566, distance: 0.9737674444270774 entropy -2.873067371574046
epoch: 19, step: 1
	action: tensor([[-0.1634, -0.1464,  0.5908, -0.0260, -0.1751,  0.3357,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-30.8560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10923104636795256, distance: 1.0800384807220431 entropy -3.1653775362975662
epoch: 19, step: 2
	action: tensor([[-0.7005, -0.6502,  0.1107, -0.0509,  0.6500,  0.4894,  0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-36.6891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7928391532319157, distance: 1.5322419815176154 entropy -2.2939332858121104
epoch: 19, step: 3
	action: tensor([[ 0.1503,  0.3884, -0.0728,  0.1954, -0.4558, -0.3547, -0.0591]],
       dtype=torch.float64)
	q_value: tensor([[-35.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5322419815176154 entropy -2.6769500221468836
epoch: 19, step: 4
	action: tensor([[-0.0440, -0.3273,  0.0063,  0.2166, -0.3757, -0.0110,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09263090437877186, distance: 1.0900556868289755 entropy -3.3478088317227552
epoch: 19, step: 5
	action: tensor([[-1.1394, -0.6270,  0.3099,  0.1831,  0.0220,  0.3360,  0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-33.9960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2934793656502395, distance: 1.7330221432910615 entropy -2.588326706859409
epoch: 19, step: 6
	action: tensor([[ 0.6001,  0.1155,  0.0022,  0.3019, -0.9353,  0.4722,  0.2339]],
       dtype=torch.float64)
	q_value: tensor([[-36.1212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7808253452366779, distance: 0.5357372682188175 entropy -2.5086302290818763
epoch: 19, step: 7
	action: tensor([[-1.8738,  0.5006,  1.2972, -1.5360,  1.7321,  0.2435,  0.1418]],
       dtype=torch.float64)
	q_value: tensor([[-40.9197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5357372682188175 entropy -2.0996971368575825
epoch: 19, step: 8
	action: tensor([[-0.1273,  0.0489, -0.1095,  0.1225, -0.1622, -0.2093,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1865057387301986, distance: 1.0321289034050383 entropy -3.3478088317227552
epoch: 19, step: 9
	action: tensor([[-0.4032, -0.0666, -0.2081, -0.1826, -0.3731,  0.1364,  0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-31.9712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31894216909270456, distance: 1.314224533280145 entropy -3.0462837181658395
epoch: 19, step: 10
	action: tensor([[-0.2410,  0.0247,  0.2688, -0.2136, -0.3019,  0.3473,  0.1298]],
       dtype=torch.float64)
	q_value: tensor([[-31.4317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03906788760968505, distance: 1.1664836472048592 entropy -2.944858380503483
epoch: 19, step: 11
	action: tensor([[-1.0576, -0.5242,  0.5124, -0.6876,  0.8081,  0.5673,  0.1653]],
       dtype=torch.float64)
	q_value: tensor([[-34.4132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3554563849776722, distance: 1.7562819048328273 entropy -2.4504540197436286
epoch: 19, step: 12
	action: tensor([[-0.6241, -0.0323,  0.1557, -0.5923,  0.6961,  0.1742, -0.2003]],
       dtype=torch.float64)
	q_value: tensor([[-39.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7345989360856859, distance: 1.5071491436169269 entropy -2.46069392556441
epoch: 19, step: 13
	action: tensor([[-0.4792,  0.1458,  0.0517,  0.1974, -0.4091,  0.1381, -0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-35.0337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09314518795437832, distance: 1.1964529282445993 entropy -3.0487621979144905
epoch: 19, step: 14
	action: tensor([[ 0.6264, -0.6184,  0.3766, -0.7603, -0.0176,  0.2811,  0.1991]],
       dtype=torch.float64)
	q_value: tensor([[-33.1798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3313452081604067, distance: 1.320389411641669 entropy -2.766623922822718
epoch: 19, step: 15
	action: tensor([[ 0.8770, -0.1128, -0.4255, -0.8991, -0.6087, -0.7979, -0.2019]],
       dtype=torch.float64)
	q_value: tensor([[-37.8832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13680649282644708, distance: 1.0631897371536114 entropy -2.1148827251424143
epoch: 19, step: 16
	action: tensor([[ 0.1807, -0.5126, -0.4095, -0.3350, -0.0431,  0.4481, -0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-36.7594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05288299926885154, distance: 1.1742126367580128 entropy -2.771075662673758
epoch: 19, step: 17
	action: tensor([[-0.2500, -0.5222,  0.1074, -0.0690,  0.0755,  0.4455, -0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-32.9591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31203266564609544, distance: 1.3107776183860074 entropy -2.608507073333167
epoch: 19, step: 18
	action: tensor([[-0.1496,  0.3556, -0.1187, -0.7927, -0.5439, -0.0369,  0.1001]],
       dtype=torch.float64)
	q_value: tensor([[-33.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11658113309404416, distance: 1.0755733385263782 entropy -2.5047549337460864
epoch: 19, step: 19
	action: tensor([[ 0.1182,  0.2071,  0.0996, -0.1001,  0.1062,  0.1544,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[-33.0846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5176828848627678, distance: 0.7947362630156228 entropy -2.9671175889066883
epoch: 19, step: 20
	action: tensor([[-0.6002, -0.3768, -0.3286,  0.6819,  0.4965, -0.0224, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[-33.9350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.540982927338405, distance: 1.4205469985453476 entropy -2.764093839897554
epoch: 19, step: 21
	action: tensor([[-0.2734, -0.2650, -0.1164, -0.1521, -0.0100, -0.0054,  0.0712]],
       dtype=torch.float64)
	q_value: tensor([[-34.5164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3166729316793222, distance: 1.313093486214536 entropy -3.2467759362190547
epoch: 19, step: 22
	action: tensor([[ 0.0331, -0.2259,  0.0048,  0.0427, -0.3687,  0.2150,  0.0657]],
       dtype=torch.float64)
	q_value: tensor([[-31.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1812700889434884, distance: 1.035444967682921 entropy -3.009459285138118
epoch: 19, step: 23
	action: tensor([[-1.1972, -0.9046,  0.3683, -0.6676,  0.1254,  0.4021,  0.1580]],
       dtype=torch.float64)
	q_value: tensor([[-34.0769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.484184118375723, distance: 1.803634706973739 entropy -2.503926212680621
epoch: 19, step: 24
	action: tensor([[-1.1589, -0.1042, -0.4696, -0.4858, -1.5095,  0.2465,  0.0593]],
       dtype=torch.float64)
	q_value: tensor([[-37.2116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3144310204715168, distance: 1.7409199979267518 entropy -2.396874678178051
epoch: 19, step: 25
	action: tensor([[ 0.0856, -0.6577,  0.4311, -0.8148,  0.8635,  0.0802,  0.2332]],
       dtype=torch.float64)
	q_value: tensor([[-37.6871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7365682842107522, distance: 1.5080044593069644 entropy -2.6805486224387347
epoch: 19, step: 26
	action: tensor([[ 0.6829, -0.5351,  0.2959, -0.0305, -0.3602,  0.9976, -0.3596]],
       dtype=torch.float64)
	q_value: tensor([[-37.3516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5626863569481612, distance: 0.7567513536485089 entropy -2.3470788932522924
epoch: 19, step: 27
	action: tensor([[-0.6402, -1.0823,  0.1525, -2.6688, -1.0155,  1.0284,  0.0700]],
       dtype=torch.float64)
	q_value: tensor([[-41.9294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7567513536485089 entropy -1.8851294710843038
epoch: 19, step: 28
	action: tensor([[ 0.2247, -0.4025,  0.0724,  0.0034,  0.4838, -0.0746,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1232272847292677, distance: 1.071519814558918 entropy -3.3478088317227552
epoch: 19, step: 29
	action: tensor([[-0.4845,  0.3085, -0.3270,  0.0045,  0.7464, -0.2151, -0.0832]],
       dtype=torch.float64)
	q_value: tensor([[-34.6420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12550587069327213, distance: 1.2140332338197732 entropy -2.7260230875015656
epoch: 19, step: 30
	action: tensor([[-0.1543, -0.0948,  0.0973,  0.2108, -0.2134,  0.0100, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-32.9438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18389856237606483, distance: 1.0337815207560745 entropy -3.639119269641316
epoch: 19, step: 31
	action: tensor([[ 0.5269,  0.3031, -0.8320, -0.1518,  0.1958,  0.2593,  0.1610]],
       dtype=torch.float64)
	q_value: tensor([[-33.6641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8740232137951178, distance: 0.4061644940328703 entropy -2.718804639834598
epoch: 19, step: 32
	action: tensor([[-0.0903,  0.0577, -0.9938, -0.0144, -0.3977, -0.0527, -0.1139]],
       dtype=torch.float64)
	q_value: tensor([[-32.8783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26613457986719724, distance: 0.9803133238231669 entropy -3.008084734218216
epoch: 19, step: 33
	action: tensor([[ 0.2125, -0.1442,  0.1115, -0.0952,  0.2993,  0.0628,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-29.7350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34507648654312095, distance: 0.9260874200056469 entropy -3.558289299728439
epoch: 19, step: 34
	action: tensor([[-0.4195,  0.3216, -0.4338,  0.1520, -0.4266,  0.0346, -0.0531]],
       dtype=torch.float64)
	q_value: tensor([[-34.2837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9260874200056469 entropy -2.715682815618422
epoch: 19, step: 35
	action: tensor([[ 0.0278,  0.1858, -0.1003, -0.0650,  0.0566, -0.0236,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40032992685009106, distance: 0.8861615066931003 entropy -3.3478088317227552
epoch: 19, step: 36
	action: tensor([[ 0.1918, -0.0738, -0.2976,  0.0611,  0.1238, -0.0557,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[-32.3708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4182957285559278, distance: 0.8727860972390727 entropy -3.016149161136952
epoch: 19, step: 37
	action: tensor([[ 0.0736,  0.0095,  0.0469, -0.5504, -0.3076, -0.0491,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[-32.3838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08256355544629801, distance: 1.0960861431169031 entropy -3.0424120996186947
epoch: 19, step: 38
	action: tensor([[ 0.1417, -0.1649, -0.2065,  0.3000,  0.4489, -0.1172,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[-32.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37906295558323366, distance: 0.9017382216418497 entropy -2.7524521398181014
epoch: 19, step: 39
	action: tensor([[-0.4438,  0.3313,  0.2893, -0.1852, -0.0614,  0.0516, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[-33.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11209266787585692, distance: 1.206777438003941 entropy -3.0653865186328426
epoch: 19, step: 40
	action: tensor([[-0.4424, -0.8484, -0.2382, -0.3812,  0.1238,  0.0543,  0.0882]],
       dtype=torch.float64)
	q_value: tensor([[-33.6231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8812324327125856, distance: 1.5695599990634956 entropy -2.8681749209693366
epoch: 19, step: 41
	action: tensor([[-0.7386,  0.0576,  0.1236,  0.6157,  0.9553,  0.3870,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-32.2910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004716128607308079, distance: 1.1416426275946985 entropy -2.873203732955645
epoch: 19, step: 42
	action: tensor([[-0.0049,  0.0955,  0.3575,  0.5497,  0.3224,  0.4021, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-38.9575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1416426275946985 entropy -2.9507218774865542
epoch: 19, step: 43
	action: tensor([[ 0.2937,  0.1722,  0.1484, -0.3184,  0.2934,  0.0189,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5314476510278138, distance: 0.7833137553034094 entropy -3.3478088317227552
epoch: 19, step: 44
	action: tensor([[-0.8187, -0.0249, -0.1697,  0.2122,  0.2807,  0.1217, -0.1170]],
       dtype=torch.float64)
	q_value: tensor([[-34.5963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6367919095209995, distance: 1.4640416369704525 entropy -2.7440798516590363
epoch: 19, step: 45
	action: tensor([[-0.2112,  0.0599,  0.5365,  0.0778, -0.0476,  0.0949,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[-32.3764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1943495787124998, distance: 1.0271408755989337 entropy -3.411406179339646
epoch: 19, step: 46
	action: tensor([[ 0.3513, -0.5041,  0.6137, -0.5681, -1.0022,  0.2813,  0.1379]],
       dtype=torch.float64)
	q_value: tensor([[-35.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21829939619593852, distance: 1.2630882868275826 entropy -2.499948098607473
epoch: 19, step: 47
	action: tensor([[-1.0545,  1.8651,  0.4916, -0.1486, -0.4930,  0.4134,  0.1504]],
       dtype=torch.float64)
	q_value: tensor([[-40.6168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2630882868275826 entropy -1.930928792566816
epoch: 19, step: 48
	action: tensor([[ 0.1572, -0.0170,  0.1626,  0.0651,  0.0227,  0.2147,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2630882868275826 entropy -3.3478088317227552
epoch: 19, step: 49
	action: tensor([[-0.2273,  0.2624,  0.0426, -0.3994, -0.3736, -0.1271,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008990577399774935, distance: 1.1391884816995108 entropy -3.3478088317227552
epoch: 19, step: 50
	action: tensor([[-0.2636,  0.4850, -0.1578, -0.4314, -0.1675, -0.1345,  0.0720]],
       dtype=torch.float64)
	q_value: tensor([[-32.3409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16293798374732316, distance: 1.046973069681779 entropy -2.9904079602935605
epoch: 19, step: 51
	action: tensor([[ 0.1279,  0.0834,  0.2541,  0.0636, -0.0992,  0.1275,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-31.6512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5126235892571614, distance: 0.7988936071940043 entropy -3.399548011605862
epoch: 19, step: 52
	action: tensor([[-0.4088, -0.6567, -0.0069,  0.5712,  0.3636,  0.1508,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-35.3620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3331475151965868, distance: 1.3212828471917533 entropy -2.571034362196223
epoch: 19, step: 53
	action: tensor([[-0.0964,  0.1233,  0.5891, -0.5001,  0.3199,  0.0753,  0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-35.7477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08419160050234176, distance: 1.1915429802969644 entropy -2.742535340016584
epoch: 19, step: 54
	action: tensor([[-0.0425, -0.6226, -0.1207, -0.7153,  0.2693,  0.5358, -0.1128]],
       dtype=torch.float64)
	q_value: tensor([[-35.9992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43767312549540494, distance: 1.3721031789617764 entropy -2.529582632762319
epoch: 19, step: 55
	action: tensor([[-0.8488, -0.6501, -0.1348, -0.8609, -0.4582,  0.1476, -0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-34.0513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0791751378116912, distance: 1.6500694738145791 entropy -2.5005270401355943
epoch: 19, step: 56
	action: tensor([[-0.9678, -0.2214,  0.1798,  0.1588,  0.0399,  0.1721,  0.1085]],
       dtype=torch.float64)
	q_value: tensor([[-33.8010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9919323310639405, distance: 1.6150797934071113 entropy -2.7939708950683064
epoch: 19, step: 57
	action: tensor([[ 0.5863,  0.2442,  0.2212, -0.3071,  0.6822,  0.3448,  0.1672]],
       dtype=torch.float64)
	q_value: tensor([[-33.9669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6150797934071113 entropy -2.823653005871165
epoch: 19, step: 58
	action: tensor([[-0.2631,  0.2579, -0.1170, -0.2758, -0.2501,  0.0124,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04437388358148464, distance: 1.1186666689733202 entropy -3.3478088317227552
epoch: 19, step: 59
	action: tensor([[-0.2277,  0.0967, -0.0325,  0.0438, -0.0302, -0.1096,  0.0662]],
       dtype=torch.float64)
	q_value: tensor([[-31.4472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06456971639856213, distance: 1.106782812077136 entropy -3.0769143136277184
epoch: 19, step: 60
	action: tensor([[-0.0416,  0.3636,  0.0116,  0.7610, -0.0026,  0.1324,  0.0724]],
       dtype=torch.float64)
	q_value: tensor([[-31.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.106782812077136 entropy -3.081050907380906
epoch: 19, step: 61
	action: tensor([[ 0.5069,  0.1156,  0.1489,  0.0601, -0.0557, -0.0426,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7596576127830139, distance: 0.5610116481846279 entropy -3.3478088317227552
epoch: 19, step: 62
	action: tensor([[-0.4731,  0.6197,  0.4668, -0.3784,  0.2733, -0.3336, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-36.6487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5610116481846279 entropy -2.569298334425597
epoch: 19, step: 63
	action: tensor([[-0.0031, -0.1670,  0.2132,  0.1653,  0.1015,  0.1858,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-37.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31596374878185596, distance: 0.9464468984836275 entropy -3.3478088317227552
LOSS epoch 19 actor 517.3047433401554 critic 141.68648221671634
epoch: 20, step: 0
	action: tensor([[-0.1111,  0.5699,  0.5100,  1.0969,  0.4632,  0.4996,  0.0874]],
       dtype=torch.float64)
	q_value: tensor([[-35.1898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9464468984836275 entropy -2.6016448693823584
epoch: 20, step: 1
	action: tensor([[-0.3979,  0.1103,  0.2903,  0.2438,  0.1148, -0.1325,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9464468984836275 entropy -3.3461270097956053
epoch: 20, step: 2
	action: tensor([[-0.2958, -0.1128, -0.0398,  0.5331, -0.1937,  0.0767,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10448423630200188, distance: 1.0829123604712017 entropy -3.3461270097956053
epoch: 20, step: 3
	action: tensor([[ 0.0399, -0.0181, -0.2466,  0.3430, -0.5813,  0.5016,  0.1909]],
       dtype=torch.float64)
	q_value: tensor([[-35.0629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36741772535355444, distance: 0.9101546715707485 entropy -2.725760500920514
epoch: 20, step: 4
	action: tensor([[ 0.7897, -0.0885,  0.3471, -0.3782,  0.9961, -0.1430,  0.1923]],
       dtype=torch.float64)
	q_value: tensor([[-36.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45335774870429346, distance: 0.8460738872385831 entropy -2.3882739992582542
epoch: 20, step: 5
	action: tensor([[ 0.2661, -0.0424, -0.1303, -0.0394, -0.1943, -0.0558,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49016964135835794, distance: 0.8170893305854687 entropy -3.3461270097956053
epoch: 20, step: 6
	action: tensor([[-0.3590,  0.3334,  0.1112, -0.5244,  0.0743,  0.2860,  0.0333]],
       dtype=torch.float64)
	q_value: tensor([[-33.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048437034571170656, distance: 1.171730864944087 entropy -2.7629707023612977
epoch: 20, step: 7
	action: tensor([[ 0.2096, -0.0007,  0.0924, -0.4306, -0.0468,  0.1787, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-33.3043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3075319470007598, distance: 0.9522622414648036 entropy -2.9142232104368384
epoch: 20, step: 8
	action: tensor([[-0.3581,  0.2840, -0.3131, -1.0980, -0.6840,  0.3926, -0.0323]],
       dtype=torch.float64)
	q_value: tensor([[-33.5988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11452136519081146, distance: 1.2080944589490767 entropy -2.6149793005245305
epoch: 20, step: 9
	action: tensor([[ 0.0450, -0.5758, -0.0311, -0.1654,  0.1400,  0.4192,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[-33.3477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11143679266370099, distance: 1.2064215269522478 entropy -2.8342126657618936
epoch: 20, step: 10
	action: tensor([[ 0.9029, -0.4683,  0.6962,  0.4064,  0.8575,  0.4557,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-34.0971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49342917455259594, distance: 0.8144731658896018 entropy -2.482491334466174
epoch: 20, step: 11
	action: tensor([[-1.1554, -0.1450, -0.0676, -0.7437,  1.0308,  0.2567, -0.2318]],
       dtype=torch.float64)
	q_value: tensor([[-48.0459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2891406596025168, distance: 1.7313821387635355 entropy -2.0545664043292553
epoch: 20, step: 12
	action: tensor([[-0.1904,  0.1361,  0.4741, -0.0684, -0.2692, -0.0363, -0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-38.8826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06907599198035963, distance: 1.10411372517207 entropy -3.1427260963723778
epoch: 20, step: 13
	action: tensor([[-0.5893,  0.2070, -0.3538,  0.1726, -0.1625,  0.3121,  0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-35.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2756237639694943, distance: 1.2924626271641475 entropy -2.574395154904099
epoch: 20, step: 14
	action: tensor([[-0.4859, -0.0032,  0.0188, -0.1927, -0.3909,  0.1599,  0.0977]],
       dtype=torch.float64)
	q_value: tensor([[-32.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42566497738852194, distance: 1.3663609253478384 entropy -3.147239200341677
epoch: 20, step: 15
	action: tensor([[-0.4264,  0.1091,  0.0556,  0.3166, -0.0237, -0.0162,  0.1694]],
       dtype=torch.float64)
	q_value: tensor([[-32.7114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026560439870370933, distance: 1.1594418049470563 entropy -2.769099861043883
epoch: 20, step: 16
	action: tensor([[-0.2715, -0.5135,  0.2632, -0.8676, -0.3821,  0.1617,  0.1279]],
       dtype=torch.float64)
	q_value: tensor([[-33.7614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8491405699893511, distance: 1.556114884430833 entropy -2.912656834405991
epoch: 20, step: 17
	action: tensor([[ 0.6107, -0.2902,  0.2796, -1.5242,  1.1588,  0.2232,  0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-34.6402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.333823961932985, distance: 1.321618017947971 entropy -2.3672297476176247
epoch: 20, step: 18
	action: tensor([[-0.5892, -0.3063, -0.0104,  0.0779,  1.5722,  0.0585, -0.5743]],
       dtype=torch.float64)
	q_value: tensor([[-41.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6493942270187194, distance: 1.4696669517698318 entropy -2.167930668225883
epoch: 20, step: 19
	action: tensor([[-0.2684,  0.1282,  0.4639,  0.4666,  0.3634,  0.1883, -0.1565]],
       dtype=torch.float64)
	q_value: tensor([[-42.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4696669517698318 entropy -2.8898935371597685
epoch: 20, step: 20
	action: tensor([[-0.0145, -0.1897, -0.2255, -0.5527, -0.1498,  0.0916,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07623646553249785, distance: 1.1871635251915278 entropy -3.3461270097956053
epoch: 20, step: 21
	action: tensor([[-0.6978, -0.4416, -0.5296, -0.1104,  0.2131,  0.0668, -0.0121]],
       dtype=torch.float64)
	q_value: tensor([[-31.7023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8836296320213111, distance: 1.5705597028258398 entropy -2.863582470458688
epoch: 20, step: 22
	action: tensor([[-0.1805, -0.0438,  0.0289, -0.0397,  0.2029, -0.0151,  0.0495]],
       dtype=torch.float64)
	q_value: tensor([[-31.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02322780176752215, distance: 1.1309758675961104 entropy -3.573915639252384
epoch: 20, step: 23
	action: tensor([[ 0.2604, -0.0643, -0.8095, -0.1495,  0.3972,  0.2110,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-32.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49601228646960094, distance: 0.8123939263030265 entropy -2.9954857279678393
epoch: 20, step: 24
	action: tensor([[-0.3520, -0.0472, -0.3169, -0.0928, -0.2625,  0.2042, -0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-31.4468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18274847978098419, distance: 1.2445229006214618 entropy -3.2267824326673784
epoch: 20, step: 25
	action: tensor([[-0.6330, -0.1101, -0.1157, -0.1762, -0.2936,  0.2440,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-31.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6294757373305531, distance: 1.4607659680394236 entropy -3.081180117596643
epoch: 20, step: 26
	action: tensor([[-0.2279, -0.1307,  0.1976, -0.0669, -0.7024,  0.1632,  0.1482]],
       dtype=torch.float64)
	q_value: tensor([[-32.3362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10625834954389646, distance: 1.2036077460402976 entropy -2.896142228431428
epoch: 20, step: 27
	action: tensor([[ 1.4694, -0.1778, -0.0735,  1.1455,  0.6821,  0.4181,  0.2541]],
       dtype=torch.float64)
	q_value: tensor([[-35.0613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8080193329388377, distance: 0.5014012836204917 entropy -2.3801768000397288
epoch: 20, step: 28
	action: tensor([[-1.0264, -1.5611, -0.3215,  0.8288,  0.5896,  0.7732, -0.2088]],
       dtype=torch.float64)
	q_value: tensor([[-53.1273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5014012836204917 entropy -2.1672919919227938
epoch: 20, step: 29
	action: tensor([[ 0.0028, -0.1644, -0.0704, -0.0046, -0.0845, -0.0916,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14365170485659773, distance: 1.0589657451085048 entropy -3.3461270097956053
epoch: 20, step: 30
	action: tensor([[ 0.2938, -0.4188, -0.4314,  0.2992, -0.3255,  0.3829,  0.0708]],
       dtype=torch.float64)
	q_value: tensor([[-32.6375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34254557386726703, distance: 0.9278750992486806 entropy -2.8599080745853263
epoch: 20, step: 31
	action: tensor([[ 0.0517,  0.1950,  0.5653, -0.0942,  0.7742,  0.5591,  0.0964]],
       dtype=torch.float64)
	q_value: tensor([[-35.8166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5945396320462387, distance: 0.7286700146860736 entropy -2.4872901120654047
epoch: 20, step: 32
	action: tensor([[ 0.4776, -0.7643, -0.7404, -0.9968,  1.3065, -0.0811, -0.2021]],
       dtype=torch.float64)
	q_value: tensor([[-38.9405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36376017187259646, distance: 1.3363668522954815 entropy -2.4017249944246015
epoch: 20, step: 33
	action: tensor([[ 0.5688, -0.1184,  0.1833,  0.4791, -0.0410,  0.1138, -0.3171]],
       dtype=torch.float64)
	q_value: tensor([[-36.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.867091692958535, distance: 0.41718891107297995 entropy -2.5695061003603716
epoch: 20, step: 34
	action: tensor([[-0.7624,  0.1019,  0.2250, -0.2258,  0.2006,  0.4455,  0.0877]],
       dtype=torch.float64)
	q_value: tensor([[-40.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5483312035680463, distance: 1.4239299552575104 entropy -2.3973527278393587
epoch: 20, step: 35
	action: tensor([[-0.3734, -0.0830,  0.2656,  0.1527,  0.6594, -0.0108,  0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-34.2844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10927102339904105, distance: 1.2052455245126548 entropy -2.770406073806512
epoch: 20, step: 36
	action: tensor([[-0.0332, -0.5810,  0.4741,  0.0279, -0.2315,  0.3364, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-35.4160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09131428305622835, distance: 1.1954505410167728 entropy -2.929375624001492
epoch: 20, step: 37
	action: tensor([[-0.0082,  0.4398,  1.0307, -0.4669, -0.1581,  0.8459,  0.2055]],
       dtype=torch.float64)
	q_value: tensor([[-37.1759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1954505410167728 entropy -2.2041372434886375
epoch: 20, step: 38
	action: tensor([[ 0.1464, -0.0555, -0.1689, -0.0149,  0.0326,  0.0382,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4084211636404388, distance: 0.880162796887867 entropy -3.3461270097956053
epoch: 20, step: 39
	action: tensor([[-0.0408, -0.0859, -0.1513,  0.1106, -0.3675,  0.3497,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-33.0213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26489348493065124, distance: 0.981141914397465 entropy -2.877424123567045
epoch: 20, step: 40
	action: tensor([[-0.7382,  0.0603,  0.5113, -0.3523,  0.4530,  0.1290,  0.1638]],
       dtype=torch.float64)
	q_value: tensor([[-34.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7817235170349857, distance: 1.5274846317027195 entropy -2.5794829854069197
epoch: 20, step: 41
	action: tensor([[-0.3058, -0.9371,  0.3570, -0.4427,  0.3955,  0.0673, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-35.9307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9935069403211938, distance: 1.6157180222196603 entropy -2.739923863490265
epoch: 20, step: 42
	action: tensor([[-0.4225, -0.6805, -0.8498, -0.5635, -0.5137,  0.6886, -0.0598]],
       dtype=torch.float64)
	q_value: tensor([[-35.1218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6789463546215, distance: 1.4827744814109403 entropy -2.4287086772461004
epoch: 20, step: 43
	action: tensor([[-0.2102,  0.1915,  0.3873, -0.5690,  0.0553,  0.0950,  0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-33.5660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12408827422025115, distance: 1.213268443724986 entropy -2.7461783752273305
epoch: 20, step: 44
	action: tensor([[ 0.5373, -0.2695, -0.5906, -0.3442, -0.2237, -0.2448, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-34.6794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.288847073372933, distance: 0.9650241772648636 entropy -2.6637829693342328
epoch: 20, step: 45
	action: tensor([[ 0.1338, -0.0637, -0.0276,  0.5478, -0.6243, -0.0666, -0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-32.2063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9650241772648636 entropy -2.952131576188276
epoch: 20, step: 46
	action: tensor([[-0.0696, -0.1418,  0.0643,  0.5388, -0.0788,  0.2161,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4276031908855259, distance: 0.865775506906581 entropy -3.3461270097956053
epoch: 20, step: 47
	action: tensor([[ 1.0307, -1.2673,  0.3427,  0.2180,  0.1489,  0.5457,  0.1754]],
       dtype=torch.float64)
	q_value: tensor([[-36.7432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.865775506906581 entropy -2.571217798212699
epoch: 20, step: 48
	action: tensor([[-0.2400,  0.0440, -0.2249, -0.0061, -0.0220,  0.1468,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03452984823354999, distance: 1.124413675908059 entropy -3.3461270097956053
epoch: 20, step: 49
	action: tensor([[ 0.2203, -0.3981,  0.0372, -0.1665, -0.5116,  0.3326,  0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-31.6495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07791309473454455, distance: 1.0988606472087865 entropy -3.0471778103306333
epoch: 20, step: 50
	action: tensor([[ 1.0526,  0.2072, -0.4129,  0.5307,  1.0166,  0.4967,  0.1357]],
       dtype=torch.float64)
	q_value: tensor([[-35.5054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9880763353880382, distance: 0.12495728171938898 entropy -2.293488383260772
epoch: 20, step: 51
	action: tensor([[-0.0202, -0.2445,  0.2060, -1.3992,  0.0383,  0.2044, -0.2472]],
       dtype=torch.float64)
	q_value: tensor([[-44.6089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5298818845540418, distance: 1.4154210312341082 entropy -2.4922179488390235
epoch: 20, step: 52
	action: tensor([[-0.5204, -0.0708,  0.6570,  1.0743,  0.4274,  0.2186, -0.2138]],
       dtype=torch.float64)
	q_value: tensor([[-36.1075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4154210312341082 entropy -2.4530360049333604
epoch: 20, step: 53
	action: tensor([[-2.3885e-04, -1.0136e-01, -1.5445e-02,  3.8112e-01,  6.6470e-02,
          2.9295e-01,  7.9911e-02]], dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4154210312341082 entropy -3.3461270097956053
epoch: 20, step: 54
	action: tensor([[ 0.2301, -0.1528,  0.1537,  0.3067,  0.3918,  0.1299,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6228922472238103, distance: 0.7027315040175478 entropy -3.3461270097956053
epoch: 20, step: 55
	action: tensor([[ 0.4853, -0.9530, -0.2829, -0.7626,  0.4594,  0.4957, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[-37.0871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5484595762172118, distance: 1.4239889832844106 entropy -2.6211790595353466
epoch: 20, step: 56
	action: tensor([[ 1.0587, -0.6218,  0.8934, -0.0608,  1.1321, -0.0693, -0.2965]],
       dtype=torch.float64)
	q_value: tensor([[-35.9084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0943419188301341, distance: 1.0890274497414985 entropy -2.3131617101075443
epoch: 20, step: 57
	action: tensor([[ 0.7263,  0.8470,  0.6636, -1.6386,  0.9735,  0.7447, -0.3762]],
       dtype=torch.float64)
	q_value: tensor([[-49.5906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7196048643956661, distance: 0.6059571730184355 entropy -1.9904171400937405
epoch: 20, step: 58
	action: tensor([[-0.7556, -0.8561, -0.0542,  0.7356, -0.9297,  0.5288, -0.5959]],
       dtype=torch.float64)
	q_value: tensor([[-47.7079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8303011721995166, distance: 1.5481675924452059 entropy -2.0158584698878355
epoch: 20, step: 59
	action: tensor([[ 1.4604,  0.4331, -0.5946, -0.6594,  0.0364,  1.4517,  0.4923]],
       dtype=torch.float64)
	q_value: tensor([[-41.7638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8086250255697377, distance: 0.5006097065756837 entropy -2.1029421207547796
epoch: 20, step: 60
	action: tensor([[ 1.5724, -0.4910,  0.8362, -1.4640,  1.3605, -0.1957, -0.3846]],
       dtype=torch.float64)
	q_value: tensor([[-43.9214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16024439113536515, distance: 1.048656250730153 entropy -1.957829824984655
epoch: 20, step: 61
	action: tensor([[ 0.1040,  0.0022,  1.6296, -0.4901,  1.5766,  0.2890, -0.7650]],
       dtype=torch.float64)
	q_value: tensor([[-52.9592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07771788913562949, distance: 1.0989769553344848 entropy -1.7938125254234591
epoch: 20, step: 62
	action: tensor([[-0.0218,  1.6240, -1.4484, -1.1275, -0.4367, -0.3975, -0.4709]],
       dtype=torch.float64)
	q_value: tensor([[-55.3997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0989769553344848 entropy -1.8846907241203492
epoch: 20, step: 63
	action: tensor([[-0.0855, -0.1370, -0.1639,  0.0081, -0.0284, -0.0875,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-36.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0671754000574184, distance: 1.1052402401363548 entropy -3.3461270097956053
LOSS epoch 20 actor 575.0456594471796 critic 224.07617839997184
epoch: 21, step: 0
	action: tensor([[ 0.2192, -0.2587, -0.2033,  0.5285,  0.3627,  0.0538,  0.0633]],
       dtype=torch.float64)
	q_value: tensor([[-30.6848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5200729657979595, distance: 0.7927646939358524 entropy -3.015325533556959
epoch: 21, step: 1
	action: tensor([[ 0.3448,  0.3564,  0.1522, -0.1212,  0.1540,  0.6191,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-35.2566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8480696746539305, distance: 0.44604525855247446 entropy -2.820196519426627
epoch: 21, step: 2
	action: tensor([[-0.0488,  1.1546, -0.3311,  0.4161,  0.6844,  0.4039, -0.0617]],
       dtype=torch.float64)
	q_value: tensor([[-34.7841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.44604525855247446 entropy -2.453347835884333
epoch: 21, step: 3
	action: tensor([[ 0.2552,  0.0656, -0.0189, -0.4289,  0.0051, -0.1479,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.346879956621962, distance: 0.9248114524339491 entropy -3.345050214139508
epoch: 21, step: 4
	action: tensor([[ 0.1072, -0.6521, -0.2869,  0.0873,  0.2379,  0.0782, -0.0731]],
       dtype=torch.float64)
	q_value: tensor([[-31.4054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12359816465149653, distance: 1.2130039185459567 entropy -2.853738578153817
epoch: 21, step: 5
	action: tensor([[0.2089, 0.2065, 0.4661, 0.3180, 0.4945, 0.2301, 0.0170]],
       dtype=torch.float64)
	q_value: tensor([[-31.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2130039185459567 entropy -2.7853955320894452
epoch: 21, step: 6
	action: tensor([[-0.2596,  0.0179,  0.1913,  0.1153, -0.0505, -0.1620,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0032682440204410135, distance: 1.1462127267218676 entropy -3.345050214139508
epoch: 21, step: 7
	action: tensor([[-0.1641, -0.7991, -0.3120, -0.3886,  0.0996,  0.1901,  0.1128]],
       dtype=torch.float64)
	q_value: tensor([[-32.1177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6462227270518743, distance: 1.4682533141546175 entropy -2.89131437558978
epoch: 21, step: 8
	action: tensor([[ 0.1910, -0.1253,  0.0921,  0.0366, -0.5003,  0.2493, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-30.7581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40479328746162124, distance: 0.8828574851708986 entropy -2.733005698709074
epoch: 21, step: 9
	action: tensor([[ 1.6100,  0.0269, -0.3586,  1.7260,  0.5268,  0.2484,  0.1629]],
       dtype=torch.float64)
	q_value: tensor([[-34.6334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8099548699692777, distance: 0.49886733252838067 entropy -2.3886162348960793
epoch: 21, step: 10
	action: tensor([[-0.8747, -0.4294, -0.0902,  0.8892, -0.2091,  0.0398, -0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-56.4100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.49886733252838067 entropy -2.1685662116714126
epoch: 21, step: 11
	action: tensor([[-0.3550,  0.0005, -0.0206,  0.2628, -0.3119, -0.0568,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0442033466517775, distance: 1.169362691764352 entropy -3.345050214139508
epoch: 21, step: 12
	action: tensor([[-0.1813,  0.1796,  0.5254,  0.1629,  0.2880,  0.4673,  0.1713]],
       dtype=torch.float64)
	q_value: tensor([[-32.5105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3183531618252605, distance: 0.9447924314200544 entropy -2.8389805346607355
epoch: 21, step: 13
	action: tensor([[-0.5464, -0.8948,  0.7643,  0.4253, -0.3401,  0.4053,  0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-35.9751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6009253514785435, distance: 1.4479122226222148 entropy -2.4161749999738937
epoch: 21, step: 14
	action: tensor([[-0.8568,  0.1237, -1.0838, -0.3217, -0.6103, -0.1219,  0.3975]],
       dtype=torch.float64)
	q_value: tensor([[-39.9652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4479122226222148 entropy -2.010259235751809
epoch: 21, step: 15
	action: tensor([[ 0.2753,  0.0906, -0.1774,  0.3881,  0.1180, -0.1109,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6808786040129049, distance: 0.6464495758304154 entropy -3.345050214139508
epoch: 21, step: 16
	action: tensor([[-0.0758, -0.0172,  0.1478,  0.0591, -0.4703, -0.1459,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2055776997105967, distance: 1.0199582770721702 entropy -3.345050214139508
epoch: 21, step: 17
	action: tensor([[ 0.8299,  0.6161,  0.1608, -0.2114,  0.2271,  0.3275,  0.1615]],
       dtype=torch.float64)
	q_value: tensor([[-32.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9902305831006019, distance: 0.11310739880770963 entropy -2.670343231789989
epoch: 21, step: 18
	action: tensor([[ 0.2049, -0.3381,  0.1991,  0.7437,  1.0215,  0.4365, -0.1932]],
       dtype=torch.float64)
	q_value: tensor([[-37.8261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7685439469522983, distance: 0.5505426446345598 entropy -2.377766070075213
epoch: 21, step: 19
	action: tensor([[ 0.2890, -0.5556,  0.0954,  1.0368,  0.1672,  0.0415, -0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-42.0225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7176134221094783, distance: 0.6081052018739451 entropy -2.482422078269327
epoch: 21, step: 20
	action: tensor([[-0.1345,  0.4394,  0.3025, -0.0063, -0.2735,  0.0551,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6081052018739451 entropy -3.345050214139508
epoch: 21, step: 21
	action: tensor([[ 0.2586, -0.0758, -0.0408,  0.3123, -0.0577,  0.0576,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6598949422770938, distance: 0.6673647113467268 entropy -3.345050214139508
epoch: 21, step: 22
	action: tensor([[ 0.2401, -0.1262, -0.9058, -0.2514,  0.1404,  0.2388,  0.0713]],
       dtype=torch.float64)
	q_value: tensor([[-34.8668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.434473081699125, distance: 0.8605643166145173 entropy -2.655287278908186
epoch: 21, step: 23
	action: tensor([[ 0.0851, -0.2913, -0.2633, -0.5864,  0.2160,  0.3152, -0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-29.6178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0005305662150891788, distance: 1.1446477889437943 entropy -3.2000004987784934
epoch: 21, step: 24
	action: tensor([[-0.2336, -0.2310, -0.2896, -0.3984,  0.1365,  0.1097, -0.1126]],
       dtype=torch.float64)
	q_value: tensor([[-29.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2180589317444015, distance: 1.2629636282987367 entropy -2.7900494773858404
epoch: 21, step: 25
	action: tensor([[-0.2206, -0.0124,  0.2963,  0.0828,  0.0106, -0.0827, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[-29.0791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08170384096621264, distance: 1.0965995849358154 entropy -3.2083216543722544
epoch: 21, step: 26
	action: tensor([[0.6901, 0.0507, 0.1984, 0.1630, 0.4728, 0.1054, 0.1132]],
       dtype=torch.float64)
	q_value: tensor([[-32.8731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8617067464884592, distance: 0.4255564626861575 entropy -2.765465884655968
epoch: 21, step: 27
	action: tensor([[-0.7930, -0.6731, -0.5753, -0.2897,  0.3485, -0.1736, -0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-37.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9955655449355265, distance: 1.6165520464879362 entropy -2.488832021613188
epoch: 21, step: 28
	action: tensor([[ 0.1801, -0.1707, -0.0015, -0.0353, -0.0738,  0.0413,  0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-30.0465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32121906108746767, distance: 0.942804208336793 entropy -3.607488099956761
epoch: 21, step: 29
	action: tensor([[-0.1635, -0.3008,  1.0912,  0.1424,  0.2311,  0.1523,  0.0481]],
       dtype=torch.float64)
	q_value: tensor([[-32.4883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06277121505813188, distance: 1.1078462769572472 entropy -2.6811771645076625
epoch: 21, step: 30
	action: tensor([[-0.5962,  0.1034, -0.5782, -0.4543,  0.9166,  1.0686,  0.0888]],
       dtype=torch.float64)
	q_value: tensor([[-39.1284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10015322595418019, distance: 1.0855278630542378 entropy -2.158311003413356
epoch: 21, step: 31
	action: tensor([[ 0.1033,  0.3358, -0.1138,  0.0715, -0.0989,  0.0279, -0.1561]],
       dtype=torch.float64)
	q_value: tensor([[-34.0137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0855278630542378 entropy -2.9689939835671173
epoch: 21, step: 32
	action: tensor([[ 0.1310,  0.0663, -0.2794,  0.2154,  0.2552, -0.1242,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5250362480393963, distance: 0.7886547558156054 entropy -3.345050214139508
epoch: 21, step: 33
	action: tensor([[-0.2244, -0.1681, -0.0374, -0.0396, -0.1048,  0.2000, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-32.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05934782654980175, distance: 1.1778120229163496 entropy -3.1155854732237316
epoch: 21, step: 34
	action: tensor([[ 0.1114, -0.2791, -0.4990,  0.7633,  0.6004, -0.0338,  0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-31.3012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34821049152643047, distance: 0.9238689601164021 entropy -2.777215719564146
epoch: 21, step: 35
	action: tensor([[ 0.5130,  0.0936,  0.0386, -0.1571,  0.0681,  0.0805, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-36.0513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6935671030661356, distance: 0.6334675705625327 entropy -3.054487967507652
epoch: 21, step: 36
	action: tensor([[ 0.1974, -0.6201, -0.1220,  0.5265, -0.6277,  0.3972, -0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-33.6109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27971842656962853, distance: 0.9711981480098528 entropy -2.6190597650138625
epoch: 21, step: 37
	action: tensor([[-0.4183,  0.0608,  0.4438, -0.1660,  0.0174,  0.2568,  0.2433]],
       dtype=torch.float64)
	q_value: tensor([[-37.9370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17567896522568316, distance: 1.2407979500333541 entropy -2.211785933475004
epoch: 21, step: 38
	action: tensor([[-0.8699,  0.0501, -0.1077, -0.1844, -1.1417,  0.1538,  0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-33.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8764114330588675, distance: 1.5675475677026451 entropy -2.4985307768625447
epoch: 21, step: 39
	action: tensor([[-0.0045, -0.6763, -0.6150,  0.1250,  0.6575,  0.6511,  0.2782]],
       dtype=torch.float64)
	q_value: tensor([[-34.6760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03049435716444937, distance: 1.161661248932424 entropy -2.560937812372103
epoch: 21, step: 40
	action: tensor([[ 0.1293, -0.1897,  0.0417, -0.0247, -0.4078,  0.2652, -0.1357]],
       dtype=torch.float64)
	q_value: tensor([[-33.1467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30074131957500216, distance: 0.9569199885873987 entropy -2.766087570075881
epoch: 21, step: 41
	action: tensor([[ 0.9819,  0.4932,  0.1127,  0.9464, -1.3127,  0.5835,  0.1512]],
       dtype=torch.float64)
	q_value: tensor([[-33.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8587059328856198, distance: 0.43014874134559805 entropy -2.46298774738708
epoch: 21, step: 42
	action: tensor([[ 0.2368,  0.3602, -0.8116, -1.1833, -1.9802,  2.0752,  0.1449]],
       dtype=torch.float64)
	q_value: tensor([[-50.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.43014874134559805 entropy -1.862861037350561
epoch: 21, step: 43
	action: tensor([[ 0.2562, -0.0193,  0.0973, -0.0692, -0.1441,  0.2136,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5163427184804693, distance: 0.7958396242256605 entropy -3.345050214139508
epoch: 21, step: 44
	action: tensor([[ 0.6502,  0.0875, -0.0153, -0.4742, -0.4397,  0.1337,  0.0562]],
       dtype=torch.float64)
	q_value: tensor([[-33.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5900333217872209, distance: 0.7327080664966605 entropy -2.524475698564774
epoch: 21, step: 45
	action: tensor([[ 0.3173,  0.3385,  0.2978,  0.5400,  0.5203,  0.7627, -0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-34.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9714146188862657, distance: 0.19347670684213228 entropy -2.3977285421611967
epoch: 21, step: 46
	action: tensor([[-0.3639, -0.3026, -1.0655, -0.8529, -0.3934,  0.0090, -0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-40.8348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.19347670684213228 entropy -2.3596761332998186
epoch: 21, step: 47
	action: tensor([[-0.0497, -0.1118,  0.1388,  0.0479,  0.2488, -0.0185,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16992172128598115, distance: 1.0425963928179305 entropy -3.345050214139508
epoch: 21, step: 48
	action: tensor([[ 0.1822, -0.0631,  0.1826, -0.1781, -0.2783,  0.2564,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-32.2781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36706667406284743, distance: 0.9104071815359931 entropy -2.8346944561200473
epoch: 21, step: 49
	action: tensor([[-0.8205,  0.1481, -0.5148, -0.0098,  0.8776,  0.2458,  0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-33.5814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5471390038533452, distance: 1.423381643696688 entropy -2.432417311409414
epoch: 21, step: 50
	action: tensor([[-0.1120, -0.0390, -0.1890,  0.0368,  0.0966,  0.0747, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[-32.0164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16491462429933823, distance: 1.0457361765034194 entropy -3.5216370633558474
epoch: 21, step: 51
	action: tensor([[-0.3050, -0.3757, -0.1451, -0.0296, -0.0529,  0.1180,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-30.7655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3487562929658299, distance: 1.3289952700512415 entropy -3.070385007477791
epoch: 21, step: 52
	action: tensor([[ 0.6772, -0.5533, -0.3740, -0.0996, -0.2994,  0.2358,  0.1104]],
       dtype=torch.float64)
	q_value: tensor([[-30.7342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18902207234922475, distance: 1.0305313554436555 entropy -2.877991241565238
epoch: 21, step: 53
	action: tensor([[-0.3440, -0.2990, -0.0545, -0.2208, -0.7122,  0.2278, -0.0464]],
       dtype=torch.float64)
	q_value: tensor([[-35.2720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4531092335014837, distance: 1.3794495583720803 entropy -2.3836058416652484
epoch: 21, step: 54
	action: tensor([[ 0.2052, -0.3810,  0.1522,  0.5596, -0.5605,  0.4161,  0.2248]],
       dtype=torch.float64)
	q_value: tensor([[-32.5631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5812204968916505, distance: 0.7405415007989262 entropy -2.5124193026169706
epoch: 21, step: 55
	action: tensor([[-2.7425, -0.1157,  1.7149, -1.1125,  0.8213,  0.1965,  0.2338]],
       dtype=torch.float64)
	q_value: tensor([[-39.6817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7405415007989262 entropy -2.1661806284954133
epoch: 21, step: 56
	action: tensor([[-0.2609, -0.0264, -0.3024,  0.3852,  0.0732,  0.0372,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04514919519116933, distance: 1.1182127826529866 entropy -3.345050214139508
epoch: 21, step: 57
	action: tensor([[-0.1223, -0.1379, -0.1292, -0.4000,  0.1490,  0.2454,  0.0851]],
       dtype=torch.float64)
	q_value: tensor([[-32.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07847597157821307, distance: 1.1883980486136883 entropy -3.1232640190889285
epoch: 21, step: 58
	action: tensor([[-0.3198, -0.3137, -0.2197,  0.4191,  0.4833, -0.0964, -0.0356]],
       dtype=torch.float64)
	q_value: tensor([[-30.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2330351118185392, distance: 1.27070405296485 entropy -2.879757303587225
epoch: 21, step: 59
	action: tensor([[ 0.2708, -0.0782,  0.2736, -0.2275, -0.0619, -0.1351,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-32.5666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2676701150873705, distance: 0.979287186167665 entropy -3.2421456549604524
epoch: 21, step: 60
	action: tensor([[ 0.8011, -0.0901,  0.5550,  0.1662, -0.6213,  0.5789, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-33.3178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8815626626521773, distance: 0.39382294081150493 entropy -2.5932188400636518
epoch: 21, step: 61
	action: tensor([[-1.3906,  0.9959,  1.5858, -0.1186, -1.0398,  0.0596,  0.1041]],
       dtype=torch.float64)
	q_value: tensor([[-42.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.39382294081150493 entropy -1.937901179606871
epoch: 21, step: 62
	action: tensor([[ 0.2107, -0.1519,  0.0967, -0.0407, -0.0926, -0.0989,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3238534694966051, distance: 0.94097287677251 entropy -3.345050214139508
epoch: 21, step: 63
	action: tensor([[-0.3004,  0.2205, -0.4611,  0.2687, -0.2472,  0.1283,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-32.9091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.94097287677251 entropy -2.664228575963599
LOSS epoch 21 actor 467.894297515613 critic 482.9934678959948
epoch: 22, step: 0
	action: tensor([[ 0.1172, -0.2900, -0.0841, -0.0683, -0.0540,  0.1399,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0893584323511527, distance: 1.0920195878739805 entropy -3.343142631046352
epoch: 22, step: 1
	action: tensor([[-0.5343, -0.4849,  0.2053, -0.5845,  0.1602,  0.2525,  0.0495]],
       dtype=torch.float64)
	q_value: tensor([[-31.7246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9675250551772585, distance: 1.6051544570840572 entropy -2.6721684065266693
epoch: 22, step: 2
	action: tensor([[-0.0776, -0.0643,  0.5305, -0.6884, -0.1122,  0.7872, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-31.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10118841810373858, distance: 1.200846539149203 entropy -2.6182522091735345
epoch: 22, step: 3
	action: tensor([[-1.1287, -0.0556,  0.7976,  0.7988, -0.3624, -0.0111,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-34.3154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46366677987937854, distance: 1.384451675590453 entropy -2.128884613759124
epoch: 22, step: 4
	action: tensor([[ 0.0376, -0.0019,  0.3643, -0.8216,  0.4072,  1.0361,  0.4051]],
       dtype=torch.float64)
	q_value: tensor([[-40.4460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18297147927716528, distance: 1.034368536844126 entropy -2.2324815276094943
epoch: 22, step: 5
	action: tensor([[-0.0153,  0.4562, -0.1296, -0.5076, -1.1408,  0.4952, -0.2545]],
       dtype=torch.float64)
	q_value: tensor([[-34.5587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2464121921558834, distance: 0.9933987861916992 entropy -2.1593380763770877
epoch: 22, step: 6
	action: tensor([[ 0.5705, -0.2204, -0.2598,  0.3463,  0.7986,  0.8001,  0.1705]],
       dtype=torch.float64)
	q_value: tensor([[-34.2142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9933987861916992 entropy -2.3399789539063165
epoch: 22, step: 7
	action: tensor([[ 0.0660, -0.2271, -0.1068,  0.1409,  0.0468, -0.1870,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20130064885263665, distance: 1.0227002428566934 entropy -3.343142631046352
epoch: 22, step: 8
	action: tensor([[ 0.4276, -0.3867, -0.0498,  0.1887, -0.1881,  0.2721,  0.0490]],
       dtype=torch.float64)
	q_value: tensor([[-31.5746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4858200401669278, distance: 0.820567413631592 entropy -2.921373969989459
epoch: 22, step: 9
	action: tensor([[-0.3335,  0.0232,  0.5902, -0.1970, -0.5373,  0.2997,  0.0691]],
       dtype=torch.float64)
	q_value: tensor([[-35.4552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14247575407366098, distance: 1.2231513228165438 entropy -2.3879718537008405
epoch: 22, step: 10
	action: tensor([[0.6748, 0.2129, 0.6894, 0.0576, 0.0702, 0.4688, 0.2632]],
       dtype=torch.float64)
	q_value: tensor([[-35.4442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9465755961348075, distance: 0.264500523342364 entropy -2.2368618966713227
epoch: 22, step: 11
	action: tensor([[-0.9857,  0.5394, -0.7755, -0.1169,  0.3206,  0.6538, -0.0733]],
       dtype=torch.float64)
	q_value: tensor([[-40.2173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.264500523342364 entropy -2.097539693091971
epoch: 22, step: 12
	action: tensor([[ 0.0047, -0.0819, -0.1298,  0.1288, -0.2126, -0.0740,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2780034044964681, distance: 0.972353693298636 entropy -3.343142631046352
epoch: 22, step: 13
	action: tensor([[-1.3693, -0.4639,  0.3302, -0.0322, -0.5743,  0.1278,  0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-32.0346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6847247829847949, distance: 1.8750229128442364 entropy -2.844034259129161
epoch: 22, step: 14
	action: tensor([[ 0.9630,  0.0571, -0.7915,  0.1390, -0.6244,  0.8041,  0.3312]],
       dtype=torch.float64)
	q_value: tensor([[-36.2354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8565015985102051, distance: 0.43349113940944767 entropy -2.4364905671730064
epoch: 22, step: 15
	action: tensor([[-0.1718,  0.0385,  0.8846, -0.7532, -1.4545, -0.3724, -0.0814]],
       dtype=torch.float64)
	q_value: tensor([[-40.1502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3187198544919705, distance: 1.3141137689031972 entropy -2.2278669225644236
epoch: 22, step: 16
	action: tensor([[ 0.4119, -0.0414,  0.3137,  0.0195, -1.7033,  0.8814,  0.2477]],
       dtype=torch.float64)
	q_value: tensor([[-40.3045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5729011162282578, distance: 0.7478610440113529 entropy -2.0643076342335482
epoch: 22, step: 17
	action: tensor([[-0.6113, -1.7949, -0.1873,  1.3808, -1.3359,  0.3629,  0.3443]],
       dtype=torch.float64)
	q_value: tensor([[-45.9719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7478610440113529 entropy -1.7140192700707388
epoch: 22, step: 18
	action: tensor([[ 0.0260,  0.0043, -0.0771,  0.0949, -0.1386, -0.0109,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36357741771802, distance: 0.9129131942968551 entropy -3.343142631046352
epoch: 22, step: 19
	action: tensor([[-0.1237, -0.0719, -0.0082,  0.6457, -0.4821,  0.1458,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9129131942968551 entropy -2.8170179917323965
epoch: 22, step: 20
	action: tensor([[-0.2363, -0.0541,  0.2861, -0.4104, -0.0054, -0.1155,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34682053113156175, distance: 1.3280412274023425 entropy -3.343142631046352
epoch: 22, step: 21
	action: tensor([[-0.3094, -0.6490,  0.1234,  0.0691,  0.0033,  0.5131,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-31.1968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3865932913012464, distance: 1.3475076536284187 entropy -2.763123058916887
epoch: 22, step: 22
	action: tensor([[-0.8374,  0.5270,  0.7746, -1.0831,  0.5216,  0.0291,  0.1654]],
       dtype=torch.float64)
	q_value: tensor([[-33.6505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0590393674776597, distance: 1.642059986267726 entropy -2.395879850810438
epoch: 22, step: 23
	action: tensor([[-0.3500, -0.0370, -0.0536,  0.2709,  0.3707, -0.0464, -0.2016]],
       dtype=torch.float64)
	q_value: tensor([[-37.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08475118249655633, distance: 1.1918504351715407 entropy -2.5880844169131256
epoch: 22, step: 24
	action: tensor([[-0.0160, -0.0601,  0.1094,  0.3539,  0.0868,  0.0889,  0.0499]],
       dtype=torch.float64)
	q_value: tensor([[-31.8796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40778048705555925, distance: 0.8806392736207715 entropy -3.232495856351856
epoch: 22, step: 25
	action: tensor([[-0.0639,  0.2220,  0.5315, -0.6610, -0.2962,  0.1227,  0.1022]],
       dtype=torch.float64)
	q_value: tensor([[-34.3034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06440446659230381, distance: 1.180619732033436 entropy -2.7032884408790174
epoch: 22, step: 26
	action: tensor([[-0.0581, -0.1947, -0.0032,  0.3235, -0.6975,  0.0711,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[-34.0333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20843756684127768, distance: 1.0181207310415628 entropy -2.4010221870910917
epoch: 22, step: 27
	action: tensor([[ 0.3544,  0.1671,  0.9243,  0.1016, -0.4864,  0.3251,  0.2362]],
       dtype=torch.float64)
	q_value: tensor([[-35.4022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0181207310415628 entropy -2.45578986404472
epoch: 22, step: 28
	action: tensor([[ 0.1339, -0.0258,  0.0483,  0.1422, -0.1667, -0.2171,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4149330702993351, distance: 0.8753051196336044 entropy -3.343142631046352
epoch: 22, step: 29
	action: tensor([[-0.2923, -0.3552,  0.4698,  0.0800, -0.6470,  0.0567,  0.0832]],
       dtype=torch.float64)
	q_value: tensor([[-32.9637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2356173566267128, distance: 1.2720339229681936 entropy -2.7879230136293667
epoch: 22, step: 30
	action: tensor([[ 0.3506, -0.3827, -0.0645, -0.1272, -1.7423, -0.3443,  0.3008]],
       dtype=torch.float64)
	q_value: tensor([[-35.9395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26622128310372506, distance: 0.9802554120819287 entropy -2.270495472245995
epoch: 22, step: 31
	action: tensor([[ 1.9503,  0.4594, -1.1146, -0.8617, -0.7233,  1.1204,  0.0977]],
       dtype=torch.float64)
	q_value: tensor([[-42.2724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9802554120819287 entropy -2.205458705600918
epoch: 22, step: 32
	action: tensor([[-0.1378, -0.1314,  0.1337,  0.2948,  0.2050, -0.0024,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1841523127701078, distance: 1.0336207914335287 entropy -3.343142631046352
epoch: 22, step: 33
	action: tensor([[-0.2198, -0.0959, -0.7657, -0.8806, -0.2896,  0.0031,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-33.2067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08932998705673534, distance: 1.0920366431991602 entropy -2.814609644444833
epoch: 22, step: 34
	action: tensor([[-0.0769, -0.1395,  0.3009, -0.2645, -0.1022, -0.0383, -0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-28.4526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11259460543540145, distance: 1.207049743814011 entropy -3.4794534044954637
epoch: 22, step: 35
	action: tensor([[ 0.0365, -0.1621, -0.6574, -0.4551,  0.0498,  0.3224,  0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-31.6874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17152447267753657, distance: 1.0415893611542184 entropy -2.637094392368165
epoch: 22, step: 36
	action: tensor([[-0.4359,  0.0775, -0.2729,  0.0775, -0.1250,  0.0038, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-28.9863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.183992213874566, distance: 1.2451770755729603 entropy -3.1085559371602542
epoch: 22, step: 37
	action: tensor([[ 0.4863, -0.0138,  0.0635, -0.2807, -0.0573,  0.2104,  0.0822]],
       dtype=torch.float64)
	q_value: tensor([[-30.0065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5533990716621127, distance: 0.7647447496420916 entropy -3.334399552052126
epoch: 22, step: 38
	action: tensor([[ 1.0144, -0.1597, -0.4846,  0.0437,  0.8129,  0.1317, -0.0583]],
       dtype=torch.float64)
	q_value: tensor([[-32.9204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6672019623480117, distance: 0.6601567552241027 entropy -2.4815029887488578
epoch: 22, step: 39
	action: tensor([[-0.0865, -0.1302,  0.1178, -0.1881, -0.2586, -0.1211, -0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-35.3514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058241472022168006, distance: 1.1771968245677866 entropy -2.6017307359162
epoch: 22, step: 40
	action: tensor([[-0.2135,  0.2390,  0.5265,  0.1222, -0.1894,  0.1343,  0.1087]],
       dtype=torch.float64)
	q_value: tensor([[-31.0752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1771968245677866 entropy -2.799403787334334
epoch: 22, step: 41
	action: tensor([[-0.1377,  0.0075, -0.1835,  0.0279,  0.0125,  0.0378,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1356053171912942, distance: 1.063929220031549 entropy -3.343142631046352
epoch: 22, step: 42
	action: tensor([[ 0.1601,  0.2746, -0.0029, -0.3251, -0.2689,  0.0966,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[-30.7355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49059736815452915, distance: 0.8167465064199823 entropy -3.033854619340214
epoch: 22, step: 43
	action: tensor([[-0.1447,  0.4735, -0.4450, -0.4977,  0.0826,  0.5659,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-31.7412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8167465064199823 entropy -2.7220935989579127
epoch: 22, step: 44
	action: tensor([[ 0.2325, -0.1177,  0.1900,  0.3394,  0.3190,  0.0015,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8167465064199823 entropy -3.343142631046352
epoch: 22, step: 45
	action: tensor([[ 0.1554, -0.1187, -0.0716,  0.0147,  0.1607, -0.1534,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2859000074460498, distance: 0.9670216727631485 entropy -3.343142631046352
epoch: 22, step: 46
	action: tensor([[ 0.1672,  0.0769,  0.1721, -0.0466, -0.1524,  0.2539, -0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-31.2634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5123364421376353, distance: 0.7991289142525329 entropy -2.9416101599274644
epoch: 22, step: 47
	action: tensor([[ 1.0029, -0.2399,  0.6038,  1.1325, -1.0780,  0.1158,  0.0751]],
       dtype=torch.float64)
	q_value: tensor([[-33.4985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8553329488679853, distance: 0.4352527339761733 entropy -2.527962713297689
epoch: 22, step: 48
	action: tensor([[ 0.6743,  0.2066, -0.0447, -0.0517, -1.0481,  0.7273,  0.1509]],
       dtype=torch.float64)
	q_value: tensor([[-52.0264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8383525029911363, distance: 0.4600882958334204 entropy -1.8607851394015547
epoch: 22, step: 49
	action: tensor([[-1.2977, -0.8130,  1.7761, -0.3407, -0.9512,  0.8881,  0.1371]],
       dtype=torch.float64)
	q_value: tensor([[-40.4036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5089993159123023, distance: 1.8126208228068286 entropy -1.990177045464481
epoch: 22, step: 50
	action: tensor([[ 0.6486, -0.0481,  3.0914,  0.7531, -1.2888,  2.9750,  0.6345]],
       dtype=torch.float64)
	q_value: tensor([[-47.7027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8126208228068286 entropy -1.51613984781269
epoch: 22, step: 51
	action: tensor([[-0.0313, -0.0864,  0.0985,  0.1638, -0.0213, -0.1310,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24511475395502702, distance: 0.9942535765957425 entropy -3.343142631046352
epoch: 22, step: 52
	action: tensor([[-0.1594, -0.0635,  0.2444, -0.1451,  0.7331,  0.0365,  0.0913]],
       dtype=torch.float64)
	q_value: tensor([[-32.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012357391093808179, distance: 1.1513930992740458 entropy -2.8081211985093213
epoch: 22, step: 53
	action: tensor([[ 0.3044, -0.5066,  0.7824,  0.3440, -0.1956, -0.0383, -0.1411]],
       dtype=torch.float64)
	q_value: tensor([[-32.1263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33785602287789795, distance: 0.93117843526617 entropy -2.8533281160458475
epoch: 22, step: 54
	action: tensor([[-0.9814, -1.4196,  0.7075,  1.1025, -0.4382,  0.4367,  0.1578]],
       dtype=torch.float64)
	q_value: tensor([[-39.4694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.93117843526617 entropy -2.159729823499085
epoch: 22, step: 55
	action: tensor([[ 0.2400, -0.3791, -0.2648, -0.4838,  0.5469,  0.1494,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0007787299335441089, distance: 1.1438985996595743 entropy -3.343142631046352
epoch: 22, step: 56
	action: tensor([[-0.2708, -0.1898, -0.5938, -0.3136,  0.4424,  0.0331, -0.1935]],
       dtype=torch.float64)
	q_value: tensor([[-29.4102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14932273551631647, distance: 1.2268110871161673 entropy -2.793124036291806
epoch: 22, step: 57
	action: tensor([[-0.3520,  0.1098,  0.0746, -0.0253,  0.1092, -0.0948, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-28.1652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07002601748997805, distance: 1.1837332918359686 entropy -3.5843262541266263
epoch: 22, step: 58
	action: tensor([[ 0.2464,  0.1510,  0.1419,  0.6845, -0.1573,  0.2272,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-30.7955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1837332918359686 entropy -3.1208357329521523
epoch: 22, step: 59
	action: tensor([[-0.1977,  0.1460, -0.0579,  0.1829, -0.1429,  0.0913,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24829821222061987, distance: 0.9921549074103982 entropy -3.343142631046352
epoch: 22, step: 60
	action: tensor([[-0.1748, -0.3310, -0.2127, -0.1536, -0.5665,  0.2645,  0.1160]],
       dtype=torch.float64)
	q_value: tensor([[-32.4146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23803730163277215, distance: 1.2732789469293064 entropy -2.868456992749746
epoch: 22, step: 61
	action: tensor([[-0.7735, -0.0313, -0.3473, -0.6333,  0.5262,  0.2787,  0.1684]],
       dtype=torch.float64)
	q_value: tensor([[-32.1502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6688849735941789, distance: 1.4783249118825161 entropy -2.551706828297991
epoch: 22, step: 62
	action: tensor([[-0.1815,  0.1328, -0.0469,  0.4055, -0.0531, -0.0699, -0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-30.5616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4783249118825161 entropy -3.3870278972782186
epoch: 22, step: 63
	action: tensor([[ 0.1533,  0.0882, -0.1851, -0.1062,  0.0533, -0.0161,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-34.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46456950703342736, distance: 0.8373523503357875 entropy -3.343142631046352
LOSS epoch 22 actor 492.1696315215636 critic 172.3564208055714
epoch: 23, step: 0
	action: tensor([[-0.1694,  0.1432,  0.1312,  0.5196, -0.3778, -0.0794, -0.0149]],
       dtype=torch.float64)
	q_value: tensor([[-31.2182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8373523503357875 entropy -2.9879920053373583
epoch: 23, step: 1
	action: tensor([[ 0.2449,  0.1814, -0.3760, -0.2075,  0.0349, -0.1397,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8373523503357875 entropy -3.341075378822325
epoch: 23, step: 2
	action: tensor([[-0.0538, -0.1341, -0.1284,  0.0607,  0.0160, -0.0497,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13818071212136163, distance: 1.0623430916397005 entropy -3.341075378822325
epoch: 23, step: 3
	action: tensor([[ 1.0073,  0.1567,  0.0353, -0.0395,  0.1196,  0.3148,  0.0589]],
       dtype=torch.float64)
	q_value: tensor([[-31.4841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8794730786015158, distance: 0.3972818504495906 entropy -2.9473775716818005
epoch: 23, step: 4
	action: tensor([[-0.1950, -0.8603, -0.2427,  0.7960, -0.3056,  0.2325, -0.1700]],
       dtype=torch.float64)
	q_value: tensor([[-37.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20557089655062022, distance: 1.2564727404356575 entropy -2.3199794416930066
epoch: 23, step: 5
	action: tensor([[0.1914, 0.2816, 0.3017, 0.4816, 0.7939, 0.5626, 0.2528]],
       dtype=torch.float64)
	q_value: tensor([[-38.1558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2564727404356575 entropy -2.423491635567185
epoch: 23, step: 6
	action: tensor([[-0.1696,  0.0194,  0.0880, -0.1349, -0.1197, -0.0852,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2564727404356575 entropy -3.341075378822325
epoch: 23, step: 7
	action: tensor([[-0.0879,  0.5957,  0.0530,  0.0185, -0.1481, -0.1991,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4842057141153482, distance: 0.8218545361886118 entropy -3.341075378822325
epoch: 23, step: 8
	action: tensor([[-0.1074,  0.0560,  0.4244,  0.2429, -0.1752,  0.0723,  0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-33.4726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3719686712580411, distance: 0.9068748283730121 entropy -3.010525896577122
epoch: 23, step: 9
	action: tensor([[ 0.0211, -0.0041, -0.1286,  0.2898,  0.0749, -0.0742,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39732295930604555, distance: 0.8883804992367471 entropy -3.341075378822325
epoch: 23, step: 10
	action: tensor([[-0.2374,  0.1435,  0.2620, -0.2672,  0.4470, -0.0710,  0.0594]],
       dtype=torch.float64)
	q_value: tensor([[-33.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06114240378929847, distance: 1.178809230911223 entropy -2.9515070334887623
epoch: 23, step: 11
	action: tensor([[ 0.3932, -0.0518, -0.0879,  0.3405, -0.1801, -0.0603, -0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-31.7690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7098137125039417, distance: 0.6164461385122538 entropy -2.9726889725318784
epoch: 23, step: 12
	action: tensor([[ 1.7072,  0.1602,  0.2902,  0.3597, -0.2500,  0.0458,  0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-35.4880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6164461385122538 entropy -2.6709669918502166
epoch: 23, step: 13
	action: tensor([[ 0.2303, -0.0079, -0.0216,  0.3713,  0.3294, -0.1641,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6455639498709609, distance: 0.6812799961676921 entropy -3.341075378822325
epoch: 23, step: 14
	action: tensor([[-0.5005, -0.0446, -0.4135, -0.0787,  0.5374,  0.3805, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[-34.4731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20290457566407527, distance: 1.255082521937932 entropy -2.8688714200061796
epoch: 23, step: 15
	action: tensor([[-0.0540, -0.3024, -0.0106,  0.0180,  0.2692,  0.0235, -0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-30.5012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029569063553457453, distance: 1.127298709319653 entropy -3.389645447395576
epoch: 23, step: 16
	action: tensor([[-0.4073,  0.8182, -0.3915, -0.4452, -0.5147,  0.1025,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-31.5899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.127298709319653 entropy -2.8843336830774864
epoch: 23, step: 17
	action: tensor([[-0.1689, -0.0187,  0.0677,  0.0380,  0.1728,  0.2907,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1744494626228581, distance: 1.0397490338559492 entropy -3.341075378822325
epoch: 23, step: 18
	action: tensor([[ 0.0595,  0.4414,  0.2916,  0.7842, -0.0219,  0.0896,  0.0422]],
       dtype=torch.float64)
	q_value: tensor([[-32.6895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0397490338559492 entropy -2.7477271309395706
epoch: 23, step: 19
	action: tensor([[-0.1637, -0.3086,  0.0812,  0.1030,  0.1454,  0.1295,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06242313700618385, distance: 1.1795203912922316 entropy -3.341075378822325
epoch: 23, step: 20
	action: tensor([[-0.3206, -0.0173,  0.0974,  0.4162,  0.0077,  0.0474,  0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-32.6469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1795203912922316 entropy -2.725089930913463
epoch: 23, step: 21
	action: tensor([[-0.2364, -0.0466, -0.0564,  0.0096,  0.4070,  0.0560,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024754744965581166, distance: 1.15842164113793 entropy -3.341075378822325
epoch: 23, step: 22
	action: tensor([[ 0.0009, -0.4001, -0.0606,  0.1582, -0.1474,  0.1486, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-31.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04796024741227656, distance: 1.1165655769284715 entropy -3.0967566653583556
epoch: 23, step: 23
	action: tensor([[-0.0912,  0.3616,  0.0519,  0.0662, -0.0293,  0.1810,  0.1367]],
       dtype=torch.float64)
	q_value: tensor([[-33.2893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1165655769284715 entropy -2.6186323909464067
epoch: 23, step: 24
	action: tensor([[ 0.1206, -0.0066, -0.0744,  0.2872, -0.2498,  0.1568,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1165655769284715 entropy -3.341075378822325
epoch: 23, step: 25
	action: tensor([[ 0.3839, -0.0706,  0.1303,  0.1155, -0.2454, -0.0721,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6264130811049052, distance: 0.6994433151266763 entropy -3.341075378822325
epoch: 23, step: 26
	action: tensor([[-0.0312,  0.4151,  0.4772,  0.1580,  0.7071,  0.6118,  0.0511]],
       dtype=torch.float64)
	q_value: tensor([[-35.2649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6994433151266763 entropy -2.5323692766322856
epoch: 23, step: 27
	action: tensor([[-0.0878, -0.1885, -0.1471,  0.1779, -0.0452,  0.2408,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6994433151266763 entropy -3.341075378822325
epoch: 23, step: 28
	action: tensor([[-0.4266,  0.0527, -0.1367,  0.1619,  0.0408, -0.1772,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19181288987233858, distance: 1.2492827184676756 entropy -3.341075378822325
epoch: 23, step: 29
	action: tensor([[ 0.0203,  0.2590,  0.0079, -0.0044,  0.0252, -0.0587,  0.0717]],
       dtype=torch.float64)
	q_value: tensor([[-31.2833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4366475759449121, distance: 0.8589082553590978 entropy -3.2913248617273716
epoch: 23, step: 30
	action: tensor([[-0.1570, -0.2600,  0.1011,  0.0196, -0.1253, -0.0584,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-32.2204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8589082553590978 entropy -2.95530340074717
epoch: 23, step: 31
	action: tensor([[ 0.0143, -0.0807,  0.2146, -0.3242,  0.1123, -0.1526,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0019033454066330613, distance: 1.1454327774769184 entropy -3.341075378822325
epoch: 23, step: 32
	action: tensor([[ 0.1720, -0.3484, -0.3132, -0.1719,  0.1308,  0.0690, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[-31.1267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09817994239328187, distance: 1.0867174440480505 entropy -2.812511186548512
epoch: 23, step: 33
	action: tensor([[ 0.0198,  0.0442, -0.0383,  0.1452,  0.0474,  0.1210, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[-30.3905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43235504362714505, distance: 0.8621743235765932 entropy -2.90958592701321
epoch: 23, step: 34
	action: tensor([[ 0.3055, -0.3901,  0.1283,  0.0092, -0.4850,  0.2270,  0.0613]],
       dtype=torch.float64)
	q_value: tensor([[-32.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2738292856836616, distance: 0.9751604035657996 entropy -2.8428338547572563
epoch: 23, step: 35
	action: tensor([[ 0.1084, -0.3383, -0.5894,  0.2153, -0.1580,  0.3124,  0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-35.5576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1774764552655722, distance: 1.0378410931265427 entropy -2.286366942857355
epoch: 23, step: 36
	action: tensor([[ 0.7193,  0.1562,  0.2000,  0.0528, -0.1048,  0.3214,  0.0464]],
       dtype=torch.float64)
	q_value: tensor([[-33.3444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9168716315873382, distance: 0.32993722817518634 entropy -2.7916071994398126
epoch: 23, step: 37
	action: tensor([[-0.3309,  0.0847,  1.2393,  0.9317, -0.8122,  0.2636, -0.0368]],
       dtype=torch.float64)
	q_value: tensor([[-37.6277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.32993722817518634 entropy -2.3101656314233145
epoch: 23, step: 38
	action: tensor([[-0.3648,  0.1216,  0.1680,  0.0077,  0.0574,  0.2189,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016785366315783534, distance: 1.153908405320545 entropy -3.341075378822325
epoch: 23, step: 39
	action: tensor([[ 0.4195, -0.0105, -0.1730, -0.2666,  0.2562,  0.0836,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-32.5245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5119724337837438, distance: 0.7994271068627438 entropy -2.8144974830032865
epoch: 23, step: 40
	action: tensor([[-0.1065, -0.3342, -0.3566, -0.0520, -0.5133,  0.2585, -0.1172]],
       dtype=torch.float64)
	q_value: tensor([[-31.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15538022718273714, distance: 1.230039785049646 entropy -2.7908432660458975
epoch: 23, step: 41
	action: tensor([[-1.0383, -0.4869,  0.0194, -0.0746,  0.1803,  0.1254,  0.1396]],
       dtype=torch.float64)
	q_value: tensor([[-31.9182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.35838461815215, distance: 1.7573732444174102 entropy -2.68847801066157
epoch: 23, step: 42
	action: tensor([[ 0.2889, -0.4582, -0.2941,  0.4390,  0.1120,  0.3252,  0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-33.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4220238997431007, distance: 0.8699847370726574 entropy -3.0134986125675667
epoch: 23, step: 43
	action: tensor([[-1.3887,  0.1886, -0.9152, -1.3144, -0.1880,  0.1931,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[-35.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6753834976923991, distance: 1.4812003635159532 entropy -2.588360380027808
epoch: 23, step: 44
	action: tensor([[-0.2317, -0.0698, -0.1738,  0.0196,  0.3249,  0.0670,  0.0595]],
       dtype=torch.float64)
	q_value: tensor([[-33.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04938136870628296, distance: 1.1722584390231279 entropy -3.510571642994807
epoch: 23, step: 45
	action: tensor([[-0.4303, -0.1463, -0.2009, -0.0380, -0.2102,  0.3453, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-30.7959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36195824445364155, distance: 1.335483694069007 entropy -3.193147351341389
epoch: 23, step: 46
	action: tensor([[-0.2116, -0.5605, -0.1463,  0.0824, -0.1481,  0.1886,  0.1343]],
       dtype=torch.float64)
	q_value: tensor([[-31.6102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3781195015841552, distance: 1.343383879431551 entropy -2.8487339377586913
epoch: 23, step: 47
	action: tensor([[ 0.3850, -0.1202, -0.2619,  0.1203, -0.5307,  0.6501,  0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-32.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5723200251443779, distance: 0.7483696237405637 entropy -2.645903354857758
epoch: 23, step: 48
	action: tensor([[ 0.3441,  0.3293, -0.1498, -0.3334,  0.9624,  0.2758,  0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-37.1076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7346036748638944, distance: 0.5895276046782577 entropy -2.262399008311999
epoch: 23, step: 49
	action: tensor([[-0.4450, -0.4176,  0.1134, -0.3309,  0.1333, -0.1263, -0.2458]],
       dtype=torch.float64)
	q_value: tensor([[-32.7775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.786707982861355, distance: 1.5296197490149845 entropy -2.7886032831994503
epoch: 23, step: 50
	action: tensor([[-0.3426,  0.0641, -0.1213,  0.5881, -0.4723,  0.3354,  0.0508]],
       dtype=torch.float64)
	q_value: tensor([[-30.3717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5296197490149845 entropy -2.9815107716354388
epoch: 23, step: 51
	action: tensor([[-0.0155,  0.0685, -0.2321,  0.0915,  0.0797,  0.0810,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3532420078745535, distance: 0.9202961290808437 entropy -3.341075378822325
epoch: 23, step: 52
	action: tensor([[-0.0881, -0.0696, -0.2750, -0.1850, -0.1507,  0.3213,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-31.7099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11846401044568311, distance: 1.0744265139588105 entropy -3.0234835445048893
epoch: 23, step: 53
	action: tensor([[-0.2090, -0.4352,  0.1725,  0.2919, -0.4121,  0.4520,  0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-30.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006945942132233318, distance: 1.1403630541735656 entropy -2.8355260866269556
epoch: 23, step: 54
	action: tensor([[ 0.9181, -0.1453,  1.2841,  0.3722,  0.0570,  0.6503,  0.2778]],
       dtype=torch.float64)
	q_value: tensor([[-36.6242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6538913474912944, distance: 0.673229164577958 entropy -2.2776601968550287
epoch: 23, step: 55
	action: tensor([[-1.6958, -0.9593, -0.5288, -1.7774, -0.1749,  0.3778, -0.1053]],
       dtype=torch.float64)
	q_value: tensor([[-49.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.673229164577958 entropy -1.8065707768364159
epoch: 23, step: 56
	action: tensor([[-0.2359, -0.0274, -0.0402,  0.3696,  0.1129, -0.2392,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-34.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0787238743936004, distance: 1.0983774336479237 entropy -3.341075378822325
epoch: 23, step: 57
	action: tensor([[ 0.0744,  0.1143, -0.2821, -0.0961,  0.2247,  0.0472,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-32.8797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40021761287844226, distance: 0.8862444887052048 entropy -3.0812172241607443
epoch: 23, step: 58
	action: tensor([[-0.1984, -0.0487, -0.2512, -0.0868, -0.3407,  0.2359, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-30.5562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022426405540344252, distance: 1.1571048711464522 entropy -3.142068805092091
epoch: 23, step: 59
	action: tensor([[ 0.1575,  0.1429,  0.3236, -0.1554, -0.1221, -0.0064,  0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-31.2296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4235332841078867, distance: 0.8688480120441765 entropy -2.858150831904094
epoch: 23, step: 60
	action: tensor([[ 0.0044,  0.1620,  0.9020, -0.2362,  0.3825,  0.1240,  0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-33.9015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2454412683804118, distance: 0.9940385285146618 entropy -2.5610059076753307
epoch: 23, step: 61
	action: tensor([[-0.0620, -0.8764,  0.0964, -0.8491,  0.4187,  1.1432, -0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-36.2720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44866967125730417, distance: 1.3773406883550625 entropy -2.326161485901321
epoch: 23, step: 62
	action: tensor([[-0.3146,  0.2891,  0.7624, -0.4321,  0.5747,  0.7092, -0.2615]],
       dtype=torch.float64)
	q_value: tensor([[-35.0797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0667599654129386, distance: 1.1054863228277345 entropy -2.0996060453582968
epoch: 23, step: 63
	action: tensor([[-0.1023, -0.3479, -0.0767,  0.0268, -1.8792,  0.3335, -0.1542]],
       dtype=torch.float64)
	q_value: tensor([[-36.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24114331238493003, distance: 1.2748751592247747 entropy -2.310876454958319
LOSS epoch 23 actor 450.6687470058296 critic 262.06354728716144
epoch: 24, step: 0
	action: tensor([[ 0.1715, -2.2536, -1.1804, -1.5600,  0.0437,  1.0298,  0.3245]],
       dtype=torch.float64)
	q_value: tensor([[-43.8785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2748751592247747 entropy -1.9849521630075397
epoch: 24, step: 1
	action: tensor([[ 0.2773, -0.0155,  0.2074, -0.0773,  0.1647,  0.1826,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5601698167759216, distance: 0.7589256092837708 entropy -3.3394536306112643
epoch: 24, step: 2
	action: tensor([[ 0.4268, -0.1137,  0.1159, -0.0581,  0.7074, -0.1978, -0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-34.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4604661293504453, distance: 0.8405548341618427 entropy -2.5532271565275884
epoch: 24, step: 3
	action: tensor([[-0.5380, -0.0514,  0.1346,  0.1847,  0.3251,  0.0678, -0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-33.9233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2443213433561071, distance: 1.2765063175371407 entropy -2.7666361157538306
epoch: 24, step: 4
	action: tensor([[ 0.4264, -0.4520,  0.3808,  0.0180,  0.1868,  0.1885,  0.0697]],
       dtype=torch.float64)
	q_value: tensor([[-33.6091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064910482183564, distance: 0.95297767981156 entropy -3.056961888739296
epoch: 24, step: 5
	action: tensor([[-0.1492,  0.4169,  0.2380,  0.1741, -0.2842,  0.1136, -0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-36.9367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.95297767981156 entropy -2.3200475059869325
epoch: 24, step: 6
	action: tensor([[ 0.4313, -0.1054, -0.1010,  0.1969,  0.0721,  0.0729,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6853602102611207, distance: 0.6418942936138727 entropy -3.3394536306112643
epoch: 24, step: 7
	action: tensor([[ 2.8654e-01, -9.9449e-03, -3.7908e-01,  2.5723e-05, -6.8672e-02,
          3.6515e-01, -1.5084e-02]], dtype=torch.float64)
	q_value: tensor([[-35.7891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5981834421720834, distance: 0.7253884023910111 entropy -2.6530531165551787
epoch: 24, step: 8
	action: tensor([[ 0.0958,  0.3058,  0.0408,  0.2283, -0.0667, -0.0748, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-33.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7253884023910111 entropy -2.7485314729663592
epoch: 24, step: 9
	action: tensor([[-0.2900, -0.2644, -0.1870,  0.2418, -0.0228, -0.1879,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22177777693836043, distance: 1.2648901306398643 entropy -3.3394536306112643
epoch: 24, step: 10
	action: tensor([[-0.1155, -0.3804, -0.0088, -0.3761,  0.3549,  0.0132,  0.0982]],
       dtype=torch.float64)
	q_value: tensor([[-32.6208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3633758673551861, distance: 1.3361785464574063 entropy -3.0909531771368974
epoch: 24, step: 11
	action: tensor([[ 0.3745,  0.0962, -0.1877, -0.5414, -0.7363, -0.0603, -0.0828]],
       dtype=torch.float64)
	q_value: tensor([[-30.6749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43784177560152127, distance: 0.8579974115844218 entropy -2.863009798171293
epoch: 24, step: 12
	action: tensor([[-0.6007, -0.0828, -0.1582, -0.3626, -0.8973,  0.5560,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[-33.6325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7323766777873184, distance: 1.506183402191363 entropy -2.632066114434821
epoch: 24, step: 13
	action: tensor([[ 0.0232,  0.5318, -0.4904, -1.1830,  0.1177,  0.5063,  0.2351]],
       dtype=torch.float64)
	q_value: tensor([[-35.4357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5781010131257431, distance: 0.743294526114849 entropy -2.44264892717
epoch: 24, step: 14
	action: tensor([[-0.1030, -0.0966, -0.4101,  0.1947, -0.1239, -0.0240, -0.1778]],
       dtype=torch.float64)
	q_value: tensor([[-31.9514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11031058891977841, distance: 1.079383821078601 entropy -2.8926941569117197
epoch: 24, step: 15
	action: tensor([[ 0.2829, -0.2537, -0.0570, -0.0887,  0.2917,  0.1146,  0.0805]],
       dtype=torch.float64)
	q_value: tensor([[-32.2260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29554467523385364, distance: 0.9604691529954089 entropy -3.1922415731098703
epoch: 24, step: 16
	action: tensor([[ 0.6074, -0.0924,  0.5391, -0.8050,  0.4062, -0.2984, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-32.9986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0861964073439121, distance: 1.0939138573096294 entropy -2.7192328680733877
epoch: 24, step: 17
	action: tensor([[ 0.2868, -0.1075,  0.6566,  0.2818,  1.3341,  0.5001, -0.3098]],
       dtype=torch.float64)
	q_value: tensor([[-35.4722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6619502192643548, distance: 0.6653451915199584 entropy -2.3705447952055336
epoch: 24, step: 18
	action: tensor([[ 1.0844, -0.7507,  0.4879,  1.2681,  0.2127,  0.4600, -0.2943]],
       dtype=torch.float64)
	q_value: tensor([[-43.3469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7086371612205395, distance: 0.6176945552300621 entropy -2.2853471091221627
epoch: 24, step: 19
	action: tensor([[ 0.9188, -0.9084, -0.8921,  1.3259,  0.2143,  0.2501,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-53.4323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5748191621501636, distance: 0.746179880739138 entropy -1.9210240201761468
epoch: 24, step: 20
	action: tensor([[-0.6502, -0.4894,  0.0124, -0.3051, -0.1706,  0.3677, -0.0829]],
       dtype=torch.float64)
	q_value: tensor([[-46.8733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9769879930267804, distance: 1.609009873543519 entropy -2.384300582757663
epoch: 24, step: 21
	action: tensor([[-0.1658,  0.1414, -0.0547,  0.0030, -1.0715,  0.1718,  0.1488]],
       dtype=torch.float64)
	q_value: tensor([[-33.0966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10183821026110007, distance: 1.0845110485662697 entropy -2.6540740149781
epoch: 24, step: 22
	action: tensor([[-0.4055,  0.4310, -0.3552, -0.1187, -0.4389,  0.0454,  0.2319]],
       dtype=torch.float64)
	q_value: tensor([[-37.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0845110485662697 entropy -2.388309490973493
epoch: 24, step: 23
	action: tensor([[ 0.1124, -0.0340,  0.0487,  0.2392,  0.0282,  0.1195,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5039806595995614, distance: 0.8059461008398048 entropy -3.3394536306112643
epoch: 24, step: 24
	action: tensor([[-0.1045,  0.2017,  0.0800,  0.4914,  0.8280, -0.2138,  0.0732]],
       dtype=torch.float64)
	q_value: tensor([[-35.2308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8059461008398048 entropy -2.6956543402962674
epoch: 24, step: 25
	action: tensor([[ 0.0365, -0.2723,  0.2002,  0.1852, -0.0133, -0.0738,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17692497215723213, distance: 1.038188959112039 entropy -3.3394536306112643
epoch: 24, step: 26
	action: tensor([[ 0.3934, -0.3647,  0.3649,  0.1562, -0.2528,  0.2349,  0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-34.9669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46777848816622003, distance: 0.8348393387572592 entropy -2.6399254251927586
epoch: 24, step: 27
	action: tensor([[-0.6845, -0.4009, -1.7550,  0.4328,  0.3254,  0.6397,  0.0921]],
       dtype=torch.float64)
	q_value: tensor([[-38.4034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1758075250666709, distance: 1.6879785770671512 entropy -2.237609756125134
epoch: 24, step: 28
	action: tensor([[-0.0758,  0.1408, -0.1178, -0.0274,  0.3069, -0.2362,  0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-37.8845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2115173946676191, distance: 1.0161381378658039 entropy -3.3120739507531067
epoch: 24, step: 29
	action: tensor([[-0.1382, -0.1623, -0.0002, -0.0257, -0.0083,  0.1925, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[-31.3990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017818959589454675, distance: 1.1341029144192014 entropy -3.3468442519493693
epoch: 24, step: 30
	action: tensor([[ 0.3516, -0.4414, -0.2275,  0.3231, -0.0437,  0.5432,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-32.8535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4922098958443635, distance: 0.8154527652825413 entropy -2.7676254385479058
epoch: 24, step: 31
	action: tensor([[ 0.8129,  0.3927, -0.5835, -0.7638, -0.2580,  0.1900,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-37.4265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7759621177722645, distance: 0.5416483476668363 entropy -2.392546197976105
epoch: 24, step: 32
	action: tensor([[ 0.0516, -0.2015,  0.3183, -0.3278,  0.3526,  0.2133, -0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-33.0103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010179932832320882, distance: 1.1385046805459174 entropy -2.634494885214259
epoch: 24, step: 33
	action: tensor([[-0.4492,  0.2398, -0.1604, -0.1787,  0.3828, -0.2441, -0.0888]],
       dtype=torch.float64)
	q_value: tensor([[-32.9537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2279436685067373, distance: 1.268077846258824 entropy -2.5750877377279946
epoch: 24, step: 34
	action: tensor([[ 0.0345, -0.1901, -0.1633, -0.2925,  0.2834,  0.0317,  0.0050]],
       dtype=torch.float64)
	q_value: tensor([[-30.6433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027796613896275302, distance: 1.1283277205643363 entropy -3.695884330515932
epoch: 24, step: 35
	action: tensor([[ 0.1015, -0.5562, -0.5922,  0.0457,  0.1901,  0.2555, -0.0675]],
       dtype=torch.float64)
	q_value: tensor([[-30.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06900871016241816, distance: 1.1831704518252188 entropy -3.0079800174202043
epoch: 24, step: 36
	action: tensor([[-0.1213,  0.1126,  0.0421, -0.0789, -0.0700,  0.0948, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[-31.9927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18927252830047026, distance: 1.0303722126157262 entropy -2.915600980819654
epoch: 24, step: 37
	action: tensor([[ 0.0510, -0.7207,  0.1881, -0.4490, -0.0136,  0.3429,  0.0672]],
       dtype=torch.float64)
	q_value: tensor([[-32.7690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5151305387384739, distance: 1.4085806535837133 entropy -2.8645368683102346
epoch: 24, step: 38
	action: tensor([[-1.4345, -0.3908,  0.1775, -0.3927,  0.3558, -0.0612, -0.0180]],
       dtype=torch.float64)
	q_value: tensor([[-33.5147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7913592416021964, distance: 1.9118972990536827 entropy -2.3072989186605177
epoch: 24, step: 39
	action: tensor([[-0.0794, -0.2834,  0.0605, -0.3531, -0.0040,  0.1473,  0.0466]],
       dtype=torch.float64)
	q_value: tensor([[-35.6323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2189100652510816, distance: 1.263404806847499 entropy -3.161832935769383
epoch: 24, step: 40
	action: tensor([[0.0199, 0.2467, 0.2143, 0.3976, 0.1939, 0.0703, 0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-31.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.263404806847499 entropy -2.676758955082022
epoch: 24, step: 41
	action: tensor([[-0.0706, -0.0661,  0.3448,  0.0399, -0.3400, -0.0808,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1649549290796528, distance: 1.0457109403559175 entropy -3.3394536306112643
epoch: 24, step: 42
	action: tensor([[-0.5818, -0.3235,  0.4634,  0.7419,  0.4841,  0.4370,  0.1542]],
       dtype=torch.float64)
	q_value: tensor([[-35.2430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18120736984799346, distance: 1.035484627238074 entropy -2.5487670122194888
epoch: 24, step: 43
	action: tensor([[-0.5016, -0.5287, -0.1754, -0.7648,  1.0904,  0.0387,  0.1266]],
       dtype=torch.float64)
	q_value: tensor([[-40.5996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7577861342906829, distance: 1.5171890878703738 entropy -2.463051225247122
epoch: 24, step: 44
	action: tensor([[-0.1017,  0.2407,  0.0143, -0.1391, -0.3524, -0.0341, -0.2257]],
       dtype=torch.float64)
	q_value: tensor([[-32.4160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2761103510465984, distance: 0.9736275999401057 entropy -2.8944979492305896
epoch: 24, step: 45
	action: tensor([[ 0.5316,  0.1345, -0.0468,  0.0633, -0.6451,  0.1486,  0.0923]],
       dtype=torch.float64)
	q_value: tensor([[-32.6637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8055014563571574, distance: 0.5046785773679126 entropy -2.914551594831683
epoch: 24, step: 46
	action: tensor([[-0.2995,  0.3986,  0.2920, -0.8173, -0.2271,  0.7123,  0.0666]],
       dtype=torch.float64)
	q_value: tensor([[-37.8873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05056612956368034, distance: 1.1729199974284 entropy -2.383030521804361
epoch: 24, step: 47
	action: tensor([[ 1.2394,  0.6635, -0.4260, -0.1864, -0.3140, -0.1990,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-35.0324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1729199974284 entropy -2.3721432567791774
epoch: 24, step: 48
	action: tensor([[ 0.2720,  0.0497, -0.1739,  0.1537, -0.1271,  0.0529,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6169905898305772, distance: 0.7082089574323479 entropy -3.3394536306112643
epoch: 24, step: 49
	action: tensor([[ 0.0232, -0.0909,  0.4658, -0.1808,  0.3287, -0.1098,  0.0407]],
       dtype=torch.float64)
	q_value: tensor([[-34.6881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08965169295056441, distance: 1.0918437381668846 entropy -2.770582669087919
epoch: 24, step: 50
	action: tensor([[-0.5394, -0.1267, -0.0297, -0.1683, -0.3573,  0.5599, -0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-33.9642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43387634616344095, distance: 1.3702901739872644 entropy -2.641465128244238
epoch: 24, step: 51
	action: tensor([[-0.3866,  0.1745,  0.0199, -0.8597, -0.3442, -0.2278,  0.1893]],
       dtype=torch.float64)
	q_value: tensor([[-33.9287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3162081032099433, distance: 1.3128616833500193 entropy -2.564104428651981
epoch: 24, step: 52
	action: tensor([[-0.5269,  0.1019, -0.1291, -0.2593, -0.0295,  0.0488,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-32.4041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33289344895919015, distance: 1.321156938635868 entropy -3.0758673482326246
epoch: 24, step: 53
	action: tensor([[ 0.1661, -0.1312,  0.3665, -0.0743, -0.3804,  0.1196,  0.0517]],
       dtype=torch.float64)
	q_value: tensor([[-31.0099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.337301789212908, distance: 0.931568065395177 entropy -3.3098846309698247
epoch: 24, step: 54
	action: tensor([[ 0.4537,  0.8242, -0.1658, -0.0444,  0.0305,  0.5826,  0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-36.0186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.931568065395177 entropy -2.356737934961083
epoch: 24, step: 55
	action: tensor([[ 0.1718, -0.1178,  0.2469, -0.1827, -0.1713,  0.0260,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2612164037771346, distance: 0.9835927416204259 entropy -3.3394536306112643
epoch: 24, step: 56
	action: tensor([[-0.2898,  0.5175,  0.1637,  0.4230,  0.4145,  0.3086,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-34.1776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9835927416204259 entropy -2.5134527532374515
epoch: 24, step: 57
	action: tensor([[-0.2316, -0.0673, -0.0098,  0.1782, -0.0789,  0.1623,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9835927416204259 entropy -3.3394536306112643
epoch: 24, step: 58
	action: tensor([[ 0.1678,  0.0319, -0.0098,  0.2434,  0.1242,  0.2714,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6655446999703467, distance: 0.6617984330197907 entropy -3.3394536306112643
epoch: 24, step: 59
	action: tensor([[ 1.3900, -0.2005, -0.3215,  0.6090, -0.7184,  0.0733,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-35.8203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7794861179923377, distance: 0.5373715385752051 entropy -2.652021006301986
epoch: 24, step: 60
	action: tensor([[ 0.2227,  0.3357,  0.1474, -0.0996,  0.2013,  0.1105,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5373715385752051 entropy -3.3394536306112643
epoch: 24, step: 61
	action: tensor([[-0.0951,  0.0165,  0.1245,  0.0860,  0.1637, -0.0046,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22957033530523963, distance: 1.0044381316497306 entropy -3.3394536306112643
epoch: 24, step: 62
	action: tensor([[-0.9431, -0.1319, -0.0927, -0.9133,  0.4769,  0.2603,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-33.5717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0232340603701013, distance: 1.6277202148073635 entropy -2.842869403839668
epoch: 24, step: 63
	action: tensor([[-0.4912,  0.0715, -0.0395,  0.1835,  0.2305, -0.0102, -0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-33.3105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1904499074425543, distance: 1.24856816104281 entropy -3.1652616604822237
LOSS epoch 24 actor 526.1939948694519 critic 236.26241289206715
epoch: 25, step: 0
	action: tensor([[ 0.2748, -0.1084,  0.2177, -0.2749,  0.4671,  0.0942,  0.0589]],
       dtype=torch.float64)
	q_value: tensor([[-32.9095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010281158508852, distance: 0.9567237312919618 entropy -3.2680289383840178
epoch: 25, step: 1
	action: tensor([[-0.2608, -0.5883, -0.4074,  0.0990, -0.4920, -0.0055, -0.1552]],
       dtype=torch.float64)
	q_value: tensor([[-33.4069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4737444563011963, distance: 1.3892096310070374 entropy -2.6300007032756025
epoch: 25, step: 2
	action: tensor([[-0.0480,  0.0299,  0.5864, -0.2328, -0.2752,  0.1808,  0.1570]],
       dtype=torch.float64)
	q_value: tensor([[-33.1477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13389803115780985, distance: 1.0649793973982857 entropy -2.8191661460955926
epoch: 25, step: 3
	action: tensor([[-0.6506,  0.3193, -0.1819,  0.5232, -0.3368,  0.3385,  0.1156]],
       dtype=torch.float64)
	q_value: tensor([[-36.4704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0649793973982857 entropy -2.3154714334133586
epoch: 25, step: 4
	action: tensor([[ 0.0702, -0.0438,  0.0582,  0.0546, -0.0033, -0.0187,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36189622040586267, distance: 0.9141181915739415 entropy -3.3379063589355575
epoch: 25, step: 5
	action: tensor([[ 0.2817, -0.0288, -0.7645, -0.0182, -0.3649,  0.1514,  0.0473]],
       dtype=torch.float64)
	q_value: tensor([[-34.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5079630263935007, distance: 0.802704250315607 entropy -2.759004702613172
epoch: 25, step: 6
	action: tensor([[-0.4139, -0.0408, -0.1277, -0.2117, -0.2230,  0.2398, -0.0058]],
       dtype=torch.float64)
	q_value: tensor([[-33.2972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27469811593721527, distance: 1.2919936085420811 entropy -2.9749543792162885
epoch: 25, step: 7
	action: tensor([[-0.1433,  0.4933,  0.2547, -0.2396, -0.0982,  0.0050,  0.1064]],
       dtype=torch.float64)
	q_value: tensor([[-32.2300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2919936085420811 entropy -2.924438724069707
epoch: 25, step: 8
	action: tensor([[ 0.0699, -0.4065, -0.1595,  0.0570, -0.4875,  0.0323,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048896453810525875, distance: 1.1160164438967375 entropy -3.3379063589355575
epoch: 25, step: 9
	action: tensor([[ 0.2357, -0.1243, -0.1143,  0.5732,  0.4331,  0.0446,  0.1348]],
       dtype=torch.float64)
	q_value: tensor([[-34.8368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6631216661440298, distance: 0.6641913777990163 entropy -2.5664933538867394
epoch: 25, step: 10
	action: tensor([[-0.7442,  0.0185,  0.0463, -0.7358, -0.2333,  0.1241, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[-37.2586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8526675021874099, distance: 1.5575981943690054 entropy -2.7983576239784553
epoch: 25, step: 11
	action: tensor([[ 0.1266, -0.6085,  0.3788,  0.1190, -0.3952,  0.0759,  0.0620]],
       dtype=torch.float64)
	q_value: tensor([[-32.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5575981943690054 entropy -3.0089075449635914
epoch: 25, step: 12
	action: tensor([[-0.1380,  0.2668,  0.1158,  0.1862, -0.4519,  0.0274,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37456180293174335, distance: 0.9050006556720361 entropy -3.3379063589355575
epoch: 25, step: 13
	action: tensor([[ 0.1544,  0.1665, -0.3332, -0.4066,  0.1762,  0.3714,  0.1603]],
       dtype=torch.float64)
	q_value: tensor([[-36.6813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5051707809085968, distance: 0.8049786489799219 entropy -2.6565041574157218
epoch: 25, step: 14
	action: tensor([[0.1515, 0.0863, 0.2907, 0.2712, 0.3378, 0.0279, 0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8049786489799219 entropy -3.3379063589355575
epoch: 25, step: 15
	action: tensor([[-0.4143,  0.3124, -0.0130, -0.2721, -0.1155, -0.3209,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1608750183984271, distance: 1.23296124534482 entropy -3.3379063589355575
epoch: 25, step: 16
	action: tensor([[-0.4542, -0.1168, -0.0484, -0.0085,  0.5496,  0.0137,  0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-32.4699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3811315628016836, distance: 1.3448511491174853 entropy -3.3766091640220823
epoch: 25, step: 17
	action: tensor([[-0.3355,  0.1864,  0.0299, -0.0163, -0.0270, -0.0143, -0.0345]],
       dtype=torch.float64)
	q_value: tensor([[-32.0266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016885853597382816, distance: 1.153965423378624 entropy -3.2539749772219717
epoch: 25, step: 18
	action: tensor([[ 0.0199, -0.0159, -0.3756,  0.3200,  0.4941,  0.2073,  0.0718]],
       dtype=torch.float64)
	q_value: tensor([[-32.7072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3912002935639828, distance: 0.8928816762367278 entropy -3.076156966793161
epoch: 25, step: 19
	action: tensor([[ 0.1304, -0.0377,  0.4355, -0.2406,  0.2864,  0.1111, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-33.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23825669227039425, distance: 0.9987597155357152 entropy -3.1230319619145166
epoch: 25, step: 20
	action: tensor([[ 0.0150,  0.0212, -0.0281, -0.2950, -0.6683,  0.3724, -0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-34.3988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16453356025569998, distance: 1.0459747431327684 entropy -2.5458870002960166
epoch: 25, step: 21
	action: tensor([[-0.9329,  0.2368,  0.5164, -1.1850,  0.9288,  0.0515,  0.1542]],
       dtype=torch.float64)
	q_value: tensor([[-34.9185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2127799667225965, distance: 1.7022596652339783 entropy -2.425491151938999
epoch: 25, step: 22
	action: tensor([[-0.2789,  0.0978,  0.0205,  0.2334, -0.1205,  0.2893, -0.2749]],
       dtype=torch.float64)
	q_value: tensor([[-37.2128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7022596652339783 entropy -2.7005800409953564
epoch: 25, step: 23
	action: tensor([[ 0.0492,  0.0006,  0.2896,  0.0998,  0.1872, -0.1074,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37274522050604075, distance: 0.906313987885246 entropy -3.3379063589355575
epoch: 25, step: 24
	action: tensor([[ 0.2864, -0.2241,  0.1278, -0.3173,  0.0659,  0.5639,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-34.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35846633856219057, distance: 0.9165716447581698 entropy -2.744668481156889
epoch: 25, step: 25
	action: tensor([[ 1.2222,  1.0254, -0.6826,  0.7459, -0.5598,  0.5227, -0.0550]],
       dtype=torch.float64)
	q_value: tensor([[-34.4826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7772737460616902, distance: 0.5400604818958944 entropy -2.351274413948011
epoch: 25, step: 26
	action: tensor([[ 0.1584,  0.0351, -0.0057,  0.0388, -0.1126, -0.2746,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5347227198533943, distance: 0.780571366863146 entropy -3.3379063589355575
epoch: 25, step: 27
	action: tensor([[-0.3590,  0.7009,  0.2257, -0.3544,  0.2239,  0.3083,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-33.8143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.780571366863146 entropy -2.919024456733585
epoch: 25, step: 28
	action: tensor([[-0.4388, -0.0627, -0.1929, -0.2251,  0.3776, -0.0338,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3960903689193733, distance: 1.352114464640265 entropy -3.3379063589355575
epoch: 25, step: 29
	action: tensor([[-0.0352,  0.0517,  0.2552,  0.2522,  0.1622,  0.0915, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[-30.5721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43439219481071867, distance: 0.8606258573464983 entropy -3.4142977958304885
epoch: 25, step: 30
	action: tensor([[-0.1884,  0.2909,  0.0412,  0.4906, -0.2239,  0.3432,  0.0687]],
       dtype=torch.float64)
	q_value: tensor([[-35.8094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8606258573464983 entropy -2.7013135503978973
epoch: 25, step: 31
	action: tensor([[-0.0465, -0.1045, -0.0127, -0.1959,  0.0764,  0.0488,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8606258573464983 entropy -3.3379063589355575
epoch: 25, step: 32
	action: tensor([[ 0.2067, -0.1306, -0.0475,  0.1343,  0.2162, -0.1068,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42130302965562993, distance: 0.8705271042622421 entropy -3.3379063589355575
epoch: 25, step: 33
	action: tensor([[ 0.0347, -0.1375,  0.1811,  0.3618,  0.2089,  0.0388, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-33.7789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4597231613102142, distance: 0.8411333803524335 entropy -2.8798488700986735
epoch: 25, step: 34
	action: tensor([[-0.3601, -0.5405, -0.2767, -0.1035,  0.5592,  0.1384,  0.0693]],
       dtype=torch.float64)
	q_value: tensor([[-36.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5130539591911272, distance: 1.4076150494255601 entropy -2.6962868666089186
epoch: 25, step: 35
	action: tensor([[-0.1610, -0.3077, -0.1790,  0.0192,  0.1831,  0.1107, -0.0569]],
       dtype=torch.float64)
	q_value: tensor([[-31.1498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07939293114280432, distance: 1.1889031509321135 entropy -3.089849469315568
epoch: 25, step: 36
	action: tensor([[ 0.3157, -0.0298,  0.0848, -0.3226,  0.1716,  0.1410,  0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-31.9924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4162039156182191, distance: 0.8743539616777714 entropy -2.9974371114042597
epoch: 25, step: 37
	action: tensor([[ 0.0205, -0.3794, -0.1530,  0.0687, -0.4809,  0.3127, -0.0940]],
       dtype=torch.float64)
	q_value: tensor([[-33.0869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06822251105651322, distance: 1.1046197407577791 entropy -2.6450254053080906
epoch: 25, step: 38
	action: tensor([[ 0.3331, -0.8877,  0.9533, -1.2939, -0.1403,  0.5595,  0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-35.1056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7147485349147928, distance: 1.498500573538398 entropy -2.4754785537931068
epoch: 25, step: 39
	action: tensor([[-0.6014, -0.8220, -0.7857, -0.1326,  0.2767,  0.0436, -0.2406]],
       dtype=torch.float64)
	q_value: tensor([[-39.0707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9237449085368796, distance: 1.587195541936895 entropy -1.7817215761919987
epoch: 25, step: 40
	action: tensor([[-0.1296, -0.0037, -0.2565, -0.1522,  0.1097,  0.1650,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-31.5954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14857073579604052, distance: 1.0559199123818366 entropy -3.399555517699433
epoch: 25, step: 41
	action: tensor([[ 0.0892, -0.0990, -0.1181, -0.1098, -0.0355,  0.1242,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[-31.3151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901090666910233, distance: 0.9641675437682022 entropy -3.1104412257973593
epoch: 25, step: 42
	action: tensor([[-0.3177, -0.4219, -0.1013,  0.1449, -0.5433,  0.1983,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-32.7475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3154381437701461, distance: 1.3124776262546405 entropy -2.8077333498058574
epoch: 25, step: 43
	action: tensor([[ 0.4607, -0.1548,  0.3757,  0.0630, -0.2124,  0.2612,  0.2270]],
       dtype=torch.float64)
	q_value: tensor([[-35.2039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6464984196367325, distance: 0.6803813069199303 entropy -2.5409444811582653
epoch: 25, step: 44
	action: tensor([[-0.5211,  0.0259, -0.5320, -0.0773,  0.2535,  1.2286,  0.0378]],
       dtype=torch.float64)
	q_value: tensor([[-38.6244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6803813069199303 entropy -2.2431475010773836
epoch: 25, step: 45
	action: tensor([[-0.1311, -0.1910,  0.0126,  0.4046,  0.0293,  0.0385,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20344263830267695, distance: 1.0213279593919187 entropy -3.3379063589355575
epoch: 25, step: 46
	action: tensor([[ 0.9133, -0.2538, -0.3775, -0.1826,  0.5100,  0.4887,  0.1207]],
       dtype=torch.float64)
	q_value: tensor([[-35.6507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5825730338469592, distance: 0.7393446659782545 entropy -2.752484313097527
epoch: 25, step: 47
	action: tensor([[-0.1958, -0.3497,  1.2006, -0.0181, -0.1405, -0.0023, -0.2633]],
       dtype=torch.float64)
	q_value: tensor([[-35.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24986996243443804, distance: 1.279349220226912 entropy -2.428077845654162
epoch: 25, step: 48
	action: tensor([[-1.1226, -1.2907,  0.2961, -0.2872,  0.1782,  1.4203,  0.1819]],
       dtype=torch.float64)
	q_value: tensor([[-39.9032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.279349220226912 entropy -2.071015845025799
epoch: 25, step: 49
	action: tensor([[ 0.0158,  0.0783, -0.0864,  0.1485,  0.0841,  0.1252,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4308413804854826, distance: 0.8633230809500304 entropy -3.3379063589355575
epoch: 25, step: 50
	action: tensor([[ 0.4320,  0.3999,  0.1911,  0.5278, -0.1647, -0.3407,  0.0395]],
       dtype=torch.float64)
	q_value: tensor([[-34.0956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8633230809500304 entropy -2.8709300740792014
epoch: 25, step: 51
	action: tensor([[-0.2589, -0.0496,  0.0562,  0.0009,  0.1124,  0.0874,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8633230809500304 entropy -3.3379063589355575
epoch: 25, step: 52
	action: tensor([[-0.2586,  0.3347, -0.1263, -0.1728,  0.2114,  0.0674,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14774046596574275, distance: 1.056434626204639 entropy -3.3379063589355575
epoch: 25, step: 53
	action: tensor([[-0.0916, -0.0252,  0.1835,  0.0996,  0.2073, -0.0100, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[-32.1343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1951811904644355, distance: 1.0266106189632904 entropy -3.259068082697144
epoch: 25, step: 54
	action: tensor([[ 0.0900,  0.1381,  0.1013, -0.1818, -0.3439, -0.1809,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3468468512215632, distance: 0.9248348905949554 entropy -3.3379063589355575
epoch: 25, step: 55
	action: tensor([[ 0.0199, -0.1543, -0.0306,  0.3765, -0.0914,  0.0173,  0.0636]],
       dtype=torch.float64)
	q_value: tensor([[-34.3061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3822525613720975, distance: 0.8994192324867503 entropy -2.7588823253030377
epoch: 25, step: 56
	action: tensor([[-0.0291, -0.6802,  0.0046,  0.1640, -0.3574,  0.3601,  0.1174]],
       dtype=torch.float64)
	q_value: tensor([[-35.8470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1471431915067578, distance: 1.2256472897123585 entropy -2.7254578526358166
epoch: 25, step: 57
	action: tensor([[ 0.7293,  0.0031, -0.8983, -0.6813,  0.8058,  0.5612,  0.1925]],
       dtype=torch.float64)
	q_value: tensor([[-36.5318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6956655520481495, distance: 0.6312948551420552 entropy -2.3145893367933756
epoch: 25, step: 58
	action: tensor([[-0.8180,  0.5803,  0.2134,  0.7912, -0.1172, -0.1914, -0.2900]],
       dtype=torch.float64)
	q_value: tensor([[-31.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6312948551420552 entropy -2.6385310379168225
epoch: 25, step: 59
	action: tensor([[ 0.0836,  0.0516, -0.1374,  0.2611,  0.0543,  0.0228,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.534109033255568, distance: 0.7810859721917968 entropy -3.3379063589355575
epoch: 25, step: 60
	action: tensor([[ 0.4601, -0.2509,  0.2288,  0.4613, -0.3955,  0.2103,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-34.8180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7597820600342984, distance: 0.5608663858397628 entropy -2.867246076881867
epoch: 25, step: 61
	action: tensor([[-0.1654, -0.0433, -0.0654, -0.1129, -0.1922,  0.0632,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-35.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004942803398893192, distance: 1.1415126162742617 entropy -3.3379063589355575
epoch: 25, step: 62
	action: tensor([[ 0.4126, -0.3045, -0.1596, -0.4508,  0.0568,  0.2228,  0.0893]],
       dtype=torch.float64)
	q_value: tensor([[-32.7804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15977574552320162, distance: 1.0489488237111642 entropy -2.846851190283431
epoch: 25, step: 63
	action: tensor([[-0.0035, -0.3089,  0.2294, -0.0948,  0.1084,  0.5185, -0.1142]],
       dtype=torch.float64)
	q_value: tensor([[-32.4409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14594938911197797, distance: 1.0575441253966735 entropy -2.5697816578317227
LOSS epoch 25 actor 470.80345059411013 critic 565.8877894016156
epoch: 26, step: 0
	action: tensor([[ 0.6866,  0.5045, -0.1079, -0.6491, -1.2906,  0.3050,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-36.5016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8096166754209161, distance: 0.4993110145635765 entropy -2.4165983701835487
epoch: 26, step: 1
	action: tensor([[ 1.0638, -1.3750, -1.5916, -1.7212, -0.8425,  0.6105,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-44.6702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4993110145635765 entropy -2.1599570879330914
epoch: 26, step: 2
	action: tensor([[-0.7274, -0.2488, -0.2900, -0.1442,  0.0228, -0.2876,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8638159979485622, distance: 1.5622776178768922 entropy -3.337324104477864
epoch: 26, step: 3
	action: tensor([[-0.1741, -0.0138,  0.0133,  0.0568,  0.1679, -0.0296,  0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-32.9914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07095464892947134, distance: 1.1029990807700993 entropy -3.633492166266859
epoch: 26, step: 4
	action: tensor([[-0.4663, -0.0924,  0.3983,  0.4688,  0.1118,  0.0835,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[-34.7120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022268440034705295, distance: 1.1315311396861873 entropy -3.0079530034514064
epoch: 26, step: 5
	action: tensor([[ 0.0424, -0.1986, -0.2041,  0.3796, -1.2441,  0.5147,  0.1678]],
       dtype=torch.float64)
	q_value: tensor([[-39.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1866005431380009, distance: 1.0320687596368816 entropy -2.6214884753440315
epoch: 26, step: 6
	action: tensor([[ 1.4407, -0.0737, -0.7623,  1.0701, -1.6577,  0.1110,  0.2558]],
       dtype=torch.float64)
	q_value: tensor([[-44.7732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9685004088098276, distance: 0.20309963924436564 entropy -2.121331319636725
epoch: 26, step: 7
	action: tensor([[-0.4576, -0.0933,  0.6525,  1.2666, -0.6444,  0.1783, -0.1390]],
       dtype=torch.float64)
	q_value: tensor([[-58.4473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.20309963924436564 entropy -2.0039900222375655
epoch: 26, step: 8
	action: tensor([[ 0.1483,  0.2976,  0.1652,  0.4138, -0.2282, -0.1184,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7172929578980913, distance: 0.6084501557637488 entropy -3.337324104477864
epoch: 26, step: 9
	action: tensor([[ 0.2385, -0.0109,  0.1044, -0.0796,  0.0466,  0.0907,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5012406747578553, distance: 0.8081690372336068 entropy -3.337324104477864
epoch: 26, step: 10
	action: tensor([[-0.1143, -0.4716, -0.3446,  0.8156,  0.3596,  0.5863, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-36.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29635789099615983, distance: 0.9599146152046479 entropy -2.643892729998362
epoch: 26, step: 11
	action: tensor([[ 0.6601, -0.6733, -0.6858,  0.1885,  0.4653,  0.2691,  0.0653]],
       dtype=torch.float64)
	q_value: tensor([[-40.1586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24613004996043308, distance: 0.9935847323298967 entropy -2.669417468229352
epoch: 26, step: 12
	action: tensor([[-0.2615, -0.0014, -0.0234,  0.3254, -0.0573,  0.0664, -0.1701]],
       dtype=torch.float64)
	q_value: tensor([[-36.3843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16880401753433116, distance: 1.043298086813842 entropy -2.664543617948546
epoch: 26, step: 13
	action: tensor([[-0.2527, -0.4656, -0.1031,  0.5034, -0.0866,  0.0961,  0.1296]],
       dtype=torch.float64)
	q_value: tensor([[-36.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07130119301014859, distance: 1.1844384234131906 entropy -2.9088741063537755
epoch: 26, step: 14
	action: tensor([[ 0.8127, -0.1609,  0.5306,  0.1785,  0.8696,  0.2392,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-37.5110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7156731519094293, distance: 0.6101907623821953 entropy -2.687879670345958
epoch: 26, step: 15
	action: tensor([[-0.0386, -0.6877, -0.0440, -0.0660,  0.0150,  0.5509, -0.2905]],
       dtype=torch.float64)
	q_value: tensor([[-42.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19283598874918617, distance: 1.2498188200543225 entropy -2.2447104824129
epoch: 26, step: 16
	action: tensor([[-0.7081,  0.3950, -0.3518, -1.1127, -0.5814,  0.3385,  0.0869]],
       dtype=torch.float64)
	q_value: tensor([[-35.6291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39111995961501345, distance: 1.3497053961329772 entropy -2.3920118250131672
epoch: 26, step: 17
	action: tensor([[-0.2250, -0.3520, -0.0399, -0.3826,  0.4517, -0.1675,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[-36.4800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4521364227484379, distance: 1.3789877321015167 entropy -3.1454311227047755
epoch: 26, step: 18
	action: tensor([[-0.4363,  0.0598, -0.2519, -0.0809,  0.2776,  0.2112, -0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-31.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14075719861543967, distance: 1.2222310212404197 entropy -3.0966571803893905
epoch: 26, step: 19
	action: tensor([[-0.3514, -0.0268, -0.1504, -0.0935,  0.2339,  0.1674,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-33.1591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1148894127260478, distance: 1.2082939165368485 entropy -3.379505700783347
epoch: 26, step: 20
	action: tensor([[ 0.2024,  0.2479, -0.0555,  0.3115, -0.3328,  0.1329,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[-33.5174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2082939165368485 entropy -3.1748129102209615
epoch: 26, step: 21
	action: tensor([[ 0.2352, -0.0167, -0.0037, -0.1527,  0.0736,  0.0282,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4281403173032179, distance: 0.8653691978265644 entropy -3.337324104477864
epoch: 26, step: 22
	action: tensor([[-0.2608, -0.5748,  0.2629,  0.1225, -0.8786,  0.0127, -0.0328]],
       dtype=torch.float64)
	q_value: tensor([[-35.0652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3697381351643849, distance: 1.339292592733514 entropy -2.767045010333921
epoch: 26, step: 23
	action: tensor([[-0.4623, -0.0078,  0.6393, -1.2159, -0.8820, -0.0723,  0.2976]],
       dtype=torch.float64)
	q_value: tensor([[-40.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8665485507911947, distance: 1.5634224311502791 entropy -2.2625890335841095
epoch: 26, step: 24
	action: tensor([[-0.1609, -1.1623, -0.5548,  0.5498, -1.6073,  0.1205,  0.0999]],
       dtype=torch.float64)
	q_value: tensor([[-41.9274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7693977239854104, distance: 1.5221919659113228 entropy -2.230508826876688
epoch: 26, step: 25
	action: tensor([[ 0.0853, -0.0077,  0.9303,  1.1456, -1.0221,  0.2636,  0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-48.3247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5221919659113228 entropy -2.1379307672530947
epoch: 26, step: 26
	action: tensor([[-0.3030, -0.0859, -0.2209, -0.1953, -0.2167,  0.0081,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22005001115942435, distance: 1.2639954478678912 entropy -3.337324104477864
epoch: 26, step: 27
	action: tensor([[ 0.2347, -0.2749,  0.0179, -0.4439, -0.0384, -0.0647,  0.0774]],
       dtype=torch.float64)
	q_value: tensor([[-33.6735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030006339623165124, distance: 1.16138614883248 entropy -3.0846427227171733
epoch: 26, step: 28
	action: tensor([[-0.8386,  0.2823, -0.0669,  0.4621, -0.1487,  0.3543, -0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-33.9164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2543482347712951, distance: 1.2816391190098335 entropy -2.6778622801779597
epoch: 26, step: 29
	action: tensor([[-0.6150,  0.1564, -0.1947, -0.3900,  0.6559,  0.1345,  0.1594]],
       dtype=torch.float64)
	q_value: tensor([[-38.4176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4476398922802187, distance: 1.3768510637495783 entropy -2.896825098013753
epoch: 26, step: 30
	action: tensor([[-0.0769, -0.1323, -0.2654, -0.1475,  0.1860,  0.0635, -0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-33.7397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03946013601792875, distance: 1.121539025408519 entropy -3.364463346955843
epoch: 26, step: 31
	action: tensor([[-0.1684,  0.1908,  0.1926, -0.1274, -0.2726,  0.0411, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-32.5388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14060273931143785, distance: 1.0608492551600244 entropy -3.173917923295773
epoch: 26, step: 32
	action: tensor([[ 0.5902, -0.0533,  0.1800, -0.2032,  0.0257,  0.1632,  0.1089]],
       dtype=torch.float64)
	q_value: tensor([[-36.3896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5920667034737916, distance: 0.7308887391757014 entropy -2.737497131765766
epoch: 26, step: 33
	action: tensor([[-1.0888, -0.3392,  0.1078, -0.1293,  0.0824, -0.1875, -0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-37.6328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.472861983989993, distance: 1.7995198115264446 entropy -2.4228096984363354
epoch: 26, step: 34
	action: tensor([[ 0.1283,  0.1462,  0.3093,  0.0175, -0.1650,  0.0166,  0.0992]],
       dtype=torch.float64)
	q_value: tensor([[-35.5334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4544636678974361, distance: 0.8452176022741411 entropy -3.225309273628717
epoch: 26, step: 35
	action: tensor([[-0.4207,  0.7033, -0.4031, -0.5127,  0.1591,  0.2455,  0.0703]],
       dtype=torch.float64)
	q_value: tensor([[-38.3445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8452176022741411 entropy -2.560805828318841
epoch: 26, step: 36
	action: tensor([[-0.1106, -0.2202, -0.2154,  0.2543, -0.2303, -0.3073,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05252219277062653, distance: 1.1138872077952167 entropy -3.337324104477864
epoch: 26, step: 37
	action: tensor([[-0.0067,  0.2324,  0.0529,  0.1361, -0.4784,  0.1440,  0.0983]],
       dtype=torch.float64)
	q_value: tensor([[-35.1857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1138872077952167 entropy -3.02698669115521
epoch: 26, step: 38
	action: tensor([[ 0.2500,  0.0794, -0.1981, -0.2028, -0.1178, -0.1113,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46388503345512244, distance: 0.8378873987802493 entropy -3.337324104477864
epoch: 26, step: 39
	action: tensor([[ 0.0470, -0.2046, -0.4103,  0.2382,  0.2909, -0.1012, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[-34.6878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18715763130889462, distance: 1.0317152729390575 entropy -2.9380598493814856
epoch: 26, step: 40
	action: tensor([[ 0.0394,  0.2181, -0.2463,  0.0410, -0.4130,  0.1335,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-33.7486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0317152729390575 entropy -3.236282760719021
epoch: 26, step: 41
	action: tensor([[-0.0260,  0.2523,  0.0730, -0.0869,  0.1030,  0.0425,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3942845347862526, distance: 0.890617089753618 entropy -3.337324104477864
epoch: 26, step: 42
	action: tensor([[-0.0168, -0.2517, -0.4783,  0.1455, -0.0810, -0.0379, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[-35.7825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09045086529988577, distance: 1.0913643815816716 entropy -2.889447814019762
epoch: 26, step: 43
	action: tensor([[-0.1961,  0.0432,  0.4370,  0.1248,  0.1192,  0.2065,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23442197689536048, distance: 1.0012705030700497 entropy -3.337324104477864
epoch: 26, step: 44
	action: tensor([[ 0.0208,  0.1496,  0.2828, -0.0425, -0.2734,  0.3303,  0.0832]],
       dtype=torch.float64)
	q_value: tensor([[-37.8171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4326471282861607, distance: 0.861952476897907 entropy -2.572040459877954
epoch: 26, step: 45
	action: tensor([[-1.2934, -0.2022, -0.4105,  0.7178,  0.2254, -0.2266,  0.1247]],
       dtype=torch.float64)
	q_value: tensor([[-38.6665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3266741610499264, distance: 1.745518582773761 entropy -2.4252392854908082
epoch: 26, step: 46
	action: tensor([[-0.0827,  0.0284,  0.0888, -0.1226,  0.0903,  0.1668,  0.0864]],
       dtype=torch.float64)
	q_value: tensor([[-38.3687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17576223716286632, distance: 1.0389220105173465 entropy -3.4517492693389493
epoch: 26, step: 47
	action: tensor([[-0.8799,  1.1713,  0.1666, -0.9766, -0.4317,  0.2429,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[-35.2427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0389220105173465 entropy -2.7699537232207874
epoch: 26, step: 48
	action: tensor([[ 0.1316, -0.3571, -0.3928, -0.0337,  0.0488,  0.0208,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0942954089550927, distance: 1.089055412767163 entropy -3.337324104477864
epoch: 26, step: 49
	action: tensor([[-0.1400,  0.1050,  0.0133,  0.0741, -0.2324,  0.0704, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-33.5013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26866824237213005, distance: 0.9786196000416326 entropy -2.9619484517446
epoch: 26, step: 50
	action: tensor([[-0.3549,  0.0727,  0.6070, -0.2654, -0.1883,  0.3754,  0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-36.2614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12019460968996531, distance: 1.2111653356489196 entropy -2.8104428509430903
epoch: 26, step: 51
	action: tensor([[ 0.5301,  0.4538,  0.2790,  0.4174, -0.5764,  0.2739,  0.1531]],
       dtype=torch.float64)
	q_value: tensor([[-38.8451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2111653356489196 entropy -2.3118226957658945
epoch: 26, step: 52
	action: tensor([[ 0.0245, -0.1802, -0.0995,  0.0934,  0.0672,  0.2295,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2836778567478965, distance: 0.9685251028017224 entropy -3.337324104477864
epoch: 26, step: 53
	action: tensor([[-0.6577,  0.6153,  0.2667, -0.1256,  0.1376, -0.1915,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-35.6713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20941628434903925, distance: 1.2584750192956486 entropy -2.725028912061092
epoch: 26, step: 54
	action: tensor([[-0.1272, -0.0619, -0.2659, -0.0316,  0.0834,  0.0791,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-37.2635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11766102105006204, distance: 1.0749157490704289 entropy -3.1677344968856787
epoch: 26, step: 55
	action: tensor([[ 0.5533, -0.2696, -0.1000,  0.4516,  0.5542,  0.5033,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-33.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0749157490704289 entropy -3.1296441012427576
epoch: 26, step: 56
	action: tensor([[ 0.2299, -0.1547, -0.0558, -0.0772,  0.1325,  0.0917,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37133304705423553, distance: 0.9073336317702038 entropy -3.337324104477864
epoch: 26, step: 57
	action: tensor([[ 0.2488, -0.0445,  0.1382, -0.0653, -0.1141,  0.1747, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-34.9456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49912420430001003, distance: 0.8098819427050248 entropy -2.7476149918424086
epoch: 26, step: 58
	action: tensor([[-0.8455, -0.6093,  0.5604, -0.0967, -0.0802,  0.5630,  0.0367]],
       dtype=torch.float64)
	q_value: tensor([[-36.8550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9686865138465259, distance: 1.6056281602021136 entropy -2.5416170129829676
epoch: 26, step: 59
	action: tensor([[-0.7964, -1.4639,  0.3535, -0.7491, -0.3508, -0.1606,  0.2442]],
       dtype=torch.float64)
	q_value: tensor([[-38.8519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6056281602021136 entropy -2.2104894103416517
epoch: 26, step: 60
	action: tensor([[-0.3247,  0.1595,  0.0932,  0.2642,  0.0368,  0.0291,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13531919402991255, distance: 1.064105290992575 entropy -3.337324104477864
epoch: 26, step: 61
	action: tensor([[-0.0430, -0.1090, -0.0035, -0.6757, -0.5793,  0.1495,  0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-36.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18193377698750512, distance: 1.2440941996072306 entropy -2.888448269599702
epoch: 26, step: 62
	action: tensor([[-0.0234,  0.0300, -0.1743,  0.1614,  0.2015,  0.1535,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37369654325428336, distance: 0.9056264488646025 entropy -3.337324104477864
epoch: 26, step: 63
	action: tensor([[ 0.0462, -0.2710,  0.0550, -0.0035, -0.1917, -0.2089,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-35.4270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05912769898338166, distance: 1.1099975875110517 entropy -2.957689415556191
LOSS epoch 26 actor 560.3713539403329 critic 302.10016647799824
epoch: 27, step: 0
	action: tensor([[-0.5845,  0.3162,  0.2327,  0.5195,  0.3929, -0.2486,  0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-36.7946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1099975875110517 entropy -2.7658576536402846
epoch: 27, step: 1
	action: tensor([[-0.5775,  0.1081,  0.0719,  0.4519, -0.1616, -0.2055,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1618548552077177, distance: 1.2334814761950688 entropy -3.336302731753657
epoch: 27, step: 2
	action: tensor([[-0.0654, -0.0020,  0.0153, -0.1730,  0.4160,  0.0176,  0.1438]],
       dtype=torch.float64)
	q_value: tensor([[-39.5126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10880444491434926, distance: 1.0802970723763434 entropy -2.9639482668011845
epoch: 27, step: 3
	action: tensor([[-0.0297, -0.0773, -0.1358,  0.3766,  0.0700, -0.0896, -0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-36.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3018646031794161, distance: 0.9561510853289913 entropy -2.979585766606217
epoch: 27, step: 4
	action: tensor([[ 0.3387, -0.2106,  0.2100, -0.1566,  0.0273,  0.1583,  0.0828]],
       dtype=torch.float64)
	q_value: tensor([[-37.5412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33720642538962664, distance: 0.9316350903962648 entropy -2.9815028917916484
epoch: 27, step: 5
	action: tensor([[-0.4094, -0.5999,  0.2022,  0.3193, -1.4724,  0.5520, -0.0313]],
       dtype=torch.float64)
	q_value: tensor([[-38.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5739363230209213, distance: 1.435655609845015 entropy -2.462948643057964
epoch: 27, step: 6
	action: tensor([[ 1.1781, -0.1674, -0.7458,  1.4734,  0.9396, -0.1716,  0.4454]],
       dtype=torch.float64)
	q_value: tensor([[-48.7839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.435655609845015 entropy -1.9172602826753882
epoch: 27, step: 7
	action: tensor([[ 0.3643, -0.0458,  0.0664,  0.1697,  0.2567, -0.0448,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6698100234107338, distance: 0.6575649196151152 entropy -3.336302731753657
epoch: 27, step: 8
	action: tensor([[ 0.3533,  0.3727,  0.3520,  0.1871, -0.3233,  0.3149, -0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-38.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8963946336334211, distance: 0.36833909666076764 entropy -2.697159806739066
epoch: 27, step: 9
	action: tensor([[ 0.5144,  1.0427,  0.1393, -0.0514,  1.0915,  0.4138,  0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-44.1158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.36833909666076764 entropy -2.3344448835756326
epoch: 27, step: 10
	action: tensor([[ 0.1325, -0.0760, -0.0301,  0.5238,  0.1544,  0.0453,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6318057971850606, distance: 0.694376742967824 entropy -3.336302731753657
epoch: 27, step: 11
	action: tensor([[ 0.0346, -0.0223,  0.5924,  0.0114,  0.0411,  0.6123,  0.0630]],
       dtype=torch.float64)
	q_value: tensor([[-40.0020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5693802111628852, distance: 0.7509373143149993 entropy -2.7415595240024238
epoch: 27, step: 12
	action: tensor([[ 1.4325,  0.2336,  0.0234, -1.5785, -0.3490, -0.5193,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-42.2595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03463304132227374, distance: 1.1639916505539525 entropy -2.212180210568532
epoch: 27, step: 13
	action: tensor([[-1.1952,  0.2457,  0.7147, -0.7301, -0.6203,  0.9060, -0.4652]],
       dtype=torch.float64)
	q_value: tensor([[-46.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.126254930512487, distance: 1.6686465701643867 entropy -2.180751295056147
epoch: 27, step: 14
	action: tensor([[-1.3769, -1.5493,  0.3850, -0.2193,  0.1517,  0.1829,  0.2883]],
       dtype=torch.float64)
	q_value: tensor([[-45.5167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6686465701643867 entropy -2.0994419510131954
epoch: 27, step: 15
	action: tensor([[-0.4289, -0.0652,  0.1025,  0.2600, -0.1719,  0.0346,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15380573614915027, distance: 1.229201382755579 entropy -3.336302731753657
epoch: 27, step: 16
	action: tensor([[ 0.2845, -0.1122, -0.1812,  0.2118,  0.8561, -0.1383,  0.1621]],
       dtype=torch.float64)
	q_value: tensor([[-38.5650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4787954567736995, distance: 0.8261535809118667 entropy -2.7964970767766038
epoch: 27, step: 17
	action: tensor([[-0.0737,  0.0392,  0.2053, -0.4546, -0.5533,  0.0461, -0.1454]],
       dtype=torch.float64)
	q_value: tensor([[-37.0352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04641598708146111, distance: 1.1706009611012558 entropy -2.9785026434256707
epoch: 27, step: 18
	action: tensor([[ 0.8069,  0.2281, -0.1372, -0.1062, -0.7299,  0.7918,  0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-38.1342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9131073575693601, distance: 0.33732474093300663 entropy -2.5708939664467505
epoch: 27, step: 19
	action: tensor([[-0.4621, -0.9181,  0.7603,  0.1847, -1.4557,  0.5339,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-45.6126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7798087072792301, distance: 1.5266636208394497 entropy -2.0641962262272613
epoch: 27, step: 20
	action: tensor([[-0.5100, -1.2707, -1.4779, -2.4308, -0.5691,  1.6831,  0.5121]],
       dtype=torch.float64)
	q_value: tensor([[-51.6830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5266636208394497 entropy -1.7439410932248
epoch: 27, step: 21
	action: tensor([[ 0.0332, -0.2693,  0.1347, -0.0506, -0.1751,  0.0454,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08487939747226836, distance: 1.094701869436818 entropy -3.336302731753657
epoch: 27, step: 22
	action: tensor([[-0.2443,  0.0630,  0.5394, -0.5283, -0.3594,  0.2249,  0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-37.5365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23945055156166384, distance: 1.274005479215293 entropy -2.604940880799894
epoch: 27, step: 23
	action: tensor([[-0.0314,  0.2449, -0.5140,  0.3714,  0.3352, -0.0162,  0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-40.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.274005479215293 entropy -2.353111746899526
epoch: 27, step: 24
	action: tensor([[ 0.1534,  0.0811, -0.1400,  0.1090, -0.2545,  0.0065,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.274005479215293 entropy -3.336302731753657
epoch: 27, step: 25
	action: tensor([[-0.1629, -0.1604,  0.4425, -0.1429, -0.0780,  0.1753,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08525601808733785, distance: 1.192127742326929 entropy -3.336302731753657
epoch: 27, step: 26
	action: tensor([[ 0.3080, -0.3025, -1.0774,  0.1508,  0.5827,  0.5811,  0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-38.7809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3793408100525767, distance: 0.9015364459283597 entropy -2.469014058183791
epoch: 27, step: 27
	action: tensor([[-0.1260, -0.1549,  0.0128, -0.1038, -0.0532, -0.1999, -0.1206]],
       dtype=torch.float64)
	q_value: tensor([[-36.2827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09155370129922091, distance: 1.1955816659257799 entropy -2.98766637168278
epoch: 27, step: 28
	action: tensor([[-0.1791,  0.0535,  0.0693, -0.0368,  0.0449,  0.1306,  0.0607]],
       dtype=torch.float64)
	q_value: tensor([[-35.2779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11774793654807625, distance: 1.0748628050541453 entropy -3.0310220050172325
epoch: 27, step: 29
	action: tensor([[-0.1712,  0.1629, -0.5586,  0.1625,  0.5697,  0.2113,  0.0511]],
       dtype=torch.float64)
	q_value: tensor([[-37.1831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0748628050541453 entropy -2.8441486473168536
epoch: 27, step: 30
	action: tensor([[-0.0437, -0.1648,  0.1064,  0.3883,  0.1168, -0.1489,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901518728534447, distance: 0.9641384738556716 entropy -3.336302731753657
epoch: 27, step: 31
	action: tensor([[ 0.0774, -0.0245,  0.2188,  0.3881, -0.4279,  0.3811,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-38.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5996771910138454, distance: 0.7240388350815186 entropy -2.8072318099320968
epoch: 27, step: 32
	action: tensor([[-0.2028, -0.2995,  0.2942,  1.0080, -0.1678,  0.5363,  0.2035]],
       dtype=torch.float64)
	q_value: tensor([[-43.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7240388350815186 entropy -2.3079965924369996
epoch: 27, step: 33
	action: tensor([[ 0.0821, -0.0163, -0.1520, -0.1526,  0.0942, -0.1337,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7240388350815186 entropy -3.336302731753657
epoch: 27, step: 34
	action: tensor([[-0.2078,  0.0057,  0.1059, -0.1042, -0.3135,  0.0330,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02360629718709295, distance: 1.15777233469722 entropy -3.336302731753657
epoch: 27, step: 35
	action: tensor([[-0.0472, -0.1804,  0.0211,  0.2785,  0.6276,  0.2154,  0.1254]],
       dtype=torch.float64)
	q_value: tensor([[-37.7709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30543456024831706, distance: 0.9537032842190838 entropy -2.7450345274864603
epoch: 27, step: 36
	action: tensor([[-0.4129, -0.2358,  0.2454, -0.6984, -0.1303,  0.3023, -0.0624]],
       dtype=torch.float64)
	q_value: tensor([[-37.9164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.707614200687412, distance: 1.495380014788604 entropy -2.82371677722089
epoch: 27, step: 37
	action: tensor([[ 1.2134, -0.1395, -0.2233, -0.5529, -0.2236, -0.2128,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[-37.0899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13415228400245915, distance: 1.0648230682788267 entropy -2.5550157022461675
epoch: 27, step: 38
	action: tensor([[-0.4737, -0.3153, -0.6446, -0.0404,  0.2690,  0.2925, -0.2783]],
       dtype=torch.float64)
	q_value: tensor([[-40.3557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5772916731694129, distance: 1.4371850753682227 entropy -2.3688991823719574
epoch: 27, step: 39
	action: tensor([[-0.0918,  0.0569, -0.1257,  0.2236,  0.0085,  0.0705,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-33.9112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2600574740554076, distance: 0.9843639201985585 entropy -3.4839080193235836
epoch: 27, step: 40
	action: tensor([[-0.3199, -0.0982,  0.2323,  0.0447,  0.2661,  0.3935,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-37.6231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03307532813465053, distance: 1.163115084637238 entropy -2.9417121174487098
epoch: 27, step: 41
	action: tensor([[ 0.0797, -0.0959,  0.1677, -0.6178,  0.4978,  0.0594,  0.0380]],
       dtype=torch.float64)
	q_value: tensor([[-38.2446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09824291743361413, distance: 1.1992394287379153 entropy -2.649523115068684
epoch: 27, step: 42
	action: tensor([[-0.2145, -0.3321, -0.1862,  0.5967, -0.5597,  0.1809, -0.2001]],
       dtype=torch.float64)
	q_value: tensor([[-35.5105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019370391931947006, distance: 1.155374294488209 entropy -2.720750710720833
epoch: 27, step: 43
	action: tensor([[ 0.8301, -0.7180, -0.8679, -0.9328, -0.7954,  0.2791,  0.2447]],
       dtype=torch.float64)
	q_value: tensor([[-40.4551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34261448572359376, distance: 1.3259659065126423 entropy -2.5261383160595186
epoch: 27, step: 44
	action: tensor([[-0.3606, -0.5014, -0.3716, -0.2795,  0.1682,  0.3931, -0.2154]],
       dtype=torch.float64)
	q_value: tensor([[-40.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5518299677883736, distance: 1.4255378747678427 entropy -2.307588625946022
epoch: 27, step: 45
	action: tensor([[-0.7994,  0.0097, -0.2993,  0.5218, -0.1136,  0.0562,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-34.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5948077749598433, distance: 1.4451431391189586 entropy -2.98936371278342
epoch: 27, step: 46
	action: tensor([[-0.0165, -0.0512, -0.2245,  0.2994, -0.0361,  0.1842,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-38.4850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29882134332991817, distance: 0.9582328104860909 entropy -3.2098326433261906
epoch: 27, step: 47
	action: tensor([[ 0.2378, -0.2210, -0.5200,  0.0832, -0.1701,  0.3148,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[-38.3044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32137006578425875, distance: 0.9426993322462293 entropy -2.8241515896385705
epoch: 27, step: 48
	action: tensor([[-0.7656, -0.2728, -0.5975, -0.2566, -0.0415,  0.1103,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-37.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9081552885051216, distance: 1.5807513119784455 entropy -2.75151844421633
epoch: 27, step: 49
	action: tensor([[-0.0024, -0.1487,  0.0954, -0.2216, -0.0918, -0.0079,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[-34.5970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020754253173478787, distance: 1.1561582761700906 entropy -3.653417737171774
epoch: 27, step: 50
	action: tensor([[ 0.2044, -0.2245, -0.2167, -0.3013,  0.4014,  0.1181,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[-36.5988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14765129203787353, distance: 1.0564898933839795 entropy -2.728153930277696
epoch: 27, step: 51
	action: tensor([[ 0.2382, -0.2890,  0.3494, -0.1792,  0.0749,  0.0568, -0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-34.1895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10400233511569279, distance: 1.0832036934576867 entropy -2.895337871342829
epoch: 27, step: 52
	action: tensor([[ 0.0286,  0.3606,  0.5135, -0.7443,  0.1684, -0.1746, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-37.8301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00013038821581456972, distance: 1.1444188560710498 entropy -2.480358591820329
epoch: 27, step: 53
	action: tensor([[ 0.2212,  0.3098, -0.2688, -0.0818,  1.0416, -0.2398, -0.1550]],
       dtype=torch.float64)
	q_value: tensor([[-39.6072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1444188560710498 entropy -2.652151076079146
epoch: 27, step: 54
	action: tensor([[-0.0565, -0.0284,  0.0514, -0.0524,  0.0129,  0.0916,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20009783714934826, distance: 1.0234700275325261 entropy -3.336302731753657
epoch: 27, step: 55
	action: tensor([[ 0.2315,  0.1393,  0.1889,  0.2723, -0.4502,  0.4262,  0.0430]],
       dtype=torch.float64)
	q_value: tensor([[-37.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0234700275325261 entropy -2.774952155028207
epoch: 27, step: 56
	action: tensor([[ 0.0743, -0.1733, -0.0459, -0.1095, -0.0719, -0.1060,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1188682037384532, distance: 1.0741801679292227 entropy -3.336302731753657
epoch: 27, step: 57
	action: tensor([[-0.0902, -0.3353,  0.0887, -0.2787,  0.2513, -0.0453,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[-36.1752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28811720619590386, distance: 1.2987763865275 entropy -2.8418394257662487
epoch: 27, step: 58
	action: tensor([[ 0.7086,  0.4253, -0.3213,  0.2769,  0.5496,  0.0289, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-34.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9626105179227116, distance: 0.22127452539858714 entropy -2.8307692603386845
epoch: 27, step: 59
	action: tensor([[-0.1591,  0.1882,  0.3901, -0.0164,  0.0098,  0.0028, -0.1313]],
       dtype=torch.float64)
	q_value: tensor([[-40.2968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.22127452539858714 entropy -2.8407191238721774
epoch: 27, step: 60
	action: tensor([[-0.1303, -0.2185, -0.2222, -0.1536, -0.2374, -0.1861,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-41.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11335004784025804, distance: 1.2074594626026796 entropy -3.336302731753657
epoch: 27, step: 61
	action: tensor([[-0.0950,  0.2077, -0.3727,  0.1860, -0.0900,  0.0225,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-35.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3242510451952394, distance: 0.9406961889968174 entropy -3.0654502821261977
epoch: 27, step: 62
	action: tensor([[ 0.2077,  0.2689,  0.2766, -0.0100, -0.1405,  0.1227,  0.0478]],
       dtype=torch.float64)
	q_value: tensor([[-36.9093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6529168656257236, distance: 0.6741762493638845 entropy -3.1952875157525287
epoch: 27, step: 63
	action: tensor([[ 0.6175,  1.1996, -0.1334, -0.2148, -0.9059,  0.7550,  0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-40.6447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6741762493638845 entropy -2.552270180439048
LOSS epoch 27 actor 598.6620422893822 critic 69.08683535174306
epoch: 28, step: 0
	action: tensor([[ 0.2635, -0.1834,  0.0069,  0.3021,  0.3155, -0.0465,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6754603728315192, distance: 0.651914378291899 entropy -3.334827226943381
epoch: 28, step: 1
	action: tensor([[-0.0985, -0.1984,  0.2445,  0.0467,  0.2438, -0.1012,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005595680178688389, distance: 1.1411380702834348 entropy -3.334827226943381
epoch: 28, step: 2
	action: tensor([[-0.1902, -0.1876, -0.0776,  0.8280, -0.4916,  0.1025,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-35.5244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1411380702834348 entropy -2.8017886594995742
epoch: 28, step: 3
	action: tensor([[-0.1717,  0.2026,  0.0545,  0.0119, -0.0228, -0.0267,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20601191452244094, distance: 1.019679494914903 entropy -3.334827226943381
epoch: 28, step: 4
	action: tensor([[-0.4156, -0.1051, -0.1202, -0.2672, -0.2008,  0.3286,  0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-36.1649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37470830168701563, distance: 1.341720239156198 entropy -2.949040645128126
epoch: 28, step: 5
	action: tensor([[-0.7995, -0.1530,  0.1778, -0.4191, -0.4265,  0.4714,  0.1015]],
       dtype=torch.float64)
	q_value: tensor([[-34.4713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0058672532424344, distance: 1.6207192387705425 entropy -2.8193205529263774
epoch: 28, step: 6
	action: tensor([[-0.4763, -0.8253, -0.4775,  0.1190,  0.0342,  0.5784,  0.2025]],
       dtype=torch.float64)
	q_value: tensor([[-37.5566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7992103639563186, distance: 1.5349621304198051 entropy -2.48229324154902
epoch: 28, step: 7
	action: tensor([[ 0.3285, -0.0370,  0.3953, -0.1544,  0.3457,  0.2889,  0.0702]],
       dtype=torch.float64)
	q_value: tensor([[-33.9494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5262990148796943, distance: 0.7876056758779322 entropy -2.67442074750933
epoch: 28, step: 8
	action: tensor([[-0.2883, -0.2159,  0.2622,  0.0457, -0.4227,  0.4278, -0.1179]],
       dtype=torch.float64)
	q_value: tensor([[-37.9875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11059393723680411, distance: 1.2059639967578621 entropy -2.4261813435163653
epoch: 28, step: 9
	action: tensor([[-0.4264,  0.7153,  0.4057,  0.2082, -0.2509, -0.2521,  0.2537]],
       dtype=torch.float64)
	q_value: tensor([[-38.1550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2059639967578621 entropy -2.3423148620300447
epoch: 28, step: 10
	action: tensor([[ 0.0747, -0.0051,  0.3380, -0.1712, -0.2649,  0.2021,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30920694005770266, distance: 0.9511098429172812 entropy -3.334827226943381
epoch: 28, step: 11
	action: tensor([[-0.6371,  0.0668,  0.5215,  0.6150, -0.4949,  0.1238,  0.0925]],
       dtype=torch.float64)
	q_value: tensor([[-38.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9511098429172812 entropy -2.4089331941385943
epoch: 28, step: 12
	action: tensor([[-0.1438, -0.0093, -0.2297, -0.2228, -0.2614, -0.0004,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017355184171064852, distance: 1.1343706384728145 entropy -3.334827226943381
epoch: 28, step: 13
	action: tensor([[-0.1477, -0.0360, -0.6735, -0.2066, -0.5315,  0.1148,  0.0617]],
       dtype=torch.float64)
	q_value: tensor([[-33.8027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04493432271913189, distance: 1.1183385926894165 entropy -3.030346600660682
epoch: 28, step: 14
	action: tensor([[ 0.0539, -0.2448, -0.1689, -0.3089,  0.0945,  0.0985,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-33.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0021804253627051295, distance: 1.1455911532949332 entropy -3.1693285827033124
epoch: 28, step: 15
	action: tensor([[-0.3756, -0.4813, -0.5229, -0.0556,  0.0245, -0.0525, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-32.8615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5662093382187352, distance: 1.4321272149490498 entropy -2.872902684419729
epoch: 28, step: 16
	action: tensor([[-0.0158, -0.0777, -0.3637,  0.0798, -0.1020,  0.0429,  0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-31.0941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20073255416144387, distance: 1.0230638886412773 entropy -3.4034770850404326
epoch: 28, step: 17
	action: tensor([[ 0.2493, -0.2503, -0.4589, -0.3036, -0.4999,  0.1993,  0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-33.9771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1811214904478493, distance: 1.0355389294287096 entropy -3.0552815293557254
epoch: 28, step: 18
	action: tensor([[ 0.0330,  0.0547, -0.3843, -0.6498,  0.5113,  0.2001,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-34.2603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24419311774817476, distance: 0.9948603314511663 entropy -2.6604542282221715
epoch: 28, step: 19
	action: tensor([[-0.5208,  0.2039, -0.1457,  0.3753,  0.1762,  0.0925, -0.1531]],
       dtype=torch.float64)
	q_value: tensor([[-31.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07707369471656444, distance: 1.1876251963746982 entropy -3.088034172361355
epoch: 28, step: 20
	action: tensor([[-0.2508, -0.0011, -0.0981,  0.1265,  0.0096,  0.1831,  0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-35.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05304796661258526, distance: 1.1135781060820389 entropy -3.271783301693318
epoch: 28, step: 21
	action: tensor([[-0.2693, -0.1522, -0.1193, -0.3686,  0.3582,  0.0785,  0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-35.4020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2899132260485937, distance: 1.2996815120993703 entropy -2.9051964616751893
epoch: 28, step: 22
	action: tensor([[ 0.1367,  0.0086,  0.0281, -0.4603, -0.0374,  0.2293, -0.0708]],
       dtype=torch.float64)
	q_value: tensor([[-32.4899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2245927532808044, distance: 1.0076776380859114 entropy -3.1086189456714663
epoch: 28, step: 23
	action: tensor([[ 0.0206, -0.5442, -0.7334, -0.4190,  0.2702, -0.2399, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-34.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21343005013049576, distance: 1.260561579573123 entropy -2.6789732466423692
epoch: 28, step: 24
	action: tensor([[-0.0811, -0.0983, -0.1369, -0.4243,  0.1776,  0.0478, -0.0580]],
       dtype=torch.float64)
	q_value: tensor([[-29.4167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06523752503839497, distance: 1.1810816489578706 entropy -3.3743600880157527
epoch: 28, step: 25
	action: tensor([[-0.1540, -0.1239, -0.1191, -0.0776, -0.2311,  0.3498, -0.0538]],
       dtype=torch.float64)
	q_value: tensor([[-32.1672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0051365528241749114, distance: 1.141401477848683 entropy -3.069292917612032
epoch: 28, step: 26
	action: tensor([[-0.0371, -0.5827,  0.3102, -0.6414,  0.2598,  0.0700,  0.1165]],
       dtype=torch.float64)
	q_value: tensor([[-34.9081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6659924558949184, distance: 1.4770432371482265 entropy -2.6910197966547877
epoch: 28, step: 27
	action: tensor([[ 0.0960, -1.1261, -0.0158,  0.2099,  0.1795, -0.0634, -0.1355]],
       dtype=torch.float64)
	q_value: tensor([[-34.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5995793778141711, distance: 1.447303430490398 entropy -2.4578316948006838
epoch: 28, step: 28
	action: tensor([[1.6756, 0.1661, 0.0442, 0.2953, 0.7269, 0.4058, 0.0579]],
       dtype=torch.float64)
	q_value: tensor([[-34.9122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.447303430490398 entropy -2.442562646295175
epoch: 28, step: 29
	action: tensor([[ 0.0961,  0.0748, -0.0777, -0.1167, -0.1351, -0.0227,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36962331646104307, distance: 0.908566592053787 entropy -3.334827226943381
epoch: 28, step: 30
	action: tensor([[ 0.0629, -0.1351, -0.2012,  0.1960, -0.3902, -0.2007,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[-35.3369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3184710541857003, distance: 0.9447107258945062 entropy -2.867697088973404
epoch: 28, step: 31
	action: tensor([[ 0.4613, -0.3502,  0.1134, -0.8498,  0.3815,  0.3015,  0.1015]],
       dtype=torch.float64)
	q_value: tensor([[-35.7564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10206784748865605, distance: 1.201325952550625 entropy -2.865055051642643
epoch: 28, step: 32
	action: tensor([[ 1.1214, -0.5294,  0.7740,  0.8110,  0.0389,  0.2145, -0.2926]],
       dtype=torch.float64)
	q_value: tensor([[-34.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49746595879205124, distance: 0.8112214697797331 entropy -2.3732506207691437
epoch: 28, step: 33
	action: tensor([[ 0.4109,  0.4609,  0.4122,  1.8108,  1.0891,  0.1892, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[-49.3537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6213475883087802, distance: 0.7041692511797911 entropy -1.927500645880883
epoch: 28, step: 34
	action: tensor([[-0.9936,  0.4678,  0.5177,  0.0783,  0.3652,  0.2257, -0.0539]],
       dtype=torch.float64)
	q_value: tensor([[-57.2138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7041692511797911 entropy -2.2968118689553596
epoch: 28, step: 35
	action: tensor([[-0.1858,  0.0416,  0.1252,  0.4381,  0.1327,  0.0566,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33500553235472963, distance: 0.9331806167113317 entropy -3.334827226943381
epoch: 28, step: 36
	action: tensor([[ 0.3432, -0.1862,  0.1711, -0.1412,  0.0971,  0.2754,  0.0996]],
       dtype=torch.float64)
	q_value: tensor([[-38.2765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4166239309986569, distance: 0.874039375644179 entropy -2.7781731610335005
epoch: 28, step: 37
	action: tensor([[-0.2395,  0.1576, -0.3071,  0.0534,  0.0209,  0.1792,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.874039375644179 entropy -3.334827226943381
epoch: 28, step: 38
	action: tensor([[-0.1863,  0.0588, -0.0170,  0.0410,  0.1356,  0.0052,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11345416893446303, distance: 1.0774752157005227 entropy -3.334827226943381
epoch: 28, step: 39
	action: tensor([[ 0.4366, -0.2751,  0.2385, -0.2153, -0.0085,  0.3987,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-35.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38503224846477113, distance: 0.8973933861141471 entropy -3.009669120438527
epoch: 28, step: 40
	action: tensor([[-0.3710, -1.0362, -0.3299, -0.5008, -0.1058,  0.4363, -0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-37.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9139264308590125, distance: 1.5831399680456706 entropy -2.2996825038021944
epoch: 28, step: 41
	action: tensor([[-0.2761,  0.2166,  0.6274,  0.2400, -0.2092, -0.0324,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-33.5087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5831399680456706 entropy -2.525701663591309
epoch: 28, step: 42
	action: tensor([[-0.1738,  0.1245,  0.2713, -0.0334,  0.3578,  0.0601,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15660314225205862, distance: 1.0509273210677568 entropy -3.334827226943381
epoch: 28, step: 43
	action: tensor([[ 0.1637,  0.0090, -0.4679, -0.3173, -0.2191,  0.1090, -0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-36.3470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36789295822007073, distance: 0.9098127265796397 entropy -2.857762896532883
epoch: 28, step: 44
	action: tensor([[-0.2333, -0.0269,  0.1410, -0.2227,  0.6391, -0.0288, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[-32.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16215540957908736, distance: 1.233641007428965 entropy -3.031635522536341
epoch: 28, step: 45
	action: tensor([[-0.0027, -0.0280, -0.0659,  0.2810,  0.4189,  0.2948, -0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-34.4343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4524795726701115, distance: 0.8467532196924158 entropy -3.0387995625699666
epoch: 28, step: 46
	action: tensor([[-0.5792, -0.0878, -0.2386, -0.3173, -0.4798,  0.1330, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-36.1169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6060023416873204, distance: 1.4502062762882026 entropy -2.867485873631054
epoch: 28, step: 47
	action: tensor([[-0.2290, -0.3687,  0.2606,  0.1182,  0.0929,  0.0935,  0.1193]],
       dtype=torch.float64)
	q_value: tensor([[-34.4883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17377280730309708, distance: 1.2397916735059984 entropy -3.044726424269878
epoch: 28, step: 48
	action: tensor([[-0.7103, -0.5532, -0.0238, -1.2839, -0.6723, -0.0535,  0.1000]],
       dtype=torch.float64)
	q_value: tensor([[-36.2738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7878506206129292, distance: 1.5301087830199362 entropy -2.597589090276421
epoch: 28, step: 49
	action: tensor([[-0.1698, -0.1736, -0.5448, -0.1324,  0.1044,  0.2377,  0.0387]],
       dtype=torch.float64)
	q_value: tensor([[-36.9213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04970502783465447, distance: 1.1724392040457614 entropy -2.6618788158664475
epoch: 28, step: 50
	action: tensor([[-0.1096, -0.0677,  0.0911, -0.3091, -0.1699, -0.0636, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[-31.9970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08834836766091536, distance: 1.1938249701285326 entropy -3.263889102053011
epoch: 28, step: 51
	action: tensor([[ 0.5183, -0.2502, -0.3062,  0.0313,  0.1380,  0.0255,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[-34.5882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4786153952720148, distance: 0.8262962749948216 entropy -2.8247382503510394
epoch: 28, step: 52
	action: tensor([[-0.2615,  0.5461,  0.6467, -0.0725, -0.2269,  0.0969, -0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-34.8434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8262962749948216 entropy -2.751394590536652
epoch: 28, step: 53
	action: tensor([[ 0.1793, -0.0622, -0.2687,  0.2570, -0.1317,  0.0421,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4870779051722056, distance: 0.8195631007970564 entropy -3.334827226943381
epoch: 28, step: 54
	action: tensor([[ 0.5624,  0.4782,  0.0274, -0.0786, -0.7579,  0.2888,  0.0603]],
       dtype=torch.float64)
	q_value: tensor([[-36.1950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9114298309237959, distance: 0.34056532686914 entropy -2.8304507303213913
epoch: 28, step: 55
	action: tensor([[ 0.5812,  0.1756,  0.0275, -0.9451, -0.4477, -0.3881,  0.0597]],
       dtype=torch.float64)
	q_value: tensor([[-42.4769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3152160325769161, distance: 0.9469640351112095 entropy -2.288904793977399
epoch: 28, step: 56
	action: tensor([[-1.0370, -0.5549,  0.0920, -0.2611,  0.0192,  0.0457, -0.1679]],
       dtype=torch.float64)
	q_value: tensor([[-38.1640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3979204697948142, distance: 1.7720422844145616 entropy -2.6198025829973806
epoch: 28, step: 57
	action: tensor([[ 0.1498,  0.0494, -0.1173, -0.1860,  0.1406,  0.1103,  0.1216]],
       dtype=torch.float64)
	q_value: tensor([[-35.1877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40899725815708154, distance: 0.8797341300422556 entropy -2.954289513084439
epoch: 28, step: 58
	action: tensor([[-0.4659,  0.4575, -0.0664,  0.0145,  0.2299,  0.1061, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-34.7149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8797341300422556 entropy -2.870812798797489
epoch: 28, step: 59
	action: tensor([[ 0.0090, -0.0418, -0.1340, -0.1233, -0.0073, -0.2753,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-38.1495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16536734870080283, distance: 1.045452675869291 entropy -3.334827226943381
epoch: 28, step: 60
	action: tensor([[ 0.1794, -0.0682, -0.1719,  0.1406, -0.0304,  0.1176,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-33.6889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48367786370766763, distance: 0.8222749608923634 entropy -3.120310197179174
epoch: 28, step: 61
	action: tensor([[ 0.1929, -0.1730,  0.1481,  0.2781,  0.4014,  0.1208,  0.0401]],
       dtype=torch.float64)
	q_value: tensor([[-35.7676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5359783727658451, distance: 0.7795173836604209 entropy -2.79374969929663
epoch: 28, step: 62
	action: tensor([[ 0.0965, -0.6868, -0.1722,  0.8019,  0.6342,  0.3375, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[-37.3212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3762729925614623, distance: 0.9037617735160497 entropy -2.659006635126023
epoch: 28, step: 63
	action: tensor([[-0.1037, -0.2114, -1.0470, -0.9843,  0.3743,  0.6998,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[-38.7508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32888050160100024, distance: 0.9374683657489011 entropy -2.6075863000385375
LOSS epoch 28 actor 551.9111893539205 critic 264.52863040681245
epoch: 29, step: 0
	action: tensor([[-0.1836, -0.0628,  0.1333, -0.0148,  0.2068,  0.1759, -0.1418]],
       dtype=torch.float64)
	q_value: tensor([[-29.1907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09191390707963787, distance: 1.0904862793247898 entropy -3.0111728728904654
epoch: 29, step: 1
	action: tensor([[-0.3310, -0.2799,  0.3825,  0.0416,  0.7068,  0.3000,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[-32.4016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1397797668738161, distance: 1.2217072886667926 entropy -2.8353522105984235
epoch: 29, step: 2
	action: tensor([[ 0.0499, -0.5431, -0.6083, -0.6171, -0.4310, -0.3115, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-34.2302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15515097905148423, distance: 1.2299177480333776 entropy -2.641993038901505
epoch: 29, step: 3
	action: tensor([[ 0.0202,  0.1250,  0.1608, -0.1747, -0.0089, -0.1556, -0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-28.4591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26941564400239326, distance: 0.9781194105955754 entropy -3.1279302465081926
epoch: 29, step: 4
	action: tensor([[ 0.1681,  0.3121, -0.0122,  0.5076, -0.3480,  0.1286,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[-33.1577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9781194105955754 entropy -2.8819544891026156
epoch: 29, step: 5
	action: tensor([[-0.4824,  0.3766, -0.0618, -0.3297, -0.1356,  0.1189,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16896743349295362, distance: 1.23725124467858 entropy -3.332865500750988
epoch: 29, step: 6
	action: tensor([[ 0.1667, -0.1870, -0.0724, -0.2765, -0.5313,  0.3597,  0.0474]],
       dtype=torch.float64)
	q_value: tensor([[-32.7027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16935645796918963, distance: 1.0429513239205117 entropy -3.1534399447701102
epoch: 29, step: 7
	action: tensor([[-0.9637, -0.2505,  0.2351,  0.4602, -0.5124,  0.6435,  0.1030]],
       dtype=torch.float64)
	q_value: tensor([[-33.4880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6443499483457875, distance: 1.467417918076283 entropy -2.389883351656637
epoch: 29, step: 8
	action: tensor([[-0.2216,  0.2322, -1.3113, -0.4185,  0.1670,  0.1731,  0.3735]],
       dtype=torch.float64)
	q_value: tensor([[-38.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3635305615281319, distance: 0.9129467999920892 entropy -2.248494308704635
epoch: 29, step: 9
	action: tensor([[ 0.2926, -0.2292, -0.0148, -0.1652, -0.0638,  0.1203,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-29.1631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2740451419117921, distance: 0.9750154582470038 entropy -3.5416699521164205
epoch: 29, step: 10
	action: tensor([[-0.0353,  0.1921, -0.4338, -0.2646,  0.7745,  0.7712, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-32.4878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9750154582470038 entropy -2.5875760187636403
epoch: 29, step: 11
	action: tensor([[ 0.0898, -0.0859,  0.0475,  0.2478,  0.1566, -0.0336,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43635578372569594, distance: 0.8591306652491569 entropy -3.332865500750988
epoch: 29, step: 12
	action: tensor([[-0.6134,  0.5379, -0.0281, -0.4175,  0.1127,  0.0110,  0.0394]],
       dtype=torch.float64)
	q_value: tensor([[-34.0587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22485020119072896, distance: 1.2664795518227852 entropy -2.7820621644230004
epoch: 29, step: 13
	action: tensor([[ 0.0459, -0.2247,  0.2561, -0.1129,  0.0386, -0.0375, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[-32.7685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06896908365768495, distance: 1.1041771221607866 entropy -3.453252227209588
epoch: 29, step: 14
	action: tensor([[ 0.4205,  0.0154, -0.6853, -0.1506,  0.0140,  0.1614,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[-33.1488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6336557701925207, distance: 0.6926301159669566 entropy -2.6373812388247164
epoch: 29, step: 15
	action: tensor([[-0.3062,  0.2657, -0.2919, -0.2991,  0.3778, -0.1103, -0.0747]],
       dtype=torch.float64)
	q_value: tensor([[-30.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6926301159669566 entropy -3.00056167374358
epoch: 29, step: 16
	action: tensor([[ 0.1067,  0.0537, -0.3252,  0.1000, -0.0025, -0.0385,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4813077383862773, distance: 0.8241600856391423 entropy -3.332865500750988
epoch: 29, step: 17
	action: tensor([[-0.0802,  0.0546, -0.1751,  0.0554,  0.1703,  0.4169,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[-31.8751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.365652096515345, distance: 0.9114239733064001 entropy -3.058036172885945
epoch: 29, step: 18
	action: tensor([[-2.5296e-01,  2.0451e-01, -4.9524e-04, -5.5513e-01,  2.7140e-01,
          4.2571e-01,  1.2915e-02]], dtype=torch.float64)
	q_value: tensor([[-32.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06066429844224874, distance: 1.1090908126756096 entropy -2.8674831775177796
epoch: 29, step: 19
	action: tensor([[ 0.0965, -0.1533,  0.6210, -0.0947,  0.0386, -0.3299, -0.0945]],
       dtype=torch.float64)
	q_value: tensor([[-31.4995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08500519897643943, distance: 1.0946266225924899 entropy -2.9018198526750063
epoch: 29, step: 20
	action: tensor([[-1.2320, -0.9889,  1.4712, -0.6588,  0.0440,  0.3564,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[-35.7924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7165430973314701, distance: 1.886101206908605 entropy -2.5210373487559434
epoch: 29, step: 21
	action: tensor([[ 1.4776,  0.0071, -0.0647, -1.0894, -3.2916,  0.0483,  0.1681]],
       dtype=torch.float64)
	q_value: tensor([[-41.8678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06668175374449348, distance: 1.1818820217464163 entropy -1.835766841603857
epoch: 29, step: 22
	action: tensor([[ 0.0093,  0.4471, -3.1600, -0.4874,  0.7246, -0.0653, -0.7776]],
       dtype=torch.float64)
	q_value: tensor([[-47.7822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1818820217464163 entropy -1.909412921609739
epoch: 29, step: 23
	action: tensor([[ 0.2511, -0.0199, -0.0416,  0.2220, -0.1144,  0.0473,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6460063207060376, distance: 0.680854711413402 entropy -3.332865500750988
epoch: 29, step: 24
	action: tensor([[ 0.1917, -0.1085,  0.4347, -0.7179,  0.4202,  0.1770,  0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-34.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01630727353341843, distance: 1.1536370893854946 entropy -2.667298495163444
epoch: 29, step: 25
	action: tensor([[-0.0222,  0.2702, -0.0409, -0.2470,  0.1124, -0.4036, -0.2372]],
       dtype=torch.float64)
	q_value: tensor([[-33.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046623669386096, distance: 0.9542332837548936 entropy -2.4359971919776457
epoch: 29, step: 26
	action: tensor([[-0.2443,  0.0189, -0.0199,  0.1740,  0.0922,  0.1002, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-31.4881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15860701233781493, distance: 1.0496781023980781 entropy -3.3775452391192937
epoch: 29, step: 27
	action: tensor([[-0.5885, -0.2669, -0.3247,  0.0155, -0.0127,  0.3827,  0.0763]],
       dtype=torch.float64)
	q_value: tensor([[-32.8067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.535371076467319, distance: 1.4179580117046775 entropy -2.9487254044541884
epoch: 29, step: 28
	action: tensor([[-0.5570, -0.2876, -0.0998, -0.2618,  0.4403,  0.2373,  0.0858]],
       dtype=torch.float64)
	q_value: tensor([[-30.8197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5804837612086697, distance: 1.438638612032157 entropy -3.0256730371511105
epoch: 29, step: 29
	action: tensor([[ 0.0446,  0.2894,  0.1154,  0.3533, -0.3989,  0.1636, -0.0509]],
       dtype=torch.float64)
	q_value: tensor([[-30.3691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.438638612032157 entropy -3.0856676842495996
epoch: 29, step: 30
	action: tensor([[ 0.3201, -0.2648,  0.1756, -0.6911, -0.0775,  0.0786,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08160910469819282, distance: 1.1901230332403536 entropy -3.332865500750988
epoch: 29, step: 31
	action: tensor([[ 0.6554,  0.2149, -0.8623, -0.1958, -1.5437,  0.1051, -0.1213]],
       dtype=torch.float64)
	q_value: tensor([[-32.1739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8180135366974555, distance: 0.488175787821892 entropy -2.4581244802762
epoch: 29, step: 32
	action: tensor([[-0.5552, -0.0908,  0.9334, -0.0546,  0.1607,  0.5900, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[-38.4645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2161073817327679, distance: 1.2619514750075322 entropy -2.419421769637729
epoch: 29, step: 33
	action: tensor([[ 1.5176, -0.3683, -0.0805,  0.1915, -0.0901, -0.2818,  0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-38.3272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1848535097474875, distance: 1.0331765126894714 entropy -2.1706183477348113
epoch: 29, step: 34
	action: tensor([[-0.6286,  0.2548, -1.0531, -0.4734,  2.1534, -0.3863, -0.2756]],
       dtype=torch.float64)
	q_value: tensor([[-40.7634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1392239012294827, distance: 1.2214093416649718 entropy -2.235275132159371
epoch: 29, step: 35
	action: tensor([[-0.0684, -0.0180, -0.1566,  0.9725, -0.0378,  0.0123, -0.0781]],
       dtype=torch.float64)
	q_value: tensor([[-37.5891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2214093416649718 entropy -2.8781576377671954
epoch: 29, step: 36
	action: tensor([[ 0.1718,  0.2373,  0.1645, -0.3335,  0.2380,  0.0830,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4510411301056525, distance: 0.8478647827288284 entropy -3.332865500750988
epoch: 29, step: 37
	action: tensor([[-0.1266,  0.1115, -0.0319, -0.7261,  0.7181,  0.3445, -0.0948]],
       dtype=torch.float64)
	q_value: tensor([[-33.5039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07422484049057376, distance: 1.1010561196381088 entropy -2.771628204253891
epoch: 29, step: 38
	action: tensor([[ 0.4339,  0.0644,  1.1047, -0.5269, -0.1058,  0.0851, -0.2223]],
       dtype=torch.float64)
	q_value: tensor([[-31.1675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42667825538541015, distance: 0.8664747274929625 entropy -2.858722453001395
epoch: 29, step: 39
	action: tensor([[-0.7504, -0.4445, -1.9344, -0.1029,  0.5583, -0.0819, -0.0941]],
       dtype=torch.float64)
	q_value: tensor([[-40.6692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8307990020969962, distance: 1.5483781238388055 entropy -2.0456047312287273
epoch: 29, step: 40
	action: tensor([[ 0.1625, -0.2719, -0.0719,  0.2604, -0.0262,  0.0711,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-29.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3718437937068101, distance: 0.9069649852438981 entropy -3.3968285756253787
epoch: 29, step: 41
	action: tensor([[-0.1666, -0.4671,  0.4350, -0.1018,  0.0128,  0.2068,  0.0724]],
       dtype=torch.float64)
	q_value: tensor([[-33.4529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2741117965054678, distance: 1.2916964369695874 entropy -2.6865330132739595
epoch: 29, step: 42
	action: tensor([[ 1.3292,  0.1428,  0.1847, -0.4771, -0.3350,  0.1942,  0.0966]],
       dtype=torch.float64)
	q_value: tensor([[-34.0027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5578644816331071, distance: 0.7609119341933783 entropy -2.3839653352226557
epoch: 29, step: 43
	action: tensor([[ 0.7348, -0.3967, -0.1524, -1.2577,  0.0966,  0.5101, -0.2580]],
       dtype=torch.float64)
	q_value: tensor([[-40.3643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19512946930834962, distance: 1.251019764142058 entropy -2.0805800288455094
epoch: 29, step: 44
	action: tensor([[-0.3424, -2.1658, -0.1192, -0.1036,  0.1824, -0.4700, -0.3469]],
       dtype=torch.float64)
	q_value: tensor([[-31.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.251019764142058 entropy -2.2244257090950312
epoch: 29, step: 45
	action: tensor([[-0.0855,  0.0087,  0.0929,  0.2157, -0.1461,  0.0589,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3147901247254733, distance: 0.9472584759452439 entropy -3.332865500750988
epoch: 29, step: 46
	action: tensor([[ 2.1901e-01, -3.2037e-01, -3.0064e-01, -6.0297e-01,  2.0815e-01,
          5.1638e-04,  1.2417e-01]], dtype=torch.float64)
	q_value: tensor([[-34.8138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04679446468897375, distance: 1.1708126389628972 entropy -2.688689739131685
epoch: 29, step: 47
	action: tensor([[ 0.0933,  0.0982, -0.3706, -0.3493, -0.1244,  0.0184, -0.1393]],
       dtype=torch.float64)
	q_value: tensor([[-28.6957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3613455122630579, distance: 0.9145125661086699 entropy -2.8801684650742585
epoch: 29, step: 48
	action: tensor([[ 0.0960, -0.5101,  0.0750, -0.1046,  0.1267, -0.0901, -0.0125]],
       dtype=torch.float64)
	q_value: tensor([[-29.4928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18037764390606537, distance: 1.243274943137816 entropy -3.1848599767873944
epoch: 29, step: 49
	action: tensor([[ 0.0756, -0.7028, -0.0935,  0.2496,  0.1342,  0.1089,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[-31.3191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0952455286299847, distance: 1.1976017933972762 entropy -2.689423386136359
epoch: 29, step: 50
	action: tensor([[ 0.0655, -1.1013,  0.3305, -0.0686, -0.0417,  0.1372,  0.0703]],
       dtype=torch.float64)
	q_value: tensor([[-32.0587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6927069579605118, distance: 1.488838473410847 entropy -2.599269975290151
epoch: 29, step: 51
	action: tensor([[ 1.0869,  0.0632, -0.6641,  0.0992, -1.1668,  0.2775,  0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-34.1046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8129569101895053, distance: 0.4949114793901085 entropy -2.209663803526592
epoch: 29, step: 52
	action: tensor([[ 0.2624,  1.2636,  0.1123, -0.4898, -0.0154,  0.0158, -0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-40.6695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4949114793901085 entropy -2.2186533486760616
epoch: 29, step: 53
	action: tensor([[-0.2267, -0.0021,  0.3438,  0.1054,  0.0392,  0.2466,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1564556165562685, distance: 1.051019230353194 entropy -3.332865500750988
epoch: 29, step: 54
	action: tensor([[-0.0234,  0.3287,  0.4373, -0.0886, -0.6764,  0.2414,  0.1080]],
       dtype=torch.float64)
	q_value: tensor([[-35.0832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39663432255696385, distance: 0.8888878993535095 entropy -2.5822754735025772
epoch: 29, step: 55
	action: tensor([[ 0.4907, -0.3733, -0.5306, -1.1927, -0.7313, -0.3659,  0.2014]],
       dtype=torch.float64)
	q_value: tensor([[-38.9567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.053625726543721663, distance: 1.1746267217054571 entropy -2.2844328850284614
epoch: 29, step: 56
	action: tensor([[-0.0059,  0.9250, -0.4491, -0.3918, -1.2250,  0.3038, -0.1594]],
       dtype=torch.float64)
	q_value: tensor([[-33.3351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1746267217054571 entropy -2.6865506500847673
epoch: 29, step: 57
	action: tensor([[ 0.0949, -0.0554,  0.0789,  0.0781, -0.0482,  0.0805,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41388630290896555, distance: 0.8760877919694401 entropy -3.332865500750988
epoch: 29, step: 58
	action: tensor([[ 0.1285, -0.4364, -0.7380, -0.3484, -0.5863,  0.2373,  0.0605]],
       dtype=torch.float64)
	q_value: tensor([[-34.0915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03711083976386509, distance: 1.1653846139880033 entropy -2.6591773053291026
epoch: 29, step: 59
	action: tensor([[-0.0814,  0.1520,  0.3243,  0.1921, -0.1514, -0.2467,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-30.3426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3691530869475488, distance: 0.9089054015255332 entropy -2.7541337546020195
epoch: 29, step: 60
	action: tensor([[-0.7585, -0.2625,  0.5585, -0.2850,  0.1511,  0.4220,  0.1100]],
       dtype=torch.float64)
	q_value: tensor([[-36.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8625913784538863, distance: 1.5617642866257306 entropy -2.7325716144300713
epoch: 29, step: 61
	action: tensor([[ 0.1466,  0.9021,  0.1002, -0.8413,  0.2022,  0.4894,  0.0952]],
       dtype=torch.float64)
	q_value: tensor([[-34.9067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5617642866257306 entropy -2.4028459731009244
epoch: 29, step: 62
	action: tensor([[-0.3738, -0.0645, -0.0636,  0.1300,  0.2867, -0.0697,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20068884014792499, distance: 1.2539260658204545 entropy -3.332865500750988
epoch: 29, step: 63
	action: tensor([[ 0.0689, -0.0077, -0.0532, -0.0740,  0.0192,  0.0684,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[-31.8249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30678778944736096, distance: 0.9527737761534006 entropy -3.1911286235436536
LOSS epoch 29 actor 505.09494124574235 critic 174.3783766092407
epoch: 30, step: 0
	action: tensor([[ 0.0228, -0.0693, -0.2269, -0.3533, -0.0650,  0.3073,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-30.4455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1588309909940896, distance: 1.0495383810415384 entropy -2.839894691455114
epoch: 30, step: 1
	action: tensor([[-0.0474, -0.1165, -0.0361, -0.6239,  0.3214, -0.0518, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-28.8196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17637472669450238, distance: 1.2411650450241498 entropy -2.8052986865029226
epoch: 30, step: 2
	action: tensor([[ 0.0630, -0.3326,  0.2979,  0.3541, -0.0328,  0.3412, -0.1237]],
       dtype=torch.float64)
	q_value: tensor([[-28.0677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4221172989101306, distance: 0.8699144407985775 entropy -2.9817928331153674
epoch: 30, step: 3
	action: tensor([[ 0.6486, -0.2870,  0.1937,  0.6285,  0.1814,  0.6944,  0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-34.3362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9434897192651631, distance: 0.27203227008134206 entropy -2.375325483113254
epoch: 30, step: 4
	action: tensor([[-0.4305, -0.0708, -0.1958,  0.3089, -0.3577,  1.0866, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-38.2957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.27203227008134206 entropy -2.1749332254237057
epoch: 30, step: 5
	action: tensor([[-0.1162,  0.1368, -0.1769,  0.1960,  0.3206, -0.0743,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27484013100092985, distance: 0.9744814450312413 entropy -3.332052627086162
epoch: 30, step: 6
	action: tensor([[-0.5887, -0.1302,  0.1542,  0.1272, -0.1803,  0.1245, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[-30.5878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4543955528965866, distance: 1.3800599805511227 entropy -3.233899377676309
epoch: 30, step: 7
	action: tensor([[ 0.1522, -0.1617, -0.1070,  0.1318, -0.0974, -0.2807,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3121729313527335, distance: 0.9490658023481551 entropy -3.332052627086162
epoch: 30, step: 8
	action: tensor([[-0.3313,  0.0582, -0.1934, -0.0448,  0.1242,  0.5801,  0.0492]],
       dtype=torch.float64)
	q_value: tensor([[-30.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0928268525832302, distance: 1.0899379806009315 entropy -2.908427948026488
epoch: 30, step: 9
	action: tensor([[-0.1582,  0.5851, -0.1384, -0.0438, -0.8278,  0.2045,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[-29.9907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0899379806009315 entropy -2.872372978952239
epoch: 30, step: 10
	action: tensor([[-0.2771,  0.1218,  0.3567, -0.2458, -0.1145,  0.0293,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12632260731436262, distance: 1.2144736427108664 entropy -3.332052627086162
epoch: 30, step: 11
	action: tensor([[-0.8790, -0.0896, -0.3516,  0.2386,  0.3641,  0.2763,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-32.2927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7913071276950383, distance: 1.531587172205909 entropy -2.693552364019341
epoch: 30, step: 12
	action: tensor([[ 0.0727,  0.0150, -0.0063, -0.0676, -0.0748,  0.0341,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-29.5126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3204597317339285, distance: 0.943331402581589 entropy -3.4453963209915552
epoch: 30, step: 13
	action: tensor([[-0.0607,  0.0164, -0.4802,  0.5489, -0.2587,  0.4747,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-30.9654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.943331402581589 entropy -2.793157753283949
epoch: 30, step: 14
	action: tensor([[ 0.2107,  0.0700,  0.0348, -0.0876, -0.2738, -0.0872,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4898759623221881, distance: 0.8173246318542668 entropy -3.332052627086162
epoch: 30, step: 15
	action: tensor([[-0.3387,  0.1596, -0.1101,  0.3766, -0.0274,  0.0714,  0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-32.0581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8173246318542668 entropy -2.715002200181176
epoch: 30, step: 16
	action: tensor([[-0.1217, -0.0883, -0.0405,  0.4977,  0.0777, -0.0042,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882036628925586, distance: 0.9654606278998197 entropy -3.332052627086162
epoch: 30, step: 17
	action: tensor([[ 0.0795, -0.4064,  0.4259, -0.0688, -0.0643,  0.6894,  0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-32.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.226539180090512, distance: 1.006412107499352 entropy -2.841996805665944
epoch: 30, step: 18
	action: tensor([[ 1.9825,  0.4655,  0.6528,  0.8121,  0.6792, -0.6799,  0.0892]],
       dtype=torch.float64)
	q_value: tensor([[-34.2612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.006412107499352 entropy -2.148436660934839
epoch: 30, step: 19
	action: tensor([[-0.3116,  0.0029, -0.0289, -0.3593, -0.3674, -0.1039,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26330486758191585, distance: 1.2862067310334935 entropy -3.332052627086162
epoch: 30, step: 20
	action: tensor([[-0.4919,  0.6142, -0.4083,  0.4727, -0.2520,  0.2595,  0.0939]],
       dtype=torch.float64)
	q_value: tensor([[-30.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07091656114419098, distance: 1.1030216901946754 entropy -2.949701958368819
epoch: 30, step: 21
	action: tensor([[-0.1433,  0.0223,  0.3597, -0.2109,  0.2394,  0.0424,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-33.1295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01182866513628289, distance: 1.1375560880588766 entropy -3.1159948333458134
epoch: 30, step: 22
	action: tensor([[ 0.0943, -0.6880, -0.3498,  0.0048, -0.3160,  0.4356, -0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-31.9736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1375560880588766 entropy -2.700897066789575
epoch: 30, step: 23
	action: tensor([[-0.4328,  0.2815, -0.2222,  0.5859,  0.2118,  0.0431,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10312308826304195, distance: 1.0837350396545644 entropy -3.332052627086162
epoch: 30, step: 24
	action: tensor([[-0.0553, -0.2193,  0.1609, -0.2745, -0.0383,  0.2692,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-32.9496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06077241216071738, distance: 1.1786037035613652 entropy -3.201365786308102
epoch: 30, step: 25
	action: tensor([[-0.2822, -0.2795,  0.0958,  0.3728, -0.1752, -0.0167,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06920507536280684, distance: 1.1832791145668584 entropy -3.332052627086162
epoch: 30, step: 26
	action: tensor([[-0.3732, -0.1589,  0.2499, -0.2533, -0.2294,  0.4393,  0.1821]],
       dtype=torch.float64)
	q_value: tensor([[-32.2602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34307061083397494, distance: 1.3261911219386102 entropy -2.6933218617137245
epoch: 30, step: 27
	action: tensor([[-0.6960,  0.5500, -0.8541, -0.0097,  0.8692,  0.1338,  0.1579]],
       dtype=torch.float64)
	q_value: tensor([[-32.3311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17659374143478757, distance: 1.2412805782733198 entropy -2.4063314734133843
epoch: 30, step: 28
	action: tensor([[-0.1136, -0.0648, -0.1386, -0.1794, -0.0256, -0.0066,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[-30.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013352882415578304, distance: 1.1366784306563569 entropy -3.510300530175401
epoch: 30, step: 29
	action: tensor([[-0.2090,  0.4715, -0.4163,  0.4151,  0.0987,  0.1003,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[-28.9167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1366784306563569 entropy -3.0512396939331596
epoch: 30, step: 30
	action: tensor([[ 0.0867,  0.0233, -0.1469, -0.2877, -0.1400, -0.1563,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26068187673777776, distance: 0.9839485033440624 entropy -3.332052627086162
epoch: 30, step: 31
	action: tensor([[ 0.2295, -0.2284, -0.4680, -0.0395,  0.2022,  0.3461, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-29.5478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3800692446941647, distance: 0.901007247949691 entropy -2.997275065716432
epoch: 30, step: 32
	action: tensor([[-0.1593, -0.3301, -0.3557,  0.1122, -0.4750,  0.1674, -0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-28.5087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12269768319559482, distance: 1.2125177543176022 entropy -2.865069733437425
epoch: 30, step: 33
	action: tensor([[ 0.6252,  0.0741,  0.4926, -0.0844,  0.2617,  0.2227,  0.1507]],
       dtype=torch.float64)
	q_value: tensor([[-29.6338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7816674440915118, distance: 0.5347070899942007 entropy -2.7444486950422933
epoch: 30, step: 34
	action: tensor([[-1.5997, -0.4915, -0.5134, -0.3715,  0.7606, -0.3198, -0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-36.1280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.674460842982206, distance: 1.8714352914986332 entropy -2.3032154585485207
epoch: 30, step: 35
	action: tensor([[ 0.1049,  0.0518, -0.0216,  0.0316,  0.2837, -0.0993,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-31.0507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4384807798807512, distance: 0.8575096308448797 entropy -3.527727235180039
epoch: 30, step: 36
	action: tensor([[-0.5150,  0.2774, -0.1580, -0.0389,  0.0861,  0.3003, -0.0315]],
       dtype=torch.float64)
	q_value: tensor([[-30.8045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8575096308448797 entropy -2.9767480086211835
epoch: 30, step: 37
	action: tensor([[-0.1048,  0.1504,  0.0779,  0.3658,  0.0708,  0.0178,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4274429705921723, distance: 0.8658966685709079 entropy -3.332052627086162
epoch: 30, step: 38
	action: tensor([[-0.2174,  0.3625, -0.0375, -0.3488, -0.1729,  0.0338,  0.0868]],
       dtype=torch.float64)
	q_value: tensor([[-33.4178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14577877249117466, distance: 1.057649754730647 entropy -2.8189107784147533
epoch: 30, step: 39
	action: tensor([[ 0.0008, -0.1638,  0.3813,  0.3168, -0.1513, -0.0771,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3582344055852752, distance: 0.9167373133681206 entropy -3.332052627086162
epoch: 30, step: 40
	action: tensor([[-0.5478, -1.0178,  0.6118, -0.9112, -0.4060,  0.0271,  0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-34.5621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2753155104463478, distance: 1.7261459234266978 entropy -2.521825228232117
epoch: 30, step: 41
	action: tensor([[-0.4074, -0.0058, -0.0881,  0.4596, -0.0997,  0.9149,  0.0823]],
       dtype=torch.float64)
	q_value: tensor([[-33.6294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21034600436238982, distance: 1.0168926583843432 entropy -2.1398620616594903
epoch: 30, step: 42
	action: tensor([[ 0.3047,  0.3067, -0.1528,  0.2636, -0.3663,  0.8076,  0.1745]],
       dtype=torch.float64)
	q_value: tensor([[-34.1760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0168926583843432 entropy -2.4328336501399876
epoch: 30, step: 43
	action: tensor([[-0.1267, -0.0766, -0.0253, -0.0527,  0.0376, -0.1142,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01990087424167175, distance: 1.1329003061904763 entropy -3.332052627086162
epoch: 30, step: 44
	action: tensor([[-0.3873, -0.0720, -0.1919,  0.1779, -0.2742,  0.3063,  0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-29.9848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15421835561778496, distance: 1.2294211541904814 entropy -2.985002577819452
epoch: 30, step: 45
	action: tensor([[ 0.0411, -0.1217,  0.4250,  0.2701,  0.2195,  0.5526,  0.1596]],
       dtype=torch.float64)
	q_value: tensor([[-30.8431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6016771628533564, distance: 0.7222279585004353 entropy -2.7849796928592254
epoch: 30, step: 46
	action: tensor([[-0.6770, -0.0342,  0.6627,  0.4304,  0.6955,  0.5711,  0.0502]],
       dtype=torch.float64)
	q_value: tensor([[-35.5047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7222279585004353 entropy -2.3223863721131006
epoch: 30, step: 47
	action: tensor([[-0.2502, -0.1230, -0.0150,  0.0758, -0.0653,  0.2108,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03521479485789669, distance: 1.164318849217628 entropy -3.332052627086162
epoch: 30, step: 48
	action: tensor([[ 0.2425, -0.3899,  0.6960,  0.0554, -0.0852,  0.1264,  0.1163]],
       dtype=torch.float64)
	q_value: tensor([[-30.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23875497208082797, distance: 0.9984330023192743 entropy -2.742661149150021
epoch: 30, step: 49
	action: tensor([[ 0.0180,  0.0197, -0.0346, -1.5654, -1.1737,  0.1768,  0.0642]],
       dtype=torch.float64)
	q_value: tensor([[-36.0255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26754065275771066, distance: 1.2883612134549944 entropy -2.2080013316891423
epoch: 30, step: 50
	action: tensor([[-0.8110, -0.4835,  0.5267, -0.1280, -0.4383,  0.4620, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-35.2227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0476382437203315, distance: 1.6375075437742794 entropy -2.3228200680146895
epoch: 30, step: 51
	action: tensor([[-1.2325, -0.0460,  0.6600, -0.7880, -0.0229,  0.7913,  0.3347]],
       dtype=torch.float64)
	q_value: tensor([[-34.0990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.467509663676048, distance: 1.797571295217941 entropy -2.1898778246092374
epoch: 30, step: 52
	action: tensor([[-0.7018, -1.5612,  0.2253,  0.0980, -0.4135,  0.3897,  0.0747]],
       dtype=torch.float64)
	q_value: tensor([[-36.7068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.797571295217941 entropy -2.1960451639636296
epoch: 30, step: 53
	action: tensor([[ 0.0490,  0.0187, -0.1726,  0.1106,  0.3826,  0.1020,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39604394937025067, distance: 0.8893226666060233 entropy -3.332052627086162
epoch: 30, step: 54
	action: tensor([[-0.2877,  0.1217,  0.0466,  0.4714,  0.2565, -0.0234, -0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-30.1084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8893226666060233 entropy -3.0515738437922266
epoch: 30, step: 55
	action: tensor([[-0.0352,  0.1317, -0.1846, -0.3670,  0.3218,  0.0190,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2049145025121829, distance: 1.0203839274825202 entropy -3.332052627086162
epoch: 30, step: 56
	action: tensor([[-0.0425, -0.2028,  0.0657,  0.0508,  0.1163,  0.2083, -0.0869]],
       dtype=torch.float64)
	q_value: tensor([[-28.7889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1512334223648122, distance: 1.0542675229780019 entropy -3.1819211139248043
epoch: 30, step: 57
	action: tensor([[ 0.1861, -0.3067, -0.2912, -0.1518,  0.2290, -0.0973,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-30.7835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0898160939419641, distance: 1.091745144971075 entropy -2.7146786424451492
epoch: 30, step: 58
	action: tensor([[-0.3548,  0.1646, -0.3196,  0.0546,  0.3001,  0.1592, -0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-28.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.001285683680138705, distance: 1.1436083850343333 entropy -3.01495331679788
epoch: 30, step: 59
	action: tensor([[-0.1143,  0.0251, -0.0523,  0.2025, -0.0112, -0.0161,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-28.9277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23474104327683487, distance: 1.0010618339078117 entropy -3.4563671925334103
epoch: 30, step: 60
	action: tensor([[-0.0761,  0.2257, -0.2577,  0.0343, -0.1625, -0.0500,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-31.1379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0010618339078117 entropy -2.942494630089185
epoch: 30, step: 61
	action: tensor([[ 0.1481, -0.0389,  0.1552,  0.1414, -0.2570,  0.0395,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49037959309371715, distance: 0.8169210716882351 entropy -3.332052627086162
epoch: 30, step: 62
	action: tensor([[-0.4865,  0.1042,  0.1023,  0.0948,  0.1257,  0.6334,  0.1084]],
       dtype=torch.float64)
	q_value: tensor([[-33.5685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07282342352857052, distance: 1.1018891810968487 entropy -2.551496667334763
epoch: 30, step: 63
	action: tensor([[ 0.0517,  0.1303, -0.7011, -0.0782,  0.2624,  0.4816,  0.0929]],
       dtype=torch.float64)
	q_value: tensor([[-32.3445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1018891810968487 entropy -2.650941442368171
LOSS epoch 30 actor 402.2521186260016 critic 591.3028351695717
epoch: 31, step: 0
	action: tensor([[-0.3072,  0.0976,  0.0205, -0.0096,  0.3877, -0.0925,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06757442249457335, distance: 1.182376456242193 entropy -3.33213994006566
epoch: 31, step: 1
	action: tensor([[ 0.0127, -0.1304,  0.0333, -0.0997, -0.1240,  0.1636, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[-32.6991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15589905217213262, distance: 1.0513659005616014 entropy -3.196757273283002
epoch: 31, step: 2
	action: tensor([[ 1.0159,  0.0979, -0.4779,  0.7915,  1.2690,  0.6417,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-32.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08775987106710385 entropy -2.664273461418818
epoch: 31, step: 3
	action: tensor([[-0.1423, -0.1417, -0.1032,  0.0544,  0.1694, -0.0318,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014980084234086366, distance: 1.1357407253668281 entropy -3.33213994006566
epoch: 31, step: 4
	action: tensor([[ 0.5709, -0.2789,  0.1553,  0.2402, -0.2551,  0.1998,  0.0287]],
       dtype=torch.float64)
	q_value: tensor([[-31.7491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6590327216862623, distance: 0.6682101138945826 entropy -3.0215591785525158
epoch: 31, step: 5
	action: tensor([[-0.6326,  0.0295, -0.6812,  0.3089,  0.3628, -0.5691,  0.0515]],
       dtype=torch.float64)
	q_value: tensor([[-36.8017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5547486160428172, distance: 1.4268778054437368 entropy -2.3041917943056736
epoch: 31, step: 6
	action: tensor([[-0.1584, -0.0824, -0.0538, -0.0363,  0.1518,  0.0191,  0.0344]],
       dtype=torch.float64)
	q_value: tensor([[-30.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015972239152420742, distance: 1.135168597524631 entropy -3.7944386225085496
epoch: 31, step: 7
	action: tensor([[ 0.7349, -0.1347,  0.2111, -0.3991, -0.0898,  0.1592,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[-31.5988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44306327876709295, distance: 0.8540034401373562 entropy -3.013999617158171
epoch: 31, step: 8
	action: tensor([[ 0.4528, -1.1399, -0.5474, -0.6475, -0.6164,  0.2094, -0.1388]],
       dtype=torch.float64)
	q_value: tensor([[-34.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7925974737458195, distance: 1.5321387028898077 entropy -2.3150272180636913
epoch: 31, step: 9
	action: tensor([[-0.6995, -0.0602,  0.2619, -0.1084, -1.0924, -0.0334, -0.0737]],
       dtype=torch.float64)
	q_value: tensor([[-32.1510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.726545247597038, distance: 1.503646249814026 entropy -2.290567009108064
epoch: 31, step: 10
	action: tensor([[-0.4509,  0.6246,  1.0996,  0.4368,  0.5601,  0.3074,  0.3123]],
       dtype=torch.float64)
	q_value: tensor([[-36.4339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.503646249814026 entropy -2.4065152364272504
epoch: 31, step: 11
	action: tensor([[ 0.0901, -0.0756, -0.1297,  0.3807,  0.1166, -0.0767,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47638224279945995, distance: 0.8280639469985854 entropy -3.33213994006566
epoch: 31, step: 12
	action: tensor([[ 0.5380, -0.1770, -0.1975,  0.0782,  0.1631,  0.2531,  0.0561]],
       dtype=torch.float64)
	q_value: tensor([[-33.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6519541461183781, distance: 0.6751105978183627 entropy -2.8955420446290527
epoch: 31, step: 13
	action: tensor([[-0.3189,  0.3102, -0.4789, -0.2800,  0.0122,  0.2314, -0.0718]],
       dtype=torch.float64)
	q_value: tensor([[-33.7154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6751105978183627 entropy -2.5970625372708236
epoch: 31, step: 14
	action: tensor([[-0.1236, -0.0752, -0.0205, -0.3307, -0.0900,  0.1212,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05447371902611109, distance: 1.1750993156573575 entropy -3.33213994006566
epoch: 31, step: 15
	action: tensor([[-0.1219, -0.5054,  0.2477, -0.1631, -0.3635,  0.2830,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-31.5470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3243234630291123, distance: 1.3169028271696608 entropy -2.7927787042969903
epoch: 31, step: 16
	action: tensor([[-0.3893,  0.4712,  0.3858,  0.9277, -0.1243,  0.0177,  0.1770]],
       dtype=torch.float64)
	q_value: tensor([[-33.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3169028271696608 entropy -2.32076394490241
epoch: 31, step: 17
	action: tensor([[-0.3102, -0.1072,  0.0825, -0.3200, -0.0122, -0.2045,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3806991614441111, distance: 1.3446406112511924 entropy -3.33213994006566
epoch: 31, step: 18
	action: tensor([[-0.1222, -0.3655, -0.1770,  0.1058, -0.7943, -0.0296,  0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-31.1363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12443561088898747, distance: 1.2134558756556952 entropy -3.0644368099737602
epoch: 31, step: 19
	action: tensor([[-0.3062,  0.6675, -0.2278,  0.5440, -0.0311, -0.2594,  0.1952]],
       dtype=torch.float64)
	q_value: tensor([[-33.3307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2128274430673609, distance: 1.015293640137617 entropy -2.5587042896958314
epoch: 31, step: 20
	action: tensor([[0.0170, 0.1582, 0.1347, 0.1292, 0.4541, 0.0597, 0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-36.8004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.015293640137617 entropy -3.199525175209739
epoch: 31, step: 21
	action: tensor([[-0.1177, -0.1148, -0.1127,  0.7695, -0.1790, -0.0835,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3574777991427921, distance: 0.9172775456990238 entropy -3.33213994006566
epoch: 31, step: 22
	action: tensor([[-0.3401, -0.0783, -0.0479,  0.1237, -0.3373,  0.4889,  0.1591]],
       dtype=torch.float64)
	q_value: tensor([[-36.1981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06836190772929562, distance: 1.1828124597376157 entropy -2.7362515966297565
epoch: 31, step: 23
	action: tensor([[ 0.3608, -0.1053, -0.0312, -0.3453, -0.2997, -0.1032,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3118431551785593, distance: 0.9492932881497459 entropy -3.33213994006566
epoch: 31, step: 24
	action: tensor([[-0.4700, -0.7727, -0.5629,  0.0541,  0.7170,  0.3966, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-32.4957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6886989877127725, distance: 1.487074802285618 entropy -2.6406391398471354
epoch: 31, step: 25
	action: tensor([[ 0.2190, -0.0736,  0.0836,  0.5169, -0.0121,  0.0788, -0.0595]],
       dtype=torch.float64)
	q_value: tensor([[-30.8109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.487074802285618 entropy -3.064305226091179
epoch: 31, step: 26
	action: tensor([[ 0.0099, -0.1714, -0.0095,  0.1698, -0.0298, -0.1008,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23419294619407305, distance: 1.001420262160661 entropy -3.33213994006566
epoch: 31, step: 27
	action: tensor([[-0.4812, -0.3868, -0.3679,  0.6492,  1.0444,  0.0063,  0.0769]],
       dtype=torch.float64)
	q_value: tensor([[-33.1575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38036126661396596, distance: 1.3444760660248227 entropy -2.8135912805924153
epoch: 31, step: 28
	action: tensor([[ 0.1801, -0.0037, -0.2105, -0.1441,  0.0367,  0.1449, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-34.4780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4356459277134068, distance: 0.8596714914977751 entropy -3.2172738677831325
epoch: 31, step: 29
	action: tensor([[ 0.0117, -0.3711,  0.1790,  0.0763, -0.2965, -0.0769, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[-31.3972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02670022177415432, distance: 1.1289637711374498 entropy -2.888380385107845
epoch: 31, step: 30
	action: tensor([[-0.1378, -0.1125, -0.0948,  0.7579,  0.2286,  0.2575,  0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-33.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1289637711374498 entropy -2.5530526435828107
epoch: 31, step: 31
	action: tensor([[ 0.1520,  0.0788,  0.0844,  0.0016, -0.0307, -0.0480,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.464809975078438, distance: 0.8371642968925518 entropy -3.33213994006566
epoch: 31, step: 32
	action: tensor([[-0.1987, -0.7008,  0.1822,  0.0363, -0.8006,  0.2468,  0.0343]],
       dtype=torch.float64)
	q_value: tensor([[-33.9119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46265073409324353, distance: 1.3839710639804472 entropy -2.7697073017808758
epoch: 31, step: 33
	action: tensor([[-0.2183, -0.8465, -0.3794,  0.4788,  1.0504,  0.3488,  0.2935]],
       dtype=torch.float64)
	q_value: tensor([[-35.6854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30438213815857096, distance: 1.3069504264703464 entropy -2.1849898581897476
epoch: 31, step: 34
	action: tensor([[-0.4418, -0.2705,  0.1768,  0.2864, -0.2968,  0.2426, -0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-33.9516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18296830519878782, distance: 1.244638548648356 entropy -2.8160586095031808
epoch: 31, step: 35
	action: tensor([[-1.2964, -0.1075,  0.5250,  0.6033,  0.4915,  0.3368,  0.2526]],
       dtype=torch.float64)
	q_value: tensor([[-34.3829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6703139161023774, distance: 1.4789576664919366 entropy -2.523626125076738
epoch: 31, step: 36
	action: tensor([[ 0.7495, -0.2685, -0.0192, -0.1108, -0.0818,  0.6209,  0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-39.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6422880496068262, distance: 0.6844211440682786 entropy -2.5587984723714174
epoch: 31, step: 37
	action: tensor([[ 0.7467, -1.0917,  0.9166, -0.8197,  1.0113,  0.0793, -0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-36.1943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7170501673381231, distance: 1.4995059223847331 entropy -2.188790338920083
epoch: 31, step: 38
	action: tensor([[ 2.1302, -1.0434, -0.9672, -0.0559, -2.2146,  0.8763, -0.5295]],
       dtype=torch.float64)
	q_value: tensor([[-39.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4995059223847331 entropy -1.9095474102410077
epoch: 31, step: 39
	action: tensor([[-0.0647,  0.0307,  0.0106, -0.1745,  0.1659, -0.3473,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06870518998938324, distance: 1.1043335963988006 entropy -3.33213994006566
epoch: 31, step: 40
	action: tensor([[ 0.1794, -0.2731, -0.0183, -0.2719, -0.0804, -0.1268, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[-31.8467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03677800478478899, distance: 1.1231037797651677 entropy -3.146615504347467
epoch: 31, step: 41
	action: tensor([[ 0.7913, -0.1141, -0.4310, -0.3840, -0.2758,  0.1039, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[-31.0994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48640389606098466, distance: 0.8201014005240737 entropy -2.774217158545808
epoch: 31, step: 42
	action: tensor([[-0.7127, -0.5488,  0.8486, -0.1355,  0.5633,  0.5785, -0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-31.9252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8079250099296307, distance: 1.5386750072340112 entropy -2.548910938714125
epoch: 31, step: 43
	action: tensor([[-0.2342, -1.1109,  0.1207, -0.4655,  1.7643,  0.2072, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-37.2749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9596347185734366, distance: 1.6019326601788504 entropy -2.2148092175397993
epoch: 31, step: 44
	action: tensor([[-0.3017,  0.5232,  0.0889,  0.1017, -1.1592,  0.3047, -0.4323]],
       dtype=torch.float64)
	q_value: tensor([[-36.7902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6019326601788504 entropy -2.3878502435070397
epoch: 31, step: 45
	action: tensor([[-0.0797, -0.0817, -0.2306, -0.1680, -0.1540, -0.0167,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07083391704079234, distance: 1.1030707472587182 entropy -3.33213994006566
epoch: 31, step: 46
	action: tensor([[ 0.5093,  0.4874,  0.0852,  0.2537, -0.1306,  0.0508,  0.0458]],
       dtype=torch.float64)
	q_value: tensor([[-30.8406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1030707472587182 entropy -3.011426114340789
epoch: 31, step: 47
	action: tensor([[ 0.4610, -0.3275, -0.2977, -0.1648, -0.1116,  0.0490,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27951887581922197, distance: 0.9713326717191585 entropy -3.33213994006566
epoch: 31, step: 48
	action: tensor([[ 1.0620, -0.1671, -0.9262, -0.6723, -0.3831,  0.3238, -0.0511]],
       dtype=torch.float64)
	q_value: tensor([[-31.5484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31199123511095805, distance: 0.9491911465740576 entropy -2.6603965329099757
epoch: 31, step: 49
	action: tensor([[ 0.6340, -0.1506,  0.3499, -0.5206, -0.9174,  0.2680, -0.2304]],
       dtype=torch.float64)
	q_value: tensor([[-31.9420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3282844377416563, distance: 0.9378845859556258 entropy -2.4605632563349746
epoch: 31, step: 50
	action: tensor([[-1.8102,  0.2969,  0.5026,  0.0399, -0.6833,  0.7768,  0.0484]],
       dtype=torch.float64)
	q_value: tensor([[-36.7706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9378845859556258 entropy -2.0502028236813605
epoch: 31, step: 51
	action: tensor([[ 0.3393,  0.1830, -0.1445,  0.2439, -0.3797,  0.2319,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7527129070801034, distance: 0.5690591591425239 entropy -3.33213994006566
epoch: 31, step: 52
	action: tensor([[-0.0732,  0.1266,  0.2331, -0.3772,  0.2241, -0.0081,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07960251984488054, distance: 1.0978535328729804 entropy -3.33213994006566
epoch: 31, step: 53
	action: tensor([[-0.8664,  0.0380, -0.5875, -0.0685,  0.2216,  0.3121, -0.0655]],
       dtype=torch.float64)
	q_value: tensor([[-33.0894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7621015735685073, distance: 1.5190503278931924 entropy -2.840969033827917
epoch: 31, step: 54
	action: tensor([[-0.1085,  0.0195,  0.1253, -0.2966,  0.0232,  0.0581,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-30.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029527181941707892, distance: 1.1273230348928105 entropy -3.7079487713923767
epoch: 31, step: 55
	action: tensor([[ 1.0467,  0.1822,  0.0198, -0.6830, -0.1882,  0.2268,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[-32.2970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6041292871284926, distance: 0.7200014645890273 entropy -2.8084527686511467
epoch: 31, step: 56
	action: tensor([[-0.2735,  0.4555, -0.5198, -0.3369,  1.2454,  0.3597, -0.2434]],
       dtype=torch.float64)
	q_value: tensor([[-35.7104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7200014645890273 entropy -2.255979944654324
epoch: 31, step: 57
	action: tensor([[ 0.1436,  0.0823,  0.0282,  0.1628,  0.0425, -0.0014,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.528775196932376, distance: 0.7855444490378523 entropy -3.33213994006566
epoch: 31, step: 58
	action: tensor([[-0.1456,  0.1571,  0.3416,  0.3028, -0.1816,  0.2185,  0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-34.4739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7855444490378523 entropy -2.7830550413675716
epoch: 31, step: 59
	action: tensor([[ 0.0291, -0.1790,  0.1837,  0.0614, -0.1554, -0.0631,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7855444490378523 entropy -3.33213994006566
epoch: 31, step: 60
	action: tensor([[-0.0527, -0.0419,  0.3167,  0.0090, -0.0743, -0.0725,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18301821766565451, distance: 1.0343389507227707 entropy -3.33213994006566
epoch: 31, step: 61
	action: tensor([[-0.4013, -0.0966,  0.0969, -0.0491, -0.1386,  0.4552,  0.0852]],
       dtype=torch.float64)
	q_value: tensor([[-34.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16616990430124612, distance: 1.2357698861358588 entropy -2.664537941692747
epoch: 31, step: 62
	action: tensor([[ 0.5584, -0.2812, -0.4961,  0.1123,  0.1870,  0.0023,  0.1568]],
       dtype=torch.float64)
	q_value: tensor([[-33.6218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5015543507909694, distance: 0.8079148634114433 entropy -2.5638725548090924
epoch: 31, step: 63
	action: tensor([[ 0.0520, -0.0493, -0.2570, -0.1487, -0.2124,  0.2470, -0.0910]],
       dtype=torch.float64)
	q_value: tensor([[-32.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2782969566339196, distance: 0.9721560015600802 entropy -2.824444868831624
LOSS epoch 31 actor 578.1997958631562 critic 1085.4932871548478
epoch: 32, step: 0
	action: tensor([[-0.5264,  0.0008, -0.0460,  0.3754, -0.0021,  0.2897,  0.0526]],
       dtype=torch.float64)
	q_value: tensor([[-32.8249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09376356730339963, distance: 1.1967912900596012 entropy -2.8154638984257088
epoch: 32, step: 1
	action: tensor([[ 0.3222, -0.0929,  0.1966, -0.8563,  0.1191,  0.5417,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-36.4740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16805418402013428, distance: 1.0437685676245732 entropy -2.851049884219606
epoch: 32, step: 2
	action: tensor([[-1.0305,  0.2452, -0.9296, -0.5063, -0.0085,  0.0698, -0.2094]],
       dtype=torch.float64)
	q_value: tensor([[-34.8182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6573980051569379, distance: 1.4732284571170697 entropy -2.3068272281816626
epoch: 32, step: 3
	action: tensor([[ 0.2420,  0.0291,  0.0201, -0.2479, -0.0092, -0.0611,  0.0538]],
       dtype=torch.float64)
	q_value: tensor([[-30.6410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39271419298111643, distance: 0.8917708228489345 entropy -3.680175194895933
epoch: 32, step: 4
	action: tensor([[-0.6609, -0.3339,  0.6795, -0.5697,  0.0410,  0.1745, -0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-34.1527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.089908809774062, distance: 1.6543232051224686 entropy -2.781838834977967
epoch: 32, step: 5
	action: tensor([[-0.0040,  0.1146, -0.6678, -1.0799, -0.5426,  0.4239,  0.0673]],
       dtype=torch.float64)
	q_value: tensor([[-36.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27880619155042397, distance: 0.9718129635587084 entropy -2.380566718441731
epoch: 32, step: 6
	action: tensor([[ 0.0592,  0.3194, -0.4275, -0.2072, -0.1678,  0.1518, -0.0633]],
       dtype=torch.float64)
	q_value: tensor([[-32.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9718129635587084 entropy -2.8624058391876344
epoch: 32, step: 7
	action: tensor([[-0.4131, -0.0867, -0.1004,  0.0672, -0.2057,  0.0851,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26583675283734065, distance: 1.2874949781979605 entropy -3.3309454557291995
epoch: 32, step: 8
	action: tensor([[-0.5227, -0.5701, -0.0499, -0.2965,  0.4568,  0.0879,  0.1324]],
       dtype=torch.float64)
	q_value: tensor([[-34.1358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8526769835721575, distance: 1.5576021800191155 entropy -2.9055982499145006
epoch: 32, step: 9
	action: tensor([[ 0.0810,  0.2049,  0.2908,  0.3937,  0.1579,  0.0767, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-32.4676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5576021800191155 entropy -2.955539942419502
epoch: 32, step: 10
	action: tensor([[-0.1229, -0.3491,  0.3015,  0.1725, -0.0235,  0.1158,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02877735123542302, distance: 1.1277584609013314 entropy -3.3309454557291995
epoch: 32, step: 11
	action: tensor([[ 0.0163, -0.4746, -0.6077,  0.7927,  0.2553,  0.1725,  0.1308]],
       dtype=torch.float64)
	q_value: tensor([[-36.3749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10058781881256584, distance: 1.0852656964025211 entropy -2.528404005650797
epoch: 32, step: 12
	action: tensor([[0.0371, 0.5328, 0.3985, 0.1693, 0.3759, 0.3117, 0.0369]],
       dtype=torch.float64)
	q_value: tensor([[-36.2464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0852656964025211 entropy -2.9206289615745433
epoch: 32, step: 13
	action: tensor([[-0.0509, -0.0276,  0.0639,  0.0260,  0.0870, -0.0476,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21097190922778597, distance: 1.016489567765425 entropy -3.3309454557291995
epoch: 32, step: 14
	action: tensor([[-0.3672, -0.2136, -0.1387, -0.2705, -0.6437,  0.1451,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[-34.9327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.416490396056433, distance: 1.3619573583515663 entropy -2.859828048965921
epoch: 32, step: 15
	action: tensor([[ 0.8357, -0.2857, -0.2440,  0.3972,  0.4928, -0.0760,  0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-33.6814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7114111925301498, distance: 0.6147470240819329 entropy -2.682332294071634
epoch: 32, step: 16
	action: tensor([[-0.2598, -0.2377, -0.7272, -0.0226, -0.0190, -0.5309, -0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-37.9092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13168348512523798, distance: 1.2173604337950708 entropy -2.6201533303719855
epoch: 32, step: 17
	action: tensor([[ 0.0148, -0.1362, -0.0877, -0.0877,  0.1405,  0.0146,  0.0385]],
       dtype=torch.float64)
	q_value: tensor([[-28.6367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15629940823541766, distance: 1.0511165401998515 entropy -3.8023106342598276
epoch: 32, step: 18
	action: tensor([[-0.2606, -0.2591, -0.1700,  0.1040,  0.1815, -0.0135, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[-33.1757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1759321256741797, distance: 1.2409315341437355 entropy -2.9376600744863066
epoch: 32, step: 19
	action: tensor([[-0.1790, -0.3118,  0.1817, -0.1148,  0.1754,  0.0480,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[-32.7462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21935285217023925, distance: 1.2636342611538294 entropy -3.113622272853611
epoch: 32, step: 20
	action: tensor([[ 0.0229, -0.0375, -0.3075,  0.0978,  0.5551,  0.0040,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-33.9485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30943948664952026, distance: 0.9509497399989817 entropy -2.723093655286395
epoch: 32, step: 21
	action: tensor([[-0.1582, -0.0360,  0.0684,  0.1740,  0.1449,  0.2615, -0.0626]],
       dtype=torch.float64)
	q_value: tensor([[-33.1998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2580463569503071, distance: 0.985700731989888 entropy -3.250487022544828
epoch: 32, step: 22
	action: tensor([[-0.2534,  0.3490, -0.6501,  0.4950, -0.4509, -0.0267,  0.0715]],
       dtype=torch.float64)
	q_value: tensor([[-35.5595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.985700731989888 entropy -2.775790576817507
epoch: 32, step: 23
	action: tensor([[-0.0936,  0.1943, -0.0687,  0.2548, -0.0392, -0.1451,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3687580113605782, distance: 0.9091899635362416 entropy -3.3309454557291995
epoch: 32, step: 24
	action: tensor([[ 0.3988,  0.1171, -0.0437, -0.0166, -0.0871,  0.2792,  0.0695]],
       dtype=torch.float64)
	q_value: tensor([[-36.0829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7304940763833685, distance: 0.59407441754373 entropy -3.0134470146469385
epoch: 32, step: 25
	action: tensor([[-0.1465,  0.1480, -0.1866,  0.0313, -0.2328,  0.0447,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2169431618051746, distance: 1.0126359386172543 entropy -3.3309454557291995
epoch: 32, step: 26
	action: tensor([[ 0.0965,  0.1865,  0.4459, -0.1483,  0.4056, -0.0051,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.5173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39703015667524477, distance: 0.8885962769613494 entropy -2.964109489771867
epoch: 32, step: 27
	action: tensor([[-0.2042, -0.1567,  0.0969,  0.3722, -0.3615,  0.2683, -0.1012]],
       dtype=torch.float64)
	q_value: tensor([[-37.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17630080043481167, distance: 1.0385825352518925 entropy -2.6446135829159068
epoch: 32, step: 28
	action: tensor([[ 0.6528,  0.3468, -0.0514, -0.3555,  1.0228,  0.0728,  0.2316]],
       dtype=torch.float64)
	q_value: tensor([[-37.2864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8237251530897802, distance: 0.48045405854790785 entropy -2.4983049011267426
epoch: 32, step: 29
	action: tensor([[ 0.3638, -0.7117, -0.3190, -0.7905,  0.5207,  0.4000, -0.3071]],
       dtype=torch.float64)
	q_value: tensor([[-38.1368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3366455754175457, distance: 1.3230151757622435 entropy -2.620770608892497
epoch: 32, step: 30
	action: tensor([[ 0.7396, -0.3225,  0.0659, -0.4860, -0.3529,  0.1880, -0.2835]],
       dtype=torch.float64)
	q_value: tensor([[-31.2229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18569618107048824, distance: 1.0326423428486793 entropy -2.4864858909493215
epoch: 32, step: 31
	action: tensor([[ 0.5504, -0.3108, -0.4856, -0.9728,  0.3259, -0.1469, -0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-34.6316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.050551314924232926, distance: 1.1729117273882603 entropy -2.2612771081148546
epoch: 32, step: 32
	action: tensor([[ 0.1677, -0.5830,  0.1268,  0.0172,  0.4657, -0.4315, -0.2411]],
       dtype=torch.float64)
	q_value: tensor([[-29.7678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24228529175108027, distance: 1.2754615324403622 entropy -2.7867937683031707
epoch: 32, step: 33
	action: tensor([[ 0.3539,  0.2847, -0.3524,  0.0695, -0.1663, -0.0649, -0.0463]],
       dtype=torch.float64)
	q_value: tensor([[-33.5476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2754615324403622 entropy -2.7741693933071625
epoch: 32, step: 34
	action: tensor([[ 0.2675, -0.2315, -0.0161,  0.1649,  0.1759, -0.2208,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3586779392286308, distance: 0.9164204732937241 entropy -3.3309454557291995
epoch: 32, step: 35
	action: tensor([[ 0.4746, -0.3610, -0.5199,  0.4262,  0.1303,  0.0792, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-34.8229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5333892721189556, distance: 0.7816890943803152 entropy -2.8400860212135686
epoch: 32, step: 36
	action: tensor([[-0.3456,  0.4723,  0.0813, -0.3874,  0.3363,  0.0456, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-34.9134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7816890943803152 entropy -2.7895873078036884
epoch: 32, step: 37
	action: tensor([[1.0935e-01, 1.3721e-01, 2.7177e-01, 6.2165e-02, 1.2251e-04, 1.7126e-01,
         8.3499e-02]], dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5748820292748795, distance: 0.7461247137212401 entropy -3.3309454557291995
epoch: 32, step: 38
	action: tensor([[-0.3489,  0.1960,  0.1553,  0.2953, -0.0678,  0.0382,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7461247137212401 entropy -3.3309454557291995
epoch: 32, step: 39
	action: tensor([[ 0.1450,  0.1206,  0.1400,  0.1265, -0.0257,  0.1860,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5912603883766945, distance: 0.7316107146843502 entropy -3.3309454557291995
epoch: 32, step: 40
	action: tensor([[ 0.0891, -0.1481,  0.0644,  0.0881, -0.1245, -0.1384,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2879065980950921, distance: 0.965662072072172 entropy -3.3309454557291995
epoch: 32, step: 41
	action: tensor([[ 0.3995, -0.6512, -0.0402, -0.3470,  0.2902,  0.2426,  0.0699]],
       dtype=torch.float64)
	q_value: tensor([[-35.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16079156560999053, distance: 1.2329169270928113 entropy -2.738370494931719
epoch: 32, step: 42
	action: tensor([[-0.7915,  0.9179,  0.3428, -0.0899,  0.6484,  0.3099, -0.1571]],
       dtype=torch.float64)
	q_value: tensor([[-33.2827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44092689970636645, distance: 1.3736549888505083 entropy -2.4438245660262337
epoch: 32, step: 43
	action: tensor([[-0.4462,  0.2289, -0.1872,  0.0296, -0.1550,  0.2462, -0.0639]],
       dtype=torch.float64)
	q_value: tensor([[-40.8365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8104967240722794, distance: 1.5397689757526305 entropy -3.039345129064613
epoch: 32, step: 44
	action: tensor([[-0.0143, -0.3221, -0.1170,  0.2497,  0.1915, -0.0248,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5397689757526305 entropy -3.3309454557291995
epoch: 32, step: 45
	action: tensor([[ 0.3977, -0.2436,  0.0779, -0.1301, -0.2210,  0.0388,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3396985200815088, distance: 0.9298819733801715 entropy -3.3309454557291995
epoch: 32, step: 46
	action: tensor([[ 0.5236, -0.5211,  0.2302,  0.3828,  0.9156,  0.2368,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-35.5271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5300715063040151, distance: 0.7844632135904094 entropy -2.4823523095837174
epoch: 32, step: 47
	action: tensor([[-0.6938,  0.0586,  0.7137,  0.2787, -0.5918,  0.3130, -0.1851]],
       dtype=torch.float64)
	q_value: tensor([[-39.4764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1545240391792595, distance: 1.2295839435603464 entropy -2.4107470062120164
epoch: 32, step: 48
	action: tensor([[ 1.9742, -0.4515, -0.1798,  0.2834, -0.5081,  0.7740,  0.3826]],
       dtype=torch.float64)
	q_value: tensor([[-41.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2295839435603464 entropy -2.164388661617583
epoch: 32, step: 49
	action: tensor([[ 0.0075, -0.2428,  0.0235,  0.1761, -0.2144, -0.0379,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19808870238894172, distance: 1.0247545593880791 entropy -3.3309454557291995
epoch: 32, step: 50
	action: tensor([[-0.7033,  0.3318,  0.1208,  0.4948, -0.2102,  0.3029,  0.1211]],
       dtype=torch.float64)
	q_value: tensor([[-35.4994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03586230094000353, distance: 1.1236375020776745 entropy -2.6694892040404112
epoch: 32, step: 51
	action: tensor([[ 0.8470, -0.3902,  0.0502,  0.3651,  0.0098,  0.6364,  0.1955]],
       dtype=torch.float64)
	q_value: tensor([[-39.7970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1236375020776745 entropy -2.6465071100235127
epoch: 32, step: 52
	action: tensor([[-4.1058e-01,  5.9396e-02,  2.3792e-04,  1.6841e-01, -3.2332e-02,
          1.0222e-01,  8.3499e-02]], dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08756566173363245, distance: 1.1933956122277278 entropy -3.3309454557291995
epoch: 32, step: 53
	action: tensor([[-0.0658, -0.2981, -0.2177,  0.2549, -0.1510,  0.0061,  0.1081]],
       dtype=torch.float64)
	q_value: tensor([[-35.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0535156260544275, distance: 1.1133030976284606 entropy -2.9194814011146697
epoch: 32, step: 54
	action: tensor([[ 0.7759,  0.0369,  0.3149, -0.3866,  0.4468,  0.1319,  0.1108]],
       dtype=torch.float64)
	q_value: tensor([[-34.3150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6105229879099408, distance: 0.7141634307692735 entropy -2.8212457228693713
epoch: 32, step: 55
	action: tensor([[-0.3456, -0.1670, -0.2526, -0.5674, -0.9222,  0.0049, -0.2690]],
       dtype=torch.float64)
	q_value: tensor([[-37.8325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3944656893133185, distance: 1.351327483966772 entropy -2.3717750371115263
epoch: 32, step: 56
	action: tensor([[ 0.4668,  0.5240,  0.3422,  0.5201,  0.3278, -0.0150,  0.1219]],
       dtype=torch.float64)
	q_value: tensor([[-32.6047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.351327483966772 entropy -2.8277722233282385
epoch: 32, step: 57
	action: tensor([[-0.3454,  0.0528, -0.0305,  0.0062, -0.0339,  0.1503,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07706454970613841, distance: 1.1876201545340561 entropy -3.3309454557291995
epoch: 32, step: 58
	action: tensor([[-0.0606, -0.1217, -0.2331, -0.9367,  0.4650,  0.0450,  0.0829]],
       dtype=torch.float64)
	q_value: tensor([[-34.5920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15691372024926542, distance: 1.2308558073264628 entropy -2.9205242276772827
epoch: 32, step: 59
	action: tensor([[ 0.0032,  0.6342,  0.2595,  0.1446, -0.0900, -0.0250, -0.1872]],
       dtype=torch.float64)
	q_value: tensor([[-31.5662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2308558073264628 entropy -2.9740503481536864
epoch: 32, step: 60
	action: tensor([[-0.0284,  0.0008, -0.0035, -0.3486, -0.1198, -0.0784,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07328950159181846, distance: 1.1016121945199273 entropy -3.3309454557291995
epoch: 32, step: 61
	action: tensor([[ 0.6118,  0.1480, -0.1216, -0.1910, -0.6109,  0.0688,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[-33.2642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7479412381056155, distance: 0.5745232289094969 entropy -2.8774392833928877
epoch: 32, step: 62
	action: tensor([[-0.7232,  0.2074,  0.6834,  0.1757,  0.4812,  0.7252, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[-36.9887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5745232289094969 entropy -2.457087270389527
epoch: 32, step: 63
	action: tensor([[-0.1617, -0.1154, -0.0775, -0.1963,  0.0372,  0.0800,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-34.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07372477921740339, distance: 1.1857774338418652 entropy -3.3309454557291995
LOSS epoch 32 actor 472.42754241416833 critic 651.6230925748885
epoch: 33, step: 0
	action: tensor([[ 0.1709,  0.1759,  0.3699, -0.4602, -0.1159, -0.0357,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-35.3694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2758656704109682, distance: 0.9737921330930713 entropy -2.9195501183798487
epoch: 33, step: 1
	action: tensor([[-0.2414, -0.7174, -0.4923, -0.1640, -0.4491,  0.5134, -0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-38.4637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6368360310475729, distance: 1.4640613692639433 entropy -2.572123235418903
epoch: 33, step: 2
	action: tensor([[-0.6820, -0.9204, -0.4286,  0.0116, -1.0875,  0.3103,  0.1248]],
       dtype=torch.float64)
	q_value: tensor([[-35.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.218438300448959, distance: 1.7044347128608854 entropy -2.5333595583120747
epoch: 33, step: 3
	action: tensor([[-0.2618,  0.1635, -0.9416,  0.3109, -0.4012,  0.0485,  0.2669]],
       dtype=torch.float64)
	q_value: tensor([[-40.4634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03347581135707811, distance: 1.1633405100874852 entropy -2.365727844347943
epoch: 33, step: 4
	action: tensor([[ 0.0510,  0.1673,  0.0377,  0.0584, -0.0306, -0.1068,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-37.3660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1633405100874852 entropy -3.3818629957270616
epoch: 33, step: 5
	action: tensor([[-0.0820, -0.0100,  0.1070,  0.1052,  0.1668, -0.0905,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21028954019660284, distance: 1.016929014161422 entropy -3.328548749596614
epoch: 33, step: 6
	action: tensor([[-0.3857,  0.0960,  0.5471,  0.1746,  0.2014,  0.0888,  0.0332]],
       dtype=torch.float64)
	q_value: tensor([[-37.7264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06340257831853979, distance: 1.1074730643090038 entropy -2.8827096761783437
epoch: 33, step: 7
	action: tensor([[-1.1033, -0.0810, -0.1070, -0.2097, -0.0243, -0.1059,  0.0910]],
       dtype=torch.float64)
	q_value: tensor([[-40.7614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2681770782611856, distance: 1.723436045324495 entropy -2.6027567069613156
epoch: 33, step: 8
	action: tensor([[-0.0391,  0.0666, -0.2982,  0.0728,  0.0784, -0.0454,  0.0684]],
       dtype=torch.float64)
	q_value: tensor([[-36.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943770500785847, distance: 0.9612648057392584 entropy -3.4927092805942266
epoch: 33, step: 9
	action: tensor([[ 0.0184, -0.1042,  0.1017, -0.3716, -0.4612,  0.0999,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-35.8258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019361611405953938, distance: 1.1332119313125069 entropy -3.195782572054962
epoch: 33, step: 10
	action: tensor([[-0.5576, -0.2638, -0.3187,  0.2602,  0.4546,  0.4001,  0.0945]],
       dtype=torch.float64)
	q_value: tensor([[-36.7724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3874046229595336, distance: 1.3479018267961491 entropy -2.5431153919402285
epoch: 33, step: 11
	action: tensor([[ 6.9949e-02, -2.4744e-01, -1.3682e-04,  1.9276e-01,  5.1925e-01,
         -1.4175e-02,  7.5100e-03]], dtype=torch.float64)
	q_value: tensor([[-36.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24872147843375914, distance: 0.9918755381249647 entropy -3.1413454194955728
epoch: 33, step: 12
	action: tensor([[-0.2107,  0.1806, -0.0908,  0.1934,  0.2480,  0.2385, -0.0437]],
       dtype=torch.float64)
	q_value: tensor([[-37.0465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9918755381249647 entropy -2.8967025574291396
epoch: 33, step: 13
	action: tensor([[-0.3825,  0.1063,  0.1182,  0.1381, -0.1386,  0.1152,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02113930147748655, distance: 1.1563763182735678 entropy -3.328548749596614
epoch: 33, step: 14
	action: tensor([[-0.3038,  0.1594,  0.1819, -0.0644, -0.1851,  0.2440,  0.1343]],
       dtype=torch.float64)
	q_value: tensor([[-38.6519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03649162587023891, distance: 1.1232707243221078 entropy -2.7965762690111506
epoch: 33, step: 15
	action: tensor([[-0.5658,  0.0688,  0.4553,  0.1015,  0.0236,  0.1019,  0.1265]],
       dtype=torch.float64)
	q_value: tensor([[-39.1004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27464264379098857, distance: 1.2919654958316182 entropy -2.6418969768824128
epoch: 33, step: 16
	action: tensor([[-0.5543, -0.4471, -0.6114,  0.1624, -0.1301,  0.4897,  0.1385]],
       dtype=torch.float64)
	q_value: tensor([[-40.1876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7400727853253453, distance: 1.5095253150600252 entropy -2.6229435615107093
epoch: 33, step: 17
	action: tensor([[-0.1539, -0.2018,  0.2764, -0.6077,  0.4462,  0.2271,  0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-35.8505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3566198745578404, distance: 1.3328638245686315 entropy -2.9528914088392253
epoch: 33, step: 18
	action: tensor([[ 0.5049,  0.5207, -0.0661,  0.3307,  0.5618, -0.1466, -0.1706]],
       dtype=torch.float64)
	q_value: tensor([[-36.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3328638245686315 entropy -2.6191995386922757
epoch: 33, step: 19
	action: tensor([[-0.0731, -0.2201, -0.1684,  0.1266,  0.0071, -0.2106,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04007665331692667, distance: 1.121179040767179 entropy -3.328548749596614
epoch: 33, step: 20
	action: tensor([[-0.7826, -0.0221, -0.1166, -0.5163,  0.1110,  0.2203,  0.0591]],
       dtype=torch.float64)
	q_value: tensor([[-35.5747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8210382172894111, distance: 1.544245069490902 entropy -3.0416494938876473
epoch: 33, step: 21
	action: tensor([[-0.0910, -0.0102,  0.0775, -0.5676, -0.2084,  0.0589,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[-35.9655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09941933310646478, distance: 1.1998815573749753 entropy -3.2568775185005348
epoch: 33, step: 22
	action: tensor([[-0.1249, -0.5778, -0.3051, -0.2504,  0.6365,  0.6747,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-35.6006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13709417807538338, distance: 1.2202671251857546 entropy -2.755614098451639
epoch: 33, step: 23
	action: tensor([[-0.4902, -0.2186, -0.5319,  0.6733, -0.1050, -0.1464, -0.1799]],
       dtype=torch.float64)
	q_value: tensor([[-35.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39031793498284073, distance: 1.34931626612488 entropy -2.6722528914067643
epoch: 33, step: 24
	action: tensor([[-0.1328, -0.0128, -0.1283, -0.1001,  0.1709,  0.1358,  0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-36.8793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12094237153735699, distance: 1.0729151229866094 entropy -3.225535639165629
epoch: 33, step: 25
	action: tensor([[-0.1993, -0.0159,  0.0958, -0.3248, -0.2635,  0.2560, -0.0061]],
       dtype=torch.float64)
	q_value: tensor([[-35.9739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07762349509675426, distance: 1.1879282738160448 entropy -2.9970451151116384
epoch: 33, step: 26
	action: tensor([[ 1.2034, -0.3244,  0.1528, -0.1675,  0.3398,  0.2679,  0.1000]],
       dtype=torch.float64)
	q_value: tensor([[-36.6759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3474847744053631, distance: 0.9243831452856021 entropy -2.6366812604456866
epoch: 33, step: 27
	action: tensor([[-0.6514, -0.1771,  0.8724, -0.8640, -0.0759,  0.2400, -0.2951]],
       dtype=torch.float64)
	q_value: tensor([[-41.3675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2105703316281393, distance: 1.7014095327696985 entropy -2.169931846415868
epoch: 33, step: 28
	action: tensor([[-0.3314,  1.1111,  0.3009,  0.7601,  0.7504,  0.0908,  0.0512]],
       dtype=torch.float64)
	q_value: tensor([[-40.1082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7014095327696985 entropy -2.2607701014266968
epoch: 33, step: 29
	action: tensor([[-0.0504, -0.1470, -0.3375, -0.2737, -0.2369, -0.3832,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03292244847374437, distance: 1.1253492981137474 entropy -3.328548749596614
epoch: 33, step: 30
	action: tensor([[0.3487, 0.1174, 0.3893, 0.0185, 0.0846, 0.1061, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-33.3434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6985039352958362, distance: 0.6283440648187051 entropy -3.297420908959244
epoch: 33, step: 31
	action: tensor([[ 0.6003,  0.5494, -0.2595,  0.4807, -0.3632, -0.2238, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[-40.7232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9292205576772063, distance: 0.30444599236438563 entropy -2.4807392988986403
epoch: 33, step: 32
	action: tensor([[-0.0258, -0.2886, -0.4574, -0.8148, -0.3632,  0.2899,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-42.3437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.30444599236438563 entropy -2.7187213477533723
epoch: 33, step: 33
	action: tensor([[ 0.0556,  0.1418,  0.0530, -0.1724,  0.0565, -0.0952,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3455201264295462, distance: 0.9257737048087853 entropy -3.328548749596614
epoch: 33, step: 34
	action: tensor([[-0.4702, -0.0143,  0.2026, -0.2785, -0.3131,  0.1642, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[-37.1102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42096413552897194, distance: 1.3641064159938157 entropy -2.901127508796501
epoch: 33, step: 35
	action: tensor([[-1.3657,  0.1367,  0.3945,  0.8471,  0.4438, -0.1680,  0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-37.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5791461053358542, distance: 1.4380296811491948 entropy -2.6801663027571023
epoch: 33, step: 36
	action: tensor([[-0.4032, -0.0393,  0.3571,  0.1580,  0.2400,  0.1646,  0.1277]],
       dtype=torch.float64)
	q_value: tensor([[-45.5899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.036020103065605324, distance: 1.164771631215738 entropy -2.820775298811395
epoch: 33, step: 37
	action: tensor([[ 0.0506, -0.1327, -0.4607,  0.6682,  0.0724, -0.2961,  0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-39.6204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33801472256942977, distance: 0.9310668382485418 entropy -2.6712988055973637
epoch: 33, step: 38
	action: tensor([[-0.0130, -0.2221, -0.1198, -0.2623,  0.3854,  0.0287,  0.0569]],
       dtype=torch.float64)
	q_value: tensor([[-37.9730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01732231739337764, distance: 1.154213047088679 entropy -3.124045649475667
epoch: 33, step: 39
	action: tensor([[-0.2813, -0.2134, -0.1751,  0.2231, -0.3248,  0.3619, -0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-34.3034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07255757351942593, distance: 1.185132751636152 entropy -2.989384814840129
epoch: 33, step: 40
	action: tensor([[-0.4877, -0.1942, -0.2785,  0.0883, -0.5732,  0.1614,  0.1868]],
       dtype=torch.float64)
	q_value: tensor([[-37.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43732194611507547, distance: 1.3719355873984582 entropy -2.644520422626875
epoch: 33, step: 41
	action: tensor([[-0.4861, -0.0157, -0.0196, -0.0884, -0.7648,  0.1083,  0.1807]],
       dtype=torch.float64)
	q_value: tensor([[-37.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36064025232030006, distance: 1.3348373515506395 entropy -2.758640929468632
epoch: 33, step: 42
	action: tensor([[0.9326, 0.3767, 0.4849, 0.6332, 0.4908, 0.2214, 0.2189]],
       dtype=torch.float64)
	q_value: tensor([[-38.9247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3348373515506395 entropy -2.6006883848318028
epoch: 33, step: 43
	action: tensor([[-0.1731, -0.2122,  0.2024,  0.1376, -0.1577, -0.0420,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020372385128822668, distance: 1.1559419943381588 entropy -3.328548749596614
epoch: 33, step: 44
	action: tensor([[ 0.2558,  0.2868, -0.2117, -1.4122, -0.4245,  0.1781,  0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-37.8853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23239949878698984, distance: 1.0025921920096572 entropy -2.6731568026441517
epoch: 33, step: 45
	action: tensor([[ 0.4419, -0.3477,  0.2177,  0.3080,  0.3243,  0.1292, -0.1822]],
       dtype=torch.float64)
	q_value: tensor([[-38.0914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5750648167106649, distance: 0.7459642913258517 entropy -2.6066718622125036
epoch: 33, step: 46
	action: tensor([[ 0.3637, -0.3709, -0.0455, -1.0473, -1.0579,  0.0214, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[-39.6490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31193961149206006, distance: 1.3107311349910524 entropy -2.4659357194581717
epoch: 33, step: 47
	action: tensor([[ 0.4722,  1.3204, -0.6889,  1.0968, -0.6759,  0.2148, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-38.6292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3107311349910524 entropy -2.2711267456221527
epoch: 33, step: 48
	action: tensor([[-0.2796, -0.0676,  0.1742,  0.3052, -0.3499, -0.0711,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05426829562398827, distance: 1.1128603454592039 entropy -3.328548749596614
epoch: 33, step: 49
	action: tensor([[-0.7202, -0.2089, -0.2483, -0.6291, -0.9007, -0.0293,  0.1922]],
       dtype=torch.float64)
	q_value: tensor([[-39.4141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8233516225806994, distance: 1.5452256449688484 entropy -2.6664840032329136
epoch: 33, step: 50
	action: tensor([[ 0.0417, -0.6042, -0.3333,  0.1562, -0.1120,  0.1969,  0.1375]],
       dtype=torch.float64)
	q_value: tensor([[-36.9157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12902414451988653, distance: 1.2159292561781982 entropy -2.883554564048994
epoch: 33, step: 51
	action: tensor([[ 0.9520,  0.2750,  0.7014, -0.2121, -0.0153,  0.4116,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-36.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9158023481074888, distance: 0.33205244566398545 entropy -2.6448915885011295
epoch: 33, step: 52
	action: tensor([[-1.4235,  0.2247,  1.1889,  0.1955, -0.9916,  0.7845, -0.1847]],
       dtype=torch.float64)
	q_value: tensor([[-46.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.33205244566398545 entropy -2.038151998834829
epoch: 33, step: 53
	action: tensor([[-0.0967, -0.3418,  0.0808,  0.0685, -0.0836, -0.0487,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08145102553424488, distance: 1.1900360607251468 entropy -3.328548749596614
epoch: 33, step: 54
	action: tensor([[-0.6236, -0.4295,  0.4563, -0.8254,  0.2649,  0.6719,  0.1014]],
       dtype=torch.float64)
	q_value: tensor([[-36.5842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9578827132327798, distance: 1.6012163985876544 entropy -2.720047238300394
epoch: 33, step: 55
	action: tensor([[ 0.4085, -0.2617, -0.1981, -0.1739, -1.3262,  0.5574, -0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-39.2312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2385749185148286, distance: 0.9985510725772263 entropy -2.295283726936218
epoch: 33, step: 56
	action: tensor([[ 1.1501, -1.6375,  0.6484,  0.6198, -1.2024, -0.2212,  0.1666]],
       dtype=torch.float64)
	q_value: tensor([[-41.5485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9985510725772263 entropy -2.0388846478532163
epoch: 33, step: 57
	action: tensor([[-0.2466, -0.1273,  0.0211,  0.1643, -0.2380,  0.1518,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007649054876168737, distance: 1.148712492686448 entropy -3.328548749596614
epoch: 33, step: 58
	action: tensor([[ 0.0550,  0.0420,  0.3039, -0.3236, -0.1873,  0.1772,  0.1622]],
       dtype=torch.float64)
	q_value: tensor([[-38.1512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19570182807962855, distance: 1.0262785078304393 entropy -2.669398591088789
epoch: 33, step: 59
	action: tensor([[ 0.0144, -0.4927,  0.0211,  0.0902, -0.6222,  0.6428,  0.0388]],
       dtype=torch.float64)
	q_value: tensor([[-38.5023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00366858865065689, distance: 1.1422432611385058 entropy -2.4652758548111025
epoch: 33, step: 60
	action: tensor([[ 0.4169,  0.0472, -0.3500,  0.3579, -0.1000,  0.6187,  0.2368]],
       dtype=torch.float64)
	q_value: tensor([[-39.9613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1422432611385058 entropy -2.1516403797851535
epoch: 33, step: 61
	action: tensor([[-0.3006,  0.3071,  0.1592,  0.1774, -0.0055, -0.1724,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-38.9764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16510677975002697, distance: 1.0456158561907232 entropy -3.328548749596614
epoch: 33, step: 62
	action: tensor([[-0.4884, -0.1205,  0.0713,  0.0771, -0.1112,  0.2857,  0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-39.2033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3132113842093902, distance: 1.3113662817231608 entropy -2.9889867075931043
epoch: 33, step: 63
	action: tensor([[-0.0069, -0.9089,  0.0730,  0.5557,  0.0860,  0.0469,  0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-37.8287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13755585825784045, distance: 1.220514824974666 entropy -2.70542319006589
LOSS epoch 33 actor 614.9185681766476 critic 89.52672848120582
epoch: 34, step: 0
	action: tensor([[ 0.2957, -0.0987,  0.0032,  0.3766, -0.1964, -0.1636,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6442442207800593, distance: 0.6825471777063773 entropy -3.326564704568379
epoch: 34, step: 1
	action: tensor([[ 0.0506,  0.5341, -0.6787, -0.0593,  0.0286, -0.2396,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-40.1410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6825471777063773 entropy -2.6601634698078063
epoch: 34, step: 2
	action: tensor([[-0.4957,  0.0176,  0.1070, -0.4883,  0.0761, -0.1638,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5900700044958831, distance: 1.4429949651663239 entropy -3.326564704568379
epoch: 34, step: 3
	action: tensor([[ 0.3603,  0.3140, -0.4751,  0.1164,  0.5575,  0.1339,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[-36.0166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4429949651663239 entropy -3.1536804833789693
epoch: 34, step: 4
	action: tensor([[-0.0332,  0.0100, -0.0550,  0.1672, -0.2059, -0.1058,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3168799948192985, distance: 0.9458128174657789 entropy -3.326564704568379
epoch: 34, step: 5
	action: tensor([[ 0.1629,  0.1588, -0.1895,  0.0199, -0.0797,  0.4873,  0.0961]],
       dtype=torch.float64)
	q_value: tensor([[-38.1544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6143961854316309, distance: 0.7106035192856421 entropy -2.8455988585500656
epoch: 34, step: 6
	action: tensor([[-0.7935,  0.4642,  0.0362,  0.6852, -0.2786,  0.4279,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-39.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7106035192856421 entropy -2.626820274341754
epoch: 34, step: 7
	action: tensor([[-0.0970,  0.2387, -0.2752,  0.0380,  0.4966, -0.0709,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3124340737382687, distance: 0.948885622730738 entropy -3.326564704568379
epoch: 34, step: 8
	action: tensor([[-0.0424,  0.0534, -0.0471, -0.1911,  0.0078,  0.0611, -0.0519]],
       dtype=torch.float64)
	q_value: tensor([[-37.1470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.193744503068217, distance: 1.0275265151157837 entropy -3.377332873360782
epoch: 34, step: 9
	action: tensor([[ 0.0989, -0.0763, -0.3549, -0.1652,  0.2055,  0.0729,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30686903191688586, distance: 0.9527179433497087 entropy -3.326564704568379
epoch: 34, step: 10
	action: tensor([[-0.4833,  0.4423, -0.1383,  0.0727,  0.1494,  0.1877, -0.0524]],
       dtype=torch.float64)
	q_value: tensor([[-35.1125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9527179433497087 entropy -3.086671586844024
epoch: 34, step: 11
	action: tensor([[ 0.0532, -0.1089, -0.0754, -0.2267,  0.0865,  0.0917,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17599186056952842, distance: 1.0387772844204146 entropy -3.326564704568379
epoch: 34, step: 12
	action: tensor([[ 0.4541, -0.5018, -0.6367,  0.6277,  0.4566, -0.1107, -0.0251]],
       dtype=torch.float64)
	q_value: tensor([[-35.9999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3789044888906722, distance: 0.9018532786826978 entropy -2.8303432073602783
epoch: 34, step: 13
	action: tensor([[-0.2662, -0.4388,  0.2801, -0.2669,  0.1418,  0.1342, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-37.9539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4894112026346962, distance: 1.3965741568678116 entropy -2.925474947565403
epoch: 34, step: 14
	action: tensor([[-0.7409,  0.2371, -0.0389, -0.1136,  0.4001,  0.0130,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-36.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5087251185273038, distance: 1.4056000169380707 entropy -2.5950596720759527
epoch: 34, step: 15
	action: tensor([[-0.2903,  0.0841,  0.0205,  0.3254, -0.0808,  0.1617,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[-37.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4056000169380707 entropy -3.484518006772894
epoch: 34, step: 16
	action: tensor([[-0.0450, -0.0294,  0.1478,  0.2159, -0.2170, -0.1310,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3082370641699901, distance: 0.951777289508768 entropy -3.326564704568379
epoch: 34, step: 17
	action: tensor([[0.0007, 0.1494, 0.0644, 0.0822, 0.4199, 0.0657, 0.1239]],
       dtype=torch.float64)
	q_value: tensor([[-39.1154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4452160182358471, distance: 0.8523513432017977 entropy -2.71753378716745
epoch: 34, step: 18
	action: tensor([[-0.1447, -0.3828, -0.2316, -0.1200,  0.4524,  0.0273, -0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-38.6206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2221470376996395, distance: 1.2650812615539644 entropy -2.9247772902473335
epoch: 34, step: 19
	action: tensor([[ 0.2439,  0.1464,  0.1273, -0.0665, -0.1073,  0.2198, -0.0490]],
       dtype=torch.float64)
	q_value: tensor([[-33.9744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2650812615539644 entropy -3.114956456435556
epoch: 34, step: 20
	action: tensor([[ 0.0410, -0.1064, -0.0296, -0.0262,  0.0871, -0.0224,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.234519759397682, distance: 1.0012065580123353 entropy -3.326564704568379
epoch: 34, step: 21
	action: tensor([[-0.2762,  0.1061, -0.0937, -0.2481, -0.4522,  0.2437,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[-36.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027908503427987563, distance: 1.1602028358581526 entropy -2.8582120298836218
epoch: 34, step: 22
	action: tensor([[ 0.6109, -0.0148, -1.0707, -0.5031,  0.0079,  0.3025,  0.1286]],
       dtype=torch.float64)
	q_value: tensor([[-37.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6662469579381074, distance: 0.661103276685896 entropy -2.7542451137622996
epoch: 34, step: 23
	action: tensor([[ 0.0661, -0.2365, -0.0317,  0.0904,  0.0169, -0.1965, -0.1533]],
       dtype=torch.float64)
	q_value: tensor([[-35.1108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16652214962924017, distance: 1.0447291793641709 entropy -2.89611610949731
epoch: 34, step: 24
	action: tensor([[ 0.0794,  0.4967,  0.1100,  0.0769, -0.1716,  0.1451,  0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-35.8837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0447291793641709 entropy -2.921990141448466
epoch: 34, step: 25
	action: tensor([[-0.1439, -0.1741, -0.3151, -0.0795, -0.1820,  0.1375,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04105516341477489, distance: 1.1675985971659804 entropy -3.326564704568379
epoch: 34, step: 26
	action: tensor([[-0.2931, -0.2982, -0.0067,  0.1912, -0.2724,  0.2651,  0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-35.5795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15434016277222584, distance: 1.2294860242131078 entropy -2.9416368905949635
epoch: 34, step: 27
	action: tensor([[ 0.1692,  0.4622,  0.2160,  0.2612, -0.2481,  0.5663,  0.1944]],
       dtype=torch.float64)
	q_value: tensor([[-38.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2294860242131078 entropy -2.5775660622273615
epoch: 34, step: 28
	action: tensor([[-0.2244,  0.0244,  0.5295, -0.0292, -0.0217, -0.1855,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06705483247134425, distance: 1.1820886890537907 entropy -3.326564704568379
epoch: 34, step: 29
	action: tensor([[-0.2289, -0.0561,  0.9569,  0.0074, -0.0040,  0.0915,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-39.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002073474254942198, distance: 1.1455300238272212 entropy -2.6399033577732127
epoch: 34, step: 30
	action: tensor([[-0.9552, -0.2647,  0.5256, -0.4042,  0.4825,  0.5164,  0.1124]],
       dtype=torch.float64)
	q_value: tensor([[-42.7091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1643188929680752, distance: 1.6835162725084203 entropy -2.243894816964916
epoch: 34, step: 31
	action: tensor([[-0.4102,  0.2008,  0.7753,  0.0813,  0.7170,  0.4611, -0.0539]],
       dtype=torch.float64)
	q_value: tensor([[-40.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6835162725084203 entropy -2.501153862496071
epoch: 34, step: 32
	action: tensor([[ 0.1185, -0.3738,  0.4631, -0.2763, -0.0070,  0.0844,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08852262895985841, distance: 1.1939205411650469 entropy -3.326564704568379
epoch: 34, step: 33
	action: tensor([[-0.2775, -0.1622, -0.4890,  0.0038, -0.8141,  0.5096, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[-37.9232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24901965525100178, distance: 1.2789139649817272 entropy -2.3841050394486563
epoch: 34, step: 34
	action: tensor([[ 0.0966,  0.3164,  0.2007,  0.0491, -0.2999,  0.2125,  0.1733]],
       dtype=torch.float64)
	q_value: tensor([[-38.1873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2789139649817272 entropy -2.5356436868547894
epoch: 34, step: 35
	action: tensor([[-0.2049,  0.1523,  0.3492,  0.2712, -0.3414, -0.0769,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2789139649817272 entropy -3.326564704568379
epoch: 34, step: 36
	action: tensor([[-0.0286,  0.1362,  0.2380,  0.2003,  0.0357,  0.0073,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4540682760482233, distance: 0.845523843673089 entropy -3.326564704568379
epoch: 34, step: 37
	action: tensor([[-0.1207, -0.2047, -0.0548, -0.5108,  0.4593,  0.2351,  0.0721]],
       dtype=torch.float64)
	q_value: tensor([[-39.9771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15712250761862157, distance: 1.2309668681376738 entropy -2.7272011214080765
epoch: 34, step: 38
	action: tensor([[-0.0138, -0.1494,  0.0606, -0.1684,  0.0637,  0.1049,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08900889101057063, distance: 1.0922291485742035 entropy -3.326564704568379
epoch: 34, step: 39
	action: tensor([[-0.3906,  0.2486, -0.1747, -0.2845,  0.0886,  0.5782,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-36.6527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05265669471327583, distance: 1.11380814245479 entropy -2.729124131042107
epoch: 34, step: 40
	action: tensor([[-0.0179, -0.1095,  0.0290,  0.0109,  0.0909,  0.0561,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[-37.3174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2107537051992473, distance: 1.0166301120556998 entropy -2.963876622268512
epoch: 34, step: 41
	action: tensor([[ 0.6139, -0.6251,  0.4780,  0.1258,  0.5468,  0.0521,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-36.8470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20303803746102256, distance: 1.02158731151862 entropy -2.824901878102421
epoch: 34, step: 42
	action: tensor([[-1.6415,  0.9038, -0.2289, -0.1125, -0.9202, -0.5780, -0.1618]],
       dtype=torch.float64)
	q_value: tensor([[-41.0806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.02158731151862 entropy -2.2701376185284086
epoch: 34, step: 43
	action: tensor([[-0.1388,  0.1162,  0.2221, -0.2519, -0.4486,  0.1293,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0707042779812187, distance: 1.1031476958631399 entropy -3.326564704568379
epoch: 34, step: 44
	action: tensor([[-0.4794,  0.0783,  0.4629, -0.7334, -0.0046,  0.2314,  0.1365]],
       dtype=torch.float64)
	q_value: tensor([[-38.7450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6529979717681473, distance: 1.4712716059724698 entropy -2.5636366104320873
epoch: 34, step: 45
	action: tensor([[-0.2631,  0.1913, -0.1264, -0.4261,  0.5323, -0.0741, -0.0118]],
       dtype=torch.float64)
	q_value: tensor([[-39.2890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06004868463560209, distance: 1.1782015751348414 entropy -2.5390387114752784
epoch: 34, step: 46
	action: tensor([[-0.1417, -0.2875,  0.0838,  0.3764, -0.1192, -0.1892, -0.0796]],
       dtype=torch.float64)
	q_value: tensor([[-35.4218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06291328380177397, distance: 1.1077623079754177 entropy -3.35297237297048
epoch: 34, step: 47
	action: tensor([[-0.4329,  0.0376, -0.3344, -0.4296,  0.2272,  0.3569,  0.1542]],
       dtype=torch.float64)
	q_value: tensor([[-37.9513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22535084760800617, distance: 1.2667383563979837 entropy -2.7656394291754154
epoch: 34, step: 48
	action: tensor([[ 0.2599,  0.4728,  0.0527, -0.4095,  0.0460, -0.0476, -0.0458]],
       dtype=torch.float64)
	q_value: tensor([[-35.6374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6347198363963864, distance: 0.6916234940744962 entropy -3.238728156881211
epoch: 34, step: 49
	action: tensor([[-0.0415,  0.2081,  0.3029,  0.0662, -0.2854,  0.0399, -0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-38.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6916234940744962 entropy -2.8878138619891947
epoch: 34, step: 50
	action: tensor([[-0.2776, -0.0983,  0.2361, -0.1330,  0.0521, -0.0331,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2316063593024411, distance: 1.2699676392875696 entropy -3.326564704568379
epoch: 34, step: 51
	action: tensor([[0.9615, 0.0996, 0.2260, 0.4259, 0.1168, 0.4180, 0.0565]],
       dtype=torch.float64)
	q_value: tensor([[-36.8190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9680190668613752, distance: 0.20464552813337583 entropy -2.8129626018512432
epoch: 34, step: 52
	action: tensor([[ 0.4568, -0.1073, -0.4251, -0.1454,  0.7735, -0.1996, -0.0917]],
       dtype=torch.float64)
	q_value: tensor([[-46.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4380492041191698, distance: 0.8578391024608205 entropy -2.221741891794751
epoch: 34, step: 53
	action: tensor([[ 0.3016,  0.3231, -0.0139, -0.0370,  0.1483, -0.3307, -0.1477]],
       dtype=torch.float64)
	q_value: tensor([[-34.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8578391024608205 entropy -3.041351018904575
epoch: 34, step: 54
	action: tensor([[-0.1627, -0.0397, -0.0222, -0.1472, -0.0764,  0.1828,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02185741470734115, distance: 1.1317689550064804 entropy -3.326564704568379
epoch: 34, step: 55
	action: tensor([[ 0.4523,  0.7191, -0.1610, -0.5357,  0.1871,  0.2156,  0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-36.7792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1317689550064804 entropy -2.7829998749373237
epoch: 34, step: 56
	action: tensor([[-0.1415, -0.1265,  0.0454,  0.2031, -0.1480, -0.0094,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12409908905309042, distance: 1.0709869581400577 entropy -3.326564704568379
epoch: 34, step: 57
	action: tensor([[-0.3800, -0.5193,  0.0398, -0.6579, -0.0488,  0.1894,  0.1286]],
       dtype=torch.float64)
	q_value: tensor([[-38.0710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7799057935915854, distance: 1.526705259055772 entropy -2.753145957036311
epoch: 34, step: 58
	action: tensor([[ 0.3976, -0.5755,  0.2675, -0.6040, -0.8383,  0.0921, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-35.7094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35977328491923255, distance: 1.3344120205820131 entropy -2.6350305634258655
epoch: 34, step: 59
	action: tensor([[ 1.5606, -1.2294,  1.5026, -0.7933, -0.0314, -0.2515,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-39.4171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1549695463862446, distance: 1.229821156322905 entropy -2.1304163071448907
epoch: 34, step: 60
	action: tensor([[ 0.3026, -1.3844, -1.7751, -1.7183, -0.2091,  0.8660, -0.5119]],
       dtype=torch.float64)
	q_value: tensor([[-51.5432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.229821156322905 entropy -1.620903885360858
epoch: 34, step: 61
	action: tensor([[-0.0012,  0.1287, -0.2174, -0.1556, -0.3280, -0.2810,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-40.1021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3251754414372563, distance: 0.9400525523287404 entropy -3.326564704568379
epoch: 34, step: 62
	action: tensor([[ 0.2741,  0.0108, -0.3739,  0.1381, -0.2768,  0.0311,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[-36.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5773616401401163, distance: 0.7439455484463239 entropy -3.130767741525914
epoch: 34, step: 63
	action: tensor([[ 0.8115,  0.4415,  0.2828, -0.3972,  0.1562, -0.0467,  0.0397]],
       dtype=torch.float64)
	q_value: tensor([[-37.7538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8696075883332525, distance: 0.41322144513813636 entropy -2.8395715438167173
LOSS epoch 34 actor 562.4636197141226 critic 281.3405279512812
epoch: 35, step: 0
	action: tensor([[-0.2466,  0.0219,  0.6793,  0.8027, -0.9345,  0.3220, -0.2124]],
       dtype=torch.float64)
	q_value: tensor([[-41.3456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.41322144513813636 entropy -2.4298804000573466
epoch: 35, step: 1
	action: tensor([[-0.0740, -0.1501,  0.1229,  0.1003, -0.1863, -0.0016,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14268737894752903, distance: 1.0595618231206552 entropy -3.326182055107691
epoch: 35, step: 2
	action: tensor([[-0.3559, -0.2975,  0.1135, -0.2759, -0.1948,  0.3715,  0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-36.8452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4278904081443611, distance: 1.3674269387409195 entropy -2.6883496048242677
epoch: 35, step: 3
	action: tensor([[ 0.0137, -0.4130, -0.5425, -0.5534,  0.7049,  0.4523,  0.1279]],
       dtype=torch.float64)
	q_value: tensor([[-36.0320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01062252603679481, distance: 1.1382501136843899 entropy -2.5057247052028657
epoch: 35, step: 4
	action: tensor([[ 0.1998,  0.0126, -0.2370, -0.4181,  0.2537,  0.1079, -0.2099]],
       dtype=torch.float64)
	q_value: tensor([[-33.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3520004646591346, distance: 0.9211790247043853 entropy -2.8667909603078465
epoch: 35, step: 5
	action: tensor([[-0.2612, -0.5556,  0.1777, -0.1620,  0.0078,  0.0412, -0.1030]],
       dtype=torch.float64)
	q_value: tensor([[-33.2017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5500226424810752, distance: 1.4247075127704574 entropy -3.019793752528048
epoch: 35, step: 6
	action: tensor([[ 0.3268, -0.3785,  0.1220, -0.6715,  0.1964,  0.8511,  0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-34.6188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13740698660258932, distance: 1.062819860707435 entropy -2.63457178403306
epoch: 35, step: 7
	action: tensor([[-0.1561,  0.0036, -0.4150,  0.1339,  1.2792,  0.2137, -0.2014]],
       dtype=torch.float64)
	q_value: tensor([[-37.1582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22019526821068125, distance: 1.0105309661426807 entropy -2.189269088927282
epoch: 35, step: 8
	action: tensor([[ 0.0952,  0.2370,  0.1586,  0.6216,  0.3646,  0.4320, -0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-38.8435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0105309661426807 entropy -3.0839935925977673
epoch: 35, step: 9
	action: tensor([[ 0.1304,  0.0941,  0.0147,  0.0075, -0.1730,  0.1879,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.499272421608592, distance: 0.8097621052078846 entropy -3.326182055107691
epoch: 35, step: 10
	action: tensor([[-0.9320, -0.2751,  0.0708, -0.0507, -0.3183,  0.0086,  0.0703]],
       dtype=torch.float64)
	q_value: tensor([[-37.7258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0958231437417965, distance: 1.6566623759628811 entropy -2.648757069184221
epoch: 35, step: 11
	action: tensor([[-0.4243, -0.4809,  0.2738,  0.1306, -0.2770,  0.2867,  0.1858]],
       dtype=torch.float64)
	q_value: tensor([[-36.1572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39227700070195715, distance: 1.350266577072328 entropy -2.8558088895869482
epoch: 35, step: 12
	action: tensor([[-0.0540, -0.2607, -0.4426,  0.2400,  0.1512,  0.2681,  0.2391]],
       dtype=torch.float64)
	q_value: tensor([[-38.2968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14311375267349058, distance: 1.0592983104324332 entropy -2.369154230742872
epoch: 35, step: 13
	action: tensor([[-0.7767, -0.2848, -0.0411,  0.0434, -0.5859,  0.1378,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-36.2228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.863853829271823, distance: 1.5622934731785256 entropy -2.926614416391156
epoch: 35, step: 14
	action: tensor([[-0.1277, -0.2003,  0.1395,  0.3166, -0.1185,  0.2952,  0.2369]],
       dtype=torch.float64)
	q_value: tensor([[-36.5505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28088509336640666, distance: 0.9704112864109615 entropy -2.6827720655345115
epoch: 35, step: 15
	action: tensor([[-0.2802,  0.5206, -0.0795, -0.2780, -0.1915,  0.6733,  0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-39.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9704112864109615 entropy -2.4690541354238142
epoch: 35, step: 16
	action: tensor([[-0.4648,  0.0341,  0.2315,  0.0276,  0.2067,  0.0389,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25281755822244634, distance: 1.2808568905419753 entropy -3.326182055107691
epoch: 35, step: 17
	action: tensor([[ 0.0936,  0.1316, -0.0596,  0.4586, -0.2037, -0.1580,  0.0517]],
       dtype=torch.float64)
	q_value: tensor([[-36.8093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2808568905419753 entropy -2.9192794989997495
epoch: 35, step: 18
	action: tensor([[ 0.0546, -0.3521,  0.1304, -0.0093,  0.1576,  0.0575,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2808568905419753 entropy -3.326182055107691
epoch: 35, step: 19
	action: tensor([[ 0.1065, -0.2187,  0.1526,  0.0235, -0.0044, -0.0403,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2808568905419753 entropy -3.326182055107691
epoch: 35, step: 20
	action: tensor([[-0.0304, -0.0447,  0.0402,  0.1498,  0.0312, -0.0213,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2944720787673545, distance: 0.9612000751279624 entropy -3.326182055107691
epoch: 35, step: 21
	action: tensor([[ 0.0633,  0.1757, -0.4677,  0.3557,  0.0079,  0.4951,  0.0654]],
       dtype=torch.float64)
	q_value: tensor([[-37.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9612000751279624 entropy -2.810605396442532
epoch: 35, step: 22
	action: tensor([[ 0.0197, -0.0361, -0.1505,  0.0699, -0.1467,  0.0407,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9612000751279624 entropy -3.326182055107691
epoch: 35, step: 23
	action: tensor([[-0.1317, -0.0541, -0.0164, -0.0126, -0.1014, -0.0655,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06359905304189706, distance: 1.1073568981359208 entropy -3.326182055107691
epoch: 35, step: 24
	action: tensor([[-0.4805,  0.0262, -0.6292,  0.1293,  0.0482,  0.3270,  0.0769]],
       dtype=torch.float64)
	q_value: tensor([[-35.6492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2890450412958081, distance: 1.2992440587976293 entropy -2.897556132597309
epoch: 35, step: 25
	action: tensor([[-0.1164,  0.2760, -0.2107,  0.0562,  0.2322,  0.1196,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-34.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2992440587976293 entropy -3.43771865008516
epoch: 35, step: 26
	action: tensor([[0.1510, 0.0374, 0.1141, 0.0410, 0.0451, 0.0467, 0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47800401139762294, distance: 0.8267805971280684 entropy -3.326182055107691
epoch: 35, step: 27
	action: tensor([[-1.2645, -0.2005,  0.5267, -1.1200, -0.5052,  0.0760,  0.0287]],
       dtype=torch.float64)
	q_value: tensor([[-37.6699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5230081037476872, distance: 1.8176740873862733 entropy -2.708563108456873
epoch: 35, step: 28
	action: tensor([[ 0.5621,  1.2145,  0.9008,  0.4664, -0.3225,  0.1192,  0.1363]],
       dtype=torch.float64)
	q_value: tensor([[-39.7723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8176740873862733 entropy -2.479815546400427
epoch: 35, step: 29
	action: tensor([[ 0.1797,  0.1672,  0.1452, -0.2572,  0.6073, -0.0349,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41901233076213174, distance: 0.8722483384392097 entropy -3.326182055107691
epoch: 35, step: 30
	action: tensor([[-0.0333, -0.6287,  0.1166, -0.3527,  0.6089,  0.3071, -0.1639]],
       dtype=torch.float64)
	q_value: tensor([[-37.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3817586181410162, distance: 1.3451564062054735 entropy -2.8674470812648023
epoch: 35, step: 31
	action: tensor([[-0.1114, -0.0483,  0.7273, -0.4783, -0.0487, -0.0435, -0.1647]],
       dtype=torch.float64)
	q_value: tensor([[-35.0739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24034421715757026, distance: 1.2744646866085951 entropy -2.549685495240745
epoch: 35, step: 32
	action: tensor([[-0.2636,  0.7072, -0.2292, -0.1560,  0.1103,  0.3267,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[-38.0131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2744646866085951 entropy -2.413330165456174
epoch: 35, step: 33
	action: tensor([[-0.1387, -0.0886, -0.2151,  0.2170,  0.2511, -0.1429,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08593706598366257, distance: 1.0940690750086368 entropy -3.326182055107691
epoch: 35, step: 34
	action: tensor([[-0.2710, -0.0517, -0.3418,  0.0803,  0.2820,  0.1001,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[-35.5103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.045983398899766215, distance: 1.1703589729753492 entropy -3.210879248492769
epoch: 35, step: 35
	action: tensor([[-0.3133, -0.1988,  0.0700,  0.0679,  0.1954,  0.0473,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[-34.4694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19920623558103046, distance: 1.2531516558739564 entropy -3.343433568816477
epoch: 35, step: 36
	action: tensor([[-0.5565, -0.0593, -0.1688,  0.3893, -0.2666,  0.3657,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-35.6183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25967189508718813, distance: 1.2843559830172562 entropy -2.9163497981493265
epoch: 35, step: 37
	action: tensor([[-0.1970, -0.4745,  0.3156,  0.0528,  0.6719, -0.0709,  0.1855]],
       dtype=torch.float64)
	q_value: tensor([[-38.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3323501029706746, distance: 1.3208876303522188 entropy -2.747296379247373
epoch: 35, step: 38
	action: tensor([[-0.0348,  0.1863, -0.4123, -0.2961,  0.4038,  0.2705, -0.0923]],
       dtype=torch.float64)
	q_value: tensor([[-37.4525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37595075990220017, distance: 0.9039951961134446 entropy -2.7071651204050275
epoch: 35, step: 39
	action: tensor([[ 0.1428,  0.1781,  0.0354, -0.3512, -0.0501, -0.0595, -0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-33.7716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3636704671722105, distance: 0.9128464546949201 entropy -3.2803256738431297
epoch: 35, step: 40
	action: tensor([[-1.1189, -0.1286, -0.0095,  0.1839,  0.0124,  0.2363, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-35.4869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1039130400266193, distance: 1.659856662313418 entropy -2.8920256063800425
epoch: 35, step: 41
	action: tensor([[-0.5334,  0.0615,  0.1432, -0.1320,  0.3025, -0.1286,  0.1357]],
       dtype=torch.float64)
	q_value: tensor([[-36.9170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46300527490934207, distance: 1.3841387884030552 entropy -3.0420071001787603
epoch: 35, step: 42
	action: tensor([[ 0.4897,  0.1166,  0.3615, -0.1868,  0.1530, -0.0738,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[-35.8740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.623371057173758, distance: 0.7022852367287808 entropy -3.1626111216981725
epoch: 35, step: 43
	action: tensor([[-0.2268, -0.4829, -0.4007, -1.1072,  0.3402,  0.2283, -0.1035]],
       dtype=torch.float64)
	q_value: tensor([[-39.2082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3711674066878252, distance: 1.339991161906288 entropy -2.530103340772638
epoch: 35, step: 44
	action: tensor([[ 0.0846,  0.1008, -0.0321, -0.1910, -0.3005,  0.6062, -0.1771]],
       dtype=torch.float64)
	q_value: tensor([[-33.5255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43103221228439037, distance: 0.8631783380762018 entropy -2.9117247108657525
epoch: 35, step: 45
	action: tensor([[-0.1315,  0.1541,  0.7277,  0.3433, -0.0463,  0.9890,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-37.3455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8631783380762018 entropy -2.4576090840053664
epoch: 35, step: 46
	action: tensor([[-2.5319e-01, -1.4712e-05, -3.1871e-01,  2.2768e-01,  3.0184e-01,
          8.7863e-02,  8.5662e-02]], dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04161104723026654, distance: 1.1202826054844368 entropy -3.326182055107691
epoch: 35, step: 47
	action: tensor([[ 0.1011, -0.0593,  0.2352,  0.0692, -0.1479, -0.0110,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-35.8768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3559248465442959, distance: 0.918385389794296 entropy -3.2648664639170764
epoch: 35, step: 48
	action: tensor([[-0.3909, -0.9271, -0.7773, -0.0454,  0.1529, -0.0479,  0.0886]],
       dtype=torch.float64)
	q_value: tensor([[-37.9810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8636666829348614, distance: 1.5622150376178017 entropy -2.6011365818883996
epoch: 35, step: 49
	action: tensor([[-0.2965,  0.0439, -0.1450,  0.0462,  0.2350, -0.1208,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[-32.6420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07050242120488792, distance: 1.1839967771030468 entropy -3.2472361194851076
epoch: 35, step: 50
	action: tensor([[ 0.1073, -0.0128, -0.1259, -0.2513,  0.0524, -0.0731,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-34.8257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23764699362156616, distance: 0.999159338769935 entropy -3.3727428528276375
epoch: 35, step: 51
	action: tensor([[ 0.0358, -0.0451, -0.5816,  0.2051,  0.0951,  0.1117, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-34.3214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2878376221150156, distance: 0.9657088397256726 entropy -3.004125981808548
epoch: 35, step: 52
	action: tensor([[-0.2793, -0.0428, -0.3992,  0.3173, -0.0444,  0.0258,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-34.9398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0382103077349194, distance: 1.1660021775126514 entropy -3.2210371490279073
epoch: 35, step: 53
	action: tensor([[-0.1175,  0.3854, -0.2143,  0.7024,  0.1968,  0.0983,  0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-35.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1660021775126514 entropy -3.2095926453327683
epoch: 35, step: 54
	action: tensor([[-0.1558, -0.0488, -0.0760, -0.5593, -0.1148, -0.1780,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18689131940289427, distance: 1.246700604450938 entropy -3.326182055107691
epoch: 35, step: 55
	action: tensor([[-0.2044,  0.4360, -0.0634, -0.3395, -0.0867,  0.2656, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[-33.7336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24000924380324185, distance: 0.997610124990547 entropy -3.074665474369549
epoch: 35, step: 56
	action: tensor([[-0.0643, -0.1947,  0.0386,  0.2473,  0.1805,  0.1957,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-36.2014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24426275639026207, distance: 0.9948144981039856 entropy -2.9964782252339135
epoch: 35, step: 57
	action: tensor([[-0.5090,  0.0857,  0.3243,  0.3553, -0.0016,  0.6760,  0.0679]],
       dtype=torch.float64)
	q_value: tensor([[-37.4470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9948144981039856 entropy -2.7257809767192644
epoch: 35, step: 58
	action: tensor([[ 2.6650e-01,  1.5457e-04, -1.2392e-02, -1.8204e-02,  3.0704e-01,
         -8.6643e-02,  8.5662e-02]], dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5150902771895868, distance: 0.7968693802190313 entropy -3.326182055107691
epoch: 35, step: 59
	action: tensor([[-0.5821, -0.0272,  0.0719,  0.1154,  0.6251,  0.0521, -0.0645]],
       dtype=torch.float64)
	q_value: tensor([[-36.5326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33963388002037953, distance: 1.3244932658234168 entropy -2.873686093545057
epoch: 35, step: 60
	action: tensor([[ 0.0626, -0.5473,  0.0307,  0.0683, -0.1553,  0.0232, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-36.8293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07137797133908608, distance: 1.1844808659933754 entropy -3.208357980377
epoch: 35, step: 61
	action: tensor([[-1.2014,  0.0865,  0.3987, -0.1439, -0.1908,  0.0041,  0.1085]],
       dtype=torch.float64)
	q_value: tensor([[-35.7876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2586511332279575, distance: 1.7198131732415498 entropy -2.5636211528420225
epoch: 35, step: 62
	action: tensor([[ 0.0738,  0.3641, -0.1304,  0.3583,  0.1909,  0.2759,  0.1793]],
       dtype=torch.float64)
	q_value: tensor([[-38.6696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7198131732415498 entropy -2.7470025961394584
epoch: 35, step: 63
	action: tensor([[ 0.0358,  0.1348, -0.1452, -0.2977, -0.1065, -0.1250,  0.0857]],
       dtype=torch.float64)
	q_value: tensor([[-37.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29937133856186915, distance: 0.9578569242123437 entropy -3.326182055107691
LOSS epoch 35 actor 518.0603536729656 critic 119.78934143108467
epoch: 36, step: 0
	action: tensor([[ 0.5152, -0.1106, -0.1861,  0.4267,  0.0843, -0.0601, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-33.6745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7578960454730876, distance: 0.5630638362269106 entropy -3.0526127224827913
epoch: 36, step: 1
	action: tensor([[ 0.5405, -0.3622, -0.5764, -0.0313,  0.0874,  0.4107,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-38.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4519123622852287, distance: 0.8471917082585326 entropy -2.717318252478598
epoch: 36, step: 2
	action: tensor([[ 0.7982, -0.5453, -0.2047, -0.0045, -0.0415,  0.0642, -0.0959]],
       dtype=torch.float64)
	q_value: tensor([[-35.3053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23639286341697252, distance: 0.9999808486000367 entropy -2.6241512439463373
epoch: 36, step: 3
	action: tensor([[-1.2530, -0.3137, -0.0585, -0.1231, -0.5252,  0.4288, -0.0988]],
       dtype=torch.float64)
	q_value: tensor([[-36.7209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5899536173054392, distance: 1.8416312957173522 entropy -2.4081777104137876
epoch: 36, step: 4
	action: tensor([[-0.8791,  0.0235, -0.0740,  0.1592, -0.0140,  0.3643,  0.2277]],
       dtype=torch.float64)
	q_value: tensor([[-36.2614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7013214888479797, distance: 1.4926221668215987 entropy -2.696875133679559
epoch: 36, step: 5
	action: tensor([[ 0.3342,  0.0073,  0.0683, -0.2391,  0.6975, -0.0100,  0.1199]],
       dtype=torch.float64)
	q_value: tensor([[-36.1299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41302876641350794, distance: 0.8767284549184879 entropy -2.9398129626009046
epoch: 36, step: 6
	action: tensor([[-0.2646, -0.1755, -0.1412, -0.3839, -0.2400, -0.4855, -0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-36.1593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32889047265844407, distance: 1.3191715824268155 entropy -2.768788801404098
epoch: 36, step: 7
	action: tensor([[-0.3642,  0.0458,  0.1457,  0.1090, -0.1392,  0.1017,  0.0465]],
       dtype=torch.float64)
	q_value: tensor([[-30.7060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0890085282076829, distance: 1.19418698503689 entropy -3.385348848578075
epoch: 36, step: 8
	action: tensor([[ 0.6866, -0.4101,  0.0843, -0.7226, -0.0959,  0.3948,  0.1400]],
       dtype=torch.float64)
	q_value: tensor([[-36.1885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011595408058446166, distance: 1.1509597013612085 entropy -2.7661123713173934
epoch: 36, step: 9
	action: tensor([[ 0.5018, -0.3392,  1.6510, -1.5307, -0.1537,  0.7658, -0.1955]],
       dtype=torch.float64)
	q_value: tensor([[-36.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14729297715665723, distance: 1.0567119364695308 entropy -2.1988302215519657
epoch: 36, step: 10
	action: tensor([[-0.2357,  0.7992,  0.0732, -3.1780, -0.3042,  0.1295, -0.3176]],
       dtype=torch.float64)
	q_value: tensor([[-47.3340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0567119364695308 entropy -1.5746272553465517
epoch: 36, step: 11
	action: tensor([[ 0.4574, -0.0887, -0.1247,  0.2172,  0.1754, -0.0820,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.656164495293622, distance: 0.6710147293127657 entropy -3.3257732627316963
epoch: 36, step: 12
	action: tensor([[-0.7274, -0.5264, -0.1498,  0.9400,  1.0206,  0.3832, -0.0307]],
       dtype=torch.float64)
	q_value: tensor([[-37.3183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25243930601013664, distance: 1.280663517004039 entropy -2.764924173896886
epoch: 36, step: 13
	action: tensor([[-0.3823, -0.1786, -0.0576,  0.3075, -0.1687,  0.0437,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-42.0267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09344860135032862, distance: 1.196618960471425 entropy -2.893768756217651
epoch: 36, step: 14
	action: tensor([[-0.1956,  0.2208,  0.1340, -0.2862, -0.3730,  0.2895,  0.1639]],
       dtype=torch.float64)
	q_value: tensor([[-35.5192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1635750572276503, distance: 1.0465745772624628 entropy -2.8392632446174244
epoch: 36, step: 15
	action: tensor([[-0.0605, -0.6839,  0.0944, -0.4801, -0.0271,  0.7887,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-36.3966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3356078649052918, distance: 1.3225015118000052 entropy -2.5563394186414894
epoch: 36, step: 16
	action: tensor([[ 0.2454, -1.0820,  0.0413, -0.0512,  0.3899,  0.3325, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[-35.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4560729423204384, distance: 1.380855579430244 entropy -2.1965424315000464
epoch: 36, step: 17
	action: tensor([[ 1.1110, -1.0981, -0.8494, -0.4677, -1.4837, -0.0674, -0.0929]],
       dtype=torch.float64)
	q_value: tensor([[-36.3393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7478965665849522, distance: 1.5129151015013411 entropy -2.2937744121533474
epoch: 36, step: 18
	action: tensor([[-0.6138,  0.8691,  0.3849, -0.9434, -0.3834,  0.0138, -0.1580]],
       dtype=torch.float64)
	q_value: tensor([[-42.1986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5129151015013411 entropy -2.110399214784498
epoch: 36, step: 19
	action: tensor([[-0.0892,  0.0514, -0.4468,  0.0984, -0.2159,  0.0568,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22390912006081065, distance: 1.0081217467404824 entropy -3.3257732627316963
epoch: 36, step: 20
	action: tensor([[ 0.2273,  0.2601,  0.5021,  0.0812, -0.0950,  0.1946,  0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-34.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0081217467404824 entropy -3.1199533357441074
epoch: 36, step: 21
	action: tensor([[ 0.2407, -0.1684,  0.0807,  0.5294,  0.0010, -0.1277,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6101382899292878, distance: 0.7145160439057436 entropy -3.3257732627316963
epoch: 36, step: 22
	action: tensor([[-0.2011,  0.3212, -0.3178,  0.2311, -0.0179, -0.0705,  0.0866]],
       dtype=torch.float64)
	q_value: tensor([[-39.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7145160439057436 entropy -2.639986124308018
epoch: 36, step: 23
	action: tensor([[-0.2215,  0.0490,  0.1456, -0.4066,  0.1921, -0.0990,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19699748175144305, distance: 1.2519970674782122 entropy -3.3257732627316963
epoch: 36, step: 24
	action: tensor([[ 0.1899, -0.4767,  0.1820, -0.3262,  0.0038,  0.5774, -0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-33.9047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013910440392540413, distance: 1.1363572146532472 entropy -3.0015002498372354
epoch: 36, step: 25
	action: tensor([[-0.2455, -0.4575, -0.4314,  0.4544,  1.3074,  0.2515, -0.0187]],
       dtype=torch.float64)
	q_value: tensor([[-35.9797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1566858936495954, distance: 1.2307346074930994 entropy -2.270166794222674
epoch: 36, step: 26
	action: tensor([[ 0.1873,  0.0302,  0.2019,  0.1194,  0.2768, -0.4820, -0.1238]],
       dtype=torch.float64)
	q_value: tensor([[-38.9975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39139193408597983, distance: 0.8927411326583645 entropy -2.9962839659896865
epoch: 36, step: 27
	action: tensor([[ 0.7372, -0.3438, -0.1741,  0.3538, -0.0595,  0.3346, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-36.8225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7319899397038898, distance: 0.5924234507533789 entropy -2.926140568124191
epoch: 36, step: 28
	action: tensor([[ 0.7013, -0.0925, -0.3529,  0.5444,  0.6782, -0.1822, -0.0231]],
       dtype=torch.float64)
	q_value: tensor([[-40.0945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8205658091342204, distance: 0.4847404856083525 entropy -2.358385607555514
epoch: 36, step: 29
	action: tensor([[-0.4525,  0.2685,  0.0693, -0.5150, -0.0190,  0.0523, -0.1032]],
       dtype=torch.float64)
	q_value: tensor([[-39.8055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.316371257328929, distance: 1.3129430504808284 entropy -2.8316499567955624
epoch: 36, step: 30
	action: tensor([[-0.2214, -0.0270,  0.3759, -0.2409,  0.1227,  0.0817,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[-33.6565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15249619356056954, distance: 1.2285036272711922 entropy -3.153348802979642
epoch: 36, step: 31
	action: tensor([[-0.1009, -0.3735,  0.6284,  0.2926,  0.7820, -0.0035,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-35.6422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09057134533232658, distance: 1.0912920974325127 entropy -2.6626006811810052
epoch: 36, step: 32
	action: tensor([[ 0.4624, -0.1470,  0.0244,  0.3291, -0.4229,  0.6873, -0.0828]],
       dtype=torch.float64)
	q_value: tensor([[-40.6165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7795274385943657, distance: 0.5373211890108822 entropy -2.4994168928179286
epoch: 36, step: 33
	action: tensor([[ 0.6235,  0.9748,  0.3911, -1.1488, -0.6631,  0.2847,  0.1298]],
       dtype=torch.float64)
	q_value: tensor([[-41.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5373211890108822 entropy -2.1679640147405754
epoch: 36, step: 34
	action: tensor([[-0.1233, -0.1413,  0.1118, -0.0663,  0.2931,  0.0849,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01802562484822079, distance: 1.1339835922203616 entropy -3.3257732627316963
epoch: 36, step: 35
	action: tensor([[ 0.2142, -0.5162,  0.1220,  0.6429, -0.9951,  0.2232, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[-34.9176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46150127012807607, distance: 0.8397481097195265 entropy -2.8333789423341473
epoch: 36, step: 36
	action: tensor([[ 0.0302, -0.3555,  0.3303, -1.0984,  0.8854,  0.1764,  0.2727]],
       dtype=torch.float64)
	q_value: tensor([[-42.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5564742388289685, distance: 1.4276694350329568 entropy -2.0962872094985676
epoch: 36, step: 37
	action: tensor([[ 0.5222,  0.3733,  0.4791,  0.5039, -0.2267, -0.0529, -0.4108]],
       dtype=torch.float64)
	q_value: tensor([[-36.8848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4276694350329568 entropy -2.3825871918601353
epoch: 36, step: 38
	action: tensor([[-0.1535, -0.0920, -0.1768, -0.1658,  0.2116, -0.2230,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08601671921754317, distance: 1.1925454751239615 entropy -3.3257732627316963
epoch: 36, step: 39
	action: tensor([[-0.1521,  0.0401, -0.1583, -0.0974,  0.0472,  0.0517, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[-32.5434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10104061411328824, distance: 1.084992481807158 entropy -3.296119655659752
epoch: 36, step: 40
	action: tensor([[-0.3425,  0.0592,  0.2962, -0.0646,  0.0884,  0.3368,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[-33.5079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01276181227894746, distance: 1.151623058214621 entropy -3.1089249800906393
epoch: 36, step: 41
	action: tensor([[-0.1943,  0.7455,  0.2263, -0.5858, -0.1675,  0.6369,  0.0803]],
       dtype=torch.float64)
	q_value: tensor([[-36.6614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.151623058214621 entropy -2.6301019036711213
epoch: 36, step: 42
	action: tensor([[-0.2596, -0.1366, -0.0708,  0.4015,  0.2229, -0.1071,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01179848544842843, distance: 1.1375734589460504 entropy -3.3257732627316963
epoch: 36, step: 43
	action: tensor([[-0.5981,  0.0907,  0.3024,  0.2663,  0.5453, -0.0105,  0.0715]],
       dtype=torch.float64)
	q_value: tensor([[-36.2630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23540316603691647, distance: 1.271923666543259 entropy -3.036320079665838
epoch: 36, step: 44
	action: tensor([[-1.0816, -0.8144,  0.1671,  0.1346,  0.1903,  0.3548,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[-38.0585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.347030872230186, distance: 1.7531379637700917 entropy -2.9687657979377917
epoch: 36, step: 45
	action: tensor([[ 0.1844,  0.4360, -0.2571, -0.2880, -0.0636,  0.3487,  0.1675]],
       dtype=torch.float64)
	q_value: tensor([[-36.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7531379637700917 entropy -2.6104384931790383
epoch: 36, step: 46
	action: tensor([[ 0.0511, -0.0791,  0.2144,  0.8700,  0.0943,  0.0240,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6987256580234866, distance: 0.6281129775846992 entropy -3.3257732627316963
epoch: 36, step: 47
	action: tensor([[-0.9840,  0.0291, -0.3880,  0.1515, -0.2696,  0.7528,  0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-42.6814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8368321941481158, distance: 1.5509272783701136 entropy -2.5550558781682517
epoch: 36, step: 48
	action: tensor([[ 0.1271, -0.0705,  0.0089, -0.0806,  0.0958, -0.2156,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27090633502448824, distance: 0.9771210191446084 entropy -3.3257732627316963
epoch: 36, step: 49
	action: tensor([[ 0.3154, -0.1946,  0.2063,  0.1271, -0.0165, -0.0418, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[-34.7930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.474079737513884, distance: 0.8298825734552553 entropy -2.903764934622555
epoch: 36, step: 50
	action: tensor([[-0.5252,  0.1901,  0.6523, -0.1290, -0.2902,  0.1293,  0.0392]],
       dtype=torch.float64)
	q_value: tensor([[-37.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26492084594361476, distance: 1.2870291049681055 entropy -2.55646891575809
epoch: 36, step: 51
	action: tensor([[-1.7177,  0.4776, -0.2541,  0.1847,  0.3468,  0.9372,  0.2011]],
       dtype=torch.float64)
	q_value: tensor([[-38.7626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2870291049681055 entropy -2.403481150977661
epoch: 36, step: 52
	action: tensor([[-0.4144,  0.0553,  0.1459,  0.2439,  0.3440,  0.0280,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04812041126225397, distance: 1.1715539228362173 entropy -3.3257732627316963
epoch: 36, step: 53
	action: tensor([[ 0.3600,  0.0835, -0.0761,  0.0908, -1.0610, -0.1162,  0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-36.7497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6799830786674705, distance: 0.6473559807550223 entropy -2.9961199349325978
epoch: 36, step: 54
	action: tensor([[-0.3761, -0.5717, -0.0367,  0.7131, -0.6392,  0.3460,  0.1149]],
       dtype=torch.float64)
	q_value: tensor([[-39.5077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19730601970050787, distance: 1.2521584144000324 entropy -2.407898495993385
epoch: 36, step: 55
	action: tensor([[-1.8730,  0.1547, -0.4684,  0.1881, -0.0193,  0.3805,  0.3215]],
       dtype=torch.float64)
	q_value: tensor([[-40.9638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2521584144000324 entropy -2.2897523654464913
epoch: 36, step: 56
	action: tensor([[-0.5355, -0.2166, -0.0973, -0.4037, -0.2574, -0.2019,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6732934090678604, distance: 1.4802761553646253 entropy -3.3257732627316963
epoch: 36, step: 57
	action: tensor([[ 0.3472, -0.4523, -0.0746, -0.2572,  0.4662, -0.0822,  0.0782]],
       dtype=torch.float64)
	q_value: tensor([[-32.2108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03737205307263536, distance: 1.1655313653253938 entropy -3.164755067450436
epoch: 36, step: 58
	action: tensor([[-0.2070, -0.2123,  0.1923, -0.2779,  0.1292, -0.4733, -0.1532]],
       dtype=torch.float64)
	q_value: tensor([[-33.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4180388173056764, distance: 1.362701559550883 entropy -2.729968159114417
epoch: 36, step: 59
	action: tensor([[-0.4667, -0.0121,  0.0813, -0.4719, -0.1208,  0.0901,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[-33.0053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.521838634574042, distance: 1.4116953880386485 entropy -3.086249418140814
epoch: 36, step: 60
	action: tensor([[ 0.2921, -0.1708,  0.1023, -0.7719, -0.7977, -0.1913,  0.0601]],
       dtype=torch.float64)
	q_value: tensor([[-33.3772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06730264836471855, distance: 1.1822259469290457 entropy -2.9223129563055843
epoch: 36, step: 61
	action: tensor([[ 6.8634e-02,  3.5532e-01, -2.2797e-01, -2.7561e-01,  1.0080e-01,
          5.2444e-01, -1.1704e-05]], dtype=torch.float64)
	q_value: tensor([[-36.0644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1822259469290457 entropy -2.450429749924106
epoch: 36, step: 62
	action: tensor([[ 0.1928, -0.1053, -0.0643,  0.5049,  0.1169, -0.0073,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-34.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6458726047593172, distance: 0.6809832907382587 entropy -3.3257732627316963
epoch: 36, step: 63
	action: tensor([[-0.0824,  0.2611,  0.2383, -0.1254, -0.1715,  0.0868,  0.0615]],
       dtype=torch.float64)
	q_value: tensor([[-38.9034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34488233629424525, distance: 0.9262246778503003 entropy -2.7177521769547552
LOSS epoch 36 actor 534.628703674481 critic 213.0697360639624
epoch: 37, step: 0
	action: tensor([[-0.3230, -0.4781, -0.4242, -0.2009,  0.5995,  0.3872,  0.0780]],
       dtype=torch.float64)
	q_value: tensor([[-35.9817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.337578340483482, distance: 1.3234767218672265 entropy -2.6759824649980866
epoch: 37, step: 1
	action: tensor([[-0.2522, -0.3723, -0.4690, -0.2064,  0.2070,  0.3546, -0.1004]],
       dtype=torch.float64)
	q_value: tensor([[-31.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23490148543124634, distance: 1.2716653847746036 entropy -3.0472457762551044
epoch: 37, step: 2
	action: tensor([[-0.4124,  0.1784, -0.2147, -0.1829, -0.1270, -0.1369, -0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-30.5822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1049197067503358, distance: 1.2028793047385398 entropy -3.1116876063061167
epoch: 37, step: 3
	action: tensor([[-0.2180,  0.2512,  0.3315, -0.0601,  0.0154, -0.0078,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-31.1206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2028793047385398 entropy -3.479871051361687
epoch: 37, step: 4
	action: tensor([[ 0.1917, -0.1023,  0.1099, -0.1727,  0.2400, -0.0456,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28031043845569625, distance: 0.9707989436849709 entropy -3.3257438184648587
epoch: 37, step: 5
	action: tensor([[-0.4004,  0.2733,  0.0543, -0.0625, -0.8230,  0.2728, -0.0674]],
       dtype=torch.float64)
	q_value: tensor([[-34.1727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03379343185488315, distance: 1.1635192624116084 entropy -2.7520100726737304
epoch: 37, step: 6
	action: tensor([[ 0.7422, -0.2626,  0.3242,  0.6441, -0.3615, -0.2095,  0.2309]],
       dtype=torch.float64)
	q_value: tensor([[-36.2761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8123734111086558, distance: 0.4956828404829259 entropy -2.4993331357473187
epoch: 37, step: 7
	action: tensor([[ 0.5539, -0.6617, -0.2967, -0.3424,  0.5712,  1.3092,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[-43.6944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4956828404829259 entropy -2.282536364274527
epoch: 37, step: 8
	action: tensor([[-0.2075, -0.1591,  0.1214,  0.1879,  0.2719,  0.0750,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03985431791093608, distance: 1.121308875803201 entropy -3.3257438184648587
epoch: 37, step: 9
	action: tensor([[-0.1596,  0.4466, -0.4464,  0.2661,  0.1618,  0.0149,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-34.9448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.121308875803201 entropy -2.8333857943135095
epoch: 37, step: 10
	action: tensor([[ 0.0835,  0.0151,  0.0291, -0.2942,  0.2476, -0.2000,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15515796992433117, distance: 1.0518273250319943 entropy -3.3257438184648587
epoch: 37, step: 11
	action: tensor([[ 0.3671,  0.0485, -0.3198,  0.3467, -0.4090,  0.2585, -0.0747]],
       dtype=torch.float64)
	q_value: tensor([[-32.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0518273250319943 entropy -2.988114690505138
epoch: 37, step: 12
	action: tensor([[ 0.4666,  0.2021,  0.1661,  0.1294, -0.1904, -0.1518,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7901985138174089, distance: 0.5241564971325748 entropy -3.3257438184648587
epoch: 37, step: 13
	action: tensor([[-0.4315, -0.2380, -0.5296,  0.6218, -0.6598,  0.2077,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[-38.6042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31259047096528914, distance: 1.3110562246475692 entropy -2.5984715679882586
epoch: 37, step: 14
	action: tensor([[-0.0012, -0.0878,  0.2243,  0.0820,  0.2396,  0.0891,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29418195979929407, distance: 0.9613976815612183 entropy -3.3257438184648587
epoch: 37, step: 15
	action: tensor([[-0.3390, -0.6763, -0.7158, -0.1718, -0.6086, -0.0962,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-35.5329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5675774477073563, distance: 1.4327525716938387 entropy -2.7087586192918094
epoch: 37, step: 16
	action: tensor([[-0.0959,  0.0259,  0.1554,  0.5530, -0.1566,  0.1183,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-30.8945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4327525716938387 entropy -3.064911496889468
epoch: 37, step: 17
	action: tensor([[-0.0339, -0.1846, -0.1293, -0.1722,  0.0502, -0.0858,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008233170769170406, distance: 1.1396237268609444 entropy -3.3257438184648587
epoch: 37, step: 18
	action: tensor([[ 0.2380, -0.2090, -0.1941,  0.0459, -0.5153,  0.2517,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-31.8652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3750175234052445, distance: 0.9046708848932479 entropy -2.983450705393922
epoch: 37, step: 19
	action: tensor([[-0.9911, -0.0823, -0.0505, -0.6133,  0.1458, -0.1924,  0.1143]],
       dtype=torch.float64)
	q_value: tensor([[-35.3868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1481487210695716, distance: 1.6772154945944426 entropy -2.477224903482902
epoch: 37, step: 20
	action: tensor([[ 0.1263, -0.0673, -0.0589,  0.1345, -0.1109,  0.0413,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[-32.0175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4282078922432554, distance: 0.8653180672773704 entropy -3.5061486718875225
epoch: 37, step: 21
	action: tensor([[-0.3245,  0.0225, -0.1451, -0.3882,  0.1919,  0.2516,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-35.1384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13930413472931802, distance: 1.2214523517443396 entropy -2.742848626795774
epoch: 37, step: 22
	action: tensor([[ 0.0669,  0.1133, -0.5788, -0.4209, -0.1260,  0.1152, -0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-31.5317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4011238702051285, distance: 0.8855746881250675 entropy -3.081160159192
epoch: 37, step: 23
	action: tensor([[-0.0768,  0.2292,  0.0862, -0.3024, -0.2064, -0.0483, -0.0276]],
       dtype=torch.float64)
	q_value: tensor([[-29.9519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21621945793022024, distance: 1.0131037713912354 entropy -3.2809888598998085
epoch: 37, step: 24
	action: tensor([[ 0.1842,  0.1908,  0.3630,  0.1893,  0.3681, -0.0615,  0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-33.5924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0131037713912354 entropy -2.8920614670054823
epoch: 37, step: 25
	action: tensor([[-0.1642,  0.2007, -0.2098,  0.2054,  0.3565,  0.1283,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29882805915228283, distance: 0.958228221543824 entropy -3.3257438184648587
epoch: 37, step: 26
	action: tensor([[-0.3516,  0.1879,  0.2669, -0.0116, -0.1605,  0.1254, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[-34.9448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003043334393036723, distance: 1.1426016160193437 entropy -3.1738685798815047
epoch: 37, step: 27
	action: tensor([[ 0.2150, -0.0686, -0.0329, -0.0922, -0.0369, -0.0260,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3713426214660228, distance: 0.9073267225324473 entropy -3.3257438184648587
epoch: 37, step: 28
	action: tensor([[ 0.5777, -0.1482,  0.5444, -0.8358, -0.3714,  0.1339,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-34.0190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11465616108994503, distance: 1.0767445394093866 entropy -2.763892962791367
epoch: 37, step: 29
	action: tensor([[ 0.8177,  0.0374, -0.0841, -0.4400,  0.7608,  0.8043, -0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-37.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7375467958447086, distance: 0.5862496991815248 entropy -2.134601742929896
epoch: 37, step: 30
	action: tensor([[ 0.1573, -0.1063,  0.8627, -0.3146, -0.9568, -0.1435, -0.3495]],
       dtype=torch.float64)
	q_value: tensor([[-37.5278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1069725464271285, distance: 1.0814068056014754 entropy -2.3116025958788
epoch: 37, step: 31
	action: tensor([[ 0.0440,  0.5602,  0.9082, -0.4121,  0.6970,  0.8340,  0.1857]],
       dtype=torch.float64)
	q_value: tensor([[-40.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0814068056014754 entropy -2.073461881889709
epoch: 37, step: 32
	action: tensor([[ 0.2972, -0.1453,  0.0636, -0.1890, -0.1033, -0.0232,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3147380909553954, distance: 0.9472944419286712 entropy -3.3257438184648587
epoch: 37, step: 33
	action: tensor([[-0.0216, -0.1059,  1.1453, -0.1180,  0.9251,  0.1134, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-34.3829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12116638218689513, distance: 1.072778408555418 entropy -2.612408786727337
epoch: 37, step: 34
	action: tensor([[ 0.3447,  0.4834, -0.8675, -0.0742, -1.0413,  0.7681, -0.2603]],
       dtype=torch.float64)
	q_value: tensor([[-42.8674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.072778408555418 entropy -2.206893722473646
epoch: 37, step: 35
	action: tensor([[ 0.0996, -0.0837,  0.2227,  0.3150,  0.0230,  0.0398,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.528425984403545, distance: 0.7858354684805526 entropy -3.3257438184648587
epoch: 37, step: 36
	action: tensor([[-0.0536, -0.1488,  0.1214,  0.3936, -0.5819,  0.3597,  0.0896]],
       dtype=torch.float64)
	q_value: tensor([[-37.8235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3898726094103737, distance: 0.8938547542636442 entropy -2.585523348647538
epoch: 37, step: 37
	action: tensor([[-0.9582, -0.8423, -0.3219, -1.2800,  0.0722,  0.8386,  0.2590]],
       dtype=torch.float64)
	q_value: tensor([[-39.1572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.789188844118861, distance: 1.5306813265234427 entropy -2.2959311613257762
epoch: 37, step: 38
	action: tensor([[ 0.6622,  0.2885,  0.3093,  0.1056,  0.0224,  0.6823, -0.1442]],
       dtype=torch.float64)
	q_value: tensor([[-34.9544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9918441163200951, distance: 0.10334565088939292 entropy -2.526293269701977
epoch: 37, step: 39
	action: tensor([[ 0.8645, -0.7837,  0.2377, -0.2053, -0.3958,  0.8221, -0.0496]],
       dtype=torch.float64)
	q_value: tensor([[-42.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14102850989053217, distance: 1.060586434704395 entropy -2.2056395252937415
epoch: 37, step: 40
	action: tensor([[ 2.1400, -1.6244,  0.0977,  0.3208, -0.5139,  0.6521, -0.0595]],
       dtype=torch.float64)
	q_value: tensor([[-40.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.060586434704395 entropy -1.8736193924989588
epoch: 37, step: 41
	action: tensor([[-0.1890,  0.3174,  0.0794,  0.0713,  0.0097,  0.1656,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32079976726964743, distance: 0.9430953559843851 entropy -3.3257438184648587
epoch: 37, step: 42
	action: tensor([[ 0.1153,  0.2482,  0.2747, -0.2679, -0.1226,  0.1940,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5098806384428212, distance: 0.8011385366535106 entropy -3.3257438184648587
epoch: 37, step: 43
	action: tensor([[-0.3743, -0.0399, -0.2574, -0.0983,  0.0911,  0.6711,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-36.3052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.045444048175492524, distance: 1.1700571922043872 entropy -2.539536591403472
epoch: 37, step: 44
	action: tensor([[ 0.3145, -0.1834, -0.1669,  0.1783, -0.4267, -0.0897,  0.0313]],
       dtype=torch.float64)
	q_value: tensor([[-33.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.491038067337504, distance: 0.8163931342759984 entropy -2.836653345416026
epoch: 37, step: 45
	action: tensor([[-0.3200,  0.0386, -0.0568, -0.6315, -0.0381,  0.0581,  0.0797]],
       dtype=torch.float64)
	q_value: tensor([[-35.6235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2676944349986645, distance: 1.2884393652149155 entropy -2.6401337048615336
epoch: 37, step: 46
	action: tensor([[-0.1303, -0.4697, -0.0143,  0.3359,  0.5904, -0.0267, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-31.4957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05088248382006366, distance: 1.1730965833154738 entropy -3.0483356834814015
epoch: 37, step: 47
	action: tensor([[ 0.1275,  0.0047, -0.1794,  0.0699,  0.2059,  0.1165, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[-35.0626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47079486246451674, distance: 0.8324702443263696 entropy -2.9072188905009844
epoch: 37, step: 48
	action: tensor([[ 0.1173,  0.1121, -0.1137,  0.0850,  0.4630,  0.1929, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-34.0785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5661709696547286, distance: 0.7537303417499938 entropy -2.943627330482992
epoch: 37, step: 49
	action: tensor([[-0.1895, -0.0756,  0.0380,  0.3065,  0.4514, -0.1241, -0.0690]],
       dtype=torch.float64)
	q_value: tensor([[-34.9926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.141500636506679, distance: 1.060294923335909 entropy -2.9656372440690313
epoch: 37, step: 50
	action: tensor([[ 0.3667, -0.1488,  0.3209, -0.1051,  0.6245,  0.2691,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-35.1591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4959616949191319, distance: 0.8124347003495307 entropy -3.086129294408381
epoch: 37, step: 51
	action: tensor([[-0.3743, -0.3703,  0.3046,  0.1684,  0.0054,  0.4531, -0.1881]],
       dtype=torch.float64)
	q_value: tensor([[-36.9547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12196917164278487, distance: 1.2121242928590557 entropy -2.4802266706340697
epoch: 37, step: 52
	action: tensor([[ 0.4687, -0.2936, -0.2824, -0.1824, -0.4310,  0.0979,  0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-36.1682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31951546761104177, distance: 0.9439865843390978 entropy -2.436863895649285
epoch: 37, step: 53
	action: tensor([[ 0.0155, -0.3038, -0.1024,  0.4817, -0.3833,  0.7204, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[-34.7829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39277669816946126, distance: 0.8917249286941998 entropy -2.5020669172330168
epoch: 37, step: 54
	action: tensor([[ 1.1246,  0.1721,  0.0195, -0.2022, -1.2484,  0.9785,  0.2179]],
       dtype=torch.float64)
	q_value: tensor([[-39.0846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8760495048891562, distance: 0.402884747610313 entropy -2.263392721790544
epoch: 37, step: 55
	action: tensor([[ 0.1184, -0.2031, -1.0652, -0.6444, -0.7339,  0.0569,  0.0343]],
       dtype=torch.float64)
	q_value: tensor([[-47.3437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3297895120708141, distance: 0.9368332647600661 entropy -1.7706984960908088
epoch: 37, step: 56
	action: tensor([[-0.0596, -0.4125,  0.2448,  0.1563, -0.2206, -0.0210, -0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-30.7357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008495056638266218, distance: 1.1491946093992298 entropy -3.1058849472998915
epoch: 37, step: 57
	action: tensor([[-0.0398,  0.2756, -0.1406,  0.4288, -0.7862,  0.5244,  0.1652]],
       dtype=torch.float64)
	q_value: tensor([[-35.4582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1491946093992298 entropy -2.507762152945357
epoch: 37, step: 58
	action: tensor([[-0.0042, -0.0052,  0.2043,  0.1650,  0.3348, -0.0408,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3529998145251716, distance: 0.9204684259384127 entropy -3.3257438184648587
epoch: 37, step: 59
	action: tensor([[ 0.2091, -0.3247,  0.4300, -0.1633, -0.3972,  0.1864, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[-35.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14320742699453726, distance: 1.0592404079031363 entropy -2.8308936250434242
epoch: 37, step: 60
	action: tensor([[ 0.2412,  0.1820, -0.5759,  0.8753,  0.7863,  0.9360,  0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-36.7523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0592404079031363 entropy -2.2411624103100785
epoch: 37, step: 61
	action: tensor([[ 0.0224,  0.0103, -0.0714,  0.0721, -0.3209, -0.2304,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-31.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3341630040376231, distance: 0.9337715857024624 entropy -3.3257438184648587
epoch: 37, step: 62
	action: tensor([[ 0.1395, -0.1244, -0.0939,  0.1895, -0.2438,  0.4684,  0.0878]],
       dtype=torch.float64)
	q_value: tensor([[-34.4753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4917579632679563, distance: 0.8158155605643579 entropy -2.8709494732230736
epoch: 37, step: 63
	action: tensor([[ 0.8777, -1.1597, -0.1274, -0.1190,  0.0013,  0.2245,  0.1189]],
       dtype=torch.float64)
	q_value: tensor([[-36.9998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5553576616964655, distance: 1.4271572553259204 entropy -2.440771832538335
LOSS epoch 37 actor 491.62861459945753 critic 386.49875039125055
epoch: 38, step: 0
	action: tensor([[ 0.3234, -0.7887, -0.0131, -1.2683,  1.0461,  0.5954, -0.1833]],
       dtype=torch.float64)
	q_value: tensor([[-36.7024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.541491396280199, distance: 1.420781343924049 entropy -2.140861241511502
epoch: 38, step: 1
	action: tensor([[-0.6810, -1.0845,  0.9861,  0.0324,  0.4431,  0.7242, -0.5082]],
       dtype=torch.float64)
	q_value: tensor([[-34.9571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7269686819438481, distance: 1.5038306227506708 entropy -2.210921611049412
epoch: 38, step: 2
	action: tensor([[-1.1269, -0.9726,  0.2219,  0.5063, -0.8901,  0.6705,  0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-40.8703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2696390302517044, distance: 1.7239913756189005 entropy -1.9341944636789428
epoch: 38, step: 3
	action: tensor([[-1.3720,  0.6792, -1.4172, -0.7447, -0.6969,  0.8230,  0.5063]],
       dtype=torch.float64)
	q_value: tensor([[-42.5232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6311900038923464, distance: 1.4615341549877614 entropy -1.999715820086544
epoch: 38, step: 4
	action: tensor([[ 0.1913, -0.2102, -0.2672,  0.2099,  0.2417, -0.0878,  0.0652]],
       dtype=torch.float64)
	q_value: tensor([[-36.6827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3812908895146844, distance: 0.9001190409745129 entropy -3.361686502272439
epoch: 38, step: 5
	action: tensor([[-0.5686,  0.1653,  0.0884,  0.1027,  0.3942,  0.1815, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-33.3263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10457511795255736, distance: 1.2026917204945469 entropy -2.9895603148954657
epoch: 38, step: 6
	action: tensor([[-0.3235,  0.1074, -0.2376, -0.2602,  0.0794, -0.0371,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-34.6061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03092850985008644, distance: 1.1619059301568317 entropy -3.1013640996794503
epoch: 38, step: 7
	action: tensor([[-0.3466, -0.0480,  0.1262,  0.1502, -0.0720, -0.1647,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-30.6614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0548572143039463, distance: 1.1753129786746317 entropy -3.4386894534437715
epoch: 38, step: 8
	action: tensor([[ 0.3910, -0.3884, -0.0603, -0.0049, -0.1565,  0.0995,  0.1206]],
       dtype=torch.float64)
	q_value: tensor([[-33.9565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3092045556839441, distance: 0.9511114843635591 entropy -2.92848432278967
epoch: 38, step: 9
	action: tensor([[-0.4151, -1.0846, -0.5424, -0.7983, -0.5064, -0.0603,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[-34.7114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6438848648027233, distance: 1.4672103831202536 entropy -2.490060045512693
epoch: 38, step: 10
	action: tensor([[ 0.1266, -0.0979,  0.0185, -0.0231,  0.4448,  0.2515,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-31.2077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4429996941823341, distance: 0.8540521888571517 entropy -2.7813758128204924
epoch: 38, step: 11
	action: tensor([[ 0.0578, -0.1643, -0.1678,  0.0264,  0.0904,  0.2950, -0.0800]],
       dtype=torch.float64)
	q_value: tensor([[-34.0775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33929835261755936, distance: 0.930163702379366 entropy -2.7605933381262773
epoch: 38, step: 12
	action: tensor([[-0.6569, -0.4451, -0.0202,  0.0638, -0.3065, -0.1930,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[-33.1097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7682707164582732, distance: 1.5217071130599833 entropy -2.782693603125184
epoch: 38, step: 13
	action: tensor([[-0.1311, -0.0822,  0.2494, -0.0305, -0.5839,  0.2533,  0.1742]],
       dtype=torch.float64)
	q_value: tensor([[-32.4772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1460030311512044, distance: 1.0575109132465195 entropy -2.9017883776292215
epoch: 38, step: 14
	action: tensor([[ 0.4533, -1.7495,  0.6638, -0.7250, -0.2745,  0.4505,  0.2249]],
       dtype=torch.float64)
	q_value: tensor([[-36.7159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0575109132465195 entropy -2.3269039122291226
epoch: 38, step: 15
	action: tensor([[ 0.0439, -0.1699, -0.0879,  0.2592,  0.2799,  0.0098,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.336382727422454, distance: 0.9322138133101973 entropy -3.327006920314306
epoch: 38, step: 16
	action: tensor([[ 0.1469,  0.0572, -0.3206,  0.0898, -0.0576,  0.0616,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-34.5675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5030111252161447, distance: 0.8067333796127525 entropy -2.878375228392845
epoch: 38, step: 17
	action: tensor([[ 0.2024, -0.0600, -0.0239, -0.4323, -0.7718,  0.2127,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-33.5284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2215645771719622, distance: 1.0096433483650111 entropy -2.973217958656359
epoch: 38, step: 18
	action: tensor([[ 0.6810,  0.2042, -0.1563,  0.4244, -0.0210,  0.6568,  0.1002]],
       dtype=torch.float64)
	q_value: tensor([[-34.5287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9671836092939549, distance: 0.20730133557153185 entropy -2.376421290083997
epoch: 38, step: 19
	action: tensor([[-0.5966, -0.1428,  0.7255,  0.4955,  0.0037,  0.6502, -0.0321]],
       dtype=torch.float64)
	q_value: tensor([[-41.1232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.20730133557153185 entropy -2.3334815313786623
epoch: 38, step: 20
	action: tensor([[ 0.0404, -0.0749,  0.0124,  0.0990, -0.0618,  0.0386,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33556405678574697, distance: 0.9327886482940654 entropy -3.327006920314306
epoch: 38, step: 21
	action: tensor([[ 0.1379, -0.7351, -0.2912, -0.1775, -0.8216,  0.1669,  0.0724]],
       dtype=torch.float64)
	q_value: tensor([[-34.7016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3614519312203084, distance: 1.3352354353855516 entropy -2.7275545513011257
epoch: 38, step: 22
	action: tensor([[-1.0745,  0.2041, -0.1549, -0.0400, -0.4680, -0.3638,  0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-34.0343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9075522955749866, distance: 1.580501526967127 entropy -2.3421913956485763
epoch: 38, step: 23
	action: tensor([[ 0.3062,  0.1399, -0.0174,  0.2491, -0.3599,  0.0436,  0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-33.2755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.580501526967127 entropy -3.421727933661176
epoch: 38, step: 24
	action: tensor([[-0.0052,  0.1616,  0.3185, -0.0522,  0.1381, -0.0091,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35653151963431684, distance: 0.9179527609602851 entropy -3.327006920314306
epoch: 38, step: 25
	action: tensor([[ 0.5291, -0.3784,  0.4178,  0.0155,  0.0748, -0.0014,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[-35.7385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3521601498696949, distance: 0.9210655155721067 entropy -2.7316370817521403
epoch: 38, step: 26
	action: tensor([[-0.4815, -0.0011,  0.2146,  0.3189,  0.2186,  0.2429, -0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-37.3530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014943198788249457, distance: 1.1357619898662399 entropy -2.3371653042029146
epoch: 38, step: 27
	action: tensor([[ 0.1629,  0.0709, -0.0920,  0.4623,  0.1764,  0.0952,  0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-36.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1357619898662399 entropy -2.7835024153230004
epoch: 38, step: 28
	action: tensor([[-0.1343,  0.0295, -0.1409,  0.0239,  0.3312,  0.0708,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16902132732693742, distance: 1.0431616967710182 entropy -3.327006920314306
epoch: 38, step: 29
	action: tensor([[-0.2446,  0.3311, -0.1170, -0.1753,  0.4144, -0.0848, -0.0259]],
       dtype=torch.float64)
	q_value: tensor([[-33.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13257405581137616, distance: 1.0657930822031387 entropy -3.118169292961723
epoch: 38, step: 30
	action: tensor([[-0.4114, -0.1798,  0.1851,  0.2173,  0.0551,  0.0578, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-32.7277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20279430180769142, distance: 1.2550249920371115 entropy -3.432162337714813
epoch: 38, step: 31
	action: tensor([[ 0.3093,  0.1574,  0.2182,  0.1416, -0.3825,  0.1806,  0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-34.5549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2550249920371115 entropy -2.7825246897648452
epoch: 38, step: 32
	action: tensor([[ 0.1041, -0.0576,  0.2134,  0.0993, -0.1248,  0.2780,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48689390174818326, distance: 0.8197100908528031 entropy -3.327006920314306
epoch: 38, step: 33
	action: tensor([[-1.1728, -0.0729, -0.0677, -0.4034, -0.6726,  0.3123,  0.0998]],
       dtype=torch.float64)
	q_value: tensor([[-36.8066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.377269896746958, distance: 1.764395488956076 entropy -2.4521055827320564
epoch: 38, step: 34
	action: tensor([[ 0.0631, -0.8111, -0.1203,  0.3492,  0.0850,  0.1493,  0.1939]],
       dtype=torch.float64)
	q_value: tensor([[-34.2224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11780368178278033, distance: 1.209872097928855 entropy -2.7662295182803947
epoch: 38, step: 35
	action: tensor([[ 0.0378,  0.0724, -0.5024, -0.7623,  0.1657,  0.2516,  0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-34.6165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31142625483585096, distance: 0.9495807958438015 entropy -2.51173540487872
epoch: 38, step: 36
	action: tensor([[-0.2608, -0.2571, -0.1045,  0.4476, -0.1641, -0.2639, -0.1178]],
       dtype=torch.float64)
	q_value: tensor([[-29.6736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024397085790771156, distance: 1.158219467745698 entropy -3.119670448760857
epoch: 38, step: 37
	action: tensor([[-0.5064, -0.2541, -0.0206, -0.2567, -0.5413,  0.2082,  0.1568]],
       dtype=torch.float64)
	q_value: tensor([[-34.1620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6063660967030917, distance: 1.4503705008120886 entropy -2.9371689973847803
epoch: 38, step: 38
	action: tensor([[ 0.5996,  0.1162,  0.7359, -0.3056, -0.1180,  0.2046,  0.1934]],
       dtype=torch.float64)
	q_value: tensor([[-33.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7025634316074192, distance: 0.6240995568592382 entropy -2.5975106142888724
epoch: 38, step: 39
	action: tensor([[-0.1467,  0.6907, -0.2932, -0.2683,  0.2736, -0.2922, -0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-40.9301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6240995568592382 entropy -2.1335646499381404
epoch: 38, step: 40
	action: tensor([[ 0.1144, -0.0293,  0.0471,  0.5111, -0.1383, -0.1215,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5881190667532379, distance: 0.7344166890677939 entropy -3.327006920314306
epoch: 38, step: 41
	action: tensor([[ 0.4128, -0.0378, -0.4240,  0.1009,  0.2022,  0.2006,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6849853298763651, distance: 0.6422765751212015 entropy -3.327006920314306
epoch: 38, step: 42
	action: tensor([[ 0.0043, -0.0069,  0.0095,  0.0123,  0.2766, -0.1061, -0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-34.4085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2856760453470518, distance: 0.9671733036463669 entropy -2.8310855513738207
epoch: 38, step: 43
	action: tensor([[-0.4273, -0.4828,  0.0190, -0.1097,  0.5623,  0.2263, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[-33.2584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5126768146856111, distance: 1.4074396071142792 entropy -3.0362721532743016
epoch: 38, step: 44
	action: tensor([[-0.5796, -0.1132, -0.0063,  0.0340, -0.3713,  0.1449, -0.0562]],
       dtype=torch.float64)
	q_value: tensor([[-32.2100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44167825364280544, distance: 1.3740130800813322 entropy -2.866600573530153
epoch: 38, step: 45
	action: tensor([[-0.0824, -0.0324, -0.0122,  0.3509, -0.0820,  0.2972,  0.1897]],
       dtype=torch.float64)
	q_value: tensor([[-33.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3740130800813322 entropy -2.796690088642808
epoch: 38, step: 46
	action: tensor([[ 0.4024, -0.1374, -0.0485,  0.0357, -0.4463, -0.0167,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5474522762427617, distance: 0.7698194606997156 entropy -3.327006920314306
epoch: 38, step: 47
	action: tensor([[ 0.1933,  0.2181, -0.9489, -0.1659, -0.1264, -0.1566,  0.0529]],
       dtype=torch.float64)
	q_value: tensor([[-36.0218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6667435498902439, distance: 0.660611265025574 entropy -2.51007999384029
epoch: 38, step: 48
	action: tensor([[-0.3284,  0.1093, -0.0367, -0.3364,  0.0660, -0.0150, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[-30.2217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12320005553212887, distance: 1.2127890059908684 entropy -3.506392600871962
epoch: 38, step: 49
	action: tensor([[ 0.1576, -0.1244, -0.2900, -0.0945, -0.1919,  0.0914,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-31.5419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33471202371244413, distance: 0.9333865329808452 entropy -3.1967606302942566
epoch: 38, step: 50
	action: tensor([[-0.2216,  0.3679, -0.2066, -0.1490, -0.2260,  0.3154,  0.0332]],
       dtype=torch.float64)
	q_value: tensor([[-32.4359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9333865329808452 entropy -2.835157887895553
epoch: 38, step: 51
	action: tensor([[ 0.1196,  0.0015, -0.3032, -0.0787,  0.4027, -0.0773,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.365996140860232, distance: 0.9111767802514998 entropy -3.327006920314306
epoch: 38, step: 52
	action: tensor([[ 0.5531, -0.0773,  0.3926, -0.1081, -0.8693,  0.2794, -0.0718]],
       dtype=torch.float64)
	q_value: tensor([[-31.9775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6294091815297758, distance: 0.6966329638674431 entropy -3.192579633057687
epoch: 38, step: 53
	action: tensor([[ 0.1761,  0.3993,  0.9886, -0.2779, -0.0435,  1.0102,  0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-39.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6966329638674431 entropy -2.0691747339312068
epoch: 38, step: 54
	action: tensor([[-0.3628, -0.4535, -0.0740, -0.1495,  0.1319,  0.1869,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.519968170267288, distance: 1.410827576685347 entropy -3.327006920314306
epoch: 38, step: 55
	action: tensor([[-0.6132, -0.6741, -0.2755, -0.2284, -0.2484, -0.1064,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-31.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9347148895220345, distance: 1.5917145281799445 entropy -2.8270300941585953
epoch: 38, step: 56
	action: tensor([[-0.4157, -0.1837,  0.0463, -0.0312,  0.2593,  0.1737,  0.1079]],
       dtype=torch.float64)
	q_value: tensor([[-30.6260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.322257122420603, distance: 1.3158750450024692 entropy -3.0374353561756693
epoch: 38, step: 57
	action: tensor([[ 0.1121,  0.3642, -0.4513, -0.1425, -0.0029,  0.2765,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-32.8520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3158750450024692 entropy -2.9056612869617773
epoch: 38, step: 58
	action: tensor([[-0.0289,  0.0749, -0.0511, -0.1112, -0.1253,  0.0251,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2669342845362008, distance: 0.9797790468217084 entropy -3.327006920314306
epoch: 38, step: 59
	action: tensor([[-0.2812, -0.2705, -0.0661, -0.0550,  0.1964,  0.4249,  0.0510]],
       dtype=torch.float64)
	q_value: tensor([[-33.4274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1278935498159719, distance: 1.2153202933338907 entropy -2.854624805818973
epoch: 38, step: 60
	action: tensor([[ 0.0922,  0.2014,  0.3893, -0.5205, -0.4362,  0.2602,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[-32.5913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26364176542773, distance: 0.9819768900702561 entropy -2.7575662057886237
epoch: 38, step: 61
	action: tensor([[-0.7181, -0.4980,  0.4362, -0.5225, -0.1551,  0.1268,  0.0645]],
       dtype=torch.float64)
	q_value: tensor([[-36.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1772225319171477, distance: 1.6885273648198331 entropy -2.3489637247464366
epoch: 38, step: 62
	action: tensor([[-0.3204,  0.4056,  0.5474,  0.6353,  0.1698, -0.0922,  0.1346]],
       dtype=torch.float64)
	q_value: tensor([[-33.7891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6885273648198331 entropy -2.4446034049549086
epoch: 38, step: 63
	action: tensor([[ 0.2896, -0.0043,  0.1207, -0.2645, -0.0604,  0.0205,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-31.6502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39347772832931827, distance: 0.8912100385406857 entropy -3.327006920314306
LOSS epoch 38 actor 479.3302339406029 critic 233.12845494183648
epoch: 39, step: 0
	action: tensor([[-0.6042,  0.2705, -0.1224,  0.1504, -0.2880,  0.3252, -0.0234]],
       dtype=torch.float64)
	q_value: tensor([[-34.4872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8912100385406857 entropy -2.6193113120721834
epoch: 39, step: 1
	action: tensor([[-0.2357,  0.0756, -0.0989,  0.0502,  0.0888, -0.0852,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042994966318146366, distance: 1.119473466057754 entropy -3.3278752279631356
epoch: 39, step: 2
	action: tensor([[ 0.0774, -0.2623,  0.0193, -0.1703, -0.1719,  0.0980,  0.0433]],
       dtype=torch.float64)
	q_value: tensor([[-33.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04841860656075159, distance: 1.116296759549504 entropy -3.143750803060379
epoch: 39, step: 3
	action: tensor([[ 0.1739, -0.2841,  0.6677,  0.8069, -0.7589,  0.3965,  0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-33.3128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8145149789844601, distance: 0.4928458626256556 entropy -2.6144161940026294
epoch: 39, step: 4
	action: tensor([[-0.9232,  2.0441,  2.0132,  0.0326,  1.5313,  0.2328,  0.3274]],
       dtype=torch.float64)
	q_value: tensor([[-46.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4928458626256556 entropy -1.9568165699243354
epoch: 39, step: 5
	action: tensor([[-0.2063,  0.0870, -0.0152,  0.2515, -0.1835,  0.0355,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2057002769243892, distance: 1.0198795856338652 entropy -3.3278752279631356
epoch: 39, step: 6
	action: tensor([[ 0.0488, -0.2392, -0.0055, -0.1577,  0.9013,  0.4188,  0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-35.9105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20475018783163457, distance: 1.0204893597901274 entropy -2.8034502578548484
epoch: 39, step: 7
	action: tensor([[ 0.0829,  0.0015,  0.1077, -0.0951,  0.3880,  0.1783,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38320938279074335, distance: 0.8987224128584432 entropy -3.3278752279631356
epoch: 39, step: 8
	action: tensor([[ 0.9860,  0.2040,  0.4078,  0.3999,  0.3329,  0.2877, -0.0754]],
       dtype=torch.float64)
	q_value: tensor([[-34.5615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9345156098200494, distance: 0.2928367559127242 entropy -2.752838375415763
epoch: 39, step: 9
	action: tensor([[-0.4790,  0.9354, -0.5957,  0.2147,  0.7419,  0.2253, -0.1338]],
       dtype=torch.float64)
	q_value: tensor([[-44.1205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.2928367559127242 entropy -2.2231373318250536
epoch: 39, step: 10
	action: tensor([[-0.1559, -0.0931,  0.0771,  0.1564, -0.0251, -0.0287,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10003309009286365, distance: 1.0856003234309224 entropy -3.3278752279631356
epoch: 39, step: 11
	action: tensor([[ 0.5776, -0.5931, -0.0143, -0.1669,  0.4033,  0.0789,  0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-34.8350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0185187144835377, distance: 1.1336988466193247 entropy -2.7936237225939586
epoch: 39, step: 12
	action: tensor([[ 0.9863, -0.4267,  0.9454,  0.9704,  0.5196,  0.0803, -0.1722]],
       dtype=torch.float64)
	q_value: tensor([[-33.7459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5874166954164461, distance: 0.7350426145084135 entropy -2.47133325186681
epoch: 39, step: 13
	action: tensor([[ 2.1796, -0.5100, -1.3809, -0.3612,  0.6220,  0.1685, -0.0553]],
       dtype=torch.float64)
	q_value: tensor([[-49.8032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7350426145084135 entropy -1.9843318152041889
epoch: 39, step: 14
	action: tensor([[-0.1216, -0.2541,  0.1097,  0.0805, -0.1026, -0.0680,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03444368099933892, distance: 1.1638851278004292 entropy -3.3278752279631356
epoch: 39, step: 15
	action: tensor([[-0.0569,  0.1421, -0.1694, -0.9949, -0.2930,  0.1555,  0.1159]],
       dtype=torch.float64)
	q_value: tensor([[-34.0490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017032828417450796, distance: 1.1345566878527977 entropy -2.737129528906675
epoch: 39, step: 16
	action: tensor([[ 0.2280,  0.0205, -0.3533, -0.2201, -0.0988, -0.5703, -0.0730]],
       dtype=torch.float64)
	q_value: tensor([[-32.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3852043712228731, distance: 0.897267792020886 entropy -2.8370596351453776
epoch: 39, step: 17
	action: tensor([[ 0.3996,  0.2886,  0.2811,  0.2091,  0.0149, -0.1430, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[-31.4274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.897267792020886 entropy -3.3599816363931465
epoch: 39, step: 18
	action: tensor([[-0.3738, -0.1604,  0.0562, -0.1804, -0.1292, -0.0177,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40093813692522207, distance: 1.3544599635768477 entropy -3.3278752279631356
epoch: 39, step: 19
	action: tensor([[-0.6599, -0.1051,  0.0308, -0.2709,  0.2905, -0.1471,  0.1020]],
       dtype=torch.float64)
	q_value: tensor([[-32.6865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7991799289468748, distance: 1.5349491478399373 entropy -2.8806224580283235
epoch: 39, step: 20
	action: tensor([[0.1236, 0.0199, 0.0580, 0.2274, 0.3110, 0.0033, 0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-32.2540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.507862976513741, distance: 0.8027858563593643 entropy -3.313109361271319
epoch: 39, step: 21
	action: tensor([[ 0.3061, -0.1087,  0.1721,  0.0433,  0.1143,  0.3618,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[-35.8790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5899242502447103, distance: 0.7328055284227898 entropy -2.8415211094622594
epoch: 39, step: 22
	action: tensor([[ 3.2588e-01, -4.5502e-01, -4.9098e-01,  9.9922e-01,  7.3390e-01,
          1.6568e-01, -3.8278e-04]], dtype=torch.float64)
	q_value: tensor([[-36.4866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5445120749807116, distance: 0.7723161694009316 entropy -2.4506637946095036
epoch: 39, step: 23
	action: tensor([[-0.1015, -0.8086,  0.1851,  0.6018, -0.0779,  0.5544, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-39.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13454920103059276, distance: 1.0645789752080472 entropy -2.7716364732225176
epoch: 39, step: 24
	action: tensor([[ 0.8294,  0.6546, -0.5656, -0.1354, -1.3016,  0.8739,  0.2362]],
       dtype=torch.float64)
	q_value: tensor([[-38.3154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0645789752080472 entropy -2.199129159054419
epoch: 39, step: 25
	action: tensor([[-0.1012,  0.1824,  0.2162, -0.0011,  0.0620,  0.0442,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2980151166058168, distance: 0.95878354812368 entropy -3.3278752279631356
epoch: 39, step: 26
	action: tensor([[ 3.0924e-01,  5.1549e-02,  5.4772e-02, -7.4192e-01, -1.0006e+00,
          8.7879e-04,  4.5733e-02]], dtype=torch.float64)
	q_value: tensor([[-35.7279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2062563955969129, distance: 1.0195224953727942 entropy -2.7904263661572153
epoch: 39, step: 27
	action: tensor([[ 0.6408, -0.2800, -0.3277,  0.0037, -0.3034,  0.6306,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-36.9249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6109866733209255, distance: 0.7137381864380531 entropy -2.3507524659387697
epoch: 39, step: 28
	action: tensor([[-0.4470, -0.5001, -0.2317, -0.6859, -0.5786, -0.0019, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-36.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7029827191688087, distance: 1.4933507134904838 entropy -2.264896982331069
epoch: 39, step: 29
	action: tensor([[-0.4174, -0.3375, -0.0469, -0.2374, -0.0231,  0.1307,  0.0931]],
       dtype=torch.float64)
	q_value: tensor([[-31.2776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.55557360929009, distance: 1.4272563259403335 entropy -2.8385100918675334
epoch: 39, step: 30
	action: tensor([[-0.3218,  0.0085, -0.1166, -0.5550,  0.4096,  0.1328,  0.0738]],
       dtype=torch.float64)
	q_value: tensor([[-31.7133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24237879901049864, distance: 1.2755095337605864 entropy -2.8555945380295094
epoch: 39, step: 31
	action: tensor([[-0.4421, -0.1230,  0.1967, -0.1601,  0.1084, -0.0607, -0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-31.4450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46812986851548155, distance: 1.3865608398873914 entropy -3.147464136905854
epoch: 39, step: 32
	action: tensor([[-0.5170,  0.0882,  0.1815, -0.3615,  0.2927,  0.1162,  0.0648]],
       dtype=torch.float64)
	q_value: tensor([[-32.9137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45521017618702575, distance: 1.3804464199970043 entropy -2.9636038082349425
epoch: 39, step: 33
	action: tensor([[-0.5346, -0.5081,  0.0495, -0.0708,  0.0226,  0.0132, -0.0261]],
       dtype=torch.float64)
	q_value: tensor([[-33.5039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7802221747000373, distance: 1.526840940180402 entropy -2.995518137883753
epoch: 39, step: 34
	action: tensor([[-0.1752, -0.2124, -0.3265,  0.1841,  0.0910,  0.1321,  0.1199]],
       dtype=torch.float64)
	q_value: tensor([[-32.1736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011385619636250954, distance: 1.1508403500188886 entropy -2.854557356251955
epoch: 39, step: 35
	action: tensor([[-0.1400, -0.4909, -0.4207, -0.5176, -0.1911,  0.2116,  0.0588]],
       dtype=torch.float64)
	q_value: tensor([[-32.6277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3743035036686402, distance: 1.3415226824521385 entropy -3.022318647680303
epoch: 39, step: 36
	action: tensor([[ 0.1849, -0.0989, -0.4199,  0.3235,  0.1557, -0.0157,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[-30.3151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45089342710922586, distance: 0.8479788384015073 entropy -2.866198719639239
epoch: 39, step: 37
	action: tensor([[ 0.0108, -0.0360,  0.1038, -0.2582,  0.3947,  0.1342,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[-33.8915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16212934003300972, distance: 1.04747866174237 entropy -3.0463198837850087
epoch: 39, step: 38
	action: tensor([[ 0.2694,  0.0429,  0.3496, -0.0203,  0.0102, -0.0276, -0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-33.3441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5297947445765254, distance: 0.7846941821740661 entropy -2.8184857658912357
epoch: 39, step: 39
	action: tensor([[ 0.4991, -0.5144,  0.4168, -0.1185, -0.1453, -0.0063,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-36.9003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08086370160146306, distance: 1.0971011038874456 entropy -2.5672964559266034
epoch: 39, step: 40
	action: tensor([[-1.5393, -0.3049, -0.4475, -0.7038, -1.2190,  0.3165, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[-36.5900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6536504427203922, distance: 1.8641401064367211 entropy -2.2696200284299968
epoch: 39, step: 41
	action: tensor([[ 0.0611, -0.0415,  0.1949,  0.6349,  0.3430,  0.2576,  0.1845]],
       dtype=torch.float64)
	q_value: tensor([[-37.0137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8641401064367211 entropy -2.853639102043286
epoch: 39, step: 42
	action: tensor([[-0.3611, -0.0005, -0.2421,  0.0227, -0.0807,  0.1545,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14644805903211222, distance: 1.2252758817014422 entropy -3.3278752279631356
epoch: 39, step: 43
	action: tensor([[-0.0592, -0.4199, -0.2909, -0.0780,  0.1510,  0.4696,  0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-32.6566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05129291891325605, distance: 1.1733256446092442 entropy -3.0608763274931126
epoch: 39, step: 44
	action: tensor([[ 0.5411,  0.1498,  0.6512, -0.3976, -0.4019,  0.1890, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[-31.9091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6234754877226231, distance: 0.7021878662017544 entropy -2.70308837227755
epoch: 39, step: 45
	action: tensor([[ 0.3092,  0.2057,  0.0607, -0.3163, -0.2510, -0.2060, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-40.0443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5090473363412007, distance: 0.8018192962397307 entropy -2.1424734622689745
epoch: 39, step: 46
	action: tensor([[ 0.4982, -0.5653, -0.4533, -0.4906,  0.3455,  0.0916, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-35.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10257391167418706, distance: 1.2016017424057799 entropy -2.773740482499064
epoch: 39, step: 47
	action: tensor([[ 0.1722,  0.1931, -0.2140,  0.4721,  0.3411, -0.2072, -0.1965]],
       dtype=torch.float64)
	q_value: tensor([[-30.4291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2016017424057799 entropy -2.701978151106288
epoch: 39, step: 48
	action: tensor([[ 0.3435, -0.0906,  0.0637,  0.3088,  0.0276,  0.1457,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.726563807865114, distance: 0.5983905026264993 entropy -3.3278752279631356
epoch: 39, step: 49
	action: tensor([[ 0.3110,  0.1003, -0.5725, -0.6039, -0.1230,  0.4662,  0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-37.7015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5673504962385334, distance: 0.7527049953160678 entropy -2.53697975131163
epoch: 39, step: 50
	action: tensor([[ 0.0840, -0.2169,  0.4537,  0.0565,  0.2140,  0.1528, -0.1027]],
       dtype=torch.float64)
	q_value: tensor([[-31.1740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.313422760413266, distance: 0.9482031513981933 entropy -2.7678912194168666
epoch: 39, step: 51
	action: tensor([[ 0.0687,  0.3256, -0.8011, -0.5237,  0.3372,  0.3614,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-36.6428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6583901505492515, distance: 0.6688394564392197 entropy -2.47216100357775
epoch: 39, step: 52
	action: tensor([[-0.3644, -0.0509,  0.0446, -0.0128, -0.1706,  0.2547, -0.0810]],
       dtype=torch.float64)
	q_value: tensor([[-30.3955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09994281171692099, distance: 1.2001671797766116 entropy -3.2778832597170897
epoch: 39, step: 53
	action: tensor([[ 0.1460, -0.1351,  0.3893, -0.8468, -0.2184,  0.2786,  0.1436]],
       dtype=torch.float64)
	q_value: tensor([[-33.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1282463018161646, distance: 1.2155103259614572 entropy -2.752447064040602
epoch: 39, step: 54
	action: tensor([[ 0.6880,  0.8372,  0.2929,  1.3492, -1.1663, -0.0343, -0.0671]],
       dtype=torch.float64)
	q_value: tensor([[-35.0688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2155103259614572 entropy -2.28112558100299
epoch: 39, step: 55
	action: tensor([[ 0.1403, -0.0147,  0.1994, -0.1047,  0.0682,  0.1555,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4019387995663285, distance: 0.8849719537234461 entropy -3.3278752279631356
epoch: 39, step: 56
	action: tensor([[ 1.2094, -0.3632,  0.0934,  0.1364, -0.0377,  0.2716,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-35.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5088979596841725, distance: 0.8019412672419889 entropy -2.5841280183574256
epoch: 39, step: 57
	action: tensor([[-0.2736, -0.3573, -0.1791, -0.2702, -0.3039,  0.5848, -0.1645]],
       dtype=torch.float64)
	q_value: tensor([[-40.4230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3424221250811226, distance: 1.3258709154362822 entropy -2.1336311315545937
epoch: 39, step: 58
	action: tensor([[0.2240, 0.0420, 0.1053, 0.3440, 0.5933, 0.2336, 0.1321]],
       dtype=torch.float64)
	q_value: tensor([[-32.4233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3258709154362822 entropy -2.528865133590287
epoch: 39, step: 59
	action: tensor([[-0.1446, -0.0174,  0.2220, -0.1229, -0.0966, -0.0020,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018862710374581204, distance: 1.1335001561664257 entropy -3.3278752279631356
epoch: 39, step: 60
	action: tensor([[-0.2749,  0.1815, -0.1617,  0.0215,  0.3721,  0.2933,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-34.4471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16853427989870584, distance: 1.0434673573214384 entropy -2.7234212053874387
epoch: 39, step: 61
	action: tensor([[ 0.3792,  0.0696, -0.0697,  0.5151,  0.5227,  0.1489, -0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-33.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0434673573214384 entropy -3.1260897286814293
epoch: 39, step: 62
	action: tensor([[-0.1241, -0.1288, -0.0971,  0.1898,  0.3403, -0.1156,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0434673573214384 entropy -3.3278752279631356
epoch: 39, step: 63
	action: tensor([[ 0.4023, -0.2395,  0.0164, -0.2932,  0.3192, -0.1753,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-32.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1824127393412316, distance: 1.0347221623418243 entropy -3.3278752279631356
LOSS epoch 39 actor 485.8842675039427 critic 253.49741779121214
epoch: 40, step: 0
	action: tensor([[-0.5752,  0.3116, -0.2591, -0.3305, -0.2100, -0.2661, -0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-33.3987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23886472319694296, distance: 1.2737043632259692 entropy -2.732273215429468
epoch: 40, step: 1
	action: tensor([[-0.0065,  0.0133, -0.0166,  0.0508, -0.2821, -0.0082,  0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-32.4814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31767618112520424, distance: 0.9452614774697522 entropy -3.740578942716987
epoch: 40, step: 2
	action: tensor([[-0.2106,  0.4964,  0.0769, -0.2057,  0.1797,  0.1481,  0.1101]],
       dtype=torch.float64)
	q_value: tensor([[-35.3187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9452614774697522 entropy -2.7590781109982663
epoch: 40, step: 3
	action: tensor([[ 0.1357, -0.0917,  0.1426, -0.1414, -0.3752, -0.1113,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-34.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25548447570223254, distance: 0.9874010215914969 entropy -3.327333210017945
epoch: 40, step: 4
	action: tensor([[-0.3353, -0.3966, -0.0778, -0.3811, -0.4201,  0.2555,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-35.5496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5410783182808794, distance: 1.4205909656839888 entropy -2.5919929480722645
epoch: 40, step: 5
	action: tensor([[-0.1627, -0.5915, -0.1046, -0.2475, -0.4381,  0.5263,  0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-33.1251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42745671563068255, distance: 1.3672192589844936 entropy -2.5806462247211917
epoch: 40, step: 6
	action: tensor([[-0.3264, -0.2018, -0.2763, -0.2716,  0.9857, -0.0471,  0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-34.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3268973459660405, distance: 1.318181936266322 entropy -2.308626608828349
epoch: 40, step: 7
	action: tensor([[ 0.3160,  0.0591, -0.1586,  0.1749,  0.5405, -0.0405, -0.1316]],
       dtype=torch.float64)
	q_value: tensor([[-32.5369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6573930687563219, distance: 0.669814840683636 entropy -3.1608385369674354
epoch: 40, step: 8
	action: tensor([[ 0.5973,  0.1558, -0.0685,  0.4891, -0.7761,  0.2448, -0.0727]],
       dtype=torch.float64)
	q_value: tensor([[-34.8313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9264104951087309, distance: 0.31043067825202797 entropy -3.0038975316826724
epoch: 40, step: 9
	action: tensor([[ 0.4607, -0.3416,  0.3416,  0.5905, -0.3615,  0.8802,  0.1253]],
       dtype=torch.float64)
	q_value: tensor([[-42.3028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8885468162500286, distance: 0.38203480672327667 entropy -2.2596652888925735
epoch: 40, step: 10
	action: tensor([[ 0.0687,  0.2403, -0.1034, -0.2433, -2.1155,  1.4260,  0.1702]],
       dtype=torch.float64)
	q_value: tensor([[-43.8898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.38203480672327667 entropy -1.9704601227685092
epoch: 40, step: 11
	action: tensor([[-0.6348, -0.0990,  0.2279,  0.2387,  0.2009, -0.0846,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-34.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4890925794438816, distance: 1.3964247673929533 entropy -3.327333210017945
epoch: 40, step: 12
	action: tensor([[-0.7085, -0.5092, -0.0274,  0.1366,  1.0280, -0.0680,  0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-35.6616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9562838650729562, distance: 1.6005624715809907 entropy -2.9563606361788777
epoch: 40, step: 13
	action: tensor([[ 0.1998, -0.3064,  0.2476, -0.0784,  0.1318, -0.2217, -0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-35.1236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06173696707770737, distance: 1.1084573720645254 entropy -3.0877065986823564
epoch: 40, step: 14
	action: tensor([[-0.3306, -0.2755,  0.0288, -0.3423,  0.0654,  0.3453, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[-34.5847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40798872347312853, distance: 1.357864022507867 entropy -2.668082840771774
epoch: 40, step: 15
	action: tensor([[-0.1463,  0.3268, -0.2395, -0.3788, -0.2157,  0.2787,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[-32.4023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24505501101990113, distance: 0.9942929192985146 entropy -2.705618070884636
epoch: 40, step: 16
	action: tensor([[ 0.2376, -0.0538,  0.0703,  0.2712, -0.0820, -0.2406,  0.0263]],
       dtype=torch.float64)
	q_value: tensor([[-33.2441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5296760611990498, distance: 0.784793207311284 entropy -2.988345237830363
epoch: 40, step: 17
	action: tensor([[ 0.2367, -0.5083, -0.3439,  0.4154, -0.4224,  0.0910,  0.0719]],
       dtype=torch.float64)
	q_value: tensor([[-37.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2266467863052679, distance: 1.0063420975121162 entropy -2.738416459747484
epoch: 40, step: 18
	action: tensor([[-0.5979, -0.0816,  0.3452, -0.6596,  0.1904,  0.3335,  0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-35.7417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8041712398465914, distance: 1.5370768122411735 entropy -2.5251803435811877
epoch: 40, step: 19
	action: tensor([[-0.0436, -0.4474, -0.2800,  0.2099, -0.5891, -0.0243, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-35.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07775608170380122, distance: 1.1880013506075235 entropy -2.615725894890247
epoch: 40, step: 20
	action: tensor([[-0.2919,  0.1288, -0.9307, -0.1644,  0.8383,  0.2435,  0.1757]],
       dtype=torch.float64)
	q_value: tensor([[-34.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11062704046987715, distance: 1.0791918422499347 entropy -2.6350041001063773
epoch: 40, step: 21
	action: tensor([[ 0.3804,  0.0708, -0.2968,  0.0830, -0.1707, -0.0760, -0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-31.6910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6570547608256891, distance: 0.6701454641175677 entropy -3.395043065957381
epoch: 40, step: 22
	action: tensor([[-0.5574, -0.3892,  0.2294,  0.1876,  0.5947,  0.0959,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[-35.1593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5484457723882017, distance: 1.4239826361557717 entropy -2.8748533949320274
epoch: 40, step: 23
	action: tensor([[-0.1828,  0.3058,  0.4267, -0.2634,  0.3333,  0.0327,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-35.3894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10258267254607734, distance: 1.0840614942903495 entropy -2.8458163042147153
epoch: 40, step: 24
	action: tensor([[ 0.2254, -0.0891,  0.5390,  0.1349,  0.1082,  0.6569, -0.0568]],
       dtype=torch.float64)
	q_value: tensor([[-36.9369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.705895927121492, distance: 0.6205934859287999 entropy -2.773599944746016
epoch: 40, step: 25
	action: tensor([[ 0.9986, -0.5043, -0.6597,  0.2306,  0.0774,  0.6941,  0.0548]],
       dtype=torch.float64)
	q_value: tensor([[-39.8938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6223171749994547, distance: 0.7032671167394232 entropy -2.1886237955909382
epoch: 40, step: 26
	action: tensor([[ 0.5254,  0.3187,  0.9606,  0.6339, -0.7311, -0.1645, -0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-37.4470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9867077309136121, distance: 0.13193385595806623 entropy -2.2908044382464836
epoch: 40, step: 27
	action: tensor([[-1.1788,  1.5646,  0.6282,  0.3788,  1.2740,  0.7593,  0.1929]],
       dtype=torch.float64)
	q_value: tensor([[-49.7333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.13193385595806623 entropy -2.022746343968481
epoch: 40, step: 28
	action: tensor([[ 0.1117,  0.0288, -0.0349, -0.1007,  0.1429,  0.2535,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-34.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43347573172475373, distance: 0.8613228178391176 entropy -3.327333210017945
epoch: 40, step: 29
	action: tensor([[-0.3650,  0.1731,  0.3452, -0.1307, -0.5681,  0.4689, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[-34.3595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009274606772446514, distance: 1.1496386778915193 entropy -2.75159340047426
epoch: 40, step: 30
	action: tensor([[-0.9505,  0.0853, -0.0171, -1.3457, -0.3101,  0.3579,  0.2561]],
       dtype=torch.float64)
	q_value: tensor([[-38.2386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9587641968250906, distance: 1.6015768101544428 entropy -2.305385691463971
epoch: 40, step: 31
	action: tensor([[-0.2999,  0.2951,  0.0605, -0.1490, -0.5395,  0.0552, -0.0276]],
       dtype=torch.float64)
	q_value: tensor([[-36.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6015768101544428 entropy -2.806878402243547
epoch: 40, step: 32
	action: tensor([[ 0.2149, -0.0430, -0.3007, -0.1170,  0.2770, -0.0344,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-34.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44348163373738947, distance: 0.8536826283741765 entropy -3.327333210017945
epoch: 40, step: 33
	action: tensor([[ 0.1537, -0.0672, -0.2940, -0.2775,  0.1135, -0.0327, -0.0693]],
       dtype=torch.float64)
	q_value: tensor([[-32.6424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2945801037689132, distance: 0.961126486534694 entropy -3.0353898984759153
epoch: 40, step: 34
	action: tensor([[-5.0466e-01,  1.1194e-01,  2.9534e-05, -3.5029e-01, -2.0209e-01,
          1.5072e-01, -4.4383e-02]], dtype=torch.float64)
	q_value: tensor([[-31.4151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33932284273234004, distance: 1.3243394959333572 entropy -3.0964035526919482
epoch: 40, step: 35
	action: tensor([[ 0.1082, -0.2017, -0.2935, -0.8032,  0.3970,  0.0021,  0.0885]],
       dtype=torch.float64)
	q_value: tensor([[-33.4776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0384576349102379, distance: 1.1661410544096724 entropy -2.996331616473668
epoch: 40, step: 36
	action: tensor([[-0.6993, -0.5415,  0.4418,  0.0395, -0.2562,  0.0116, -0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-30.4719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.890765428061681, distance: 1.5735317838178522 entropy -2.9546548218548603
epoch: 40, step: 37
	action: tensor([[-0.6464,  0.4329,  0.5264, -0.8084, -0.6140,  0.1343,  0.2764]],
       dtype=torch.float64)
	q_value: tensor([[-35.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6669112446844028, distance: 1.4774504729994433 entropy -2.4577482704109137
epoch: 40, step: 38
	action: tensor([[ 0.6023, -0.0587, -0.4106, -0.0922,  0.1383,  0.6945,  0.1559]],
       dtype=torch.float64)
	q_value: tensor([[-40.1438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7877740785160682, distance: 0.5271763359126196 entropy -2.393193624068286
epoch: 40, step: 39
	action: tensor([[ 0.2131, -0.4987,  0.4609, -0.2737, -0.4501, -0.4223, -0.1277]],
       dtype=torch.float64)
	q_value: tensor([[-35.4388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2626972902000604, distance: 1.2858973979020258 entropy -2.4475275704498167
epoch: 40, step: 40
	action: tensor([[-0.3187,  0.0429, -0.8386, -0.8702,  1.4945,  0.9561,  0.0705]],
       dtype=torch.float64)
	q_value: tensor([[-35.8543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.548631969346044, distance: 0.7688154303277128 entropy -2.4088169140759015
epoch: 40, step: 41
	action: tensor([[-0.9719, -0.2243, -0.3043,  0.4733,  0.5383, -0.3896, -0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-35.9447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0506332899002522, distance: 1.6387046836106582 entropy -2.6944098651591597
epoch: 40, step: 42
	action: tensor([[-0.3064,  0.0350, -0.1425,  0.3971, -0.1928, -0.0410,  0.0609]],
       dtype=torch.float64)
	q_value: tensor([[-35.0748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6387046836106582 entropy -3.59063734612263
epoch: 40, step: 43
	action: tensor([[-0.1836,  0.0686,  0.0911,  0.3369,  0.3845, -0.1220,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-34.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23421217409540307, distance: 1.0014076902401219 entropy -3.327333210017945
epoch: 40, step: 44
	action: tensor([[-0.4343, -0.2063,  0.4856, -0.3072, -0.3640,  0.3968,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[-36.6200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5258867665039961, distance: 1.4135717150547022 entropy -2.9972476673758868
epoch: 40, step: 45
	action: tensor([[-0.4158,  0.2133,  0.1582, -0.3844, -0.6157,  0.8732,  0.2259]],
       dtype=torch.float64)
	q_value: tensor([[-36.8142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1805584665542299, distance: 1.2433701684496723 entropy -2.273575045513599
epoch: 40, step: 46
	action: tensor([[-0.1545, -0.2217, -0.2889,  0.3098,  0.1173, -0.1089,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-34.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0012060464777550184, distance: 1.1450341122408392 entropy -3.327333210017945
epoch: 40, step: 47
	action: tensor([[ 0.0695, -0.1036, -0.1056, -0.1077, -0.2695,  0.0220,  0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-33.6448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20930131356606252, distance: 1.0175650967421221 entropy -3.1082907182790036
epoch: 40, step: 48
	action: tensor([[-0.0768, -0.0705, -0.1143, -0.5589, -0.1819,  0.5474,  0.0761]],
       dtype=torch.float64)
	q_value: tensor([[-33.8070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0010297831298141658, distance: 1.1437548890276568 entropy -2.75061225005862
epoch: 40, step: 49
	action: tensor([[-0.1958, -0.4516,  0.3724, -0.6329,  0.7411,  0.2771,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-33.1683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6113590291557034, distance: 1.4526227842640054 entropy -2.5462803131547482
epoch: 40, step: 50
	action: tensor([[-0.6105,  0.0869,  0.2150, -0.4342,  1.1407, -0.1077, -0.2606]],
       dtype=torch.float64)
	q_value: tensor([[-33.8150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7095500439613458, distance: 1.4962273962919523 entropy -2.490056670981904
epoch: 40, step: 51
	action: tensor([[-0.6217,  0.0958, -0.2012, -0.2038,  0.0506, -0.0670, -0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-37.1114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5083545364168076, distance: 1.4054273803840798 entropy -3.021238570787614
epoch: 40, step: 52
	action: tensor([[ 0.3079, -0.0324, -0.0728, -0.0143, -0.0114,  0.0425,  0.0438]],
       dtype=torch.float64)
	q_value: tensor([[-31.8962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5210459312004139, distance: 0.7919606926026608 entropy -3.631892904051574
epoch: 40, step: 53
	action: tensor([[ 3.8170e-01, -1.6723e-01,  1.8741e-01,  1.1440e-01, -2.2943e-01,
          5.2743e-01, -2.6811e-04]], dtype=torch.float64)
	q_value: tensor([[-34.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.662682416395865, distance: 0.6646242504825371 entropy -2.7187179127504204
epoch: 40, step: 54
	action: tensor([[ 0.1748, -0.2387, -0.9754, -1.2021,  1.3397, -0.0760,  0.0929]],
       dtype=torch.float64)
	q_value: tensor([[-38.2802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2755582054395026, distance: 0.973998845565559 entropy -2.2401159945234466
epoch: 40, step: 55
	action: tensor([[-0.8002, -0.0504,  0.4116, -0.3073,  0.0914,  0.0690, -0.2132]],
       dtype=torch.float64)
	q_value: tensor([[-33.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.976396720700327, distance: 1.6087692463475034 entropy -2.742594484326337
epoch: 40, step: 56
	action: tensor([[ 0.0330, -0.7050,  0.4677, -0.4410,  0.2665, -0.1647,  0.1004]],
       dtype=torch.float64)
	q_value: tensor([[-35.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7022848142525253, distance: 1.4930446846642949 entropy -2.808237057866553
epoch: 40, step: 57
	action: tensor([[-0.5347, -0.2965, -0.1049,  0.0484,  0.2756,  0.1094, -0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-34.0864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5618871518559483, distance: 1.4301497662867675 entropy -2.4100673319928374
epoch: 40, step: 58
	action: tensor([[-0.3724, -0.2963,  0.0933, -0.7397,  0.3211, -0.1258,  0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-32.3917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7165740890341259, distance: 1.4992980275484362 entropy -3.1345277443775212
epoch: 40, step: 59
	action: tensor([[ 0.1911, -0.2385, -0.2959,  0.0989,  0.1638,  0.1161, -0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-32.4269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32097383377488764, distance: 0.9429744992590476 entropy -2.962295417481801
epoch: 40, step: 60
	action: tensor([[ 0.2892, -0.1692, -0.5357, -0.4088,  1.0643,  0.0942,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-32.8786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3118634349942313, distance: 0.9492793003249619 entropy -2.8993479667409012
epoch: 40, step: 61
	action: tensor([[-0.3342, -0.0728, -0.2812,  0.3786,  0.5319, -0.2559, -0.2004]],
       dtype=torch.float64)
	q_value: tensor([[-31.8311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1545750189683146, distance: 1.2296110903481048 entropy -2.895712844353342
epoch: 40, step: 62
	action: tensor([[-0.2633, -0.0153,  0.1644, -0.0965, -0.1213,  0.0318,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[-34.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10051126740731275, distance: 1.2004772657791778 entropy -3.499878588422549
epoch: 40, step: 63
	action: tensor([[-0.9173, -0.1048,  0.2191, -0.4391,  0.1030,  0.2140,  0.1089]],
       dtype=torch.float64)
	q_value: tensor([[-34.6391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1407838101585845, distance: 1.6743378659618149 entropy -2.7763931835533975
LOSS epoch 40 actor 551.7649366562501 critic 156.59637702067624
epoch: 41, step: 0
	action: tensor([[ 0.6008, -0.4280,  0.0454, -0.0345,  0.1582, -0.0581,  0.0569]],
       dtype=torch.float64)
	q_value: tensor([[-33.2506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26877399050486817, distance: 0.9785488449188499 entropy -2.8591032397975105
epoch: 41, step: 1
	action: tensor([[ 0.1307,  0.5362,  0.0211,  1.3326,  0.5011, -0.1713, -0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-32.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9785488449188499 entropy -2.5082726198493943
epoch: 41, step: 2
	action: tensor([[-0.4229, -0.1277,  0.2424,  0.1941, -0.2518, -0.2687,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2771755165254095, distance: 1.2932485064656987 entropy -3.3262248863369686
epoch: 41, step: 3
	action: tensor([[ 0.2410, -0.2313,  0.5120, -0.7825, -0.1158,  0.0016,  0.1726]],
       dtype=torch.float64)
	q_value: tensor([[-34.5468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24156223805193222, distance: 1.2750902966958433 entropy -2.786985127405689
epoch: 41, step: 4
	action: tensor([[ 0.9635, -0.3616,  0.2399, -1.5943, -0.2578,  0.3943, -0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-34.0846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.286358309758177, distance: 1.2978893579594268 entropy -2.3060882672612
epoch: 41, step: 5
	action: tensor([[ 0.7633, -0.2564, -0.2427, -0.9987, -1.6410,  0.1585, -0.3857]],
       dtype=torch.float64)
	q_value: tensor([[-35.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07425116471335835, distance: 1.1860680574607052 entropy -1.9638479433259826
epoch: 41, step: 6
	action: tensor([[-1.6749,  0.2657,  1.3751,  1.5812, -0.7151,  0.8756, -0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-39.6309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1860680574607052 entropy -2.0466287055608037
epoch: 41, step: 7
	action: tensor([[ 0.0083, -0.1755,  0.0300,  0.1548,  0.3071,  0.0191,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2548030651966371, distance: 0.9878527727831055 entropy -3.3262248863369686
epoch: 41, step: 8
	action: tensor([[-0.0338, -0.1300, -0.2720,  0.4155,  0.4251,  0.0072,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-32.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962626438639331, distance: 0.9599795814845861 entropy -2.825548154951618
epoch: 41, step: 9
	action: tensor([[-0.0610,  0.3073, -0.2796, -0.1579, -0.3162,  0.2788,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-32.7453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9599795814845861 entropy -3.1168064783234675
epoch: 41, step: 10
	action: tensor([[-0.3979, -0.1528, -0.0444,  0.0104, -0.3358, -0.1207,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9599795814845861 entropy -3.3262248863369686
epoch: 41, step: 11
	action: tensor([[-0.4921,  0.0596,  0.3226,  0.1613, -0.1943,  0.0239,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18362217056840024, distance: 1.244982477397357 entropy -3.3262248863369686
epoch: 41, step: 12
	action: tensor([[-0.1264, -0.1945,  0.1453,  0.3688,  0.9933,  0.5307,  0.1835]],
       dtype=torch.float64)
	q_value: tensor([[-35.5173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4135050427114658, distance: 0.8763726881468316 entropy -2.6800770566523764
epoch: 41, step: 13
	action: tensor([[-1.0374, -0.3239, -0.0439,  0.1250,  0.3621,  0.0360, -0.1597]],
       dtype=torch.float64)
	q_value: tensor([[-35.7617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2370510616319894, distance: 1.7115699061477536 entropy -2.6338994811940037
epoch: 41, step: 14
	action: tensor([[-0.3100, -0.0386,  0.0193, -0.8092, -0.2805, -0.1249,  0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-32.4185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4377251345884883, distance: 1.3721279972585456 entropy -3.272224222340268
epoch: 41, step: 15
	action: tensor([[ 0.4040, -0.3024,  0.0585, -0.0521,  0.1651,  0.2222,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-31.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38678401298306764, distance: 0.8961143410091984 entropy -2.9437309562530776
epoch: 41, step: 16
	action: tensor([[-0.6852,  0.0498, -0.2255, -0.2904, -0.6313,  0.5983, -0.0466]],
       dtype=torch.float64)
	q_value: tensor([[-32.8657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6785682400821538, distance: 1.4826075046167135 entropy -2.491727924076296
epoch: 41, step: 17
	action: tensor([[-0.4662,  0.8524, -0.3994, -0.9350, -0.2598,  0.1851,  0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-33.4426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4826075046167135 entropy -2.6467908954980968
epoch: 41, step: 18
	action: tensor([[ 0.2049, -0.1416,  0.1979, -0.0476,  0.0919, -0.0748,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3290066905185517, distance: 0.937380226691742 entropy -3.3262248863369686
epoch: 41, step: 19
	action: tensor([[-1.7243e-01, -5.2010e-01, -3.8873e-01, -7.7921e-01,  5.6289e-01,
         -1.6572e-01, -4.6603e-04]], dtype=torch.float64)
	q_value: tensor([[-33.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4472226902814824, distance: 1.3766526489326982 entropy -2.6338587046318067
epoch: 41, step: 20
	action: tensor([[-0.2293, -0.1813, -0.1775,  0.0786,  0.1490,  0.4142, -0.1367]],
       dtype=torch.float64)
	q_value: tensor([[-27.9744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05120217932219395, distance: 1.114662864134022 entropy -3.1241789771544672
epoch: 41, step: 21
	action: tensor([[ 0.0230, -0.0435, -0.2544,  0.5998, -0.0479,  0.5752,  0.0610]],
       dtype=torch.float64)
	q_value: tensor([[-31.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.114662864134022 entropy -2.866051503602331
epoch: 41, step: 22
	action: tensor([[-0.2782, -0.1369,  0.0067,  0.1172,  0.1121, -0.2344,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17211730103146272, distance: 1.238917054882191 entropy -3.3262248863369686
epoch: 41, step: 23
	action: tensor([[ 0.3934, -0.0281, -0.1767, -0.3810, -0.5259,  0.0727,  0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-32.3667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4117057353104766, distance: 0.8777159701998295 entropy -3.064367766080533
epoch: 41, step: 24
	action: tensor([[ 0.0937, -1.2858, -1.1834, -0.4000, -0.0966,  0.2880,  0.0115]],
       dtype=torch.float64)
	q_value: tensor([[-32.8636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8777159701998295 entropy -2.5516899050809485
epoch: 41, step: 25
	action: tensor([[-0.0774,  0.1109, -0.1909, -0.1780, -0.3175, -0.2029,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21291790079424255, distance: 1.0152353023610883 entropy -3.3262248863369686
epoch: 41, step: 26
	action: tensor([[-0.2604,  0.0682, -0.3506, -0.7223,  0.7083,  0.0745,  0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-32.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05314542013797596, distance: 1.1743589581931513 entropy -3.0883450651151567
epoch: 41, step: 27
	action: tensor([[-0.4074, -0.0085,  0.0337,  0.5492, -0.0414,  0.2265, -0.1258]],
       dtype=torch.float64)
	q_value: tensor([[-29.7220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13601568453849888, distance: 1.0636766423310524 entropy -3.1890744835933047
epoch: 41, step: 28
	action: tensor([[ 0.2417, -0.5925, -0.1076, -0.2803,  0.1988,  0.2539,  0.1848]],
       dtype=torch.float64)
	q_value: tensor([[-36.1171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0636766423310524 entropy -2.7450025079503484
epoch: 41, step: 29
	action: tensor([[-0.2133,  0.0703, -0.0654,  0.3574,  0.0692, -0.0675,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19874424973827154, distance: 1.0243356149960121 entropy -3.3262248863369686
epoch: 41, step: 30
	action: tensor([[ 0.3699,  0.2931, -0.0558,  0.2900, -0.4549,  0.3786,  0.0917]],
       dtype=torch.float64)
	q_value: tensor([[-34.6211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8089931086680376, distance: 0.5001280483542401 entropy -2.987134275164205
epoch: 41, step: 31
	action: tensor([[ 0.0292, -0.0049,  0.0852,  0.1356, -0.1413,  0.1779,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42569678016757884, distance: 0.8672160737169554 entropy -3.3262248863369686
epoch: 41, step: 32
	action: tensor([[-0.0364,  0.2235,  0.8902, -0.5069, -0.5724,  0.0738,  0.1132]],
       dtype=torch.float64)
	q_value: tensor([[-34.9488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05950420982040194, distance: 1.1097754702591818 entropy -2.5901383325281615
epoch: 41, step: 33
	action: tensor([[-2.2394,  0.0858,  0.4301, -0.6616,  0.7320,  0.6502,  0.1291]],
       dtype=torch.float64)
	q_value: tensor([[-40.3429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1097754702591818 entropy -2.1386343017556544
epoch: 41, step: 34
	action: tensor([[-0.0974,  0.2447,  0.1995,  0.0759, -0.1794, -0.0115,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36349096597723995, distance: 0.9129751973251742 entropy -3.3262248863369686
epoch: 41, step: 35
	action: tensor([[-0.1434, -0.1968,  0.1818, -0.0678,  0.4537,  0.4248,  0.1091]],
       dtype=torch.float64)
	q_value: tensor([[-35.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09840057983429518, distance: 1.0865844988883295 entropy -2.7318842045188654
epoch: 41, step: 36
	action: tensor([[-0.1983,  0.1872,  0.3475, -0.2766,  0.1464, -0.0052,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0065333733706518515, distance: 1.148076382215414 entropy -3.3262248863369686
epoch: 41, step: 37
	action: tensor([[ 0.2154,  0.3902, -0.1786,  0.6613,  0.3189,  0.1748, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-34.1171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.148076382215414 entropy -2.7838506419318825
epoch: 41, step: 38
	action: tensor([[ 0.0174, -0.1633,  0.0373, -0.0787,  0.1003,  0.1447,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1629549185904552, distance: 1.0469624788211067 entropy -3.3262248863369686
epoch: 41, step: 39
	action: tensor([[-0.3900, -0.1216, -0.0036, -0.6211, -0.8558,  0.3000,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-32.1986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5571576568640693, distance: 1.4279828317911494 entropy -2.7080432399640157
epoch: 41, step: 40
	action: tensor([[ 0.5172, -0.6060,  0.0114,  0.2989, -0.1741,  0.0477,  0.1948]],
       dtype=torch.float64)
	q_value: tensor([[-33.7470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3184466618239121, distance: 0.9447276316472032 entropy -2.441888590392691
epoch: 41, step: 41
	action: tensor([[-0.5518, -0.7375, -0.1475,  0.1576, -0.0902, -0.2693,  0.0369]],
       dtype=torch.float64)
	q_value: tensor([[-35.1594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8818143088955721, distance: 1.5698027173355211 entropy -2.3529575161150262
epoch: 41, step: 42
	action: tensor([[-0.1802, -0.1629,  0.2924, -0.1508,  0.3931,  0.1916,  0.1425]],
       dtype=torch.float64)
	q_value: tensor([[-31.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0775970423945671, distance: 1.187913693535532 entropy -2.9643379067397073
epoch: 41, step: 43
	action: tensor([[-0.2253,  0.5536, -0.4679, -0.0768,  0.2961,  0.0653, -0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-32.7426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.187913693535532 entropy -2.656431054228851
epoch: 41, step: 44
	action: tensor([[ 0.1382,  0.0831, -0.0574, -0.2279,  0.1436,  0.0972,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39292424029586903, distance: 0.8916165871790538 entropy -3.3262248863369686
epoch: 41, step: 45
	action: tensor([[-0.1499,  0.0966, -0.4265, -0.6080,  0.1705,  0.0800, -0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-31.8993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12999712382467177, distance: 1.067375026767356 entropy -2.8722230687970005
epoch: 41, step: 46
	action: tensor([[-0.0641, -0.2007,  0.0566,  0.3867,  0.3280, -0.0257, -0.0533]],
       dtype=torch.float64)
	q_value: tensor([[-28.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2771633398683364, distance: 0.9729192101463355 entropy -3.4064607959737185
epoch: 41, step: 47
	action: tensor([[ 0.5856, -0.0151, -0.0948, -0.2007,  0.0855, -0.0389,  0.0613]],
       dtype=torch.float64)
	q_value: tensor([[-34.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5880861843403986, distance: 0.734446004470433 entropy -2.854171590521393
epoch: 41, step: 48
	action: tensor([[-0.5247,  0.3527,  0.1159,  0.3020,  0.4092,  0.2015, -0.1045]],
       dtype=torch.float64)
	q_value: tensor([[-32.8668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.734446004470433 entropy -2.6694775421391337
epoch: 41, step: 49
	action: tensor([[-0.4242, -0.2517,  0.1317, -0.1551, -0.1580,  0.1081,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5045088767858592, distance: 1.4036346172807246 entropy -3.3262248863369686
epoch: 41, step: 50
	action: tensor([[-0.2316, -0.1435,  0.2252,  0.0947,  0.3503,  0.7546,  0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-32.1237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27140292414288236, distance: 0.9767882016071405 entropy -2.7213727851694736
epoch: 41, step: 51
	action: tensor([[-0.4758, -0.6625,  0.2457, -0.0072,  0.8283,  0.3533,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[-34.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6301131319579496, distance: 1.461051640702224 entropy -2.4351717357416263
epoch: 41, step: 52
	action: tensor([[-1.0463,  0.2706, -0.0304, -0.5748, -0.4204,  0.4559, -0.1171]],
       dtype=torch.float64)
	q_value: tensor([[-32.6742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.063108165098329, distance: 1.6436815949897097 entropy -2.6136314900067954
epoch: 41, step: 53
	action: tensor([[-0.2856,  0.1016,  0.5716,  0.1576, -0.3701,  0.1847,  0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-34.0867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6436815949897097 entropy -2.932840961958621
epoch: 41, step: 54
	action: tensor([[-0.3508,  0.1797,  0.0259,  0.0842,  0.2072, -0.0718,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012426401936936449, distance: 1.1514323429517999 entropy -3.3262248863369686
epoch: 41, step: 55
	action: tensor([[-0.3053, -0.1025, -0.0091, -0.0726, -0.1696,  0.2274,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[-33.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16885656752313305, distance: 1.2371925722520913 entropy -3.1310823091234683
epoch: 41, step: 56
	action: tensor([[-0.4930,  0.2306, -0.3765,  0.2583,  0.4303,  0.4046,  0.1338]],
       dtype=torch.float64)
	q_value: tensor([[-32.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019916132407639342, distance: 1.1556835295573802 entropy -2.7439295434361384
epoch: 41, step: 57
	action: tensor([[ 0.0111, -0.0421, -0.0332,  0.3315,  0.0474, -0.2495, -0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-33.5760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1556835295573802 entropy -3.2880527970484326
epoch: 41, step: 58
	action: tensor([[-0.1562, -0.4522,  0.2421, -0.4225,  0.0416,  0.1861,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4815606678584632, distance: 1.3928886939455343 entropy -3.3262248863369686
epoch: 41, step: 59
	action: tensor([[ 0.3694,  0.3788,  0.0591,  0.2563, -0.6807,  0.1491,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-31.1181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3928886939455343 entropy -2.5098367479338237
epoch: 41, step: 60
	action: tensor([[ 0.1229, -0.2460, -0.0635, -0.1691,  0.2716,  0.1240,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13900146315340223, distance: 1.0618371115041718 entropy -3.3262248863369686
epoch: 41, step: 61
	action: tensor([[ 0.3424,  0.5881,  0.3561,  0.0916, -0.5550, -0.2926, -0.0569]],
       dtype=torch.float64)
	q_value: tensor([[-30.8590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0618371115041718 entropy -2.779124941417063
epoch: 41, step: 62
	action: tensor([[ 0.1373,  0.1087,  0.1173, -0.0249,  0.1442,  0.0372,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48024976841665845, distance: 0.8250001718700668 entropy -3.3262248863369686
epoch: 41, step: 63
	action: tensor([[1.5300e-01, 4.1611e-01, 8.2908e-02, 1.4540e-01, 2.9444e-01, 1.4008e-01,
         2.0298e-04]], dtype=torch.float64)
	q_value: tensor([[-33.9827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8250001718700668 entropy -2.763394459544934
LOSS epoch 41 actor 419.253847094841 critic 445.05074643323644
epoch: 42, step: 0
	action: tensor([[ 0.2518, -0.0463, -0.0177, -0.1922, -0.0045,  0.0281,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8250001718700668 entropy -3.325356707326005
epoch: 42, step: 1
	action: tensor([[-0.0121,  0.2377, -0.2171, -0.1874,  0.2169,  0.0310,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38341928861588204, distance: 0.8985694735001143 entropy -3.325356707326005
epoch: 42, step: 2
	action: tensor([[ 0.0506, -0.2603,  0.0266, -0.2567,  0.0580,  0.0335, -0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-32.2542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02020750225591872, distance: 1.1558485957221525 entropy -3.1775693925833557
epoch: 42, step: 3
	action: tensor([[-0.0982, -0.1800,  0.2791, -0.4819,  0.1617, -0.0986,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[-31.1049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30497776463138226, distance: 1.3072487920347062 entropy -2.75564983429599
epoch: 42, step: 4
	action: tensor([[-0.1918, -0.3110, -0.4198, -0.4002,  0.0191,  0.2121, -0.0510]],
       dtype=torch.float64)
	q_value: tensor([[-31.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2294768012397681, distance: 1.2688692201844272 entropy -2.728497181042176
epoch: 42, step: 5
	action: tensor([[ 0.1219, -0.2566,  0.3090, -0.1437, -0.0633,  0.0005, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-29.0331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08487493312382277, distance: 1.094704539644775 entropy -3.1089814281266617
epoch: 42, step: 6
	action: tensor([[ 0.4749,  0.4580,  0.6246, -0.6167,  0.7106,  0.1557,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[-33.7028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6696277545382963, distance: 0.6577463864816926 entropy -2.5086970298211138
epoch: 42, step: 7
	action: tensor([[-0.8568, -0.0671, -0.4265, -0.5835,  0.3292, -0.4481, -0.3364]],
       dtype=torch.float64)
	q_value: tensor([[-38.5990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7910429021311081, distance: 1.5314742101676557 entropy -2.360812802538143
epoch: 42, step: 8
	action: tensor([[-0.0403,  0.0506,  0.0023,  0.1309,  0.1552,  0.0243,  0.0349]],
       dtype=torch.float64)
	q_value: tensor([[-30.0957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3379613060006753, distance: 0.931104402069242 entropy -3.749285881297954
epoch: 42, step: 9
	action: tensor([[ 0.0471, -0.2053, -0.3293,  0.1247, -0.5639,  0.3634,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[-33.8654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1954765461523682, distance: 1.0264222268022778 entropy -2.9081105702917474
epoch: 42, step: 10
	action: tensor([[ 0.7482,  0.5511, -0.3352,  0.3228,  0.2251,  0.7790,  0.1608]],
       dtype=torch.float64)
	q_value: tensor([[-34.3111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0264222268022778 entropy -2.499901977382678
epoch: 42, step: 11
	action: tensor([[ 0.0030,  0.0731, -0.1948, -0.0665, -0.0102, -0.1345,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2999546791101204, distance: 0.9574580873054558 entropy -3.325356707326005
epoch: 42, step: 12
	action: tensor([[-0.3633, -0.1732, -0.1320,  0.1157,  0.6248,  0.1424,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-32.4723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16536903929222313, distance: 1.2353454819252083 entropy -3.0986815905846394
epoch: 42, step: 13
	action: tensor([[ 0.2467,  0.2247, -0.0444, -0.1835,  0.0500,  0.0865, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-31.7077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6080860855685195, distance: 0.7163941564484251 entropy -3.1638531877119993
epoch: 42, step: 14
	action: tensor([[ 0.5766,  0.4976, -0.0589, -0.1188,  0.0081,  0.0062, -0.0308]],
       dtype=torch.float64)
	q_value: tensor([[-33.4703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7163941564484251 entropy -2.834942245428597
epoch: 42, step: 15
	action: tensor([[-0.5806, -0.1493,  0.3132, -0.4047,  0.2204, -0.1723,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8467614735998659, distance: 1.555113516843731 entropy -3.325356707326005
epoch: 42, step: 16
	action: tensor([[-0.1760, -0.5393,  0.1137, -0.1407, -0.3244,  0.1701,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-32.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4333318796635861, distance: 1.3700299883950036 entropy -2.965714814079551
epoch: 42, step: 17
	action: tensor([[-0.0574, -0.8278, -0.3331, -0.2215,  0.2403, -0.1366,  0.1782]],
       dtype=torch.float64)
	q_value: tensor([[-32.8359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.617652224084072, distance: 1.4554566437105976 entropy -2.4494328113909063
epoch: 42, step: 18
	action: tensor([[-0.4159,  0.0238,  0.2234,  0.1339, -0.0807,  0.3879, -0.0486]],
       dtype=torch.float64)
	q_value: tensor([[-28.8871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014626369573637499, distance: 1.152682675463035 entropy -2.8912692728898937
epoch: 42, step: 19
	action: tensor([[-0.4522,  0.2464, -0.4226, -0.1682, -1.0063,  0.6177,  0.1786]],
       dtype=torch.float64)
	q_value: tensor([[-35.4219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28630861396240626, distance: 1.2978642870802863 entropy -2.5854437154454066
epoch: 42, step: 20
	action: tensor([[ 0.8083,  0.6491, -0.0163, -0.8460, -0.0896,  0.4412,  0.2074]],
       dtype=torch.float64)
	q_value: tensor([[-36.8381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2978642870802863 entropy -2.4668455622365926
epoch: 42, step: 21
	action: tensor([[ 0.0431, -0.2265, -0.3851,  0.2831,  0.0082,  0.0670,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25467689689479545, distance: 0.9879363952632586 entropy -3.325356707326005
epoch: 42, step: 22
	action: tensor([[-1.0078, -0.1044, -0.0517, -0.0170,  0.1248,  0.0949,  0.0653]],
       dtype=torch.float64)
	q_value: tensor([[-32.4750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0481167232208026, distance: 1.637698853940429 entropy -2.928803375737767
epoch: 42, step: 23
	action: tensor([[-0.3546, -0.4961,  0.0689,  0.0103, -0.2192,  0.0627,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-33.1483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48050698450272367, distance: 1.3923932958468126 entropy -3.2635537796341976
epoch: 42, step: 24
	action: tensor([[ 0.5280, -0.7383,  0.3950,  0.4028,  0.8269,  0.0063,  0.1814]],
       dtype=torch.float64)
	q_value: tensor([[-32.6864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2659459058253203, distance: 0.980439333160476 entropy -2.6184621197862157
epoch: 42, step: 25
	action: tensor([[-0.3296,  0.4305,  0.3407,  0.2493, -0.4610,  0.3469, -0.1630]],
       dtype=torch.float64)
	q_value: tensor([[-36.1385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.980439333160476 entropy -2.324650629920421
epoch: 42, step: 26
	action: tensor([[-0.6519, -0.1004,  0.1683,  0.4244, -0.0497, -0.0505,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3790501442459846, distance: 1.343837395745911 entropy -3.325356707326005
epoch: 42, step: 27
	action: tensor([[-0.9061, -0.4916,  0.1315,  0.1287, -0.2355,  0.0598,  0.1779]],
       dtype=torch.float64)
	q_value: tensor([[-35.8025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.145041227169806, distance: 1.6760019326288502 entropy -2.838543325707746
epoch: 42, step: 28
	action: tensor([[-1.3017, -0.3291, -0.5219, -0.4984,  0.1171,  0.0374,  0.2302]],
       dtype=torch.float64)
	q_value: tensor([[-34.5330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4420532161565123, distance: 1.7882747938229704 entropy -2.6586644530581975
epoch: 42, step: 29
	action: tensor([[-0.1424, -0.0050, -0.0289, -0.0308, -0.0653,  0.1168,  0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-31.9953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10727550795481455, distance: 1.0812233552225845 entropy -3.6176036415938784
epoch: 42, step: 30
	action: tensor([[-0.0685, -0.3185, -0.2728,  0.5510, -0.1575,  0.0957,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-33.0598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1536086145110349, distance: 1.0527913560760396 entropy -2.835346016112354
epoch: 42, step: 31
	action: tensor([[ 0.1875,  0.1570,  0.2105,  0.0553, -0.6105,  0.6110,  0.1516]],
       dtype=torch.float64)
	q_value: tensor([[-34.6293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0527913560760396 entropy -2.727899901010017
epoch: 42, step: 32
	action: tensor([[ 0.0284, -0.2065, -0.0276,  0.1625,  0.0040, -0.1340,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21239078558633595, distance: 1.0155752010832921 entropy -3.325356707326005
epoch: 42, step: 33
	action: tensor([[ 0.4860, -0.0691, -0.4891,  0.2831, -0.5788,  0.1056,  0.0752]],
       dtype=torch.float64)
	q_value: tensor([[-33.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6929401785971756, distance: 0.6341152383041566 entropy -2.8219593619572647
epoch: 42, step: 34
	action: tensor([[ 0.2674, -0.4812,  0.6279, -0.1259, -0.3038,  0.3288,  0.0459]],
       dtype=torch.float64)
	q_value: tensor([[-36.1878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12735672501009343, distance: 1.0689935045550907 entropy -2.5757685991691823
epoch: 42, step: 35
	action: tensor([[-1.0187, -0.4603,  0.9260,  0.9254, -1.6615,  0.8182,  0.1167]],
       dtype=torch.float64)
	q_value: tensor([[-36.5662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41979054564119167, distance: 1.363542984397361 entropy -2.0870107227527472
epoch: 42, step: 36
	action: tensor([[-0.4876, -0.9586, -2.2793, -1.3942,  0.5310,  1.1836,  0.8173]],
       dtype=torch.float64)
	q_value: tensor([[-55.5860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08788094632146626, distance: 1.1935685820039839 entropy -1.5810087655631546
epoch: 42, step: 37
	action: tensor([[-0.1669, -0.5312, -0.0765, -0.1109, -0.1456,  0.0861, -0.1494]],
       dtype=torch.float64)
	q_value: tensor([[-38.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3644266569715606, distance: 1.336693361244338 entropy -2.694397020815604
epoch: 42, step: 38
	action: tensor([[ 1.0422,  0.1311,  0.2943, -0.7782, -0.2584, -0.0962,  0.1226]],
       dtype=torch.float64)
	q_value: tensor([[-30.6109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4558336964426212, distance: 0.8441556196852994 entropy -2.733606062185582
epoch: 42, step: 39
	action: tensor([[ 1.1327, -0.2165,  0.5158, -0.2208, -0.2066, -0.3976, -0.2533]],
       dtype=torch.float64)
	q_value: tensor([[-38.0793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3282953436514907, distance: 0.937876972222357 entropy -2.21432422040018
epoch: 42, step: 40
	action: tensor([[ 0.6790,  0.0602, -0.4548,  0.3584, -0.0318, -1.6276, -0.1831]],
       dtype=torch.float64)
	q_value: tensor([[-38.5693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7150000688462868, distance: 0.6109125835731766 entropy -2.159578475928002
epoch: 42, step: 41
	action: tensor([[-0.1036, -0.3652,  0.3012, -0.3991,  0.0920,  0.0794, -0.0461]],
       dtype=torch.float64)
	q_value: tensor([[-38.3203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35856663370165887, distance: 1.3338198162496508 entropy -3.047110234921098
epoch: 42, step: 42
	action: tensor([[-0.3084, -0.1680,  0.5224,  0.0106,  0.0579,  0.0506, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[-31.6178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15762552506555894, distance: 1.2312343983811527 entropy -2.553058565694049
epoch: 42, step: 43
	action: tensor([[-0.5761, -0.5045,  0.2290, -0.3114,  1.0849,  0.4446,  0.1268]],
       dtype=torch.float64)
	q_value: tensor([[-35.2207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.711965309004664, distance: 1.4972839638504403 entropy -2.525992830148443
epoch: 42, step: 44
	action: tensor([[ 0.1325, -0.2801,  0.0211, -0.3778,  0.3213,  0.4803, -0.2598]],
       dtype=torch.float64)
	q_value: tensor([[-33.1476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15117395181350146, distance: 1.0543044570269193 entropy -2.6359461495104624
epoch: 42, step: 45
	action: tensor([[ 0.8723, -0.1800,  0.3027, -1.6069,  0.3489,  0.7590, -0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-30.6469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0333447896345167, distance: 1.163266764912637 entropy -2.546732477290767
epoch: 42, step: 46
	action: tensor([[ 1.8128,  1.0322, -0.7352, -0.8202, -0.4355,  0.4434, -0.5065]],
       dtype=torch.float64)
	q_value: tensor([[-35.0894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.163266764912637 entropy -1.9576114379695961
epoch: 42, step: 47
	action: tensor([[ 0.0653, -0.3640, -0.0861, -0.1197, -0.0721, -0.0397,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027576474286913788, distance: 1.1600154396731874 entropy -3.325356707326005
epoch: 42, step: 48
	action: tensor([[ 0.2798, -0.2184, -0.6810, -0.7504,  0.7865,  0.1352,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-31.4982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24935293291949168, distance: 0.991458611577356 entropy -2.7473300365993962
epoch: 42, step: 49
	action: tensor([[ 0.1846, -0.0949, -0.2382,  0.4817, -0.2889, -0.2875, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-28.1254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5433696647894333, distance: 0.773284086749881 entropy -2.9131278009830806
epoch: 42, step: 50
	action: tensor([[-0.1562,  0.1786, -0.3445,  0.3554, -0.2339,  0.4628,  0.1206]],
       dtype=torch.float64)
	q_value: tensor([[-35.0416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.773284086749881 entropy -2.8457425125941325
epoch: 42, step: 51
	action: tensor([[-0.0508, -0.0318, -0.0745,  0.2684,  0.1048, -0.1091,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950001197946053, distance: 0.9608403104306158 entropy -3.325356707326005
epoch: 42, step: 52
	action: tensor([[-0.4448, -0.0688, -0.5576,  0.6140, -0.0460,  0.3707,  0.0679]],
       dtype=torch.float64)
	q_value: tensor([[-34.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.211406371791526, distance: 1.2595100003591901 entropy -2.953842840405778
epoch: 42, step: 53
	action: tensor([[-0.0439, -0.4940,  0.1469,  0.0285,  0.1336,  0.1667,  0.1073]],
       dtype=torch.float64)
	q_value: tensor([[-34.8210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10211900590756828, distance: 1.2013538352364606 entropy -3.012299028479138
epoch: 42, step: 54
	action: tensor([[-0.4648, -0.1921,  0.1297, -0.3330, -0.3588, -0.3467,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-33.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6108702129639012, distance: 1.4524024362895334 entropy -3.325356707326005
epoch: 42, step: 55
	action: tensor([[-0.3069,  0.2183, -0.1813, -0.0419,  0.1472, -0.0088,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[-32.7960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03687807158609957, distance: 1.1230454399835013 entropy -2.966919921673838
epoch: 42, step: 56
	action: tensor([[-0.4583,  0.0760, -0.0594, -0.4303, -0.2711,  0.0380,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-32.7989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3987794221258678, distance: 1.3534160131262676 entropy -3.306134037636345
epoch: 42, step: 57
	action: tensor([[ 0.4192,  0.0897,  0.4315, -0.0543,  0.1698,  0.1289,  0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-32.3308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6826811347976005, distance: 0.6446212819785048 entropy -3.0592110324122137
epoch: 42, step: 58
	action: tensor([[-0.0733, -0.1796, -0.1934, -0.3050, -0.4967,  0.3862, -0.0574]],
       dtype=torch.float64)
	q_value: tensor([[-36.9183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06647853445447138, distance: 1.1817694330267017 entropy -2.419172861172271
epoch: 42, step: 59
	action: tensor([[ 0.2013,  0.1002,  0.3744, -1.1298,  0.7274, -0.0674,  0.1303]],
       dtype=torch.float64)
	q_value: tensor([[-32.4757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13269577685064604, distance: 1.2179047770035232 entropy -2.544306667849568
epoch: 42, step: 60
	action: tensor([[-0.4493, -0.4349, -0.0821, -0.0362,  0.7953,  0.0275, -0.3637]],
       dtype=torch.float64)
	q_value: tensor([[-34.4824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6158698135579483, distance: 1.4546545750680802 entropy -2.491829462954478
epoch: 42, step: 61
	action: tensor([[ 3.8323e-01, -7.8490e-02, -6.1791e-02,  2.3071e-04,  5.5173e-01,
         -1.9977e-01, -5.4386e-02]], dtype=torch.float64)
	q_value: tensor([[-31.6751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47030994358925726, distance: 0.832851559693134 entropy -3.10292161564643
epoch: 42, step: 62
	action: tensor([[-0.5844, -0.4010,  0.1449, -0.6750, -0.1734,  0.3911, -0.1116]],
       dtype=torch.float64)
	q_value: tensor([[-31.8032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9426788430747033, distance: 1.5949871866923389 entropy -2.917229644394656
epoch: 42, step: 63
	action: tensor([[ 0.3097, -0.3180, -0.6580, -0.1350,  0.7629,  0.3037,  0.0876]],
       dtype=torch.float64)
	q_value: tensor([[-32.3435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3373098218504974, distance: 0.9315624195587972 entropy -2.542615685781493
LOSS epoch 42 actor 511.4618383498041 critic 227.28295084511004
epoch: 43, step: 0
	action: tensor([[ 0.0491,  0.0558, -0.3259, -0.5611,  0.1170,  0.1208, -0.1679]],
       dtype=torch.float64)
	q_value: tensor([[-30.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22665993994863987, distance: 1.0063335392484969 entropy -2.9485984180737135
epoch: 43, step: 1
	action: tensor([[ 0.1984, -0.1484,  0.0158, -0.4362,  0.2532, -0.0291, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-29.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08212402564622423, distance: 1.096348670749611 entropy -3.149404528005511
epoch: 43, step: 2
	action: tensor([[ 0.1792,  0.1193,  0.1263, -0.2730, -0.1229,  0.1269, -0.1059]],
       dtype=torch.float64)
	q_value: tensor([[-30.9420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4098022227591933, distance: 0.8791348129151706 entropy -2.8108142856653884
epoch: 43, step: 3
	action: tensor([[ 0.8493,  0.5077,  0.8445, -0.3052,  0.3261,  0.1483,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-34.3829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9509088976554769, distance: 0.25354677181396973 entropy -2.6496185786744304
epoch: 43, step: 4
	action: tensor([[ 0.3537, -0.6089,  0.9093, -0.0249, -0.2965,  1.4529, -0.2564]],
       dtype=torch.float64)
	q_value: tensor([[-43.5444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.560339663533576, distance: 0.7587790601091035 entropy -2.135998750382649
epoch: 43, step: 5
	action: tensor([[ 0.3627, -0.3956, -0.4000,  0.6862,  0.1522, -0.2820,  0.1594]],
       dtype=torch.float64)
	q_value: tensor([[-42.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4492396996144362, distance: 0.8492547944186416 entropy -1.6484317268328634
epoch: 43, step: 6
	action: tensor([[-0.1662,  0.3943,  0.5027, -0.4255,  0.2639,  0.1770,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-35.7446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12038795270884817, distance: 1.0732534117200656 entropy -2.8597110274660595
epoch: 43, step: 7
	action: tensor([[ 0.4559,  0.2303, -0.2013, -0.0739, -0.0885,  0.5607, -0.0749]],
       dtype=torch.float64)
	q_value: tensor([[-37.0907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0732534117200656 entropy -2.6199774176854556
epoch: 43, step: 8
	action: tensor([[ 0.1538, -0.0595, -0.0715,  0.2828,  0.1949, -0.0271,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5214781850635528, distance: 0.7916032414780225 entropy -3.3245173192316804
epoch: 43, step: 9
	action: tensor([[-0.2671,  0.0841, -0.2784, -0.0891,  0.4566,  0.1995,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[-35.2148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10355690648235116, distance: 1.0834729073236278 entropy -2.8367681735376276
epoch: 43, step: 10
	action: tensor([[ 0.0143,  0.0332, -0.1850,  0.0994,  0.2558, -0.0955, -0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-32.1143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35598575015141765, distance: 0.9183419676059213 entropy -3.3158604755950543
epoch: 43, step: 11
	action: tensor([[-0.1391,  0.0316, -0.2946,  0.2289, -0.2048, -0.0517,  0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-33.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23575824377052967, distance: 1.0003962950150458 entropy -3.176440088686263
epoch: 43, step: 12
	action: tensor([[-0.4024, -0.3208,  0.3447, -0.0153, -0.4000,  0.2163,  0.1018]],
       dtype=torch.float64)
	q_value: tensor([[-34.5383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36442957989029323, distance: 1.3366947929973576 entropy -3.0662689788122264
epoch: 43, step: 13
	action: tensor([[-0.5834, -0.1587,  0.0573,  0.2990, -0.5662,  0.1866,  0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-36.5449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3362771451501312, distance: 1.3228328265898985 entropy -2.361193188801749
epoch: 43, step: 14
	action: tensor([[-1.1953,  0.2813, -0.0962,  0.4553,  0.7558,  0.9683,  0.2785]],
       dtype=torch.float64)
	q_value: tensor([[-38.3860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12858914888924633, distance: 1.2156949941951614 entropy -2.4802530061449124
epoch: 43, step: 15
	action: tensor([[ 0.0354, -0.3560,  0.0936,  0.0791, -0.0260,  0.3160, -0.0273]],
       dtype=torch.float64)
	q_value: tensor([[-41.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19630285235343947, distance: 1.0258949849086754 entropy -2.863536681145883
epoch: 43, step: 16
	action: tensor([[ 0.3487, -0.4687,  0.5701, -0.3238,  0.6278,  0.7990,  0.1144]],
       dtype=torch.float64)
	q_value: tensor([[-34.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19721682634631532, distance: 1.0253114889585597 entropy -2.4809801356696504
epoch: 43, step: 17
	action: tensor([[-1.2128,  0.3529,  1.0209,  1.5806,  0.6818,  1.2893, -0.2610]],
       dtype=torch.float64)
	q_value: tensor([[-35.1339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0253114889585597 entropy -2.0934209583458374
epoch: 43, step: 18
	action: tensor([[-0.2894, -0.1552,  0.0939,  0.3832,  0.1663,  0.0940,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.057749816613912985, distance: 1.1108100710732367 entropy -3.3245173192316804
epoch: 43, step: 19
	action: tensor([[ 0.3161, -0.5154, -0.2317, -0.0335,  0.3868,  0.2591,  0.1142]],
       dtype=torch.float64)
	q_value: tensor([[-35.8631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14781680506708395, distance: 1.0563873113476048 entropy -2.7979815711469427
epoch: 43, step: 20
	action: tensor([[ 0.1345,  0.3133,  0.0500, -0.5214, -0.0917, -0.0380, -0.1035]],
       dtype=torch.float64)
	q_value: tensor([[-31.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37983453967403547, distance: 0.9011777919262458 entropy -2.639643815989107
epoch: 43, step: 21
	action: tensor([[ 0.0533, -0.3136,  0.0363, -1.0165, -0.5083,  0.2482, -0.0502]],
       dtype=torch.float64)
	q_value: tensor([[-34.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41396926500203235, distance: 1.360744783168674 entropy -2.8907580866166533
epoch: 43, step: 22
	action: tensor([[ 0.1480, -0.2303,  0.2159,  0.6932,  0.5734,  0.8774, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-33.1674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.360744783168674 entropy -2.396071589429766
epoch: 43, step: 23
	action: tensor([[-0.1753,  0.1266,  0.1038,  0.2115,  0.2447,  0.1868,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31806931378557, distance: 0.9449891238655251 entropy -3.3245173192316804
epoch: 43, step: 24
	action: tensor([[-0.7869, -0.7428,  0.1331,  0.0734, -0.0163,  0.2087,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-36.3293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.098689214774688, distance: 1.6577947448583086 entropy -2.809745210581304
epoch: 43, step: 25
	action: tensor([[ 0.1318, -0.3591, -0.0510,  0.1049, -0.1153, -0.0660,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16380543474182374, distance: 1.0464304376980038 entropy -3.3245173192316804
epoch: 43, step: 26
	action: tensor([[ 0.7739,  0.2252, -0.3111,  0.2200, -1.6873,  0.3405,  0.0799]],
       dtype=torch.float64)
	q_value: tensor([[-33.6276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9097405006160075, distance: 0.343797847668835 entropy -2.6764769953688523
epoch: 43, step: 27
	action: tensor([[-0.7506, -1.5701,  0.3351,  0.8793,  0.2539,  0.2872,  0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-47.6106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.343797847668835 entropy -2.0169150059432925
epoch: 43, step: 28
	action: tensor([[ 0.2089,  0.3271, -0.0620, -0.3738,  0.5285, -0.0833,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5407590802621705, distance: 0.7754913934512642 entropy -3.3245173192316804
epoch: 43, step: 29
	action: tensor([[-0.1259,  0.0017,  0.0590,  0.1834,  0.2695,  0.1474,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2727215246231385, distance: 0.9759039154162107 entropy -3.3245173192316804
epoch: 43, step: 30
	action: tensor([[ 0.1513,  0.1718, -0.4666, -0.3299,  0.1016,  0.3161,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[-35.2368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5241628563709624, distance: 0.7893795354722332 entropy -2.823278353043062
epoch: 43, step: 31
	action: tensor([[-0.3111, -0.2803,  0.0538,  0.1748,  0.1027,  0.0404, -0.0684]],
       dtype=torch.float64)
	q_value: tensor([[-31.7341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2002179439466829, distance: 1.2536801540961118 entropy -3.0326672445108622
epoch: 43, step: 32
	action: tensor([[-0.1146, -0.5687,  0.3834, -0.0293, -0.0365,  0.3315,  0.1147]],
       dtype=torch.float64)
	q_value: tensor([[-33.5257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2220508438492903, distance: 1.2650314739962993 entropy -2.8543319997555336
epoch: 43, step: 33
	action: tensor([[ 0.9004, -0.8270,  0.1070, -0.2921, -0.6235, -0.0530,  0.1310]],
       dtype=torch.float64)
	q_value: tensor([[-34.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36983363357617827, distance: 1.3393392797910684 entropy -2.2899321478226993
epoch: 43, step: 34
	action: tensor([[ 0.2499,  0.1756, -0.5463, -0.3637,  0.6187,  1.0162, -0.0990]],
       dtype=torch.float64)
	q_value: tensor([[-36.7249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7887821178147464, distance: 0.5259228438866919 entropy -2.110347872357228
epoch: 43, step: 35
	action: tensor([[ 0.1569, -0.0109, -0.7905, -0.1681, -0.5007,  0.1102, -0.2243]],
       dtype=torch.float64)
	q_value: tensor([[-32.6070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3985293097439184, distance: 0.8874909390234819 entropy -2.6697552052488507
epoch: 43, step: 36
	action: tensor([[-0.0843, -0.0156,  0.0416,  0.1716,  0.2385,  0.1712,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-31.6077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053906121430725, distance: 0.9537334561704354 entropy -3.1072148627262677
epoch: 43, step: 37
	action: tensor([[ 0.4329, -0.3274,  0.7975, -0.2542, -0.1056,  0.3576,  0.0388]],
       dtype=torch.float64)
	q_value: tensor([[-34.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966053614197517, distance: 0.9597457997204313 entropy -2.819597704050537
epoch: 43, step: 38
	action: tensor([[ 0.5430,  0.3754,  0.7707, -0.2622, -0.3948,  0.6043, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-37.9130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8898727483136468, distance: 0.3797555187106218 entropy -2.048460357618693
epoch: 43, step: 39
	action: tensor([[-0.9766, -1.0559, -0.3270, -1.6944, -0.4462,  0.4718,  0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-43.0546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3008166763685023, distance: 1.3051629631122583 entropy -1.9686626677240429
epoch: 43, step: 40
	action: tensor([[ 0.6578,  0.2523,  0.4884,  1.1887,  0.1753,  0.2530, -0.0705]],
       dtype=torch.float64)
	q_value: tensor([[-36.3259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8847372445275503, distance: 0.3885090967770655 entropy -2.504004847454354
epoch: 43, step: 41
	action: tensor([[-0.8507, -0.7352, -0.6745,  0.8944,  0.3316,  0.7550,  0.0803]],
       dtype=torch.float64)
	q_value: tensor([[-49.2132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3885090967770655 entropy -2.1741510347224677
epoch: 43, step: 42
	action: tensor([[-0.3079,  0.0214, -0.1158,  0.2382, -0.1507,  0.0292,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011343605537719292, distance: 1.1378352475445674 entropy -3.3245173192316804
epoch: 43, step: 43
	action: tensor([[-0.3955,  0.1433, -0.0470,  0.3035, -0.0543,  0.2463,  0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-35.5241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0701055708030962, distance: 1.1035029950888993 entropy -2.917479332224076
epoch: 43, step: 44
	action: tensor([[ 0.4256,  0.5293, -0.0106, -0.1311,  0.1169,  0.6076,  0.1316]],
       dtype=torch.float64)
	q_value: tensor([[-37.0778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44335660955398337, distance: 0.8537785146793314 entropy -2.824964255767529
epoch: 43, step: 45
	action: tensor([[ 0.8620, -0.5111,  0.1837,  0.1639, -0.5462, -0.4202, -0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-38.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8537785146793314 entropy -2.484530614204714
epoch: 43, step: 46
	action: tensor([[ 0.3473, -0.2861, -0.1961,  0.4394, -0.0702,  0.0448,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5761004568368199, distance: 0.7450547151513052 entropy -3.3245173192316804
epoch: 43, step: 47
	action: tensor([[ 0.1721, -0.7430, -1.0482, -0.4322, -0.4244,  0.7625,  0.0666]],
       dtype=torch.float64)
	q_value: tensor([[-35.8896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27995360728856067, distance: 1.2946542686640103 entropy -2.627218078468466
epoch: 43, step: 48
	action: tensor([[-0.4155, -0.2350, -0.3922, -0.1146,  0.2488, -0.2269, -0.0499]],
       dtype=torch.float64)
	q_value: tensor([[-32.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39156694335070474, distance: 1.3499222170843443 entropy -2.4994347700929582
epoch: 43, step: 49
	action: tensor([[ 0.0565, -0.1214, -0.0216, -0.3373, -0.1366,  0.0540,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-29.6617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11587463583260549, distance: 1.0760033371264457 entropy -3.673856543956156
epoch: 43, step: 50
	action: tensor([[ 0.3262, -0.9017,  0.1279, -0.4207,  0.3185,  0.4321,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[-32.4331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45124418079293127, distance: 1.3785640185481678 entropy -2.7369840879780476
epoch: 43, step: 51
	action: tensor([[ 0.3769, -0.1473, -0.3043,  0.8455,  1.3948,  0.3818, -0.1608]],
       dtype=torch.float64)
	q_value: tensor([[-31.5152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3785640185481678 entropy -2.229049519698114
epoch: 43, step: 52
	action: tensor([[-0.0692, -0.1150, -0.2349,  0.2855,  0.1980,  0.1548,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25505487783753333, distance: 0.9876858539313049 entropy -3.3245173192316804
epoch: 43, step: 53
	action: tensor([[-0.9856, -0.0183,  0.7204, -0.2104, -0.5997, -0.0510,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-34.2797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1780095583142507, distance: 1.6888325232305725 entropy -2.9360514663234043
epoch: 43, step: 54
	action: tensor([[-0.4233, -0.1403, -0.1051,  0.2461,  0.2383,  0.5656,  0.3219]],
       dtype=torch.float64)
	q_value: tensor([[-39.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022779938757167217, distance: 1.1573049049331599 entropy -2.320597145373164
epoch: 43, step: 55
	action: tensor([[ 0.7827, -0.1220, -0.9773, -0.3725, -0.1865,  0.1767,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-35.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5629090341097749, distance: 0.756558662769987 entropy -2.6913273551149177
epoch: 43, step: 56
	action: tensor([[ 0.4086, -0.3980,  0.0201, -0.4153, -0.2774, -0.0128, -0.1484]],
       dtype=torch.float64)
	q_value: tensor([[-31.8874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019083231785025134, distance: 1.155211546568618 entropy -2.7597402246565257
epoch: 43, step: 57
	action: tensor([[-0.6465, -0.1157, -0.1456,  0.8672,  0.0614,  0.5378, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-32.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014998892698779409, distance: 1.1528942615003719 entropy -2.4847313841300784
epoch: 43, step: 58
	action: tensor([[ 0.0854,  0.1035,  0.1475, -1.3747, -0.0579,  0.2803,  0.1972]],
       dtype=torch.float64)
	q_value: tensor([[-39.8565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14450764082019063, distance: 1.224238523533616 entropy -2.6814749479622697
epoch: 43, step: 59
	action: tensor([[ 0.9084,  0.6868,  0.2639,  0.1353,  0.1737,  0.6867, -0.2254]],
       dtype=torch.float64)
	q_value: tensor([[-34.7760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9180541569959539, distance: 0.3275820956687765 entropy -2.4523316424616586
epoch: 43, step: 60
	action: tensor([[-0.1757, -0.3828,  0.4347, -1.0064,  0.6608, -0.0190, -0.1384]],
       dtype=torch.float64)
	q_value: tensor([[-43.4974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7870894593361641, distance: 1.5297830333379818 entropy -2.2010900181609134
epoch: 43, step: 61
	action: tensor([[-0.3074,  0.3714, -0.4630,  0.2070,  0.7531,  0.2354, -0.2836]],
       dtype=torch.float64)
	q_value: tensor([[-32.6874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5297830333379818 entropy -2.5173317794842087
epoch: 43, step: 62
	action: tensor([[ 0.0036, -0.0379,  0.0271,  0.0276,  0.4526, -0.1750,  0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-35.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22166260629990575, distance: 1.009579773680294 entropy -3.3245173192316804
epoch: 43, step: 63
	action: tensor([[-0.0892,  0.2289, -0.0718,  0.2713, -0.1597, -0.2030, -0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-33.4728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.009579773680294 entropy -3.0192867449751293
LOSS epoch 43 actor 539.8166252606065 critic 263.0085184660446
epoch: 44, step: 0
	action: tensor([[0.2624, 0.0167, 0.0888, 0.0062, 0.4110, 0.0639, 0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.539878784749112, distance: 0.7762342875816692 entropy -3.322718201048031
epoch: 44, step: 1
	action: tensor([[-0.3874,  0.0694, -0.3382,  0.0972,  0.0709, -0.3045, -0.0815]],
       dtype=torch.float64)
	q_value: tensor([[-34.1275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16022179803788106, distance: 1.2326143050167724 entropy -2.7530389846138794
epoch: 44, step: 2
	action: tensor([[-0.2310, -0.1727,  0.3284,  0.1464, -0.0269,  0.1137,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[-32.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00953509634405536, distance: 1.1388754700735197 entropy -3.6827987105244895
epoch: 44, step: 3
	action: tensor([[ 0.9304, -0.4392, -0.1795, -0.2988, -0.1038,  0.4401,  0.1475]],
       dtype=torch.float64)
	q_value: tensor([[-35.3367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28981449311323815, distance: 0.9643675666252062 entropy -2.5656880594252582
epoch: 44, step: 4
	action: tensor([[ 0.4278, -0.2660,  0.6483,  0.0858, -0.4263, -0.0092, -0.1682]],
       dtype=torch.float64)
	q_value: tensor([[-33.9354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44875587870522227, distance: 0.8496277307296958 entropy -2.2030307577071384
epoch: 44, step: 5
	action: tensor([[ 1.4153,  0.1594, -0.4470,  0.2397, -1.8453,  0.9001,  0.1149]],
       dtype=torch.float64)
	q_value: tensor([[-37.8617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7904474976397022, distance: 0.5238453810545993 entropy -2.174232921310916
epoch: 44, step: 6
	action: tensor([[-0.3161,  0.6943,  2.0843, -1.0715,  1.2913,  0.0438, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-51.2782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5238453810545993 entropy -1.7382887992489675
epoch: 44, step: 7
	action: tensor([[-0.1633, -0.1914, -0.1638,  0.1859, -0.2582,  0.0196,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5238453810545993 entropy -3.322718201048031
epoch: 44, step: 8
	action: tensor([[-0.1708, -0.2082,  0.0474,  0.0596,  0.2450,  0.1207,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017353225100815495, distance: 1.1542305802773336 entropy -3.322718201048031
epoch: 44, step: 9
	action: tensor([[-0.3650, -0.4614, -0.1833, -0.2711,  0.2710,  0.2938,  0.0299]],
       dtype=torch.float64)
	q_value: tensor([[-32.8307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5253995084022847, distance: 1.4133460006437086 entropy -2.8398680185822935
epoch: 44, step: 10
	action: tensor([[-0.4120,  0.3734, -0.1871,  0.4993,  0.4903,  0.1839, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[-30.1775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4133460006437086 entropy -2.894554074428871
epoch: 44, step: 11
	action: tensor([[ 0.0859,  0.1360, -0.1115, -0.1167,  0.2712, -0.3285,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3523904796468932, distance: 0.9209017654763987 entropy -3.322718201048031
epoch: 44, step: 12
	action: tensor([[ 0.0034,  0.0425,  0.4574,  0.2355,  0.2301,  0.2677, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-33.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5638553627649099, distance: 0.7557392211223432 entropy -3.188599882617189
epoch: 44, step: 13
	action: tensor([[ 0.6395,  0.2363,  0.2728, -0.3185, -0.2447,  0.3863,  0.0631]],
       dtype=torch.float64)
	q_value: tensor([[-37.3800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8191449549927386, distance: 0.4866559160866713 entropy -2.4898641068063534
epoch: 44, step: 14
	action: tensor([[ 0.1677, -0.0815,  0.6101, -0.2303,  1.1863, -0.1844, -0.0568]],
       dtype=torch.float64)
	q_value: tensor([[-37.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4866559160866713 entropy -2.237366441515682
epoch: 44, step: 15
	action: tensor([[-0.4446, -0.1676, -0.0192,  0.0588, -0.0084, -0.0881,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3989132133731619, distance: 1.3534807377297149 entropy -3.322718201048031
epoch: 44, step: 16
	action: tensor([[-0.4858, -0.4337,  0.2323,  0.1248, -0.0715,  0.4069,  0.1028]],
       dtype=torch.float64)
	q_value: tensor([[-32.9883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4258035180059807, distance: 1.3664273125744026 entropy -3.0167683709982605
epoch: 44, step: 17
	action: tensor([[-0.1191,  0.7606,  0.2397, -0.6465, -0.7213,  0.3975,  0.2003]],
       dtype=torch.float64)
	q_value: tensor([[-34.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3664273125744026 entropy -2.4399903993698904
epoch: 44, step: 18
	action: tensor([[-0.2313, -0.2117, -0.0034, -0.0392,  0.2064, -0.0132,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1952224300268257, distance: 1.251068417211755 entropy -3.322718201048031
epoch: 44, step: 19
	action: tensor([[-0.0138,  0.0095, -0.1549, -0.0486, -0.3006,  0.1267,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-32.0985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24392781000189612, distance: 0.9950349269727583 entropy -2.9482338319758226
epoch: 44, step: 20
	action: tensor([[ 0.4350, -0.6053, -0.2867, -0.0986, -0.3726,  0.4391,  0.0986]],
       dtype=torch.float64)
	q_value: tensor([[-33.7394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07413388920665542, distance: 1.1011102040490242 entropy -2.7704832105139485
epoch: 44, step: 21
	action: tensor([[-0.5564,  0.2098, -0.2331,  0.7896,  0.6359, -0.0151,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[-33.1096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1011102040490242 entropy -2.2931495364830794
epoch: 44, step: 22
	action: tensor([[-0.0498,  0.0652,  0.5167,  0.0461, -0.0184, -0.1522,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26235237259830013, distance: 0.9828362545171 entropy -3.322718201048031
epoch: 44, step: 23
	action: tensor([[-0.8863, -0.3429,  0.9984, -0.2313, -0.4024,  0.2458,  0.0782]],
       dtype=torch.float64)
	q_value: tensor([[-36.8791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.166740606301084, distance: 1.6844578745039525 entropy -2.582091143238182
epoch: 44, step: 24
	action: tensor([[-1.1272,  0.4101, -1.3407,  0.0897,  0.1956,  0.2817,  0.3324]],
       dtype=torch.float64)
	q_value: tensor([[-38.9007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8894676609061705, distance: 1.5729916775828052 entropy -2.0536859788066444
epoch: 44, step: 25
	action: tensor([[ 0.1421,  0.0294, -0.1552,  0.4578, -0.0851,  0.0559,  0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-36.4058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5729916775828052 entropy -3.5823858686621457
epoch: 44, step: 26
	action: tensor([[-0.1073, -0.0686, -0.0282,  0.1681, -0.1074, -0.1673,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1533916242636526, distance: 1.0529263000398426 entropy -3.322718201048031
epoch: 44, step: 27
	action: tensor([[ 0.3844,  0.2922, -0.5164, -0.1884,  0.1879,  0.4169,  0.1012]],
       dtype=torch.float64)
	q_value: tensor([[-34.3846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7974236623108875, distance: 0.5150519680384563 entropy -2.89705277827574
epoch: 44, step: 28
	action: tensor([[-0.1513, -0.3464, -0.0171, -0.4919,  0.2119,  0.2152, -0.1070]],
       dtype=torch.float64)
	q_value: tensor([[-33.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.341902514304109, distance: 1.3256142884471838 entropy -2.869565310239913
epoch: 44, step: 29
	action: tensor([[-0.2561, -0.2290, -0.1453, -0.1001,  0.0306,  0.2236, -0.0593]],
       dtype=torch.float64)
	q_value: tensor([[-29.9276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1871982892594939, distance: 1.2468618132987856 entropy -2.7752459236893086
epoch: 44, step: 30
	action: tensor([[ 0.1693,  0.0450,  0.0184, -0.6085,  0.2635, -0.0952,  0.0674]],
       dtype=torch.float64)
	q_value: tensor([[-31.3848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1477263814380193, distance: 1.0564433555416028 entropy -2.9129417737370806
epoch: 44, step: 31
	action: tensor([[ 0.6032, -0.1745,  0.0510, -0.5089, -0.5610,  0.1270, -0.1436]],
       dtype=torch.float64)
	q_value: tensor([[-31.5958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27895522839334386, distance: 0.971712544352548 entropy -2.866888076610771
epoch: 44, step: 32
	action: tensor([[-0.1928, -0.8134, -0.3767, -0.1980, -0.3860,  0.5935, -0.0299]],
       dtype=torch.float64)
	q_value: tensor([[-33.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6350915472720815, distance: 1.463280988053124 entropy -2.3101023243579646
epoch: 44, step: 33
	action: tensor([[ 0.5522, -0.3404, -0.5678, -0.7724,  0.3300,  0.2827,  0.1336]],
       dtype=torch.float64)
	q_value: tensor([[-31.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1413825870966513, distance: 1.0603678196553132 entropy -2.3879632628415424
epoch: 44, step: 34
	action: tensor([[-0.6769,  0.5559,  0.0797, -0.0203,  0.9546,  0.0224, -0.2456]],
       dtype=torch.float64)
	q_value: tensor([[-29.1434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0603678196553132 entropy -2.6586131896506395
epoch: 44, step: 35
	action: tensor([[-0.1817, -0.0464, -0.0065,  0.2187,  0.0933, -0.2838,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05851428854345142, distance: 1.1103593651756307 entropy -3.322718201048031
epoch: 44, step: 36
	action: tensor([[ 0.2853, -0.0421, -0.0650,  0.5003, -0.2675,  0.2394,  0.0690]],
       dtype=torch.float64)
	q_value: tensor([[-34.2724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6788169208250432, distance: 0.6485344071330884 entropy -3.063173881945047
epoch: 44, step: 37
	action: tensor([[ 2.4349e-01,  6.2680e-05, -2.9094e-01,  9.2399e-01, -3.9085e-01,
          3.3357e-01,  1.2199e-01]], dtype=torch.float64)
	q_value: tensor([[-38.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6485344071330884 entropy -2.486294275167628
epoch: 44, step: 38
	action: tensor([[-0.2057, -0.0368,  0.1665,  0.1001, -0.0640,  0.0239,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061871924523839716, distance: 1.1083776503071148 entropy -3.322718201048031
epoch: 44, step: 39
	action: tensor([[-0.0661,  0.1287, -0.4479, -0.1826, -0.0743,  0.1832,  0.1169]],
       dtype=torch.float64)
	q_value: tensor([[-34.8013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2656523384523948, distance: 0.9806353651901349 entropy -2.746260279140382
epoch: 44, step: 40
	action: tensor([[ 0.3517, -0.2394, -0.1379,  0.5101,  0.4747, -0.0331,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[-32.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6089268847844953, distance: 0.7156252796212099 entropy -3.1476326140802438
epoch: 44, step: 41
	action: tensor([[-0.0601,  0.2588, -0.2544, -0.2923, -0.1067,  0.2208, -0.0149]],
       dtype=torch.float64)
	q_value: tensor([[-34.9637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3226998589827019, distance: 0.9417752572735862 entropy -2.780915714042831
epoch: 44, step: 42
	action: tensor([[ 0.3768, -0.4302, -0.0986,  0.0715, -0.0987,  0.2398,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-32.6609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31360205431903954, distance: 0.948079335651685 entropy -3.0344418467318985
epoch: 44, step: 43
	action: tensor([[-1.0197,  0.3228, -0.6719, -0.4678,  0.2524, -0.0993,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[-33.1704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7297893453272002, distance: 1.5050582273370785 entropy -2.4541547220142554
epoch: 44, step: 44
	action: tensor([[-0.0291, -0.0522, -0.0066, -0.2141, -0.1006,  0.0701,  0.0462]],
       dtype=torch.float64)
	q_value: tensor([[-32.0289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09884857596171481, distance: 1.0863145086235078 entropy -3.7183973916184105
epoch: 44, step: 45
	action: tensor([[ 0.0668, -0.1860, -0.2815, -0.5438, -0.1426,  0.1957,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-32.6652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014813131174741034, distance: 1.135836970809074 entropy -2.7867039501898674
epoch: 44, step: 46
	action: tensor([[ 0.4283, -0.2775, -0.2250,  0.0049,  0.6922,  0.7524, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-30.6322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6448904026734514, distance: 0.6819270190018326 entropy -2.8092317752171
epoch: 44, step: 47
	action: tensor([[ 0.2165, -0.1719, -0.5881, -0.4961, -0.3801, -0.3864, -0.2121]],
       dtype=torch.float64)
	q_value: tensor([[-32.9137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2506308427695516, distance: 0.9906143170498813 entropy -2.4990249042220727
epoch: 44, step: 48
	action: tensor([[ 0.1441,  0.1980, -0.1402, -0.2235, -0.2367,  0.1569, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-29.3824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.478173890577145, distance: 0.8266460518042474 entropy -3.300045789536505
epoch: 44, step: 49
	action: tensor([[ 0.4816, -0.2322,  0.2195, -0.9427, -0.1972,  0.0410,  0.0292]],
       dtype=torch.float64)
	q_value: tensor([[-33.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12953678936796997, distance: 1.2162052773463254 entropy -2.7883689236784313
epoch: 44, step: 50
	action: tensor([[ 0.2291, -0.1802, -0.2452, -0.0622, -0.2698, -0.6740, -0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-32.6112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25427774576206996, distance: 0.9882009001121946 entropy -2.345254926445187
epoch: 44, step: 51
	action: tensor([[-0.2568, -0.2978, -0.0212,  0.1830, -0.2175, -0.0143,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-31.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1781670449014583, distance: 1.2421102013461578 entropy -3.1593099302490217
epoch: 44, step: 52
	action: tensor([[-0.6656, -0.2265, -0.3122, -0.2442,  0.5472,  0.3889,  0.1654]],
       dtype=torch.float64)
	q_value: tensor([[-33.5696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6503766611191704, distance: 1.470104577889975 entropy -2.754535927449499
epoch: 44, step: 53
	action: tensor([[ 0.2055, -0.0101,  0.1207,  0.1729,  0.0511, -0.0906, -0.0620]],
       dtype=torch.float64)
	q_value: tensor([[-31.2529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5159293254934483, distance: 0.7961796627842129 entropy -3.220973021416697
epoch: 44, step: 54
	action: tensor([[ 0.8683, -0.0061, -0.4527, -0.1184, -0.2028,  0.0181,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-35.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7027350591395145, distance: 0.6239194711970024 entropy -2.7376363184070867
epoch: 44, step: 55
	action: tensor([[-0.6787, -0.0029, -0.1966, -0.4007,  0.2675,  0.5746, -0.1171]],
       dtype=torch.float64)
	q_value: tensor([[-34.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5837377659274521, distance: 1.4401188327919887 entropy -2.5860979820875007
epoch: 44, step: 56
	action: tensor([[-2.1355e-01,  1.0742e-04, -9.3870e-03, -3.9242e-02, -3.2664e-01,
          4.5064e-03, -1.8163e-02]], dtype=torch.float64)
	q_value: tensor([[-32.1362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048651622907670555, distance: 1.171850770529416 entropy -3.1010425316556294
epoch: 44, step: 57
	action: tensor([[-0.3349,  0.5063,  0.2903, -0.7238, -0.1621,  0.3953,  0.1311]],
       dtype=torch.float64)
	q_value: tensor([[-33.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13597081161116198, distance: 1.2196642085918237 entropy -2.8347321324142696
epoch: 44, step: 58
	action: tensor([[-0.2260,  0.2942, -0.0714, -0.3284, -0.1842,  0.1755,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[-36.8142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.062480461474653914, distance: 1.1080181054916451 entropy -2.5792574182764025
epoch: 44, step: 59
	action: tensor([[ 0.5250, -0.8047,  0.3000, -0.1753, -0.0463,  0.2829,  0.0482]],
       dtype=torch.float64)
	q_value: tensor([[-33.6374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19843654902268448, distance: 1.2527494361502445 entropy -2.9726623044624616
epoch: 44, step: 60
	action: tensor([[ 0.7868, -0.3912, -0.8802, -0.5439, -0.2391, -0.0283, -0.0531]],
       dtype=torch.float64)
	q_value: tensor([[-33.6107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1780648967972337, distance: 1.0374697857808453 entropy -2.1433263227929578
epoch: 44, step: 61
	action: tensor([[ 0.5077,  0.1935, -0.6404, -0.4974, -0.1125, -0.3727, -0.1655]],
       dtype=torch.float64)
	q_value: tensor([[-30.3974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6474953721535859, distance: 0.6794212167617623 entropy -2.727066107652044
epoch: 44, step: 62
	action: tensor([[-0.0955,  0.0890, -0.0712,  0.1242, -0.0207,  0.1787, -0.0779]],
       dtype=torch.float64)
	q_value: tensor([[-30.7707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25905671517125095, distance: 0.9850293624746319 entropy -3.2341259671873424
epoch: 44, step: 63
	action: tensor([[-0.7420,  0.1135,  0.4520, -0.4146, -0.4531,  0.3884,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-34.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.842919318722571, distance: 1.55349498136423 entropy -2.8688708394769344
LOSS epoch 44 actor 507.3154095374142 critic 109.16419187167809
epoch: 45, step: 0
	action: tensor([[-2.1696,  0.1220, -0.2015, -0.2657,  0.2374,  0.7106,  0.2288]],
       dtype=torch.float64)
	q_value: tensor([[-34.4917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.55349498136423 entropy -2.36447896893178
epoch: 45, step: 1
	action: tensor([[-0.2414,  0.0608,  0.2275, -0.1368, -0.3054, -0.0068,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05686870690035417, distance: 1.176433038983756 entropy -3.321334490581692
epoch: 45, step: 2
	action: tensor([[-0.7510,  0.6564, -0.0794, -0.0298,  0.1174,  0.2659,  0.1348]],
       dtype=torch.float64)
	q_value: tensor([[-32.5390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13829156151924327, distance: 1.2209094392675635 entropy -2.6943501550808397
epoch: 45, step: 3
	action: tensor([[-0.1259, -0.1969, -0.2654, -0.5379,  0.4652,  0.1380,  0.0257]],
       dtype=torch.float64)
	q_value: tensor([[-35.1072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14883140347561907, distance: 1.2265488300447234 entropy -3.254174850916846
epoch: 45, step: 4
	action: tensor([[-0.1159,  0.0576, -0.3121, -0.1343, -0.2439, -0.1388, -0.1277]],
       dtype=torch.float64)
	q_value: tensor([[-27.3492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14095959100036115, distance: 1.0606289815029455 entropy -3.080576537346058
epoch: 45, step: 5
	action: tensor([[ 0.0631, -0.4585, -0.0491, -0.2119, -0.0881,  0.0486,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-29.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.180530501877872, distance: 1.2433554420929311 entropy -3.294628365851167
epoch: 45, step: 6
	action: tensor([[ 0.2569,  0.3471, -0.2593, -0.4648,  0.1745,  0.0685,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[-28.9769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6175285468088574, distance: 0.7077114241950764 entropy -2.642942293902696
epoch: 45, step: 7
	action: tensor([[ 0.0902, -0.0459,  0.4866,  0.2617, -0.2071, -0.0501, -0.1136]],
       dtype=torch.float64)
	q_value: tensor([[-30.3966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47961159865134884, distance: 0.8255065003158176 entropy -3.028983548164199
epoch: 45, step: 8
	action: tensor([[ 0.8123, -0.2469,  0.0810,  0.8024,  0.4598,  0.2353,  0.1504]],
       dtype=torch.float64)
	q_value: tensor([[-35.3779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9554076847736264, distance: 0.24164994687029834 entropy -2.4414148597211147
epoch: 45, step: 9
	action: tensor([[-0.2581, -0.3172,  0.2355, -0.7881, -1.3508, -0.3467, -0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-37.7714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5722195017629244, distance: 1.4348724026304285 entropy -2.360663133479032
epoch: 45, step: 10
	action: tensor([[-0.4233,  0.1583,  0.1668, -0.8436, -1.3625,  0.0649,  0.1753]],
       dtype=torch.float64)
	q_value: tensor([[-34.6257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5303619892520595, distance: 1.4156431062152521 entropy -2.372589225107023
epoch: 45, step: 11
	action: tensor([[ 0.5161, -1.2851, -0.6501,  0.2524, -0.3557,  0.3411,  0.2216]],
       dtype=torch.float64)
	q_value: tensor([[-36.3921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4156431062152521 entropy -2.309409642159618
epoch: 45, step: 12
	action: tensor([[ 0.0430, -0.1116, -0.0269, -0.4016, -0.2751,  0.1175,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07869233314324542, distance: 1.0983962357747439 entropy -3.321334490581692
epoch: 45, step: 13
	action: tensor([[-0.2432, -0.1880, -0.1188, -0.9747,  0.5563,  0.3804,  0.0369]],
       dtype=torch.float64)
	q_value: tensor([[-30.1923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29277590422238875, distance: 1.301122891202868 entropy -2.632186568350829
epoch: 45, step: 14
	action: tensor([[-0.1085,  0.2806,  0.0284,  0.2697, -0.3179,  0.0425, -0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-29.0701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.301122891202868 entropy -2.7750130293562685
epoch: 45, step: 15
	action: tensor([[ 0.2015, -0.3841, -0.0673,  0.1315,  0.1213, -0.1719,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18044525785734333, distance: 1.035966416933869 entropy -3.321334490581692
epoch: 45, step: 16
	action: tensor([[ 0.6904,  0.0284, -0.8496,  0.7157, -0.1098, -0.0472,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[-30.1401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8194176638476942, distance: 0.48628886675910044 entropy -2.7828805432735737
epoch: 45, step: 17
	action: tensor([[-0.0537,  0.2416,  0.3384, -0.3561,  0.1608, -0.1225, -0.0394]],
       dtype=torch.float64)
	q_value: tensor([[-35.4001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1677787096130391, distance: 1.0439413599650265 entropy -2.813662921704477
epoch: 45, step: 18
	action: tensor([[ 0.3577, -0.0292,  0.1306, -0.2242, -0.9997, -0.1073, -0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-32.4700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45031784075772285, distance: 0.848423157620481 entropy -2.8191160495602157
epoch: 45, step: 19
	action: tensor([[-0.2725, -0.2543, -0.9191, -0.6143,  0.8761,  0.7854,  0.1089]],
       dtype=torch.float64)
	q_value: tensor([[-34.5484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17753255918210153, distance: 1.0378056972139431 entropy -2.337895101195227
epoch: 45, step: 20
	action: tensor([[ 0.3537,  0.1761, -0.1901, -0.0306,  0.1159, -0.1241, -0.1575]],
       dtype=torch.float64)
	q_value: tensor([[-28.6134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6822052125412666, distance: 0.6451045098956785 entropy -2.970446611902843
epoch: 45, step: 21
	action: tensor([[ 0.4752,  0.5239, -0.1955, -0.6465, -0.3746,  0.3087, -0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-31.0560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8152350280171579, distance: 0.4918883236895811 entropy -3.005038408237079
epoch: 45, step: 22
	action: tensor([[ 0.0808,  0.5418, -0.4868,  0.0955,  0.1441,  0.1258, -0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-32.6020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4918883236895811 entropy -2.570316655918172
epoch: 45, step: 23
	action: tensor([[ 0.0284, -0.0618, -0.2925, -0.2989, -0.0374,  0.0961,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1863438505270052, distance: 1.032231596927027 entropy -3.321334490581692
epoch: 45, step: 24
	action: tensor([[-0.7606, -0.1017,  0.1397,  0.0013, -0.1481,  0.3148, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-29.0583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6614790441488685, distance: 1.4750411195734705 entropy -2.9756096861613477
epoch: 45, step: 25
	action: tensor([[ 0.1051, -0.1883, -0.2729, -0.1225, -0.0950,  0.1056,  0.1782]],
       dtype=torch.float64)
	q_value: tensor([[-32.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20702034411531145, distance: 1.0190317511373232 entropy -2.722318844033761
epoch: 45, step: 26
	action: tensor([[-0.1879, -0.7824, -0.1302,  0.0020, -0.6718,  0.2472,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-29.9198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5776516596407473, distance: 1.4373490709298327 entropy -2.8090264112416454
epoch: 45, step: 27
	action: tensor([[-0.6068,  0.4489, -0.2377,  0.5134,  0.8492,  0.0947,  0.2351]],
       dtype=torch.float64)
	q_value: tensor([[-30.8830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4373490709298327 entropy -2.3268784487040195
epoch: 45, step: 28
	action: tensor([[-0.3403, -0.0514,  0.0566,  0.2384, -0.1274,  0.0693,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035341108121527576, distance: 1.1643898800938037 entropy -3.321334490581692
epoch: 45, step: 29
	action: tensor([[-0.6123, -0.4912, -0.4284, -0.4388, -0.1447, -0.1143,  0.1532]],
       dtype=torch.float64)
	q_value: tensor([[-33.0445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7853700065781983, distance: 1.5290469138741132 entropy -2.7643686792073576
epoch: 45, step: 30
	action: tensor([[-0.0540, -0.3307,  0.4530,  0.1633, -0.0956,  0.1492,  0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-28.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11675194108206233, distance: 1.0754693530863997 entropy -3.355169701961261
epoch: 45, step: 31
	action: tensor([[ 7.5744e-01,  8.6374e-04, -1.7452e-01,  7.4646e-01, -9.2708e-01,
          4.6559e-01,  1.5634e-01]], dtype=torch.float64)
	q_value: tensor([[-33.3643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9338501825674871, distance: 0.2943208426514015 entropy -2.3732132572957387
epoch: 45, step: 32
	action: tensor([[ 0.3329, -0.5867, -1.3413, -0.5907,  0.3102, -0.6101,  0.0949]],
       dtype=torch.float64)
	q_value: tensor([[-41.5314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19336690778440113, distance: 1.0277670987467011 entropy -2.087939972506363
epoch: 45, step: 33
	action: tensor([[ 0.3395, -0.3005,  0.0531, -0.3568,  0.3849, -0.0555, -0.0589]],
       dtype=torch.float64)
	q_value: tensor([[-25.9373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05597904656390962, distance: 1.1118533534024877 entropy -3.2285984069224143
epoch: 45, step: 34
	action: tensor([[ 0.2954, -0.4044,  0.4298,  0.1557, -0.1784,  0.2185, -0.1475]],
       dtype=torch.float64)
	q_value: tensor([[-28.5931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36916773874067443, distance: 0.9088948465311569 entropy -2.6849685847042144
epoch: 45, step: 35
	action: tensor([[-0.5092,  0.4097,  0.8149,  0.1290, -0.8546, -0.0270,  0.1223]],
       dtype=torch.float64)
	q_value: tensor([[-33.4051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9088948465311569 entropy -2.2452814077980396
epoch: 45, step: 36
	action: tensor([[-0.3222, -0.5093,  0.0508, -0.0101,  0.2109, -0.0783,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5464763867637326, distance: 1.4230768042457609 entropy -3.321334490581692
epoch: 45, step: 37
	action: tensor([[-0.3464, -0.1952,  0.1671, -0.4608, -0.7336,  0.2684,  0.0492]],
       dtype=torch.float64)
	q_value: tensor([[-29.1668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5454483313988012, distance: 1.4226037142659809 entropy -2.8898317757476053
epoch: 45, step: 38
	action: tensor([[ 0.0745, -0.9748, -0.3309, -0.1341, -0.8321,  0.5208,  0.2154]],
       dtype=torch.float64)
	q_value: tensor([[-32.3703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6477248168996734, distance: 1.4689230125806063 entropy -2.3639489210312896
epoch: 45, step: 39
	action: tensor([[ 1.7830, -0.8866,  0.4213, -0.6458,  0.5384,  0.2384,  0.1628]],
       dtype=torch.float64)
	q_value: tensor([[-32.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4689230125806063 entropy -2.138397282421428
epoch: 45, step: 40
	action: tensor([[ 0.0316,  0.1494,  0.0676,  0.1971, -0.0275, -0.1452,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47727537216315397, distance: 0.8273574355657587 entropy -3.321334490581692
epoch: 45, step: 41
	action: tensor([[ 0.2947, -0.1349,  0.1049, -0.0012,  0.0885,  0.1890,  0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-33.9914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49661762195625225, distance: 0.8119058998950841 entropy -2.841441467289331
epoch: 45, step: 42
	action: tensor([[-0.0052, -0.4564, -0.0950,  0.2149, -0.0636,  0.0235,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016889691211796842, distance: 1.1346392904905611 entropy -3.321334490581692
epoch: 45, step: 43
	action: tensor([[-0.0541, -0.6180, -0.0025, -0.1930, -0.4279,  0.4817,  0.1087]],
       dtype=torch.float64)
	q_value: tensor([[-30.6343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3255961575760835, distance: 1.3175354568172994 entropy -2.6761825279634706
epoch: 45, step: 44
	action: tensor([[-1.2193,  0.5175,  0.5883, -0.2901,  0.1997,  0.2658,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-30.6949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9388314730848579, distance: 1.5934070111721794 entropy -2.2588545487885923
epoch: 45, step: 45
	action: tensor([[-0.8277,  0.4116,  0.0488, -0.4842,  0.6490, -0.2581,  0.0592]],
       dtype=torch.float64)
	q_value: tensor([[-37.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7411808559413657, distance: 1.5100058680745014 entropy -2.6706958375138794
epoch: 45, step: 46
	action: tensor([[-0.1094, -0.1581, -0.0506, -0.1136,  0.0619, -0.2784, -0.0370]],
       dtype=torch.float64)
	q_value: tensor([[-32.8355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07038580057944221, distance: 1.183932282991352 entropy -3.443820860750427
epoch: 45, step: 47
	action: tensor([[-0.2600, -0.0214,  0.2363, -0.0336, -0.0697,  0.0543,  0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-29.2399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03132779819362175, distance: 1.162130916941469 entropy -3.1335754330775387
epoch: 45, step: 48
	action: tensor([[-0.6409,  0.1101,  0.1649,  0.6757, -0.1491,  0.0260,  0.1111]],
       dtype=torch.float64)
	q_value: tensor([[-32.0797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02505553815479966, distance: 1.1299172309675172 entropy -2.716337124615758
epoch: 45, step: 49
	action: tensor([[ 0.5871,  0.0328, -0.2758,  0.6143,  0.1672,  0.5049,  0.2079]],
       dtype=torch.float64)
	q_value: tensor([[-37.4051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1299172309675172 entropy -2.684673195272203
epoch: 45, step: 50
	action: tensor([[ 0.0754, -0.1674,  0.0537, -0.0746, -0.1181, -0.2043,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14528785161732605, distance: 1.0579536266945295 entropy -3.321334490581692
epoch: 45, step: 51
	action: tensor([[-0.0585, -0.4670,  0.1814, -0.7545,  0.7886,  0.5231,  0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-30.8231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.369417664143209, distance: 1.3391359096561106 entropy -2.7794252902729157
epoch: 45, step: 52
	action: tensor([[-0.1295, -0.0823,  0.6226, -0.2711, -0.0299, -0.1756, -0.3263]],
       dtype=torch.float64)
	q_value: tensor([[-29.2103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17316473794025766, distance: 1.2394704967885866 entropy -2.426310242865802
epoch: 45, step: 53
	action: tensor([[-0.5617,  0.2604, -0.7616, -0.4426, -1.0264,  0.5393,  0.0671]],
       dtype=torch.float64)
	q_value: tensor([[-32.1041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28535171168522044, distance: 1.2973814479170291 entropy -2.5295565087169503
epoch: 45, step: 54
	action: tensor([[ 0.0902, -0.0220,  0.0602,  0.1625,  0.5598,  0.0532,  0.0954]],
       dtype=torch.float64)
	q_value: tensor([[-33.1254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46680495628034213, distance: 0.835602527930407 entropy -2.8849648304930886
epoch: 45, step: 55
	action: tensor([[ 0.0570, -0.1841,  0.7501, -0.2245, -0.8263,  0.1678, -0.0684]],
       dtype=torch.float64)
	q_value: tensor([[-32.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07441271100963898, distance: 1.100944393546658 entropy -2.858200271714137
epoch: 45, step: 56
	action: tensor([[-0.3070, -0.4283,  1.7481, -0.7931, -0.0468,  0.6659,  0.2335]],
       dtype=torch.float64)
	q_value: tensor([[-36.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.559427005936854, distance: 1.429022999971408 entropy -2.0495661085227677
epoch: 45, step: 57
	action: tensor([[-3.5324, -0.2526,  0.9238, -1.4663, -1.3610,  0.9068, -0.0298]],
       dtype=torch.float64)
	q_value: tensor([[-40.0519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.429022999971408 entropy -1.6781467764281912
epoch: 45, step: 58
	action: tensor([[-0.1121,  0.0628, -0.3321,  0.4166, -0.0664, -0.1300,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2693996449412196, distance: 0.9781301204516539 entropy -3.321334490581692
epoch: 45, step: 59
	action: tensor([[ 0.0573, -0.2611, -0.0299, -0.1551,  0.1658,  0.0086,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-33.1703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031075184155213154, distance: 1.1264235789891675 entropy -3.1063281729380963
epoch: 45, step: 60
	action: tensor([[-0.0595, -0.2758,  0.2655,  0.3040,  0.0017,  0.0415,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2188689710502525, distance: 1.0113899600021399 entropy -3.321334490581692
epoch: 45, step: 61
	action: tensor([[ 0.5806, -0.5125, -0.7601,  0.6333, -0.7194, -0.0856,  0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-33.1782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44597225658361794, distance: 0.8517702155889579 entropy -2.5641822804390197
epoch: 45, step: 62
	action: tensor([[-0.1209,  0.5337,  0.0278, -0.3430, -1.1424,  0.7379,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-35.1457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8517702155889579 entropy -2.535894765195015
epoch: 45, step: 63
	action: tensor([[ 0.0271, -0.2089,  0.0488,  0.1377, -0.2060, -0.0028,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-32.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2507951929630454, distance: 0.9905056812855468 entropy -3.321334490581692
LOSS epoch 45 actor 448.56291765300324 critic 402.9409631514818
epoch: 46, step: 0
	action: tensor([[-0.4222, -0.1733, -0.0675, -0.0735,  0.6812,  0.6859,  0.1200]],
       dtype=torch.float64)
	q_value: tensor([[-31.4184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04795169134564081, distance: 1.171459624302678 entropy -2.6260733363834476
epoch: 46, step: 1
	action: tensor([[ 0.2436,  0.7641, -0.1622,  0.3346, -0.3059,  0.4396, -0.1225]],
       dtype=torch.float64)
	q_value: tensor([[-29.8437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.171459624302678 entropy -2.7943267145633532
epoch: 46, step: 2
	action: tensor([[ 0.2886,  0.1282,  0.0160, -0.0584,  0.0829,  0.0975,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7501541312690069, distance: 0.5719957205740118 entropy -3.3220514967469987
epoch: 46, step: 3
	action: tensor([[ 0.0919,  0.2506,  0.0060, -0.0961, -0.3299,  0.1674,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49635942007548395, distance: 0.8121141002255782 entropy -3.3220514967469987
epoch: 46, step: 4
	action: tensor([[-0.1821,  0.3974,  0.1777,  1.1136,  0.3693,  0.5980,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-32.5887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8121141002255782 entropy -2.6368319552527977
epoch: 46, step: 5
	action: tensor([[-0.2297,  0.0395, -0.1936,  0.1255,  0.1393,  0.0122,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08944746922794311, distance: 1.0919662011224278 entropy -3.3220514967469987
epoch: 46, step: 6
	action: tensor([[ 0.5914, -0.0956, -0.0089,  0.2348,  0.1181,  0.1310,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-30.5981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7779690371465763, distance: 0.5392168616630437 entropy -3.1494975002575454
epoch: 46, step: 7
	action: tensor([[-0.1953, -0.1958, -0.7604, -0.1789, -0.2239, -0.0607, -0.0317]],
       dtype=torch.float64)
	q_value: tensor([[-32.4867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06436302474406252, distance: 1.1805967485053008 entropy -2.525456480210686
epoch: 46, step: 8
	action: tensor([[-0.0683,  0.0064, -0.0669,  0.1723,  0.0878,  0.0607,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[-26.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967418301585638, distance: 0.9596526929173111 entropy -3.5384560382812293
epoch: 46, step: 9
	action: tensor([[-0.1795, -0.2436, -0.1829,  0.6318, -0.3810,  0.4901,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-31.2439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9596526929173111 entropy -2.8984346562806715
epoch: 46, step: 10
	action: tensor([[ 0.6227,  0.1344, -0.0017,  0.0722,  0.2526, -0.0443,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8392407358658031, distance: 0.4588224906200136 entropy -3.3220514967469987
epoch: 46, step: 11
	action: tensor([[-0.1384, -0.1976,  0.3394,  0.1937, -0.0920,  0.0035,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09883316740988168, distance: 1.0863237958872325 entropy -3.3220514967469987
epoch: 46, step: 12
	action: tensor([[-0.0539,  0.0081, -0.5320,  0.3856,  0.0215,  0.6047,  0.1507]],
       dtype=torch.float64)
	q_value: tensor([[-32.4875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0863237958872325 entropy -2.5585547634887007
epoch: 46, step: 13
	action: tensor([[ 0.0056,  0.0424,  0.1216, -0.1100,  0.2424, -0.0850,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24925939128925434, distance: 0.9915203847926347 entropy -3.3220514967469987
epoch: 46, step: 14
	action: tensor([[-0.0716, -0.2018, -0.0410,  0.3500,  0.3612, -0.5591, -0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-30.4724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06752445894408743, distance: 1.1050334327552556 entropy -2.8843053919928723
epoch: 46, step: 15
	action: tensor([[-0.3021,  0.0137,  0.0019,  0.5148, -0.0803,  0.3605,  0.0327]],
       dtype=torch.float64)
	q_value: tensor([[-30.5782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1050334327552556 entropy -3.1800544511735738
epoch: 46, step: 16
	action: tensor([[-0.0107,  0.1071,  0.0437,  0.0348, -0.1148, -0.0111,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32992020412578904, distance: 0.9367419183575841 entropy -3.3220514967469987
epoch: 46, step: 17
	action: tensor([[ 0.6405, -0.5598,  0.4954, -0.5928,  0.2349,  0.0347,  0.0753]],
       dtype=torch.float64)
	q_value: tensor([[-31.8855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23592923591318327, distance: 1.272194448392766 entropy -2.797017155580707
epoch: 46, step: 18
	action: tensor([[ 0.3461, -0.5015, -0.0603,  0.0948, -0.4397, -0.4921, -0.2387]],
       dtype=torch.float64)
	q_value: tensor([[-29.9129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08425181054756137, distance: 1.0950771767072642 entropy -2.1888004813724065
epoch: 46, step: 19
	action: tensor([[ 0.2799,  0.3927,  0.5404, -0.2751,  0.3825,  0.0063,  0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-29.3732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0950771767072642 entropy -2.64820489730205
epoch: 46, step: 20
	action: tensor([[-0.0036, -0.2359, -0.1439, -0.2076,  0.0420,  0.0104,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0006246224182455684, distance: 1.1439868066362524 entropy -3.3220514967469987
epoch: 46, step: 21
	action: tensor([[ 0.2352, -0.5152, -0.3676,  0.3692, -0.1949,  0.4091,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-28.3805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2814435724928529, distance: 0.9700343927416792 entropy -2.879512628779786
epoch: 46, step: 22
	action: tensor([[-0.9648,  0.0425, -0.5354,  0.0488,  0.3653,  0.3170,  0.1054]],
       dtype=torch.float64)
	q_value: tensor([[-30.7509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.835602175266428, distance: 1.5504079088508815 entropy -2.458065604741356
epoch: 46, step: 23
	action: tensor([[0.1728, 0.0504, 0.1292, 0.0126, 0.0382, 0.0357, 0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-30.0743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5032222845267307, distance: 0.8065619800390997 entropy -3.650586106691244
epoch: 46, step: 24
	action: tensor([[ 1.8393, -0.0337,  0.3792,  0.7398, -0.0599,  0.2622,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-31.6579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8065619800390997 entropy -2.6909192402756004
epoch: 46, step: 25
	action: tensor([[-0.1587, -0.1357,  0.0292,  0.0669, -0.2459,  0.0583,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028912280056858863, distance: 1.1276801202625624 entropy -3.3220514967469987
epoch: 46, step: 26
	action: tensor([[ 0.0315, -0.4590,  0.7964,  0.1108, -0.5718,  0.7166,  0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-31.2251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27974471277813817, distance: 0.9711804262251467 entropy -2.681297601781289
epoch: 46, step: 27
	action: tensor([[-1.1254, -0.5685, -1.1475, -0.2471,  0.1255,  0.0975,  0.2832]],
       dtype=torch.float64)
	q_value: tensor([[-36.7661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3447078159123826, distance: 1.7522701340092708 entropy -1.8754321624164116
epoch: 46, step: 28
	action: tensor([[ 0.0261,  0.0572,  0.1565, -0.0470,  0.0846,  0.0888,  0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-29.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3335565352066159, distance: 0.9341967456677783 entropy -3.6048686436129147
epoch: 46, step: 29
	action: tensor([[-0.8450, -0.5513,  0.5922, -0.4577,  0.1199,  0.2135,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-31.2552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3280138149507663, distance: 1.746021028349163 entropy -2.7212033658641728
epoch: 46, step: 30
	action: tensor([[-0.2504, -0.1394, -1.0479, -0.3857, -0.6002,  0.2329,  0.0887]],
       dtype=torch.float64)
	q_value: tensor([[-31.1338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06298552864393181, distance: 1.179832538410767 entropy -2.387760175627469
epoch: 46, step: 31
	action: tensor([[ 0.2084,  0.0501, -0.0369,  0.2858, -0.0436,  0.1694,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-28.4665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.179832538410767 entropy -3.2942669721295332
epoch: 46, step: 32
	action: tensor([[ 0.0065,  0.1465,  0.3713, -0.0871,  0.2278,  0.0280,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3508131241121756, distance: 0.9220225844449932 entropy -3.3220514967469987
epoch: 46, step: 33
	action: tensor([[ 0.2043,  0.4810,  0.6598,  0.1205,  1.3068, -0.0218, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[-32.2720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9220225844449932 entropy -2.6811078432688924
epoch: 46, step: 34
	action: tensor([[-0.0035, -0.0087, -0.0150, -0.3217, -0.2762,  0.0747,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14574256208120784, distance: 1.0576721713623891 entropy -3.3220514967469987
epoch: 46, step: 35
	action: tensor([[-0.2084, -0.6490,  0.9189, -0.0579,  0.6639,  0.2872,  0.0589]],
       dtype=torch.float64)
	q_value: tensor([[-29.7588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4128940202890403, distance: 1.3602272995381117 entropy -2.7339361639666904
epoch: 46, step: 36
	action: tensor([[ 0.2229, -0.0703, -1.3388, -0.8999, -0.5240,  0.3903, -0.1173]],
       dtype=torch.float64)
	q_value: tensor([[-33.1874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.563997989615815, distance: 0.7556156411105241 entropy -2.1747095018578095
epoch: 46, step: 37
	action: tensor([[-0.2920, -0.2866, -0.3447,  0.3892,  0.1078, -0.1005, -0.0653]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16329038009895158, distance: 1.2342432523964837 entropy -3.0343744475597414
epoch: 46, step: 38
	action: tensor([[-0.3562, -0.1205,  0.0787,  0.1900,  0.2717, -0.0148,  0.0930]],
       dtype=torch.float64)
	q_value: tensor([[-29.6682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10348133338300602, distance: 1.202096101670423 entropy -3.200335271316134
epoch: 46, step: 39
	action: tensor([[-0.0978,  0.2576, -0.3562,  0.2932, -0.3748, -0.1938,  0.0556]],
       dtype=torch.float64)
	q_value: tensor([[-31.0694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.202096101670423 entropy -2.9579567467187218
epoch: 46, step: 40
	action: tensor([[ 0.0056, -0.0880, -0.0737,  0.0446,  0.2030, -0.1241,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2144588997582637, distance: 1.014240969584998 entropy -3.3220514967469987
epoch: 46, step: 41
	action: tensor([[ 0.5571,  0.1216,  0.5102, -0.1655,  0.3819,  0.0144,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-29.9408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7032017598774336, distance: 0.6234295075052105 entropy -2.9977608578011727
epoch: 46, step: 42
	action: tensor([[-0.0906, -0.0709,  0.1170, -0.1472,  0.3028,  0.1338, -0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-33.3313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1034250701850824, distance: 1.0835525754062678 entropy -2.4109302034523816
epoch: 46, step: 43
	action: tensor([[-0.1557,  0.1292,  0.3837,  0.2796, -0.0528,  0.4515, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[-29.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0835525754062678 entropy -2.856590909830097
epoch: 46, step: 44
	action: tensor([[-0.0710, -0.3249,  0.0046,  0.3198, -0.0934, -0.0486,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10983412044197605, distance: 1.079672811551894 entropy -3.3220514967469987
epoch: 46, step: 45
	action: tensor([[-0.1983, -0.3581,  0.1017, -0.3690, -0.1986,  0.2660,  0.1375]],
       dtype=torch.float64)
	q_value: tensor([[-31.4852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37713319743485196, distance: 1.3429030712553165 entropy -2.6930409892899347
epoch: 46, step: 46
	action: tensor([[-0.1627, -0.3154,  0.6442, -0.4913, -0.4354, -0.1291,  0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-29.3781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5548245250607706, distance: 1.4269126379464958 entropy -2.4903183660120782
epoch: 46, step: 47
	action: tensor([[ 0.7293, -0.1925, -0.1975,  0.2479,  0.8083,  0.6356,  0.1143]],
       dtype=torch.float64)
	q_value: tensor([[-32.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8519261069045724, distance: 0.4403479112827958 entropy -2.3047655383667824
epoch: 46, step: 48
	action: tensor([[-0.3993, -0.2809, -0.9996, -0.3172, -0.1904, -0.2600, -0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-32.7861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19638379437759235, distance: 1.2516760838050778 entropy -2.441707431940902
epoch: 46, step: 49
	action: tensor([[-0.1847, -0.1067,  0.0564, -0.1755, -0.0014,  0.0881,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-25.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07995967682816518, distance: 1.1892152325141883 entropy -3.7508072509738244
epoch: 46, step: 50
	action: tensor([[ 0.2734, -0.2777, -0.1594, -0.1746,  0.9127,  0.2253,  0.0537]],
       dtype=torch.float64)
	q_value: tensor([[-29.4699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28237499092000706, distance: 0.969405492314746 entropy -2.8081447189942836
epoch: 46, step: 51
	action: tensor([[-0.1055, -0.1606, -0.0926, -0.0109,  0.4710, -0.0503, -0.2316]],
       dtype=torch.float64)
	q_value: tensor([[-28.1689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015263375020691194, distance: 1.1355773946488266 entropy -2.755129049892689
epoch: 46, step: 52
	action: tensor([[ 0.2680, -0.0281,  0.5049,  0.2971,  0.0125, -0.0225, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[-28.3446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6735077201124293, distance: 0.6538726189229547 entropy -3.154623897171743
epoch: 46, step: 53
	action: tensor([[-0.6284, -0.3643, -0.3383,  0.7335, -0.6553,  0.6586,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-34.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6538726189229547 entropy -2.421087870647027
epoch: 46, step: 54
	action: tensor([[-0.0334,  0.1017,  0.0795,  0.0920,  0.2751,  0.1527,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41176389378666367, distance: 0.8776725838470387 entropy -3.3220514967469987
epoch: 46, step: 55
	action: tensor([[-0.4697,  0.2787,  0.5071,  0.5640,  0.1619,  0.2619, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-31.8500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8776725838470387 entropy -2.812708947759871
epoch: 46, step: 56
	action: tensor([[-0.1333,  0.3532,  0.0095,  0.1853,  0.5418,  0.0147,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8776725838470387 entropy -3.3220514967469987
epoch: 46, step: 57
	action: tensor([[-0.2655,  0.1615, -0.0805,  0.2740, -0.1728, -0.0757,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15988129375997717, distance: 1.048882937624557 entropy -3.3220514967469987
epoch: 46, step: 58
	action: tensor([[-0.0127,  0.1004, -0.2338,  0.4892, -0.8233,  0.4527,  0.1173]],
       dtype=torch.float64)
	q_value: tensor([[-33.1684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36277723809540496, distance: 0.9134869208470924 entropy -2.9596378365551685
epoch: 46, step: 59
	action: tensor([[ 0.4740,  0.0033,  0.0901, -0.3379,  0.0034, -0.0062,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4625171284009252, distance: 0.838955658673676 entropy -3.3220514967469987
epoch: 46, step: 60
	action: tensor([[-0.3641,  0.1717, -0.1220,  0.1700, -0.0986, -0.1804, -0.0896]],
       dtype=torch.float64)
	q_value: tensor([[-30.2434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.838955658673676 entropy -2.6003962389627255
epoch: 46, step: 61
	action: tensor([[-0.1592,  0.0790, -0.0429, -0.2655,  0.0559, -0.0703,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017307242027058578, distance: 1.1343983104740862 entropy -3.3220514967469987
epoch: 46, step: 62
	action: tensor([[-0.3142,  0.1632,  0.2777,  0.3773, -0.1565, -0.0721,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-29.4679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1343983104740862 entropy -3.0568817259739354
epoch: 46, step: 63
	action: tensor([[-0.2308,  0.0043,  0.1659,  0.5070,  0.2341,  0.0595,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-31.9128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1343983104740862 entropy -3.3220514967469987
LOSS epoch 46 actor 387.66667247185296 critic 508.99672708335453
epoch: 47, step: 0
	action: tensor([[ 0.1221,  0.0149, -0.1306,  0.0829,  0.1778,  0.0636,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4681809753238071, distance: 0.8345236096336691 entropy -3.3227005543790447
epoch: 47, step: 1
	action: tensor([[-1.9049e-01, -4.5532e-01, -2.9441e-02,  3.9519e-01,  1.0391e-01,
          1.9072e-01, -3.6680e-04]], dtype=torch.float64)
	q_value: tensor([[-33.7053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015389735245801672, distance: 1.1355045343453465 entropy -2.8964009354946607
epoch: 47, step: 2
	action: tensor([[-0.3532,  0.4222, -0.0983,  0.1157,  0.0414,  0.1916,  0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-33.9105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1355045343453465 entropy -2.6746911349472766
epoch: 47, step: 3
	action: tensor([[ 0.1843,  0.1167,  0.1479,  0.1673,  0.1710, -0.2658,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5411896885299974, distance: 0.7751277375723239 entropy -3.3227005543790447
epoch: 47, step: 4
	action: tensor([[-0.2617, -0.1924, -0.0487,  0.3941,  0.6865,  0.3071,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-35.6615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18489248410265324, distance: 1.033151812914541 entropy -2.854611558483274
epoch: 47, step: 5
	action: tensor([[ 0.2656, -0.4118, -0.8986, -0.1033,  0.0635, -0.2321, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-34.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15031056885313476, distance: 1.0548405133561547 entropy -2.909633932332493
epoch: 47, step: 6
	action: tensor([[-0.1094, -0.0311, -0.1495, -0.2346,  0.0513, -0.0125, -0.0380]],
       dtype=torch.float64)
	q_value: tensor([[-28.9275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06623023814179174, distance: 1.1058000273165356 entropy -3.3146577950037197
epoch: 47, step: 7
	action: tensor([[-0.1079, -0.2862, -0.0954,  0.4933,  0.1208,  0.3121,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-31.1062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29109518646784804, distance: 0.9634976414950631 entropy -3.115200459017155
epoch: 47, step: 8
	action: tensor([[ 0.1353, -0.5967, -1.0288, -0.0184,  0.3779,  0.0909,  0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-35.2744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10740462090096803, distance: 1.2042311555175222 entropy -2.665905491605513
epoch: 47, step: 9
	action: tensor([[-0.2787,  0.1224, -0.5553,  0.3926, -0.1250,  0.0712, -0.0654]],
       dtype=torch.float64)
	q_value: tensor([[-29.5065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2042311555175222 entropy -3.2152278723216705
epoch: 47, step: 10
	action: tensor([[-0.1635,  0.0359,  0.1965,  0.1802,  0.0403, -0.0114,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20602026862254408, distance: 1.0196741305227057 entropy -3.3227005543790447
epoch: 47, step: 11
	action: tensor([[-0.2552, -0.7987,  0.1607, -0.4221, -0.2330,  0.1434,  0.0956]],
       dtype=torch.float64)
	q_value: tensor([[-35.4442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8512959623689729, distance: 1.5570215384205708 entropy -2.772618416517734
epoch: 47, step: 12
	action: tensor([[ 0.2145, -0.0056,  0.7657, -0.1676, -0.5532,  0.0967,  0.0979]],
       dtype=torch.float64)
	q_value: tensor([[-31.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3762408033741452, distance: 0.9037850937998798 entropy -2.3876829254669367
epoch: 47, step: 13
	action: tensor([[-2.0652,  0.8469, -0.0187, -0.1625,  0.4593, -0.3360,  0.1336]],
       dtype=torch.float64)
	q_value: tensor([[-38.4950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9037850937998798 entropy -2.1329570428824765
epoch: 47, step: 14
	action: tensor([[ 0.0854, -0.0382,  0.1756,  0.1992,  0.1112, -0.0449,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44952460260427773, distance: 0.8490351103489383 entropy -3.3227005543790447
epoch: 47, step: 15
	action: tensor([[-1.1370, -0.0229,  0.2637, -0.2399,  0.0809, -0.0851,  0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-35.4117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3226742136260659, distance: 1.744017516130951 entropy -2.6946860528205123
epoch: 47, step: 16
	action: tensor([[-0.0747,  0.2252, -0.0866, -0.3882,  0.4092,  0.2078,  0.0885]],
       dtype=torch.float64)
	q_value: tensor([[-35.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972039548223805, distance: 0.95933733784874 entropy -3.0642965607034087
epoch: 47, step: 17
	action: tensor([[-0.0993,  0.1511,  0.1434, -0.2851,  0.3954,  0.2763, -0.1150]],
       dtype=torch.float64)
	q_value: tensor([[-32.4575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24848129037839894, distance: 0.9920340796055607 entropy -3.0329484139080702
epoch: 47, step: 18
	action: tensor([[ 0.2652, -0.2424,  0.0348,  0.1161,  0.0896,  0.1860, -0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-32.5492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4566019160493776, distance: 0.8435595464514445 entropy -2.842716686235287
epoch: 47, step: 19
	action: tensor([[-0.0080,  0.0584, -0.0070, -0.9070,  0.3552,  0.3531,  0.0333]],
       dtype=torch.float64)
	q_value: tensor([[-33.1125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05033049373986742, distance: 1.1151747816691935 entropy -2.577124086283953
epoch: 47, step: 20
	action: tensor([[ 0.1348, -0.5972,  0.1611,  0.1884, -0.1812,  0.2597, -0.2104]],
       dtype=torch.float64)
	q_value: tensor([[-31.3036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11027877817652787, distance: 1.0794031175287213 entropy -2.7103252901564727
epoch: 47, step: 21
	action: tensor([[ 0.2195, -0.1155, -0.9977, -0.5106,  0.5517,  0.3694,  0.1663]],
       dtype=torch.float64)
	q_value: tensor([[-33.1335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5022313036264843, distance: 0.8073660512374357 entropy -2.319773142426741
epoch: 47, step: 22
	action: tensor([[-0.3161,  0.2226, -0.0766, -0.1967, -0.1919, -0.1298, -0.1305]],
       dtype=torch.float64)
	q_value: tensor([[-29.9170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007742980018851653, distance: 1.139905327325309 entropy -3.092121806876225
epoch: 47, step: 23
	action: tensor([[-0.1233,  0.0411,  0.3273, -0.0031, -0.2068,  0.0017,  0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-32.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19168017094282908, distance: 1.0288411107925215 entropy -3.2653612776557397
epoch: 47, step: 24
	action: tensor([[-0.6237, -0.4146,  0.4335,  0.1471,  0.0215, -0.0154,  0.1298]],
       dtype=torch.float64)
	q_value: tensor([[-35.6187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6624919908508389, distance: 1.4754906920134112 entropy -2.59213135083348
epoch: 47, step: 25
	action: tensor([[-0.4401,  0.0563, -0.4003,  0.0809,  0.5461,  0.2320,  0.1828]],
       dtype=torch.float64)
	q_value: tensor([[-34.9848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10180677067200117, distance: 1.201183648729047 entropy -2.5567490534281214
epoch: 47, step: 26
	action: tensor([[-0.0203, -0.2239,  0.1408, -0.0699, -0.1278, -0.0179, -0.0348]],
       dtype=torch.float64)
	q_value: tensor([[-33.4223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046578047052839744, distance: 1.1173758149385074 entropy -3.386040906944905
epoch: 47, step: 27
	action: tensor([[ 1.1223,  0.3300,  0.1209, -0.6908,  0.2786,  0.3550,  0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-32.7170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7168860745863461, distance: 0.6088878512648723 entropy -2.6698497980344236
epoch: 47, step: 28
	action: tensor([[-0.1614,  0.8429,  0.1331, -1.2194,  0.8271,  0.0992, -0.3421]],
       dtype=torch.float64)
	q_value: tensor([[-35.6616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6088878512648723 entropy -2.2247825301927024
epoch: 47, step: 29
	action: tensor([[ 0.2706,  0.2543,  0.0149,  0.3482,  0.1097, -0.1403,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7482976619529929, distance: 0.5741168827474646 entropy -3.3227005543790447
epoch: 47, step: 30
	action: tensor([[-0.0825, -0.0153,  0.0172, -0.4950,  0.1391, -0.1575,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10158864963888226, distance: 1.2010647456719634 entropy -3.3227005543790447
epoch: 47, step: 31
	action: tensor([[ 0.0191,  0.1100, -0.1070,  0.0028, -0.5815, -0.0488, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-31.4776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3756703288826593, distance: 0.9041982889181918 entropy -2.993605380555119
epoch: 47, step: 32
	action: tensor([[-0.2257, -0.0974, -0.0682,  0.1232, -0.4994,  0.4325,  0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-35.0092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03136796477355486, distance: 1.1262533800718044 entropy -2.7449816002766725
epoch: 47, step: 33
	action: tensor([[ 0.9039,  0.3169,  0.1361, -0.6838,  0.6910,  0.2158,  0.2178]],
       dtype=torch.float64)
	q_value: tensor([[-36.0989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7179718677101, distance: 0.607719132092523 entropy -2.4380602019930033
epoch: 47, step: 34
	action: tensor([[-0.2627,  0.0124,  0.5761,  0.1821,  0.1248, -0.6221, -0.3799]],
       dtype=torch.float64)
	q_value: tensor([[-35.4601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08288546244076533, distance: 1.190825031251134 entropy -2.362370164901903
epoch: 47, step: 35
	action: tensor([[ 0.1295, -0.0242,  0.0512, -0.1701, -0.1745,  0.0453,  0.1008]],
       dtype=torch.float64)
	q_value: tensor([[-36.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30657762043550363, distance: 0.9529181968263091 entropy -2.7408037534996077
epoch: 47, step: 36
	action: tensor([[-0.3598, -0.1189,  0.2482,  0.4476,  0.4292,  0.6941,  0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-33.4631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9529181968263091 entropy -2.649109156398155
epoch: 47, step: 37
	action: tensor([[ 0.0157, -0.0498, -0.0988, -0.2432,  0.1069,  0.0446,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1698469352129992, distance: 1.0426433582281718 entropy -3.3227005543790447
epoch: 47, step: 38
	action: tensor([[-0.0841,  0.5327, -0.2068,  0.1215,  0.3560,  0.2592, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-31.8374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0426433582281718 entropy -2.908531879237493
epoch: 47, step: 39
	action: tensor([[-0.0533,  0.0398,  0.1648, -0.5631,  0.0435, -0.3905,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0426433582281718 entropy -3.3227005543790447
epoch: 47, step: 40
	action: tensor([[-0.1881, -0.1440, -0.1024, -0.1778,  0.1215, -0.0733,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15512524915186465, distance: 1.2299040503276262 entropy -3.3227005543790447
epoch: 47, step: 41
	action: tensor([[-0.4622, -0.1185,  0.0606,  0.2112,  0.8266, -0.2464,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-31.5129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37808824548875397, distance: 1.3433686452037752 entropy -3.0615264997701916
epoch: 47, step: 42
	action: tensor([[-0.0572,  0.0360,  0.0905,  0.2526,  0.4765, -0.1623, -0.0523]],
       dtype=torch.float64)
	q_value: tensor([[-34.5899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29863783049906134, distance: 0.9583581968692524 entropy -3.235008433797804
epoch: 47, step: 43
	action: tensor([[-0.0367, -0.5545, -0.0388, -0.2066, -0.2471,  0.0200, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-34.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3631547281477685, distance: 1.336070178158343 entropy -3.033525485195677
epoch: 47, step: 44
	action: tensor([[-0.1406, -0.2496,  0.4440, -0.3072,  0.3045,  0.5000,  0.0973]],
       dtype=torch.float64)
	q_value: tensor([[-31.1132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09856281541605827, distance: 1.199414074226311 entropy -2.6069137489864285
epoch: 47, step: 45
	action: tensor([[-0.5032, -0.5793,  0.2466, -0.5297, -0.0994,  0.4723, -0.0604]],
       dtype=torch.float64)
	q_value: tensor([[-33.0986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9023899546124194, distance: 1.5783614505358112 entropy -2.3647132016177634
epoch: 47, step: 46
	action: tensor([[ 1.0878, -0.9928, -0.8848,  0.4370,  0.5370,  0.2341,  0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-32.4401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05274801398823503, distance: 1.1137544583242842 entropy -2.3723555516990507
epoch: 47, step: 47
	action: tensor([[-0.6471,  0.1100, -0.8031,  0.8654,  0.3661,  0.2489, -0.2400]],
       dtype=torch.float64)
	q_value: tensor([[-33.8524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.518072140663306, distance: 1.4099473590519818 entropy -2.43324894516387
epoch: 47, step: 48
	action: tensor([[-0.0270, -0.1060,  0.3230, -0.2586,  0.0432, -0.0217,  0.0658]],
       dtype=torch.float64)
	q_value: tensor([[-38.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08083019786628376, distance: 1.1896944301949657 entropy -3.4056447196118804
epoch: 47, step: 49
	action: tensor([[-0.7386,  0.0771,  0.7187, -1.1462,  0.5655,  0.0745,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-33.1031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1984263482177697, distance: 1.6967296671070045 entropy -2.6178290216598907
epoch: 47, step: 50
	action: tensor([[ 0.7624, -0.1804,  0.1742,  0.2041, -0.6493,  0.2668, -0.2398]],
       dtype=torch.float64)
	q_value: tensor([[-37.1814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7622879333993458, distance: 0.5579333311648009 entropy -2.523724093479507
epoch: 47, step: 51
	action: tensor([[-1.1816, -1.2778, -0.3843,  0.3273,  0.2375,  0.6680,  0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-36.9879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5579333311648009 entropy -2.1246030491054104
epoch: 47, step: 52
	action: tensor([[ 0.2772,  0.1307,  0.0905, -0.1265, -0.0682,  0.3624,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6766799432175596, distance: 0.6506883280127109 entropy -3.3227005543790447
epoch: 47, step: 53
	action: tensor([[ 0.4741, -0.4403,  0.1831, -0.4738, -0.1764,  0.5399,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[-34.9109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11879581001130424, distance: 1.0742242943068612 entropy -2.4754672815403755
epoch: 47, step: 54
	action: tensor([[-0.3062,  0.1556,  0.4157,  0.3040,  1.5418, -0.2435, -0.0601]],
       dtype=torch.float64)
	q_value: tensor([[-32.4671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0742242943068612 entropy -2.159082353459375
epoch: 47, step: 55
	action: tensor([[-0.0863,  0.0168,  0.0169, -0.1307, -0.3670,  0.0581,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-36.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14064210792626353, distance: 1.0608249563601937 entropy -3.3227005543790447
epoch: 47, step: 56
	action: tensor([[ 0.0224, -0.4687,  0.1439,  0.3311,  0.5516,  0.0489,  0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-34.1787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1507451724420522, distance: 1.0545707111837208 entropy -2.6828676157430635
epoch: 47, step: 57
	action: tensor([[ 0.3878, -0.3039, -0.6530,  0.3995, -0.4845,  0.2732, -0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-33.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4320832991274781, distance: 0.8623806700737723 entropy -2.669019637903848
epoch: 47, step: 58
	action: tensor([[ 0.1251, -0.2988,  0.1353,  0.3111, -0.4624,  0.1314,  0.0592]],
       dtype=torch.float64)
	q_value: tensor([[-35.0329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3770844844770629, distance: 0.9031736684131597 entropy -2.5708632896340213
epoch: 47, step: 59
	action: tensor([[-0.5830, -0.0353,  0.2125, -0.3513, -1.0174, -0.1218,  0.1909]],
       dtype=torch.float64)
	q_value: tensor([[-36.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6419210168190357, distance: 1.4663337282887925 entropy -2.3659635492204267
epoch: 47, step: 60
	action: tensor([[ 0.4028, -0.4482,  0.4914, -0.0351, -0.5381,  0.6471,  0.2486]],
       dtype=torch.float64)
	q_value: tensor([[-37.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4052091269019775, distance: 0.8825490283774279 entropy -2.4672331959118163
epoch: 47, step: 61
	action: tensor([[-1.5933,  0.4977, -1.0178, -1.2052, -2.2175,  0.4665,  0.1393]],
       dtype=torch.float64)
	q_value: tensor([[-38.2437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.752805476791299, distance: 1.5150380981872973 entropy -1.9421742691678106
epoch: 47, step: 62
	action: tensor([[-0.2641, -0.4211, -0.0657, -0.3129, -0.4068, -0.0234,  0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-47.7870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5200009122016072, distance: 1.410842772061143 entropy -2.790794620805176
epoch: 47, step: 63
	action: tensor([[-0.1987,  0.0357,  0.6117, -0.7164,  0.2112,  0.2604,  0.1261]],
       dtype=torch.float64)
	q_value: tensor([[-32.3752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3603968042548569, distance: 1.3347179305253458 entropy -2.679640909591683
LOSS epoch 47 actor 530.0669028950055 critic 173.34381302753573
epoch: 48, step: 0
	action: tensor([[ 1.3964,  0.3816, -0.2951,  0.3088,  0.6435,  0.3673, -0.1222]],
       dtype=torch.float64)
	q_value: tensor([[-36.3689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8592376093852234, distance: 0.4293386736204145 entropy -2.3892659406014682
epoch: 48, step: 1
	action: tensor([[-0.2574,  1.0355, -0.1788, -1.1716, -0.3923, -0.5725, -0.2576]],
       dtype=torch.float64)
	q_value: tensor([[-41.8343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4293386736204145 entropy -2.327254652685155
epoch: 48, step: 2
	action: tensor([[ 0.0409,  0.1330, -0.0801,  0.3179, -0.0803,  0.0037,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5058934625014682, distance: 0.8043906119469149 entropy -3.3231119846225345
epoch: 48, step: 3
	action: tensor([[-0.6714, -0.1109, -0.1858,  0.4877, -0.1773, -0.2240,  0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-38.2956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4605292564954988, distance: 1.382967020788954 entropy -2.8346111800078293
epoch: 48, step: 4
	action: tensor([[-0.0829, -0.1781,  0.0957,  0.0603,  0.0352,  0.0685,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10608595594196568, distance: 1.081943478430014 entropy -3.3231119846225345
epoch: 48, step: 5
	action: tensor([[-0.2812,  0.4345,  0.1535, -0.6390, -0.1737, -0.2177,  0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-35.2965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.065558278769968, distance: 1.1812594533553529 entropy -2.703931996561511
epoch: 48, step: 6
	action: tensor([[ 0.2048, -0.2798, -0.1763, -0.1105,  0.1356,  0.1034, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-37.1196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20982589508590732, distance: 1.0172274937974262 entropy -3.075902319161061
epoch: 48, step: 7
	action: tensor([[-0.1147,  0.5522,  0.2189, -0.1112, -0.6007,  0.3024, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[-32.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0172274937974262 entropy -2.7878318958375834
epoch: 48, step: 8
	action: tensor([[-0.3548, -0.0506, -0.1960, -0.2106,  0.3621, -0.1861,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31100914361169174, distance: 1.3102662471523951 entropy -3.3231119846225345
epoch: 48, step: 9
	action: tensor([[-0.1700,  0.2343, -0.0491,  0.2480,  0.0018,  0.0324, -0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-33.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3102662471523951 entropy -3.4691639284599267
epoch: 48, step: 10
	action: tensor([[ 0.1532,  0.1582,  0.1609, -0.2496, -0.1582,  0.2180,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4435188210775449, distance: 0.853654105760585 entropy -3.3231119846225345
epoch: 48, step: 11
	action: tensor([[-0.3635,  0.1059, -0.0291, -0.8405,  0.2012, -0.1847,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[-36.2984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3971710047215209, distance: 1.3526376616673264 entropy -2.5413871642064696
epoch: 48, step: 12
	action: tensor([[ 0.2814, -0.2921, -0.0824, -0.3338, -0.2796,  0.1676, -0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-33.9066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1163750865979204, distance: 1.0756987632852986 entropy -3.2514211276752505
epoch: 48, step: 13
	action: tensor([[-0.2556, -0.7214, -0.2097, -0.3062, -0.1803,  0.1130,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-33.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.688996610254212, distance: 1.4872058402876878 entropy -2.527278473303863
epoch: 48, step: 14
	action: tensor([[ 0.1409, -0.4244, -0.0833,  0.8058, -0.4034,  0.1174,  0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-32.2529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49118545546156067, distance: 0.816274917807031 entropy -2.69613167446036
epoch: 48, step: 15
	action: tensor([[ 1.0607, -0.7290,  0.3625, -0.0894, -0.3411,  0.3083,  0.1947]],
       dtype=torch.float64)
	q_value: tensor([[-40.0615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04215302220558481, distance: 1.1199657972622077 entropy -2.399805056960606
epoch: 48, step: 16
	action: tensor([[ 0.9464,  0.3947, -1.1449,  0.4958,  2.0206, -0.2576, -0.1337]],
       dtype=torch.float64)
	q_value: tensor([[-38.8014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9353361660373745, distance: 0.2909962675721384 entropy -1.9702941570677461
epoch: 48, step: 17
	action: tensor([[ 0.3408, -0.4656,  0.0802, -0.3919, -0.0515,  0.6014, -0.1854]],
       dtype=torch.float64)
	q_value: tensor([[-45.2616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09244595655743615, distance: 1.0901667734602563 entropy -2.650667506626323
epoch: 48, step: 18
	action: tensor([[-0.7310, -0.5385,  0.0724,  0.7665,  0.4401,  0.1116, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-32.8426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4996654575428221, distance: 1.401373457074851 entropy -2.2459797027635404
epoch: 48, step: 19
	action: tensor([[-0.0909,  0.0270, -0.4015,  0.1789, -0.4074,  0.1611,  0.1486]],
       dtype=torch.float64)
	q_value: tensor([[-38.7809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19834999179262314, distance: 1.0245875962070066 entropy -2.780685629472534
epoch: 48, step: 20
	action: tensor([[-0.6339,  0.1905,  0.1912,  0.4196, -0.3320,  0.1828,  0.1069]],
       dtype=torch.float64)
	q_value: tensor([[-37.0408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0245875962070066 entropy -2.838735275271542
epoch: 48, step: 21
	action: tensor([[-0.1443, -0.1692,  0.1976, -0.2088, -0.0794, -0.0499,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1657300259443697, distance: 1.2355367984464338 entropy -3.3231119846225345
epoch: 48, step: 22
	action: tensor([[-0.2000, -0.7262, -0.6157,  0.2325,  0.7438,  0.0593,  0.0654]],
       dtype=torch.float64)
	q_value: tensor([[-34.3397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5158751656007696, distance: 1.4089267419662088 entropy -2.722062805862897
epoch: 48, step: 23
	action: tensor([[ 0.0042,  0.1875, -0.1072, -0.2789, -0.3318,  0.0468, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-32.7279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065979611708989, distance: 0.9529042203527965 entropy -3.1674518086737913
epoch: 48, step: 24
	action: tensor([[-0.7807,  0.0298, -0.3724,  0.3810,  0.6871, -0.0043,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-34.7868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6155474914049317, distance: 1.4545094857872147 entropy -2.8785637663258714
epoch: 48, step: 25
	action: tensor([[-0.5969, -0.0658,  0.2481, -0.2839, -0.0112, -0.0112,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[-37.3310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6958666230297401, distance: 1.490227384135384 entropy -3.590320271981838
epoch: 48, step: 26
	action: tensor([[-0.4957, -0.0113,  0.2301,  0.0798,  0.1488, -0.0216,  0.0876]],
       dtype=torch.float64)
	q_value: tensor([[-35.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.305170313384139, distance: 1.3073452303982822 entropy -2.878254229804498
epoch: 48, step: 27
	action: tensor([[ 0.4603, -0.2762,  0.1663, -0.1239, -0.2271,  0.0569,  0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-36.4588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33807457444565414, distance: 0.9310247471638129 entropy -2.8884385543239293
epoch: 48, step: 28
	action: tensor([[ 0.0236, -0.0883, -0.1587,  0.0368, -1.0607,  0.3713,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-35.3922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18030396153790074, distance: 1.036055716602132 entropy -2.3884753270493753
epoch: 48, step: 29
	action: tensor([[-0.5385, -0.3008,  1.1154, -0.6970, -0.1155,  0.8194,  0.2247]],
       dtype=torch.float64)
	q_value: tensor([[-38.8011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7552871008700996, distance: 1.516110215216065 entropy -2.267178678768937
epoch: 48, step: 30
	action: tensor([[-1.1951, -1.6227, -0.8841, -3.3823, -0.9547,  0.7503,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.516110215216065 entropy -1.8694898851213189
epoch: 48, step: 31
	action: tensor([[-0.2186,  0.1581,  0.0861, -0.0805, -0.0143,  0.1185,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10933765129386697, distance: 1.0799738506814496 entropy -3.3231119846225345
epoch: 48, step: 32
	action: tensor([[-0.2379,  0.2516,  0.1714,  0.1816,  0.0079,  0.1442,  0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-36.3505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0799738506814496 entropy -2.828861090721143
epoch: 48, step: 33
	action: tensor([[ 0.0111,  0.2829, -0.0403, -0.0301, -0.2252,  0.2347,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0799738506814496 entropy -3.3231119846225345
epoch: 48, step: 34
	action: tensor([[ 0.0194, -0.0503,  0.0289,  0.2587, -0.3160, -0.0697,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39442574716724466, distance: 0.8905132674988294 entropy -3.3231119846225345
epoch: 48, step: 35
	action: tensor([[-0.6500,  0.3732, -0.2533,  0.0014,  0.3972,  0.1746,  0.1379]],
       dtype=torch.float64)
	q_value: tensor([[-37.9337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17693891105382664, distance: 1.241462638108749 entropy -2.6639681551376126
epoch: 48, step: 36
	action: tensor([[ 0.1232,  0.1977,  0.0485,  0.1093, -0.1397,  0.0311, -0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-37.3077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.241462638108749 entropy -3.5198175150043842
epoch: 48, step: 37
	action: tensor([[-0.2303, -0.0305,  0.2718, -0.1479,  0.2424,  0.0277,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11194793632990763, distance: 1.2066989083758324 entropy -3.3231119846225345
epoch: 48, step: 38
	action: tensor([[-0.2254, -0.3611,  0.1082, -0.4639,  0.0210,  0.1300,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[-35.1452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4961763052038426, distance: 1.3997422756575844 entropy -2.7991407925121483
epoch: 48, step: 39
	action: tensor([[-0.6455, -0.1699,  0.2078, -0.1828, -0.4008, -0.0018,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-32.5892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7818282322197756, distance: 1.527529517590979 entropy -2.6758573117014364
epoch: 48, step: 40
	action: tensor([[-1.8155,  0.1047,  0.1544, -0.4736, -0.6348,  0.0077,  0.1948]],
       dtype=torch.float64)
	q_value: tensor([[-36.0231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.527529517590979 entropy -2.7006116173686516
epoch: 48, step: 41
	action: tensor([[ 0.4747,  0.2632,  0.1091,  0.0338, -0.2228, -0.0688,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.814648043431383, distance: 0.4926690504292085 entropy -3.3231119846225345
epoch: 48, step: 42
	action: tensor([[-0.1973,  0.1832, -0.0681,  0.1945, -0.2439, -0.0510,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4926690504292085 entropy -3.3231119846225345
epoch: 48, step: 43
	action: tensor([[-0.2575,  0.0055,  0.2780, -0.0818, -0.0503,  0.2347,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013532168966156943, distance: 1.1520609656637781 entropy -3.3231119846225345
epoch: 48, step: 44
	action: tensor([[-0.0784, -0.0717,  0.5324,  0.3230,  0.3557,  0.2282,  0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-36.3030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4481142076703115, distance: 0.8501220878783797 entropy -2.611318989826997
epoch: 48, step: 45
	action: tensor([[ 0.0724, -0.7590, -0.7694,  0.5180,  0.6161,  0.4051,  0.0449]],
       dtype=torch.float64)
	q_value: tensor([[-39.1925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12852718873749414, distance: 1.2156616225823527 entropy -2.4552372001927636
epoch: 48, step: 46
	action: tensor([[-0.3229, -0.3461,  0.1526, -0.3923, -0.0549,  0.1718, -0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-34.2209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5587189234587497, distance: 1.4286985278951982 entropy -2.8925560872095324
epoch: 48, step: 47
	action: tensor([[0.2868, 0.1893, 0.3466, 0.7792, 0.3035, 0.2072, 0.0674]],
       dtype=torch.float64)
	q_value: tensor([[-33.0558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4286985278951982 entropy -2.6469400201355664
epoch: 48, step: 48
	action: tensor([[-0.1391,  0.0454,  0.0772,  0.6074, -0.1376,  0.1950,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4671707834598565, distance: 0.8353158236833493 entropy -3.3231119846225345
epoch: 48, step: 49
	action: tensor([[-0.0861, -0.3751, -0.2162,  0.6147,  0.7921, -0.0314,  0.1774]],
       dtype=torch.float64)
	q_value: tensor([[-40.9107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12168038646003343, distance: 1.072464644194989 entropy -2.5799394261808253
epoch: 48, step: 50
	action: tensor([[-0.2812, -0.0775,  0.0072,  0.3734,  0.5377, -0.0695,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03784656038947665, distance: 1.12248064622831 entropy -3.3231119846225345
epoch: 48, step: 51
	action: tensor([[-0.0869, -0.0992,  0.2098,  0.0727,  0.2507,  0.0701,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-36.9851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15985080675746088, distance: 1.0489019688676546 entropy -3.0734706929950635
epoch: 48, step: 52
	action: tensor([[-0.6221, -0.4031,  0.1594,  0.4391,  0.3709,  0.2206,  0.0307]],
       dtype=torch.float64)
	q_value: tensor([[-35.4789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41878468672684943, distance: 1.3630598938197804 entropy -2.738795815965387
epoch: 48, step: 53
	action: tensor([[-0.6432, -0.2004, -0.5683,  0.0789, -0.3241,  0.0886,  0.1146]],
       dtype=torch.float64)
	q_value: tensor([[-37.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7109094930270885, distance: 1.4968221845905914 entropy -2.7580541869662407
epoch: 48, step: 54
	action: tensor([[-0.2302, -0.0773,  0.3133,  0.1582,  0.3734,  0.0072,  0.0953]],
       dtype=torch.float64)
	q_value: tensor([[-35.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0487505526138875, distance: 1.116102040197594 entropy -3.297558157784255
epoch: 48, step: 55
	action: tensor([[-0.5029,  0.2464,  0.3803, -0.3970,  0.8282,  0.3079,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-36.6284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3051805270255694, distance: 1.3073503457189226 entropy -2.757822339257508
epoch: 48, step: 56
	action: tensor([[-0.1882, -0.2356,  0.0906, -0.7797,  0.0585,  0.3311, -0.1988]],
       dtype=torch.float64)
	q_value: tensor([[-37.6086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4223257069010564, distance: 1.364759804593487 entropy -2.799450499818757
epoch: 48, step: 57
	action: tensor([[-0.8728,  0.2442,  0.2043, -0.2104, -0.5236,  0.3163, -0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-32.3750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7602293125812383, distance: 1.518243105830825 entropy -2.632250973575162
epoch: 48, step: 58
	action: tensor([[0.4859, 0.0122, 0.0723, 0.6438, 0.7495, 0.3168, 0.2127]],
       dtype=torch.float64)
	q_value: tensor([[-38.3565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.518243105830825 entropy -2.6236309029558984
epoch: 48, step: 59
	action: tensor([[ 0.1549,  0.1973,  0.1745, -0.3852, -0.6306, -0.0268,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4100709476369383, distance: 0.8789346492566713 entropy -3.3231119846225345
epoch: 48, step: 60
	action: tensor([[-0.3824,  0.4008, -0.5340, -0.0066, -0.0533,  0.4876,  0.0789]],
       dtype=torch.float64)
	q_value: tensor([[-37.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8789346492566713 entropy -2.498207708846106
epoch: 48, step: 61
	action: tensor([[-0.3342, -0.2554,  0.3245,  0.2750,  0.0275, -0.0298,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-39.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14321314650142591, distance: 1.2235459907257524 entropy -3.3231119846225345
epoch: 48, step: 62
	action: tensor([[ 0.5326, -0.2067, -0.8628,  0.4617,  0.0454,  0.5938,  0.1551]],
       dtype=torch.float64)
	q_value: tensor([[-37.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6000961050247586, distance: 0.7236599041335846 entropy -2.6311556014094535
epoch: 48, step: 63
	action: tensor([[-0.4871,  0.0421,  0.0425,  0.4179,  0.2207,  0.0602, -0.0671]],
       dtype=torch.float64)
	q_value: tensor([[-37.8055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7236599041335846 entropy -2.639520590501806
LOSS epoch 48 actor 528.2526912589141 critic 340.7923049816246
epoch: 49, step: 0
	action: tensor([[ 0.0735, -0.0051,  0.3821,  0.3994,  0.4615, -0.0976,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5586056733068869, distance: 0.7602738739965584 entropy -3.323042468691012
epoch: 49, step: 1
	action: tensor([[-0.4345, -0.7046,  0.1626,  0.0470,  0.2513,  0.4281,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[-39.6739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5337349858993459, distance: 1.4172023226999482 entropy -2.663493656622222
epoch: 49, step: 2
	action: tensor([[-0.0227, -0.1832,  0.3941, -0.0703,  0.2088,  0.2036,  0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-34.7504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4172023226999482 entropy -2.4907028337239154
epoch: 49, step: 3
	action: tensor([[-0.0651, -0.2412, -0.3398,  0.1523, -0.2249, -0.0821,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061573975437607786, distance: 1.1085536464743087 entropy -3.323042468691012
epoch: 49, step: 4
	action: tensor([[-0.1171,  0.1235, -0.4014,  0.2478,  0.1035,  0.3365,  0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-35.4451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1085536464743087 entropy -2.955551247412262
epoch: 49, step: 5
	action: tensor([[ 0.1564, -0.1130, -0.3131, -0.0962, -0.0844,  0.0251,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962823052538449, distance: 0.9599661711799263 entropy -3.323042468691012
epoch: 49, step: 6
	action: tensor([[ 0.6931, -0.0580,  0.0346, -0.1365, -0.3375,  0.4687,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[-34.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7256041881139271, distance: 0.5994396037212847 entropy -2.9206982686217597
epoch: 49, step: 7
	action: tensor([[ 0.9977,  0.3102, -1.6035,  0.6942,  0.0591,  0.4776, -0.0125]],
       dtype=torch.float64)
	q_value: tensor([[-37.9369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8292449316314541, distance: 0.47287188662524693 entropy -2.2198695175646113
epoch: 49, step: 8
	action: tensor([[ 0.4611, -0.2365, -0.6492,  0.2538,  0.5132,  0.4225, -0.1336]],
       dtype=torch.float64)
	q_value: tensor([[-44.3971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6141279362405274, distance: 0.710850645574455 entropy -2.701489785430636
epoch: 49, step: 9
	action: tensor([[-0.0783, -0.0470,  0.1331,  0.1953, -0.0450,  0.4312, -0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-34.8981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36873295357897573, distance: 0.9092080089592842 entropy -2.8391016362674923
epoch: 49, step: 10
	action: tensor([[-0.8805, -0.7832,  0.0571, -0.5486,  0.8467,  0.4208,  0.1342]],
       dtype=torch.float64)
	q_value: tensor([[-37.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1350427080449372, distance: 1.6720912593815564 entropy -2.528403552968242
epoch: 49, step: 11
	action: tensor([[-0.6605, -0.2600,  0.3818,  0.3381, -0.2722, -0.1800, -0.2037]],
       dtype=torch.float64)
	q_value: tensor([[-36.0690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5520250443777834, distance: 1.425627472318328 entropy -2.691047128313894
epoch: 49, step: 12
	action: tensor([[-0.0806, -0.4643, -0.1920,  0.5740,  0.1096,  0.0921,  0.2491]],
       dtype=torch.float64)
	q_value: tensor([[-37.7745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08974348686574807, distance: 1.091788689380967 entropy -2.6174887647982517
epoch: 49, step: 13
	action: tensor([[-0.4939,  0.2508, -0.0219, -0.2965, -0.4020,  0.0332,  0.1023]],
       dtype=torch.float64)
	q_value: tensor([[-37.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2843109666572121, distance: 1.2968560988193216 entropy -2.708900515608196
epoch: 49, step: 14
	action: tensor([[ 0.6591, -0.2159, -0.2592,  0.2536,  0.4056,  0.1112,  0.1093]],
       dtype=torch.float64)
	q_value: tensor([[-37.4397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.70061641591292, distance: 0.6261388984942916 entropy -2.984153702886956
epoch: 49, step: 15
	action: tensor([[ 0.0149, -0.3724, -0.1539, -0.0502, -0.4510,  0.1446, -0.1037]],
       dtype=torch.float64)
	q_value: tensor([[-36.5315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049852366425589434, distance: 1.1725214840550986 entropy -2.650889058508688
epoch: 49, step: 16
	action: tensor([[ 0.9855,  0.4148, -0.5803, -0.6621,  0.3443,  0.0113,  0.1452]],
       dtype=torch.float64)
	q_value: tensor([[-34.4586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7481744318574446, distance: 0.5742574055205303 entropy -2.5794853534995568
epoch: 49, step: 17
	action: tensor([[-0.1553,  0.7422,  0.1581, -0.8774,  0.1435, -0.1722, -0.2592]],
       dtype=torch.float64)
	q_value: tensor([[-36.8868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5742574055205303 entropy -2.6558879082775304
epoch: 49, step: 18
	action: tensor([[ 0.2165,  0.1328, -0.0744,  0.3454, -0.3074,  0.1418,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6820583501358575, distance: 0.6452535536506486 entropy -3.323042468691012
epoch: 49, step: 19
	action: tensor([[ 0.0306,  0.2380,  0.0103,  0.2345,  0.1050, -0.1131,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6452535536506486 entropy -3.323042468691012
epoch: 49, step: 20
	action: tensor([[-0.3208,  0.1605,  0.0353,  0.5168,  0.3096,  0.1595,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28889623601186354, distance: 0.9649908201953891 entropy -3.323042468691012
epoch: 49, step: 21
	action: tensor([[ 0.1695, -0.3216,  0.2146,  0.4030,  0.5594,  0.2746,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[-40.6870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5287207442791579, distance: 0.7855898347471166 entropy -2.886109841220054
epoch: 49, step: 22
	action: tensor([[-0.1198,  0.1606, -0.0416,  0.4592, -0.1079,  0.1677,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4349480125413885, distance: 0.860202888743331 entropy -3.323042468691012
epoch: 49, step: 23
	action: tensor([[ 0.1654,  0.1595,  0.4184,  0.0821, -0.1907,  0.1224,  0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-40.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.860202888743331 entropy -2.7215562664607207
epoch: 49, step: 24
	action: tensor([[ 0.0680,  0.0324, -0.0150,  0.3440,  0.4244,  0.0255,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5100079177972201, distance: 0.8010345058554601 entropy -3.323042468691012
epoch: 49, step: 25
	action: tensor([[-0.0722, -0.1367, -0.1571,  0.0152, -0.0569,  0.0341,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10982711108516607, distance: 1.0796770623310188 entropy -3.323042468691012
epoch: 49, step: 26
	action: tensor([[-0.9972,  0.1208, -0.0572, -0.5154, -0.0931,  0.3180,  0.0706]],
       dtype=torch.float64)
	q_value: tensor([[-35.3298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0312754458801074, distance: 1.6309517108654652 entropy -2.881824217472969
epoch: 49, step: 27
	action: tensor([[ 0.0115,  0.1266, -0.1733, -0.1249,  0.2531,  0.4342,  0.0582]],
       dtype=torch.float64)
	q_value: tensor([[-37.3619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4491214213042718, distance: 0.8493459802117637 entropy -3.133284444102308
epoch: 49, step: 28
	action: tensor([[-0.1914, -0.4613, -0.0920, -0.6260, -0.4058,  0.2110, -0.0499]],
       dtype=torch.float64)
	q_value: tensor([[-35.8263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5361767780753715, distance: 1.4183300068623288 entropy -2.8601991762648344
epoch: 49, step: 29
	action: tensor([[ 0.0436,  0.5006,  0.3512,  0.1248, -1.2346,  0.5534,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[-33.6641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4183300068623288 entropy -2.582851438062865
epoch: 49, step: 30
	action: tensor([[ 0.0092,  0.0419, -0.2504,  0.3812,  0.1444,  0.1106,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4701408728630091, distance: 0.8329844672131649 entropy -3.323042468691012
epoch: 49, step: 31
	action: tensor([[-0.1490, -0.1576, -0.1978, -0.4962,  0.4422, -0.1401,  0.0504]],
       dtype=torch.float64)
	q_value: tensor([[-38.2381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20425575913588134, distance: 1.2557872207306011 entropy -2.923643384389463
epoch: 49, step: 32
	action: tensor([[ 0.4345,  0.1767,  0.1517,  0.0177, -0.0717, -0.0776,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7699601765445189, distance: 0.5488557344549386 entropy -3.323042468691012
epoch: 49, step: 33
	action: tensor([[ 0.2023,  0.0747, -0.0427,  0.5106, -0.2168,  0.1875,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7112965974503539, distance: 0.6148690662236156 entropy -3.323042468691012
epoch: 49, step: 34
	action: tensor([[-0.4340, -0.5131, -0.0298,  0.0597,  0.7263,  0.4542,  0.1264]],
       dtype=torch.float64)
	q_value: tensor([[-41.2158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6148690662236156 entropy -2.5315964467743255
epoch: 49, step: 35
	action: tensor([[-0.2426,  0.2932,  0.2803, -0.0404,  0.1228, -0.0894,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12843940945814936, distance: 1.0683301511045338 entropy -3.323042468691012
epoch: 49, step: 36
	action: tensor([[-0.0688, -0.3494,  0.0215, -0.2899, -0.0078, -0.0466,  0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-38.5090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2714854040022414, distance: 1.2903644298659331 entropy -2.8938035689840924
epoch: 49, step: 37
	action: tensor([[ 0.1934, -0.2481,  0.0407, -0.0354, -0.0201,  0.1958,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2978808499956306, distance: 0.9588752356122315 entropy -3.323042468691012
epoch: 49, step: 38
	action: tensor([[ 0.1207,  0.0225, -0.1615, -0.9293,  0.1666, -0.0291,  0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-35.6332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06643320224064986, distance: 1.105679842495558 entropy -2.5398200067692414
epoch: 49, step: 39
	action: tensor([[ 0.3951, -0.0084, -0.0749, -0.0962, -0.6141,  0.1998, -0.1672]],
       dtype=torch.float64)
	q_value: tensor([[-33.1282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5883236847443404, distance: 0.7342342412730709 entropy -2.922787899731809
epoch: 49, step: 40
	action: tensor([[ 0.0693, -0.1450, -0.6489, -0.0764,  0.7415, -0.1103,  0.0873]],
       dtype=torch.float64)
	q_value: tensor([[-36.8998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21628718174412487, distance: 1.0130600010181672 entropy -2.4207555310688535
epoch: 49, step: 41
	action: tensor([[-0.3789,  0.0449, -0.2227, -0.4155, -0.2767, -0.0724, -0.0769]],
       dtype=torch.float64)
	q_value: tensor([[-32.9156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2098179179916395, distance: 1.2586839647003008 entropy -3.330611969955512
epoch: 49, step: 42
	action: tensor([[ 0.3772, -0.3423,  0.0631, -0.0949, -0.4869,  0.0208,  0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-33.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2502565840029073, distance: 0.990861658303457 entropy -3.3254123854118944
epoch: 49, step: 43
	action: tensor([[ 0.9026, -1.0544, -1.0884,  0.5651, -0.5358,  0.2695,  0.0727]],
       dtype=torch.float64)
	q_value: tensor([[-36.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10365678525439792, distance: 1.2021916636075956 entropy -2.391122376038518
epoch: 49, step: 44
	action: tensor([[-0.3850, -0.6690, -0.1091,  0.6000, -0.0793,  0.1010, -0.0878]],
       dtype=torch.float64)
	q_value: tensor([[-39.3633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3242773979919462, distance: 1.3168799235109607 entropy -2.298846362849294
epoch: 49, step: 45
	action: tensor([[ 0.3165,  0.1006,  0.7460, -0.3318,  0.3637,  0.4084,  0.2179]],
       dtype=torch.float64)
	q_value: tensor([[-36.7447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5640209687546396, distance: 0.7555957287877727 entropy -2.633803164624846
epoch: 49, step: 46
	action: tensor([[-1.3196, -0.6876, -0.1244, -0.6020,  0.3623,  0.5607, -0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-39.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4374206265041929, distance: 1.7865778046165566 entropy -2.1905228413845155
epoch: 49, step: 47
	action: tensor([[-0.1027,  0.2620, -0.4313, -0.0138,  0.2986, -0.2969, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[-37.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32738013686123535, distance: 0.9385156898383757 entropy -2.905381892593696
epoch: 49, step: 48
	action: tensor([[ 0.0085,  0.1405, -0.0947,  0.2082,  0.2183, -0.0512, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[-35.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9385156898383757 entropy -3.652372129469857
epoch: 49, step: 49
	action: tensor([[ 0.3244,  0.1889, -0.0882, -0.1481,  0.0790,  0.0128,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6725191093284688, distance: 0.6548618259600126 entropy -3.323042468691012
epoch: 49, step: 50
	action: tensor([[-0.5507,  0.1868, -0.2704,  0.1884, -0.3807,  0.2573, -0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-36.6464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04037811764481036, distance: 1.1672188640412275 entropy -2.816085702933417
epoch: 49, step: 51
	action: tensor([[ 0.0550, -0.2943, -0.3164, -0.4173,  0.1845,  0.1930,  0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-37.8054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1672188640412275 entropy -3.015996202135454
epoch: 49, step: 52
	action: tensor([[-0.3872,  0.0054, -0.1790,  0.1895, -0.0864, -0.0555,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1432611849129064, distance: 1.2235716974754074 entropy -3.323042468691012
epoch: 49, step: 53
	action: tensor([[-0.2374, -0.4711,  0.0171,  0.0544,  0.1627,  0.2272,  0.1011]],
       dtype=torch.float64)
	q_value: tensor([[-36.9259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27996908877776594, distance: 1.2946620982897732 entropy -3.1074034140639983
epoch: 49, step: 54
	action: tensor([[ 0.4439, -0.6644, -0.3427,  0.2962, -0.0530,  0.1747,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[-34.6613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2026376207247178, distance: 1.0218439167948326 entropy -2.671710775153522
epoch: 49, step: 55
	action: tensor([[-0.1711, -0.5301, -0.4924, -1.1285,  0.3877, -0.0328,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[-35.1565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32635749561077043, distance: 1.3179137567479382 entropy -2.490859066588974
epoch: 49, step: 56
	action: tensor([[-0.2243, -0.0833,  0.3962,  0.5938,  0.3023, -0.2025, -0.1650]],
       dtype=torch.float64)
	q_value: tensor([[-32.3788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3179137567479382 entropy -3.007631878773022
epoch: 49, step: 57
	action: tensor([[-0.0994, -0.1807,  0.2415,  0.5735, -0.2295,  0.0440,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3969524456159814, distance: 0.8886535364859566 entropy -3.323042468691012
epoch: 49, step: 58
	action: tensor([[ 0.4572,  0.0700, -1.2557, -0.7360, -0.7356,  0.2318,  0.2072]],
       dtype=torch.float64)
	q_value: tensor([[-40.4836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6926537903867909, distance: 0.6344108823301822 entropy -2.493474700425479
epoch: 49, step: 59
	action: tensor([[-0.4254,  0.3733,  0.3292, -0.0858,  0.4300,  0.1832, -0.0919]],
       dtype=torch.float64)
	q_value: tensor([[-37.2609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6344108823301822 entropy -2.889066912519397
epoch: 49, step: 60
	action: tensor([[-0.2034,  0.0674, -0.0514,  0.1095,  0.4163, -0.2900,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6344108823301822 entropy -3.323042468691012
epoch: 49, step: 61
	action: tensor([[-0.2426, -0.1129,  0.0277, -0.3362, -0.2429, -0.1120,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27887276092795354, distance: 1.2941075231089623 entropy -3.323042468691012
epoch: 49, step: 62
	action: tensor([[ 0.6217, -0.0233,  0.2473, -0.2793, -0.1008,  0.2216,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-34.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.601723978750416, distance: 0.7221855146079955 entropy -2.9016419479548614
epoch: 49, step: 63
	action: tensor([[-0.9593,  0.1450, -0.3554, -0.8339, -0.5284,  0.7221, -0.0797]],
       dtype=torch.float64)
	q_value: tensor([[-37.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.000474408843631, distance: 1.6185390922013143 entropy -2.316544521776254
LOSS epoch 49 actor 544.0746614387301 critic 713.8429176260377
epoch: 50, step: 0
	action: tensor([[ 0.2732, -0.4628,  0.5330, -0.0406, -0.1703,  0.1206,  0.0691]],
       dtype=torch.float64)
	q_value: tensor([[-40.3370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08255219390466473, distance: 1.0960929300664883 entropy -2.934262294499455
epoch: 50, step: 1
	action: tensor([[ 0.5798, -0.7981, -0.3536,  1.0445, -0.2814,  0.6499,  0.0737]],
       dtype=torch.float64)
	q_value: tensor([[-37.8243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6218310650903538, distance: 0.7037195534931441 entropy -2.223329350538702
epoch: 50, step: 2
	action: tensor([[-0.5331, -0.1115, -0.0605, -0.2760, -0.7002,  0.4220,  0.0912]],
       dtype=torch.float64)
	q_value: tensor([[-43.2001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6543634131883755, distance: 1.4718791446784658 entropy -2.132731854376317
epoch: 50, step: 3
	action: tensor([[ 0.2606,  0.0976, -0.1536,  1.1652,  0.2568,  0.3184,  0.2214]],
       dtype=torch.float64)
	q_value: tensor([[-39.9171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4718791446784658 entropy -2.479592835750897
epoch: 50, step: 4
	action: tensor([[ 0.2173,  0.2425, -0.3540,  0.3481, -0.0931, -0.0607,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.659331609596818, distance: 0.6679171771163515 entropy -3.3229197997099087
epoch: 50, step: 5
	action: tensor([[ 0.1534, -0.0398,  0.0448, -0.1386,  0.0341,  0.1610,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3617378714673378, distance: 0.9142316062495482 entropy -3.3229197997099087
epoch: 50, step: 6
	action: tensor([[-0.6691, -0.2587, -0.2045, -0.0963, -0.3400, -0.4967,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-37.3573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6704382771867905, distance: 1.4790127224170802 entropy -2.663767965425618
epoch: 50, step: 7
	action: tensor([[ 0.5383,  0.1861,  0.4209, -0.2390, -0.1333,  0.1306,  0.0925]],
       dtype=torch.float64)
	q_value: tensor([[-36.8821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7334836983087185, distance: 0.5907702028711266 entropy -3.4042440702293946
epoch: 50, step: 8
	action: tensor([[-0.7570, -0.5938,  0.5291, -0.8087, -0.6394,  0.9656, -0.0545]],
       dtype=torch.float64)
	q_value: tensor([[-40.8557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.162080348158165, distance: 1.682645420805624 entropy -2.319002037603441
epoch: 50, step: 9
	action: tensor([[ 0.3318,  0.2010, -0.8156,  1.2056,  0.1281,  0.5301,  0.2480]],
       dtype=torch.float64)
	q_value: tensor([[-41.1082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.682645420805624 entropy -1.935378282726137
epoch: 50, step: 10
	action: tensor([[ 0.1167, -0.1734,  0.0705,  0.2959,  0.1216, -0.0148,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.682645420805624 entropy -3.3229197997099087
epoch: 50, step: 11
	action: tensor([[ 0.1246, -0.2508,  0.3376,  0.2055, -0.0215,  0.2313,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4351546748262676, distance: 0.8600455689250518 entropy -3.3229197997099087
epoch: 50, step: 12
	action: tensor([[-0.1518, -0.1239,  0.2255,  0.1431,  0.5876,  1.0697,  0.1105]],
       dtype=torch.float64)
	q_value: tensor([[-39.2768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5668692979263721, distance: 0.7531234630539808 entropy -2.3967461340980223
epoch: 50, step: 13
	action: tensor([[ 0.0336, -0.1092,  0.6438, -0.2745,  0.1375,  0.6394, -0.0986]],
       dtype=torch.float64)
	q_value: tensor([[-40.7207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31191265778593946, distance: 0.9492453484793063 entropy -2.3366812018000367
epoch: 50, step: 14
	action: tensor([[-0.4772, -0.6811,  0.2401,  0.7073, -1.1929,  0.2012, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-38.4239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2638226894625042, distance: 1.28647030864221 entropy -2.185440092879629
epoch: 50, step: 15
	action: tensor([[ 0.4843, -0.5646, -0.6369, -0.7942,  1.4351,  0.7837,  0.4493]],
       dtype=torch.float64)
	q_value: tensor([[-46.3550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16862814976535467, distance: 1.0434084535662758 entropy -2.0282861469146405
epoch: 50, step: 16
	action: tensor([[-1.0110, -0.1014,  0.8289,  0.4828, -0.7279, -0.9612, -0.4447]],
       dtype=torch.float64)
	q_value: tensor([[-38.3893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7650185667792126, distance: 1.520307130360051 entropy -2.3571093986147007
epoch: 50, step: 17
	action: tensor([[-0.2267, -0.0404,  0.3083, -1.0760,  0.3137,  0.5372,  0.3293]],
       dtype=torch.float64)
	q_value: tensor([[-45.3006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37901965502184076, distance: 1.3438225403085622 entropy -2.391633712134422
epoch: 50, step: 18
	action: tensor([[-0.0983, -0.1377, -0.4790, -1.7191,  0.9375, -0.1283, -0.2387]],
       dtype=torch.float64)
	q_value: tensor([[-39.5333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05164048344589944, distance: 1.1735195832688017 entropy -2.3894624843539076
epoch: 50, step: 19
	action: tensor([[ 0.0289,  0.0774,  0.1717,  0.3975, -0.0678, -0.1060, -0.2502]],
       dtype=torch.float64)
	q_value: tensor([[-38.5151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1735195832688017 entropy -2.736999257896336
epoch: 50, step: 20
	action: tensor([[ 0.4292,  0.0039, -0.1476, -0.2726, -0.3340,  0.0126,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5436833832512109, distance: 0.7730184066613746 entropy -3.3229197997099087
epoch: 50, step: 21
	action: tensor([[-0.2151, -0.6058, -0.2107,  0.4455, -0.0039,  0.1412, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[-37.6729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15118009881075967, distance: 1.2278019811263703 entropy -2.6147206978672055
epoch: 50, step: 22
	action: tensor([[ 0.4866, -0.0910, -0.5228,  0.2531, -0.3303,  0.1962,  0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-36.9331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6978457859182936, distance: 0.6290295112696146 entropy -2.706700745074998
epoch: 50, step: 23
	action: tensor([[-0.2058, -0.0902, -0.3005,  0.7029, -0.3660, -0.0338,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-40.4248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6290295112696146 entropy -2.627700899117016
epoch: 50, step: 24
	action: tensor([[-0.1360, -0.3335, -0.1858,  0.2978, -0.1124, -0.0010,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020591508853546436, distance: 1.1560661062417412 entropy -3.3229197997099087
epoch: 50, step: 25
	action: tensor([[-0.3397, -0.2031, -0.5627, -0.0576,  0.0906,  0.2017,  0.1298]],
       dtype=torch.float64)
	q_value: tensor([[-37.8628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2753261832240117, distance: 1.2923118638919369 entropy -2.807492394882378
epoch: 50, step: 26
	action: tensor([[-0.1121,  0.2078,  0.2108,  0.4688,  0.0540, -0.0026,  0.0232]],
       dtype=torch.float64)
	q_value: tensor([[-36.1650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2923118638919369 entropy -3.336885977198126
epoch: 50, step: 27
	action: tensor([[-0.1394,  0.0626,  0.2359, -0.2514, -0.0286, -0.1122,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01695360113257771, distance: 1.1540038628016878 entropy -3.3229197997099087
epoch: 50, step: 28
	action: tensor([[ 0.1938, -0.2719,  0.6377, -1.1312,  0.0098,  0.0578,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[-37.8762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4716945449565062, distance: 1.3882431311722874 entropy -2.8146019700150995
epoch: 50, step: 29
	action: tensor([[-1.5531, -1.3485, -0.1797,  0.2564, -0.0642,  0.4334, -0.2183]],
       dtype=torch.float64)
	q_value: tensor([[-37.4428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3882431311722874 entropy -2.212395665168674
epoch: 50, step: 30
	action: tensor([[-0.1637, -0.0840,  0.0805, -0.3978, -0.2361, -0.1050,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19752840874098587, distance: 1.2522746976953045 entropy -3.3229197997099087
epoch: 50, step: 31
	action: tensor([[-0.2759, -0.1549, -0.2428, -0.3679, -0.0341,  0.3951,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-36.6086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21704528516322807, distance: 1.2624380111955589 entropy -2.834596196582779
epoch: 50, step: 32
	action: tensor([[ 0.2802, -0.1581, -0.0234, -0.2434, -0.0635,  0.0805,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-36.0751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27999381216411434, distance: 0.971012470937122 entropy -2.8645945631305345
epoch: 50, step: 33
	action: tensor([[-0.7315, -0.5808,  0.5138, -0.3576,  0.6564,  0.0231, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-36.0121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2212959341404845, distance: 1.7055321250726783 entropy -2.6320408096568744
epoch: 50, step: 34
	action: tensor([[ 0.4117, -0.3504,  0.3275,  0.1792, -0.7448,  0.1772, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-37.1836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46808428055457063, distance: 0.8345994722829143 entropy -2.6337631520105544
epoch: 50, step: 35
	action: tensor([[ 0.0594,  0.5946, -0.4254, -1.4654,  1.5432,  0.4963,  0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-39.9631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6149105941979699, distance: 0.7101293763204471 entropy -2.1284947719540965
epoch: 50, step: 36
	action: tensor([[-3.1209e-01, -7.9676e-01,  6.7711e-02,  9.5643e-02,  2.8111e-04,
         -4.0470e-03, -3.6778e-01]], dtype=torch.float64)
	q_value: tensor([[-44.0147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6697114710498435, distance: 1.4786909288819825 entropy -2.486780510174246
epoch: 50, step: 37
	action: tensor([[ 0.1853,  0.2005, -0.9046,  0.8733, -0.0465, -0.2388,  0.1737]],
       dtype=torch.float64)
	q_value: tensor([[-34.2326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4786909288819825 entropy -2.5993474811101613
epoch: 50, step: 38
	action: tensor([[ 0.3101, -0.0615,  0.0257, -0.2492,  0.0544,  0.1002,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41211139452237844, distance: 0.8774133028183744 entropy -3.3229197997099087
epoch: 50, step: 39
	action: tensor([[-0.1151,  0.7097,  0.3053, -0.6020, -0.0700,  0.0562, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[-36.5931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8774133028183744 entropy -2.6410588889229993
epoch: 50, step: 40
	action: tensor([[ 0.1930, -0.0959, -0.0324,  0.1587,  0.0742,  0.1505,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5243011155973196, distance: 0.7892648460933708 entropy -3.3229197997099087
epoch: 50, step: 41
	action: tensor([[ 0.7314, -0.3310, -0.3359, -0.6206, -0.0798,  0.6162,  0.0367]],
       dtype=torch.float64)
	q_value: tensor([[-38.6448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2755144283779226, distance: 0.9740282738534001 entropy -2.6707073841922697
epoch: 50, step: 42
	action: tensor([[ 0.5793, -0.5826,  1.0810, -0.4742, -0.3631,  1.2194, -0.1838]],
       dtype=torch.float64)
	q_value: tensor([[-35.5847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37372341624774386, distance: 0.9056070196610438 entropy -2.2718530158100703
epoch: 50, step: 43
	action: tensor([[ 1.0755, -4.4909, -0.7249, -0.0533, -0.3963,  1.4502,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-41.6565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9056070196610438 entropy -1.6201606372704245
epoch: 50, step: 44
	action: tensor([[ 0.1046,  0.0268, -0.2646, -0.3010,  0.2352, -0.0835,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28549856162602594, distance: 0.9672934500129698 entropy -3.3229197997099087
epoch: 50, step: 45
	action: tensor([[-0.7548, -0.2468,  0.0399, -0.5616, -0.5792, -0.0661, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-35.8187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9744316113410201, distance: 1.60796925672526 entropy -3.1438795350531477
epoch: 50, step: 46
	action: tensor([[ 0.2428,  0.2039,  0.1175, -0.7100, -0.0271,  0.0839,  0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-37.4507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3314808488527863, distance: 0.935650426302005 entropy -2.8520232232339526
epoch: 50, step: 47
	action: tensor([[ 0.3079, -0.1329,  0.2267, -0.2868,  0.7082,  0.2862, -0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-38.1016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3694182780238805, distance: 0.9087143417509165 entropy -2.625306448046083
epoch: 50, step: 48
	action: tensor([[-1.1379, -0.5255, -0.2503,  0.4147,  0.6789,  0.9492, -0.2233]],
       dtype=torch.float64)
	q_value: tensor([[-35.6759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8308018127399903, distance: 1.5483793123735519 entropy -2.5436037442377937
epoch: 50, step: 49
	action: tensor([[-0.5387, -0.2681, -0.1610,  0.4461, -0.5406, -0.0051,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-41.7477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3862798433258692, distance: 1.3473553388071873 entropy -2.844660146995043
epoch: 50, step: 50
	action: tensor([[-0.7620, -0.0347, -0.2788,  0.0324, -0.1557,  0.2530,  0.2294]],
       dtype=torch.float64)
	q_value: tensor([[-40.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.671503951267745, distance: 1.479484423241231 entropy -2.711492681072488
epoch: 50, step: 51
	action: tensor([[-0.2343, -0.2491, -0.0309,  0.0019,  0.2075,  0.0164,  0.1051]],
       dtype=torch.float64)
	q_value: tensor([[-39.8423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1716145616845095, distance: 1.2386513309855103 entropy -3.0977541264263904
epoch: 50, step: 52
	action: tensor([[-0.2629, -0.0872,  0.0234,  0.1472,  0.1563,  0.0513,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[-36.3826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00826191714847746, distance: 1.139607210734197 entropy -2.9400808221908763
epoch: 50, step: 53
	action: tensor([[-0.2406, -0.4395, -0.3972, -0.2884, -0.2099,  0.0257,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-37.9666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39249811147600044, distance: 1.350373792315817 entropy -2.918800281282916
epoch: 50, step: 54
	action: tensor([[ 0.0522, -0.0408,  0.1888, -0.2981,  0.2104,  0.1009,  0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-34.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16152804448054603, distance: 1.0478544544092623 entropy -3.0437712380142012
epoch: 50, step: 55
	action: tensor([[-0.3028,  0.2881,  0.2641,  0.4082, -0.4110, -0.0501, -0.0554]],
       dtype=torch.float64)
	q_value: tensor([[-36.6469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0478544544092623 entropy -2.6903305796837356
epoch: 50, step: 56
	action: tensor([[ 0.2322, -0.1725,  0.3113,  0.1826,  0.0409,  0.0198,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5085660104485701, distance: 0.802212248423504 entropy -3.3229197997099087
epoch: 50, step: 57
	action: tensor([[-0.2435, -0.0486,  0.5487,  0.0518, -0.4451,  0.3890,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[-39.2663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15516520340394946, distance: 1.0518228221855848 entropy -2.481009094658251
epoch: 50, step: 58
	action: tensor([[-0.3560, -0.0116,  0.4029,  0.0286, -0.2508, -0.1431,  0.2653]],
       dtype=torch.float64)
	q_value: tensor([[-41.6597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12998963272714326, distance: 1.2164490477569248 entropy -2.2038236465556373
epoch: 50, step: 59
	action: tensor([[-0.4637,  0.1390, -0.9850, -0.9702,  0.3491,  0.0126,  0.1612]],
       dtype=torch.float64)
	q_value: tensor([[-41.0665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23821441736282623, distance: 0.9987874295231133 entropy -2.564973467865962
epoch: 50, step: 60
	action: tensor([[-0.1358,  0.1651,  0.0321, -0.1843,  0.0475, -0.0613,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-36.8102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18115861425861557, distance: 1.0355154561133624 entropy -3.5040391976329635
epoch: 50, step: 61
	action: tensor([[-0.0749, -0.0705,  0.0424, -0.3122,  0.7091,  0.3487,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-37.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1545118163040513, distance: 1.0522294783980646 entropy -3.029234805946
epoch: 50, step: 62
	action: tensor([[-0.3163,  0.1891, -0.3151,  0.2073,  0.3570,  0.0212, -0.1860]],
       dtype=torch.float64)
	q_value: tensor([[-36.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0522294783980646 entropy -2.796616129906767
epoch: 50, step: 63
	action: tensor([[-0.1529,  0.1600, -0.0246, -0.0288,  0.1747,  0.2688,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-45.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2629967130102615, distance: 0.9824069035316758 entropy -3.3229197997099087
LOSS epoch 50 actor 660.9893434961568 critic 115.8082038845402
epoch: 51, step: 0
	action: tensor([[ 0.0767,  0.1355,  0.0802,  0.1009,  0.2563, -0.0184,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4787656969801771, distance: 0.8261771664785694 entropy -3.321510760490687
epoch: 51, step: 1
	action: tensor([[-0.0733, -0.7773, -0.4263, -0.1926, -0.4788,  0.3978, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[-39.1555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5236229957629335, distance: 1.4125227545082861 entropy -2.86571161883839
epoch: 51, step: 2
	action: tensor([[-0.3327,  0.4839, -0.0898,  0.3634, -0.1688,  0.2753,  0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-34.4258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4125227545082861 entropy -2.4506168637016734
epoch: 51, step: 3
	action: tensor([[-0.1412,  0.1471, -0.2451,  0.1170, -0.4065, -0.0758,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24025191541219937, distance: 0.9974508395119115 entropy -3.321510760490687
epoch: 51, step: 4
	action: tensor([[ 0.1095, -0.1718, -0.1569, -0.1899, -0.1027,  0.3983,  0.1085]],
       dtype=torch.float64)
	q_value: tensor([[-39.7359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23630193127831878, distance: 1.000040386866463 entropy -2.9618289044699186
epoch: 51, step: 5
	action: tensor([[-0.1054, -0.3094, -0.1466, -0.0689, -0.0270,  0.1044,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10018455632268397, distance: 1.200299058452044 entropy -3.321510760490687
epoch: 51, step: 6
	action: tensor([[-0.1283, -0.3609, -0.0667, -0.5517, -0.1513,  0.0467,  0.0653]],
       dtype=torch.float64)
	q_value: tensor([[-35.3218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.393266786996203, distance: 1.35074645209225 entropy -2.805103846692871
epoch: 51, step: 7
	action: tensor([[ 0.6595, -0.6355, -0.2683, -0.0581, -0.6423,  0.2439,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-34.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09340049497651548, distance: 1.0895933200997499 entropy -2.750156116182289
epoch: 51, step: 8
	action: tensor([[-0.7001, -1.0247,  0.9939, -0.4340,  0.3928,  0.6197,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-36.3399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1604276409316063, distance: 1.6820021857171763 entropy -2.2194459745741244
epoch: 51, step: 9
	action: tensor([[-0.9566, -0.5829, -0.9569, -2.0912,  0.6289,  0.6367, -0.0234]],
       dtype=torch.float64)
	q_value: tensor([[-38.0468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34202981401425114, distance: 0.9282389776311608 entropy -1.9649770583332482
epoch: 51, step: 10
	action: tensor([[-0.3029,  0.2285, -0.7764, -0.5985, -0.5791, -0.2698, -0.1895]],
       dtype=torch.float64)
	q_value: tensor([[-43.2643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28736845056834237, distance: 0.9660268911462357 entropy -2.765473086788771
epoch: 51, step: 11
	action: tensor([[-0.1508,  0.1119,  0.0457,  0.2354, -0.2937,  0.0679,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[-35.9585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9660268911462357 entropy -3.705870122873968
epoch: 51, step: 12
	action: tensor([[-0.0605,  0.2677,  0.1727,  0.0841, -0.1046, -0.0603,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.414019025288619, distance: 0.8759885936053625 entropy -3.321510760490687
epoch: 51, step: 13
	action: tensor([[ 0.4768, -0.1860, -0.1509,  0.5042,  0.2659,  0.2006,  0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-40.7923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7787298174297597, distance: 0.5382922663719971 entropy -2.7823680627618366
epoch: 51, step: 14
	action: tensor([[-0.3912, -0.2790,  0.1295, -0.2041, -0.1207, -0.0312,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5358204522449661, distance: 1.418165502055638 entropy -3.321510760490687
epoch: 51, step: 15
	action: tensor([[ 0.2628,  0.1387, -0.3802, -1.1579,  0.2841,  0.2177,  0.1112]],
       dtype=torch.float64)
	q_value: tensor([[-35.7578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31633120608132503, distance: 0.9461926535228183 entropy -2.7923442264711653
epoch: 51, step: 16
	action: tensor([[-0.2700,  0.4325, -0.1462,  0.0098,  0.4760,  0.1299, -0.2323]],
       dtype=torch.float64)
	q_value: tensor([[-35.4907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9461926535228183 entropy -2.790557688078217
epoch: 51, step: 17
	action: tensor([[-0.0948, -0.0437,  0.0690,  0.0180,  0.0462,  0.0886,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17437736267162862, distance: 1.0397944364173384 entropy -3.321510760490687
epoch: 51, step: 18
	action: tensor([[ 0.4551,  0.2712,  0.4434, -0.2623, -0.3047,  0.2297,  0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-37.5668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7372336998897988, distance: 0.5865992809288076 entropy -2.76281575754919
epoch: 51, step: 19
	action: tensor([[-1.4562,  0.1005,  0.5926,  0.3331, -0.0276,  0.0873,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[-40.9590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5865992809288076 entropy -2.2697288911264506
epoch: 51, step: 20
	action: tensor([[ 0.0522, -0.0198, -0.2607, -0.0678,  0.1925,  0.2937,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40157582459767593, distance: 0.8852404663293824 entropy -3.321510760490687
epoch: 51, step: 21
	action: tensor([[ 0.3893, -0.2083, -0.0052,  0.2364,  0.4170,  0.0162, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-36.7245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5775465660913931, distance: 0.7437827734788613 entropy -2.878376981660751
epoch: 51, step: 22
	action: tensor([[ 0.5190, -0.0603, -0.0573, -0.6513, -0.2699,  0.4258, -0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-36.4664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3897733715231192, distance: 0.8939274445345825 entropy -2.702813342541544
epoch: 51, step: 23
	action: tensor([[-0.4591,  0.0290,  0.5321, -0.5592, -0.0544,  0.0988, -0.0919]],
       dtype=torch.float64)
	q_value: tensor([[-35.2905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5938568981264409, distance: 1.4447122539541686 entropy -2.337805437987288
epoch: 51, step: 24
	action: tensor([[ 0.8303,  0.3912,  0.3866,  0.3021,  0.5645, -0.0550,  0.0534]],
       dtype=torch.float64)
	q_value: tensor([[-37.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4447122539541686 entropy -2.5660250692959794
epoch: 51, step: 25
	action: tensor([[-0.2233, -0.2958, -0.0354, -0.0484,  0.0297, -0.1506,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.288351366295982, distance: 1.2988944300515177 entropy -3.321510760490687
epoch: 51, step: 26
	action: tensor([[-0.3098,  0.2318, -0.1459, -0.4118,  0.4977,  0.4354,  0.0687]],
       dtype=torch.float64)
	q_value: tensor([[-35.3098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0830060563916255, distance: 1.0958217773096128 entropy -2.949414333105447
epoch: 51, step: 27
	action: tensor([[ 0.2964,  0.0976,  0.3988, -0.0378,  0.1069,  0.3006, -0.1151]],
       dtype=torch.float64)
	q_value: tensor([[-37.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6634188987776064, distance: 0.6638983003014641 entropy -3.0740023179865537
epoch: 51, step: 28
	action: tensor([[-0.7367,  0.5194, -1.1247,  0.3878, -1.3339,  0.0464, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-38.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6638983003014641 entropy -2.4154733628080867
epoch: 51, step: 29
	action: tensor([[-0.0529,  0.1773,  0.0934,  0.0633, -0.1322,  0.1796,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4076128890058496, distance: 0.8807638752091842 entropy -3.321510760490687
epoch: 51, step: 30
	action: tensor([[ 0.2914, -0.4192,  0.1129, -0.4351,  0.3285, -0.0530,  0.0986]],
       dtype=torch.float64)
	q_value: tensor([[-40.1178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14609132694748017, distance: 1.2250852366812304 entropy -2.663891152376626
epoch: 51, step: 31
	action: tensor([[-0.0352,  0.1915,  0.0953, -0.1554,  0.0717, -0.0143,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010385288500155, distance: 0.9567166048247091 entropy -3.321510760490687
epoch: 51, step: 32
	action: tensor([[-0.0231, -0.0816,  0.4831,  0.0776,  0.5299,  0.4075,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-38.4715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4126642968225972, distance: 0.8770006073299522 entropy -2.863517560040555
epoch: 51, step: 33
	action: tensor([[ 0.2603, -0.1195,  0.4853, -0.1201,  0.1014,  0.6283, -0.0662]],
       dtype=torch.float64)
	q_value: tensor([[-38.3892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5665921422553734, distance: 0.7533643823157854 entropy -2.458065674538737
epoch: 51, step: 34
	action: tensor([[-0.1927, -0.1883,  0.3250, -0.0961, -0.2031,  0.8588, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-38.0312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.155439325724267, distance: 1.0516521666290852 entropy -2.200385055672918
epoch: 51, step: 35
	action: tensor([[-0.5498, -0.6131, -0.6530, -0.2006,  0.9250,  1.3707,  0.1872]],
       dtype=torch.float64)
	q_value: tensor([[-38.6799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1966163320988359, distance: 1.2517977202581065 entropy -2.1616771539090633
epoch: 51, step: 36
	action: tensor([[-0.0028,  0.0292,  0.2045, -0.2683,  0.8311,  0.3218, -0.2304]],
       dtype=torch.float64)
	q_value: tensor([[-39.0511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2715965097037262, distance: 0.9766584284718666 entropy -2.6172757723759212
epoch: 51, step: 37
	action: tensor([[-0.0388, -0.5093, -0.0446,  0.0390, -0.0542,  0.1439, -0.2075]],
       dtype=torch.float64)
	q_value: tensor([[-36.1556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13106087098104346, distance: 1.2170255123650147 entropy -2.7235707353962875
epoch: 51, step: 38
	action: tensor([[-0.3833,  0.2889, -0.2752,  0.1480,  0.7766, -0.0697,  0.1215]],
       dtype=torch.float64)
	q_value: tensor([[-33.7211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2170255123650147 entropy -2.6333271092164785
epoch: 51, step: 39
	action: tensor([[-0.4660, -0.0176,  0.2102,  0.4521, -0.0219,  0.1237,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009799759784619444, distance: 1.1387232996950014 entropy -3.321510760490687
epoch: 51, step: 40
	action: tensor([[-0.4418, -0.1847,  0.5457, -0.1575,  0.0594,  0.4810,  0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-41.2081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3239636242605892, distance: 1.3167239037801732 entropy -2.6846123434240696
epoch: 51, step: 41
	action: tensor([[-0.4468,  0.2283, -0.3447, -0.6666, -0.1218, -0.2398,  0.1132]],
       dtype=torch.float64)
	q_value: tensor([[-38.7263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17015617313372267, distance: 1.2378801740381014 entropy -2.3066304062604153
epoch: 51, step: 42
	action: tensor([[-0.0110, -0.0674, -0.0822, -0.1260, -0.0903, -0.0623,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[-37.0340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12882750515476338, distance: 1.068092267184179 entropy -3.675631053514336
epoch: 51, step: 43
	action: tensor([[ 0.3349, -0.9885,  0.2414,  0.1975, -0.2377,  0.2824,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-35.8485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16951948553555396, distance: 1.2375433599400367 entropy -2.9258101398961034
epoch: 51, step: 44
	action: tensor([[-0.8988,  0.9871, -0.2413,  0.1789,  0.9031,  0.6056,  0.1089]],
       dtype=torch.float64)
	q_value: tensor([[-35.9069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2375433599400367 entropy -2.113773315090181
epoch: 51, step: 45
	action: tensor([[ 0.0753,  0.1190,  0.1795, -0.2331,  0.0277,  0.0368,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3371837021177291, distance: 0.9316510603881564 entropy -3.321510760490687
epoch: 51, step: 46
	action: tensor([[-0.4857,  0.2741, -0.4064, -0.6458, -0.3201,  0.2269, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-37.9144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16150883726338527, distance: 1.2332977878001894 entropy -2.68858883807539
epoch: 51, step: 47
	action: tensor([[ 0.4041, -0.0689, -0.2705,  0.2359,  0.0518, -0.0329,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[-37.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6470385941852383, distance: 0.6798612736389363 entropy -3.3745203787251614
epoch: 51, step: 48
	action: tensor([[ 0.1290, -0.2398, -0.4094, -0.4686,  1.2653,  0.2640,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[-37.6134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17125910297022873, distance: 1.0417561639892763 entropy -2.8146433015768046
epoch: 51, step: 49
	action: tensor([[ 0.2864, -0.3734,  0.0857,  0.1025,  0.0453, -0.0693, -0.2580]],
       dtype=torch.float64)
	q_value: tensor([[-35.4327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25649064648334663, distance: 0.9867335875927759 entropy -2.758052969584457
epoch: 51, step: 50
	action: tensor([[-0.2693, -0.1761,  0.1772,  0.3719,  0.4907,  0.3199,  0.0543]],
       dtype=torch.float64)
	q_value: tensor([[-34.4001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22016512772648367, distance: 1.010550495133126 entropy -2.5996960432706557
epoch: 51, step: 51
	action: tensor([[-0.0137, -0.3470, -0.2988, -0.2085,  0.2237,  0.6525,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-39.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10457934608958674, distance: 1.0828548526619575 entropy -2.703729229889419
epoch: 51, step: 52
	action: tensor([[-0.0802,  0.1883, -0.2924,  1.0030,  0.8082,  0.3834, -0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-34.8222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0828548526619575 entropy -2.626614523200294
epoch: 51, step: 53
	action: tensor([[-0.0882,  0.0005, -0.1007, -0.1473, -0.0762, -0.0620,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11143990937870907, distance: 1.0786985497438322 entropy -3.321510760490687
epoch: 51, step: 54
	action: tensor([[ 0.1483,  0.1495, -0.2061, -0.3143, -0.1675,  0.0891,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-36.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43603772913782224, distance: 0.8593730272791705 entropy -2.972194192185987
epoch: 51, step: 55
	action: tensor([[ 0.1970,  0.1941,  0.5290,  0.1655, -0.5654,  0.1089, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-36.7075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8593730272791705 entropy -2.8781324602161757
epoch: 51, step: 56
	action: tensor([[ 0.0173,  0.1237, -0.3112,  0.1524,  0.0637, -0.1405,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-44.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4106643132095935, distance: 0.8784925106916219 entropy -3.321510760490687
epoch: 51, step: 57
	action: tensor([[ 0.0283, -0.0391, -0.1843,  0.2123,  0.3158,  0.2506,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[-38.6467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4292535578939135, distance: 0.8645264793688483 entropy -3.190138298268668
epoch: 51, step: 58
	action: tensor([[-0.3367, -0.3439,  0.0165, -0.3353,  0.2694,  0.0665,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-37.7758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5079925256196711, distance: 1.4052587163183896 entropy -2.8955038957068497
epoch: 51, step: 59
	action: tensor([[-0.1570, -0.2352,  0.2232, -0.1234, -0.4298,  0.2156, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[-33.8939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11102372041108577, distance: 1.2061973191644804 entropy -2.925403509795949
epoch: 51, step: 60
	action: tensor([[-1.0040, -0.0130,  0.2834,  0.1030, -0.7618,  0.2253,  0.1972]],
       dtype=torch.float64)
	q_value: tensor([[-37.0996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9199277966948283, distance: 1.585620096255119 entropy -2.4117241257666886
epoch: 51, step: 61
	action: tensor([[ 0.8612, -0.4434,  0.2367,  0.7366, -0.7487,  0.3502,  0.3377]],
       dtype=torch.float64)
	q_value: tensor([[-43.1653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8474374326684908, distance: 0.4469723801159533 entropy -2.362619748675582
epoch: 51, step: 62
	action: tensor([[ 0.3417, -1.0566,  0.0182,  0.6105,  0.9011,  1.0494,  0.0721]],
       dtype=torch.float64)
	q_value: tensor([[-46.3187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41655217640964526, distance: 0.8740931269129755 entropy -1.9755290048354561
epoch: 51, step: 63
	action: tensor([[-2.3436, -0.5005, -0.1194, -0.8521, -0.7448,  1.1444, -0.1764]],
       dtype=torch.float64)
	q_value: tensor([[-39.2132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8740931269129755 entropy -2.1259087245807753
LOSS epoch 51 actor 640.7978051155549 critic 322.92474025285225
epoch: 52, step: 0
	action: tensor([[-0.1945,  0.1295,  0.1003,  0.1278,  0.4920,  0.0894,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22855914885094886, distance: 1.0050970764117657 entropy -3.3201727931787164
epoch: 52, step: 1
	action: tensor([[-0.4171, -0.4305, -0.0631,  0.2206,  0.3986,  0.2135, -0.0366]],
       dtype=torch.float64)
	q_value: tensor([[-37.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3776142163677172, distance: 1.343137582124098 entropy -2.9591779696817455
epoch: 52, step: 2
	action: tensor([[ 0.1389, -0.1022,  0.3299,  0.0045,  0.2992,  0.1142,  0.0531]],
       dtype=torch.float64)
	q_value: tensor([[-33.7130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.343137582124098 entropy -2.906709652755192
epoch: 52, step: 3
	action: tensor([[ 0.0133, -0.2244,  0.3009,  0.3342,  0.3476,  0.1077,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39362281668495636, distance: 0.8911034373993801 entropy -3.3201727931787164
epoch: 52, step: 4
	action: tensor([[-0.0670, -0.3829,  0.2261,  0.0427,  0.8627,  0.2268,  0.0443]],
       dtype=torch.float64)
	q_value: tensor([[-36.8384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027276315988895683, distance: 1.1286296059699883 entropy -2.5931137216789253
epoch: 52, step: 5
	action: tensor([[-0.5768,  0.0149,  0.1441,  0.5395,  0.4909,  0.5565, -0.1626]],
       dtype=torch.float64)
	q_value: tensor([[-34.4650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1286296059699883 entropy -2.6471587029884103
epoch: 52, step: 6
	action: tensor([[ 0.0599,  0.1211,  0.1323,  0.1400, -0.0400, -0.1190,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4808634826942344, distance: 0.8245129532943343 entropy -3.3201727931787164
epoch: 52, step: 7
	action: tensor([[-0.4698,  0.0162,  0.1690, -0.0062, -0.1129,  0.1182,  0.0686]],
       dtype=torch.float64)
	q_value: tensor([[-38.2204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2346326187579435, distance: 1.2715269417255155 entropy -2.761241320231054
epoch: 52, step: 8
	action: tensor([[-0.1030, -0.6814, -0.2336,  0.2433,  0.1871,  0.1206,  0.1385]],
       dtype=torch.float64)
	q_value: tensor([[-36.0570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2558798554147317, distance: 1.282421352371385 entropy -2.7648338696231165
epoch: 52, step: 9
	action: tensor([[ 0.0049, -0.5724,  0.2887,  0.4367, -0.2760,  0.0606,  0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-32.8466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1564975244608502, distance: 1.0509931223252367 entropy -2.738450012065915
epoch: 52, step: 10
	action: tensor([[-0.1418,  0.7625, -0.0234, -0.7515,  0.1773,  0.6323,  0.2171]],
       dtype=torch.float64)
	q_value: tensor([[-36.5255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0509931223252367 entropy -2.33000139937652
epoch: 52, step: 11
	action: tensor([[-0.2386, -0.1467,  0.0020, -0.0146,  0.0651,  0.1134,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10401662937037615, distance: 1.2023876331996952 entropy -3.3201727931787164
epoch: 52, step: 12
	action: tensor([[-0.1868,  0.0842, -0.2695,  0.0316,  0.6197,  0.4822,  0.0690]],
       dtype=torch.float64)
	q_value: tensor([[-34.5272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28164950365235, distance: 0.969895381653625 entropy -2.828615179997505
epoch: 52, step: 13
	action: tensor([[-0.2346, -0.3224, -0.0903, -0.4540,  0.0641, -0.1061, -0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-35.5387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.969895381653625 entropy -3.0521262578619823
epoch: 52, step: 14
	action: tensor([[-0.2710,  0.1043,  0.1056, -0.1203,  0.5981, -0.0109,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04834377146073443, distance: 1.1716787484641353 entropy -3.3201727931787164
epoch: 52, step: 15
	action: tensor([[-0.3582,  0.0349, -0.1575, -0.4636, -0.2518,  0.0545, -0.0886]],
       dtype=torch.float64)
	q_value: tensor([[-35.7181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2777091004938905, distance: 1.293518627761759 entropy -3.0721671985031214
epoch: 52, step: 16
	action: tensor([[-0.3857,  0.3180,  0.1395, -0.1604, -0.0485,  0.3052,  0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-33.1683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014144807415222727, distance: 1.1362221658227716 entropy -3.1353291959819303
epoch: 52, step: 17
	action: tensor([[-0.1896, -0.4773, -0.2960,  0.1035,  0.0878,  0.3006,  0.0794]],
       dtype=torch.float64)
	q_value: tensor([[-37.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23550431097289004, distance: 1.2719757329488988 entropy -2.7796772794847016
epoch: 52, step: 18
	action: tensor([[ 0.2600, -0.5611, -0.6041, -0.3120, -0.0502, -0.0420,  0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-33.0126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10940993703504054, distance: 1.2053209883953462 entropy -2.7889886925233753
epoch: 52, step: 19
	action: tensor([[ 0.3501,  0.1700,  0.2415,  0.2946, -0.0452, -0.0560, -0.0591]],
       dtype=torch.float64)
	q_value: tensor([[-31.2699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2053209883953462 entropy -2.907897723525141
epoch: 52, step: 20
	action: tensor([[-0.2588,  0.0024, -0.0888,  0.0977,  0.0517, -0.1690,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03440515871366001, distance: 1.1638634562809276 entropy -3.3201727931787164
epoch: 52, step: 21
	action: tensor([[-0.0865, -0.3097,  0.1340,  0.0264, -0.2202,  0.0594,  0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-35.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05455221394950893, distance: 1.1751430519841124 entropy -3.140755908768022
epoch: 52, step: 22
	action: tensor([[-1.4930,  0.1319,  0.0947, -0.1177,  0.1955,  0.0933,  0.1479]],
       dtype=torch.float64)
	q_value: tensor([[-34.6824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5432940363705496, distance: 1.824966848727809 entropy -2.561047373077428
epoch: 52, step: 23
	action: tensor([[-0.0248,  0.0393,  0.1064, -0.1200, -0.0453, -0.0747,  0.0639]],
       dtype=torch.float64)
	q_value: tensor([[-38.8055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19250230562573456, distance: 1.0283177652583935 entropy -3.2331409502998976
epoch: 52, step: 24
	action: tensor([[ 0.4998,  0.2237,  0.3746, -0.3546,  0.6761,  0.0127,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-35.4933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6433576555533934, distance: 0.6833971232265977 entropy -2.814496231177024
epoch: 52, step: 25
	action: tensor([[-0.7700, -0.5474, -0.1445, -0.0860, -1.1852,  0.3396, -0.2624]],
       dtype=torch.float64)
	q_value: tensor([[-37.4735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1595543445640089, distance: 1.681662198660683 entropy -2.5249554373021867
epoch: 52, step: 26
	action: tensor([[-0.4699,  0.7185,  0.1563, -0.8546,  0.5183, -0.0405,  0.3439]],
       dtype=torch.float64)
	q_value: tensor([[-37.5869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.681662198660683 entropy -2.3064840958405233
epoch: 52, step: 27
	action: tensor([[ 0.1091, -0.2750, -0.1142, -0.1178,  0.3563,  0.2033,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18033450598168466, distance: 1.0360364130805104 entropy -3.3201727931787164
epoch: 52, step: 28
	action: tensor([[ 0.6543, -0.2995, -0.3803, -0.0321, -0.1846, -0.1024, -0.0698]],
       dtype=torch.float64)
	q_value: tensor([[-33.1104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42343550545384356, distance: 0.8689216946911547 entropy -2.766604250729612
epoch: 52, step: 29
	action: tensor([[-0.8690,  0.4473, -0.6091,  1.1748,  0.0038, -0.1302, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-33.6587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8689216946911547 entropy -2.6604639473086924
epoch: 52, step: 30
	action: tensor([[ 0.0407, -0.0439,  0.3822, -0.2025,  0.0073, -0.1629,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11501075971491814, distance: 1.0765288885204758 entropy -3.3201727931787164
epoch: 52, step: 31
	action: tensor([[-0.1305, -0.1325, -0.3288, -0.1248, -0.0351,  0.4364,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[-35.6509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0829134262523834, distance: 1.0958771231280031 entropy -2.6356179550392875
epoch: 52, step: 32
	action: tensor([[ 0.2572, -0.0284, -0.1782, -0.2566, -0.3717,  0.1488,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-33.4850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4109573349452643, distance: 0.8782740872943904 entropy -2.8292239492141547
epoch: 52, step: 33
	action: tensor([[ 0.4038,  0.1616,  0.4553, -0.8644, -0.6834,  0.6586,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[-34.6604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3989617444462903, distance: 0.8871718454315002 entropy -2.624479354781671
epoch: 52, step: 34
	action: tensor([[-3.2998,  0.2377, -0.9466, -1.7555, -0.5860, -0.5373,  0.0312]],
       dtype=torch.float64)
	q_value: tensor([[-38.3557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8871718454315002 entropy -1.9775318449520114
epoch: 52, step: 35
	action: tensor([[ 0.3005,  0.0810, -0.4594, -0.0207, -0.1922, -0.0020,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6438298233609907, distance: 0.6829445900408957 entropy -3.3201727931787164
epoch: 52, step: 36
	action: tensor([[-1.1421e-01, -3.1925e-03,  1.1789e-01,  5.0688e-02, -5.0815e-01,
          2.5298e-01, -1.3935e-04]], dtype=torch.float64)
	q_value: tensor([[-36.1203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2561879017789861, distance: 0.9869344579407568 entropy -2.9400796324234615
epoch: 52, step: 37
	action: tensor([[-0.1887, -0.3239,  0.6018, -0.0307, -0.7122,  0.6130,  0.2055]],
       dtype=torch.float64)
	q_value: tensor([[-37.1128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011265432235593842, distance: 1.1378802311105725 entropy -2.457651371462448
epoch: 52, step: 38
	action: tensor([[ 1.3527, -0.4992,  1.5437, -0.7333,  0.0701,  0.8468,  0.3286]],
       dtype=torch.float64)
	q_value: tensor([[-39.4897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24957112948492943, distance: 0.9913145035161962 entropy -1.9589736830051556
epoch: 52, step: 39
	action: tensor([[ 1.0678,  1.0396, -1.9466, -0.4343,  2.5819,  0.7251, -0.4403]],
       dtype=torch.float64)
	q_value: tensor([[-45.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9913145035161962 entropy -1.5417028555507468
epoch: 52, step: 40
	action: tensor([[-0.1460, -0.2573,  0.1081, -0.2669,  0.4422,  0.0134,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24470287848231287, distance: 1.2767020043963753 entropy -3.3201727931787164
epoch: 52, step: 41
	action: tensor([[-0.2790, -0.0353, -0.2010,  0.0186,  0.4361,  0.4489, -0.0823]],
       dtype=torch.float64)
	q_value: tensor([[-32.6933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07704762693443745, distance: 1.099376219791006 entropy -2.873736326378645
epoch: 52, step: 42
	action: tensor([[ 0.2548, -0.3150,  0.0363, -0.3764, -0.0870,  0.0611, -0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-34.0386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016564734843321327, distance: 1.1348267963060248 entropy -3.023190903263939
epoch: 52, step: 43
	action: tensor([[-0.5257,  0.4132, -0.5807, -0.4118,  0.0534,  0.1263, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-32.3505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05141679589634962, distance: 1.173394770804703 entropy -2.563255199440597
epoch: 52, step: 44
	action: tensor([[-0.2242, -0.0865, -0.0193,  0.2214,  0.1063,  0.0930,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[-34.2637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07995106901661908, distance: 1.0976456378156383 entropy -3.7399417216614417
epoch: 52, step: 45
	action: tensor([[-0.0546,  0.1812, -0.2331, -0.1571, -0.4378,  0.2917,  0.0926]],
       dtype=torch.float64)
	q_value: tensor([[-35.8819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28863068521538304, distance: 0.9651709838893546 entropy -2.87232949416487
epoch: 52, step: 46
	action: tensor([[ 0.0363, -0.6858,  0.0618, -0.2921,  0.1699,  0.2108,  0.1009]],
       dtype=torch.float64)
	q_value: tensor([[-36.4540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4176138137113483, distance: 1.3624973350772447 entropy -2.7004776454645314
epoch: 52, step: 47
	action: tensor([[-0.1640, -0.3481,  0.6787,  0.0526,  0.5586,  0.1548, -0.0349]],
       dtype=torch.float64)
	q_value: tensor([[-31.5458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0872273227578404, distance: 1.1932099665828781 entropy -2.4704665947037934
epoch: 52, step: 48
	action: tensor([[-1.0160, -0.1047,  0.7070, -0.2996,  0.7115, -0.2124, -0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-36.0729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3803901202583853, distance: 1.7655530148680514 entropy -2.4150639921172044
epoch: 52, step: 49
	action: tensor([[-0.5968, -0.1211, -0.5468,  0.2068,  0.4967,  0.2086, -0.0622]],
       dtype=torch.float64)
	q_value: tensor([[-38.2080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5036476166052748, distance: 1.4032328025487233 entropy -2.777765388984842
epoch: 52, step: 50
	action: tensor([[-0.4429, -0.0864, -0.0769, -0.0269,  0.3380,  0.0213,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-33.9108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33936093665289246, distance: 1.3243583296765034 entropy -3.5777749303692237
epoch: 52, step: 51
	action: tensor([[-0.3391,  0.0122, -0.0339,  0.0978,  0.1440, -0.0761,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-34.0591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10155616604321294, distance: 1.2010470370702973 entropy -3.228213225138084
epoch: 52, step: 52
	action: tensor([[ 0.1261,  0.1048,  0.2867, -0.4939, -0.4226,  0.0138,  0.0604]],
       dtype=torch.float64)
	q_value: tensor([[-35.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1829426074570305, distance: 1.034386812729095 entropy -3.141298111592581
epoch: 52, step: 53
	action: tensor([[ 0.0016, -0.3271,  0.2585, -0.2788, -0.1505,  0.0566,  0.0433]],
       dtype=torch.float64)
	q_value: tensor([[-36.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17739498025004563, distance: 1.24170315102842 entropy -2.4879095024288476
epoch: 52, step: 54
	action: tensor([[-0.7480, -1.0008, -1.0438, -0.7268,  0.3827,  0.0056,  0.0655]],
       dtype=torch.float64)
	q_value: tensor([[-33.5977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5880969192020566, distance: 1.4420993958113901 entropy -2.4875467331898027
epoch: 52, step: 55
	action: tensor([[ 0.0091,  0.0631, -0.0272,  0.3091,  0.0992, -0.0671, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-32.3729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4420993958113901 entropy -3.3257910720382156
epoch: 52, step: 56
	action: tensor([[ 0.1297, -0.3451,  0.1160,  0.2712,  0.0068, -0.1347,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2676780492927491, distance: 0.9792818812587264 entropy -3.3201727931787164
epoch: 52, step: 57
	action: tensor([[ 0.7142, -0.2528, -0.1401, -0.4733,  0.0857,  0.4735,  0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-35.5851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3925409304945473, distance: 0.8918980277080812 entropy -2.6328155532816706
epoch: 52, step: 58
	action: tensor([[-0.4684, -0.5368,  0.4639, -1.0328, -0.0929,  0.7449, -0.1824]],
       dtype=torch.float64)
	q_value: tensor([[-33.5519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9267783472621551, distance: 1.5884464260606919 entropy -2.3240859518217376
epoch: 52, step: 59
	action: tensor([[ 0.3416, -0.9667, -0.9901, -0.4389,  0.5874, -0.5167, -0.0328]],
       dtype=torch.float64)
	q_value: tensor([[-34.3164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4624821370750132, distance: 1.3838912978019506 entropy -2.124527502593407
epoch: 52, step: 60
	action: tensor([[ 0.2098, -0.1811,  0.3492, -0.0626, -0.2406, -0.0376, -0.1256]],
       dtype=torch.float64)
	q_value: tensor([[-29.8786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2924506184879365, distance: 0.9625760929481734 entropy -3.0318863697686673
epoch: 52, step: 61
	action: tensor([[ 0.9576,  0.0018, -1.0279, -0.2907,  0.6214,  0.5819,  0.0943]],
       dtype=torch.float64)
	q_value: tensor([[-35.1624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.789295205587363, distance: 0.5252836729679258 entropy -2.448620278288918
epoch: 52, step: 62
	action: tensor([[-0.1642,  0.0868, -0.1123,  0.0198,  0.5352,  0.8371, -0.2556]],
       dtype=torch.float64)
	q_value: tensor([[-35.0929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5252836729679258 entropy -2.6136770217660223
epoch: 52, step: 63
	action: tensor([[-0.3244,  0.0021, -0.1602, -0.0594,  0.3263, -0.0530,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-39.7004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1587439858465951, distance: 1.2318290444446578 entropy -3.3201727931787164
LOSS epoch 52 actor 561.3467761552513 critic 128.63474288499845
epoch: 53, step: 0
	action: tensor([[-0.1380, -0.0110, -0.0329,  0.0445,  0.1373,  0.0161, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-32.1285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12129046535766108, distance: 1.072702672692165 entropy -3.320115505865004
epoch: 53, step: 1
	action: tensor([[ 0.3902,  0.2181, -0.0865,  0.2737, -0.0722,  0.2369,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[-32.9778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.072702672692165 entropy -2.9825703056890025
epoch: 53, step: 2
	action: tensor([[-0.0334, -0.0220,  0.0899,  0.3759,  0.1130,  0.0065,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4269639533149243, distance: 0.8662588095677816 entropy -3.3191093420229527
epoch: 53, step: 3
	action: tensor([[-0.3228,  0.5712, -0.3021, -1.0003, -0.1246,  0.2788,  0.0929]],
       dtype=torch.float64)
	q_value: tensor([[-36.5059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17434746361785436, distance: 1.0398132637767794 entropy -2.7385592402379006
epoch: 53, step: 4
	action: tensor([[-0.1987, -0.2347,  0.2563,  0.0290, -0.2124, -0.0332, -0.0679]],
       dtype=torch.float64)
	q_value: tensor([[-32.7590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11352967285475413, distance: 1.207556862866184 entropy -3.1913200815518534
epoch: 53, step: 5
	action: tensor([[ 0.7967, -0.1460,  0.2727, -0.2798,  0.2443,  0.4772,  0.1660]],
       dtype=torch.float64)
	q_value: tensor([[-33.1039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6075557853009292, distance: 0.7168786704875375 entropy -2.608556137153074
epoch: 53, step: 6
	action: tensor([[-0.4601, -1.0171,  0.3704,  1.3043, -0.6634,  0.4910, -0.2013]],
       dtype=torch.float64)
	q_value: tensor([[-35.2462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16209451948365972, distance: 1.0475004272756658 entropy -2.186958403206682
epoch: 53, step: 7
	action: tensor([[-0.3548, -0.5903,  0.0640, -0.4203,  0.8436, -0.0393,  0.4992]],
       dtype=torch.float64)
	q_value: tensor([[-42.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7808872888692611, distance: 1.527126137366014 entropy -1.9334686440663014
epoch: 53, step: 8
	action: tensor([[-0.3652,  0.2521,  0.1492, -0.4377,  0.0759,  0.2388, -0.2165]],
       dtype=torch.float64)
	q_value: tensor([[-31.5846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12195203889684203, distance: 1.2121150381052863 entropy -2.7520327171184142
epoch: 53, step: 9
	action: tensor([[ 0.1862, -0.0174, -0.1456, -0.1543, -0.0138,  0.0508,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[-31.8053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3919469273158793, distance: 0.8923339919068837 entropy -2.9353641554345944
epoch: 53, step: 10
	action: tensor([[-0.2178, -0.1211, -0.0405,  0.0680, -0.0604,  0.0232, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-32.4855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0181485088362785, distance: 1.1546816337866277 entropy -2.838790167025544
epoch: 53, step: 11
	action: tensor([[-0.6499, -0.4427,  0.2951, -0.5490, -0.9006,  0.4878,  0.1070]],
       dtype=torch.float64)
	q_value: tensor([[-32.6914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0887721068461529, distance: 1.6538732501317208 entropy -2.8899276093226085
epoch: 53, step: 12
	action: tensor([[ 1.4566, -0.2118,  1.4781,  0.5745, -1.2897,  0.2944,  0.3088]],
       dtype=torch.float64)
	q_value: tensor([[-34.4282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21837357350188658, distance: 1.0117106236761273 entropy -2.1329016270438257
epoch: 53, step: 13
	action: tensor([[-2.2982,  0.3180,  2.2736, -2.5733,  0.8156, -0.7241,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-55.2315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0117106236761273 entropy -1.548924211439415
epoch: 53, step: 14
	action: tensor([[-0.0071, -0.0352, -0.0734,  0.1017, -0.1233,  0.0879,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3262267915046795, distance: 0.9393199844750186 entropy -3.3191093420229527
epoch: 53, step: 15
	action: tensor([[-0.4256,  0.0270,  0.0670,  0.6110, -0.0744, -0.0877,  0.0943]],
       dtype=torch.float64)
	q_value: tensor([[-34.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9393199844750186 entropy -2.7696073800336807
epoch: 53, step: 16
	action: tensor([[-0.3991, -0.0253,  0.0009, -0.3587,  0.2100,  0.0303,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.387351299986725, distance: 1.3478759241784564 entropy -3.3191093420229527
epoch: 53, step: 17
	action: tensor([[-0.4975, -0.2574,  0.0273,  0.5748,  0.5004, -0.0487, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[-31.1602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2329648441775578, distance: 1.270667845298311 entropy -3.0811417909852965
epoch: 53, step: 18
	action: tensor([[-0.0412,  0.1619, -0.3353,  0.1053,  0.5803,  0.1421,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-35.1513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.270667845298311 entropy -3.013102392496587
epoch: 53, step: 19
	action: tensor([[ 0.1045, -0.0697, -0.1440, -0.2003, -0.0372,  0.1628,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27585028666199607, distance: 0.9738024768190571 entropy -3.3191093420229527
epoch: 53, step: 20
	action: tensor([[-0.3857, -0.0842, -0.2181,  0.1402,  0.2301,  0.0405,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[-32.3245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16026128787701044, distance: 1.2326352817500639 entropy -2.7632305528359002
epoch: 53, step: 21
	action: tensor([[ 0.1984,  0.1471,  0.0124, -0.0617, -0.1560,  0.1221,  0.0469]],
       dtype=torch.float64)
	q_value: tensor([[-32.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5756060418397477, distance: 0.7454890856081832 entropy -3.2649134081178097
epoch: 53, step: 22
	action: tensor([[-1.3232,  0.1315,  0.0693,  0.2561,  0.6158, -0.0784,  0.0458]],
       dtype=torch.float64)
	q_value: tensor([[-35.1463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1645079163439482, distance: 1.683589786855679 entropy -2.658675440610332
epoch: 53, step: 23
	action: tensor([[-0.1992, -0.1653,  0.1935, -0.0822, -0.1012, -0.0045,  0.0376]],
       dtype=torch.float64)
	q_value: tensor([[-35.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13656665924592537, distance: 1.2199840402078757 entropy -3.367729915862422
epoch: 53, step: 24
	action: tensor([[-0.1464,  0.2868,  0.7037,  0.2187,  0.5974,  0.1996,  0.1105]],
       dtype=torch.float64)
	q_value: tensor([[-32.6801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2199840402078757 entropy -2.6973074001950255
epoch: 53, step: 25
	action: tensor([[ 0.0355, -0.1908,  0.1417,  0.0542, -0.0691,  0.0021,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22875484351960884, distance: 1.0049695847093487 entropy -3.3191093420229527
epoch: 53, step: 26
	action: tensor([[-0.9460, -0.5357,  0.4253,  0.4581,  0.2510,  0.4602,  0.0877]],
       dtype=torch.float64)
	q_value: tensor([[-33.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6434015743239496, distance: 1.4669946925484456 entropy -2.639129363362151
epoch: 53, step: 27
	action: tensor([[-0.4135,  0.4591, -0.2781, -1.4867,  0.2034,  0.3497,  0.2180]],
       dtype=torch.float64)
	q_value: tensor([[-35.3894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02189337445196904, distance: 1.1317481510371676 entropy -2.4272483648933005
epoch: 53, step: 28
	action: tensor([[-0.2716,  0.0655,  0.6857, -0.0705,  0.2136,  0.0618, -0.1671]],
       dtype=torch.float64)
	q_value: tensor([[-33.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006707173059299354, distance: 1.1405001398952161 entropy -2.948471006758266
epoch: 53, step: 29
	action: tensor([[ 0.4628,  0.5640,  0.4209, -0.3277,  0.0527,  0.4868,  0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-34.7786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1405001398952161 entropy -2.517370716288851
epoch: 53, step: 30
	action: tensor([[-0.1940,  0.1854,  0.1509,  0.0513,  0.0107,  0.0291,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20219909228674027, distance: 1.0221248718746092 entropy -3.3191093420229527
epoch: 53, step: 31
	action: tensor([[-0.4964,  0.0813,  0.2967, -0.0971, -0.3729, -0.4851,  0.0791]],
       dtype=torch.float64)
	q_value: tensor([[-35.4176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40627439460107007, distance: 1.3570371215301507 entropy -2.8244085215552603
epoch: 53, step: 32
	action: tensor([[ 0.3201, -0.1748, -0.2923,  0.3064, -0.1443,  0.4599,  0.1352]],
       dtype=torch.float64)
	q_value: tensor([[-35.2076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5904062975495508, distance: 0.7323746924097321 entropy -2.885146280835913
epoch: 53, step: 33
	action: tensor([[-0.1935,  0.0387, -0.1025, -0.5405, -0.1421,  0.0207,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1251995867060971, distance: 1.2138680351079398 entropy -3.3191093420229527
epoch: 53, step: 34
	action: tensor([[-0.2954, -0.4887,  0.2870,  0.1692,  0.5975,  0.0966,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-31.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30291041559266185, distance: 1.3062129082066485 entropy -2.982909266248597
epoch: 53, step: 35
	action: tensor([[ 0.2642, -0.7093,  0.0888,  1.1078, -0.1420,  0.3444, -0.0177]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6947059999092746, distance: 0.6322892933340205 entropy -2.683970101326143
epoch: 53, step: 36
	action: tensor([[ 0.2333,  0.3719, -0.2968, -0.0108,  1.4092,  0.3927,  0.2085]],
       dtype=torch.float64)
	q_value: tensor([[-39.6945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6322892933340205 entropy -2.191387398601823
epoch: 53, step: 37
	action: tensor([[-0.1164, -0.1140, -0.2416, -0.1014, -0.1316,  0.0821,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03150687802768848, distance: 1.1261726181694693 entropy -3.3191093420229527
epoch: 53, step: 38
	action: tensor([[-0.4028,  0.0061,  0.4185, -0.2777,  0.0564,  0.0934,  0.0669]],
       dtype=torch.float64)
	q_value: tensor([[-31.9894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3692778700608108, distance: 1.3390675564946157 entropy -2.9321211355914207
epoch: 53, step: 39
	action: tensor([[ 0.5236,  0.1241,  0.9130,  0.1343, -0.0425, -0.1517,  0.0648]],
       dtype=torch.float64)
	q_value: tensor([[-33.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7868487821008312, distance: 0.5283243197107648 entropy -2.63454580070828
epoch: 53, step: 40
	action: tensor([[-0.6108, -0.6981,  0.4337,  0.4085, -0.4409,  0.1504, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[-41.0252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6043562670946885, distance: 1.4494628901688766 entropy -2.190271537939878
epoch: 53, step: 41
	action: tensor([[ 0.0261, -0.0483,  0.0120, -0.5277, -1.1098,  0.7568,  0.3690]],
       dtype=torch.float64)
	q_value: tensor([[-35.3931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035061569636196, distance: 1.164232678880483 entropy -2.2361832382549616
epoch: 53, step: 42
	action: tensor([[ 0.0440, -1.0952, -0.3660,  0.0947, -0.3762,  0.0820,  0.2300]],
       dtype=torch.float64)
	q_value: tensor([[-37.5810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.660212452368798, distance: 1.4744787799041268 entropy -1.9926736701498178
epoch: 53, step: 43
	action: tensor([[ 0.2536,  0.2278,  0.1476,  0.2895,  0.3085, -0.1594,  0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-32.1043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4744787799041268 entropy -2.432394580694056
epoch: 53, step: 44
	action: tensor([[-0.1750,  0.1801, -0.1914, -0.0281, -0.1481, -0.0145,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17990375457355168, distance: 1.0363086067563838 entropy -3.3191093420229527
epoch: 53, step: 45
	action: tensor([[ 0.0031,  0.3820, -0.2361,  0.0730,  0.0148,  0.2961,  0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-34.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0363086067563838 entropy -3.071394862859264
epoch: 53, step: 46
	action: tensor([[-0.1685, -0.0573,  0.0504, -0.1776,  0.1342,  0.0979,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0363086067563838 entropy -3.3191093420229527
epoch: 53, step: 47
	action: tensor([[-0.2408,  0.0403, -0.0629, -0.3191, -0.1775,  0.1610,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09577135483714572, distance: 1.1978892425706198 entropy -3.3191093420229527
epoch: 53, step: 48
	action: tensor([[-0.1823, -0.3276,  0.5168,  0.2710,  0.3204,  0.1733,  0.0677]],
       dtype=torch.float64)
	q_value: tensor([[-32.1689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09363349809739752, distance: 1.0894532941252366 entropy -2.8358669447505056
epoch: 53, step: 49
	action: tensor([[-0.6851, -1.1375, -0.2765, -0.0711,  0.2784,  1.1405,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-35.3004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8428212610679495, distance: 1.5534536518010857 entropy -2.4541253101811966
epoch: 53, step: 50
	action: tensor([[ 0.5155, -1.4263,  0.5947,  0.5606, -1.2841,  0.5621,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-32.3019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5534536518010857 entropy -2.293283397929866
epoch: 53, step: 51
	action: tensor([[-0.2976, -0.0084,  0.0404, -0.0161,  0.0009,  0.1634,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-34.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06154579780592684, distance: 1.179033272191939 entropy -3.3191093420229527
epoch: 53, step: 52
	action: tensor([[-0.5330, -0.4676,  0.1769,  0.7783,  0.6930,  0.0907,  0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-33.3746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.143649473905616, distance: 1.2237794623742784 entropy -2.8043977494795826
epoch: 53, step: 53
	action: tensor([[-1.1890, -0.1114,  0.3461, -0.8483,  0.3527,  0.2328,  0.0712]],
       dtype=torch.float64)
	q_value: tensor([[-36.7479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5185175788315948, distance: 1.8160557917725584 entropy -2.7485163395526757
epoch: 53, step: 54
	action: tensor([[-0.7378, -0.4515, -0.1151, -0.3108,  0.3197,  0.3464, -0.0639]],
       dtype=torch.float64)
	q_value: tensor([[-33.7105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9531332863946855, distance: 1.5992731060577992 entropy -2.823680011721105
epoch: 53, step: 55
	action: tensor([[ 0.3461, -0.1937,  0.2831,  0.0406, -0.4532,  0.3171,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[-30.2229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5134243714037803, distance: 0.7982370276794527 entropy -2.9836351453892616
epoch: 53, step: 56
	action: tensor([[-0.4297, -0.6335,  1.0931, -1.6154,  0.7111,  0.1927,  0.1357]],
       dtype=torch.float64)
	q_value: tensor([[-35.7831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0725999956058003, distance: 1.6474583343423421 entropy -2.2180733057214357
epoch: 53, step: 57
	action: tensor([[-0.3050, -0.4195,  0.3767, -1.0452, -0.3219,  1.0138, -0.4603]],
       dtype=torch.float64)
	q_value: tensor([[-36.6372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6701852684629366, distance: 1.4789007107082133 entropy -2.0015300011872657
epoch: 53, step: 58
	action: tensor([[-1.4114, -1.0359,  0.1918, -1.2776,  0.0840,  0.3335,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[-32.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9010702768000751, distance: 1.5778139050823796 entropy -2.024528797695584
epoch: 53, step: 59
	action: tensor([[-0.8451, -0.3622, -1.2259, -0.1369, -1.0271,  0.0843, -0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-33.1140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0135107254731124, distance: 1.6238042245003195 entropy -2.486462816909478
epoch: 53, step: 60
	action: tensor([[ 0.1366,  0.0376,  0.0700, -0.3903,  0.1310,  0.2061,  0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-34.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843281556649675, distance: 0.9680853747818401 entropy -3.311912538355028
epoch: 53, step: 61
	action: tensor([[ 0.1791, -0.4410, -0.3756,  0.4036, -0.0380,  0.0139, -0.0701]],
       dtype=torch.float64)
	q_value: tensor([[-32.5448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21484074642655349, distance: 1.0139944314913476 entropy -2.6664842380910723
epoch: 53, step: 62
	action: tensor([[ 0.4208, -0.3789,  0.4841, -0.7085, -0.0512,  0.0226,  0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-32.7944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2136408350942185, distance: 1.2606710609102563 entropy -2.7763644853048057
epoch: 53, step: 63
	action: tensor([[ 0.1507,  0.1044,  0.3570,  0.1002,  0.0268, -0.4106, -0.1489]],
       dtype=torch.float64)
	q_value: tensor([[-32.5660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4305769074789507, distance: 0.8635236393927321 entropy -2.248895211936764
LOSS epoch 53 actor 490.13642293384447 critic 289.9029420562116
epoch: 54, step: 0
	action: tensor([[ 0.2894, -0.6694, -0.5866,  0.2992, -0.3882, -0.0167,  0.0460]],
       dtype=torch.float64)
	q_value: tensor([[-35.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006518370952198316, distance: 1.148067826122323 entropy -2.740196390382779
epoch: 54, step: 1
	action: tensor([[ 0.3556, -0.1693, -0.4322,  0.2723,  0.5630,  0.4807,  0.0613]],
       dtype=torch.float64)
	q_value: tensor([[-32.3149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6554122501878907, distance: 0.671748353033155 entropy -2.6523978495452574
epoch: 54, step: 2
	action: tensor([[-0.5584, -0.4142,  0.8666, -0.5265,  0.1756,  0.5734, -0.1097]],
       dtype=torch.float64)
	q_value: tensor([[-32.8184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8719667093236687, distance: 1.5656899136229887 entropy -2.767234249175771
epoch: 54, step: 3
	action: tensor([[-1.3075, -0.4857, -0.1679,  1.4197, -1.8341, -0.0099,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-31.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7114649101312744, distance: 1.4970651235613197 entropy -2.109496103082702
epoch: 54, step: 4
	action: tensor([[-0.1385,  0.7798, -0.4392,  0.0218,  1.1466,  0.9548,  0.5087]],
       dtype=torch.float64)
	q_value: tensor([[-49.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4970651235613197 entropy -2.05122559011376
epoch: 54, step: 5
	action: tensor([[ 0.0477, -0.0034,  0.0033, -0.2171,  0.0277, -0.0710,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21507178260364568, distance: 1.0138452346041011 entropy -3.3177087893176638
epoch: 54, step: 6
	action: tensor([[ 1.7498e-01,  4.7190e-01,  3.7109e-01,  1.2148e-01,  5.6053e-05,
          2.1356e-01, -4.7060e-03]], dtype=torch.float64)
	q_value: tensor([[-31.2774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0138452346041011 entropy -2.864524892559964
epoch: 54, step: 7
	action: tensor([[ 0.0354, -0.2509,  0.5315, -0.0682,  0.1621, -0.0542,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019101627466712956, distance: 1.1333621382496013 entropy -3.3177087893176638
epoch: 54, step: 8
	action: tensor([[-4.1682e-04, -2.6496e-02, -6.4515e-03, -1.0827e+00,  4.6813e-02,
          1.2983e-01,  1.0031e-02]], dtype=torch.float64)
	q_value: tensor([[-32.7934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1333621382496013 entropy -2.466465967384895
epoch: 54, step: 9
	action: tensor([[-0.0440, -0.2634, -0.0993, -0.0526, -0.0492,  0.1696,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04088705490171485, distance: 1.1207056711357082 entropy -3.3177087893176638
epoch: 54, step: 10
	action: tensor([[-0.5739, -0.1423,  1.0955, -0.2358,  0.1489,  0.2709,  0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-30.8346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6103198850213785, distance: 1.452154320110927 entropy -2.7054032152459055
epoch: 54, step: 11
	action: tensor([[ 8.6506e-04,  4.9906e-02,  9.8572e-01,  2.7416e-01, -2.3017e-02,
          3.8084e-01,  8.9829e-02]], dtype=torch.float64)
	q_value: tensor([[-34.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.452154320110927 entropy -2.1474895857110394
epoch: 54, step: 12
	action: tensor([[ 0.0092, -0.0744,  0.1708, -0.3650, -0.1928, -0.2580,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017489970837244462, distance: 1.1543081496029894 entropy -3.3177087893176638
epoch: 54, step: 13
	action: tensor([[ 0.0634, -0.0140, -0.0839, -1.1263, -0.3478,  0.4034,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-31.1301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06592942836411497, distance: 1.181465160450263 entropy -2.7873782386862915
epoch: 54, step: 14
	action: tensor([[ 0.1991, -0.3059,  0.0754, -0.7952,  1.3054, -0.2174, -0.1025]],
       dtype=torch.float64)
	q_value: tensor([[-29.5146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29094574330623946, distance: 1.300201576212875 entropy -2.4966980903893163
epoch: 54, step: 15
	action: tensor([[-0.0162, -0.8946,  1.5065,  0.1141,  0.4274,  0.0107, -0.3497]],
       dtype=torch.float64)
	q_value: tensor([[-30.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4579269100367671, distance: 1.3817343977914363 entropy -2.5736326084366246
epoch: 54, step: 16
	action: tensor([[-1.0819, -0.0175, -0.0838, -0.3283, -0.8041,  0.7633,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-36.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2020296108670423, distance: 1.6981195839327137 entropy -1.865197859068166
epoch: 54, step: 17
	action: tensor([[ 0.5572, -0.8084,  0.3732, -0.7557, -0.1346,  0.4888,  0.2620]],
       dtype=torch.float64)
	q_value: tensor([[-32.7230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4801608672838167, distance: 1.392230527460596 entropy -2.4243443834615612
epoch: 54, step: 18
	action: tensor([[-1.3887,  0.0109,  1.2653, -0.7486, -0.6804,  0.3541, -0.1785]],
       dtype=torch.float64)
	q_value: tensor([[-31.8773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6644902011899703, distance: 1.8679435906421014 entropy -1.9863185387674018
epoch: 54, step: 19
	action: tensor([[ 0.9793,  1.0032, -0.1055, -0.3778,  0.2659,  0.1034,  0.3659]],
       dtype=torch.float64)
	q_value: tensor([[-36.6472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8679435906421014 entropy -1.9395643081338727
epoch: 54, step: 20
	action: tensor([[-0.0321, -0.2399,  0.1952,  0.0123,  0.2600, -0.3088,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0338152454390539, distance: 1.1635315377805064 entropy -3.3177087893176638
epoch: 54, step: 21
	action: tensor([[-0.1387, -0.1633,  0.0714, -0.1090,  0.2506, -0.0960,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[-31.5980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08146831421118272, distance: 1.1900455729759496 entropy -2.8786980745020943
epoch: 54, step: 22
	action: tensor([[0.0679, 0.3131, 0.1903, 0.0224, 0.0695, 0.0074, 0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-30.2359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1900455729759496 entropy -2.950979080292788
epoch: 54, step: 23
	action: tensor([[ 0.1145, -0.1723,  0.0602,  0.1047,  0.1180,  0.1056,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3666804556336818, distance: 0.9106849062141952 entropy -3.3177087893176638
epoch: 54, step: 24
	action: tensor([[-1.1429, -0.5279,  0.1052,  0.6204, -0.0320,  0.4148,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[-32.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9239942319643102, distance: 1.5872983913868894 entropy -2.6540420314116497
epoch: 54, step: 25
	action: tensor([[-1.2980, -0.0415,  0.1660, -0.1339, -0.1912,  0.3552,  0.2679]],
       dtype=torch.float64)
	q_value: tensor([[-34.1905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3486191395538825, distance: 1.7537310494565803 entropy -2.546258860061986
epoch: 54, step: 26
	action: tensor([[ 0.0223,  0.0509, -0.0937,  0.2989, -0.1459,  0.1300,  0.1663]],
       dtype=torch.float64)
	q_value: tensor([[-32.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7537310494565803 entropy -2.7199972467916496
epoch: 54, step: 27
	action: tensor([[ 0.2975, -0.0510,  0.1970,  0.2973,  0.0547, -0.2272,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6319307256481859, distance: 0.6942589317987804 entropy -3.3177087893176638
epoch: 54, step: 28
	action: tensor([[-0.2123, -0.2531,  1.1033, -0.1860, -1.1856, -0.0437,  0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-35.8592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2826661101671091, distance: 1.2960253710364262 entropy -2.6522812193472745
epoch: 54, step: 29
	action: tensor([[-1.9007, -0.1498,  1.3007,  0.2751,  0.7382,  1.2400,  0.3426]],
       dtype=torch.float64)
	q_value: tensor([[-39.1668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2960253710364262 entropy -1.893707925671708
epoch: 54, step: 30
	action: tensor([[-0.2589, -0.1292,  0.2976,  0.0732,  0.2062, -0.0123,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09497444283200251, distance: 1.1974535741529835 entropy -3.3177087893176638
epoch: 54, step: 31
	action: tensor([[-0.1246,  0.3308, -0.2652,  0.3399, -0.1042,  0.2575,  0.0614]],
       dtype=torch.float64)
	q_value: tensor([[-32.2744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1974535741529835 entropy -2.7500889090410565
epoch: 54, step: 32
	action: tensor([[-0.2260,  0.0579,  0.1051,  0.1355, -0.2449,  0.1293,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13984869060749194, distance: 1.061314556046598 entropy -3.3177087893176638
epoch: 54, step: 33
	action: tensor([[-0.8974, -0.1013, -0.1250,  0.4952,  0.1677,  0.4484,  0.1568]],
       dtype=torch.float64)
	q_value: tensor([[-34.0861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.061314556046598 entropy -2.666324560658931
epoch: 54, step: 34
	action: tensor([[ 0.2501,  0.2502,  0.0613,  0.1357,  0.0680, -0.2643,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6975915114414026, distance: 0.6292941319606484 entropy -3.3177087893176638
epoch: 54, step: 35
	action: tensor([[-0.0218,  0.1817, -0.2802,  0.6098, -0.0900, -0.2090,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4632270155921573, distance: 0.8384014450538979 entropy -3.3177087893176638
epoch: 54, step: 36
	action: tensor([[-0.0206,  0.3447,  0.2782, -0.2162, -0.2034,  0.1360,  0.0923]],
       dtype=torch.float64)
	q_value: tensor([[-37.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3907187236821531, distance: 0.8932347479623044 entropy -3.009635926393739
epoch: 54, step: 37
	action: tensor([[ 0.2155,  0.2319,  0.3087, -0.1594,  0.5864, -0.1919,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4799531496695263, distance: 0.8252355499408215 entropy -3.3177087893176638
epoch: 54, step: 38
	action: tensor([[-0.0076, -0.4849,  0.2162, -0.2392, -0.3616, -0.2527, -0.1507]],
       dtype=torch.float64)
	q_value: tensor([[-34.4317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3445633504660328, distance: 1.3269279068366422 entropy -2.7924717218097577
epoch: 54, step: 39
	action: tensor([[-0.3265,  0.4277, -0.1429,  0.4598,  0.7639,  0.3820,  0.1114]],
       dtype=torch.float64)
	q_value: tensor([[-30.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3269279068366422 entropy -2.567729214963236
epoch: 54, step: 40
	action: tensor([[-0.1925,  0.0488,  0.1756,  0.1377, -0.0111, -0.1657,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11213415795288895, distance: 1.0782770637494405 entropy -3.3177087893176638
epoch: 54, step: 41
	action: tensor([[-0.2228, -0.1284, -0.1723, -0.4043, -0.5698,  0.2557,  0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-33.8036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24020706226304211, distance: 1.27439422072511 entropy -2.869487715367564
epoch: 54, step: 42
	action: tensor([[ 0.8083,  0.2396,  0.4905,  0.2971, -0.3284, -0.1147,  0.1277]],
       dtype=torch.float64)
	q_value: tensor([[-30.6633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9545653820535005, distance: 0.24392152889271396 entropy -2.623428317889098
epoch: 54, step: 43
	action: tensor([[ 0.2457, -0.2189, -0.1932, -0.3042, -0.2717,  0.3211, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[-41.8620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22188138010731984, distance: 1.0094378781891078 entropy -2.2349146689747292
epoch: 54, step: 44
	action: tensor([[-0.2029, -0.2905, -0.0775,  0.5259, -0.5897,  0.1147,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-30.5082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07026762537494202, distance: 1.1034068360564169 entropy -2.5242664454537542
epoch: 54, step: 45
	action: tensor([[-0.7981,  0.0741,  0.2498, -0.3293,  0.9949,  0.5776,  0.2558]],
       dtype=torch.float64)
	q_value: tensor([[-35.5233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6085890442890396, distance: 1.451373691484336 entropy -2.4751405764758094
epoch: 54, step: 46
	action: tensor([[-0.5694,  0.0064,  0.0765, -0.5097, -0.2578,  0.0412, -0.2286]],
       dtype=torch.float64)
	q_value: tensor([[-32.6259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6580627531340515, distance: 1.4735238681941112 entropy -2.7489825743434615
epoch: 54, step: 47
	action: tensor([[-0.0320, -0.1665,  0.4116,  0.2658, -0.2480,  0.2491,  0.0856]],
       dtype=torch.float64)
	q_value: tensor([[-28.8887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3658997782584352, distance: 0.9112460227552394 entropy -3.0129614416952686
epoch: 54, step: 48
	action: tensor([[ 0.9213,  0.3285, -1.2956, -0.0938, -0.9798,  0.5837,  0.1995]],
       dtype=torch.float64)
	q_value: tensor([[-35.5496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8876778574083632, distance: 0.38352120646432164 entropy -2.3195622425079785
epoch: 54, step: 49
	action: tensor([[-0.1649, -0.3695,  0.3310, -0.0041, -0.1016,  0.6118, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-39.9824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01595883602923731, distance: 1.135176328380072 entropy -2.440054805073578
epoch: 54, step: 50
	action: tensor([[ 0.7886,  0.2291,  0.7824, -1.2424, -0.1982,  0.2319,  0.1710]],
       dtype=torch.float64)
	q_value: tensor([[-31.8034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41287710353171436, distance: 0.8768417130889398 entropy -2.263411132466047
epoch: 54, step: 51
	action: tensor([[-1.6265, -1.4340, -0.0423, -1.0738, -0.6828,  1.1490, -0.3263]],
       dtype=torch.float64)
	q_value: tensor([[-37.3508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8768417130889398 entropy -1.9699388668382005
epoch: 54, step: 52
	action: tensor([[ 0.0460,  0.1737,  0.3284, -0.0051, -0.0450, -0.1685,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.404985110853726, distance: 0.8827152099235411 entropy -3.3177087893176638
epoch: 54, step: 53
	action: tensor([[ 0.1128, -0.5400, -0.5364,  0.6255, -0.0325,  0.1345,  0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-35.3383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13589871092081562, distance: 1.0637486447341191 entropy -2.667936354135009
epoch: 54, step: 54
	action: tensor([[ 0.1397, -0.1394,  0.1340,  0.2802, -0.5814,  0.5764,  0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-33.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5405574661172624, distance: 0.7756616013652112 entropy -2.744479636484293
epoch: 54, step: 55
	action: tensor([[ 0.7976,  0.2310, -0.8149, -0.6483,  1.2475,  1.0613,  0.2360]],
       dtype=torch.float64)
	q_value: tensor([[-36.6382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8739385630697412, distance: 0.40630093323809485 entropy -2.1592292182791284
epoch: 54, step: 56
	action: tensor([[ 0.2531, -0.1775, -0.7552,  0.3831,  0.8445, -0.0538, -0.4000]],
       dtype=torch.float64)
	q_value: tensor([[-33.2636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39011948886330317, distance: 0.8936738930993641 entropy -2.3902485007144447
epoch: 54, step: 57
	action: tensor([[-0.3966,  0.1079,  0.6138, -0.5013,  0.1110,  0.0041, -0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-31.7726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4658763684231939, distance: 1.3854962832028577 entropy -3.201790176042721
epoch: 54, step: 58
	action: tensor([[-0.8837,  0.1834, -0.5596, -0.0080, -1.1417,  0.2580, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-31.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.789161328604258, distance: 1.5306695564840582 entropy -2.6014862549631017
epoch: 54, step: 59
	action: tensor([[ 0.0180,  0.6038,  0.2805, -0.1661, -0.1873,  0.1856,  0.1743]],
       dtype=torch.float64)
	q_value: tensor([[-34.8798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5306695564840582 entropy -2.8709707354503515
epoch: 54, step: 60
	action: tensor([[ 0.1777,  0.0392,  0.0875, -0.1169, -0.0195,  0.1056,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46975050673402585, distance: 0.8332912554107091 entropy -3.3177087893176638
epoch: 54, step: 61
	action: tensor([[ 0.4419,  0.0743,  0.4680, -0.0104, -0.0767,  0.3169,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[-32.7539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7688575147297041, distance: 0.5501695912195392 entropy -2.6440537219858222
epoch: 54, step: 62
	action: tensor([[ 0.0348, -0.2438, -0.4285, -1.5824,  0.3180,  0.5473,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[-35.9773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01795975250525461, distance: 1.1545745946025439 entropy -2.2676609810403994
epoch: 54, step: 63
	action: tensor([[ 0.1748, -0.2987, -0.1101, -0.2002, -0.5963, -0.4524, -0.3106]],
       dtype=torch.float64)
	q_value: tensor([[-28.9465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08962970054929709, distance: 1.0918569265918003 entropy -2.5583416873193014
LOSS epoch 54 actor 433.5860375187181 critic 493.5467354229554
epoch: 55, step: 0
	action: tensor([[ 0.4925, -0.2152, -0.8051,  0.0131, -0.0087,  0.1850,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[-31.2149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5117945982381387, distance: 0.7995727478230816 entropy -2.8120011583217535
epoch: 55, step: 1
	action: tensor([[ 0.4214,  0.0717,  0.0711, -0.0443, -0.1569,  0.0432, -0.0741]],
       dtype=torch.float64)
	q_value: tensor([[-32.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6713370761404099, distance: 0.6560426142091225 entropy -2.8931732855655414
epoch: 55, step: 2
	action: tensor([[-0.0171,  0.6351, -0.6630,  0.0789,  0.0056,  0.8302,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-34.3945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6560426142091225 entropy -2.5797040972906586
epoch: 55, step: 3
	action: tensor([[ 0.0621, -0.1227, -0.2358, -0.0659, -0.2020,  0.0743,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23354199973028256, distance: 1.0018457822111846 entropy -3.316630914382504
epoch: 55, step: 4
	action: tensor([[ 0.0157,  0.1509, -0.4573, -0.3398,  0.6307, -0.0856,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[-31.9919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3465512092424543, distance: 0.925044174654015 entropy -2.8055178015850073
epoch: 55, step: 5
	action: tensor([[-0.2465, -0.1680, -0.0811, -0.2458, -0.1362,  0.0132, -0.0886]],
       dtype=torch.float64)
	q_value: tensor([[-29.9289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21571522522019393, distance: 1.261747988703869 entropy -3.322794600791798
epoch: 55, step: 6
	action: tensor([[-0.0946, -0.1251, -0.1532,  0.2103,  0.3676,  0.3584,  0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-29.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008425715506593, distance: 0.956850705538255 entropy -2.9774553286325345
epoch: 55, step: 7
	action: tensor([[-0.6052,  0.0063, -0.3906, -0.0192, -0.1082, -0.0726, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[-32.6968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44899054471933386, distance: 1.3774932171247236 entropy -2.845262894489536
epoch: 55, step: 8
	action: tensor([[ 0.0560,  0.0642,  0.1458,  0.1285, -0.1997, -0.0026,  0.0608]],
       dtype=torch.float64)
	q_value: tensor([[-30.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4608010953149142, distance: 0.840293867251019 entropy -3.599639972514521
epoch: 55, step: 9
	action: tensor([[-0.1490, -0.3699,  0.1261, -0.4324,  0.2404,  0.3930,  0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-35.5201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2732058603776424, distance: 1.2912371356125611 entropy -2.6456045942321076
epoch: 55, step: 10
	action: tensor([[-0.1657,  0.0970,  1.1962, -0.4469,  0.3392,  0.3112, -0.0747]],
       dtype=torch.float64)
	q_value: tensor([[-29.0751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05027078591587064, distance: 1.172755115460948 entropy -2.5340501170168754
epoch: 55, step: 11
	action: tensor([[ 2.6837, -0.9475,  0.3010,  0.1045,  1.1132,  0.3088, -0.1176]],
       dtype=torch.float64)
	q_value: tensor([[-35.7463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.172755115460948 entropy -2.0998875394848175
epoch: 55, step: 12
	action: tensor([[ 0.0624,  0.0106, -0.0382,  0.1151,  0.0355, -0.1536,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37532302895009495, distance: 0.9044497460888447 entropy -3.316630914382504
epoch: 55, step: 13
	action: tensor([[-0.0287,  0.4961, -0.3037, -0.0964, -0.4801,  0.3346,  0.0443]],
       dtype=torch.float64)
	q_value: tensor([[-34.2749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9044497460888447 entropy -2.901238667441222
epoch: 55, step: 14
	action: tensor([[-0.1299, -0.0516, -0.0453, -0.2685,  0.4917,  0.1141,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9044497460888447 entropy -3.316630914382504
epoch: 55, step: 15
	action: tensor([[ 0.2190, -0.1293, -0.4098, -0.0734,  0.1560,  0.1163,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43344566493849357, distance: 0.8613456737580232 entropy -3.316630914382504
epoch: 55, step: 16
	action: tensor([[-0.0158,  0.0481, -0.6319,  0.2856, -0.2723,  0.2182, -0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-31.2699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8613456737580232 entropy -2.954170329512202
epoch: 55, step: 17
	action: tensor([[-0.0062, -0.2031, -0.0212,  0.2280,  0.1865, -0.1152,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20948025412499882, distance: 1.017449949231982 entropy -3.316630914382504
epoch: 55, step: 18
	action: tensor([[-1.4402, -0.0623,  0.2541, -0.9478,  0.2609,  0.0128,  0.0482]],
       dtype=torch.float64)
	q_value: tensor([[-33.3120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6167527112608466, distance: 1.8511347546732528 entropy -2.8732228057579356
epoch: 55, step: 19
	action: tensor([[-0.2423,  0.0880, -0.0177, -0.1566,  0.2506,  0.1735, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-32.0748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06144244751672667, distance: 1.1086313300870212 entropy -3.086407623332359
epoch: 55, step: 20
	action: tensor([[-0.2900,  0.0819, -0.3938,  0.0174, -0.1845,  0.0230, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-30.9979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016530586886044807, distance: 1.1348464985080595 entropy -3.036738710173059
epoch: 55, step: 21
	action: tensor([[-0.0956, -0.0181,  0.1381, -0.0088, -0.1794,  0.1750,  0.0693]],
       dtype=torch.float64)
	q_value: tensor([[-31.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20743360789103515, distance: 1.0187661807734503 entropy -3.301205591194331
epoch: 55, step: 22
	action: tensor([[-0.1580,  0.1527,  0.1838,  0.0775,  0.7397,  0.0055,  0.1171]],
       dtype=torch.float64)
	q_value: tensor([[-33.3361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0187661807734503 entropy -2.605858008299164
epoch: 55, step: 23
	action: tensor([[-0.3791, -0.3613,  0.0452,  0.2147, -0.1125,  0.0412,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34357670552409614, distance: 1.3264409654923266 entropy -3.316630914382504
epoch: 55, step: 24
	action: tensor([[ 0.5912,  0.1882,  0.1218, -0.4370, -0.1629,  0.2552,  0.1676]],
       dtype=torch.float64)
	q_value: tensor([[-32.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6873080723973546, distance: 0.6399042994094762 entropy -2.7317266946099257
epoch: 55, step: 25
	action: tensor([[-0.8599, -0.2103,  0.3846, -0.3325, -0.7264,  0.5132, -0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-34.4757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1362794681156068, distance: 1.6725754830238857 entropy -2.3661069233409653
epoch: 55, step: 26
	action: tensor([[-0.2256, -0.1473,  0.4345, -1.4677, -0.1299,  0.5296,  0.3234]],
       dtype=torch.float64)
	q_value: tensor([[-33.1607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6872898909693956, distance: 1.4864542448090305 entropy -2.223440754299433
epoch: 55, step: 27
	action: tensor([[ 0.8140, -0.0438,  1.1466, -0.6450, -1.2835, -0.0417, -0.2041]],
       dtype=torch.float64)
	q_value: tensor([[-32.4852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48806832682197976, distance: 0.8187714549822419 entropy -2.17194150466698
epoch: 55, step: 28
	action: tensor([[ 0.6306, -1.8047,  0.5061, -0.5231,  2.3126,  1.3335,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-42.0350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8187714549822419 entropy -1.7692089097223005
epoch: 55, step: 29
	action: tensor([[-0.4367,  0.1828,  0.0712,  0.1413,  0.0347, -0.3641,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15011951518670452, distance: 1.2272362630684863 entropy -3.316630914382504
epoch: 55, step: 30
	action: tensor([[ 0.1541, -0.0989, -0.2699, -0.3304,  0.3074,  0.1580,  0.0675]],
       dtype=torch.float64)
	q_value: tensor([[-34.8912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2565499777341047, distance: 0.9866942166651429 entropy -3.193845196528693
epoch: 55, step: 31
	action: tensor([[ 0.2400, -0.2580, -0.0483, -0.2331,  0.3223, -0.0282, -0.1055]],
       dtype=torch.float64)
	q_value: tensor([[-29.5956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12796866157658415, distance: 1.0686186257502746 entropy -2.9461314099101377
epoch: 55, step: 32
	action: tensor([[-0.2029,  0.6794, -0.2078, -0.5702,  0.1246,  0.3482, -0.0914]],
       dtype=torch.float64)
	q_value: tensor([[-29.7551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0686186257502746 entropy -2.8152299475976195
epoch: 55, step: 33
	action: tensor([[-0.0240,  0.1107,  0.1810,  0.3924, -0.3916, -0.2723,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49695369979017456, distance: 0.8116348245188817 entropy -3.316630914382504
epoch: 55, step: 34
	action: tensor([[-0.9570,  0.0568, -0.1375,  0.8033,  0.7093,  0.5920,  0.1545]],
       dtype=torch.float64)
	q_value: tensor([[-38.6317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11158697444686072, distance: 1.2065030324491157 entropy -2.667184976784386
epoch: 55, step: 35
	action: tensor([[-0.0120, -0.2318, -0.1627, -0.0875,  0.1492, -0.0198,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04463503704786731, distance: 1.1185138039378788 entropy -3.316630914382504
epoch: 55, step: 36
	action: tensor([[-8.5082e-03, -5.4565e-01,  4.2273e-01,  4.4769e-01, -1.2847e-01,
          1.3031e-01, -4.0187e-04]], dtype=torch.float64)
	q_value: tensor([[-30.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22605161602478108, distance: 1.0067292619446255 entropy -2.9484963266877178
epoch: 55, step: 37
	action: tensor([[ 0.6065, -0.4426, -0.1224,  0.4650, -0.5478,  0.6930,  0.2038]],
       dtype=torch.float64)
	q_value: tensor([[-35.2412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7055209286936122, distance: 0.6209890048405886 entropy -2.2991340081579152
epoch: 55, step: 38
	action: tensor([[ 0.7186, -0.5696, -0.3439, -0.4707,  1.1007, -0.2974,  0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-38.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19428529143859552, distance: 1.2505778581464233 entropy -2.0638161588622927
epoch: 55, step: 39
	action: tensor([[-0.0431, -0.1580,  0.8967,  0.5653, -0.2876,  0.2821, -0.3155]],
       dtype=torch.float64)
	q_value: tensor([[-30.3256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6475996457767335, distance: 0.6793207202770755 entropy -2.593712859327443
epoch: 55, step: 40
	action: tensor([[-0.4966, -0.8859,  0.9973,  0.2615,  1.2437,  0.0828,  0.2929]],
       dtype=torch.float64)
	q_value: tensor([[-39.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6655407023752795, distance: 1.4768429647164292 entropy -2.0640764897675536
epoch: 55, step: 41
	action: tensor([[-0.1944, -0.2971, -0.5987,  0.0523, -0.9959, -0.8974, -0.2325]],
       dtype=torch.float64)
	q_value: tensor([[-36.9726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19487374836832694, distance: 1.0268066837050946 entropy -2.218856328418956
epoch: 55, step: 42
	action: tensor([[ 0.2949,  0.0617,  0.3015, -0.0723, -0.6368,  0.0616,  0.0851]],
       dtype=torch.float64)
	q_value: tensor([[-33.8714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5619900757732998, distance: 0.7573535554805911 entropy -3.201513072955484
epoch: 55, step: 43
	action: tensor([[-0.5338, -0.2578, -0.2541,  0.8427,  0.7170,  0.4655,  0.1226]],
       dtype=torch.float64)
	q_value: tensor([[-36.6400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7573535554805911 entropy -2.3109567560672106
epoch: 55, step: 44
	action: tensor([[-0.1589,  0.0913, -0.2460,  0.2099,  0.3178, -0.1114,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18044786705150795, distance: 1.0359647678434896 entropy -3.316630914382504
epoch: 55, step: 45
	action: tensor([[-0.2890, -0.1518, -0.0038,  0.0179,  0.0472, -0.0245,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[-33.7922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18805882253128847, distance: 1.2473136214434835 entropy -3.307153029179925
epoch: 55, step: 46
	action: tensor([[ 0.1272,  0.1908, -0.4427,  0.4916, -0.2413, -0.1059,  0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-31.3985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2473136214434835 entropy -2.961636005329915
epoch: 55, step: 47
	action: tensor([[ 0.0399,  0.0778,  0.0063, -0.4274,  0.1138,  0.0032,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2473136214434835 entropy -3.316630914382504
epoch: 55, step: 48
	action: tensor([[ 0.3120,  0.1473, -0.3149,  0.2632,  0.2902, -0.1331,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6842470030719268, distance: 0.6430288138575815 entropy -3.316630914382504
epoch: 55, step: 49
	action: tensor([[-0.4020, -0.2343,  0.5085, -0.0176, -0.0804, -0.2554,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49755832286267543, distance: 1.4003885972258459 entropy -3.316630914382504
epoch: 55, step: 50
	action: tensor([[ 0.0880,  0.1982, -0.2275, -0.8079, -0.1550,  0.5313,  0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-33.1430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3214281186105379, distance: 0.9426590101735949 entropy -2.6161966014376037
epoch: 55, step: 51
	action: tensor([[-0.4080, -0.0401, -0.5665,  0.1029, -0.6391,  0.3951, -0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-30.2131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3379331969678123, distance: 1.3236522679035378 entropy -2.620736174413459
epoch: 55, step: 52
	action: tensor([[ 0.1544, -0.0420, -0.0687,  0.5771, -0.3863, -0.0388,  0.1384]],
       dtype=torch.float64)
	q_value: tensor([[-32.2979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3236522679035378 entropy -2.8600858377532425
epoch: 55, step: 53
	action: tensor([[-0.2784,  0.0041, -0.1265, -0.2466,  0.0790, -0.0265,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15744646209946356, distance: 1.231139170259103 entropy -3.316630914382504
epoch: 55, step: 54
	action: tensor([[-0.3248,  0.4254,  0.0372,  0.1689,  0.0720,  0.1248,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[-30.4152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17406103444428722, distance: 1.0399936102389293 entropy -3.154176925049005
epoch: 55, step: 55
	action: tensor([[-0.0961, -0.2819,  0.1780,  0.4033,  0.2725,  0.0881,  0.0626]],
       dtype=torch.float64)
	q_value: tensor([[-35.8742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0399936102389293 entropy -2.975717638208931
epoch: 55, step: 56
	action: tensor([[-0.0940,  0.0130, -0.0943, -0.1712,  0.0846, -0.1322,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08535412421705568, distance: 1.0944178895609853 entropy -3.316630914382504
epoch: 55, step: 57
	action: tensor([[ 0.2817, -0.0450, -0.2729,  0.2001,  0.2772,  0.1285,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[-31.3869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.593849600916133, distance: 0.7292897932034038 entropy -3.0875401023157365
epoch: 55, step: 58
	action: tensor([[-0.5983,  0.2205, -0.0876, -0.3786, -0.4366, -0.0682, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[-33.2267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.429596788248257, distance: 1.3682437568298662 entropy -2.880918996836158
epoch: 55, step: 59
	action: tensor([[ 0.2749,  0.2751,  0.1206, -0.0950, -0.4285,  0.2794,  0.0907]],
       dtype=torch.float64)
	q_value: tensor([[-31.5351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3682437568298662 entropy -3.212368384481976
epoch: 55, step: 60
	action: tensor([[ 0.0172, -0.1202,  0.0111, -0.3128, -0.1351, -0.0079,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.053018997625748576, distance: 1.1135951391426617 entropy -3.316630914382504
epoch: 55, step: 61
	action: tensor([[-0.2976, -0.3513,  0.1085, -0.1869,  0.0329,  0.5607,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-30.9993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2361307214939825, distance: 1.2722981429991471 entropy -2.739738669249252
epoch: 55, step: 62
	action: tensor([[ 1.2554,  0.2852,  0.1622,  1.1559, -0.4342,  0.3115,  0.0797]],
       dtype=torch.float64)
	q_value: tensor([[-30.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7825067800657844, distance: 0.5336783129183201 entropy -2.4653522428966883
epoch: 55, step: 63
	action: tensor([[-1.1849, -0.7463,  0.1702,  1.1935,  1.0215, -0.3716, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-49.5126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5336783129183201 entropy -2.022617570220563
LOSS epoch 55 actor 415.5235198073583 critic 366.3916915856602
epoch: 56, step: 0
	action: tensor([[-0.4061,  0.2969,  0.1509,  0.3724, -0.3057, -0.1856,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-35.5863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.147167853905254, distance: 1.0567894627357362 entropy -3.3168868703268712
