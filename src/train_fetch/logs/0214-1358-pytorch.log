epoch: 0, step: 0
	action: tensor([[ 0.0356, -0.0631,  0.0705,  0.0578,  0.0161,  0.0037, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-0.0906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.338445123740281, distance: 0.9307641144358516 entropy -4.612273175343211
epoch: 0, step: 1
	action: tensor([[-2.5505e-02, -4.2360e-02,  6.3519e-03,  1.6390e-01,  7.9060e-02,
         -1.1073e-04,  7.8525e-03]], dtype=torch.float64)
	q_value: tensor([[-0.1535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3413521333406274, distance: 0.9287168774957465 entropy -5.098586592787639
epoch: 0, step: 2
	action: tensor([[-0.0065, -0.0202,  0.0364,  0.0342,  0.0258, -0.0068,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-0.1565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128364900745988, distance: 0.9486079017521257 entropy -5.079221957830584
epoch: 0, step: 3
	action: tensor([[-0.0109, -0.0236,  0.0216, -0.0465,  0.0719, -0.0008,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-0.1545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26709236567399386, distance: 0.9796733994465012 entropy -5.104325072079503
epoch: 0, step: 4
	action: tensor([[-0.0643, -0.0421,  0.0103, -0.1254,  0.0129,  0.0007,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-0.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15929233319918146, distance: 1.0492505299950314 entropy -5.131910910302753
epoch: 0, step: 5
	action: tensor([[-0.0118, -0.0289,  0.0187,  0.0072,  0.0171, -0.0015,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-0.1479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2886914971439608, distance: 0.9651297288322807 entropy -5.25735090746644
epoch: 0, step: 6
	action: tensor([[ 0.0286, -0.0151, -0.0006, -0.0101, -0.0267,  0.0009,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-0.1533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3312505958029647, distance: 0.9358115419684473 entropy -5.1256304781488256
epoch: 0, step: 7
	action: tensor([[-0.0442, -0.0054,  0.0502,  0.0164,  0.1015,  0.0062,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-0.1520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28005707506957356, distance: 0.9709698112901616 entropy -5.104798036992063
epoch: 0, step: 8
	action: tensor([[-0.0381, -0.0399,  0.0697, -0.0549,  0.0293, -0.0011,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-0.1583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2208312525793421, distance: 1.010118803351414 entropy -5.063001698098811
epoch: 0, step: 9
	action: tensor([[-0.0155, -0.0504,  0.0291,  0.0156, -0.0112, -0.0015,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-0.1522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27294128430400066, distance: 0.9757564612128112 entropy -5.178067190653032
epoch: 0, step: 10
	action: tensor([[-0.0078, -0.0173,  0.0357,  0.0374,  0.0399, -0.0025,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-0.1514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.316351535479369, distance: 0.9461785855484945 entropy -5.163751673854066
epoch: 0, step: 11
	action: tensor([[ 0.0278, -0.0933,  0.0203,  0.0069,  0.0294, -0.0019,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-0.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27761119023548786, distance: 0.9726177660159394 entropy -5.083146801997132
epoch: 0, step: 12
	action: tensor([[-0.1382, -0.0264,  0.0342,  0.0291, -0.0402, -0.0020,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-0.1518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16724344221567267, distance: 1.0442770266680563 entropy -5.263374162412174
epoch: 0, step: 13
	action: tensor([[ 0.0037, -0.0245,  0.0424, -0.0519,  0.0577,  0.0006,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-0.1478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27806071881988514, distance: 0.9723150983104841 entropy -5.2151333567261
epoch: 0, step: 14
	action: tensor([[-0.0249, -0.0406, -0.0095,  0.0691,  0.0243, -0.0053,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-0.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29534357578149517, distance: 0.9606062348168734 entropy -5.124482993190464
epoch: 0, step: 15
	action: tensor([[-0.0481, -0.0148,  0.0582, -0.0385,  0.0333, -0.0009,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-0.1533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23892304955859056, distance: 0.9983227727882745 entropy -5.156703432206728
epoch: 0, step: 16
	action: tensor([[-0.0570, -0.0113,  0.0203, -0.0867,  0.0714, -0.0007,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-0.1532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20950385540803274, distance: 1.0174347609311312 entropy -5.12413019198818
epoch: 0, step: 17
	action: tensor([[ 0.0062, -0.0315,  0.0209, -0.0363,  0.0012, -0.0080,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-0.1534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2814717000306961, distance: 0.9700154067963428 entropy -5.1684361858512
epoch: 0, step: 18
	action: tensor([[-0.0291,  0.0102,  0.0125,  0.0113,  0.0334, -0.0050,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-0.1517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3012891039869072, distance: 0.9565450997464632 entropy -5.153729564706279
epoch: 0, step: 19
	action: tensor([[-0.0520,  0.0044,  0.0377,  0.1004,  0.0223,  0.0007,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-0.1547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3186171323177234, distance: 0.9446094763624386 entropy -5.090290476062696
epoch: 0, step: 20
	action: tensor([[-0.0274, -0.0345,  0.0082, -0.0317,  0.0211, -0.0001,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-0.1547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24974166700340683, distance: 0.991201857316236 entropy -5.074045164891627
epoch: 0, step: 21
	action: tensor([[-0.0452, -0.0460,  0.0258, -0.0568,  0.0113, -0.0053,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-0.1519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2081355217924552, distance: 1.018314960190683 entropy -5.166977937856261
epoch: 0, step: 22
	action: tensor([[-0.0750,  0.0030,  0.0089,  0.0824,  0.0070, -0.0019,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-0.1502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28272002210549463, distance: 0.9691724211875997 entropy -5.222979273709965
epoch: 0, step: 23
	action: tensor([[-0.1075, -0.0569,  0.0359, -0.0677,  0.0350,  0.0020,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-0.1532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12938376125969564, distance: 1.0677512166352696 entropy -5.11451782545503
epoch: 0, step: 24
	action: tensor([[-0.0260, -0.0465,  0.0316, -0.0467,  0.0744, -0.0012,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-0.1490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23345014507746142, distance: 1.0019058125321698 entropy -5.276720690983858
epoch: 0, step: 25
	action: tensor([[ 0.0258, -0.0580,  0.0270, -0.0017,  0.0499, -0.0005,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-0.1541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2993304520897154, distance: 0.9578848725545029 entropy -5.189822391051551
epoch: 0, step: 26
	action: tensor([[-0.0634, -0.0367,  0.0504,  0.0258, -0.0249, -0.0003,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-0.1544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23970973697372944, distance: 0.9978066810743835 entropy -5.149005406300344
epoch: 0, step: 27
	action: tensor([[-0.0087, -0.0586,  0.0052,  0.0260, -0.0093,  0.0031,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-0.1507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27960067607826244, distance: 0.9712775297335653 entropy -5.162517322181253
epoch: 0, step: 28
	action: tensor([[-0.0407, -0.0067,  0.0376, -0.1022, -0.0435, -0.0040,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-0.1512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2205293373751166, distance: 1.0103144866926594 entropy -5.1744883600849345
epoch: 0, step: 29
	action: tensor([[ 0.0015, -0.0183,  0.0207, -0.0955,  0.0028, -0.0035,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-0.1485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2586931350967888, distance: 0.9852710092596814 entropy -5.172689437410078
epoch: 0, step: 30
	action: tensor([[-0.0901, -0.0314,  0.0325,  0.0061, -0.0028, -0.0018,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-0.1509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20464619047126176, distance: 1.0205560839368224 entropy -5.1568996877122055
epoch: 0, step: 31
	action: tensor([[-0.0617, -0.0350,  0.0332,  0.0528,  0.0632,  0.0005,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-0.1508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25628024982214004, distance: 0.9868731895806263 entropy -5.194206507388314
epoch: 0, step: 32
	action: tensor([[-0.0506, -0.0418,  0.0388, -0.0606,  0.1105,  0.0043,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-0.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20606469676065264, distance: 1.0196456015470532 entropy -5.127971615787451
epoch: 0, step: 33
	action: tensor([[-0.0062, -0.0282,  0.0604, -0.0771,  0.0626, -0.0026,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-0.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2507761827991336, distance: 0.9905182476463981 entropy -5.184168459475397
epoch: 0, step: 34
	action: tensor([[-0.0583, -0.0167,  0.0316,  0.1165, -0.0074, -0.0040,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-0.1545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30367400243535536, distance: 0.9549112244799128 entropy -5.1516172784439815
epoch: 0, step: 35
	action: tensor([[-0.0489, -0.0142,  0.0167, -0.0641,  0.0452,  0.0055,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[-0.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2285997237442723, distance: 1.005070643897303 entropy -5.122853219344357
epoch: 0, step: 36
	action: tensor([[-0.0750, -0.0365,  0.0599,  0.0493,  0.0202, -0.0034,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-0.1523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.239134180901882, distance: 0.9981842901762608 entropy -5.137575599230359
epoch: 0, step: 37
	action: tensor([[-0.0322, -0.0008,  0.0377, -0.0838, -0.0356,  0.0015,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-0.1528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24447291253296044, distance: 0.9946761689983631 entropy -5.154733447389271
epoch: 0, step: 38
	action: tensor([[-0.1252, -0.0488,  0.0146,  0.1046,  0.0402, -0.0029,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-0.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2020407569348135, distance: 1.0222262947175293 entropy -5.139864802150311
epoch: 0, step: 39
	action: tensor([[-0.0230, -0.0068,  0.0422, -0.0107,  0.0480, -0.0025,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-0.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2846975193258766, distance: 0.9678355244421166 entropy -5.174190953518663
epoch: 0, step: 40
	action: tensor([[-0.1852,  0.0196,  0.0415, -0.0293,  0.0775,  0.0035,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-0.1553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12037114603399846, distance: 1.0732636649533045 entropy -5.099446757760473
epoch: 0, step: 41
	action: tensor([[-0.0580, -0.0392,  0.0402, -0.1428, -0.0133,  0.0017,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-0.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15734010765191064, distance: 1.050468067318875 entropy -5.166064133199781
epoch: 0, step: 42
	action: tensor([[-0.0430, -0.0553,  0.0227, -0.0218,  0.0337,  0.0013,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[-0.1465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22275158144955542, distance: 1.0088732728044378 entropy -5.241908486663296
epoch: 0, step: 43
	action: tensor([[-0.0611, -0.0247,  0.0237, -0.1072,  0.0502, -0.0019,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-0.1517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18398750617235893, distance: 1.0337251852601606 entropy -5.208150634076536
epoch: 0, step: 44
	action: tensor([[-0.0574, -0.0149,  0.0098,  0.0111,  0.0137, -0.0061,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-0.1508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2531097328304375, distance: 0.9889744990049714 entropy -5.203712510404977
epoch: 0, step: 45
	action: tensor([[-0.0289, -0.0404,  0.0442,  0.0126,  0.0040, -0.0034,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-0.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2650970602245194, distance: 0.9810060496759839 entropy -5.155434612149347
epoch: 0, step: 46
	action: tensor([[ 0.0025, -0.0028,  0.0417,  0.0991,  0.0039, -0.0007,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-0.1522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36828612113848136, distance: 0.9095297363244829 entropy -5.151141787859117
epoch: 0, step: 47
	action: tensor([[-0.0171, -0.0191,  0.0550, -0.0109, -0.0249, -0.0025,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-0.1546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2813736464819159, distance: 0.9700815908341411 entropy -5.0607334734561515
epoch: 0, step: 48
	action: tensor([[-0.1881, -0.0453,  0.0188, -0.0026,  0.0417, -0.0029,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-0.1515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0800392525339797, distance: 1.097593033780484 entropy -5.131496596878505
epoch: 0, step: 49
	action: tensor([[-0.0207, -0.0285,  0.0190,  0.1246,  0.0446, -0.0004,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-0.1498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33801839135095346, distance: 0.9310642582171855 entropy -5.230829686738169
epoch: 0, step: 50
	action: tensor([[-0.0474, -0.0548,  0.0038,  0.0328, -0.0491, -0.0025,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-0.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24493842981030278, distance: 0.9943696874062788 entropy -5.090885782061531
epoch: 0, step: 51
	action: tensor([[ 0.0075, -0.0482,  0.0192, -0.0212,  0.0368,  0.0002,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[-0.1488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27940373065976365, distance: 0.9714102863736728 entropy -5.236177788592715
epoch: 0, step: 52
	action: tensor([[-0.0738, -0.0493,  0.0295,  0.0106, -0.0055, -0.0041,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-0.1532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21008200328208415, distance: 1.017062630503344 entropy -5.157716924223488
epoch: 0, step: 53
	action: tensor([[-0.0582, -0.0441,  0.0371,  0.0885,  0.0133,  0.0015,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-0.1505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2717607318775096, distance: 0.9765483260262816 entropy -5.229641282876754
epoch: 0, step: 54
	action: tensor([[-0.0156, -0.0426,  0.0190, -0.0975, -0.0103, -0.0034,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-0.1528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2212430982200031, distance: 1.0098518085478636 entropy -5.140992416325759
epoch: 0, step: 55
	action: tensor([[-0.0230, -0.0360,  0.0557,  0.0003, -0.0226, -0.0009,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-0.1487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26872298947326745, distance: 0.9785829698882338 entropy -5.217055910323855
epoch: 0, step: 56
	action: tensor([[ 0.0236, -0.0013,  0.0254,  0.0256, -0.0156, -0.0037,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-0.1513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35294935248356685, distance: 0.9205043206811074 entropy -5.142481534538662
epoch: 0, step: 57
	action: tensor([[ 0.0417,  0.0189,  0.0189,  0.0562,  0.0008, -0.0044,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-0.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3992682852575128, distance: 0.8869455794168282 entropy -5.082531945804042
epoch: 0, step: 58
	action: tensor([[-0.0796, -0.0268,  0.0613,  0.0220,  0.0907,  0.0020,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-0.1556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22869711339113508, distance: 1.0050071965744616 entropy -5.046947061797561
epoch: 0, step: 59
	action: tensor([[ 0.0409, -0.0477,  0.0203, -0.0197, -0.0144, -0.0017,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-0.1567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31254574387410783, distance: 0.9488085635825775 entropy -5.118108336252298
epoch: 0, step: 60
	action: tensor([[-8.1958e-02, -8.0632e-03,  1.7887e-02,  5.0693e-02,  7.2038e-02,
          9.5232e-05,  8.0930e-03]], dtype=torch.float64)
	q_value: tensor([[-0.1512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2529812519296992, distance: 0.9890595575979515 entropy -5.125730843618801
epoch: 0, step: 61
	action: tensor([[-0.0581,  0.0078,  0.0241,  0.0314, -0.0023,  0.0016,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-0.1550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2809754152781847, distance: 0.9703503419400654 entropy -5.103565385020085
epoch: 0, step: 62
	action: tensor([[ 0.0095, -0.0286,  0.0379,  0.1081,  0.0064, -0.0021,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-0.1529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36073491162967586, distance: 0.9149496322831495 entropy -5.11132510271237
epoch: 0, step: 63
	action: tensor([[-0.0818, -0.0180,  0.0391, -0.0288, -0.0147, -0.0019,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-0.1539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20602718407779985, distance: 1.0196696899016793 entropy -5.090106171471848
LOSS epoch 0 actor 0.14302722137766102 critic 11.563734072513316
epoch: 1, step: 0
	action: tensor([[-0.0743, -0.0657, -0.0188,  0.0636,  0.0231, -0.0427,  0.2579]],
       dtype=torch.float64)
	q_value: tensor([[0.1070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2130770729238225, distance: 1.0151326413197141 entropy -7.286514975358598
epoch: 1, step: 1
	action: tensor([[-0.1207, -0.0934, -0.0286,  0.0407,  0.0588, -0.0386,  0.2493]],
       dtype=torch.float64)
	q_value: tensor([[0.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13073296805786694, distance: 1.066923540958303 entropy -7.322868639778134
epoch: 1, step: 2
	action: tensor([[ 0.0218, -0.1028,  0.0071, -0.0837,  0.1071, -0.0402,  0.2506]],
       dtype=torch.float64)
	q_value: tensor([[0.1118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2061510274790891, distance: 1.0195901630685904 entropy -7.325999464879205
epoch: 1, step: 3
	action: tensor([[-0.0567, -0.0442, -0.0033, -0.0864,  0.0273, -0.0419,  0.2553]],
       dtype=torch.float64)
	q_value: tensor([[0.1078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17516355858702226, distance: 1.0392992484188845 entropy -7.298622999791284
epoch: 1, step: 4
	action: tensor([[-0.0946, -0.0184, -0.0280,  0.0478, -0.0185, -0.0404,  0.2558]],
       dtype=torch.float64)
	q_value: tensor([[0.1131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22019889028433293, distance: 1.0105286192591034 entropy -7.328969022574172
epoch: 1, step: 5
	action: tensor([[-0.0796, -0.1186,  0.0565,  0.1423,  0.0151, -0.0382,  0.2501]],
       dtype=torch.float64)
	q_value: tensor([[0.1079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2128530563095412, distance: 1.015277122048157 entropy -7.329392349920428
epoch: 1, step: 6
	action: tensor([[-0.2251, -0.0673,  0.0368,  0.0844,  0.0898, -0.0373,  0.2491]],
       dtype=torch.float64)
	q_value: tensor([[0.1024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05554342201820306, distance: 1.1121098597577848 entropy -7.31707433457605
epoch: 1, step: 7
	action: tensor([[-0.0183, -0.0878,  0.0005,  0.0225,  0.0046, -0.0408,  0.2528]],
       dtype=torch.float64)
	q_value: tensor([[0.1199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23453758004365932, distance: 1.0011949037262295 entropy -7.324361883814091
epoch: 1, step: 8
	action: tensor([[ 0.0512, -0.1056, -0.0266, -0.0181,  0.0962, -0.0384,  0.2510]],
       dtype=torch.float64)
	q_value: tensor([[0.1018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26744153959867745, distance: 0.9794400022606009 entropy -7.319999040834264
epoch: 1, step: 9
	action: tensor([[-0.0845, -0.1077,  0.0038,  0.0175,  0.0865, -0.0408,  0.2515]],
       dtype=torch.float64)
	q_value: tensor([[0.1008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1463962648877264, distance: 1.0572674130557946 entropy -7.295264908344175
epoch: 1, step: 10
	action: tensor([[-0.0216, -0.0558, -0.0390, -0.0431,  0.0398, -0.0408,  0.2521]],
       dtype=torch.float64)
	q_value: tensor([[0.1101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22470043849546872, distance: 1.0076076646939232 entropy -7.320009786087817
epoch: 1, step: 11
	action: tensor([[-0.0710, -0.0202, -0.0175, -0.1016,  0.0213, -0.0401,  0.2522]],
       dtype=torch.float64)
	q_value: tensor([[0.1072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17353971256372835, distance: 1.0403217736513457 entropy -7.318553263816583
epoch: 1, step: 12
	action: tensor([[-0.0408, -0.1527, -0.0191, -0.0629,  0.0089, -0.0406,  0.2566]],
       dtype=torch.float64)
	q_value: tensor([[0.1149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11494489648716277, distance: 1.0765689468313466 entropy -7.33237493945841
epoch: 1, step: 13
	action: tensor([[-0.0007, -0.0083, -0.0243, -0.0079,  0.0769, -0.0405,  0.2553]],
       dtype=torch.float64)
	q_value: tensor([[0.1090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984756965028339, distance: 0.9584689623711032 entropy -7.325448386129328
epoch: 1, step: 14
	action: tensor([[-0.1667, -0.1114,  0.0394, -0.0715,  0.1150, -0.0392,  0.2508]],
       dtype=torch.float64)
	q_value: tensor([[0.1063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006276488186345852, distance: 1.1407473695697201 entropy -7.311909712328876
epoch: 1, step: 15
	action: tensor([[-0.0040, -0.0270, -0.0313,  0.0212, -0.0356, -0.0421,  0.2587]],
       dtype=torch.float64)
	q_value: tensor([[0.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955823846113249, distance: 0.9604434457736203 entropy -7.327567338807768
epoch: 1, step: 16
	action: tensor([[ 0.0775, -0.0749,  0.0150, -0.1712, -0.0070, -0.0378,  0.2497]],
       dtype=torch.float64)
	q_value: tensor([[0.0993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23751035221449768, distance: 0.9992488776185457 entropy -7.322945630180258
epoch: 1, step: 17
	action: tensor([[-1.2545e-01,  4.4235e-03, -1.6400e-02, -3.7105e-03,  1.4804e-04,
         -4.1991e-02,  2.6020e-01]], dtype=torch.float64)
	q_value: tensor([[0.1066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1790325122701556, distance: 1.0368589301079163 entropy -7.310888990417416
epoch: 1, step: 18
	action: tensor([[-0.1021, -0.0439,  0.0170, -0.0483, -0.0038, -0.0392,  0.2531]],
       dtype=torch.float64)
	q_value: tensor([[0.1135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14594277504833386, distance: 1.0575482203826851 entropy -7.333559791543297
epoch: 1, step: 19
	action: tensor([[ 0.1377, -0.0673, -0.0032,  0.0446,  0.0004, -0.0398,  0.2557]],
       dtype=torch.float64)
	q_value: tensor([[0.1131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4083148036689751, distance: 0.8802419155757454 entropy -7.33742255218229
epoch: 1, step: 20
	action: tensor([[ 0.0141, -0.1424, -0.0234,  0.1260,  0.0376, -0.0377,  0.2496]],
       dtype=torch.float64)
	q_value: tensor([[0.0844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2761815867444608, distance: 0.9735796929503968 entropy -7.297082957622749
epoch: 1, step: 21
	action: tensor([[-0.1495, -0.0314,  0.0046, -0.0672, -0.0077, -0.0372,  0.2469]],
       dtype=torch.float64)
	q_value: tensor([[0.0959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0952973669113103, distance: 1.0884528489170222 entropy -7.306415080649933
epoch: 1, step: 22
	action: tensor([[-0.0075, -0.1476,  0.0032, -0.0610,  0.0025, -0.0405,  0.2575]],
       dtype=torch.float64)
	q_value: tensor([[0.1184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15260757791810464, distance: 1.0534137461810402 entropy -7.343803781709269
epoch: 1, step: 23
	action: tensor([[-0.1979, -0.0355,  0.0003, -0.1867, -0.0662, -0.0403,  0.2556]],
       dtype=torch.float64)
	q_value: tensor([[0.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018850171214319644, distance: 1.1550794427085447 entropy -7.3220684031366545
epoch: 1, step: 24
	action: tensor([[-0.0937, -0.0557, -0.0348,  0.0540,  0.0101, -0.0411,  0.2680]],
       dtype=torch.float64)
	q_value: tensor([[0.1286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19553168829517753, distance: 1.0263870506426498 entropy -7.369346995172024
epoch: 1, step: 25
	action: tensor([[-0.0610, -0.0897, -0.0236,  0.0305,  0.0292, -0.0384,  0.2497]],
       dtype=torch.float64)
	q_value: tensor([[0.1087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1931854775936227, distance: 1.0278826763857503 entropy -7.328213030185948
epoch: 1, step: 26
	action: tensor([[ 0.0155, -0.1122, -0.0084,  0.0199,  0.0586, -0.0391,  0.2505]],
       dtype=torch.float64)
	q_value: tensor([[0.1057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2471888913883976, distance: 0.9928867217858901 entropy -7.322223647063559
epoch: 1, step: 27
	action: tensor([[-0.1182, -0.1448, -0.0245,  0.0287, -0.0350, -0.0393,  0.2509]],
       dtype=torch.float64)
	q_value: tensor([[0.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08763969610680422, distance: 1.0930496357758193 entropy -7.308510885636123
epoch: 1, step: 28
	action: tensor([[-0.0526, -0.0283, -0.0148, -0.0015,  0.0032, -0.0382,  0.2516]],
       dtype=torch.float64)
	q_value: tensor([[0.1105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2345357982483185, distance: 1.0011960689848562 entropy -7.342049334458582
epoch: 1, step: 29
	action: tensor([[ 0.0553, -0.0683,  0.0310,  0.0303, -0.0229, -0.0388,  0.2516]],
       dtype=torch.float64)
	q_value: tensor([[0.1060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3267419675304568, distance: 0.9389608074790998 entropy -7.324518128482639
epoch: 1, step: 30
	action: tensor([[ 0.0252, -0.0711, -0.0487, -0.0598,  0.1145, -0.0376,  0.2513]],
       dtype=torch.float64)
	q_value: tensor([[0.0937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2510744442392795, distance: 0.9903210683785586 entropy -7.30941074924929
epoch: 1, step: 31
	action: tensor([[ 0.0440, -0.0558,  0.0025, -0.0544, -0.0040, -0.0413,  0.2524]],
       dtype=torch.float64)
	q_value: tensor([[0.1069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822688295095136, distance: 0.9694771938630994 entropy -7.297962258795375
epoch: 1, step: 32
	action: tensor([[-0.0259, -0.0097,  0.0065, -0.0654, -0.0724, -0.0396,  0.2539]],
       dtype=torch.float64)
	q_value: tensor([[0.1008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2455756701993147, distance: 0.9939499956208994 entropy -7.313276781234763
epoch: 1, step: 33
	action: tensor([[-0.1540, -0.0403,  0.0018, -0.0119, -0.0434, -0.0391,  0.2559]],
       dtype=torch.float64)
	q_value: tensor([[0.1052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11019123267761788, distance: 1.079456220994 entropy -7.3323341974648715
epoch: 1, step: 34
	action: tensor([[ 0.1575, -0.0510, -0.0116, -0.0105, -0.0097, -0.0394,  0.2550]],
       dtype=torch.float64)
	q_value: tensor([[0.1145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41062440055668437, distance: 0.8785222580612537 entropy -7.345032703318628
epoch: 1, step: 35
	action: tensor([[ 0.0174,  0.0047, -0.0142,  0.0848,  0.0058, -0.0393,  0.2513]],
       dtype=torch.float64)
	q_value: tensor([[0.0850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3699133983315289, distance: 0.9083575194223379 entropy -7.299120882387507
epoch: 1, step: 36
	action: tensor([[ 0.1067, -0.0186,  0.0039, -0.0111,  0.0431, -0.0376,  0.2473]],
       dtype=torch.float64)
	q_value: tensor([[0.0967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39135668717781225, distance: 0.8927669833748685 entropy -7.305705053628985
epoch: 1, step: 37
	action: tensor([[-0.0931, -0.0549, -0.0301,  0.0417,  0.0524, -0.0392,  0.2514]],
       dtype=torch.float64)
	q_value: tensor([[0.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1909490905648541, distance: 1.0293062704514842 entropy -7.296301261831281
epoch: 1, step: 38
	action: tensor([[ 0.0056, -0.1403,  0.0043,  0.0299,  0.0307, -0.0397,  0.2499]],
       dtype=torch.float64)
	q_value: tensor([[0.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21987658269669363, distance: 1.0107374336540182 entropy -7.320920219528354
epoch: 1, step: 39
	action: tensor([[ 0.0273, -0.0487,  0.0331,  0.0648, -0.0006, -0.0387,  0.2514]],
       dtype=torch.float64)
	q_value: tensor([[0.0996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33258066181012547, distance: 0.9348804677749479 entropy -7.314943833489127
epoch: 1, step: 40
	action: tensor([[ 0.0853, -0.0060, -0.0164,  0.0452,  0.0454, -0.0376,  0.2497]],
       dtype=torch.float64)
	q_value: tensor([[0.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.408258839386861, distance: 0.8802835432372061 entropy -7.30668496478232
epoch: 1, step: 41
	action: tensor([[-0.0909, -0.0804,  0.0288, -0.0090,  0.0327, -0.0386,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[0.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14857379922087033, distance: 1.055918012791158 entropy -7.295116422313448
epoch: 1, step: 42
	action: tensor([[ 0.1129, -0.0540,  0.0278,  0.0267,  0.0896, -0.0398,  0.2535]],
       dtype=torch.float64)
	q_value: tensor([[0.1105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38738338172494924, distance: 0.8956762945098325 entropy -7.327294037855227
epoch: 1, step: 43
	action: tensor([[-0.0681, -0.0550, -0.0018,  0.0919,  0.1010, -0.0389,  0.2514]],
       dtype=torch.float64)
	q_value: tensor([[0.0919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24217265873103344, distance: 0.9961891979979212 entropy -7.288818891659232
epoch: 1, step: 44
	action: tensor([[ 0.0100, -0.0469, -0.0053,  0.0065,  0.0144, -0.0397,  0.2489]],
       dtype=torch.float64)
	q_value: tensor([[0.1076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2870741305564021, distance: 0.966226357263024 entropy -7.305015857570553
epoch: 1, step: 45
	action: tensor([[ 0.0828, -0.1432, -0.0410,  0.0449, -0.0272, -0.0387,  0.2508]],
       dtype=torch.float64)
	q_value: tensor([[0.0998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29895674683108875, distance: 0.9581402846079082 entropy -7.314404855681947
epoch: 1, step: 46
	action: tensor([[-0.1050, -0.0497, -0.0077,  0.0111, -0.0255, -0.0374,  0.2493]],
       dtype=torch.float64)
	q_value: tensor([[0.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16872877370985984, distance: 1.0433453079044535 entropy -7.308512753324615
epoch: 1, step: 47
	action: tensor([[-0.0627, -0.0560, -0.0317, -0.0267,  0.0336, -0.0388,  0.2525]],
       dtype=torch.float64)
	q_value: tensor([[0.1093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19025673924295505, distance: 1.0297465945100828 entropy -7.332837162807022
epoch: 1, step: 48
	action: tensor([[ 0.0083,  0.0052,  0.0114, -0.0400,  0.0517, -0.0397,  0.2522]],
       dtype=torch.float64)
	q_value: tensor([[0.1100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30204053395719044, distance: 0.9560306022508617 entropy -7.324373474601037
epoch: 1, step: 49
	action: tensor([[-0.0377, -0.1328,  0.0402,  0.0037, -0.0927, -0.0393,  0.2530]],
       dtype=torch.float64)
	q_value: tensor([[0.1047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1699257444672715, distance: 1.0425938662132168 entropy -7.3141063850761805
epoch: 1, step: 50
	action: tensor([[ 0.0508, -0.0352,  0.0122,  0.0391,  0.0435, -0.0373,  0.2545]],
       dtype=torch.float64)
	q_value: tensor([[0.1055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.351919020123729, distance: 0.921236912585741 entropy -7.334462821966719
epoch: 1, step: 51
	action: tensor([[ 0.0269, -0.0349, -0.0062,  0.0161,  0.0733, -0.0383,  0.2499]],
       dtype=torch.float64)
	q_value: tensor([[0.0957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31725339038407907, distance: 0.9455542899877517 entropy -7.30245798946546
epoch: 1, step: 52
	action: tensor([[-0.0158, -0.0733, -0.0022, -0.0842,  0.0068, -0.0392,  0.2501]],
       dtype=torch.float64)
	q_value: tensor([[0.1015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19524049430985435, distance: 1.0265727948727568 entropy -7.303186152070665
epoch: 1, step: 53
	action: tensor([[-0.1052, -0.0599, -0.0442,  0.0522,  0.0441, -0.0404,  0.2555]],
       dtype=torch.float64)
	q_value: tensor([[0.1087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17844635960242727, distance: 1.0372290112587224 entropy -7.321384809453802
epoch: 1, step: 54
	action: tensor([[-0.0343, -0.0369,  0.0323, -0.1346,  0.0925, -0.0393,  0.2495]],
       dtype=torch.float64)
	q_value: tensor([[0.1108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17738748255468328, distance: 1.0378972234581012 entropy -7.324166209194359
epoch: 1, step: 55
	action: tensor([[-0.1152,  0.0205, -0.0078, -0.0727,  0.1339, -0.0423,  0.2588]],
       dtype=torch.float64)
	q_value: tensor([[0.1161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16867709824635035, distance: 1.0433777368595307 entropy -7.314512158679649
epoch: 1, step: 56
	action: tensor([[-0.0740, -0.0830, -0.0404, -0.0923,  0.0403, -0.0414,  0.2557]],
       dtype=torch.float64)
	q_value: tensor([[0.1234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12521761558874733, distance: 1.0703029138605646 entropy -7.321397707701548
epoch: 1, step: 57
	action: tensor([[-0.1571, -0.1070,  0.0195,  0.0875, -0.1266, -0.0410,  0.2553]],
       dtype=torch.float64)
	q_value: tensor([[0.1155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10648448121740861, distance: 1.0817022752104375 entropy -7.327194016310136
epoch: 1, step: 58
	action: tensor([[-0.0356, -0.0904, -0.0148, -0.0436, -0.0454, -0.0354,  0.2517]],
       dtype=torch.float64)
	q_value: tensor([[0.1108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18357961122022204, distance: 1.0339835137841018 entropy -7.349893335542285
epoch: 1, step: 59
	action: tensor([[-0.1582, -0.0742,  0.0102, -0.0543, -0.0082, -0.0394,  0.2540]],
       dtype=torch.float64)
	q_value: tensor([[0.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05732944932596451, distance: 1.1110578270072746 entropy -7.328994901025083
epoch: 1, step: 60
	action: tensor([[-0.1300, -0.0534, -0.0023,  0.0022, -0.0426, -0.0404,  0.2571]],
       dtype=torch.float64)
	q_value: tensor([[0.1181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1336452012790732, distance: 1.0651348288452538 entropy -7.345824374869772
epoch: 1, step: 61
	action: tensor([[-0.0824, -0.0679, -0.0158, -0.1483,  0.0689, -0.0390,  0.2534]],
       dtype=torch.float64)
	q_value: tensor([[0.1119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10008980455012506, distance: 1.0855661165048136 entropy -7.340001810927828
epoch: 1, step: 62
	action: tensor([[-0.1354, -0.0270, -0.0357, -0.0951,  0.0640, -0.0423,  0.2591]],
       dtype=torch.float64)
	q_value: tensor([[0.1210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10189960274554066, distance: 1.0844739828804255 entropy -7.321950425469182
epoch: 1, step: 63
	action: tensor([[-0.0375, -0.0742, -0.0189,  0.0545,  0.0679, -0.0411,  0.2568]],
       dtype=torch.float64)
	q_value: tensor([[0.1223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2404806264649043, distance: 0.9973006942154697 entropy -7.333312491881925
LOSS epoch 1 actor 0.08395389571592989 critic 8.759372414933567
epoch: 2, step: 0
	action: tensor([[-0.0402, -0.1030,  0.0445,  0.1156, -0.0277,  0.1399,  0.1644]],
       dtype=torch.float64)
	q_value: tensor([[0.3833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2972818696624787, distance: 0.959284158346959 entropy -7.31412626755892
epoch: 2, step: 1
	action: tensor([[-0.0086,  0.0130, -0.0160,  0.0458,  0.0109,  0.1399,  0.1705]],
       dtype=torch.float64)
	q_value: tensor([[0.3514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37345918429091407, distance: 0.9057980415567588 entropy -7.267606415840314
epoch: 2, step: 2
	action: tensor([[-0.0066,  0.0210, -0.0537,  0.1150, -0.0458,  0.1367,  0.1708]],
       dtype=torch.float64)
	q_value: tensor([[0.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4040187949046705, distance: 0.8834316926263097 entropy -7.262798021374178
epoch: 2, step: 3
	action: tensor([[-0.0700, -0.0644, -0.0152,  0.0623,  0.0737,  0.1369,  0.1685]],
       dtype=torch.float64)
	q_value: tensor([[0.3483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2651601267033814, distance: 0.9809639557331421 entropy -7.272792621331544
epoch: 2, step: 4
	action: tensor([[ 0.1866, -0.0859,  0.0535,  0.0272, -0.0149,  0.1386,  0.1722]],
       dtype=torch.float64)
	q_value: tensor([[0.3702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4725920250952428, distance: 0.8310555219131425 entropy -7.262799732884
epoch: 2, step: 5
	action: tensor([[-0.0791, -0.0647, -0.0201, -0.0312, -0.0154,  0.1330,  0.1646]],
       dtype=torch.float64)
	q_value: tensor([[0.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20543834242768777, distance: 1.0200477335118947 entropy -7.244677380784637
epoch: 2, step: 6
	action: tensor([[ 0.1027,  0.0036, -0.0007, -0.0169,  0.0438,  0.1377,  0.1763]],
       dtype=torch.float64)
	q_value: tensor([[0.3717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4452228893523925, distance: 0.852346064909937 entropy -7.2718746245721215
epoch: 2, step: 7
	action: tensor([[-0.1804, -0.1374, -0.0346, -0.0510,  0.0242,  0.1345,  0.1683]],
       dtype=torch.float64)
	q_value: tensor([[0.3506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02898709242106612, distance: 1.1276366813233965 entropy -7.246239454718124
epoch: 2, step: 8
	action: tensor([[ 0.0860,  0.0082,  0.0026, -0.0187, -0.0215,  0.1404,  0.1835]],
       dtype=torch.float64)
	q_value: tensor([[0.3905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4311192873578522, distance: 0.8631122849579748 entropy -7.261144760155718
epoch: 2, step: 9
	action: tensor([[-0.0707, -0.0505, -0.0460, -0.0055,  0.0060,  0.1345,  0.1691]],
       dtype=torch.float64)
	q_value: tensor([[0.3494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2376890774197037, distance: 0.9991317603385006 entropy -7.254283448871715
epoch: 2, step: 10
	action: tensor([[-0.0293, -0.0291, -0.0362,  0.0441, -0.0425,  0.1372,  0.1745]],
       dtype=torch.float64)
	q_value: tensor([[0.3702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31837883765923447, distance: 0.9447746373361597 entropy -7.272410881228262
epoch: 2, step: 11
	action: tensor([[-0.0526,  0.0079,  0.0346,  0.0011,  0.0159,  0.1372,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[0.3561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3064447757343398, distance: 0.9530094716931294 entropy -7.274843459600492
epoch: 2, step: 12
	action: tensor([[ 0.0389, -0.0392,  0.0110,  0.0285,  0.0349,  0.1386,  0.1749]],
       dtype=torch.float64)
	q_value: tensor([[0.3684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3773443640423405, distance: 0.9029852473027362 entropy -7.260897737744977
epoch: 2, step: 13
	action: tensor([[-0.0916, -0.1350, -0.0072, -0.1066, -0.1231,  0.1362,  0.1687]],
       dtype=torch.float64)
	q_value: tensor([[0.3532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0948280400305902, distance: 1.0887351372223482 entropy -7.257663859510804
epoch: 2, step: 14
	action: tensor([[-0.0571, -0.0068,  0.0217,  0.0482, -0.0643,  0.1385,  0.1810]],
       dtype=torch.float64)
	q_value: tensor([[0.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31055278915484497, distance: 0.9501828834010622 entropy -7.266898672675125
epoch: 2, step: 15
	action: tensor([[-0.0901, -0.0602,  0.0126,  0.1226,  0.0548,  0.1403,  0.1737]],
       dtype=torch.float64)
	q_value: tensor([[0.3591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27898818328287933, distance: 0.9716903383490355 entropy -7.271620067524622
epoch: 2, step: 16
	action: tensor([[ 0.0903, -0.0524, -0.0364, -0.0082,  0.0812,  0.1409,  0.1720]],
       dtype=torch.float64)
	q_value: tensor([[0.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3983247805687665, distance: 0.8876418211562823 entropy -7.265212063027278
epoch: 2, step: 17
	action: tensor([[ 0.0251, -0.0823,  0.0072,  0.0489,  0.0320,  0.1339,  0.1676]],
       dtype=torch.float64)
	q_value: tensor([[0.3537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34097237669681857, distance: 0.9289845740904227 entropy -7.246214465730363
epoch: 2, step: 18
	action: tensor([[-0.0637, -0.1464, -0.0186,  0.1104,  0.0643,  0.1363,  0.1684]],
       dtype=torch.float64)
	q_value: tensor([[0.3525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.234026260093394, distance: 1.0015292411493395 entropy -7.2625978841075565
epoch: 2, step: 19
	action: tensor([[ 0.0738, -0.0825,  0.0577,  0.0451, -0.0321,  0.1388,  0.1707]],
       dtype=torch.float64)
	q_value: tensor([[0.3644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3870198482410955, distance: 0.8959420072112738 entropy -7.268927484075563
epoch: 2, step: 20
	action: tensor([[-0.0153, -0.0584,  0.0376, -0.0681, -0.0251,  0.1360,  0.1673]],
       dtype=torch.float64)
	q_value: tensor([[0.3426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25893136468674827, distance: 0.9851126810555357 entropy -7.258879820834077
epoch: 2, step: 21
	action: tensor([[ 0.0065, -0.1584, -0.0535,  0.0189, -0.1997,  0.1369,  0.1751]],
       dtype=torch.float64)
	q_value: tensor([[0.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23664704350550647, distance: 0.9998144041264013 entropy -7.258848332188846
epoch: 2, step: 22
	action: tensor([[-0.0124, -0.0824,  0.0029,  0.0142,  0.0314,  0.1379,  0.1717]],
       dtype=torch.float64)
	q_value: tensor([[0.3437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2873061304845722, distance: 0.9660691300568558 entropy -7.278116919454871
epoch: 2, step: 23
	action: tensor([[-0.0220, -0.0431,  0.0541,  0.0115,  0.0094,  0.1371,  0.1711]],
       dtype=torch.float64)
	q_value: tensor([[0.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30677537950984013, distance: 0.9527823044294004 entropy -7.262675607575502
epoch: 2, step: 24
	action: tensor([[-0.0810, -0.0160, -0.0181, -0.0953,  0.0418,  0.1385,  0.1725]],
       dtype=torch.float64)
	q_value: tensor([[0.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21360752729912846, distance: 1.0147904398734222 entropy -7.259727176838399
epoch: 2, step: 25
	action: tensor([[-0.1313, -0.0771, -0.0619,  0.1502, -0.1076,  0.1376,  0.1802]],
       dtype=torch.float64)
	q_value: tensor([[0.3840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21805400973620248, distance: 1.0119174187683815 entropy -7.253182020341205
epoch: 2, step: 26
	action: tensor([[ 0.1528, -0.0591, -0.0140,  0.0207, -0.0301,  0.1429,  0.1742]],
       dtype=torch.float64)
	q_value: tensor([[0.3558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4593437995570675, distance: 0.8414286343932661 entropy -7.285441752536568
epoch: 2, step: 27
	action: tensor([[ 0.0619, -0.0765, -0.0153,  0.0584, -0.0347,  0.1324,  0.1646]],
       dtype=torch.float64)
	q_value: tensor([[0.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3808514903512852, distance: 0.9004386106640456 entropy -7.252287868051718
epoch: 2, step: 28
	action: tensor([[ 0.0206,  0.0230, -0.0589,  0.0694,  0.0524,  0.1349,  0.1666]],
       dtype=torch.float64)
	q_value: tensor([[0.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4163641785833737, distance: 0.8742339401664903 entropy -7.268286101269458
epoch: 2, step: 29
	action: tensor([[-0.1728, -0.0942, -0.0184, -0.0761, -0.0402,  0.1350,  0.1680]],
       dtype=torch.float64)
	q_value: tensor([[0.3577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05685267136942518, distance: 1.1113387633939618 entropy -7.261416172812867
epoch: 2, step: 30
	action: tensor([[-1.3830e-01, -1.9938e-04, -2.0787e-02,  1.7699e-02,  2.0830e-01,
          1.4037e-01,  1.8463e-01]], dtype=torch.float64)
	q_value: tensor([[0.3880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2221428642774388, distance: 1.009268254819423 entropy -7.26347082304789
epoch: 2, step: 31
	action: tensor([[-0.0043, -0.0914,  0.0358,  0.0083, -0.0525,  0.1405,  0.1799]],
       dtype=torch.float64)
	q_value: tensor([[0.4034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845218790184466, distance: 0.9679543416556324 entropy -7.238824776874602
epoch: 2, step: 32
	action: tensor([[-0.0954, -0.1494,  0.0231,  0.0584,  0.1546,  0.1380,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[0.3556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17827291837776238, distance: 1.0373384921053646 entropy -7.2657812228749155
epoch: 2, step: 33
	action: tensor([[ 0.0059,  0.0116, -0.0508,  0.0797,  0.0024,  0.1398,  0.1752]],
       dtype=torch.float64)
	q_value: tensor([[0.3816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3985174526638199, distance: 0.8874996867475691 entropy -7.253693338626975
epoch: 2, step: 34
	action: tensor([[-0.0629, -0.0379,  0.0499,  0.0105,  0.0234,  0.1360,  0.1686]],
       dtype=torch.float64)
	q_value: tensor([[0.3531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.267934289172046, distance: 0.9791105406175765 entropy -7.267237682393241
epoch: 2, step: 35
	action: tensor([[-0.1196, -0.1456, -0.0472, -0.0023, -0.0609,  0.1394,  0.1746]],
       dtype=torch.float64)
	q_value: tensor([[0.3691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.111571772723433, distance: 1.078618506718163 entropy -7.258682488048035
epoch: 2, step: 36
	action: tensor([[ 0.1183,  0.0417, -0.0325, -0.0343,  0.0463,  0.1397,  0.1777]],
       dtype=torch.float64)
	q_value: tensor([[0.3694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4779328998124841, distance: 0.826836911427293 entropy -7.275801888262722
epoch: 2, step: 37
	action: tensor([[-0.1355, -0.0361,  0.0016, -0.0017,  0.0493,  0.1337,  0.1685]],
       dtype=torch.float64)
	q_value: tensor([[0.3529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18305778543036555, distance: 1.0343139030554513 entropy -7.24061974981747
epoch: 2, step: 38
	action: tensor([[-0.1574, -0.0102,  0.0062, -0.0216,  0.0121,  0.1400,  0.1784]],
       dtype=torch.float64)
	q_value: tensor([[0.3842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16835209708127563, distance: 1.0435816683545183 entropy -7.26132809651662
epoch: 2, step: 39
	action: tensor([[-0.1602, -0.0270, -0.0664,  0.0109, -0.0423,  0.1408,  0.1813]],
       dtype=torch.float64)
	q_value: tensor([[0.3865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16252714066633933, distance: 1.0472299735110042 entropy -7.263830355726301
epoch: 2, step: 40
	action: tensor([[-0.0323, -0.0535,  0.0167,  0.1263,  0.0193,  0.1403,  0.1801]],
       dtype=torch.float64)
	q_value: tensor([[0.3781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.344711797563131, distance: 0.9263452263499153 entropy -7.279845253438403
epoch: 2, step: 41
	action: tensor([[-0.2775, -0.0816,  0.0068,  0.0429, -0.0011,  0.1399,  0.1691]],
       dtype=torch.float64)
	q_value: tensor([[0.3548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006404582655564317, distance: 1.1406738440184274 entropy -7.267034051194791
epoch: 2, step: 42
	action: tensor([[ 0.0784, -0.0737, -0.0002, -0.0073,  0.0096,  0.1448,  0.1861]],
       dtype=torch.float64)
	q_value: tensor([[0.3957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37146302375952633, distance: 0.9072398314425834 entropy -7.257869702827591
epoch: 2, step: 43
	action: tensor([[-0.0804, -0.0867,  0.0253, -0.1041,  0.0637,  0.1348,  0.1679]],
       dtype=torch.float64)
	q_value: tensor([[0.3497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1537017934239211, distance: 1.0527334037765612 entropy -7.254960980903541
epoch: 2, step: 44
	action: tensor([[-0.1257, -0.0491, -0.0007,  0.0217,  0.0347,  0.1385,  0.1802]],
       dtype=torch.float64)
	q_value: tensor([[0.3861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19645478343875145, distance: 1.0257980126189077 entropy -7.247057338481139
epoch: 2, step: 45
	action: tensor([[ 0.0069, -0.1263,  0.0180, -0.0734, -0.0762,  0.1403,  0.1767]],
       dtype=torch.float64)
	q_value: tensor([[0.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22337072048867845, distance: 1.0084713695933274 entropy -7.264892130443107
epoch: 2, step: 46
	action: tensor([[-0.0668, -0.0381,  0.0180, -0.0439,  0.0534,  0.1362,  0.1736]],
       dtype=torch.float64)
	q_value: tensor([[0.3607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23652664102232857, distance: 0.9998932506085435 entropy -7.261657471333635
epoch: 2, step: 47
	action: tensor([[-0.0155, -0.0746, -0.0218, -0.1528, -0.0948,  0.1384,  0.1767]],
       dtype=torch.float64)
	q_value: tensor([[0.3775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2023721172264744, distance: 1.022014028002069 entropy -7.254992255102444
epoch: 2, step: 48
	action: tensor([[ 0.0476, -0.1015,  0.0108,  0.0916,  0.0488,  0.1356,  0.1791]],
       dtype=torch.float64)
	q_value: tensor([[0.3717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3711365019236036, distance: 0.9074754541375072 entropy -7.261693095607706
epoch: 2, step: 49
	action: tensor([[-0.1750, -0.0432, -0.0230,  0.0187, -0.0283,  0.1366,  0.1665]],
       dtype=torch.float64)
	q_value: tensor([[0.3489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13950484139105535, distance: 1.0615266673989772 entropy -7.262590865384758
epoch: 2, step: 50
	action: tensor([[-0.0408, -0.0839,  0.0354, -0.1141,  0.0990,  0.1410,  0.1803]],
       dtype=torch.float64)
	q_value: tensor([[0.3804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1947959572321295, distance: 1.0268562874343856 entropy -7.272741377565041
epoch: 2, step: 51
	action: tensor([[-0.0977,  0.0013,  0.0101,  0.0768, -0.0149,  0.1383,  0.1790]],
       dtype=torch.float64)
	q_value: tensor([[0.3849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877395430608155, distance: 0.9657753361819104 entropy -7.239872273818995
epoch: 2, step: 52
	action: tensor([[-0.0662, -0.0636,  0.0219, -0.0867, -0.0188,  0.1409,  0.1744]],
       dtype=torch.float64)
	q_value: tensor([[0.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19437459702443294, distance: 1.0271249272858394 entropy -7.269577684297056
epoch: 2, step: 53
	action: tensor([[ 0.0078, -0.0661,  0.0069, -0.0520, -0.0812,  0.1382,  0.1787]],
       dtype=torch.float64)
	q_value: tensor([[0.3763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2824591689660183, distance: 0.9693486345880369 entropy -7.260214222502621
epoch: 2, step: 54
	action: tensor([[ 0.0064, -0.0338, -0.0180,  0.0238,  0.0482,  0.1364,  0.1732]],
       dtype=torch.float64)
	q_value: tensor([[0.3578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3458931947092393, distance: 0.925509811257921 entropy -7.267565481571482
epoch: 2, step: 55
	action: tensor([[ 0.0227,  0.0403,  0.0149, -0.0853, -0.0092,  0.1362,  0.1700]],
       dtype=torch.float64)
	q_value: tensor([[0.3596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3628944113719047, distance: 0.913402930457157 entropy -7.2586921436994825
epoch: 2, step: 56
	action: tensor([[-0.1384, -0.0629,  0.0022, -0.0498, -0.0386,  0.1358,  0.1755]],
       dtype=torch.float64)
	q_value: tensor([[0.3655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1331372794348905, distance: 1.0654470138624033 entropy -7.252377501385645
epoch: 2, step: 57
	action: tensor([[ 0.1546, -0.0211, -0.0120, -0.0315,  0.0305,  0.1401,  0.1812]],
       dtype=torch.float64)
	q_value: tensor([[0.3812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4654146139034986, distance: 0.8366912638942237 entropy -7.267290129361976
epoch: 2, step: 58
	action: tensor([[-0.1076, -0.0782,  0.0388,  0.0216,  0.0203,  0.1328,  0.1664]],
       dtype=torch.float64)
	q_value: tensor([[0.3462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19452659888657675, distance: 1.0270280260051945 entropy -7.240681382772507
epoch: 2, step: 59
	action: tensor([[-0.1762, -0.1235, -0.0268,  0.0906, -0.0751,  0.1401,  0.1760]],
       dtype=torch.float64)
	q_value: tensor([[0.3747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11201464568133279, distance: 1.0783496327113593 entropy -7.262333270703865
epoch: 2, step: 60
	action: tensor([[ 0.1040, -0.0433,  0.0143,  0.0442, -0.0801,  0.1434,  0.1783]],
       dtype=torch.float64)
	q_value: tensor([[0.3685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43955448043407963, distance: 0.8566894014714381 entropy -7.2785677299275475
epoch: 2, step: 61
	action: tensor([[ 0.0600, -0.0481,  0.0326, -0.0311, -0.0602,  0.1349,  0.1663]],
       dtype=torch.float64)
	q_value: tensor([[0.3358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35767135847656384, distance: 0.9171393707037242 entropy -7.262576486309883
epoch: 2, step: 62
	action: tensor([[-0.0983, -0.0543, -0.0097,  0.0045,  0.0147,  0.1353,  0.1701]],
       dtype=torch.float64)
	q_value: tensor([[0.3490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21211902773976932, distance: 1.0157503937560566 entropy -7.259547492498805
epoch: 2, step: 63
	action: tensor([[-0.0864, -0.0036, -0.0007,  0.1340, -0.0170,  0.1390,  0.1756]],
       dtype=torch.float64)
	q_value: tensor([[0.3741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32197548823345223, distance: 0.9422787357933794 entropy -7.269051810636749
LOSS epoch 2 actor 0.0885413213732479 critic 12.609454838971773
epoch: 3, step: 0
	action: tensor([[ 0.0891,  0.0128,  0.0025,  0.0323, -0.0833,  0.2941,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[0.7200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49156126431130587, distance: 0.8159734130586195 entropy -7.118376454555947
epoch: 3, step: 1
	action: tensor([[-0.2012, -0.0721,  0.0247,  0.0666,  0.0411,  0.2979,  0.1733]],
       dtype=torch.float64)
	q_value: tensor([[0.6866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16590763292568445, distance: 1.0451142437295189 entropy -4.739733102796204
epoch: 3, step: 2
	action: tensor([[ 0.0776, -0.0318,  0.0128,  0.0687,  0.0591,  0.3040,  0.1937]],
       dtype=torch.float64)
	q_value: tensor([[0.7678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4827593795920476, distance: 0.8230060073254853 entropy -4.648455546493188
epoch: 3, step: 3
	action: tensor([[ 0.0005, -0.1295,  0.0802, -0.0691, -0.0295,  0.2946,  0.1698]],
       dtype=torch.float64)
	q_value: tensor([[0.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26459141194062785, distance: 0.981343481126122 entropy -4.608721299619012
epoch: 3, step: 4
	action: tensor([[-0.0139, -0.0199,  0.0569,  0.0260, -0.0276,  0.3040,  0.1818]],
       dtype=torch.float64)
	q_value: tensor([[0.7332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3815849865160774, distance: 0.8999050843896693 entropy -4.704465110363309
epoch: 3, step: 5
	action: tensor([[-0.0404,  0.0038, -0.0216, -0.0167, -0.0234,  0.3022,  0.1807]],
       dtype=torch.float64)
	q_value: tensor([[0.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34396629224976205, distance: 0.9268720166835513 entropy -4.696430799399343
epoch: 3, step: 6
	action: tensor([[ 0.0354, -0.0884,  0.0462,  0.0290, -0.0113,  0.3028,  0.1850]],
       dtype=torch.float64)
	q_value: tensor([[0.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3835576667692585, distance: 0.8984686356378542 entropy -4.677557606079291
epoch: 3, step: 7
	action: tensor([[-0.0317, -0.0253, -0.0493, -0.0603,  0.0580,  0.3001,  0.1751]],
       dtype=torch.float64)
	q_value: tensor([[0.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31785906525169294, distance: 0.9451347891582461 entropy -4.703210381793265
epoch: 3, step: 8
	action: tensor([[-0.0619, -0.1117, -0.0081,  0.0958,  0.0416,  0.3060,  0.1846]],
       dtype=torch.float64)
	q_value: tensor([[0.7506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3026311975382705, distance: 0.9556259856448367 entropy -4.589451807015016
epoch: 3, step: 9
	action: tensor([[-0.0166, -0.0621, -0.0236,  0.1833,  0.1066,  0.2952,  0.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.7334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4217165196363273, distance: 0.8702160446096916 entropy -4.6458969622165025
epoch: 3, step: 10
	action: tensor([[-0.0487, -0.1239,  0.0502, -0.0992,  0.0632,  0.2936,  0.1722]],
       dtype=torch.float64)
	q_value: tensor([[0.7335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20876026306504625, distance: 1.017913181268695 entropy -4.613814497959419
epoch: 3, step: 11
	action: tensor([[ 0.0464, -0.1276,  0.0036,  0.0595,  0.0273,  0.3049,  0.1857]],
       dtype=torch.float64)
	q_value: tensor([[0.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3803245503437849, distance: 0.9008216982514679 entropy -4.617106612271127
epoch: 3, step: 12
	action: tensor([[-0.1225, -0.0759, -0.0196, -0.0228, -0.0247,  0.2960,  0.1719]],
       dtype=torch.float64)
	q_value: tensor([[0.7189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1960434685911252, distance: 1.0260605192986938 entropy -4.675833373327497
epoch: 3, step: 13
	action: tensor([[-0.1215, -0.0418, -0.0741,  0.0095, -0.1462,  0.3041,  0.1911]],
       dtype=torch.float64)
	q_value: tensor([[0.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22115333131049453, distance: 1.0099100094212856 entropy -4.710069169273198
epoch: 3, step: 14
	action: tensor([[-0.0573, -0.0837, -0.0216, -0.0039, -0.1207,  0.3035,  0.1933]],
       dtype=torch.float64)
	q_value: tensor([[0.7276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2621917903165293, distance: 0.9829432280190391 entropy -4.894327211083289
epoch: 3, step: 15
	action: tensor([[-0.0755, -0.0459,  0.0357,  0.0450,  0.0588,  0.3024,  0.1865]],
       dtype=torch.float64)
	q_value: tensor([[0.7199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31502981662463636, distance: 0.9470927821550945 entropy -4.885908913532133
epoch: 3, step: 16
	action: tensor([[-0.1346, -0.0134, -0.1122,  0.1040, -0.0519,  0.3029,  0.1833]],
       dtype=torch.float64)
	q_value: tensor([[0.7451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2686344306478937, distance: 0.978642222082357 entropy -4.622454041706102
epoch: 3, step: 17
	action: tensor([[ 0.0214, -0.0160,  0.0521,  0.0835,  0.0831,  0.2962,  0.1897]],
       dtype=torch.float64)
	q_value: tensor([[0.7335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4518161910994123, distance: 0.8472660320088291 entropy -4.670123159272637
epoch: 3, step: 18
	action: tensor([[ 0.0486, -0.0427, -0.0419,  0.0956,  0.0478,  0.3021,  0.1734]],
       dtype=torch.float64)
	q_value: tensor([[0.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4529025850928504, distance: 0.8464260571580883 entropy -4.600294916087437
epoch: 3, step: 19
	action: tensor([[-0.1054, -0.0958, -0.0618, -0.0755,  0.1277,  0.2911,  0.1712]],
       dtype=torch.float64)
	q_value: tensor([[0.7101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18435652614095654, distance: 1.0334914214545066 entropy -4.630175719326565
epoch: 3, step: 20
	action: tensor([[-0.0781, -0.0032, -0.0448,  0.0166,  0.0812,  0.2981,  0.1905]],
       dtype=torch.float64)
	q_value: tensor([[0.7842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3205955140959559, distance: 0.9432371519748894 entropy -4.562593175930206
epoch: 3, step: 21
	action: tensor([[-0.1148, -0.0472, -0.0508,  0.0591, -0.0770,  0.2983,  0.1853]],
       dtype=torch.float64)
	q_value: tensor([[0.7566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25509469098588367, distance: 0.9876594604375646 entropy -4.585980080577781
epoch: 3, step: 22
	action: tensor([[ 0.0125, -0.0862,  0.0145, -0.1669,  0.0025,  0.2990,  0.1884]],
       dtype=torch.float64)
	q_value: tensor([[0.7279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2609654847986512, distance: 0.9837597601729151 entropy -4.731668809537487
epoch: 3, step: 23
	action: tensor([[-0.0488, -0.0021, -0.0039, -0.0888,  0.0732,  0.3048,  0.1851]],
       dtype=torch.float64)
	q_value: tensor([[0.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30829306819988356, distance: 0.9517387615392172 entropy -4.634228750133274
epoch: 3, step: 24
	action: tensor([[-0.1123, -0.0757, -0.0511,  0.0679, -0.0916,  0.3050,  0.1876]],
       dtype=torch.float64)
	q_value: tensor([[0.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2421678629844798, distance: 0.9961923500769599 entropy -4.569359906961071
epoch: 3, step: 25
	action: tensor([[-0.0443, -0.1329, -0.0485, -0.0979,  0.0006,  0.2974,  0.1883]],
       dtype=torch.float64)
	q_value: tensor([[0.7251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20042428449867744, distance: 1.0232611625060763 entropy -4.725295886023083
epoch: 3, step: 26
	action: tensor([[ 0.1549, -0.0276, -0.0148, -0.0939,  0.0656,  0.3041,  0.1866]],
       dtype=torch.float64)
	q_value: tensor([[0.7527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47648155639118006, distance: 0.8279854146116763 entropy -4.663126647199634
epoch: 3, step: 27
	action: tensor([[ 0.0625, -0.1150, -0.0204,  0.0565, -0.0669,  0.3005,  0.1702]],
       dtype=torch.float64)
	q_value: tensor([[0.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.391740960416748, distance: 0.8924851100511776 entropy -4.571539912046938
epoch: 3, step: 28
	action: tensor([[-0.1557,  0.0858, -0.0763, -0.0719,  0.0819,  0.2929,  0.1728]],
       dtype=torch.float64)
	q_value: tensor([[0.6940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.256356876516161, distance: 0.986822348670557 entropy -4.748444689514293
epoch: 3, step: 29
	action: tensor([[-0.0857, -0.0894,  0.0157,  0.0916, -0.1194,  0.3075,  0.1995]],
       dtype=torch.float64)
	q_value: tensor([[0.7861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28033496828730353, distance: 0.9707823992338606 entropy -4.577413997272197
epoch: 3, step: 30
	action: tensor([[-0.0355, -0.0514, -0.0483,  0.0488,  0.1076,  0.3046,  0.1854]],
       dtype=torch.float64)
	q_value: tensor([[0.7139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3497392421335611, distance: 0.9227848721156955 entropy -4.73959932745845
epoch: 3, step: 31
	action: tensor([[-0.0165, -0.1421,  0.0269, -0.0923, -0.1099,  0.2968,  0.1793]],
       dtype=torch.float64)
	q_value: tensor([[0.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21685521860792556, distance: 1.0126928003529012 entropy -4.575011057530511
epoch: 3, step: 32
	action: tensor([[-0.0653, -0.0304, -0.0047,  0.0973, -0.0837,  0.3038,  0.1856]],
       dtype=torch.float64)
	q_value: tensor([[0.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34354375690944683, distance: 0.9271704564876644 entropy -5.056592075872274
epoch: 3, step: 33
	action: tensor([[-0.0023, -0.0523,  0.0194, -0.0093,  0.0428,  0.3029,  0.1834]],
       dtype=torch.float64)
	q_value: tensor([[0.7097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3555457626239732, distance: 0.918655617515475 entropy -4.703678932781983
epoch: 3, step: 34
	action: tensor([[-0.1113, -0.0165,  0.0089,  0.0466, -0.1417,  0.3031,  0.1791]],
       dtype=torch.float64)
	q_value: tensor([[0.7334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27650840558570455, distance: 0.9733598725465072 entropy -4.638658532121965
epoch: 3, step: 35
	action: tensor([[ 5.4436e-02, -4.0187e-02, -1.6458e-02, -3.0508e-04, -4.1475e-02,
          3.0551e-01,  1.9098e-01]], dtype=torch.float64)
	q_value: tensor([[0.7185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4141548026697117, distance: 0.875887100269496 entropy -4.8098632194763695
epoch: 3, step: 36
	action: tensor([[-0.1266, -0.0058, -0.0104,  0.0754, -0.0641,  0.2975,  0.1758]],
       dtype=torch.float64)
	q_value: tensor([[0.7077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28375876590909044, distance: 0.9684704034131132 entropy -4.694443392093043
epoch: 3, step: 37
	action: tensor([[-0.1210, -0.0246,  0.0647,  0.0294,  0.0492,  0.3039,  0.1894]],
       dtype=torch.float64)
	q_value: tensor([[0.7283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2762004841058775, distance: 0.9735669838184694 entropy -4.703514687792183
epoch: 3, step: 38
	action: tensor([[-0.0757, -0.0144, -0.0041,  0.1181, -0.0589,  0.3077,  0.1884]],
       dtype=torch.float64)
	q_value: tensor([[0.7561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3555527347513101, distance: 0.9186506481934532 entropy -4.6146876912738195
epoch: 3, step: 39
	action: tensor([[-0.0783, -0.1206,  0.0434,  0.0065, -0.0187,  0.3003,  0.1837]],
       dtype=torch.float64)
	q_value: tensor([[0.7139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23245418199704793, distance: 1.0025564794639987 entropy -4.662413126745915
epoch: 3, step: 40
	action: tensor([[ 0.0037, -0.0525, -0.0639,  0.1029, -0.0599,  0.3053,  0.1854]],
       dtype=torch.float64)
	q_value: tensor([[0.7381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39518720604260216, distance: 0.8899532184856783 entropy -4.723657971945999
epoch: 3, step: 41
	action: tensor([[ 0.1690, -0.0237, -0.0004,  0.0631, -0.0103,  0.2899,  0.1769]],
       dtype=torch.float64)
	q_value: tensor([[0.6996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5543285602660749, distance: 0.7639485222727819 entropy -4.68437967828884
epoch: 3, step: 42
	action: tensor([[-0.1373, -0.0054, -0.0128,  0.0228,  0.0241,  0.2962,  0.1636]],
       dtype=torch.float64)
	q_value: tensor([[0.6866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2560120676357033, distance: 0.9870511046857271 entropy -4.673129143582908
epoch: 3, step: 43
	action: tensor([[-0.1550, -0.0009, -0.0292,  0.0485,  0.1214,  0.3027,  0.1912]],
       dtype=torch.float64)
	q_value: tensor([[0.7519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2595966248927557, distance: 0.9846704120274548 entropy -4.637243305223975
epoch: 3, step: 44
	action: tensor([[-0.1222, -0.0197,  0.0133,  0.0330, -0.1886,  0.3046,  0.1911]],
       dtype=torch.float64)
	q_value: tensor([[0.7806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25218968491975835, distance: 0.9895834397827387 entropy -4.563026782296515
epoch: 3, step: 45
	action: tensor([[ 0.0416, -0.0744, -0.0080,  0.0757, -0.1265,  0.3085,  0.1938]],
       dtype=torch.float64)
	q_value: tensor([[0.7189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4078455734659775, distance: 0.8805908800581944 entropy -4.961545146841416
epoch: 3, step: 46
	action: tensor([[-0.1604, -0.1008, -0.0727,  0.0017,  0.1179,  0.2988,  0.1764]],
       dtype=torch.float64)
	q_value: tensor([[0.6876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15659318860369031, distance: 1.0509335224959784 entropy -4.766124634583447
epoch: 3, step: 47
	action: tensor([[-0.0850, -0.0588, -0.0486,  0.1609,  0.0992,  0.3020,  0.1924]],
       dtype=torch.float64)
	q_value: tensor([[0.7867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3414022220087165, distance: 0.9286815634241351 entropy -4.57776291042674
epoch: 3, step: 48
	action: tensor([[-0.1603, -0.0390, -0.0436,  0.0934,  0.0732,  0.2963,  0.1794]],
       dtype=torch.float64)
	q_value: tensor([[0.7487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24028270307588462, distance: 0.9974306291996188 entropy -4.5983163200833
epoch: 3, step: 49
	action: tensor([[-0.1769, -0.0724, -0.0144,  0.0209,  0.0292,  0.3010,  0.1895]],
       dtype=torch.float64)
	q_value: tensor([[0.7649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16590390099438113, distance: 1.0451165817740526 entropy -4.599607652720482
epoch: 3, step: 50
	action: tensor([[-0.0086, -0.1329,  0.0063, -0.0993,  0.0456,  0.2995,  0.1934]],
       dtype=torch.float64)
	q_value: tensor([[0.7691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24146760045975713, distance: 0.9966525015141362 entropy -4.642991230425452
epoch: 3, step: 51
	action: tensor([[-0.0769, -0.0511, -0.0108,  0.0090, -0.1086,  0.3023,  0.1822]],
       dtype=torch.float64)
	q_value: tensor([[0.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27293041605125057, distance: 0.9757637541095083 entropy -4.624844402532631
epoch: 3, step: 52
	action: tensor([[ 0.1606, -0.1431, -0.0562,  0.0692,  0.0632,  0.3016,  0.1878]],
       dtype=torch.float64)
	q_value: tensor([[0.7218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4701099689897539, distance: 0.8330087586424672 entropy -4.801149339815368
epoch: 3, step: 53
	action: tensor([[ 0.0696, -0.0371, -0.0388,  0.0824,  0.0088,  0.2931,  0.1620]],
       dtype=torch.float64)
	q_value: tensor([[0.7069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4657385186463372, distance: 0.8364377502937156 entropy -4.618695138876208
epoch: 3, step: 54
	action: tensor([[ 0.0829, -0.0081, -0.0374,  0.0213, -0.0646,  0.2965,  0.1701]],
       dtype=torch.float64)
	q_value: tensor([[0.6971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4660315789829035, distance: 0.8362083117496183 entropy -4.664418630192159
epoch: 3, step: 55
	action: tensor([[ 0.1805, -0.1220, -0.0263,  0.1826, -0.0220,  0.2968,  0.1738]],
       dtype=torch.float64)
	q_value: tensor([[0.6917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.550287122213956, distance: 0.7674045242788966 entropy -4.717395368596373
epoch: 3, step: 56
	action: tensor([[-0.0765, -0.0783, -0.0321,  0.1630, -0.0435,  0.2906,  0.1595]],
       dtype=torch.float64)
	q_value: tensor([[0.6724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32618850159326584, distance: 0.939346674440997 entropy -4.690395183094172
epoch: 3, step: 57
	action: tensor([[-0.1550, -0.1394, -0.0821,  0.0549, -0.0340,  0.2922,  0.1802]],
       dtype=torch.float64)
	q_value: tensor([[0.7088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1412897413447438, distance: 1.060425149022685 entropy -4.685588407062049
epoch: 3, step: 58
	action: tensor([[-0.0303, -0.0719,  0.0642,  0.1348, -0.0851,  0.2976,  0.1904]],
       dtype=torch.float64)
	q_value: tensor([[0.7489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3791254802510515, distance: 0.9016928206606631 entropy -4.718953206934539
epoch: 3, step: 59
	action: tensor([[-0.0413,  0.0653,  0.0154,  0.0319,  0.0800,  0.3027,  0.1788]],
       dtype=torch.float64)
	q_value: tensor([[0.7018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41575645517759563, distance: 0.874688979229468 entropy -4.697011296055911
epoch: 3, step: 60
	action: tensor([[ 0.0957, -0.1156, -0.0244,  0.0679,  0.0111,  0.3051,  0.1828]],
       dtype=torch.float64)
	q_value: tensor([[0.7435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.434180962542823, distance: 0.8607865472981945 entropy -4.574522657475973
epoch: 3, step: 61
	action: tensor([[-0.1171,  0.0289,  0.0581, -0.0058, -0.1141,  0.2938,  0.1681]],
       dtype=torch.float64)
	q_value: tensor([[0.7038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2812857016756489, distance: 0.970140947847115 entropy -4.683391051562588
epoch: 3, step: 62
	action: tensor([[-0.0819, -0.0555, -0.0077, -0.0662, -0.0562,  0.3080,  0.1934]],
       dtype=torch.float64)
	q_value: tensor([[0.7310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23504401085058524, distance: 1.0008636530851753 entropy -4.7739258544229335
epoch: 3, step: 63
	action: tensor([[-0.1010, -0.0278, -0.0929,  0.0163,  0.0225,  0.3060,  0.1907]],
       dtype=torch.float64)
	q_value: tensor([[0.7459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2714477756563489, distance: 0.9767581362219782 entropy -4.715670559866149
LOSS epoch 3 actor 0.14923086110626796 critic 17.431376842040535
epoch: 4, step: 0
	action: tensor([[-0.0405, -0.0695, -0.0173,  0.2010, -0.1264,  0.4616,  0.3272]],
       dtype=torch.float64)
	q_value: tensor([[1.3070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4180977427356477, distance: 0.8729346130563967 entropy -3.9310985355580805
epoch: 4, step: 1
	action: tensor([[-0.2301,  0.0122,  0.0050, -0.0332,  0.0530,  0.5279,  0.3416]],
       dtype=torch.float64)
	q_value: tensor([[1.3070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21035090359770214, distance: 1.0168895038355308 entropy -3.9001034883901196
epoch: 4, step: 2
	action: tensor([[-0.1007,  0.0972, -0.0948, -0.2177, -0.0105,  0.4734,  0.3735]],
       dtype=torch.float64)
	q_value: tensor([[1.4613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30119540991270455, distance: 0.9566092318530087 entropy -3.817387169823618
epoch: 4, step: 3
	action: tensor([[ 0.3277, -0.2050, -0.0471, -0.0094, -0.0812,  0.4955,  0.3718]],
       dtype=torch.float64)
	q_value: tensor([[1.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5498871045898148, distance: 0.7677457498883445 entropy -3.829514970309329
epoch: 4, step: 4
	action: tensor([[-0.0132, -0.0464,  0.0374,  0.1566, -0.0851,  0.4725,  0.3356]],
       dtype=torch.float64)
	q_value: tensor([[1.2941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4597592214898023, distance: 0.8411053096258103 entropy -3.864777002462417
epoch: 4, step: 5
	action: tensor([[ 0.0135,  0.1312, -0.0083, -0.0627, -0.0649,  0.4043,  0.3418]],
       dtype=torch.float64)
	q_value: tensor([[1.3095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47818337438663605, distance: 0.8266385399251454 entropy -3.8842957947382457
epoch: 4, step: 6
	action: tensor([[-0.0784, -0.0539, -0.0191, -0.0132, -0.0781,  0.5086,  0.3447]],
       dtype=torch.float64)
	q_value: tensor([[1.3205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31303765735594613, distance: 0.9484690389936645 entropy -3.8782836667990908
epoch: 4, step: 7
	action: tensor([[ 0.0978, -0.0334,  0.0410,  0.0869, -0.0455,  0.4950,  0.3594]],
       dtype=torch.float64)
	q_value: tensor([[1.3760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5495496597114191, distance: 0.7680334814065736 entropy -3.8546448856749804
epoch: 4, step: 8
	action: tensor([[-0.0872, -0.0309,  0.0663,  0.2488, -0.2772,  0.4698,  0.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7680334814065736 entropy -3.8698626617471485
epoch: 4, step: 9
	action: tensor([[-0.0296,  0.0075, -0.2359,  0.1663, -0.6281,  0.5730,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2813519831172958, distance: 0.970096212530614 entropy -3.1850759688219275
epoch: 4, step: 10
	action: tensor([[-0.2429, -0.0400,  0.0029, -0.1515,  0.0912,  0.6023,  0.4301]],
       dtype=torch.float64)
	q_value: tensor([[1.6269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0902997495793979, distance: 1.091455039391635 entropy -3.796314367768642
epoch: 4, step: 11
	action: tensor([[-0.0787, -0.0855,  0.0058,  0.0495, -0.0484,  0.5824,  0.3932]],
       dtype=torch.float64)
	q_value: tensor([[1.5582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30966117710598995, distance: 0.9507970861763476 entropy -3.7713316844315594
epoch: 4, step: 12
	action: tensor([[ 0.1175,  0.1482, -0.0495, -0.0165,  0.0318,  0.4889,  0.3659]],
       dtype=torch.float64)
	q_value: tensor([[1.4108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9507970861763476 entropy -3.8301057541621764
epoch: 4, step: 13
	action: tensor([[ 0.2971, -0.3832,  0.0532,  0.3448,  0.0849,  0.8274,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7068018379788156, distance: 0.6196369606266131 entropy -3.1850759688219275
epoch: 4, step: 14
	action: tensor([[-0.2269,  0.0572,  0.1826,  0.2038, -0.1153,  0.6010,  0.4034]],
       dtype=torch.float64)
	q_value: tensor([[1.6824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6196369606266131 entropy -3.705695515220757
epoch: 4, step: 15
	action: tensor([[ 0.7111, -0.2498,  0.1381,  0.1379,  0.1379,  0.5562,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6196369606266131 entropy -3.1850759688219275
epoch: 4, step: 16
	action: tensor([[-0.0580,  0.0634, -0.4293,  0.0054,  0.2444,  0.7895,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40653310791767294, distance: 0.8815662214925539 entropy -3.1850759688219275
epoch: 4, step: 17
	action: tensor([[-0.0661, -0.0481,  0.0796,  0.0219, -0.1775,  0.5187,  0.4357]],
       dtype=torch.float64)
	q_value: tensor([[1.8951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29924112505066447, distance: 0.9579459300763513 entropy -3.706874641728114
epoch: 4, step: 18
	action: tensor([[ 0.1282, -0.1577, -0.1452, -0.0206, -0.3550,  0.6818,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36202294892370657, distance: 0.9140274143370914 entropy -3.1850759688219275
epoch: 4, step: 19
	action: tensor([[-0.2841, -0.0156,  0.0341,  0.1869,  0.0630,  0.6664,  0.4208]],
       dtype=torch.float64)
	q_value: tensor([[1.6282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9140274143370914 entropy -3.772308951971745
epoch: 4, step: 20
	action: tensor([[-0.4275, -0.0267,  0.1872,  0.0327, -0.2576,  0.5132,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12724066627511554, distance: 1.2149684970699015 entropy -3.1850759688219275
epoch: 4, step: 21
	action: tensor([[-0.0619, -0.0031, -0.1183, -0.0221,  0.1084,  0.6313,  0.4355]],
       dtype=torch.float64)
	q_value: tensor([[1.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3425090583647996, distance: 0.9279008663236953 entropy -3.7627480466839747
epoch: 4, step: 22
	action: tensor([[ 0.2337, -0.0427, -0.1512,  0.0244, -0.1306,  0.5218,  0.3746]],
       dtype=torch.float64)
	q_value: tensor([[1.5006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5510632420684751, distance: 0.7667420403790098 entropy -3.800929733909865
epoch: 4, step: 23
	action: tensor([[ 0.2094, -0.0546, -0.0581,  0.2733,  0.0399,  0.4522,  0.3447]],
       dtype=torch.float64)
	q_value: tensor([[1.2912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7667420403790098 entropy -3.876054315996032
epoch: 4, step: 24
	action: tensor([[ 0.2992, -0.2696, -0.0590,  0.1797, -0.1529,  0.7135,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.63008699054112, distance: 0.6959956029557482 entropy -3.1850759688219275
epoch: 4, step: 25
	action: tensor([[ 0.0652, -0.0246, -0.0760, -0.0873, -0.0925,  0.6732,  0.3916]],
       dtype=torch.float64)
	q_value: tensor([[1.5836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46215764992382646, distance: 0.8392361663034689 entropy -3.7692760292163787
epoch: 4, step: 26
	action: tensor([[ 0.0943, -0.0881, -0.0801,  0.2409, -0.1342,  0.5538,  0.3827]],
       dtype=torch.float64)
	q_value: tensor([[1.4151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5429128980065265, distance: 0.7736707476575031 entropy -3.7950870667695678
epoch: 4, step: 27
	action: tensor([[-0.2687, -0.0252, -0.1587,  0.0107, -0.1535,  0.4460,  0.3471]],
       dtype=torch.float64)
	q_value: tensor([[1.3140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06854425318932467, distance: 1.1044290120693683 entropy -3.8820685922088356
epoch: 4, step: 28
	action: tensor([[-1.7940e-01, -3.0189e-02,  3.8022e-04,  3.6024e-02, -1.4871e-01,
          4.3305e-01,  3.7083e-01]], dtype=torch.float64)
	q_value: tensor([[1.4207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20600292685239918, distance: 1.0196852661078728 entropy -3.8744123207683594
epoch: 4, step: 29
	action: tensor([[ 0.0450,  0.0401, -0.0587, -0.1182,  0.1371,  0.4722,  0.3581]],
       dtype=torch.float64)
	q_value: tensor([[1.3732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45919146948280654, distance: 0.8415471624515768 entropy -3.876936930824241
epoch: 4, step: 30
	action: tensor([[-0.1222,  0.1006, -0.0142,  0.2789,  0.1826,  0.4814,  0.3469]],
       dtype=torch.float64)
	q_value: tensor([[1.4167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8415471624515768 entropy -3.842790541619713
epoch: 4, step: 31
	action: tensor([[ 0.5880, -0.0950,  0.2076,  0.2850, -0.0466,  0.7234,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8415471624515768 entropy -3.1850759688219275
epoch: 4, step: 32
	action: tensor([[-0.4225, -0.2412, -0.3156,  0.1898,  0.2221,  0.7490,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17543071559318135, distance: 1.2406669432252582 entropy -3.1850759688219275
epoch: 4, step: 33
	action: tensor([[ 0.0052, -0.0351, -0.0161, -0.1405,  0.1321,  0.5797,  0.4516]],
       dtype=torch.float64)
	q_value: tensor([[2.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3254647222701691, distance: 0.939851041927599 entropy -3.6804511163127946
epoch: 4, step: 34
	action: tensor([[-0.1470, -0.2036,  0.1108, -0.0633, -0.0491,  0.4484,  0.3668]],
       dtype=torch.float64)
	q_value: tensor([[1.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015664462195104534, distance: 1.1353461084832297 entropy -3.7987736408669157
epoch: 4, step: 35
	action: tensor([[ 0.1173, -0.0294, -0.0336, -0.0199,  0.0163,  0.5281,  0.3589]],
       dtype=torch.float64)
	q_value: tensor([[1.4178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45944357386389223, distance: 0.8413509909367959 entropy -3.850984948720277
epoch: 4, step: 36
	action: tensor([[-0.0442, -0.1757, -0.0323, -0.1063, -0.0329,  0.4419,  0.3469]],
       dtype=torch.float64)
	q_value: tensor([[1.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11781042178310519, distance: 1.0748247409639478 entropy -3.851251101383068
epoch: 4, step: 37
	action: tensor([[ 0.0276,  0.0374, -0.0493,  0.0286,  0.0881,  0.4715,  0.3509]],
       dtype=torch.float64)
	q_value: tensor([[1.3905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42502263870016366, distance: 0.8677249119335548 entropy -3.8706090743256687
epoch: 4, step: 38
	action: tensor([[ 1.4607e-01,  4.4321e-04,  3.6820e-03,  1.7451e-01, -1.6252e-01,
          5.3806e-01,  3.3992e-01]], dtype=torch.float64)
	q_value: tensor([[1.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8677249119335548 entropy -3.8664160304808632
epoch: 4, step: 39
	action: tensor([[-0.2622, -0.0417,  0.1949,  0.2059, -0.0092,  0.7551,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954963373783668, distance: 0.9605021048527429 entropy -3.1850759688219275
epoch: 4, step: 40
	action: tensor([[ 0.0095,  0.0595,  0.0693,  0.0616, -0.0998,  0.5568,  0.4362]],
       dtype=torch.float64)
	q_value: tensor([[1.7961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9605021048527429 entropy -3.7039165346712015
epoch: 4, step: 41
	action: tensor([[ 6.6117e-04, -8.5924e-02,  5.8716e-02, -2.1368e-01, -2.7800e-02,
          5.5769e-01,  9.1672e-01]], dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2712418723541302, distance: 0.9768961520779738 entropy -3.1850759688219275
epoch: 4, step: 42
	action: tensor([[ 0.1239,  0.0421, -0.0185, -0.0683, -0.1164,  0.6274,  0.4068]],
       dtype=torch.float64)
	q_value: tensor([[1.6992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5357598280940923, distance: 0.7797009304032018 entropy -3.744386964678965
epoch: 4, step: 43
	action: tensor([[-0.2861,  0.1379, -0.1364,  0.0305, -0.1849,  0.4930,  0.3738]],
       dtype=torch.float64)
	q_value: tensor([[1.3700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7797009304032018 entropy -3.8087405030970243
epoch: 4, step: 44
	action: tensor([[-0.1951, -0.2158, -0.1458,  0.3799, -0.3367,  0.6024,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10517923829837261, distance: 1.0824920594746685 entropy -3.1850759688219275
epoch: 4, step: 45
	action: tensor([[ 0.2356, -0.0149, -0.0871,  0.0085,  0.0467,  0.5702,  0.4146]],
       dtype=torch.float64)
	q_value: tensor([[1.7356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6066517408457923, distance: 0.7177039054544855 entropy -3.804328150298199
epoch: 4, step: 46
	action: tensor([[-0.2206, -0.1115,  0.1171,  0.2087,  0.0975,  0.4056,  0.3480]],
       dtype=torch.float64)
	q_value: tensor([[1.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18287284656603287, distance: 1.0344309700956122 entropy -3.8365778180940104
epoch: 4, step: 47
	action: tensor([[0.3417, 0.0115, 0.0442, 0.0449, 0.0731, 0.5168, 0.3430]],
       dtype=torch.float64)
	q_value: tensor([[1.4225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7170811657806555, distance: 0.608678025610719 entropy -3.876464126657713
epoch: 4, step: 48
	action: tensor([[ 0.0411, -0.1047,  0.0891, -0.1695,  0.0580,  0.4572,  0.3326]],
       dtype=torch.float64)
	q_value: tensor([[1.3110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27110471121494484, distance: 0.9769880796187143 entropy -3.8484836332264343
epoch: 4, step: 49
	action: tensor([[ 0.1772, -0.0796,  0.0279,  0.0052,  0.2026,  0.4511,  0.3479]],
       dtype=torch.float64)
	q_value: tensor([[1.3914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5128065712995943, distance: 0.7987436236246864 entropy -3.844150385706591
epoch: 4, step: 50
	action: tensor([[ 0.1109, -0.1695,  0.0617, -0.0300,  0.1055,  0.4980,  0.3267]],
       dtype=torch.float64)
	q_value: tensor([[1.3804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3819977708649156, distance: 0.8996046965156439 entropy -3.862782316419325
epoch: 4, step: 51
	action: tensor([[-0.3679, -0.0270, -0.0183,  0.0420, -0.0571,  0.5177,  0.3399]],
       dtype=torch.float64)
	q_value: tensor([[1.3797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0451157520039589, distance: 1.1698734638363437 entropy -3.8497746826470793
epoch: 4, step: 52
	action: tensor([[-0.1676,  0.1089, -0.0539, -0.0739,  0.1218,  0.4944,  0.3856]],
       dtype=torch.float64)
	q_value: tensor([[1.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25054197812378687, distance: 0.9906730517735365 entropy -3.822282926118624
epoch: 4, step: 53
	action: tensor([[0.0270, 0.0846, 0.1820, 0.0495, 0.1388, 0.4349, 0.3671]],
       dtype=torch.float64)
	q_value: tensor([[1.4896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9906730517735365 entropy -3.82170195025642
epoch: 4, step: 54
	action: tensor([[ 0.4040, -0.0904, -0.1033, -0.3115, -0.3126,  0.7668,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5468093231118618, distance: 0.7703661236587311 entropy -3.1850759688219275
epoch: 4, step: 55
	action: tensor([[-0.0836, -0.1085,  0.1955,  0.1612, -0.1144,  0.6482,  0.4369]],
       dtype=torch.float64)
	q_value: tensor([[1.6254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42016575242387955, distance: 0.8713820808492406 entropy -3.6987253533984092
epoch: 4, step: 56
	action: tensor([[-0.0440, -0.0471, -0.2051,  0.2173, -0.0142,  0.6077,  0.3787]],
       dtype=torch.float64)
	q_value: tensor([[1.4166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8713820808492406 entropy -3.799737198995562
epoch: 4, step: 57
	action: tensor([[-0.0340, -0.2139,  0.0468,  0.1535, -0.0501,  0.7771,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38404333336330954, distance: 0.8981146348183705 entropy -3.1850759688219275
epoch: 4, step: 58
	action: tensor([[ 0.2017, -0.0033, -0.1772, -0.0740, -0.0295,  0.5998,  0.4206]],
       dtype=torch.float64)
	q_value: tensor([[1.7440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5557655585617198, distance: 0.762715911237455 entropy -3.727251487944269
epoch: 4, step: 59
	action: tensor([[-0.0837,  0.0526,  0.0165,  0.1305,  0.2454,  0.5723,  0.3636]],
       dtype=torch.float64)
	q_value: tensor([[1.3956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.762715911237455 entropy -3.830734163420728
epoch: 4, step: 60
	action: tensor([[ 0.3262, -0.2592,  0.2380, -0.1990,  0.0727,  0.6886,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4628494840885271, distance: 0.8386962319857051 entropy -3.1850759688219275
epoch: 4, step: 61
	action: tensor([[-0.1035, -0.0266, -0.0708, -0.0696, -0.0486,  0.5999,  0.4058]],
       dtype=torch.float64)
	q_value: tensor([[1.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28514708177297343, distance: 0.9675313378184847 entropy -3.6864524729795045
epoch: 4, step: 62
	action: tensor([[ 0.1078,  0.1683, -0.0453, -0.1178, -0.1174,  0.6059,  0.3801]],
       dtype=torch.float64)
	q_value: tensor([[1.4506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9675313378184847 entropy -3.812627727745435
epoch: 4, step: 63
	action: tensor([[ 0.1096, -0.1814, -0.2420, -0.1467,  0.2377,  0.9109,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4508158394025208, distance: 0.8480387451780071 entropy -3.1850759688219275
LOSS epoch 4 actor 334.8392465284777 critic 2187.052718784591
epoch: 5, step: 0
	action: tensor([[-0.2572, -0.2265,  0.0476,  0.2083, -0.2539,  0.6514,  0.5789]],
       dtype=torch.float64)
	q_value: tensor([[1.2020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10412870680918662, distance: 1.0831273031290822 entropy -3.3374723048284447
epoch: 5, step: 1
	action: tensor([[ 0.2184, -0.1708, -0.1325,  0.1546,  0.1029,  0.4748,  0.5303]],
       dtype=torch.float64)
	q_value: tensor([[0.9926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5563174203638579, distance: 0.7622420121629407 entropy -3.440485519288892
epoch: 5, step: 2
	action: tensor([[ 0.0224, -0.0389, -0.2752,  0.1214, -0.1807,  0.6015,  0.4618]],
       dtype=torch.float64)
	q_value: tensor([[0.9087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3942218500769239, distance: 0.8906631729661835 entropy -3.520670709975565
epoch: 5, step: 3
	action: tensor([[ 0.2458, -0.0041, -0.2054, -0.1654,  0.2054,  0.5097,  0.4816]],
       dtype=torch.float64)
	q_value: tensor([[0.8970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5799265084747537, distance: 0.7416847194042147 entropy -3.5188750578825294
epoch: 5, step: 4
	action: tensor([[ 0.1490, -0.0098, -0.0315,  0.0971,  0.0460,  0.5685,  0.4584]],
       dtype=torch.float64)
	q_value: tensor([[0.9565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6067940951508843, distance: 0.7175740237481457 entropy -3.506182686607986
epoch: 5, step: 5
	action: tensor([[ 0.0445,  0.0080, -0.0576,  0.2887,  0.1527,  0.6760,  0.4729]],
       dtype=torch.float64)
	q_value: tensor([[0.8973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7175740237481457 entropy -3.493095811850043
epoch: 5, step: 6
	action: tensor([[ 0.3783, -0.0532, -0.3715,  0.1912,  0.0943,  0.7300,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7379997282697908, distance: 0.5857436162800465 entropy -3.252776988696606
epoch: 5, step: 7
	action: tensor([[ 0.2024, -0.1687, -0.0574,  0.1851, -0.5518,  0.5817,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46332993862949556, distance: 0.8383210619472817 entropy -3.252776988696606
epoch: 5, step: 8
	action: tensor([[ 0.1081, -0.0443,  0.0924, -0.0849,  0.2765,  0.5345,  0.5320]],
       dtype=torch.float64)
	q_value: tensor([[0.8034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4961254791714951, distance: 0.812302691710509 entropy -3.4660587116113803
epoch: 5, step: 9
	action: tensor([[-0.4451, -0.0417,  0.0172,  0.1791,  0.0062,  0.5767,  0.4734]],
       dtype=torch.float64)
	q_value: tensor([[0.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0015640404185475187, distance: 1.145238804691295 entropy -3.459077452204691
epoch: 5, step: 10
	action: tensor([[ 0.0261, -0.0992, -0.0316, -0.1002, -0.3237,  0.6226,  0.4847]],
       dtype=torch.float64)
	q_value: tensor([[1.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013953447671618, distance: 0.95647237441638 entropy -3.476590154098704
epoch: 5, step: 11
	action: tensor([[-0.3551,  0.0785, -0.0916,  0.3712, -0.1171,  0.4651,  0.5153]],
       dtype=torch.float64)
	q_value: tensor([[0.8842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.95647237441638 entropy -3.4589010770220696
epoch: 5, step: 12
	action: tensor([[ 0.2244, -0.2437,  0.1012,  0.1860,  0.0464,  0.7706,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6518571146315765, distance: 0.6752046980917961 entropy -3.252776988696606
epoch: 5, step: 13
	action: tensor([[-0.2318, -0.2780, -0.2201, -0.3271, -0.1606,  0.8656,  0.5370]],
       dtype=torch.float64)
	q_value: tensor([[0.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1077793485822145, distance: 1.2044348843858514 entropy -3.3911221115524874
epoch: 5, step: 14
	action: tensor([[ 0.1410, -0.3650,  0.1013, -0.0251,  0.0021,  0.7857,  0.5574]],
       dtype=torch.float64)
	q_value: tensor([[1.1132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39524480972690645, distance: 0.8899108369376595 entropy -3.3922396595197055
epoch: 5, step: 15
	action: tensor([[ 0.6870, -0.1000, -0.3756,  0.0413, -0.1870,  0.7143,  0.5499]],
       dtype=torch.float64)
	q_value: tensor([[0.9832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8052277886681729, distance: 0.505033504629506 entropy -3.3741515535222786
epoch: 5, step: 16
	action: tensor([[-0.2515, -0.0410,  0.0329, -0.0048,  0.4403,  0.6877,  0.5389]],
       dtype=torch.float64)
	q_value: tensor([[0.8736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2749360111275462, distance: 0.9744170202768125 entropy -3.4220323750211032
epoch: 5, step: 17
	action: tensor([[-0.1724,  0.0905,  0.2070, -0.0689, -0.2537,  0.6513,  0.4948]],
       dtype=torch.float64)
	q_value: tensor([[1.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9744170202768125 entropy -3.4125409596200025
epoch: 5, step: 18
	action: tensor([[-0.1535, -0.2462,  0.1643,  0.2893,  0.2915,  0.8840,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42773104793338756, distance: 0.8656788067844782 entropy -3.252776988696606
epoch: 5, step: 19
	action: tensor([[ 0.0850, -0.4090, -0.2741,  0.1424,  0.1189,  0.8740,  0.5445]],
       dtype=torch.float64)
	q_value: tensor([[1.0805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3211259685449469, distance: 0.9428688573470664 entropy -3.3592671728471672
epoch: 5, step: 20
	action: tensor([[ 0.0882, -0.2315, -0.1708,  0.0691,  0.2997,  0.6051,  0.5468]],
       dtype=torch.float64)
	q_value: tensor([[1.0453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39602661938263206, distance: 0.8893354256803209 entropy -3.3999671862940732
epoch: 5, step: 21
	action: tensor([[ 0.2319, -0.1968,  0.0863, -0.2237, -0.1311,  0.5138,  0.4785]],
       dtype=torch.float64)
	q_value: tensor([[1.0266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3396521509081567, distance: 0.9299146229370624 entropy -3.4754752119665
epoch: 5, step: 22
	action: tensor([[ 0.1905, -0.2667, -0.2188,  0.2739,  0.1134,  0.6892,  0.4973]],
       dtype=torch.float64)
	q_value: tensor([[0.8929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5267620855639361, distance: 0.7872206162227712 entropy -3.458707406756711
epoch: 5, step: 23
	action: tensor([[ 0.0856, -0.2046, -0.0242, -0.0616, -0.1333,  0.6547,  0.5022]],
       dtype=torch.float64)
	q_value: tensor([[0.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32310572322068853, distance: 0.9414930410497032 entropy -3.4648352833059115
epoch: 5, step: 24
	action: tensor([[ 0.0160,  0.0299, -0.1426,  0.1136,  0.0648,  0.7144,  0.5126]],
       dtype=torch.float64)
	q_value: tensor([[0.9159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9414930410497032 entropy -3.448283866430996
epoch: 5, step: 25
	action: tensor([[ 0.2404, -0.0671, -0.0456,  0.0669,  0.6418,  0.9359,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7827897716407834, distance: 0.5333310018034794 entropy -3.252776988696606
epoch: 5, step: 26
	action: tensor([[ 0.3403, -0.1075, -0.1896,  0.1619,  0.4131,  0.9409,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7873077482788381, distance: 0.5277552081006431 entropy -3.252776988696606
epoch: 5, step: 27
	action: tensor([[ 2.7584e-01,  1.8394e-01, -2.6123e-02,  2.1046e-04, -2.1479e-01,
          6.4208e-01,  5.4670e-01]], dtype=torch.float64)
	q_value: tensor([[1.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5277552081006431 entropy -3.3683915735357384
epoch: 5, step: 28
	action: tensor([[-0.0552, -0.1070,  0.0451, -0.0960,  0.2050,  0.6624,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32332596590840523, distance: 0.9413398606825902 entropy -3.252776988696606
epoch: 5, step: 29
	action: tensor([[-0.1188, -0.0565, -0.2005, -0.0220, -0.1016,  0.6762,  0.4987]],
       dtype=torch.float64)
	q_value: tensor([[1.0327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22583440083093276, distance: 1.0068705255967527 entropy -3.42688572604227
epoch: 5, step: 30
	action: tensor([[ 0.1476, -0.3755,  0.0181,  0.2087,  0.0347,  0.5377,  0.4966]],
       dtype=torch.float64)
	q_value: tensor([[0.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4099006759873486, distance: 0.8790614838773939 entropy -3.478701961728024
epoch: 5, step: 31
	action: tensor([[-0.3164,  0.0652,  0.0086,  0.0443, -0.0459,  0.6059,  0.4910]],
       dtype=torch.float64)
	q_value: tensor([[0.9070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8790614838773939 entropy -3.478175179612637
epoch: 5, step: 32
	action: tensor([[ 0.2636, -0.0062,  0.0914,  0.1239, -0.3354,  0.6680,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7157539830755655, distance: 0.6101040207727025 entropy -3.252776988696606
epoch: 5, step: 33
	action: tensor([[-0.1710, -0.1732, -0.0599,  0.0645,  0.1409,  0.7640,  0.5357]],
       dtype=torch.float64)
	q_value: tensor([[0.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6101040207727025 entropy -3.427536438255863
epoch: 5, step: 34
	action: tensor([[ 0.2312, -0.2207,  0.1135,  0.0614,  0.2029,  0.8176,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6655831517986128, distance: 0.661760388935657 entropy -3.252776988696606
epoch: 5, step: 35
	action: tensor([[ 0.0173,  0.1738, -0.1668, -0.2396, -0.3231,  0.6751,  0.5397]],
       dtype=torch.float64)
	q_value: tensor([[0.9982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4320616422858091, distance: 0.8623971128580342 entropy -3.3671235704007176
epoch: 5, step: 36
	action: tensor([[ 0.0617, -0.2500, -0.1012,  0.1064,  0.1388,  0.5737,  0.5187]],
       dtype=torch.float64)
	q_value: tensor([[0.9247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4013842448205477, distance: 0.8853821556251185 entropy -3.448089200835152
epoch: 5, step: 37
	action: tensor([[ 0.1191, -0.0369, -0.0403, -0.0174, -0.2046,  0.6636,  0.4784]],
       dtype=torch.float64)
	q_value: tensor([[0.9641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5181481106742005, distance: 0.7943528834798643 entropy -3.4876696955139614
epoch: 5, step: 38
	action: tensor([[-0.0820, -0.0863, -0.2616,  0.1336,  0.0051,  0.7802,  0.5109]],
       dtype=torch.float64)
	q_value: tensor([[0.8748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35961633451187613, distance: 0.9157497654606673 entropy -3.455468279390459
epoch: 5, step: 39
	action: tensor([[-0.0667, -0.2682, -0.0370,  0.2464, -0.2006,  0.7497,  0.5116]],
       dtype=torch.float64)
	q_value: tensor([[1.0043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33229741157930137, distance: 0.9350788265904102 entropy -3.4543400759992315
epoch: 5, step: 40
	action: tensor([[-0.3287, -0.2624, -0.0875,  0.1668,  0.0485,  0.5958,  0.5362]],
       dtype=torch.float64)
	q_value: tensor([[0.9451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0001864965376014993, distance: 1.1442375409037646 entropy -3.4319136337226466
epoch: 5, step: 41
	action: tensor([[-0.3639,  0.1286,  0.0474,  0.1024, -0.2102,  0.6800,  0.4889]],
       dtype=torch.float64)
	q_value: tensor([[1.0496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1442375409037646 entropy -3.475330427732118
epoch: 5, step: 42
	action: tensor([[-0.1596, -0.1500, -0.3299,  0.1337, -0.2237,  0.6740,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07771486366495894, distance: 1.0989787578848 entropy -3.252776988696606
epoch: 5, step: 43
	action: tensor([[ 0.0255, -0.1252, -0.0173,  0.0203, -0.1903,  0.6370,  0.5064]],
       dtype=torch.float64)
	q_value: tensor([[0.9896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3434096942605326, distance: 0.9272651258454558 entropy -3.488755715355599
epoch: 5, step: 44
	action: tensor([[-0.3606, -0.1983,  0.1164,  0.2663,  0.0894,  0.6714,  0.5083]],
       dtype=torch.float64)
	q_value: tensor([[0.8993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09920595244494546, distance: 1.086099083254244 entropy -3.462512491485218
epoch: 5, step: 45
	action: tensor([[ 0.3372,  0.0293, -0.0088, -0.1703,  0.0972,  0.6045,  0.5100]],
       dtype=torch.float64)
	q_value: tensor([[1.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6616403610039219, distance: 0.6656500511805082 entropy -3.431693419508847
epoch: 5, step: 46
	action: tensor([[-0.0930, -0.1594,  0.0531, -0.2378,  0.0604,  0.6223,  0.4941]],
       dtype=torch.float64)
	q_value: tensor([[0.9329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11287151700594134, distance: 1.0778292244135397 entropy -3.447307740196931
epoch: 5, step: 47
	action: tensor([[-0.3690,  0.0596, -0.0132, -0.1316, -0.0354,  0.6165,  0.4995]],
       dtype=torch.float64)
	q_value: tensor([[1.0063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031034144005246622, distance: 1.1619654560144608 entropy -3.4407901976325985
epoch: 5, step: 48
	action: tensor([[-0.0942, -0.0255, -0.1752,  0.0510, -0.1743,  0.6179,  0.4942]],
       dtype=torch.float64)
	q_value: tensor([[1.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.252439565307638, distance: 0.9894180915994191 entropy -3.4570242279115484
epoch: 5, step: 49
	action: tensor([[-0.1749, -0.3649, -0.1043,  0.3313,  0.0784,  0.6906,  0.4900]],
       dtype=torch.float64)
	q_value: tensor([[0.9362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16059284754234382, distance: 1.048438658313921 entropy -3.496536172080662
epoch: 5, step: 50
	action: tensor([[-0.0937, -0.1192, -0.0017,  0.1857,  0.0159,  0.6271,  0.5055]],
       dtype=torch.float64)
	q_value: tensor([[1.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3381879908069354, distance: 0.9309449814088094 entropy -3.4543875710169822
epoch: 5, step: 51
	action: tensor([[-0.3814, -0.4136, -0.0365, -0.0465, -0.4406,  0.6148,  0.4905]],
       dtype=torch.float64)
	q_value: tensor([[0.9497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44364714505540537, distance: 1.374951000706284 entropy -3.471926699359711
epoch: 5, step: 52
	action: tensor([[-0.1215, -0.1223,  0.1979, -0.0052, -0.5279,  0.6909,  0.5449]],
       dtype=torch.float64)
	q_value: tensor([[1.0320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15863252886442536, distance: 1.0496621857320876 entropy -3.431402829998762
epoch: 5, step: 53
	action: tensor([[-0.3074,  0.2508, -0.2125,  0.0850, -0.2768,  0.6811,  0.5682]],
       dtype=torch.float64)
	q_value: tensor([[0.9267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0496621857320876 entropy -3.397502934488684
epoch: 5, step: 54
	action: tensor([[ 0.1179,  0.0009,  0.3754, -0.2080, -0.0790,  0.7802,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5310049746266856, distance: 0.7836836954524198 entropy -3.252776988696606
epoch: 5, step: 55
	action: tensor([[-0.0579, -0.2395, -0.2623, -0.3486, -0.1771,  0.7852,  0.5612]],
       dtype=torch.float64)
	q_value: tensor([[0.9729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056658779879861565, distance: 1.1114529916000617 entropy -3.335542773990805
epoch: 5, step: 56
	action: tensor([[ 0.0072, -0.0836, -0.0170,  0.1545, -0.1275,  0.5727,  0.5419]],
       dtype=torch.float64)
	q_value: tensor([[1.0583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4368944779544267, distance: 0.8587200166953594 entropy -3.4185694368688715
epoch: 5, step: 57
	action: tensor([[ 0.3304,  0.0660, -0.0202, -0.1581, -0.1611,  0.5694,  0.4913]],
       dtype=torch.float64)
	q_value: tensor([[0.9024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6661161712359365, distance: 0.6612327961772151 entropy -3.486479856696974
epoch: 5, step: 58
	action: tensor([[ 0.1242, -0.2292, -0.1564,  0.0221, -0.1252,  0.6321,  0.4973]],
       dtype=torch.float64)
	q_value: tensor([[0.8534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38029310618316914, distance: 0.9008445531369244 entropy -3.462866935123661
epoch: 5, step: 59
	action: tensor([[ 0.2185, -0.0840, -0.3499, -0.1252,  0.2430,  0.7769,  0.5016]],
       dtype=torch.float64)
	q_value: tensor([[0.9027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5803849485844683, distance: 0.7412798963793323 entropy -3.478221886873859
epoch: 5, step: 60
	action: tensor([[ 0.2295, -0.4009, -0.0131,  0.0743,  0.0139,  0.5249,  0.5101]],
       dtype=torch.float64)
	q_value: tensor([[1.0418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3734610607652913, distance: 0.9057966851342832 entropy -3.438854637879188
epoch: 5, step: 61
	action: tensor([[ 0.0179, -0.0262, -0.0521, -0.1356, -0.0591,  0.5668,  0.4953]],
       dtype=torch.float64)
	q_value: tensor([[0.9115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36471794068065544, distance: 0.9120948187494484 entropy -3.4725826837257623
epoch: 5, step: 62
	action: tensor([[-0.3302,  0.1521, -0.0051,  0.4044, -0.1561,  0.3278,  0.4820]],
       dtype=torch.float64)
	q_value: tensor([[0.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9120948187494484 entropy -3.4842674439675916
epoch: 5, step: 63
	action: tensor([[ 0.0301,  0.1840,  0.2513,  0.3536, -0.0784,  1.0141,  0.5196]],
       dtype=torch.float64)
	q_value: tensor([[1.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7609592238863149, distance: 0.5594904634733284 entropy -3.252776988696606
LOSS epoch 5 actor 245.02777393388254 critic 1952.2775885785431
epoch: 6, step: 0
	action: tensor([[-0.3075, -0.1629,  0.0736, -0.0325,  0.0068,  0.6235,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04200278384529721, distance: 1.1681298795871322 entropy -3.5360962747529094
epoch: 6, step: 1
	action: tensor([[-0.4320, -0.0649,  0.0876,  0.3728,  0.0577,  0.7494,  0.2858]],
       dtype=torch.float64)
	q_value: tensor([[0.3895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1653611471351415, distance: 1.0454565598722483 entropy -3.500056946290374
epoch: 6, step: 2
	action: tensor([[-0.1131, -0.1035, -0.1231, -0.2357, -0.0820,  0.5422,  0.2909]],
       dtype=torch.float64)
	q_value: tensor([[0.4025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07521996220384242, distance: 1.1004641943531404 entropy -3.444537939770611
epoch: 6, step: 3
	action: tensor([[ 0.2867, -0.2113,  0.0235, -0.0873, -0.0298,  0.6421,  0.2731]],
       dtype=torch.float64)
	q_value: tensor([[0.3862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4928364167679894, distance: 0.8149495495820654 entropy -3.5183669745762947
epoch: 6, step: 4
	action: tensor([[-0.4123,  0.0271, -0.0151, -0.1457,  0.2475,  0.4797,  0.3219]],
       dtype=torch.float64)
	q_value: tensor([[0.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11823497570577857, distance: 1.2101054842415637 entropy -3.4333902351690364
epoch: 6, step: 5
	action: tensor([[ 0.4614, -0.2288, -0.0569, -0.1110,  0.0059,  0.5570,  0.2353]],
       dtype=torch.float64)
	q_value: tensor([[0.4553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5511101794039095, distance: 0.7667019570393944 entropy -3.5336828129376867
epoch: 6, step: 6
	action: tensor([[ 0.4163, -0.3047, -0.0387,  0.0018,  0.0580,  0.6300,  0.3140]],
       dtype=torch.float64)
	q_value: tensor([[0.3552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5524968987762062, distance: 0.7655167855412378 entropy -3.45740631860415
epoch: 6, step: 7
	action: tensor([[ 0.1308,  0.0591, -0.1787, -0.0608,  0.1546,  0.6776,  0.3224]],
       dtype=torch.float64)
	q_value: tensor([[0.3516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5648506499413319, distance: 0.754876425731689 entropy -3.4290084165583146
epoch: 6, step: 8
	action: tensor([[0.3245, 0.1157, 0.0295, 0.1525, 0.0550, 0.6971, 0.2767]],
       dtype=torch.float64)
	q_value: tensor([[0.3890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.754876425731689 entropy -3.48026127105902
epoch: 6, step: 9
	action: tensor([[-0.1611,  0.0318, -0.0270, -0.1237,  0.1861,  0.4658,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19436252161390888, distance: 1.0271326249755066 entropy -3.5360962747529094
epoch: 6, step: 10
	action: tensor([[ 0.0611, -0.1319, -0.0278,  0.0287, -0.0697,  0.6147,  0.2443]],
       dtype=torch.float64)
	q_value: tensor([[0.4056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.393796499665211, distance: 0.8909758100833122 entropy -3.5640860204331646
epoch: 6, step: 11
	action: tensor([[-0.3673,  0.1640, -0.0316, -0.3885, -0.1512,  0.6054,  0.3009]],
       dtype=torch.float64)
	q_value: tensor([[0.3421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09506197644394021, distance: 1.19750143615672 entropy -3.4795567882811294
epoch: 6, step: 12
	action: tensor([[ 0.0499, -0.1972,  0.1621, -0.1264, -0.1392,  0.6694,  0.2644]],
       dtype=torch.float64)
	q_value: tensor([[0.4243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28768342035020833, distance: 0.9658133846664956 entropy -3.478192730305506
epoch: 6, step: 13
	action: tensor([[ 0.2935, -0.1177, -0.0566,  0.0089,  0.3275,  0.4404,  0.3314]],
       dtype=torch.float64)
	q_value: tensor([[0.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5805520654139718, distance: 0.7411322697840609 entropy -3.4112917049906457
epoch: 6, step: 14
	action: tensor([[ 0.0607, -0.0174, -0.0354, -0.1699, -0.2597,  0.5402,  0.2707]],
       dtype=torch.float64)
	q_value: tensor([[0.3985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33068070235237235, distance: 0.9362101960042032 entropy -3.515878439870955
epoch: 6, step: 15
	action: tensor([[ 0.3838, -0.2177,  0.0243, -0.1552, -0.0397,  0.5208,  0.2986]],
       dtype=torch.float64)
	q_value: tensor([[0.3416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.481308322346099, distance: 0.8241596217065298 entropy -3.4849058625687297
epoch: 6, step: 16
	action: tensor([[-6.6912e-02,  6.8455e-02,  4.7464e-03, -7.1821e-02, -5.1943e-04,
          5.5217e-01,  3.1536e-01]], dtype=torch.float64)
	q_value: tensor([[0.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34537465627563857, distance: 0.9258765841838713 entropy -3.455596348232585
epoch: 6, step: 17
	action: tensor([[ 0.4571, -0.1589, -0.0383,  0.4019, -0.3755,  0.6874,  0.2754]],
       dtype=torch.float64)
	q_value: tensor([[0.3745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9258765841838713 entropy -3.4947793750432696
epoch: 6, step: 18
	action: tensor([[ 0.4537, -0.0964,  0.0442,  0.1992, -0.0811,  0.5743,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7726840364448863, distance: 0.5455966073518601 entropy -3.5360962747529094
epoch: 6, step: 19
	action: tensor([[-0.2739,  0.0174, -0.1557, -0.3488,  0.0597,  0.6967,  0.3153]],
       dtype=torch.float64)
	q_value: tensor([[0.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09099260472535076, distance: 1.0910393176826632 entropy -3.464779538339065
epoch: 6, step: 20
	action: tensor([[-0.4474,  0.1955,  0.0961,  0.4302, -0.2657,  0.4712,  0.2568]],
       dtype=torch.float64)
	q_value: tensor([[0.4375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0910393176826632 entropy -3.484209680000269
epoch: 6, step: 21
	action: tensor([[ 0.1342, -0.2156, -0.0737, -0.1007,  0.0069,  0.5346,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36097766612109383, distance: 0.9147758943248783 entropy -3.5360962747529094
epoch: 6, step: 22
	action: tensor([[-0.0781, -0.0915,  0.0212,  0.1391,  0.1171,  0.5517,  0.2902]],
       dtype=torch.float64)
	q_value: tensor([[0.3567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38303946021323665, distance: 0.8988462009869594 entropy -3.519610970774539
epoch: 6, step: 23
	action: tensor([[-0.0154, -0.1261, -0.1938, -0.3425,  0.1968,  0.6112,  0.2764]],
       dtype=torch.float64)
	q_value: tensor([[0.3745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22660391522501633, distance: 1.0063699905719525 entropy -3.5053610521646825
epoch: 6, step: 24
	action: tensor([[-0.3058,  0.2185, -0.0126, -0.0726, -0.0762,  0.6199,  0.2603]],
       dtype=torch.float64)
	q_value: tensor([[0.4286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0063699905719525 entropy -3.511417190774449
epoch: 6, step: 25
	action: tensor([[-0.0294, -0.0798,  0.1085,  0.0684,  0.0667,  0.4977,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0063699905719525 entropy -3.5360962747529094
epoch: 6, step: 26
	action: tensor([[-0.0714, -0.2210,  0.2428,  0.0360,  0.0878,  0.5621,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.262807105219658, distance: 0.9825332666204148 entropy -3.5360962747529094
epoch: 6, step: 27
	action: tensor([[-0.4088, -0.1070,  0.2003,  0.0973, -0.1397,  0.5990,  0.3052]],
       dtype=torch.float64)
	q_value: tensor([[0.3786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031312251076906605, distance: 1.1621221574310938 entropy -3.4723849630412085
epoch: 6, step: 28
	action: tensor([[-0.0309, -0.1631,  0.1099, -0.1873, -0.0740,  0.6692,  0.3027]],
       dtype=torch.float64)
	q_value: tensor([[0.3904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21763322363272464, distance: 1.0121896520969544 entropy -3.4464239656386395
epoch: 6, step: 29
	action: tensor([[ 0.2809, -0.2150,  0.1068, -0.0538, -0.1430,  0.6405,  0.3170]],
       dtype=torch.float64)
	q_value: tensor([[0.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5107633920302346, distance: 0.8004167465165193 entropy -3.4238850824337788
epoch: 6, step: 30
	action: tensor([[ 0.4448, -0.1286, -0.1489,  0.2241, -0.0725,  0.5919,  0.3376]],
       dtype=torch.float64)
	q_value: tensor([[0.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7522979982781677, distance: 0.5695363548915936 entropy -3.4105481073410258
epoch: 6, step: 31
	action: tensor([[-0.1698, -0.3056,  0.0561,  0.1563, -0.0526,  0.5262,  0.3150]],
       dtype=torch.float64)
	q_value: tensor([[0.3108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10476459176722941, distance: 1.082742835725584 entropy -3.460238106058432
epoch: 6, step: 32
	action: tensor([[ 0.3463, -0.1385, -0.0585,  0.0682, -0.2697,  0.6237,  0.2973]],
       dtype=torch.float64)
	q_value: tensor([[0.3685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6231446681870245, distance: 0.7024962743580707 entropy -3.4958201254492587
epoch: 6, step: 33
	action: tensor([[-0.3887,  0.2061,  0.0653,  0.0673, -0.0319,  0.6604,  0.3307]],
       dtype=torch.float64)
	q_value: tensor([[0.3028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7024962743580707 entropy -3.442121267674362
epoch: 6, step: 34
	action: tensor([[-0.1249,  0.1590,  0.0213, -0.0866,  0.1134,  0.6249,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37399345822279284, distance: 0.9054117558733621 entropy -3.5360962747529094
epoch: 6, step: 35
	action: tensor([[-0.2042, -0.1975,  0.1478, -0.4053,  0.1105,  0.5513,  0.2668]],
       dtype=torch.float64)
	q_value: tensor([[0.3836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9054117558733621 entropy -3.495796840622926
epoch: 6, step: 36
	action: tensor([[ 0.0628,  0.0349, -0.0327, -0.0977, -0.1224,  0.7034,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5081935924724655, distance: 0.8025161566681366 entropy -3.5360962747529094
epoch: 6, step: 37
	action: tensor([[ 0.0032, -0.2480, -0.0630,  0.0161,  0.1684,  0.5528,  0.3041]],
       dtype=torch.float64)
	q_value: tensor([[0.3374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881207740191656, distance: 0.9655168403372509 entropy -3.453306124840549
epoch: 6, step: 38
	action: tensor([[-0.1090, -0.1532,  0.0753, -0.3526,  0.2464,  0.5248,  0.2800]],
       dtype=torch.float64)
	q_value: tensor([[0.3873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05853779870401321, distance: 1.1103455015080208 entropy -3.504374199476254
epoch: 6, step: 39
	action: tensor([[ 0.0675, -0.0493, -0.0316,  0.0435, -0.0501,  0.5267,  0.2724]],
       dtype=torch.float64)
	q_value: tensor([[0.4561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4663656163988089, distance: 0.835946715229143 entropy -3.4786933880535336
epoch: 6, step: 40
	action: tensor([[ 0.2100,  0.0593,  0.1436, -0.0718, -0.1157,  0.4178,  0.2859]],
       dtype=torch.float64)
	q_value: tensor([[0.3433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5912334879782885, distance: 0.7316347890526569 entropy -3.5069718644726815
epoch: 6, step: 41
	action: tensor([[ 0.2989,  0.1520, -0.0698, -0.3603,  0.1757,  0.5304,  0.2958]],
       dtype=torch.float64)
	q_value: tensor([[0.3439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6478643017374434, distance: 0.6790655842596521 entropy -3.4853723843429982
epoch: 6, step: 42
	action: tensor([[-0.0878, -0.0321, -0.1228, -0.1050,  0.1361,  0.4896,  0.2698]],
       dtype=torch.float64)
	q_value: tensor([[0.3984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2647836795427707, distance: 0.9812151899319582 entropy -3.476791961898826
epoch: 6, step: 43
	action: tensor([[-0.1944, -0.0218, -0.0068, -0.0896, -0.0355,  0.4037,  0.2497]],
       dtype=torch.float64)
	q_value: tensor([[0.3960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12093060048264925, distance: 1.0729223064180036 entropy -3.552511925199839
epoch: 6, step: 44
	action: tensor([[-0.0823, -0.0379,  0.0200, -0.0616, -0.1331,  0.6023,  0.2572]],
       dtype=torch.float64)
	q_value: tensor([[0.3826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29002607850040774, distance: 0.9642238990319553 entropy -3.556772004236263
epoch: 6, step: 45
	action: tensor([[-0.2721, -0.1324,  0.1501,  0.1939, -0.0127,  0.5994,  0.2955]],
       dtype=torch.float64)
	q_value: tensor([[0.3549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20166111201809767, distance: 1.0224694380054395 entropy -3.47686123267584
epoch: 6, step: 46
	action: tensor([[ 0.0042, -0.0549,  0.0408, -0.4143,  0.0932,  0.5979,  0.2954]],
       dtype=torch.float64)
	q_value: tensor([[0.3739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2319084028236288, distance: 1.002912860280951 entropy -3.4664981593346624
epoch: 6, step: 47
	action: tensor([[ 0.4916, -0.1451, -0.0098,  0.0092, -0.0860,  0.5711,  0.2861]],
       dtype=torch.float64)
	q_value: tensor([[0.4247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7039239576531924, distance: 0.6226705514740448 entropy -3.4504980887994963
epoch: 6, step: 48
	action: tensor([[ 0.1396, -0.1330, -0.2967, -0.2085,  0.1512,  0.5101,  0.3244]],
       dtype=torch.float64)
	q_value: tensor([[0.3254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3745104894832385, distance: 0.9050377798453711 entropy -3.4394113979618006
epoch: 6, step: 49
	action: tensor([[ 0.1494, -0.1073, -0.0277, -0.0510,  0.1277,  0.5725,  0.2557]],
       dtype=torch.float64)
	q_value: tensor([[0.4023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4963067451715253, distance: 0.8121565679252396 entropy -3.5447039598374324
epoch: 6, step: 50
	action: tensor([[-0.0240, -0.1507, -0.0262, -0.0892, -0.3107,  0.5406,  0.2872]],
       dtype=torch.float64)
	q_value: tensor([[0.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20563591845930485, distance: 1.0199209028805323 entropy -3.4872297590646655
epoch: 6, step: 51
	action: tensor([[ 0.1706, -0.2382,  0.0552,  0.0653, -0.2202,  0.6513,  0.3075]],
       dtype=torch.float64)
	q_value: tensor([[0.3378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4641749649151147, distance: 0.8376608030078284 entropy -3.486495883806235
epoch: 6, step: 52
	action: tensor([[ 0.4424,  0.1081,  0.0518,  0.2118, -0.2555,  0.5552,  0.3355]],
       dtype=torch.float64)
	q_value: tensor([[0.3213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8605260401626365, distance: 0.42736923840512026 entropy -3.4279199829203053
epoch: 6, step: 53
	action: tensor([[-0.0492, -0.0806, -0.1008,  0.1136,  0.2500,  0.4972,  0.3262]],
       dtype=torch.float64)
	q_value: tensor([[0.2918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.42736923840512026 entropy -3.4389145562910706
epoch: 6, step: 54
	action: tensor([[ 0.4274, -0.0358, -0.2848, -0.1769, -0.0010,  0.7541,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6930756727764886, distance: 0.633975317028041 entropy -3.5360962747529094
epoch: 6, step: 55
	action: tensor([[ 0.1743, -0.1618,  0.0930,  0.3080, -0.1261,  0.6161,  0.3022]],
       dtype=torch.float64)
	q_value: tensor([[0.3439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6560006576705378, distance: 0.671174579537102 entropy -3.4501676321625405
epoch: 6, step: 56
	action: tensor([[-0.0374,  0.0417, -0.0282,  0.4107,  0.1408,  0.6550,  0.3227]],
       dtype=torch.float64)
	q_value: tensor([[0.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.671174579537102 entropy -3.442403016628753
epoch: 6, step: 57
	action: tensor([[-0.0482, -0.0205, -0.0232,  0.0299, -0.1163,  0.3644,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070826079767893, distance: 0.9525711504412662 entropy -3.5360962747529094
epoch: 6, step: 58
	action: tensor([[-0.0129,  0.1061, -0.1093,  0.2037, -0.0424,  0.5252,  0.2630]],
       dtype=torch.float64)
	q_value: tensor([[0.3410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9525711504412662 entropy -3.5808270783274976
epoch: 6, step: 59
	action: tensor([[-0.1166, -0.0521,  0.2174,  0.2055, -0.1034,  0.6628,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40757967153565255, distance: 0.8807885688065875 entropy -3.5360962747529094
epoch: 6, step: 60
	action: tensor([[ 0.0332,  0.1186,  0.1279, -0.0072, -0.2050,  0.4946,  0.3137]],
       dtype=torch.float64)
	q_value: tensor([[0.3303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8807885688065875 entropy -3.449907926954957
epoch: 6, step: 61
	action: tensor([[-0.2513, -0.4417, -0.1506, -0.0875,  0.0602,  0.5303,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26479506433045574, distance: 1.286965113366533 entropy -3.5360962747529094
epoch: 6, step: 62
	action: tensor([[ 0.1394,  0.0832, -0.0638,  0.0348, -0.1829,  0.4036,  0.2726]],
       dtype=torch.float64)
	q_value: tensor([[0.4115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5308470399158437, distance: 0.7838156376190635 entropy -3.548655608630127
epoch: 6, step: 63
	action: tensor([[ 0.0433, -0.1449, -0.1913,  0.0765, -0.0084,  0.5341,  0.2752]],
       dtype=torch.float64)
	q_value: tensor([[0.3203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3405372807921435, distance: 0.9292911854354851 entropy -3.540081699401847
LOSS epoch 6 actor 218.35144968728355 critic 1509.822815862913
epoch: 7, step: 0
	action: tensor([[ 0.1996, -0.1891, -0.0400,  0.0151,  0.0361,  0.4022,  0.1560]],
       dtype=torch.float64)
	q_value: tensor([[-0.0012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4228879539253322, distance: 0.8693341953746809 entropy -3.5369463178523035
epoch: 7, step: 1
	action: tensor([[ 0.1364, -0.0517, -0.0249, -0.0581,  0.0329,  0.5213,  0.1697]],
       dtype=torch.float64)
	q_value: tensor([[0.0128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46632461915555223, distance: 0.8359788260279151 entropy -3.53527863809726
epoch: 7, step: 2
	action: tensor([[ 0.2986, -0.3115, -0.1828,  0.0982,  0.2748,  0.6006,  0.1624]],
       dtype=torch.float64)
	q_value: tensor([[0.0105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.513280601431485, distance: 0.7983549477236922 entropy -3.5096327382040777
epoch: 7, step: 3
	action: tensor([[ 0.0665, -0.2324, -0.0554, -0.0691,  0.3000,  0.5764,  0.1652]],
       dtype=torch.float64)
	q_value: tensor([[-0.0166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31150710927167724, distance: 0.9495250428659523 entropy -3.492759013355172
epoch: 7, step: 4
	action: tensor([[-0.2570, -0.2243,  0.1040, -0.2670,  0.1168,  0.4548,  0.1463]],
       dtype=torch.float64)
	q_value: tensor([[0.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20156551568820835, distance: 1.2543837554767678 entropy -3.513170106134251
epoch: 7, step: 5
	action: tensor([[-0.0683, -0.1296, -0.0364,  0.0928, -0.0235,  0.5603,  0.1309]],
       dtype=torch.float64)
	q_value: tensor([[0.0453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27867597236403874, distance: 0.9719006951643041 entropy -3.5236021467681593
epoch: 7, step: 6
	action: tensor([[ 0.2482, -0.1026, -0.0739,  0.4162, -0.1037,  0.6628,  0.1654]],
       dtype=torch.float64)
	q_value: tensor([[-0.0025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9719006951643041 entropy -3.524226935848185
epoch: 7, step: 7
	action: tensor([[-0.1283,  0.0878, -0.0731, -0.0272,  0.1163,  0.4571, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29101021670165206, distance: 0.9635553824743316 entropy -3.6120856031081963
epoch: 7, step: 8
	action: tensor([[-2.4522e-01, -8.5267e-02, -1.0494e-01, -7.8818e-02, -1.8819e-04,
          3.9370e-01,  1.1347e-01]], dtype=torch.float64)
	q_value: tensor([[0.0147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9635553824743316 entropy -3.6049637189523795
epoch: 7, step: 9
	action: tensor([[ 0.1817,  0.0804, -0.0249,  0.1116, -0.2082,  0.5223, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6682779430951229, distance: 0.6590887035155603 entropy -3.6120856031081963
epoch: 7, step: 10
	action: tensor([[-0.1977, -0.0176, -0.1886,  0.1777, -0.0688,  0.5183,  0.1734]],
       dtype=torch.float64)
	q_value: tensor([[0.0045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6590887035155603 entropy -3.5228209805523663
epoch: 7, step: 11
	action: tensor([[ 0.3054, -0.0710,  0.0281,  0.0637, -0.1279,  0.2501, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.629863285403205, distance: 0.6962060230489812 entropy -3.6120856031081963
epoch: 7, step: 12
	action: tensor([[-0.0019, -0.0574,  0.2141, -0.4372, -0.0761,  0.4386,  0.1704]],
       dtype=torch.float64)
	q_value: tensor([[0.0307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14657356013499445, distance: 1.0571576090483348 entropy -3.5729747364078634
epoch: 7, step: 13
	action: tensor([[-0.0319, -0.0315, -0.0694,  0.4029,  0.0440,  0.5252,  0.1549]],
       dtype=torch.float64)
	q_value: tensor([[0.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0571576090483348 entropy -3.4692916676321515
epoch: 7, step: 14
	action: tensor([[-0.2405,  0.0530,  0.1757,  0.0654,  0.0833,  0.2125, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12365631000281019, distance: 1.071257622702415 entropy -3.6120856031081963
epoch: 7, step: 15
	action: tensor([[ 0.4333, -0.1576, -0.0022,  0.1720,  0.0439,  0.4679,  0.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.0514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7083212812456785, distance: 0.6180293001929091 entropy -3.6190920510486593
epoch: 7, step: 16
	action: tensor([[-4.0000e-04, -8.3631e-02, -1.3936e-02,  1.1947e-01,  1.1736e-02,
          5.5288e-01,  1.8816e-01]], dtype=torch.float64)
	q_value: tensor([[-0.0115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3990709448590003, distance: 0.8870912482884805 entropy -3.4827929923782532
epoch: 7, step: 17
	action: tensor([[-0.1073,  0.1039, -0.0991,  0.2502, -0.0821,  0.3655,  0.1643]],
       dtype=torch.float64)
	q_value: tensor([[-0.0007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8870912482884805 entropy -3.514943108859108
epoch: 7, step: 18
	action: tensor([[ 0.2694, -0.1319,  0.0021, -0.1573,  0.1609,  0.5869, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.566508581584328, distance: 0.75343700282582 entropy -3.6120856031081963
epoch: 7, step: 19
	action: tensor([[-0.0781,  0.0047, -0.1163, -0.0133,  0.1468,  0.4928,  0.1686]],
       dtype=torch.float64)
	q_value: tensor([[0.0029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3641238999113253, distance: 0.9125211607948409 entropy -3.488864442419356
epoch: 7, step: 20
	action: tensor([[-0.0733, -0.0804,  0.0104,  0.0006, -0.0766,  0.3293,  0.1176]],
       dtype=torch.float64)
	q_value: tensor([[0.0179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26147535299405245, distance: 0.9834203481793421 entropy -3.5760279498926004
epoch: 7, step: 21
	action: tensor([[ 0.1429,  0.1331,  0.2192,  0.2809, -0.0997,  0.5468,  0.1478]],
       dtype=torch.float64)
	q_value: tensor([[0.0304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9834203481793421 entropy -3.579272064490212
epoch: 7, step: 22
	action: tensor([[ 0.0798, -0.1978,  0.2697, -0.0950, -0.0769,  0.3413, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2644791929549707, distance: 0.9814183518653427 entropy -3.6120856031081963
epoch: 7, step: 23
	action: tensor([[ 0.0443, -0.0550,  0.0657,  0.0255, -0.0748,  0.5708,  0.1839]],
       dtype=torch.float64)
	q_value: tensor([[0.0403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4659635389517476, distance: 0.8362615862739183 entropy -3.511430958521892
epoch: 7, step: 24
	action: tensor([[-0.0262, -0.2564, -0.1322,  0.0676, -0.0059,  0.5054,  0.1780]],
       dtype=torch.float64)
	q_value: tensor([[0.0031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23495403769313095, distance: 1.0009225115140032 entropy -3.4789681592102184
epoch: 7, step: 25
	action: tensor([[ 0.3420, -0.2453,  0.0365,  0.2033, -0.1446,  0.4651,  0.1634]],
       dtype=torch.float64)
	q_value: tensor([[-0.0017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6240080347502289, distance: 0.7016911118496477 entropy -3.54227913297727
epoch: 7, step: 26
	action: tensor([[-0.0773,  0.0746, -0.0720, -0.0563, -0.0321,  0.4643,  0.2049]],
       dtype=torch.float64)
	q_value: tensor([[-0.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3507916036695372, distance: 0.9220378667700346 entropy -3.4658167431995817
epoch: 7, step: 27
	action: tensor([[-0.2036, -0.3668,  0.1380, -0.0178,  0.2311,  0.6106,  0.1285]],
       dtype=torch.float64)
	q_value: tensor([[0.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029517404151124138, distance: 1.1273287139291452 entropy -3.562660560503353
epoch: 7, step: 28
	action: tensor([[-0.0733, -0.2700, -0.0275, -0.1377,  0.3229,  0.6306,  0.1632]],
       dtype=torch.float64)
	q_value: tensor([[0.0079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17901231054457134, distance: 1.0368716871360766 entropy -3.476581971243789
epoch: 7, step: 29
	action: tensor([[ 0.3624, -0.1232, -0.0112, -0.2289, -0.1553,  0.5252,  0.1378]],
       dtype=torch.float64)
	q_value: tensor([[0.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5182009787284337, distance: 0.7943093046911858 entropy -3.502859625474452
epoch: 7, step: 30
	action: tensor([[-0.0976, -0.4542, -0.0712, -0.0158, -0.0494,  0.5893,  0.1936]],
       dtype=torch.float64)
	q_value: tensor([[0.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00911091503969641, distance: 1.149545445595301 entropy -3.4547209864878
epoch: 7, step: 31
	action: tensor([[-0.1852, -0.0735, -0.1443,  0.1537, -0.1018,  0.5999,  0.1847]],
       dtype=torch.float64)
	q_value: tensor([[-0.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22325689838424656, distance: 1.0085452672244988 entropy -3.487169701049012
epoch: 7, step: 32
	action: tensor([[-0.2193, -0.2669, -0.0519,  0.0882,  0.0333,  0.5092,  0.1512]],
       dtype=torch.float64)
	q_value: tensor([[-0.0029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049731934536821454, distance: 1.1155261633758617 entropy -3.5404492589038843
epoch: 7, step: 33
	action: tensor([[-0.3715, -0.0493, -0.0399, -0.2354,  0.1571,  0.4365,  0.1525]],
       dtype=torch.float64)
	q_value: tensor([[0.0018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13309869016476683, distance: 1.2181213693268673 entropy -3.5508999727757056
epoch: 7, step: 34
	action: tensor([[0.1812, 0.0779, 0.0491, 0.0530, 0.1584, 0.3824, 0.0836]],
       dtype=torch.float64)
	q_value: tensor([[0.0442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6459637298199244, distance: 0.6808956688301435 entropy -3.594932659244406
epoch: 7, step: 35
	action: tensor([[-0.1315,  0.0770,  0.1303,  0.0630,  0.0037,  0.5901,  0.1437]],
       dtype=torch.float64)
	q_value: tensor([[0.0214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6808956688301435 entropy -3.555303135176485
epoch: 7, step: 36
	action: tensor([[ 0.2204,  0.2268,  0.0721, -0.0022,  0.0021,  0.5674, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7239547291431059, distance: 0.6012385917771776 entropy -3.6120856031081963
epoch: 7, step: 37
	action: tensor([[ 0.2899,  0.1520, -0.0077, -0.0517, -0.1838,  0.5084, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6858760808711586, distance: 0.6413678658516546 entropy -3.6120856031081963
epoch: 7, step: 38
	action: tensor([[ 0.1215, -0.0050,  0.1167, -0.2272, -0.0264,  0.4758,  0.1689]],
       dtype=torch.float64)
	q_value: tensor([[0.0091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3913166939056385, distance: 0.892796314253638 entropy -3.5121182097103065
epoch: 7, step: 39
	action: tensor([[-0.0069, -0.1397, -0.0211, -0.0953,  0.1593,  0.7422,  0.1646]],
       dtype=torch.float64)
	q_value: tensor([[0.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33942593353021244, distance: 0.9300738911431501 entropy -3.482938928193442
epoch: 7, step: 40
	action: tensor([[-0.2989, -0.1322, -0.0977, -0.2311, -0.0188,  0.3883,  0.1589]],
       dtype=torch.float64)
	q_value: tensor([[-0.0025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19604009713658765, distance: 1.2514962800832712 entropy -3.4610981248667163
epoch: 7, step: 41
	action: tensor([[ 0.0495, -0.1615, -0.0730, -0.1817, -0.2036,  0.5545,  0.1035]],
       dtype=torch.float64)
	q_value: tensor([[0.0388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21404872751900683, distance: 1.0145057292507027 entropy -3.6041107620110107
epoch: 7, step: 42
	action: tensor([[-0.2247, -0.0740, -0.0981,  0.2908,  0.2130,  0.6391,  0.1802]],
       dtype=torch.float64)
	q_value: tensor([[0.0146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0145057292507027 entropy -3.491641735844895
epoch: 7, step: 43
	action: tensor([[ 0.2364, -0.2196, -0.1198, -0.0558, -0.0743,  0.5392, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4707726884252701, distance: 0.8324876846658531 entropy -3.6120856031081963
epoch: 7, step: 44
	action: tensor([[-0.4962, -0.2663, -0.0972,  0.3268,  0.2459,  0.5267,  0.1820]],
       dtype=torch.float64)
	q_value: tensor([[0.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11044686981095375, distance: 1.205884145839064 entropy -3.516907829095149
epoch: 7, step: 45
	action: tensor([[-0.0577, -0.0648, -0.2851,  0.2364, -0.1335,  0.5080,  0.1125]],
       dtype=torch.float64)
	q_value: tensor([[-0.0043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34632052024146065, distance: 0.9252074458152562 entropy -3.5735625446031953
epoch: 7, step: 46
	action: tensor([[ 0.0465,  0.1126, -0.1631,  0.2547,  0.1103,  0.3833,  0.1497]],
       dtype=torch.float64)
	q_value: tensor([[-0.0056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9252074458152562 entropy -3.583825983099963
epoch: 7, step: 47
	action: tensor([[-0.0027, -0.1352,  0.1402, -0.1340, -0.0896,  0.2844, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17992616124603789, distance: 1.0362944496459936 entropy -3.6120856031081963
epoch: 7, step: 48
	action: tensor([[ 0.0802, -0.3448,  0.0305,  0.0763, -0.0729,  0.5079,  0.1573]],
       dtype=torch.float64)
	q_value: tensor([[0.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28696597879003216, distance: 0.9662996433709276 entropy -3.5685862464852036
epoch: 7, step: 49
	action: tensor([[-0.0033,  0.1081,  0.2231,  0.1214,  0.1734,  0.5066,  0.1967]],
       dtype=torch.float64)
	q_value: tensor([[-0.0016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9662996433709276 entropy -3.4827013317003996
epoch: 7, step: 50
	action: tensor([[ 0.4291, -0.1651, -0.0034,  0.3373,  0.0020,  0.2691, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7159058719549825, distance: 0.6099409922810325 entropy -3.6120856031081963
epoch: 7, step: 51
	action: tensor([[ 0.0633, -0.2112, -0.0249, -0.0382, -0.2986,  0.5877,  0.1689]],
       dtype=torch.float64)
	q_value: tensor([[-0.0045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31931696978511326, distance: 0.944124255081858 entropy -3.5624207027921293
epoch: 7, step: 52
	action: tensor([[ 0.3949, -0.0321, -0.1927, -0.0323, -0.0171,  0.5089,  0.2020]],
       dtype=torch.float64)
	q_value: tensor([[0.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7028457128838363, distance: 0.6238033366615908 entropy -3.460311896170144
epoch: 7, step: 53
	action: tensor([[-0.3205,  0.0577, -0.1858, -0.1609, -0.0321,  0.6029,  0.1700]],
       dtype=torch.float64)
	q_value: tensor([[0.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0737791670253155, distance: 1.101321115111158 entropy -3.500296491892762
epoch: 7, step: 54
	action: tensor([[-0.1140,  0.0567,  0.0673,  0.1226,  0.1695,  0.4787,  0.0947]],
       dtype=torch.float64)
	q_value: tensor([[0.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.101321115111158 entropy -3.5697250777975795
epoch: 7, step: 55
	action: tensor([[ 0.3280, -0.0677,  0.0178,  0.0865, -0.0013,  0.3512, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6863568967161982, distance: 0.6408768209010485 entropy -3.6120856031081963
epoch: 7, step: 56
	action: tensor([[ 0.1460, -0.1933, -0.0662,  0.4377,  0.0425,  0.5807,  0.1681]],
       dtype=torch.float64)
	q_value: tensor([[0.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6408768209010485 entropy -3.556619929493169
epoch: 7, step: 57
	action: tensor([[ 0.3915, -0.3248, -0.0173,  0.0202,  0.0072,  0.2655, -0.2920]],
       dtype=torch.float64)
	q_value: tensor([[0.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4414568102593337, distance: 0.8552342278380686 entropy -3.6120856031081963
epoch: 7, step: 58
	action: tensor([[-0.6318, -0.1257,  0.1166,  0.1014, -0.0492,  0.4536,  0.1782]],
       dtype=torch.float64)
	q_value: tensor([[0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33874373786743717, distance: 1.3240531518417769 entropy -3.549558828647733
epoch: 7, step: 59
	action: tensor([[-0.0462, -0.0225,  0.0825, -0.1500, -0.2370,  0.5558,  0.1177]],
       dtype=torch.float64)
	q_value: tensor([[0.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293946520908015, distance: 0.9615580143372472 entropy -3.538426363849958
epoch: 7, step: 60
	action: tensor([[-0.0134, -0.4747, -0.1051,  0.1099, -0.1677,  0.5135,  0.1785]],
       dtype=torch.float64)
	q_value: tensor([[0.0187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08651943333044632, distance: 1.093720493090351 entropy -3.474708459518286
epoch: 7, step: 61
	action: tensor([[ 0.2142, -0.2144, -0.1037, -0.0907,  0.2879,  0.4782,  0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-0.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44210521149284643, distance: 0.8547376718250559 entropy -3.497143973449649
epoch: 7, step: 62
	action: tensor([[ 0.0776, -0.2886, -0.0037, -0.0871, -0.3716,  0.5711,  0.1452]],
       dtype=torch.float64)
	q_value: tensor([[0.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21876712322066216, distance: 1.0114558929320996 entropy -3.532350450681412
epoch: 7, step: 63
	action: tensor([[ 0.3007,  0.0286,  0.0824, -0.1107,  0.1980,  0.5542,  0.2128]],
       dtype=torch.float64)
	q_value: tensor([[0.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6892006574211278, distance: 0.6379648323569321 entropy -3.4453159696070896
LOSS epoch 7 actor 254.23007289397995 critic 1696.9622198378108
epoch: 8, step: 0
	action: tensor([[ 0.2308,  0.1527,  0.1219,  0.0413, -0.3286,  0.5187,  0.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6379648323569321 entropy -3.37882268381022
epoch: 8, step: 1
	action: tensor([[ 0.3348, -0.2626,  0.0791, -0.0777, -0.1986,  0.5157, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.479583149958031, distance: 0.8255290644815854 entropy -3.5332669797433596
epoch: 8, step: 2
	action: tensor([[ 0.1988, -0.5751,  0.1487,  0.0572,  0.1329,  0.5891,  0.3146]],
       dtype=torch.float64)
	q_value: tensor([[-0.1906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24637198161457297, distance: 0.9934252891218639 entropy -3.3054338465649655
epoch: 8, step: 3
	action: tensor([[ 0.2470, -0.4253,  0.0369,  0.5300, -0.0173,  0.3364,  0.2701]],
       dtype=torch.float64)
	q_value: tensor([[-0.2421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5840162633036712, distance: 0.7380654387173053 entropy -3.279685800442691
epoch: 8, step: 4
	action: tensor([[ 0.0007, -0.2110, -0.3116, -0.1261, -0.1104,  0.5899,  0.2698]],
       dtype=torch.float64)
	q_value: tensor([[-0.2290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19669562689446707, distance: 1.0256442718907914 entropy -3.3562737065769856
epoch: 8, step: 5
	action: tensor([[ 0.3296, -0.1907, -0.1714, -0.1877, -0.1444,  0.5251,  0.1730]],
       dtype=torch.float64)
	q_value: tensor([[-0.2170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4577215099497721, distance: 0.8426900815734046 entropy -3.445586365123945
epoch: 8, step: 6
	action: tensor([[ 0.3230, -0.1411,  0.1445, -0.1122,  0.0498,  0.5656,  0.2388]],
       dtype=torch.float64)
	q_value: tensor([[-0.1899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5783329087208114, distance: 0.7430902231613683 entropy -3.368334629952969
epoch: 8, step: 7
	action: tensor([[-0.1462,  0.1367, -0.1035, -0.0513,  0.2895,  0.4953,  0.2325]],
       dtype=torch.float64)
	q_value: tensor([[-0.1976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7430902231613683 entropy -3.3240794135572216
epoch: 8, step: 8
	action: tensor([[ 0.0533,  0.2334,  0.1742, -0.0355,  0.0218,  0.1871, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5453998053299229, distance: 0.7715631934734898 entropy -3.5332669797433596
epoch: 8, step: 9
	action: tensor([[ 0.0635, -0.2986,  0.0946,  0.1866,  0.2695,  0.4499,  0.1537]],
       dtype=torch.float64)
	q_value: tensor([[-0.1799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7715631934734898 entropy -3.505921879665838
epoch: 8, step: 10
	action: tensor([[-0.2300, -0.0266,  0.0693, -0.1538,  0.0082,  0.5507, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06215667278716652, distance: 1.108209425641451 entropy -3.5332669797433596
epoch: 8, step: 11
	action: tensor([[ 0.1267, -0.0115, -0.1010,  0.0877, -0.0745,  0.2854,  0.1571]],
       dtype=torch.float64)
	q_value: tensor([[-0.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4676204808401133, distance: 0.8349632542131968 entropy -3.4410906434521897
epoch: 8, step: 12
	action: tensor([[ 0.0229, -0.2548, -0.0373, -0.1691, -0.3045,  0.5291,  0.1799]],
       dtype=torch.float64)
	q_value: tensor([[-0.1815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.094467149130401, distance: 1.0889521542452358 entropy -3.500272210364688
epoch: 8, step: 13
	action: tensor([[ 0.4541, -0.1141, -0.1207, -0.0598, -0.3219,  0.5870,  0.2386]],
       dtype=torch.float64)
	q_value: tensor([[-0.1923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6226928625360224, distance: 0.7029172538266 entropy -3.3642657408837926
epoch: 8, step: 14
	action: tensor([[ 0.3469,  0.2554,  0.2407, -0.0875, -0.3261,  0.5383,  0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-0.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7029172538266 entropy -3.2993057189918287
epoch: 8, step: 15
	action: tensor([[ 0.1176, -0.0274,  0.1801, -0.0357, -0.0327,  0.6769, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5694507674543712, distance: 0.7508757919187985 entropy -3.5332669797433596
epoch: 8, step: 16
	action: tensor([[-0.0629, -0.2855,  0.0285, -0.2399, -0.3132,  0.5651,  0.2470]],
       dtype=torch.float64)
	q_value: tensor([[-0.2112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006726357051585197, distance: 1.1481864378743372 entropy -3.3320372326768823
epoch: 8, step: 17
	action: tensor([[-0.2507,  0.0088, -0.1831, -0.1368, -0.1849,  0.4218,  0.2279]],
       dtype=torch.float64)
	q_value: tensor([[-0.1977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03264080060564145, distance: 1.1255131573391859 entropy -3.3445603242608763
epoch: 8, step: 18
	action: tensor([[-0.0725, -0.1189, -0.1738,  0.0574,  0.2250,  0.4685,  0.0980]],
       dtype=torch.float64)
	q_value: tensor([[-0.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2904470018802361, distance: 0.9639380261678925 entropy -3.5405903084177583
epoch: 8, step: 19
	action: tensor([[ 0.0024, -0.1944, -0.1225, -0.1018, -0.4564,  0.3801,  0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-0.2301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13638813503056646, distance: 1.0634473502094646 entropy -3.5220391052998457
epoch: 8, step: 20
	action: tensor([[ 0.2766, -0.1173, -0.0863,  0.0724, -0.2292,  0.6155,  0.2237]],
       dtype=torch.float64)
	q_value: tensor([[-0.1743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5998120283209931, distance: 0.7239168889086547 entropy -3.4302900597054284
epoch: 8, step: 21
	action: tensor([[ 0.1868, -0.0064, -0.1022,  0.4023,  0.0179,  0.6439,  0.2672]],
       dtype=torch.float64)
	q_value: tensor([[-0.1998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7239168889086547 entropy -3.331650380732995
epoch: 8, step: 22
	action: tensor([[-0.2086, -0.1155,  0.0209,  0.1398, -0.2437,  0.3370, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07703601079190969, distance: 1.0993831380627672 entropy -3.5332669797433596
epoch: 8, step: 23
	action: tensor([[ 0.1158, -0.2522, -0.1260,  0.3717, -0.5692,  0.5078,  0.1992]],
       dtype=torch.float64)
	q_value: tensor([[-0.1887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37610296239627417, distance: 0.9038849494333275 entropy -3.467290262300011
epoch: 8, step: 24
	action: tensor([[ 0.0905, -0.5102,  0.0161,  0.1115,  0.0219,  0.6395,  0.2990]],
       dtype=torch.float64)
	q_value: tensor([[-0.2025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20908251596052274, distance: 1.0177058743927154 entropy -3.334531435185714
epoch: 8, step: 25
	action: tensor([[-0.0590, -0.2704,  0.0340,  0.2834,  0.0444,  0.5545,  0.2688]],
       dtype=torch.float64)
	q_value: tensor([[-0.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3191009617495377, distance: 0.9442740474519723 entropy -3.308219220590785
epoch: 8, step: 26
	action: tensor([[ 0.0851, -0.1408,  0.3056,  0.0093,  0.2244,  0.5169,  0.2143]],
       dtype=torch.float64)
	q_value: tensor([[-0.2335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43736591584586604, distance: 0.8583604766923836 entropy -3.391091042363267
epoch: 8, step: 27
	action: tensor([[ 0.4511, -0.4462, -0.0306,  0.1634, -0.0643,  0.3895,  0.1876]],
       dtype=torch.float64)
	q_value: tensor([[-0.2056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4577209613780101, distance: 0.8426905078081479 entropy -3.3552444190177835
epoch: 8, step: 28
	action: tensor([[ 0.2276, -0.2526, -0.1942,  0.2458,  0.0555,  0.5678,  0.3005]],
       dtype=torch.float64)
	q_value: tensor([[-0.2126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5224647486159337, distance: 0.7907868002783137 entropy -3.3228640170224817
epoch: 8, step: 29
	action: tensor([[ 0.0204, -0.3278,  0.0317,  0.1177,  0.1792,  0.5767,  0.2354]],
       dtype=torch.float64)
	q_value: tensor([[-0.2291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29319782463307387, distance: 0.9620676953863384 entropy -3.3867943750532055
epoch: 8, step: 30
	action: tensor([[-0.4367,  0.0894, -0.0361,  0.0862,  0.6086,  0.5319,  0.2109]],
       dtype=torch.float64)
	q_value: tensor([[-0.2364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06567583816421063, distance: 1.1061282477013696 entropy -3.3812499398807323
epoch: 8, step: 31
	action: tensor([[ 0.1901, -0.2297, -0.0552, -0.3955,  0.1463,  0.4828,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-0.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20196032264087949, distance: 1.0222778136259696 entropy -3.5156894966269356
epoch: 8, step: 32
	action: tensor([[ 0.3186, -0.1991,  0.0735,  0.1339,  0.0289,  0.4405,  0.1780]],
       dtype=torch.float64)
	q_value: tensor([[-0.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5921591406700352, distance: 0.7308059252277445 entropy -3.4167402638501407
epoch: 8, step: 33
	action: tensor([[-2.5595e-04, -1.2821e-01,  2.9729e-01, -1.5750e-01,  1.5041e-01,
          6.1911e-01,  2.5303e-01]], dtype=torch.float64)
	q_value: tensor([[-0.2027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2957810034165893, distance: 0.9603080320826812 entropy -3.364137774373114
epoch: 8, step: 34
	action: tensor([[-0.5359, -0.2018, -0.1830,  0.0260, -0.2315,  0.5565,  0.1721]],
       dtype=torch.float64)
	q_value: tensor([[-0.2060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46106459809007605, distance: 1.3832204535553736 entropy -3.3330639712877734
epoch: 8, step: 35
	action: tensor([[-0.1884,  0.1124,  0.0168,  0.1979, -0.1345,  0.4224,  0.1158]],
       dtype=torch.float64)
	q_value: tensor([[-0.2365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3832204535553736 entropy -3.5079568423309233
epoch: 8, step: 36
	action: tensor([[ 0.1677, -0.0611,  0.1908,  0.0479, -0.0445,  0.2399, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5167440308298044, distance: 0.7955093836192785 entropy -3.5332669797433596
epoch: 8, step: 37
	action: tensor([[-0.1320, -0.1936,  0.2471, -0.0466, -0.1332,  0.5494,  0.2340]],
       dtype=torch.float64)
	q_value: tensor([[-0.1763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16065139951566343, distance: 1.0484020913018797 entropy -3.428030353852033
epoch: 8, step: 38
	action: tensor([[-0.0688, -0.4366, -0.2328, -0.0499,  0.2013,  0.6089,  0.2064]],
       dtype=torch.float64)
	q_value: tensor([[-0.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04197845762711638, distance: 1.1200678477227919 entropy -3.345326710871586
epoch: 8, step: 39
	action: tensor([[-0.1819, -0.1373,  0.0267,  0.1104, -0.1458,  0.6267,  0.1786]],
       dtype=torch.float64)
	q_value: tensor([[-0.2582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2150229569560156, distance: 1.0138767667106705 entropy -3.433508646369649
epoch: 8, step: 40
	action: tensor([[-0.1534, -0.0269,  0.0662,  0.2741,  0.1201,  0.5026,  0.1968]],
       dtype=torch.float64)
	q_value: tensor([[-0.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0138767667106705 entropy -3.395725730180433
epoch: 8, step: 41
	action: tensor([[ 0.2221,  0.1089,  0.0570,  0.0653,  0.1796,  0.3532, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7144946182424137, distance: 0.6114540737857805 entropy -3.5332669797433596
epoch: 8, step: 42
	action: tensor([[ 0.1534, -0.2283, -0.2406, -0.6302, -0.1211,  0.4661,  0.1932]],
       dtype=torch.float64)
	q_value: tensor([[-0.2005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6114540737857805 entropy -3.458347764625272
epoch: 8, step: 43
	action: tensor([[-0.1947, -0.0243, -0.1448,  0.0417,  0.1848,  0.5856, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20645321231863878, distance: 1.019396087100303 entropy -3.5332669797433596
epoch: 8, step: 44
	action: tensor([[ 0.2296, -0.1733, -0.1076,  0.0908, -0.1724,  0.4625,  0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-0.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4789721833673697, distance: 0.8260135056874096 entropy -3.5000245356724418
epoch: 8, step: 45
	action: tensor([[ 0.4594,  0.0959,  0.2767, -0.1701, -0.2300,  0.6547,  0.2534]],
       dtype=torch.float64)
	q_value: tensor([[-0.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7669994265532747, distance: 0.5523764927470709 entropy -3.3926203554462506
epoch: 8, step: 46
	action: tensor([[ 0.2876, -0.0875,  0.0303, -0.0994,  0.2408,  0.8513,  0.2619]],
       dtype=torch.float64)
	q_value: tensor([[-0.1852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.66649202055223, distance: 0.6608605201750887 entropy -3.242479266426617
epoch: 8, step: 47
	action: tensor([[-0.0407, -0.1524, -0.1820,  0.2458,  0.0990,  0.8300,  0.1994]],
       dtype=torch.float64)
	q_value: tensor([[-0.2295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6608605201750887 entropy -3.293904485716404
epoch: 8, step: 48
	action: tensor([[-0.0792, -0.3880, -0.1439,  0.0721, -0.1251,  0.5210, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030421218459036448, distance: 1.1268036487905781 entropy -3.5332669797433596
epoch: 8, step: 49
	action: tensor([[ 0.1874, -0.0586, -0.0328, -0.1932,  0.1473,  0.4020,  0.2527]],
       dtype=torch.float64)
	q_value: tensor([[-0.2309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4328496611563887, distance: 0.8617986138433562 entropy -3.4181776151419143
epoch: 8, step: 50
	action: tensor([[-0.2509, -0.1450, -0.0074,  0.0857, -0.0589,  0.4825,  0.1495]],
       dtype=torch.float64)
	q_value: tensor([[-0.1913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04977241551891032, distance: 1.1155024026723224 entropy -3.4533765048733303
epoch: 8, step: 51
	action: tensor([[-0.0330, -0.0716, -0.0361, -0.4025, -0.1328,  0.4152,  0.1605]],
       dtype=torch.float64)
	q_value: tensor([[-0.2127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08757103082246631, distance: 1.093090767087427 entropy -3.469420165101584
epoch: 8, step: 52
	action: tensor([[-0.2366, -0.1311, -0.1219, -0.0275, -0.2716,  0.4769,  0.1382]],
       dtype=torch.float64)
	q_value: tensor([[-0.1767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03691606803424019, distance: 1.165275177934369 entropy -3.456851796616296
epoch: 8, step: 53
	action: tensor([[-0.1522, -0.0305,  0.0293,  0.0788,  0.2261,  0.4625,  0.1654]],
       dtype=torch.float64)
	q_value: tensor([[-0.2010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.278007815127567, distance: 0.9723507232707886 entropy -3.4765373759710916
epoch: 8, step: 54
	action: tensor([[-0.1396, -0.0663, -0.0467, -0.0310, -0.0486,  0.6214,  0.1177]],
       dtype=torch.float64)
	q_value: tensor([[-0.2150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21264656726512698, distance: 1.0154102800674398 entropy -3.4906906022232924
epoch: 8, step: 55
	action: tensor([[ 0.0447, -0.1119, -0.0584, -0.0062,  0.2178,  0.5718,  0.1647]],
       dtype=torch.float64)
	q_value: tensor([[-0.2145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4120172246744773, distance: 0.8774835734172172 entropy -3.430617955710028
epoch: 8, step: 56
	action: tensor([[-0.0594, -0.0337, -0.1156, -0.1667, -0.1992,  0.5136,  0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-0.2227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20432967836456217, distance: 1.0207591295639755 entropy -3.43815907837721
epoch: 8, step: 57
	action: tensor([[ 0.2186, -0.0167, -0.0953, -0.0332, -0.0095,  0.4708,  0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-0.1903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5580089472063606, distance: 0.7607876119493177 entropy -3.4499516934856316
epoch: 8, step: 58
	action: tensor([[-0.2834, -0.2288,  0.1971,  0.1569,  0.0720,  0.6220,  0.1961]],
       dtype=torch.float64)
	q_value: tensor([[-0.1916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11538750152851118, distance: 1.0762997238030174 entropy -3.4288232538325008
epoch: 8, step: 59
	action: tensor([[ 0.1641,  0.0813, -0.3571,  0.3090, -0.1024,  0.5016,  0.1774]],
       dtype=torch.float64)
	q_value: tensor([[-0.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0762997238030174 entropy -3.3789139940692374
epoch: 8, step: 60
	action: tensor([[ 0.5461,  0.0261,  0.0732, -0.0778,  0.0404,  0.4294, -0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7470735184375539, distance: 0.575511285805118 entropy -3.5332669797433596
epoch: 8, step: 61
	action: tensor([[-0.4656, -0.2665, -0.0834, -0.2563, -0.0659,  0.4746,  0.2550]],
       dtype=torch.float64)
	q_value: tensor([[-0.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.402667868856843, distance: 1.3552958784028148 entropy -3.3657092440001724
epoch: 8, step: 62
	action: tensor([[ 0.1369, -0.2248, -0.0044,  0.3361, -0.0789,  0.5571,  0.0904]],
       dtype=torch.float64)
	q_value: tensor([[-0.2230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5930071954401728, distance: 0.7300457194359901 entropy -3.500946903664528
epoch: 8, step: 63
	action: tensor([[-0.1762,  0.0105, -0.1344, -0.1446,  0.1043,  0.4891,  0.2665]],
       dtype=torch.float64)
	q_value: tensor([[-0.2190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22609469461125242, distance: 1.006701243876522 entropy -3.366701785291053
LOSS epoch 8 actor 193.90130250677038 critic 1505.6894104186686
epoch: 9, step: 0
	action: tensor([[ 0.0461, -0.0827, -0.0356, -0.3379,  0.0495,  0.5584,  0.2267]],
       dtype=torch.float64)
	q_value: tensor([[-0.3535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3119451473639946, distance: 0.9492229378469554 entropy -3.401778448828991
epoch: 9, step: 1
	action: tensor([[ 0.7466,  0.1100, -0.1238, -0.3841,  0.1922,  0.4640,  0.3069]],
       dtype=torch.float64)
	q_value: tensor([[-0.3517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.76882101092116, distance: 0.55021303302241 entropy -3.283003473776574
epoch: 9, step: 2
	action: tensor([[-0.1994,  0.0082, -0.1177, -0.4509, -0.3241,  0.4730,  0.3585]],
       dtype=torch.float64)
	q_value: tensor([[-0.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009559607255312907, distance: 1.1388613781818329 entropy -3.2002430220053695
epoch: 9, step: 3
	action: tensor([[-0.1155, -0.0437, -0.2145, -0.2042,  0.3397,  0.4721,  0.2676]],
       dtype=torch.float64)
	q_value: tensor([[-0.3333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2568704125654806, distance: 0.9864815559205781 entropy -3.3368647716706095
epoch: 9, step: 4
	action: tensor([[-0.0478, -0.2032,  0.3285, -0.3314, -0.2448,  0.3977,  0.1952]],
       dtype=torch.float64)
	q_value: tensor([[-0.3885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022590924624922737, distance: 1.1313445181918005 entropy -3.435456173576424
epoch: 9, step: 5
	action: tensor([[ 0.3737, -0.2048,  0.0801, -0.2193, -0.3769,  0.7812,  0.3735]],
       dtype=torch.float64)
	q_value: tensor([[-0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5295306540976842, distance: 0.7849145127322712 entropy -3.194487232497291
epoch: 9, step: 6
	action: tensor([[-0.3504, -0.3528,  0.1740, -0.2156,  0.0444,  0.5538,  0.4696]],
       dtype=torch.float64)
	q_value: tensor([[-0.3364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26032294206458295, distance: 1.2846878424759807 entropy -3.0679236181757905
epoch: 9, step: 7
	action: tensor([[-0.1392, -0.0559, -0.1747, -0.1120, -0.0299,  0.6100,  0.2903]],
       dtype=torch.float64)
	q_value: tensor([[-0.4033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23547273847177908, distance: 1.0005831415007351 entropy -3.249844828604501
epoch: 9, step: 8
	action: tensor([[ 0.2731, -0.0821,  0.0296, -0.3332, -0.2304,  0.5702,  0.2813]],
       dtype=torch.float64)
	q_value: tensor([[-0.3533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4630358197220885, distance: 0.8385507489743022 entropy -3.338304871751377
epoch: 9, step: 9
	action: tensor([[ 0.1538, -0.3989,  0.2794,  0.0238, -0.2318,  0.5977,  0.3909]],
       dtype=torch.float64)
	q_value: tensor([[-0.3249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35252541002721216, distance: 0.9208058248713463 entropy -3.1826550003340044
epoch: 9, step: 10
	action: tensor([[ 0.6369, -0.2171, -0.1467,  0.2122, -0.1232,  0.7988,  0.4285]],
       dtype=torch.float64)
	q_value: tensor([[-0.3535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8381633676579621, distance: 0.4603573798450005 entropy -3.1126715138857466
epoch: 9, step: 11
	action: tensor([[ 0.1793, -0.5757, -0.2355,  0.2423,  0.4389,  0.7242,  0.4561]],
       dtype=torch.float64)
	q_value: tensor([[-0.3709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39050281043380597, distance: 0.8933930033857491 entropy -3.0900529674428276
epoch: 9, step: 12
	action: tensor([[ 0.1135, -0.0445, -0.2628,  0.4856, -0.3708,  0.7415,  0.3446]],
       dtype=torch.float64)
	q_value: tensor([[-0.4704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8933930033857491 entropy -3.2206291893725885
epoch: 9, step: 13
	action: tensor([[-0.1226, -0.1183, -0.0887,  0.0329,  0.3257,  0.3767, -0.2233]],
       dtype=torch.float64)
	q_value: tensor([[-0.5523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18905595845085732, distance: 1.030509825230658 entropy -3.5086896264262317
epoch: 9, step: 14
	action: tensor([[ 0.2976, -0.2977,  0.0667, -0.4263,  0.1688,  0.4295,  0.2894]],
       dtype=torch.float64)
	q_value: tensor([[-0.3781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17108449374368873, distance: 1.0418659031480768 entropy -3.3942773837862
epoch: 9, step: 15
	action: tensor([[-0.1189, -0.2441, -0.1945, -0.2930, -0.4265,  0.3644,  0.3469]],
       dtype=torch.float64)
	q_value: tensor([[-0.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14593279704666728, distance: 1.2250005054940476 entropy -3.232083803456827
epoch: 9, step: 16
	action: tensor([[-0.3094, -0.2704,  0.3445,  0.1065,  0.2962,  0.5564,  0.3264]],
       dtype=torch.float64)
	q_value: tensor([[-0.3266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03851642673016731, distance: 1.1220898338636498 entropy -3.3194115085529226
epoch: 9, step: 17
	action: tensor([[ 0.2727, -0.3508, -0.2504, -0.0998, -0.4670,  0.6571,  0.2710]],
       dtype=torch.float64)
	q_value: tensor([[-0.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25545454486895647, distance: 0.9874208690207091 entropy -3.255670156934226
epoch: 9, step: 18
	action: tensor([[-0.8538,  0.1496, -0.6033,  0.1339, -0.3195,  0.3591,  0.4676]],
       dtype=torch.float64)
	q_value: tensor([[-0.3464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7293932519892241, distance: 1.5048859006692674 entropy -3.1452215754321995
epoch: 9, step: 19
	action: tensor([[ 0.1460, -0.0900, -0.0858,  0.1998, -0.1705,  0.4174,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-0.4226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.507319468287684, distance: 0.8032290259401449 entropy -3.5993501352431245
epoch: 9, step: 20
	action: tensor([[ 0.2484, -0.2990,  0.2987, -0.0224, -0.0215,  0.5187,  0.3619]],
       dtype=torch.float64)
	q_value: tensor([[-0.3095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4210361322388919, distance: 0.8707278264572795 entropy -3.298960516726868
epoch: 9, step: 21
	action: tensor([[-0.5868, -0.7591,  0.1525,  0.1551, -0.4437,  0.6958,  0.3887]],
       dtype=torch.float64)
	q_value: tensor([[-0.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7266888723399301, distance: 1.5037087898242798 entropy -3.1590447353053763
epoch: 9, step: 22
	action: tensor([[-0.2572, -0.2930,  0.3003,  0.2817,  0.2895,  0.6082,  0.4400]],
       dtype=torch.float64)
	q_value: tensor([[-0.4828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22414370608907364, distance: 1.0079693746689782 entropy -3.1049660803741945
epoch: 9, step: 23
	action: tensor([[ 0.1277, -0.4226,  0.2066,  0.3418, -0.1842,  0.6082,  0.2730]],
       dtype=torch.float64)
	q_value: tensor([[-0.4224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46469333709428784, distance: 0.8372555166760767 entropy -3.2406796313784905
epoch: 9, step: 24
	action: tensor([[0.9245, 0.4265, 0.3403, 0.3075, 0.6157, 0.5582, 0.4306]],
       dtype=torch.float64)
	q_value: tensor([[-0.3687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9506418345073704, distance: 0.254235503103453 entropy -3.1367286702087243
epoch: 9, step: 25
	action: tensor([[ 0.4011, -0.0108, -0.4330, -0.1561, -0.4527,  0.5889,  0.3485]],
       dtype=torch.float64)
	q_value: tensor([[-0.4608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5789277214673758, distance: 0.7425659286061393 entropy -3.098268628678606
epoch: 9, step: 26
	action: tensor([[-0.6013, -0.1297, -0.0475, -0.2192, -0.3472,  0.2384,  0.4009]],
       dtype=torch.float64)
	q_value: tensor([[-0.3193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6430436454879993, distance: 1.4668349299243144 entropy -3.212075576652251
epoch: 9, step: 27
	action: tensor([[ 0.4549, -0.0045, -0.0193,  0.2689,  0.4506,  0.4701,  0.2089]],
       dtype=torch.float64)
	q_value: tensor([[-0.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8554701042124464, distance: 0.4350463587363432 entropy -3.4378297863835248
epoch: 9, step: 28
	action: tensor([[-0.3637, -0.1168, -0.0668, -0.1524, -0.2808,  0.5806,  0.3161]],
       dtype=torch.float64)
	q_value: tensor([[-0.3965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22092623763496277, distance: 1.2644492602037571 entropy -3.2720485290723738
epoch: 9, step: 29
	action: tensor([[-0.0936, -0.0101,  0.1253,  0.1134,  0.2749,  0.6596,  0.2944]],
       dtype=torch.float64)
	q_value: tensor([[-0.3624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2644492602037571 entropy -3.311991665399033
epoch: 9, step: 30
	action: tensor([[ 0.1828, -0.0651,  0.4081,  0.3155, -0.1136,  0.2633, -0.2233]],
       dtype=torch.float64)
	q_value: tensor([[-0.5523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7047050058567326, distance: 0.6218487070692041 entropy -3.5086896264262317
epoch: 9, step: 31
	action: tensor([[ 0.4668, -0.0233,  0.0770, -0.1493, -0.0941,  0.6286,  0.4140]],
       dtype=torch.float64)
	q_value: tensor([[-0.3169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7243133634036312, distance: 0.6008479043424493 entropy -3.213746399369249
epoch: 9, step: 32
	action: tensor([[-0.1432,  0.0480, -0.2991,  0.2427, -0.2641,  0.6067,  0.3907]],
       dtype=torch.float64)
	q_value: tensor([[-0.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6008479043424493 entropy -3.1534374891428336
epoch: 9, step: 33
	action: tensor([[ 0.1290, -0.1629, -0.1215, -0.0293, -0.0852,  0.1717, -0.2233]],
       dtype=torch.float64)
	q_value: tensor([[-0.5523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976827215150454, distance: 0.9590105168482681 entropy -3.5086896264262317
epoch: 9, step: 34
	action: tensor([[-0.0244, -0.3320,  0.1535, -0.1186, -0.8206,  0.4608,  0.3567]],
       dtype=torch.float64)
	q_value: tensor([[-0.3056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03433089406046541, distance: 1.1638216760042897 entropy -3.3614494423696195
epoch: 9, step: 35
	action: tensor([[ 0.2601, -0.1355, -0.1267, -0.0134,  0.0391,  0.6273,  0.4532]],
       dtype=torch.float64)
	q_value: tensor([[-0.3244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5788427356621656, distance: 0.742640861560199 entropy -3.118503522679289
epoch: 9, step: 36
	action: tensor([[-0.5103, -0.0445,  0.0724, -0.5360,  0.0411,  0.7130,  0.3368]],
       dtype=torch.float64)
	q_value: tensor([[-0.3590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36897535175156393, distance: 1.33891962639254 entropy -3.236600493560939
epoch: 9, step: 37
	action: tensor([[-0.2098, -0.2334, -0.1515, -0.1797,  0.1072,  0.5920,  0.2220]],
       dtype=torch.float64)
	q_value: tensor([[-0.4258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01510490165094125, distance: 1.152954465468326 entropy -3.281561021455931
epoch: 9, step: 38
	action: tensor([[-0.3253, -0.1755, -0.0479,  0.3041,  0.0510,  0.3862,  0.2857]],
       dtype=torch.float64)
	q_value: tensor([[-0.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05878052782252474, distance: 1.1102023568558719 entropy -3.3379887012337144
epoch: 9, step: 39
	action: tensor([[ 0.2627, -0.0979, -0.0841, -0.3188, -0.0615,  0.5614,  0.2527]],
       dtype=torch.float64)
	q_value: tensor([[-0.3800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44438289417555776, distance: 0.8529910949239751 entropy -3.4027491513853065
epoch: 9, step: 40
	action: tensor([[-0.5636, -0.2862, -0.3011, -0.3650, -0.4376,  0.7149,  0.3591]],
       dtype=torch.float64)
	q_value: tensor([[-0.3336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.650819880172552, distance: 1.4703019675629625 entropy -3.2321282171132175
epoch: 9, step: 41
	action: tensor([[-0.3731, -0.3557,  0.1046, -0.0927,  0.0276,  0.5625,  0.2972]],
       dtype=torch.float64)
	q_value: tensor([[-0.4284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25904512992577056, distance: 1.2840364197459881 entropy -3.2961905047218614
epoch: 9, step: 42
	action: tensor([[ 0.1957, -0.1876,  0.0937,  0.1701,  0.2829,  0.4426,  0.3081]],
       dtype=torch.float64)
	q_value: tensor([[-0.3999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5759584857412896, distance: 0.7451794703919844 entropy -3.2767522272756797
epoch: 9, step: 43
	action: tensor([[-0.3074, -0.1908,  0.1289, -0.1536,  0.2579,  0.4614,  0.3097]],
       dtype=torch.float64)
	q_value: tensor([[-0.3754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10207495804543187, distance: 1.2013298280300853 entropy -3.2809779009260214
epoch: 9, step: 44
	action: tensor([[-0.4690, -0.2275, -0.2938,  0.1314, -0.2238,  0.3932,  0.2365]],
       dtype=torch.float64)
	q_value: tensor([[-0.3899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35343256179769567, distance: 1.331297154288473 entropy -3.3395715971162248
epoch: 9, step: 45
	action: tensor([[-0.0235, -0.0674, -0.3152, -0.0656,  0.2602,  0.4274,  0.2577]],
       dtype=torch.float64)
	q_value: tensor([[-0.3843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3143881330111694, distance: 0.9475362990176147 entropy -3.4515511492107933
epoch: 9, step: 46
	action: tensor([[-0.1449, -0.2682, -0.0210, -0.1561,  0.3203,  0.3515,  0.2194]],
       dtype=torch.float64)
	q_value: tensor([[-0.3727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019531063590716213, distance: 1.1554653450972545 entropy -3.4489378967380753
epoch: 9, step: 47
	action: tensor([[ 0.2821, -0.2045, -0.0439, -0.0261, -0.1281,  0.5866,  0.2523]],
       dtype=torch.float64)
	q_value: tensor([[-0.3824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5175282352873238, distance: 0.794863664440235 entropy -3.3855277058354774
epoch: 9, step: 48
	action: tensor([[ 0.3541, -0.6015, -0.0041,  0.3671, -0.2584,  0.6395,  0.4005]],
       dtype=torch.float64)
	q_value: tensor([[-0.3332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4745591776300927, distance: 0.8295042178888863 entropy -3.1983850450266353
epoch: 9, step: 49
	action: tensor([[ 0.0028, -0.3247,  0.1847,  0.0100, -0.3073,  0.5868,  0.4763]],
       dtype=torch.float64)
	q_value: tensor([[-0.3932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2105383557327677, distance: 1.0167687986872702 entropy -3.090429867850973
epoch: 9, step: 50
	action: tensor([[-0.9756,  0.0515, -0.1563, -0.0840,  0.5049,  0.6864,  0.3941]],
       dtype=torch.float64)
	q_value: tensor([[-0.3556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6209575873341386, distance: 1.4569428586843274 entropy -3.1573553113465507
epoch: 9, step: 51
	action: tensor([[ 0.3786, -0.0482,  0.0776, -0.1162,  0.0668,  0.5850,  0.0636]],
       dtype=torch.float64)
	q_value: tensor([[-0.5159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6771078304254269, distance: 0.6502576195183604 entropy -3.4371485290393617
epoch: 9, step: 52
	action: tensor([[ 0.2630,  0.0688, -0.1229, -0.4126,  0.1459,  0.4652,  0.3972]],
       dtype=torch.float64)
	q_value: tensor([[-0.3366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5335147932132125, distance: 0.7815839477729458 entropy -3.194820326214377
epoch: 9, step: 53
	action: tensor([[-0.1094, -0.0316,  0.0185, -0.0196, -0.2436,  0.5729,  0.2617]],
       dtype=torch.float64)
	q_value: tensor([[-0.3494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26482129246022235, distance: 0.981190090626523 entropy -3.3091732287719
epoch: 9, step: 54
	action: tensor([[ 0.4257, -0.0188, -0.1880, -0.3268,  0.0409,  0.4360,  0.3272]],
       dtype=torch.float64)
	q_value: tensor([[-0.3274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5876836985265116, distance: 0.7348047347585789 entropy -3.2758181747949426
epoch: 9, step: 55
	action: tensor([[ 0.2793,  0.2762, -0.2078, -0.3114, -0.4180,  0.5616,  0.3241]],
       dtype=torch.float64)
	q_value: tensor([[-0.3324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6612031682501469, distance: 0.6660799538647596 entropy -3.2771127610757644
epoch: 9, step: 56
	action: tensor([[ 0.0775, -0.0918,  0.0450,  0.1538,  0.0442,  0.5752,  0.3409]],
       dtype=torch.float64)
	q_value: tensor([[-0.3115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5438668354954549, distance: 0.7728630033599019 entropy -3.242930944355407
epoch: 9, step: 57
	action: tensor([[-0.0308,  0.1318,  0.0557, -0.1548, -0.0577,  0.4745,  0.3183]],
       dtype=torch.float64)
	q_value: tensor([[-0.3531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4003626783557106, distance: 0.886137307119737 entropy -3.2665558181155196
epoch: 9, step: 58
	action: tensor([[-0.0102,  0.1576, -0.0955, -0.2089, -0.2042,  0.4750,  0.2651]],
       dtype=torch.float64)
	q_value: tensor([[-0.3297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3895973528859267, distance: 0.8940563610208186 entropy -3.326518654090839
epoch: 9, step: 59
	action: tensor([[-0.1880,  0.0381, -0.0489, -0.1686,  0.0062,  0.6400,  0.2815]],
       dtype=torch.float64)
	q_value: tensor([[-0.3139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21355939019711923, distance: 1.0148214983620882 entropy -3.3384152422188778
epoch: 9, step: 60
	action: tensor([[ 0.6005, -0.4079, -0.0674, -0.1988,  0.1738,  0.5240,  0.2613]],
       dtype=torch.float64)
	q_value: tensor([[-0.3544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3999993994018044, distance: 0.8864056912584125 entropy -3.3207167272984113
epoch: 9, step: 61
	action: tensor([[ 0.3827, -0.3164, -0.1332, -0.1062, -0.3812,  0.3810,  0.4233]],
       dtype=torch.float64)
	q_value: tensor([[-0.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35672610242863323, distance: 0.917813957488061 entropy -3.162377020507931
epoch: 9, step: 62
	action: tensor([[-0.2985, -0.0947, -0.3349, -0.1771,  0.0806,  0.7990,  0.4147]],
       dtype=torch.float64)
	q_value: tensor([[-0.3107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020037691432295834, distance: 1.1328212296752656 entropy -3.197102296190455
epoch: 9, step: 63
	action: tensor([[-0.0588, -0.1719,  0.0543, -0.1303, -0.0257,  0.4518,  0.2365]],
       dtype=torch.float64)
	q_value: tensor([[-0.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16683040903900137, distance: 1.0445359664415643 entropy -3.348906970445504
LOSS epoch 9 actor 58.014308626655755 critic 623.5287783199536
epoch: 10, step: 0
	action: tensor([[ 0.1182, -0.9123,  0.0946, -0.8423, -0.5921,  0.2441,  0.7938]],
       dtype=torch.float64)
	q_value: tensor([[-0.4637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8481573693709528, distance: 1.5557011309904865 entropy -2.9615375795778
epoch: 10, step: 1
	action: tensor([[ 0.2655, -0.8513,  0.0335, -0.4048,  0.4581,  0.6159,  1.1362]],
       dtype=torch.float64)
	q_value: tensor([[-0.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3066434269438345, distance: 1.3080828065034407 entropy -2.672137077188808
epoch: 10, step: 2
	action: tensor([[ 0.0370, -0.1775, -0.0935,  0.1153,  0.4304,  0.6051,  0.9741]],
       dtype=torch.float64)
	q_value: tensor([[-0.7122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4653096622502526, distance: 0.8367733909259509 entropy -2.7610686543497667
epoch: 10, step: 3
	action: tensor([[ 0.1559, -0.3022, -0.3891, -0.3699,  0.2929,  0.4755,  0.7477]],
       dtype=torch.float64)
	q_value: tensor([[-0.6155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20017742740921407, distance: 1.0234191086354323 entropy -2.9453436298156817
epoch: 10, step: 4
	action: tensor([[-0.7114, -0.2714, -0.3595,  0.4687, -0.2362,  0.5342,  0.7342]],
       dtype=torch.float64)
	q_value: tensor([[-0.5672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.572237183939702, distance: 1.4348804713498764 entropy -2.999868427818002
epoch: 10, step: 5
	action: tensor([[-0.9413, -0.1849,  0.2623,  0.5373,  0.5530,  0.6383,  0.7364]],
       dtype=torch.float64)
	q_value: tensor([[-0.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25266878141248017, distance: 1.2807808349907237 entropy -3.0156979295208513
epoch: 10, step: 6
	action: tensor([[ 0.0016, -0.4420,  0.5692,  0.1959, -0.3877,  0.7662,  0.7149]],
       dtype=torch.float64)
	q_value: tensor([[-0.7787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37073979621066433, distance: 0.9077616402027149 entropy -2.9309376676110697
epoch: 10, step: 7
	action: tensor([[ 1.3627,  0.2234, -0.0433, -0.5935,  0.2353,  0.7678,  1.1113]],
       dtype=torch.float64)
	q_value: tensor([[-0.5699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6168024003098389, distance: 0.7083829233108304 entropy -2.6748930706679594
epoch: 10, step: 8
	action: tensor([[ 0.1561, -0.9504,  0.3554, -1.0762, -1.0272,  1.1440,  1.1742]],
       dtype=torch.float64)
	q_value: tensor([[-0.6979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6955240882725213, distance: 1.4900768768824615 entropy -2.6057750716777393
epoch: 10, step: 9
	action: tensor([[-0.4342,  0.2956, -0.4789, -0.8104,  0.4987,  1.3693,  1.5997]],
       dtype=torch.float64)
	q_value: tensor([[-0.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4900768768824615 entropy -2.377356186262144
epoch: 10, step: 10
	action: tensor([[-0.3189,  0.0886, -0.0053,  0.0259,  0.2436,  0.3853,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-0.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0875975182960792, distance: 1.0930749009630731 entropy -3.405058221698418
epoch: 10, step: 11
	action: tensor([[-0.3449, -0.3612,  0.0785, -0.1324, -0.2123,  0.4386,  0.6357]],
       dtype=torch.float64)
	q_value: tensor([[-0.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3930173428923105, distance: 1.3506255309507713 entropy -3.117373937082243
epoch: 10, step: 12
	action: tensor([[ 0.5560, -0.2211, -0.4595,  0.1252, -0.2985,  0.5719,  0.8207]],
       dtype=torch.float64)
	q_value: tensor([[-0.5450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6217702051977353, distance: 0.7037761770937652 entropy -2.9240419376673943
epoch: 10, step: 13
	action: tensor([[-0.2604, -0.6651,  0.0615, -0.1709, -0.2128,  0.8484,  0.9716]],
       dtype=torch.float64)
	q_value: tensor([[-0.5080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40556449956683793, distance: 1.3566945583985088 entropy -2.823661923924088
epoch: 10, step: 14
	action: tensor([[ 0.2567, -0.4616,  0.1587, -0.0801,  0.4456,  0.8380,  1.0418]],
       dtype=torch.float64)
	q_value: tensor([[-0.6766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38442722126825457, distance: 0.8978347213923928 entropy -2.7327523871391937
epoch: 10, step: 15
	action: tensor([[-0.3419, -0.8139,  0.0663,  0.0242, -0.4572,  0.4894,  0.9393]],
       dtype=torch.float64)
	q_value: tensor([[-0.6695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.672107478193311, distance: 1.4797514965254488 entropy -2.7707125910884867
epoch: 10, step: 16
	action: tensor([[-0.4050, -0.7089, -0.8326, -0.4736, -0.0032,  0.4325,  1.0294]],
       dtype=torch.float64)
	q_value: tensor([[-0.6654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6410224590360376, distance: 1.465932440442008 entropy -2.7593603986559363
epoch: 10, step: 17
	action: tensor([[-0.1248, -0.3136, -0.3241, -0.6215,  0.1541,  0.5019,  0.7328]],
       dtype=torch.float64)
	q_value: tensor([[-0.7464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17617209534523082, distance: 1.241058144657632 entropy -3.016509457854035
epoch: 10, step: 18
	action: tensor([[ 0.7957, -0.0713, -0.0624,  0.5707,  0.5702,  0.7179,  0.7322]],
       dtype=torch.float64)
	q_value: tensor([[-0.5846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9899832389243421, distance: 0.11453028745113993 entropy -2.9932905845731947
epoch: 10, step: 19
	action: tensor([[-0.3628, -0.1839, -0.0218, -0.3993, -0.5330,  0.6105,  0.9345]],
       dtype=torch.float64)
	q_value: tensor([[-0.6867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43877253037599373, distance: 1.3726277101030802 entropy -2.7909240473282715
epoch: 10, step: 20
	action: tensor([[-0.1591, -0.2245,  0.0264,  0.3393, -0.3068,  0.6680,  0.8985]],
       dtype=torch.float64)
	q_value: tensor([[-0.5956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22574222645332276, distance: 1.0069304642596228 entropy -2.8439849714153858
epoch: 10, step: 21
	action: tensor([[-0.0699,  0.0893, -0.2142,  0.2378,  0.3039,  0.7389,  0.9055]],
       dtype=torch.float64)
	q_value: tensor([[-0.5758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0069304642596228 entropy -2.8470533882469136
epoch: 10, step: 22
	action: tensor([[ 0.1019, -0.2832, -0.0404, -0.0397, -0.2964,  0.3523,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-0.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21299650243240975, distance: 1.015184608062267 entropy -3.405058221698418
epoch: 10, step: 23
	action: tensor([[-0.7989, -0.3082, -0.4060, -0.9161,  0.2291,  0.6698,  0.8660]],
       dtype=torch.float64)
	q_value: tensor([[-0.4228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7191215105704196, distance: 1.5004101054274641 entropy -2.9175735247999697
epoch: 10, step: 24
	action: tensor([[-0.5654, -0.2779, -0.1487, -0.2505, -0.2332,  0.6912,  0.6807]],
       dtype=torch.float64)
	q_value: tensor([[-0.7596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5652121360500051, distance: 1.4316712261733193 entropy -2.9986837121283374
epoch: 10, step: 25
	action: tensor([[ 1.0048, -0.2950,  0.1439, -0.2330, -0.0277,  0.5901,  0.8187]],
       dtype=torch.float64)
	q_value: tensor([[-0.6111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.520439802106161, distance: 0.7924616578013769 entropy -2.926784135633973
epoch: 10, step: 26
	action: tensor([[ 0.1999, -0.6640, -0.2417, -0.3385,  0.0809,  0.9994,  1.1132]],
       dtype=torch.float64)
	q_value: tensor([[-0.5709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06734162206316441, distance: 1.1051417631909353 entropy -2.6836928326605403
epoch: 10, step: 27
	action: tensor([[-0.4362, -0.5311, -0.8006, -1.2877, -0.6328,  1.0700,  1.0614]],
       dtype=torch.float64)
	q_value: tensor([[-0.6978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3821535121396695, distance: 1.3453486091843068 entropy -2.7121545584250493
epoch: 10, step: 28
	action: tensor([[ 0.0766, -0.5236,  0.1076, -0.6949,  0.6281,  0.8069,  1.0711]],
       dtype=torch.float64)
	q_value: tensor([[-0.8529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12359834008900528, distance: 1.2130040132445705 entropy -2.708485942093812
epoch: 10, step: 29
	action: tensor([[ 1.2382, -0.7251,  0.3076,  0.3632, -0.2273,  0.9006,  0.9128]],
       dtype=torch.float64)
	q_value: tensor([[-0.7493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4732825302709581, distance: 0.8305113169369038 entropy -2.7771449121160523
epoch: 10, step: 30
	action: tensor([[ 0.5502, -0.5938,  0.5467, -0.2070, -0.0286,  1.0647,  1.4138]],
       dtype=torch.float64)
	q_value: tensor([[-0.6473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4105204425864102, distance: 0.8785997344310843 entropy -2.4901715770887063
epoch: 10, step: 31
	action: tensor([[ 1.2719, -0.1056,  0.1356,  0.4316, -0.2248,  0.8690,  1.2754]],
       dtype=torch.float64)
	q_value: tensor([[-0.7023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8590088422216399, distance: 0.42968741278402073 entropy -2.5365333121599143
epoch: 10, step: 32
	action: tensor([[ 0.4810,  0.3792,  0.3164, -0.2666, -0.0183,  0.9475,  1.3144]],
       dtype=torch.float64)
	q_value: tensor([[-0.6749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9403307561998168, distance: 0.27953227576343315 entropy -2.5410006681807404
epoch: 10, step: 33
	action: tensor([[-0.4193, -0.0984, -0.2865, -0.3224,  0.1111,  0.9330,  1.0444]],
       dtype=torch.float64)
	q_value: tensor([[-0.6386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.27953227576343315 entropy -2.6784609646912614
epoch: 10, step: 34
	action: tensor([[ 0.0912,  0.0819,  0.0932,  0.2301, -0.0045,  0.4040,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-0.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6095890467687435, distance: 0.7150191773469229 entropy -3.405058221698418
epoch: 10, step: 35
	action: tensor([[ 0.2373, -0.8544, -0.4010, -0.2219,  0.2991,  0.5015,  0.7699]],
       dtype=torch.float64)
	q_value: tensor([[-0.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7150191773469229 entropy -2.989605966533264
epoch: 10, step: 36
	action: tensor([[ 0.0363,  0.2732,  0.1635,  0.1039, -0.1711,  0.3516,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-0.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6148034222480603, distance: 0.7102281853948978 entropy -3.405058221698418
epoch: 10, step: 37
	action: tensor([[-0.1319,  0.2817,  0.3044, -0.1158,  0.1006,  0.5776,  0.7692]],
       dtype=torch.float64)
	q_value: tensor([[-0.4206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7102281853948978 entropy -2.9845351406328264
epoch: 10, step: 38
	action: tensor([[ 0.1113, -0.4734,  0.0305,  0.1959,  0.3352,  0.3352,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-0.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2561237458892044, distance: 0.9869770199622566 entropy -3.405058221698418
epoch: 10, step: 39
	action: tensor([[ 0.2990, -0.4053,  0.1539, -0.1819, -0.0584,  0.4516,  0.8232]],
       dtype=torch.float64)
	q_value: tensor([[-0.5465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23558782971846692, distance: 1.0005078252227877 entropy -2.9523958751283357
epoch: 10, step: 40
	action: tensor([[ 0.3347, -0.4440,  0.0879, -0.4321, -0.3837,  0.8089,  0.9249]],
       dtype=torch.float64)
	q_value: tensor([[-0.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14482268308119617, distance: 1.058241477873552 entropy -2.82529166747894
epoch: 10, step: 41
	action: tensor([[ 0.5594, -0.4658,  0.3345, -0.4579,  0.4850,  0.7564,  1.1315]],
       dtype=torch.float64)
	q_value: tensor([[-0.5785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2086096487421989, distance: 1.018010057728644 entropy -2.667881234160762
epoch: 10, step: 42
	action: tensor([[ 0.0894, -0.1635, -0.1706,  0.1738,  0.0865,  0.7999,  1.0159]],
       dtype=torch.float64)
	q_value: tensor([[-0.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5299137927122489, distance: 0.7845948401398803 entropy -2.706217635283682
epoch: 10, step: 43
	action: tensor([[ 0.9898,  0.0120,  0.0346, -0.1562,  0.0555,  0.5820,  0.8719]],
       dtype=torch.float64)
	q_value: tensor([[-0.5961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8089692940376569, distance: 0.5001592252225464 entropy -2.8605402768660513
epoch: 10, step: 44
	action: tensor([[ 0.5056, -0.5559, -0.0144, -0.7224,  0.9571,  0.7045,  1.0365]],
       dtype=torch.float64)
	q_value: tensor([[-0.5776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07100628704942036, distance: 1.1842753871002918 entropy -2.7319764188280664
epoch: 10, step: 45
	action: tensor([[-0.3022, -0.0670, -0.0666, -0.2002,  0.1444,  0.8153,  0.9238]],
       dtype=torch.float64)
	q_value: tensor([[-0.7741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08290086874400626, distance: 1.0958846259305954 entropy -2.7706007658301957
epoch: 10, step: 46
	action: tensor([[ 0.8831, -0.2549,  0.0080, -0.2222, -0.0347,  0.6695,  0.7887]],
       dtype=torch.float64)
	q_value: tensor([[-0.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6145444082647513, distance: 0.7104669311781271 entropy -2.911054432484247
epoch: 10, step: 47
	action: tensor([[ 0.3593, -0.6127, -0.0398, -0.0346, -0.5582,  1.2797,  1.0787]],
       dtype=torch.float64)
	q_value: tensor([[-0.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32922652381238293, distance: 0.937226660135416 entropy -2.710976506497935
epoch: 10, step: 48
	action: tensor([[-0.6687, -0.2279, -0.0704, -0.9392,  0.0798,  0.7429,  1.3602]],
       dtype=torch.float64)
	q_value: tensor([[-0.6752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.720849394119554, distance: 1.5011639447612346 entropy -2.5346722085119753
epoch: 10, step: 49
	action: tensor([[-0.5183, -0.3918,  0.1079,  0.0935, -0.1806,  0.5618,  0.8129]],
       dtype=torch.float64)
	q_value: tensor([[-0.8060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39295457915395193, distance: 1.3505951037408193 entropy -2.843308175127357
epoch: 10, step: 50
	action: tensor([[ 1.2624, -0.2720,  0.3470, -0.0638, -0.2077,  0.9198,  0.8466]],
       dtype=torch.float64)
	q_value: tensor([[-0.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5997866479098961, distance: 0.7239398443923719 entropy -2.8927250266076485
epoch: 10, step: 51
	action: tensor([[0.6713, 0.1082, 0.3903, 0.4473, 0.0589, 0.9274, 1.3628]],
       dtype=torch.float64)
	q_value: tensor([[-0.6113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08706690602957493 entropy -2.519535773911424
epoch: 10, step: 52
	action: tensor([[ 0.1983, -0.4432, -0.3252, -0.3911, -0.0162,  0.2470,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-0.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03852425701880535, distance: 1.1661784606202388 entropy -3.405058221698418
epoch: 10, step: 53
	action: tensor([[-0.6762, -0.1283, -0.5233, -0.2906, -0.2172,  0.6735,  0.8100]],
       dtype=torch.float64)
	q_value: tensor([[-0.4802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6504333559097295, distance: 1.4701298286576694 entropy -2.9793010986320163
epoch: 10, step: 54
	action: tensor([[-0.1923,  0.1206,  0.0355, -0.7525, -0.1906,  0.5013,  0.7040]],
       dtype=torch.float64)
	q_value: tensor([[-0.6494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07120797344989915, distance: 1.1843868901787937 entropy -3.0344610615821854
epoch: 10, step: 55
	action: tensor([[-0.0769, -0.4396,  0.5155, -0.1875,  0.3826,  0.4104,  0.7736]],
       dtype=torch.float64)
	q_value: tensor([[-0.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12863367522165237, distance: 1.2157189754223319 entropy -2.934366881297243
epoch: 10, step: 56
	action: tensor([[ 0.0140, -0.1015, -0.1534, -0.3207, -0.2454,  0.9646,  0.8484]],
       dtype=torch.float64)
	q_value: tensor([[-0.6136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2698076041977324, distance: 0.9778569937319533 entropy -2.8498915972149668
epoch: 10, step: 57
	action: tensor([[-0.0762, -0.3358, -0.2145,  0.1962,  0.0311,  0.8394,  0.9751]],
       dtype=torch.float64)
	q_value: tensor([[-0.5659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24469229532754255, distance: 0.9945317461944898 entropy -2.7821541987949905
epoch: 10, step: 58
	action: tensor([[-0.4501, -0.2061, -0.3772,  0.2775,  0.9177,  0.3920,  0.8946]],
       dtype=torch.float64)
	q_value: tensor([[-0.6301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1666213597436228, distance: 1.236009062715105 entropy -2.850253644704715
epoch: 10, step: 59
	action: tensor([[-0.1473, -0.0423, -0.1467, -0.7877, -0.0853,  0.6083,  0.5515]],
       dtype=torch.float64)
	q_value: tensor([[-0.7608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06358352812636392, distance: 1.1801643585664165 entropy -3.114752890281338
epoch: 10, step: 60
	action: tensor([[ 0.2441, -0.2387, -0.0980,  0.1354,  0.1977,  0.6476,  0.7882]],
       dtype=torch.float64)
	q_value: tensor([[-0.5593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5801811719329539, distance: 0.741459867521784 entropy -2.933148792822728
epoch: 10, step: 61
	action: tensor([[ 0.9605, -0.0803,  0.0712, -1.0975, -0.4240,  0.7305,  0.8445]],
       dtype=torch.float64)
	q_value: tensor([[-0.5602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30602537596239743, distance: 0.9532975753184345 entropy -2.891880360439931
epoch: 10, step: 62
	action: tensor([[ 1.0346, -0.1895, -0.8793,  0.3236, -0.0217,  0.9647,  1.2622]],
       dtype=torch.float64)
	q_value: tensor([[-0.6576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8582542052597388, distance: 0.4308358013740388 entropy -2.5776771141923236
epoch: 10, step: 63
	action: tensor([[-0.9149, -0.1956, -0.7695, -0.4213, -0.0376,  0.9168,  1.1747]],
       dtype=torch.float64)
	q_value: tensor([[-0.6881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0073915224928887, distance: 1.6213349184356856 entropy -2.658010789848846
LOSS epoch 10 actor 174.91043659289585 critic 2012.3872242623713
epoch: 11, step: 0
	action: tensor([[ 0.0671, -0.1523, -0.6891, -0.2831,  0.8211,  0.9142,  1.8508]],
       dtype=torch.float64)
	q_value: tensor([[-1.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44826865518570413, distance: 0.850003124468266 entropy -2.389077705098155
epoch: 11, step: 1
	action: tensor([[ 1.7696,  0.6699, -0.1347,  1.8500,  1.2201,  0.9530,  2.2442]],
       dtype=torch.float64)
	q_value: tensor([[-1.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.850003124468266 entropy -2.2090792637448584
epoch: 11, step: 2
	action: tensor([[-0.5778, -0.4391,  0.1598,  0.2323,  0.4341,  0.4437,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4191427347595369, distance: 1.3632318755626984 entropy -2.885192186300056
epoch: 11, step: 3
	action: tensor([[-0.5462,  0.0633, -0.1239, -0.9462,  0.4043,  0.9279,  1.6436]],
       dtype=torch.float64)
	q_value: tensor([[-0.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27764780687346713, distance: 1.2934876013760892 entropy -2.4631654230454174
epoch: 11, step: 4
	action: tensor([[-0.6786,  0.2472, -0.1677, -0.0625, -0.4198,  1.2951,  2.1695]],
       dtype=torch.float64)
	q_value: tensor([[-1.1578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22939623844800128, distance: 1.2688276474921667 entropy -2.219664410364882
epoch: 11, step: 5
	action: tensor([[ 1.7035, -1.0654, -1.0094,  0.4346,  1.1631,  1.2180,  2.6551]],
       dtype=torch.float64)
	q_value: tensor([[-1.3096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2688276474921667 entropy -2.075967814438976
epoch: 11, step: 6
	action: tensor([[-0.1939,  0.2373,  0.2016, -0.2606, -0.0455,  0.4488,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18881966760054125, distance: 1.030659947985967 entropy -2.885192186300056
epoch: 11, step: 7
	action: tensor([[-0.6949, -0.5241,  0.3879, -0.5691, -0.6029,  1.0218,  1.6253]],
       dtype=torch.float64)
	q_value: tensor([[-0.7083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.030659947985967 entropy -2.465826164552517
epoch: 11, step: 8
	action: tensor([[ 0.6034, -0.0033, -0.0336,  0.1563,  0.4293,  0.4064,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8635278649400024, distance: 0.422745201073466 entropy -2.885192186300056
epoch: 11, step: 9
	action: tensor([[ 0.7323, -0.5346, -0.3338,  0.1378, -0.1064,  0.5155,  1.7839]],
       dtype=torch.float64)
	q_value: tensor([[-0.7779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.422745201073466 entropy -2.396897699887325
epoch: 11, step: 10
	action: tensor([[-0.4555, -0.3939,  0.0390, -0.0097, -0.1202,  0.5819,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43194995906157607, distance: 1.3693693845061423 entropy -2.885192186300056
epoch: 11, step: 11
	action: tensor([[-0.7874, -0.6885, -0.7839, -0.0733,  0.7223,  0.7295,  1.7655]],
       dtype=torch.float64)
	q_value: tensor([[-0.8222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9424900295488743, distance: 1.5949096745308988 entropy -2.4120965233588443
epoch: 11, step: 12
	action: tensor([[ 0.9704, -0.4508, -1.0638, -0.5272,  0.1423,  1.1807,  2.0542]],
       dtype=torch.float64)
	q_value: tensor([[-1.3895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44849802781508963, distance: 0.8498264191557388 entropy -2.291956036790661
epoch: 11, step: 13
	action: tensor([[ 1.3117, -0.4698, -0.6151, -0.3799, -0.9622,  1.6744,  3.1318]],
       dtype=torch.float64)
	q_value: tensor([[-1.1940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3730279437340599, distance: 0.9061097129203315 entropy -1.9509689871915188
epoch: 11, step: 14
	action: tensor([[-0.6260, -1.9322,  1.2459,  0.8963,  2.7588,  2.6807,  4.5054]],
       dtype=torch.float64)
	q_value: tensor([[-1.5243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9061097129203315 entropy -1.6447568718373264
epoch: 11, step: 15
	action: tensor([[-0.7837, -0.2037, -0.1612,  0.3435, -0.3679,  0.4824,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6754998984584284, distance: 1.481251817365956 entropy -2.885192186300056
epoch: 11, step: 16
	action: tensor([[-0.1793, -0.9502,  0.1861, -1.7672,  0.0064,  1.1960,  1.6544]],
       dtype=torch.float64)
	q_value: tensor([[-0.8882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7168339954918004, distance: 1.4994115275985327 entropy -2.479991921893272
epoch: 11, step: 17
	action: tensor([[-1.2456, -1.1069,  0.0803,  0.5538, -0.2543,  1.7750,  3.2096]],
       dtype=torch.float64)
	q_value: tensor([[-1.4027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7092326177020698, distance: 1.4960884814015256 entropy -1.8995382419724778
epoch: 11, step: 18
	action: tensor([[-1.7383,  0.2321,  0.9668,  1.4504, -0.3489,  2.0690,  3.8444]],
       dtype=torch.float64)
	q_value: tensor([[-1.6880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4960884814015256 entropy -1.7554374520224327
epoch: 11, step: 19
	action: tensor([[ 0.7335, -0.4666,  0.2244,  0.3019, -0.8042,  0.5588,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.657757764070136, distance: 0.6694582463583453 entropy -2.885192186300056
epoch: 11, step: 20
	action: tensor([[ 0.7102, -0.6272, -0.0966, -0.8329, -1.5496,  1.0900,  2.5760]],
       dtype=torch.float64)
	q_value: tensor([[-0.7102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24920163666963324, distance: 1.279007130089188 entropy -2.101979575382173
epoch: 11, step: 21
	action: tensor([[-5.0805, -0.8509,  0.1888, -2.6717,  1.1445,  1.7202,  3.9857]],
       dtype=torch.float64)
	q_value: tensor([[-1.3813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.279007130089188 entropy -1.7423670509174045
epoch: 11, step: 22
	action: tensor([[ 0.0539, -0.4436, -0.3822, -0.3151, -0.6512,  0.5128,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16448533102371465, distance: 1.234877007125891 entropy -2.885192186300056
epoch: 11, step: 23
	action: tensor([[ 1.9660, -0.9025, -0.0107, -0.3995,  0.9832,  0.6876,  2.0380]],
       dtype=torch.float64)
	q_value: tensor([[-0.7757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.234877007125891 entropy -2.30887493707654
epoch: 11, step: 24
	action: tensor([[-0.5816, -0.6074,  0.2470,  0.4613, -0.0901,  0.4873,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37920719326523455, distance: 1.3439139130276079 entropy -2.885192186300056
epoch: 11, step: 25
	action: tensor([[ 1.1022, -0.3382, -0.3145, -0.8245,  0.5864,  1.5777,  1.8687]],
       dtype=torch.float64)
	q_value: tensor([[-0.9234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44527748812676826, distance: 0.8523041217631802 entropy -2.3645818292784466
epoch: 11, step: 26
	action: tensor([[-1.4131, -2.0428,  0.5646,  0.3784, -0.6562,  1.4742,  3.3625]],
       dtype=torch.float64)
	q_value: tensor([[-1.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8523041217631802 entropy -1.8684784987041325
epoch: 11, step: 27
	action: tensor([[ 0.1219, -0.1304, -0.0317, -0.2272,  0.7283,  0.5412,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4042312958728568, distance: 0.8832741819317743 entropy -2.885192186300056
epoch: 11, step: 28
	action: tensor([[ 0.2442,  0.1006,  0.4060,  0.1308, -1.4572,  0.6817,  1.6920]],
       dtype=torch.float64)
	q_value: tensor([[-0.8352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6186293080987757, distance: 0.7066922858360272 entropy -2.428826064609982
epoch: 11, step: 29
	action: tensor([[ 0.5888, -0.8435,  0.6592,  0.8002, -0.1443,  1.3830,  2.9583]],
       dtype=torch.float64)
	q_value: tensor([[-0.9707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7769766183183641, distance: 0.5404205954150265 entropy -1.9908633462513425
epoch: 11, step: 30
	action: tensor([[-2.8534,  0.6666,  0.5948,  1.6729,  0.6557,  1.9727,  3.9769]],
       dtype=torch.float64)
	q_value: tensor([[-1.5089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5404205954150265 entropy -1.7279525146731827
epoch: 11, step: 31
	action: tensor([[-4.7077e-02, -1.2330e-01,  1.9990e-02,  2.9034e-01, -1.1334e-04,
          5.3818e-01,  8.8071e-01]], dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43447064488846354, distance: 0.8605661706642317 entropy -2.885192186300056
epoch: 11, step: 32
	action: tensor([[ 0.7121, -0.8914,  0.3427,  0.3824, -0.1001,  0.8132,  1.6994]],
       dtype=torch.float64)
	q_value: tensor([[-0.7359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4485669808821048, distance: 0.849773291539973 entropy -2.442937695561806
epoch: 11, step: 33
	action: tensor([[-1.1774, -0.4782, -0.0228,  0.1000, -0.0363,  0.6241,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3159531197203895, distance: 1.7414923670032294 entropy -2.885192186300056
epoch: 11, step: 34
	action: tensor([[-8.8865e-04, -6.5961e-01, -2.9777e-01, -1.6205e-01,  1.5829e-01,
          8.7570e-01,  1.7793e+00]], dtype=torch.float64)
	q_value: tensor([[-1.0793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08871115622931236, distance: 1.1940239275478506 entropy -2.408769967942573
epoch: 11, step: 35
	action: tensor([[-0.6686,  0.2157,  0.0679, -1.6900,  1.2810,  1.2492,  2.4113]],
       dtype=torch.float64)
	q_value: tensor([[-1.1215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043835079834938506, distance: 1.1691564697419272 entropy -2.1562049592484938
epoch: 11, step: 36
	action: tensor([[ 0.2761,  1.5331,  1.1760, -0.3800,  1.1246,  1.8070,  2.9201]],
       dtype=torch.float64)
	q_value: tensor([[-1.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1691564697419272 entropy -1.9592824240209392
epoch: 11, step: 37
	action: tensor([[ 0.5579, -0.2946,  0.2995, -0.4066, -0.2313,  0.3490,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2869616701560509, distance: 0.9663025628846296 entropy -2.885192186300056
epoch: 11, step: 38
	action: tensor([[-0.3645, -0.2121, -0.6185,  0.1608,  0.6255,  1.3992,  2.1299]],
       dtype=torch.float64)
	q_value: tensor([[-0.7147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19971087667221687, distance: 1.0237175544069406 entropy -2.2491847032796324
epoch: 11, step: 39
	action: tensor([[-2.0067, -0.3070,  0.0558,  0.3588,  0.2878,  1.2841,  2.6015]],
       dtype=torch.float64)
	q_value: tensor([[-1.3944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0237175544069406 entropy -2.0890660427421595
epoch: 11, step: 40
	action: tensor([[ 0.1953, -0.2430,  0.0033,  0.3143, -0.9207,  0.4152,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45136375904060577, distance: 0.8476155965601716 entropy -2.885192186300056
epoch: 11, step: 41
	action: tensor([[-0.5829, -0.6491, -0.5802, -0.4098, -0.6522,  1.0470,  2.1502]],
       dtype=torch.float64)
	q_value: tensor([[-0.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8923810817753905, distance: 1.574203929492577 entropy -2.2623300267019375
epoch: 11, step: 42
	action: tensor([[ 1.2553, -1.1811,  0.2150,  0.9993,  0.7284,  1.3578,  2.7752]],
       dtype=torch.float64)
	q_value: tensor([[-1.3761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31096408348105253, distance: 0.9498994221553341 entropy -2.0518280175686803
epoch: 11, step: 43
	action: tensor([[-0.2910,  1.4573,  1.0117, -1.3618,  1.5782,  1.9520,  4.0237]],
       dtype=torch.float64)
	q_value: tensor([[-1.5816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9498994221553341 entropy -1.7183131810935317
epoch: 11, step: 44
	action: tensor([[ 0.8931, -0.9003,  0.0677, -0.1830, -0.0265,  0.5059,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1476625468896351, distance: 1.2259247069045751 entropy -2.885192186300056
epoch: 11, step: 45
	action: tensor([[ 0.8418, -1.1440, -0.1449,  0.4375,  0.0800,  1.6895,  2.4948]],
       dtype=torch.float64)
	q_value: tensor([[-0.7907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5269284865194699, distance: 0.7870822019327212 entropy -2.1217886988507835
epoch: 11, step: 46
	action: tensor([[-2.9850, -0.2992, -0.4151,  1.5222, -1.2056,  2.0706,  3.8915]],
       dtype=torch.float64)
	q_value: tensor([[-1.4478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7870822019327212 entropy -1.757208331861681
epoch: 11, step: 47
	action: tensor([[ 0.4804, -0.5641, -0.2031,  0.4309, -0.3469,  0.6430,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.525604974673906, distance: 0.7881824426001279 entropy -2.885192186300056
epoch: 11, step: 48
	action: tensor([[ 0.1321, -0.3885, -0.0051,  0.8435,  0.7708,  1.0542,  2.2426]],
       dtype=torch.float64)
	q_value: tensor([[-0.7590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8539707622381743, distance: 0.43729710518926634 entropy -2.2259099494307977
epoch: 11, step: 49
	action: tensor([[ 0.9886,  0.3719, -0.5128, -1.5645,  0.4316,  1.9000,  2.8008]],
       dtype=torch.float64)
	q_value: tensor([[-1.3735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.773348327821814, distance: 0.5447988184683061 entropy -2.019130108291001
epoch: 11, step: 50
	action: tensor([[-2.0924,  1.1512, -2.1975, -2.3457, -2.2197,  1.9691,  3.9536]],
       dtype=torch.float64)
	q_value: tensor([[-1.5778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5447988184683061 entropy -1.7296488956870089
epoch: 11, step: 51
	action: tensor([[ 0.0298, -0.4547,  0.7516, -0.1701,  0.1146,  0.5362,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02572110876142686, distance: 1.1295314817685265 entropy -2.885192186300056
epoch: 11, step: 52
	action: tensor([[ 1.1908, -0.5099,  0.4533, -1.0770,  1.5429,  1.3283,  2.0970]],
       dtype=torch.float64)
	q_value: tensor([[-0.8044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23443007308214292, distance: 1.2714226382900904 entropy -2.2475400221838044
epoch: 11, step: 53
	action: tensor([[ 1.0285,  0.0785, -1.0029, -0.2774,  2.6040,  2.1310,  3.6034]],
       dtype=torch.float64)
	q_value: tensor([[-1.4680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6307614795457439, distance: 0.6953607840167976 entropy -1.7923827497245841
epoch: 11, step: 54
	action: tensor([[-4.4514, -3.1033, -2.3553, -0.0146,  0.0997,  1.6668,  4.1597]],
       dtype=torch.float64)
	q_value: tensor([[-2.1148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6953607840167976 entropy -1.6761099922806983
epoch: 11, step: 55
	action: tensor([[-0.1914, -0.0751,  0.0455,  0.5570,  0.3273,  0.8526,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5586689378220091, distance: 0.7602193874765886 entropy -2.885192186300056
epoch: 11, step: 56
	action: tensor([[ 0.5439, -1.0966,  0.0767, -0.8587,  0.1410,  0.6200,  1.8268]],
       dtype=torch.float64)
	q_value: tensor([[-0.8991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7917801272293916, distance: 1.531789368744905 entropy -2.3708729855535418
epoch: 11, step: 57
	action: tensor([[-0.3100,  0.1410, -0.6433,  1.1214, -0.2877,  0.3902,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05344790844788494, distance: 1.174527597939265 entropy -2.885192186300056
epoch: 11, step: 58
	action: tensor([[ 1.2699, -0.3563,  0.0305, -0.8620, -0.2781,  1.2188,  1.6887]],
       dtype=torch.float64)
	q_value: tensor([[-0.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997335160718775, distance: 0.9576093186694739 entropy -2.467000219590638
epoch: 11, step: 59
	action: tensor([[ 0.3215, -0.3879,  0.0706,  0.4621, -0.4110,  0.3029,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-1.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5893849454251197, distance: 0.7332872390601316 entropy -2.885192186300056
epoch: 11, step: 60
	action: tensor([[-0.1500, -0.7353, -0.9878, -0.1806, -1.2343,  1.0601,  1.9587]],
       dtype=torch.float64)
	q_value: tensor([[-0.7015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6334482423345518, distance: 1.4625454875380595 entropy -2.3345144338681245
epoch: 11, step: 61
	action: tensor([[-0.0473, -1.1495, -0.1189, -0.6780, -0.3215,  0.7396,  3.0539]],
       dtype=torch.float64)
	q_value: tensor([[-1.2990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8681137381794137, distance: 1.5640777948413656 entropy -1.9846310306806956
epoch: 11, step: 62
	action: tensor([[ 0.3789, -0.2245,  1.1218, -1.0015, -0.7489,  1.1189,  3.4400]],
       dtype=torch.float64)
	q_value: tensor([[-1.6434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3426107924127687, distance: 0.9278290761611544 entropy -1.8538739536338544
epoch: 11, step: 63
	action: tensor([[-1.3020,  2.9277,  0.9268,  0.5644,  0.8012,  2.1209,  4.2824]],
       dtype=torch.float64)
	q_value: tensor([[-1.3158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9278290761611544 entropy -1.6663595354765133
LOSS epoch 11 actor 371.4038601766951 critic 2773.7712980014567
epoch: 12, step: 0
	action: tensor([[ 0.6102, -0.3890,  0.8783, -1.3120, -0.4696,  0.7102,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06098417563000336, distance: 1.1787213408374202 entropy -2.2539794868388894
epoch: 12, step: 1
	action: tensor([[-3.5067, -0.2732, -0.5753, -2.0326,  0.4744,  3.7990,  5.2677]],
       dtype=torch.float64)
	q_value: tensor([[-2.0162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1787213408374202 entropy -1.4796644338259257
epoch: 12, step: 2
	action: tensor([[-0.3787, -1.2723,  0.1908, -0.2542,  0.9078,  0.4610,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1787213408374202 entropy -2.2539794868388894
epoch: 12, step: 3
	action: tensor([[ 0.1788, -0.6324,  0.4010, -1.2500,  0.7467,  0.9407,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47000473721511904, distance: 1.3874459080771941 entropy -2.2539794868388894
epoch: 12, step: 4
	action: tensor([[0.0591, 0.8462, 1.5894, 1.0260, 3.3428, 3.0113, 4.9450]],
       dtype=torch.float64)
	q_value: tensor([[-2.1790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3874459080771941 entropy -1.5320170902396686
epoch: 12, step: 5
	action: tensor([[ 0.0496, -0.4432, -0.6405, -1.8428,  1.8124,  1.1313,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4832910937692567, distance: 0.8225828807719325 entropy -2.2539794868388894
epoch: 12, step: 6
	action: tensor([[-2.3773,  0.2289,  0.0983, -0.9633,  1.3554,  1.2686,  5.3460]],
       dtype=torch.float64)
	q_value: tensor([[-2.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8225828807719325 entropy -1.4729882289747593
epoch: 12, step: 7
	action: tensor([[-1.1674, -0.4564, -0.0730,  0.9770, -0.3760,  1.2658,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4325282064020075, distance: 1.3696458446842843 entropy -2.2539794868388894
epoch: 12, step: 8
	action: tensor([[ 0.8271, -0.2498, -1.4484,  2.5759, -2.0473,  1.0887,  4.5644]],
       dtype=torch.float64)
	q_value: tensor([[-2.3778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3696458446842843 entropy -1.618441974301897
epoch: 12, step: 9
	action: tensor([[-0.2808,  0.0232, -0.4327, -0.6570,  1.3630,  0.8459,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822292717792826, distance: 0.9695039098470724 entropy -2.2539794868388894
epoch: 12, step: 10
	action: tensor([[ 1.8743,  0.7887, -0.5032, -1.1674, -1.2899,  0.8508,  4.1576]],
       dtype=torch.float64)
	q_value: tensor([[-2.1459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9695039098470724 entropy -1.6845909596354183
epoch: 12, step: 11
	action: tensor([[-2.2511, -0.3352,  0.0124,  0.2001,  0.4063,  0.5905,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9695039098470724 entropy -2.2539794868388894
epoch: 12, step: 12
	action: tensor([[ 1.2935, -1.0410, -0.6875, -1.7166,  0.4243,  0.8836,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7266377334376157, distance: 1.503686522180259 entropy -2.2539794868388894
epoch: 12, step: 13
	action: tensor([[ 6.5460, -3.0287,  0.2061,  1.0171,  2.9517,  2.6642,  5.7966]],
       dtype=torch.float64)
	q_value: tensor([[-2.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.503686522180259 entropy -1.4130923626573668
epoch: 12, step: 14
	action: tensor([[ 0.0711, -0.9188, -0.2729, -0.4278,  0.1983,  0.5743,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4967479354372959, distance: 1.400009643410495 entropy -2.2539794868388894
epoch: 12, step: 15
	action: tensor([[ 2.2735, -0.0057, -2.2222, -1.3063,  1.8382,  2.0184,  4.1510]],
       dtype=torch.float64)
	q_value: tensor([[-1.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.400009643410495 entropy -1.6929408682991112
epoch: 12, step: 16
	action: tensor([[-0.8470, -0.3564,  0.1431, -1.3575, -0.7154,  0.3806,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0944486125551944, distance: 1.6561190315515462 entropy -2.2539794868388894
epoch: 12, step: 17
	action: tensor([[-0.9271, -0.9088, -1.9118,  1.7914, -1.7146,  2.9670,  4.2625]],
       dtype=torch.float64)
	q_value: tensor([[-2.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6561190315515462 entropy -1.6680183256571468
epoch: 12, step: 18
	action: tensor([[-1.0682, -0.2624,  0.2066,  0.2725,  1.2476,  0.8187,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6822862941122168, distance: 1.484248595378312 entropy -2.2539794868388894
epoch: 12, step: 19
	action: tensor([[-0.3051, -1.8908,  1.0811, -0.3528, -0.0939,  1.7479,  4.1625]],
       dtype=torch.float64)
	q_value: tensor([[-2.3569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.484248595378312 entropy -1.6824258940436576
epoch: 12, step: 20
	action: tensor([[-0.7968, -1.0059,  0.5398,  0.3914, -0.2482,  0.3316,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8823969169020665, distance: 1.5700457032800195 entropy -2.2539794868388894
epoch: 12, step: 21
	action: tensor([[ 3.8476,  0.2201, -1.2956, -1.1876, -0.8420,  2.8379,  4.1957]],
       dtype=torch.float64)
	q_value: tensor([[-2.0719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5700457032800195 entropy -1.678508536582164
epoch: 12, step: 22
	action: tensor([[-1.7004, -1.0341, -0.6704,  0.4246, -0.0875,  0.9311,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5700457032800195 entropy -2.2539794868388894
epoch: 12, step: 23
	action: tensor([[-0.7035,  0.0128, -0.4098, -1.7469, -1.8121,  0.7714,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4397635111275511, distance: 1.3731003398857091 entropy -2.2539794868388894
epoch: 12, step: 24
	action: tensor([[ 4.7482,  2.0412,  1.4436, -0.2337,  1.4561,  2.0902,  5.0843]],
       dtype=torch.float64)
	q_value: tensor([[-2.3986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3731003398857091 entropy -1.5283224193052471
epoch: 12, step: 25
	action: tensor([[-0.2173,  0.6071,  0.0524, -0.0610, -1.0701,  0.7482,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3731003398857091 entropy -2.2539794868388894
epoch: 12, step: 26
	action: tensor([[-0.0995, -1.2659, -0.5352,  0.2007,  0.2073,  0.8574,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3731003398857091 entropy -2.2539794868388894
epoch: 12, step: 27
	action: tensor([[ 0.9104, -1.0869,  0.6811, -0.5047, -0.1453,  0.7577,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4407634240275995, distance: 1.3735770648589236 entropy -2.2539794868388894
epoch: 12, step: 28
	action: tensor([[ 0.9981, -2.1991, -0.3394,  3.4590,  1.8051,  1.3621,  5.3370]],
       dtype=torch.float64)
	q_value: tensor([[-1.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3735770648589236 entropy -1.4704856305180216
epoch: 12, step: 29
	action: tensor([[-1.5262,  1.1235,  0.1697,  0.0827, -0.6149,  0.6738,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3735770648589236 entropy -2.2539794868388894
epoch: 12, step: 30
	action: tensor([[-0.1292, -0.4043,  0.5503,  0.1037, -0.3407,  0.5283,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08484496925354779, distance: 1.0947224614117017 entropy -2.2539794868388894
epoch: 12, step: 31
	action: tensor([[-1.0117,  0.3413,  0.5015, -1.5937,  0.0808,  1.9619,  4.2202]],
       dtype=torch.float64)
	q_value: tensor([[-1.7833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5450097034249048, distance: 1.422401818785617 entropy -1.6738570208616685
epoch: 12, step: 32
	action: tensor([[-0.7236, -2.4601, -0.9106, -1.3645,  2.6167,  3.2647,  5.5694]],
       dtype=torch.float64)
	q_value: tensor([[-2.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.422401818785617 entropy -1.4335435215491825
epoch: 12, step: 33
	action: tensor([[0.2400, 0.7722, 0.6763, 0.3639, 0.7983, 1.3348, 2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.422401818785617 entropy -2.2539794868388894
epoch: 12, step: 34
	action: tensor([[ 0.5260, -0.6725, -0.9814,  0.8003,  1.4910,  0.7878,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5170338942483897, distance: 0.7952707692284128 entropy -2.2539794868388894
epoch: 12, step: 35
	action: tensor([[ 0.1297, -3.1688, -4.4430,  3.1303,  2.2153,  1.1557,  4.6825]],
       dtype=torch.float64)
	q_value: tensor([[-2.3375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7952707692284128 entropy -1.5942248338202094
epoch: 12, step: 36
	action: tensor([[ 1.1927,  0.8671, -1.0706,  0.7277, -0.3137,  0.7253,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7952707692284128 entropy -2.2539794868388894
epoch: 12, step: 37
	action: tensor([[-0.5379, -1.3692, -0.6949, -0.5445, -0.6626,  1.1107,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7952707692284128 entropy -2.2539794868388894
epoch: 12, step: 38
	action: tensor([[ 1.2380, -0.6764,  0.8406, -0.7725,  0.0515,  1.1102,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02090388679277777, distance: 1.1323204647923333 entropy -2.2539794868388894
epoch: 12, step: 39
	action: tensor([[-0.5246, -3.1173,  0.8798,  3.7121, -2.0041,  2.8157,  5.8399]],
       dtype=torch.float64)
	q_value: tensor([[-1.9828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1323204647923333 entropy -1.3929297148022626
epoch: 12, step: 40
	action: tensor([[ 0.4353,  0.6357, -0.1880, -0.1163,  1.7588,  0.9248,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9898589518658457, distance: 0.11523863762320292 entropy -2.2539794868388894
epoch: 12, step: 41
	action: tensor([[ 0.5237, -0.2709, -0.9697, -1.4269,  0.2271,  0.3789,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.11523863762320292 entropy -2.2539794868388894
epoch: 12, step: 42
	action: tensor([[-0.5177, -0.0872,  0.8074, -1.6778,  1.3523,  1.0348,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6222177928857187, distance: 1.4575090952025773 entropy -2.2539794868388894
epoch: 12, step: 43
	action: tensor([[ 2.2231, -0.8945, -1.4144,  2.0648, -3.6708,  2.4298,  5.3012]],
       dtype=torch.float64)
	q_value: tensor([[-2.5559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4575090952025773 entropy -1.4667627182450247
epoch: 12, step: 44
	action: tensor([[ 0.8800, -1.3621,  0.6037, -0.7567, -0.9109,  0.4361,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4575090952025773 entropy -2.2539794868388894
epoch: 12, step: 45
	action: tensor([[ 0.4855,  0.1981, -0.6052,  0.8713, -1.2095,  1.1607,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4928789859081306, distance: 0.8149153471760469 entropy -2.2539794868388894
epoch: 12, step: 46
	action: tensor([[-2.3168, -1.6258,  0.2423,  0.3595, -1.6054,  2.0489,  5.0859]],
       dtype=torch.float64)
	q_value: tensor([[-1.7677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8149153471760469 entropy -1.5341185912014104
epoch: 12, step: 47
	action: tensor([[ 1.3223, -0.7785, -0.0824,  0.8083, -1.4754,  1.3496,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5974653703399382, distance: 0.7260362707587956 entropy -2.2539794868388894
epoch: 12, step: 48
	action: tensor([[-0.2773, -3.1502, -1.2391, -3.5787, -5.3736,  2.9641,  6.3304]],
       dtype=torch.float64)
	q_value: tensor([[-1.9024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7260362707587956 entropy -1.3373145748449653
epoch: 12, step: 49
	action: tensor([[-1.7943, -0.6821,  0.4602,  0.2439,  0.3064,  0.2658,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7260362707587956 entropy -2.2539794868388894
epoch: 12, step: 50
	action: tensor([[1.3736, 0.5657, 0.5503, 0.3927, 0.2016, 1.0320, 2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6332935671183667, distance: 0.6929724316884286 entropy -2.2539794868388894
epoch: 12, step: 51
	action: tensor([[-0.2927, -0.3782, -0.1377,  0.6502,  0.0293,  0.6273,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12473207148813892, distance: 1.0705999061088847 entropy -2.2539794868388894
epoch: 12, step: 52
	action: tensor([[-0.2635, -0.9600, -1.5313, -1.9426,  0.4864,  1.5417,  3.9938]],
       dtype=torch.float64)
	q_value: tensor([[-1.8881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3797037509083713, distance: 0.901272813114191 entropy -1.7310755468971772
epoch: 12, step: 53
	action: tensor([[-1.6168, -1.8202, -1.6446,  0.5868, -0.0349,  1.9852,  5.5915]],
       dtype=torch.float64)
	q_value: tensor([[-2.5212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.901272813114191 entropy -1.4378219379760044
epoch: 12, step: 54
	action: tensor([[ 0.4403, -0.3700, -0.0433, -1.0123, -0.7557,  0.8359,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12373632900938802, distance: 1.2130784953657785 entropy -2.2539794868388894
epoch: 12, step: 55
	action: tensor([[ 2.1427, -1.4397,  0.0081,  1.2569, -2.7496,  3.1259,  4.8174]],
       dtype=torch.float64)
	q_value: tensor([[-1.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2130784953657785 entropy -1.5676461861833002
epoch: 12, step: 56
	action: tensor([[-0.3663, -1.0576, -0.5225, -0.1252,  0.5011,  0.8852,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6410702932340642, distance: 1.4659538055340202 entropy -2.2539794868388894
epoch: 12, step: 57
	action: tensor([[-3.5258, -0.3219, -0.7721, -2.4529, -1.4669,  1.1401,  4.2526]],
       dtype=torch.float64)
	q_value: tensor([[-2.2018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4659538055340202 entropy -1.6753258764656997
epoch: 12, step: 58
	action: tensor([[-0.0385, -1.3659,  0.4608,  1.2934, -0.2643,  0.7135,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4659538055340202 entropy -2.2539794868388894
epoch: 12, step: 59
	action: tensor([[ 0.0635, -0.3599, -0.8587, -0.3231,  0.1118,  1.2707,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20412629676341987, distance: 1.0208895795479644 entropy -2.2539794868388894
epoch: 12, step: 60
	action: tensor([[ 1.1416, -0.6000,  0.0874, -0.4070, -0.8367,  2.9693,  4.4707]],
       dtype=torch.float64)
	q_value: tensor([[-1.9935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0208895795479644 entropy -1.638405071869953
epoch: 12, step: 61
	action: tensor([[ 0.9040, -1.3152,  0.6843, -1.2123, -0.4615,  0.7788,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0208895795479644 entropy -2.2539794868388894
epoch: 12, step: 62
	action: tensor([[ 1.2151, -1.0560,  0.2593, -1.0016,  0.5703,  1.2136,  2.1583]],
       dtype=torch.float64)
	q_value: tensor([[-1.5461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5945578693569562, distance: 1.445029908069361 entropy -2.2539794868388894
epoch: 12, step: 63
	action: tensor([[-2.9112, -3.3588, -2.6844, -1.3387,  2.1559,  2.2167,  5.8128]],
       dtype=torch.float64)
	q_value: tensor([[-2.1502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.445029908069361 entropy -1.3999411907697024
LOSS epoch 12 actor 704.625034120957 critic 2414.933103608447
epoch: 13, step: 0
	action: tensor([[-0.1678, -0.2875, -0.7408,  0.7053, -0.3872,  0.7971,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16595779639064268, distance: 1.2356574975004766 entropy -2.419840012662815
epoch: 13, step: 1
	action: tensor([[ 0.9714, -0.4651,  0.1502, -2.1956, -0.8209,  1.5275,  3.4237]],
       dtype=torch.float64)
	q_value: tensor([[-2.6251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01893514410280872, distance: 1.1334583143372885 entropy -1.8047189362630516
epoch: 13, step: 2
	action: tensor([[-2.1169, -4.2742, -0.1939, -1.1781, -3.6584,  2.4779,  7.0883]],
       dtype=torch.float64)
	q_value: tensor([[-3.8215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1334583143372885 entropy -1.1637767302521318
epoch: 13, step: 3
	action: tensor([[-0.2482,  0.3481, -0.4444,  0.6082, -0.8548,  0.8910,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02852541138832565, distance: 1.1279047244130787 entropy -2.419840012662815
epoch: 13, step: 4
	action: tensor([[ 1.3530, -0.1978,  0.6425, -1.1112, -2.9126,  1.8996,  3.5035]],
       dtype=torch.float64)
	q_value: tensor([[-2.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1279047244130787 entropy -1.7848663968052192
epoch: 13, step: 5
	action: tensor([[-0.4490, -0.6471,  0.2891,  0.1487, -0.5450,  0.3585,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5858587011728837, distance: 1.4410828108350366 entropy -2.419840012662815
epoch: 13, step: 6
	action: tensor([[ 2.5459, -0.9509, -0.1881,  2.0416, -1.3366,  2.1828,  3.3553]],
       dtype=torch.float64)
	q_value: tensor([[-2.5019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4410828108350366 entropy -1.8063536895359713
epoch: 13, step: 7
	action: tensor([[ 0.0525, -0.6929,  0.0973, -0.8653,  0.1246,  0.7984,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4732974838506472, distance: 1.3889989481037905 entropy -2.419840012662815
epoch: 13, step: 8
	action: tensor([[ 5.7031, -0.8456,  0.2508,  0.5085, -1.8904,  2.0503,  3.9033]],
       dtype=torch.float64)
	q_value: tensor([[-2.9014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5325232964726168, distance: 1.4166423992518202 entropy -1.6721265934018708
epoch: 13, step: 9
	action: tensor([[ 0.4948,  0.4885, -2.4313, -3.7192, -3.0464,  0.9810,  6.2544]],
       dtype=torch.float64)
	q_value: tensor([[-3.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4166423992518202 entropy -1.2782878011612027
epoch: 13, step: 10
	action: tensor([[-0.2102, -0.5392,  0.7519,  0.5371,  0.3163,  0.7821,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4216691985959895, distance: 0.8702516488484001 entropy -2.419840012662815
epoch: 13, step: 11
	action: tensor([[-1.4295, -0.9106, -1.1733, -2.6531,  0.5363,  1.3900,  3.9463]],
       dtype=torch.float64)
	q_value: tensor([[-2.7126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8702516488484001 entropy -1.6604519009625862
epoch: 13, step: 12
	action: tensor([[ 1.2298, -1.0051,  0.2780,  0.1022, -0.9394,  0.2358,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23763664571915233, distance: 1.2730728998177052 entropy -2.419840012662815
epoch: 13, step: 13
	action: tensor([[-0.5670, -1.7567, -1.2539,  0.6667, -4.3739,  1.7258,  4.6965]],
       dtype=torch.float64)
	q_value: tensor([[-2.4319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2730728998177052 entropy -1.5178022087648908
epoch: 13, step: 14
	action: tensor([[ 0.0725, -0.2416, -0.8531,  0.1855, -0.8937,  0.6080,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009679946795965066, distance: 1.138792189657564 entropy -2.419840012662815
epoch: 13, step: 15
	action: tensor([[-2.4448,  0.0798,  0.2835, -0.6259,  2.9137,  1.5274,  3.4156]],
       dtype=torch.float64)
	q_value: tensor([[-2.4740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.138792189657564 entropy -1.8076901846856017
epoch: 13, step: 16
	action: tensor([[-0.3625, -0.4677, -0.3778, -0.4720,  0.5304,  0.7052,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34810349990877354, distance: 1.3286736167292479 entropy -2.419840012662815
epoch: 13, step: 17
	action: tensor([[ 2.5397,  0.3689, -1.0329, -1.1181, -3.4773,  2.2986,  3.3007]],
       dtype=torch.float64)
	q_value: tensor([[-3.0063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3286736167292479 entropy -1.818012496098494
epoch: 13, step: 18
	action: tensor([[-0.4018, -0.8823,  0.4055,  1.1035, -1.2624,  1.0038,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043964310443330046, distance: 1.169228840429931 entropy -2.419840012662815
epoch: 13, step: 19
	action: tensor([[-0.8084,  0.7922, -1.1219, -1.1813, -0.1295,  3.2063,  4.4641]],
       dtype=torch.float64)
	q_value: tensor([[-2.9136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.169228840429931 entropy -1.5702686430577102
epoch: 13, step: 20
	action: tensor([[ 0.7242, -0.4441, -0.1786, -0.3233,  0.6916,  0.6028,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3251187019023094, distance: 0.940092071506608 entropy -2.419840012662815
epoch: 13, step: 21
	action: tensor([[-1.3578, -3.5914, -0.1091, -1.8303, -0.5087,  1.7630,  3.8988]],
       dtype=torch.float64)
	q_value: tensor([[-2.6784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.940092071506608 entropy -1.6748461066628777
epoch: 13, step: 22
	action: tensor([[ 0.5143, -0.2765,  0.0737,  0.3114,  0.6100,  0.5315,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7800590135506555, distance: 0.5366730383747501 entropy -2.419840012662815
epoch: 13, step: 23
	action: tensor([[-0.2106, -0.6068, -1.0401, -4.2705, -2.6678,  2.4525,  3.6877]],
       dtype=torch.float64)
	q_value: tensor([[-2.5323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5366730383747501 entropy -1.7223617068183537
epoch: 13, step: 24
	action: tensor([[ 0.2010,  0.1252, -0.5781, -1.1806,  0.6805,  0.1924,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3997563875520014, distance: 0.8865851788121754 entropy -2.419840012662815
epoch: 13, step: 25
	action: tensor([[-0.5681,  1.0302, -0.8268,  0.6626, -2.5747,  0.9052,  3.2077]],
       dtype=torch.float64)
	q_value: tensor([[-2.9530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8865851788121754 entropy -1.8416210533467903
epoch: 13, step: 26
	action: tensor([[-1.0644, -0.2712, -0.1399, -0.9275,  0.1125,  0.6410,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1660572488387482, distance: 1.6841922271928762 entropy -2.419840012662815
epoch: 13, step: 27
	action: tensor([[-0.1319, -1.8932, -0.3374, -0.6394, -0.8867,  1.5952,  3.1991]],
       dtype=torch.float64)
	q_value: tensor([[-3.2234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6841922271928762 entropy -1.841605979513212
epoch: 13, step: 28
	action: tensor([[ 0.0990, -0.7113, -0.4037, -1.7351, -0.2330,  0.7120,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41048611496229115, distance: 1.3590677308888812 entropy -2.419840012662815
epoch: 13, step: 29
	action: tensor([[ 0.8505, -0.7034,  0.4596,  1.5716,  1.8380,  0.0404,  4.1123]],
       dtype=torch.float64)
	q_value: tensor([[-3.3465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.871427436663278, distance: 0.410327708146867 entropy -1.6328830999745716
epoch: 13, step: 30
	action: tensor([[ 2.3939, -0.9758,  1.8941, -0.7735,  0.1507,  2.8024,  4.6784]],
       dtype=torch.float64)
	q_value: tensor([[-4.3859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.410327708146867 entropy -1.5155426948640982
epoch: 13, step: 31
	action: tensor([[ 0.3925, -0.5117, -0.3476, -1.2360, -1.2762,  0.7575,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4099690520975734, distance: 1.3588186012233792 entropy -2.419840012662815
epoch: 13, step: 32
	action: tensor([[ 0.5563, -1.7010, -1.8321, -1.5311, -0.9168,  2.9564,  4.3517]],
       dtype=torch.float64)
	q_value: tensor([[-2.9961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3588186012233792 entropy -1.590506447736615
epoch: 13, step: 33
	action: tensor([[ 0.7438, -0.9629,  0.5921, -0.2691, -0.5701,  0.9150,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05622098706636858, distance: 1.1760724853054652 entropy -2.419840012662815
epoch: 13, step: 34
	action: tensor([[-1.3335, -2.1428, -1.0478, -0.5510,  1.2991,  0.9454,  4.8268]],
       dtype=torch.float64)
	q_value: tensor([[-2.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1760724853054652 entropy -1.491479001613561
epoch: 13, step: 35
	action: tensor([[-0.1227, -0.7658, -0.8526, -0.4561,  0.9742,  0.8114,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1282128658868027, distance: 1.2154923148151127 entropy -2.419840012662815
epoch: 13, step: 36
	action: tensor([[ 0.0697, -0.8238, -1.1470, -0.1568, -0.3730,  1.7565,  3.6316]],
       dtype=torch.float64)
	q_value: tensor([[-3.4034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2776068203688151, distance: 1.2934668538897738 entropy -1.7406856827225958
epoch: 13, step: 37
	action: tensor([[ 6.4420, -1.4280, -2.1645, -1.7918, -1.6913,  4.2371,  5.2191]],
       dtype=torch.float64)
	q_value: tensor([[-3.3249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2934668538897738 entropy -1.424550791950108
epoch: 13, step: 38
	action: tensor([[ 0.5917, -0.3702,  1.1065, -0.3221,  0.9547,  0.2896,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20651123022946416, distance: 1.0193588212999387 entropy -2.419840012662815
epoch: 13, step: 39
	action: tensor([[-0.9479,  0.1362, -2.2141, -0.2373,  2.6992,  0.8513,  4.3913]],
       dtype=torch.float64)
	q_value: tensor([[-2.9368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1728101649365128, distance: 1.2392831761272933 entropy -1.5639311925493717
epoch: 13, step: 40
	action: tensor([[-1.6429,  0.2182, -0.6888,  1.4592,  3.4704,  0.8935,  3.3981]],
       dtype=torch.float64)
	q_value: tensor([[-5.4388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2392831761272933 entropy -1.769514501222497
epoch: 13, step: 41
	action: tensor([[ 0.0964, -0.9139,  0.7170, -0.3512,  0.7565,  0.5944,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5496941264380133, distance: 1.424556526564257 entropy -2.419840012662815
epoch: 13, step: 42
	action: tensor([[-0.6043, -0.2057, -0.4744,  0.3831, -2.9262,  1.8816,  4.2013]],
       dtype=torch.float64)
	q_value: tensor([[-2.9813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31792440274709244, distance: 1.313717372440449 entropy -1.602358649646631
epoch: 13, step: 43
	action: tensor([[-1.9947, -0.2890, -0.1166, -1.8513, -0.5473,  4.7476,  6.1941]],
       dtype=torch.float64)
	q_value: tensor([[-2.9694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.313717372440449 entropy -1.2930054738670766
epoch: 13, step: 44
	action: tensor([[ 0.2471, -0.1732, -0.5751, -1.0591, -0.0187,  0.8821,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27184973991715167, distance: 0.9764886455668352 entropy -2.419840012662815
epoch: 13, step: 45
	action: tensor([[ 2.9659,  0.4921, -0.0593, -0.5569, -2.1109,  1.6437,  3.7454]],
       dtype=torch.float64)
	q_value: tensor([[-2.8958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9764886455668352 entropy -1.7130886255681825
epoch: 13, step: 46
	action: tensor([[ 0.8125, -0.4858,  0.9005, -1.1333,  1.3270,  0.4124,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24177032847192126, distance: 1.2751971471421681 entropy -2.419840012662815
epoch: 13, step: 47
	action: tensor([[-2.2744, -1.0338, -0.7314,  0.8951, -3.7983,  1.5791,  4.9519]],
       dtype=torch.float64)
	q_value: tensor([[-3.4816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2751971471421681 entropy -1.4621840185352908
epoch: 13, step: 48
	action: tensor([[-0.5137,  0.1512,  0.0847, -0.4039, -1.0432,  0.9262,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4798979296093815, distance: 1.3921068631603215 entropy -2.419840012662815
epoch: 13, step: 49
	action: tensor([[ 2.5043,  0.4375,  1.2071, -0.4691,  2.3273,  1.8519,  3.6909]],
       dtype=torch.float64)
	q_value: tensor([[-2.7060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3921068631603215 entropy -1.7281062061427033
epoch: 13, step: 50
	action: tensor([[-0.4652, -0.1059,  0.1330, -0.4872, -0.5921,  0.7044,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5507401514017256, distance: 1.4250372247532885 entropy -2.419840012662815
epoch: 13, step: 51
	action: tensor([[ 0.1670, -0.2162,  1.0190, -0.3047, -0.3514,  1.2234,  3.4070]],
       dtype=torch.float64)
	q_value: tensor([[-2.6668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5226177712304729, distance: 0.7906600892564848 entropy -1.7914350912702788
epoch: 13, step: 52
	action: tensor([[ 0.6394,  0.2862,  1.0192,  0.0113, -1.1603,  0.9625,  5.8186]],
       dtype=torch.float64)
	q_value: tensor([[-3.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9591644884333966, distance: 0.23124677970618113 entropy -1.3322187709028641
epoch: 13, step: 53
	action: tensor([[ 2.7726,  1.2656,  1.9925, -2.0513, -1.1217,  2.1739,  4.4483]],
       dtype=torch.float64)
	q_value: tensor([[-1.6499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.23124677970618113 entropy -1.5636037090047898
epoch: 13, step: 54
	action: tensor([[-0.4689,  0.5121, -0.2009,  0.2358, -1.0151,  0.5441,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03676049041036289, distance: 1.1651877564208994 entropy -2.419840012662815
epoch: 13, step: 55
	action: tensor([[-0.9828,  0.7300,  0.0562, -0.0723,  0.7934,  1.4657,  3.2198]],
       dtype=torch.float64)
	q_value: tensor([[-2.3701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29160132178860554, distance: 0.9631536268593599 entropy -1.854265825155903
epoch: 13, step: 56
	action: tensor([[1.0564, 0.3533, 0.0560, 0.4552, 0.0934, 0.8655, 1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9631536268593599 entropy -2.419840012662815
epoch: 13, step: 57
	action: tensor([[ 0.8318, -0.4622, -0.1127, -0.8412, -0.4225,  0.6675,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07983582721734717, distance: 1.189147041048456 entropy -2.419840012662815
epoch: 13, step: 58
	action: tensor([[-0.3285, -1.3517,  0.6881, -1.6537,  0.1893,  2.3751,  4.1839]],
       dtype=torch.float64)
	q_value: tensor([[-2.6137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.189147041048456 entropy -1.616426563095233
epoch: 13, step: 59
	action: tensor([[-0.9463,  0.0907, -0.8352,  0.1065, -0.6826,  0.3926,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0112379242297744, distance: 1.6228875106576757 entropy -2.419840012662815
epoch: 13, step: 60
	action: tensor([[-0.3506, -0.9607, -0.8921,  0.5308, -1.9637,  1.7440,  2.7168]],
       dtype=torch.float64)
	q_value: tensor([[-2.7508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7127254253661286, distance: 1.4976163256482797 entropy -2.005965468302081
epoch: 13, step: 61
	action: tensor([[ 1.9493, -1.1409, -3.6040, -2.3892,  2.5202,  0.7635,  5.9550]],
       dtype=torch.float64)
	q_value: tensor([[-4.3899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4976163256482797 entropy -1.33275821973331
epoch: 13, step: 62
	action: tensor([[-0.8852,  0.1773,  0.2748, -0.4463, -0.0022,  0.5649,  1.6306]],
       dtype=torch.float64)
	q_value: tensor([[-2.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8106909664366473, distance: 1.5398515719652686 entropy -2.419840012662815
epoch: 13, step: 63
	action: tensor([[ 0.8524,  0.8513,  1.9193,  2.2442, -0.5281,  1.4730,  3.1145]],
       dtype=torch.float64)
	q_value: tensor([[-2.8062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286294950020315, distance: 0.9667542236288372 entropy -1.8629475166565437
LOSS epoch 13 actor 468.08927770040714 critic 2175.179393440554
epoch: 14, step: 0
	action: tensor([[-0.2717, -0.1282, -0.3061,  0.2360,  0.2104,  0.3887,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013518557070667736, distance: 1.136582992931091 entropy -2.5594635447493057
epoch: 14, step: 1
	action: tensor([[-1.5194, -1.3678, -0.6044, -0.2860,  0.1717,  1.9159,  3.2709]],
       dtype=torch.float64)
	q_value: tensor([[-3.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.136582992931091 entropy -1.8167808000590766
epoch: 14, step: 2
	action: tensor([[-0.5798, -0.8051,  0.4100, -0.3925,  0.6514,  0.4355,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9880891353063264, distance: 1.6135209892865379 entropy -2.5594635447493057
epoch: 14, step: 3
	action: tensor([[-0.3682, -2.4366, -0.0459, -1.4287,  0.6044,  1.6700,  4.0976]],
       dtype=torch.float64)
	q_value: tensor([[-4.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6135209892865379 entropy -1.6142475595717958
epoch: 14, step: 4
	action: tensor([[-0.4575, -0.1636,  0.1840,  0.6943,  1.1813,  0.9281,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.421674333069327, distance: 0.8702477857532696 entropy -2.5594635447493057
epoch: 14, step: 5
	action: tensor([[ 1.0397, -0.6624,  1.5134, -2.8061, -3.2924,  1.6916,  4.3501]],
       dtype=torch.float64)
	q_value: tensor([[-4.8502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8702477857532696 entropy -1.5637223453596045
epoch: 14, step: 6
	action: tensor([[-0.2311, -0.6257, -0.2733,  0.0131, -0.9858,  0.2625,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5587972648257491, distance: 1.4287344307086243 entropy -2.5594635447493057
epoch: 14, step: 7
	action: tensor([[ 0.7958, -0.8897, -1.3690,  1.2413, -1.2536,  2.0717,  3.7276]],
       dtype=torch.float64)
	q_value: tensor([[-3.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00274873180319668, distance: 1.145915922439787 entropy -1.7133264066288536
epoch: 14, step: 8
	action: tensor([[ 1.1101, -1.7336, -0.5411, -0.8348,  1.5831,  4.3111,  7.3696]],
       dtype=torch.float64)
	q_value: tensor([[-5.4019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.145915922439787 entropy -1.1186875073484142
epoch: 14, step: 9
	action: tensor([[ 0.0869, -0.2338, -0.2281, -0.6359,  0.9100,  0.6608,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20518112655381304, distance: 1.0202128253183458 entropy -2.5594635447493057
epoch: 14, step: 10
	action: tensor([[ 1.2621, -1.2721,  1.7249,  0.4141,  2.4828,  1.3315,  3.9419]],
       dtype=torch.float64)
	q_value: tensor([[-4.5084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0202128253183458 entropy -1.6471928717304867
epoch: 14, step: 11
	action: tensor([[ 0.1266, -0.2610, -0.8978, -0.2559,  0.0568,  0.4931,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22984204008887577, distance: 1.0042610001583032 entropy -2.5594635447493057
epoch: 14, step: 12
	action: tensor([[ 3.3264, -1.5423,  0.8331,  0.1354, -1.8064,  0.0650,  3.3740]],
       dtype=torch.float64)
	q_value: tensor([[-3.7317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0042610001583032 entropy -1.7950733621669508
epoch: 14, step: 13
	action: tensor([[-0.2638, -0.0421, -0.4503, -0.4828, -0.4903,  0.2415,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15655044922378214, distance: 1.2306625477302264 entropy -2.5594635447493057
epoch: 14, step: 14
	action: tensor([[-0.8955,  0.2346, -0.0435, -2.0549, -0.3775,  1.6640,  3.1445]],
       dtype=torch.float64)
	q_value: tensor([[-3.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5993845908839839, distance: 1.4472153059561612 entropy -1.85401325144792
epoch: 14, step: 15
	action: tensor([[ 2.6199, -2.2386,  0.2612, -1.2033,  4.3382,  2.8941,  6.6202]],
       dtype=torch.float64)
	q_value: tensor([[-7.2088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4472153059561612 entropy -1.201230527153705
epoch: 14, step: 16
	action: tensor([[-0.9565, -0.7760, -0.1168, -0.9532, -1.3823,  0.7427,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1929243578013526, distance: 1.694605138667792 entropy -2.5594635447493057
epoch: 14, step: 17
	action: tensor([[ 1.5019,  0.6055, -0.5301, -0.6431,  1.5387,  2.2197,  4.6714]],
       dtype=torch.float64)
	q_value: tensor([[-4.9862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.694605138667792 entropy -1.5156893687118713
epoch: 14, step: 18
	action: tensor([[ 0.9065, -0.4167,  0.0777, -0.1901,  0.0334,  0.1874,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983111352711215, distance: 0.9585813730057354 entropy -2.5594635447493057
epoch: 14, step: 19
	action: tensor([[ 0.9377, -0.3377, -1.1853, -0.8733, -0.3697,  2.3335,  4.2297]],
       dtype=torch.float64)
	q_value: tensor([[-3.3280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9585813730057354 entropy -1.5947480652235122
epoch: 14, step: 20
	action: tensor([[-0.4425, -0.8761, -0.4826, -0.5262, -0.1575,  0.2260,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8360415787383242, distance: 1.5505934648124768 entropy -2.5594635447493057
epoch: 14, step: 21
	action: tensor([[-3.7016, -0.1896, -1.0371,  0.0561,  0.9043,  1.8083,  3.3884]],
       dtype=torch.float64)
	q_value: tensor([[-4.1395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5505934648124768 entropy -1.791230359250761
epoch: 14, step: 22
	action: tensor([[ 0.9958, -0.8462, -0.0392, -0.7113,  0.0093,  0.5663,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43747148532546265, distance: 1.3720069537437303 entropy -2.5594635447493057
epoch: 14, step: 23
	action: tensor([[ 0.1723, -1.9141, -0.3537, -4.4797, -3.0083,  1.8798,  4.9343]],
       dtype=torch.float64)
	q_value: tensor([[-3.9869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3720069537437303 entropy -1.46110629643457
epoch: 14, step: 24
	action: tensor([[ 0.1514, -0.5957, -0.4498, -0.4814, -0.3722,  0.5423,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23753836110255566, distance: 1.2730223494536297 entropy -2.5594635447493057
epoch: 14, step: 25
	action: tensor([[ 1.8654,  0.0282, -0.3539, -1.5952,  2.0548,  1.6102,  3.8699]],
       dtype=torch.float64)
	q_value: tensor([[-3.7080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2730223494536297 entropy -1.6771472883047591
epoch: 14, step: 26
	action: tensor([[-0.8966, -0.5839, -0.6609, -0.4308,  0.6031,  0.5017,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0040669434170284, distance: 1.6199917599815867 entropy -2.5594635447493057
epoch: 14, step: 27
	action: tensor([[-2.0060, -0.4764,  0.1569,  0.8722,  0.5794,  1.5623,  3.2474]],
       dtype=torch.float64)
	q_value: tensor([[-5.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6199917599815867 entropy -1.8199063062257343
epoch: 14, step: 28
	action: tensor([[ 1.0916e-01, -5.6252e-01,  4.3710e-04,  6.5684e-01,  5.4309e-02,
          5.2418e-01,  1.3156e+00]], dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48704118742881974, distance: 0.8195924346563307 entropy -2.5594635447493057
epoch: 14, step: 29
	action: tensor([[ 0.9303, -1.5959,  0.0274, -2.6407,  2.3797,  2.2486,  4.0124]],
       dtype=torch.float64)
	q_value: tensor([[-3.4495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8195924346563307 entropy -1.644182167922322
epoch: 14, step: 30
	action: tensor([[ 0.2759, -0.5673, -0.5202, -0.1441, -0.6193,  0.4484,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04703749751130348, distance: 1.1709485440502494 entropy -2.5594635447493057
epoch: 14, step: 31
	action: tensor([[ 0.6118, -0.5975, -2.8127,  0.0621, -1.0586,  1.0645,  3.9020]],
       dtype=torch.float64)
	q_value: tensor([[-3.4448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002777588158983546, distance: 1.1459324104784254 entropy -1.6736103804897324
epoch: 14, step: 32
	action: tensor([[-2.6332, -1.1384,  0.9659, -1.1677,  4.1855,  2.2896,  5.2714]],
       dtype=torch.float64)
	q_value: tensor([[-5.2354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1459324104784254 entropy -1.39598991777388
epoch: 14, step: 33
	action: tensor([[ 0.3712, -0.6005,  0.2038,  0.0630, -0.6888,  0.3815,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1779775516626404, distance: 1.0375249090688097 entropy -2.5594635447493057
epoch: 14, step: 34
	action: tensor([[-1.7027, -1.0871, -0.2365,  0.4231,  0.4167,  2.3011,  4.3358]],
       dtype=torch.float64)
	q_value: tensor([[-3.2095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0375249090688097 entropy -1.5776499411849394
epoch: 14, step: 35
	action: tensor([[ 0.4670, -0.8993,  0.1118,  0.2945, -0.1577,  0.0964,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00018934722147057048, distance: 1.1442359096686323 entropy -2.5594635447493057
epoch: 14, step: 36
	action: tensor([[ 1.5438, -1.1550, -0.6955, -0.4009, -1.0239,  2.6012,  4.0816]],
       dtype=torch.float64)
	q_value: tensor([[-3.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1442359096686323 entropy -1.6266885527651898
epoch: 14, step: 37
	action: tensor([[-0.0199, -0.6237,  0.1127, -0.4283, -0.1597,  0.6729,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.332873708982115, distance: 1.3211471555191974 entropy -2.5594635447493057
epoch: 14, step: 38
	action: tensor([[ 3.9094, -3.3431, -2.8476,  1.7145,  2.6702,  0.8904,  4.1767]],
       dtype=torch.float64)
	q_value: tensor([[-3.7332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3211471555191974 entropy -1.6053446831354068
epoch: 14, step: 39
	action: tensor([[ 0.0980, -1.0170,  0.1700,  1.5556, -0.0966,  0.4365,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6963414117510769, distance: 0.6305934821908136 entropy -2.5594635447493057
epoch: 14, step: 40
	action: tensor([[ 5.2024,  0.8506, -0.4226, -2.3953, -4.3423,  2.8148,  4.7410]],
       dtype=torch.float64)
	q_value: tensor([[-4.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6305934821908136 entropy -1.5016181928757217
epoch: 14, step: 41
	action: tensor([[-0.1908,  0.1477, -0.1079, -0.0208, -0.1590, -0.0352,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13663640504962982, distance: 1.0632944799893287 entropy -2.5594635447493057
epoch: 14, step: 42
	action: tensor([[ 0.5350, -1.1592, -0.9803,  1.4715,  0.1397,  0.9150,  3.0407]],
       dtype=torch.float64)
	q_value: tensor([[-3.1087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35663734512930156, distance: 0.9178772741239866 entropy -1.8805776846998647
epoch: 14, step: 43
	action: tensor([[-0.1408,  0.2569,  3.8531,  2.3083,  4.8738,  3.6619,  7.0888]],
       dtype=torch.float64)
	q_value: tensor([[-6.8293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9178772741239866 entropy -1.163094882217749
epoch: 14, step: 44
	action: tensor([[-0.6753, -0.1745, -0.5768,  0.0244,  0.4553,  0.3208,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6473926437305124, distance: 1.4687749413125073 entropy -2.5594635447493057
epoch: 14, step: 45
	action: tensor([[-1.0053,  0.1144, -1.3942, -2.1354, -1.8275,  0.8885,  2.9244]],
       dtype=torch.float64)
	q_value: tensor([[-4.1496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2464461215376823, distance: 0.9933764226490972 entropy -1.9102870921302764
epoch: 14, step: 46
	action: tensor([[ 0.2574, -0.4418, -2.4638,  3.2912,  5.4374,  3.2794,  6.0526]],
       dtype=torch.float64)
	q_value: tensor([[-8.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9933764226490972 entropy -1.2975358647206063
epoch: 14, step: 47
	action: tensor([[ 0.8631, -0.2871,  0.1536,  0.2697, -0.5684,  0.3670,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.757110064293675, distance: 0.5639770781734713 entropy -2.5594635447493057
epoch: 14, step: 48
	action: tensor([[ 0.2654, -1.1769,  0.4247, -2.0817,  0.5478,  1.7435,  4.5693]],
       dtype=torch.float64)
	q_value: tensor([[-3.0936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44902035380339766, distance: 1.3775073861612555 entropy -1.5331886584436856
epoch: 14, step: 49
	action: tensor([[ 0.9931, -0.7765, -1.1216, -4.1967,  4.2734,  0.9139,  7.2567]],
       dtype=torch.float64)
	q_value: tensor([[-6.4648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3775073861612555 entropy -1.1236576104542757
epoch: 14, step: 50
	action: tensor([[-0.1956, -0.9715, -0.3647, -0.0232,  0.5519,  0.7792,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42608775782630914, distance: 1.3665635072512647 entropy -2.5594635447493057
epoch: 14, step: 51
	action: tensor([[ 1.5121, -0.2400, -0.3121, -2.2276,  1.7747,  1.9144,  4.0944]],
       dtype=torch.float64)
	q_value: tensor([[-4.5638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13960773927516945, distance: 1.061463196900334 entropy -1.6212910404514382
epoch: 14, step: 52
	action: tensor([[ 3.9567, -3.7346,  1.3402, -0.3773,  5.1692, -0.4126,  7.8396]],
       dtype=torch.float64)
	q_value: tensor([[-8.3183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.061463196900334 entropy -1.0508270416204224
epoch: 14, step: 53
	action: tensor([[ 0.9777, -0.4889,  0.3257, -0.6330, -0.5088,  0.1566,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07152424530161183, distance: 1.1845617211169368 entropy -2.5594635447493057
epoch: 14, step: 54
	action: tensor([[-4.1378,  0.9239, -0.4885, -0.6047,  2.8379,  1.3362,  4.6819]],
       dtype=torch.float64)
	q_value: tensor([[-3.4890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1845617211169368 entropy -1.50658044191406
epoch: 14, step: 55
	action: tensor([[-0.1115,  0.0486, -0.4740,  0.3830, -0.1558,  0.4831,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1845617211169368 entropy -2.5594635447493057
epoch: 14, step: 56
	action: tensor([[ 0.3238, -0.6408,  0.4145, -0.5803,  0.2431,  0.3102,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3629283132885597, distance: 1.3359592154417272 entropy -2.5594635447493057
epoch: 14, step: 57
	action: tensor([[ 0.2820, -1.4780, -0.4332, -3.9398,  1.3734,  1.3802,  4.2435]],
       dtype=torch.float64)
	q_value: tensor([[-3.7503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3359592154417272 entropy -1.58602233639399
epoch: 14, step: 58
	action: tensor([[ 0.4405, -0.8378,  0.0031,  0.0094, -1.3821,  0.4828,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22272249260329247, distance: 1.2653790618667686 entropy -2.5594635447493057
epoch: 14, step: 59
	action: tensor([[ 4.5090, -0.4571, -2.4378,  2.8485,  3.2000,  1.1022,  4.9717]],
       dtype=torch.float64)
	q_value: tensor([[-3.6732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2653790618667686 entropy -1.4641563002374556
epoch: 14, step: 60
	action: tensor([[-0.2076,  0.3474, -0.4997, -0.0601, -0.4739,  0.3936,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19142310474878887, distance: 1.0290046965630313 entropy -2.5594635447493057
epoch: 14, step: 61
	action: tensor([[ 0.0268, -0.4327, -0.2677,  0.0112, -0.1002,  0.5088,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06575810685150663, distance: 1.1060795484779677 entropy -2.5594635447493057
epoch: 14, step: 62
	action: tensor([[ 1.1589,  0.5188, -1.7745, -3.5126, -1.9613,  0.7406,  3.6478]],
       dtype=torch.float64)
	q_value: tensor([[-3.4100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1060795484779677 entropy -1.7259610729485881
epoch: 14, step: 63
	action: tensor([[ 0.7653,  0.1776, -0.7510, -0.5828, -0.7219,  0.2552,  1.3156]],
       dtype=torch.float64)
	q_value: tensor([[-4.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7014941978451095, distance: 0.6252203162894714 entropy -2.5594635447493057
LOSS epoch 14 actor 471.2592944754884 critic 1982.162707101236
epoch: 15, step: 0
	action: tensor([[ 0.2549, -0.1056, -0.1957, -0.3429,  0.5304,  0.0993,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32500979418494824, distance: 0.9401679212536382 entropy -3.164051615025985
epoch: 15, step: 1
	action: tensor([[ 0.4737, -1.1421, -0.3265,  0.2031,  2.9467,  1.3411,  2.4325]],
       dtype=torch.float64)
	q_value: tensor([[-4.1690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46776037281278837, distance: 1.3863863456133085 entropy -2.0240583153487632
epoch: 15, step: 2
	action: tensor([[ 0.1637,  2.0133, -1.1118,  3.2937,  0.9176,  4.1201,  7.6445]],
       dtype=torch.float64)
	q_value: tensor([[-13.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3863863456133085 entropy -1.0457277322857332
epoch: 15, step: 3
	action: tensor([[-0.1969,  0.3180, -0.3571, -0.0516,  0.1439,  0.2568,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3863863456133085 entropy -3.164051615025985
epoch: 15, step: 4
	action: tensor([[-0.3273,  0.2255,  0.0058,  0.1538,  0.1735,  0.1632,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14305361746873002, distance: 1.0593354798953707 entropy -3.164051615025985
epoch: 15, step: 5
	action: tensor([[ 0.8191, -1.6470, -0.3410, -0.7549,  0.3835,  1.1595,  2.2846]],
       dtype=torch.float64)
	q_value: tensor([[-3.4797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0593354798953707 entropy -2.083448519220768
epoch: 15, step: 6
	action: tensor([[-0.6661,  0.1028, -0.0621, -0.2260, -0.0656,  0.2792,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5424730975042817, distance: 1.4212336853354675 entropy -3.164051615025985
epoch: 15, step: 7
	action: tensor([[ 0.4991, -0.0461, -0.2708, -0.6844,  2.0142,  0.8296,  2.2559]],
       dtype=torch.float64)
	q_value: tensor([[-4.0610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42567483798171557, distance: 0.8672326402569508 entropy -2.094189259295995
epoch: 15, step: 8
	action: tensor([[ 1.3469, -2.2858, -0.1060, -5.0019, -2.8091,  2.0345,  6.1497]],
       dtype=torch.float64)
	q_value: tensor([[-10.3952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8672326402569508 entropy -1.2316777192851067
epoch: 15, step: 9
	action: tensor([[ 0.5226, -0.0465,  0.1694,  0.2577,  0.5953,  0.2692,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8275139413272099, distance: 0.4752626580244297 entropy -3.164051615025985
epoch: 15, step: 10
	action: tensor([[ 1.4803,  0.3428,  1.3259, -0.0058,  1.5135,  1.5957,  3.1896]],
       dtype=torch.float64)
	q_value: tensor([[-4.0721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4902159019910799, distance: 0.817052259502639 entropy -1.7951953385358421
epoch: 15, step: 11
	action: tensor([[ 3.5923,  3.4101, -4.4528, -3.8415, -1.1613,  4.2149,  9.6310]],
       dtype=torch.float64)
	q_value: tensor([[-11.3883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.817052259502639 entropy -0.8437950808222018
epoch: 15, step: 12
	action: tensor([[-0.2791, -0.2421, -0.2443,  0.6171, -0.1964,  0.2781,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010182698965854997, distance: 1.1385030897223525 entropy -3.164051615025985
epoch: 15, step: 13
	action: tensor([[ 2.2271,  0.2858,  0.1168, -1.1889,  1.6633,  1.6517,  2.5920]],
       dtype=torch.float64)
	q_value: tensor([[-3.7363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1385030897223525 entropy -1.986553136435837
epoch: 15, step: 14
	action: tensor([[-0.4623, -0.5237, -0.0566,  0.3402,  0.3310,  0.0973,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48713252274164276, distance: 1.3955054246197236 entropy -3.164051615025985
epoch: 15, step: 15
	action: tensor([[-0.0081,  0.1107, -0.0374,  0.5245,  0.4420,  0.5435,  2.4517]],
       dtype=torch.float64)
	q_value: tensor([[-4.2044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3955054246197236 entropy -2.026754881426601
epoch: 15, step: 16
	action: tensor([[ 0.0398, -0.1050, -0.1343, -0.0581, -0.0516,  0.0310,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3955054246197236 entropy -3.164051615025985
epoch: 15, step: 17
	action: tensor([[ 0.5120, -0.5141, -0.2037,  0.1276,  0.4371,  0.0583,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29619808638164136, distance: 0.9600236124309759 entropy -3.164051615025985
epoch: 15, step: 18
	action: tensor([[-0.1091, -0.4596, -1.9923, -0.3072, -2.3013,  1.2806,  2.8772]],
       dtype=torch.float64)
	q_value: tensor([[-3.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3338277755266249, distance: 0.9340066188038404 entropy -1.8895422673717046
epoch: 15, step: 19
	action: tensor([[-1.7312, -1.0091, -0.7203, -1.6354, -8.0931,  1.1554,  7.1812]],
       dtype=torch.float64)
	q_value: tensor([[-12.1207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9340066188038404 entropy -1.131028256970991
epoch: 15, step: 20
	action: tensor([[ 0.2641,  0.0040, -0.2966, -0.4723, -0.2561,  0.2378,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4191402973229884, distance: 0.8721522737782333 entropy -3.164051615025985
epoch: 15, step: 21
	action: tensor([[ 0.7769, -0.3714,  1.3125, -0.3629,  3.3410,  0.7542,  2.6787]],
       dtype=torch.float64)
	q_value: tensor([[-3.5842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33873385279358337, distance: 0.9305609808941517 entropy -1.9479633644799106
epoch: 15, step: 22
	action: tensor([[-1.0390, -2.6375, -3.5672, -1.9952, -3.6815,  4.5432, 10.1984]],
       dtype=torch.float64)
	q_value: tensor([[-9.8177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9305609808941517 entropy -0.8183627326085736
epoch: 15, step: 23
	action: tensor([[ 0.1146, -0.4594,  0.2663, -0.2487, -0.0347,  0.1981,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11861460880025443, distance: 1.2103108780611842 entropy -3.164051615025985
epoch: 15, step: 24
	action: tensor([[ 0.1629, -1.2021, -0.1498, -3.2517,  1.4096,  1.0712,  3.0962]],
       dtype=torch.float64)
	q_value: tensor([[-3.5051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2103108780611842 entropy -1.8233427725806297
epoch: 15, step: 25
	action: tensor([[-0.0975, -0.1366,  0.1065, -0.5097,  0.0576,  0.0436,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1952650532409923, distance: 1.2510907243908818 entropy -3.164051615025985
epoch: 15, step: 26
	action: tensor([[ 0.3252, -0.4841,  0.3880, -1.2154, -0.6707,  1.4035,  2.4868]],
       dtype=torch.float64)
	q_value: tensor([[-3.7951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1028809150481278, distance: 1.2017690192248935 entropy -2.0077275454707446
epoch: 15, step: 27
	action: tensor([[ 1.2208, -2.1250,  0.6983, -5.9497, -0.7732,  2.7654,  7.6509]],
       dtype=torch.float64)
	q_value: tensor([[-9.6091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2017690192248935 entropy -1.0558973429098717
epoch: 15, step: 28
	action: tensor([[-0.2860, -0.3378, -0.0902, -0.2278,  0.0991,  0.2138,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3785770235082253, distance: 1.3436068559486403 entropy -3.164051615025985
epoch: 15, step: 29
	action: tensor([[ 2.6960,  0.9706,  0.0818, -3.4394, -0.3525,  1.2809,  2.4562]],
       dtype=torch.float64)
	q_value: tensor([[-3.8703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3436068559486403 entropy -2.023401708497015
epoch: 15, step: 30
	action: tensor([[-0.5710, -0.2478, -0.0888,  0.0978,  0.0208,  0.3967,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48083855874027837, distance: 1.3925492072689203 entropy -3.164051615025985
epoch: 15, step: 31
	action: tensor([[-0.4727, -0.6711, -0.3210, -1.4783, -0.8708,  1.3358,  2.5068]],
       dtype=torch.float64)
	q_value: tensor([[-4.0866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7114038122950146, distance: 1.4970384013493743 entropy -2.0070524674168317
epoch: 15, step: 32
	action: tensor([[-4.2435, -0.1096, -2.7530, -2.6257,  1.4648, -0.3834,  6.7434]],
       dtype=torch.float64)
	q_value: tensor([[-11.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4970384013493743 entropy -1.1664432325706429
epoch: 15, step: 33
	action: tensor([[-0.4067,  0.1017,  0.0404,  0.2572,  0.1797,  0.3544,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4970384013493743 entropy -3.164051615025985
epoch: 15, step: 34
	action: tensor([[-0.0795,  0.0311, -0.2271,  0.0175, -0.1439,  0.3026,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25536715151226885, distance: 0.9874788181301309 entropy -3.164051615025985
epoch: 15, step: 35
	action: tensor([[ 1.2386, -0.7492,  0.3177, -0.0212, -0.2617,  1.3295,  2.4392]],
       dtype=torch.float64)
	q_value: tensor([[-3.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4292115798725372, distance: 0.8645582714581912 entropy -2.031976576751394
epoch: 15, step: 36
	action: tensor([[ 7.6317, -1.3401, -4.7720, -0.7136, -1.3886,  1.1241,  8.1573]],
       dtype=torch.float64)
	q_value: tensor([[-8.4830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8645582714581912 entropy -1.0045971119107855
epoch: 15, step: 37
	action: tensor([[-0.3686, -0.4432, -0.4420, -0.0901,  0.2120,  0.2331,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48967361407311616, distance: 1.3966971789359375 entropy -3.164051615025985
epoch: 15, step: 38
	action: tensor([[ 1.1503, -0.6648, -0.4228, -0.0089,  0.0767,  0.6619,  2.1870]],
       dtype=torch.float64)
	q_value: tensor([[-4.3716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067497304492217, distance: 0.9527999305669577 entropy -2.127284064002535
epoch: 15, step: 39
	action: tensor([[ 3.7187, -1.1763,  0.8559,  0.5230, -1.6062,  3.4630,  6.2696]],
       dtype=torch.float64)
	q_value: tensor([[-7.5762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9527999305669577 entropy -1.2314681462203885
epoch: 15, step: 40
	action: tensor([[-0.0740, -0.1233, -0.1182,  0.0832, -0.0843,  0.3203,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20591992786508073, distance: 1.0197385601537292 entropy -3.164051615025985
epoch: 15, step: 41
	action: tensor([[ 0.2585, -0.8834, -1.2271,  1.4495, -0.8759,  0.7209,  2.5932]],
       dtype=torch.float64)
	q_value: tensor([[-3.2939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2891311242289676, distance: 1.2992874400871597 entropy -1.9792199270350517
epoch: 15, step: 42
	action: tensor([[ 4.1962,  0.3970,  1.0716, -0.3825, -3.2996,  2.8530,  6.5449]],
       dtype=torch.float64)
	q_value: tensor([[-9.1713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2992874400871597 entropy -1.2089714870495338
epoch: 15, step: 43
	action: tensor([[-0.2899, -0.3933,  0.4769,  0.0806, -0.3442,  0.3263,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21353984815444393, distance: 1.2606186096588963 entropy -3.164051615025985
epoch: 15, step: 44
	action: tensor([[ 1.0545, -1.8253,  0.5719, -1.2745,  0.6204,  0.9468,  3.2855]],
       dtype=torch.float64)
	q_value: tensor([[-3.4010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2606186096588963 entropy -1.7754256528835233
epoch: 15, step: 45
	action: tensor([[-0.0807, -0.2744, -0.2761,  0.0770,  0.1809,  0.5952,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1660828840019687, distance: 1.0450044435292705 entropy -3.164051615025985
epoch: 15, step: 46
	action: tensor([[-0.4361, -1.5530, -0.8545, -0.6162, -3.0924,  1.8864,  2.7978]],
       dtype=torch.float64)
	q_value: tensor([[-4.0971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0450044435292705 entropy -1.9129073095576283
epoch: 15, step: 47
	action: tensor([[-0.4941,  0.0228,  0.1242,  0.2064, -0.1098,  0.2150,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15221837274760075, distance: 1.2283555467614542 entropy -3.164051615025985
epoch: 15, step: 48
	action: tensor([[-1.2824,  0.2114, -0.3485, -0.0099,  1.6207,  1.3759,  2.4740]],
       dtype=torch.float64)
	q_value: tensor([[-3.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4087131627289098, distance: 1.3582133021170926 entropy -2.0178494048066384
epoch: 15, step: 49
	action: tensor([[ 1.2211, -0.7515, -2.1199, -3.1178, -0.8029,  2.8735,  5.7427]],
       dtype=torch.float64)
	q_value: tensor([[-11.6004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3582133021170926 entropy -1.2920950129812538
epoch: 15, step: 50
	action: tensor([[ 0.0974, -0.1401, -0.0062, -0.4026,  0.2003,  0.1878,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12095376827415294, distance: 1.0729081679415147 entropy -3.164051615025985
epoch: 15, step: 51
	action: tensor([[ 7.4517e-05,  2.8456e-02, -1.5671e+00, -1.1371e+00,  1.9196e-01,
          7.4490e-01,  2.6296e+00]], dtype=torch.float64)
	q_value: tensor([[-3.7938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.732948668735248, distance: 0.5913628890498578 entropy -1.9591008997582722
epoch: 15, step: 52
	action: tensor([[-6.8233, -2.2691, -0.6596, -1.3740,  1.7368,  1.6994,  4.9890]],
       dtype=torch.float64)
	q_value: tensor([[-10.7322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5913628890498578 entropy -1.4280405164812113
epoch: 15, step: 53
	action: tensor([[-0.2457,  0.1521,  0.0730,  0.0538,  0.3223,  0.4213,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2579481536544128, distance: 0.9857659623844952 entropy -3.164051615025985
epoch: 15, step: 54
	action: tensor([[-1.9650, -0.7626, -0.1301, -0.7351,  1.0258,  1.8477,  2.6264]],
       dtype=torch.float64)
	q_value: tensor([[-3.8208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9857659623844952 entropy -1.959863119268368
epoch: 15, step: 55
	action: tensor([[ 0.3376, -0.0393,  0.0583, -0.3344, -0.3999,  0.4288,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4500310564184846, distance: 0.8486444516381879 entropy -3.164051615025985
epoch: 15, step: 56
	action: tensor([[-1.2235, -2.0508, -0.0266, -1.7288, -0.3093,  1.3477,  3.2764]],
       dtype=torch.float64)
	q_value: tensor([[-3.3539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8486444516381879 entropy -1.7760140963734308
epoch: 15, step: 57
	action: tensor([[-0.1003,  0.0579,  0.1836,  0.6297, -0.0717,  0.2106,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5521974168338415, distance: 0.7657728956212149 entropy -3.164051615025985
epoch: 15, step: 58
	action: tensor([[ 1.4831, -0.4681, -0.4439,  2.0411, -0.0684,  1.3665,  2.8062]],
       dtype=torch.float64)
	q_value: tensor([[-3.2081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5956873617701504, distance: 0.7276379669490154 entropy -1.9130173582316365
epoch: 15, step: 59
	action: tensor([[-0.2654, -0.1397,  0.2146,  0.1969,  0.1619,  0.3544,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10019989647262739, distance: 1.085499712258717 entropy -3.164051615025985
epoch: 15, step: 60
	action: tensor([[-2.6945, -1.2291,  0.6525,  0.3200, -0.0301,  0.6577,  2.7719]],
       dtype=torch.float64)
	q_value: tensor([[-3.4966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.085499712258717 entropy -1.9175684904444756
epoch: 15, step: 61
	action: tensor([[ 0.3929,  0.0535,  0.0768, -0.1411,  0.6247,  0.2974,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6930273995958971, distance: 0.6340251710132047 entropy -3.164051615025985
epoch: 15, step: 62
	action: tensor([[ 0.6861, -1.9643, -0.8389, -1.5066, -0.1356,  0.6603,  2.9810]],
       dtype=torch.float64)
	q_value: tensor([[-4.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6340251710132047 entropy -1.849577947955449
epoch: 15, step: 63
	action: tensor([[-0.2736, -0.5331,  0.0999, -0.1167, -0.2736,  0.1622,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-7.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5152721346417228, distance: 1.4086464712097198 entropy -3.164051615025985
LOSS epoch 15 actor 403.2913597586663 critic 1861.7702916384483
epoch: 16, step: 0
	action: tensor([[ 0.1366, -0.2356,  0.4297, -1.2862, -1.3696,  1.3995,  2.5008]],
       dtype=torch.float64)
	q_value: tensor([[-5.3670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1997766505952021, distance: 1.2534496578005876 entropy -1.9531679278893102
epoch: 16, step: 1
	action: tensor([[-4.7799,  1.6949,  0.8195,  6.5726,  0.9496,  1.2460,  7.1175]],
       dtype=torch.float64)
	q_value: tensor([[-14.9496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2534496578005876 entropy -1.059427939313233
epoch: 16, step: 2
	action: tensor([[-0.1293,  0.0164, -0.0009, -0.1673, -0.0496,  0.0987, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08260877240766407, distance: 1.0960591318204949 entropy -3.8377961227156576
epoch: 16, step: 3
	action: tensor([[ 0.9686, -0.7111, -0.2649,  0.2216,  0.6343,  0.8523,  1.7837]],
       dtype=torch.float64)
	q_value: tensor([[-4.2303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4980153599026438, distance: 0.8107779099217561 entropy -2.228690739418472
epoch: 16, step: 4
	action: tensor([[ 1.2724, -5.0222,  2.1896,  2.1040, -0.9192,  2.6190,  5.4990]],
       dtype=torch.float64)
	q_value: tensor([[-10.7723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8107779099217561 entropy -1.2796369554321363
epoch: 16, step: 5
	action: tensor([[-0.2664, -0.0446,  0.1003, -0.0376, -0.0830,  0.1794, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05932003769343308, distance: 1.1777965746079844 entropy -3.8377961227156576
epoch: 16, step: 6
	action: tensor([[-0.1528,  0.3533,  0.2149,  1.3376, -1.6617,  0.0201,  1.8866]],
       dtype=torch.float64)
	q_value: tensor([[-4.2069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1777965746079844 entropy -2.182705659859471
epoch: 16, step: 7
	action: tensor([[-0.2124, -0.0846, -0.0434,  0.0658, -0.1812,  0.0748, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1777965746079844 entropy -3.8377961227156576
epoch: 16, step: 8
	action: tensor([[-0.1037, -0.0398,  0.0422, -0.2525,  0.0560,  0.0620, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018144000287231177, distance: 1.1339152402069699 entropy -3.8377961227156576
epoch: 16, step: 9
	action: tensor([[-0.8813, -0.0025,  0.2700, -0.6688,  0.0877,  1.1418,  1.8312]],
       dtype=torch.float64)
	q_value: tensor([[-4.4437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7172426685158975, distance: 1.4995899760051936 entropy -2.2052777388630966
epoch: 16, step: 10
	action: tensor([[ 0.1270, -2.7467, -3.2110, -3.2785, -0.1451,  1.5268,  4.6136]],
       dtype=torch.float64)
	q_value: tensor([[-12.6253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4995899760051936 entropy -1.4210695697967222
epoch: 16, step: 11
	action: tensor([[-0.0392, -0.0979,  0.0203, -0.2013, -0.2381,  0.0726, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08065097513978858, distance: 1.0972280540113237 entropy -3.8377961227156576
epoch: 16, step: 12
	action: tensor([[-0.7112, -1.4486, -1.3060, -2.5994, -0.3582,  1.4001,  1.9755]],
       dtype=torch.float64)
	q_value: tensor([[-3.8877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0972280540113237 entropy -2.144669308282641
epoch: 16, step: 13
	action: tensor([[-0.0123, -0.0191,  0.1136, -0.0939, -0.2083,  0.0910, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2291235532372471, distance: 1.0047293327313533 entropy -3.8377961227156576
epoch: 16, step: 14
	action: tensor([[ 1.5417, -1.4834, -0.3225, -0.2433,  0.3523,  1.1127,  2.0427]],
       dtype=torch.float64)
	q_value: tensor([[-3.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0047293327313533 entropy -2.1160931101965867
epoch: 16, step: 15
	action: tensor([[ 0.0447, -0.0909,  0.2884,  0.4970,  0.1793,  0.1431, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6128817217235019, distance: 0.7119976038219902 entropy -3.8377961227156576
epoch: 16, step: 16
	action: tensor([[-0.1093,  0.0454,  0.0085, -0.2770,  0.0700,  0.1036, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08313336775801228, distance: 1.0957457051548558 entropy -3.8377961227156576
epoch: 16, step: 17
	action: tensor([[ 0.4516, -0.0765, -0.6363, -0.9453, -0.1617,  0.8744,  1.7762]],
       dtype=torch.float64)
	q_value: tensor([[-4.5915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4441508977131424, distance: 0.8531691584334706 entropy -2.2294042740118956
epoch: 16, step: 18
	action: tensor([[ 0.7046, -0.4859, -0.2308, -0.3061, -0.2014,  1.9636,  4.3791]],
       dtype=torch.float64)
	q_value: tensor([[-11.6401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6558304804005463, distance: 0.6713405748182186 entropy -1.4744414147335718
epoch: 16, step: 19
	action: tensor([[ 7.6085, -1.0291,  0.4191,  2.9774, -1.2473,  2.0859,  7.1174]],
       dtype=torch.float64)
	q_value: tensor([[-9.8653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6713405748182186 entropy -1.054893128059031
epoch: 16, step: 20
	action: tensor([[ 0.0693, -0.2132,  0.0459, -0.1806, -0.0193,  0.0598, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10086445288989832, distance: 1.0850987848522042 entropy -3.8377961227156576
epoch: 16, step: 21
	action: tensor([[ 1.7810,  0.0260,  0.5730, -0.8133,  0.7938,  0.9676,  2.0955]],
       dtype=torch.float64)
	q_value: tensor([[-4.0972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0850987848522042 entropy -2.093170571850085
epoch: 16, step: 22
	action: tensor([[-0.1260, -0.1038,  0.0150,  0.4250,  0.0575,  0.0632, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2845127863107534, distance: 0.9679604922964892 entropy -3.8377961227156576
epoch: 16, step: 23
	action: tensor([[ 2.0782, -0.4055,  0.8059,  0.0367,  0.9271,  0.5002,  1.9340]],
       dtype=torch.float64)
	q_value: tensor([[-4.0338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9679604922964892 entropy -2.1653898380079877
epoch: 16, step: 24
	action: tensor([[ 0.0276,  0.0217,  0.0430, -0.1493, -0.0029,  0.0880, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2811547274194526, distance: 0.9702293402034716 entropy -3.8377961227156576
epoch: 16, step: 25
	action: tensor([[ 1.2436, -0.8051,  0.3979, -1.0222, -0.7414,  0.6386,  1.9492]],
       dtype=torch.float64)
	q_value: tensor([[-4.1198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3373902124430683, distance: 1.3233836462580995 entropy -2.1534520467971596
epoch: 16, step: 26
	action: tensor([[-1.0765, -0.6826,  0.6114, -1.9431,  2.1635,  2.5244,  6.6038]],
       dtype=torch.float64)
	q_value: tensor([[-11.5295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3233836462580995 entropy -1.121847061029191
epoch: 16, step: 27
	action: tensor([[ 0.0543, -0.1290, -0.1826,  0.0927, -0.1352,  0.0682, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843055211655736, distance: 0.9681006834423327 entropy -3.8377961227156576
epoch: 16, step: 28
	action: tensor([[-1.2600,  0.4313, -0.3744, -0.6533,  0.8190,  0.2256,  1.8339]],
       dtype=torch.float64)
	q_value: tensor([[-3.7393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9430974447575557, distance: 1.595159018583193 entropy -2.2112773918307465
epoch: 16, step: 29
	action: tensor([[ 0.3729,  0.2229, -0.1588,  1.5149, -0.9273,  1.1653,  2.7431]],
       dtype=torch.float64)
	q_value: tensor([[-13.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.595159018583193 entropy -1.8511621869731498
epoch: 16, step: 30
	action: tensor([[ 0.0657, -0.1618, -0.0210,  0.0211, -0.0713,  0.0184, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24728933028713607, distance: 0.9928204848678506 entropy -3.8377961227156576
epoch: 16, step: 31
	action: tensor([[ 0.0331, -0.0595, -1.6583,  0.2378,  0.3463,  0.3932,  1.9662]],
       dtype=torch.float64)
	q_value: tensor([[-3.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0746854595701546, distance: 1.1007821705306353 entropy -2.149923319662471
epoch: 16, step: 32
	action: tensor([[-1.2013, -0.3976, -1.3552,  0.6789,  0.8864,  1.8195,  3.1054]],
       dtype=torch.float64)
	q_value: tensor([[-12.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5971439151869984, distance: 1.4462012044331851 entropy -1.7790708567944287
epoch: 16, step: 33
	action: tensor([[-3.7623, -2.1600, -1.9223, -5.1302, -3.3961,  3.4497,  5.7514]],
       dtype=torch.float64)
	q_value: tensor([[-20.4937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4462012044331851 entropy -1.2413772457846786
epoch: 16, step: 34
	action: tensor([[-0.0555, -0.2381,  0.0224,  0.0624, -0.2379, -0.0256, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06400133302743083, distance: 1.1071190110576583 entropy -3.8377961227156576
epoch: 16, step: 35
	action: tensor([[ 1.2235, -1.3426,  0.0927, -0.9193,  0.8696,  1.2426,  1.9523]],
       dtype=torch.float64)
	q_value: tensor([[-3.5601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1071190110576583 entropy -2.158790515215068
epoch: 16, step: 36
	action: tensor([[-0.0552, -0.1597,  0.1968, -0.1587,  0.2186,  0.1075, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041143131201471284, distance: 1.1205560509469608 entropy -3.8377961227156576
epoch: 16, step: 37
	action: tensor([[ 0.7764,  0.7836, -0.5659, -0.7500,  0.3320,  1.4090,  2.0939]],
       dtype=torch.float64)
	q_value: tensor([[-4.6186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9784119754026446, distance: 0.16813694238173402 entropy -2.092154677518304
epoch: 16, step: 38
	action: tensor([[ 1.3335, -0.7054, -1.5651,  0.9424, -2.5297,  1.9680,  5.4750]],
       dtype=torch.float64)
	q_value: tensor([[-13.1216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.037082147926632, distance: 1.1229264523772582 entropy -1.2776242686337458
epoch: 16, step: 39
	action: tensor([[ 0.4451, -2.9909, -2.5204,  3.6342, -0.6865,  4.1466,  8.2701]],
       dtype=torch.float64)
	q_value: tensor([[-10.3985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1229264523772582 entropy -0.9402712418677645
epoch: 16, step: 40
	action: tensor([[ 0.0892, -0.1560,  0.2520,  0.2084, -0.0432,  0.0697, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4363909943693288, distance: 0.8591038300441955 entropy -3.8377961227156576
epoch: 16, step: 41
	action: tensor([[-0.2111, -0.5974,  0.1828, -0.2107, -1.8460,  0.0702,  2.3410]],
       dtype=torch.float64)
	q_value: tensor([[-3.7254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5765215390755469, distance: 1.4368341699576463 entropy -2.0009130976095704
epoch: 16, step: 42
	action: tensor([[-0.2115, -1.3895,  2.7255, -3.0502,  2.8100,  1.6993,  5.3286]],
       dtype=torch.float64)
	q_value: tensor([[-12.1015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4368341699576463 entropy -1.3181899362852434
epoch: 16, step: 43
	action: tensor([[-0.0983, -0.1406, -0.0783, -0.3650,  0.1227,  0.0503, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09137026828419348, distance: 1.195481204370933 entropy -3.8377961227156576
epoch: 16, step: 44
	action: tensor([[ 0.2482,  1.0694, -0.0056,  0.2752,  1.8267,  0.5656,  1.7335]],
       dtype=torch.float64)
	q_value: tensor([[-4.8603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.195481204370933 entropy -2.250415661966406
epoch: 16, step: 45
	action: tensor([[-0.1759, -0.0570,  0.1981,  0.0751,  0.0961,  0.0754, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.195481204370933 entropy -3.8377961227156576
epoch: 16, step: 46
	action: tensor([[-0.0763, -0.0508, -0.1199, -0.0939,  0.1165,  0.0603, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13462294608272152, distance: 1.0645336178609912 entropy -3.8377961227156576
epoch: 16, step: 47
	action: tensor([[-0.5192,  0.4112, -0.5819,  0.7388, -0.3236,  0.9527,  1.6546]],
       dtype=torch.float64)
	q_value: tensor([[-4.3252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1778153273055314, distance: 1.2419247842504046 entropy -2.2911075345108984
epoch: 16, step: 48
	action: tensor([[ 0.1416, -0.0229,  0.1089, -0.2544,  0.1110,  0.0175, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24854777547205653, distance: 0.991990197173463 entropy -3.8377961227156576
epoch: 16, step: 49
	action: tensor([[-3.3355e+00, -1.1469e-03, -5.6277e-01, -6.2566e-01,  5.8326e-01,
          9.6619e-01,  2.0861e+00]], dtype=torch.float64)
	q_value: tensor([[-4.3856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.991990197173463 entropy -2.093944303784754
epoch: 16, step: 50
	action: tensor([[-0.1732, -0.0640, -0.1526, -0.1513, -0.3577, -0.0293, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03920642471639857, distance: 1.1665614072267474 entropy -3.8377961227156576
epoch: 16, step: 51
	action: tensor([[ 0.2585, -0.1141, -1.3104,  0.6340, -0.3191,  0.5152,  1.6077]],
       dtype=torch.float64)
	q_value: tensor([[-3.9780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10638901819653157, distance: 1.0817600580954125 entropy -2.320912738110796
epoch: 16, step: 52
	action: tensor([[-0.9799, -4.0881, -0.5020,  1.3938,  0.3193,  1.1474,  3.5069]],
       dtype=torch.float64)
	q_value: tensor([[-9.9166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0817600580954125 entropy -1.6796620821114063
epoch: 16, step: 53
	action: tensor([[ 0.1595, -0.0980, -0.0278,  0.4377,  0.0827,  0.1174, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.622208941293235, distance: 0.7033678782103796 entropy -3.8377961227156576
epoch: 16, step: 54
	action: tensor([[ 1.4192,  0.0771, -0.2658,  0.3646, -1.0760,  1.3398,  2.2027]],
       dtype=torch.float64)
	q_value: tensor([[-4.0197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9356161678914019, distance: 0.2903655603837289 entropy -2.055045503372171
epoch: 16, step: 55
	action: tensor([[-6.4918,  3.8061,  1.5168, -3.3433, -2.3491,  1.5979,  7.1942]],
       dtype=torch.float64)
	q_value: tensor([[-11.3517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.2903655603837289 entropy -1.0585534696226753
epoch: 16, step: 56
	action: tensor([[-0.1902, -0.0673, -0.1483,  0.0040, -0.2731,  0.0959, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0071611889544750396, distance: 1.140239459277551 entropy -3.8377961227156576
epoch: 16, step: 57
	action: tensor([[ 1.2348, -0.0201, -0.8508, -1.2049, -0.2751,  0.3510,  1.7124]],
       dtype=torch.float64)
	q_value: tensor([[-3.9172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060888120218507646, distance: 1.1089586695746678 entropy -2.2691169651623815
epoch: 16, step: 58
	action: tensor([[-2.0922,  0.7911,  2.1307, -2.4484, -0.9785,  2.0331,  4.5443]],
       dtype=torch.float64)
	q_value: tensor([[-11.2652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1089586695746678 entropy -1.442971145468284
epoch: 16, step: 59
	action: tensor([[-0.0081, -0.0862,  0.2024, -0.3042, -0.1324,  0.1163, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06483818037330613, distance: 1.1066239800187825 entropy -3.8377961227156576
epoch: 16, step: 60
	action: tensor([[-1.0500e+00,  1.9413e-01,  1.7140e-03, -1.2481e+00,  1.3686e+00,
          1.5838e+00,  2.2135e+00]], dtype=torch.float64)
	q_value: tensor([[-4.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1904927997636383, distance: 1.2485906540111158 entropy -2.0451977118738944
epoch: 16, step: 61
	action: tensor([[ 1.2265, -1.3691, -0.4830, -1.2971, -0.2625,  2.8071,  5.3091]],
       dtype=torch.float64)
	q_value: tensor([[-18.3612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2485906540111158 entropy -1.2903531562979822
epoch: 16, step: 62
	action: tensor([[-0.1216, -0.0829,  0.0465,  0.1486,  0.1731,  0.0978, -0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-12.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18316340662823138, distance: 1.0342470384702123 entropy -3.8377961227156576
epoch: 16, step: 63
	action: tensor([[ 1.5627, -0.7114, -1.3693, -0.6605, -0.7440,  0.9206,  1.8812]],
       dtype=torch.float64)
	q_value: tensor([[-4.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4453634483939286, distance: 1.375768074336802 entropy -2.1844068557833296
LOSS epoch 16 actor 354.1780126874911 critic 1688.1406332973252
epoch: 17, step: 0
	action: tensor([[ 4.3687, -6.1701,  5.4203, -0.2684,  0.6481,  1.5881,  5.8968]],
       dtype=torch.float64)
	q_value: tensor([[-18.3964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.375768074336802 entropy -1.1951455231943195
epoch: 17, step: 1
	action: tensor([[-0.0360, -0.1103, -0.2890, -0.0981,  0.1850,  0.0627,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14483290215121825, distance: 1.0582351550467795 entropy -3.623293297737406
epoch: 17, step: 2
	action: tensor([[ 0.2740, -1.1949,  0.4775, -0.0956,  1.2865,  1.1999,  1.6739]],
       dtype=torch.float64)
	q_value: tensor([[-6.8411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35038624989464195, distance: 1.3297980655586914 entropy -2.2616530818360867
epoch: 17, step: 3
	action: tensor([[-1.1057,  2.0910,  4.2709, -1.5222,  0.5996,  1.9536,  6.6481]],
       dtype=torch.float64)
	q_value: tensor([[-18.0333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3297980655586914 entropy -1.079844989496112
epoch: 17, step: 4
	action: tensor([[-0.0095, -0.1778,  0.2391, -0.1281, -0.1496,  0.0636,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07726289902349115, distance: 1.0992480014646244 entropy -3.623293297737406
epoch: 17, step: 5
	action: tensor([[ 0.5158, -0.0826, -1.3580, -0.9458, -0.2069,  0.6049,  2.4745]],
       dtype=torch.float64)
	q_value: tensor([[-5.4767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6294459554113232, distance: 0.6965983994201218 entropy -1.9298401812055235
epoch: 17, step: 6
	action: tensor([[0.6134, 1.3872, 0.0720, 0.8322, 0.8457, 1.5002, 4.1575]],
       dtype=torch.float64)
	q_value: tensor([[-21.1420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6965983994201218 entropy -1.4926487540902582
epoch: 17, step: 7
	action: tensor([[-0.2059, -0.0381, -0.1789, -0.0016,  0.0239,  0.1517,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025595411207497087, distance: 1.1296043432286873 entropy -3.623293297737406
epoch: 17, step: 8
	action: tensor([[-2.0335, -0.1942, -0.6604, -1.1380,  1.3409,  0.6668,  1.7312]],
       dtype=torch.float64)
	q_value: tensor([[-6.4070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1296043432286873 entropy -2.237577373195573
epoch: 17, step: 9
	action: tensor([[ 0.1129, -0.1087,  0.0604,  0.1675, -0.2410,  0.1210,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4327688718175463, distance: 0.8618599923317252 entropy -3.623293297737406
epoch: 17, step: 10
	action: tensor([[-2.0008, -1.2201, -0.3279, -1.4542,  0.3369,  1.4210,  2.4745]],
       dtype=torch.float64)
	q_value: tensor([[-5.0933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8618599923317252 entropy -1.9338165379111423
epoch: 17, step: 11
	action: tensor([[-0.0367, -0.0154, -0.1190, -0.3842, -0.0569,  0.0914,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09419802554394441, distance: 1.0891139600470283 entropy -3.623293297737406
epoch: 17, step: 12
	action: tensor([[-0.0142, -0.6368, -0.1526, -0.6985, -0.8326,  0.1130,  1.8720]],
       dtype=torch.float64)
	q_value: tensor([[-6.8737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5747675189501757, distance: 1.4360346447449412 entropy -2.163059760394476
epoch: 17, step: 13
	action: tensor([[ 4.5968,  0.1667, -3.5809,  3.0649, -0.3724,  1.2719,  4.0326]],
       dtype=torch.float64)
	q_value: tensor([[-15.6721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4360346447449412 entropy -1.5182935517288896
epoch: 17, step: 14
	action: tensor([[ 0.0885, -0.0603,  0.0473,  0.3982, -0.1027,  0.0220,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5379056199020507, distance: 0.7778968925828523 entropy -3.623293297737406
epoch: 17, step: 15
	action: tensor([[ 0.0043, -1.7271, -0.7537, -1.8272,  0.1081,  1.4389,  2.3860]],
       dtype=torch.float64)
	q_value: tensor([[-5.0119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7778968925828523 entropy -1.9667171996842914
epoch: 17, step: 16
	action: tensor([[-0.0503, -0.0849, -0.1355,  0.2030, -0.1127,  0.0060,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2514582871069676, distance: 0.9900672538341164 entropy -3.623293297737406
epoch: 17, step: 17
	action: tensor([[ 0.2111,  0.2457,  0.6915, -1.9113, -0.0893,  0.8528,  1.9397]],
       dtype=torch.float64)
	q_value: tensor([[-5.3425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0834688617455972, distance: 1.191145763671396 entropy -2.145173806185684
epoch: 17, step: 18
	action: tensor([[-0.3449, -1.9841, -0.8300, -0.9941, -2.8430, -0.1443,  6.0217]],
       dtype=torch.float64)
	q_value: tensor([[-20.0365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.191145763671396 entropy -1.1573071383021387
epoch: 17, step: 19
	action: tensor([[ 0.2294,  0.1394,  0.1590, -0.0324, -0.0224,  0.2093,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.65039290844803, distance: 0.6766230835030707 entropy -3.623293297737406
epoch: 17, step: 20
	action: tensor([[ 0.0174,  0.2897, -0.4435, -0.2223, -1.6991, -0.0969,  2.5973]],
       dtype=torch.float64)
	q_value: tensor([[-5.8961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.522490197272348, distance: 0.7907657288185181 entropy -1.885739805577149
epoch: 17, step: 21
	action: tensor([[-2.0112, -1.7736, -0.2033,  3.3920,  0.4460,  2.5851,  4.6631]],
       dtype=torch.float64)
	q_value: tensor([[-18.1208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7907657288185181 entropy -1.4005912219998575
epoch: 17, step: 22
	action: tensor([[-0.0936, -0.1064,  0.0367, -0.3694, -0.0564,  0.0034,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09563387185982597, distance: 1.1978140925199596 entropy -3.623293297737406
epoch: 17, step: 23
	action: tensor([[ 0.1676, -0.1152,  1.2209,  0.4484,  1.9313,  0.7352,  1.9976]],
       dtype=torch.float64)
	q_value: tensor([[-6.5702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45146987854633236, distance: 0.8475336179271413 entropy -2.109965040809271
epoch: 17, step: 24
	action: tensor([[ 5.8408, -1.4559, -1.5574, -5.2565,  6.5326,  2.3084,  7.2063]],
       dtype=torch.float64)
	q_value: tensor([[-17.9727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8475336179271413 entropy -1.009279483828471
epoch: 17, step: 25
	action: tensor([[-0.0462, -0.1501, -0.0943,  0.1457, -0.0254,  0.0349,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19603694224066426, distance: 1.0260646839622618 entropy -3.623293297737406
epoch: 17, step: 26
	action: tensor([[-0.4351,  0.1412,  0.5882, -0.6276,  0.7884,  1.0473,  2.0090]],
       dtype=torch.float64)
	q_value: tensor([[-5.5726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0724919424491921, distance: 1.1850964912419775 entropy -2.1138365402542965
epoch: 17, step: 27
	action: tensor([[ 1.3159, -0.9593,  1.0535, -2.0217,  0.4945,  0.5498,  5.3484]],
       dtype=torch.float64)
	q_value: tensor([[-18.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0625533308758246, distance: 1.1795926608119425 entropy -1.2572192294278253
epoch: 17, step: 28
	action: tensor([[ 5.3508, -6.7491,  5.7321, -0.3060,  5.8684,  3.0576,  9.0432]],
       dtype=torch.float64)
	q_value: tensor([[-15.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1795926608119425 entropy -0.8199736865359816
epoch: 17, step: 29
	action: tensor([[-0.1085, -0.0459, -0.1585, -0.0384, -0.0277, -0.0125,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10450953863632773, distance: 1.0828970617990574 entropy -3.623293297737406
epoch: 17, step: 30
	action: tensor([[ 0.2629, -1.0833,  0.2731, -0.4703, -0.5247,  0.3936,  1.7077]],
       dtype=torch.float64)
	q_value: tensor([[-5.8648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7926468619150338, distance: 1.5321598088562718 entropy -2.2487551586762553
epoch: 17, step: 31
	action: tensor([[ 0.9188, -1.8669,  0.5526, -1.8326,  0.3393,  1.7531,  5.1090]],
       dtype=torch.float64)
	q_value: tensor([[-14.1417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5321598088562718 entropy -1.3126992619628841
epoch: 17, step: 32
	action: tensor([[-0.0052, -0.0531, -0.1100,  0.0801,  0.0206, -0.0562,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2659855580189757, distance: 0.9804128520771076 entropy -3.623293297737406
epoch: 17, step: 33
	action: tensor([[ 0.5803, -1.2570,  0.3246, -0.3942, -0.0102,  0.5693,  1.8704]],
       dtype=torch.float64)
	q_value: tensor([[-5.5578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9804128520771076 entropy -2.1726645482496743
epoch: 17, step: 34
	action: tensor([[ 0.1534, -0.2338, -0.0224,  0.1121,  0.0320,  0.0822,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32810501438247075, distance: 0.938009837756087 entropy -3.623293297737406
epoch: 17, step: 35
	action: tensor([[-0.4886,  0.4773, -1.7903, -0.3653,  1.7276,  1.5189,  2.3755]],
       dtype=torch.float64)
	q_value: tensor([[-5.7211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.938009837756087 entropy -1.967216826419078
epoch: 17, step: 36
	action: tensor([[-0.1154, -0.1029, -0.1911, -0.4186,  0.0424,  0.0096,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.938009837756087 entropy -3.623293297737406
epoch: 17, step: 37
	action: tensor([[-0.2136, -0.1749, -0.1625,  0.1050, -0.2525, -0.0322,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08050153791553849, distance: 1.189513534693334 entropy -3.623293297737406
epoch: 17, step: 38
	action: tensor([[ 0.4958, -0.9729, -1.4608,  0.9173,  1.0190,  0.5363,  1.7782]],
       dtype=torch.float64)
	q_value: tensor([[-5.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14958214723488505, distance: 1.2269495300472901 entropy -2.2202082150637614
epoch: 17, step: 39
	action: tensor([[-3.6547, -1.9144,  0.9367, -0.7306,  0.0413,  2.8913,  4.3300]],
       dtype=torch.float64)
	q_value: tensor([[-18.3476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2269495300472901 entropy -1.458312637418045
epoch: 17, step: 40
	action: tensor([[-0.0274, -0.3139, -0.0772,  0.0368,  0.1966,  0.0887,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0488654053109997, distance: 1.1160346597647064 entropy -3.623293297737406
epoch: 17, step: 41
	action: tensor([[-1.6265, -0.3037,  1.4427, -0.4306,  0.8333, -0.0703,  2.1629]],
       dtype=torch.float64)
	q_value: tensor([[-6.5059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1160346597647064 entropy -2.0485056394845724
epoch: 17, step: 42
	action: tensor([[-0.0838,  0.0298,  0.2498,  0.0718, -0.1330,  0.0936,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.275136318512342, distance: 0.9742824139453161 entropy -3.623293297737406
epoch: 17, step: 43
	action: tensor([[ 1.6287, -0.0069,  0.6561, -1.7663, -0.3529,  1.1392,  2.3327]],
       dtype=torch.float64)
	q_value: tensor([[-5.2987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9742824139453161 entropy -1.9815611358858691
epoch: 17, step: 44
	action: tensor([[ 0.2111, -0.1211,  0.1347, -0.0475, -0.2145,  0.0919,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38833114437935357, distance: 0.8949831880102708 entropy -3.623293297737406
epoch: 17, step: 45
	action: tensor([[ 0.7727, -0.1142, -0.8676,  0.5242,  1.1869,  0.9205,  2.5983]],
       dtype=torch.float64)
	q_value: tensor([[-5.2652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9457689478062385, distance: 0.26648987224661563 entropy -1.8882747072363049
epoch: 17, step: 46
	action: tensor([[-4.4886,  1.2120, -2.2926, -2.0116,  6.9727,  2.0350,  5.8792]],
       dtype=torch.float64)
	q_value: tensor([[-20.2585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.26648987224661563 entropy -1.1907402498633854
epoch: 17, step: 47
	action: tensor([[ 0.1441, -0.1467,  0.1180, -0.2163,  0.0322,  0.0315,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20296895386258618, distance: 1.0216315880346625 entropy -3.623293297737406
epoch: 17, step: 48
	action: tensor([[ 0.2978, -0.6676, -0.2639, -0.1288, -0.6218,  0.5301,  2.4173]],
       dtype=torch.float64)
	q_value: tensor([[-6.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09804872601326675, distance: 1.1991333992339817 entropy -1.9475733020484471
epoch: 17, step: 49
	action: tensor([[ 4.3541e-01, -3.8725e+00, -1.4049e+00, -2.4920e+00, -4.9559e-03,
          3.5111e+00,  5.2309e+00]], dtype=torch.float64)
	q_value: tensor([[-17.2591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1991333992339817 entropy -1.298610819467956
epoch: 17, step: 50
	action: tensor([[-0.3560, -0.1987,  0.1732,  0.2584,  0.1525,  0.0106,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1510203011147906, distance: 1.2277167612997353 entropy -3.623293297737406
epoch: 17, step: 51
	action: tensor([[ 1.7542, -0.9906, -0.6170, -0.5856, -0.1974,  1.0610,  2.0916]],
       dtype=torch.float64)
	q_value: tensor([[-6.1571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2277167612997353 entropy -2.078124657237445
epoch: 17, step: 52
	action: tensor([[ 0.1625, -0.3293, -0.1902, -0.3039, -0.0999,  0.0451,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023825195221933493, distance: 1.1306299625231888 entropy -3.623293297737406
epoch: 17, step: 53
	action: tensor([[ 2.1828, -0.7125, -0.3374,  0.5886, -0.8571,  1.3486,  2.1956]],
       dtype=torch.float64)
	q_value: tensor([[-6.4536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1306299625231888 entropy -2.03107598969792
epoch: 17, step: 54
	action: tensor([[-0.0963,  0.0348,  0.1542,  0.3547,  0.1335,  0.0037,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3807957987037355, distance: 0.9004791064266545 entropy -3.623293297737406
epoch: 17, step: 55
	action: tensor([[-0.9576, -1.2753,  0.0092, -0.8437,  0.8669,  0.8973,  2.2048]],
       dtype=torch.float64)
	q_value: tensor([[-5.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9004791064266545 entropy -2.0314437900301985
epoch: 17, step: 56
	action: tensor([[ 0.1980, -0.1103, -0.1967,  0.0868, -0.2268,  0.1966,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.494562075996671, distance: 0.8135619070373584 entropy -3.623293297737406
epoch: 17, step: 57
	action: tensor([[ 1.4313, -0.4100, -0.6515, -0.5005,  0.3671,  0.4923,  2.3486]],
       dtype=torch.float64)
	q_value: tensor([[-5.5486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014153821230399544, distance: 1.152414221113177 entropy -1.978432125704588
epoch: 17, step: 58
	action: tensor([[ 3.4067,  0.7402,  4.5968,  1.3231, -2.4448,  1.5920,  5.9778]],
       dtype=torch.float64)
	q_value: tensor([[-18.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.152414221113177 entropy -1.1773856986537876
epoch: 17, step: 59
	action: tensor([[-0.2291, -0.0465,  0.0839, -0.2416, -0.0456,  0.0888,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14197787981102894, distance: 1.2228847780251602 entropy -3.623293297737406
epoch: 17, step: 60
	action: tensor([[-0.6243, -0.6876,  0.0770, -1.4827,  0.3347,  0.7547,  1.9665]],
       dtype=torch.float64)
	q_value: tensor([[-6.5124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7622796491390968, distance: 1.5191270825264274 entropy -2.1249352868606675
epoch: 17, step: 61
	action: tensor([[ 1.1334, -0.5292, -0.9158, -3.1484, -3.3404,  2.2028,  4.6117]],
       dtype=torch.float64)
	q_value: tensor([[-21.0578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5191270825264274 entropy -1.3880376599340478
epoch: 17, step: 62
	action: tensor([[ 0.0858, -0.0178,  0.1344,  0.1359,  0.1169, -0.0267,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43897497472739955, distance: 0.8571321993089529 entropy -3.623293297737406
epoch: 17, step: 63
	action: tensor([[ 0.6226, -1.8352, -0.4561,  1.1513,  0.8955,  1.4822,  2.3056]],
       dtype=torch.float64)
	q_value: tensor([[-5.5318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8571321993089529 entropy -1.990770022945295
LOSS epoch 17 actor 356.8796483058445 critic 1234.9831363409128
epoch: 18, step: 0
	action: tensor([[ 0.0889,  0.0577,  0.2374,  0.2811,  0.3123,  0.0343, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6074820020751948, distance: 0.7169460573069835 entropy -3.588663177422626
epoch: 18, step: 1
	action: tensor([[ 1.8712,  0.5690, -0.9291, -0.6188, -0.3429,  0.8516,  2.3907]],
       dtype=torch.float64)
	q_value: tensor([[-7.9325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7169460573069835 entropy -1.900821646536431
epoch: 18, step: 2
	action: tensor([[-0.2414, -0.1277,  0.0847,  0.3002,  0.1982,  0.0508, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07370292690132274, distance: 1.1013664407654735 entropy -3.588663177422626
epoch: 18, step: 3
	action: tensor([[-0.9832, -1.0652, -0.2558,  0.3651,  0.5071,  0.3015,  1.9523]],
       dtype=torch.float64)
	q_value: tensor([[-8.1230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3342829159015586, distance: 1.748370374994808 entropy -2.0771963226785113
epoch: 18, step: 4
	action: tensor([[ 0.3418, -0.0223,  0.8949, -0.7631, -2.6043,  1.8669,  2.9315]],
       dtype=torch.float64)
	q_value: tensor([[-23.7235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22435973075846716, distance: 1.007829038493501 entropy -1.7103863634426764
epoch: 18, step: 5
	action: tensor([[-2.3784,  1.4305,  1.8022,  0.3369, -5.9109,  3.1821,  8.5256]],
       dtype=torch.float64)
	q_value: tensor([[-33.3560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.007829038493501 entropy -0.8133381218162229
epoch: 18, step: 6
	action: tensor([[-0.0966,  0.0395,  0.0658, -0.2626,  0.0430,  0.0691, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08441948339747085, distance: 1.0949769181870155 entropy -3.588663177422626
epoch: 18, step: 7
	action: tensor([[-1.5522,  0.1238,  0.6397, -0.7296, -0.5797,  0.6829,  1.8344]],
       dtype=torch.float64)
	q_value: tensor([[-8.6393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.682045960653043, distance: 1.8740872290711001 entropy -2.1207798291720454
epoch: 18, step: 8
	action: tensor([[ 2.3667, -1.8490, -0.8811, -0.9632, -1.1500,  0.2488,  3.6281]],
       dtype=torch.float64)
	q_value: tensor([[-24.8958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8740872290711001 entropy -1.5241562888064852
epoch: 18, step: 9
	action: tensor([[ 0.0881, -0.1966,  0.0767, -0.0936,  0.3377,  0.1265, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21589293394275655, distance: 1.0133147795876396 entropy -3.588663177422626
epoch: 18, step: 10
	action: tensor([[ 3.3889, -0.6927, -1.7094, -0.4573,  1.2260,  1.0852,  2.2309]],
       dtype=torch.float64)
	q_value: tensor([[-9.1714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0133147795876396 entropy -1.9583075676010726
epoch: 18, step: 11
	action: tensor([[ 0.0578, -0.2127,  0.4046,  0.1292,  0.0055,  0.0390, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27821815430260255, distance: 0.9722090746827067 entropy -3.588663177422626
epoch: 18, step: 12
	action: tensor([[-1.5690, -0.9321, -1.3392, -0.1642, -2.2507,  1.3475,  2.6607]],
       dtype=torch.float64)
	q_value: tensor([[-7.1356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9009364872715118, distance: 1.5777583840700247 entropy -1.810163889297418
epoch: 18, step: 13
	action: tensor([[-5.1756,  0.2654, -1.0422,  2.7684,  1.5049,  2.1385,  3.9808]],
       dtype=torch.float64)
	q_value: tensor([[-38.7612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5777583840700247 entropy -1.4611819194770914
epoch: 18, step: 14
	action: tensor([[ 0.1913, -0.0920,  0.0649,  0.2993,  0.0418,  0.0418, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5825442148267095, distance: 0.7393701875938669 entropy -3.588663177422626
epoch: 18, step: 15
	action: tensor([[-0.4818, -0.3329, -0.5638, -1.0340,  2.1528,  0.9686,  2.3711]],
       dtype=torch.float64)
	q_value: tensor([[-6.9799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17086922499928625, distance: 1.0420011802683569 entropy -1.9121324980886336
epoch: 18, step: 16
	action: tensor([[-1.6128, -1.4181, -1.1254, -0.2743, -6.6524,  1.0416,  3.7512]],
       dtype=torch.float64)
	q_value: tensor([[-38.2407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0420011802683569 entropy -1.4750381519658728
epoch: 18, step: 17
	action: tensor([[-0.1692, -0.0104,  0.0582, -0.1519,  0.1934,  0.1004, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01824098521201256, distance: 1.133859236373341 entropy -3.588663177422626
epoch: 18, step: 18
	action: tensor([[ 0.9603, -0.7076, -0.8772, -0.6707,  1.0713,  0.3200,  1.7985]],
       dtype=torch.float64)
	q_value: tensor([[-8.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1822441558025858, distance: 1.244257540034588 entropy -2.1390733351550177
epoch: 18, step: 19
	action: tensor([[-3.9670, -3.4006,  0.5497, -2.3475, -1.7663,  2.0534,  3.8296]],
       dtype=torch.float64)
	q_value: tensor([[-25.1503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.244257540034588 entropy -1.4856224399488416
epoch: 18, step: 20
	action: tensor([[-0.0441, -0.2711, -0.0936,  0.0160, -0.2498,  0.1030, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043807423212740115, distance: 1.118998172290929 entropy -3.588663177422626
epoch: 18, step: 21
	action: tensor([[ 1.9614,  0.6149,  0.3684, -1.0281, -1.1046,  0.6994,  2.0235]],
       dtype=torch.float64)
	q_value: tensor([[-7.4184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.118998172290929 entropy -2.049837232327396
epoch: 18, step: 22
	action: tensor([[-0.0822, -0.0929,  0.1513,  0.1725,  0.0323,  0.0258, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2255418239040795, distance: 1.0070607686511452 entropy -3.588663177422626
epoch: 18, step: 23
	action: tensor([[ 1.8065,  0.8253, -0.5974,  0.3053, -0.5432,  0.1780,  2.0630]],
       dtype=torch.float64)
	q_value: tensor([[-7.1927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0070607686511452 entropy -2.0305709295734515
epoch: 18, step: 24
	action: tensor([[-0.2062, -0.0903, -0.1082, -0.1415,  0.3468, -0.0080, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10455559127236969, distance: 1.2026810898547653 entropy -3.588663177422626
epoch: 18, step: 25
	action: tensor([[ 0.3897, -1.0216, -0.3998, -0.7331,  0.1891,  0.3000,  1.4868]],
       dtype=torch.float64)
	q_value: tensor([[-9.6255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6707726445140669, distance: 1.4791607400938949 entropy -2.295845446524296
epoch: 18, step: 26
	action: tensor([[-6.3469, -0.1186, -0.9873,  1.8137, -2.4844,  1.6579,  3.4811]],
       dtype=torch.float64)
	q_value: tensor([[-20.5792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16788005502688064, distance: 1.2366756626167503 entropy -1.5740809451902344
epoch: 18, step: 27
	action: tensor([[ 8.3250, -3.5669, -2.4842, -1.7346, -7.2086,  3.5789,  8.5490]],
       dtype=torch.float64)
	q_value: tensor([[-18.4667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2366756626167503 entropy -0.8240441327803519
epoch: 18, step: 28
	action: tensor([[ 0.0320, -0.1219, -0.1857,  0.1003, -0.0968,  0.0687, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28630051449505234, distance: 0.9667504549221845 entropy -3.588663177422626
epoch: 18, step: 29
	action: tensor([[ 1.1595,  0.5171, -0.0726, -1.0155, -0.4381,  0.6003,  1.8403]],
       dtype=torch.float64)
	q_value: tensor([[-7.3859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7658386706464726, distance: 0.5537506904669377 entropy -2.1298414204729625
epoch: 18, step: 30
	action: tensor([[-3.0592,  0.1784, -0.4781, -3.5114,  1.3198,  1.1718,  4.9198]],
       dtype=torch.float64)
	q_value: tensor([[-22.2614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5537506904669377 entropy -1.2780849407407306
epoch: 18, step: 31
	action: tensor([[-0.1225, -0.0899,  0.0148, -0.3694, -0.4735,  0.0376, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10663049712350547, distance: 1.2038101770753853 entropy -3.588663177422626
epoch: 18, step: 32
	action: tensor([[ 1.6521, -0.3678, -0.6121, -1.2501, -0.4574,  0.5378,  1.9375]],
       dtype=torch.float64)
	q_value: tensor([[-8.2976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2038101770753853 entropy -2.076496277360851
epoch: 18, step: 33
	action: tensor([[ 0.0137, -0.1408, -0.1254,  0.1622, -0.0262, -0.0658, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2570383676522249, distance: 0.9863700720281516 entropy -3.588663177422626
epoch: 18, step: 34
	action: tensor([[ 0.7313, -0.0991, -0.5405,  0.0917, -1.3360,  1.3536,  1.8233]],
       dtype=torch.float64)
	q_value: tensor([[-7.0488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5814484466535867, distance: 0.7403399278530948 entropy -2.1372572834253267
epoch: 18, step: 35
	action: tensor([[-2.9307,  0.9389,  3.0348, -3.4266,  0.0957,  4.2360,  5.1447]],
       dtype=torch.float64)
	q_value: tensor([[-22.7995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7403399278530948 entropy -1.2544008274510259
epoch: 18, step: 36
	action: tensor([[ 0.0176, -0.1108,  0.0239, -0.0492, -0.0663, -0.0075, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2102199451199107, distance: 1.0169738227951792 entropy -3.588663177422626
epoch: 18, step: 37
	action: tensor([[-0.6152, -0.7738, -0.8566,  0.5890,  0.1791,  0.2485,  1.9839]],
       dtype=torch.float64)
	q_value: tensor([[-7.2593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0205875801695772, distance: 1.6266553012112641 entropy -2.0619821028585323
epoch: 18, step: 38
	action: tensor([[ 3.1231,  0.5313,  0.4823, -0.1284, -0.5955,  0.3242,  2.4804]],
       dtype=torch.float64)
	q_value: tensor([[-23.4108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6266553012112641 entropy -1.8587038621787266
epoch: 18, step: 39
	action: tensor([[-0.1589, -0.2250,  0.1194,  0.5649, -0.1149,  0.0167, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2512077146913583, distance: 0.9902329512036004 entropy -3.588663177422626
epoch: 18, step: 40
	action: tensor([[ 0.9684, -1.0879, -0.9329, -2.3966,  1.7176,  0.7860,  2.2534]],
       dtype=torch.float64)
	q_value: tensor([[-6.7978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9902329512036004 entropy -1.9594493482212172
epoch: 18, step: 41
	action: tensor([[ 0.1926, -0.0890, -0.0915,  0.0980, -0.1474,  0.0089, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47092457796047393, distance: 0.8323682130903945 entropy -3.588663177422626
epoch: 18, step: 42
	action: tensor([[ 1.0412, -0.7486, -0.7209, -0.4020,  0.7270,  0.7165,  2.1209]],
       dtype=torch.float64)
	q_value: tensor([[-6.7748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00243844841047669, distance: 1.1429481902095666 entropy -2.007251610117856
epoch: 18, step: 43
	action: tensor([[ 6.2261,  1.5105,  0.7809,  0.5287, -1.7620,  0.0726,  4.5924]],
       dtype=torch.float64)
	q_value: tensor([[-25.8530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1429481902095666 entropy -1.3343337844686889
epoch: 18, step: 44
	action: tensor([[-0.2866,  0.0082, -0.0196,  0.2724, -0.2719,  0.1559, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0803774631787364, distance: 1.0973912579040843 entropy -3.588663177422626
epoch: 18, step: 45
	action: tensor([[ 1.4847, -1.2369,  1.6541, -0.1074, -1.9180,  0.9898,  1.8690]],
       dtype=torch.float64)
	q_value: tensor([[-7.1789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17494136490022982, distance: 1.2404086615494132 entropy -2.1171851800780166
epoch: 18, step: 46
	action: tensor([[ -0.6806,  -8.3019,  -4.3830,  -2.5399, -11.5790,   3.0621,   8.9997]],
       dtype=torch.float64)
	q_value: tensor([[-22.1801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2404086615494132 entropy -0.7642920769088724
epoch: 18, step: 47
	action: tensor([[-0.1691, -0.1129, -0.0410, -0.1921, -0.0799,  0.0109, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10061958496646306, distance: 1.2005363426699112 entropy -3.588663177422626
epoch: 18, step: 48
	action: tensor([[-0.6256,  0.2544, -1.0183, -0.9320, -2.8556,  0.8123,  1.6899]],
       dtype=torch.float64)
	q_value: tensor([[-8.2768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19987072209392287, distance: 1.02361531339728 entropy -2.194445467150348
epoch: 18, step: 49
	action: tensor([[ 0.3767,  0.9102,  2.7460, -2.2488, -1.3197,  2.1481,  3.7374]],
       dtype=torch.float64)
	q_value: tensor([[-30.2141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6978013278916185, distance: 0.6290757862856132 entropy -1.514771268543288
epoch: 18, step: 50
	action: tensor([[ 3.1885,  2.5191, -4.4441,  0.3002, -9.0726,  2.0505, 13.9193]],
       dtype=torch.float64)
	q_value: tensor([[-26.5170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6290757862856132 entropy -0.39873197541346395
epoch: 18, step: 51
	action: tensor([[-0.1645, -0.3681,  0.2152,  0.1186, -0.0631, -0.1599, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1833578814095278, distance: 1.2448434745341779 entropy -3.588663177422626
epoch: 18, step: 52
	action: tensor([[ 1.0543, -0.4895, -0.3003,  0.5764,  1.2298,  0.5904,  2.1662]],
       dtype=torch.float64)
	q_value: tensor([[-6.9620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7281604947280025, distance: 0.5966408419063686 entropy -1.9904046060808214
epoch: 18, step: 53
	action: tensor([[ 4.3014, -1.8464, -0.5673, -2.8940,  1.0211,  2.5523,  4.8890]],
       dtype=torch.float64)
	q_value: tensor([[-23.8220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5966408419063686 entropy -1.277076462720933
epoch: 18, step: 54
	action: tensor([[ 0.0122,  0.0323,  0.1277, -0.0700,  0.0859, -0.0773, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2755152282089195, distance: 0.9740277361903666 entropy -3.588663177422626
epoch: 18, step: 55
	action: tensor([[ 0.4312, -1.3600,  0.2908, -2.4512, -0.9218,  0.4543,  1.9336]],
       dtype=torch.float64)
	q_value: tensor([[-7.5533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9740277361903666 entropy -2.079649212951423
epoch: 18, step: 56
	action: tensor([[-0.2044, -0.2501, -0.0922,  0.1091,  0.2736,  0.1670, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06104728062713449, distance: 1.1787563941881376 entropy -3.588663177422626
epoch: 18, step: 57
	action: tensor([[ 2.3356, -0.6810, -0.6593, -1.7582, -0.8342,  0.4552,  1.8178]],
       dtype=torch.float64)
	q_value: tensor([[-9.3571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1787563941881376 entropy -2.136323689979389
epoch: 18, step: 58
	action: tensor([[-0.0294,  0.0064, -0.2106,  0.2575, -0.2957,  0.1144, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34848786660853426, distance: 0.9236723586872567 entropy -3.588663177422626
epoch: 18, step: 59
	action: tensor([[-0.3044, -0.4651,  0.7416,  0.2036, -0.0057,  1.0798,  1.8377]],
       dtype=torch.float64)
	q_value: tensor([[-6.9185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27154022383872534, distance: 0.9766961623674802 entropy -2.1336065800212096
epoch: 18, step: 60
	action: tensor([[ 3.5620, -1.3340,  1.8367,  0.5071, -4.3978,  1.7264,  4.8766]],
       dtype=torch.float64)
	q_value: tensor([[-19.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9766961623674802 entropy -1.2804836410516653
epoch: 18, step: 61
	action: tensor([[-0.2548, -0.0112,  0.0558, -0.1349,  0.0919,  0.0885, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-25.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08825106960571061, distance: 1.1937716051180662 entropy -3.588663177422626
epoch: 18, step: 62
	action: tensor([[-1.2714, -0.4725,  0.0852,  1.0616, -1.4570,  1.6057,  1.7219]],
       dtype=torch.float64)
	q_value: tensor([[-8.6881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8340177802437867, distance: 1.549738649218477 entropy -2.1766120430093294
epoch: 18, step: 63
	action: tensor([[ 5.2039, -4.2691, -0.8211,  0.1870,  3.6177,  2.8606,  4.8216]],
       dtype=torch.float64)
	q_value: tensor([[-25.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.549738649218477 entropy -1.3017650605463558
LOSS epoch 18 actor 322.7617682988995 critic 925.6817704437724
epoch: 19, step: 0
	action: tensor([[-0.1847, -0.1804, -0.1140,  0.1185,  0.1146,  0.1740, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012766780026438718, distance: 1.137015993612493 entropy -3.4215856410958803
epoch: 19, step: 1
	action: tensor([[-1.5910,  0.0205, -0.3398,  0.2608,  0.2270,  0.7863,  1.3272]],
       dtype=torch.float64)
	q_value: tensor([[-11.0516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1565645683728132, distance: 1.6804977141936135 entropy -2.299830322278954
epoch: 19, step: 2
	action: tensor([[ 0.1436, -0.5029, -0.0463, -0.5075,  0.4528,  1.0062,  1.5469]],
       dtype=torch.float64)
	q_value: tensor([[-31.2686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14452764403991314, distance: 1.0584240105601008 entropy -2.1244829963697396
epoch: 19, step: 3
	action: tensor([[ 3.0132, -5.8816, -1.8049, -3.3793, -1.7763,  1.6671,  3.2833]],
       dtype=torch.float64)
	q_value: tensor([[-29.2554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0584240105601008 entropy -1.5272461541011586
epoch: 19, step: 4
	action: tensor([[ 0.1020, -0.1798,  0.1570,  0.2039, -0.2176, -0.0340, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3969663790967213, distance: 0.8886432702073125 entropy -3.4215856410958803
epoch: 19, step: 5
	action: tensor([[-0.8840, -0.6211,  1.1112, -0.4030,  0.3143,  0.6408,  1.8712]],
       dtype=torch.float64)
	q_value: tensor([[-8.0024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1729867867074955, distance: 1.6868840661713134 entropy -2.014464129071307
epoch: 19, step: 6
	action: tensor([[-0.7702, -1.2745, -3.1304,  0.5278, -6.3160,  2.6171,  4.0225]],
       dtype=torch.float64)
	q_value: tensor([[-28.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6868840661713134 entropy -1.3492252034985601
epoch: 19, step: 7
	action: tensor([[-0.0662, -0.1167, -0.0233,  0.1592, -0.2277,  0.0391, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20468834882608977, distance: 1.0205290358890535 entropy -3.4215856410958803
epoch: 19, step: 8
	action: tensor([[ 1.5503, -0.3582,  0.5113,  1.6112, -0.4465,  0.5956,  1.4792]],
       dtype=torch.float64)
	q_value: tensor([[-8.6561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5556829515980429, distance: 0.7627868228000672 entropy -2.2140916282211793
epoch: 19, step: 9
	action: tensor([[ 1.9094, -2.8624, -1.8053, -0.6222, -4.8578,  1.0321,  4.9497]],
       dtype=torch.float64)
	q_value: tensor([[-21.7958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7627868228000672 entropy -1.1853256472851383
epoch: 19, step: 10
	action: tensor([[-0.4910, -0.0160,  0.2106,  0.2494,  0.0669,  0.0544, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1774048572646154, distance: 1.2417083592612554 entropy -3.4215856410958803
epoch: 19, step: 11
	action: tensor([[ 0.0866,  0.0377, -1.1382, -0.8210, -1.8400,  0.4632,  1.3571]],
       dtype=torch.float64)
	q_value: tensor([[-10.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44005243774011715, distance: 0.8563087315121582 entropy -2.275778213818393
epoch: 19, step: 12
	action: tensor([[ 3.5490, -0.2597,  0.9092, -0.2307, -0.6510,  1.9207,  2.2649]],
       dtype=torch.float64)
	q_value: tensor([[-30.8734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8563087315121582 entropy -1.8359831069256995
epoch: 19, step: 13
	action: tensor([[-0.0621,  0.1774, -0.3370, -0.1775,  0.2476,  0.0934, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3135725110678078, distance: 0.9480997385715917 entropy -3.4215856410958803
epoch: 19, step: 14
	action: tensor([[-0.3526,  0.2407, -0.6947, -0.3349, -0.9192,  0.3402,  0.8390]],
       dtype=torch.float64)
	q_value: tensor([[-12.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03692542996589032, distance: 1.165280438341784 entropy -2.632490368296167
epoch: 19, step: 15
	action: tensor([[ 0.1259, -0.2442, -0.1971, -0.5589,  1.4609,  1.1175,  1.3030]],
       dtype=torch.float64)
	q_value: tensor([[-21.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.401555032551862, distance: 0.8852558448862776 entropy -2.290587020406954
epoch: 19, step: 16
	action: tensor([[-0.2854,  2.3218, -1.3650, -1.2691,  3.2069,  2.5483,  3.0538]],
       dtype=torch.float64)
	q_value: tensor([[-34.2295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8852558448862776 entropy -1.5743277472140282
epoch: 19, step: 17
	action: tensor([[-0.1155, -0.0174,  0.2004,  0.0836, -0.0509, -0.0376, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16877559727936853, distance: 1.043315922886684 entropy -3.4215856410958803
epoch: 19, step: 18
	action: tensor([[ 2.7498, -0.5217,  0.2997,  0.8591, -0.1452,  1.1889,  1.5089]],
       dtype=torch.float64)
	q_value: tensor([[-8.7542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.043315922886684 entropy -2.1919534786365964
epoch: 19, step: 19
	action: tensor([[-0.3140, -0.1410, -0.1252,  0.2494, -0.0808,  0.0588, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10026673701489464, distance: 1.2003438870968957 entropy -3.4215856410958803
epoch: 19, step: 20
	action: tensor([[ 0.9151,  0.1492,  0.3601,  0.4626, -0.1594,  0.5935,  1.1995]],
       dtype=torch.float64)
	q_value: tensor([[-9.8363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2003438870968957 entropy -2.3854845060438494
epoch: 19, step: 21
	action: tensor([[-0.0309, -0.0758,  0.3800,  0.0260,  0.2267,  0.0890, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24944582061339626, distance: 0.9913972664011311 entropy -3.4215856410958803
epoch: 19, step: 22
	action: tensor([[ 0.7247,  0.1987,  0.4307, -0.8885, -0.2577,  1.5156,  1.8683]],
       dtype=torch.float64)
	q_value: tensor([[-10.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8545178411740044, distance: 0.4364771991014727 entropy -2.01144046523548
epoch: 19, step: 23
	action: tensor([[-2.2950, -1.2588, -4.5217,  0.3127,  0.7554,  2.1954,  4.7612]],
       dtype=torch.float64)
	q_value: tensor([[-31.8388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4364771991014727 entropy -1.2098327417970922
epoch: 19, step: 24
	action: tensor([[-0.1228, -0.1634,  0.1124,  0.0736,  0.3344, -0.1050, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011063031888310926, distance: 1.1379966908644372 entropy -3.4215856410958803
epoch: 19, step: 25
	action: tensor([[ 0.3732, -1.1438, -1.0720,  0.7168,  0.4675,  0.5692,  1.4296]],
       dtype=torch.float64)
	q_value: tensor([[-10.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23814576238843554, distance: 1.273334719793361 entropy -2.2355588698338624
epoch: 19, step: 26
	action: tensor([[-1.3479, -1.1474, -1.0924,  0.2973,  2.3642,  1.4824,  2.5815]],
       dtype=torch.float64)
	q_value: tensor([[-28.0706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9647377005412248, distance: 1.604017058494352 entropy -1.739435942120798
epoch: 19, step: 27
	action: tensor([[-4.8092,  0.9142,  1.3870,  0.8620, -0.5517,  2.5817,  3.0478]],
       dtype=torch.float64)
	q_value: tensor([[-54.4283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.604017058494352 entropy -1.5538159050545344
epoch: 19, step: 28
	action: tensor([[-0.0086, -0.1390, -0.0435, -0.3549, -0.1207, -0.0879, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018364269136166733, distance: 1.1548039741210803 entropy -3.4215856410958803
epoch: 19, step: 29
	action: tensor([[ 1.2361, -0.7757,  0.6285, -0.4578, -0.4635,  0.4801,  1.3252]],
       dtype=torch.float64)
	q_value: tensor([[-10.6496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14457225172575394, distance: 1.2242730790230225 entropy -2.287753191412894
epoch: 19, step: 30
	action: tensor([[-3.1239, -3.9569, -2.5732, -3.0327, -5.2632,  3.1097,  4.5599]],
       dtype=torch.float64)
	q_value: tensor([[-23.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2242730790230225 entropy -1.2526755978889663
epoch: 19, step: 31
	action: tensor([[-0.2368, -0.1187,  0.2169,  0.2551,  0.1724,  0.0060, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05936545918884195, distance: 1.1098573294074103 entropy -3.4215856410958803
epoch: 19, step: 32
	action: tensor([[ 0.2218, -1.1005, -0.4575,  0.1001, -0.1125,  1.1300,  1.5570]],
       dtype=torch.float64)
	q_value: tensor([[-9.9675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2064543253997746, distance: 1.2569330207041391 entropy -2.1684871008408644
epoch: 19, step: 33
	action: tensor([[ 1.4648, -1.1357, -1.3121, -4.1871, -0.5366,  1.6961,  3.4604]],
       dtype=torch.float64)
	q_value: tensor([[-29.5672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2569330207041391 entropy -1.494737445684992
epoch: 19, step: 34
	action: tensor([[-0.1721,  0.1451,  0.0771,  0.0786,  0.1112, -0.0706, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18896027796901838, distance: 1.0305706165820823 entropy -3.4215856410958803
epoch: 19, step: 35
	action: tensor([[ 1.1266, -0.3909, -0.5650, -0.3223,  0.8521,  0.3179,  1.2413]],
       dtype=torch.float64)
	q_value: tensor([[-9.6200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23438530498438126, distance: 1.00129448368466 entropy -2.346036130023463
epoch: 19, step: 36
	action: tensor([[ 2.1387, -0.8944, -0.6483, -1.8996, -1.0464,  1.2135,  3.0266]],
       dtype=torch.float64)
	q_value: tensor([[-25.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.00129448368466 entropy -1.5989759424323733
epoch: 19, step: 37
	action: tensor([[-0.1211, -0.2749,  0.0803, -0.1726, -0.2495,  0.0651, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16860213282138603, distance: 1.2370579099401422 entropy -3.4215856410958803
epoch: 19, step: 38
	action: tensor([[-0.0213, -0.3477, -0.3794, -0.4639, -1.1309,  1.4422,  1.5879]],
       dtype=torch.float64)
	q_value: tensor([[-10.1216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22142863898323495, distance: 1.2647093888076844 entropy -2.149357678505074
epoch: 19, step: 39
	action: tensor([[-1.9686,  0.3743, -1.7432,  1.4510, -1.3435,  0.7722,  3.6150]],
       dtype=torch.float64)
	q_value: tensor([[-32.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2647093888076844 entropy -1.4590079527186681
epoch: 19, step: 40
	action: tensor([[ 0.1711,  0.0103,  0.0318,  0.2042, -0.0044,  0.2839, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6455313012838054, distance: 0.6813113732202345 entropy -3.4215856410958803
epoch: 19, step: 41
	action: tensor([[-4.1262e-01, -7.7181e-01,  5.3296e-04,  1.8249e-01, -1.1394e+00,
          8.6515e-01,  1.8304e+00]], dtype=torch.float64)
	q_value: tensor([[-9.8400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6877609116011527, distance: 1.4866617081989482 entropy -2.031317544031717
epoch: 19, step: 42
	action: tensor([[-0.3373, -0.0145, -0.0119,  0.0707, -0.0539,  0.1783, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0760693990429342, distance: 1.1870713786481601 entropy -3.4215856410958803
epoch: 19, step: 43
	action: tensor([[-0.1500,  0.2774, -0.3941, -0.5598, -0.8616,  0.7519,  1.2477]],
       dtype=torch.float64)
	q_value: tensor([[-10.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0873026000953755, distance: 1.0932515453956375 entropy -2.345474623914022
epoch: 19, step: 44
	action: tensor([[ 1.0229, -0.8234, -0.5573, -1.8421,  0.6738,  0.8772,  2.3013]],
       dtype=torch.float64)
	q_value: tensor([[-25.2546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4422994284097419, distance: 1.3743090581664117 entropy -1.8330968285234246
epoch: 19, step: 45
	action: tensor([[-1.2438, -1.5606, -2.9627, -5.0475, -1.3996,  1.5740,  4.6271]],
       dtype=torch.float64)
	q_value: tensor([[-43.4001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3743090581664117 entropy -1.235299044268055
epoch: 19, step: 46
	action: tensor([[-0.2910,  0.0674, -0.2521, -0.1712,  0.1078,  0.0132, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07521541311428304, distance: 1.1866002457152152 entropy -3.4215856410958803
epoch: 19, step: 47
	action: tensor([[-0.2246,  0.2620,  0.0176, -0.3626, -0.0251,  0.5940,  0.7559]],
       dtype=torch.float64)
	q_value: tensor([[-12.1555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17032241462126851, distance: 1.0423447226658054 entropy -2.7191961784506367
epoch: 19, step: 48
	action: tensor([[-0.5750, -0.5236, -1.2729,  0.7957, -0.4111,  0.3338,  1.8536]],
       dtype=torch.float64)
	q_value: tensor([[-19.1658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1104270345945424, distance: 1.6624242449391244 entropy -2.006651218813802
epoch: 19, step: 49
	action: tensor([[ 7.9211e-01, -1.8485e-01,  5.0295e-01, -7.0511e-04, -6.7573e-01,
          1.1728e+00,  1.8336e+00]], dtype=torch.float64)
	q_value: tensor([[-30.9907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8728754287705179, distance: 0.4080105978861942 entropy -2.02883459393792
epoch: 19, step: 50
	action: tensor([[ 3.5808, -0.4590, -1.9567, -2.3696,  2.2852,  2.5868,  4.6820]],
       dtype=torch.float64)
	q_value: tensor([[-26.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4080105978861942 entropy -1.2327126619990725
epoch: 19, step: 51
	action: tensor([[-0.0296, -0.1517, -0.0976, -0.2361, -0.0263,  0.0255, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0332661934477686, distance: 1.1251492792293072 entropy -3.4215856410958803
epoch: 19, step: 52
	action: tensor([[ 0.2146, -0.8086, -0.0795, -0.3657,  0.7960,  1.1584,  1.3100]],
       dtype=torch.float64)
	q_value: tensor([[-10.9102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05298700166416492, distance: 1.1136139516881312 entropy -2.30255973933177
epoch: 19, step: 53
	action: tensor([[-2.3113, -0.4195, -0.4479, -2.0283, -1.3050,  0.5819,  3.4485]],
       dtype=torch.float64)
	q_value: tensor([[-29.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1136139516881312 entropy -1.484804116440756
epoch: 19, step: 54
	action: tensor([[-0.1303, -0.1271, -0.0597,  0.4764, -0.2910,  0.1313, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2650801557989362, distance: 0.9810173322872673 entropy -3.4215856410958803
epoch: 19, step: 55
	action: tensor([[-0.0683,  0.0422,  0.0929, -0.4893, -0.7698,  0.5517,  1.6056]],
       dtype=torch.float64)
	q_value: tensor([[-8.5699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006424596597581322, distance: 1.1480143438261967 entropy -2.1493735314969133
epoch: 19, step: 56
	action: tensor([[ 0.6560, -0.3653, -0.0109,  0.1533, -0.9907,  0.4500,  3.0035]],
       dtype=torch.float64)
	q_value: tensor([[-25.4510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439105959863619, distance: 0.7728259290159625 entropy -1.6117211896887758
epoch: 19, step: 57
	action: tensor([[-2.9767, -2.0448, -1.9446, -3.5398,  2.7917,  3.1243,  4.7503]],
       dtype=torch.float64)
	q_value: tensor([[-35.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7728259290159625 entropy -1.2252685262314222
epoch: 19, step: 58
	action: tensor([[-0.0423, -0.1615, -0.2164,  0.1425,  0.0189,  0.1536, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.200707448478184, distance: 1.0230799561768593 entropy -3.4215856410958803
epoch: 19, step: 59
	action: tensor([[-0.5426, -0.2881, -0.0582,  0.4749,  0.7160,  0.9362,  1.3354]],
       dtype=torch.float64)
	q_value: tensor([[-10.4174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12595550522265297, distance: 1.0698514118334155 entropy -2.2975132368073146
epoch: 19, step: 60
	action: tensor([[-0.5791, -0.9045, -0.3714, -0.2216, -0.5612,  1.8240,  2.5236]],
       dtype=torch.float64)
	q_value: tensor([[-26.1844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.655847023990265, distance: 1.4725389774645632 entropy -1.7420372718214376
epoch: 19, step: 61
	action: tensor([[ 2.6467,  2.5502,  2.2964, -0.8020,  3.0865,  1.7129,  4.3035]],
       dtype=torch.float64)
	q_value: tensor([[-44.2889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4725389774645632 entropy -1.3035346543434354
epoch: 19, step: 62
	action: tensor([[ 0.2153,  0.2268,  0.2952, -0.0028, -0.0580,  0.1948, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-33.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6157084339639898, distance: 0.7093933612416221 entropy -3.4215856410958803
epoch: 19, step: 63
	action: tensor([[-0.6854, -1.4318, -1.0133, -1.7317, -1.1105,  0.7191,  1.9803]],
       dtype=torch.float64)
	q_value: tensor([[-9.5400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7093933612416221 entropy -1.957984034638743
LOSS epoch 19 actor 330.75657303355496 critic 797.5245834021972
epoch: 20, step: 0
	action: tensor([[-0.0866, -0.0286,  0.0704,  0.3251, -0.6967,  0.0769, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3218251196737183, distance: 0.9423832167074345 entropy -3.148072306893647
epoch: 20, step: 1
	action: tensor([[-0.9787, -0.7859, -0.3238,  0.2476,  1.2088,  0.7341,  1.1185]],
       dtype=torch.float64)
	q_value: tensor([[-10.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9786793539418193, distance: 1.6096979997767393 entropy -2.26192227712744
epoch: 20, step: 2
	action: tensor([[-1.7803, -0.3894, -0.1475, -0.9258,  2.3293,  0.7892,  1.1927]],
       dtype=torch.float64)
	q_value: tensor([[-36.7892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6096979997767393 entropy -2.132410353256781
epoch: 20, step: 3
	action: tensor([[ 0.1209, -0.0361,  0.0375,  0.1421, -0.0835, -0.0684, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44460232867642024, distance: 0.8528226388437395 entropy -3.148072306893647
epoch: 20, step: 4
	action: tensor([[-1.5187, -0.4694, -0.0268, -0.5771, -0.3366,  1.2351,  1.0059]],
       dtype=torch.float64)
	q_value: tensor([[-10.3863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5783034341337712, distance: 1.837484595189317 entropy -2.352553975856368
epoch: 20, step: 5
	action: tensor([[-1.0558, -1.0312,  0.8481,  0.3144, -0.2704,  0.7401,  1.4558]],
       dtype=torch.float64)
	q_value: tensor([[-40.3023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8927184459187869, distance: 1.5743442438029092 entropy -1.9795335644824927
epoch: 20, step: 6
	action: tensor([[-0.6368,  0.6386,  0.5814, -1.8289, -2.6356,  1.9799,  2.5112]],
       dtype=torch.float64)
	q_value: tensor([[-31.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04213029858538653, distance: 1.1199790820180529 entropy -1.5888832339847627
epoch: 20, step: 7
	action: tensor([[-0.4884,  2.5478, -2.5804,  1.4688,  1.1170,  1.1784,  3.8392]],
       dtype=torch.float64)
	q_value: tensor([[-61.8622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1199790820180529 entropy -1.2030404734785283
epoch: 20, step: 8
	action: tensor([[ 0.2227, -0.0519,  0.3207,  0.0493, -0.1553,  0.1012, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5330784626853606, distance: 0.7819493926174934 entropy -3.148072306893647
epoch: 20, step: 9
	action: tensor([[-1.0432,  0.2645,  0.5035,  0.7558, -0.5805,  0.1909,  1.4046]],
       dtype=torch.float64)
	q_value: tensor([[-11.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7819493926174934 entropy -2.084203932002968
epoch: 20, step: 10
	action: tensor([[-0.3351, -0.1729,  0.2172,  0.1932,  0.2165,  0.2710, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7819493926174934 entropy -3.148072306893647
epoch: 20, step: 11
	action: tensor([[-0.1485,  0.0680,  0.3631,  0.2034,  0.0213,  0.1661, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3304435888581737, distance: 0.9363760125395479 entropy -3.148072306893647
epoch: 20, step: 12
	action: tensor([[ 0.4788, -0.0439,  0.9262, -0.2268,  0.3537,  0.5119,  1.1405]],
       dtype=torch.float64)
	q_value: tensor([[-11.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6124316622537401, distance: 0.7124113639061147 entropy -2.246174786061087
epoch: 20, step: 13
	action: tensor([[-0.2911, -0.0049,  0.3174, -0.0303,  0.4011,  0.1186, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.056786640214959716, distance: 1.1763873626221806 entropy -3.148072306893647
epoch: 20, step: 14
	action: tensor([[ 1.1726, -0.3071,  0.0311,  0.0213, -1.2006, -0.0240,  0.9555]],
       dtype=torch.float64)
	q_value: tensor([[-15.0809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42931631629010425, distance: 0.8644789470647695 entropy -2.3714622519615864
epoch: 20, step: 15
	action: tensor([[ 3.4469, -0.3576, -1.7487, -1.6023, -1.5481,  1.5283,  2.3671]],
       dtype=torch.float64)
	q_value: tensor([[-22.5018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8644789470647695 entropy -1.647893606740564
epoch: 20, step: 16
	action: tensor([[-0.0446, -0.4887,  0.5237,  0.2909, -0.0669, -0.2146, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023696992762291424, distance: 1.1578236251700682 entropy -3.148072306893647
epoch: 20, step: 17
	action: tensor([[ 1.5767, -0.1529, -0.9007,  0.0549,  1.3696,  0.9392,  1.5249]],
       dtype=torch.float64)
	q_value: tensor([[-10.6652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4243553727188165, distance: 0.8682282669060548 entropy -2.024188666001275
epoch: 20, step: 18
	action: tensor([[-1.6695, -3.0436,  0.0444,  0.9306, -2.5240,  2.5713,  2.6891]],
       dtype=torch.float64)
	q_value: tensor([[-41.7090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8682282669060548 entropy -1.5251347166420413
epoch: 20, step: 19
	action: tensor([[-0.0859, -0.2354, -0.2542, -0.6152,  0.4056,  0.0424, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18671429073545576, distance: 1.2466076262762305 entropy -3.148072306893647
epoch: 20, step: 20
	action: tensor([[ 1.0693, -0.3346, -0.4854, -0.1807, -0.5084,  0.2722,  0.6871]],
       dtype=torch.float64)
	q_value: tensor([[-19.2253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37780414213352176, distance: 0.9026517969329547 entropy -2.5816434657484324
epoch: 20, step: 21
	action: tensor([[ 2.0582, -0.3865,  0.7271, -0.2131, -0.5756,  1.0434,  1.9487]],
       dtype=torch.float64)
	q_value: tensor([[-22.7154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9026517969329547 entropy -1.8218888423440334
epoch: 20, step: 22
	action: tensor([[-0.0583,  0.0349, -0.0388,  0.2217,  0.1669,  0.1820, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3851211595683688, distance: 0.8973285118827379 entropy -3.148072306893647
epoch: 20, step: 23
	action: tensor([[ 0.6135, -0.6079,  0.4243, -0.5789,  0.4832,  0.6047,  0.8644]],
       dtype=torch.float64)
	q_value: tensor([[-12.6972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10115653059777219, distance: 1.2008291523524635 entropy -2.4611849227736
epoch: 20, step: 24
	action: tensor([[ 0.5019, -0.2244,  0.1041,  0.2646, -0.1221,  0.1242, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6710432943117863, distance: 0.6563357570227335 entropy -3.148072306893647
epoch: 20, step: 25
	action: tensor([[ 2.1605, -0.0731, -0.3246, -1.3847, -0.1606,  0.8613,  1.5627]],
       dtype=torch.float64)
	q_value: tensor([[-11.7674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6563357570227335 entropy -2.0005834267496057
epoch: 20, step: 26
	action: tensor([[ 0.0371, -0.1484,  0.3331,  0.1234,  0.0236, -0.0410, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843797080914757, distance: 0.9680505068117553 entropy -3.148072306893647
epoch: 20, step: 27
	action: tensor([[ 0.2695,  1.1956,  0.2532, -0.3859,  0.2287,  0.0882,  1.2466]],
       dtype=torch.float64)
	q_value: tensor([[-11.1355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9680505068117553 entropy -2.1852199600214717
epoch: 20, step: 28
	action: tensor([[-0.2326,  0.0805,  0.3669, -0.2091, -0.2309,  0.1619, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9680505068117553 entropy -3.148072306893647
epoch: 20, step: 29
	action: tensor([[-0.0968, -0.0324, -0.1286,  0.2999,  0.0974, -0.1149, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23710671630506475, distance: 0.9995133266947583 entropy -3.148072306893647
epoch: 20, step: 30
	action: tensor([[ 0.0557,  0.4800, -0.3067, -0.8848, -0.8957,  0.3251,  0.7098]],
       dtype=torch.float64)
	q_value: tensor([[-11.3364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4205856870082648, distance: 0.8710664822528511 entropy -2.606797799086838
epoch: 20, step: 31
	action: tensor([[-0.2064,  0.0479, -0.8695, -0.6181,  0.1908,  0.7094,  1.1088]],
       dtype=torch.float64)
	q_value: tensor([[-25.5917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22202075317182224, distance: 1.0093474711821655 entropy -2.2247805913462715
epoch: 20, step: 32
	action: tensor([[ 0.5040,  0.3574, -0.4801, -0.3011,  1.1526,  0.8025,  0.8685]],
       dtype=torch.float64)
	q_value: tensor([[-35.8963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9541646917543105, distance: 0.24499474632984106 entropy -2.3942480176984384
epoch: 20, step: 33
	action: tensor([[-0.8812,  0.1108, -0.1417,  1.3774, -0.4623,  1.1889,  1.5288]],
       dtype=torch.float64)
	q_value: tensor([[-32.9954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.24499474632984106 entropy -1.9752741812797938
epoch: 20, step: 34
	action: tensor([[-0.2387, -0.2507,  0.0805,  0.4496,  0.2782,  0.0863, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08486693853461003, distance: 1.0947093213376313 entropy -3.148072306893647
epoch: 20, step: 35
	action: tensor([[ 0.0455, -0.5109, -0.6284,  0.0378, -0.9318,  0.8202,  0.9812]],
       dtype=torch.float64)
	q_value: tensor([[-13.5538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2558931364442085, distance: 1.2824281332074183 entropy -2.3647975061216555
epoch: 20, step: 36
	action: tensor([[-0.4904, -0.9518, -0.4240,  0.5494,  0.1956,  0.7836,  1.7385]],
       dtype=torch.float64)
	q_value: tensor([[-30.1920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5926886337453665, distance: 1.4441826846859542 entropy -1.9130530705398276
epoch: 20, step: 37
	action: tensor([[ 0.6132, -0.0767, -0.0571,  0.8218, -2.2794,  0.5196,  1.8022]],
       dtype=torch.float64)
	q_value: tensor([[-37.5504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4441826846859542 entropy -1.8531077853925637
epoch: 20, step: 38
	action: tensor([[-0.2691, -0.1923,  0.0784, -0.2551,  0.0612,  0.1935, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2787571392109698, distance: 1.2940490222477286 entropy -3.148072306893647
epoch: 20, step: 39
	action: tensor([[ 0.7203,  0.2637,  0.3933, -0.3095, -0.0268,  0.5532,  0.8764]],
       dtype=torch.float64)
	q_value: tensor([[-15.0761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8900414674827256, distance: 0.37946450720525843 entropy -2.4437542934522645
epoch: 20, step: 40
	action: tensor([[-0.8815, -0.7942, -2.3367, -1.8508, -1.5916,  1.5215,  2.2290]],
       dtype=torch.float64)
	q_value: tensor([[-22.5003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3930954641247334, distance: 0.8914908394698456 entropy -1.7002624466551122
epoch: 20, step: 41
	action: tensor([[ 0.0927,  0.0545, -0.8035, -0.4114,  1.1588,  0.4654,  1.2449]],
       dtype=torch.float64)
	q_value: tensor([[-71.3789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5643962602531583, distance: 0.7552704497222912 entropy -1.9782697420395203
epoch: 20, step: 42
	action: tensor([[-1.7514, -1.1128, -0.6494, -0.0617, -0.1370,  0.6638,  1.1120]],
       dtype=torch.float64)
	q_value: tensor([[-37.2206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7552704497222912 entropy -2.207127485548088
epoch: 20, step: 43
	action: tensor([[-0.5119, -0.3000,  0.2279,  0.0736,  0.4138, -0.0011, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5487560779140546, distance: 1.4241253106857288 entropy -3.148072306893647
epoch: 20, step: 44
	action: tensor([[-0.5375, -0.1831, -0.1652,  0.6331,  1.0284,  0.4533,  0.8714]],
       dtype=torch.float64)
	q_value: tensor([[-15.6305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0322433374973401, distance: 1.1626466310076036 entropy -2.436768150425162
epoch: 20, step: 45
	action: tensor([[-0.2777, -0.3723,  0.2972,  0.5162,  0.7249,  0.3690,  1.0641]],
       dtype=torch.float64)
	q_value: tensor([[-26.7685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2043173695888738, distance: 1.0207670249486749 entropy -2.2486269720353578
epoch: 20, step: 46
	action: tensor([[-0.8398, -0.9394, -0.3142, -1.5843,  1.9330,  1.4677,  1.6724]],
       dtype=torch.float64)
	q_value: tensor([[-24.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973607135840657, distance: 0.9592303417145771 entropy -1.9273407810023957
epoch: 20, step: 47
	action: tensor([[-1.1460,  0.4008,  1.5526,  2.0835,  1.8169,  1.9543,  2.3668]],
       dtype=torch.float64)
	q_value: tensor([[-63.1864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9592303417145771 entropy -1.584329591417228
epoch: 20, step: 48
	action: tensor([[-0.3821, -0.0447,  0.2422,  0.1368,  0.0835,  0.0574, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12438915433593412, distance: 1.2134308081645022 entropy -3.148072306893647
epoch: 20, step: 49
	action: tensor([[-0.0245,  0.1300, -0.2530, -0.9624,  0.3874,  0.1044,  0.8614]],
       dtype=torch.float64)
	q_value: tensor([[-12.3774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09888979157218791, distance: 1.0862896661684005 entropy -2.4551569033564817
epoch: 20, step: 50
	action: tensor([[ 0.8395, -0.6032, -0.7490,  0.9950, -0.3428,  0.5344,  1.0751]],
       dtype=torch.float64)
	q_value: tensor([[-29.0496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6562846354805223, distance: 0.6708974888341781 entropy -2.258768607442899
epoch: 20, step: 51
	action: tensor([[-2.9184, -0.5600,  0.1986, -1.5209, -1.8689,  1.1386,  1.9400]],
       dtype=torch.float64)
	q_value: tensor([[-26.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6708974888341781 entropy -1.816524221306077
epoch: 20, step: 52
	action: tensor([[ 0.2613, -0.1355,  0.7854, -0.4640, -0.0011, -0.0024, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10021097827323266, distance: 1.0854930278138282 entropy -3.148072306893647
epoch: 20, step: 53
	action: tensor([[ 2.1981, -1.3010, -0.7255, -1.3765,  0.0128,  1.6790,  1.9143]],
       dtype=torch.float64)
	q_value: tensor([[-14.5174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0854930278138282 entropy -1.8217746754656836
epoch: 20, step: 54
	action: tensor([[ 0.3105, -0.3349, -0.0415, -0.0509,  0.2317,  0.1527, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.287201156055111, distance: 0.9661402747839499 entropy -3.148072306893647
epoch: 20, step: 55
	action: tensor([[-0.7143, -0.0284, -1.3817, -0.1909, -1.1422, -0.1489,  1.2976]],
       dtype=torch.float64)
	q_value: tensor([[-14.7238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3926891299216453, distance: 1.3504664091227145 entropy -2.153100057348222
epoch: 20, step: 56
	action: tensor([[-0.0743,  0.2833, -1.0437,  0.1140, -0.7988,  0.3429,  0.2399]],
       dtype=torch.float64)
	q_value: tensor([[-33.2197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3504664091227145 entropy -2.9309578688163227
epoch: 20, step: 57
	action: tensor([[ 0.1554, -0.0064,  0.3118,  0.0871, -0.3974,  0.0513, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5331633834272727, distance: 0.7818782813741701 entropy -3.148072306893647
epoch: 20, step: 58
	action: tensor([[ 0.0606, -0.1075,  0.1873, -0.0344,  1.5843,  1.4960,  1.3470]],
       dtype=torch.float64)
	q_value: tensor([[-9.8410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5933531094144213, distance: 0.7297354108816315 entropy -2.1140249123848007
epoch: 20, step: 59
	action: tensor([[ 0.2696, -0.2136, -2.5485,  1.2002,  0.7419,  1.6841,  2.4450]],
       dtype=torch.float64)
	q_value: tensor([[-41.6571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7297354108816315 entropy -1.5837573751509066
epoch: 20, step: 60
	action: tensor([[ 0.1346, -0.1084,  0.0442,  0.0119, -0.2272,  0.0417, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3697229407322423, distance: 0.9084947946162721 entropy -3.148072306893647
epoch: 20, step: 61
	action: tensor([[0.4112, 0.0057, 0.7146, 0.6677, 1.6610, 1.0616, 1.1085]],
       dtype=torch.float64)
	q_value: tensor([[-10.8157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8287273992604558, distance: 0.47358794547892336 entropy -2.2747644099981494
epoch: 20, step: 62
	action: tensor([[ 0.0431, -0.6886, -0.1974, -0.2339, -1.9255,  2.5997,  2.7312]],
       dtype=torch.float64)
	q_value: tensor([[-33.5859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.47358794547892336 entropy -1.5003983049568002
epoch: 20, step: 63
	action: tensor([[-0.0761,  0.0880,  0.0595, -0.0102,  0.1617,  0.0928, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2794032973172462, distance: 0.9714105784604721 entropy -3.148072306893647
LOSS epoch 20 actor 473.4065932488578 critic 769.6302643990814
epoch: 21, step: 0
	action: tensor([[ 0.1978, -0.5582,  0.4183,  0.1348, -0.0362,  0.7124,  0.6799]],
       dtype=torch.float64)
	q_value: tensor([[-14.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3644423939878265, distance: 0.912292602915343 entropy -2.588222225407138
epoch: 21, step: 1
	action: tensor([[ 0.7135,  1.6437,  0.5940, -0.1454, -3.2674,  1.7769,  2.1617]],
       dtype=torch.float64)
	q_value: tensor([[-25.9471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.912292602915343 entropy -1.729828042906259
epoch: 21, step: 2
	action: tensor([[-0.7651, -0.2668,  0.1793, -0.1315, -0.4053, -0.0619, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9738201773856252, distance: 1.607720262755369 entropy -2.992571459059388
epoch: 21, step: 3
	action: tensor([[ 0.2411, -0.0077, -0.4302, -0.8027,  0.1303,  0.5451,  0.5790]],
       dtype=torch.float64)
	q_value: tensor([[-16.6364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37217336793447997, distance: 0.9067270257209736 entropy -2.64816440265722
epoch: 21, step: 4
	action: tensor([[-0.6267, -0.3043,  0.6229, -1.6869, -1.9526,  0.7709,  1.4423]],
       dtype=torch.float64)
	q_value: tensor([[-31.0785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8549043904070022, distance: 1.5585382232719527 entropy -2.0720895457252904
epoch: 21, step: 5
	action: tensor([[-1.7384,  1.5138, -2.4370,  0.7863,  0.8218,  1.9896,  3.3174]],
       dtype=torch.float64)
	q_value: tensor([[-47.1865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5585382232719527 entropy -1.382113005879239
epoch: 21, step: 6
	action: tensor([[-0.0868, -0.0390, -0.2657, -0.1918,  0.2499,  0.3249, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.178112186233785, distance: 1.0374399403179155 entropy -2.992571459059388
epoch: 21, step: 7
	action: tensor([[-0.6686,  0.0904,  0.4913, -0.7704,  0.9625,  0.0683,  0.6036]],
       dtype=torch.float64)
	q_value: tensor([[-19.4135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9220463018778826, distance: 1.5864946651488283 entropy -2.6533695525447016
epoch: 21, step: 8
	action: tensor([[ 0.1219, -0.5174,  0.6384, -0.8076,  0.1890,  0.9364,  1.4342]],
       dtype=torch.float64)
	q_value: tensor([[-32.1212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23515948267803988, distance: 1.2717982168416655 entropy -2.0364213321832105
epoch: 21, step: 9
	action: tensor([[ 0.8769, -1.2804, -0.8552, -1.5186,  3.1462,  0.3638,  3.1388]],
       dtype=torch.float64)
	q_value: tensor([[-39.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2717982168416655 entropy -1.421579681300171
epoch: 21, step: 10
	action: tensor([[ 0.1423, -0.0910,  0.0583, -0.0147, -0.0174, -0.1771, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3184142247690104, distance: 0.9447501125124094 entropy -2.992571459059388
epoch: 21, step: 11
	action: tensor([[ 0.7533, -0.3638, -0.2902, -0.2883, -0.3034,  0.7228,  0.8344]],
       dtype=torch.float64)
	q_value: tensor([[-13.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44932806481646814, distance: 0.8491866635259886 entropy -2.4402003612341483
epoch: 21, step: 12
	action: tensor([[-0.1554, -1.3158, -0.1122, -1.9302, -0.3900,  1.4932,  2.2608]],
       dtype=torch.float64)
	q_value: tensor([[-31.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8491866635259886 entropy -1.7096239588224942
epoch: 21, step: 13
	action: tensor([[-0.0253,  0.1499, -0.5268,  0.3368,  0.5847, -0.0081, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3661909236745693, distance: 0.9110368006528627 entropy -2.992571459059388
epoch: 21, step: 14
	action: tensor([[-0.2698, -0.1143,  0.0433, -0.4626, -0.2415,  0.1415,  0.2575]],
       dtype=torch.float64)
	q_value: tensor([[-18.9581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31876183290705384, distance: 1.314134684632216 entropy -3.0130801239700338
epoch: 21, step: 15
	action: tensor([[ 0.2308, -0.0837,  0.3467,  0.3215,  0.1434, -0.0274, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6358812929382551, distance: 0.6905230644985614 entropy -2.992571459059388
epoch: 21, step: 16
	action: tensor([[-1.5451,  0.5643,  0.5262, -0.8226, -0.5078,  0.2939,  1.1957]],
       dtype=torch.float64)
	q_value: tensor([[-14.3716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5134326924705364, distance: 1.8142215573806426 entropy -2.1711796630892595
epoch: 21, step: 17
	action: tensor([[-0.4141, -1.2332, -0.1238, -0.6874, -1.0927,  0.9511,  1.4997]],
       dtype=torch.float64)
	q_value: tensor([[-36.6602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.11680490971181, distance: 1.664934337816413 entropy -1.98225355337394
epoch: 21, step: 18
	action: tensor([[-0.4402,  0.4650,  0.3103, -0.0245, -3.4407,  1.1903,  2.8665]],
       dtype=torch.float64)
	q_value: tensor([[-46.4297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.664934337816413 entropy -1.5151637124767332
epoch: 21, step: 19
	action: tensor([[ 0.5300, -0.3264, -0.4615, -0.0071, -0.0930,  0.0475, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3910225976300088, distance: 0.8930119734969825 entropy -2.992571459059388
epoch: 21, step: 20
	action: tensor([[-0.9052, -0.8688,  0.2974,  0.3415,  0.1119,  0.0675,  1.0584]],
       dtype=torch.float64)
	q_value: tensor([[-16.6837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0352946498235323, distance: 1.63256446313218 entropy -2.278526085970685
epoch: 21, step: 21
	action: tensor([[-0.4396, -0.2385,  0.4353, -1.1011,  0.3454,  0.7174,  1.5525]],
       dtype=torch.float64)
	q_value: tensor([[-28.5458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.642565685822307, distance: 1.4666215640382314 entropy -1.9948622664537108
epoch: 21, step: 22
	action: tensor([[-1.0342, -1.8616, -0.4149, -2.0791, -0.4285,  1.6176,  2.6549]],
       dtype=torch.float64)
	q_value: tensor([[-43.5490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4666215640382314 entropy -1.5609659504123596
epoch: 21, step: 23
	action: tensor([[ 0.2761, -0.0765,  0.5471,  0.0448,  0.1602, -0.0847, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4891775141545669, distance: 0.8178839699581397 entropy -2.992571459059388
epoch: 21, step: 24
	action: tensor([[ 0.8968,  0.0205,  0.7798, -0.3543, -0.1643,  0.9792,  1.4012]],
       dtype=torch.float64)
	q_value: tensor([[-15.2089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8413493755401891, distance: 0.4558034270595962 entropy -2.0488178205267498
epoch: 21, step: 25
	action: tensor([[-0.4290, -1.5969,  0.9716, -4.5903, -2.2541,  0.8770,  3.4359]],
       dtype=torch.float64)
	q_value: tensor([[-35.6769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4558034270595962 entropy -1.3417067034090937
epoch: 21, step: 26
	action: tensor([[ 0.0197, -0.2152, -0.0241,  0.1890,  0.3347,  0.3165, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3558770910197081, distance: 0.9184194364136121 entropy -2.992571459059388
epoch: 21, step: 27
	action: tensor([[ 0.6577, -0.3373,  0.1721, -0.7765,  0.6392,  0.0810,  0.9469]],
       dtype=torch.float64)
	q_value: tensor([[-18.5206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06545936203072045, distance: 1.181204623390853 entropy -2.3520549793044165
epoch: 21, step: 28
	action: tensor([[-0.3513, -1.2545,  0.8910, -2.5832,  0.3943,  2.2856,  2.3333]],
       dtype=torch.float64)
	q_value: tensor([[-31.7051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.181204623390853 entropy -1.6746683055362908
epoch: 21, step: 29
	action: tensor([[-0.1087, -0.0192, -0.2265,  0.6557,  0.0776, -0.0214, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3304633567830976, distance: 0.9363621896957713 entropy -2.992571459059388
epoch: 21, step: 30
	action: tensor([[-0.5233,  0.0165, -0.3923,  0.2156,  0.6216,  0.3935,  0.5812]],
       dtype=torch.float64)
	q_value: tensor([[-14.4988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17910558197219495, distance: 1.242604840201787 entropy -2.6722112254471666
epoch: 21, step: 31
	action: tensor([[-0.0060, -0.3153,  0.5414,  0.0287, -0.2396,  0.0362, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058443578693510356, distance: 1.110401060906652 entropy -2.992571459059388
epoch: 21, step: 32
	action: tensor([[-0.7087, -1.0688, -1.0715, -0.4863, -0.3574,  0.4019,  1.3722]],
       dtype=torch.float64)
	q_value: tensor([[-13.7168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9523732038637673, distance: 1.598961888731904 entropy -2.0703740379466757
epoch: 21, step: 33
	action: tensor([[-0.4555,  0.1613,  0.7624, -0.7694,  0.5555,  0.7996,  1.2642]],
       dtype=torch.float64)
	q_value: tensor([[-45.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3374393608947843, distance: 1.3234079628932436 entropy -2.154917214846059
epoch: 21, step: 34
	action: tensor([[-0.9066,  1.8641,  0.6547, -1.5129, -2.0497,  1.8468,  2.4995]],
       dtype=torch.float64)
	q_value: tensor([[-38.2219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3234079628932436 entropy -1.598652248167765
epoch: 21, step: 35
	action: tensor([[ 0.0331, -0.0634, -0.4690, -0.0071, -0.3423, -0.2309, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3076897960397371, distance: 0.9521537005393074 entropy -2.992571459059388
epoch: 21, step: 36
	action: tensor([[-0.2532, -0.4143,  0.1632,  0.1207, -0.3575,  0.1243,  0.3856]],
       dtype=torch.float64)
	q_value: tensor([[-13.0626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2125076220653157, distance: 1.260082360172098 entropy -2.911962999137175
epoch: 21, step: 37
	action: tensor([[-1.1569,  0.2258, -0.0216, -0.1013, -0.1688,  0.3247,  1.2740]],
       dtype=torch.float64)
	q_value: tensor([[-17.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9730277492767936, distance: 1.6073975052327114 entropy -2.177425238330797
epoch: 21, step: 38
	action: tensor([[ 1.4285, -0.3900,  0.3786, -1.5009,  0.6655,  0.4399,  1.1297]],
       dtype=torch.float64)
	q_value: tensor([[-33.0834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28092510364266876, distance: 1.295145502613406 entropy -2.219056807914184
epoch: 21, step: 39
	action: tensor([[-0.6743, -1.4032,  0.4878, -2.0823, -0.4333,  0.6817,  3.7857]],
       dtype=torch.float64)
	q_value: tensor([[-42.8037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.295145502613406 entropy -1.2619269575597518
epoch: 21, step: 40
	action: tensor([[-0.4395, -0.1580,  0.0818,  0.0031, -0.4184, -0.0017, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39221076383515774, distance: 1.3502344575691239 entropy -2.992571459059388
epoch: 21, step: 41
	action: tensor([[ 0.3364,  0.0488,  0.2376, -0.2145,  0.3488,  0.6754,  0.6454]],
       dtype=torch.float64)
	q_value: tensor([[-14.0648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.688474856621427, distance: 0.6387093084689689 entropy -2.605696680485377
epoch: 21, step: 42
	action: tensor([[-0.2745, -0.5468, -3.1904, -0.6418, -0.1343,  0.8508,  1.9057]],
       dtype=torch.float64)
	q_value: tensor([[-26.6660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2638056442223329, distance: 1.2864616332680834 entropy -1.8363529835245476
epoch: 21, step: 43
	action: tensor([[ 6.9895, -0.4256,  2.2154, -2.1294,  0.0299,  3.0744,  5.4746]],
       dtype=torch.float64)
	q_value: tensor([[-42.5282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2864616332680834 entropy -0.9329529313793982
epoch: 21, step: 44
	action: tensor([[-0.2492, -0.0758,  0.0358, -0.0981, -0.4210,  0.2720, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10714566374445567, distance: 1.2040903476761085 entropy -2.992571459059388
epoch: 21, step: 45
	action: tensor([[ 0.3143,  0.1546, -0.3966, -0.2035,  1.1789,  0.2705,  0.8280]],
       dtype=torch.float64)
	q_value: tensor([[-15.3231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6805153687516017, distance: 0.6468173770862273 entropy -2.4509241781059976
epoch: 21, step: 46
	action: tensor([[-0.8757,  0.7508,  0.8852, -1.5044, -0.5207,  0.9737,  1.3210]],
       dtype=torch.float64)
	q_value: tensor([[-33.7430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8042528636749586, distance: 1.537111581855094 entropy -2.1044162911561957
epoch: 21, step: 47
	action: tensor([[-1.7133, -1.0885,  1.1867, -3.5381,  1.2342,  0.8109,  2.7605]],
       dtype=torch.float64)
	q_value: tensor([[-44.7876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.537111581855094 entropy -1.5148689513139177
epoch: 21, step: 48
	action: tensor([[ 0.0424, -0.3445, -0.1660, -0.6519,  0.1123,  0.1520, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2077481803408574, distance: 1.2576068353578804 entropy -2.992571459059388
epoch: 21, step: 49
	action: tensor([[ 0.3464, -0.4305, -0.4533, -0.3198, -0.3865,  0.4175,  0.9879]],
       dtype=torch.float64)
	q_value: tensor([[-20.8306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10236893666258129, distance: 1.0841905808589747 entropy -2.3254647444625767
epoch: 21, step: 50
	action: tensor([[ 0.7091, -0.0169,  0.7458, -2.0755,  2.7707,  1.7756,  1.7999]],
       dtype=torch.float64)
	q_value: tensor([[-30.6825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29614765435320656, distance: 0.9600580078126056 entropy -1.9076664601401332
epoch: 21, step: 51
	action: tensor([[ 3.4431, -1.7983, -2.1816,  0.1949, -2.4209,  3.4880,  5.1833]],
       dtype=torch.float64)
	q_value: tensor([[-76.6848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9600580078126056 entropy -0.9763169471839629
epoch: 21, step: 52
	action: tensor([[ 0.2053,  0.3608,  0.0270,  0.4524, -0.0896,  0.1322, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7946892086647596, distance: 0.518516501022293 entropy -2.992571459059388
epoch: 21, step: 53
	action: tensor([[ 0.7614,  0.3056,  0.5082, -0.6927,  0.7222,  0.1663,  0.9014]],
       dtype=torch.float64)
	q_value: tensor([[-13.0647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6843566073702432, distance: 0.6429171999697677 entropy -2.3709598287592217
epoch: 21, step: 54
	action: tensor([[-0.3510,  0.1066,  0.6824,  0.2620,  0.2453,  0.1915, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6429171999697677 entropy -2.992571459059388
epoch: 21, step: 55
	action: tensor([[ 0.1637, -0.4997,  0.1776, -0.2454, -0.3055, -0.0299, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18829534552466187, distance: 1.2474377750897336 entropy -2.992571459059388
epoch: 21, step: 56
	action: tensor([[-0.2630, -0.1756,  0.1336,  0.2144, -1.1370,  0.6707,  1.3072]],
       dtype=torch.float64)
	q_value: tensor([[-14.9940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06725802133802605, distance: 1.182201230518716 entropy -2.119377829704594
epoch: 21, step: 57
	action: tensor([[-3.2140, -0.7044,  0.7595,  0.5270,  0.4573,  1.3624,  2.2724]],
       dtype=torch.float64)
	q_value: tensor([[-31.6116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.182201230518716 entropy -1.702358812931124
epoch: 21, step: 58
	action: tensor([[-0.0596, -0.0753, -0.2914,  0.8178, -0.3137,  0.0844, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3673100062198972, distance: 0.9102321610162124 entropy -2.992571459059388
epoch: 21, step: 59
	action: tensor([[-0.2091,  0.2445, -0.4616, -0.3160, -0.2242,  0.1688,  0.8025]],
       dtype=torch.float64)
	q_value: tensor([[-14.1101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17182644512818612, distance: 1.0413995185168563 entropy -2.4666725564118543
epoch: 21, step: 60
	action: tensor([[-0.1739, -0.2307, -0.1329,  0.8834,  0.1639,  0.1701, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33494233301188325, distance: 0.9332249591859146 entropy -2.992571459059388
epoch: 21, step: 61
	action: tensor([[-1.0702,  0.1336,  0.1604, -0.9036,  0.6174,  0.9283,  0.8271]],
       dtype=torch.float64)
	q_value: tensor([[-16.9262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9210268960413917, distance: 1.586073890576929 entropy -2.426582469850343
epoch: 21, step: 62
	action: tensor([[ 0.2664, -0.0824,  0.0953, -0.1799,  0.0086,  0.1336, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-42.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3912500704542684, distance: 0.8928451734417341 entropy -2.992571459059388
epoch: 21, step: 63
	action: tensor([[-0.4350, -0.3510, -0.0581, -1.4152, -1.4599,  1.0682,  1.0908]],
       dtype=torch.float64)
	q_value: tensor([[-15.5881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7209447505599618, distance: 1.5012055357533844 entropy -2.250154042697086
LOSS epoch 21 actor 451.08576036066876 critic 1041.6660900560667
epoch: 22, step: 0
	action: tensor([[-1.2312, -0.6238,  0.3459,  0.3939, -1.3073,  2.1711,  2.5283]],
       dtype=torch.float64)
	q_value: tensor([[-47.2882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6696105109574333, distance: 1.4786462232450532 entropy -1.5531730656674927
epoch: 22, step: 1
	action: tensor([[-6.3630,  0.4431, -0.0233, -2.9362, -6.0629,  1.2490,  3.6565]],
       dtype=torch.float64)
	q_value: tensor([[-68.1852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4786462232450532 entropy -1.2271504079188607
epoch: 22, step: 2
	action: tensor([[-0.4236, -0.1012, -0.3256,  0.0356,  0.0689,  0.1684, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29418848094706695, distance: 1.301833545739383 entropy -3.01477310643845
epoch: 22, step: 3
	action: tensor([[-0.2539,  0.0975,  0.4473,  0.0584,  0.2829,  0.4193,  0.2498]],
       dtype=torch.float64)
	q_value: tensor([[-19.6673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25260167667291344, distance: 0.9893108062170106 entropy -3.088052241007913
epoch: 22, step: 4
	action: tensor([[-1.5935,  0.0769, -0.2531, -0.2445, -0.1360,  0.6312,  1.0723]],
       dtype=torch.float64)
	q_value: tensor([[-22.3868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5611585775245436, distance: 1.8313650756379827 entropy -2.2044375630556385
epoch: 22, step: 5
	action: tensor([[ 0.0524,  0.0960, -0.2909, -0.8257, -1.3719,  0.9113,  0.8462]],
       dtype=torch.float64)
	q_value: tensor([[-40.8275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8313650756379827 entropy -2.3680591074478614
epoch: 22, step: 6
	action: tensor([[-0.3164, -0.6380, -0.1105, -0.3186,  0.7524,  0.0734, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6871374735954325, distance: 1.4863871056142697 entropy -3.01477310643845
epoch: 22, step: 7
	action: tensor([[-0.6847,  0.0622,  0.3981, -0.2381,  0.5356,  0.6018,  0.7007]],
       dtype=torch.float64)
	q_value: tensor([[-28.3001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42739159770843993, distance: 1.36718807362812 entropy -2.4796757973648362
epoch: 22, step: 8
	action: tensor([[ 0.6753, -0.5437,  0.2365, -0.1847,  0.4736,  0.7000,  1.2765]],
       dtype=torch.float64)
	q_value: tensor([[-32.3706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31516362688541355, distance: 0.9470002694259896 entropy -2.0681855928134727
epoch: 22, step: 9
	action: tensor([[ 0.1580, -0.1937, -1.8453, -0.0470, -0.7313,  1.5660,  2.2799]],
       dtype=torch.float64)
	q_value: tensor([[-40.9722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03603326036836485, distance: 1.1235378768158761 entropy -1.6042249424594175
epoch: 22, step: 10
	action: tensor([[ 2.0369, -2.7813, -0.2212,  1.5710, -0.5059,  0.8816,  2.3066]],
       dtype=torch.float64)
	q_value: tensor([[-67.0207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1235378768158761 entropy -1.6308684209001139
epoch: 22, step: 11
	action: tensor([[-0.2061, -0.4338,  0.1202,  0.6128, -0.2848,  0.2006, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12753325461012188, distance: 1.068885374150725 entropy -3.01477310643845
epoch: 22, step: 12
	action: tensor([[-0.7121,  0.1334,  0.5017, -0.4331,  0.0687,  0.6620,  0.9464]],
       dtype=torch.float64)
	q_value: tensor([[-17.8881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5667581174694518, distance: 1.432378092289857 entropy -2.2934184006153786
epoch: 22, step: 13
	action: tensor([[-0.4980, -0.7681,  0.4235,  0.0113,  0.7999,  1.0701,  1.6334]],
       dtype=torch.float64)
	q_value: tensor([[-33.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2748776084064266, distance: 1.2920845692789829 entropy -1.8860916728878354
epoch: 22, step: 14
	action: tensor([[-5.2289, -0.7646, -1.5526, -1.4620, -1.7961,  2.3049,  2.3079]],
       dtype=torch.float64)
	q_value: tensor([[-48.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2920845692789829 entropy -1.586333437318677
epoch: 22, step: 15
	action: tensor([[ 0.3189, -0.4041,  0.1560,  0.2529, -0.0069,  0.3081, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5029941275054784, distance: 0.8067471751965387 entropy -3.01477310643845
epoch: 22, step: 16
	action: tensor([[-0.5519, -1.4238,  0.1039, -1.1060, -1.5079,  0.5106,  1.1993]],
       dtype=torch.float64)
	q_value: tensor([[-20.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8067471751965387 entropy -2.0999276809458403
epoch: 22, step: 17
	action: tensor([[-0.0010, -0.0296, -0.0186,  0.1750,  0.2263,  0.3364, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4479655180957517, distance: 0.8502366005008135 entropy -3.01477310643845
epoch: 22, step: 18
	action: tensor([[0.7384, 0.2601, 0.0202, 0.2655, 0.0181, 0.9573, 0.6680]],
       dtype=torch.float64)
	q_value: tensor([[-20.0808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.985013036063517, distance: 0.14009206094521492 entropy -2.522639953969922
epoch: 22, step: 19
	action: tensor([[-0.1220, -0.4934, -0.0813,  0.5221,  0.0988,  0.0835, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042518111528016767, distance: 1.1197523359612462 entropy -3.01477310643845
epoch: 22, step: 20
	action: tensor([[-1.2456,  0.0663,  0.0182, -0.8997,  0.8751, -0.2604,  0.7427]],
       dtype=torch.float64)
	q_value: tensor([[-19.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4001477853715123, distance: 1.77286507677484 entropy -2.4634535747257673
epoch: 22, step: 21
	action: tensor([[ 0.5341, -0.4417, -0.1387, -0.1873, -0.2166,  0.3184,  0.5450]],
       dtype=torch.float64)
	q_value: tensor([[-37.4685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26320546209034434, distance: 0.9822677650335545 entropy -2.6039700499875513
epoch: 22, step: 22
	action: tensor([[ 0.2593,  0.1076,  0.0084,  0.0348, -0.5649,  0.4782,  1.5620]],
       dtype=torch.float64)
	q_value: tensor([[-27.6341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9822677650335545 entropy -1.933477558281658
epoch: 22, step: 23
	action: tensor([[ 0.1527, -0.0315, -0.1187,  0.5938, -0.3845, -0.1602, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6408272222216648, distance: 0.6858172428824578 entropy -3.01477310643845
epoch: 22, step: 24
	action: tensor([[ 0.3369,  0.1538, -0.7399, -0.6973, -0.1205,  0.2982,  0.7542]],
       dtype=torch.float64)
	q_value: tensor([[-13.9029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6227206869694546, distance: 0.702891335108149 entropy -2.45330636787405
epoch: 22, step: 25
	action: tensor([[ 0.2512, -0.3350,  0.1115,  0.0861, -0.1296, -0.2068, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2308135747506529, distance: 1.0036273754259417 entropy -3.01477310643845
epoch: 22, step: 26
	action: tensor([[ 0.0532, -0.4406,  0.1573, -1.7192, -0.5959,  0.6386,  0.8746]],
       dtype=torch.float64)
	q_value: tensor([[-15.7561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.592121304033135, distance: 1.4439254464833677 entropy -2.3357648901810237
epoch: 22, step: 27
	action: tensor([[ 2.6801, -1.6069, -2.2064, -1.4954,  0.9721,  2.2415,  2.3458]],
       dtype=torch.float64)
	q_value: tensor([[-43.3880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4439254464833677 entropy -1.599347186688495
epoch: 22, step: 28
	action: tensor([[-0.0222,  0.0156,  0.0075, -0.0567,  0.0362, -0.0569, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23271897031974986, distance: 1.0023835333405127 entropy -3.01477310643845
epoch: 22, step: 29
	action: tensor([[ 1.0400,  0.3488, -0.5246, -0.2709,  0.2573,  0.5683,  0.4957]],
       dtype=torch.float64)
	q_value: tensor([[-15.8454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9235849569986375, distance: 0.3163341829820491 entropy -2.71713584040812
epoch: 22, step: 30
	action: tensor([[-0.8841,  0.5833, -0.6675, -0.5232, -1.5370,  0.9750,  1.5046]],
       dtype=torch.float64)
	q_value: tensor([[-31.0707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6574886747621185, distance: 1.473268753779463 entropy -1.9349231086922365
epoch: 22, step: 31
	action: tensor([[ 1.3091, -1.7220,  0.5599, -0.3433,  2.6162,  1.8411,  1.7885]],
       dtype=torch.float64)
	q_value: tensor([[-46.3590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.473268753779463 entropy -1.8342696153948468
epoch: 22, step: 32
	action: tensor([[-0.3716,  0.1636,  0.2510, -0.5349, -0.1605,  0.1367, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33057602650866436, distance: 1.320007930551082 entropy -3.01477310643845
epoch: 22, step: 33
	action: tensor([[ 0.4122, -0.2666,  0.1403,  0.3037, -0.1959,  0.4189,  0.5722]],
       dtype=torch.float64)
	q_value: tensor([[-19.8183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6782877170973026, distance: 0.6490684725797389 entropy -2.607489759098229
epoch: 22, step: 34
	action: tensor([[-0.1105, -0.3763,  0.8095, -0.2740, -2.3792,  0.8188,  1.5398]],
       dtype=torch.float64)
	q_value: tensor([[-24.9136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49198253574764106, distance: 1.3977791662016228 entropy -1.9369952261655445
epoch: 22, step: 35
	action: tensor([[ 1.4331,  2.7564, -3.6992,  1.8073, -0.5920,  1.8682,  3.4640]],
       dtype=torch.float64)
	q_value: tensor([[-45.9970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3977791662016228 entropy -1.2793069517846392
epoch: 22, step: 36
	action: tensor([[ 0.0746, -0.1226, -0.6730, -0.5076,  0.0658, -0.0479, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0024290272540254954, distance: 1.1457332327352556 entropy -3.01477310643845
epoch: 22, step: 37
	action: tensor([[-0.0463, -0.2318, -0.3852,  0.4066, -0.4995,  0.1595,  0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-21.3825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17571916257774656, distance: 1.2408191617617206 entropy -3.054150888275221
epoch: 22, step: 38
	action: tensor([[-0.9268,  0.1965,  0.5421,  0.6895, -1.3508,  0.5752,  0.8746]],
       dtype=torch.float64)
	q_value: tensor([[-20.4479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38388336765065634, distance: 1.3461902418110627 entropy -2.4106549473666163
epoch: 22, step: 39
	action: tensor([[-0.1685,  0.5430,  0.8108, -1.0639,  1.9586,  0.6188,  1.8964]],
       dtype=torch.float64)
	q_value: tensor([[-29.0969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4549341865137031, distance: 1.3803155086590153 entropy -1.7720431854510188
epoch: 22, step: 40
	action: tensor([[-1.4237, -1.0316,  0.8508, -1.1409,  2.0629,  0.9907,  2.6656]],
       dtype=torch.float64)
	q_value: tensor([[-57.8489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5979867517627195, distance: 1.4465827450358757 entropy -1.4372018989354396
epoch: 22, step: 41
	action: tensor([[ 0.6290,  1.0957, -0.2991, -3.4294, -0.0543,  3.5263,  3.3410]],
       dtype=torch.float64)
	q_value: tensor([[-75.9439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4465827450358757 entropy -1.2652681444911147
epoch: 22, step: 42
	action: tensor([[ 0.3047, -0.2139,  0.4709,  0.1669,  0.3857,  0.2587, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6071712251640442, distance: 0.717229822888636 entropy -3.01477310643845
epoch: 22, step: 43
	action: tensor([[-1.2314, -0.2225,  0.4673, -0.2697,  0.1352,  0.3792,  1.2398]],
       dtype=torch.float64)
	q_value: tensor([[-22.6066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4084885019394342, distance: 1.775942833276526 entropy -2.0522706129741244
epoch: 22, step: 44
	action: tensor([[-2.0156,  0.2169, -1.5324,  1.1084,  2.1414,  0.8992,  1.5837]],
       dtype=torch.float64)
	q_value: tensor([[-36.4086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.775942833276526 entropy -1.9134848665582367
epoch: 22, step: 45
	action: tensor([[ 0.0041, -0.1972,  0.2883, -0.5239, -0.2042,  0.0966, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1777012180262345, distance: 1.241864622622614 entropy -3.01477310643845
epoch: 22, step: 46
	action: tensor([[-0.2508, -0.0282,  0.3736, -0.0094,  0.2486,  0.3388,  0.9512]],
       dtype=torch.float64)
	q_value: tensor([[-19.1064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11634478666308334, distance: 1.0757172062482174 entropy -2.274351050703407
epoch: 22, step: 47
	action: tensor([[ 2.2920, -0.3973,  0.8462, -1.3169,  1.3628,  0.5448,  1.4821]],
       dtype=torch.float64)
	q_value: tensor([[-28.2648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0757172062482174 entropy -1.9666972578643434
epoch: 22, step: 48
	action: tensor([[ 0.0923, -0.0352, -0.1576,  0.1383,  0.5662,  0.1405, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4609562698342128, distance: 0.8401729456593416 entropy -3.01477310643845
epoch: 22, step: 49
	action: tensor([[ 0.0646, -0.7661,  0.3678,  0.0152,  0.8528,  0.2320,  0.5235]],
       dtype=torch.float64)
	q_value: tensor([[-22.1697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24203069703826774, distance: 1.275330828807392 entropy -2.655427918688859
epoch: 22, step: 50
	action: tensor([[ 3.4424, -0.4498,  0.0711, -0.0148,  1.2481,  0.6107,  1.5442]],
       dtype=torch.float64)
	q_value: tensor([[-31.6489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.275330828807392 entropy -1.9195528401650148
epoch: 22, step: 51
	action: tensor([[-0.0033, -0.0753, -0.1781, -0.2732, -0.2833, -0.0167, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11648190712763029, distance: 1.0756337412477186 entropy -3.01477310643845
epoch: 22, step: 52
	action: tensor([[ 0.6075,  0.0023,  0.0869, -0.3146,  0.0799,  0.1906,  0.5158]],
       dtype=torch.float64)
	q_value: tensor([[-16.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6039172346466065, distance: 0.7201942771059863 entropy -2.7092985354380446
epoch: 22, step: 53
	action: tensor([[ 1.8462, -0.9196, -0.6202, -0.6609,  0.1217,  0.9388,  1.4137]],
       dtype=torch.float64)
	q_value: tensor([[-24.9748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7201942771059863 entropy -1.998390407100165
epoch: 22, step: 54
	action: tensor([[-0.5834,  0.1486,  0.7180,  0.0015, -0.0875,  0.0696, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33026274716634063, distance: 1.3198525258421132 entropy -3.01477310643845
epoch: 22, step: 55
	action: tensor([[-0.1884, -0.8997,  0.3497, -1.2393, -0.8603,  0.2862,  0.8023]],
       dtype=torch.float64)
	q_value: tensor([[-17.1946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0551531756894108, distance: 1.6405096579946161 entropy -2.362860893673062
epoch: 22, step: 56
	action: tensor([[ 2.0415, -2.8911,  0.0868, -0.9074, -0.7201,  1.7537,  2.2288]],
       dtype=torch.float64)
	q_value: tensor([[-37.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6405096579946161 entropy -1.6455192487974364
epoch: 22, step: 57
	action: tensor([[-0.2242, -0.3817,  0.4468, -0.2069,  0.0312,  0.0034, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45306525363224637, distance: 1.379428682973022 entropy -3.01477310643845
epoch: 22, step: 58
	action: tensor([[ 0.5669, -0.0172, -0.3287,  0.1573,  0.4857,  0.3020,  0.9211]],
       dtype=torch.float64)
	q_value: tensor([[-18.9173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8016079222278618, distance: 0.5097049557853967 entropy -2.2952016865591984
epoch: 22, step: 59
	action: tensor([[ 0.4855,  0.2748,  1.3813, -0.0785, -0.0536, -0.0173,  1.3151]],
       dtype=torch.float64)
	q_value: tensor([[-31.8989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7849111057011693, distance: 0.5307202842411336 entropy -2.055042178698401
epoch: 22, step: 60
	action: tensor([[ 2.6296, -1.7741,  0.5420, -2.2975, -0.0992,  1.7286,  2.4818]],
       dtype=torch.float64)
	q_value: tensor([[-28.8345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5307202842411336 entropy -1.5224432471292944
epoch: 22, step: 61
	action: tensor([[ 0.4345,  0.1770, -0.1245,  0.3411, -0.1689, -0.0051, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8324842398678827, distance: 0.46836510333411246 entropy -3.01477310643845
epoch: 22, step: 62
	action: tensor([[ 0.0080, -0.0255, -0.1070, -0.3075, -0.1488,  0.2943, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-43.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20113257700096687, distance: 1.0228078415930573 entropy -3.01477310643845
epoch: 22, step: 63
	action: tensor([[ 0.6711, -0.4676,  0.5051,  0.3494,  0.9633,  0.3364,  0.6907]],
       dtype=torch.float64)
	q_value: tensor([[-19.1785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5546326349056205, distance: 0.7636878627514794 entropy -2.515354973377106
LOSS epoch 22 actor 482.3050598560556 critic 620.1625486598531
epoch: 23, step: 0
	action: tensor([[ 0.1808,  0.3159, -0.7965, -0.0496, -0.0033,  2.2974,  1.4446]],
       dtype=torch.float64)
	q_value: tensor([[-38.1520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7636878627514794 entropy -1.844509161477801
epoch: 23, step: 1
	action: tensor([[-0.2604, -0.2946, -0.0797, -0.4832,  0.0598,  0.0911, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44114961105737405, distance: 1.37376114161169 entropy -3.1822808235859172
epoch: 23, step: 2
	action: tensor([[ 0.1497, -0.6574, -0.2649, -0.0390,  0.2139,  0.4043,  0.5068]],
       dtype=torch.float64)
	q_value: tensor([[-23.1980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0759216618176386, distance: 1.186989887318826 entropy -2.6929570287125038
epoch: 23, step: 3
	action: tensor([[ 1.8306, -0.4850, -0.0258, -0.4289, -0.6351,  0.6963,  1.1014]],
       dtype=torch.float64)
	q_value: tensor([[-32.5452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.186989887318826 entropy -2.1422331796035694
epoch: 23, step: 4
	action: tensor([[-0.1163, -0.1798,  0.0548, -0.2386,  0.2839,  0.0458, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11244205972468735, distance: 1.2069669928407425 entropy -3.1822808235859172
epoch: 23, step: 5
	action: tensor([[ 1.2135, -0.4711,  0.3731, -0.2912,  0.1234,  0.4468,  0.4934]],
       dtype=torch.float64)
	q_value: tensor([[-21.8658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1912505116364278, distance: 1.0291145127400732 entropy -2.677014588473449
epoch: 23, step: 6
	action: tensor([[ 2.9623,  0.0114, -0.3900,  1.4776, -2.2768,  1.0980,  1.8504]],
       dtype=torch.float64)
	q_value: tensor([[-36.8422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0291145127400732 entropy -1.6657650140217246
epoch: 23, step: 7
	action: tensor([[ 0.1384, -0.0558, -0.1524,  0.3024, -0.3584,  0.1392, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5432209482455943, distance: 0.7734099990755189 entropy -3.1822808235859172
epoch: 23, step: 8
	action: tensor([[-0.2734, -0.4622, -0.8736,  0.3754, -0.3086,  0.0657,  0.7410]],
       dtype=torch.float64)
	q_value: tensor([[-18.0836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4284928464194666, distance: 1.3677153724501938 entropy -2.437805344222533
epoch: 23, step: 9
	action: tensor([[ 0.3624,  0.1785, -0.2750, -0.1392,  0.4383,  0.8426,  0.7258]],
       dtype=torch.float64)
	q_value: tensor([[-30.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3677153724501938 entropy -2.515252399273702
epoch: 23, step: 10
	action: tensor([[ 0.0380, -0.0836, -0.0130, -0.0827, -0.0823, -0.1614, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19049848975057482, distance: 1.029592866558706 entropy -3.1822808235859172
epoch: 23, step: 11
	action: tensor([[0.1972, 0.1973, 0.3861, 0.1852, 0.4662, 0.3842, 0.4843]],
       dtype=torch.float64)
	q_value: tensor([[-16.9571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7149354689001185, distance: 0.6109818163687109 entropy -2.7041235558408734
epoch: 23, step: 12
	action: tensor([[-1.8050, -0.0718, -0.4139, -0.8452, -1.9228,  0.1403,  1.0715]],
       dtype=torch.float64)
	q_value: tensor([[-26.5357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6109818163687109 entropy -2.1128749185645486
epoch: 23, step: 13
	action: tensor([[ 0.1195, -0.1761, -0.0674,  0.1823,  0.1558,  0.1045, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37600067348287547, distance: 0.9039590430853393 entropy -3.1822808235859172
epoch: 23, step: 14
	action: tensor([[-0.7305, -0.0876, -0.1424, -0.6857,  0.8453,  0.8221,  0.5815]],
       dtype=torch.float64)
	q_value: tensor([[-21.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.426721750024335, distance: 1.3668672383246447 entropy -2.579184503739033
epoch: 23, step: 15
	action: tensor([[-0.9915,  0.0237, -0.6194, -1.2126,  0.1727, -0.0329,  0.8108]],
       dtype=torch.float64)
	q_value: tensor([[-42.3031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4779494043873065, distance: 1.3911900942574593 entropy -2.3179436846362793
epoch: 23, step: 16
	action: tensor([[-0.3859, -0.0713,  0.6467,  0.0704,  0.0978,  0.2517,  0.2877]],
       dtype=torch.float64)
	q_value: tensor([[-39.0796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028284909222128896, distance: 1.1604152414840643 entropy -2.9799230038235582
epoch: 23, step: 17
	action: tensor([[-0.6072,  0.0506, -2.1484, -0.5240,  0.6547,  0.8952,  1.0346]],
       dtype=torch.float64)
	q_value: tensor([[-22.2373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1675105413437099, distance: 1.236480006937642 entropy -2.162858369218081
epoch: 23, step: 18
	action: tensor([[-0.0505,  0.0085,  0.1241,  0.0427,  0.0344,  0.3148,  0.1781]],
       dtype=torch.float64)
	q_value: tensor([[-55.9205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3675575428259876, distance: 0.9100540818425089 entropy -2.9968614728555827
epoch: 23, step: 19
	action: tensor([[-0.9341, -0.0835,  0.5893, -0.1036, -0.5958,  0.5247,  0.8047]],
       dtype=torch.float64)
	q_value: tensor([[-21.8866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8611337338290814, distance: 1.5611530567797505 entropy -2.3754107775035083
epoch: 23, step: 20
	action: tensor([[-1.6464, -0.0148, -0.1792,  0.7352,  2.0961,  0.9195,  1.5345]],
       dtype=torch.float64)
	q_value: tensor([[-29.1681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5611530567797505 entropy -1.8864125960987164
epoch: 23, step: 21
	action: tensor([[-0.4009,  0.0196, -0.1594,  0.2708,  0.0736, -0.1027, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13012910082303653, distance: 1.2165241150818549 entropy -3.1822808235859172
epoch: 23, step: 22
	action: tensor([[ 0.5525, -0.2291, -0.3105,  0.2243, -0.0122,  0.3180,  0.2503]],
       dtype=torch.float64)
	q_value: tensor([[-17.8469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6657222991668117, distance: 0.6616226988079017 entropy -3.103447488287478
epoch: 23, step: 23
	action: tensor([[-0.5159,  0.9051, -0.8566,  0.2359, -1.3707,  0.8056,  1.0239]],
       dtype=torch.float64)
	q_value: tensor([[-26.9800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6616226988079017 entropy -2.189566930227533
epoch: 23, step: 24
	action: tensor([[ 0.1360, -0.1016, -0.1966, -0.1936,  0.5283,  0.0363, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2835637345216415, distance: 0.9686022509395912 entropy -3.1822808235859172
epoch: 23, step: 25
	action: tensor([[ 0.1907, -0.4874,  0.2804, -0.0109, -0.0584,  0.6875,  0.4240]],
       dtype=torch.float64)
	q_value: tensor([[-23.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30802636500636793, distance: 0.9519222260185388 entropy -2.745005251426573
epoch: 23, step: 26
	action: tensor([[ 0.2034, -0.7721, -0.3562, -0.4388,  0.2070,  1.4730,  1.4544]],
       dtype=torch.float64)
	q_value: tensor([[-31.3119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1262100391557034, distance: 1.0696956226730705 entropy -1.8999709375304736
epoch: 23, step: 27
	action: tensor([[ 2.4070, -2.3615, -0.4498,  0.7585,  3.2394,  2.0820,  2.1515]],
       dtype=torch.float64)
	q_value: tensor([[-56.4521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0696956226730705 entropy -1.5882952197561038
epoch: 23, step: 28
	action: tensor([[ 0.1177, -0.2009,  0.2721, -0.0038, -0.0149,  0.2054, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3234188110329508, distance: 0.9412752787639984 entropy -3.1822808235859172
epoch: 23, step: 29
	action: tensor([[ 0.1589, -1.0480,  0.1830, -0.8994,  0.3675,  0.6359,  0.8295]],
       dtype=torch.float64)
	q_value: tensor([[-21.2744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8042222633261147, distance: 1.5370985469997807 entropy -2.3180012752889807
epoch: 23, step: 30
	action: tensor([[0.3312, 0.2193, 0.3860, 0.6751, 0.7796, 2.0455, 1.8418]],
       dtype=torch.float64)
	q_value: tensor([[-43.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5370985469997807 entropy -1.698219130923672
epoch: 23, step: 31
	action: tensor([[-0.1234,  0.4221,  0.0352, -0.1579,  0.2707,  0.0160, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33753899601953685, distance: 0.9314013273084981 entropy -3.1822808235859172
epoch: 23, step: 32
	action: tensor([[ 0.0717,  0.0086, -0.2422,  0.2328,  0.3779,  0.0991, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49175392483286107, distance: 0.8158188017480935 entropy -3.1822808235859172
epoch: 23, step: 33
	action: tensor([[ 0.0887, -0.2908,  0.2775,  0.4295,  0.1129,  0.2424,  0.4043]],
       dtype=torch.float64)
	q_value: tensor([[-21.7987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5172257836010237, distance: 0.7951127672980407 entropy -2.7867454804094476
epoch: 23, step: 34
	action: tensor([[-1.1114, -0.2344,  0.1168, -0.1106, -0.8015,  0.6850,  1.0590]],
       dtype=torch.float64)
	q_value: tensor([[-24.8244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2747521613381076, distance: 1.7259322205444865 entropy -2.1493167759776197
epoch: 23, step: 35
	action: tensor([[-0.0476, -0.3932,  0.1886, -0.5099,  2.3670,  1.5802,  1.5586]],
       dtype=torch.float64)
	q_value: tensor([[-36.7557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035939363873066066, distance: 1.1647262437985315 entropy -1.895745224417195
epoch: 23, step: 36
	action: tensor([[-0.6627,  0.4543,  1.2512, -0.1516, -0.5017,  1.3594,  1.9279]],
       dtype=torch.float64)
	q_value: tensor([[-69.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1647262437985315 entropy -1.574729009847964
epoch: 23, step: 37
	action: tensor([[-0.2225, -0.1638,  0.0864, -0.5991,  0.3745,  0.1143, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3534835533204217, distance: 1.33132223283011 entropy -3.1822808235859172
epoch: 23, step: 38
	action: tensor([[-0.6793,  0.1915, -0.1475, -0.8615,  0.6997,  0.4563,  0.5298]],
       dtype=torch.float64)
	q_value: tensor([[-25.3195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.433123998522031, distance: 1.3699306346691669 entropy -2.6140292853923506
epoch: 23, step: 39
	action: tensor([[-0.8070, -0.1672, -0.1427,  0.5044, -0.2910,  0.1373,  0.5906]],
       dtype=torch.float64)
	q_value: tensor([[-37.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6134981280058056, distance: 1.4535866517538802 entropy -2.543597381433171
epoch: 23, step: 40
	action: tensor([[ 0.9447,  0.0742, -0.0403, -0.7761, -0.5968,  0.6776,  0.7463]],
       dtype=torch.float64)
	q_value: tensor([[-25.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5825891479164331, distance: 0.7393303952587519 entropy -2.4693660030111175
epoch: 23, step: 41
	action: tensor([[ 1.5755, -0.5497, -1.5286, -0.2127,  2.6491,  1.6339,  1.9787]],
       dtype=torch.float64)
	q_value: tensor([[-36.4412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02673569093627981, distance: 1.1289432000016333 entropy -1.6566827882988817
epoch: 23, step: 42
	action: tensor([[-1.5234,  0.0900,  1.4530, -1.7779, -2.9913,  1.0978,  2.0419]],
       dtype=torch.float64)
	q_value: tensor([[-84.6494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28829345988370436, distance: 1.298865239582401 entropy -1.5184197551820178
epoch: 23, step: 43
	action: tensor([[-1.1172, -3.4209,  4.1611, -0.7151, -5.4118,  0.6914,  4.0203]],
       dtype=torch.float64)
	q_value: tensor([[-64.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.298865239582401 entropy -1.0880738900656175
epoch: 23, step: 44
	action: tensor([[ 0.0368, -0.1021,  0.4388,  0.1997,  0.1447,  0.1045, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4295457263441349, distance: 0.8643051729562891 entropy -3.1822808235859172
epoch: 23, step: 45
	action: tensor([[ 0.0936, -0.2948, -0.0878, -0.1248, -0.1830,  0.5148,  0.7741]],
       dtype=torch.float64)
	q_value: tensor([[-20.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1984125128103087, distance: 1.0245476414215027 entropy -2.3488405365962883
epoch: 23, step: 46
	action: tensor([[ 0.5171,  0.3926, -0.1936, -1.4711,  0.9050,  1.3446,  1.3355]],
       dtype=torch.float64)
	q_value: tensor([[-31.5601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7514607858657981, distance: 0.5704980360168932 entropy -1.9983216962429615
epoch: 23, step: 47
	action: tensor([[ 1.6780, -1.2270, -0.2273,  0.6372, -1.1271,  1.8365,  1.9583]],
       dtype=torch.float64)
	q_value: tensor([[-56.5292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5704980360168932 entropy -1.6126190052999971
epoch: 23, step: 48
	action: tensor([[ 0.0283, -0.1158,  0.1667,  0.5424, -0.0028,  0.2069, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5785135373899088, distance: 0.7429310480939568 entropy -3.1822808235859172
epoch: 23, step: 49
	action: tensor([[ 0.2707, -0.5517, -0.5527, -0.7004,  0.0579,  0.6566,  0.7745]],
       dtype=torch.float64)
	q_value: tensor([[-21.0681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05164389369766287, distance: 1.1735214860074499 entropy -2.3773653114023072
epoch: 23, step: 50
	action: tensor([[-0.4305, -0.2135, -0.2831, -0.2507,  0.2033,  0.0760, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4627016973506195, distance: 1.3839951746780719 entropy -3.1822808235859172
epoch: 23, step: 51
	action: tensor([[-0.4577, -0.1379, -0.0264,  0.3311, -0.0060,  0.2635,  0.2470]],
       dtype=torch.float64)
	q_value: tensor([[-22.9986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15923818841387827, distance: 1.232091703059197 entropy -3.1080187258948895
epoch: 23, step: 52
	action: tensor([[ 1.1174, -0.9887, -0.2609,  0.9828,  0.4953,  0.6053,  0.6718]],
       dtype=torch.float64)
	q_value: tensor([[-22.3866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6702168635859982, distance: 0.6571596888334348 entropy -2.5289433206943195
epoch: 23, step: 53
	action: tensor([[-0.5723, -0.6723, -0.3782, -1.9534, -3.7905,  0.3437,  1.6656]],
       dtype=torch.float64)
	q_value: tensor([[-45.8558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0820181347818204, distance: 1.096411909119277 entropy -1.7526822436059144
epoch: 23, step: 54
	action: tensor([[ 1.2485, -0.5636,  0.2306, -1.9588, -0.3584,  0.9517,  1.4840]],
       dtype=torch.float64)
	q_value: tensor([[-71.8818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2627589834612476, distance: 1.2859288109080056 entropy -1.7891772987585948
epoch: 23, step: 55
	action: tensor([[-5.4038, -0.4525, -3.0865, -0.8373, -1.7233,  1.2616,  3.0973]],
       dtype=torch.float64)
	q_value: tensor([[-59.1914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7808570625486252, distance: 0.5356985028769382 entropy -1.2633946468706299
epoch: 23, step: 56
	action: tensor([[ 1.2075, -1.2552,  0.1826, -1.2200,  0.6400,  0.4202,  2.8116]],
       dtype=torch.float64)
	q_value: tensor([[-88.8950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5356985028769382 entropy -1.4103945938344862
epoch: 23, step: 57
	action: tensor([[ 0.4167,  0.1184,  0.1763,  0.0785,  0.4263,  0.0681, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7809325590653396, distance: 0.5356062186917527 entropy -3.1822808235859172
epoch: 23, step: 58
	action: tensor([[-0.3870, -0.1607,  1.3195,  0.1656,  0.9068,  0.5257,  0.7074]],
       dtype=torch.float64)
	q_value: tensor([[-22.6562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003547244145237549, distance: 1.1463720915108222 entropy -2.3765022167933876
epoch: 23, step: 59
	action: tensor([[ 0.0435, -0.2260,  0.0074, -0.1013, -0.0473,  0.0646, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13291192565787957, distance: 1.0655854942126146 entropy -3.1822808235859172
epoch: 23, step: 60
	action: tensor([[-0.7424, -0.3870, -0.1441,  0.2418,  0.8534,  0.1927,  0.6417]],
       dtype=torch.float64)
	q_value: tensor([[-19.8404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7210282232151284, distance: 1.5012419425271966 entropy -2.521276234046677
epoch: 23, step: 61
	action: tensor([[ 0.9715,  0.3989, -0.1545, -0.4762,  0.2576,  0.0261,  0.5530]],
       dtype=torch.float64)
	q_value: tensor([[-32.5637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7945212651903237, distance: 0.518728529969647 entropy -2.5972919508366212
epoch: 23, step: 62
	action: tensor([[ 2.5067, -0.7026,  0.6185, -0.3226,  1.6453,  0.6462,  1.1442]],
       dtype=torch.float64)
	q_value: tensor([[-29.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.518728529969647 entropy -2.0520674387023767
epoch: 23, step: 63
	action: tensor([[-0.1920,  0.1331, -0.1200, -0.2165,  0.1735, -0.0045, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.3371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05928628743318198, distance: 1.1099040359127585 entropy -3.1822808235859172
LOSS epoch 23 actor 582.3573214663717 critic 626.4291021892959
epoch: 24, step: 0
	action: tensor([[-0.0062, -0.0240,  0.3424, -0.3503,  0.6745,  0.2380,  0.1633]],
       dtype=torch.float64)
	q_value: tensor([[-19.3751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0992303617620739, distance: 1.0860843678392023 entropy -3.1312763093180607
epoch: 24, step: 1
	action: tensor([[-0.4714,  0.0099, -0.4575,  0.3279,  0.9674,  0.8598,  0.5676]],
       dtype=torch.float64)
	q_value: tensor([[-27.4766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0860843678392023 entropy -2.3791658784167224
epoch: 24, step: 2
	action: tensor([[ 0.1661, -0.2473, -0.0228,  0.2317, -0.1011,  0.0054, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35271835887897807, distance: 0.9206686135678234 entropy -3.4117532429367428
epoch: 24, step: 3
	action: tensor([[ 0.1634, -0.0978,  0.3072, -1.4506, -0.0345,  0.6430,  0.4984]],
       dtype=torch.float64)
	q_value: tensor([[-20.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24687207019710966, distance: 1.2778139990411528 entropy -2.5299758097917686
epoch: 24, step: 4
	action: tensor([[-1.3094,  0.7016, -0.5853, -1.3715,  1.6264,  0.9007,  1.1846]],
       dtype=torch.float64)
	q_value: tensor([[-37.1310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12437846842943956, distance: 1.2134250420830208 entropy -1.8716061934298889
epoch: 24, step: 5
	action: tensor([[ 0.0255,  0.4854,  0.0053, -0.0652,  0.2754, -0.0435,  0.2166]],
       dtype=torch.float64)
	q_value: tensor([[-53.7416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2134250420830208 entropy -2.6924240100376378
epoch: 24, step: 6
	action: tensor([[-0.1577, -0.1226, -0.0114,  0.1863, -0.0975, -0.1059, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06584114532968055, distance: 1.1060303913998446 entropy -3.4117532429367428
epoch: 24, step: 7
	action: tensor([[ 0.3525, -0.2615, -0.7177, -0.2641,  0.2180,  0.3737,  0.3589]],
       dtype=torch.float64)
	q_value: tensor([[-17.9755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37051308239221237, distance: 0.9079251525088126 entropy -2.782767717497486
epoch: 24, step: 8
	action: tensor([[ 0.1139, -0.4635,  0.0482, -0.7106,  0.4469,  0.4162,  0.4808]],
       dtype=torch.float64)
	q_value: tensor([[-29.9409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2722119697930181, distance: 1.2907330541354918 entropy -2.545125217672692
epoch: 24, step: 9
	action: tensor([[-2.1511,  0.3953,  0.2118, -1.3702, -0.5944,  0.8068,  0.7981]],
       dtype=torch.float64)
	q_value: tensor([[-32.8984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2907330541354918 entropy -2.1703650309636413
epoch: 24, step: 10
	action: tensor([[ 0.1018,  0.0094,  0.3111,  0.0913, -0.0787,  0.2047, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.52281517590236, distance: 0.7904965974832523 entropy -3.4117532429367428
epoch: 24, step: 11
	action: tensor([[ 0.2944,  0.3902,  1.1028, -0.3667, -0.4100,  0.7339,  0.6055]],
       dtype=torch.float64)
	q_value: tensor([[-21.3239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7904965974832523 entropy -2.3899640199340113
epoch: 24, step: 12
	action: tensor([[ 0.1229, -0.1924,  0.0373,  0.3097,  0.1274,  0.0745, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42424604100538954, distance: 0.8683107139339126 entropy -3.4117532429367428
epoch: 24, step: 13
	action: tensor([[0.8980, 0.2526, 0.0844, 0.0056, 0.5890, 0.5568, 0.4577]],
       dtype=torch.float64)
	q_value: tensor([[-22.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9545298853495334, distance: 0.2440167945757417 entropy -2.5659902370445224
epoch: 24, step: 14
	action: tensor([[ 0.5499,  0.6230,  0.5430, -0.0522,  1.0503,  0.5528,  0.8121]],
       dtype=torch.float64)
	q_value: tensor([[-32.8707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.2440167945757417 entropy -2.0737708560060875
epoch: 24, step: 15
	action: tensor([[-0.0751, -0.0466, -0.0819,  0.0670, -0.1957, -0.0685, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18241913148087463, distance: 1.0347181174515725 entropy -3.4117532429367428
epoch: 24, step: 16
	action: tensor([[-0.1233, -0.2060, -0.0935,  0.5195, -0.0239,  0.3866,  0.3559]],
       dtype=torch.float64)
	q_value: tensor([[-17.5838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31687523484540725, distance: 0.9458161126675301 entropy -2.7902359313505523
epoch: 24, step: 17
	action: tensor([[ 0.3515,  0.2466, -0.1892,  0.8466,  0.8034,  0.3841,  0.6578]],
       dtype=torch.float64)
	q_value: tensor([[-25.7151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9458161126675301 entropy -2.3788357508593454
epoch: 24, step: 18
	action: tensor([[-0.1072, -0.2825, -0.0044, -0.2475,  0.3556,  0.0996, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9458161126675301 entropy -3.4117532429367428
epoch: 24, step: 19
	action: tensor([[-0.0671, -0.1793,  0.3610, -0.2797,  0.1228, -0.0294, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13598011821849876, distance: 1.2196692047204687 entropy -3.4117532429367428
epoch: 24, step: 20
	action: tensor([[ 1.5956,  0.3278,  0.1092, -0.1805, -0.0321,  0.8065,  0.5060]],
       dtype=torch.float64)
	q_value: tensor([[-21.5532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6346180371469112, distance: 0.6917198609935526 entropy -2.5028372505620653
epoch: 24, step: 21
	action: tensor([[-1.4275, -0.2528,  0.5575, -1.0061,  1.0571,  0.7468,  1.3438]],
       dtype=torch.float64)
	q_value: tensor([[-38.4627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4358985727926514, distance: 1.7860199009057833 entropy -1.6997784455246052
epoch: 24, step: 22
	action: tensor([[0.0938, 0.1602, 0.8686, 0.0732, 1.0709, 0.6031, 1.0621]],
       dtype=torch.float64)
	q_value: tensor([[-47.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6839942422377804, distance: 0.6432861352244025 entropy -1.9232602599728583
epoch: 24, step: 23
	action: tensor([[ 0.8272,  0.8069, -0.3750,  1.0673,  1.1807,  1.2463,  1.1096]],
       dtype=torch.float64)
	q_value: tensor([[-37.2244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6432861352244025 entropy -1.8557455583290745
epoch: 24, step: 24
	action: tensor([[ 0.1946, -0.2237,  0.1953, -0.2748, -0.0895,  0.0104, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12977960580840364, distance: 1.0675084509172197 entropy -3.4117532429367428
epoch: 24, step: 25
	action: tensor([[-0.2902,  1.1721, -0.3831, -0.5398,  0.0889,  0.4645,  0.5722]],
       dtype=torch.float64)
	q_value: tensor([[-21.5992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0675084509172197 entropy -2.422371813375259
epoch: 24, step: 26
	action: tensor([[-0.0314, -0.3728,  0.0144,  0.0684, -0.2062,  0.1561, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002170752970687584, distance: 1.1455856250321663 entropy -3.4117532429367428
epoch: 24, step: 27
	action: tensor([[-0.8724, -0.1726,  0.4480, -0.2810, -0.4170,  0.2187,  0.5857]],
       dtype=torch.float64)
	q_value: tensor([[-21.8438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1134323834117996, distance: 1.6636075095646072 entropy -2.458644018520119
epoch: 24, step: 28
	action: tensor([[-0.5149, -1.7514,  1.0622,  1.0060, -2.1819,  0.7574,  0.9735]],
       dtype=torch.float64)
	q_value: tensor([[-25.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6636075095646072 entropy -2.1323909018833094
epoch: 24, step: 29
	action: tensor([[ 0.2147, -0.1869,  0.0762, -0.1802,  0.0848,  0.0978, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2675277244928622, distance: 0.979382385414682 entropy -3.4117532429367428
epoch: 24, step: 30
	action: tensor([[ 8.6349e-01, -5.1971e-01, -2.1829e-01,  7.0090e-05,  3.3024e-01,
          6.1049e-01,  5.1672e-01]], dtype=torch.float64)
	q_value: tensor([[-22.6901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4793824867527656, distance: 0.8256882035860962 entropy -2.482061405244789
epoch: 24, step: 31
	action: tensor([[-0.6943,  0.0977,  1.5324, -2.6773, -0.8451,  0.6533,  0.9292]],
       dtype=torch.float64)
	q_value: tensor([[-37.5700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8256882035860962 entropy -2.0130307462763435
epoch: 24, step: 32
	action: tensor([[-0.0410, -0.1965, -0.2701, -0.0290,  0.1764,  0.0501, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08706060153741868, distance: 1.093396471722084 entropy -3.4117532429367428
epoch: 24, step: 33
	action: tensor([[-0.3008, -0.2228, -0.0215,  0.6573, -0.2284,  0.3280,  0.2683]],
       dtype=torch.float64)
	q_value: tensor([[-21.3965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17764043990375467, distance: 1.0377376319668714 entropy -2.920631261605859
epoch: 24, step: 34
	action: tensor([[-1.0536, -0.3622,  0.5640, -1.0594, -0.3223,  0.9944,  0.6886]],
       dtype=torch.float64)
	q_value: tensor([[-24.6165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.372326999678347, distance: 1.7625602401723244 entropy -2.3651300864902884
epoch: 24, step: 35
	action: tensor([[ 1.1076, -3.5097, -0.9582,  0.4117, -0.7707,  0.8128,  1.4420]],
       dtype=torch.float64)
	q_value: tensor([[-39.6186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7625602401723244 entropy -1.7825335830452584
epoch: 24, step: 36
	action: tensor([[ 0.0573, -0.0564,  0.0429,  0.2508,  0.3179,  0.1075, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.468323943629823, distance: 0.8344114300755272 entropy -3.4117532429367428
epoch: 24, step: 37
	action: tensor([[-0.8334, -0.1176, -0.3124,  0.4005,  0.1335,  0.7191,  0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-22.7185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.469928734988718, distance: 1.3874100407298409 entropy -2.6601995319758345
epoch: 24, step: 38
	action: tensor([[-1.1121,  0.2090, -0.0216, -0.5464,  1.0354,  0.1935,  0.4887]],
       dtype=torch.float64)
	q_value: tensor([[-30.4668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0733038316543113, distance: 1.647738041507485 entropy -2.5961115677832067
epoch: 24, step: 39
	action: tensor([[-0.1922, -0.0450,  0.1286,  0.2937,  0.3822,  0.2877,  0.1938]],
       dtype=torch.float64)
	q_value: tensor([[-33.3868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3230130957616999, distance: 0.9415574566665981 entropy -2.9393845609335534
epoch: 24, step: 40
	action: tensor([[-0.4651,  0.1471,  0.0841,  0.4990,  0.0483,  0.0891,  0.4895]],
       dtype=torch.float64)
	q_value: tensor([[-24.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9415574566665981 entropy -2.54148839429348
epoch: 24, step: 41
	action: tensor([[ 0.0591,  0.1612,  0.1278,  0.0305,  0.2969,  0.1716, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5298817300823143, distance: 0.7846215966631339 entropy -3.4117532429367428
epoch: 24, step: 42
	action: tensor([[-0.1841, -0.0537, -0.1092, -0.1605, -0.1243, -0.2134, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07871728055519545, distance: 1.188530993197273 entropy -3.4117532429367428
epoch: 24, step: 43
	action: tensor([[ 2.7019e-01,  1.0928e-01,  3.7896e-06, -3.7593e-01, -2.3979e-01,
          3.2128e-01,  2.3481e-01]], dtype=torch.float64)
	q_value: tensor([[-17.0472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47883163505328175, distance: 0.826124907587448 entropy -3.0316807135223383
epoch: 24, step: 44
	action: tensor([[ 1.0399, -0.2039, -1.1547, -0.8208,  0.3968,  0.5551,  0.7335]],
       dtype=torch.float64)
	q_value: tensor([[-23.4327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4089119591494198, distance: 0.8797976134540471 entropy -2.28319122183611
epoch: 24, step: 45
	action: tensor([[-1.1650, -0.0727, -0.9543, -0.2055,  0.7177,  0.5192,  0.6839]],
       dtype=torch.float64)
	q_value: tensor([[-40.1012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0904262104521711, distance: 1.6545279736209162 entropy -2.181052699429903
epoch: 24, step: 46
	action: tensor([[-0.2732, -0.2132,  0.2013,  0.2386, -0.3034,  0.0497,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-38.5395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1443225915734394, distance: 1.2241395492819513 entropy -3.4178563174765344
epoch: 24, step: 47
	action: tensor([[ 0.4987, -0.9805,  0.5005, -0.4770, -1.5472,  0.4788,  0.6136]],
       dtype=torch.float64)
	q_value: tensor([[-19.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6716613252211676, distance: 1.4795540691527584 entropy -2.453933639117614
epoch: 24, step: 48
	action: tensor([[-1.7001,  0.1432,  0.0655, -2.5758,  1.3818,  1.7222,  1.9049]],
       dtype=torch.float64)
	q_value: tensor([[-40.0836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4795540691527584 entropy -1.5331582870647937
epoch: 24, step: 49
	action: tensor([[ 0.0328, -0.2172,  0.0204, -0.1473,  0.0162,  0.0793, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0968752538685147, distance: 1.0875032523335855 entropy -3.4117532429367428
epoch: 24, step: 50
	action: tensor([[ 0.4117, -0.7741, -0.0917,  0.2321, -0.0877,  0.6853,  0.4616]],
       dtype=torch.float64)
	q_value: tensor([[-21.4045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2810135872967161, distance: 0.9703245843208976 entropy -2.5905794000838585
epoch: 24, step: 51
	action: tensor([[ 0.7451,  1.0632,  0.8382,  0.4399, -0.1232,  0.6132,  1.0717]],
       dtype=torch.float64)
	q_value: tensor([[-35.9677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9703245843208976 entropy -1.9654616961575848
epoch: 24, step: 52
	action: tensor([[ 0.2032,  0.0751, -0.2690, -0.2986, -0.0191, -0.0113, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4646673060716523, distance: 0.8372758735610778 entropy -3.4117532429367428
epoch: 24, step: 53
	action: tensor([[-0.7105, -0.4449, -0.3408, -0.6996,  0.3907,  0.2280,  0.2920]],
       dtype=torch.float64)
	q_value: tensor([[-19.5374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7776780470336448, distance: 1.525749540702434 entropy -2.841903804172721
epoch: 24, step: 54
	action: tensor([[ 0.6225,  0.6665,  0.1932, -0.3562, -0.5283,  0.1858,  0.3109]],
       dtype=torch.float64)
	q_value: tensor([[-31.6614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.939911237911768, distance: 0.28051321245946914 entropy -2.8570954232894588
epoch: 24, step: 55
	action: tensor([[ 0.6866, -1.3395, -0.4557, -0.2749,  0.2929,  0.1915,  0.9083]],
       dtype=torch.float64)
	q_value: tensor([[-22.9234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.28051321245946914 entropy -2.0840612452423346
epoch: 24, step: 56
	action: tensor([[ 0.1479, -0.1296, -0.0745,  0.0005, -0.1717,  0.0622, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33251998243357384, distance: 0.9349229648014585 entropy -3.4117532429367428
epoch: 24, step: 57
	action: tensor([[ 0.2805, -0.8164, -0.3769,  0.2655, -0.3408,  0.9584,  0.4687]],
       dtype=torch.float64)
	q_value: tensor([[-19.9508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11396793056225807, distance: 1.077162966957332 entropy -2.5890392670014983
epoch: 24, step: 58
	action: tensor([[-2.0231, -0.0662, -0.9999,  0.2662,  2.6266,  0.8814,  1.1799]],
       dtype=torch.float64)
	q_value: tensor([[-38.1945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.077162966957332 entropy -1.9125615691940474
epoch: 24, step: 59
	action: tensor([[-0.0892, -0.1614,  0.2066, -0.0736, -0.4841,  0.0061, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009234033128204477, distance: 1.1390485440810285 entropy -3.4117532429367428
epoch: 24, step: 60
	action: tensor([[-0.5356, -0.8283,  0.8202, -0.1215,  0.5345,  0.5271,  0.6129]],
       dtype=torch.float64)
	q_value: tensor([[-19.0752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7754232946094761, distance: 1.5247816266780712 entropy -2.4330268454396977
epoch: 24, step: 61
	action: tensor([[ 1.3757, -0.8502,  1.6748,  0.2473, -1.3518,  0.1186,  1.1329]],
       dtype=torch.float64)
	q_value: tensor([[-35.2178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3471604029640334, distance: 1.3282087832582123 entropy -1.9030359435257242
epoch: 24, step: 62
	action: tensor([[-1.8427,  0.9203,  0.0505,  0.4915,  2.5153,  2.0996,  2.3111]],
       dtype=torch.float64)
	q_value: tensor([[-48.2069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3282087832582123 entropy -1.3135160103642254
epoch: 24, step: 63
	action: tensor([[-0.0299, -0.0639, -0.2530, -0.0715, -0.0698,  0.2901, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-37.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22719197713982908, distance: 1.0059873144663083 entropy -3.4117532429367428
LOSS epoch 24 actor 404.65136088405706 critic 403.096979187363
epoch: 25, step: 0
	action: tensor([[-0.0386, -0.1322,  0.0610, -0.6642, -0.9409,  0.4827,  0.2062]],
       dtype=torch.float64)
	q_value: tensor([[-22.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20935793923520163, distance: 1.2584446630176682 entropy -2.809803232173411
epoch: 25, step: 1
	action: tensor([[ 0.5372, -0.1431,  0.3170, -1.6535, -0.1462,  0.3095,  0.6794]],
       dtype=torch.float64)
	q_value: tensor([[-27.7027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23134794197491293, distance: 1.2698343991262147 entropy -2.107931905457724
epoch: 25, step: 2
	action: tensor([[ 0.0161, -0.8851, -0.6721, -1.0150,  1.4076,  1.5364,  0.5987]],
       dtype=torch.float64)
	q_value: tensor([[-38.3909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2794819926393496, distance: 0.9713575338411166 entropy -1.9421493282238391
epoch: 25, step: 3
	action: tensor([[-0.1037, -0.3836, -0.7918,  0.2876,  0.3220,  0.4403,  0.1949]],
       dtype=torch.float64)
	q_value: tensor([[-55.4370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07227736368213722, distance: 1.184977931253445 entropy -2.182199586221788
epoch: 25, step: 4
	action: tensor([[-0.2412,  0.0095,  0.4073, -0.2285,  0.1782, -0.0343,  0.0749]],
       dtype=torch.float64)
	q_value: tensor([[-29.5134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15292725255723627, distance: 1.2287333495195776 entropy -3.0076449081977406
epoch: 25, step: 5
	action: tensor([[-1.3571, -0.0238, -0.1306,  0.4863,  0.4410,  0.2037,  0.2654]],
       dtype=torch.float64)
	q_value: tensor([[-21.5528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.06751146033853, distance: 1.6454347162675915 entropy -2.61703255194047
epoch: 25, step: 6
	action: tensor([[ 0.0796, -0.0937, -0.0154,  0.5561,  0.1398,  0.0895,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-28.4159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6454347162675915 entropy -3.15338587854703
epoch: 25, step: 7
	action: tensor([[ 0.1925, -0.1457, -0.2843,  0.0399, -0.0367, -0.0868, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36282079437971815, distance: 0.9134557003867175 entropy -3.4865329430798058
epoch: 25, step: 8
	action: tensor([[ 0.7438, -0.1879, -0.2246, -0.2771,  0.1641,  0.2923,  0.1413]],
       dtype=torch.float64)
	q_value: tensor([[-20.6804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5223189069368191, distance: 0.7909075462025339 entropy -2.9149773951708453
epoch: 25, step: 9
	action: tensor([[-2.1595, -0.0161,  0.0874,  0.1364,  0.5706,  0.8441,  0.2990]],
       dtype=torch.float64)
	q_value: tensor([[-29.3492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7909075462025339 entropy -2.388008857627405
epoch: 25, step: 10
	action: tensor([[-0.0797, -0.0062,  0.0622, -0.1269, -0.0053,  0.0201, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12585862305102324, distance: 1.0699107032405797 entropy -3.4865329430798058
epoch: 25, step: 11
	action: tensor([[ 4.8528e-01, -1.6752e-01,  8.0930e-02, -4.4438e-01,  6.8647e-02,
          3.0128e-04,  1.9869e-01]], dtype=torch.float64)
	q_value: tensor([[-20.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24960334209023716, distance: 0.9912932269057249 entropy -2.8002037262216843
epoch: 25, step: 12
	action: tensor([[-0.2547, -0.4587,  0.3681,  0.5617, -0.1929,  0.3954,  0.2984]],
       dtype=torch.float64)
	q_value: tensor([[-25.3209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20972662289373034, distance: 1.0172913906216416 entropy -2.444740272186172
epoch: 25, step: 13
	action: tensor([[-0.1267, -0.4151,  0.0114,  0.4917, -1.2488,  0.5284,  0.5657]],
       dtype=torch.float64)
	q_value: tensor([[-29.1519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00697023541682229, distance: 1.1403491056209065 entropy -2.1990841026558776
epoch: 25, step: 14
	action: tensor([[ 0.4072,  0.1878,  1.5684, -0.9248,  2.0166,  0.8357,  0.8550]],
       dtype=torch.float64)
	q_value: tensor([[-33.3389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19511238161097022, distance: 1.0266545036172119 entropy -1.9391527371244752
epoch: 25, step: 15
	action: tensor([[ 2.1213, -1.4133, -0.8690, -1.4081,  0.0758, -0.1367,  0.5597]],
       dtype=torch.float64)
	q_value: tensor([[-54.7813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0266545036172119 entropy -1.6737677555928339
epoch: 25, step: 16
	action: tensor([[-0.1468,  0.0633,  0.2333,  0.1215, -0.2669,  0.1858, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2562550838064691, distance: 0.9868898863646328 entropy -3.4865329430798058
epoch: 25, step: 17
	action: tensor([[-0.1457, -0.3903, -0.5622, -0.7914,  0.4808,  0.4940,  0.3417]],
       dtype=torch.float64)
	q_value: tensor([[-20.7216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1176290768353323, distance: 1.2097776010562586 entropy -2.552202781822657
epoch: 25, step: 18
	action: tensor([[-0.1105, -0.0065,  0.3338, -0.5942, -0.4176,  0.1657,  0.1168]],
       dtype=torch.float64)
	q_value: tensor([[-33.6423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20245817696689938, distance: 1.2548496193442815 entropy -2.7958562151791186
epoch: 25, step: 19
	action: tensor([[ 1.0396, -0.6525,  0.8767,  0.1336, -0.4374,  0.3233,  0.4649]],
       dtype=torch.float64)
	q_value: tensor([[-23.5653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2383419955636773, distance: 0.9987037912647712 entropy -2.3489608299139033
epoch: 25, step: 20
	action: tensor([[ 2.0173, -0.0104, -0.5410, -0.9670, -1.7267,  1.1284,  0.7769]],
       dtype=torch.float64)
	q_value: tensor([[-39.7700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9987037912647712 entropy -1.7765537166083702
epoch: 25, step: 21
	action: tensor([[ 0.0614, -0.0567,  0.1537, -0.0633,  0.0007,  0.0977, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3379677126440408, distance: 0.931099896842942 entropy -3.4865329430798058
epoch: 25, step: 22
	action: tensor([[ 0.2119,  0.1700, -0.0865,  0.2459,  0.5701, -0.1021,  0.2664]],
       dtype=torch.float64)
	q_value: tensor([[-22.2262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.647501924521664, distance: 0.6794149021800115 entropy -2.6164595850650314
epoch: 25, step: 23
	action: tensor([[ 0.2420, -0.1232,  0.0888, -0.0661, -0.6882,  0.2167,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-23.2464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4011239437510322, distance: 0.8855746337478854 entropy -2.883706199415958
epoch: 25, step: 24
	action: tensor([[ 0.4463, -0.5704, -0.1614,  0.6169, -1.1870,  0.5327,  0.5255]],
       dtype=torch.float64)
	q_value: tensor([[-23.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8855746337478854 entropy -2.254807816497821
epoch: 25, step: 25
	action: tensor([[-0.1160, -0.0063,  0.0573, -0.0779, -0.0943,  0.1582, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13471578994240818, distance: 1.0644765109268022 entropy -3.4865329430798058
epoch: 25, step: 26
	action: tensor([[ 0.4760, -0.1498, -0.5927,  0.4976,  0.2815,  0.2093,  0.2520]],
       dtype=torch.float64)
	q_value: tensor([[-20.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6798994661733514, distance: 0.6474405442725639 entropy -2.7101555309391685
epoch: 25, step: 27
	action: tensor([[ 0.2697,  0.0319,  0.0127, -0.0897, -0.0512, -0.0230,  0.1189]],
       dtype=torch.float64)
	q_value: tensor([[-28.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49298177039824953, distance: 0.8148327585031717 entropy -2.74759907585797
epoch: 25, step: 28
	action: tensor([[ 0.3980,  0.1582,  0.1419, -0.2439,  0.7036,  0.5443,  0.2525]],
       dtype=torch.float64)
	q_value: tensor([[-21.1365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7502421946999037, distance: 0.5718949057290241 entropy -2.632727637273636
epoch: 25, step: 29
	action: tensor([[ 0.1665,  0.5628,  0.5698,  0.7641,  0.8790, -0.0023,  0.2589]],
       dtype=torch.float64)
	q_value: tensor([[-30.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5718949057290241 entropy -2.392540865786201
epoch: 25, step: 30
	action: tensor([[ 0.2754, -0.0650, -0.0455,  0.0352, -0.4439,  0.1764, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5465361555318773, distance: 0.7705982636371671 entropy -3.4865329430798058
epoch: 25, step: 31
	action: tensor([[ 0.1426, -0.7117,  0.0967, -0.5637, -0.2487,  0.2907,  0.3734]],
       dtype=torch.float64)
	q_value: tensor([[-22.3511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5042188008392279, distance: 1.4034992972855755 entropy -2.451350826472948
epoch: 25, step: 32
	action: tensor([[ 0.3857,  0.6908, -1.4462,  1.1912, -1.9616,  0.8402,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-31.2323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8349442964864623, distance: 0.4649132901945345 entropy -2.164455410638172
epoch: 25, step: 33
	action: tensor([[ 0.3373, -0.4056,  0.6106, -0.4693,  0.7876,  1.1155,  0.5974]],
       dtype=torch.float64)
	q_value: tensor([[-41.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4649132901945345 entropy -1.991862325368073
epoch: 25, step: 34
	action: tensor([[-0.0999,  0.0082,  0.4362,  0.1011, -0.1186,  0.0920, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26841501419237224, distance: 0.9787890119462379 entropy -3.4865329430798058
epoch: 25, step: 35
	action: tensor([[-0.1790,  0.1846,  0.1950, -0.9762, -0.2155,  0.3270,  0.3436]],
       dtype=torch.float64)
	q_value: tensor([[-21.6077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20869737284275547, distance: 1.2581009269863774 entropy -2.4983728518647252
epoch: 25, step: 36
	action: tensor([[-0.0143, -0.0601,  0.4905, -0.3652, -0.7512,  0.1623,  0.4256]],
       dtype=torch.float64)
	q_value: tensor([[-27.4261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02104810122035805, distance: 1.1563246778302538 entropy -2.368780351121415
epoch: 25, step: 37
	action: tensor([[-0.7533, -0.8041,  0.4754,  0.2969, -1.1955,  0.7893,  0.6876]],
       dtype=torch.float64)
	q_value: tensor([[-25.3520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.917039322185194, distance: 1.584426888062016 entropy -2.090457863898419
epoch: 25, step: 38
	action: tensor([[-1.0741, -0.0226, -1.1650, -2.8641, -2.0673,  1.3206,  1.1317]],
       dtype=torch.float64)
	q_value: tensor([[-38.7250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.584426888062016 entropy -1.745391373282951
epoch: 25, step: 39
	action: tensor([[ 0.2149, -0.1861,  0.0339,  0.1822, -0.2651,  0.0413, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4418892064847042, distance: 0.8549031239443903 entropy -3.4865329430798058
epoch: 25, step: 40
	action: tensor([[-0.0868, -0.6923, -0.3549,  0.7453, -0.1488,  0.9217,  0.3213]],
       dtype=torch.float64)
	q_value: tensor([[-22.2721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15478399408706633, distance: 1.0520600990118798 entropy -2.531117830265516
epoch: 25, step: 41
	action: tensor([[-1.4287, -0.1383, -0.0927, -0.6439, -1.7271,  0.0611,  0.5131]],
       dtype=torch.float64)
	q_value: tensor([[-37.6266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5830075329572404, distance: 1.8391600710084026 entropy -2.198233433984625
epoch: 25, step: 42
	action: tensor([[-0.2725,  0.4097, -0.3174, -0.6872,  0.1063,  0.1765,  0.6270]],
       dtype=torch.float64)
	q_value: tensor([[-32.4429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20310607551120363, distance: 1.0215437032314145 entropy -2.2512091898383844
epoch: 25, step: 43
	action: tensor([[ 0.6900, -0.0199, -0.2929, -0.1178, -0.2415,  0.2685,  0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-25.5283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7232354775723231, distance: 0.602021362803638 entropy -2.9011412530095706
epoch: 25, step: 44
	action: tensor([[-0.3449,  0.3446,  0.6203,  0.0308, -0.0648,  0.3591,  0.3480]],
       dtype=torch.float64)
	q_value: tensor([[-26.3934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.602021362803638 entropy -2.392342443878687
epoch: 25, step: 45
	action: tensor([[-0.0841,  0.0015,  0.0083,  0.2566, -0.0210,  0.0245, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117150606279996, distance: 0.9493816354791865 entropy -3.4865329430798058
epoch: 25, step: 46
	action: tensor([[-0.7069, -0.0387,  0.1243,  0.0566, -0.0315,  0.4450,  0.2057]],
       dtype=torch.float64)
	q_value: tensor([[-20.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46021357773001426, distance: 1.3828175554794868 entropy -2.7951868660364885
epoch: 25, step: 47
	action: tensor([[ 0.1469,  0.0062, -0.6492, -0.2951,  0.7671,  0.1477,  0.3409]],
       dtype=torch.float64)
	q_value: tensor([[-24.4499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4386026571970264, distance: 0.8574165648906213 entropy -2.5849617856694165
epoch: 25, step: 48
	action: tensor([[-0.2797, -0.1877,  0.0580, -0.3318, -0.0571,  0.0016, -0.0449]],
       dtype=torch.float64)
	q_value: tensor([[-27.4866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.358981561652826, distance: 1.3340234856387772 entropy -3.1981959679025684
epoch: 25, step: 49
	action: tensor([[-0.4570, -0.2812, -0.0654, -0.1528, -0.2713,  0.1563,  0.2200]],
       dtype=torch.float64)
	q_value: tensor([[-21.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5345625346762557, distance: 1.417584607091126 entropy -2.800728532049473
epoch: 25, step: 50
	action: tensor([[-0.0945, -0.3162, -0.3191, -0.7775,  0.2186,  0.4590,  0.3313]],
       dtype=torch.float64)
	q_value: tensor([[-22.8844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1729858553232413, distance: 1.2393759967701592 entropy -2.6529263414275484
epoch: 25, step: 51
	action: tensor([[ 0.6321, -0.1592,  0.5672,  0.0691,  0.2531,  0.0553,  0.2525]],
       dtype=torch.float64)
	q_value: tensor([[-30.8206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6422485160169547, distance: 0.6844589634327592 entropy -2.597086463061138
epoch: 25, step: 52
	action: tensor([[ 0.2572, -0.9188,  0.2112, -0.0916, -0.8309,  0.6583,  0.4226]],
       dtype=torch.float64)
	q_value: tensor([[-29.4302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3127452940592583, distance: 1.3111335434306808 entropy -2.187611145975557
epoch: 25, step: 53
	action: tensor([[ 1.6809, -1.8425, -0.6565,  0.0181, -1.3828,  1.7219,  0.8788]],
       dtype=torch.float64)
	q_value: tensor([[-36.8658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3111335434306808 entropy -1.8465494971848002
epoch: 25, step: 54
	action: tensor([[-0.0220,  0.0277,  0.2631, -0.0372, -0.0216, -0.1590, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22065763579368203, distance: 1.0102313359780886 entropy -3.4865329430798058
epoch: 25, step: 55
	action: tensor([[-0.1119, -0.1943,  0.5128,  0.1222, -0.0139,  0.2898,  0.2116]],
       dtype=torch.float64)
	q_value: tensor([[-19.3809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2251985714539575, distance: 1.0072839167778624 entropy -2.7370026958736124
epoch: 25, step: 56
	action: tensor([[-1.3250,  0.1849,  0.0425, -0.3928, -0.2393,  0.2395,  0.4904]],
       dtype=torch.float64)
	q_value: tensor([[-25.4505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3860010275814645, distance: 1.7676326159102844 entropy -2.272447233947781
epoch: 25, step: 57
	action: tensor([[-1.0495, -0.1565,  0.3234, -0.1319, -0.9740,  0.2297,  0.2566]],
       dtype=torch.float64)
	q_value: tensor([[-26.9179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.250020194182127, distance: 1.7165240840148097 entropy -2.7643045559610586
epoch: 25, step: 58
	action: tensor([[-0.3652, -0.0370,  0.0309, -1.3204, -0.0687,  1.0611,  0.6612]],
       dtype=torch.float64)
	q_value: tensor([[-25.5857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34637230431833554, distance: 1.3278202205168077 entropy -2.213351977677472
epoch: 25, step: 59
	action: tensor([[-2.4083,  0.4194,  0.7013,  0.2122, -0.0966,  0.3292,  0.6138]],
       dtype=torch.float64)
	q_value: tensor([[-38.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3278202205168077 entropy -2.0635707604964773
epoch: 25, step: 60
	action: tensor([[-0.1404, -0.0344,  0.1411,  0.0245,  0.2191,  0.0455, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-34.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11212456482537414, distance: 1.0782828889658969 entropy -3.4865329430798058
epoch: 25, step: 61
	action: tensor([[ 0.2466, -0.4725, -0.0832, -0.0352, -0.0329,  0.1017,  0.1666]],
       dtype=torch.float64)
	q_value: tensor([[-21.7471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07774194236796594, distance: 1.0989626245131017 entropy -2.8303935921781784
epoch: 25, step: 62
	action: tensor([[-1.5616, -0.3488, -0.0678,  0.2478, -0.8950, -0.3153,  0.3318]],
       dtype=torch.float64)
	q_value: tensor([[-26.3013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7969480237689641, distance: 1.9138103158962214 entropy -2.483443403804668
epoch: 25, step: 63
	action: tensor([[ 0.2390,  0.1540,  0.4790, -0.2602,  0.8310,  0.5024,  0.3111]],
       dtype=torch.float64)
	q_value: tensor([[-26.6479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.596506849607316, distance: 0.7269001803152476 entropy -2.6937060182683106
LOSS epoch 25 actor 373.7859093176347 critic 337.00132744152387
epoch: 26, step: 0
	action: tensor([[-0.9207, -0.1842,  1.0641, -0.1268,  0.2586,  0.7218, -0.2406]],
       dtype=torch.float64)
	q_value: tensor([[-33.4551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7951393455198179, distance: 1.533224590986406 entropy -2.3974767793556695
epoch: 26, step: 1
	action: tensor([[-1.6155, -0.6602, -0.9632,  0.4071,  0.0308,  0.7611, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-35.2362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.738961876470957, distance: 1.893867916966997 entropy -2.1431022152315053
epoch: 26, step: 2
	action: tensor([[-0.0434, -0.0539, -0.0811,  0.1832, -0.1659,  0.1335, -0.1493]],
       dtype=torch.float64)
	q_value: tensor([[-41.2405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289425429168616, distance: 0.9646316872479367 entropy -3.238526059246108
epoch: 26, step: 3
	action: tensor([[-0.0013,  0.1469, -0.3417,  0.0241,  0.2286,  0.1540, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[-22.7219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.413460962830168, distance: 0.8764056208070069 entropy -2.8342008750054064
epoch: 26, step: 4
	action: tensor([[-0.3234, -0.3158,  0.1620, -0.1677, -0.0112,  0.0539, -0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-22.1485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41856117063456044, distance: 1.3629525210012965 entropy -3.2219537408852736
epoch: 26, step: 5
	action: tensor([[ 0.5106, -0.2720,  0.3368,  0.6237,  0.3732,  0.2341, -0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-24.1505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8543635175579329, distance: 0.43670863946027116 entropy -2.793062068992861
epoch: 26, step: 6
	action: tensor([[-0.2679, -0.2510,  0.2661,  0.2669,  0.8120,  0.0014, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-35.7991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027835639205296525, distance: 1.1601617141164338 entropy -2.3850641024133004
epoch: 26, step: 7
	action: tensor([[ 0.4228, -0.3954, -0.3014, -0.6573, -0.1683, -0.1871, -0.1969]],
       dtype=torch.float64)
	q_value: tensor([[-30.3071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09244004376407333, distance: 1.196066974055141 entropy -2.8788737873881547
epoch: 26, step: 8
	action: tensor([[-0.2182, -0.2355, -0.3706, -0.7447,  0.2915,  0.0175, -0.1384]],
       dtype=torch.float64)
	q_value: tensor([[-25.6276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2521075686432652, distance: 1.2804938991722248 entropy -2.801513211541869
epoch: 26, step: 9
	action: tensor([[-0.2427, -0.1662,  0.1035, -0.1290, -0.4279, -0.1098, -0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-25.8403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20245640525509345, distance: 1.2548486948910462 entropy -3.340293034520446
epoch: 26, step: 10
	action: tensor([[ 0.0677, -0.1671, -0.3207, -0.4485,  0.4183,  0.3671,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-21.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18190039170964156, distance: 1.035046320047689 entropy -2.8080013262853605
epoch: 26, step: 11
	action: tensor([[ 0.1987, -0.1272, -0.5650, -0.6974, -0.0731,  0.3636, -0.1742]],
       dtype=torch.float64)
	q_value: tensor([[-28.3875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2573899999246172, distance: 0.9861366275386183 entropy -2.958660131434104
epoch: 26, step: 12
	action: tensor([[ 0.5270,  0.0577,  0.2060,  0.4413, -0.0773, -0.3200, -0.1357]],
       dtype=torch.float64)
	q_value: tensor([[-27.5700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.845573942839894, distance: 0.4496938879007805 entropy -2.9309561388241923
epoch: 26, step: 13
	action: tensor([[ 0.0198,  0.0694,  0.2208,  0.1095, -0.7218,  0.0289, -0.1032]],
       dtype=torch.float64)
	q_value: tensor([[-24.3972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41396836485109756, distance: 0.8760264591745607 entropy -2.5947914975934516
epoch: 26, step: 14
	action: tensor([[ 0.2850, -1.2899, -0.0150, -0.6192,  0.3024,  0.4711,  0.0784]],
       dtype=torch.float64)
	q_value: tensor([[-22.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8760264591745607 entropy -2.4699448021528374
epoch: 26, step: 15
	action: tensor([[ 0.0879,  0.2072, -0.1930, -0.2197,  0.0185,  0.0113, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4402774039986037, distance: 0.8561366976135594 entropy -3.4703144889402644
epoch: 26, step: 16
	action: tensor([[-0.0527,  0.0396, -0.2404, -0.1597,  0.1088,  0.3390, -0.1059]],
       dtype=torch.float64)
	q_value: tensor([[-20.2655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2842863315401688, distance: 0.9681136620005673 entropy -3.135635412413071
epoch: 26, step: 17
	action: tensor([[-0.2031, -0.0783, -0.2345,  0.1609,  0.4078,  0.1066, -0.0916]],
       dtype=torch.float64)
	q_value: tensor([[-23.9510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08917006856445864, distance: 1.092132522708144 entropy -3.0034958222347163
epoch: 26, step: 18
	action: tensor([[ 0.3357,  0.0236, -0.4810, -0.0529,  0.0757,  0.0893, -0.1293]],
       dtype=torch.float64)
	q_value: tensor([[-24.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5950027183180709, distance: 0.7282537798194444 entropy -3.2574685397928898
epoch: 26, step: 19
	action: tensor([[-0.0227, -0.0721, -0.2266, -0.7819,  0.3677,  0.2582, -0.1262]],
       dtype=torch.float64)
	q_value: tensor([[-23.4276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026279574564716524, distance: 1.1292077063415458 entropy -3.0706627605982715
epoch: 26, step: 20
	action: tensor([[ 0.2028, -0.0695, -0.0184,  0.4708,  0.4289,  0.4822, -0.1959]],
       dtype=torch.float64)
	q_value: tensor([[-27.5982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1292077063415458 entropy -3.0093359643780198
epoch: 26, step: 21
	action: tensor([[ 0.1208, -0.0548, -0.2417,  0.0661, -0.1767,  0.0206, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4198328861699714, distance: 0.871632162725441 entropy -3.4703144889402644
epoch: 26, step: 22
	action: tensor([[-0.0822, -0.0189,  1.0503, -0.5431, -0.2308,  0.1390, -0.0422]],
       dtype=torch.float64)
	q_value: tensor([[-21.9740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15455938159930294, distance: 1.229602763497467 entropy -2.915021167318618
epoch: 26, step: 23
	action: tensor([[-1.2942, -0.4178, -0.5637, -0.9060, -0.3900,  0.2677,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-29.1797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1896270515681824, distance: 1.693330645533972 entropy -2.1457919593750936
epoch: 26, step: 24
	action: tensor([[ 0.2400, -0.1717,  0.4239, -0.2769,  0.0361, -0.1254, -0.1234]],
       dtype=torch.float64)
	q_value: tensor([[-32.7885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16614227661773462, distance: 1.044967229607567 entropy -3.3511011881534722
epoch: 26, step: 25
	action: tensor([[-0.8803, -0.2413, -0.1383, -0.1681, -0.0271,  0.2281, -0.0858]],
       dtype=torch.float64)
	q_value: tensor([[-25.7969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9500011412809737, distance: 1.597990253185875 entropy -2.544870069037536
epoch: 26, step: 26
	action: tensor([[ 0.1602, -0.0587, -0.1626,  0.0187,  0.2129,  0.1952, -0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-25.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.489246754496539, distance: 0.8178285373112855 entropy -3.203042535220796
epoch: 26, step: 27
	action: tensor([[ 0.1331,  0.0308,  0.2826, -0.5404, -0.1620,  0.1594, -0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-25.2558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19676032387755638, distance: 1.0256029690991937 entropy -2.8995118376810782
epoch: 26, step: 28
	action: tensor([[-1.0049, -0.0258,  0.0926,  0.6907, -0.0405,  0.2175, -0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-25.1230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3791958801158457, distance: 1.3439084011907656 entropy -2.542571525549657
epoch: 26, step: 29
	action: tensor([[ 0.0092, -0.3077, -0.2225,  0.1446,  0.0896,  0.0648, -0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-25.9903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.164140052491214, distance: 1.0462210431087635 entropy -2.852581798504155
epoch: 26, step: 30
	action: tensor([[-0.0808,  0.1481, -0.0919,  0.2615,  0.4430,  0.3351, -0.0639]],
       dtype=torch.float64)
	q_value: tensor([[-24.9685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0462210431087635 entropy -2.934598376938491
epoch: 26, step: 31
	action: tensor([[-0.1049, -0.1349, -0.0200, -0.0883, -0.3177,  0.0574, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01821464781998361, distance: 1.1338744451437652 entropy -3.4703144889402644
epoch: 26, step: 32
	action: tensor([[-0.5453, -0.4621,  0.2200, -0.2278,  0.1032,  0.1575,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[-22.2089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8056555491366213, distance: 1.5377089662967054 entropy -2.77642399362916
epoch: 26, step: 33
	action: tensor([[-0.4867, -0.8043,  0.9735,  1.3217, -0.0477, -0.1796, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-26.7993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3420072478346369, distance: 0.928254895242887 entropy -2.7120540955145738
epoch: 26, step: 34
	action: tensor([[-1.3328, -0.9320,  0.5856,  1.1153, -0.0796,  0.6980, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[-37.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5078511674527104, distance: 1.40519285078912 entropy -2.1190152773139324
epoch: 26, step: 35
	action: tensor([[ 0.4176,  1.0716,  0.0050, -0.1974,  0.4667,  0.1028,  0.0687]],
       dtype=torch.float64)
	q_value: tensor([[-42.7464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.40519285078912 entropy -2.1482692205322658
epoch: 26, step: 36
	action: tensor([[-0.1893, -0.2506,  0.2223,  0.3054,  0.3334, -0.1188, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003196859594723378, distance: 1.1461719483990067 entropy -3.4703144889402644
epoch: 26, step: 37
	action: tensor([[ 0.2090,  0.2903, -0.4520, -0.1229,  0.3496,  0.2552, -0.1025]],
       dtype=torch.float64)
	q_value: tensor([[-25.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1461719483990067 entropy -2.894543424671572
epoch: 26, step: 38
	action: tensor([[-0.2977, -0.1123,  0.0177, -0.1337,  0.2769, -0.0842, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2672826937777053, distance: 1.288230108683536 entropy -3.4703144889402644
epoch: 26, step: 39
	action: tensor([[ 0.0821, -0.4694, -0.2195,  0.1344, -0.3268, -0.0154, -0.1162]],
       dtype=torch.float64)
	q_value: tensor([[-22.1683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0015791357764955993, distance: 1.1452474350553337 entropy -3.168810973943645
epoch: 26, step: 40
	action: tensor([[-0.3567, -0.0611,  0.3621, -0.1563,  0.8051,  0.2930,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[-25.0746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1889842341056196, distance: 1.2477993102633196 entropy -2.717680029708363
epoch: 26, step: 41
	action: tensor([[-0.4513, -0.4376, -0.0815, -0.3502,  0.1967,  0.1110, -0.2123]],
       dtype=torch.float64)
	q_value: tensor([[-30.4649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7023696601145952, distance: 1.4930818926226423 entropy -2.78059059141037
epoch: 26, step: 42
	action: tensor([[ 0.1334, -0.0074, -0.7891,  0.2166,  0.1276, -0.0091, -0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-26.5170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37459021553999916, distance: 0.9049800991076485 entropy -3.047731865637972
epoch: 26, step: 43
	action: tensor([[-0.4273,  0.0545,  0.2254, -0.0961,  0.3233,  0.0602, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-22.4260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25532070826072095, distance: 1.2821358385631443 entropy -3.4054445012790557
epoch: 26, step: 44
	action: tensor([[ 0.1747, -0.1320,  0.3246,  0.0359,  0.0382,  0.1278, -0.1209]],
       dtype=torch.float64)
	q_value: tensor([[-23.3427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3928293246672202, distance: 0.8916862860921666 entropy -3.028819590457791
epoch: 26, step: 45
	action: tensor([[-0.0798, -0.0525,  0.1725,  0.0819,  0.7259,  0.4667, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[-26.7512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.372876098217955, distance: 0.9062194310218278 entropy -2.545148329784003
epoch: 26, step: 46
	action: tensor([[ 0.3450, -0.0636, -0.1848,  0.5915, -0.8014,  0.1154, -0.1868]],
       dtype=torch.float64)
	q_value: tensor([[-30.9573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9062194310218278 entropy -2.7356914868284936
epoch: 26, step: 47
	action: tensor([[-0.0324, -0.0161, -0.1161, -0.1108, -0.0440,  0.0568, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2030303650958012, distance: 1.0215922289252617 entropy -3.4703144889402644
epoch: 26, step: 48
	action: tensor([[ 0.4415, -0.3093,  0.2923, -0.5686,  0.4544,  0.1128, -0.0579]],
       dtype=torch.float64)
	q_value: tensor([[-21.5937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013192740679469073, distance: 1.136770673500011 entropy -2.97915570145114
epoch: 26, step: 49
	action: tensor([[ 0.1106, -0.0733, -0.0726, -0.4160, -0.6912,  0.4698, -0.1938]],
       dtype=torch.float64)
	q_value: tensor([[-31.6437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16049023416082564, distance: 1.0485027395812963 entropy -2.455781402286777
epoch: 26, step: 50
	action: tensor([[-0.5443, -0.8446,  0.2811, -0.7620,  0.2362, -0.2534,  0.0788]],
       dtype=torch.float64)
	q_value: tensor([[-27.0831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1217436431623153, distance: 1.6668754416858866 entropy -2.385958103032082
epoch: 26, step: 51
	action: tensor([[-0.2523, -0.3758, -0.4234, -0.3872, -0.5396,  0.5048, -0.1541]],
       dtype=torch.float64)
	q_value: tensor([[-29.5721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4033066565938572, distance: 1.3556044503255087 entropy -2.673286449930032
epoch: 26, step: 52
	action: tensor([[ 0.4412, -0.4786,  0.1146, -0.3730,  0.1774, -0.4272,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[-28.5930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19060623836512747, distance: 1.2486501398827374 entropy -2.6573565065603337
epoch: 26, step: 53
	action: tensor([[-0.0888,  0.5270, -0.2838,  0.0996, -0.1941,  0.3140, -0.1723]],
       dtype=torch.float64)
	q_value: tensor([[-26.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2486501398827374 entropy -2.6720444037676256
epoch: 26, step: 54
	action: tensor([[-0.0821,  0.0650,  0.2626,  0.0620, -0.1550, -0.2197, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22332502908672347, distance: 1.0085010348365846 entropy -3.4703144889402644
epoch: 26, step: 55
	action: tensor([[-0.4739,  0.1865,  0.2033,  0.8550,  0.3387,  0.4368, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-19.6523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0085010348365846 entropy -2.83063401758216
epoch: 26, step: 56
	action: tensor([[-0.0571, -0.0871, -0.0346,  0.0482, -0.0925, -0.0822, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13836299241797168, distance: 1.0622307395161752 entropy -3.4703144889402644
epoch: 26, step: 57
	action: tensor([[ 0.4806,  0.1457, -0.1821,  0.1350, -0.0077,  0.2083, -0.0420]],
       dtype=torch.float64)
	q_value: tensor([[-21.0154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8130585786108747, distance: 0.4947769550186177 entropy -2.9554834142380066
epoch: 26, step: 58
	action: tensor([[ 0.7634, -0.5054,  0.1594, -0.5760,  0.4746, -0.1094, -0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-25.7351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1781879573539591, distance: 1.2421212250186637 entropy -2.6986324154691164
epoch: 26, step: 59
	action: tensor([[-0.4494,  0.5673,  0.6451,  1.5584, -0.5532,  0.0794, -0.2571]],
       dtype=torch.float64)
	q_value: tensor([[-33.8437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2421212250186637 entropy -2.420789790396353
epoch: 26, step: 60
	action: tensor([[-0.0185, -0.0437,  0.1196, -0.2894,  0.0467, -0.0800, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-33.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06014568831893086, distance: 1.1093969366489858 entropy -3.4703144889402644
epoch: 26, step: 61
	action: tensor([[ 0.2764, -0.1470, -0.2695,  0.4245, -0.0581,  0.0254, -0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-21.8023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.581606478514616, distance: 0.7402001501586092 entropy -2.9231160913218663
epoch: 26, step: 62
	action: tensor([[ 0.7032, -0.0334,  0.1233,  0.0446, -0.4942, -0.2286, -0.0636]],
       dtype=torch.float64)
	q_value: tensor([[-25.3857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6919743473468161, distance: 0.6351117338164192 entropy -2.8088755205369162
epoch: 26, step: 63
	action: tensor([[ 0.1689,  0.3608, -0.0180, -0.3821,  0.0427,  0.5889, -0.0693]],
       dtype=torch.float64)
	q_value: tensor([[-25.7002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6351117338164192 entropy -2.451853177285764
LOSS epoch 26 actor 376.10904987606034 critic 332.14035265204967
epoch: 27, step: 0
	action: tensor([[-0.2360,  0.0640, -0.0116,  0.2681,  0.0901, -0.0960, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-32.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13224727630952426, distance: 1.065993817804697 entropy -3.4767828071491027
epoch: 27, step: 1
	action: tensor([[ 0.1939, -0.0689, -0.2752,  0.2332,  0.2511,  0.2455, -0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-21.8645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.527560677930602, distance: 0.7865561155557448 entropy -3.1234348685663518
epoch: 27, step: 2
	action: tensor([[-0.3412,  0.4455, -0.1682, -0.2232,  0.1616,  0.0938, -0.2063]],
       dtype=torch.float64)
	q_value: tensor([[-28.8596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7865561155557448 entropy -2.950326670461256
epoch: 27, step: 3
	action: tensor([[-0.1503, -0.0645,  0.2362,  0.2521,  0.3377, -0.0161, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-32.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2058620972476295, distance: 1.0197756918228063 entropy -3.4767828071491027
epoch: 27, step: 4
	action: tensor([[ 0.2404,  0.0277, -0.1366,  0.0959, -0.3528, -0.1635, -0.2065]],
       dtype=torch.float64)
	q_value: tensor([[-26.6142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5371224886159773, distance: 0.7785557812549487 entropy -2.893180128653254
epoch: 27, step: 5
	action: tensor([[-0.0037,  0.1102,  0.1323, -0.3117,  0.1502,  0.1639, -0.1386]],
       dtype=torch.float64)
	q_value: tensor([[-22.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2307592448739032, distance: 1.0036628193619006 entropy -2.902199235020897
epoch: 27, step: 6
	action: tensor([[-0.3942, -0.1807, -0.1914,  0.2871,  0.5386, -0.3340, -0.2271]],
       dtype=torch.float64)
	q_value: tensor([[-25.3574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3444797438321543, distance: 1.3268866511753519 entropy -2.886937524279888
epoch: 27, step: 7
	action: tensor([[-0.1269, -0.1027,  0.1019, -0.0186,  0.0687, -0.0233, -0.1432]],
       dtype=torch.float64)
	q_value: tensor([[-24.9264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03127734981311281, distance: 1.1263060590123501 entropy -3.45281005962134
epoch: 27, step: 8
	action: tensor([[-0.1944, -0.0029,  0.0006,  0.0147,  0.4383,  0.2623, -0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-24.3470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14926134257651313, distance: 1.055491589435257 entropy -2.9687274175016403
epoch: 27, step: 9
	action: tensor([[-0.5475,  0.1746,  0.3772, -0.2242,  0.4513, -0.0277, -0.2162]],
       dtype=torch.float64)
	q_value: tensor([[-27.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4471503352720383, distance: 1.3766182350995315 entropy -3.051265857159283
epoch: 27, step: 10
	action: tensor([[-0.4347, -0.1751,  0.5764,  0.0221, -0.0555,  0.0006, -0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-25.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3715684225217899, distance: 1.340187096548222 entropy -3.0816560210875386
epoch: 27, step: 11
	action: tensor([[-0.1489, -0.1313,  0.6317,  0.0327, -0.1891,  0.3132, -0.1363]],
       dtype=torch.float64)
	q_value: tensor([[-26.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14883414113762417, distance: 1.0557565656002708 entropy -2.6313215715270752
epoch: 27, step: 12
	action: tensor([[-1.0448, -0.6046,  0.1526, -0.3348,  1.2979,  0.7066, -0.1269]],
       dtype=torch.float64)
	q_value: tensor([[-30.4011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0857134956151762, distance: 1.6526619144235226 entropy -2.373488207347894
epoch: 27, step: 13
	action: tensor([[-0.0556, -0.5500, -0.0968,  0.2276, -0.7213, -0.0297, -0.4099]],
       dtype=torch.float64)
	q_value: tensor([[-44.3703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13616328515338405, distance: 1.2197675312939247 entropy -2.7449134763805545
epoch: 27, step: 14
	action: tensor([[ 0.5920, -0.2634,  0.1932, -0.4412,  0.3654,  0.1682, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[-29.1720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23647935351223137, distance: 0.9999242154939016 entropy -2.5365501766833205
epoch: 27, step: 15
	action: tensor([[ 0.9303, -0.2277,  0.1836,  0.3725, -0.3804, -0.0509, -0.4241]],
       dtype=torch.float64)
	q_value: tensor([[-34.4464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7489638964056726, distance: 0.5733565602680247 entropy -2.484643741639114
epoch: 27, step: 16
	action: tensor([[-0.8820, -0.9729,  0.6469,  0.3930, -0.1711, -0.4006, -0.3335]],
       dtype=torch.float64)
	q_value: tensor([[-36.2091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1536077415448847, distance: 1.679345268963502 entropy -2.2652072807599097
epoch: 27, step: 17
	action: tensor([[ 0.0759,  0.6932,  0.3096,  0.0309, -0.0967,  0.3289, -0.0943]],
       dtype=torch.float64)
	q_value: tensor([[-32.2466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.679345268963502 entropy -2.459042182430113
epoch: 27, step: 18
	action: tensor([[ 0.4760, -0.2643,  0.2298, -0.0075,  0.4740, -0.0435, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-32.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41399381494248055, distance: 0.8760074369971189 entropy -3.4767828071491027
epoch: 27, step: 19
	action: tensor([[-0.7424,  0.0236,  0.3407, -0.0595, -1.1567,  0.2118, -0.3668]],
       dtype=torch.float64)
	q_value: tensor([[-32.3146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6409035481722594, distance: 1.465879327686094 entropy -2.6251430769617445
epoch: 27, step: 20
	action: tensor([[ 0.0243,  0.0494,  0.5625, -0.0438, -0.4013,  0.4341,  0.0523]],
       dtype=torch.float64)
	q_value: tensor([[-27.8108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4685000418960761, distance: 0.8342732344830914 entropy -2.352449679218411
epoch: 27, step: 21
	action: tensor([[-1.3438, -0.2671, -0.0744,  0.1133, -0.6402,  0.4326, -0.1062]],
       dtype=torch.float64)
	q_value: tensor([[-30.3120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.468290869357967, distance: 1.797855825366021 entropy -2.27134809268041
epoch: 27, step: 22
	action: tensor([[-0.2783, -0.1679, -0.0926, -0.6322, -0.5228,  0.3011,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-31.7686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35476621203376735, distance: 1.3319529119694156 entropy -2.7656511600687237
epoch: 27, step: 23
	action: tensor([[ 0.1830,  0.1459, -0.6293, -0.2300, -0.0631,  0.2811, -0.0852]],
       dtype=torch.float64)
	q_value: tensor([[-27.8333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5754215310073876, distance: 0.7456511235705543 entropy -2.698384043399372
epoch: 27, step: 24
	action: tensor([[ 0.0650, -0.1274, -0.3347, -0.1177, -0.4810,  0.0586, -0.1781]],
       dtype=torch.float64)
	q_value: tensor([[-24.4141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24127899073101644, distance: 0.9967764030602541 entropy -3.1709850146678646
epoch: 27, step: 25
	action: tensor([[ 0.3291,  0.0569, -0.1435,  0.1076, -0.1212,  0.4692, -0.0881]],
       dtype=torch.float64)
	q_value: tensor([[-24.2224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7366414635226844, distance: 0.5872599628677011 entropy -2.929509006596905
epoch: 27, step: 26
	action: tensor([[-0.4893,  0.2898, -0.1768, -0.2590,  0.2632,  0.5735, -0.1840]],
       dtype=torch.float64)
	q_value: tensor([[-30.1461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5872599628677011 entropy -2.613703215510673
epoch: 27, step: 27
	action: tensor([[ 0.0818, -0.1272, -0.0816,  0.1255,  0.0488,  0.1827, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-32.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36327263449222913, distance: 0.9131317655194483 entropy -3.4767828071491027
epoch: 27, step: 28
	action: tensor([[ 0.0313, -0.1407,  0.4667,  0.1879,  0.1021,  0.3325, -0.1540]],
       dtype=torch.float64)
	q_value: tensor([[-27.0756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4747718066005594, distance: 0.8293363640912429 entropy -2.840825849820976
epoch: 27, step: 29
	action: tensor([[-0.8946, -0.1422, -0.0802,  0.0871,  0.6529, -0.1146, -0.1943]],
       dtype=torch.float64)
	q_value: tensor([[-32.1331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9490534503129047, distance: 1.5976018982919933 entropy -2.4843353223614133
epoch: 27, step: 30
	action: tensor([[-0.0890, -0.0799, -0.1583,  0.1682,  0.1305,  0.0214, -0.1370]],
       dtype=torch.float64)
	q_value: tensor([[-26.9975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21355641472710163, distance: 1.0148234181307572 entropy -3.475581934890011
epoch: 27, step: 31
	action: tensor([[ 0.2344, -0.2187, -0.0221,  0.3380, -0.3402,  0.1759, -0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-24.3478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5411940178134196, distance: 0.7751240805538386 entropy -3.125438306343105
epoch: 27, step: 32
	action: tensor([[-0.2560, -0.8095, -0.2844,  0.4932, -0.0311,  0.6284, -0.1123]],
       dtype=torch.float64)
	q_value: tensor([[-29.7522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2549807769652359, distance: 1.2819622304950515 entropy -2.5612319888857646
epoch: 27, step: 33
	action: tensor([[-0.2833,  0.1109, -0.0276, -0.7065, -0.4582,  0.7091, -0.0805]],
       dtype=torch.float64)
	q_value: tensor([[-38.5210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15252734086788267, distance: 1.2285202278993317 entropy -2.535421732247108
epoch: 27, step: 34
	action: tensor([[-0.0808,  0.0223,  0.4443, -0.6918, -0.3066,  0.1567, -0.1190]],
       dtype=torch.float64)
	q_value: tensor([[-30.0683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18303621802529757, distance: 1.2446742747571358 entropy -2.5287123598148598
epoch: 27, step: 35
	action: tensor([[ 0.6256, -0.1212,  0.2290, -0.2922,  0.2345,  0.6953, -0.1989]],
       dtype=torch.float64)
	q_value: tensor([[-27.9024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.666607786856791, distance: 0.6607458122442462 entropy -2.5177488591353985
epoch: 27, step: 36
	action: tensor([[ 0.2990,  0.2710, -0.5554, -1.0210, -0.1546,  0.3066, -0.4072]],
       dtype=torch.float64)
	q_value: tensor([[-38.9472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5634377807984472, distance: 0.7561009217388288 entropy -2.287641191147802
epoch: 27, step: 37
	action: tensor([[ 0.0838, -0.0852, -0.1773, -0.2082, -0.2086,  0.0211, -0.2767]],
       dtype=torch.float64)
	q_value: tensor([[-28.0460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23079768348751095, distance: 1.0036377427594132 entropy -2.9660037222816613
epoch: 27, step: 38
	action: tensor([[-0.1141, -0.0473, -0.2280, -0.5780,  0.1034, -0.0008, -0.1417]],
       dtype=torch.float64)
	q_value: tensor([[-23.9879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03893161822337987, distance: 1.1664071549830417 entropy -3.000492558950795
epoch: 27, step: 39
	action: tensor([[-0.1307,  0.0561, -0.0110,  0.4374, -0.1743, -0.0631, -0.1846]],
       dtype=torch.float64)
	q_value: tensor([[-24.0970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1664071549830417 entropy -3.2775962526845817
epoch: 27, step: 40
	action: tensor([[-0.3520, -0.0012,  0.0227,  0.2301, -0.2654,  0.0653, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-32.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03330091635012411, distance: 1.1632420699241661 entropy -3.4767828071491027
epoch: 27, step: 41
	action: tensor([[-0.3520, -0.1343,  0.1694,  0.1701, -0.6166,  0.3381, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-22.7429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11793608246940201, distance: 1.2099437487723488 entropy -2.8846063033668825
epoch: 27, step: 42
	action: tensor([[ 1.1292, -0.1699, -0.4831,  0.3830,  0.0083, -0.3976,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-27.6813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6827439381088245, distance: 0.6445574875452337 entropy -2.4623680213516055
epoch: 27, step: 43
	action: tensor([[-0.1718,  0.2673,  0.5219,  0.0749,  0.5727, -0.0126, -0.4126]],
       dtype=torch.float64)
	q_value: tensor([[-33.2957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6445574875452337 entropy -2.6325232332323196
epoch: 27, step: 44
	action: tensor([[ 0.1933,  0.0757,  0.2137, -0.0303,  0.1652,  0.0723, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-32.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5161079061083884, distance: 0.7960327881882511 entropy -3.4767828071491027
epoch: 27, step: 45
	action: tensor([[-0.3003, -0.1470,  0.0981,  0.7304,  0.6721, -0.0266, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-26.2372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871985735615177, distance: 0.9661420249613184 entropy -2.758730431968465
epoch: 27, step: 46
	action: tensor([[ 0.3400,  0.2439,  0.3186, -0.3699,  0.3719,  0.0599, -0.2306]],
       dtype=torch.float64)
	q_value: tensor([[-32.0040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5949107069269363, distance: 0.728336501162246 entropy -2.9677950060626803
epoch: 27, step: 47
	action: tensor([[-0.0622, -0.5394,  0.2245,  0.1473, -0.4857, -0.1233, -0.3676]],
       dtype=torch.float64)
	q_value: tensor([[-28.7635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.728336501162246 entropy -2.7007722005103374
epoch: 27, step: 48
	action: tensor([[-0.4747,  0.0367,  0.0602,  0.3541, -0.1252,  0.1590, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-32.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.054082485027283855, distance: 1.1748813009988044 entropy -3.4767828071491027
epoch: 27, step: 49
	action: tensor([[-0.4071, -0.1886,  0.0805,  0.0332,  0.0198,  0.3608, -0.0609]],
       dtype=torch.float64)
	q_value: tensor([[-23.6613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2382662200595902, distance: 1.2733966588675714 entropy -2.8929815133864922
epoch: 27, step: 50
	action: tensor([[ 0.1777,  0.0708,  0.1642, -0.0753,  0.1868,  0.4710, -0.1008]],
       dtype=torch.float64)
	q_value: tensor([[-27.5615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.584552938554284, distance: 0.7375891829674753 entropy -2.804022425336427
epoch: 27, step: 51
	action: tensor([[ 0.0129, -0.2763,  0.0708, -0.1515,  0.3089,  0.2456, -0.2479]],
       dtype=torch.float64)
	q_value: tensor([[-30.2805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04986458348836176, distance: 1.1154483019041006 entropy -2.618937868799186
epoch: 27, step: 52
	action: tensor([[ 0.3600, -0.3980,  0.1188, -0.4955, -0.3302,  0.2429, -0.2364]],
       dtype=torch.float64)
	q_value: tensor([[-30.3471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04850921224372673, distance: 1.1717711970519729 entropy -2.7985955683550334
epoch: 27, step: 53
	action: tensor([[-0.9266,  0.0613,  0.1167,  0.3653, -0.0108,  0.5861, -0.2248]],
       dtype=torch.float64)
	q_value: tensor([[-32.6504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3932167289960222, distance: 1.3507221867203496 entropy -2.407583429678625
epoch: 27, step: 54
	action: tensor([[-0.2776, -0.3941,  0.0977,  0.0074, -0.1867,  0.3411, -0.0780]],
       dtype=torch.float64)
	q_value: tensor([[-28.7614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2771299237034679, distance: 1.293225422960595 entropy -2.8199202485571773
epoch: 27, step: 55
	action: tensor([[-0.4255,  0.0956, -0.4017, -0.0379, -0.5051,  0.3109, -0.0603]],
       dtype=torch.float64)
	q_value: tensor([[-29.6015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22678678625627158, distance: 1.2674803585714527 entropy -2.6090783919702654
epoch: 27, step: 56
	action: tensor([[-0.0046, -0.0157, -0.3313, -0.2043,  0.5605,  0.0607, -0.0352]],
       dtype=torch.float64)
	q_value: tensor([[-23.7118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20812038988660442, distance: 1.018324689742213 entropy -3.0883762816710445
epoch: 27, step: 57
	action: tensor([[-0.0177, -0.0499, -0.2740, -0.0535, -0.0043,  0.0140, -0.2165]],
       dtype=torch.float64)
	q_value: tensor([[-25.6167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18852726971573208, distance: 1.0308456869790832 entropy -3.2615260420075183
epoch: 27, step: 58
	action: tensor([[-0.2974, -0.2712,  0.4781,  0.0324, -0.1757, -0.0481, -0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-22.9956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2901246599469116, distance: 1.2997880252598453 entropy -3.223537334554235
epoch: 27, step: 59
	action: tensor([[-1.2662, -0.2864,  0.1891, -0.0373, -0.6223,  0.1914, -0.1146]],
       dtype=torch.float64)
	q_value: tensor([[-26.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5512036865095093, distance: 1.8278024712309626 entropy -2.6078641785598244
epoch: 27, step: 60
	action: tensor([[ 0.7760, -0.4298, -0.7230,  0.2334,  0.3377,  0.0966,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[-28.9469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4984559823073753, distance: 0.8104219972975611 entropy -2.686808263184641
epoch: 27, step: 61
	action: tensor([[ 0.3294, -0.3188, -0.0159,  0.1115,  0.6582,  0.1544, -0.3543]],
       dtype=torch.float64)
	q_value: tensor([[-36.1129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4396290695759031, distance: 0.8566323916004841 entropy -2.7645092309780033
epoch: 27, step: 62
	action: tensor([[-0.1132,  0.5919,  0.4500,  0.1335,  0.4009, -0.0428, -0.3428]],
       dtype=torch.float64)
	q_value: tensor([[-35.0777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8566323916004841 entropy -2.7382045578395586
epoch: 27, step: 63
	action: tensor([[-0.2027, -0.0621,  0.0029,  0.1445, -0.0275, -0.0113, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-32.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051419725556129836, distance: 1.1145350684095527 entropy -3.4767828071491027
LOSS epoch 27 actor 412.0363091429665 critic 234.62507328149354
epoch: 28, step: 0
	action: tensor([[ 0.0206, -0.3123,  0.0742,  0.1097,  0.3360,  0.0979, -0.0876]],
       dtype=torch.float64)
	q_value: tensor([[-24.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1294306734132552, distance: 1.0677224489753094 entropy -2.984560338160157
epoch: 28, step: 1
	action: tensor([[-0.3614, -0.1232, -0.1371,  0.7325, -0.6188, -0.0149, -0.2085]],
       dtype=torch.float64)
	q_value: tensor([[-31.8663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05560758680329225, distance: 1.112072081683153 entropy -2.8237525664711667
epoch: 28, step: 2
	action: tensor([[ 0.0318, -0.3483, -0.0585, -0.5308, -0.0524,  0.1976,  0.0299]],
       dtype=torch.float64)
	q_value: tensor([[-27.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21802255946212568, distance: 1.2629447715704911 entropy -2.7030537012352243
epoch: 28, step: 3
	action: tensor([[ 0.1166, -0.0744,  0.1447, -0.5224,  0.0252,  0.3536, -0.2174]],
       dtype=torch.float64)
	q_value: tensor([[-31.3120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1291526201894061, distance: 1.0678929465785651 entropy -2.7092072430193257
epoch: 28, step: 4
	action: tensor([[ 1.2062,  0.4747, -0.4902,  0.4636,  0.3393,  0.3165, -0.2666]],
       dtype=torch.float64)
	q_value: tensor([[-31.4398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9587761512808733, distance: 0.23234373212361048 entropy -2.616351467670495
epoch: 28, step: 5
	action: tensor([[-0.7739,  0.0324,  0.3612, -0.6202, -0.6314, -0.0408, -0.4692]],
       dtype=torch.float64)
	q_value: tensor([[-39.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.133279549323531, distance: 1.67140069445998 entropy -2.487851534625713
epoch: 28, step: 6
	action: tensor([[-0.0923, -0.3056,  0.1719, -0.1410,  0.5043,  0.2727, -0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-26.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1591787041994066, distance: 1.2320600913731534 entropy -2.7785083195653733
epoch: 28, step: 7
	action: tensor([[-0.0314, -0.5588, -0.6509, -0.4977, -0.0520,  0.1474, -0.2889]],
       dtype=torch.float64)
	q_value: tensor([[-33.2111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33433629465003323, distance: 1.3218718157358142 entropy -2.745055985366491
epoch: 28, step: 8
	action: tensor([[-0.1349, -0.3754,  0.3383, -0.2114, -0.0013,  0.5088, -0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-31.7775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24145896100957098, distance: 1.2750372625871977 entropy -3.124521546471524
epoch: 28, step: 9
	action: tensor([[-0.2994, -0.6932, -0.1113,  0.2080, -0.9700,  0.1548, -0.1737]],
       dtype=torch.float64)
	q_value: tensor([[-35.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6636101120766937, distance: 1.4759867846776518 entropy -2.4296653019939227
epoch: 28, step: 10
	action: tensor([[ 0.9741,  0.1569, -0.9391, -0.2912,  1.3355,  0.3277,  0.0484]],
       dtype=torch.float64)
	q_value: tensor([[-34.7845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8327405149825906, distance: 0.4680067004503015 entropy -2.3923934008265495
epoch: 28, step: 11
	action: tensor([[-0.1530, -0.5452, -0.0717,  0.5993, -0.4863,  0.5650, -0.4909]],
       dtype=torch.float64)
	q_value: tensor([[-40.3345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04587286927191436, distance: 1.1702971351108953 entropy -2.5760728757695577
epoch: 28, step: 12
	action: tensor([[ 0.0688, -0.3394,  0.7768, -0.3929, -0.4023,  0.7161,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-39.2171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05490790835854087, distance: 1.1753412197785837 entropy -2.322075592286248
epoch: 28, step: 13
	action: tensor([[-1.4357, -0.8611, -0.6682,  0.0119, -1.1581,  1.4529, -0.2094]],
       dtype=torch.float64)
	q_value: tensor([[-41.3805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4120896788583952, distance: 1.7772700340262302 entropy -2.0162966708618653
epoch: 28, step: 14
	action: tensor([[-0.1197,  0.8166,  1.0859, -0.2366, -0.0753, -0.0416,  0.1020]],
       dtype=torch.float64)
	q_value: tensor([[-51.9546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7772700340262302 entropy -2.222215922661632
epoch: 28, step: 15
	action: tensor([[-0.4422,  0.0568, -0.1290,  0.0264,  0.3073, -0.1049,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-32.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2573168980001961, distance: 1.2831548489736573 entropy -3.437607184740993
epoch: 28, step: 16
	action: tensor([[-0.1799, -0.2100,  0.3749,  0.0698,  0.0529, -0.0743, -0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-22.6016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08410261184391676, distance: 1.191494079352204 entropy -3.42775514883192
epoch: 28, step: 17
	action: tensor([[-0.3312, -0.1285, -0.5719,  0.7036,  0.1119,  0.2476, -0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-28.1633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12878076665702665, distance: 1.2157981933417878 entropy -2.7414119884061465
epoch: 28, step: 18
	action: tensor([[-0.1022, -0.2473, -0.1440,  0.1260, -0.0615,  0.1023, -0.0588]],
       dtype=torch.float64)
	q_value: tensor([[-29.7953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027681262331518908, distance: 1.1283946564079688 entropy -3.186122092670082
epoch: 28, step: 19
	action: tensor([[ 0.1769, -0.2715,  0.2542,  0.1279, -0.0776, -0.0568, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-27.6240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27610125784370876, distance: 0.9736337150748607 entropy -2.9306809890505483
epoch: 28, step: 20
	action: tensor([[ 0.4732,  0.1928, -0.8028,  0.8076, -0.0503,  0.0013, -0.1763]],
       dtype=torch.float64)
	q_value: tensor([[-30.6777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9736337150748607 entropy -2.6233632902339608
epoch: 28, step: 21
	action: tensor([[-0.3883, -0.1630,  0.2412, -0.0427, -0.1290, -0.1064,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-32.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3840573326777301, distance: 1.3462748525043906 entropy -3.437607184740993
epoch: 28, step: 22
	action: tensor([[-0.1764, -0.3143,  0.2591, -0.3731, -0.1256,  0.3205, -0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-24.3619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3334207625677812, distance: 1.3214182480241712 entropy -2.874766481547918
epoch: 28, step: 23
	action: tensor([[ 0.0498, -0.5315,  0.4698, -0.4631,  0.2766,  0.2988, -0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-31.7614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3824753568386605, distance: 1.3455052372689824 entropy -2.53744770238646
epoch: 28, step: 24
	action: tensor([[-0.0774, -0.4308,  0.1399, -0.2732,  0.4799,  0.3411, -0.3653]],
       dtype=torch.float64)
	q_value: tensor([[-37.6951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20869701912748218, distance: 1.2581007428999569 entropy -2.3906056454400586
epoch: 28, step: 25
	action: tensor([[ 0.3423,  0.1742,  0.5830, -0.1407, -0.4348, -0.0053, -0.3002]],
       dtype=torch.float64)
	q_value: tensor([[-36.1352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6005525354112886, distance: 0.7232468115445505 entropy -2.6776371101690883
epoch: 28, step: 26
	action: tensor([[-0.4120, -0.3508, -0.4709,  0.1583, -0.4187,  0.6284, -0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-30.2236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48671665867253955, distance: 1.395310290314955 entropy -2.378727598507271
epoch: 28, step: 27
	action: tensor([[-9.4065e-01,  4.9653e-02,  4.2811e-02,  1.8376e-01,  6.1270e-01,
         -1.1243e-01,  9.2049e-04]], dtype=torch.float64)
	q_value: tensor([[-32.9533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.865328895585221, distance: 1.562911555635313 entropy -2.753568367421014
epoch: 28, step: 28
	action: tensor([[-0.1394,  0.1482, -0.0052, -0.2976, -0.0396,  0.0255, -0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-26.6422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08762759493514638, distance: 1.0930568846320863 entropy -3.401287206396191
epoch: 28, step: 29
	action: tensor([[-0.1543, -0.3409, -0.1684,  0.4015, -0.6576, -0.1098, -0.1419]],
       dtype=torch.float64)
	q_value: tensor([[-23.1454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0006355126659391974, distance: 1.1439805735761321 entropy -3.10046710706992
epoch: 28, step: 30
	action: tensor([[-0.6420, -0.7291,  0.0081,  0.0973,  0.2209,  0.3989, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[-27.5943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8418327043305567, distance: 1.5530369312243917 entropy -2.722990659907226
epoch: 28, step: 31
	action: tensor([[ 0.6364,  0.0856, -0.2917,  0.0811,  0.1532, -0.1565, -0.1151]],
       dtype=torch.float64)
	q_value: tensor([[-35.5175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7756645334257557, distance: 0.5420079577884832 entropy -2.752191594303946
epoch: 28, step: 32
	action: tensor([[ 0.2074, -0.3419,  0.0829, -0.2002,  0.2163,  0.1239, -0.2807]],
       dtype=torch.float64)
	q_value: tensor([[-28.4597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07973343611429795, distance: 1.0977754513901214 entropy -2.876687798157834
epoch: 28, step: 33
	action: tensor([[-0.1591,  0.0214,  0.1324,  0.0977, -0.4014, -0.3028, -0.2654]],
       dtype=torch.float64)
	q_value: tensor([[-33.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11799850385776856, distance: 1.0747101589919126 entropy -2.6947668310370014
epoch: 28, step: 34
	action: tensor([[-0.0855, -0.3399, -0.0392, -0.8038, -0.0907, -0.2050, -0.0658]],
       dtype=torch.float64)
	q_value: tensor([[-22.2768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45210692847778833, distance: 1.3789737277547898 entropy -2.939465550959272
epoch: 28, step: 35
	action: tensor([[ 0.6951,  0.1650,  0.0987,  0.4423,  0.3099,  0.1168, -0.2247]],
       dtype=torch.float64)
	q_value: tensor([[-28.1462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9719409790970658, distance: 0.19168712758549064 entropy -2.9546816227903063
epoch: 28, step: 36
	action: tensor([[ 0.1469,  0.0726, -0.0089,  0.4814,  0.0435,  0.5649, -0.3445]],
       dtype=torch.float64)
	q_value: tensor([[-35.6443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.19168712758549064 entropy -2.5583281910078455
epoch: 28, step: 37
	action: tensor([[-0.1643,  0.0185, -0.0292,  0.2580, -0.3679, -0.1570,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-32.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19669752672603447, distance: 1.0256430590550458 entropy -3.437607184740993
epoch: 28, step: 38
	action: tensor([[-0.1134,  0.1037,  0.0203, -0.2505, -0.0408,  0.2472, -0.0454]],
       dtype=torch.float64)
	q_value: tensor([[-22.7065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15440443132293613, distance: 1.0522962978150938 entropy -2.911261553640337
epoch: 28, step: 39
	action: tensor([[ 0.3518, -0.3914, -0.3725, -0.3999, -0.2816,  0.3251, -0.1495]],
       dtype=torch.float64)
	q_value: tensor([[-25.7543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0913033770600673, distance: 1.0908527989465695 entropy -2.8811645985563916
epoch: 28, step: 40
	action: tensor([[-0.6404,  0.2516, -0.1452, -0.0862,  0.8870,  0.1711, -0.2249]],
       dtype=torch.float64)
	q_value: tensor([[-34.2631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30990034315444115, distance: 1.3097120438745387 entropy -2.61415609109984
epoch: 28, step: 41
	action: tensor([[ 0.1307,  0.0375, -0.0909,  0.4130, -0.0130,  0.0117, -0.1504]],
       dtype=torch.float64)
	q_value: tensor([[-30.2287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3097120438745387 entropy -3.3246635792668586
epoch: 28, step: 42
	action: tensor([[-0.1670,  0.1646, -0.2372,  0.1020,  0.0102,  0.2521,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-32.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2482510890348152, distance: 0.9921860053591481 entropy -3.437607184740993
epoch: 28, step: 43
	action: tensor([[-0.6354, -0.3194,  0.0220, -0.2584, -0.3597, -0.0116, -0.1125]],
       dtype=torch.float64)
	q_value: tensor([[-24.4178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8639817429489822, distance: 1.5623470812649736 entropy -3.067075069418288
epoch: 28, step: 44
	action: tensor([[ 0.0366,  0.1008, -0.7513, -0.3092, -0.1429,  0.1114, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[-26.0897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3827481086493396, distance: 0.8990584101311347 entropy -2.9800429227685767
epoch: 28, step: 45
	action: tensor([[-0.3959, -0.0387,  0.0391,  0.2897, -0.2508, -0.0709,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-32.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10069010226969821, distance: 1.2005748015663773 entropy -3.437607184740993
epoch: 28, step: 46
	action: tensor([[ 0.0456, -0.2094,  0.5084, -0.4353, -0.3704, -0.0168, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[-23.3679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18620317999862035, distance: 1.246339144145854 entropy -2.92452497430812
epoch: 28, step: 47
	action: tensor([[ 0.0702, -0.0114, -0.4890,  0.8581,  0.3653,  0.6818, -0.2017]],
       dtype=torch.float64)
	q_value: tensor([[-30.2964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.246339144145854 entropy -2.434744587307095
epoch: 28, step: 48
	action: tensor([[-0.0132,  0.0519,  0.1145, -0.0334, -0.1161,  0.0506,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-32.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.246339144145854 entropy -3.437607184740993
epoch: 28, step: 49
	action: tensor([[ 0.0311, -0.1106, -0.0232,  0.0546, -0.0786, -0.1068,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-32.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2443580007545284, distance: 0.9947518086833324 entropy -3.437607184740993
epoch: 28, step: 50
	action: tensor([[-0.0732,  0.0644,  0.0737, -0.1517, -1.1727,  0.2608, -0.1359]],
       dtype=torch.float64)
	q_value: tensor([[-25.1027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14879846132795715, distance: 1.0557786933666573 entropy -2.906891744299274
epoch: 28, step: 51
	action: tensor([[ 0.8846, -0.0356, -1.3589,  0.3361,  0.6912,  0.2272, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-29.3703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7854926722424979, distance: 0.530002306499053 entropy -2.3421494264448848
epoch: 28, step: 52
	action: tensor([[ 0.1068, -0.8794, -0.0380,  0.2713, -0.2992,  0.0041, -0.3356]],
       dtype=torch.float64)
	q_value: tensor([[-38.5661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24954463722088982, distance: 1.279182710246877 entropy -2.8206977297411306
epoch: 28, step: 53
	action: tensor([[-0.8198, -0.1422,  0.1847,  0.7842, -0.5354,  0.7734, -0.1062]],
       dtype=torch.float64)
	q_value: tensor([[-37.2296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10072339687714815, distance: 1.2005929594313263 entropy -2.461296396143669
epoch: 28, step: 54
	action: tensor([[ 0.3553, -0.2182,  0.1263,  0.4878,  0.2598,  0.7862,  0.1262]],
       dtype=torch.float64)
	q_value: tensor([[-36.7257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2005929594313263 entropy -2.280546360481562
epoch: 28, step: 55
	action: tensor([[ 0.1010, -0.1627,  0.0852,  0.2184,  0.2683, -0.0490,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-32.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3763302151636815, distance: 0.9037203156458269 entropy -3.437607184740993
epoch: 28, step: 56
	action: tensor([[-0.3133,  0.2998, -0.3774, -0.3563, -0.2250,  0.3083, -0.2063]],
       dtype=torch.float64)
	q_value: tensor([[-29.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1034300413439484, distance: 1.0835495714635524 entropy -2.8332490026188686
epoch: 28, step: 57
	action: tensor([[ 0.1643,  0.0242,  0.0054, -0.1740, -0.2969,  0.0323, -0.0780]],
       dtype=torch.float64)
	q_value: tensor([[-23.3230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39225447497243826, distance: 0.8921082961979959 entropy -3.3260440901050417
epoch: 28, step: 58
	action: tensor([[-0.0247, -0.0397, -0.3844, -0.2161,  0.4252,  0.3992, -0.1523]],
       dtype=torch.float64)
	q_value: tensor([[-26.0769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30235611173562515, distance: 0.9558144463427389 entropy -2.770237769039096
epoch: 28, step: 59
	action: tensor([[-0.1850, -0.3725, -0.0695, -0.1239, -0.0882,  0.0885, -0.2113]],
       dtype=torch.float64)
	q_value: tensor([[-29.5756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2474594035314106, distance: 1.2781149178022688 entropy -3.1174182752994115
epoch: 28, step: 60
	action: tensor([[-0.4827,  0.0866, -0.3410, -0.0988,  0.1691, -0.0453, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-28.2971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24010982568192807, distance: 1.2743442612587528 entropy -2.9232849611188763
epoch: 28, step: 61
	action: tensor([[ 0.0367, -0.0921, -0.0228, -0.1717, -0.0032,  0.0073, -0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-21.9438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.179500623148368, distance: 1.0365632824973783 entropy -3.7066288831876397
epoch: 28, step: 62
	action: tensor([[-0.6166,  0.1130,  0.0085, -0.0393,  0.0054,  0.1610, -0.1667]],
       dtype=torch.float64)
	q_value: tensor([[-26.1547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35435105629622554, distance: 1.3317488139272182 entropy -2.9283475866187634
epoch: 28, step: 63
	action: tensor([[ 0.2536, -0.4751,  0.0054, -0.0855, -0.0256,  0.1076, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-23.9055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07095399236109268, distance: 1.1029994705218964 entropy -3.207347953445199
LOSS epoch 28 actor 428.13697779790493 critic 371.44519050224056
epoch: 29, step: 0
	action: tensor([[-0.4741,  0.2209, -0.0358, -0.0779, -0.8242,  0.3526, -0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-34.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17531949274218794, distance: 1.2406082439854162 entropy -2.5910652060530963
epoch: 29, step: 1
	action: tensor([[-1.1605, -0.5410,  0.6771,  0.0557, -1.1111, -0.0314,  0.1018]],
       dtype=torch.float64)
	q_value: tensor([[-26.8017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4333086702687785, distance: 1.7850701799957902 entropy -2.622148569572022
epoch: 29, step: 2
	action: tensor([[-0.8453, -0.3843,  1.1196, -0.9755, -0.8831,  0.1348,  0.2583]],
       dtype=torch.float64)
	q_value: tensor([[-35.7319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4276050345532383, distance: 1.7829768649524866 entropy -2.146744872918583
epoch: 29, step: 3
	action: tensor([[-0.0096,  0.0338,  0.8974,  1.7372, -2.6801,  0.1255,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-38.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7829768649524866 entropy -1.9796057522267279
epoch: 29, step: 4
	action: tensor([[-0.0755,  0.0757, -0.0457,  0.2390,  0.1713,  0.1259,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37617209699743837, distance: 0.9038348678870605 entropy -3.37546067344547
epoch: 29, step: 5
	action: tensor([[ 0.3375, -0.6210, -0.2028,  0.5280,  0.0986, -0.3261, -0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-26.9442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20741750412031634, distance: 1.018776530627935 entropy -2.922749466513881
epoch: 29, step: 6
	action: tensor([[-0.2681,  0.1411,  0.0522, -0.2763,  0.0374,  0.1451,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04523934024212828, distance: 1.1699426324073439 entropy -3.37546067344547
epoch: 29, step: 7
	action: tensor([[-0.3549,  0.0238,  0.1381, -0.1250,  0.1737,  0.1408, -0.0910]],
       dtype=torch.float64)
	q_value: tensor([[-25.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16904393559457032, distance: 1.2372917294556465 entropy -2.947055767427052
epoch: 29, step: 8
	action: tensor([[ 0.2046, -0.5298, -0.0230,  0.7709,  0.5496,  0.1428, -0.0834]],
       dtype=torch.float64)
	q_value: tensor([[-26.4196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5201345032245925, distance: 0.792713867185495 entropy -2.9786310776700735
epoch: 29, step: 9
	action: tensor([[ 0.3912,  0.3949,  1.1801,  0.8985,  0.3087, -0.1124, -0.1678]],
       dtype=torch.float64)
	q_value: tensor([[-42.5111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05685679502861107 entropy -2.639537252471101
epoch: 29, step: 10
	action: tensor([[ 0.2070, -0.0294,  0.1291,  0.3091,  0.1270,  0.0975,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6309750873933913, distance: 0.69515961864463 entropy -3.37546067344547
epoch: 29, step: 11
	action: tensor([[-0.4091, -0.3039,  0.2016, -0.1266, -0.7679,  0.4541, -0.1118]],
       dtype=torch.float64)
	q_value: tensor([[-30.9567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4470202014350293, distance: 1.3765563380608465 entropy -2.6951432069293344
epoch: 29, step: 12
	action: tensor([[ 1.2562,  0.4124,  0.4769,  0.1605, -0.8574,  0.8748,  0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-33.9882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9241208896709554, distance: 0.31522293499761317 entropy -2.307540249552889
epoch: 29, step: 13
	action: tensor([[-1.7461, -1.0682, -0.8897,  2.2664, -3.6227,  1.3392, -0.3249]],
       dtype=torch.float64)
	q_value: tensor([[-50.0168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.31522293499761317 entropy -1.819944148932801
epoch: 29, step: 14
	action: tensor([[ 0.1787,  0.0840, -0.1925, -0.2282, -0.5339,  0.2026,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44534440354159766, distance: 0.8522527140749369 entropy -3.37546067344547
epoch: 29, step: 15
	action: tensor([[-0.0554, -0.3075,  0.1145,  0.0189, -0.2064,  0.1267, -0.0538]],
       dtype=torch.float64)
	q_value: tensor([[-27.9323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03307832094258423, distance: 1.125258603193866 entropy -2.6372840233908796
epoch: 29, step: 16
	action: tensor([[-0.2124, -0.5321,  0.5187, -0.2024,  1.0969,  0.1158, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-30.8044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4928502686366265, distance: 1.3981855793537865 entropy -2.6257725350035295
epoch: 29, step: 17
	action: tensor([[-0.6417, -0.5035,  0.6719,  0.2931, -0.1349, -0.3715, -0.4356]],
       dtype=torch.float64)
	q_value: tensor([[-41.3853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7223397326613756, distance: 1.5018138442561553 entropy -2.522177068188174
epoch: 29, step: 18
	action: tensor([[-0.1278,  0.1625,  0.0791, -0.2270,  0.0742,  0.1786,  0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-31.4691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2136751359510346, distance: 1.0147468165614772 entropy -2.52258107739908
epoch: 29, step: 19
	action: tensor([[-0.6279, -0.3431, -0.0576,  0.3985,  0.4768,  0.0922, -0.1126]],
       dtype=torch.float64)
	q_value: tensor([[-25.8749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49010432426516526, distance: 1.3968990782740145 entropy -2.8849577111930294
epoch: 29, step: 20
	action: tensor([[-0.3326, -0.2928, -0.0714,  0.1037, -0.0790,  0.1573, -0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-31.4043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22778691442715338, distance: 1.2679969049563558 entropy -3.1490230479917947
epoch: 29, step: 21
	action: tensor([[ 0.2838,  0.3084, -0.1912,  0.2935, -0.0052,  0.1874,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-28.5969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2679969049563558 entropy -2.893208815426419
epoch: 29, step: 22
	action: tensor([[ 0.0438,  0.0916,  0.1273,  0.2055, -0.0120, -0.0136,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4973054616906665, distance: 0.81135100160235 entropy -3.37546067344547
epoch: 29, step: 23
	action: tensor([[ 0.4369, -0.3524,  0.4669, -0.2701, -0.1246,  0.2608, -0.0723]],
       dtype=torch.float64)
	q_value: tensor([[-26.5419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22756098378519762, distance: 1.0057471122966863 entropy -2.778684449093721
epoch: 29, step: 24
	action: tensor([[-0.2011, -0.0150,  0.2234,  0.0319, -0.1579,  0.0389,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04857638513296547, distance: 1.11620421097116 entropy -3.37546067344547
epoch: 29, step: 25
	action: tensor([[-0.1574, -0.3265, -0.1464, -0.9934,  0.7892, -0.3785, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-26.0101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5252911118113559, distance: 1.4132957827797328 entropy -2.749788296046195
epoch: 29, step: 26
	action: tensor([[ 0.0249, -0.2259, -0.2071,  0.6496, -0.4672,  0.3506, -0.2676]],
       dtype=torch.float64)
	q_value: tensor([[-31.4910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4132957827797328 entropy -2.946536202601748
epoch: 29, step: 27
	action: tensor([[ 0.4521, -0.1252, -0.0991,  0.4471, -0.2170, -0.0854,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7577819954478574, distance: 0.5631964443120739 entropy -3.37546067344547
epoch: 29, step: 28
	action: tensor([[-0.4151,  0.2559,  0.3128, -0.3549, -0.5500,  0.1659, -0.1207]],
       dtype=torch.float64)
	q_value: tensor([[-32.5987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18571492793054412, distance: 1.2460826154555367 entropy -2.64432151165234
epoch: 29, step: 29
	action: tensor([[ 0.0820,  0.2238,  0.5219, -0.6714, -0.6190,  0.4266,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[-25.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21756893408568057, distance: 1.0122312386516616 entropy -2.6252839260700758
epoch: 29, step: 30
	action: tensor([[-0.4789, -0.2269, -0.6139, -0.7797,  0.5634,  0.5161, -0.0935]],
       dtype=torch.float64)
	q_value: tensor([[-33.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20128070078417526, distance: 1.2542350792880275 entropy -2.205024982276405
epoch: 29, step: 31
	action: tensor([[-0.1124,  0.2757, -0.4065, -0.3185,  0.4676, -0.0035, -0.1595]],
       dtype=torch.float64)
	q_value: tensor([[-34.9121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34185288570251027, distance: 0.9283637710638257 entropy -3.1526618496852348
epoch: 29, step: 32
	action: tensor([[-0.0014, -0.2033, -0.0921, -0.1179, -0.2556,  0.1403, -0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-24.0439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11399368636866858, distance: 1.0771473109763443 entropy -3.4548673794224087
epoch: 29, step: 33
	action: tensor([[-0.2750,  0.1863,  0.1894, -0.2697, -0.3347,  0.2901, -0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-28.9066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04175732543618649, distance: 1.1201971082640187 entropy -2.769753479718419
epoch: 29, step: 34
	action: tensor([[-0.2847,  0.1803,  0.1689, -0.4319,  0.1630,  0.2164,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[-26.7648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03647383899191081, distance: 1.1650266653027843 entropy -2.6613887542471693
epoch: 29, step: 35
	action: tensor([[-0.0757,  0.1776, -0.1847, -0.5101,  0.0856,  0.0854, -0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-26.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21922769293643352, distance: 1.0111577010317234 entropy -2.912379674487756
epoch: 29, step: 36
	action: tensor([[ 0.3548,  0.1449, -0.0221, -0.1321, -0.1297,  0.2662, -0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-24.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.69634023347271, distance: 0.6305947056272095 entropy -3.2127920727757506
epoch: 29, step: 37
	action: tensor([[ 0.1279,  0.4115,  0.0767, -0.0641, -0.4098, -0.0167, -0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-30.2110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6305947056272095 entropy -2.6462553851508774
epoch: 29, step: 38
	action: tensor([[-0.0036, -0.0644, -0.2160, -0.1689,  0.5278,  0.1300,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2145321606002275, distance: 1.0141936736001824 entropy -3.37546067344547
epoch: 29, step: 39
	action: tensor([[ 0.1512, -0.0404,  0.2011,  0.1158, -0.2025,  0.1128, -0.2001]],
       dtype=torch.float64)
	q_value: tensor([[-28.6668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4934283304654913, distance: 0.8144738444581014 entropy -3.076416534098449
epoch: 29, step: 40
	action: tensor([[-0.4402, -0.4388, -0.6870, -0.0710, -0.1579,  0.0166, -0.0509]],
       dtype=torch.float64)
	q_value: tensor([[-30.6607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5641057114971086, distance: 1.4311651234567753 entropy -2.6038668909381046
epoch: 29, step: 41
	action: tensor([[ 0.3322, -0.1331, -0.2092,  0.1184,  0.0109, -0.0085,  0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-27.8976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5178169593167965, distance: 0.7946257949924064 entropy -3.4957689917593116
epoch: 29, step: 42
	action: tensor([[-0.0516, -0.2342,  0.4724, -0.8658, -0.2299,  0.0663, -0.1305]],
       dtype=torch.float64)
	q_value: tensor([[-30.0996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5006135023378668, distance: 1.4018163408198778 entropy -2.8436313763065297
epoch: 29, step: 43
	action: tensor([[ 0.2634, -0.6498, -0.0525, -1.1139, -0.3001,  0.0318, -0.2031]],
       dtype=torch.float64)
	q_value: tensor([[-33.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5993892035247224, distance: 1.4472173928461758 entropy -2.433716307569454
epoch: 29, step: 44
	action: tensor([[-0.1914, -0.3773, -1.0614, -1.2390, -0.7441, -0.3754, -0.3011]],
       dtype=torch.float64)
	q_value: tensor([[-38.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39143250160082643, distance: 0.8927113787866102 entropy -2.4672442769467793
epoch: 29, step: 45
	action: tensor([[-0.4946,  0.0348, -0.4112,  0.5552,  0.2766,  0.1217, -0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-31.1372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15352398504126175, distance: 1.229051292499526 entropy -3.3477872975516925
epoch: 29, step: 46
	action: tensor([[ 0.1377,  0.0641,  0.1517,  0.2961,  0.2121, -0.0306, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[-27.1759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.229051292499526 entropy -3.3909849244742936
epoch: 29, step: 47
	action: tensor([[-0.3863, -0.1697,  0.0583, -0.0065,  0.0127,  0.0346,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32707820628420636, distance: 1.3182717693953774 entropy -3.37546067344547
epoch: 29, step: 48
	action: tensor([[ 0.4925, -0.0680,  0.3495,  0.3600,  0.2229,  0.5045, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[-26.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8962547405326856, distance: 0.36858768762639776 entropy -2.925966093383728
epoch: 29, step: 49
	action: tensor([[ 1.0195, -0.2573, -1.1661, -0.2469,  0.8980,  0.1076, -0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-41.3478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46408471283651953, distance: 0.837731346040055 entropy -2.3313660577461426
epoch: 29, step: 50
	action: tensor([[ 0.0228,  0.3637, -0.0914,  0.1824, -0.9706,  0.1205, -0.3260]],
       dtype=torch.float64)
	q_value: tensor([[-40.1725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.837731346040055 entropy -2.6899952028312626
epoch: 29, step: 51
	action: tensor([[ 0.0185, -0.1348,  0.1514,  0.3799, -0.2288, -0.1483,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39691198599844324, distance: 0.8886833467214045 entropy -3.37546067344547
epoch: 29, step: 52
	action: tensor([[ 0.4199,  0.2411, -0.8465,  0.1716, -0.5307,  0.3991, -0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-27.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8886833467214045 entropy -2.706810203934789
epoch: 29, step: 53
	action: tensor([[-0.0410, -0.0311, -0.1578,  0.4113,  0.0164,  0.0384,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35345482753594215, distance: 0.9201447020960992 entropy -3.37546067344547
epoch: 29, step: 54
	action: tensor([[-0.1238, -0.2969,  0.0608, -0.4198,  0.3038,  0.2206, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-27.4444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25807657861210576, distance: 1.2835424364876982 entropy -2.9329810914861647
epoch: 29, step: 55
	action: tensor([[-0.2603, -0.6449, -0.2500,  0.1815,  0.3493,  0.0502, -0.2032]],
       dtype=torch.float64)
	q_value: tensor([[-31.3877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.469612924916635, distance: 1.3872609921428827 entropy -2.7894006174993544
epoch: 29, step: 56
	action: tensor([[-0.0141, -0.0225,  0.7221, -0.0197,  0.1120,  0.0496, -0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-33.8132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2375080644286902, distance: 0.9992503766983643 entropy -3.023210100710618
epoch: 29, step: 57
	action: tensor([[ 0.0507, -0.1166, -0.0179, -0.6924, -0.2166,  0.1361, -0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-32.2229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08430596139363056, distance: 1.1916058208151985 entropy -2.453890256480818
epoch: 29, step: 58
	action: tensor([[ 0.9352,  0.1578,  0.3679,  0.4539, -0.1140,  0.2156, -0.1568]],
       dtype=torch.float64)
	q_value: tensor([[-29.9054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.967333301162834, distance: 0.20682799295312743 entropy -2.749656178576484
epoch: 29, step: 59
	action: tensor([[ 1.6880, -1.2005,  0.3153,  0.6776, -0.7235,  0.3026, -0.2641]],
       dtype=torch.float64)
	q_value: tensor([[-41.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.20682799295312743 entropy -2.238607670120479
epoch: 29, step: 60
	action: tensor([[ 0.1039, -0.0439,  0.0546, -0.1011,  0.3255, -0.1040,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-32.5321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28303878251071524, distance: 0.9689570463454678 entropy -3.37546067344547
epoch: 29, step: 61
	action: tensor([[-0.3137, -0.3763,  0.1124, -0.5245, -0.7415, -0.0522, -0.1851]],
       dtype=torch.float64)
	q_value: tensor([[-27.4302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6101431029735078, distance: 1.4520746086626328 entropy -2.9212011913369023
epoch: 29, step: 62
	action: tensor([[ 0.1495, -0.0281, -0.5857, -0.5834,  0.0588, -0.1272,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[-30.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074701652528732, distance: 0.9523047207627843 entropy -2.6214645707288713
epoch: 29, step: 63
	action: tensor([[-0.2940, -0.1312, -0.3153,  0.1306, -0.3582, -0.0569, -0.1280]],
       dtype=torch.float64)
	q_value: tensor([[-25.2067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10762356594138822, distance: 1.2043501939445127 entropy -3.3321510145544893
LOSS epoch 29 actor 579.3866511151504 critic 1327.57033635162
epoch: 30, step: 0
	action: tensor([[ 0.0621,  0.0091, -0.0026,  0.0533,  0.4707,  0.0576,  0.1156]],
       dtype=torch.float64)
	q_value: tensor([[-23.5193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4043296260981547, distance: 0.8832012877593332 entropy -3.1344210665310075
epoch: 30, step: 1
	action: tensor([[-0.1399, -0.3158,  0.2555, -0.4598,  0.1596,  0.1810, -0.0667]],
       dtype=torch.float64)
	q_value: tensor([[-27.2936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34493814216933294, distance: 1.327112831897411 entropy -2.945745171887885
epoch: 30, step: 2
	action: tensor([[-0.0242,  0.1069, -0.2303, -0.4490, -0.1952,  0.0091, -0.0356]],
       dtype=torch.float64)
	q_value: tensor([[-30.6744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21105331291501428, distance: 1.0164371310181533 entropy -2.6180320806749164
epoch: 30, step: 3
	action: tensor([[-0.1868, -0.0050, -0.1370, -0.0998,  0.2726, -0.0728,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-23.7700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025395021031909493, distance: 1.1297204910683234 entropy -3.118348331631804
epoch: 30, step: 4
	action: tensor([[-0.0808, -0.0930,  0.3358,  0.3156, -0.5530,  0.2459, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-23.3169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3904061468792118, distance: 0.8934638446636793 entropy -3.2966353286291397
epoch: 30, step: 5
	action: tensor([[-0.7701, -0.6505, -0.2033, -0.1321, -0.1355,  0.2638,  0.2413]],
       dtype=torch.float64)
	q_value: tensor([[-31.7364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0822891612083128, distance: 1.6513046829940599 entropy -2.3297137650677677
epoch: 30, step: 6
	action: tensor([[ 0.2217,  0.1274, -0.0735,  0.0680, -0.4266,  0.0920,  0.1129]],
       dtype=torch.float64)
	q_value: tensor([[-31.5366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6128336894168658, distance: 0.7120417735607956 entropy -2.8255315726347514
epoch: 30, step: 7
	action: tensor([[-0.8408, -0.6047,  0.0866,  0.3664, -0.2020,  0.5112,  0.0831]],
       dtype=torch.float64)
	q_value: tensor([[-27.1217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8058858072422672, distance: 1.5378070078824078 entropy -2.628443910472182
epoch: 30, step: 8
	action: tensor([[ 0.8750, -0.2096,  0.2694,  0.6994, -0.0907,  0.6225,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-35.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9596471787782144, distance: 0.22987600710728395 entropy -2.4678980353697773
epoch: 30, step: 9
	action: tensor([[ 0.6123, -1.0558,  0.0954, -0.3707, -1.1245,  0.7938, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-47.6714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48945333043736383, distance: 1.3965939076875855 entropy -2.1055944671846136
epoch: 30, step: 10
	action: tensor([[-0.0671, -0.8647, -0.5018,  0.2500, -0.0481,  0.0700,  0.0892]],
       dtype=torch.float64)
	q_value: tensor([[-53.8891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4315482625831477, distance: 1.3691773004747176 entropy -1.802462950282177
epoch: 30, step: 11
	action: tensor([[-0.3431,  0.1202,  0.0846, -0.1640, -0.1061, -0.0805,  0.0650]],
       dtype=torch.float64)
	q_value: tensor([[-34.4761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0705147567405977, distance: 1.1840035987562072 entropy -2.801656905354581
epoch: 30, step: 12
	action: tensor([[-0.0582, -0.0035, -0.0966,  0.0536,  0.2837, -0.1187,  0.0689]],
       dtype=torch.float64)
	q_value: tensor([[-22.2326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2550856464036094, distance: 0.9876654564615056 entropy -3.035227511703163
epoch: 30, step: 13
	action: tensor([[-0.3409, -0.2836,  0.2196, -0.0939,  0.1182, -0.0104, -0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-24.0683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3410025412959037, distance: 1.325169689393488 entropy -3.1615728373093597
epoch: 30, step: 14
	action: tensor([[-0.6623, -0.5440,  0.3518,  0.4463,  0.3102,  0.0224,  0.0635]],
       dtype=torch.float64)
	q_value: tensor([[-27.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5108337277015731, distance: 1.4065819142357676 entropy -2.822431102382967
epoch: 30, step: 15
	action: tensor([[-0.0155,  0.1241,  0.2867, -0.0467, -0.4284,  0.2053,  0.1349]],
       dtype=torch.float64)
	q_value: tensor([[-31.9872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40701301445634397, distance: 0.881209710486722 entropy -2.6846766459606317
epoch: 30, step: 16
	action: tensor([[-1.9705,  0.8972, -0.7137, -0.6769, -0.0450,  0.6030,  0.1483]],
       dtype=torch.float64)
	q_value: tensor([[-27.9937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.881209710486722 entropy -2.4455217420645043
epoch: 30, step: 17
	action: tensor([[-0.0434, -0.1476,  0.2033, -0.2768,  0.4671,  0.0228,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0369608715025147, distance: 1.1653003524914658 entropy -3.3657999214414027
epoch: 30, step: 18
	action: tensor([[-0.1532, -0.5414, -0.2004, -0.3095, -0.0581, -0.1144, -0.1032]],
       dtype=torch.float64)
	q_value: tensor([[-28.4885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45926526910256427, distance: 1.3823684599453738 entropy -2.8367498362207746
epoch: 30, step: 19
	action: tensor([[ 0.6101, -0.2578, -0.3360,  0.0738, -0.1574,  0.3112,  0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-28.1885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6003921721506267, distance: 0.7233919752881537 entropy -3.009865155158575
epoch: 30, step: 20
	action: tensor([[-0.4050, -0.3352,  0.0212,  0.4703, -0.6540,  0.4717, -0.0313]],
       dtype=torch.float64)
	q_value: tensor([[-36.3002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15038334866690972, distance: 1.2273770168959464 entropy -2.505004511927599
epoch: 30, step: 21
	action: tensor([[-0.9638,  0.1484, -0.3157, -0.4178,  0.7952,  0.7721,  0.3100]],
       dtype=torch.float64)
	q_value: tensor([[-34.3819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5503607815667535, distance: 1.4248629050246928 entropy -2.32243436877306
epoch: 30, step: 22
	action: tensor([[-0.1502,  0.3509,  0.0389, -0.4010, -0.2019, -0.0546, -0.1133]],
       dtype=torch.float64)
	q_value: tensor([[-33.4264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1872468405971508, distance: 1.0316586561126162 entropy -3.124558002136186
epoch: 30, step: 23
	action: tensor([[ 0.0126, -0.0648,  0.1259, -0.1554, -0.2479,  0.1963,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[-21.9590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20369977190957012, distance: 1.0211631006185744 entropy -3.0788698241075108
epoch: 30, step: 24
	action: tensor([[-0.4977, -0.0580, -0.5593,  0.7091, -0.2270,  0.3480,  0.0912]],
       dtype=torch.float64)
	q_value: tensor([[-28.3880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27660718755359803, distance: 1.2929607338109879 entropy -2.5795535576697333
epoch: 30, step: 25
	action: tensor([[ 0.0690,  0.2011, -0.1459,  0.2525, -0.1592,  0.4100,  0.1207]],
       dtype=torch.float64)
	q_value: tensor([[-29.8564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2929607338109879 entropy -2.9652190204655793
epoch: 30, step: 26
	action: tensor([[ 0.0427, -0.1788, -0.0620, -0.0094, -0.0707, -0.2409,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1436137590445069, distance: 1.0589892068648192 entropy -3.3657999214414027
epoch: 30, step: 27
	action: tensor([[ 0.1545,  0.2403,  0.2878, -0.5170,  0.0711,  0.2566,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-24.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40739836425017273, distance: 0.8809233389692137 entropy -2.9503950681744913
epoch: 30, step: 28
	action: tensor([[-0.6294, -0.2732,  0.0741, -0.8004,  0.3790,  0.4477, -0.0866]],
       dtype=torch.float64)
	q_value: tensor([[-29.0768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.780920189028006, distance: 1.5271402433851353 entropy -2.5721009210191323
epoch: 30, step: 29
	action: tensor([[-0.3208,  0.1817,  0.2864,  0.5550, -1.1638,  0.2666, -0.1157]],
       dtype=torch.float64)
	q_value: tensor([[-32.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5271402433851353 entropy -2.8028539364393597
epoch: 30, step: 30
	action: tensor([[ 0.0953, -0.0760, -0.0957,  0.0446, -0.2131,  0.1914,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36224722720413816, distance: 0.9138667388739216 entropy -3.3657999214414027
epoch: 30, step: 31
	action: tensor([[-0.0214, -0.2216, -0.1947,  0.3508, -0.3389,  0.4345,  0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-28.2443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28949614832268655, distance: 0.9645836840987068 entropy -2.6891450072003615
epoch: 30, step: 32
	action: tensor([[-0.0602, -0.1172,  0.0694,  0.0733,  0.1341,  0.6985,  0.1587]],
       dtype=torch.float64)
	q_value: tensor([[-32.6073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41435546692402625, distance: 0.8757370825809297 entropy -2.5160014650812803
epoch: 30, step: 33
	action: tensor([[-0.9792, -0.1989, -0.0849, -1.2029, -0.2020,  0.2213,  0.0311]],
       dtype=torch.float64)
	q_value: tensor([[-33.9932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0035788708034237, distance: 1.6197944807037792 entropy -2.472715643673196
epoch: 30, step: 34
	action: tensor([[ 0.0634, -0.1525, -0.1196, -0.1338,  0.5349,  0.1785, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-30.9410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24083522683737835, distance: 0.9970678598096836 entropy -3.0040543924002283
epoch: 30, step: 35
	action: tensor([[ 0.2768, -0.0417,  0.3557, -0.0939,  0.1356, -0.0566, -0.1052]],
       dtype=torch.float64)
	q_value: tensor([[-29.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4333562668686529, distance: 0.8614136280552218 entropy -2.9602919902671885
epoch: 30, step: 36
	action: tensor([[-0.5500, -1.1708, -0.0861,  1.0673,  0.5789,  0.0130, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-30.1865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5532817939719512, distance: 1.426204555208451 entropy -2.6140777820459897
epoch: 30, step: 37
	action: tensor([[-0.2892, -0.3713, -0.2170, -0.1687, -0.3797,  0.2081,  0.1267]],
       dtype=torch.float64)
	q_value: tensor([[-42.7405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37469210603066583, distance: 1.3417123356235412 entropy -2.6336755604040647
epoch: 30, step: 38
	action: tensor([[-0.5040, -0.8419,  0.2385,  0.3512,  0.7051,  0.3244,  0.1269]],
       dtype=torch.float64)
	q_value: tensor([[-28.8975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.501101775749951, distance: 1.4020443855419937 entropy -2.731463218504183
epoch: 30, step: 39
	action: tensor([[-0.5414, -0.2304,  0.3049, -0.1416, -0.5060,  0.3893, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[-39.3546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5461192031992774, distance: 1.422912453531076 entropy -2.5809757944089378
epoch: 30, step: 40
	action: tensor([[-0.3476, -0.7512, -0.1846, -1.3748,  1.4166,  0.8807,  0.2568]],
       dtype=torch.float64)
	q_value: tensor([[-30.8417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17898806046767568, distance: 1.242542913419516 entropy -2.3760620139847366
epoch: 30, step: 41
	action: tensor([[-1.3442, -0.2488,  0.0875,  0.5138,  0.3044,  0.0677, -0.4970]],
       dtype=torch.float64)
	q_value: tensor([[-48.6757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2136553776383354, distance: 1.7025963524237415 entropy -2.3230118210689463
epoch: 30, step: 42
	action: tensor([[-0.0575, -0.1594, -0.0555,  0.2242, -0.3180,  0.2472,  0.1434]],
       dtype=torch.float64)
	q_value: tensor([[-31.4878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2585091609847455, distance: 0.9853932616781396 entropy -3.065303882667083
epoch: 30, step: 43
	action: tensor([[-0.3341, -0.3458, -0.0517, -0.6413, -0.1665,  0.6629,  0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-29.6665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46945176775365804, distance: 1.3871849268159466 entropy -2.5772075275017383
epoch: 30, step: 44
	action: tensor([[-0.1391, -0.1623, -0.4811,  0.2382, -0.4572,  0.8004,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-34.4570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04542790862639878, distance: 1.11804957198669 entropy -2.4643367604025457
epoch: 30, step: 45
	action: tensor([[ 0.0476, -0.5111, -0.2325,  0.0323, -0.0826,  0.3427,  0.1355]],
       dtype=torch.float64)
	q_value: tensor([[-34.3654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004016122130686917, distance: 1.1466398645882028 entropy -2.4796802762848857
epoch: 30, step: 46
	action: tensor([[ 0.0074, -0.1427,  0.0147, -0.1860, -0.5365,  0.3028,  0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-33.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12674885932723645, distance: 1.0693657593838761 entropy -2.589063255681463
epoch: 30, step: 47
	action: tensor([[ 0.2673, -0.1634, -0.2244, -1.1764, -1.3644,  0.3266,  0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-30.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1426804406243969, distance: 1.2232608881231302 entropy -2.4547817462669435
epoch: 30, step: 48
	action: tensor([[-2.3922, -0.0084, -0.3671, -0.7808, -0.1275,  0.8937,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-39.9559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2232608881231302 entropy -2.197361394017583
epoch: 30, step: 49
	action: tensor([[-0.0490,  0.0861,  0.2323,  0.0738,  0.0844, -0.0399,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3200598445923928, distance: 0.9436089215423789 entropy -3.3657999214414027
epoch: 30, step: 50
	action: tensor([[-0.3166, -0.3014,  0.1666,  0.3303,  0.1527,  0.2910,  0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-25.1827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012642635196251306, distance: 1.1370874813913239 entropy -2.8032187791487613
epoch: 30, step: 51
	action: tensor([[ 0.1539, -0.0874, -0.5476, -0.5388, -0.3014,  0.3320,  0.1199]],
       dtype=torch.float64)
	q_value: tensor([[-31.4132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26345782758800096, distance: 0.9820995283603823 entropy -2.649617084392322
epoch: 30, step: 52
	action: tensor([[-0.8713, -0.0749,  0.5847,  0.3405, -0.2675,  0.0579, -0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-29.5657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5616085799137494, distance: 1.4300222227048318 entropy -2.822073669257154
epoch: 30, step: 53
	action: tensor([[ 0.7480, -0.3145, -1.3647, -0.5734, -0.2706,  0.4472,  0.2737]],
       dtype=torch.float64)
	q_value: tensor([[-27.3688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40077043988491445, distance: 0.8858359631747613 entropy -2.4688482501643976
epoch: 30, step: 54
	action: tensor([[-0.5934,  0.2877,  0.0642, -0.3358,  0.7120, -0.0934, -0.1859]],
       dtype=torch.float64)
	q_value: tensor([[-38.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3746441745004485, distance: 1.3416889446090499 entropy -2.685489973688616
epoch: 30, step: 55
	action: tensor([[ 0.0805, -0.1112, -0.1307,  0.2052,  0.2908,  0.0517, -0.0608]],
       dtype=torch.float64)
	q_value: tensor([[-25.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41753293884743514, distance: 0.8733581526440702 entropy -3.4025367557123722
epoch: 30, step: 56
	action: tensor([[ 0.5256, -0.6673,  0.5997, -0.0076,  0.7030,  0.1946,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-28.1336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061715414354588494, distance: 1.1084701031094444 entropy -2.9773429070340436
epoch: 30, step: 57
	action: tensor([[ 0.7022, -0.5288,  0.2203,  0.3419, -0.2156, -0.2363, -0.2141]],
       dtype=torch.float64)
	q_value: tensor([[-45.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.400210701915528, distance: 0.8862495945446581 entropy -2.2196151456036963
epoch: 30, step: 58
	action: tensor([[ 0.4090,  0.5677, -0.0759, -0.1370, -1.2834,  0.1012,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-39.0656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8862495945446581 entropy -2.3514120491087716
epoch: 30, step: 59
	action: tensor([[-0.1692, -0.0551, -0.0347, -0.0657,  0.2970,  0.0650,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02031954211843323, distance: 1.1326583104491064 entropy -3.3657999214414027
epoch: 30, step: 60
	action: tensor([[-0.2359, -0.3594,  0.6591,  0.4280, -0.0724, -0.0452, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[-25.5296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06515103599154293, distance: 1.106438855646091 entropy -3.032029520178547
epoch: 30, step: 61
	action: tensor([[ 0.1171, -0.6064,  0.1006, -0.9123, -0.2766,  0.8792,  0.1937]],
       dtype=torch.float64)
	q_value: tensor([[-33.1270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4198029156132601, distance: 1.3635489243410885 entropy -2.39220953089224
epoch: 30, step: 62
	action: tensor([[-1.6322, -0.7595,  0.5575,  1.4140,  0.3662,  1.2363, -0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-44.0228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3635489243410885 entropy -2.077001086585991
epoch: 30, step: 63
	action: tensor([[ 0.3382,  0.0304, -0.0241,  0.1913, -0.0703, -0.0291,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-30.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6684616340534504, distance: 0.6589061931908463 entropy -3.3657999214414027
LOSS epoch 30 actor 474.0133641356424 critic 193.1983923215519
epoch: 31, step: 0
	action: tensor([[ 0.0214, -0.4269,  0.6615,  0.3260,  0.2682,  0.2009,  0.2260]],
       dtype=torch.float64)
	q_value: tensor([[-26.9610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31672064111388665, distance: 0.9459231275044998 entropy -2.703738693799662
epoch: 31, step: 1
	action: tensor([[-0.7895, -0.5809, -0.4157, -0.9618,  0.0416,  0.6149,  0.3576]],
       dtype=torch.float64)
	q_value: tensor([[-35.3934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8169900941822179, distance: 1.5425277051669035 entropy -2.289278639567775
epoch: 31, step: 2
	action: tensor([[ 0.2310, -0.0234, -0.3087, -0.2370,  0.5194,  0.2226,  0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-34.6543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4810023931086449, distance: 0.8244026344331652 entropy -2.8232735083471034
epoch: 31, step: 3
	action: tensor([[-0.5432, -0.3515, -0.3124,  0.1164, -0.1846,  0.0067,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[-26.8443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5772056748890619, distance: 1.4371458951925244 entropy -3.0059030894101006
epoch: 31, step: 4
	action: tensor([[ 0.0893, -0.0946,  0.2902,  0.1086,  0.3669,  0.2623,  0.2413]],
       dtype=torch.float64)
	q_value: tensor([[-24.3113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48393505764668443, distance: 0.8220701367483603 entropy -3.085009535654758
epoch: 31, step: 5
	action: tensor([[ 0.3188, -0.7078,  0.1457, -0.5708, -0.2148,  0.3647,  0.2010]],
       dtype=torch.float64)
	q_value: tensor([[-29.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39277659820777333, distance: 1.3505088166941934 entropy -2.539878818095199
epoch: 31, step: 6
	action: tensor([[-1.3033, -1.3930, -0.3565, -1.2330, -0.8022,  0.7286,  0.3354]],
       dtype=torch.float64)
	q_value: tensor([[-38.4257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3505088166941934 entropy -2.1971963626755473
epoch: 31, step: 7
	action: tensor([[-0.0816, -0.3734,  0.1382,  0.1128, -0.0276,  0.0945,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01818763029132464, distance: 1.1547038173832807 entropy -3.3641942008635572
epoch: 31, step: 8
	action: tensor([[ 0.0377, -0.1471,  0.7956, -0.6216,  1.1264,  0.1197,  0.3228]],
       dtype=torch.float64)
	q_value: tensor([[-28.3673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24136053944490166, distance: 1.274986719776828 entropy -2.6227094455225948
epoch: 31, step: 9
	action: tensor([[-0.7615, -1.0813,  0.1253, -0.1981,  0.3964, -0.0804, -0.0600]],
       dtype=torch.float64)
	q_value: tensor([[-36.6890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.249939945994496, distance: 1.7164934733617676 entropy -2.3395363590580254
epoch: 31, step: 10
	action: tensor([[ 0.1552, -0.8405, -0.0568,  0.7684,  0.1944,  0.6019,  0.2978]],
       dtype=torch.float64)
	q_value: tensor([[-32.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4086730696245825, distance: 0.8799753815596276 entropy -2.723950298824405
epoch: 31, step: 11
	action: tensor([[ 0.5283,  0.4627,  0.2481,  0.0604, -0.3578,  0.7676,  0.3959]],
       dtype=torch.float64)
	q_value: tensor([[-42.6418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8799753815596276 entropy -2.280533991250599
epoch: 31, step: 12
	action: tensor([[-0.0505,  0.0444, -0.1338, -0.1318, -0.1272, -0.0059,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.210329134342081, distance: 1.0169035206786332 entropy -3.3641942008635572
epoch: 31, step: 13
	action: tensor([[-0.3134,  0.1168, -0.1281, -0.0378, -0.0097,  0.4154,  0.1944]],
       dtype=torch.float64)
	q_value: tensor([[-22.5408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07401494822060461, distance: 1.1011809285987213 entropy -2.973370891330472
epoch: 31, step: 14
	action: tensor([[ 0.0320, -0.1198,  0.2761, -0.0791,  0.5819,  0.1593,  0.2269]],
       dtype=torch.float64)
	q_value: tensor([[-24.4172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23155970102978807, distance: 1.0031404879702415 entropy -2.817571209469912
epoch: 31, step: 15
	action: tensor([[-0.2164, -0.8271,  0.3303, -0.6629,  0.4672,  0.2322,  0.1036]],
       dtype=torch.float64)
	q_value: tensor([[-28.7035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9059771188955545, distance: 1.579848836262234 entropy -2.675223190591807
epoch: 31, step: 16
	action: tensor([[ 1.4916, -0.5314, -0.2571, -1.5766, -0.1985, -0.0222,  0.1736]],
       dtype=torch.float64)
	q_value: tensor([[-34.9786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6399688321569337, distance: 1.4654617601614899 entropy -2.4047271660126226
epoch: 31, step: 17
	action: tensor([[ 0.0192,  0.0693,  0.2663,  0.0636, -0.2441,  0.7981,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[-47.2302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4654617601614899 entropy -2.085876918447748
epoch: 31, step: 18
	action: tensor([[ 0.0992,  0.0659, -0.1126,  0.2864, -0.1474, -0.0071,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5628694005874517, distance: 0.756592962729596 entropy -3.3641942008635572
epoch: 31, step: 19
	action: tensor([[ 0.2720,  0.0659, -0.4260,  0.1931,  0.0770,  0.5334,  0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-24.7419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6745727373868547, distance: 0.6528052821010809 entropy -2.7908742292491637
epoch: 31, step: 20
	action: tensor([[ 0.0011, -0.1152, -0.0379,  0.2351,  0.0519,  0.2632,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38743511615058013, distance: 0.8956384745457775 entropy -3.3641942008635572
epoch: 31, step: 21
	action: tensor([[-0.2126,  0.3490,  0.4732, -0.9334,  0.3887,  0.5139,  0.2710]],
       dtype=torch.float64)
	q_value: tensor([[-27.7522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0678021793265211, distance: 1.1825025738920998 entropy -2.6828524360249175
epoch: 31, step: 22
	action: tensor([[ 1.2312, -1.4202, -0.2058, -0.5784, -0.5033,  0.6813,  0.0691]],
       dtype=torch.float64)
	q_value: tensor([[-31.2489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1825025738920998 entropy -2.4451038815817974
epoch: 31, step: 23
	action: tensor([[-0.0056,  0.4012,  0.0589, -0.3485, -0.1708, -0.1227,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3773079629276085, distance: 0.903011641658866 entropy -3.3641942008635572
epoch: 31, step: 24
	action: tensor([[-0.1712, -0.7094, -0.2843, -0.2612,  0.8323,  0.0019,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-21.4287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5752517028057913, distance: 1.4362553920443655 entropy -2.97874785687546
epoch: 31, step: 25
	action: tensor([[ 0.3476, -0.1601, -0.0149, -0.0986,  0.1637, -0.1474,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3545499847144621, distance: 0.9193650735352851 entropy -3.3641942008635572
epoch: 31, step: 26
	action: tensor([[-0.0773,  0.1277, -0.1768,  0.1270, -0.3157,  0.3945,  0.1480]],
       dtype=torch.float64)
	q_value: tensor([[-27.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9193650735352851 entropy -2.813901812012284
epoch: 31, step: 27
	action: tensor([[-0.3478,  0.2682, -0.0225, -0.2646, -0.1363, -0.0312,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07446519175640542, distance: 1.18618620394492 entropy -3.3641942008635572
epoch: 31, step: 28
	action: tensor([[-0.2756, -0.1201,  0.5712,  0.5459, -0.3568,  0.0141,  0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-20.3973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29659122267809557, distance: 0.9597554454647863 entropy -3.1391930633643272
epoch: 31, step: 29
	action: tensor([[ 0.2280, -1.3795,  1.1929, -0.4631, -0.1766,  0.4882,  0.5159]],
       dtype=torch.float64)
	q_value: tensor([[-29.2246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9597554454647863 entropy -2.3051104003261407
epoch: 31, step: 30
	action: tensor([[ 1.6948e-01, -3.5293e-02, -5.5412e-02,  4.1103e-05, -3.8338e-02,
         -4.9945e-02,  4.4004e-02]], dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40255264873517926, distance: 0.8845176701447335 entropy -3.3641942008635572
epoch: 31, step: 31
	action: tensor([[ 0.0212, -0.3896,  0.3000,  0.0598, -0.3948,  0.0552,  0.2068]],
       dtype=torch.float64)
	q_value: tensor([[-24.7217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0528290559344482, distance: 1.1137068137899453 entropy -2.8471977665945047
epoch: 31, step: 32
	action: tensor([[-0.3466, -0.3113,  0.0986, -0.5971,  0.7243,  0.1552,  0.4365]],
       dtype=torch.float64)
	q_value: tensor([[-30.9316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5516819347074369, distance: 1.4254698802707615 entropy -2.3469696300862126
epoch: 31, step: 33
	action: tensor([[ 0.1508,  0.0233, -0.0647,  0.2348,  0.3433, -0.0656,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-29.5382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5397348663374952, distance: 0.776355674802866 entropy -2.812720884434637
epoch: 31, step: 34
	action: tensor([[-0.2469, -0.7014,  0.0412,  0.3849,  0.1296,  0.2354,  0.1504]],
       dtype=torch.float64)
	q_value: tensor([[-25.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20987173449596086, distance: 1.25871195949883 entropy -2.9820243372993387
epoch: 31, step: 35
	action: tensor([[-0.9030, -0.5300,  0.0037, -0.1930, -0.0166,  0.0122,  0.3758]],
       dtype=torch.float64)
	q_value: tensor([[-32.8166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2042294843943298, distance: 1.6989676006370598 entropy -2.53602467858775
epoch: 31, step: 36
	action: tensor([[-0.0654, -0.0321,  0.3829, -0.1040, -0.2788,  0.4463,  0.2622]],
       dtype=torch.float64)
	q_value: tensor([[-26.7279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2720115226621117, distance: 0.9763801598611999 entropy -2.8760160184864887
epoch: 31, step: 37
	action: tensor([[ 0.5402, -0.4856,  0.9059,  0.1023,  0.8333,  0.5774,  0.4288]],
       dtype=torch.float64)
	q_value: tensor([[-30.4520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34330587580064353, distance: 0.9273384313994277 entropy -2.26035227336189
epoch: 31, step: 38
	action: tensor([[-1.7870, -1.5694,  0.4907,  1.0618,  0.1402,  0.4705,  0.1489]],
       dtype=torch.float64)
	q_value: tensor([[-46.3465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9273384313994277 entropy -2.0335496542294655
epoch: 31, step: 39
	action: tensor([[-0.1647,  0.0772,  0.1043,  0.5394, -0.0635, -0.1157,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3824626831596458, distance: 0.899266254393723 entropy -3.3641942008635572
epoch: 31, step: 40
	action: tensor([[ 0.2467,  0.1992,  0.2425,  0.0951, -0.9682,  0.2516,  0.2962]],
       dtype=torch.float64)
	q_value: tensor([[-24.1335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6587092777526911, distance: 0.6685269731512877 entropy -2.777794669775255
epoch: 31, step: 41
	action: tensor([[ 0.0817, -0.0851, -0.0844, -0.0142,  0.0920,  0.0490,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31997820391442633, distance: 0.9436655695864415 entropy -3.3641942008635572
epoch: 31, step: 42
	action: tensor([[-0.2041, -0.1618,  0.6890, -0.1953, -0.2929, -0.1455,  0.1874]],
       dtype=torch.float64)
	q_value: tensor([[-25.2914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2596330819530528, distance: 1.284336196013497 entropy -2.864790163823156
epoch: 31, step: 43
	action: tensor([[-0.7042,  0.6279,  0.7272, -0.4885,  0.5502,  1.0889,  0.4128]],
       dtype=torch.float64)
	q_value: tensor([[-28.0132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031068393084622392, distance: 1.1264275264619286 entropy -2.359506082672211
epoch: 31, step: 44
	action: tensor([[-2.0491,  0.5097, -0.5243,  0.4221, -0.7919,  0.2606,  0.1869]],
       dtype=torch.float64)
	q_value: tensor([[-33.6467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1264275264619286 entropy -2.2135760224086787
epoch: 31, step: 45
	action: tensor([[-0.1429,  0.0543, -0.0768,  0.3912, -0.2157, -0.1188,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28527957743827936, distance: 0.9674416692687298 entropy -3.3641942008635572
epoch: 31, step: 46
	action: tensor([[-0.0344,  0.4448, -0.1122, -0.4084, -0.2010,  0.3943,  0.2707]],
       dtype=torch.float64)
	q_value: tensor([[-22.6652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42327020232097556, distance: 0.8690462472659146 entropy -2.87747576167152
epoch: 31, step: 47
	action: tensor([[ 0.0848, -0.0839, -0.0134, -0.0112,  0.0860, -0.0062,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31585587637085366, distance: 0.946521522806424 entropy -3.3641942008635572
epoch: 31, step: 48
	action: tensor([[-0.5230,  0.1202,  0.2367, -0.1920,  0.1993,  0.2183,  0.1945]],
       dtype=torch.float64)
	q_value: tensor([[-25.2625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28276215366816393, distance: 1.2960738920361954 entropy -2.8418063572774734
epoch: 31, step: 49
	action: tensor([[-1.1253, -0.0297,  0.2350, -0.5500,  0.1128,  0.0918,  0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-24.0229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3635419946894607, distance: 1.7592937297021207 entropy -2.819847898231211
epoch: 31, step: 50
	action: tensor([[-0.1169,  0.0529,  0.1760, -0.2989, -0.1155,  0.2355,  0.1919]],
       dtype=torch.float64)
	q_value: tensor([[-25.9780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1039208158855296, distance: 1.0832529680966125 entropy -3.0048687004738817
epoch: 31, step: 51
	action: tensor([[-0.1044,  0.0155, -0.1177, -0.7210, -0.3101,  0.1086,  0.2711]],
       dtype=torch.float64)
	q_value: tensor([[-25.8846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05934389167249576, distance: 1.1778098354619613 entropy -2.594587237434581
epoch: 31, step: 52
	action: tensor([[ 0.3802, -0.2901,  0.0053, -0.8040,  0.5444, -0.0278,  0.2027]],
       dtype=torch.float64)
	q_value: tensor([[-26.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.125659058892148, distance: 1.2141158496646283 entropy -2.7741283741315415
epoch: 31, step: 53
	action: tensor([[ 0.1681, -0.9834, -0.5406, -0.1611,  0.5477,  0.1958, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[-31.2249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4908344018815465, distance: 1.3972412421899814 entropy -2.653178774191265
epoch: 31, step: 54
	action: tensor([[-0.0079,  0.1038, -0.0223, -0.0472,  0.1241,  0.2459,  0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-35.7334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.403957853158012, distance: 0.8834768588947434 entropy -2.774417438703707
epoch: 31, step: 55
	action: tensor([[-0.3503,  0.2038,  0.2897, -0.1063, -0.2320,  0.3625,  0.1829]],
       dtype=torch.float64)
	q_value: tensor([[-24.7869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06355758621536456, distance: 1.1073814165156861 entropy -2.8100045558780424
epoch: 31, step: 56
	action: tensor([[ 0.4609,  0.9715,  0.1289, -0.1858,  0.0619,  0.4511,  0.3642]],
       dtype=torch.float64)
	q_value: tensor([[-25.4903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1073814165156861 entropy -2.4920273492679987
epoch: 31, step: 57
	action: tensor([[-0.2347, -0.3886, -0.2554,  0.0553,  0.1609,  0.1437,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26415551332519005, distance: 1.286639691513497 entropy -3.3641942008635572
epoch: 31, step: 58
	action: tensor([[ 0.5931, -0.2050, -0.0393,  0.0411, -0.2382,  0.1251,  0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-26.2348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5844165865288207, distance: 0.737710213464126 entropy -2.991557867133382
epoch: 31, step: 59
	action: tensor([[ 0.4647, -0.1629,  0.1847, -0.9300,  0.0774,  0.0214,  0.2651]],
       dtype=torch.float64)
	q_value: tensor([[-33.3718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05522179940161953, distance: 1.1755160699425733 entropy -2.4161361405396877
epoch: 31, step: 60
	action: tensor([[ 0.3460, -0.5000, -0.0544,  0.1551, -0.3803,  0.2731,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-32.5731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27137361132828863, distance: 0.9768078504119614 entropy -2.4387592568374226
epoch: 31, step: 61
	action: tensor([[-0.8278,  0.0137,  0.4390,  1.2121,  0.2145,  1.4480,  0.4120]],
       dtype=torch.float64)
	q_value: tensor([[-35.2809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9768078504119614 entropy -2.3086486417821575
epoch: 31, step: 62
	action: tensor([[-0.1980,  0.1560,  0.2659,  0.1025, -0.4190,  0.1448,  0.0440]],
       dtype=torch.float64)
	q_value: tensor([[-27.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22534129963608807, distance: 1.0071911353124419 entropy -3.3641942008635572
epoch: 31, step: 63
	action: tensor([[ 0.1612,  0.4108,  0.0117, -0.3184,  0.2454,  0.4358,  0.3859]],
       dtype=torch.float64)
	q_value: tensor([[-24.9086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6548839024752159, distance: 0.6722631431903787 entropy -2.530889563453252
LOSS epoch 31 actor 358.4799357568176 critic 809.4047583369957
epoch: 32, step: 0
	action: tensor([[ 0.1982, -0.0705, -0.1250,  0.1595,  0.0455,  0.0406,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.538755195184089, distance: 0.7771814694538279 entropy -3.3274933347734397
epoch: 32, step: 1
	action: tensor([[ 0.4767, -0.3670,  0.2133, -0.2871,  0.0598,  0.1116,  0.3055]],
       dtype=torch.float64)
	q_value: tensor([[-27.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17076779697161582, distance: 1.0420649126105321 entropy -2.790191664226051
epoch: 32, step: 2
	action: tensor([[-0.4864,  0.2074,  0.3187, -0.6272,  1.2993,  0.8773,  0.3988]],
       dtype=torch.float64)
	q_value: tensor([[-34.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08980648506650923, distance: 1.1946244174245275 entropy -2.3674808831405008
epoch: 32, step: 3
	action: tensor([[ 0.7000,  0.5745, -0.4329, -0.1140,  0.3717, -0.0862,  0.0979]],
       dtype=torch.float64)
	q_value: tensor([[-35.7403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1946244174245275 entropy -2.583093180304251
epoch: 32, step: 4
	action: tensor([[ 0.2171,  0.0219, -0.1037,  0.1121,  0.2238, -0.1300,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5291873177299071, distance: 0.7852008656598763 entropy -3.3274933347734397
epoch: 32, step: 5
	action: tensor([[-0.4001, -0.2496,  0.0343, -0.2792, -0.0140, -0.1969,  0.2256]],
       dtype=torch.float64)
	q_value: tensor([[-24.7126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5103984123087526, distance: 1.40637926093927 entropy -2.986157268083798
epoch: 32, step: 6
	action: tensor([[ 0.3905,  0.0712, -0.1550, -0.1891,  0.5976,  0.4238,  0.2882]],
       dtype=torch.float64)
	q_value: tensor([[-23.7073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7166812199027359, distance: 0.6091081000084282 entropy -2.976077147922311
epoch: 32, step: 7
	action: tensor([[ 0.0814, -0.1365, -0.1477,  0.0631,  0.3609, -0.3536,  0.1663]],
       dtype=torch.float64)
	q_value: tensor([[-30.6492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2089685345560327, distance: 1.017779204018312 entropy -2.7139158592950987
epoch: 32, step: 8
	action: tensor([[ 0.4262, -0.2264,  0.0807,  0.3558, -0.2051, -0.0802,  0.1720]],
       dtype=torch.float64)
	q_value: tensor([[-23.6173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6418405078797951, distance: 0.6848491578282692 entropy -3.1992870392192505
epoch: 32, step: 9
	action: tensor([[-0.0210,  0.2417, -0.1189,  0.1849,  0.3898, -0.1873,  0.4343]],
       dtype=torch.float64)
	q_value: tensor([[-31.9839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6848491578282692 entropy -2.480179585895995
epoch: 32, step: 10
	action: tensor([[-0.0664, -0.2381, -0.0266,  0.0868, -0.1526,  0.1493,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09850890030558257, distance: 1.0865192244006976 entropy -3.3274933347734397
epoch: 32, step: 11
	action: tensor([[-0.2051, -0.7433,  0.6423,  0.3943, -0.4508,  0.4142,  0.4183]],
       dtype=torch.float64)
	q_value: tensor([[-27.6368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.041487283873245406, distance: 1.1678408950351733 entropy -2.643094818694737
epoch: 32, step: 12
	action: tensor([[ 0.7743,  0.1013,  0.4804, -0.1504,  0.7917,  0.9504,  0.9022]],
       dtype=torch.float64)
	q_value: tensor([[-40.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8428227334139341, distance: 0.4536820108395713 entropy -1.9642819492431713
epoch: 32, step: 13
	action: tensor([[ 0.3619, -0.5991,  0.7414,  0.7099,  0.1270,  0.5916,  0.2982]],
       dtype=torch.float64)
	q_value: tensor([[-45.1055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7209243489180153, distance: 0.6045297339123501 entropy -2.023223144538234
epoch: 32, step: 14
	action: tensor([[ 0.6659,  0.1742, -0.3648, -1.3711,  0.8732,  0.6833,  0.7199]],
       dtype=torch.float64)
	q_value: tensor([[-46.6378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46252903796780753, distance: 0.8389463638157323 entropy -1.9866775516008186
epoch: 32, step: 15
	action: tensor([[ 0.2534,  0.1966, -0.6244, -0.6073,  0.4526, -0.0704,  0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-38.7920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5598187308688075, distance: 0.7592284476809364 entropy -2.423539008662209
epoch: 32, step: 16
	action: tensor([[-0.1107, -0.0426, -0.1586, -0.3990, -0.1448,  0.0327,  0.0657]],
       dtype=torch.float64)
	q_value: tensor([[-23.6527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01130678237246907, distance: 1.1507954952816695 entropy -3.426945707724282
epoch: 32, step: 17
	action: tensor([[ 0.0997, -0.1338, -0.1870, -0.2239, -0.0759,  0.0692,  0.2750]],
       dtype=torch.float64)
	q_value: tensor([[-24.4624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.194265678168288, distance: 1.0271943575016584 entropy -2.9636806349129934
epoch: 32, step: 18
	action: tensor([[-0.3876, -0.0969, -0.1698,  0.1827,  0.3809,  0.1451,  0.3076]],
       dtype=torch.float64)
	q_value: tensor([[-26.8943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13156202859283828, distance: 1.2172951061896706 entropy -2.7669721380241943
epoch: 32, step: 19
	action: tensor([[-0.0066,  0.4473, -0.2359,  0.0234,  0.1748,  0.1133,  0.2064]],
       dtype=torch.float64)
	q_value: tensor([[-24.5509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2172951061896706 entropy -3.107485629469237
epoch: 32, step: 20
	action: tensor([[-0.1370,  0.0627, -0.0076, -0.1369,  0.1023, -0.1250,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06863107307701122, distance: 1.1043775396094138 entropy -3.3274933347734397
epoch: 32, step: 21
	action: tensor([[-0.1172, -0.3065,  0.0881, -0.4746,  0.1441, -0.1217,  0.2264]],
       dtype=torch.float64)
	q_value: tensor([[-22.2893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3959539552325517, distance: 1.352048404652643 entropy -3.09709737729288
epoch: 32, step: 22
	action: tensor([[ 0.1978,  0.1729, -0.1059,  0.0364,  0.1357,  0.0015,  0.2808]],
       dtype=torch.float64)
	q_value: tensor([[-26.6131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5840170698288597, distance: 0.7380647232221993 entropy -2.8137767830721736
epoch: 32, step: 23
	action: tensor([[-0.1871, -0.3635, -0.1247, -0.1426, -0.2320,  0.1781,  0.2351]],
       dtype=torch.float64)
	q_value: tensor([[-24.9032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2731637299449099, distance: 1.2912157718916533 entropy -2.8631779908592843
epoch: 32, step: 24
	action: tensor([[ 0.9452, -0.9972,  0.1264,  0.3366, -0.8273,  0.1698,  0.4328]],
       dtype=torch.float64)
	q_value: tensor([[-28.2570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0008308143765234366, distance: 1.1448195241341375 entropy -2.606110753598472
epoch: 32, step: 25
	action: tensor([[0.7275, 0.7536, 0.0680, 0.0101, 0.5595, 0.3395, 0.7876]],
       dtype=torch.float64)
	q_value: tensor([[-51.1316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9559534065353822, distance: 0.24016673625355578 entropy -1.938533526236096
epoch: 32, step: 26
	action: tensor([[-0.0392, -0.6580, -1.1065, -0.7954, -0.7960,  0.1024,  0.1668]],
       dtype=torch.float64)
	q_value: tensor([[-33.7845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08771507506663245, distance: 1.1934775857115039 entropy -2.3980883739170755
epoch: 32, step: 27
	action: tensor([[-0.8604,  0.1029,  0.1365, -0.4069,  0.0060,  0.2035,  0.3190]],
       dtype=torch.float64)
	q_value: tensor([[-34.7670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9140070154621236, distance: 1.58317329622409 entropy -2.902413281844826
epoch: 32, step: 28
	action: tensor([[-0.6015, -0.0160,  0.1234, -0.5772, -0.1595,  0.0340,  0.3114]],
       dtype=torch.float64)
	q_value: tensor([[-25.4426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7430588447465278, distance: 1.5108199735913954 entropy -2.8637030084615778
epoch: 32, step: 29
	action: tensor([[ 0.0141, -0.2768, -0.0151,  0.2627, -0.1361,  0.1446,  0.3373]],
       dtype=torch.float64)
	q_value: tensor([[-25.5700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2359961714744564, distance: 1.0002405586135774 entropy -2.8175353401283885
epoch: 32, step: 30
	action: tensor([[ 0.6992, -1.0315,  0.5190, -0.0732, -0.6310,  0.1624,  0.4409]],
       dtype=torch.float64)
	q_value: tensor([[-29.8211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3940398799573164, distance: 1.3511211498048887 entropy -2.5246221677734226
epoch: 32, step: 31
	action: tensor([[ 0.9192, -0.5356,  1.3359, -0.3414, -0.4499, -0.8262,  0.8224]],
       dtype=torch.float64)
	q_value: tensor([[-48.6710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08137746684042169, distance: 1.096794440338563 entropy -1.905622582260752
epoch: 32, step: 32
	action: tensor([[-0.8664,  0.4180,  0.2621,  0.5396, -0.2391,  0.7550,  0.5922]],
       dtype=torch.float64)
	q_value: tensor([[-48.6219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.096794440338563 entropy -1.9353608587456972
epoch: 32, step: 33
	action: tensor([[ 0.0912, -0.1459, -0.2116, -0.1642, -0.1919, -0.0761,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19206593427461383, distance: 1.0285955789344914 entropy -3.3274933347734397
epoch: 32, step: 34
	action: tensor([[ 0.0591, -0.2047, -0.2802, -0.3579, -0.2887,  0.2326,  0.2947]],
       dtype=torch.float64)
	q_value: tensor([[-25.2323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07612817302140618, distance: 1.09992368798841 entropy -2.900094751345733
epoch: 32, step: 35
	action: tensor([[-0.4271, -0.6044, -0.2087, -0.2941, -0.0780,  0.6073,  0.3899]],
       dtype=torch.float64)
	q_value: tensor([[-29.0880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6347646071731627, distance: 1.463134687636884 entropy -2.6094328218053655
epoch: 32, step: 36
	action: tensor([[ 0.7910, -0.5320,  0.3969, -0.4585, -0.6357,  0.4169,  0.4926]],
       dtype=torch.float64)
	q_value: tensor([[-33.3928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07492393215133364, distance: 1.1006403142917276 entropy -2.449270812532368
epoch: 32, step: 37
	action: tensor([[ 2.2120,  0.0276,  0.6426,  0.4507, -0.7718,  0.2982,  0.7603]],
       dtype=torch.float64)
	q_value: tensor([[-45.8842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1006403142917276 entropy -1.9037011076822505
epoch: 32, step: 38
	action: tensor([[ 0.2531, -0.1080, -0.0941, -0.3798, -0.0669, -0.0457,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23811229792851207, distance: 0.9988543723687507 entropy -3.3274933347734397
epoch: 32, step: 39
	action: tensor([[ 0.4583, -0.0786,  0.2574, -1.1323, -0.0312,  0.4562,  0.2911]],
       dtype=torch.float64)
	q_value: tensor([[-26.8039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06490962175916581, distance: 1.106581709120274 entropy -2.7943907151056844
epoch: 32, step: 40
	action: tensor([[-0.7850, -0.4157,  1.0164,  0.0605, -0.7610,  0.8404,  0.4325]],
       dtype=torch.float64)
	q_value: tensor([[-37.2156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5615781299032354, distance: 1.4300082805414283 entropy -2.1875482846405094
epoch: 32, step: 41
	action: tensor([[ 0.4334, -0.0539, -0.9170, -0.0210,  0.5929, -0.0154,  1.2195]],
       dtype=torch.float64)
	q_value: tensor([[-42.7136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5930934917702764, distance: 0.7299683180598556 entropy -1.7215659105936851
epoch: 32, step: 42
	action: tensor([[ 0.1944, -0.2976, -0.4969, -0.0190,  0.0406,  0.4131,  0.0750]],
       dtype=torch.float64)
	q_value: tensor([[-33.3434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917811272854339, distance: 0.963031385449076 entropy -2.9044275472999668
epoch: 32, step: 43
	action: tensor([[-0.6922, -0.4904, -0.5730, -0.4430, -0.1371,  0.1202,  0.3279]],
       dtype=torch.float64)
	q_value: tensor([[-30.5538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8211948932622608, distance: 1.5443114988750994 entropy -2.7383504121516586
epoch: 32, step: 44
	action: tensor([[ 0.1618, -0.1681,  0.2042, -0.2043,  0.5873, -0.0285,  0.2148]],
       dtype=torch.float64)
	q_value: tensor([[-28.1383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17006344372763627, distance: 1.042507385782295 entropy -3.2478538558301246
epoch: 32, step: 45
	action: tensor([[-0.3338,  0.1171,  0.1733, -0.7338,  0.3693,  0.2189,  0.1990]],
       dtype=torch.float64)
	q_value: tensor([[-28.7595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2956853160240229, distance: 1.3025861667958503 entropy -2.7486563940990583
epoch: 32, step: 46
	action: tensor([[-0.3726,  0.4371,  0.1206, -0.3364, -0.5128, -0.0250,  0.2048]],
       dtype=torch.float64)
	q_value: tensor([[-27.6520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3025861667958503 entropy -2.862474630270313
epoch: 32, step: 47
	action: tensor([[ 0.0826, -0.1933,  0.2101, -0.0399, -0.1473, -0.1223,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1759847696266884, distance: 1.0387817539724422 entropy -3.3274933347734397
epoch: 32, step: 48
	action: tensor([[-1.0426, -0.2651,  0.2050, -0.5695,  0.0190,  0.2055,  0.3996]],
       dtype=torch.float64)
	q_value: tensor([[-27.1898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3335816214155938, distance: 1.7481077216119087 entropy -2.6435184249187773
epoch: 32, step: 49
	action: tensor([[ 0.0570, -0.1360,  0.2079, -0.2991,  0.7371,  0.0490,  0.3812]],
       dtype=torch.float64)
	q_value: tensor([[-28.7771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08717238293162632, distance: 1.0933295312856768 entropy -2.7096033484698636
epoch: 32, step: 50
	action: tensor([[-0.1030, -0.0717, -0.2428,  0.2421, -0.4562,  0.2068,  0.1569]],
       dtype=torch.float64)
	q_value: tensor([[-29.2182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.224120431400884, distance: 1.0079844934462203 entropy -2.73668062998992
epoch: 32, step: 51
	action: tensor([[-0.8462, -0.2997,  0.0610,  1.0537,  0.0995,  0.2301,  0.4380]],
       dtype=torch.float64)
	q_value: tensor([[-27.3221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14580798381360993, distance: 1.2249337910944271 entropy -2.607423301563476
epoch: 32, step: 52
	action: tensor([[-0.1670, -0.1267,  0.1760,  0.1236, -0.1041,  0.4121,  0.5085]],
       dtype=torch.float64)
	q_value: tensor([[-33.2294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22952559933622807, distance: 1.0044672932091272 entropy -2.5471527353063825
epoch: 32, step: 53
	action: tensor([[ 0.4155, -0.0651, -0.6222,  0.4959, -0.2083,  0.4283,  0.5132]],
       dtype=torch.float64)
	q_value: tensor([[-30.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0044672932091272 entropy -2.338243345835078
epoch: 32, step: 54
	action: tensor([[ 0.0174,  0.3910, -0.0999,  0.1977, -0.1744, -0.2122,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5509499733909611, distance: 0.766838760438175 entropy -3.3274933347734397
epoch: 32, step: 55
	action: tensor([[ 0.6230, -0.1254,  0.0184,  0.1045,  0.2921,  0.1423,  0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-20.8464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7113796958027389, distance: 0.6147805700661553 entropy -3.0359966103783043
epoch: 32, step: 56
	action: tensor([[-0.2475, -0.1489, -0.0984, -0.1030,  0.2045, -0.1381,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-28.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21988449000364452, distance: 1.2639097033975741 entropy -3.3274933347734397
epoch: 32, step: 57
	action: tensor([[ 0.2449,  0.1822,  0.2336, -0.1259,  0.3169,  0.1248,  0.2080]],
       dtype=torch.float64)
	q_value: tensor([[-22.6580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5857271371569686, distance: 0.7365461030022217 entropy -3.2087358588780015
epoch: 32, step: 58
	action: tensor([[-0.5227,  0.0223, -0.6921, -0.7384,  0.0567,  0.5200,  0.2583]],
       dtype=torch.float64)
	q_value: tensor([[-27.9012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23531265985725436, distance: 1.2718770748461101 entropy -2.6393668932206684
epoch: 32, step: 59
	action: tensor([[0.5579, 0.2918, 0.2489, 0.0323, 0.0536, 0.0449, 0.1345]],
       dtype=torch.float64)
	q_value: tensor([[-27.9822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8748702623462794, distance: 0.4047966971202691 entropy -3.3971903566188666
epoch: 32, step: 60
	action: tensor([[ 0.3283, -0.4902, -0.7167, -0.0818, -0.1224, -0.0360,  0.3311]],
       dtype=torch.float64)
	q_value: tensor([[-30.1826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08037563618705346, distance: 1.0973923479837773 entropy -2.4938118425227933
epoch: 32, step: 61
	action: tensor([[ 0.0878, -0.2652, -0.4044, -0.5458,  0.0688,  0.2245,  0.2470]],
       dtype=torch.float64)
	q_value: tensor([[-30.2641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014165459091580512, distance: 1.1362102649799792 entropy -2.910243224234031
epoch: 32, step: 62
	action: tensor([[-0.0866, -0.1134,  0.2313, -0.1978,  0.5487,  0.1344,  0.2468]],
       dtype=torch.float64)
	q_value: tensor([[-28.7741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005694823780597935, distance: 1.1410811822770073 entropy -2.876453895410152
epoch: 32, step: 63
	action: tensor([[-0.1341, -0.1624,  0.1056, -0.7892,  0.0686, -0.0891,  0.2233]],
       dtype=torch.float64)
	q_value: tensor([[-27.8170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41339617864394596, distance: 1.3604689980707505 entropy -2.7535120445559294
LOSS epoch 32 actor 448.5660895234745 critic 290.1604969324741
epoch: 33, step: 0
	action: tensor([[-0.1178, -0.8378,  0.5421, -0.9922,  0.7303,  0.0289,  0.4898]],
       dtype=torch.float64)
	q_value: tensor([[-28.9001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0488786004544144, distance: 1.6380034287345975 entropy -2.66684121871391
epoch: 33, step: 1
	action: tensor([[-0.0426,  0.4429,  0.8356, -0.0644, -0.2591,  0.2990,  0.6961]],
       dtype=torch.float64)
	q_value: tensor([[-38.7487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6380034287345975 entropy -2.188012539665113
epoch: 33, step: 2
	action: tensor([[-0.2424, -0.0833, -0.1110,  0.1615,  0.2263, -0.0041, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01684172730079503, distance: 1.1539403857734896 entropy -3.2559069664939413
epoch: 33, step: 3
	action: tensor([[-0.1820, -0.2720, -0.3653,  0.0482,  0.4334,  0.1495,  0.3685]],
       dtype=torch.float64)
	q_value: tensor([[-24.8833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10437758196897517, distance: 1.2025841743844268 entropy -2.9674486653940524
epoch: 33, step: 4
	action: tensor([[-0.6768, -0.1848, -0.1391,  0.0156, -0.2004,  0.4258,  0.3112]],
       dtype=torch.float64)
	q_value: tensor([[-27.5708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6565499983334213, distance: 1.4728515206108233 entropy -3.012283166036726
epoch: 33, step: 5
	action: tensor([[ 1.1525, -0.0699, -0.3011, -0.9933, -0.5567,  0.5132,  0.6284]],
       dtype=torch.float64)
	q_value: tensor([[-28.9303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1929394818083764, distance: 1.0280393639070646 entropy -2.541481738717042
epoch: 33, step: 6
	action: tensor([[-2.2293, -0.2869, -0.3478, -0.6097,  1.0283,  0.9880,  1.0345]],
       dtype=torch.float64)
	q_value: tensor([[-45.5256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0280393639070646 entropy -1.9152783116046632
epoch: 33, step: 7
	action: tensor([[-0.5297,  0.2133, -0.1165,  0.2769, -0.0219, -0.0985, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15012867821789122, distance: 1.2272411517698805 entropy -3.2559069664939413
epoch: 33, step: 8
	action: tensor([[ 0.2608, -0.1134, -0.0350, -0.2706,  0.0123,  0.3185,  0.3360]],
       dtype=torch.float64)
	q_value: tensor([[-22.0255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3608294771569711, distance: 0.9148819562157172 entropy -3.051704426230556
epoch: 33, step: 9
	action: tensor([[ 0.5608, -0.3690, -0.3323, -1.6393,  0.1281,  0.7815,  0.6436]],
       dtype=torch.float64)
	q_value: tensor([[-31.7584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13366301581596463, distance: 1.2184246666355558 entropy -2.375427880749489
epoch: 33, step: 10
	action: tensor([[ 1.2378,  0.6483, -0.2447,  0.9254, -0.0449,  1.3053,  0.8083]],
       dtype=torch.float64)
	q_value: tensor([[-44.8411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6455818617220386, distance: 0.681262781299162 entropy -2.0575237963422706
epoch: 33, step: 11
	action: tensor([[ 1.0243, -1.4276, -1.6767, -1.1206, -1.0529, -0.2196,  1.0197]],
       dtype=torch.float64)
	q_value: tensor([[-52.3797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.681262781299162 entropy -1.8147764117187066
epoch: 33, step: 12
	action: tensor([[ 0.3076,  0.1019,  0.0527,  0.4245, -0.2088,  0.0320, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7288229367989894, distance: 0.595913425416338 entropy -3.2559069664939413
epoch: 33, step: 13
	action: tensor([[ 0.4344,  0.3897,  0.6780, -0.1126, -0.3939,  0.1307,  0.6382]],
       dtype=torch.float64)
	q_value: tensor([[-30.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7824069690022971, distance: 0.5338007555630246 entropy -2.459906091921346
epoch: 33, step: 14
	action: tensor([[-0.0550, -0.2944, -0.3139, -0.1935,  0.0416,  0.2360, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034939487122658264, distance: 1.1641640179200026 entropy -3.2559069664939413
epoch: 33, step: 15
	action: tensor([[ 0.9472, -0.6082,  0.6361,  0.0897,  0.2891,  0.3402,  0.4745]],
       dtype=torch.float64)
	q_value: tensor([[-28.3360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26592661765267345, distance: 0.9804522141995252 entropy -2.7553902991324186
epoch: 33, step: 16
	action: tensor([[ 1.0894, -0.4964,  1.3860,  0.3872,  1.3525,  1.3457,  1.0177]],
       dtype=torch.float64)
	q_value: tensor([[-47.9352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007729386932974869, distance: 1.1487582807501004 entropy -1.8987331484261685
epoch: 33, step: 17
	action: tensor([[-0.6454, -1.3292,  2.0511, -0.2695, -0.6131,  0.3369,  1.2543]],
       dtype=torch.float64)
	q_value: tensor([[-65.9119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1487582807501004 entropy -1.5385615473238026
epoch: 33, step: 18
	action: tensor([[ 0.3839, -0.2314,  0.0089,  0.2806, -0.0303,  0.3867, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7095910937829335, distance: 0.6166825489616566 entropy -3.2559069664939413
epoch: 33, step: 19
	action: tensor([[-0.0747,  0.7113,  0.8325, -0.3440,  0.5710,  0.2110,  0.7952]],
       dtype=torch.float64)
	q_value: tensor([[-35.6525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6166825489616566 entropy -2.2491606729725246
epoch: 33, step: 20
	action: tensor([[-0.4461, -0.2793, -0.2131,  0.0247,  0.0336, -0.0489, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4945946727387349, distance: 1.399002234791632 entropy -3.2559069664939413
epoch: 33, step: 21
	action: tensor([[ 0.2191, -0.2577,  0.2396, -0.1628,  0.0481,  0.2376,  0.3804]],
       dtype=torch.float64)
	q_value: tensor([[-24.6509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2265419024087132, distance: 1.0064103363841652 entropy -2.9928915821606275
epoch: 33, step: 22
	action: tensor([[ 0.1936, -0.1939,  0.3028, -1.0009, -0.7872,  0.7296,  0.7372]],
       dtype=torch.float64)
	q_value: tensor([[-33.2289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18480326682763337, distance: 1.2456034853253055 entropy -2.2588996627605478
epoch: 33, step: 23
	action: tensor([[-0.7653,  0.5799,  1.1155, -1.0544,  0.4263,  0.3466,  1.3583]],
       dtype=torch.float64)
	q_value: tensor([[-43.7135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8952959179021434, distance: 1.5754158367962265 entropy -1.7666429714108882
epoch: 33, step: 24
	action: tensor([[ 1.0946,  0.0332,  0.2292, -1.8297,  0.3565,  1.8427,  0.9379]],
       dtype=torch.float64)
	q_value: tensor([[-40.3415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4656013228254441, distance: 0.8365451400284651 entropy -1.891838252729799
epoch: 33, step: 25
	action: tensor([[-0.2795, -2.1654,  1.4634, -3.6709,  0.9389, -0.1055,  1.3521]],
       dtype=torch.float64)
	q_value: tensor([[-58.4323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8365451400284651 entropy -1.5287503478107693
epoch: 33, step: 26
	action: tensor([[ 0.2988,  0.0528,  0.1270,  0.1875,  0.4232,  0.1726, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7496322408445186, distance: 0.5725928154212673 entropy -3.2559069664939413
epoch: 33, step: 27
	action: tensor([[-0.1523, -0.3221, -0.4227, -0.3227,  0.1821,  0.4636,  0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-30.5534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07750784201401029, distance: 1.1878645264803764 entropy -2.575015648945594
epoch: 33, step: 28
	action: tensor([[-0.0354, -0.4798, -0.1109, -0.0450, -0.2788,  0.2112, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1513696536517779, distance: 1.2279030627163476 entropy -3.2559069664939413
epoch: 33, step: 29
	action: tensor([[-1.6010, -0.3112, -0.1657,  0.7785, -0.2155,  0.8226,  0.7642]],
       dtype=torch.float64)
	q_value: tensor([[-31.3379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9946446874805646, distance: 1.6161790229664377 entropy -2.370790731124337
epoch: 33, step: 30
	action: tensor([[ 0.5102,  0.2437,  0.3928, -0.2904,  0.8544,  0.7157,  0.9238]],
       dtype=torch.float64)
	q_value: tensor([[-42.1745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6161790229664377 entropy -2.2341700709247996
epoch: 33, step: 31
	action: tensor([[ 0.0427,  0.0542, -0.2294,  0.0373,  0.3602, -0.1032, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33836605542938025, distance: 0.9308197347254004 entropy -3.2559069664939413
epoch: 33, step: 32
	action: tensor([[ 0.3447,  0.0051, -0.0747, -0.2490, -0.2355,  0.3000,  0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-23.9219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5305720948107209, distance: 0.7840452798960272 entropy -3.155260867013204
epoch: 33, step: 33
	action: tensor([[-1.1273,  0.3149,  0.1860,  0.0671, -0.9910,  0.5974,  0.7000]],
       dtype=torch.float64)
	q_value: tensor([[-31.6684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8125877222007212, distance: 1.5406578823496155 entropy -2.3241309017666407
epoch: 33, step: 34
	action: tensor([[-1.5422, -0.1667, -0.3242,  1.6859, -1.1649,  0.3355,  1.1771]],
       dtype=torch.float64)
	q_value: tensor([[-35.3492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37028832030248604, distance: 1.3395615437403452 entropy -1.9861164437917316
epoch: 33, step: 35
	action: tensor([[ 0.8660, -1.3866, -0.4011, -2.6888, -0.1982,  1.0910,  1.2883]],
       dtype=torch.float64)
	q_value: tensor([[-50.1342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3395615437403452 entropy -1.9456764706970795
epoch: 33, step: 36
	action: tensor([[ 0.0784, -0.8397, -0.0676,  0.5226, -0.2386,  0.1215, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.045246490602779854, distance: 1.1699466341211786 entropy -3.2559069664939413
epoch: 33, step: 37
	action: tensor([[ 1.4335,  0.1436, -0.0202, -0.0471,  1.2307,  0.2466,  0.9722]],
       dtype=torch.float64)
	q_value: tensor([[-36.6125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5830030573501426, distance: 0.7389637400513671 entropy -2.184570956480399
epoch: 33, step: 38
	action: tensor([[ 0.1423, -0.7918, -1.3260,  0.3783, -0.3889,  0.4476,  0.4943]],
       dtype=torch.float64)
	q_value: tensor([[-46.3276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5002916827087207, distance: 1.401666016900987 entropy -2.1388997600728517
epoch: 33, step: 39
	action: tensor([[ 0.3942, -0.1077, -0.5416, -0.1511, -0.5090,  0.4037,  0.6111]],
       dtype=torch.float64)
	q_value: tensor([[-41.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4833043967023384, distance: 0.8225722917968659 entropy -2.5349169952443766
epoch: 33, step: 40
	action: tensor([[-0.7590, -0.4364,  0.0869, -0.3701, -1.4892,  0.8944,  0.7434]],
       dtype=torch.float64)
	q_value: tensor([[-35.3685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1691689252789508, distance: 1.6854015166734215 entropy -2.266860668391462
epoch: 33, step: 41
	action: tensor([[-3.6641,  0.7567, -1.0899, -0.5393, -0.4510,  1.2000,  1.6719]],
       dtype=torch.float64)
	q_value: tensor([[-47.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6854015166734215 entropy -1.677587190396226
epoch: 33, step: 42
	action: tensor([[-0.0055,  0.0943, -0.0336, -0.2694,  0.1830, -0.1196, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2000798974598439, distance: 1.0234815043308267 entropy -3.2559069664939413
epoch: 33, step: 43
	action: tensor([[-0.7798,  0.2803,  0.2827, -0.8707,  0.0762,  0.3263,  0.3354]],
       dtype=torch.float64)
	q_value: tensor([[-24.3202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8337591139262421, distance: 1.5496293592952337 entropy -2.9793878471866315
epoch: 33, step: 44
	action: tensor([[-0.2458,  0.0420, -0.5216, -0.4592,  0.5057,  0.2093,  0.5326]],
       dtype=torch.float64)
	q_value: tensor([[-30.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06690570955458552, distance: 1.1053999975308118 entropy -2.5877476752305872
epoch: 33, step: 45
	action: tensor([[-0.3307,  0.1458,  0.1553,  0.1340, -0.0989,  0.2467,  0.1666]],
       dtype=torch.float64)
	q_value: tensor([[-27.5260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1053999975308118 entropy -3.2768775682524565
epoch: 33, step: 46
	action: tensor([[-0.1705,  0.0735, -0.0559, -0.3079,  0.3960,  0.2213, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07119751140352315, distance: 1.102854903419606 entropy -3.2559069664939413
epoch: 33, step: 47
	action: tensor([[-0.3498,  0.0487,  0.5064, -0.4682, -0.2641,  0.0455,  0.3234]],
       dtype=torch.float64)
	q_value: tensor([[-26.1090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43577283462164673, distance: 1.3711960682720588 entropy -2.960091231335528
epoch: 33, step: 48
	action: tensor([[ 0.1926,  0.1794,  0.4692,  1.1587, -0.5600,  0.6448,  0.7841]],
       dtype=torch.float64)
	q_value: tensor([[-29.3619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3711960682720588 entropy -2.2931703945919226
epoch: 33, step: 49
	action: tensor([[-0.2954, -0.1773,  0.1567, -0.3850, -0.0383, -0.1749, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4627276871614574, distance: 1.3840074702859446 entropy -3.2559069664939413
epoch: 33, step: 50
	action: tensor([[ 0.0369,  0.7333, -0.0778, -0.4440,  0.4631,  0.0694,  0.4994]],
       dtype=torch.float64)
	q_value: tensor([[-25.5596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3840074702859446 entropy -2.7411457957517804
epoch: 33, step: 51
	action: tensor([[-0.2829, -0.1786,  0.1053, -0.2493,  0.0484,  0.2602, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26379861273970917, distance: 1.286458054495808 entropy -3.2559069664939413
epoch: 33, step: 52
	action: tensor([[-0.4566,  0.7591,  0.1434, -0.0144,  0.4829,  0.0520,  0.5750]],
       dtype=torch.float64)
	q_value: tensor([[-27.7723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03298697740105194, distance: 1.1253117526271708 entropy -2.590281654456774
epoch: 33, step: 53
	action: tensor([[ 0.0724, -0.4070,  0.0856, -0.2447,  0.0099,  0.0522,  0.3059]],
       dtype=torch.float64)
	q_value: tensor([[-25.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30093671628466523, distance: 1.3052231822197053 entropy -2.8300017743275214
epoch: 33, step: 54
	action: tensor([[-0.6956,  0.0405, -0.6710, -0.2763,  1.1523,  0.6646,  0.6632]],
       dtype=torch.float64)
	q_value: tensor([[-30.8909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3052231822197053 entropy -2.4160151846351368
epoch: 33, step: 55
	action: tensor([[-0.5380, -0.2091, -0.0926, -0.0608,  0.2810, -0.0916, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.610456519939349, distance: 1.452215926124607 entropy -3.2559069664939413
epoch: 33, step: 56
	action: tensor([[ 0.6700, -0.0922,  0.0032, -0.0338,  0.1840,  0.2872,  0.3040]],
       dtype=torch.float64)
	q_value: tensor([[-24.2574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.714939688307422, distance: 0.6109772946018884 entropy -3.1299793935173548
epoch: 33, step: 57
	action: tensor([[ 0.7722, -0.0302, -0.3461, -0.7379,  0.4313,  0.6345,  0.6424]],
       dtype=torch.float64)
	q_value: tensor([[-35.3252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5542943376642703, distance: 0.763977853077796 entropy -2.302148466694392
epoch: 33, step: 58
	action: tensor([[-0.6332,  0.1371,  0.8571, -0.8587, -1.5110,  0.5061,  0.5556]],
       dtype=torch.float64)
	q_value: tensor([[-37.9885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9700207094272555, distance: 1.6061721419770163 entropy -2.265715709641287
epoch: 33, step: 59
	action: tensor([[-0.0645,  1.3738, -1.4001,  0.3845, -0.3425,  0.5806,  1.7456]],
       dtype=torch.float64)
	q_value: tensor([[-42.1601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6061721419770163 entropy -1.6251506391903767
epoch: 33, step: 60
	action: tensor([[-0.0386, -0.2501, -0.0903,  0.1575, -0.0421,  0.1140, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15005394311081444, distance: 1.0549997943731861 entropy -3.2559069664939413
epoch: 33, step: 61
	action: tensor([[-0.7695, -0.0135,  0.2703,  0.9895,  0.3767,  0.0295,  0.5787]],
       dtype=torch.float64)
	q_value: tensor([[-28.2671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09302647918789386, distance: 1.089818051682102 entropy -2.598021233634163
epoch: 33, step: 62
	action: tensor([[ 0.3097,  0.0853, -0.1991, -0.6505,  0.0608,  0.4551,  0.6313]],
       dtype=torch.float64)
	q_value: tensor([[-31.4455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47313916270585743, distance: 0.8306243379499875 entropy -2.4854697678078557
epoch: 33, step: 63
	action: tensor([[-0.3423, -0.3473, -0.2364, -0.1961,  0.2093, -0.1397, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-30.6088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4914843517274676, distance: 1.3975457823091963 entropy -3.2559069664939413
LOSS epoch 33 actor 408.7108232420898 critic 596.436180442615
epoch: 34, step: 0
	action: tensor([[ 0.7432, -0.0951, -0.2726, -0.3411,  0.2181,  0.1557,  0.5759]],
       dtype=torch.float64)
	q_value: tensor([[-26.2459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5360587530144962, distance: 0.779449864701 entropy -2.7633973535167367
epoch: 34, step: 1
	action: tensor([[1.1192, 0.1536, 0.7318, 0.1109, 0.1162, 0.9193, 1.0761]],
       dtype=torch.float64)
	q_value: tensor([[-35.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8368015781714123, distance: 0.46229018255030185 entropy -2.159724971790466
epoch: 34, step: 2
	action: tensor([[-0.5484,  0.1713,  1.2568,  0.1964, -2.6560, -0.3841,  2.4056]],
       dtype=torch.float64)
	q_value: tensor([[-52.1671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.46229018255030185 entropy -1.440239021616535
epoch: 34, step: 3
	action: tensor([[ 0.1841, -0.3281, -0.3037,  0.2752,  0.4298, -0.0190,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31480345353977124, distance: 0.9472492627872218 entropy -3.1058572809283254
epoch: 34, step: 4
	action: tensor([[ 0.4033, -0.1931, -0.3551,  0.1053,  0.2220,  0.3503,  0.7619]],
       dtype=torch.float64)
	q_value: tensor([[-30.3018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6010093701422217, distance: 0.7228331166109371 entropy -2.5383570031167344
epoch: 34, step: 5
	action: tensor([[-1.1621, -0.3085,  1.3427, -1.3373, -0.1275,  0.3574,  1.0854]],
       dtype=torch.float64)
	q_value: tensor([[-36.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4948409389741388, distance: 1.807499243735448 entropy -2.1695822109485907
epoch: 34, step: 6
	action: tensor([[-2.3813,  2.6472,  0.5847, -2.0020, -0.1030,  1.9399,  2.5151]],
       dtype=torch.float64)
	q_value: tensor([[-48.2168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.807499243735448 entropy -1.4766031581864483
epoch: 34, step: 7
	action: tensor([[ 0.2709,  0.1887, -0.1451,  0.1034,  0.0918,  0.1743,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6954390819162801, distance: 0.6315297001330048 entropy -3.1058572809283254
epoch: 34, step: 8
	action: tensor([[ 0.0529, -0.4522,  0.0597, -0.1540,  0.0784, -0.1269,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1810176832514374, distance: 1.2436119696158863 entropy -3.1058572809283254
epoch: 34, step: 9
	action: tensor([[ 1.3629, -0.4666,  0.6502,  0.6022,  0.5895,  0.2125,  1.0206]],
       dtype=torch.float64)
	q_value: tensor([[-29.7980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3152116702580292, distance: 0.9469670513562687 entropy -2.3020014204752988
epoch: 34, step: 10
	action: tensor([[-4.0077,  3.0160,  0.7865, -1.0737,  2.2215,  1.2574,  2.1180]],
       dtype=torch.float64)
	q_value: tensor([[-54.6933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9469670513562687 entropy -1.5752249868859995
epoch: 34, step: 11
	action: tensor([[-0.4242,  0.1033, -0.1815, -0.0412, -0.0578, -0.1466,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2154861109388091, distance: 1.2616290882824308 entropy -3.1058572809283254
epoch: 34, step: 12
	action: tensor([[ 0.2716, -0.1614, -0.3584, -0.1184, -0.8236,  0.6203,  0.5311]],
       dtype=torch.float64)
	q_value: tensor([[-23.3437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965501569477257, distance: 0.9597834608126098 entropy -2.8284904667854134
epoch: 34, step: 13
	action: tensor([[-1.4662,  1.1646,  0.6340, -1.7181, -0.9059,  0.9761,  1.7307]],
       dtype=torch.float64)
	q_value: tensor([[-39.4469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8515550809229915, distance: 1.5571304996729487 entropy -1.8063476722974576
epoch: 34, step: 14
	action: tensor([[-3.8629,  0.8316,  1.4210,  0.1042,  1.1229,  0.3261,  2.4809]],
       dtype=torch.float64)
	q_value: tensor([[-54.4510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5571304996729487 entropy -1.4847228008908946
epoch: 34, step: 15
	action: tensor([[-0.6095,  0.0727, -0.1186, -0.1431,  0.3381,  0.3564,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3819497633562303, distance: 1.3452494439169218 entropy -3.1058572809283254
epoch: 34, step: 16
	action: tensor([[-0.3388,  0.1847,  0.8059,  0.2869,  0.2845,  0.3762,  0.5914]],
       dtype=torch.float64)
	q_value: tensor([[-27.8913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3856821944265346, distance: 0.8969190430324183 entropy -2.7139589006829152
epoch: 34, step: 17
	action: tensor([[ 2.6360, -0.6837,  0.2144, -1.7088, -0.2095,  0.6451,  1.5993]],
       dtype=torch.float64)
	q_value: tensor([[-33.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8969190430324183 entropy -1.852787644560047
epoch: 34, step: 18
	action: tensor([[-0.4431,  0.1033,  0.1251, -0.2800,  0.0884, -0.1411,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3772392602710799, distance: 1.3429547835248556 entropy -3.1058572809283254
epoch: 34, step: 19
	action: tensor([[ 0.3195, -0.2952, -0.0787, -0.7730,  0.0577, -0.0988,  0.6418]],
       dtype=torch.float64)
	q_value: tensor([[-25.0106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14981876169509012, distance: 1.2270757929091518 entropy -2.6727397242472146
epoch: 34, step: 20
	action: tensor([[ 0.6051, -0.2542,  0.3051, -0.6484, -0.6252,  0.5065,  1.0614]],
       dtype=torch.float64)
	q_value: tensor([[-33.9879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20830085570551804, distance: 1.0182086473109204 entropy -2.1989748178014588
epoch: 34, step: 21
	action: tensor([[-0.5522, -1.1169,  1.6580, -2.8878,  3.4768,  1.2312,  2.2237]],
       dtype=torch.float64)
	q_value: tensor([[-46.9491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0182086473109204 entropy -1.5612505576207347
epoch: 34, step: 22
	action: tensor([[-0.6281, -0.4011, -0.1549, -0.2965,  0.2659,  0.1306,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8485104190211181, distance: 1.555849715067155 entropy -3.1058572809283254
epoch: 34, step: 23
	action: tensor([[ 1.4403,  0.3992,  0.1983, -0.0385,  0.4227,  0.2773,  0.6813]],
       dtype=torch.float64)
	q_value: tensor([[-29.3981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.745000384834448, distance: 0.577865086178565 entropy -2.6240482511086087
epoch: 34, step: 24
	action: tensor([[-0.0260, -0.3221, -0.8950, -2.4236, -1.2227,  0.9574,  1.6008]],
       dtype=torch.float64)
	q_value: tensor([[-44.9453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.577865086178565 entropy -1.7679687647969868
epoch: 34, step: 25
	action: tensor([[ 0.0505,  0.0710, -0.0356, -0.1479, -0.1686,  0.0217,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3311185742432998, distance: 0.9359039093048581 entropy -3.1058572809283254
epoch: 34, step: 26
	action: tensor([[-0.2699, -0.4850,  0.1275, -0.2670,  0.8577,  0.3139,  0.8887]],
       dtype=torch.float64)
	q_value: tensor([[-27.2750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4138390035468862, distance: 1.3606821026414349 entropy -2.392505653580645
epoch: 34, step: 27
	action: tensor([[ 0.3367,  0.4564, -0.2111, -0.8405,  2.0007,  0.5938,  1.0181]],
       dtype=torch.float64)
	q_value: tensor([[-36.5101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6912885938860346, distance: 0.6358183112255351 entropy -2.193745225732838
epoch: 34, step: 28
	action: tensor([[-0.3633, -0.2895, -0.6575,  0.4792,  0.9733,  0.4833,  0.6996]],
       dtype=torch.float64)
	q_value: tensor([[-44.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13491487076687747, distance: 1.2190972078070146 entropy -2.2487109467566637
epoch: 34, step: 29
	action: tensor([[-0.5181, -0.0641, -0.3022,  0.0302, -0.3169,  0.0941,  0.5644]],
       dtype=torch.float64)
	q_value: tensor([[-35.6960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3821870220365886, distance: 1.3453649178720062 entropy -2.6992078763656466
epoch: 34, step: 30
	action: tensor([[-0.2941,  0.0358, -1.5227, -0.3981, -0.9480,  0.5021,  0.8935]],
       dtype=torch.float64)
	q_value: tensor([[-28.8424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07454763390779784, distance: 1.1008641482668686 entropy -2.4075314644325334
epoch: 34, step: 31
	action: tensor([[ 1.0047, -1.5256, -0.2376, -0.6348, -0.1060,  0.7494,  0.8091]],
       dtype=torch.float64)
	q_value: tensor([[-40.7483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1008641482668686 entropy -2.4335482117185956
epoch: 34, step: 32
	action: tensor([[ 0.3713,  0.0650, -0.0615, -0.3253,  0.4240,  0.0082,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5168656832538672, distance: 0.7954092485539338 entropy -3.1058572809283254
epoch: 34, step: 33
	action: tensor([[ 0.8919, -1.1320, -0.4417,  0.2874, -0.4371,  0.2611,  0.7421]],
       dtype=torch.float64)
	q_value: tensor([[-29.2932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1960786350637338, distance: 1.2515164423186673 entropy -2.492550904357478
epoch: 34, step: 34
	action: tensor([[-7.2624e-01, -1.0027e-01, -8.9146e-04, -3.5586e+00, -6.2502e-01,
          1.8458e+00,  2.0015e+00]], dtype=torch.float64)
	q_value: tensor([[-50.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2515164423186673 entropy -1.7093455321178026
epoch: 34, step: 35
	action: tensor([[0.1525, 0.3556, 0.1603, 0.0116, 0.2012, 0.3127, 0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7357216553001324, distance: 0.5882846035042701 entropy -3.1058572809283254
epoch: 34, step: 36
	action: tensor([[-0.0290, -0.5265,  0.4990,  0.1874, -0.7506,  0.0363,  0.9577]],
       dtype=torch.float64)
	q_value: tensor([[-29.6479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029778006628726317, distance: 1.1612574129864 entropy -2.284314863987103
epoch: 34, step: 37
	action: tensor([[ 0.3267,  0.0638,  0.0722, -0.1830, -0.1898,  0.0778,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5653000825834603, distance: 0.7544864978227389 entropy -3.1058572809283254
epoch: 34, step: 38
	action: tensor([[ 0.3551,  0.3555, -0.0694, -0.8403,  0.4174,  0.3325,  1.1127]],
       dtype=torch.float64)
	q_value: tensor([[-30.5251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6041405110418725, distance: 0.7199912576059254 entropy -2.184575943259483
epoch: 34, step: 39
	action: tensor([[-0.2978,  1.3219, -0.7114, -0.4790,  2.3807,  0.6071,  1.1037]],
       dtype=torch.float64)
	q_value: tensor([[-37.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7199912576059254 entropy -2.068366494655126
epoch: 34, step: 40
	action: tensor([[ 0.2120, -0.2514, -0.0935, -0.0889, -0.4271, -0.1293,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2245063889624166, distance: 1.0077337537397233 entropy -3.1058572809283254
epoch: 34, step: 41
	action: tensor([[ 0.2111,  0.2471,  0.5275, -0.5826, -1.5811,  0.8915,  1.1380]],
       dtype=torch.float64)
	q_value: tensor([[-30.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2909326344078135, distance: 0.9636081002945633 entropy -2.200509967973236
epoch: 34, step: 42
	action: tensor([[ 0.0938, -1.0639, -1.2629, -4.7193, -0.7219,  1.9628,  3.0894]],
       dtype=torch.float64)
	q_value: tensor([[-53.2490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9636081002945633 entropy -1.2933045685730566
epoch: 34, step: 43
	action: tensor([[-0.1677, -0.2975, -0.0950,  0.2278, -0.0630,  0.1940,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002566606054475784, distance: 1.1458118533614696 entropy -3.1058572809283254
epoch: 34, step: 44
	action: tensor([[ 1.2178, -1.9005,  0.6196,  0.3572,  0.0102,  0.8822,  1.0590]],
       dtype=torch.float64)
	q_value: tensor([[-30.3169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1458118533614696 entropy -2.272351451201144
epoch: 34, step: 45
	action: tensor([[-0.5174,  0.2312,  0.0565,  0.1824, -0.1726, -0.0859,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1337475485791575, distance: 1.2184700923407397 entropy -3.1058572809283254
epoch: 34, step: 46
	action: tensor([[ 0.5278, -0.1160, -0.3053, -0.1033,  0.3153,  0.4327,  0.8042]],
       dtype=torch.float64)
	q_value: tensor([[-24.5462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6559523887943928, distance: 0.6712216664095022 entropy -2.501644015404122
epoch: 34, step: 47
	action: tensor([[ 0.1247,  0.0316,  0.2391, -0.3274, -0.1022, -0.0739,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2591681734852599, distance: 0.9849552718907777 entropy -3.1058572809283254
epoch: 34, step: 48
	action: tensor([[-1.4258, -0.8826,  0.9429, -1.2683, -0.9938,  0.3831,  1.0266]],
       dtype=torch.float64)
	q_value: tensor([[-29.0689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4393285255878006, distance: 1.7872768926924651 entropy -2.266512521509991
epoch: 34, step: 49
	action: tensor([[-1.1938, -2.5789, -0.9646, -1.6894, -5.2149,  2.3042,  2.8689]],
       dtype=torch.float64)
	q_value: tensor([[-50.9441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7872768926924651 entropy -1.4076211781845014
epoch: 34, step: 50
	action: tensor([[-0.1702, -0.2091, -0.2288,  0.6840, -0.1692, -0.2481,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1430261796082971, distance: 1.0593524387547497 entropy -3.1058572809283254
epoch: 34, step: 51
	action: tensor([[ 0.2658, -0.0789,  0.2218,  1.3034, -0.0525,  0.3430,  0.9547]],
       dtype=torch.float64)
	q_value: tensor([[-28.6845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0593524387547497 entropy -2.3726635319961344
epoch: 34, step: 52
	action: tensor([[-0.0135, -0.4814, -0.0813, -0.0174,  0.1101, -0.1556,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0593524387547497 entropy -3.1058572809283254
epoch: 34, step: 53
	action: tensor([[ 0.1271,  0.0488, -0.1118, -0.8446,  0.4016, -0.0780,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07111505907600557, distance: 1.1029038540505611 entropy -3.1058572809283254
epoch: 34, step: 54
	action: tensor([[ 1.6408,  0.2228, -0.2127, -0.2473,  0.1019,  0.5984,  0.6151]],
       dtype=torch.float64)
	q_value: tensor([[-29.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1029038540505611 entropy -2.6373036913154304
epoch: 34, step: 55
	action: tensor([[ 0.2363,  0.0235,  0.0707,  0.0114, -0.1719,  0.2555,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6192541709076014, distance: 0.706113102950203 entropy -3.1058572809283254
epoch: 34, step: 56
	action: tensor([[-0.3519,  0.0434, -0.3642,  0.0290,  0.3822,  0.6341,  1.1920]],
       dtype=torch.float64)
	q_value: tensor([[-31.7901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13687743479179348, distance: 1.063146046881018 entropy -2.125019211599036
epoch: 34, step: 57
	action: tensor([[ 1.2672,  0.4158, -0.4186,  0.4351, -0.1824,  0.6968,  1.0034]],
       dtype=torch.float64)
	q_value: tensor([[-37.5971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.063146046881018 entropy -2.2271252914992674
epoch: 34, step: 58
	action: tensor([[-0.3984, -0.2416, -0.0975,  0.1186,  0.4093, -0.2144,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42155359097020484, distance: 1.364389321296573 entropy -3.1058572809283254
epoch: 34, step: 59
	action: tensor([[-0.7132, -0.1678, -0.2158, -0.3330,  0.3528,  0.1871,  0.5756]],
       dtype=torch.float64)
	q_value: tensor([[-26.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7865257008294604, distance: 1.5295417202295436 entropy -2.765087273917684
epoch: 34, step: 60
	action: tensor([[ 0.3363, -0.2777,  0.2149, -0.1861, -0.3671,  0.3667,  0.5909]],
       dtype=torch.float64)
	q_value: tensor([[-29.5859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30221098647875266, distance: 0.9559138563757823 entropy -2.7002429885021795
epoch: 34, step: 61
	action: tensor([[ 0.0186, -1.9892, -0.5571, -1.2144, -1.2684,  0.4990,  1.7113]],
       dtype=torch.float64)
	q_value: tensor([[-38.5721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9559138563757823 entropy -1.8029647351507128
epoch: 34, step: 62
	action: tensor([[ 0.2384, -0.2040, -0.1161, -0.2309,  0.3455,  0.1820,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-33.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.266081767969362, distance: 0.9803485968271016 entropy -3.1058572809283254
epoch: 34, step: 63
	action: tensor([[ 0.0198, -0.1458, -0.0396,  0.6902,  0.1275,  0.4647,  0.8626]],
       dtype=torch.float64)
	q_value: tensor([[-30.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9803485968271016 entropy -2.3992059195404485
LOSS epoch 34 actor 374.5022244886448 critic 540.0507490872163
epoch: 35, step: 0
	action: tensor([[-0.5022,  0.3401, -0.0243,  0.0681, -0.1557, -0.1483,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9803485968271016 entropy -2.976443239084695
epoch: 35, step: 1
	action: tensor([[ 0.0027, -0.2703, -0.1372, -0.1159,  0.2432, -0.1506,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02787101846086859, distance: 1.1601816809783616 entropy -2.976443239084695
epoch: 35, step: 2
	action: tensor([[ 0.1574,  0.1455,  0.3325, -1.2813, -0.7643,  0.8618,  0.9171]],
       dtype=torch.float64)
	q_value: tensor([[-29.3222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0026930095923620723, distance: 1.142802350183225 entropy -2.4087385192412856
epoch: 35, step: 3
	action: tensor([[-0.9804, -0.3306,  0.1603, -2.5111,  0.4188,  0.1795,  2.5771]],
       dtype=torch.float64)
	q_value: tensor([[-49.2627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.142802350183225 entropy -1.4723535920498523
epoch: 35, step: 4
	action: tensor([[ 0.0242,  0.0117, -0.0909, -0.0691,  0.0138, -0.3567,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22705359999223718, distance: 1.006077375270591 entropy -2.976443239084695
epoch: 35, step: 5
	action: tensor([[-0.7890,  0.0460, -0.1068,  0.3525,  1.1672,  0.4603,  0.8382]],
       dtype=torch.float64)
	q_value: tensor([[-26.8611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2660265826416073, distance: 1.2875915134568976 entropy -2.4777817669140947
epoch: 35, step: 6
	action: tensor([[ 0.2255, -0.0924,  0.1177, -1.0382,  0.0596,  0.4039,  0.9547]],
       dtype=torch.float64)
	q_value: tensor([[-38.9237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02488550632376052, distance: 1.1584955475804153 entropy -2.325922181607351
epoch: 35, step: 7
	action: tensor([[ 2.0466, -1.5136,  1.5157, -2.9931,  1.0859,  0.3613,  1.7175]],
       dtype=torch.float64)
	q_value: tensor([[-41.8508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1584955475804153 entropy -1.8103126121553785
epoch: 35, step: 8
	action: tensor([[-0.2577,  0.2418, -0.2136,  0.4308, -0.2101,  0.2998,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24559762194907409, distance: 0.9939355348596036 entropy -2.976443239084695
epoch: 35, step: 9
	action: tensor([[-2.2019, -0.1752,  0.5763, -0.0520, -0.6222,  0.6061,  1.1541]],
       dtype=torch.float64)
	q_value: tensor([[-31.1852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9939355348596036 entropy -2.2079471171594296
epoch: 35, step: 10
	action: tensor([[-0.1502, -0.1389, -0.3711,  0.3402, -0.3498,  0.1188,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07305776888552618, distance: 1.1017499201736172 entropy -2.976443239084695
epoch: 35, step: 11
	action: tensor([[-0.2206,  1.1206,  0.8059, -0.2176, -0.6157,  0.6495,  1.1973]],
       dtype=torch.float64)
	q_value: tensor([[-32.0020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1017499201736172 entropy -2.1932126650978727
epoch: 35, step: 12
	action: tensor([[ 0.4532, -0.2352,  0.2989, -0.2398,  0.2291,  0.1213,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3338767602567747, distance: 0.9339722786575643 entropy -2.976443239084695
epoch: 35, step: 13
	action: tensor([[-2.2953, -1.2106, -0.2491,  0.0608,  0.7132,  0.5114,  1.5644]],
       dtype=torch.float64)
	q_value: tensor([[-36.6048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9339722786575643 entropy -1.922017988455617
epoch: 35, step: 14
	action: tensor([[ 0.6713, -0.3498,  0.3337,  0.3856,  0.0067,  0.2497,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7348017143772441, distance: 0.5893076100197793 entropy -2.976443239084695
epoch: 35, step: 15
	action: tensor([[ 0.9219,  0.2840,  0.3208, -2.7047,  1.3187,  0.8160,  2.0594]],
       dtype=torch.float64)
	q_value: tensor([[-42.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5893076100197793 entropy -1.7010486931848732
epoch: 35, step: 16
	action: tensor([[ 0.2309, -0.0069, -0.1667,  0.0125,  0.2592, -0.2569,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44993755180122796, distance: 0.84871659098626 entropy -2.976443239084695
epoch: 35, step: 17
	action: tensor([[-0.2475,  1.0952,  0.1734, -0.5038,  0.5454,  0.4410,  0.8418]],
       dtype=torch.float64)
	q_value: tensor([[-28.2975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.84871659098626 entropy -2.467354661219012
epoch: 35, step: 18
	action: tensor([[ 0.4872, -0.3420,  0.3559, -0.6131, -0.3009,  0.0717,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.84871659098626 entropy -2.976443239084695
epoch: 35, step: 19
	action: tensor([[-0.1775, -0.3721, -0.1510,  0.4586, -0.0248, -0.2198,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06489106695529756, distance: 1.18088956571279 entropy -2.976443239084695
epoch: 35, step: 20
	action: tensor([[-1.6943,  0.4076,  0.5469,  0.7398,  0.6435,  0.6824,  1.1715]],
       dtype=torch.float64)
	q_value: tensor([[-30.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.18088956571279 entropy -2.220639363414548
epoch: 35, step: 21
	action: tensor([[ 0.5026, -0.0029, -0.0034,  0.1096, -0.2580,  0.1245,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7602949532589705, distance: 0.5602673084295787 entropy -2.976443239084695
epoch: 35, step: 22
	action: tensor([[-2.1158,  0.2565,  0.4021, -1.8495,  2.5474,  0.5560,  1.5254]],
       dtype=torch.float64)
	q_value: tensor([[-35.6683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5602673084295787 entropy -1.9496007609572057
epoch: 35, step: 23
	action: tensor([[-0.3171, -0.0620, -0.2889, -0.0066, -0.3004,  0.0060,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16334736173914832, distance: 1.234273480594409 entropy -2.976443239084695
epoch: 35, step: 24
	action: tensor([[-0.7934,  0.0119, -0.8532, -0.2946, -0.7628,  0.8504,  0.9404]],
       dtype=torch.float64)
	q_value: tensor([[-28.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8587823505450212, distance: 1.5601665534121643 entropy -2.397149310437257
epoch: 35, step: 25
	action: tensor([[-0.0814, -2.4606, -0.8936,  0.4458, -0.7787,  1.0397,  1.5493]],
       dtype=torch.float64)
	q_value: tensor([[-45.8662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5601665534121643 entropy -1.9610717157489623
epoch: 35, step: 26
	action: tensor([[ 0.3995,  0.1755, -0.3913, -0.3753,  0.2295,  0.2862,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7185808388909379, distance: 0.6070626666776048 entropy -2.976443239084695
epoch: 35, step: 27
	action: tensor([[-1.0132, -0.8307,  0.4397,  0.5204,  0.1417, -0.1933,  0.9573]],
       dtype=torch.float64)
	q_value: tensor([[-31.5328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0806960137650305, distance: 1.6506728602692207 entropy -2.326002189179292
epoch: 35, step: 28
	action: tensor([[-1.8528,  0.0356, -0.2266,  1.4092,  1.8456,  0.4789,  2.0004]],
       dtype=torch.float64)
	q_value: tensor([[-40.6638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6506728602692207 entropy -1.7676055708988714
epoch: 35, step: 29
	action: tensor([[-0.2406, -0.6068, -0.0367,  0.1707,  0.0348, -0.0818,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43673069039136836, distance: 1.3716533791541603 entropy -2.976443239084695
epoch: 35, step: 30
	action: tensor([[-1.0376, -1.8700, -1.1433, -0.0618,  0.2564,  0.5133,  1.3189]],
       dtype=torch.float64)
	q_value: tensor([[-32.7762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3716533791541603 entropy -2.119416331014882
epoch: 35, step: 31
	action: tensor([[ 0.5763,  0.1879,  0.2311, -0.3463,  0.5056, -0.2402,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6190477969812967, distance: 0.7063044426593331 entropy -2.976443239084695
epoch: 35, step: 32
	action: tensor([[ 0.2738, -0.4733, -0.2189,  0.4277,  0.7084, -0.0251,  1.1790]],
       dtype=torch.float64)
	q_value: tensor([[-33.8233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3497173646010834, distance: 0.9228003951854514 entropy -2.1451401498144977
epoch: 35, step: 33
	action: tensor([[-0.7039, -0.7111,  0.4260, -1.0878, -0.4784,  0.7907,  1.4222]],
       dtype=torch.float64)
	q_value: tensor([[-41.4600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1972674060462642, distance: 1.6962823765274098 entropy -2.0107455318235274
epoch: 35, step: 34
	action: tensor([[ 0.7860,  0.7556, -0.0647, -0.8880, -1.4931,  0.1342,  2.7742]],
       dtype=torch.float64)
	q_value: tensor([[-53.8882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8900383701961101, distance: 0.37946985150204005 entropy -1.4389826307697662
epoch: 35, step: 35
	action: tensor([[-5.2900,  0.2618,  0.3572, -1.8140,  3.2142,  1.8819,  3.2626]],
       dtype=torch.float64)
	q_value: tensor([[-71.0519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.37946985150204005 entropy -1.2592308742353393
epoch: 35, step: 36
	action: tensor([[-0.1117, -0.2636, -0.0248,  0.4308, -0.1577, -0.2368,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1254002250755516, distance: 1.0701911959806594 entropy -2.976443239084695
epoch: 35, step: 37
	action: tensor([[ 1.4861, -0.4345, -0.1147,  0.4698, -0.1346,  1.6711,  1.2625]],
       dtype=torch.float64)
	q_value: tensor([[-30.7142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.790553113631574, distance: 0.5237133534675381 entropy -2.1542544935787493
epoch: 35, step: 38
	action: tensor([[-0.0800, -3.2537,  0.8881,  0.3720,  2.5596,  0.6144,  3.6724]],
       dtype=torch.float64)
	q_value: tensor([[-68.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5237133534675381 entropy -1.1755647130084073
epoch: 35, step: 39
	action: tensor([[ 0.2693,  0.2269,  0.3969,  0.0236, -0.5118, -0.0015,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6821296008419495, distance: 0.6451812489567681 entropy -2.976443239084695
epoch: 35, step: 40
	action: tensor([[ 0.1242,  0.0552,  0.4217, -0.0755,  0.1790,  0.3651,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5363828513608839, distance: 0.7791775645470393 entropy -2.976443239084695
epoch: 35, step: 41
	action: tensor([[ 1.6407, -1.8794, -0.0549,  1.0753, -0.2357,  1.0361,  1.4963]],
       dtype=torch.float64)
	q_value: tensor([[-34.8646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7791775645470393 entropy -1.9542871432807318
epoch: 35, step: 42
	action: tensor([[-0.2148, -0.0220, -0.3095, -0.1995,  0.2111,  0.1045,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02952249921487793, distance: 1.1611133390774544 entropy -2.976443239084695
epoch: 35, step: 43
	action: tensor([[-0.5060, -0.8060,  0.8058,  0.1665, -0.1339,  0.2163,  0.6844]],
       dtype=torch.float64)
	q_value: tensor([[-28.6621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6749354066748527, distance: 1.4810022724629581 entropy -2.63140128602996
epoch: 35, step: 44
	action: tensor([[ 3.8065, -1.9720,  1.1528, -1.2099, -2.5757,  1.7891,  2.5093]],
       dtype=torch.float64)
	q_value: tensor([[-43.1466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4810022724629581 entropy -1.546139501828623
epoch: 35, step: 45
	action: tensor([[-0.0633, -0.0563, -0.2000, -0.5241,  0.0952,  0.1488,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016582558013830906, distance: 1.1348165128110919 entropy -2.976443239084695
epoch: 35, step: 46
	action: tensor([[ 0.0676, -0.2037,  0.0084, -0.0214, -0.5102,  0.7168,  0.8820]],
       dtype=torch.float64)
	q_value: tensor([[-30.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2789881521302957, distance: 0.9716903593408304 entropy -2.4200565207362623
epoch: 35, step: 47
	action: tensor([[-0.7171, -2.2736, -0.0066,  1.5279, -0.8003,  1.2550,  2.1923]],
       dtype=torch.float64)
	q_value: tensor([[-44.2953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9716903593408304 entropy -1.6278587913844824
epoch: 35, step: 48
	action: tensor([[-0.9288, -0.5553, -0.0161, -0.1902,  0.2581,  0.0791,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2806028021210203, distance: 1.7281503357920878 entropy -2.976443239084695
epoch: 35, step: 49
	action: tensor([[ 0.3527, -0.6736, -0.3640, -1.7071, -0.1315,  0.6205,  1.0152]],
       dtype=torch.float64)
	q_value: tensor([[-34.2548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4289755384908913, distance: 1.3679464305222249 entropy -2.328951299603573
epoch: 35, step: 50
	action: tensor([[-1.6356,  0.9114, -0.3647, -0.0727,  1.0548,  2.4706,  2.0900]],
       dtype=torch.float64)
	q_value: tensor([[-51.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3679464305222249 entropy -1.6698628588159294
epoch: 35, step: 51
	action: tensor([[-0.7122,  0.4242, -0.0610, -0.2241,  0.0721,  0.1622,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3790729106553161, distance: 1.3438484882453705 entropy -2.976443239084695
epoch: 35, step: 52
	action: tensor([[-0.9560,  0.2299,  0.0664, -0.6247,  0.1151,  0.4255,  0.7096]],
       dtype=torch.float64)
	q_value: tensor([[-28.6895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9425288007879042, distance: 1.5949255912961657 entropy -2.598685388245726
epoch: 35, step: 53
	action: tensor([[-0.0182, -0.2163, -0.2706, -0.2858, -0.0558,  0.6348,  1.1235]],
       dtype=torch.float64)
	q_value: tensor([[-36.2596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09552777945155144, distance: 1.0883142347763568 entropy -2.2091838841763742
epoch: 35, step: 54
	action: tensor([[-2.1490,  0.0254, -1.1777, -1.0842,  1.1112, -0.1201,  1.6859]],
       dtype=torch.float64)
	q_value: tensor([[-42.2375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0883142347763568 entropy -1.8469654460342764
epoch: 35, step: 55
	action: tensor([[ 0.0782, -0.1971,  0.2582,  0.0672,  0.1149, -0.1015,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2421012066429602, distance: 0.9962361599618879 entropy -2.976443239084695
epoch: 35, step: 56
	action: tensor([[-1.7214,  0.0589,  0.3083, -0.4505,  0.2462,  0.1774,  1.2890]],
       dtype=torch.float64)
	q_value: tensor([[-31.9683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9962361599618879 entropy -2.1147494956474007
epoch: 35, step: 57
	action: tensor([[-0.2152, -0.1457,  0.0628,  0.5159,  0.3851,  0.1365,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24577599304842357, distance: 0.993818024721765 entropy -2.976443239084695
epoch: 35, step: 58
	action: tensor([[-0.6799, -0.0118, -0.8191,  1.3862,  0.3975,  0.4619,  1.1976]],
       dtype=torch.float64)
	q_value: tensor([[-33.6410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2700247065204169, distance: 1.2896230239056599 entropy -2.1873028929752687
epoch: 35, step: 59
	action: tensor([[-1.5142, -0.2047,  0.5653, -0.7508,  0.4101,  0.9537,  1.3592]],
       dtype=torch.float64)
	q_value: tensor([[-48.9323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2896230239056599 entropy -2.0824221288518645
epoch: 35, step: 60
	action: tensor([[-0.4105,  0.1944, -0.0618,  0.2880, -0.0952,  0.1305,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045267307816836855, distance: 1.1181436204698798 entropy -2.976443239084695
epoch: 35, step: 61
	action: tensor([[ 0.9861,  0.3524,  0.3681, -0.7292, -0.0251,  0.3044,  1.0328]],
       dtype=torch.float64)
	q_value: tensor([[-29.1051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7762347367678212, distance: 0.5413186967719901 entropy -2.309781271391365
epoch: 35, step: 62
	action: tensor([[-0.3635, -0.2268,  0.0616,  0.6567,  0.4051,  0.4398,  0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-37.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23617643979994218, distance: 1.0001225472101407 entropy -2.976443239084695
epoch: 35, step: 63
	action: tensor([[ 0.1164, -0.0626, -0.0777, -1.5513,  1.2374,  1.0345,  1.4070]],
       dtype=torch.float64)
	q_value: tensor([[-37.2849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0001225472101407 entropy -2.053283426702131
LOSS epoch 35 actor 433.01506147802615 critic 351.7852301453461
epoch: 36, step: 0
	action: tensor([[ 0.0163, -0.4258,  0.1953,  0.4936, -0.1047, -0.0874,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2611556644468944, distance: 0.9836331739857141 entropy -2.8962625926876027
epoch: 36, step: 1
	action: tensor([[ 0.5713, -0.0266,  0.3629, -1.9341,  0.7943,  0.9131,  1.7414]],
       dtype=torch.float64)
	q_value: tensor([[-37.9545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015185336337386168, distance: 1.1530001433552897 entropy -1.8623793704878266
epoch: 36, step: 2
	action: tensor([[ 0.4838, -2.0914,  0.7134, -3.5614,  0.2009,  2.6001,  2.9455]],
       dtype=torch.float64)
	q_value: tensor([[-62.0025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1530001433552897 entropy -1.3060912609144015
epoch: 36, step: 3
	action: tensor([[-0.2064, -0.3780, -0.2120, -0.1458,  0.0755, -0.2619,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36923577876082114, distance: 1.339046975015643 entropy -2.8962625926876027
epoch: 36, step: 4
	action: tensor([[ 1.9234, -0.0581,  0.2487,  0.5916, -0.4580,  0.1080,  0.9689]],
       dtype=torch.float64)
	q_value: tensor([[-30.8551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.339046975015643 entropy -2.3622536041815847
epoch: 36, step: 5
	action: tensor([[-0.2222,  0.4440, -0.0256, -0.4687,  0.0381, -0.2685,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09793283464247271, distance: 1.0868663195960093 entropy -2.8962625926876027
epoch: 36, step: 6
	action: tensor([[ 0.2576, -0.3188,  0.1460, -0.5680, -0.1231,  0.1075,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09595613129783231, distance: 1.1979902364519195 entropy -2.8962625926876027
epoch: 36, step: 7
	action: tensor([[-2.1664,  0.7809,  1.0968, -1.9482, -2.1772,  0.9758,  1.6403]],
       dtype=torch.float64)
	q_value: tensor([[-37.5651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1979902364519195 entropy -1.8872828673764814
epoch: 36, step: 8
	action: tensor([[ 0.3514,  0.2677,  0.0694, -0.4172,  0.3487,  0.0522,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6142970874910603, distance: 0.7106948239082933 entropy -2.8962625926876027
epoch: 36, step: 9
	action: tensor([[ 0.1448,  0.4540, -0.1612, -0.7524,  0.7588,  0.0708,  1.1250]],
       dtype=torch.float64)
	q_value: tensor([[-34.0565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5268507271508694, distance: 0.7871468861279401 entropy -2.1888984635514204
epoch: 36, step: 10
	action: tensor([[-1.1788, -0.2059,  0.1581, -0.2579,  0.4650,  0.8950,  1.2423]],
       dtype=torch.float64)
	q_value: tensor([[-39.4493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.024648097484723, distance: 1.6282889218158243 entropy -2.0588064779012205
epoch: 36, step: 11
	action: tensor([[ 1.6922,  0.7294,  0.0823,  0.4644, -1.1257,  0.7495,  1.9757]],
       dtype=torch.float64)
	q_value: tensor([[-49.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6282889218158243 entropy -1.7301543895776252
epoch: 36, step: 12
	action: tensor([[ 0.0799, -0.2202, -0.0264, -0.1118,  0.0278,  0.0525,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14670259758205173, distance: 1.0570776853024737 entropy -2.8962625926876027
epoch: 36, step: 13
	action: tensor([[-0.7648, -0.0316, -0.8066, -0.2967,  0.3439,  1.2892,  1.2727]],
       dtype=torch.float64)
	q_value: tensor([[-34.1209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37609544999264055, distance: 1.342396999341999 entropy -2.1154904291751886
epoch: 36, step: 14
	action: tensor([[ 0.4952,  0.5442, -0.3111,  1.5061, -0.9390,  0.7695,  1.5590]],
       dtype=torch.float64)
	q_value: tensor([[-53.1335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.342396999341999 entropy -1.9326748856329254
epoch: 36, step: 15
	action: tensor([[-0.6742,  0.2412,  0.1280,  0.1710,  0.8355,  0.0911,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2681066533743186, distance: 1.2886488302083892 entropy -2.8962625926876027
epoch: 36, step: 16
	action: tensor([[ 0.7070,  0.5822, -0.1375, -0.5392,  0.2452,  0.3895,  0.8716]],
       dtype=torch.float64)
	q_value: tensor([[-36.5495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7055068936788217, distance: 0.6210038029822428 entropy -2.413029351055242
epoch: 36, step: 17
	action: tensor([[ 0.0965,  0.6121,  0.1825, -0.1571,  0.7905,  0.8620,  1.6638]],
       dtype=torch.float64)
	q_value: tensor([[-40.3026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6210038029822428 entropy -1.8174464994491477
epoch: 36, step: 18
	action: tensor([[ 0.0723,  0.2463, -0.1835,  0.0653,  0.4515,  0.0050,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5321207209257458, distance: 0.7827509426479133 entropy -2.8962625926876027
epoch: 36, step: 19
	action: tensor([[-0.4814,  0.6155, -0.0697,  0.3950,  0.4579,  0.5651,  0.8189]],
       dtype=torch.float64)
	q_value: tensor([[-31.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7827509426479133 entropy -2.4674248999198123
epoch: 36, step: 20
	action: tensor([[ 0.0076, -0.1920,  0.0382,  0.2603, -0.0122,  0.0428,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30753657894284214, distance: 0.9522590566024071 entropy -2.8962625926876027
epoch: 36, step: 21
	action: tensor([[ 0.8903, -0.1675,  0.8734,  0.6553,  0.3871,  0.3558,  1.3898]],
       dtype=torch.float64)
	q_value: tensor([[-34.9003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7599930994156793, distance: 0.5606199618977925 entropy -2.046850372231155
epoch: 36, step: 22
	action: tensor([[ 0.4795,  1.1098, -0.5614,  0.2756,  1.3989,  2.2117,  3.0676]],
       dtype=torch.float64)
	q_value: tensor([[-56.8395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5606199618977925 entropy -1.3141301863443764
epoch: 36, step: 23
	action: tensor([[-0.0578,  0.1211, -0.1301, -0.1675,  0.5397, -0.0280,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23471002753717485, distance: 1.0010821200856859 entropy -2.8962625926876027
epoch: 36, step: 24
	action: tensor([[ 0.0410, -1.6187,  0.2425,  0.3093,  0.3443,  0.2488,  0.7575]],
       dtype=torch.float64)
	q_value: tensor([[-31.3433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0010821200856859 entropy -2.528642040659381
epoch: 36, step: 25
	action: tensor([[ 0.5277,  0.1722, -0.0543,  0.4495, -0.4423,  0.0836,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9099122511911404, distance: 0.3434705935022534 entropy -2.8962625926876027
epoch: 36, step: 26
	action: tensor([[-0.1919, -0.7767,  0.5373, -0.3324,  0.0252,  0.0949,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8003690877524103, distance: 1.535456322458793 entropy -2.8962625926876027
epoch: 36, step: 27
	action: tensor([[-2.0298,  1.1192,  0.3195, -2.1872,  1.3027,  0.6962,  2.0857]],
       dtype=torch.float64)
	q_value: tensor([[-41.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.535456322458793 entropy -1.6959343188177185
epoch: 36, step: 28
	action: tensor([[-0.0272, -0.1578, -0.1977,  0.4236,  0.5398, -0.2955,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20790984104062538, distance: 1.018460059333443 entropy -2.8962625926876027
epoch: 36, step: 29
	action: tensor([[-0.4962, -0.6524,  0.6581, -0.3358,  0.0166,  0.6684,  0.9700]],
       dtype=torch.float64)
	q_value: tensor([[-32.8446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7269809007486241, distance: 1.5038359427603751 entropy -2.351715349624784
epoch: 36, step: 30
	action: tensor([[ 6.1725, -1.5027, -1.7351,  0.9546, -1.4616,  2.2499,  2.6354]],
       dtype=torch.float64)
	q_value: tensor([[-48.0581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5038359427603751 entropy -1.4731303396235798
epoch: 36, step: 31
	action: tensor([[-0.3365, -0.2692, -0.1649,  0.0654,  0.5063, -0.1170,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3614026772259815, distance: 1.3352112823942222 entropy -2.8962625926876027
epoch: 36, step: 32
	action: tensor([[-0.1214, -0.1270,  0.4050, -0.1755,  0.3846,  0.2106,  0.8470]],
       dtype=torch.float64)
	q_value: tensor([[-32.4136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011908880353857176, distance: 1.1511380167382539 entropy -2.464579068603185
epoch: 36, step: 33
	action: tensor([[-0.0663,  1.3893, -0.0196,  1.5446, -0.0401,  1.1198,  1.7155]],
       dtype=torch.float64)
	q_value: tensor([[-38.5330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1511380167382539 entropy -1.8192692192645652
epoch: 36, step: 34
	action: tensor([[-0.6887, -0.1497,  0.1448, -0.4996,  0.0736,  0.1436,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8973335233335238, distance: 1.5762624627827742 entropy -2.8962625926876027
epoch: 36, step: 35
	action: tensor([[-1.5311,  0.4547,  0.0376,  0.6382,  1.0680,  0.1504,  1.1282]],
       dtype=torch.float64)
	q_value: tensor([[-35.3722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5430807474574202, distance: 1.4215136019014094 entropy -2.221466643941263
epoch: 36, step: 36
	action: tensor([[-0.6897, -0.5644,  0.6299, -0.0120, -1.9238,  0.2914,  1.3423]],
       dtype=torch.float64)
	q_value: tensor([[-45.6866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.067878898474191, distance: 1.6455809231003722 entropy -2.0534584588441467
epoch: 36, step: 37
	action: tensor([[ 1.7178,  1.7418, -2.4554, -1.4517,  1.5405,  0.6909,  4.1758]],
       dtype=torch.float64)
	q_value: tensor([[-62.7680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6455809231003722 entropy -1.1088289020355317
epoch: 36, step: 38
	action: tensor([[ 0.2938,  0.0968, -0.2832,  0.2709,  0.3859, -0.1053,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.646857212757894, distance: 0.6800359363186641 entropy -2.8962625926876027
epoch: 36, step: 39
	action: tensor([[-1.0316,  0.2796,  0.1082,  0.0561,  0.3040, -0.0051,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8188806605432344, distance: 1.5433299915439767 entropy -2.8962625926876027
epoch: 36, step: 40
	action: tensor([[-1.5725, -0.4004, -0.1802,  0.2621,  0.3043,  0.4320,  0.8875]],
       dtype=torch.float64)
	q_value: tensor([[-33.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6595914619874548, distance: 1.866225667593294 entropy -2.4171180145973055
epoch: 36, step: 41
	action: tensor([[ 1.7288,  0.2921,  0.6922, -1.9028,  0.3086,  1.0295,  1.5177]],
       dtype=torch.float64)
	q_value: tensor([[-46.2857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.866225667593294 entropy -1.9914235870438157
epoch: 36, step: 42
	action: tensor([[-0.7409, -0.2554, -0.0549,  0.1156, -0.4807, -0.0018,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.818942244781635, distance: 1.543356118601043 entropy -2.8962625926876027
epoch: 36, step: 43
	action: tensor([[ 1.2083, -0.0705, -0.0522, -1.3319,  1.1855,  1.0915,  1.4847]],
       dtype=torch.float64)
	q_value: tensor([[-35.2464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0768641197997576, distance: 1.0994855067699143 entropy -2.013826905567702
epoch: 36, step: 44
	action: tensor([[-2.0091,  2.8159,  0.1842, -1.3075,  3.4355, -0.3985,  2.7135]],
       dtype=torch.float64)
	q_value: tensor([[-59.3428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0994855067699143 entropy -1.3716222548654124
epoch: 36, step: 45
	action: tensor([[-0.2328,  0.0822,  0.0869, -0.0563, -0.2290,  0.0037,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022621724643342223, distance: 1.1313266926429117 entropy -2.8962625926876027
epoch: 36, step: 46
	action: tensor([[ 0.2230, -0.2878, -0.7478,  0.8574, -0.0472, -0.2155,  1.2058]],
       dtype=torch.float64)
	q_value: tensor([[-31.6215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30648386386428694, distance: 0.9529826159492429 entropy -2.1683351294667874
epoch: 36, step: 47
	action: tensor([[ 0.3314,  0.4966,  0.1466, -0.2300,  2.6207,  0.6217,  1.5889]],
       dtype=torch.float64)
	q_value: tensor([[-44.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9529826159492429 entropy -1.942235548060911
epoch: 36, step: 48
	action: tensor([[ 0.2191, -0.0449, -0.5209,  0.2473, -0.2192, -0.0139,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5289802781829371, distance: 0.7853734924489436 entropy -2.8962625926876027
epoch: 36, step: 49
	action: tensor([[ 2.4448,  0.3884, -0.5750, -0.3228,  0.0748,  0.3286,  1.1102]],
       dtype=torch.float64)
	q_value: tensor([[-32.9620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7853734924489436 entropy -2.2371380510029617
epoch: 36, step: 50
	action: tensor([[ 0.3263, -0.3812, -0.0462, -0.9787, -0.3379,  0.4785,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21656695404205917, distance: 1.2621899009806337 entropy -2.8962625926876027
epoch: 36, step: 51
	action: tensor([[ 1.7078, -0.5749, -1.3696, -1.5317, -1.0475,  1.1333,  1.9936]],
       dtype=torch.float64)
	q_value: tensor([[-43.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2621899009806337 entropy -1.7138929438187065
epoch: 36, step: 52
	action: tensor([[ 0.2920, -0.3186,  0.0711,  0.1160, -0.0820, -0.2616,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29057556732560075, distance: 0.9638506932017703 entropy -2.8962625926876027
epoch: 36, step: 53
	action: tensor([[-0.3818,  0.4988,  0.1728,  0.3990, -1.5073,  0.1375,  1.4994]],
       dtype=torch.float64)
	q_value: tensor([[-34.8896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9638506932017703 entropy -1.9821665008170444
epoch: 36, step: 54
	action: tensor([[ 0.1042,  0.3174, -0.0611,  0.4003,  0.0630,  0.0763,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6695295353405966, distance: 0.6578441527474007 entropy -2.8962625926876027
epoch: 36, step: 55
	action: tensor([[ 0.3662,  1.4028, -0.0296,  0.2570, -0.9888,  0.6680,  1.1666]],
       dtype=torch.float64)
	q_value: tensor([[-33.2480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6578441527474007 entropy -2.180837351791369
epoch: 36, step: 56
	action: tensor([[-0.0227, -0.0672,  0.0409,  0.2236,  0.4847,  0.0919,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36181592576261856, distance: 0.9141757029753518 entropy -2.8962625926876027
epoch: 36, step: 57
	action: tensor([[-0.3634, -1.3499, -0.8300, -1.0145, -0.1425,  0.4999,  1.1376]],
       dtype=torch.float64)
	q_value: tensor([[-34.4668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9141757029753518 entropy -2.2076999534579835
epoch: 36, step: 58
	action: tensor([[-0.2815, -0.2584, -0.1158, -0.2094,  0.3612, -0.0397,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3642816987623443, distance: 1.3366223534618218 entropy -2.8962625926876027
epoch: 36, step: 59
	action: tensor([[ 0.1826, -0.0557, -0.1758, -1.5116, -1.1489,  0.6388,  0.9061]],
       dtype=torch.float64)
	q_value: tensor([[-32.2266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2006887145488343, distance: 1.2539260002364447 entropy -2.4054425446142518
epoch: 36, step: 60
	action: tensor([[ 3.0630, -0.4212,  0.4075, -1.5825, -1.4698,  1.5179,  2.6551]],
       dtype=torch.float64)
	q_value: tensor([[-53.2076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2539260002364447 entropy -1.4669528998209882
epoch: 36, step: 61
	action: tensor([[ 0.0321,  0.2652, -0.0389,  0.3160, -0.6069,  0.5304,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5489817782904848, distance: 0.7685174576948143 entropy -2.8962625926876027
epoch: 36, step: 62
	action: tensor([[ 2.3460, -2.1911,  1.3600, -1.0779,  1.1947,  1.3068,  1.8499]],
       dtype=torch.float64)
	q_value: tensor([[-38.7037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7685174576948143 entropy -1.7773262195012778
epoch: 36, step: 63
	action: tensor([[ 0.6861, -0.3018, -0.2237,  0.2015, -0.1147,  0.4177,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-41.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6997854275510202, distance: 0.6270072720643102 entropy -2.8962625926876027
LOSS epoch 36 actor 483.82906985627864 critic 273.27927385753446
epoch: 37, step: 0
	action: tensor([[ 0.1295, -0.0118,  1.2288,  1.2212,  1.2432,  0.5522,  2.5788]],
       dtype=torch.float64)
	q_value: tensor([[-44.7855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8435268615415807, distance: 0.4526646600276935 entropy -1.543930713929505
epoch: 37, step: 1
	action: tensor([[ 5.7814, -1.8389,  2.4919,  2.5340,  1.7491,  1.0228,  5.9599]],
       dtype=torch.float64)
	q_value: tensor([[-70.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4526646600276935 entropy -0.8220080960267558
epoch: 37, step: 2
	action: tensor([[-1.0164,  0.2925, -0.2347,  0.6821,  0.1651, -0.1818,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5291863209359193, distance: 1.4150992327632683 entropy -2.555685484527024
epoch: 37, step: 3
	action: tensor([[-1.0794,  1.2545, -1.0878,  1.1346, -0.1721,  0.6723,  1.6563]],
       dtype=torch.float64)
	q_value: tensor([[-36.0925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24240335188374518, distance: 1.2755221375128412 entropy -1.9341554190847863
epoch: 37, step: 4
	action: tensor([[-0.1695, -2.6395,  0.5253, -0.3680,  2.6282,  1.3900,  3.0610]],
       dtype=torch.float64)
	q_value: tensor([[-56.4510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2755221375128412 entropy -1.416878558147934
epoch: 37, step: 5
	action: tensor([[ 0.2431, -0.3506, -0.2002, -0.7416,  0.4964,  0.1691,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09792559091743391, distance: 1.1990661619874545 entropy -2.555685484527024
epoch: 37, step: 6
	action: tensor([[ 1.8524,  0.6885,  0.5329, -0.3793,  0.6661,  1.4954,  1.9920]],
       dtype=torch.float64)
	q_value: tensor([[-38.9370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1990661619874545 entropy -1.7657935890696144
epoch: 37, step: 7
	action: tensor([[-0.2251, -0.3011,  0.3286, -0.0913, -0.3473, -0.1045,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.323098542323504, distance: 1.316293657747028 entropy -2.555685484527024
epoch: 37, step: 8
	action: tensor([[-1.7042,  1.0163, -1.4085,  0.3287, -2.6766,  0.4618,  2.5188]],
       dtype=torch.float64)
	q_value: tensor([[-37.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.316293657747028 entropy -1.5803658300856613
epoch: 37, step: 9
	action: tensor([[-0.0024, -0.5843, -0.2571, -0.4331, -0.2071,  0.5727,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30611442941430855, distance: 1.3078179895650055 entropy -2.555685484527024
epoch: 37, step: 10
	action: tensor([[-0.9616,  1.7627, -0.0737, -0.5462, -1.5846,  1.3097,  2.6679]],
       dtype=torch.float64)
	q_value: tensor([[-43.8034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3078179895650055 entropy -1.5238402343874795
epoch: 37, step: 11
	action: tensor([[ 0.0840, -0.4862,  0.2999, -0.2261, -0.1006, -0.3357,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30980736594606584, distance: 1.3096655611282717 entropy -2.555685484527024
epoch: 37, step: 12
	action: tensor([[ 1.2793,  0.7794,  0.6545, -3.5303, -2.6551,  1.6199,  2.4696]],
       dtype=torch.float64)
	q_value: tensor([[-37.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3096655611282717 entropy -1.5970498404600977
epoch: 37, step: 13
	action: tensor([[-0.1268, -1.1378,  0.2209,  0.2991,  1.1818,  0.2616,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5128857120963188, distance: 1.4075367859488281 entropy -2.555685484527024
epoch: 37, step: 14
	action: tensor([[-0.5785, -2.1081, -0.2470, -1.2599, -1.7865,  0.4606,  2.8611]],
       dtype=torch.float64)
	q_value: tensor([[-51.0995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4075367859488281 entropy -1.4703606398524707
epoch: 37, step: 15
	action: tensor([[ 0.3123, -0.1921,  0.1416, -0.0603, -0.1886,  0.7635,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6197232097583194, distance: 0.7056780403506371 entropy -2.555685484527024
epoch: 37, step: 16
	action: tensor([[ 1.3265,  4.4600,  0.6707, -3.0344,  0.1474,  0.1058,  2.9958]],
       dtype=torch.float64)
	q_value: tensor([[-46.1266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7056780403506371 entropy -1.4049351497329023
epoch: 37, step: 17
	action: tensor([[ 0.0742, -0.3654, -0.0786, -0.9183, -0.5167, -0.1637,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3490630937070953, distance: 1.3291464143134957 entropy -2.555685484527024
epoch: 37, step: 18
	action: tensor([[ 1.7562, -0.4604, -0.7730, -0.7418, -0.0682,  0.7534,  2.5011]],
       dtype=torch.float64)
	q_value: tensor([[-39.5628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3291464143134957 entropy -1.586001228653688
epoch: 37, step: 19
	action: tensor([[-0.8222, -0.2631, -0.1618,  0.9028,  0.2192,  1.0301,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.040086567865904366, distance: 1.16705530511951 entropy -2.555685484527024
epoch: 37, step: 20
	action: tensor([[ 2.1271, -0.3454,  1.5397, -1.1335,  2.7260,  0.6869,  2.8523]],
       dtype=torch.float64)
	q_value: tensor([[-51.6764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.16705530511951 entropy -1.479484059398431
epoch: 37, step: 21
	action: tensor([[-0.2483, -0.4173, -0.1061,  0.1848, -0.3194,  0.1715,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24656225095856898, distance: 1.2776552353775863 entropy -2.555685484527024
epoch: 37, step: 22
	action: tensor([[-0.2940,  0.9713, -1.3617,  1.0414,  0.4202,  0.5968,  2.3952]],
       dtype=torch.float64)
	q_value: tensor([[-39.2615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4934500685356167, distance: 1.398466433826301 entropy -1.6272508759313982
epoch: 37, step: 23
	action: tensor([[-6.8042, -2.2751, -1.2652, -0.3910,  1.4206,  0.1199,  3.1888]],
       dtype=torch.float64)
	q_value: tensor([[-61.7677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.398466433826301 entropy -1.3721682438568137
epoch: 37, step: 24
	action: tensor([[-0.1049, -0.8968,  0.1063,  0.7183,  0.3818,  0.7075,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23405967133853767, distance: 1.0015073979044864 entropy -2.555685484527024
epoch: 37, step: 25
	action: tensor([[-4.0154,  0.7563,  2.5706,  0.0575, -1.0037,  1.6694,  3.1515]],
       dtype=torch.float64)
	q_value: tensor([[-52.6103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0015073979044864 entropy -1.3862168260115284
epoch: 37, step: 26
	action: tensor([[-0.1487,  0.6000, -0.2374,  0.2664, -1.5197,  0.0966,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40699827659871735, distance: 0.8812206610327507 entropy -2.555685484527024
epoch: 37, step: 27
	action: tensor([[-5.8798, -2.4655,  0.7736, -3.7766, -3.5148,  1.9160,  3.3421]],
       dtype=torch.float64)
	q_value: tensor([[-42.4029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8812206610327507 entropy -1.339945740641475
epoch: 37, step: 28
	action: tensor([[ 0.6901, -0.3788, -0.4107,  0.3343, -0.4456, -0.0498,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5611442193637832, distance: 0.7580844788509842 entropy -2.555685484527024
epoch: 37, step: 29
	action: tensor([[ 2.6493, -0.2397,  0.8744, -2.4297,  0.5257,  0.0235,  2.5854]],
       dtype=torch.float64)
	q_value: tensor([[-43.6571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7580844788509842 entropy -1.5542122511120415
epoch: 37, step: 30
	action: tensor([[-1.6361, -0.0527, -0.0399, -1.1452,  0.5565, -0.0473,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7580844788509842 entropy -2.555685484527024
epoch: 37, step: 31
	action: tensor([[-0.0383,  0.2387, -0.2653,  0.2799,  0.5398, -0.1022,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4174672982819986, distance: 0.8734073623863539 entropy -2.555685484527024
epoch: 37, step: 32
	action: tensor([[-0.7484, -1.2109,  0.3860,  0.4816, -1.0106,  0.7625,  1.3114]],
       dtype=torch.float64)
	q_value: tensor([[-33.5114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8734073623863539 entropy -2.12156147894038
epoch: 37, step: 33
	action: tensor([[ 1.1063, -0.8582,  0.4482,  0.1163, -0.4939,  0.0439,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10222882578464154, distance: 1.2014136877766046 entropy -2.555685484527024
epoch: 37, step: 34
	action: tensor([[-1.2427,  0.8248, -1.3455, -0.3702, -4.1627,  0.3841,  4.3052]],
       dtype=torch.float64)
	q_value: tensor([[-55.7649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2014136877766046 entropy -1.1110146011921747
epoch: 37, step: 35
	action: tensor([[-0.9956, -0.5892, -0.7436, -0.4985,  0.1256,  0.1423,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1106101446240673, distance: 1.6624963630329874 entropy -2.555685484527024
epoch: 37, step: 36
	action: tensor([[ 1.1503, -0.3442,  0.4286, -1.1703,  0.3216,  0.5503,  1.3200]],
       dtype=torch.float64)
	q_value: tensor([[-40.9566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05998550361635391, distance: 1.1781664630278685 entropy -2.1006421550594445
epoch: 37, step: 37
	action: tensor([[-4.3796,  3.5255,  1.6193, -4.0183, -2.2562,  1.0546,  4.6509]],
       dtype=torch.float64)
	q_value: tensor([[-57.8083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1781664630278685 entropy -1.0203913130523146
epoch: 37, step: 38
	action: tensor([[-1.2056,  0.2949, -0.2813, -0.7656,  0.5971,  0.3646,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0416490871704425, distance: 1.6351110093945866 entropy -2.555685484527024
epoch: 37, step: 39
	action: tensor([[ 1.3363,  0.6847, -0.5461, -0.3155,  1.4893,  0.5789,  1.1233]],
       dtype=torch.float64)
	q_value: tensor([[-40.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6351110093945866 entropy -2.2011512882052737
epoch: 37, step: 40
	action: tensor([[-0.7646, -0.1004, -0.6260,  0.3717, -0.3395,  0.5748,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7732547385877793, distance: 1.5238501348211477 entropy -2.555685484527024
epoch: 37, step: 41
	action: tensor([[-0.4524,  0.2640,  0.7260, -1.4707,  0.5456,  1.3841,  2.0705]],
       dtype=torch.float64)
	q_value: tensor([[-42.6463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30597793067662393, distance: 1.307749649393206 entropy -1.7581238128088998
epoch: 37, step: 42
	action: tensor([[ 8.6446, -4.3371,  3.6752,  6.9102,  2.9616,  2.3201,  5.3737]],
       dtype=torch.float64)
	q_value: tensor([[-65.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.307749649393206 entropy -0.8950314268785332
epoch: 37, step: 43
	action: tensor([[ 0.6008, -0.3235, -0.1912,  0.1782,  0.3502,  0.0401,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5402462554428693, distance: 0.7759242602268323 entropy -2.555685484527024
epoch: 37, step: 44
	action: tensor([[-0.1952,  0.6340,  0.8991,  0.1553, -0.2144,  0.0117,  2.1064]],
       dtype=torch.float64)
	q_value: tensor([[-40.8293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7759242602268323 entropy -1.7236589396016686
epoch: 37, step: 45
	action: tensor([[ 0.0234,  0.0020, -0.1484, -0.0299, -0.2494, -0.4908,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2667461693600327, distance: 0.9799047514354944 entropy -2.555685484527024
epoch: 37, step: 46
	action: tensor([[-1.0408,  0.3653, -0.1256, -0.4715, -0.4938,  0.0926,  1.7208]],
       dtype=torch.float64)
	q_value: tensor([[-31.3006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9163669518669644, distance: 1.5841490077321647 entropy -1.909756249016958
epoch: 37, step: 47
	action: tensor([[-0.3891,  0.9893, -0.1121, -1.9258, -0.5568,  1.6451,  3.4598]],
       dtype=torch.float64)
	q_value: tensor([[-48.6337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17219946189751234, distance: 1.0411649642782255 entropy -1.3130628400498299
epoch: 37, step: 48
	action: tensor([[-1.7226, -4.9409,  3.9934, -3.7383,  3.1885,  2.0735,  4.6025]],
       dtype=torch.float64)
	q_value: tensor([[-72.9266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0411649642782255 entropy -1.0387413920410558
epoch: 37, step: 49
	action: tensor([[-0.5212,  0.2294,  0.0032,  0.4431,  0.3790,  0.1561,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05886106083882281, distance: 1.110154860039394 entropy -2.555685484527024
epoch: 37, step: 50
	action: tensor([[ 0.7006, -0.4020, -1.2149,  1.1352,  0.8131, -0.2320,  1.7076]],
       dtype=torch.float64)
	q_value: tensor([[-36.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36497739876341406, distance: 0.9119085435845344 entropy -1.9091283724030659
epoch: 37, step: 51
	action: tensor([[-0.3908,  0.3305,  0.1002, -0.3454, -0.5220,  0.2835,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12567117929624838, distance: 1.2141223860732302 entropy -2.555685484527024
epoch: 37, step: 52
	action: tensor([[ 0.7338,  2.6212,  0.8982, -1.6817, -1.1331,  0.6714,  2.2861]],
       dtype=torch.float64)
	q_value: tensor([[-35.8635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2141223860732302 entropy -1.661107269851158
epoch: 37, step: 53
	action: tensor([[ 0.0704, -0.2919,  0.6348, -0.3502, -0.4018, -0.3517,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23288493317335512, distance: 1.270626667324211 entropy -2.555685484527024
epoch: 37, step: 54
	action: tensor([[ 2.6242, -0.3976, -0.9798, -1.3706, -0.5765,  2.6105,  2.9581]],
       dtype=torch.float64)
	q_value: tensor([[-39.4765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.270626667324211 entropy -1.4401333770288802
epoch: 37, step: 55
	action: tensor([[ 0.4394,  0.7106, -0.7447, -0.6446,  0.5200, -0.2512,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9350677251623326, distance: 0.2915996526885256 entropy -2.555685484527024
epoch: 37, step: 56
	action: tensor([[ 0.1108, -0.0866, -0.6305, -0.8815,  0.8479,  0.4800,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39678547623257177, distance: 0.8887765513815675 entropy -2.555685484527024
epoch: 37, step: 57
	action: tensor([[ 1.8072,  0.9412, -0.4058,  0.4287,  0.4666,  0.3559,  1.5721]],
       dtype=torch.float64)
	q_value: tensor([[-40.6329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8887765513815675 entropy -1.9495145867500945
epoch: 37, step: 58
	action: tensor([[ 0.2602, -0.1281,  0.3083, -0.5468,  0.3013,  0.4451,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21273742307272026, distance: 1.0153516922853871 entropy -2.555685484527024
epoch: 37, step: 59
	action: tensor([[ 2.6256, -0.6107,  0.3903, -1.1050, -1.8733, -0.6707,  2.5746]],
       dtype=torch.float64)
	q_value: tensor([[-41.3116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0153516922853871 entropy -1.5350119959720363
epoch: 37, step: 60
	action: tensor([[-0.1607,  0.3777, -0.7194, -0.0642,  0.4223, -0.3790,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3169726989809023, distance: 0.9457486385817336 entropy -2.555685484527024
epoch: 37, step: 61
	action: tensor([[-0.1673, -0.2664, -0.3851,  0.6213,  0.3567,  0.0039,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03878991463201675, distance: 1.1219302368469348 entropy -2.555685484527024
epoch: 37, step: 62
	action: tensor([[-1.0458e+00, -2.2569e+00,  2.4641e-01, -5.5092e-01,  4.0853e-01,
          1.8997e-03,  1.6601e+00]], dtype=torch.float64)
	q_value: tensor([[-38.3705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1219302368469348 entropy -1.9395025696492854
epoch: 37, step: 63
	action: tensor([[-0.2417,  0.4514, -0.1414,  0.1663, -0.1053, -0.1079,  0.3411]],
       dtype=torch.float64)
	q_value: tensor([[-41.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1219302368469348 entropy -2.555685484527024
LOSS epoch 37 actor 469.0107148382354 critic 291.9788995189074
epoch: 38, step: 0
	action: tensor([[-0.2259, -0.5556, -0.1077, -0.4376, -1.4337,  0.0171,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6039078988892419, distance: 1.4492603358793834 entropy -2.3304294176019895
epoch: 38, step: 1
	action: tensor([[ 3.2244,  1.2371, -0.1533, -0.5265, -3.7840,  1.9731,  4.3861]],
       dtype=torch.float64)
	q_value: tensor([[-48.6080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4492603358793834 entropy -1.0991167513912596
epoch: 38, step: 2
	action: tensor([[ 1.2353, -0.0665,  0.2240,  0.0164, -0.5226, -0.1844,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5331655144503127, distance: 0.7818764968070706 entropy -2.3304294176019895
epoch: 38, step: 3
	action: tensor([[ 4.0937,  0.3587,  1.0885,  0.0318, -3.0146,  0.7504,  3.8405]],
       dtype=torch.float64)
	q_value: tensor([[-51.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7818764968070706 entropy -1.1917954919309426
epoch: 38, step: 4
	action: tensor([[-0.6008, -0.7367, -0.5967,  0.3583,  0.5641,  0.2391,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9179323535380444, distance: 1.584795888876319 entropy -2.3304294176019895
epoch: 38, step: 5
	action: tensor([[ 2.2469, -1.2642, -0.0721, -1.9041, -0.5569, -0.1901,  1.9147]],
       dtype=torch.float64)
	q_value: tensor([[-43.6688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.584795888876319 entropy -1.8013604081536916
epoch: 38, step: 6
	action: tensor([[ 0.8226,  0.1444,  0.4891, -0.5888,  0.4314,  0.6447,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7263032951740049, distance: 0.5986754890551489 entropy -2.3304294176019895
epoch: 38, step: 7
	action: tensor([[-0.0513, -2.7721, -0.1282, -1.5183, -2.8438,  3.4156,  3.6865]],
       dtype=torch.float64)
	q_value: tensor([[-47.8199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5986754890551489 entropy -1.2024171978387292
epoch: 38, step: 8
	action: tensor([[-0.2375, -0.3997,  0.2413,  1.7365,  0.1504, -0.2574,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5819817479960723, distance: 0.739868122024514 entropy -2.3304294176019895
epoch: 38, step: 9
	action: tensor([[-2.7069, -0.9640, -2.7860, -1.0773, -2.7198,  2.1532,  3.6576]],
       dtype=torch.float64)
	q_value: tensor([[-55.1591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.739868122024514 entropy -1.2507519857580527
epoch: 38, step: 10
	action: tensor([[ 0.1949,  0.2085,  0.3092, -0.1380, -0.1398,  0.5841,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6551352874105154, distance: 0.6720182580433325 entropy -2.3304294176019895
epoch: 38, step: 11
	action: tensor([[-0.4428,  0.3139,  0.4246, -0.1393, -0.5645,  0.1638,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12700244875646627, distance: 1.214840111853736 entropy -2.3304294176019895
epoch: 38, step: 12
	action: tensor([[-1.3272,  0.2292,  1.8641,  1.8456,  2.7051,  0.0294,  3.2276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.214840111853736 entropy -1.3525425794753185
epoch: 38, step: 13
	action: tensor([[ 0.5069, -0.5756,  0.7771, -0.3900,  0.3223,  0.0705,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1400182191833954, distance: 1.2218350781301734 entropy -2.3304294176019895
epoch: 38, step: 14
	action: tensor([[-0.5349,  0.2174, -1.0352,  0.6476, -1.1726,  2.0594,  3.6712]],
       dtype=torch.float64)
	q_value: tensor([[-48.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2218350781301734 entropy -1.2223701923132084
epoch: 38, step: 15
	action: tensor([[ 1.7713,  0.8207, -0.4554, -0.2496,  0.0754, -0.1655,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2218350781301734 entropy -2.3304294176019895
epoch: 38, step: 16
	action: tensor([[-0.9813, -0.6583, -0.4930,  0.5486, -0.7618,  0.1960,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3492713837678512, distance: 1.7539745503088693 entropy -2.3304294176019895
epoch: 38, step: 17
	action: tensor([[-1.8254,  0.0781,  3.5785,  0.2869,  2.0025,  1.7325,  3.4598]],
       dtype=torch.float64)
	q_value: tensor([[-49.8029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7539745503088693 entropy -1.3065119475155644
epoch: 38, step: 18
	action: tensor([[ 0.2441, -0.2397,  0.2121,  0.0414,  0.4272, -0.0938,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32651568793873464, distance: 0.9391185848902179 entropy -2.3304294176019895
epoch: 38, step: 19
	action: tensor([[ 1.6721, -1.5869, -1.0300,  1.1138,  1.3529, -0.0984,  2.3699]],
       dtype=torch.float64)
	q_value: tensor([[-39.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9391185848902179 entropy -1.6033435741142117
epoch: 38, step: 20
	action: tensor([[-0.3256, -0.2457,  1.2509, -0.7790, -0.3615,  0.2130,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7764690290489433, distance: 1.5252306131081796 entropy -2.3304294176019895
epoch: 38, step: 21
	action: tensor([[-4.6625,  1.8582,  0.9313, -1.7710,  6.0639,  5.6861,  4.7117]],
       dtype=torch.float64)
	q_value: tensor([[-48.7924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5252306131081796 entropy -1.0174933717407173
epoch: 38, step: 22
	action: tensor([[-0.2835, -0.0269,  0.6105, -0.0211,  0.0849, -0.1331,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16100341575764276, distance: 1.2330294286497632 entropy -2.3304294176019895
epoch: 38, step: 23
	action: tensor([[-5.1827, -4.2651,  0.0153, -1.6765,  0.0164,  0.7176,  2.7308]],
       dtype=torch.float64)
	q_value: tensor([[-37.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2330294286497632 entropy -1.4902448377526343
epoch: 38, step: 24
	action: tensor([[-0.2078, -0.0407,  0.4205, -0.4533,  1.6180,  0.2832,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24020495241220274, distance: 1.2743931367194865 entropy -2.3304294176019895
epoch: 38, step: 25
	action: tensor([[-2.9623,  0.8844, -0.6131,  0.0371, -0.2656,  0.6969,  2.4309]],
       dtype=torch.float64)
	q_value: tensor([[-47.9117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2743931367194865 entropy -1.5584576340523364
epoch: 38, step: 26
	action: tensor([[ 1.7283, -0.4761, -0.9775,  0.2429,  0.2689,  0.0248,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2743931367194865 entropy -2.3304294176019895
epoch: 38, step: 27
	action: tensor([[-0.6675, -0.1288,  0.1841,  0.4081,  0.1753, -0.2698,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5094206277665596, distance: 1.4059239643375177 entropy -2.3304294176019895
epoch: 38, step: 28
	action: tensor([[ 1.5763, -0.0653,  2.2777,  0.3796,  1.1520,  2.5077,  2.2210]],
       dtype=torch.float64)
	q_value: tensor([[-36.1669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4059239643375177 entropy -1.6783149007916145
epoch: 38, step: 29
	action: tensor([[-0.2932,  1.3975, -0.3781, -1.6065, -0.2170,  0.1835,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.594050454295192, distance: 0.729109443222461 entropy -2.3304294176019895
epoch: 38, step: 30
	action: tensor([[ 0.3806, -0.6459,  0.1962, -1.2996,  0.3289,  0.6323,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5412994040303414, distance: 1.4206928622427382 entropy -2.3304294176019895
epoch: 38, step: 31
	action: tensor([[ 4.4697, -0.5250, -2.7461,  3.4283, -1.6482,  2.2481,  4.0143]],
       dtype=torch.float64)
	q_value: tensor([[-50.4335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4206928622427382 entropy -1.1470964216059838
epoch: 38, step: 32
	action: tensor([[-0.1833, -0.5625, -0.1249,  0.8047, -1.4593, -0.2479,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11338183616468145, distance: 1.077519170098791 entropy -2.3304294176019895
epoch: 38, step: 33
	action: tensor([[-1.6025,  4.5506,  1.3890, -0.7305, -1.8099,  1.8370,  4.6736]],
       dtype=torch.float64)
	q_value: tensor([[-53.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.077519170098791 entropy -1.0461912625878091
epoch: 38, step: 34
	action: tensor([[ 0.4988, -0.3412, -0.4655, -0.4997,  0.2657,  0.4082,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.275598712891298, distance: 0.9739716144179902 entropy -2.3304294176019895
epoch: 38, step: 35
	action: tensor([[-1.8732,  1.3084,  0.4173,  4.2095, -1.5853, -0.0953,  2.5213]],
       dtype=torch.float64)
	q_value: tensor([[-42.1210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9739716144179902 entropy -1.552533115915951
epoch: 38, step: 36
	action: tensor([[-1.0442, -1.0492,  1.2294, -0.1139,  0.4488,  0.1298,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4590795802085736, distance: 1.7944980263833523 entropy -2.3304294176019895
epoch: 38, step: 37
	action: tensor([[-4.4250, -5.9254, -0.5212,  0.1755,  4.3890,  0.9981,  4.4647]],
       dtype=torch.float64)
	q_value: tensor([[-52.5268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7944980263833523 entropy -1.0706626037222964
epoch: 38, step: 38
	action: tensor([[-0.2223, -0.2071, -0.8887,  0.1464,  0.4045, -0.2458,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1980784569227798, distance: 1.2525622617888046 entropy -2.3304294176019895
epoch: 38, step: 39
	action: tensor([[ 0.6208, -0.8155,  0.7694,  0.7518, -1.0070,  0.4187,  1.1876]],
       dtype=torch.float64)
	q_value: tensor([[-35.0198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5397205571415336, distance: 0.7763677427810088 entropy -2.182911744669519
epoch: 38, step: 40
	action: tensor([[ 0.6624, -9.9028, -2.2629, -2.7738,  3.6049,  1.0198,  6.0255]],
       dtype=torch.float64)
	q_value: tensor([[-67.3624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7763677427810088 entropy -0.8129845682360771
epoch: 38, step: 41
	action: tensor([[ 0.1265, -0.3506, -0.2807, -0.3739, -0.3373,  0.6197,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04507055748324573, distance: 1.1182588274797614 entropy -2.3304294176019895
epoch: 38, step: 42
	action: tensor([[-3.0033, -1.9406,  3.0992, -3.5611, -1.1489,  1.6203,  3.1335]],
       dtype=torch.float64)
	q_value: tensor([[-44.3018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1182588274797614 entropy -1.3696107696762496
epoch: 38, step: 43
	action: tensor([[0.0449, 0.2525, 0.1235, 0.5305, 0.9145, 0.8060, 0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8610565240576782, distance: 0.4265557228123085 entropy -2.3304294176019895
epoch: 38, step: 44
	action: tensor([[ 0.3345, -1.6307, -0.4281, -1.2793,  0.4221,  0.7783,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4265557228123085 entropy -2.3304294176019895
epoch: 38, step: 45
	action: tensor([[-0.0796, -0.1063,  0.6805,  0.1205,  0.7550,  0.0503,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2223242647557505, distance: 1.0091505645579868 entropy -2.3304294176019895
epoch: 38, step: 46
	action: tensor([[-3.8990,  0.4608, -0.6477, -1.4085, -0.0554,  0.8593,  2.6463]],
       dtype=torch.float64)
	q_value: tensor([[-42.2341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0091505645579868 entropy -1.5030265501878157
epoch: 38, step: 47
	action: tensor([[ 0.6293,  0.9002, -0.0917,  0.3774,  0.5532,  0.0524,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0091505645579868 entropy -2.3304294176019895
epoch: 38, step: 48
	action: tensor([[ 0.4104, -0.1606,  0.5258, -0.0221,  1.0014,  0.0640,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5049665101145419, distance: 0.8051447837340883 entropy -2.3304294176019895
epoch: 38, step: 49
	action: tensor([[ 4.3748,  1.6609,  1.1198, -3.8115, -0.7819,  0.8502,  2.7731]],
       dtype=torch.float64)
	q_value: tensor([[-44.9569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8051447837340883 entropy -1.453474276171877
epoch: 38, step: 50
	action: tensor([[ 0.7239, -0.6556,  0.3878,  0.1382, -0.0113,  0.1535,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24231201012147296, distance: 0.9960976027726132 entropy -2.3304294176019895
epoch: 38, step: 51
	action: tensor([[-1.6307, -2.2596,  0.4233,  2.3631, -0.1378, -0.3169,  3.6087]],
       dtype=torch.float64)
	q_value: tensor([[-51.3416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9960976027726132 entropy -1.2439350955321455
epoch: 38, step: 52
	action: tensor([[ 0.3149, -0.1814, -0.6536, -0.0521, -0.7444,  0.1299,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42296409221467357, distance: 0.869276847930461 entropy -2.3304294176019895
epoch: 38, step: 53
	action: tensor([[-3.1255, -1.1631,  0.4720, -0.5983,  1.9405,  0.8128,  2.8118]],
       dtype=torch.float64)
	q_value: tensor([[-41.7069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.869276847930461 entropy -1.4736049875490094
epoch: 38, step: 54
	action: tensor([[-0.8091,  0.6451,  0.7529, -0.0588,  0.0103,  0.3499,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16062030446215037, distance: 1.2328259725394015 entropy -2.3304294176019895
epoch: 38, step: 55
	action: tensor([[0.5791, 3.0260, 0.5126, 1.5704, 2.4158, 0.1177, 3.3196]],
       dtype=torch.float64)
	q_value: tensor([[-39.5318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2328259725394015 entropy -1.3259218180960428
epoch: 38, step: 56
	action: tensor([[ 0.3368,  0.0481,  0.1328, -0.9935,  0.0939,  0.4120,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20111376513312873, distance: 1.022819884149879 entropy -2.3304294176019895
epoch: 38, step: 57
	action: tensor([[-3.5092, -2.2615,  1.6477, -2.6909,  1.2084, -0.0798,  3.1186]],
       dtype=torch.float64)
	q_value: tensor([[-41.8463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.022819884149879 entropy -1.36203387030271
epoch: 38, step: 58
	action: tensor([[ 0.9455, -0.0186,  0.0547, -0.1267, -1.3389,  0.5311,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7278366204253097, distance: 0.5969961602298184 entropy -2.3304294176019895
epoch: 38, step: 59
	action: tensor([[-4.1413, -3.1724,  1.4300,  0.9714, -2.0017,  2.7196,  4.9332]],
       dtype=torch.float64)
	q_value: tensor([[-55.1581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5969961602298184 entropy -0.975829686658118
epoch: 38, step: 60
	action: tensor([[ 0.4644, -1.1972,  0.3714, -0.8601, -0.4753,  0.7234,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8902850041095169, distance: 1.5733318620364736 entropy -2.3304294176019895
epoch: 38, step: 61
	action: tensor([[-1.5974,  4.0355,  1.5124,  3.8473, -3.0962,  2.3156,  5.2585]],
       dtype=torch.float64)
	q_value: tensor([[-58.8058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5733318620364736 entropy -0.9244058031982545
epoch: 38, step: 62
	action: tensor([[ 0.0281, -1.3064,  1.1431,  0.1573, -0.8607,  0.0987,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5733318620364736 entropy -2.3304294176019895
epoch: 38, step: 63
	action: tensor([[ 0.8258,  0.0227, -0.5084,  0.6359,  1.2371,  0.4138,  0.6120]],
       dtype=torch.float64)
	q_value: tensor([[-38.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5733318620364736 entropy -2.3304294176019895
LOSS epoch 38 actor 347.7974698911677 critic 253.52822925479978
epoch: 39, step: 0
	action: tensor([[ 1.1233,  0.3467, -0.1268, -0.2227, -0.9706,  0.4829,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.89341169947523, distance: 0.37360395249291267 entropy -2.2712220176974087
epoch: 39, step: 1
	action: tensor([[ 0.9116,  0.6259,  0.2991, -0.2570,  0.2957,  0.6787,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.37360395249291267 entropy -2.2712220176974087
epoch: 39, step: 2
	action: tensor([[-0.2323,  0.5482, -0.7089, -0.5711, -0.7706,  0.6230,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27335081962075924, distance: 0.975481612008098 entropy -2.2712220176974087
epoch: 39, step: 3
	action: tensor([[ 0.9764, -0.0019, -0.1583,  0.0942,  0.5051,  0.4346,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8726710011640597, distance: 0.408338524753287 entropy -2.2712220176974087
epoch: 39, step: 4
	action: tensor([[-4.6300,  1.5304, -1.6222,  0.8002, -0.4868,  1.1115,  2.1652]],
       dtype=torch.float64)
	q_value: tensor([[-47.5267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.408338524753287 entropy -1.6003967758446613
epoch: 39, step: 5
	action: tensor([[-1.4049,  0.5952, -1.1667, -0.2048, -0.9968, -0.9417,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3242108500274741, distance: 1.3168468349830218 entropy -2.2712220176974087
epoch: 39, step: 6
	action: tensor([[ 0.2388,  0.5816, -0.1139, -2.6269,  0.2865,  1.6665,  1.5987]],
       dtype=torch.float64)
	q_value: tensor([[-35.3142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3168468349830218 entropy -1.884403485385327
epoch: 39, step: 7
	action: tensor([[-0.4732,  0.3334, -0.0482,  0.6417, -0.5050,  0.5444,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1946242972501322, distance: 1.0269657384112667 entropy -2.2712220176974087
epoch: 39, step: 8
	action: tensor([[-1.8879, -0.7286, -1.9789, -2.1053,  2.8043,  0.7727,  2.7235]],
       dtype=torch.float64)
	q_value: tensor([[-44.3111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0269657384112667 entropy -1.4425530996610614
epoch: 39, step: 9
	action: tensor([[ 0.2344,  0.0898,  0.2622, -1.0836,  0.4036,  0.3437,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07979915373471347, distance: 1.097736253791699 entropy -2.2712220176974087
epoch: 39, step: 10
	action: tensor([[ 1.2055,  0.0262, -1.4715, -3.2550,  1.3643, -0.2145,  2.5068]],
       dtype=torch.float64)
	q_value: tensor([[-43.2727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.097736253791699 entropy -1.4819307759208435
epoch: 39, step: 11
	action: tensor([[ 0.0803, -0.2658,  0.4311,  0.9647, -0.2864,  0.3990,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8073701056300183, distance: 0.5022483706398542 entropy -2.2712220176974087
epoch: 39, step: 12
	action: tensor([[-3.4784,  0.9733,  0.9261,  1.6698,  0.8334,  0.6238,  2.8967]],
       dtype=torch.float64)
	q_value: tensor([[-52.2399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5022483706398542 entropy -1.3835122236674018
epoch: 39, step: 13
	action: tensor([[-0.0890, -0.0823,  0.2232,  0.2622, -0.0462,  0.5194,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4254424602685182, distance: 0.867408068020167 entropy -2.2712220176974087
epoch: 39, step: 14
	action: tensor([[ 0.4897, -0.6939, -0.7123,  1.4407, -2.0165,  0.5313,  2.3500]],
       dtype=torch.float64)
	q_value: tensor([[-44.1921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30351152449544416, distance: 0.9550226255761751 entropy -1.5520787491043708
epoch: 39, step: 15
	action: tensor([[ 1.2576, -5.2933, -3.1354, -2.1710, -1.0747,  3.8631,  5.7160]],
       dtype=torch.float64)
	q_value: tensor([[-89.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9550226255761751 entropy -0.823202323939493
epoch: 39, step: 16
	action: tensor([[-0.8961, -1.4964,  0.9508,  0.5177, -0.7326,  0.2885,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9550226255761751 entropy -2.2712220176974087
epoch: 39, step: 17
	action: tensor([[-0.0905, -1.1867, -0.4665, -0.1674,  0.1566, -0.0333,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.866714587157807, distance: 1.5634919656950235 entropy -2.2712220176974087
epoch: 39, step: 18
	action: tensor([[1.9261, 0.7939, 0.1175, 0.0854, 0.7756, 0.2044, 2.2062]],
       dtype=torch.float64)
	q_value: tensor([[-46.8756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5634919656950235 entropy -1.6340381514350761
epoch: 39, step: 19
	action: tensor([[ 0.2858,  0.1728, -0.5021, -1.1086,  0.8007, -0.0553,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3622650222537346, distance: 0.9138539890921261 entropy -2.2712220176974087
epoch: 39, step: 20
	action: tensor([[ 3.2186, -2.3029, -0.2629,  0.2530, -0.3340,  1.3082,  1.6250]],
       dtype=torch.float64)
	q_value: tensor([[-39.4395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9138539890921261 entropy -1.852737520268391
epoch: 39, step: 21
	action: tensor([[-0.8503,  0.7362,  0.1645, -0.1289,  0.0639, -0.5122,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4370233013404692, distance: 1.3717930505505278 entropy -2.2712220176974087
epoch: 39, step: 22
	action: tensor([[ 1.9932, -0.3883,  0.5127, -0.5322,  0.5386,  0.1625,  1.6057]],
       dtype=torch.float64)
	q_value: tensor([[-31.7817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3717930505505278 entropy -1.9070525174457735
epoch: 39, step: 23
	action: tensor([[-9.0426e-01,  3.2124e-01,  4.9926e-01, -3.3775e-01,  2.2228e-01,
         -5.8569e-04,  7.2759e-01]], dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9049273283379935, distance: 1.5794136949351623 entropy -2.2712220176974087
epoch: 39, step: 24
	action: tensor([[ 0.3193,  0.5647,  1.3687,  0.5901,  1.3152, -0.1779,  2.0335]],
       dtype=torch.float64)
	q_value: tensor([[-37.9434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9778228092202, distance: 0.1704158399763634 entropy -1.6947161922890814
epoch: 39, step: 25
	action: tensor([[ 1.7313,  0.1123,  0.0525, -0.0317,  0.4492, -0.1245,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.1704158399763634 entropy -2.2712220176974087
epoch: 39, step: 26
	action: tensor([[-0.7484, -0.5853, -0.1293,  0.9464, -0.5403,  0.3238,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5731696748491522, distance: 1.4353059207270165 entropy -2.2712220176974087
epoch: 39, step: 27
	action: tensor([[ 4.5532,  0.2444, -0.3006,  0.7814, -1.5780, -0.2213,  3.0003]],
       dtype=torch.float64)
	q_value: tensor([[-53.2209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4353059207270165 entropy -1.378694383527541
epoch: 39, step: 28
	action: tensor([[ 0.1576, -0.5415,  0.1187,  0.8157, -0.3944,  0.4497,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5998051972560712, distance: 0.7239230673829136 entropy -2.2712220176974087
epoch: 39, step: 29
	action: tensor([[ 1.9174, -0.0853, -0.5585, -3.0118,  2.0335,  0.4324,  2.9411]],
       dtype=torch.float64)
	q_value: tensor([[-53.8115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7239230673829136 entropy -1.3741267821135317
epoch: 39, step: 30
	action: tensor([[-0.8742,  0.7087,  1.1245,  0.4107, -0.4539, -0.0131,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7239230673829136 entropy -2.2712220176974087
epoch: 39, step: 31
	action: tensor([[-0.4160, -0.0232,  0.2621,  0.3560, -0.2216, -0.2464,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08141223903570172, distance: 1.190014720070455 entropy -2.2712220176974087
epoch: 39, step: 32
	action: tensor([[-1.9517,  0.8787, -3.0134, -1.8681,  2.8362,  0.9575,  2.1091]],
       dtype=torch.float64)
	q_value: tensor([[-37.2888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.190014720070455 entropy -1.6705690821136236
epoch: 39, step: 33
	action: tensor([[ 0.6054, -0.5650,  0.0099, -0.1767, -0.7587,  0.1631,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06430701971114905, distance: 1.1069382099585674 entropy -2.2712220176974087
epoch: 39, step: 34
	action: tensor([[-0.1928, -2.3694,  3.4999, -3.8160,  3.7229,  2.6885,  3.2410]],
       dtype=torch.float64)
	q_value: tensor([[-50.2642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1069382099585674 entropy -1.2870095923870062
epoch: 39, step: 35
	action: tensor([[ 0.0599, -0.2832,  0.2382, -0.4896,  0.0869,  0.2116,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14643285411374696, distance: 1.2252677564831294 entropy -2.2712220176974087
epoch: 39, step: 36
	action: tensor([[-0.2848, -1.4540,  1.6466, -0.9533, -2.3746,  0.1120,  2.2680]],
       dtype=torch.float64)
	q_value: tensor([[-41.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2252677564831294 entropy -1.5805592989992976
epoch: 39, step: 37
	action: tensor([[-0.5483,  0.3035, -0.1174, -1.4700, -0.0571,  0.3306,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41498013598438366, distance: 1.3612311062057885 entropy -2.2712220176974087
epoch: 39, step: 38
	action: tensor([[-1.0907, -0.1688,  1.3681,  1.2390, -1.6166,  1.4929,  2.3373]],
       dtype=torch.float64)
	q_value: tensor([[-42.2300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03935170550736378, distance: 1.1216023260699173 entropy -1.5754647616552873
epoch: 39, step: 39
	action: tensor([[ -2.9855, -12.8282,  -3.6848,   0.4879,  -5.9443,   2.4810,   7.8101]],
       dtype=torch.float64)
	q_value: tensor([[-87.2214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1216023260699173 entropy -0.5385946202659424
epoch: 39, step: 40
	action: tensor([[ 0.4918, -0.9986,  1.0789,  0.0935,  0.4865,  0.2877,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28392999267974095, distance: 1.2966637368897869 entropy -2.2712220176974087
epoch: 39, step: 41
	action: tensor([[ 5.1635, -2.9421,  1.1730, -1.1182,  5.1196,  2.3150,  3.4028]],
       dtype=torch.float64)
	q_value: tensor([[-59.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2966637368897869 entropy -1.22683433108741
epoch: 39, step: 42
	action: tensor([[ 0.7220, -0.2861,  0.1604, -0.3229, -0.0576, -0.2086,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21157285497461675, distance: 1.0161024006639223 entropy -2.2712220176974087
epoch: 39, step: 43
	action: tensor([[-0.8735,  0.3065,  1.6363, -0.9497,  1.7867,  0.8925,  2.3806]],
       dtype=torch.float64)
	q_value: tensor([[-44.3186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.02285101097794, distance: 1.6275661232059893 entropy -1.5423482971151155
epoch: 39, step: 44
	action: tensor([[ 5.9617,  1.9608,  2.7531,  2.6992, -1.8865,  1.4852,  4.3701]],
       dtype=torch.float64)
	q_value: tensor([[-67.9890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6275661232059893 entropy -0.9820347201835018
epoch: 39, step: 45
	action: tensor([[ 1.6050, -0.2699, -0.0048, -0.8069,  0.0092,  0.5601,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12057389162622223, distance: 1.2113703599598475 entropy -2.2712220176974087
epoch: 39, step: 46
	action: tensor([[-6.3731, -0.8041,  0.6161, -2.9070,  4.0053,  1.1685,  3.8215]],
       dtype=torch.float64)
	q_value: tensor([[-56.1734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2113703599598475 entropy -1.1221377916801567
epoch: 39, step: 47
	action: tensor([[ 0.5113,  0.2345,  1.0701, -1.1621, -0.5696,  0.2730,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31059068586357585, distance: 0.9501567687847126 entropy -2.2712220176974087
epoch: 39, step: 48
	action: tensor([[ 7.9161, -2.6753, -2.9364, -3.3661,  4.0462,  0.5347,  4.1407]],
       dtype=torch.float64)
	q_value: tensor([[-52.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9501567687847126 entropy -1.0590744953337228
epoch: 39, step: 49
	action: tensor([[-0.2164, -0.4611,  0.2342, -0.3525, -0.6343,  0.2194,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5507639617778823, distance: 1.4250481648653648 entropy -2.2712220176974087
epoch: 39, step: 50
	action: tensor([[-4.2936, -3.1412,  0.6656, -2.8045,  1.2009,  1.2037,  3.0421]],
       dtype=torch.float64)
	q_value: tensor([[-44.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4250481648653648 entropy -1.3447378620901635
epoch: 39, step: 51
	action: tensor([[-0.7767, -0.6096,  0.1855, -0.7133,  0.7362,  0.3259,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.110870930409134, distance: 1.662599068411494 entropy -2.2712220176974087
epoch: 39, step: 52
	action: tensor([[-2.9131,  0.1721, -0.0343, -0.1435,  0.0701,  2.1060,  2.0573]],
       dtype=torch.float64)
	q_value: tensor([[-46.1902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.662599068411494 entropy -1.674914870365416
epoch: 39, step: 53
	action: tensor([[-0.5994,  0.1526, -0.0911,  0.8450,  0.9355,  0.4319,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23741230186042328, distance: 0.9993131234460016 entropy -2.2712220176974087
epoch: 39, step: 54
	action: tensor([[ 1.1362, -1.5205, -0.8410,  0.1863,  1.4529,  0.7921,  1.5517]],
       dtype=torch.float64)
	q_value: tensor([[-46.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9993131234460016 entropy -1.9076405001822512
epoch: 39, step: 55
	action: tensor([[ 0.8128,  0.8573, -0.0803,  0.9214, -0.2089,  0.1635,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9993131234460016 entropy -2.2712220176974087
epoch: 39, step: 56
	action: tensor([[ 1.9583,  0.5121,  0.2595,  0.4309, -0.9130,  0.8162,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9993131234460016 entropy -2.2712220176974087
epoch: 39, step: 57
	action: tensor([[ 0.5051,  0.2346,  0.1720, -0.7238, -0.3504,  0.5282,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5772552102579587, distance: 0.7440392136936079 entropy -2.2712220176974087
epoch: 39, step: 58
	action: tensor([[-3.8877, -1.6012, -1.7921,  1.0861,  2.4054,  0.3205,  2.9751]],
       dtype=torch.float64)
	q_value: tensor([[-43.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7440392136936079 entropy -1.3413840234294288
epoch: 39, step: 59
	action: tensor([[ 1.2006, -0.4232, -0.4048, -0.4973, -0.5048, -0.5740,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14949800734885588, distance: 1.2269046279570248 entropy -2.2712220176974087
epoch: 39, step: 60
	action: tensor([[-1.7886,  2.3473, -0.6923, -0.5281,  2.0964, -0.1773,  2.7407]],
       dtype=torch.float64)
	q_value: tensor([[-48.3248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2269046279570248 entropy -1.4387955675060262
epoch: 39, step: 61
	action: tensor([[-0.3116, -0.4334,  0.1133,  1.1414,  0.2186,  0.3036,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3905679096502448, distance: 0.8933452913219869 entropy -2.2712220176974087
epoch: 39, step: 62
	action: tensor([[-0.8922, -0.8665, -0.1563, -1.1681, -1.9471,  2.1214,  2.3051]],
       dtype=torch.float64)
	q_value: tensor([[-52.0162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4702849055207272, distance: 1.3875781184842928 entropy -1.592117427295374
epoch: 39, step: 63
	action: tensor([[ 0.0922, -0.0642, -0.3758,  0.8540, -0.2838,  0.1102,  0.7276]],
       dtype=torch.float64)
	q_value: tensor([[-37.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3875781184842928 entropy -2.2712220176974087
LOSS epoch 39 actor 381.99068839431067 critic 449.3585532611405
epoch: 40, step: 0
	action: tensor([[ 1.2789,  0.7372,  1.0358, -0.6039,  0.6579,  0.8308,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388765351500427, distance: 0.28291806274505643 entropy -2.3509751735272806
epoch: 40, step: 1
	action: tensor([[-0.0447, -0.3684, -0.5396,  0.2837,  0.2133,  0.3514,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058419153635852594, distance: 1.1104154633542551 entropy -2.3509751735272806
epoch: 40, step: 2
	action: tensor([[ 0.3783, -1.1408,  0.1823,  1.8773, -1.6170, -0.2220,  1.5307]],
       dtype=torch.float64)
	q_value: tensor([[-42.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8417974830546391, distance: 0.4551592651774099 entropy -1.9063616698062873
epoch: 40, step: 3
	action: tensor([[-5.6572, -0.2245, -0.4157, -1.0321,  2.3020,  3.0276,  5.0567]],
       dtype=torch.float64)
	q_value: tensor([[-75.7478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4551592651774099 entropy -0.894707308328819
epoch: 40, step: 4
	action: tensor([[-0.1836, -0.7243, -0.1398, -0.0896, -0.4277,  0.1367,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5752894650814064, distance: 1.4362726070541427 entropy -2.3509751735272806
epoch: 40, step: 5
	action: tensor([[ 1.6030,  1.7716, -0.7153,  2.5056, -1.6264,  0.7283,  2.4499]],
       dtype=torch.float64)
	q_value: tensor([[-43.5775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4362726070541427 entropy -1.5065433754990651
epoch: 40, step: 6
	action: tensor([[ 0.0908, -0.3919, -0.7525, -0.1109, -0.4826,  0.5845,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018902157109703666, distance: 1.155108910767153 entropy -2.3509751735272806
epoch: 40, step: 7
	action: tensor([[ 0.2732,  0.3719, -1.0874,  0.9528,  0.9260,  0.9540,  2.4703]],
       dtype=torch.float64)
	q_value: tensor([[-44.7026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.155108910767153 entropy -1.5008758025576354
epoch: 40, step: 8
	action: tensor([[ 0.4588, -0.4435,  0.4712, -0.2146,  0.3945,  0.7272,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.155108910767153 entropy -2.3509751735272806
epoch: 40, step: 9
	action: tensor([[-0.9937, -0.3509,  0.2933,  0.6683, -0.5032,  0.1728,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7090528980256454, distance: 1.496009825166202 entropy -2.3509751735272806
epoch: 40, step: 10
	action: tensor([[ 3.1326, -1.8078,  0.8554,  7.1257,  1.3804,  1.3056,  2.7620]],
       dtype=torch.float64)
	q_value: tensor([[-47.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.496009825166202 entropy -1.4138836624687496
epoch: 40, step: 11
	action: tensor([[-0.3090, -0.3433,  0.1366,  0.2371, -0.6522,  0.4385,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17779506356287023, distance: 1.2419141008470338 entropy -2.3509751735272806
epoch: 40, step: 12
	action: tensor([[ 3.1925, -1.8598, -2.9746, -1.1498, -0.4588,  1.4998,  2.7920]],
       dtype=torch.float64)
	q_value: tensor([[-44.7675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2419141008470338 entropy -1.38758413612498
epoch: 40, step: 13
	action: tensor([[ 0.6433, -0.5871,  0.1105, -0.9347, -0.7688,  0.2382,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42013951099316516, distance: 1.3637105443323216 entropy -2.3509751735272806
epoch: 40, step: 14
	action: tensor([[-0.3993,  2.7842, -1.2504, -1.0363,  0.3530,  1.2884,  3.6183]],
       dtype=torch.float64)
	q_value: tensor([[-49.9225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3637105443323216 entropy -1.158057373898241
epoch: 40, step: 15
	action: tensor([[ 0.3517, -0.7367, -0.8975,  0.1922, -0.1334,  0.4359,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08115024770403845, distance: 1.1898705602323225 entropy -2.3509751735272806
epoch: 40, step: 16
	action: tensor([[ 1.8401, -0.9861, -1.2637,  1.5794, -1.0096, -0.0884,  2.2265]],
       dtype=torch.float64)
	q_value: tensor([[-47.7541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1898705602323225 entropy -1.5934300212827694
epoch: 40, step: 17
	action: tensor([[-0.4311, -0.0555,  0.9271, -0.0500, -1.1430, -0.3029,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37662071219751514, distance: 1.3426531745711707 entropy -2.3509751735272806
epoch: 40, step: 18
	action: tensor([[-2.2518, -4.6177,  2.3621, -2.1457, -4.8147,  3.8527,  3.5421]],
       dtype=torch.float64)
	q_value: tensor([[-45.4206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3426531745711707 entropy -1.1881409701845225
epoch: 40, step: 19
	action: tensor([[ 0.1283, -0.6741, -0.6152, -0.1218,  0.0088,  0.3833,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.180165497095935, distance: 1.2431632125120946 entropy -2.3509751735272806
epoch: 40, step: 20
	action: tensor([[ 0.0417,  0.7897, -0.7977, -0.0222,  1.8683,  1.5516,  2.0346]],
       dtype=torch.float64)
	q_value: tensor([[-44.7146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2431632125120946 entropy -1.6660636450326047
epoch: 40, step: 21
	action: tensor([[ 0.3950, -0.9562, -0.2830,  0.6944, -1.1624,  0.5153,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03329197663674377, distance: 1.1251342750318103 entropy -2.3509751735272806
epoch: 40, step: 22
	action: tensor([[ 1.1982, -3.8949,  0.7829,  5.9397, -0.0931, -1.2940,  3.9343]],
       dtype=torch.float64)
	q_value: tensor([[-58.9271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1251342750318103 entropy -1.1000247420193845
epoch: 40, step: 23
	action: tensor([[-0.0914, -0.4705,  0.2368,  0.4649, -0.4918,  1.0780,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3433310119764612, distance: 0.9273206834372828 entropy -2.3509751735272806
epoch: 40, step: 24
	action: tensor([[-1.5687, -0.0590,  1.7069, -0.7775, -0.9252,  2.0119,  3.3157]],
       dtype=torch.float64)
	q_value: tensor([[-53.4794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6964694791489752, distance: 1.490492237767785 entropy -1.2298564735258053
epoch: 40, step: 25
	action: tensor([[ 4.3570, -7.3051,  4.4610,  0.2144,  3.3076,  2.5582,  8.6909]],
       dtype=torch.float64)
	q_value: tensor([[-86.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.490492237767785 entropy -0.40155910675492973
epoch: 40, step: 26
	action: tensor([[-0.4110, -0.2636, -0.4897,  0.1349,  0.1674,  0.2597,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3784103987389129, distance: 1.34352565448369 entropy -2.3509751735272806
epoch: 40, step: 27
	action: tensor([[ 0.1623, -0.6671, -0.2331,  0.4544, -0.1624,  0.5569,  1.3774]],
       dtype=torch.float64)
	q_value: tensor([[-39.6742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20344364600087583, distance: 1.021327313367715 entropy -2.0013152078852365
epoch: 40, step: 28
	action: tensor([[-0.0288,  1.2490,  0.4709,  0.8308,  3.4317,  2.0678,  2.7991]],
       dtype=torch.float64)
	q_value: tensor([[-55.8358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.021327313367715 entropy -1.3823082193818739
epoch: 40, step: 29
	action: tensor([[-0.3592, -0.0088, -0.2258,  0.2531, -0.0245,  0.2520,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043737033965265404, distance: 1.1691015598921914 entropy -2.3509751735272806
epoch: 40, step: 30
	action: tensor([[ 1.9483,  0.4430,  0.6763,  0.5819, -0.2849,  0.7050,  1.5648]],
       dtype=torch.float64)
	q_value: tensor([[-38.3253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1691015598921914 entropy -1.886484355475173
epoch: 40, step: 31
	action: tensor([[ 0.3467, -0.6266, -0.2179,  0.6628,  0.7084, -0.0301,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39609396826572707, distance: 0.8892858395401276 entropy -2.3509751735272806
epoch: 40, step: 32
	action: tensor([[-1.8482,  0.2536,  0.1479, -1.7515, -0.2507,  0.3388,  1.5020]],
       dtype=torch.float64)
	q_value: tensor([[-48.7566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8892858395401276 entropy -1.9122655934880632
epoch: 40, step: 33
	action: tensor([[ 0.2111, -0.4831,  0.3407, -0.0678,  0.1033,  0.1563,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07748257883784082, distance: 1.0991171424306536 entropy -2.3509751735272806
epoch: 40, step: 34
	action: tensor([[ 0.1110,  1.6529, -1.4725, -0.5721,  0.4700,  1.3394,  2.1154]],
       dtype=torch.float64)
	q_value: tensor([[-44.3993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0991171424306536 entropy -1.607410838548698
epoch: 40, step: 35
	action: tensor([[ 0.3869, -0.4120, -0.2928,  0.2074,  1.3051,  0.3448,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20939806280864337, distance: 1.0175028406224498 entropy -2.3509751735272806
epoch: 40, step: 36
	action: tensor([[ 0.5726, -0.4076, -1.3212,  2.1616, -0.5924,  0.2118,  1.4741]],
       dtype=torch.float64)
	q_value: tensor([[-49.6871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.778209194517821, distance: 0.5389251637427567 entropy -1.898947063099859
epoch: 40, step: 37
	action: tensor([[ 2.7252,  2.3042, -3.9474, -2.2874, -2.3576,  1.2079,  3.2093]],
       dtype=torch.float64)
	q_value: tensor([[-70.8756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5389251637427567 entropy -1.2847268410460357
epoch: 40, step: 38
	action: tensor([[-1.0861,  1.0615,  0.2104, -0.5733,  0.5153,  0.6094,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2592332325906539, distance: 1.2841323343586024 entropy -2.3509751735272806
epoch: 40, step: 39
	action: tensor([[ 0.9685, -2.2339,  0.0154,  0.2622, -1.1687,  1.5796,  1.7092]],
       dtype=torch.float64)
	q_value: tensor([[-42.7559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2841323343586024 entropy -1.8011386756208523
epoch: 40, step: 40
	action: tensor([[-0.0425, -0.0534, -0.0145, -0.2857, -0.2402,  0.6720,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19878750884858343, distance: 1.024307963122402 entropy -2.3509751735272806
epoch: 40, step: 41
	action: tensor([[-0.3084, -1.2153, -1.2906, -1.1932, -1.4445,  0.1936,  2.3723]],
       dtype=torch.float64)
	q_value: tensor([[-42.0625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015171159981436588, distance: 1.1529920929053274 entropy -1.5089900006155414
epoch: 40, step: 42
	action: tensor([[ 0.3603, -1.5563, -6.9413,  6.5353, -3.2912,  0.5300,  4.7544]],
       dtype=torch.float64)
	q_value: tensor([[-74.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1529920929053274 entropy -0.9569773378185177
epoch: 40, step: 43
	action: tensor([[-1.1352, -0.2141, -0.1166, -1.6074, -0.9843,  1.0986,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0735217283694176, distance: 1.6478246248794566 entropy -2.3509751735272806
epoch: 40, step: 44
	action: tensor([[ 1.4317,  2.3017,  0.9798, -0.6774, -2.2134,  1.8086,  4.0485]],
       dtype=torch.float64)
	q_value: tensor([[-53.9448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6478246248794566 entropy -1.0741115032748438
epoch: 40, step: 45
	action: tensor([[-1.3041,  0.5972, -0.1491,  0.3446,  0.6642, -0.0277,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6213318986899528, distance: 1.4571110675201469 entropy -2.3509751735272806
epoch: 40, step: 46
	action: tensor([[-0.5053, -0.5405, -0.4354, -0.8087,  0.3066,  0.4620,  1.0333]],
       dtype=torch.float64)
	q_value: tensor([[-41.2764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6040058282093943, distance: 1.4493045787293661 entropy -2.2035093173586304
epoch: 40, step: 47
	action: tensor([[ 0.1502,  1.5840, -1.4749, -3.9305,  2.3076,  0.8015,  2.0775]],
       dtype=torch.float64)
	q_value: tensor([[-47.7634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4493045787293661 entropy -1.6423418986617673
epoch: 40, step: 48
	action: tensor([[-0.2306,  0.0295, -0.1836, -1.3056, -0.7043,  0.5738,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33639392488649533, distance: 1.3228906277333126 entropy -2.3509751735272806
epoch: 40, step: 49
	action: tensor([[-0.3770, -2.1817,  1.8124,  0.5359, -2.9757,  1.1596,  3.0511]],
       dtype=torch.float64)
	q_value: tensor([[-44.3160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3228906277333126 entropy -1.3096475420857634
epoch: 40, step: 50
	action: tensor([[-0.6147, -0.7640, -0.0677,  1.1166,  0.8631, -0.0782,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4085528662764675, distance: 1.3581360248637948 entropy -2.3509751735272806
epoch: 40, step: 51
	action: tensor([[-0.0116, -2.0418,  0.2447, -0.4853,  1.4325,  0.5160,  1.5551]],
       dtype=torch.float64)
	q_value: tensor([[-52.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3581360248637948 entropy -1.8894308662380028
epoch: 40, step: 52
	action: tensor([[ 0.4903,  0.3417,  0.4298, -0.7277,  0.0639,  0.0348,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5260817860730653, distance: 0.7877862444605801 entropy -2.3509751735272806
epoch: 40, step: 53
	action: tensor([[-0.9811, -0.2781,  0.7086, -2.3272,  1.4888,  0.4546,  2.2347]],
       dtype=torch.float64)
	q_value: tensor([[-41.5475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7877862444605801 entropy -1.5488110977002216
epoch: 40, step: 54
	action: tensor([[ 0.4582,  0.0752,  0.7030, -0.2593, -0.1406,  0.5334,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7242441294290249, distance: 0.6009233459337945 entropy -2.3509751735272806
epoch: 40, step: 55
	action: tensor([[6.5120, 0.3788, 0.3441, 1.0627, 4.6166, 1.1014, 2.7340]],
       dtype=torch.float64)
	q_value: tensor([[-45.7900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6009233459337945 entropy -1.371356726521806
epoch: 40, step: 56
	action: tensor([[-0.2087, -0.1184,  0.8358, -0.8966,  1.0834,  0.4324,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5734963179114212, distance: 1.4354549219424895 entropy -2.3509751735272806
epoch: 40, step: 57
	action: tensor([[-0.2860,  1.3450, -0.9533, -0.4751,  1.7663,  0.8860,  2.4025]],
       dtype=torch.float64)
	q_value: tensor([[-50.5454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4354549219424895 entropy -1.471937973908074
epoch: 40, step: 58
	action: tensor([[ 0.7292, -0.3322, -0.8069,  0.3197,  0.1492,  0.1246,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5645125066740317, distance: 0.7551696662156178 entropy -2.3509751735272806
epoch: 40, step: 59
	action: tensor([[-0.9708,  1.0104, -0.8119,  0.8963,  0.4871,  2.3935,  1.6995]],
       dtype=torch.float64)
	q_value: tensor([[-44.1198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7551696662156178 entropy -1.8074825508610401
epoch: 40, step: 60
	action: tensor([[-0.3326,  0.3322, -0.6701,  0.9356,  0.0731,  0.2718,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010025297659617127, distance: 1.1500661447351976 entropy -2.3509751735272806
epoch: 40, step: 61
	action: tensor([[ 1.2264,  0.3650,  1.5785, -1.1307, -0.3298,  0.2145,  1.4225]],
       dtype=torch.float64)
	q_value: tensor([[-43.0594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9111973036271763, distance: 0.3410120846655848 entropy -1.9636604360795218
epoch: 40, step: 62
	action: tensor([[ 0.1849, -1.3667, -0.2194,  0.4874,  0.5052, -0.3671,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3410120846655848 entropy -2.3509751735272806
epoch: 40, step: 63
	action: tensor([[-0.8392, -0.2107,  0.6824, -0.5716,  0.2353,  0.0197,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-40.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2987439119052064, distance: 1.7350100280992737 entropy -2.3509751735272806
LOSS epoch 40 actor 479.1886865404339 critic 249.44572970347247
epoch: 41, step: 0
	action: tensor([[-1.8974, -1.6694,  0.9344, -1.2986, -0.3409,  2.7076,  2.7620]],
       dtype=torch.float64)
	q_value: tensor([[-42.2621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7350100280992737 entropy -1.410874479833012
epoch: 41, step: 1
	action: tensor([[-0.9637, -1.1238, -0.8295,  0.5749,  0.6094, -0.7123,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4650283858304105, distance: 1.7966672672113062 entropy -2.1860493222655486
epoch: 41, step: 2
	action: tensor([[-1.2099,  0.1645,  0.6244,  0.8920,  0.0118,  0.8732,  1.5698]],
       dtype=torch.float64)
	q_value: tensor([[-46.5595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2099187730051586, distance: 1.017167709030458 entropy -1.872376113754403
epoch: 41, step: 3
	action: tensor([[ 2.7126,  0.7056,  2.2353, -5.6812,  2.8468,  1.1248,  4.2274]],
       dtype=torch.float64)
	q_value: tensor([[-54.9124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.017167709030458 entropy -1.0480486289546607
epoch: 41, step: 4
	action: tensor([[ 0.4705,  0.1713, -0.7745, -0.4429,  2.0438,  0.0268,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7283506324698119, distance: 0.5964321456276066 entropy -2.1860493222655486
epoch: 41, step: 5
	action: tensor([[ 1.6994,  0.7000, -0.2520, -0.7501, -0.6487,  0.2836,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5964321456276066 entropy -2.1860493222655486
epoch: 41, step: 6
	action: tensor([[-0.8822,  0.4869,  0.0088, -0.5655,  0.8242, -0.1714,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7880130549141744, distance: 1.5301782900869965 entropy -2.1860493222655486
epoch: 41, step: 7
	action: tensor([[-0.1661,  2.2893,  1.3959, -0.4744,  1.7191,  1.2138,  1.3328]],
       dtype=torch.float64)
	q_value: tensor([[-39.4512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5301782900869965 entropy -1.9980706038333744
epoch: 41, step: 8
	action: tensor([[ 1.8200, -1.3811, -0.3280,  0.2991,  0.0713,  0.7111,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5301782900869965 entropy -2.1860493222655486
epoch: 41, step: 9
	action: tensor([[ 0.6914, -0.3554, -0.1757, -2.1562, -1.4312,  0.7171,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29297234279307127, distance: 1.301221740899717 entropy -2.1860493222655486
epoch: 41, step: 10
	action: tensor([[-7.4794, -5.2318, -2.1036,  3.9932,  7.6682,  3.5323,  7.1390]],
       dtype=torch.float64)
	q_value: tensor([[-58.5284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.301221740899717 entropy -0.5993461937615436
epoch: 41, step: 11
	action: tensor([[-0.1018,  0.0655, -0.6530, -0.1497, -0.4353,  0.6999,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1286595974740653, distance: 1.0681951929857945 entropy -2.1860493222655486
epoch: 41, step: 12
	action: tensor([[-0.0948,  0.4596, -0.3142,  0.8770,  4.4111,  0.8597,  2.9678]],
       dtype=torch.float64)
	q_value: tensor([[-40.2255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0681951929857945 entropy -1.3519391971359411
epoch: 41, step: 13
	action: tensor([[-0.8142,  0.3869,  0.0820, -0.0425, -0.7280,  0.0853,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49254873473846383, distance: 1.3980443657142207 entropy -2.1860493222655486
epoch: 41, step: 14
	action: tensor([[ 3.0216,  0.8158, -0.1238,  0.9661,  0.0516,  0.3261,  3.0272]],
       dtype=torch.float64)
	q_value: tensor([[-35.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3980443657142207 entropy -1.345960836579596
epoch: 41, step: 15
	action: tensor([[-0.0916, -1.0448, -0.3083,  0.0245,  0.3702,  0.5243,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5259004868615251, distance: 1.4135780702660297 entropy -2.1860493222655486
epoch: 41, step: 16
	action: tensor([[ 0.9555,  0.2833, -1.0620,  0.6242,  1.0734,  1.1472,  2.8858]],
       dtype=torch.float64)
	q_value: tensor([[-47.9938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9521601982064148, distance: 0.2502945418508706 entropy -1.3790724770751182
epoch: 41, step: 17
	action: tensor([[-2.8172, -0.9129,  0.9278,  0.8093,  4.2776,  0.1405,  3.8892]],
       dtype=torch.float64)
	q_value: tensor([[-69.7014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.2502945418508706 entropy -1.0901268242051179
epoch: 41, step: 18
	action: tensor([[ 0.4966, -1.1377,  0.7905,  0.7357,  0.3576,  0.4181,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21123936177533864, distance: 1.0163172762022343 entropy -2.1860493222655486
epoch: 41, step: 19
	action: tensor([[ 7.3657, -2.8232,  0.5363,  2.5558, -0.2686,  1.7176,  3.7369]],
       dtype=torch.float64)
	q_value: tensor([[-58.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0163172762022343 entropy -1.1431900424411705
epoch: 41, step: 20
	action: tensor([[-0.4077, -0.1721,  0.5797, -0.6962,  0.0920,  0.1760,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7639111779738681, distance: 1.519830128224382 entropy -2.1860493222655486
epoch: 41, step: 21
	action: tensor([[-1.1962,  0.9169, -0.7993,  0.9823,  4.8682,  1.0471,  2.9565]],
       dtype=torch.float64)
	q_value: tensor([[-41.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5717111673458166, distance: 1.4346404203936476 entropy -1.3447023407829057
epoch: 41, step: 22
	action: tensor([[-4.7743, -2.6069, -1.7974, -2.2681,  2.5651,  3.1437,  6.5305]],
       dtype=torch.float64)
	q_value: tensor([[-69.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4346404203936476 entropy -0.6854186295420189
epoch: 41, step: 23
	action: tensor([[ 0.5147, -0.5437,  0.4966, -0.0704, -0.2788,  0.3811,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2660010680850481, distance: 0.9804024937329294 entropy -2.1860493222655486
epoch: 41, step: 24
	action: tensor([[-1.4570,  2.4044, -0.1792,  1.4599,  0.3341,  2.4658,  3.6697]],
       dtype=torch.float64)
	q_value: tensor([[-47.3030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9804024937329294 entropy -1.1567416836286781
epoch: 41, step: 25
	action: tensor([[-2.1917, -0.8731,  0.3640, -1.0314, -0.0117,  0.3383,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9804024937329294 entropy -2.1860493222655486
epoch: 41, step: 26
	action: tensor([[-0.5332, -0.3338, -0.1061,  1.0733,  1.2756, -0.7929,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3720813724756704, distance: 1.3404376799809208 entropy -2.1860493222655486
epoch: 41, step: 27
	action: tensor([[ 0.2644, -1.0778,  0.5444,  0.8566, -0.2728,  0.2079,  1.2593]],
       dtype=torch.float64)
	q_value: tensor([[-49.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3013596805607365, distance: 0.9564967883625219 entropy -2.048260249673396
epoch: 41, step: 28
	action: tensor([[ 1.5476, -0.9072,  4.3442, -1.4880,  0.5352,  0.2191,  4.0945]],
       dtype=torch.float64)
	q_value: tensor([[-56.6322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2585917846166519, distance: 1.2838052269620315 entropy -1.074952594268186
epoch: 41, step: 29
	action: tensor([[-2.6142, -7.8127, -1.4636, -2.9251,  7.0928,  0.7416,  6.9451]],
       dtype=torch.float64)
	q_value: tensor([[-65.1136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2838052269620315 entropy -0.6137209326203694
epoch: 41, step: 30
	action: tensor([[-0.0341,  0.2435, -0.5210, -0.0997,  1.1944, -0.2741,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117488978916425, distance: 0.9493582985806921 entropy -2.1860493222655486
epoch: 41, step: 31
	action: tensor([[ 1.1647, -0.4162,  0.7561, -1.0424,  0.5160,  0.5835,  1.0236]],
       dtype=torch.float64)
	q_value: tensor([[-40.7885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0005731538460321817, distance: 1.1440162643409477 entropy -2.190415129187383
epoch: 41, step: 32
	action: tensor([[ 0.2273,  0.3035,  0.6464,  0.4797,  0.4842, -0.0325,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1440162643409477 entropy -2.1860493222655486
epoch: 41, step: 33
	action: tensor([[ 0.3696,  0.6181,  1.0605, -1.2065, -0.6006,  0.0164,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2687977964339666, distance: 0.9785329158806844 entropy -2.1860493222655486
epoch: 41, step: 34
	action: tensor([[-2.1474, -0.5595, -0.1796,  0.5682, -0.6125, -0.6107,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9785329158806844 entropy -2.1860493222655486
epoch: 41, step: 35
	action: tensor([[-0.1715, -1.1908, -0.2864, -0.4864,  0.2316, -0.1401,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9253510998028145, distance: 1.587858001834211 entropy -2.1860493222655486
epoch: 41, step: 36
	action: tensor([[-1.4306,  0.9973, -1.6674, -3.2382,  2.1348,  2.5303,  2.8343]],
       dtype=torch.float64)
	q_value: tensor([[-45.5363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.587858001834211 entropy -1.3975230777196157
epoch: 41, step: 37
	action: tensor([[ 0.7867, -0.3019,  0.0962,  0.4033, -0.7761,  0.4632,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8013484251494902, distance: 0.5100381942021576 entropy -2.1860493222655486
epoch: 41, step: 38
	action: tensor([[ 6.0561,  0.2457, -2.4531, -4.1612,  1.6565,  0.6156,  4.0801]],
       dtype=torch.float64)
	q_value: tensor([[-47.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5100381942021576 entropy -1.0698284640165139
epoch: 41, step: 39
	action: tensor([[ 0.7416, -1.3582, -0.4363, -1.6308, -0.6814,  0.4544,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5100381942021576 entropy -2.1860493222655486
epoch: 41, step: 40
	action: tensor([[-0.3753,  0.1996,  0.2103,  1.5847, -0.2842, -0.1335,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5100381942021576 entropy -2.1860493222655486
epoch: 41, step: 41
	action: tensor([[ 0.6863,  0.0878,  0.0121, -0.0731,  0.8321,  0.2974,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8192650322351633, distance: 0.4864943336347984 entropy -2.1860493222655486
epoch: 41, step: 42
	action: tensor([[-2.7305,  0.1359,  0.4440, -0.2151, -0.4199,  0.8656,  2.1404]],
       dtype=torch.float64)
	q_value: tensor([[-44.5394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4864943336347984 entropy -1.5926245133598462
epoch: 41, step: 43
	action: tensor([[-1.8288,  0.2032, -0.7902, -0.9757,  1.0497,  0.2085,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4864943336347984 entropy -2.1860493222655486
epoch: 41, step: 44
	action: tensor([[-0.5983, -1.3487,  0.0200,  1.5500, -0.1654,  0.1127,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4864943336347984 entropy -2.1860493222655486
epoch: 41, step: 45
	action: tensor([[-0.4871, -0.6530, -0.3052, -1.0490,  0.1182,  0.0866,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.664865862768754, distance: 1.4765437426387367 entropy -2.1860493222655486
epoch: 41, step: 46
	action: tensor([[-0.5778,  0.1251, -1.7132,  0.1545, -3.5310,  1.0088,  2.7022]],
       dtype=torch.float64)
	q_value: tensor([[-44.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025196954732646604, distance: 1.1298352802364173 entropy -1.435228630271093
epoch: 41, step: 47
	action: tensor([[-1.7546,  1.4565,  0.8790, -2.2480, -1.9742,  0.4010,  2.1434]],
       dtype=torch.float64)
	q_value: tensor([[-75.4728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1298352802364173 entropy -1.5436715249189086
epoch: 41, step: 48
	action: tensor([[-0.2336, -0.4117,  0.8186, -2.9473, -1.6436, -0.9761,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1298352802364173 entropy -2.1860493222655486
epoch: 41, step: 49
	action: tensor([[ 0.3075,  0.8576, -0.8212, -0.5544, -0.0924,  0.4309,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9066575616526449, distance: 0.3496199964544826 entropy -2.1860493222655486
epoch: 41, step: 50
	action: tensor([[-0.1536, -0.2342,  0.0541, -0.0868, -0.5557,  0.0486,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13561307139451184, distance: 1.2194721449882062 entropy -2.1860493222655486
epoch: 41, step: 51
	action: tensor([[ 2.9639, -0.8049,  0.6524, -0.7451,  1.2711,  0.8131,  3.0071]],
       dtype=torch.float64)
	q_value: tensor([[-38.1411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2194721449882062 entropy -1.3431509244949786
epoch: 41, step: 52
	action: tensor([[-0.4807, -0.2192,  0.1278, -0.1826,  0.1447, -0.6109,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6808832030700611, distance: 1.4836295062477634 entropy -2.1860493222655486
epoch: 41, step: 53
	action: tensor([[-0.0497, -0.3598,  0.6360, -0.1440, -1.3142,  1.2714,  1.7697]],
       dtype=torch.float64)
	q_value: tensor([[-35.5299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013999480045415402, distance: 1.1363059094066288 entropy -1.7962256574192546
epoch: 41, step: 54
	action: tensor([[-0.5001, -8.3849,  2.2462, -3.3485,  7.2078,  3.3565,  6.9147]],
       dtype=torch.float64)
	q_value: tensor([[-62.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1363059094066288 entropy -0.6200232531825767
epoch: 41, step: 55
	action: tensor([[ 1.8780, -0.2610,  0.4086, -0.7616, -0.1791, -0.1010,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1363059094066288 entropy -2.1860493222655486
epoch: 41, step: 56
	action: tensor([[ 0.6168, -0.6685, -0.8183, -1.0003,  1.3496,  0.1146,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1814268913301682, distance: 1.2438273990705295 entropy -2.1860493222655486
epoch: 41, step: 57
	action: tensor([[ 0.8957,  2.8298, -0.9901, -0.8226, -0.8733,  1.3003,  2.4834]],
       dtype=torch.float64)
	q_value: tensor([[-50.4378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2438273990705295 entropy -1.4732247789589679
epoch: 41, step: 58
	action: tensor([[ 1.6226,  0.5530,  0.1786, -0.7316,  1.2809, -0.0312,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5768402947431639, distance: 0.7444042539129244 entropy -2.1860493222655486
epoch: 41, step: 59
	action: tensor([[ 0.2279,  0.1423, -0.4265,  0.6367,  0.0763,  0.7519,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7444042539129244 entropy -2.1860493222655486
epoch: 41, step: 60
	action: tensor([[-0.0035, -0.6082,  0.0783, -0.2274, -0.7179, -0.0955,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39894110581066755, distance: 1.3534942309501494 entropy -2.1860493222655486
epoch: 41, step: 61
	action: tensor([[-2.2102, -4.8360,  1.3799, -2.2472, -4.2881,  0.4574,  3.5386]],
       dtype=torch.float64)
	q_value: tensor([[-41.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3534942309501494 entropy -1.207601259588387
epoch: 41, step: 62
	action: tensor([[0.2083, 0.3728, 0.4383, 0.1822, 0.9952, 0.4970, 0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8559324249682692, distance: 0.43434999047987544 entropy -2.1860493222655486
epoch: 41, step: 63
	action: tensor([[ 0.8702, -0.7188,  0.3541,  1.0064,  1.8214,  0.0404,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-39.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6826893561636364, distance: 0.6446129312272223 entropy -2.1860493222655486
LOSS epoch 41 actor 398.44039037152544 critic 451.02843908772104
epoch: 42, step: 0
	action: tensor([[ 1.4129, -0.7018, -0.9188,  2.5338, -2.1094,  3.8332,  3.6301]],
       dtype=torch.float64)
	q_value: tensor([[-62.2761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6446129312272223 entropy -1.1673408598514725
epoch: 42, step: 1
	action: tensor([[ 0.3487,  1.5632,  0.7653,  0.8102, -1.0794, -0.2970,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6446129312272223 entropy -1.705390481482262
epoch: 42, step: 2
	action: tensor([[ 0.8029, -1.1099, -0.2155, -1.2508,  1.1012,  0.6507,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7686072764479845, distance: 1.5218519215891098 entropy -1.705390481482262
epoch: 42, step: 3
	action: tensor([[-6.8551,  3.3005, -0.3418, -1.2440,  2.5467, -0.8262,  5.7816]],
       dtype=torch.float64)
	q_value: tensor([[-58.7056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5218519215891098 entropy -0.7735869465793991
epoch: 42, step: 4
	action: tensor([[-0.1667,  0.4620, -1.3452, -1.2935, -0.6986,  0.0714,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5218519215891098 entropy -1.705390481482262
epoch: 42, step: 5
	action: tensor([[-0.7106, -1.5364,  0.2364,  0.1662,  0.1983,  1.0627,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5218519215891098 entropy -1.705390481482262
epoch: 42, step: 6
	action: tensor([[ 0.2482, -0.8113, -0.6474,  1.4730, -0.2922,  0.2609,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31248304754163214, distance: 0.9488518286146721 entropy -1.705390481482262
epoch: 42, step: 7
	action: tensor([[-3.0758, -2.6759,  1.3319, -2.6731,  5.6151,  0.1329,  4.7375]],
       dtype=torch.float64)
	q_value: tensor([[-55.8941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9488518286146721 entropy -0.9654817155658602
epoch: 42, step: 8
	action: tensor([[-0.7192, -1.8854, -0.6230, -0.8956,  3.5608,  1.8087,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9488518286146721 entropy -1.705390481482262
epoch: 42, step: 9
	action: tensor([[-1.5552e-03, -1.5017e+00, -1.9272e-01,  4.0369e-01,  2.8964e+00,
          4.4134e-01,  1.6871e+00]], dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9488518286146721 entropy -1.705390481482262
epoch: 42, step: 10
	action: tensor([[ 0.5177, -2.1587,  0.5911,  0.9233,  0.7518, -0.0777,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9488518286146721 entropy -1.705390481482262
epoch: 42, step: 11
	action: tensor([[ 1.1648,  0.8548, -0.3201, -0.0466,  2.2230,  0.9280,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8130695868211117, distance: 0.4947623871157814 entropy -1.705390481482262
epoch: 42, step: 12
	action: tensor([[ 1.6248,  0.7241,  0.8260, -0.1916, -1.6968,  0.3090,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8246819395041723, distance: 0.47914837766719615 entropy -1.705390481482262
epoch: 42, step: 13
	action: tensor([[-0.6177,  0.8202, -2.2210, -0.6130, -1.9523,  0.0398,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.47914837766719615 entropy -1.705390481482262
epoch: 42, step: 14
	action: tensor([[-1.5286, -0.5039,  1.3825, -0.2767,  0.0671,  0.8581,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6296924307055534, distance: 1.8557059977730026 entropy -1.705390481482262
epoch: 42, step: 15
	action: tensor([[-5.7420,  1.6361,  3.4820, -0.3376, -0.7245,  1.0256,  7.4064]],
       dtype=torch.float64)
	q_value: tensor([[-55.8151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8557059977730026 entropy -0.5780382327681822
epoch: 42, step: 16
	action: tensor([[ 2.3654, -0.4883,  0.3002,  2.1639,  0.7652,  0.6993,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8557059977730026 entropy -1.705390481482262
epoch: 42, step: 17
	action: tensor([[ 0.4447, -0.7229,  1.0263,  0.3000,  1.8121,  1.6387,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8557059977730026 entropy -1.705390481482262
epoch: 42, step: 18
	action: tensor([[-1.3953,  0.4546,  1.7417, -1.8531,  0.8143,  0.4370,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8552025779542534, distance: 1.558663490648714 entropy -1.705390481482262
epoch: 42, step: 19
	action: tensor([[-1.8525,  2.0963, -3.8988, -1.5355,  0.1741, -2.8111,  7.4511]],
       dtype=torch.float64)
	q_value: tensor([[-63.6656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.558663490648714 entropy -0.5575819149347797
epoch: 42, step: 20
	action: tensor([[ 1.6315, -0.8578, -2.1853,  0.9925,  1.3457,  0.5214,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.558663490648714 entropy -1.705390481482262
epoch: 42, step: 21
	action: tensor([[-1.4456, -2.5163, -1.3734,  0.1332, -3.4716,  0.7703,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.558663490648714 entropy -1.705390481482262
epoch: 42, step: 22
	action: tensor([[ 0.0980, -0.7987, -0.5943, -1.3237, -0.6584,  0.3507,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.437549855812279, distance: 1.372044353924695 entropy -1.705390481482262
epoch: 42, step: 23
	action: tensor([[-5.4070, -1.9120, -1.8003, -4.6127,  6.0957,  1.1519,  6.1822]],
       dtype=torch.float64)
	q_value: tensor([[-52.6141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.372044353924695 entropy -0.7387822679727487
epoch: 42, step: 24
	action: tensor([[-1.7818,  0.6750,  0.5340, -2.2883,  0.1987,  0.8957,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.372044353924695 entropy -1.705390481482262
epoch: 42, step: 25
	action: tensor([[-1.5695,  1.2895,  1.1064, -1.4371,  1.2840,  0.3430,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9516401891296313, distance: 1.5986616969821248 entropy -1.705390481482262
epoch: 42, step: 26
	action: tensor([[ 1.0743, -2.1218, -2.2829,  3.2137, -3.1767,  1.4844,  5.5774]],
       dtype=torch.float64)
	q_value: tensor([[-59.0124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5986616969821248 entropy -0.809801216992823
epoch: 42, step: 27
	action: tensor([[ 0.6562, -1.7637, -3.0137, -0.4728, -0.0906,  0.1006,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5986616969821248 entropy -1.705390481482262
epoch: 42, step: 28
	action: tensor([[ 1.4636,  0.5338,  0.2414, -0.7400,  0.0378,  0.1936,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.689658837641904, distance: 0.6374944152174168 entropy -1.705390481482262
epoch: 42, step: 29
	action: tensor([[ 0.0805,  1.0246,  0.7184, -0.7343, -1.9683,  1.2374,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19068247360700008, distance: 1.029475856751863 entropy -1.705390481482262
epoch: 42, step: 30
	action: tensor([[ 2.6907, -7.7517,  3.4746,  8.6134, -8.0363,  2.0938,  9.9828]],
       dtype=torch.float64)
	q_value: tensor([[-56.8906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.029475856751863 entropy -0.3169288071105601
epoch: 42, step: 31
	action: tensor([[ 0.1346,  0.1418, -0.8925, -0.6424,  0.1798,  0.3747,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5869126995769607, distance: 0.7354914273300467 entropy -1.705390481482262
epoch: 42, step: 32
	action: tensor([[-0.7288, -0.1404, -0.1101, -0.7085, -0.0386,  0.6216,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8210762465061194, distance: 1.5442611938432753 entropy -1.705390481482262
epoch: 42, step: 33
	action: tensor([[-2.2674, -3.4586, -3.0204, -3.2125,  1.4069,  4.2014,  4.8878]],
       dtype=torch.float64)
	q_value: tensor([[-48.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5442611938432753 entropy -0.9349607054138279
epoch: 42, step: 34
	action: tensor([[-1.7698, -0.5493, -0.0795, -0.0911,  1.0503, -0.3600,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5442611938432753 entropy -1.705390481482262
epoch: 42, step: 35
	action: tensor([[ 1.7095,  0.2203, -0.2462,  0.6820,  0.2075,  1.7337,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5442611938432753 entropy -1.705390481482262
epoch: 42, step: 36
	action: tensor([[ 1.7532,  0.7493,  0.6090, -2.0731,  0.4976,  0.4298,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5442611938432753 entropy -1.705390481482262
epoch: 42, step: 37
	action: tensor([[-0.3192,  1.2773, -0.1089, -0.1533,  1.6665,  0.6354,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5442611938432753 entropy -1.705390481482262
epoch: 42, step: 38
	action: tensor([[ 3.0874,  0.4609, -1.5433, -0.1341,  2.8648,  0.1587,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5442611938432753 entropy -1.705390481482262
epoch: 42, step: 39
	action: tensor([[ 0.5546, -1.1623, -1.0438,  1.6538, -1.9963,  1.4923,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2572600197334314, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 40
	action: tensor([[ 3.3881, -2.9570,  6.3435,  4.7855,  5.8331,  5.0423, 10.1894]],
       dtype=torch.float64)
	q_value: tensor([[-74.0332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -0.31293463195551663
epoch: 42, step: 41
	action: tensor([[ 2.5691,  2.1919, -1.9987,  1.2331,  0.8336,  1.5594,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 42
	action: tensor([[ 2.6652, -2.1896,  0.9604,  1.1136, -0.4219,  0.4073,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 43
	action: tensor([[-1.7697, -1.0566, -2.1698, -4.1731,  1.6919,  0.8823,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 44
	action: tensor([[-2.4321, -0.8798,  1.4461, -0.2383, -0.3215,  0.1913,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 45
	action: tensor([[-1.1541, -1.6823, -1.3689,  0.5310, -1.9068,  0.6525,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 46
	action: tensor([[ 1.6628,  1.6039, -1.5930, -1.1570,  1.2609,  0.9562,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 47
	action: tensor([[-0.5934, -1.5488, -1.4570,  2.0828, -0.3455, -0.3681,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 48
	action: tensor([[-0.8739,  1.8914, -0.7841,  2.2086, -0.3499,  1.6312,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2831258250858293 entropy -1.705390481482262
epoch: 42, step: 49
	action: tensor([[-0.9109, -1.2096, -1.3040,  0.0695, -0.2980,  0.0591,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3993720583245013, distance: 1.7725785588908254 entropy -1.705390481482262
epoch: 42, step: 50
	action: tensor([[-5.1388,  0.7649,  2.6475,  2.9041, -0.1592,  3.7635,  4.6647]],
       dtype=torch.float64)
	q_value: tensor([[-53.9577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7725785588908254 entropy -0.9795183222658322
epoch: 42, step: 51
	action: tensor([[-2.8928, -0.5547,  1.5954, -0.0678, -1.2954,  0.4370,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7725785588908254 entropy -1.705390481482262
epoch: 42, step: 52
	action: tensor([[-2.1775, -0.3087,  0.8216,  0.0442, -2.0780, -0.1456,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7725785588908254 entropy -1.705390481482262
epoch: 42, step: 53
	action: tensor([[-2.0842,  0.0377,  1.3835, -1.5676, -0.7234,  0.3975,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7725785588908254 entropy -1.705390481482262
epoch: 42, step: 54
	action: tensor([[-1.9996, -1.7517, -0.8087, -1.9553, -1.9627, -0.1123,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7725785588908254 entropy -1.705390481482262
epoch: 42, step: 55
	action: tensor([[ 0.5926, -4.0908,  1.1459,  1.6851, -1.9925, -0.4901,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7725785588908254 entropy -1.705390481482262
epoch: 42, step: 56
	action: tensor([[ 1.2494, -1.3510,  0.0049, -1.1496, -0.3201,  0.6151,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7725785588908254 entropy -1.705390481482262
epoch: 42, step: 57
	action: tensor([[ 0.1981, -0.3746,  1.5283, -1.8353,  0.1042,  0.1269,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3322312702793686, distance: 1.32082872386369 entropy -1.705390481482262
epoch: 42, step: 58
	action: tensor([[ 8.1458, -6.3681,  0.8581, -0.9188,  2.4958,  1.9245,  7.4863]],
       dtype=torch.float64)
	q_value: tensor([[-62.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.32082872386369 entropy -0.5526140342612056
epoch: 42, step: 59
	action: tensor([[-0.6903, -0.9884,  0.1301, -2.1530, -2.1289, -0.1832,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13435867143163494, distance: 1.2187984434809458 entropy -1.705390481482262
epoch: 42, step: 60
	action: tensor([[-2.5163, -1.8875, -3.6484,  3.2625, -2.0031,  2.1190,  9.7741]],
       dtype=torch.float64)
	q_value: tensor([[-64.6839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -0.35000508600864944
epoch: 42, step: 61
	action: tensor([[-0.3122,  2.6422,  0.8118,  0.0834,  1.1782, -0.5473,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.705390481482262
epoch: 42, step: 62
	action: tensor([[ 0.6223, -1.3175,  0.6388, -2.2156, -0.9681,  0.1464,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.705390481482262
epoch: 42, step: 63
	action: tensor([[-2.7153, -1.7131,  0.8566, -2.8937, -0.4127,  1.5385,  1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-41.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.705390481482262
LOSS epoch 42 actor 190.8937753705536 critic 406.16454575387684
epoch: 43, step: 0
	action: tensor([[-1.3618, -3.8106,  0.8233,  0.4882,  3.1070,  0.3453,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.1605565288902857
epoch: 43, step: 1
	action: tensor([[ 6.1248, -1.7717, -0.8843, -1.1568, -2.4940, -0.9873,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.1605565288902857
epoch: 43, step: 2
	action: tensor([[ 1.0366, -5.0544, -1.6722, -2.9360,  1.2252,  0.9187,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.1605565288902857
epoch: 43, step: 3
	action: tensor([[ 1.5886,  0.9409, -2.5989, -3.1745,  5.8578, -1.3456,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.1605565288902857
epoch: 43, step: 4
	action: tensor([[-0.2612, -2.7408,  1.6670, -3.0975,  1.5981, -0.7555,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.1605565288902857
epoch: 43, step: 5
	action: tensor([[ 3.3169,  0.9001, -1.4807,  1.3729,  4.3515,  0.6915,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.1605565288902857
epoch: 43, step: 6
	action: tensor([[-2.8866,  2.0622,  2.9580, -2.6482, -1.9623,  0.8356,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.1605565288902857
epoch: 43, step: 7
	action: tensor([[ 1.8044, -3.0346, -1.9917,  1.4785, -2.2766,  1.7650,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2187984434809458 entropy -1.1605565288902857
epoch: 43, step: 8
	action: tensor([[-5.9242,  1.1403, -0.2710, -0.1197,  1.4266,  2.1334,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8147359120927719, distance: 0.49255225829341703 entropy -1.1605565288902857
epoch: 43, step: 9
	action: tensor([[ 5.9441,  3.9156,  3.3600, -4.9978, -4.1742,  1.5740, 12.9544]],
       dtype=torch.float64)
	q_value: tensor([[-78.7344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.49255225829341703 entropy -0.0906255105380133
epoch: 43, step: 10
	action: tensor([[ 1.5305,  0.4471,  2.8278, -1.2747,  2.9803,  1.7449,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6645547902720936, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 11
	action: tensor([[ 4.0223, -4.6659,  1.6496, -2.6642,  4.8051,  0.0779,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 12
	action: tensor([[-2.1567, -5.4084,  0.8344,  0.4621,  0.9617,  1.1753,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 13
	action: tensor([[-0.5620, -3.0076,  2.2222, -1.2569,  0.7796,  2.7795,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 14
	action: tensor([[-1.7510, -1.2253,  0.9191, -6.2054, -2.7387, -0.7310,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 15
	action: tensor([[-3.2272,  2.1454, -1.9301,  1.1596, -2.3830,  0.2191,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 16
	action: tensor([[ 2.1668, -1.9814, -1.1317,  0.1088,  2.5887,  2.2174,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 17
	action: tensor([[-3.5619, -0.7903, -1.7555, -4.6342, -0.7973, -0.3916,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 18
	action: tensor([[-1.0840, -1.4078, -0.9600, -3.1056,  2.3042,  0.2636,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 19
	action: tensor([[ 5.2039,  4.5919, -0.7511, -6.3717, -1.8751,  2.0953,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 20
	action: tensor([[-0.8344, -3.3910,  0.6467,  3.8460, -1.3035,  0.2615,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 21
	action: tensor([[ 2.6150, -2.6827,  0.7853, -0.8492, -0.3478,  0.8764,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 22
	action: tensor([[ 3.5588, -0.7878, -1.8175, -3.6775, -3.4427, -0.0094,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 23
	action: tensor([[ 1.8772, -0.6733, -0.6713,  0.7232,  1.6989,  0.7392,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 24
	action: tensor([[ 5.4108, -1.1490, -1.0317, -1.3670,  5.2407,  0.2911,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 25
	action: tensor([[ 4.6236, -2.0577, -0.6523,  2.9619,  1.1039,  0.5140,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 26
	action: tensor([[ 3.0947,  4.7369, -0.6405, -4.6366,  1.1667, -0.7578,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 27
	action: tensor([[-5.3039, -3.1638,  1.2076,  1.1706, -0.3278,  1.2965,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 28
	action: tensor([[-1.5225,  1.8452, -2.6665, -2.6038,  2.0081, -0.9862,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 29
	action: tensor([[-3.3172, -3.1258,  3.0625, -5.5400,  5.3101,  3.8801,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 30
	action: tensor([[-4.4434,  2.9136,  0.4977,  1.2646,  0.2959,  0.1175,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 31
	action: tensor([[-4.1015,  1.6179,  1.7346,  0.2108, -4.8299,  0.4945,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 32
	action: tensor([[ 2.3436, -1.4642, -4.2519,  2.5548, -0.8348,  0.2368,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6627770939230718 entropy -1.1605565288902857
epoch: 43, step: 33
	action: tensor([[ 4.8264,  0.2316,  1.8908, -7.3136, -4.1679,  1.4469,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9776461686059206, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 34
	action: tensor([[-3.7029, -4.2114, -0.3088,  0.7532, -2.5323, -0.1767,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 35
	action: tensor([[-3.7477, -3.8047,  2.9226,  1.9510,  1.3334,  1.6690,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 36
	action: tensor([[ 0.9958,  2.4674,  2.4733, -0.6302,  2.1355,  1.4397,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 37
	action: tensor([[-2.4420, -1.4037,  0.1767,  0.2590, -1.3938,  1.0299,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 38
	action: tensor([[-1.7160, -3.2780,  0.0411,  2.9253,  1.2141,  1.5860,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 39
	action: tensor([[ 7.0141, -2.7510,  0.0932, -2.9499,  2.5032,  0.3213,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 40
	action: tensor([[-2.4003, -0.7999, -1.6281, -0.0423, -1.6424, -0.8568,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 41
	action: tensor([[ 1.1458, -2.3002,  2.7491, -5.3770,  0.7859,  1.3279,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6092776857109972 entropy -1.1605565288902857
epoch: 43, step: 42
	action: tensor([[-0.4028, -0.7413,  4.4699, -0.0446,  2.6822, -0.0661,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7626912387122111, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 43
	action: tensor([[1.8260, 6.5920, 3.2456, 8.9476, 0.1869, 3.2731, 8.4026]],
       dtype=torch.float64)
	q_value: tensor([[-86.3512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.4499110870506303
epoch: 43, step: 44
	action: tensor([[ 3.7743, -1.0543, -3.9466, -3.4365,  0.4814,  2.7446,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 45
	action: tensor([[-2.5120,  1.6067, -0.2211,  3.3580,  1.7384,  0.1438,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 46
	action: tensor([[ 0.6195,  2.7253, -1.5823,  2.4767,  0.3053,  0.2912,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 47
	action: tensor([[-5.6724, -1.5044,  1.5148, -5.2199,  5.6345,  1.0602,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 48
	action: tensor([[-2.3534, -1.6364, -2.2838, -3.6859,  1.4011,  2.8302,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 49
	action: tensor([[ 4.7000,  1.9295, -3.1547, -3.7621, -2.2551,  2.2765,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 50
	action: tensor([[-6.1786,  1.2783,  1.3239,  2.3066,  0.9638,  0.7139,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 51
	action: tensor([[ 3.0942,  3.0828, -0.4849, -1.9734, -1.8660,  0.2535,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 52
	action: tensor([[-0.6418, -3.4929, -1.8049, -3.3830,  2.1648, -0.4736,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 53
	action: tensor([[ 4.1519,  1.9990,  1.7633, -0.2133,  5.1251,  0.2679,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 54
	action: tensor([[-2.2793,  0.8734, -1.1343, -4.2932, -0.9349,  2.2340,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 55
	action: tensor([[-0.5030,  0.6641, -0.4770, -1.7762, -2.6942,  1.0887,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 56
	action: tensor([[ 2.3809, -2.4713,  2.6922,  2.2221,  1.6720,  1.4875,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 57
	action: tensor([[-2.2548,  3.9743,  1.6038,  0.0510,  2.8325, -0.7613,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 58
	action: tensor([[-0.4788,  0.7146, -1.0291, -4.0237,  0.3350,  2.2196,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 59
	action: tensor([[-0.0213, -1.8693, -1.6783, -1.6718, -2.0890,  0.7717,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 60
	action: tensor([[ 0.1390,  2.0995, -1.9400, -4.5653, -1.9271,  0.4749,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 61
	action: tensor([[-1.8982,  1.1633, -0.2458, -1.0546,  0.7983,  1.2942,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 62
	action: tensor([[ 1.1249, -3.1784, -1.7236,  2.7461,  4.3777,  2.2849,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
epoch: 43, step: 63
	action: tensor([[-3.1565, -2.3868,  0.9986,  0.2103, -0.2223,  0.7792,  3.6435]],
       dtype=torch.float64)
	q_value: tensor([[-49.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -1.1605565288902857
LOSS epoch 43 actor 55.07109961563959 critic 112.64948171260512
epoch: 44, step: 0
	action: tensor([[ 5.5515,  0.2504,  2.4284,  1.2533, -1.8203, -0.8254,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 1
	action: tensor([[-5.8549, -0.5406,  2.1325,  0.5091, -7.5816,  3.9300,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 2
	action: tensor([[ 3.4666, -2.7086, -7.1001, -4.6784, -7.7087,  3.2341,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 3
	action: tensor([[-4.8607, -5.0833, -0.3797,  0.1112, -0.8233, -0.5191,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 4
	action: tensor([[-3.5815,  2.4809, -1.9131,  0.0948, -2.0328,  3.3107,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 5
	action: tensor([[ 8.4908, -5.2740,  1.2698, -7.1268, -4.8544, -0.7999,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 6
	action: tensor([[-3.6387,  2.8130, -3.1693, -4.3886,  7.0583, -0.3271,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 7
	action: tensor([[ 8.8924, -6.9590, -1.2143, -1.0541, -1.6867,  2.3955,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 8
	action: tensor([[-4.8743, -3.6192,  1.9961,  1.2255, -3.0400,  0.7377,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 9
	action: tensor([[ 0.6516, -1.4072,  5.1205, -1.4752,  7.8902, -0.6809,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 10
	action: tensor([[11.5065,  4.5321,  0.8704,  3.7196, -0.7435,  0.9165,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 11
	action: tensor([[ -2.0305,  -2.4514,  -1.6972, -10.1515,  -7.7972,   2.0305,   5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.51930447218305 entropy -0.7832265444551723
epoch: 44, step: 12
	action: tensor([[-0.6605,  1.5124, -0.1877, -0.4014, -9.3493,  0.7429,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22164453057562616, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 13
	action: tensor([[-17.2025,  -2.2442,   4.5544,   2.3201,  -3.6788,   3.1364,  16.0067]],
       dtype=torch.float64)
	q_value: tensor([[-58.1923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy 0.08081623628623179
epoch: 44, step: 14
	action: tensor([[-6.2855, -6.4904,  2.2245,  4.4288, -1.9725,  3.3721,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 15
	action: tensor([[ 0.3817, -1.3101, -2.8229, -3.4678,  3.0234,  2.4955,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 16
	action: tensor([[-1.5085, -2.6339, -2.5065, -0.3574,  2.3609,  1.6474,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 17
	action: tensor([[ 0.9042, -3.1879,  8.2112,  5.7065, -5.3876, -1.3568,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 18
	action: tensor([[-4.5309,  2.7699,  2.5607,  1.4105,  7.5279,  1.0431,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 19
	action: tensor([[-2.4489, -1.0926, -0.4166,  6.6124, -2.5664,  2.8759,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 20
	action: tensor([[-9.8026,  2.6725,  1.1400,  0.5843,  6.6933,  1.2589,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 21
	action: tensor([[ 4.9777, -7.7200,  0.3614, -5.8835,  3.9025, -5.1719,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 22
	action: tensor([[-1.6611,  2.3180,  4.8391, -4.7598, -9.3848,  4.6976,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 23
	action: tensor([[-1.2509,  2.6703,  0.0799,  4.3828, -6.1425,  4.3258,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 24
	action: tensor([[-3.5730, -3.9734,  2.8555,  7.4522,  0.6602, -1.4143,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 25
	action: tensor([[-2.6722,  0.9779, -0.7440,  0.4802, -7.6506,  3.9489,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 26
	action: tensor([[ -4.0709, -11.1056,  -4.9623,  -1.7119,  -4.5486,   2.4176,   5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 27
	action: tensor([[ 3.7255, -5.7445, -4.3911,  5.9174, -1.4972, -1.7390,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 28
	action: tensor([[-2.2172, -4.3978,  1.9552, -0.0385, -1.9918,  5.0704,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0095914966039423 entropy -0.7832265444551723
epoch: 44, step: 29
	action: tensor([[-1.3544,  1.1367,  2.1022, -2.0043, -1.0996,  0.0787,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13075481105595688, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 30
	action: tensor([[-7.3906, -1.0939,  1.4699, -4.1282, -0.8951,  0.4642,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 31
	action: tensor([[ 7.5751, -4.7702,  1.4279,  3.6208, -3.9211,  1.9592,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 32
	action: tensor([[ 6.5124,  1.3632, -0.5765, -3.6604, -1.4659,  2.9052,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 33
	action: tensor([[-0.5711,  2.3669, -2.3990, -1.2127, -5.2132,  2.3421,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 34
	action: tensor([[ 9.9632, -2.0607,  0.0652, -2.5815,  7.4354,  3.7133,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 35
	action: tensor([[-1.5283,  2.3464, -3.9631, -0.3190,  6.1356,  3.2317,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 36
	action: tensor([[-7.6192, -5.4301, -4.3417,  6.0062, -0.3056,  3.9812,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 37
	action: tensor([[-2.4023, -0.8294,  6.1765,  4.9620,  0.8372,  1.8058,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2168608404414498 entropy -0.7832265444551723
epoch: 44, step: 38
	action: tensor([[ 5.2588,  0.5734, -6.5902,  0.1291, -1.1221,  0.0699,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5554640251204883, distance: 1.4272060526993318 entropy -0.7832265444551723
epoch: 44, step: 39
	action: tensor([[ 0.1959,  1.1411, -6.7577, -0.9218, -2.4334, -1.2664,  7.6143]],
       dtype=torch.float64)
	q_value: tensor([[-42.3428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9137949123255256, distance: 0.3359875169526415 entropy -0.552863282731889
epoch: 44, step: 40
	action: tensor([[ 4.1899, -1.0658, -1.6130, -2.1374, -2.2687,  2.1231,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 41
	action: tensor([[ 5.2824, -1.9074,  1.4122, -4.1999, -5.9150,  1.8528,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 42
	action: tensor([[ 8.6989, -7.3280, -0.7380, -2.8563,  3.7888, -1.9780,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 43
	action: tensor([[-0.9286, -8.3474,  1.0929, -0.4053,  0.8116, -0.4380,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 44
	action: tensor([[ 1.6420,  6.0764, -3.2353,  2.0115, -5.8047,  6.3765,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 45
	action: tensor([[ 7.8994, -2.6547, -0.2233,  4.0464,  4.2853, -2.0805,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 46
	action: tensor([[-1.9237, -4.1914, -4.2824,  0.5533, -7.6054, -0.2948,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 47
	action: tensor([[ -8.5227, -10.8493,   0.8768,   5.5279,   1.1238,   2.4305,   5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 48
	action: tensor([[-1.8989, -9.7259, -1.2082, -2.7405, 11.5961,  0.5108,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 49
	action: tensor([[ 1.3162,  2.2430,  0.1814,  7.6578,  4.7993, -0.6801,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 50
	action: tensor([[ 4.2260,  1.3064, -1.2346, -6.8734,  3.0234,  0.6302,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 51
	action: tensor([[-2.0587, -0.9078, -2.2238, -7.6225,  1.8122,  1.2548,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3359875169526415 entropy -0.7832265444551723
epoch: 44, step: 52
	action: tensor([[-6.6462,  0.6725,  1.1465, -0.9685, -6.5826,  1.5968,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24902884563810135, distance: 0.9916726165776549 entropy -0.7832265444551723
epoch: 44, step: 53
	action: tensor([[ -0.6686,   5.6036,   0.5630,  -5.9272, -10.3380,   0.5247,  10.9683]],
       dtype=torch.float64)
	q_value: tensor([[-53.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7166468482977619, distance: 1.4993298020769052 entropy -0.23186426375486288
epoch: 44, step: 54
	action: tensor([[-2.6248, -0.9023, -1.5617,  1.3661, -3.3915,  1.0513,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4993298020769052 entropy -0.7832265444551723
epoch: 44, step: 55
	action: tensor([[ 4.2768,  2.7549,  1.2766, -2.0100, -4.7594, -1.5971,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4993298020769052 entropy -0.7832265444551723
epoch: 44, step: 56
	action: tensor([[-0.2409, -0.0383,  2.7206,  1.4435,  0.9310,  0.2055,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12418421754428977, distance: 1.2133202201407551 entropy -0.7832265444551723
epoch: 44, step: 57
	action: tensor([[-17.1877,   6.8174,   2.0845, -11.5509,   4.6494,   1.4061,  10.9107]],
       dtype=torch.float64)
	q_value: tensor([[-67.4560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.23640314796202372
epoch: 44, step: 58
	action: tensor([[-5.7670,  2.8684, -0.5170,  4.3948, -0.8099,  4.2273,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.7832265444551723
epoch: 44, step: 59
	action: tensor([[-8.6502, -0.6656, -0.7790, -4.1468,  4.8291,  1.9632,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.7832265444551723
epoch: 44, step: 60
	action: tensor([[ 0.5051,  3.7963,  4.6928, -3.3096, -2.8283, -1.0624,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.7832265444551723
epoch: 44, step: 61
	action: tensor([[-4.8895, -3.1800, -2.1187,  1.5071,  4.6035,  0.2182,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.7832265444551723
epoch: 44, step: 62
	action: tensor([[-2.9198, -0.5484, -1.2049, -9.1414, -5.0395,  4.7195,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.7832265444551723
epoch: 44, step: 63
	action: tensor([[-3.1377, -5.2699, -7.2375, -6.6769, -0.7852,  0.2393,  5.6825]],
       dtype=torch.float64)
	q_value: tensor([[-56.7782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.7832265444551723
LOSS epoch 44 actor 124.15776364607002 critic 210.48107232162215
epoch: 45, step: 0
	action: tensor([[-9.8569, -0.9804, -4.9278, -4.1655, -8.4750,  2.9346,  5.9901]],
       dtype=torch.float64)
	q_value: tensor([[-53.1832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.6972734034431182
epoch: 45, step: 1
	action: tensor([[ 6.5587,  3.1275, -1.6876, -1.1022,  5.6416,  0.0654,  5.9901]],
       dtype=torch.float64)
	q_value: tensor([[-53.1832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.6972734034431182
epoch: 45, step: 2
	action: tensor([[ 4.7694, 10.7806,  6.3751,  8.8875,  1.2850, -0.7166,  5.9901]],
       dtype=torch.float64)
	q_value: tensor([[-53.1832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2133202201407551 entropy -0.6972734034431182
