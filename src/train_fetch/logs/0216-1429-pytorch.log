epoch: 0, step: 0
	action: tensor([[ 0.0354,  0.0080, -0.0016,  0.0056, -0.0247,  0.0646,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[-0.0733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3549318652172133, distance: 0.9190930621217053 entropy -13.422486671979302
epoch: 0, step: 1
	action: tensor([[ 0.0219,  0.0128,  0.0078,  0.0134, -0.0115,  0.0026, -0.0277]],
       dtype=torch.float64)
	q_value: tensor([[-0.0888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32979220642993823, distance: 0.9368313816443606 entropy -10.81136716858626
epoch: 0, step: 2
	action: tensor([[ 0.0218,  0.0127,  0.0002,  0.0144, -0.0111, -0.0662, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3147405945192977, distance: 0.9472927114849499 entropy -10.803635122708025
epoch: 0, step: 3
	action: tensor([[ 0.0210,  0.0124, -0.0016, -0.0045, -0.0109, -0.0702, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039742036838704, distance: 0.9547053608137503 entropy -10.80648177504001
epoch: 0, step: 4
	action: tensor([[ 0.0211,  0.0124, -0.0013,  0.0062, -0.0111, -0.0315, -0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31765921792604856, distance: 0.9452732274325962 entropy -10.808635446957387
epoch: 0, step: 5
	action: tensor([[ 2.1353e-02,  1.2549e-02,  9.5924e-05,  1.6031e-03, -1.1090e-02,
          4.8534e-02, -1.2108e-02]], dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33404804810779976, distance: 0.9338521894754482 entropy -10.805558086617504
epoch: 0, step: 6
	action: tensor([[ 0.0221,  0.0127, -0.0161,  0.0022, -0.0113, -0.0521, -0.0442]],
       dtype=torch.float64)
	q_value: tensor([[-0.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31262751771305763, distance: 0.9487521307178336 entropy -10.805519775550739
epoch: 0, step: 7
	action: tensor([[ 2.1071e-02,  1.2465e-02, -3.0878e-03,  1.7725e-02, -1.1219e-02,
         -8.3973e-05, -2.5977e-02]], dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33006397086265127, distance: 0.9366414231971334 entropy -10.808097195066187
epoch: 0, step: 8
	action: tensor([[ 0.0216,  0.0126, -0.0104,  0.0067, -0.0111, -0.0093, -0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3235062507176718, distance: 0.9412144527505483 entropy -10.804590562310699
epoch: 0, step: 9
	action: tensor([[ 0.0215,  0.0126,  0.0058,  0.0082, -0.0112, -0.0288, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31968360165629417, distance: 0.9438699570826019 entropy -10.805921920417797
epoch: 0, step: 10
	action: tensor([[ 2.1414e-02,  1.2608e-02, -7.1935e-05, -1.1262e-02, -1.1074e-02,
          3.6873e-03, -3.2027e-02]], dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.317827267962531, distance: 0.9451568171384228 entropy -10.804746686362117
epoch: 0, step: 11
	action: tensor([[ 0.0218,  0.0126,  0.0054,  0.0001, -0.0114, -0.0740, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061319723578019, distance: 0.9532243579526546 entropy -10.805355673898342
epoch: 0, step: 12
	action: tensor([[ 0.0211,  0.0125, -0.0046,  0.0116, -0.0111,  0.0058,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32853088945968023, distance: 0.9377125157476763 entropy -10.808192181421486
epoch: 0, step: 13
	action: tensor([[ 0.0216,  0.0126, -0.0165,  0.0133, -0.0111, -0.0642, -0.0558]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3145235813642021, distance: 0.9474426975378879 entropy -10.804895156274174
epoch: 0, step: 14
	action: tensor([[ 0.0209,  0.0124,  0.0199,  0.0047, -0.0111, -0.0028, -0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32322755842561257, distance: 0.9414083068721801 entropy -10.807816294299451
epoch: 0, step: 15
	action: tensor([[ 0.0219,  0.0127, -0.0080,  0.0043, -0.0111, -0.0272, -0.0130]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31871898210699967, distance: 0.9445388759143397 entropy -10.801664151680498
epoch: 0, step: 16
	action: tensor([[ 0.0214,  0.0126,  0.0046,  0.0074, -0.0112, -0.0749, -0.0307]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090048635117315, distance: 0.9512489460360941 entropy -10.80672084121312
epoch: 0, step: 17
	action: tensor([[ 0.0211,  0.0124,  0.0183, -0.0196, -0.0109,  0.0241, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31799770320558374, distance: 0.9450387399365846 entropy -10.80714754344066
epoch: 0, step: 18
	action: tensor([[ 0.0220,  0.0127,  0.0089, -0.0028, -0.0114,  0.0139, -0.0321]],
       dtype=torch.float64)
	q_value: tensor([[-0.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3248740191531503, distance: 0.9402624744113343 entropy -10.80454652275383
epoch: 0, step: 19
	action: tensor([[ 0.0219,  0.0127, -0.0007, -0.0052, -0.0113, -0.0378, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-0.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31192496101318623, distance: 0.9492368620323653 entropy -10.803738465447482
epoch: 0, step: 20
	action: tensor([[ 0.0213,  0.0125, -0.0033,  0.0086, -0.0112, -0.0508, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31484268630771217, distance: 0.9472221437491251 entropy -10.806246856588405
epoch: 0, step: 21
	action: tensor([[ 0.0212,  0.0125, -0.0095,  0.0168, -0.0111,  0.0214, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3345532633416526, distance: 0.93349789524645 entropy -10.80690815255774
epoch: 0, step: 22
	action: tensor([[ 0.0217,  0.0127, -0.0070,  0.0167, -0.0112, -0.0324, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3231472347621742, distance: 0.9414641713812822 entropy -10.805283497293487
epoch: 0, step: 23
	action: tensor([[ 0.0213,  0.0126, -0.0048,  0.0032, -0.0111,  0.0272, -0.0484]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32978922272939526, distance: 0.9368334669835776 entropy -10.805906415112597
epoch: 0, step: 24
	action: tensor([[ 0.0220,  0.0127, -0.0051, -0.0060, -0.0113, -0.0567, -0.0509]],
       dtype=torch.float64)
	q_value: tensor([[-0.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074850466791683, distance: 0.9522944889085491 entropy -10.805782593657208
epoch: 0, step: 25
	action: tensor([[ 0.0211,  0.0124, -0.0062,  0.0141, -0.0112,  0.0077,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33011722102613306, distance: 0.9366041977771759 entropy -10.8066158918793
epoch: 0, step: 26
	action: tensor([[ 0.0216,  0.0126,  0.0042,  0.0013, -0.0111, -0.0277, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3167811731855684, distance: 0.9458812266670055 entropy -10.804747866677332
epoch: 0, step: 27
	action: tensor([[ 0.0215,  0.0126,  0.0024,  0.0038, -0.0111, -0.0355, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3160751419552884, distance: 0.9463698323809075 entropy -10.804898363491409
epoch: 0, step: 28
	action: tensor([[ 0.0214,  0.0126,  0.0085,  0.0093, -0.0111, -0.0100, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3244111057034311, distance: 0.940584773932023 entropy -10.806261202755
epoch: 0, step: 29
	action: tensor([[ 0.0216,  0.0126, -0.0099, -0.0024, -0.0111, -0.0129, -0.0343]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31850667421475387, distance: 0.944686037975227 entropy -10.803759604008064
epoch: 0, step: 30
	action: tensor([[ 0.0216,  0.0126,  0.0017,  0.0147, -0.0113, -0.0059, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32798400003941264, distance: 0.9380943059661685 entropy -10.806420533491744
epoch: 0, step: 31
	action: tensor([[ 0.0216,  0.0127,  0.0144,  0.0072, -0.0111, -0.0129, -0.0367]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32299967820109643, distance: 0.9415667872323453 entropy -10.804771397317962
epoch: 0, step: 32
	action: tensor([[ 0.0217,  0.0127,  0.0126,  0.0178, -0.0111, -0.0352, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3230534934954036, distance: 0.9415293635420333 entropy -10.803217713284193
epoch: 0, step: 33
	action: tensor([[ 0.0214,  0.0126,  0.0104,  0.0013, -0.0109, -0.0454, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31258691678513084, distance: 0.948780150204409 entropy -10.803842591184111
epoch: 0, step: 34
	action: tensor([[ 0.0214,  0.0125,  0.0107,  0.0015, -0.0110, -0.0696, -0.0365]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30724058030798995, distance: 0.9524625599623456 entropy -10.80538716487284
epoch: 0, step: 35
	action: tensor([[ 0.0211,  0.0124, -0.0018,  0.0190, -0.0109, -0.0500, -0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3195528780675547, distance: 0.9439606355829061 entropy -10.805629626374579
epoch: 0, step: 36
	action: tensor([[ 0.0212,  0.0125,  0.0016,  0.0302, -0.0109, -0.0471, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.325538669279104, distance: 0.9397995241786143 entropy -10.806044003400102
epoch: 0, step: 37
	action: tensor([[ 0.0212,  0.0125,  0.0029,  0.0018, -0.0108, -0.0604, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093057108293732, distance: 0.9510418448315968 entropy -10.804693131351641
epoch: 0, step: 38
	action: tensor([[ 0.0212,  0.0125, -0.0124,  0.0035, -0.0111, -0.0426, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3141647389853024, distance: 0.9476906550891588 entropy -10.80733326271168
epoch: 0, step: 39
	action: tensor([[ 0.0212,  0.0125, -0.0037, -0.0037, -0.0112, -0.0916, -0.0349]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000056521229396, distance: 0.9574232285539251 entropy -10.807363117559786
epoch: 0, step: 40
	action: tensor([[ 0.0209,  0.0123, -0.0077,  0.0002, -0.0111,  0.0346, -0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32946058641714504, distance: 0.9370631259011281 entropy -10.809428468118446
epoch: 0, step: 41
	action: tensor([[ 0.0219,  0.0127, -0.0137,  0.0079, -0.0113, -0.0414, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-0.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3173330801911719, distance: 0.945499106090267 entropy -10.806160972755283
epoch: 0, step: 42
	action: tensor([[ 0.0212,  0.0125,  0.0027, -0.0071, -0.0112, -0.0681, -0.0236]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3033577426666838, distance: 0.9551280523169773 entropy -10.807378093622564
epoch: 0, step: 43
	action: tensor([[ 0.0212,  0.0125,  0.0066,  0.0049, -0.0111,  0.0006, -0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3243910754652878, distance: 0.9405987173209176 entropy -10.808153954929134
epoch: 0, step: 44
	action: tensor([[ 0.0217,  0.0127,  0.0138,  0.0027, -0.0112, -0.0138, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3207404906788829, distance: 0.9431365089869139 entropy -10.803899149726158
epoch: 0, step: 45
	action: tensor([[ 0.0216,  0.0127, -0.0095, -0.0105, -0.0111, -0.0362, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3095978593736013, distance: 0.9508406886314678 entropy -10.803714332253955
epoch: 0, step: 46
	action: tensor([[ 0.0213,  0.0125,  0.0134,  0.0130, -0.0113, -0.0730, -0.0376]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3118141649440598, distance: 0.9493132835515258 entropy -10.80757170546559
epoch: 0, step: 47
	action: tensor([[ 0.0211,  0.0124, -0.0090,  0.0158, -0.0108, -0.0490,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3183392681947189, distance: 0.9448020599618141 entropy -10.805525535160708
epoch: 0, step: 48
	action: tensor([[ 0.0211,  0.0125,  0.0020, -0.0182, -0.0111,  0.0197, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3178665486361555, distance: 0.9451296048729804 entropy -10.806834664066702
epoch: 0, step: 49
	action: tensor([[ 0.0220,  0.0127,  0.0064,  0.0181, -0.0115, -0.0304,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3245819404101, distance: 0.9404658445915761 entropy -10.806899735854445
epoch: 0, step: 50
	action: tensor([[ 0.0214,  0.0126,  0.0018,  0.0022, -0.0110, -0.0299,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31653443753432253, distance: 0.9460520076410716 entropy -10.804049834060871
epoch: 0, step: 51
	action: tensor([[ 0.0214,  0.0126,  0.0043,  0.0082, -0.0112, -0.0328, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3187844847196679, distance: 0.9444934678890741 entropy -10.806355693386962
epoch: 0, step: 52
	action: tensor([[ 0.0214,  0.0126,  0.0049, -0.0078, -0.0110, -0.0482, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077190273456484, distance: 0.9521335990084214 entropy -10.80555987743278
epoch: 0, step: 53
	action: tensor([[ 0.0214,  0.0126,  0.0061,  0.0108, -0.0112,  0.0413, -0.0379]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3368144298748049, distance: 0.9319105478009682 entropy -10.807143904692694
epoch: 0, step: 54
	action: tensor([[ 0.0222,  0.0127,  0.0131, -0.0051, -0.0112, -0.0160, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[-0.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31700416809618714, distance: 0.9457268515933467 entropy -10.804221789118847
epoch: 0, step: 55
	action: tensor([[ 0.0216,  0.0127, -0.0102,  0.0223, -0.0112, -0.0270, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3268543648335246, distance: 0.9388824266336894 entropy -10.80447419491774
epoch: 0, step: 56
	action: tensor([[ 0.0213,  0.0126, -0.0050,  0.0091, -0.0110, -0.0482, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31562598334664826, distance: 0.9466805393140435 entropy -10.805928743967362
epoch: 0, step: 57
	action: tensor([[ 0.0212,  0.0125,  0.0024,  0.0063, -0.0111, -0.0440,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3151132389238448, distance: 0.947035107335719 entropy -10.80718594528697
epoch: 0, step: 58
	action: tensor([[ 2.1340e-02,  1.2599e-02,  3.1651e-03, -6.6305e-05, -1.1100e-02,
         -2.8795e-02, -3.7888e-02]], dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31566297655362985, distance: 0.9466549529960723 entropy -10.806620196941976
epoch: 0, step: 59
	action: tensor([[ 0.0215,  0.0126, -0.0003,  0.0006, -0.0111,  0.0698, -0.0386]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3387225861166533, distance: 0.930568908326489 entropy -10.805821644200932
epoch: 0, step: 60
	action: tensor([[ 0.0223,  0.0127, -0.0058, -0.0191, -0.0114, -0.0605, -0.0460]],
       dtype=torch.float64)
	q_value: tensor([[-0.0887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30085455908361003, distance: 0.9568425025727556 entropy -10.806760766387594
epoch: 0, step: 61
	action: tensor([[ 0.0212,  0.0124, -0.0050,  0.0006, -0.0113, -0.0083, -0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3203725130507248, distance: 0.9433919387166377 entropy -10.807892354280451
epoch: 0, step: 62
	action: tensor([[ 0.0215,  0.0126,  0.0051,  0.0094, -0.0112,  0.0126, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3297230319574711, distance: 0.9368797272060964 entropy -10.80613230555279
epoch: 0, step: 63
	action: tensor([[ 0.0219,  0.0127, -0.0057,  0.0069, -0.0112,  0.0046, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32705041121200074, distance: 0.9387456970067038 entropy -10.80391837011261
epoch: 0, step: 64
	action: tensor([[ 0.0217,  0.0127,  0.0033, -0.0112, -0.0112, -0.0222, -0.0025]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31225011403003033, distance: 0.9490125524122779 entropy -10.80603803370436
epoch: 0, step: 65
	action: tensor([[ 0.0216,  0.0127, -0.0032,  0.0079, -0.0113, -0.0118, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32350847975555486, distance: 0.9412129021047354 entropy -10.807004365339338
epoch: 0, step: 66
	action: tensor([[ 0.0216,  0.0126, -0.0001,  0.0104, -0.0112, -0.0504, -0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3161172429192297, distance: 0.9463407036726128 entropy -10.805628146859343
epoch: 0, step: 67
	action: tensor([[ 0.0212,  0.0125, -0.0050, -0.0043, -0.0110, -0.0417, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3106196650534613, distance: 0.9501367987449845 entropy -10.805595827489963
epoch: 0, step: 68
	action: tensor([[ 0.0213,  0.0126, -0.0110, -0.0025, -0.0112, -0.0309, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3141311881164208, distance: 0.9477138351863812 entropy -10.807483332391522
epoch: 0, step: 69
	action: tensor([[ 0.0214,  0.0126, -0.0105,  0.0145, -0.0113, -0.0704,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.313379410604431, distance: 0.9482330852309634 entropy -10.806965425763845
epoch: 0, step: 70
	action: tensor([[ 0.0210,  0.0125,  0.0027,  0.0006, -0.0111,  0.0158, -0.0295]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.325674814708794, distance: 0.9397046663552159 entropy -10.80859415852186
epoch: 0, step: 71
	action: tensor([[ 0.0219,  0.0127,  0.0053,  0.0021, -0.0113, -0.0091, -0.0380]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3218186530108995, distance: 0.9423877096930626 entropy -10.804622295431992
epoch: 0, step: 72
	action: tensor([[ 0.0217,  0.0126,  0.0028,  0.0024, -0.0112, -0.0618,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098317486432287, distance: 0.9506796156427516 entropy -10.804190100979113
epoch: 0, step: 73
	action: tensor([[ 0.0211,  0.0125,  0.0049,  0.0138, -0.0112, -0.0865, -0.0371]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090976684821988, distance: 0.9511850645483373 entropy -10.808279252626813
epoch: 0, step: 74
	action: tensor([[ 0.0209,  0.0124, -0.0019, -0.0078, -0.0109, -0.0516, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30637120962935616, distance: 0.9530600136927804 entropy -10.80715854732267
epoch: 0, step: 75
	action: tensor([[ 0.0212,  0.0125,  0.0005,  0.0010, -0.0112,  0.0361, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3307467867893312, distance: 0.9361639770687046 entropy -10.807289727708453
epoch: 0, step: 76
	action: tensor([[ 0.0220,  0.0127, -0.0065, -0.0035, -0.0113, -0.0057, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[-0.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31996294075954945, distance: 0.9436761598594323 entropy -10.804980814458057
epoch: 0, step: 77
	action: tensor([[ 0.0216,  0.0127,  0.0056, -0.0192, -0.0113,  0.0332, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3211393564746998, distance: 0.9428595602435698 entropy -10.806933470329565
epoch: 0, step: 78
	action: tensor([[ 0.0221,  0.0127, -0.0122,  0.0100, -0.0115, -0.0120, -0.0486]],
       dtype=torch.float64)
	q_value: tensor([[-0.0887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.324945056964032, distance: 0.9402130051664296 entropy -10.806064074406596
epoch: 0, step: 79
	action: tensor([[ 0.0214,  0.0126, -0.0005,  0.0026, -0.0112,  0.0058, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3248583599494447, distance: 0.9402733788030068 entropy -10.80591987494289
epoch: 0, step: 80
	action: tensor([[ 0.0217,  0.0126,  0.0035,  0.0010, -0.0112,  0.0371, -0.0387]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33157311215275764, distance: 0.9355858588363546 entropy -10.805422704108736
epoch: 0, step: 81
	action: tensor([[ 0.0221,  0.0127,  0.0134,  0.0108, -0.0113,  0.0611, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-0.0887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34242633723192517, distance: 0.9279592356424835 entropy -10.804863028435642
epoch: 0, step: 82
	action: tensor([[ 0.0223,  0.0127, -0.0034,  0.0085, -0.0112, -0.0342, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-0.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31951427869773463, distance: 0.9439874089851171 entropy -10.803939261675847
epoch: 0, step: 83
	action: tensor([[ 0.0213,  0.0126, -0.0132,  0.0105, -0.0111,  0.0259, -0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3328487250035732, distance: 0.9346927056006041 entropy -10.806434469699793
epoch: 0, step: 84
	action: tensor([[ 0.0219,  0.0127, -0.0134,  0.0025, -0.0113, -0.0470, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31360603792346076, distance: 0.9480765844936019 entropy -10.80654448568705
epoch: 0, step: 85
	action: tensor([[ 0.0212,  0.0125, -0.0008, -0.0146, -0.0112,  0.0377, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32379387473382826, distance: 0.9410143439820969 entropy -10.80850580752839
epoch: 0, step: 86
	action: tensor([[ 0.0220,  0.0127, -0.0044, -0.0139, -0.0115, -0.0286, -0.0118]],
       dtype=torch.float64)
	q_value: tensor([[-0.0887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31003289373540965, distance: 0.9505410708217112 entropy -10.806582894278407
epoch: 0, step: 87
	action: tensor([[ 0.0214,  0.0126, -0.0044,  0.0062, -0.0113, -0.0513, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3137540116223404, distance: 0.9479743851965569 entropy -10.806807548128146
epoch: 0, step: 88
	action: tensor([[ 0.0212,  0.0125, -0.0118,  0.0143, -0.0111,  0.0068, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3300329010798515, distance: 0.9366631423665117 entropy -10.80709467769572
epoch: 0, step: 89
	action: tensor([[ 0.0216,  0.0126, -0.0135,  0.0034, -0.0112, -0.0647, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098197468524382, distance: 0.9506878816041533 entropy -10.805578593288015
epoch: 0, step: 90
	action: tensor([[ 0.0209,  0.0124,  0.0024,  0.0001, -0.0112, -0.0387, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3129753206712397, distance: 0.948512071246347 entropy -10.80835301782878
epoch: 0, step: 91
	action: tensor([[ 0.0214,  0.0126,  0.0150, -0.0033, -0.0111, -0.0112, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3180234614556916, distance: 0.9450208933881276 entropy -10.805968482034046
epoch: 0, step: 92
	action: tensor([[ 0.0218,  0.0127, -0.0021,  0.0174, -0.0112,  0.0215, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3357227669749059, distance: 0.9326772365726317 entropy -10.8041481686796
epoch: 0, step: 93
	action: tensor([[ 0.0217,  0.0127, -0.0085,  0.0152, -0.0111, -0.0675, -0.0213]],
       dtype=torch.float64)
	q_value: tensor([[-0.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31485255797012146, distance: 0.947215319994402 entropy -10.803869397093028
epoch: 0, step: 94
	action: tensor([[ 0.0210,  0.0125,  0.0047,  0.0204, -0.0110,  0.0769,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34908785804582654, distance: 0.9232469459212697 entropy -10.807831037256435
epoch: 0, step: 95
	action: tensor([[ 0.0221,  0.0127,  0.0006,  0.0129, -0.0111,  0.0018, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3295199428672475, distance: 0.9370216503588285 entropy -10.804764573291434
epoch: 0, step: 96
	action: tensor([[ 0.0217,  0.0127,  0.0156, -0.0157, -0.0111, -0.0351, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070011290502881, distance: 0.9526271543705543 entropy -10.80476833121572
epoch: 0, step: 97
	action: tensor([[ 0.0216,  0.0126, -0.0128,  0.0131, -0.0112,  0.0234, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3338051032701723, distance: 0.9340225124846641 entropy -10.80634220013248
epoch: 0, step: 98
	action: tensor([[ 0.0217,  0.0127,  0.0031,  0.0088, -0.0112, -0.0881, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30716451816191903, distance: 0.9525148467706047 entropy -10.806220640777239
epoch: 0, step: 99
	action: tensor([[ 0.0208,  0.0124, -0.0068, -0.0056, -0.0109, -0.0130, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3160201809317943, distance: 0.9464078573255018 entropy -10.807611869046779
epoch: 0, step: 100
	action: tensor([[ 0.0216,  0.0127, -0.0064,  0.0114, -0.0113, -0.0228, -0.0547]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32268732616881823, distance: 0.9417839705741631 entropy -10.806820069969376
epoch: 0, step: 101
	action: tensor([[ 0.0214,  0.0126,  0.0223,  0.0094, -0.0111, -0.0395, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3176518391836266, distance: 0.9452783384482283 entropy -10.805944866888945
epoch: 0, step: 102
	action: tensor([[ 0.0215,  0.0126, -0.0025,  0.0232, -0.0109, -0.0064, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33166398832223565, distance: 0.9355222577525081 entropy -10.803015339926331
epoch: 0, step: 103
	action: tensor([[ 0.0215,  0.0127, -0.0006,  0.0209, -0.0110, -0.0600, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3188152154429599, distance: 0.9444721638420362 entropy -10.804321259150502
epoch: 0, step: 104
	action: tensor([[ 0.0211,  0.0125,  0.0060,  0.0029, -0.0109, -0.0789,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055319082075605, distance: 0.9536364479525278 entropy -10.806316162308292
epoch: 0, step: 105
	action: tensor([[ 0.0211,  0.0125, -0.0071,  0.0052, -0.0111, -0.1000, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30230970183150985, distance: 0.9558462380059652 entropy -10.808830803765206
epoch: 0, step: 106
	action: tensor([[ 0.0207,  0.0123, -0.0047,  0.0015, -0.0111,  0.0096, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32423770809486563, distance: 0.9407054721022519 entropy -10.809346766377582
epoch: 0, step: 107
	action: tensor([[ 0.0218,  0.0127, -0.0004, -0.0068, -0.0113, -0.0102, -0.0266]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31724190770643557, distance: 0.9455622412901108 entropy -10.805687728254862
epoch: 0, step: 108
	action: tensor([[ 0.0217,  0.0127, -0.0033, -0.0200, -0.0113,  0.0246, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31884315072363656, distance: 0.9444527973161729 entropy -10.806109420152028
epoch: 0, step: 109
	action: tensor([[ 0.0219,  0.0127,  0.0142, -0.0084, -0.0115, -0.0193, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3143360884451566, distance: 0.9475722619158806 entropy -10.806995612883185
epoch: 0, step: 110
	action: tensor([[ 0.0217,  0.0127,  0.0028,  0.0075, -0.0112, -0.0247, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.320600174326673, distance: 0.9432339170019254 entropy -10.805321753516319
epoch: 0, step: 111
	action: tensor([[ 0.0214,  0.0126,  0.0002,  0.0073, -0.0111,  0.0123, -0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3285357870654929, distance: 0.9377090959666544 entropy -10.804940699060584
epoch: 0, step: 112
	action: tensor([[ 0.0218,  0.0127,  0.0034,  0.0362, -0.0112, -0.0668, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32481063244818587, distance: 0.9403066133889962 entropy -10.804472494013305
epoch: 0, step: 113
	action: tensor([[ 0.0209,  0.0124,  0.0052,  0.0066, -0.0107, -0.1098, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30036360708813126, distance: 0.9571783992749306 entropy -10.804452756083018
epoch: 0, step: 114
	action: tensor([[ 0.0208,  0.0123,  0.0092,  0.0026, -0.0110, -0.0481, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.311764751068508, distance: 0.9493473647575289 entropy -10.81033254223807
epoch: 0, step: 115
	action: tensor([[ 0.0213,  0.0125,  0.0067, -0.0046, -0.0110, -0.0172, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3160415749598279, distance: 0.9463930559863412 entropy -10.805412284863197
epoch: 0, step: 116
	action: tensor([[ 0.0216,  0.0126, -0.0035, -0.0071, -0.0112,  0.0156, -0.0266]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3227357897108186, distance: 0.941750276384418 entropy -10.804388708099415
epoch: 0, step: 117
	action: tensor([[ 0.0218,  0.0126,  0.0055, -0.0082, -0.0114,  0.0462, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[-0.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3295760455360158, distance: 0.936982446720736 entropy -10.805823939491871
epoch: 0, step: 118
	action: tensor([[ 2.2104e-02,  1.2679e-02,  2.3397e-03,  2.8123e-05, -1.1397e-02,
         -8.5302e-03, -2.4892e-02]], dtype=torch.float64)
	q_value: tensor([[-0.0887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3211124212867611, distance: 0.94287826500116 entropy -10.80520860662659
epoch: 0, step: 119
	action: tensor([[ 0.0216,  0.0126,  0.0018,  0.0314, -0.0112, -0.0519, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3256142909945928, distance: 0.9397468367662981 entropy -10.804814394214679
epoch: 0, step: 120
	action: tensor([[ 0.0211,  0.0125,  0.0011,  0.0001, -0.0108, -0.0500, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31071238788422084, distance: 0.9500728990863341 entropy -10.804364556900035
epoch: 0, step: 121
	action: tensor([[ 0.0213,  0.0126,  0.0140, -0.0016, -0.0111, -0.0423, -0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117478131359157, distance: 0.9493590467243872 entropy -10.80729236965007
epoch: 0, step: 122
	action: tensor([[ 0.0215,  0.0125, -0.0052, -0.0032, -0.0110, -0.0214, -0.0460]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.316019579816316, distance: 0.9464082732005548 entropy -10.804259298756621
epoch: 0, step: 123
	action: tensor([[ 0.0215,  0.0126, -0.0152, -0.0092, -0.0113, -0.0132,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31518856145832397, distance: 0.9469830293500168 entropy -10.806060991146756
epoch: 0, step: 124
	action: tensor([[ 0.0215,  0.0126, -0.0002, -0.0015, -0.0114, -0.0816, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3035477086289805, distance: 0.9549978174736256 entropy -10.80856141328778
epoch: 0, step: 125
	action: tensor([[ 0.0210,  0.0124, -0.0039, -0.0128, -0.0111, -0.0056, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31446531571783165, distance: 0.9474829631011119 entropy -10.809230093647608
epoch: 0, step: 126
	action: tensor([[ 0.0216,  0.0126, -0.0003,  0.0214, -0.0114, -0.0303, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-0.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32571513725241397, distance: 0.9396765702306105 entropy -10.806434757371283
epoch: 0, step: 127
	action: tensor([[ 0.0214,  0.0126, -0.0057, -0.0070, -0.0110, -0.0365, -0.0298]],
       dtype=torch.float64)
	q_value: tensor([[-0.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3108365767436341, distance: 0.9499873079756036 entropy -10.805226487774728
LOSS epoch 0 actor 0.1470862172475864 critic 23.519386829397412
epoch: 1, step: 0
	action: tensor([[ 0.0901, -0.1692, -0.0342, -0.0193,  0.0394,  0.0502,  0.0325]],
       dtype=torch.float64)
	q_value: tensor([[0.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24856863500154291, distance: 0.9919764287716306 entropy -8.130923995372706
epoch: 1, step: 1
	action: tensor([[ 0.0900, -0.1718, -0.0474,  0.0189,  0.0398, -0.0245,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[0.0445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24618185757830868, distance: 0.9935505910681707 entropy -7.962715808222408
epoch: 1, step: 2
	action: tensor([[ 0.0891, -0.1694, -0.0221,  0.0317,  0.0385,  0.0232, -0.0479]],
       dtype=torch.float64)
	q_value: tensor([[0.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26717882446819397, distance: 0.9796156132569496 entropy -7.964840105909391
epoch: 1, step: 3
	action: tensor([[ 0.0896, -0.1698, -0.0385,  0.0179,  0.0394, -0.0437,  0.0519]],
       dtype=torch.float64)
	q_value: tensor([[0.0465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24195148540738642, distance: 0.996334557231011 entropy -7.9990311118017186
epoch: 1, step: 4
	action: tensor([[ 0.0892, -0.1697, -0.0089,  0.0268,  0.0384,  0.0403,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[0.0463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2693869297746433, distance: 0.978138631967708 entropy -7.948514465328688
epoch: 1, step: 5
	action: tensor([[ 0.0900, -0.1722, -0.0191, -0.0233,  0.0395,  0.0332, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.0443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23909969325723712, distance: 0.9982069121906423 entropy -7.973248959039892
epoch: 1, step: 6
	action: tensor([[ 0.0898, -0.1725, -0.0353,  0.0341,  0.0396, -0.0696, -0.0432]],
       dtype=torch.float64)
	q_value: tensor([[0.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2416446808258219, distance: 0.9965361598183623 entropy -7.977228604535877
epoch: 1, step: 7
	action: tensor([[ 0.0887, -0.1704, -0.0041,  0.0018,  0.0381, -0.0844, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[0.0499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22093910038424747, distance: 1.0100488936778491 entropy -7.985427976679629
epoch: 1, step: 8
	action: tensor([[ 0.0890, -0.1779, -0.0149, -0.0115,  0.0382,  0.0796, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[0.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2527394839368158, distance: 0.9892195961513114 entropy -7.965581443687851
epoch: 1, step: 9
	action: tensor([[ 0.0904, -0.1724, -0.0015,  0.0517,  0.0404,  0.0002, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.0450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27083100084197387, distance: 0.9771714987437053 entropy -7.983681109959415
epoch: 1, step: 10
	action: tensor([[ 0.0895, -0.1723, -0.0245,  0.0309,  0.0390,  0.0031, -0.0561]],
       dtype=torch.float64)
	q_value: tensor([[0.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2591171780209036, distance: 0.9849891712086266 entropy -7.983479339381115
epoch: 1, step: 11
	action: tensor([[ 0.0893, -0.1742, -0.0486,  0.0361,  0.0391,  0.0300, -0.0816]],
       dtype=torch.float64)
	q_value: tensor([[0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.266854374045178, distance: 0.979832447553384 entropy -8.002638616869419
epoch: 1, step: 12
	action: tensor([[ 0.0893, -0.1721, -0.0261,  0.0568,  0.0396,  0.0394, -0.0702]],
       dtype=torch.float64)
	q_value: tensor([[0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2827297258201038, distance: 0.9691658654459927 entropy -8.023770305328311
epoch: 1, step: 13
	action: tensor([[ 0.0896, -0.1719, -0.0331,  0.0156,  0.0397, -0.0177,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[0.0465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24575937440945994, distance: 0.9938289735989015 entropy -8.024371731716535
epoch: 1, step: 14
	action: tensor([[ 0.0893, -0.1687, -0.0304,  0.0465,  0.0387, -0.0818,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[0.0459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.247796349913739, distance: 0.9924860507430309 entropy -7.95846232885556
epoch: 1, step: 15
	action: tensor([[ 0.0887, -0.1703,  0.0137, -0.0144,  0.0379, -0.1146, -0.0394]],
       dtype=torch.float64)
	q_value: tensor([[0.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20368975921300414, distance: 1.0211695206620262 entropy -7.971724922081821
epoch: 1, step: 16
	action: tensor([[ 0.0891, -0.1691, -0.0029, -0.0096,  0.0381,  0.0431, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[0.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25105849135562763, distance: 0.9903316157504103 entropy -7.9673729314088435
epoch: 1, step: 17
	action: tensor([[ 0.0901, -0.1716, -0.0274,  0.0314,  0.0397, -0.0311,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.0450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25127670143310066, distance: 0.9901873347357818 entropy -7.973599468470665
epoch: 1, step: 18
	action: tensor([[ 0.0892, -0.1741, -0.0234,  0.0091,  0.0385, -0.0357, -0.0633]],
       dtype=torch.float64)
	q_value: tensor([[0.0466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23529682540115937, distance: 1.0006982489258203 entropy -7.9647934378978675
epoch: 1, step: 19
	action: tensor([[ 0.0891, -0.1737, -0.0083, -0.0032,  0.0388, -0.0693, -0.0712]],
       dtype=torch.float64)
	q_value: tensor([[0.0492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21981142025489941, distance: 1.0107796454005493 entropy -7.993629885304715
epoch: 1, step: 20
	action: tensor([[ 0.0891, -0.1744, -0.0229,  0.0251,  0.0385, -0.0931,  0.0357]],
       dtype=torch.float64)
	q_value: tensor([[0.0501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22842981713840604, distance: 1.0051813249177899 entropy -7.9865950095852325
epoch: 1, step: 21
	action: tensor([[ 0.0890, -0.1744, -0.0172,  0.0041,  0.0380, -0.0585, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[0.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2260896375600543, distance: 1.0067045329943116 entropy -7.953086910961696
epoch: 1, step: 22
	action: tensor([[ 0.0890, -0.1716, -0.0300, -0.0035,  0.0384, -0.0254,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[0.0489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2333366382674522, distance: 1.001979988348919 entropy -7.970847126659831
epoch: 1, step: 23
	action: tensor([[ 0.0893, -0.1749, -0.0022,  0.0136,  0.0387, -0.0015, -0.0449]],
       dtype=torch.float64)
	q_value: tensor([[0.0463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24650271453524075, distance: 0.9933391199381535 entropy -7.954714320919755
epoch: 1, step: 24
	action: tensor([[ 0.0895, -0.1715, -0.0148,  0.0552,  0.0392, -0.0844, -0.0599]],
       dtype=torch.float64)
	q_value: tensor([[0.0471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24969272983887048, distance: 0.9912341833933261 entropy -7.990712876067162
epoch: 1, step: 25
	action: tensor([[ 0.0885, -0.1678, -0.0322, -0.0142,  0.0382, -0.0596, -0.0421]],
       dtype=torch.float64)
	q_value: tensor([[0.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22157323463733314, distance: 1.0096377339130138 entropy -8.00401444200686
epoch: 1, step: 26
	action: tensor([[ 0.0890, -0.1700, -0.0204,  0.0155,  0.0384,  0.0184, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[0.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25662719282901636, distance: 0.9866429760547236 entropy -7.972306228656886
epoch: 1, step: 27
	action: tensor([[ 0.0896, -0.1714, -0.0182,  0.0247,  0.0393, -0.0983,  0.0565]],
       dtype=torch.float64)
	q_value: tensor([[0.0460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22984787600927947, distance: 1.004257195225634 entropy -7.98089232107445
epoch: 1, step: 28
	action: tensor([[ 0.0891, -0.1701, -0.0047, -0.0203,  0.0380, -0.0802, -0.0321]],
       dtype=torch.float64)
	q_value: tensor([[0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2107183391566746, distance: 1.016652889344167 entropy -7.949737299472434
epoch: 1, step: 29
	action: tensor([[ 0.0892, -0.1747, -0.0139,  0.0199,  0.0383,  0.1006,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[0.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2783750283314078, distance: 0.9721034176596274 entropy -7.965122006912866
epoch: 1, step: 30
	action: tensor([[ 0.0907, -0.1759, -0.0187,  0.0162,  0.0407, -0.0436, -0.0535]],
       dtype=torch.float64)
	q_value: tensor([[0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23690138184423437, distance: 0.9996478282865529 entropy -7.973609407403222
epoch: 1, step: 31
	action: tensor([[ 0.0890, -0.1721, -0.0104,  0.0238,  0.0386,  0.0297,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[0.0490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2626755578431704, distance: 0.9826209261115874 entropy -7.987565859291064
epoch: 1, step: 32
	action: tensor([[ 0.0899, -0.1701, -0.0154,  0.0378,  0.0394, -0.0495, -0.0743]],
       dtype=torch.float64)
	q_value: tensor([[0.0439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2511388847774675, distance: 0.9902784618771802 entropy -7.962452604750924
epoch: 1, step: 33
	action: tensor([[ 0.0888, -0.1710, -0.0152,  0.0624,  0.0386,  0.1487, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[0.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3172880764845517, distance: 0.9455302708222123 entropy -8.009721990204609
epoch: 1, step: 34
	action: tensor([[ 0.0910, -0.1730, -0.0187,  0.0487,  0.0417,  0.0110, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[0.0426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.271756260272087, distance: 0.9765513241700771 entropy -8.020075078513463
epoch: 1, step: 35
	action: tensor([[ 0.0895, -0.1693, -0.0134,  0.0124,  0.0391,  0.0898, -0.0013]],
       dtype=torch.float64)
	q_value: tensor([[0.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27609583141454896, distance: 0.9736373643038023 entropy -7.983717774938301
epoch: 1, step: 36
	action: tensor([[ 0.0905, -0.1694, -0.0236, -0.0111,  0.0404, -0.0768, -0.0610]],
       dtype=torch.float64)
	q_value: tensor([[0.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2190523375269281, distance: 1.0112712437486775 entropy -7.981139030966341
epoch: 1, step: 37
	action: tensor([[ 0.0889, -0.1685, -0.0098, -0.0197,  0.0383, -0.0274,  0.0331]],
       dtype=torch.float64)
	q_value: tensor([[0.0502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22641913388764712, distance: 1.0064902053560365 entropy -7.977772266797394
epoch: 1, step: 38
	action: tensor([[ 0.0895, -0.1671, -0.0177,  0.0097,  0.0388, -0.0272, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[0.0459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2442229264088298, distance: 0.9948407128507534 entropy -7.9489273899607875
epoch: 1, step: 39
	action: tensor([[ 0.0893, -0.1716, -0.0097,  0.0166,  0.0386,  0.0081, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[0.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2534765403500394, distance: 0.9887316200646112 entropy -7.968939119073332
epoch: 1, step: 40
	action: tensor([[ 0.0896, -0.1653, -0.0161,  0.0036,  0.0391,  0.0373, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[0.0457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26010957244487487, distance: 0.9843292656978158 entropy -7.975440556480853
epoch: 1, step: 41
	action: tensor([[ 0.0899, -0.1695, -0.0209,  0.0158,  0.0395, -0.0072, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[0.0449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25108988747558547, distance: 0.9903108578569638 entropy -7.974062967155592
epoch: 1, step: 42
	action: tensor([[ 0.0894, -0.1731, -0.0069,  0.0678,  0.0388, -0.1100, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[0.0461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2483937011323346, distance: 0.9920918885358933 entropy -7.969902071317207
epoch: 1, step: 43
	action: tensor([[ 0.0885, -0.1684, -0.0127,  0.0128,  0.0379,  0.0101, -0.0357]],
       dtype=torch.float64)
	q_value: tensor([[0.0501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2539331384668361, distance: 0.9884292035485137 entropy -7.979441021942839
epoch: 1, step: 44
	action: tensor([[ 0.0896, -0.1714, -0.0119,  0.0354,  0.0392, -0.0934, -0.0451]],
       dtype=torch.float64)
	q_value: tensor([[0.0465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.236847683700823, distance: 0.9996829995520145 entropy -7.98673109934059
epoch: 1, step: 45
	action: tensor([[ 0.0886, -0.1692, -0.0042, -0.0159,  0.0380, -0.0738, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[0.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21519180397613435, distance: 1.0137677193958503 entropy -7.985043647337416
epoch: 1, step: 46
	action: tensor([[ 0.0892, -0.1708, -0.0179, -0.0298,  0.0383, -0.0366,  0.0727]],
       dtype=torch.float64)
	q_value: tensor([[0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21707252920609432, distance: 1.0125522872881503 entropy -7.955479918994429
epoch: 1, step: 47
	action: tensor([[ 0.0897, -0.1698, -0.0281,  0.0363,  0.0390,  0.0270, -0.0036]],
       dtype=torch.float64)
	q_value: tensor([[0.0454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2706820565243958, distance: 0.9772712950258705 entropy -7.936452056635752
epoch: 1, step: 48
	action: tensor([[ 0.0897, -0.1674, -0.0170,  0.0410,  0.0393, -0.0594, -0.0714]],
       dtype=torch.float64)
	q_value: tensor([[0.0453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25218751125564876, distance: 0.9895848779953442 entropy -7.979218042366526
epoch: 1, step: 49
	action: tensor([[ 0.0887, -0.1725, -0.0226, -0.0225,  0.0385, -0.1241, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[0.0500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19642711956183234, distance: 1.025815670185118 entropy -8.00653627692767
epoch: 1, step: 50
	action: tensor([[ 0.0890, -0.1728, -0.0113,  0.0306,  0.0379,  0.0155, -0.0636]],
       dtype=torch.float64)
	q_value: tensor([[0.0510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2617756785156402, distance: 0.983220370925777 entropy -7.954344281236456
epoch: 1, step: 51
	action: tensor([[ 0.0895, -0.1758, -0.0023, -0.0311,  0.0394, -0.0177,  0.0332]],
       dtype=torch.float64)
	q_value: tensor([[0.0472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21701402214483256, distance: 1.0125901198806935 entropy -8.010329081524008
epoch: 1, step: 52
	action: tensor([[ 0.0897, -0.1746, -0.0245,  0.0468,  0.0391,  0.0784,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[0.0455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286606700924381, distance: 0.9665430583706471 entropy -7.946167165266757
epoch: 1, step: 53
	action: tensor([[ 0.0904, -0.1694, -0.0156, -0.0116,  0.0402,  0.0038, -0.0376]],
       dtype=torch.float64)
	q_value: tensor([[0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24007271325975543, distance: 0.9975684671848724 entropy -7.970484392546718
epoch: 1, step: 54
	action: tensor([[ 0.0896, -0.1731, -0.0268,  0.0172,  0.0392,  0.0087, -0.0746]],
       dtype=torch.float64)
	q_value: tensor([[0.0470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2527789449241863, distance: 0.989193476681255 entropy -7.982533087631553
epoch: 1, step: 55
	action: tensor([[ 0.0894, -0.1683, -0.0144, -0.0195,  0.0394, -0.0550, -0.0367]],
       dtype=torch.float64)
	q_value: tensor([[0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21991601169413, distance: 1.010711890985792 entropy -8.007753854322084
epoch: 1, step: 56
	action: tensor([[ 0.0892, -0.1694, -0.0026,  0.0232,  0.0385, -0.0429, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.0486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24493228189115712, distance: 0.9943737356138929 entropy -7.967560235857365
epoch: 1, step: 57
	action: tensor([[ 0.0892, -0.1685, -0.0210, -0.0170,  0.0385,  0.0339,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[0.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24505875233802166, distance: 0.9942904555622297 entropy -7.973586983749031
epoch: 1, step: 58
	action: tensor([[ 0.0899, -0.1731, -0.0261,  0.0222,  0.0396, -0.1123,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[0.0452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22378267106422844, distance: 1.0082038703645253 entropy -7.9673991047653505
epoch: 1, step: 59
	action: tensor([[ 0.0887, -0.1709,  0.0092, -0.0120,  0.0377, -0.0683, -0.0384]],
       dtype=torch.float64)
	q_value: tensor([[0.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21705740957744335, distance: 1.0125620642733966 entropy -7.9619588227888585
epoch: 1, step: 60
	action: tensor([[ 0.0893, -0.1803, -0.0373,  0.0035,  0.0385,  0.0558,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.0489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2519600341782112, distance: 0.9897353774608004 entropy -7.972305987114905
epoch: 1, step: 61
	action: tensor([[ 0.0900, -0.1729, -0.0005,  0.0556,  0.0399, -0.0071,  0.0360]],
       dtype=torch.float64)
	q_value: tensor([[0.0450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27010774747902955, distance: 0.977656000558852 entropy -7.974309457854157
epoch: 1, step: 62
	action: tensor([[ 0.0896, -0.1689, -0.0156, -0.0213,  0.0389, -0.0114, -0.0251]],
       dtype=torch.float64)
	q_value: tensor([[0.0448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23036985033934276, distance: 1.0039168174550328 entropy -7.964400422462511
epoch: 1, step: 63
	action: tensor([[ 0.0895, -0.1699, -0.0056,  0.0299,  0.0390, -0.0990,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[0.0471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23355429573556685, distance: 1.0018377460562784 entropy -7.973073271228396
epoch: 1, step: 64
	action: tensor([[ 0.0890, -0.1688, -0.0412,  0.0164,  0.0379, -0.0098, -0.0130]],
       dtype=torch.float64)
	q_value: tensor([[0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25036179669681013, distance: 0.9907921312627237 entropy -7.955451245720722
epoch: 1, step: 65
	action: tensor([[ 0.0892, -0.1687, -0.0401,  0.0373,  0.0388,  0.0005,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[0.0470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2642906460070059, distance: 0.9815441345596844 entropy -7.975755464240564
epoch: 1, step: 66
	action: tensor([[ 0.0894, -0.1731, -0.0269,  0.0430,  0.0388,  0.0250, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[0.0455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27069445529096803, distance: 0.9772629879423329 entropy -7.96654192999753
epoch: 1, step: 67
	action: tensor([[ 0.0895, -0.1734, -0.0352, -0.0073,  0.0394,  0.0519,  0.0511]],
       dtype=torch.float64)
	q_value: tensor([[0.0464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2513089062691666, distance: 0.9901660390423215 entropy -8.001074577926085
epoch: 1, step: 68
	action: tensor([[ 0.0901, -0.1702, -0.0100,  0.0447,  0.0398,  0.0638,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[0.0438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2860231906522016, distance: 0.9669382628983676 entropy -7.955778399610745
epoch: 1, step: 69
	action: tensor([[ 0.0902, -0.1718, -0.0217,  0.0007,  0.0400, -0.0399, -0.0353]],
       dtype=torch.float64)
	q_value: tensor([[0.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2326769807238004, distance: 1.0024109607771998 entropy -7.97965856940686
epoch: 1, step: 70
	action: tensor([[ 0.0891, -0.1679, -0.0163, -0.0054,  0.0386, -0.0090,  0.0391]],
       dtype=torch.float64)
	q_value: tensor([[0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2399788068529568, distance: 0.9976301014873148 entropy -7.976223119209576
epoch: 1, step: 71
	action: tensor([[ 0.0896, -0.1646, -0.0083,  0.0221,  0.0389,  0.0893,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.0450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2852565654806154, distance: 0.9674572435736164 entropy -7.952036201531377
epoch: 1, step: 72
	action: tensor([[ 0.0906, -0.1752, -0.0157,  0.0589,  0.0404, -0.0079, -0.0712]],
       dtype=torch.float64)
	q_value: tensor([[0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2699700030163471, distance: 0.9777482472891617 entropy -7.974972611987974
epoch: 1, step: 73
	action: tensor([[ 0.0891, -0.1721, -0.0037,  0.0077,  0.0392, -0.1216, -0.0274]],
       dtype=torch.float64)
	q_value: tensor([[0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21324526208002648, distance: 1.0150241532961441 entropy -8.021765432163411
epoch: 1, step: 74
	action: tensor([[ 0.0889, -0.1732, -0.0050,  0.0599,  0.0379, -0.1260, -0.0963]],
       dtype=torch.float64)
	q_value: tensor([[0.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23950362082722587, distance: 0.9979419255599228 entropy -7.968414519766976
epoch: 1, step: 75
	action: tensor([[ 0.0882, -0.1696, -0.0229,  0.0369,  0.0383, -0.0202,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[0.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25713579307971723, distance: 0.9863053979775004 entropy -8.02404418128444
epoch: 1, step: 76
	action: tensor([[ 0.0893, -0.1713, -0.0248,  0.0282,  0.0386, -0.1114, -0.0495]],
       dtype=torch.float64)
	q_value: tensor([[0.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.228288417568823, distance: 1.0052734265169627 entropy -7.967560689987706
epoch: 1, step: 77
	action: tensor([[ 0.0885, -0.1657, -0.0247,  0.0349,  0.0378,  0.0629,  0.0263]],
       dtype=torch.float64)
	q_value: tensor([[0.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822187254766323, distance: 0.9695110323471229 entropy -7.9837342987715925
epoch: 1, step: 78
	action: tensor([[ 0.0902, -0.1776, -0.0263,  0.0047,  0.0398, -0.0644,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22329343472603247, distance: 1.0085215470344882 entropy -7.970239794195456
epoch: 1, step: 79
	action: tensor([[ 0.0891, -0.1713, -0.0147,  0.0343,  0.0383, -0.0467,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24820217215574403, distance: 0.9922182859867696 entropy -7.951160661265556
epoch: 1, step: 80
	action: tensor([[ 0.0891, -0.1708, -0.0220,  0.0280,  0.0384, -0.0503, -0.0378]],
       dtype=torch.float64)
	q_value: tensor([[0.0470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24431916395350373, distance: 0.9947773713715637 entropy -7.9619217982685155
epoch: 1, step: 81
	action: tensor([[ 0.0889, -0.1749, -0.0159,  0.0734,  0.0384, -0.0171,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[0.0488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2738723380756001, distance: 0.9751314960311469 entropy -7.9833782022078825
epoch: 1, step: 82
	action: tensor([[ 0.0892, -0.1744, -0.0072,  0.0370,  0.0388, -0.0334, -0.1159]],
       dtype=torch.float64)
	q_value: tensor([[0.0464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25078821222245407, distance: 0.9905102958095355 entropy -7.984171219544995
epoch: 1, step: 83
	action: tensor([[ 0.0890, -0.1713, -0.0189,  0.0065,  0.0392,  0.0094, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[0.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24827445534021741, distance: 0.9921705853824551 entropy -8.04280933541642
epoch: 1, step: 84
	action: tensor([[ 0.0896, -0.1632, -0.0262,  0.0034,  0.0392, -0.0252,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[0.0466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24469234465230316, distance: 0.9945317137209425 entropy -7.981818165400138
epoch: 1, step: 85
	action: tensor([[ 0.0893, -0.1719,  0.0021,  0.0058,  0.0386, -0.1545, -0.0175]],
       dtype=torch.float64)
	q_value: tensor([[0.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20408297774711803, distance: 1.0209173624300998 entropy -7.957450027629576
epoch: 1, step: 86
	action: tensor([[ 0.0888, -0.1704, -0.0399, -0.0118,  0.0377,  0.0333,  0.0405]],
       dtype=torch.float64)
	q_value: tensor([[0.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2457025171951417, distance: 0.9938664319909323 entropy -7.964213789791253
epoch: 1, step: 87
	action: tensor([[ 0.0898, -0.1674, -0.0211, -0.0583,  0.0395, -0.0972, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[0.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18960818819599812, distance: 1.0301588915882054 entropy -7.959351531856285
epoch: 1, step: 88
	action: tensor([[ 0.0893, -0.1693, -0.0416, -0.0298,  0.0383, -0.0728,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[0.0495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20965940589351162, distance: 1.017334652753242 entropy -7.943912108665886
epoch: 1, step: 89
	action: tensor([[ 0.0892, -0.1793, -0.0377,  0.0083,  0.0383,  0.0351, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24948922136689933, distance: 0.9913686022409703 entropy -7.9457592467482545
epoch: 1, step: 90
	action: tensor([[ 0.0896, -0.1787, -0.0250,  0.0510,  0.0396, -0.1060, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[0.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23579272130063134, distance: 1.0003737291305557 entropy -7.99096544024984
epoch: 1, step: 91
	action: tensor([[ 0.0885, -0.1689, -0.0454,  0.0450,  0.0378, -0.0283,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[0.0507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25978506806657853, distance: 0.9845450977515398 entropy -7.980211960654985
epoch: 1, step: 92
	action: tensor([[ 0.0890, -0.1710, -0.0260,  0.0192,  0.0384,  0.0006, -0.0336]],
       dtype=torch.float64)
	q_value: tensor([[0.0473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2528657569449091, distance: 0.9891360128435048 entropy -7.974302287041324
epoch: 1, step: 93
	action: tensor([[ 0.0894, -0.1720, -0.0074, -0.0224,  0.0390, -0.0558,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.0470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21471810240441103, distance: 1.0140736227502958 entropy -7.9854130154415115
epoch: 1, step: 94
	action: tensor([[ 0.0893, -0.1684, -0.0271, -0.0259,  0.0385,  0.0040, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23245849169880506, distance: 1.0025536648271491 entropy -7.954433350495829
epoch: 1, step: 95
	action: tensor([[ 0.0895, -0.1694, -0.0087,  0.0109,  0.0392, -0.0657, -0.0567]],
       dtype=torch.float64)
	q_value: tensor([[0.0463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2325263093060259, distance: 1.0025093725694312 entropy -7.964403745954362
epoch: 1, step: 96
	action: tensor([[ 0.0890, -0.1774, -0.0255, -0.0351,  0.0384,  0.0018, -0.0536]],
       dtype=torch.float64)
	q_value: tensor([[0.0496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21880851113395638, distance: 1.011429100278785 entropy -7.984994604958404
epoch: 1, step: 97
	action: tensor([[ 0.0895, -0.1723, -0.0042, -0.0066,  0.0394, -0.1004, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21127348168623472, distance: 1.016295294229351 entropy -7.984087935502198
epoch: 1, step: 98
	action: tensor([[ 0.0890, -0.1730, -0.0043,  0.0556,  0.0381, -0.0156,  0.0452]],
       dtype=torch.float64)
	q_value: tensor([[0.0499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26677717679421464, distance: 0.9798840323802089 entropy -7.9613952442908555
epoch: 1, step: 99
	action: tensor([[ 0.0895, -0.1733, -0.0005,  0.0470,  0.0388, -0.0714, -0.0340]],
       dtype=torch.float64)
	q_value: tensor([[0.0450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2471986263118099, distance: 0.9928803020430549 entropy -7.961190132551322
epoch: 1, step: 100
	action: tensor([[ 0.0888, -0.1761, -0.0173,  0.0184,  0.0382,  0.1115, -0.0442]],
       dtype=torch.float64)
	q_value: tensor([[0.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2790803311046228, distance: 0.9716282436599829 entropy -7.988633854295577
epoch: 1, step: 101
	action: tensor([[ 0.0907, -0.1723,  0.0054,  0.0729,  0.0409,  0.0398, -0.0604]],
       dtype=torch.float64)
	q_value: tensor([[0.0448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29396006247948636, distance: 0.961548793315473 entropy -8.006214409254573
epoch: 1, step: 102
	action: tensor([[ 0.0898, -0.1719, -0.0075, -0.0081,  0.0399, -0.0713, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.0455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21882853131479907, distance: 1.0114161398692558 entropy -8.02643719437721
epoch: 1, step: 103
	action: tensor([[ 0.0891, -0.1698, -0.0176, -0.0123,  0.0383,  0.0218, -0.0395]],
       dtype=torch.float64)
	q_value: tensor([[0.0488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2431090727161801, distance: 0.9955735341169303 entropy -7.961030940861531
epoch: 1, step: 104
	action: tensor([[ 0.0897, -0.1689, -0.0093, -0.0277,  0.0395, -0.0228, -0.0389]],
       dtype=torch.float64)
	q_value: tensor([[0.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22381749432685727, distance: 1.0081812546996642 entropy -7.987191571455399
epoch: 1, step: 105
	action: tensor([[ 0.0895, -0.1742, -0.0324,  0.0217,  0.0390, -0.0205, -0.0529]],
       dtype=torch.float64)
	q_value: tensor([[0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24625197788018105, distance: 0.9935043798643935 entropy -7.973709749319616
epoch: 1, step: 106
	action: tensor([[ 0.0891, -0.1744, -0.0085,  0.0128,  0.0388, -0.0116, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24346520338919253, distance: 0.9953392890125188 entropy -7.992253782514661
epoch: 1, step: 107
	action: tensor([[ 0.0895, -0.1693, -0.0057,  0.0079,  0.0389, -0.0316, -0.0356]],
       dtype=torch.float64)
	q_value: tensor([[0.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24009668979276055, distance: 0.99755272987529 entropy -7.970819882116879
epoch: 1, step: 108
	action: tensor([[ 0.0893, -0.1683, -0.0162,  0.0285,  0.0387,  0.0188, -0.1104]],
       dtype=torch.float64)
	q_value: tensor([[0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26546931048312605, distance: 0.980757563793533 entropy -7.98134693089265
epoch: 1, step: 109
	action: tensor([[ 0.0894, -0.1700, -0.0121,  0.0173,  0.0397, -0.0279,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24547749993588874, distance: 0.9940146629141909 entropy -8.04091371294982
epoch: 1, step: 110
	action: tensor([[ 0.0894, -0.1687, -0.0309,  0.0320,  0.0387,  0.0658, -0.0574]],
       dtype=torch.float64)
	q_value: tensor([[0.0453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2795747164701733, distance: 0.9712950295812455 entropy -7.951024081106415
epoch: 1, step: 111
	action: tensor([[ 0.0900, -0.1698, -0.0398,  0.0673,  0.0400, -0.0370,  0.0478]],
       dtype=torch.float64)
	q_value: tensor([[0.0460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2697857779274554, distance: 0.9778716082442869 entropy -8.013427616072843
epoch: 1, step: 112
	action: tensor([[ 0.0891, -0.1720, -0.0087, -0.0068,  0.0383,  0.1282, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[0.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27432111535652703, distance: 0.9748301134220191 entropy -7.962872778081902
epoch: 1, step: 113
	action: tensor([[ 0.0910, -0.1679, -0.0134,  0.0156,  0.0412, -0.0517, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[0.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24138115969666918, distance: 0.9967092881118795 entropy -8.002830681595531
epoch: 1, step: 114
	action: tensor([[ 0.0891, -0.1693, -0.0279,  0.0651,  0.0383, -0.0760, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25848989359417496, distance: 0.9854060641515519 entropy -7.964235819533791
epoch: 1, step: 115
	action: tensor([[ 0.0885, -0.1692, -0.0142, -0.0152,  0.0381, -0.0181,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[0.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2305048289130588, distance: 1.003828779582502 entropy -7.990988735976752
epoch: 1, step: 116
	action: tensor([[ 0.0898, -0.1622,  0.0079,  0.0343,  0.0392, -0.0399, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[0.0441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25850718625029423, distance: 0.9853945738243076 entropy -7.9373858240259905
epoch: 1, step: 117
	action: tensor([[ 0.0892, -0.1762, -0.0475, -0.0219,  0.0385, -0.0427, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[0.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21566069927480636, distance: 1.0134648288663162 entropy -7.982343524923588
epoch: 1, step: 118
	action: tensor([[ 0.0891, -0.1687, -0.0256,  0.0314,  0.0386, -0.0182, -0.0322]],
       dtype=torch.float64)
	q_value: tensor([[0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2562625542752618, distance: 0.9868849299955534 entropy -7.9589003589249225
epoch: 1, step: 119
	action: tensor([[ 0.0891, -0.1716, -0.0079,  0.0080,  0.0387, -0.0551, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[0.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2315327541374783, distance: 1.0031580763774104 entropy -7.986467472622773
epoch: 1, step: 120
	action: tensor([[ 0.0892, -0.1691, -0.0300,  0.0183,  0.0384, -0.1022, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22722645307950895, distance: 1.0059648750351942 entropy -7.962270699769955
epoch: 1, step: 121
	action: tensor([[ 0.0887, -0.1714, -0.0308, -0.0579,  0.0378,  0.0475,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[0.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22427372138611312, distance: 1.0078849151308982 entropy -7.9654587735346984
epoch: 1, step: 122
	action: tensor([[ 0.0900, -0.1751, -0.0318,  0.0081,  0.0400,  0.0902, -0.0367]],
       dtype=torch.float64)
	q_value: tensor([[0.0455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2691542662606866, distance: 0.9782943635481011 entropy -7.964128732715227
epoch: 1, step: 123
	action: tensor([[ 0.0904, -0.1717,  0.0046,  0.0444,  0.0405,  0.0217, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[0.0454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2735049657304225, distance: 0.9753781407080047 entropy -7.998003636180312
epoch: 1, step: 124
	action: tensor([[ 0.0898, -0.1764, -0.0220,  0.0350,  0.0394, -0.0374, -0.0666]],
       dtype=torch.float64)
	q_value: tensor([[0.0454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24726501547807178, distance: 0.9928365202759752 entropy -7.98863753220144
epoch: 1, step: 125
	action: tensor([[ 0.0889, -0.1687, -0.0160,  0.0511,  0.0387, -0.0216, -0.0563]],
       dtype=torch.float64)
	q_value: tensor([[0.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2658782404944946, distance: 0.980484520717891 entropy -8.003901723563356
epoch: 1, step: 126
	action: tensor([[ 0.0890, -0.1737, -0.0346,  0.0234,  0.0389, -0.0848, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23021357668517406, distance: 1.0040187351011236 entropy -8.00842361953803
epoch: 1, step: 127
	action: tensor([[ 0.0888, -0.1757, -0.0380,  0.0385,  0.0379,  0.0306, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.0498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2667178983907801, distance: 0.97992364161303 entropy -7.9671822470963525
LOSS epoch 1 actor 0.078294368820673 critic 13.800534732969748
epoch: 2, step: 0
	action: tensor([[ 0.0354, -0.2295, -0.0376,  0.0408, -0.0522, -0.1595,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[0.2216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12559477637044902, distance: 1.0700721594402118 entropy -7.644856075111698
epoch: 2, step: 1
	action: tensor([[ 0.0363, -0.2262, -0.0504,  0.0183, -0.0538, -0.0721,  0.0623]],
       dtype=torch.float64)
	q_value: tensor([[0.2344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13742785134582802, distance: 1.0628070066747353 entropy -7.65291417237389
epoch: 2, step: 2
	action: tensor([[ 0.0365, -0.2344, -0.0351,  0.0600, -0.0534,  0.0877, -0.0568]],
       dtype=torch.float64)
	q_value: tensor([[0.2304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19281905865427806, distance: 1.0281160589765108 entropy -7.646444416924896
epoch: 2, step: 3
	action: tensor([[ 0.0345, -0.2539, -0.0341,  0.0444, -0.0501, -0.0049, -0.0628]],
       dtype=torch.float64)
	q_value: tensor([[0.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1419073832001485, distance: 1.0600437160738974 entropy -7.643710787215311
epoch: 2, step: 4
	action: tensor([[ 0.0347, -0.2206, -0.0495,  0.0403, -0.0516, -0.0614, -0.0907]],
       dtype=torch.float64)
	q_value: tensor([[0.2349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1548240054173936, distance: 1.052035197198595 entropy -7.649838713061267
epoch: 2, step: 5
	action: tensor([[ 0.0349, -0.2704, -0.0422,  0.0459, -0.0515,  0.1196, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[0.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1611367546790391, distance: 1.04809892706844 entropy -7.656358511351092
epoch: 2, step: 6
	action: tensor([[ 0.0351, -0.2325, -0.0441,  0.0236, -0.0504,  0.1227, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[0.2310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18234783110224606, distance: 1.0347632348142588 entropy -7.638710398311553
epoch: 2, step: 7
	action: tensor([[ 0.0350, -0.2431, -0.0461,  0.0595, -0.0498,  0.2108, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[0.2305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21627002344452184, distance: 1.0130710907256906 entropy -7.644876117577632
epoch: 2, step: 8
	action: tensor([[ 0.0350, -0.2211, -0.0523,  0.0853, -0.0481, -0.0944, -0.1700]],
       dtype=torch.float64)
	q_value: tensor([[0.2290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.170700748766887, distance: 1.0421070402371992 entropy -7.6398305625955985
epoch: 2, step: 9
	action: tensor([[ 0.0343, -0.2699, -0.0523, -0.0166, -0.0506,  0.0738, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[0.2403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1142687868744855, distance: 1.0769800735814965 entropy -7.663433120285188
epoch: 2, step: 10
	action: tensor([[ 0.0357, -0.2082, -0.0403,  0.0808, -0.0514, -0.1112,  0.0953]],
       dtype=torch.float64)
	q_value: tensor([[0.2335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17636710504178588, distance: 1.0385407334675154 entropy -7.644039916676498
epoch: 2, step: 11
	action: tensor([[ 0.0367, -0.2094, -0.0284, -0.0413, -0.0536, -0.0668, -0.1266]],
       dtype=torch.float64)
	q_value: tensor([[0.2288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12185647654061171, distance: 1.0723571321634473 entropy -7.645420260084353
epoch: 2, step: 12
	action: tensor([[ 0.0355, -0.2420, -0.0338,  0.0324, -0.0515,  0.0850, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[0.2364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16985231628804787, distance: 1.0426399790012018 entropy -7.661609103838125
epoch: 2, step: 13
	action: tensor([[ 0.0351, -0.2601, -0.0344,  0.0131, -0.0508,  0.0637,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[0.2306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13761882130603809, distance: 1.0626893495910938 entropy -7.643318218913088
epoch: 2, step: 14
	action: tensor([[ 0.0359, -0.2668, -0.0503,  0.0335, -0.0519, -0.2956, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[0.2299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06127701954531495, distance: 1.1087290281721587 entropy -7.639409812575087
epoch: 2, step: 15
	action: tensor([[ 0.0367, -0.2413, -0.0484, -0.0078, -0.0544,  0.0034, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[0.2437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12870937923061454, distance: 1.068164678279694 entropy -7.666535928771139
epoch: 2, step: 16
	action: tensor([[ 0.0352, -0.2329, -0.0374,  0.0135, -0.0517, -0.0791, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[0.2345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1259923389838864, distance: 1.0698288688925988 entropy -7.651800458350303
epoch: 2, step: 17
	action: tensor([[ 0.0357, -0.2686, -0.0449,  0.0130, -0.0528, -0.0701, -0.0862]],
       dtype=torch.float64)
	q_value: tensor([[0.2341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09683616491697022, distance: 1.087526786680394 entropy -7.652452240253867
epoch: 2, step: 18
	action: tensor([[ 0.0350, -0.2477, -0.0208,  0.0174, -0.0522,  0.0064,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[0.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13608974426139508, distance: 1.0636310527965072 entropy -7.654185936973513
epoch: 2, step: 19
	action: tensor([[ 0.0358, -0.2304, -0.0528,  0.0004, -0.0525,  0.1841, -0.0745]],
       dtype=torch.float64)
	q_value: tensor([[0.2309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18838105126268267, distance: 1.030938556318223 entropy -7.643465413231303
epoch: 2, step: 20
	action: tensor([[ 0.0351, -0.2427, -0.0408,  0.0934, -0.0485, -0.0177, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[0.2316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17535645954749512, distance: 1.0391777130841204 entropy -7.648682202773254
epoch: 2, step: 21
	action: tensor([[ 0.0347, -0.2423, -0.0454,  0.0410, -0.0516, -0.0751,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[0.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13310982376874525, distance: 1.0654638863874253 entropy -7.648113387400436
epoch: 2, step: 22
	action: tensor([[ 0.0357, -0.2440, -0.0327,  0.0099, -0.0529, -0.0153,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[0.2338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13020418615468254, distance: 1.0672480005712117 entropy -7.649490042754446
epoch: 2, step: 23
	action: tensor([[ 0.0362, -0.2418, -0.0336, -0.0039, -0.0529, -0.1401, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[0.2303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09565235338360212, distance: 1.0882392848503022 entropy -7.643024107565796
epoch: 2, step: 24
	action: tensor([[ 0.0362, -0.2146, -0.0338,  0.0414, -0.0537, -0.2365, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.2353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1232910178597203, distance: 1.0714808691446656 entropy -7.655235471179917
epoch: 2, step: 25
	action: tensor([[ 0.0363, -0.2221, -0.0400,  0.0537, -0.0538,  0.0705,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[0.2380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19480424112725847, distance: 1.0268510053001536 entropy -7.662805374580176
epoch: 2, step: 26
	action: tensor([[ 0.0355, -0.2899, -0.0298,  0.0292, -0.0512, -0.0106, -0.0991]],
       dtype=torch.float64)
	q_value: tensor([[0.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10124865574360642, distance: 1.0848669273775742 entropy -7.640558372808692
epoch: 2, step: 27
	action: tensor([[ 0.0345, -0.2530, -0.0212,  0.0140, -0.0514, -0.0078, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[0.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12536936170179358, distance: 1.0702100785715503 entropy -7.650174376965775
epoch: 2, step: 28
	action: tensor([[ 0.0352, -0.2648, -0.0284, -0.0042, -0.0522, -0.0651,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[0.2338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09124437034467103, distance: 1.0908882159350994 entropy -7.649096164422079
epoch: 2, step: 29
	action: tensor([[ 0.0361, -0.2496, -0.0351, -0.0026, -0.0534,  0.0172, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[0.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12711418151793696, distance: 1.0691420528861522 entropy -7.648093150616954
epoch: 2, step: 30
	action: tensor([[ 0.0353, -0.2220, -0.0438,  0.0582, -0.0518, -0.0168,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[0.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17430775236300045, distance: 1.0398382693276642 entropy -7.649343563332922
epoch: 2, step: 31
	action: tensor([[ 0.0354, -0.2489, -0.0416,  0.0450, -0.0520,  0.0369, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[0.2307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1578967056179431, distance: 1.0501210797755058 entropy -7.646615940592503
epoch: 2, step: 32
	action: tensor([[ 0.0351, -0.2433, -0.0511,  0.0270, -0.0515, -0.2039, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[0.2319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09646705542755118, distance: 1.087748991908576 entropy -7.644522785097841
epoch: 2, step: 33
	action: tensor([[ 0.0362, -0.2632, -0.0461,  0.0680, -0.0538, -0.2092, -0.0649]],
       dtype=torch.float64)
	q_value: tensor([[0.2387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10048191415242091, distance: 1.0853295888613923 entropy -7.659208430348733
epoch: 2, step: 34
	action: tensor([[ 0.0356, -0.2000, -0.0186,  0.0412, -0.0532, -0.1135, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[0.2418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1617675261087097, distance: 1.0477048013349828 entropy -7.659528953006338
epoch: 2, step: 35
	action: tensor([[ 0.0357, -0.2282, -0.0517,  0.0949, -0.0527, -0.0514,  0.0569]],
       dtype=torch.float64)
	q_value: tensor([[0.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18036574193875443, distance: 1.0360166721633473 entropy -7.654832319902952
epoch: 2, step: 36
	action: tensor([[ 0.0360, -0.2508, -0.0474,  0.0222, -0.0527,  0.0326,  0.0333]],
       dtype=torch.float64)
	q_value: tensor([[0.2298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14315941258941234, distance: 1.0592700872486185 entropy -7.642609259216722
epoch: 2, step: 37
	action: tensor([[ 0.0359, -0.2743, -0.0489,  0.0595, -0.0521,  0.0494, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[0.2304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14708299347795717, distance: 1.056842038951945 entropy -7.641628799283462
epoch: 2, step: 38
	action: tensor([[ 0.0348, -0.2332, -0.0563,  0.0269, -0.0512, -0.1420,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[0.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11855719241807627, distance: 1.0743697268118377 entropy -7.642494067455778
epoch: 2, step: 39
	action: tensor([[ 0.0363, -0.2222, -0.0414, -0.0280, -0.0536, -0.1470,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[0.2345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09933252724000652, distance: 1.0860227741362902 entropy -7.652561908095213
epoch: 2, step: 40
	action: tensor([[ 0.0366, -0.2131, -0.0533,  0.0611, -0.0538, -0.1164, -0.0488]],
       dtype=torch.float64)
	q_value: tensor([[0.2337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.161620733307675, distance: 1.0477965353077092 entropy -7.656337873523507
epoch: 2, step: 41
	action: tensor([[ 0.0353, -0.2498, -0.0530,  0.0242, -0.0523,  0.0327,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[0.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14426612879897482, distance: 1.0585857766637474 entropy -7.65623613445978
epoch: 2, step: 42
	action: tensor([[ 0.0355, -0.2132, -0.0436,  0.0468, -0.0518, -0.0771, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[0.2318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16173702101755905, distance: 1.0477238652824485 entropy -7.64463860729188
epoch: 2, step: 43
	action: tensor([[ 0.0356, -0.2163, -0.0441,  0.0474, -0.0525, -0.0078, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[0.2326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1760390695420525, distance: 1.0387475272442745 entropy -7.651580074219482
epoch: 2, step: 44
	action: tensor([[ 0.0351, -0.2499, -0.0509,  0.0222, -0.0516, -0.0852, -0.0433]],
       dtype=torch.float64)
	q_value: tensor([[0.2315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1143563085980357, distance: 1.0769268624721364 entropy -7.649744465082906
epoch: 2, step: 45
	action: tensor([[ 0.0354, -0.2408, -0.0414,  0.0172, -0.0527, -0.0120, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[0.2365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13759943640225147, distance: 1.0627012932742024 entropy -7.652911166604193
epoch: 2, step: 46
	action: tensor([[ 0.0351, -0.2365, -0.0368,  0.0187, -0.0519, -0.0514,  0.0703]],
       dtype=torch.float64)
	q_value: tensor([[0.2340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13211958404769497, distance: 1.0660722469186394 entropy -7.650796252450557
epoch: 2, step: 47
	action: tensor([[ 0.0365, -0.2333, -0.0516,  0.0629, -0.0534,  0.0464, -0.0409]],
       dtype=torch.float64)
	q_value: tensor([[0.2295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18379139099117925, distance: 1.0338493972203553 entropy -7.6434787273205425
epoch: 2, step: 48
	action: tensor([[ 0.0347, -0.2357, -0.0450,  0.1088, -0.0508, -0.0003,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[0.2318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19311000542483792, distance: 1.02793075107862 entropy -7.646072804279063
epoch: 2, step: 49
	action: tensor([[ 0.0353, -0.2719, -0.0316,  0.0472, -0.0518,  0.1428, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[0.2296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16799836514160127, distance: 1.043803582533323 entropy -7.642633186293549
epoch: 2, step: 50
	action: tensor([[ 0.0349, -0.2881, -0.0247,  0.0125, -0.0497, -0.1138, -0.0810]],
       dtype=torch.float64)
	q_value: tensor([[0.2315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06725987621068286, distance: 1.1051901939873647 entropy -7.638789575715331
epoch: 2, step: 51
	action: tensor([[ 0.0352, -0.2449, -0.0319,  0.0507, -0.0529,  0.0923,  0.1100]],
       dtype=torch.float64)
	q_value: tensor([[0.2400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17902140271541167, distance: 1.0368659456123355 entropy -7.655118687204409
epoch: 2, step: 52
	action: tensor([[ 0.0364, -0.2460, -0.0423,  0.0665, -0.0517,  0.0686,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[0.2247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18102499795938853, distance: 1.035599938957986 entropy -7.6346110311306825
epoch: 2, step: 53
	action: tensor([[ 0.0350, -0.2291, -0.0275,  0.0455, -0.0510, -0.1527, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[0.2297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12933556951224168, distance: 1.067780768165913 entropy -7.640422948337537
epoch: 2, step: 54
	action: tensor([[ 0.0356, -0.2578, -0.0394,  0.0817, -0.0531, -0.1955, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[0.2365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1149548767567442, distance: 1.0765628768823912 entropy -7.656070865120257
epoch: 2, step: 55
	action: tensor([[ 0.0359, -0.2135, -0.0125,  0.0006, -0.0537, -0.0852, -0.0217]],
       dtype=torch.float64)
	q_value: tensor([[0.2385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13492644426779477, distance: 1.064346929092296 entropy -7.654400863451312
epoch: 2, step: 56
	action: tensor([[ 0.0360, -0.2009, -0.0542,  0.0317, -0.0530, -0.0669, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[0.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16722405915830652, distance: 1.0442891797778358 entropy -7.6527245621100395
epoch: 2, step: 57
	action: tensor([[ 0.0358, -0.2663, -0.0275,  0.0009, -0.0525,  0.0688,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[0.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12764792146817094, distance: 1.0688151309262506 entropy -7.651278050231041
epoch: 2, step: 58
	action: tensor([[ 0.0356, -0.2293, -0.0426,  0.0299, -0.0516, -0.1887,  0.0751]],
       dtype=torch.float64)
	q_value: tensor([[0.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11382601313890006, distance: 1.0772492291126612 entropy -7.641775924917929
epoch: 2, step: 59
	action: tensor([[ 0.0371, -0.2452, -0.0434,  0.0781, -0.0544, -0.1915, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[0.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12647746126523562, distance: 1.0695319207748908 entropy -7.652734500804009
epoch: 2, step: 60
	action: tensor([[ 0.0360, -0.2339, -0.0295,  0.0594, -0.0536,  0.1038, -0.0345]],
       dtype=torch.float64)
	q_value: tensor([[0.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1969641120484621, distance: 1.0254728591098337 entropy -7.654438193670581
epoch: 2, step: 61
	action: tensor([[ 0.0348, -0.2708, -0.0440,  0.0438, -0.0501,  0.0031, -0.0502]],
       dtype=torch.float64)
	q_value: tensor([[0.2298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12878660405946252, distance: 1.068117340077963 entropy -7.641273825251324
epoch: 2, step: 62
	action: tensor([[ 0.0347, -0.2116, -0.0546,  0.0817, -0.0516, -0.0428, -0.0419]],
       dtype=torch.float64)
	q_value: tensor([[0.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.188433189555117, distance: 1.0309054421090933 entropy -7.648657420747696
epoch: 2, step: 63
	action: tensor([[ 0.0349, -0.2288, -0.0449,  0.0594, -0.0515, -0.0297, -0.0376]],
       dtype=torch.float64)
	q_value: tensor([[0.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1656690899421518, distance: 1.045263679725754 entropy -7.65204206185512
epoch: 2, step: 64
	action: tensor([[ 0.0350, -0.2494, -0.0360,  0.0220, -0.0518, -0.0264, -0.0370]],
       dtype=torch.float64)
	q_value: tensor([[0.2332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12861137431805536, distance: 1.0682247514858931 entropy -7.650638018241217
epoch: 2, step: 65
	action: tensor([[ 0.0352, -0.2603, -0.0401,  0.0198, -0.0522, -0.1801, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[0.2342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08220339518463804, distance: 1.0963012686105678 entropy -7.649703708926496
epoch: 2, step: 66
	action: tensor([[ 0.0362, -0.2593, -0.0335, -0.0338, -0.0539, -0.0397,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[0.2382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08724503413674667, distance: 1.0932860218173526 entropy -7.655805408415246
epoch: 2, step: 67
	action: tensor([[ 0.0365, -0.2256, -0.0616,  0.0448, -0.0533, -0.1009, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[0.2328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14562410140031845, distance: 1.0577455030167053 entropy -7.646649804345641
epoch: 2, step: 68
	action: tensor([[ 0.0355, -0.2393, -0.0441, -0.0556, -0.0525, -0.1774,  0.0789]],
       dtype=torch.float64)
	q_value: tensor([[0.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0622273594062418, distance: 1.1081676611743085 entropy -7.653868457624912
epoch: 2, step: 69
	action: tensor([[ 0.0377, -0.2376, -0.0367, -0.0404, -0.0545,  0.0087, -0.0449]],
       dtype=torch.float64)
	q_value: tensor([[0.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11663401856211564, distance: 1.0755411436905442 entropy -7.65483729671207
epoch: 2, step: 70
	action: tensor([[ 0.0355, -0.1967, -0.0337, -0.0258, -0.0519,  0.0367,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.2343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16503172109015685, distance: 1.0456628566688349 entropy -7.651578188874268
epoch: 2, step: 71
	action: tensor([[ 0.0361, -0.2502, -0.0451,  0.0753, -0.0517,  0.0134,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[0.2278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16773345244856974, distance: 1.0439697449535306 entropy -7.648550459855345
epoch: 2, step: 72
	action: tensor([[ 0.0352, -0.2379, -0.0468,  0.0173, -0.0518, -0.0344, -0.0545]],
       dtype=torch.float64)
	q_value: tensor([[0.2311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1343849193878104, distance: 1.0646800106940524 entropy -7.643450492350274
epoch: 2, step: 73
	action: tensor([[ 0.0351, -0.2416, -0.0311,  0.0443, -0.0520, -0.0431,  0.0526]],
       dtype=torch.float64)
	q_value: tensor([[0.2348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14364596133544882, distance: 1.0589692963399309 entropy -7.652730511119614
epoch: 2, step: 74
	action: tensor([[ 0.0361, -0.2857, -0.0302,  0.0089, -0.0531,  0.0186, -0.0429]],
       dtype=torch.float64)
	q_value: tensor([[0.2299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10187448978330393, distance: 1.084489144973595 entropy -7.643143065612355
epoch: 2, step: 75
	action: tensor([[ 0.0350, -0.2254, -0.0380,  0.0418, -0.0519, -0.0580,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[0.2362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1526217225427432, distance: 1.0534049543840998 entropy -7.647274006154324
epoch: 2, step: 76
	action: tensor([[ 0.0358, -0.2545, -0.0400,  0.0670, -0.0528, -0.0341,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[0.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14750369053446277, distance: 1.0565813657735696 entropy -7.648065542510013
epoch: 2, step: 77
	action: tensor([[ 0.0357, -0.2240, -0.0440,  0.0292, -0.0528,  0.0154, -0.0756]],
       dtype=torch.float64)
	q_value: tensor([[0.2312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16562302287032316, distance: 1.0452925361254015 entropy -7.642620158522748
epoch: 2, step: 78
	action: tensor([[ 0.0346, -0.2496, -0.0366,  0.0673, -0.0510,  0.1063, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[0.2335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18707042699845422, distance: 1.0317706143001104 entropy -7.651984411998358
epoch: 2, step: 79
	action: tensor([[ 0.0349, -0.2088, -0.0435,  0.0232, -0.0503,  0.0418,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[0.2299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18118116420995556, distance: 1.0355011975619997 entropy -7.639145970311814
epoch: 2, step: 80
	action: tensor([[ 0.0355, -0.2542, -0.0435,  0.0135, -0.0514, -0.2354, -0.0576]],
       dtype=torch.float64)
	q_value: tensor([[0.2286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07305522489190008, distance: 1.1017514320493076 entropy -7.646871459683333
epoch: 2, step: 81
	action: tensor([[ 0.0363, -0.2267, -0.0471,  0.0807, -0.0538, -0.1280, -0.0275]],
       dtype=torch.float64)
	q_value: tensor([[0.2416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15721018770399564, distance: 1.05054904393696 entropy -7.663818320397099
epoch: 2, step: 82
	action: tensor([[ 0.0354, -0.2926, -0.0465,  0.0301, -0.0526,  0.0466, -0.0616]],
       dtype=torch.float64)
	q_value: tensor([[0.2359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11380640705420408, distance: 1.0772611458034682 entropy -7.653655674756487
epoch: 2, step: 83
	action: tensor([[ 0.0346, -0.2436, -0.0394,  0.0631, -0.0510, -0.0800, -0.0686]],
       dtype=torch.float64)
	q_value: tensor([[0.2371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1426368478092046, distance: 1.059593048647268 entropy -7.64589440081525
epoch: 2, step: 84
	action: tensor([[ 0.0349, -0.2688, -0.0433,  0.1275, -0.0521,  0.0247,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[0.2365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1819903505449173, distance: 1.034989411255496 entropy -7.654037337394144
epoch: 2, step: 85
	action: tensor([[ 0.0352, -0.2753, -0.0443,  0.0509, -0.0517,  0.0123, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[0.2297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.131479853317234, distance: 1.0664650853399553 entropy -7.639485024138607
epoch: 2, step: 86
	action: tensor([[ 0.0351, -0.2063, -0.0480,  0.0703, -0.0519, -0.1211, -0.0604]],
       dtype=torch.float64)
	q_value: tensor([[0.2337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16955510155410813, distance: 1.0428266085840967 entropy -7.644367288492194
epoch: 2, step: 87
	action: tensor([[ 0.0352, -0.2280, -0.0613,  0.0332, -0.0521, -0.0967,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13706153771801444, distance: 1.0630326569962076 entropy -7.657788980746729
epoch: 2, step: 88
	action: tensor([[ 3.6004e-02, -2.5936e-01, -5.4731e-02,  3.7493e-02, -5.3106e-02,
          3.6190e-05,  6.9463e-02]], dtype=torch.float64)
	q_value: tensor([[0.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13554618479370162, distance: 1.0639656105895825 entropy -7.650835719053257
epoch: 2, step: 89
	action: tensor([[ 0.0362, -0.2638, -0.0273,  0.0667, -0.0527, -0.0608,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[0.2300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1333585754417852, distance: 1.065311009502253 entropy -7.639718239843677
epoch: 2, step: 90
	action: tensor([[ 0.0359, -0.2380, -0.0300,  0.0585, -0.0533, -0.0055, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[0.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16464997937573156, distance: 1.0459018642618931 entropy -7.6432230872352775
epoch: 2, step: 91
	action: tensor([[ 0.0351, -0.2119, -0.0291,  0.0752, -0.0519, -0.0880, -0.0280]],
       dtype=torch.float64)
	q_value: tensor([[0.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1752050793827593, distance: 1.0392730898541644 entropy -7.646943553936603
epoch: 2, step: 92
	action: tensor([[ 0.0353, -0.2596, -0.0485,  0.0515, -0.0524,  0.0838,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[0.2330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16387052542924996, distance: 1.0463897090177912 entropy -7.6525822937313865
epoch: 2, step: 93
	action: tensor([[ 0.0352, -0.2323, -0.0368,  0.0367, -0.0509, -0.0268,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[0.2308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15152626927807555, distance: 1.054085632200627 entropy -7.639897225688303
epoch: 2, step: 94
	action: tensor([[ 0.0356, -0.2282, -0.0411, -0.0108, -0.0525, -0.0655, -0.0727]],
       dtype=torch.float64)
	q_value: tensor([[0.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12097745026805506, distance: 1.0728937154730855 entropy -7.646610874035732
epoch: 2, step: 95
	action: tensor([[ 0.0354, -0.2206, -0.0535,  0.0315, -0.0522,  0.0388,  0.0487]],
       dtype=torch.float64)
	q_value: tensor([[0.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17498368166123712, distance: 1.0394125652859636 entropy -7.655624078512394
epoch: 2, step: 96
	action: tensor([[ 0.0360, -0.2266, -0.0321,  0.0852, -0.0519, -0.0389,  0.0247]],
       dtype=torch.float64)
	q_value: tensor([[0.2276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18065442705053447, distance: 1.0358342072860962 entropy -7.641428898489212
epoch: 2, step: 97
	action: tensor([[ 3.5596e-02, -2.2499e-01, -3.2798e-02, -1.3668e-05, -5.2496e-02,
         -7.9910e-02, -1.0078e-02]], dtype=torch.float64)
	q_value: tensor([[0.2299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1258922271986238, distance: 1.0698901380415853 entropy -7.643866139453566
epoch: 2, step: 98
	action: tensor([[ 0.0360, -0.2344, -0.0546, -0.0077, -0.0530, -0.1128, -0.0321]],
       dtype=torch.float64)
	q_value: tensor([[0.2327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10699278822592218, distance: 1.0813945496831792 entropy -7.652050280680015
epoch: 2, step: 99
	action: tensor([[ 0.0359, -0.2484, -0.0520,  0.0123, -0.0530,  0.0322,  0.0742]],
       dtype=torch.float64)
	q_value: tensor([[0.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13965725116583372, distance: 1.0614326551298827 entropy -7.655574935383795
epoch: 2, step: 100
	action: tensor([[ 0.0364, -0.2266, -0.0386,  0.0379, -0.0524,  0.1006, -0.0202]],
       dtype=torch.float64)
	q_value: tensor([[0.2289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19060376703188942, distance: 1.0295259140818533 entropy -7.639430432396115
epoch: 2, step: 101
	action: tensor([[ 0.0350, -0.2465, -0.0419,  0.0396, -0.0503,  0.1425, -0.0772]],
       dtype=torch.float64)
	q_value: tensor([[0.2294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1844359262371722, distance: 1.0334411168066566 entropy -7.6431525156652524
epoch: 2, step: 102
	action: tensor([[ 0.0346, -0.2221, -0.0419,  0.0869, -0.0491, -0.0252, -0.0584]],
       dtype=torch.float64)
	q_value: tensor([[0.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18692455524342988, distance: 1.0318631803994391 entropy -7.64544461075063
epoch: 2, step: 103
	action: tensor([[ 0.0346, -0.2937, -0.0506,  0.1071, -0.0513, -0.1073,  0.0946]],
       dtype=torch.float64)
	q_value: tensor([[0.2332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11587843874078374, distance: 1.0760010230053625 entropy -7.651406715370686
epoch: 2, step: 104
	action: tensor([[ 0.0363, -0.2428, -0.0471, -0.0058, -0.0538,  0.0874,  0.0386]],
       dtype=torch.float64)
	q_value: tensor([[0.2327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14945330562177617, distance: 1.0553725006828216 entropy -7.641816589400633
epoch: 2, step: 105
	action: tensor([[ 0.0360, -0.2537, -0.0404,  0.0068, -0.0514,  0.0906,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[0.2294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14756848861903815, distance: 1.05654120973478 entropy -7.6416412806642615
epoch: 2, step: 106
	action: tensor([[ 0.0356, -0.2454, -0.0438,  0.0330, -0.0512,  0.0772,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[0.2305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16485632182772614, distance: 1.045772680519296 entropy -7.641296212449036
epoch: 2, step: 107
	action: tensor([[ 0.0356, -0.2918, -0.0408,  0.0179, -0.0513, -0.1569, -0.0777]],
       dtype=torch.float64)
	q_value: tensor([[0.2291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05813395072525063, distance: 1.1105836219067098 entropy -7.639963838047151
epoch: 2, step: 108
	action: tensor([[ 0.0355, -0.2440, -0.0732,  0.1186, -0.0532, -0.0194, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[0.2418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1857133753739595, distance: 1.0326314404931105 entropy -7.656725779375156
epoch: 2, step: 109
	action: tensor([[ 0.0347, -0.2403, -0.0254,  0.0331, -0.0512,  0.1465,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[0.2341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1878843897666641, distance: 1.0312539439628863 entropy -7.6482520581158004
epoch: 2, step: 110
	action: tensor([[ 0.0355, -0.2293, -0.0466,  0.0491, -0.0502, -0.0032, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[0.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1667976510311766, distance: 1.0445565004249275 entropy -7.638908109403725
epoch: 2, step: 111
	action: tensor([[ 0.0353, -0.2428, -0.0348,  0.0113, -0.0519, -0.0666, -0.0648]],
       dtype=torch.float64)
	q_value: tensor([[0.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11911843268434474, distance: 1.0740276311247536 entropy -7.647476810292232
epoch: 2, step: 112
	action: tensor([[ 0.0353, -0.2504, -0.0398,  0.0648, -0.0523,  0.0229, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[0.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16369377583774358, distance: 1.0465003014476617 entropy -7.653852583055316
epoch: 2, step: 113
	action: tensor([[ 0.0347, -0.2525, -0.0426,  0.0526, -0.0513, -0.0034, -0.0544]],
       dtype=torch.float64)
	q_value: tensor([[0.2329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14791026783038885, distance: 1.056329380385639 entropy -7.646492357996048
epoch: 2, step: 114
	action: tensor([[ 0.0347, -0.2127, -0.0360,  0.0411, -0.0515, -0.0550, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[0.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16359820471636777, distance: 1.0465600955441465 entropy -7.6497542623316575
epoch: 2, step: 115
	action: tensor([[ 0.0356, -0.2697, -0.0407,  0.0085, -0.0525,  0.0976,  0.0633]],
       dtype=torch.float64)
	q_value: tensor([[0.2311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13600664990235933, distance: 1.063682203720385 entropy -7.650015211261696
epoch: 2, step: 116
	action: tensor([[ 0.0361, -0.2554, -0.0529,  0.0189, -0.0516, -0.1045, -0.0504]],
       dtype=torch.float64)
	q_value: tensor([[0.2292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10432360523390716, distance: 1.0830094785658377 entropy -7.637028728351625
epoch: 2, step: 117
	action: tensor([[ 0.0354, -0.2171, -0.0467,  0.0639, -0.0528, -0.1301,  0.0686]],
       dtype=torch.float64)
	q_value: tensor([[0.2377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15530964684699633, distance: 1.0517329019994182 entropy -7.654437178643258
epoch: 2, step: 118
	action: tensor([[ 0.0365, -0.2527, -0.0421,  0.0473, -0.0537, -0.0558, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[0.2312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13380580506659545, distance: 1.0650360975768045 entropy -7.648168561376949
epoch: 2, step: 119
	action: tensor([[ 0.0351, -0.2118, -0.0489,  0.0917, -0.0524, -0.1084,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[0.2354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17901537215078367, distance: 1.0368697537966374 entropy -7.650026451703861
epoch: 2, step: 120
	action: tensor([[ 0.0356, -0.2402, -0.0571,  0.0301, -0.0527, -0.0073,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[0.2332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14616858084745232, distance: 1.0574084075572974 entropy -7.651165205700399
epoch: 2, step: 121
	action: tensor([[ 0.0359, -0.2409, -0.0537,  0.0843, -0.0524,  0.0275, -0.0596]],
       dtype=torch.float64)
	q_value: tensor([[0.2309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18325933885639323, distance: 1.0341863038404435 entropy -7.643833179393615
epoch: 2, step: 122
	action: tensor([[ 0.0344, -0.2418, -0.0318,  0.0329, -0.0507,  0.1212,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[0.2335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17893845787316376, distance: 1.036918322440694 entropy -7.647569714238732
epoch: 2, step: 123
	action: tensor([[ 0.0356, -0.2619, -0.0350, -0.0121, -0.0507, -0.0298, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[0.2278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09873984134368985, distance: 1.0863800450178747 entropy -7.6390899326576385
epoch: 2, step: 124
	action: tensor([[ 0.0357, -0.2718, -0.0441,  0.0370, -0.0527,  0.0070, -0.0616]],
       dtype=torch.float64)
	q_value: tensor([[0.2346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1260749697759006, distance: 1.0697782955966173 entropy -7.6487842419085
epoch: 2, step: 125
	action: tensor([[ 0.0346, -0.2588, -0.0550,  0.0799, -0.0515,  0.0476, -0.1045]],
       dtype=torch.float64)
	q_value: tensor([[0.2362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16948752386798926, distance: 1.0428690378745624 entropy -7.649410244975185
epoch: 2, step: 126
	action: tensor([[ 0.0340, -0.2332, -0.0226,  0.1084, -0.0501, -0.0695,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[0.2361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17856431411472018, distance: 1.0371545485408258 entropy -7.648878253918018
epoch: 2, step: 127
	action: tensor([[ 0.0355, -0.2796, -0.0478,  0.0484, -0.0526, -0.0215,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[0.2311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11791945639530499, distance: 1.0747583171911756 entropy -7.646064941593243
LOSS epoch 2 actor 0.06212552490853525 critic 4.607829799605113
epoch: 3, step: 0
	action: tensor([[-0.1065, -0.2626, -0.0650,  0.0481, -0.1517,  0.1097, -0.0592]],
       dtype=torch.float64)
	q_value: tensor([[0.4841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01362768912057788, distance: 1.136520122482039 entropy -5.434802015273648
epoch: 3, step: 1
	action: tensor([[-0.1078, -0.2615, -0.0663,  0.0528, -0.1460, -0.0447, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[0.4983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015914436803701637, distance: 1.1534141081892009 entropy -5.340105879343035
epoch: 3, step: 2
	action: tensor([[-0.1068, -0.1959, -0.0675,  0.0004, -0.1503, -0.0610, -0.0574]],
       dtype=torch.float64)
	q_value: tensor([[0.5034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011490099788211583, distance: 1.1377509450013106 entropy -5.483685175226211
epoch: 3, step: 3
	action: tensor([[-0.1055, -0.2321, -0.0679,  0.0753, -0.1512,  0.0808, -0.1265]],
       dtype=torch.float64)
	q_value: tensor([[0.4977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047296357323466154, distance: 1.116954818886905 entropy -5.517852698604623
epoch: 3, step: 4
	action: tensor([[-0.1070, -0.2547, -0.0679,  0.0195, -0.1470, -0.1043, -0.1210]],
       dtype=torch.float64)
	q_value: tensor([[0.4995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03677800841470624, distance: 1.1651976003907187 entropy -5.3451029838973385
epoch: 3, step: 5
	action: tensor([[-0.1068, -0.2700, -0.0683,  0.0118, -0.1515,  0.1271, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[0.5136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007600375402092352, distance: 1.148684745230569 entropy -5.5656673303758435
epoch: 3, step: 6
	action: tensor([[-0.1080, -0.2824, -0.0673, -0.0060, -0.1481, -0.2334, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[0.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09479304675509015, distance: 1.1973543835377025 entropy -5.3634176300921705
epoch: 3, step: 7
	action: tensor([[-0.1068, -0.3382, -0.0695,  0.0310, -0.1562,  0.0109, -0.0378]],
       dtype=torch.float64)
	q_value: tensor([[0.5167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08090254184628476, distance: 1.1897342448624701 entropy -5.925229168016871
epoch: 3, step: 8
	action: tensor([[-0.1091, -0.2449, -0.0689,  0.1100, -0.1498,  0.0183, -0.0198]],
       dtype=torch.float64)
	q_value: tensor([[0.5131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03769599797966805, distance: 1.122568468384841 entropy -5.409609791012387
epoch: 3, step: 9
	action: tensor([[-0.1068, -0.2934, -0.0695, -0.0123, -0.1485,  0.1978,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[0.4984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023404376281198624, distance: 1.1576581355322422 entropy -5.3872220082376625
epoch: 3, step: 10
	action: tensor([[-0.1091, -0.2569, -0.0699,  0.0225, -0.1488,  0.1288, -0.0232]],
       dtype=torch.float64)
	q_value: tensor([[0.4967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006632091585256239, distance: 1.140543243404685 entropy -5.335769315317598
epoch: 3, step: 11
	action: tensor([[-0.1077, -0.2473, -0.0635,  0.0199, -0.1437, -0.1752,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.4943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04324597194840307, distance: 1.1688265054993892 entropy -5.356391904646549
epoch: 3, step: 12
	action: tensor([[-0.1060, -0.2714, -0.0682,  0.0384, -0.1545,  0.0051, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[0.5066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01990833687795357, distance: 1.155679112928311 entropy -5.731296210926764
epoch: 3, step: 13
	action: tensor([[-0.1077, -0.2573, -0.0670,  0.0407, -0.1493, -0.0128,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[0.5106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0122862002119426, distance: 1.1513526144946926 entropy -5.407265192376813
epoch: 3, step: 14
	action: tensor([[-0.1070, -0.2651, -0.0682,  0.0080, -0.1522,  0.0993, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[0.4986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011782466100127786, distance: 1.1510661106599156 entropy -5.460872412406753
epoch: 3, step: 15
	action: tensor([[-0.1078, -0.2533, -0.0665,  0.0739, -0.1503,  0.3045, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[0.4962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07857190055661034, distance: 1.098468024158741 entropy -5.380727563496147
epoch: 3, step: 16
	action: tensor([[-0.1089, -0.3216, -0.0712,  0.0534, -0.1460,  0.2620, -0.0531]],
       dtype=torch.float64)
	q_value: tensor([[0.4878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0008466332695953316, distance: 1.143859731466407 entropy -5.229799040441391
epoch: 3, step: 17
	action: tensor([[-0.1100, -0.2584, -0.0667,  0.0813, -0.1473, -0.0508,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[0.5026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0024652187676683646, distance: 1.1457539152197682 entropy -5.2719228254626485
epoch: 3, step: 18
	action: tensor([[-0.1067, -0.2754, -0.0668,  0.0066, -0.1517, -0.0651, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[0.5014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.053674373082796434, distance: 1.1746538380066858 entropy -5.478808379507423
epoch: 3, step: 19
	action: tensor([[-0.1072, -0.2643, -0.0682,  0.0279, -0.1543, -0.1287, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[0.5050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04494579037792823, distance: 1.1697783348134327 entropy -5.5361108978364175
epoch: 3, step: 20
	action: tensor([[-0.1067, -0.2099, -0.0687,  0.0515, -0.1529, -0.0470, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[0.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027456779173612222, distance: 1.1285249074086379 entropy -5.618482173684152
epoch: 3, step: 21
	action: tensor([[-1.0590e-01, -2.3434e-01, -6.8294e-02,  9.4637e-02, -1.4906e-01,
         -3.3635e-02,  2.0475e-04]], dtype=torch.float64)
	q_value: tensor([[0.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032051064502301885, distance: 1.1258561812194103 entropy -5.478391087328951
epoch: 3, step: 22
	action: tensor([[-0.1063, -0.1588, -0.0689,  0.0598, -0.1485,  0.1739, -0.0536]],
       dtype=torch.float64)
	q_value: tensor([[0.4979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11676768767971746, distance: 1.0754597662786494 entropy -5.454210413172165
epoch: 3, step: 23
	action: tensor([[-0.1059, -0.2566, -0.0676,  0.1238, -0.1444, -0.2095, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[0.4793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0028729386489902975, distance: 1.1459868904652435 entropy -5.32540612536368
epoch: 3, step: 24
	action: tensor([[-0.1064, -0.2720, -0.0660,  0.0881, -0.1538,  0.0933, -0.0751]],
       dtype=torch.float64)
	q_value: tensor([[0.5157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023009053921836276, distance: 1.1311025013625866 entropy -5.706767176444567
epoch: 3, step: 25
	action: tensor([[-0.1078, -0.1920, -0.0701,  0.0698, -0.1503, -0.1649,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[0.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02984253374344703, distance: 1.1271398601411995 entropy -5.326436118225581
epoch: 3, step: 26
	action: tensor([[-0.1049, -0.2811, -0.0674,  0.0229, -0.1518, -0.0096, -0.0217]],
       dtype=torch.float64)
	q_value: tensor([[0.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.037865781752672856, distance: 1.1658086948554371 entropy -5.669377697043708
epoch: 3, step: 27
	action: tensor([[-0.1076, -0.3025, -0.0681,  0.0324, -0.1492, -0.0822,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[0.5038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06815429781069504, distance: 1.1826975288664927 entropy -5.445787420273078
epoch: 3, step: 28
	action: tensor([[-0.1077, -0.2351, -0.0682,  0.0720, -0.1559,  0.1008,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[0.5083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04486797408104626, distance: 1.1183774376378197 entropy -5.548309835966554
epoch: 3, step: 29
	action: tensor([[-0.1071, -0.2985, -0.0685,  0.0626, -0.1470, -0.0703, -0.0566]],
       dtype=torch.float64)
	q_value: tensor([[0.4910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04650134465567435, distance: 1.1706487038833246 entropy -5.336363243312741
epoch: 3, step: 30
	action: tensor([[-0.1077, -0.2675, -0.0677,  0.0748, -0.1509,  0.0134, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[0.5128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0017562965575825817, distance: 1.143338908447785 entropy -5.510933356221961
epoch: 3, step: 31
	action: tensor([[-0.1073, -0.2739, -0.0669,  0.0882, -0.1509,  0.1383, -0.0872]],
       dtype=torch.float64)
	q_value: tensor([[0.5007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030481227763756613, distance: 1.1267687781069853 entropy -5.405320565473359
epoch: 3, step: 32
	action: tensor([[-0.1081, -0.2620, -0.0713,  0.0414, -0.1455, -0.1665,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[0.5002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04387349112948025, distance: 1.1691779809946425 entropy -5.30769612869967
epoch: 3, step: 33
	action: tensor([[-0.1064, -0.2669, -0.0678,  0.0700, -0.1537, -0.0355, -0.0497]],
       dtype=torch.float64)
	q_value: tensor([[0.5080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008408832090258889, distance: 1.1491454812933528 entropy -5.692893722526747
epoch: 3, step: 34
	action: tensor([[-0.1071, -0.2158, -0.0664, -0.0057, -0.1485, -0.2003, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[0.5062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03130291216253078, distance: 1.1621168956959207 entropy -5.452181748995983
epoch: 3, step: 35
	action: tensor([[-0.1053, -0.2479, -0.0679, -0.0227, -0.1532, -0.1568, -0.0072]],
       dtype=torch.float64)
	q_value: tensor([[0.5075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0581931694087201, distance: 1.177169958141957 entropy -5.772761959132111
epoch: 3, step: 36
	action: tensor([[-0.1063, -0.2873, -0.0678,  0.0177, -0.1530, -0.0914, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[0.5076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.062556132305301, distance: 1.179594215813172 entropy -5.668219755480261
epoch: 3, step: 37
	action: tensor([[-0.1074, -0.2509, -0.0680, -0.0390, -0.1515,  0.0542, -0.0271]],
       dtype=torch.float64)
	q_value: tensor([[0.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03320477413008982, distance: 1.163187952447438 entropy -5.552761600746575
epoch: 3, step: 38
	action: tensor([[-0.1074, -0.2924, -0.0683,  0.0501, -0.1507, -0.1048,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[0.4968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05423959243326104, distance: 1.1749688537656964 entropy -5.453902078182872
epoch: 3, step: 39
	action: tensor([[-0.1074, -0.1886, -0.0681, -0.0330, -0.1540,  0.0484, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[0.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020928858799551975, distance: 1.1323060246907761 entropy -5.56011400671219
epoch: 3, step: 40
	action: tensor([[-0.1061, -0.1946, -0.0688,  0.0216, -0.1503,  0.1554,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[0.4865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06619157471832604, distance: 1.1058229203111016 entropy -5.4557017056877
epoch: 3, step: 41
	action: tensor([[-0.1067, -0.2798, -0.0682,  0.0675, -0.1466,  0.0379,  0.0387]],
       dtype=torch.float64)
	q_value: tensor([[0.4813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006190051030555033, distance: 1.1478805646212213 entropy -5.352474936033701
epoch: 3, step: 42
	action: tensor([[-0.1077, -0.2684, -0.0687,  0.0781, -0.1480,  0.1651,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[0.4976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03535286565106921, distance: 1.1239343191202675 entropy -5.3963182834752255
epoch: 3, step: 43
	action: tensor([[-0.1083, -0.2009, -0.0680,  0.0362, -0.1505,  0.0936, -0.0564]],
       dtype=torch.float64)
	q_value: tensor([[0.4907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05283399726017968, distance: 1.113703908720196 entropy -5.287305431134519
epoch: 3, step: 44
	action: tensor([[-0.1063, -0.2791, -0.0655,  0.0330, -0.1455,  0.0387, -0.0315]],
       dtype=torch.float64)
	q_value: tensor([[0.4890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022580416530967318, distance: 1.1571920168598495 entropy -5.382798995134684
epoch: 3, step: 45
	action: tensor([[-0.1077, -0.2091, -0.0659,  0.0789, -0.1493, -0.0393, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[0.5006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041999565574160735, distance: 1.120055508510155 entropy -5.40728288515644
epoch: 3, step: 46
	action: tensor([[-0.1058, -0.3053, -0.0678,  0.0895, -0.1488, -0.1932, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[0.4961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05872722287334087, distance: 1.1774669702625193 entropy -5.463905453894697
epoch: 3, step: 47
	action: tensor([[-0.1075, -0.2384, -0.0685,  0.0368, -0.1544, -0.2595,  0.0814]],
       dtype=torch.float64)
	q_value: tensor([[0.5206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.038488445228989976, distance: 1.166158353580045 entropy -5.684419276782997
epoch: 3, step: 48
	action: tensor([[-0.1057, -0.3311, -0.0687,  0.0561, -0.1558, -0.0783, -0.0619]],
       dtype=torch.float64)
	q_value: tensor([[0.5081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0777692587486547, distance: 1.1880086130574878 entropy -5.821386244110164
epoch: 3, step: 49
	action: tensor([[-0.1086, -0.2586, -0.0687,  0.0010, -0.1526, -0.0935,  0.0591]],
       dtype=torch.float64)
	q_value: tensor([[0.5193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048923564153366295, distance: 1.1720027055924915 entropy -5.5169488873607575
epoch: 3, step: 50
	action: tensor([[-0.1068, -0.2562, -0.0682,  0.0224, -0.1550, -0.0096, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[0.5003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018955277135040527, distance: 1.155139020927615 entropy -5.610521064635746
epoch: 3, step: 51
	action: tensor([[-0.1070, -0.2546, -0.0685,  0.0102, -0.1520, -0.0258, -0.0897]],
       dtype=torch.float64)
	q_value: tensor([[0.4994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02697894715433269, distance: 1.1596781209869615 entropy -5.459063378117777
epoch: 3, step: 52
	action: tensor([[-1.0699e-01, -1.4410e-01, -6.6879e-02,  7.9511e-02, -1.5037e-01,
          7.6362e-04,  4.4796e-05]], dtype=torch.float64)
	q_value: tensor([[0.5059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10268064866539695, distance: 1.0840023161164007 entropy -5.469197175914467
epoch: 3, step: 53
	action: tensor([[-0.1046, -0.2862, -0.0663,  0.0976, -0.1476, -0.1833,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.4832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035383269686722896, distance: 1.164413588222302 entropy -5.425790535867732
epoch: 3, step: 54
	action: tensor([[-0.1070, -0.2854, -0.0678,  0.0387, -0.1547, -0.0599, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[0.5128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04534741796964714, distance: 1.1700031168666172 entropy -5.64382921152063
epoch: 3, step: 55
	action: tensor([[-0.1075, -0.2341, -0.0684,  0.0409, -0.1511,  0.0575,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[0.5084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021401990369792734, distance: 1.1320324008228184 entropy -5.496369247736292
epoch: 3, step: 56
	action: tensor([[-0.1070, -0.2111, -0.0693,  0.1154, -0.1509, -0.0051, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06555750818759387, distance: 1.1061982897655296 entropy -5.402291373804458
epoch: 3, step: 57
	action: tensor([[-0.1060, -0.2478, -0.0635,  0.0455, -0.1486,  0.0066, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[0.4948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003930241628199216, distance: 1.1420932653782312 entropy -5.410256871468692
epoch: 3, step: 58
	action: tensor([[-0.1068, -0.2510, -0.0684,  0.0601, -0.1492,  0.0915,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[0.4970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025085807934747018, distance: 1.1298996901676261 entropy -5.434868148332328
epoch: 3, step: 59
	action: tensor([[-0.1074, -0.2229, -0.0687,  0.0261, -0.1495, -0.1313, -0.0727]],
       dtype=torch.float64)
	q_value: tensor([[0.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011244199191045468, distance: 1.1507598871166835 entropy -5.352226790292534
epoch: 3, step: 60
	action: tensor([[-0.1058, -0.2667, -0.0672,  0.0540, -0.1520, -0.0176, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[0.5083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012257702412113991, distance: 1.1513364079874517 entropy -5.625274589844108
epoch: 3, step: 61
	action: tensor([[-0.1071, -0.3011, -0.0670,  0.0291, -0.1502, -0.0848, -0.0863]],
       dtype=torch.float64)
	q_value: tensor([[0.5025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06852486131231483, distance: 1.1829026614650888 entropy -5.44309094947738
epoch: 3, step: 62
	action: tensor([[-0.1078, -0.2662, -0.0679,  0.0374, -0.1503, -0.1000,  0.0499]],
       dtype=torch.float64)
	q_value: tensor([[0.5163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.037752582245857313, distance: 1.16574511603327 entropy -5.544135453922637
epoch: 3, step: 63
	action: tensor([[-0.1068, -0.2543, -0.0684,  0.0774, -0.1543,  0.1113,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.5025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0352525321750804, distance: 1.123992768112073 entropy -5.583691253167472
epoch: 3, step: 64
	action: tensor([[-0.1076, -0.2186, -0.0653,  0.0727, -0.1460,  0.0168, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[0.4925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04203975654319625, distance: 1.1200320134329123 entropy -5.321905646235968
epoch: 3, step: 65
	action: tensor([[-0.1062, -0.2481, -0.0657,  0.0884, -0.1478, -0.0192,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[0.4937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020108945423097935, distance: 1.132780044672486 entropy -5.411031752826852
epoch: 3, step: 66
	action: tensor([[-0.1066, -0.3580, -0.0677,  0.0361, -0.1494, -0.0852,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[0.4982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11427716403312194, distance: 1.2079620997872411 entropy -5.443114423423282
epoch: 3, step: 67
	action: tensor([[-0.1090, -0.3071, -0.0686,  0.0195, -0.1554, -0.0385, -0.0802]],
       dtype=torch.float64)
	q_value: tensor([[0.5150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07185906687874088, distance: 1.184746777986771 entropy -5.542018001260169
epoch: 3, step: 68
	action: tensor([[-0.1082, -0.2640, -0.0681,  0.0138, -0.1507,  0.0300,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.5143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023590982976892327, distance: 1.1577636739282977 entropy -5.47034817261019
epoch: 3, step: 69
	action: tensor([[-0.1074, -0.3009, -0.0678,  0.0175, -0.1492,  0.0460, -0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.4968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04907851694169452, distance: 1.1720892697554746 entropy -5.43841934058714
epoch: 3, step: 70
	action: tensor([[-0.1083, -0.2379, -0.0682,  0.0385, -0.1492, -0.0701,  0.0981]],
       dtype=torch.float64)
	q_value: tensor([[0.5031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008512613516457712, distance: 1.149204612513192 entropy -5.410946449055464
epoch: 3, step: 71
	action: tensor([[-0.1064, -0.2625, -0.0682,  0.0070, -0.1532,  0.1218,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[0.4937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0046512942271674795, distance: 1.1470025074176124 entropy -5.589049760066419
epoch: 3, step: 72
	action: tensor([[-0.1079, -0.2548, -0.0702, -0.0025, -0.1474, -0.0385, -0.0777]],
       dtype=torch.float64)
	q_value: tensor([[0.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.036880772494813474, distance: 1.1652553453908987 entropy -5.369077430466141
epoch: 3, step: 73
	action: tensor([[-0.1069, -0.2773, -0.0675,  0.0168, -0.1514,  0.0391, -0.0470]],
       dtype=torch.float64)
	q_value: tensor([[0.5056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030126717153385663, distance: 1.1614540128379327 entropy -5.492029091550208
epoch: 3, step: 74
	action: tensor([[-0.1077, -0.2572, -0.0660,  0.0708, -0.1514, -0.1042,  0.0641]],
       dtype=torch.float64)
	q_value: tensor([[0.5018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013941928161057326, distance: 1.152293824513755 entropy -5.418248893035114
epoch: 3, step: 75
	action: tensor([[-0.1066, -0.2244, -0.0688,  0.0585, -0.1544, -0.0151,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[0.5010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024939799991493983, distance: 1.1299842966667453 entropy -5.558691527338946
epoch: 3, step: 76
	action: tensor([[-0.1063, -0.2599, -0.0670,  0.0761, -0.1519,  0.0217, -0.0340]],
       dtype=torch.float64)
	q_value: tensor([[0.4939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011953190428050986, distance: 1.1374844107283717 entropy -5.4599530539010885
epoch: 3, step: 77
	action: tensor([[-0.1072, -0.2571, -0.0686, -0.0009, -0.1486,  0.0646,  0.0595]],
       dtype=torch.float64)
	q_value: tensor([[0.5005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016969814751934376, distance: 1.154013062092968 entropy -5.392497520875471
epoch: 3, step: 78
	action: tensor([[-0.1075, -0.2387, -0.0682,  0.0160, -0.1524, -0.0969,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[0.4912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023981143075775924, distance: 1.1579843041216888 entropy -5.440772058597552
epoch: 3, step: 79
	action: tensor([[-0.1062, -0.2305, -0.0686,  0.0222, -0.1521, -0.0114,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[0.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0029428102345702145, distance: 1.1426592194110867 entropy -5.5921374052311155
epoch: 3, step: 80
	action: tensor([[-0.1064, -0.2648, -0.0694,  0.0030, -0.1508, -0.0203, -0.0088]],
       dtype=torch.float64)
	q_value: tensor([[0.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0376061281879867, distance: 1.1656628545532297 entropy -5.470705659037601
epoch: 3, step: 81
	action: tensor([[-0.1072, -0.2057, -0.0670,  0.0423, -0.1522, -0.2634,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[0.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008497799446249621, distance: 1.149196172132723 entropy -5.478812147482764
epoch: 3, step: 82
	action: tensor([[-0.1050, -0.2281, -0.0683,  0.0982, -0.1548, -0.0139,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[0.5100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043791808650819886, distance: 1.1190073088369255 entropy -6.047593548755061
epoch: 3, step: 83
	action: tensor([[-0.1064, -0.2403, -0.0701,  0.0184, -0.1513, -0.0142, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[0.4955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007945908144537661, distance: 1.148881685494872 entropy -5.425325762846133
epoch: 3, step: 84
	action: tensor([[-0.1066, -0.2760, -0.0690,  0.0395, -0.1500, -0.0367, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.4983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.032132926833087216, distance: 1.1625844499234652 entropy -5.464364381676937
epoch: 3, step: 85
	action: tensor([[-0.1072, -0.2288, -0.0677,  0.0305, -0.1531,  0.1432,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[0.5047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03891726436678977, distance: 1.1218559126939693 entropy -5.479189370001832
epoch: 3, step: 86
	action: tensor([[-0.1074, -0.2436, -0.0683,  0.0413, -0.1449,  0.1225, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[0.4870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027883226570350583, distance: 1.1282774586236677 entropy -5.333969370478336
epoch: 3, step: 87
	action: tensor([[-0.1074, -0.2988, -0.0694,  0.0320, -0.1503, -0.1060, -0.0827]],
       dtype=torch.float64)
	q_value: tensor([[0.4901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06904577045678129, distance: 1.183190960668964 entropy -5.346790748664259
epoch: 3, step: 88
	action: tensor([[-0.1077, -0.2232, -0.0681,  0.0344, -0.1530,  0.1137, -0.0209]],
       dtype=torch.float64)
	q_value: tensor([[0.5175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.038605268867817344, distance: 1.122037991499831 entropy -5.592104054937778
epoch: 3, step: 89
	action: tensor([[-0.1070, -0.2782, -0.0691,  0.0453, -0.1447, -0.0296, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[0.4892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0302308376449949, distance: 1.16151270858145 entropy -5.356540498613825
epoch: 3, step: 90
	action: tensor([[-0.1073, -0.1935, -0.0670,  0.0281, -0.1512,  0.0490,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[0.5036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04698310268344752, distance: 1.1171384344978768 entropy -5.47075088115132
epoch: 3, step: 91
	action: tensor([[-0.1060, -0.2816, -0.0690,  0.0258, -0.1472, -0.0629,  0.0312]],
       dtype=torch.float64)
	q_value: tensor([[0.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04817326871738481, distance: 1.1715834636110791 entropy -5.4201970543903775
epoch: 3, step: 92
	action: tensor([[-0.1073, -0.2988, -0.0684,  0.0346, -0.1529, -0.0012, -0.0722]],
       dtype=torch.float64)
	q_value: tensor([[0.5034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04797887410824164, distance: 1.1714748174182732 entropy -5.538891168618099
epoch: 3, step: 93
	action: tensor([[-0.1080, -0.2482, -0.0665,  0.0400, -0.1482, -0.0734, -0.0435]],
       dtype=torch.float64)
	q_value: tensor([[0.5097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016627548622802912, distance: 1.153818851401231 entropy -5.427966836537337
epoch: 3, step: 94
	action: tensor([[-0.1065, -0.2127, -0.0664, -0.0158, -0.1505, -0.1467, -0.0699]],
       dtype=torch.float64)
	q_value: tensor([[0.5048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02449383107101366, distance: 1.1582741582671185 entropy -5.522871952377261
epoch: 3, step: 95
	action: tensor([[-0.1056, -0.2398, -0.0667,  0.0369, -0.1522, -0.0343,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[0.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0012536980969015676, distance: 1.1450613604183828 entropy -5.643658257496612
epoch: 3, step: 96
	action: tensor([[-0.1065, -0.2843, -0.0687,  0.0398, -0.1498, -0.0823, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[0.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04752480883803778, distance: 1.1712210033037993 entropy -5.493536349825881
epoch: 3, step: 97
	action: tensor([[-0.1073, -0.2679, -0.0667,  0.0243, -0.1517,  0.1554, -0.0500]],
       dtype=torch.float64)
	q_value: tensor([[0.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006487378600945504, distance: 1.1406263170596678 entropy -5.543738071676084
epoch: 3, step: 98
	action: tensor([[-0.1082, -0.3252, -0.0705,  0.0027, -0.1465, -0.0454, -0.0847]],
       dtype=torch.float64)
	q_value: tensor([[0.4976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0966985994096552, distance: 1.198395963893512 entropy -5.339837408975805
epoch: 3, step: 99
	action: tensor([[-0.1084, -0.1829, -0.0672, -0.0184, -0.1529,  0.0898,  0.0931]],
       dtype=torch.float64)
	q_value: tensor([[0.5167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039829103709149494, distance: 1.1213235989453096 entropy -5.509556436737761
epoch: 3, step: 100
	action: tensor([[-0.1063, -0.2728, -0.0681,  0.0922, -0.1502, -0.1499, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[0.4774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02273936618104133, distance: 1.15728195018735 entropy -5.478469711529557
epoch: 3, step: 101
	action: tensor([[-0.1068, -0.2214, -0.0686, -0.0249, -0.1538,  0.0844, -0.0527]],
       dtype=torch.float64)
	q_value: tensor([[0.5118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0054864265709666515, distance: 1.141200756066457 entropy -5.620503946781609
epoch: 3, step: 102
	action: tensor([[-0.1069, -0.2547, -0.0671,  0.0249, -0.1482,  0.0131,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[0.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012004370963843769, distance: 1.1511923300664528 entropy -5.431021047115907
epoch: 3, step: 103
	action: tensor([[-0.1070, -0.2565, -0.0673,  0.0593, -0.1500,  0.0450, -0.0911]],
       dtype=torch.float64)
	q_value: tensor([[0.4965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010217971382303515, distance: 1.1384828041035664 entropy -5.448346646299988
epoch: 3, step: 104
	action: tensor([[-0.1073, -0.1785, -0.0676, -0.0030, -0.1481, -0.0873, -0.0276]],
       dtype=torch.float64)
	q_value: tensor([[0.5019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01931142780517281, distance: 1.1332409266732262 entropy -5.3753554221579085
epoch: 3, step: 105
	action: tensor([[-0.1049, -0.2079, -0.0681,  0.0045, -0.1502, -0.2088, -0.1187]],
       dtype=torch.float64)
	q_value: tensor([[0.4947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017965836002266977, distance: 1.1545780445624196 entropy -5.577434548512692
epoch: 3, step: 106
	action: tensor([[-0.1056, -0.2623, -0.0678, -0.0095, -0.1523,  0.1408, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[0.5158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007602615050783745, distance: 1.1486860218521942 entropy -5.895411650130252
epoch: 3, step: 107
	action: tensor([[-0.1080, -0.3043, -0.0672,  0.0401, -0.1490,  0.0596, -0.0556]],
       dtype=torch.float64)
	q_value: tensor([[0.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.037905887321580956, distance: 1.1658312194294673 entropy -5.373235308810516
epoch: 3, step: 108
	action: tensor([[-0.1083, -0.2169, -0.0684,  0.0832, -0.1492, -0.1302, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[0.5055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020796772550292664, distance: 1.1323824016799022 entropy -5.38759040502756
epoch: 3, step: 109
	action: tensor([[-0.1057, -0.2184, -0.0685, -0.0018, -0.1508,  0.0871, -0.0741]],
       dtype=torch.float64)
	q_value: tensor([[0.5047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021257121707850435, distance: 1.1321161890324052 entropy -5.598304255391623
epoch: 3, step: 110
	action: tensor([[-0.1067, -0.2983, -0.0671, -0.0102, -0.1453, -0.0499,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[0.4931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07927831521887696, distance: 1.1888400270933486 entropy -5.410180709892801
epoch: 3, step: 111
	action: tensor([[-0.1079, -0.2650, -0.0687,  0.0341, -0.1509,  0.1413,  0.0397]],
       dtype=torch.float64)
	q_value: tensor([[0.5062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009952959973978048, distance: 1.1386352067183254 entropy -5.527089139482561
epoch: 3, step: 112
	action: tensor([[-0.1081, -0.3084, -0.0692,  0.0017, -0.1468,  0.1011,  0.0534]],
       dtype=torch.float64)
	q_value: tensor([[0.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.052423374621433094, distance: 1.1739563138892215 entropy -5.331448988398271
epoch: 3, step: 113
	action: tensor([[-0.1089, -0.2186, -0.0688, -0.0202, -0.1510, -0.0745,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[0.4983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022160541047173465, distance: 1.1569544187032605 entropy -5.405136410395004
epoch: 3, step: 114
	action: tensor([[-0.1058, -0.2651, -0.0677,  0.0486, -0.1510,  0.0730,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[0.4966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004640468211071558, distance: 1.1416860199848173 entropy -5.5776724075737985
epoch: 3, step: 115
	action: tensor([[-0.1076, -0.2415, -0.0669,  0.0502, -0.1487, -0.0991, -0.0557]],
       dtype=torch.float64)
	q_value: tensor([[0.4950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009990688681721593, distance: 1.150046440795681 entropy -5.37420013213526
epoch: 3, step: 116
	action: tensor([[-0.1063, -0.2305, -0.0688,  0.1266, -0.1505,  0.0355, -0.0387]],
       dtype=torch.float64)
	q_value: tensor([[0.5072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06435472362797534, distance: 1.1069099923901635 entropy -5.559445962468534
epoch: 3, step: 117
	action: tensor([[-0.1067, -0.2921, -0.0688,  0.0776, -0.1464, -0.1273, -0.0206]],
       dtype=torch.float64)
	q_value: tensor([[0.4969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04348177331328973, distance: 1.1689585909905547 entropy -5.351903700178342
epoch: 3, step: 118
	action: tensor([[-0.1072, -0.1883, -0.0680,  0.0783, -0.1531,  0.0032, -0.0238]],
       dtype=torch.float64)
	q_value: tensor([[0.5134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06707830761422962, distance: 1.1052977577463332 entropy -5.600365588540618
epoch: 3, step: 119
	action: tensor([[-0.1055, -0.2615, -0.0687,  0.0181, -0.1475, -0.1572, -0.0440]],
       dtype=torch.float64)
	q_value: tensor([[0.4912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05063353713055729, distance: 1.1729576259092453 entropy -5.417767947366421
epoch: 3, step: 120
	action: tensor([[-0.1065, -0.2143, -0.0677,  0.1040, -0.1531,  0.1509,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[0.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0897789708391118, distance: 1.091767408907949 entropy -5.6877010981470475
epoch: 3, step: 121
	action: tensor([[-0.1070, -0.2604, -0.0661,  0.1070, -0.1464, -0.0376,  0.0411]],
       dtype=torch.float64)
	q_value: tensor([[0.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014826291058221086, distance: 1.1358293846685268 entropy -5.278841998847226
epoch: 3, step: 122
	action: tensor([[-0.1069, -0.2788, -0.0675,  0.0097, -0.1527,  0.1373,  0.0482]],
       dtype=torch.float64)
	q_value: tensor([[0.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013944586370302714, distance: 1.1522953349730845 entropy -5.450723799593886
epoch: 3, step: 123
	action: tensor([[-0.1085, -0.2416, -0.0673,  0.0239, -0.1498, -0.0179, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[0.4931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009304098897675583, distance: 1.149655474628475 entropy -5.357629884181199
epoch: 3, step: 124
	action: tensor([[-0.1066, -0.2660, -0.0690,  0.0185, -0.1508,  0.0473,  0.0723]],
       dtype=torch.float64)
	q_value: tensor([[0.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017650037756803627, distance: 1.1543989413006654 entropy -5.479479966076221
epoch: 3, step: 125
	action: tensor([[-0.1077, -0.2726, -0.0674, -0.0130, -0.1479,  0.1096, -0.0643]],
       dtype=torch.float64)
	q_value: tensor([[0.4927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02708219943059298, distance: 1.1597364164352566 entropy -5.439795536873228
epoch: 3, step: 126
	action: tensor([[-0.1080, -0.2835, -0.0670,  0.0058, -0.1469,  0.0405, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[0.5012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04180797974235051, distance: 1.16802068259968 entropy -5.40129941632832
epoch: 3, step: 127
	action: tensor([[-0.1078, -0.2288, -0.0685,  0.0376, -0.1505, -0.0799, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[0.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002336112148077385, distance: 1.145680132521287 entropy -5.437875820406227
LOSS epoch 3 actor 0.19491171925184322 critic 0.0494540393541111
epoch: 4, step: 0
	action: tensor([[-0.0934, -0.2409, -0.0666, -0.0109, -0.2282,  0.0172,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[0.6748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00332089363147392, distance: 1.146242801860232 entropy -4.654434394841355
epoch: 4, step: 1
	action: tensor([[-0.0939, -0.3063, -0.0845,  0.0410, -0.2591,  0.1905,  0.0782]],
       dtype=torch.float64)
	q_value: tensor([[0.6747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003306514176116382, distance: 1.1462345879346778 entropy -4.648747888470221
epoch: 4, step: 2
	action: tensor([[-0.0951, -0.1750, -0.0738, -0.0181, -0.1975,  0.1485, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[0.6804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07060432497763525, distance: 1.1032070203377045 entropy -4.623306993737905
epoch: 4, step: 3
	action: tensor([[-0.0938, -0.2504, -0.0983,  0.0505, -0.2019, -0.0247, -0.0868]],
       dtype=torch.float64)
	q_value: tensor([[0.6635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011155884982722242, distance: 1.137943265318621 entropy -4.647564001987285
epoch: 4, step: 4
	action: tensor([[-0.0938, -0.3240, -0.0657,  0.0527, -0.2149,  0.1412,  0.0914]],
       dtype=torch.float64)
	q_value: tensor([[0.6921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01758660489529662, distance: 1.1543629623459204 entropy -4.641046442750127
epoch: 4, step: 5
	action: tensor([[-0.0949, -0.2831, -0.0490,  0.0226, -0.2298, -0.2656, -0.0311]],
       dtype=torch.float64)
	q_value: tensor([[0.6760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06473036801846055, distance: 1.180800460421435 entropy -4.628277040310103
epoch: 4, step: 6
	action: tensor([[-0.0949, -0.2587, -0.0743,  0.1204, -0.1950,  0.0794,  0.0548]],
       dtype=torch.float64)
	q_value: tensor([[0.7112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05813145111876239, distance: 1.1105850955877006 entropy -4.6426633758288975
epoch: 4, step: 7
	action: tensor([[-0.0943, -0.2989, -0.0744, -0.0587, -0.1786,  0.0848,  0.0700]],
       dtype=torch.float64)
	q_value: tensor([[0.6702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06484854363055903, distance: 1.1808659877838579 entropy -4.63633640956346
epoch: 4, step: 8
	action: tensor([[-0.0941, -0.2140, -0.0801,  0.0692, -0.2232, -0.0068,  0.0889]],
       dtype=torch.float64)
	q_value: tensor([[0.6709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05373303005176244, distance: 1.113175229461454 entropy -4.646001770826155
epoch: 4, step: 9
	action: tensor([[-0.0940, -0.2145, -0.0735,  0.0388, -0.2185,  0.0660, -0.0521]],
       dtype=torch.float64)
	q_value: tensor([[0.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05123711208676229, distance: 1.114642344159708 entropy -4.650157175864533
epoch: 4, step: 10
	action: tensor([[-0.0938, -0.3527, -0.0755,  0.0349, -0.2536,  0.0159,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[0.6735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07736009381112763, distance: 1.187783083515906 entropy -4.6447266769305955
epoch: 4, step: 11
	action: tensor([[-0.0949, -0.2951, -0.0830,  0.0064, -0.1995,  0.0588, -0.0404]],
       dtype=torch.float64)
	q_value: tensor([[0.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03486403590801568, distance: 1.1641215810474732 entropy -4.630002307934829
epoch: 4, step: 12
	action: tensor([[-0.0939, -0.2320, -0.0692,  0.0114, -0.2345,  0.0192,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[0.6859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014995904222412482, distance: 1.135731605004452 entropy -4.638790699079034
epoch: 4, step: 13
	action: tensor([[-0.0939, -0.2717, -0.0693,  0.0484, -0.2223,  0.1434, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[0.6738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023793051063843995, distance: 1.1306485774520416 entropy -4.648361753180043
epoch: 4, step: 14
	action: tensor([[-0.0943, -0.1186, -0.0717,  0.1188, -0.2017,  0.2796,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[0.6789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20445742054625793, distance: 1.0206771865602906 entropy -4.634713584781143
epoch: 4, step: 15
	action: tensor([[-0.0944, -0.2134, -0.0626,  0.0507, -0.1981, -0.0385, -0.0940]],
       dtype=torch.float64)
	q_value: tensor([[0.6372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03991897444661319, distance: 1.1212711205076924 entropy -4.631912463944817
epoch: 4, step: 16
	action: tensor([[-0.0938, -0.2220, -0.0717,  0.0539, -0.2325,  0.0432, -0.0439]],
       dtype=torch.float64)
	q_value: tensor([[0.6834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04874197527608981, distance: 1.1161070720862651 entropy -4.647625866130805
epoch: 4, step: 17
	action: tensor([[-0.0940, -0.2804, -0.0711,  0.0737, -0.2031,  0.1940, -0.0203]],
       dtype=torch.float64)
	q_value: tensor([[0.6773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04111076699494598, distance: 1.1205749617993628 entropy -4.6445453348258114
epoch: 4, step: 18
	action: tensor([[-0.0945, -0.2220, -0.0773,  0.0756, -0.2002, -0.0887,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[0.6757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036569918306518345, distance: 1.1232250862194377 entropy -4.6296093529497995
epoch: 4, step: 19
	action: tensor([[-0.0939, -0.2317, -0.0764,  0.1102, -0.2402, -0.2342, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[0.6784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029299494885627997, distance: 1.127455270337752 entropy -4.651412988928266
epoch: 4, step: 20
	action: tensor([[-0.0949, -0.1728, -0.0750,  0.0081, -0.2175,  0.0089, -0.0610]],
       dtype=torch.float64)
	q_value: tensor([[0.7073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05996639420577787, distance: 1.1095027503117993 entropy -4.644190422904664
epoch: 4, step: 21
	action: tensor([[-0.0936, -0.1805, -0.0705,  0.0741, -0.1880,  0.2258,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[0.6706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12895468264986742, distance: 1.0680143019948658 entropy -4.652146424160053
epoch: 4, step: 22
	action: tensor([[-0.0940, -0.2539, -0.0789,  0.0117, -0.2201, -0.1029, -0.1052]],
       dtype=torch.float64)
	q_value: tensor([[0.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022514905452087897, distance: 1.1571549488158732 entropy -4.636569004223635
epoch: 4, step: 23
	action: tensor([[-0.0943, -0.2837, -0.0692,  0.0890, -0.2568, -0.1393, -0.0648]],
       dtype=torch.float64)
	q_value: tensor([[0.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013480702583413828, distance: 1.1520317149082802 entropy -4.641523595996508
epoch: 4, step: 24
	action: tensor([[-0.0947, -0.1917, -0.0738,  0.0582, -0.2229,  0.3373, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[0.7088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1309378443119379, distance: 1.06679780273773 entropy -4.6372951487988106
epoch: 4, step: 25
	action: tensor([[-0.0953, -0.1865, -0.0713,  0.0732, -0.1767,  0.1075, -0.0694]],
       dtype=torch.float64)
	q_value: tensor([[0.6601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09862276747683985, distance: 1.0864506032143018 entropy -4.62454936824028
epoch: 4, step: 26
	action: tensor([[-0.0937, -0.2260, -0.0870,  0.0276, -0.1972, -0.1254,  0.0474]],
       dtype=torch.float64)
	q_value: tensor([[0.6634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005147671639549323, distance: 1.141395099552347 entropy -4.645469209393362
epoch: 4, step: 27
	action: tensor([[-0.0939, -0.1901, -0.0702,  0.0632, -0.1902,  0.0808,  0.1101]],
       dtype=torch.float64)
	q_value: tensor([[0.6787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08680730885644006, distance: 1.0935481412179493 entropy -4.653357728804671
epoch: 4, step: 28
	action: tensor([[-0.0939, -0.3118, -0.0681,  0.0756, -0.2082,  0.0892, -0.0292]],
       dtype=torch.float64)
	q_value: tensor([[0.6497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005912513024723065, distance: 1.1477222434103806 entropy -4.649515501466967
epoch: 4, step: 29
	action: tensor([[-0.0944, -0.1576, -0.0707,  0.0420, -0.1770,  0.1006, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[0.6873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10601250259279271, distance: 1.081987929426243 entropy -4.632189962567281
epoch: 4, step: 30
	action: tensor([[-0.0935, -0.2326, -0.0813,  0.0249, -0.2375,  0.0483,  0.1318]],
       dtype=torch.float64)
	q_value: tensor([[0.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026116021607074158, distance: 1.1293025372091807 entropy -4.650262144365661
epoch: 4, step: 31
	action: tensor([[-0.0943, -0.3305, -0.0777, -0.0031, -0.2210,  0.0248,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[0.6632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.076358258562232, distance: 1.1872306963740598 entropy -4.6488595719165735
epoch: 4, step: 32
	action: tensor([[-0.0944, -0.2433, -0.0887, -0.0635, -0.2100,  0.1011, -0.0739]],
       dtype=torch.float64)
	q_value: tensor([[0.6905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01717834890570291, distance: 1.1541313737667616 entropy -4.637400582220798
epoch: 4, step: 33
	action: tensor([[-0.0939, -0.2847, -0.0748,  0.0519, -0.2329,  0.0741, -0.1501]],
       dtype=torch.float64)
	q_value: tensor([[0.6818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0006482930623424865, distance: 1.1439732586414224 entropy -4.64378458136886
epoch: 4, step: 34
	action: tensor([[-0.0945, -0.2706, -0.0748,  0.0908, -0.2063, -0.0498, -0.0661]],
       dtype=torch.float64)
	q_value: tensor([[0.7011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010006981640558577, distance: 1.138604141623379 entropy -4.632458611507616
epoch: 4, step: 35
	action: tensor([[-0.0942, -0.3158, -0.0703,  0.0228, -0.2306,  0.0486, -0.0324]],
       dtype=torch.float64)
	q_value: tensor([[0.6942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0456778495878396, distance: 1.1701880197372263 entropy -4.640704837670799
epoch: 4, step: 36
	action: tensor([[-0.0944, -0.2754, -0.0736,  0.0058, -0.2248,  0.0507, -0.1040]],
       dtype=torch.float64)
	q_value: tensor([[0.6936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019673091070475213, distance: 1.1555458243106118 entropy -4.634843996767946
epoch: 4, step: 37
	action: tensor([[-0.0941, -0.2591, -0.0691,  0.0896, -0.2244,  0.1467, -0.0946]],
       dtype=torch.float64)
	q_value: tensor([[0.6934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.055565972677276365, distance: 1.1120965828318854 entropy -4.637323312428443
epoch: 4, step: 38
	action: tensor([[-0.0944, -0.2304, -0.0752,  0.0683, -0.2255, -0.0148, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[0.6847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03845198335073763, distance: 1.122127437235826 entropy -4.632834789764286
epoch: 4, step: 39
	action: tensor([[-0.0939, -0.3220, -0.0636,  0.0472, -0.2015, -0.1546,  0.1669]],
       dtype=torch.float64)
	q_value: tensor([[0.6827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07317458348277084, distance: 1.185473588155818 entropy -4.645606132818394
epoch: 4, step: 40
	action: tensor([[-0.0954, -0.3674, -0.0767,  0.1337, -0.1910,  0.0202, -0.0591]],
       dtype=torch.float64)
	q_value: tensor([[0.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03849547930556252, distance: 1.1661623029900274 entropy -4.6524177420980095
epoch: 4, step: 41
	action: tensor([[-0.0948, -0.2514, -0.0754, -0.0252, -0.2139,  0.2007, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[0.7069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014048478597929348, distance: 1.1362776751230887 entropy -4.626999837879408
epoch: 4, step: 42
	action: tensor([[-0.0943, -0.3114, -0.0831, -0.0244, -0.1907,  0.0829,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[0.6729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.059153208863233386, distance: 1.1777038273009715 entropy -4.638079147138834
epoch: 4, step: 43
	action: tensor([[-0.0942, -0.2806, -0.0731, -0.0258, -0.1969, -0.0153, -0.0480]],
       dtype=torch.float64)
	q_value: tensor([[0.6787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05132796588508293, distance: 1.1733452020357993 entropy -4.639780307903784
epoch: 4, step: 44
	action: tensor([[-0.0939, -0.2225, -0.0685,  0.0790, -0.1854,  0.0723, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[0.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0671655932565659, distance: 1.1052460498257877 entropy -4.642887511903389
epoch: 4, step: 45
	action: tensor([[-0.0938, -0.2092, -0.0691,  0.0354, -0.1653,  0.0931,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[0.6685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06103574753934404, distance: 1.1088715026272589 entropy -4.643942552114135
epoch: 4, step: 46
	action: tensor([[-0.0937, -0.2900, -0.0811,  0.0923, -0.1858,  0.1287, -0.0790]],
       dtype=torch.float64)
	q_value: tensor([[0.6552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028971764216279672, distance: 1.1276455816055806 entropy -4.648144889581888
epoch: 4, step: 47
	action: tensor([[-9.4278e-02, -2.6033e-01, -7.9933e-02,  1.0808e-01, -1.7013e-01,
          5.4952e-02, -5.4050e-05]], dtype=torch.float64)
	q_value: tensor([[0.6844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046517623137161124, distance: 1.117411221692109 entropy -4.631633781754145
epoch: 4, step: 48
	action: tensor([[-0.0940, -0.3353, -0.0699,  0.0262, -0.2135,  0.3370,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[0.6745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0030151769151953722, distance: 1.1426177514009657 entropy -4.639459325643222
epoch: 4, step: 49
	action: tensor([[-0.0963, -0.2316, -0.0815, -0.0152, -0.2523,  0.1954, -0.0312]],
       dtype=torch.float64)
	q_value: tensor([[0.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029542354854774522, distance: 1.1273142222599655 entropy -4.610503470153581
epoch: 4, step: 50
	action: tensor([[-0.0943, -0.2688, -0.0923,  0.1213, -0.2031,  0.0218, -0.0325]],
       dtype=torch.float64)
	q_value: tensor([[0.6776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03831022234127246, distance: 1.122210151802786 entropy -4.637064382838893
epoch: 4, step: 51
	action: tensor([[-0.0943, -0.2847, -0.0630,  0.0177, -0.1897,  0.3129,  0.0929]],
       dtype=torch.float64)
	q_value: tensor([[0.6871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.037710643498794494, distance: 1.1225599260424555 entropy -4.638170294092724
epoch: 4, step: 52
	action: tensor([[-0.0956, -0.1912, -0.0802, -0.0257, -0.2044, -0.1903, -0.0327]],
       dtype=torch.float64)
	q_value: tensor([[0.6569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00028584085742711096, distance: 1.1445077924857172 entropy -4.617329230838161
epoch: 4, step: 53
	action: tensor([[-0.0941, -0.2841, -0.0769,  0.0718, -0.1481,  0.2202,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[0.6876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0462761854831214, distance: 1.117552686324359 entropy -4.651030449182324
epoch: 4, step: 54
	action: tensor([[-0.0946, -0.2335, -0.0701, -0.0077, -0.1760, -0.1052, -0.0725]],
       dtype=torch.float64)
	q_value: tensor([[0.6614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017576513676867078, distance: 1.1543572385295768 entropy -4.627904471751755
epoch: 4, step: 55
	action: tensor([[-0.0937, -0.2666, -0.0656,  0.0610, -0.2098,  0.0634,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[0.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019825615031967003, distance: 1.1329438015588504 entropy -4.6483516100885565
epoch: 4, step: 56
	action: tensor([[-0.0941, -0.2207, -0.0756,  0.0897, -0.1865,  0.1473, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[0.6741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08794694885947463, distance: 1.0928655688080398 entropy -4.639956371122312
epoch: 4, step: 57
	action: tensor([[-0.0939, -0.2953, -0.0702,  0.0804, -0.1881, -0.0733, -0.0455]],
       dtype=torch.float64)
	q_value: tensor([[0.6666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020103024629055888, distance: 1.155789410012433 entropy -4.639019559453443
epoch: 4, step: 58
	action: tensor([[-0.0942, -0.2668, -0.0813,  0.0622, -0.1825, -0.0080, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.6952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005787793262165053, distance: 1.1410278343671294 entropy -4.640952437830365
epoch: 4, step: 59
	action: tensor([[-0.0939, -0.2967, -0.0658,  0.0136, -0.2103, -0.2059,  0.0614]],
       dtype=torch.float64)
	q_value: tensor([[0.6815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07448175495825415, distance: 1.1861953466169959 entropy -4.643511039302675
epoch: 4, step: 60
	action: tensor([[-0.0948, -0.2861, -0.0638,  0.1045, -0.2008, -0.1883,  0.2042]],
       dtype=torch.float64)
	q_value: tensor([[0.6950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01860334985410561, distance: 1.1549395224553223 entropy -4.649972673645036
epoch: 4, step: 61
	action: tensor([[-0.0957, -0.3291, -0.0822,  0.1390, -0.2069,  0.0803,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[0.6810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007670117939455712, distance: 1.1399471785524282 entropy -4.654320845933262
epoch: 4, step: 62
	action: tensor([[-0.0948, -0.2460, -0.0761,  0.0234, -0.1991, -0.1121, -0.1039]],
       dtype=torch.float64)
	q_value: tensor([[0.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013264890895291037, distance: 1.1519090509318493 entropy -4.627737991420046
epoch: 4, step: 63
	action: tensor([[-0.0939, -0.3716, -0.0985,  0.0376, -0.2026,  0.0014,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[0.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09572959168558337, distance: 1.197866414766965 entropy -4.644096370867648
epoch: 4, step: 64
	action: tensor([[-0.0949, -0.2249, -0.0812,  0.0799, -0.2379, -0.1598, -0.0361]],
       dtype=torch.float64)
	q_value: tensor([[0.6945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027773882862895727, distance: 1.1283409111711211 entropy -4.633396308157572
epoch: 4, step: 65
	action: tensor([[-0.0944, -0.3245, -0.0818, -0.0088, -0.2145,  0.0992,  0.1203]],
       dtype=torch.float64)
	q_value: tensor([[0.6977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0602878857136846, distance: 1.1783344988364626 entropy -4.645010462129998
epoch: 4, step: 66
	action: tensor([[-0.0948, -0.2539, -0.0797,  0.0781, -0.2170,  0.1189, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[0.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04702473227507087, distance: 1.1171140348650765 entropy -4.636741514231919
epoch: 4, step: 67
	action: tensor([[-0.0942, -0.2335, -0.0826,  0.0289, -0.2030, -0.0470, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[0.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011030179113943772, distance: 1.1380155929983597 entropy -4.636613803504481
epoch: 4, step: 68
	action: tensor([[-0.0938, -0.3065, -0.0747,  0.0688, -0.1902,  0.0657, -0.0090]],
       dtype=torch.float64)
	q_value: tensor([[0.6803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009474480480342384, distance: 1.1497525077493764 entropy -4.648024099616793
epoch: 4, step: 69
	action: tensor([[-0.0942, -0.2576, -0.0898,  0.0902, -0.2260, -0.3135, -0.0674]],
       dtype=torch.float64)
	q_value: tensor([[0.6827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011490003407357507, distance: 1.150899736841241 entropy -4.635634294092853
epoch: 4, step: 70
	action: tensor([[-0.0952, -0.2703, -0.0691,  0.0959, -0.1927, -0.0472, -0.0371]],
       dtype=torch.float64)
	q_value: tensor([[0.7250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012624011314118944, distance: 1.1370982054128997 entropy -4.637939328414585
epoch: 4, step: 71
	action: tensor([[-0.0941, -0.2232, -0.0830,  0.0820, -0.2229,  0.0829, -0.0634]],
       dtype=torch.float64)
	q_value: tensor([[0.6890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06767374422366623, distance: 1.10494497369077 entropy -4.642757694140654
epoch: 4, step: 72
	action: tensor([[-0.0940, -0.3099, -0.0705,  0.0175, -0.2187,  0.0905,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[0.6778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03513730933952419, distance: 1.1642752739430624 entropy -4.640822929720883
epoch: 4, step: 73
	action: tensor([[-0.0945, -0.2214, -0.0885, -0.0127, -0.1950, -0.1444, -0.1249]],
       dtype=torch.float64)
	q_value: tensor([[0.6811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012713739471558627, distance: 1.1515957258202856 entropy -4.6356063293830445
epoch: 4, step: 74
	action: tensor([[-0.0941, -0.2697, -0.0752, -0.0107, -0.1722,  0.1191, -0.1184]],
       dtype=torch.float64)
	q_value: tensor([[0.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007794887819722263, distance: 1.1487956139192854 entropy -4.644352989480538
epoch: 4, step: 75
	action: tensor([[-0.0939, -0.2350, -0.0723, -0.0245, -0.2274, -0.0490, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[0.6832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016050339094175525, distance: 1.153491253651231 entropy -4.6387118339903415
epoch: 4, step: 76
	action: tensor([[-0.0939, -0.2621, -0.0733, -0.0685, -0.2179, -0.1645, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[0.6820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0774113522504325, distance: 1.1878113392417675 entropy -4.647961950129109
epoch: 4, step: 77
	action: tensor([[-0.0942, -0.2806, -0.0682,  0.1156, -0.2279,  0.1952,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[0.6979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06122121249212931, distance: 1.1087619846361763 entropy -4.644769837699593
epoch: 4, step: 78
	action: tensor([[-0.0947, -0.3477, -0.0822,  0.0637, -0.1841,  0.2486, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[0.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008285864147336008, distance: 1.149075414292953 entropy -4.625512817556412
epoch: 4, step: 79
	action: tensor([[-0.0954, -0.3077, -0.0905,  0.0277, -0.1955, -0.1083, -0.0424]],
       dtype=torch.float64)
	q_value: tensor([[0.6819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06440080737024645, distance: 1.1806177026575673 entropy -4.617103511048934
epoch: 4, step: 80
	action: tensor([[-0.0942, -0.2837, -0.0580,  0.0299, -0.2192, -0.0539, -0.0722]],
       dtype=torch.float64)
	q_value: tensor([[0.7013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03217159145730997, distance: 1.162606225447115 entropy -4.640272200096417
epoch: 4, step: 81
	action: tensor([[-0.0942, -0.2844, -0.0806,  0.0089, -0.2068,  0.0023, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[0.6949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034058089150704074, distance: 1.163668186822633 entropy -4.640575082592768
epoch: 4, step: 82
	action: tensor([[-0.0940, -0.1835, -0.0945, -0.0563, -0.1795, -0.1033, -0.0416]],
       dtype=torch.float64)
	q_value: tensor([[0.6836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004592614188499988, distance: 1.1417134641445286 entropy -4.6430907131485055
epoch: 4, step: 83
	action: tensor([[-0.0935, -0.1908, -0.0878,  0.0629, -0.2222,  0.1294,  0.0467]],
       dtype=torch.float64)
	q_value: tensor([[0.6756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09285563809441721, distance: 1.089920688052694 entropy -4.655188699617001
epoch: 4, step: 84
	action: tensor([[-0.0939, -0.2828, -0.0941,  0.0173, -0.1949,  0.1203,  0.1370]],
       dtype=torch.float64)
	q_value: tensor([[0.6588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006384962042562892, distance: 1.1479917383146612 entropy -4.64141829002829
epoch: 4, step: 85
	action: tensor([[-0.0946, -0.2405, -0.0596,  0.0350, -0.1972,  0.1863, -0.1161]],
       dtype=torch.float64)
	q_value: tensor([[0.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0529936889616327, distance: 1.1136100198092027 entropy -4.637728894230464
epoch: 4, step: 86
	action: tensor([[-0.0943, -0.2044, -0.0711,  0.0668, -0.1888, -0.2942,  0.0737]],
       dtype=torch.float64)
	q_value: tensor([[0.6782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018023618253983242, distance: 1.1339847508268572 entropy -4.6361859636504175
epoch: 4, step: 87
	action: tensor([[-0.0947, -0.1846, -0.0740,  0.0210, -0.2410, -0.0326,  0.0855]],
       dtype=torch.float64)
	q_value: tensor([[0.6902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05023922094319877, distance: 1.1152283701412358 entropy -4.654071688204048
epoch: 4, step: 88
	action: tensor([[-0.0939, -0.2675, -0.0787,  0.0565, -0.1948,  0.0573, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[0.6634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0149171130348561, distance: 1.1357770280909414 entropy -4.657396070445121
epoch: 4, step: 89
	action: tensor([[-0.0939, -0.2486, -0.0744,  0.1240, -0.2255, -0.1477,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[0.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03141003351269722, distance: 1.1262289226037545 entropy -4.639878961134342
epoch: 4, step: 90
	action: tensor([[-0.0946, -0.1949, -0.0801,  0.0776, -0.2117,  0.2849,  0.0474]],
       dtype=torch.float64)
	q_value: tensor([[0.6919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12728773983042274, distance: 1.0690357573597087 entropy -4.646413878555827
epoch: 4, step: 91
	action: tensor([[-0.0946, -0.3098, -0.0728,  0.0727, -0.2131, -0.0353, -0.0443]],
       dtype=torch.float64)
	q_value: tensor([[0.6506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03024247227081922, distance: 1.1615192671735186 entropy -4.626140504240304
epoch: 4, step: 92
	action: tensor([[-0.0944, -0.3721, -0.0771,  0.0337, -0.2114,  0.1202, -0.0660]],
       dtype=torch.float64)
	q_value: tensor([[0.6975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07472720433234747, distance: 1.1863308232217227 entropy -4.637233767868571
epoch: 4, step: 93
	action: tensor([[-0.0948, -0.2453, -0.0828, -0.0084, -0.1947,  0.0521, -0.0442]],
       dtype=torch.float64)
	q_value: tensor([[0.7038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.000863356925849823, distance: 1.144838136192469 entropy -4.6229354268707885
epoch: 4, step: 94
	action: tensor([[-0.0936, -0.3459, -0.0663,  0.0078, -0.1919, -0.0819,  0.1512]],
       dtype=torch.float64)
	q_value: tensor([[0.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10282586568631946, distance: 1.2017390262119558 entropy -4.645939451531518
epoch: 4, step: 95
	action: tensor([[-0.0951, -0.2277, -0.0838,  0.0398, -0.2482,  0.0900, -0.0998]],
       dtype=torch.float64)
	q_value: tensor([[0.6821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041979603002759824, distance: 1.1200671781664413 entropy -4.65033123129008
epoch: 4, step: 96
	action: tensor([[-0.0943, -0.2708, -0.0760,  0.0297, -0.1870,  0.1836,  0.1071]],
       dtype=torch.float64)
	q_value: tensor([[0.6887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024329480790736602, distance: 1.1303378867467608 entropy -4.6378479253052065
epoch: 4, step: 97
	action: tensor([[-0.0946, -0.2602, -0.0667,  0.0962, -0.2005, -0.1010, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[0.6593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0131218636141478, distance: 1.1368114968329501 entropy -4.633535408527614
epoch: 4, step: 98
	action: tensor([[-0.0941, -0.2380, -0.0729,  0.0238, -0.2236, -0.1408,  0.0372]],
       dtype=torch.float64)
	q_value: tensor([[0.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008757674129883819, distance: 1.1493442278622954 entropy -4.644793256569601
epoch: 4, step: 99
	action: tensor([[-0.0942, -0.2194, -0.0730,  0.1011, -0.2098, -0.1037,  0.0529]],
       dtype=torch.float64)
	q_value: tensor([[0.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04967404570175926, distance: 1.115560140907561 entropy -4.650954846686977
epoch: 4, step: 100
	action: tensor([[-0.0941, -0.2275, -0.0771,  0.0313, -0.2283,  0.0763,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[0.6777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03806622149361871, distance: 1.1223525068887852 entropy -4.652170150760641
epoch: 4, step: 101
	action: tensor([[-0.0939, -0.2051, -0.0731,  0.1177, -0.2187, -0.0513, -0.0796]],
       dtype=torch.float64)
	q_value: tensor([[0.6682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0783268451405118, distance: 1.0986140841951573 entropy -4.644551354262133
epoch: 4, step: 102
	action: tensor([[-0.0941, -0.2293, -0.0737,  0.0583, -0.1924, -0.1744,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[0.6876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009132404437621977, distance: 1.139106962036187 entropy -4.64685470784571
epoch: 4, step: 103
	action: tensor([[-0.0942, -0.2842, -0.0708,  0.0906, -0.2249, -0.0483, -0.0462]],
       dtype=torch.float64)
	q_value: tensor([[0.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0004992444213989256, distance: 1.1446298720986223 entropy -4.652445055931213
epoch: 4, step: 104
	action: tensor([[-0.0944, -0.3231, -0.0865,  0.0580, -0.1979, -0.0020, -0.0847]],
       dtype=torch.float64)
	q_value: tensor([[0.6963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04366503614308104, distance: 1.1690612364126598 entropy -4.6390924570220164
epoch: 4, step: 105
	action: tensor([[-0.0943, -0.2122, -0.0812,  0.0578, -0.1973,  0.1617,  0.0191]],
       dtype=torch.float64)
	q_value: tensor([[0.7006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08050230393641677, distance: 1.0973167687520562 entropy -4.632961798912066
epoch: 4, step: 106
	action: tensor([[-0.0939, -0.1411, -0.0750,  0.0999, -0.2014,  0.2175, -0.1080]],
       dtype=torch.float64)
	q_value: tensor([[0.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16736093057819146, distance: 1.0442033588449446 entropy -4.639219631331263
epoch: 4, step: 107
	action: tensor([[-0.0942, -0.2186, -0.0751,  0.0690, -0.2117,  0.1232,  0.1072]],
       dtype=torch.float64)
	q_value: tensor([[0.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07326938155349805, distance: 1.101624153140933 entropy -4.639959161512223
epoch: 4, step: 108
	action: tensor([[-0.0942, -0.2049, -0.0727, -0.0254, -0.1890,  0.0566, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[0.6562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026420914546285146, distance: 1.1291257485308641 entropy -4.639602722511453
epoch: 4, step: 109
	action: tensor([[-0.0935, -0.2270, -0.0815, -0.0360, -0.2046, -0.0612,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[0.6639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01634570224219356, distance: 1.1536588998978385 entropy -4.652346780442524
epoch: 4, step: 110
	action: tensor([[-0.0937, -0.3198, -0.0761,  0.0281, -0.2123, -0.0685,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[0.6744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06646098681781942, distance: 1.1817597106810336 entropy -4.653580555840103
epoch: 4, step: 111
	action: tensor([[-0.0945, -0.2158, -0.0816, -0.0578, -0.1906, -0.0426, -0.1073]],
       dtype=torch.float64)
	q_value: tensor([[0.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015449233623104686, distance: 1.1531499947301933 entropy -4.643280390220341
epoch: 4, step: 112
	action: tensor([[-0.0937, -0.2720, -0.0752, -0.0089, -0.2313,  0.0426,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[0.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024915349852288715, distance: 1.1585124145099543 entropy -4.647877712442606
epoch: 4, step: 113
	action: tensor([[-0.0941, -0.3236, -0.0805,  0.0257, -0.2032,  0.0674,  0.0350]],
       dtype=torch.float64)
	q_value: tensor([[0.6804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047212947584834186, distance: 1.1710466467461607 entropy -4.643673190253687
epoch: 4, step: 114
	action: tensor([[-0.0944, -0.2617, -0.0577, -0.0324, -0.2231, -0.2309, -0.0758]],
       dtype=torch.float64)
	q_value: tensor([[0.6839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06896113847645724, distance: 1.183144125547969 entropy -4.635936051375971
epoch: 4, step: 115
	action: tensor([[-0.0946, -0.2275, -0.0809,  0.0663, -0.2228, -0.0211,  0.0531]],
       dtype=torch.float64)
	q_value: tensor([[0.7067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03839822680190741, distance: 1.1221588037688865 entropy -4.643139903918272
epoch: 4, step: 116
	action: tensor([[-0.0940, -0.2103, -0.0683,  0.0445, -0.2162, -0.1036,  0.0785]],
       dtype=torch.float64)
	q_value: tensor([[0.6736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029880701006349097, distance: 1.1271176883453888 entropy -4.648597455836126
epoch: 4, step: 117
	action: tensor([[-0.0940, -0.2735, -0.0824,  0.0631, -0.1942,  0.0851, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[0.6713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01817653797993901, distance: 1.133896451661947 entropy -4.656436726650759
epoch: 4, step: 118
	action: tensor([[-0.0940, -0.2098, -0.0852,  0.0534, -0.1974,  0.2460, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[0.6780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09731428932180253, distance: 1.0872388865715956 entropy -4.63772456131714
epoch: 4, step: 119
	action: tensor([[-0.0943, -0.2011, -0.0685,  0.0683, -0.2063,  0.0574,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[0.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07549931798102882, distance: 1.1002979687440713 entropy -4.633320837985545
epoch: 4, step: 120
	action: tensor([[-0.0938, -0.3502, -0.0566,  0.0852, -0.1874, -0.0202,  0.0538]],
       dtype=torch.float64)
	q_value: tensor([[0.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.054473328534634824, distance: 1.1750990980766343 entropy -4.647050071774485
epoch: 4, step: 121
	action: tensor([[-0.0947, -0.2092, -0.0711,  0.1291, -0.2155,  0.1423,  0.1225]],
       dtype=torch.float64)
	q_value: tensor([[0.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11400256626963956, distance: 1.0771419131675073 entropy -4.637638545846016
epoch: 4, step: 122
	action: tensor([[-0.0944, -0.3113, -0.0699,  0.0390, -0.1743,  0.1036, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[0.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02119365563675779, distance: 1.1564070942049718 entropy -4.633068281434954
epoch: 4, step: 123
	action: tensor([[-0.0941, -0.2010, -0.0626,  0.0411, -0.1943,  0.1928, -0.0429]],
       dtype=torch.float64)
	q_value: tensor([[0.6810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08974407452821098, distance: 1.0917883369510066 entropy -4.634997082274103
epoch: 4, step: 124
	action: tensor([[-0.0940, -0.2609, -0.0836, -0.0945, -0.1728,  0.0085,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[0.6614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0630669335719738, distance: 1.17987771416414 entropy -4.641236482805943
epoch: 4, step: 125
	action: tensor([[-0.0937, -0.2546, -0.0783,  0.0360, -0.2201,  0.0436, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.6680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012603907036412232, distance: 1.1371097817637514 entropy -4.657281572556036
epoch: 4, step: 126
	action: tensor([[-0.0939, -0.2119, -0.0845,  0.0846, -0.1780, -0.0619, -0.0578]],
       dtype=torch.float64)
	q_value: tensor([[0.6793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0534950989571078, distance: 1.1133151700713635 entropy -4.642530452885619
epoch: 4, step: 127
	action: tensor([[-0.0936, -0.1965, -0.0685,  0.0344, -0.2436, -0.0894,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[0.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039901562459606965, distance: 1.1212812881233114 entropy -4.648173018914867
LOSS epoch 4 actor 0.2865974452017557 critic 0.03597752645049997
epoch: 5, step: 0
	action: tensor([[-0.1471, -0.1342, -0.1017,  0.0733, -0.1833,  0.1110, -0.0889]],
       dtype=torch.float64)
	q_value: tensor([[0.7094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07835761763918281, distance: 1.0985957439720222 entropy -4.342587387693479
epoch: 5, step: 1
	action: tensor([[-0.1453, -0.2437, -0.0632,  0.1132, -0.2115, -0.0392,  0.0094]],
       dtype=torch.float64)
	q_value: tensor([[0.6936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010191022458348353, distance: 1.1501604922051414 entropy -4.34857216379092
epoch: 5, step: 2
	action: tensor([[-0.1469, -0.2647, -0.0804,  0.1517, -0.1952, -0.0676, -0.0758]],
       dtype=torch.float64)
	q_value: tensor([[0.7126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015973569411007915, distance: 1.1534476756770708 entropy -4.34065764249579
epoch: 5, step: 3
	action: tensor([[-0.1475, -0.2649, -0.0769, -0.0521, -0.1966, -0.1503, -0.0600]],
       dtype=torch.float64)
	q_value: tensor([[0.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12759637910293398, distance: 1.2151601800899374 entropy -4.3350217174000365
epoch: 5, step: 4
	action: tensor([[-0.1466, -0.2155, -0.0725,  0.0029, -0.2362,  0.2424,  0.1164]],
       dtype=torch.float64)
	q_value: tensor([[0.7286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005930868802881073, distance: 1.1409457296378431 entropy -4.343223222604295
epoch: 5, step: 5
	action: tensor([[-0.1447, -0.3085, -0.0607,  0.0549, -0.2129,  0.1874, -0.0333]],
       dtype=torch.float64)
	q_value: tensor([[0.6801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04942217741102528, distance: 1.1722812323968559 entropy -4.343967881108666
epoch: 5, step: 6
	action: tensor([[-0.1456, -0.2911, -0.1034,  0.1495, -0.2590,  0.2919, -0.2131]],
       dtype=torch.float64)
	q_value: tensor([[0.7139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022489382786201273, distance: 1.1314032836677916 entropy -4.335347109711101
epoch: 5, step: 7
	action: tensor([[-0.1502, -0.2130, -0.0871, -0.1293, -0.2461,  0.1975,  0.0919]],
       dtype=torch.float64)
	q_value: tensor([[0.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0693016140861511, distance: 1.1833325326057904 entropy -4.313462977008969
epoch: 5, step: 8
	action: tensor([[-0.1447, -0.2229, -0.0985,  0.0643, -0.2053,  0.0991, -0.0520]],
       dtype=torch.float64)
	q_value: tensor([[0.6899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004608375911829388, distance: 1.1417044249093184 entropy -4.356869681026105
epoch: 5, step: 9
	action: tensor([[-0.1455, -0.2182, -0.0882,  0.1070, -0.1785, -0.0405,  0.1383]],
       dtype=torch.float64)
	q_value: tensor([[0.7066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00502112770050922, distance: 1.1414676892397984 entropy -4.343507095254178
epoch: 5, step: 10
	action: tensor([[-1.4614e-01, -1.6123e-01, -7.7151e-02,  1.6446e-01, -2.4412e-01,
         -1.9097e-05,  4.3790e-02]], dtype=torch.float64)
	q_value: tensor([[0.6944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08451391931006302, distance: 1.0949204470142548 entropy -4.34262413723421
epoch: 5, step: 11
	action: tensor([[-0.1474, -0.3015, -0.0819,  0.2740, -0.2045, -0.0542,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[0.6993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018474609407988396, distance: 1.1337243189906752 entropy -4.3400579607392435
epoch: 5, step: 12
	action: tensor([[-0.1494, -0.3186, -0.0941, -0.0226, -0.1843,  0.0766, -0.0499]],
       dtype=torch.float64)
	q_value: tensor([[0.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1253478357860196, distance: 1.213947998207129 entropy -4.3192525799022485
epoch: 5, step: 13
	action: tensor([[-0.1451, -0.2708, -0.0608,  0.2556, -0.2346, -0.1161, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[0.7211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03016104953891241, distance: 1.1269548173344444 entropy -4.341824980850267
epoch: 5, step: 14
	action: tensor([[-0.1495, -0.3278, -0.0875,  0.0384, -0.2020,  0.2928, -0.0722]],
       dtype=torch.float64)
	q_value: tensor([[0.7373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05789929610572231, distance: 1.1770064894790528 entropy -4.322822413491972
epoch: 5, step: 15
	action: tensor([[-0.1467, -0.2085, -0.0649,  0.0439, -0.2169, -0.0391, -0.1567]],
       dtype=torch.float64)
	q_value: tensor([[0.7207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01720544983966099, distance: 1.1541467485679646 entropy -4.328694521947549
epoch: 5, step: 16
	action: tensor([[-0.1468, -0.2433, -0.0853, -0.0598, -0.2388, -0.3248,  0.1803]],
       dtype=torch.float64)
	q_value: tensor([[0.7264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12351846055042537, distance: 1.2129608946675876 entropy -4.343832918605675
epoch: 5, step: 17
	action: tensor([[-0.1494, -0.2185, -0.0807,  0.1543, -0.2931,  0.0683, -0.0717]],
       dtype=torch.float64)
	q_value: tensor([[0.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.040009473329768896, distance: 1.121218272793382 entropy -4.337658907731873
epoch: 5, step: 18
	action: tensor([[-0.1484, -0.2497, -0.0683,  0.1316, -0.2360, -0.0709,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[0.7238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01398167828252661, distance: 1.1523164112957547 entropy -4.332798289459927
epoch: 5, step: 19
	action: tensor([[-0.1478, -0.2061, -0.1148,  0.0842, -0.2766,  0.2263, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[0.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03886139751981643, distance: 1.1218885184413727 entropy -4.3367316400479
epoch: 5, step: 20
	action: tensor([[-0.1469, -0.1803, -0.1013,  0.0519, -0.2549,  0.0390, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.7083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019394517108631937, distance: 1.1331929184693759 entropy -4.334972932201718
epoch: 5, step: 21
	action: tensor([[-0.1463, -0.1572, -0.0719, -0.0147, -0.1914, -0.0552, -0.1401]],
       dtype=torch.float64)
	q_value: tensor([[0.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005480847539212297, distance: 1.1474759569613102 entropy -4.346283081148201
epoch: 5, step: 22
	action: tensor([[-0.1460, -0.2789, -0.0713,  0.0197, -0.3288,  0.2248, -0.1359]],
       dtype=torch.float64)
	q_value: tensor([[0.7137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04780435650985937, distance: 1.1713772718050839 entropy -4.351773112915882
epoch: 5, step: 23
	action: tensor([[-0.1486, -0.3283, -0.0782,  0.0197, -0.3247,  0.1990,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[0.7390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09618438855675393, distance: 1.1981149840231273 entropy -4.3288953809472215
epoch: 5, step: 24
	action: tensor([[-0.1473, -0.3554, -0.1133,  0.0762, -0.2245,  0.0982,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[0.7202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1040658931832037, distance: 1.2024144595793687 entropy -4.328831755156059
epoch: 5, step: 25
	action: tensor([[-0.1467, -0.1689, -0.1060,  0.0843, -0.2777,  0.1848, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[0.7201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06344319253164177, distance: 1.107449052052712 entropy -4.329202165988604
epoch: 5, step: 26
	action: tensor([[-0.1466, -0.3230, -0.1143, -0.0207, -0.2358,  0.2530,  0.0922]],
       dtype=torch.float64)
	q_value: tensor([[0.6989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09506601618778165, distance: 1.1975036449790033 entropy -4.340046307194225
epoch: 5, step: 27
	action: tensor([[-0.1456, -0.3362, -0.0924,  0.0740, -0.2687,  0.0127, -0.0533]],
       dtype=torch.float64)
	q_value: tensor([[0.7069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10057894383767296, distance: 1.2005141771536847 entropy -4.3350809543995625
epoch: 5, step: 28
	action: tensor([[-0.1482, -0.2544, -0.0983,  0.0444, -0.2218,  0.0414, -0.0292]],
       dtype=torch.float64)
	q_value: tensor([[0.7392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04477094152745087, distance: 1.1696804622941854 entropy -4.32812828216221
epoch: 5, step: 29
	action: tensor([[-0.1459, -0.3010, -0.0783,  0.0773, -0.2763,  0.1081,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[0.7139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05353876448515793, distance: 1.1745782462065213 entropy -4.342737723398469
epoch: 5, step: 30
	action: tensor([[-0.1469, -0.2185, -0.1174,  0.1941, -0.2623,  0.1835,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[0.7187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07513873331653564, distance: 1.1005125234236002 entropy -4.33223813053641
epoch: 5, step: 31
	action: tensor([[-0.1479, -0.1808, -0.0730,  0.0048, -0.2366,  0.0042, -0.1885]],
       dtype=torch.float64)
	q_value: tensor([[0.7027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008268081822503826, distance: 1.1490652815899707 entropy -4.327870675073487
epoch: 5, step: 32
	action: tensor([[-0.1469, -0.2122, -0.0923, -0.0798, -0.2666,  0.0782, -0.0739]],
       dtype=torch.float64)
	q_value: tensor([[0.7257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.061933587434325865, distance: 1.1792486068138575 entropy -4.344985302577576
epoch: 5, step: 33
	action: tensor([[-0.1461, -0.2665, -0.0752,  0.0570, -0.2829, -0.1217, -0.0481]],
       dtype=torch.float64)
	q_value: tensor([[0.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06761396637026085, distance: 1.182398354160341 entropy -4.349244172474932
epoch: 5, step: 34
	action: tensor([[-0.1487, -0.2594, -0.0782, -0.0467, -0.3091, -0.0686, -0.0604]],
       dtype=torch.float64)
	q_value: tensor([[0.7357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10745240881539031, distance: 1.2042571383757712 entropy -4.334330099135932
epoch: 5, step: 35
	action: tensor([[-0.1483, -0.1791, -0.0869,  0.0498, -0.2738, -0.1105, -0.0360]],
       dtype=torch.float64)
	q_value: tensor([[0.7334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00011884651439020288, distance: 1.1444122526426346 entropy -4.341529445320659
epoch: 5, step: 36
	action: tensor([[-0.1479, -0.2748, -0.0819, -0.0169, -0.2357,  0.0863, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[0.7189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08351057426138309, distance: 1.1911686924375198 entropy -4.34385392896141
epoch: 5, step: 37
	action: tensor([[-0.1454, -0.2832, -0.0743, -0.0337, -0.2561,  0.0868, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[0.7142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09642472451047523, distance: 1.1982463188281958 entropy -4.344881107113333
epoch: 5, step: 38
	action: tensor([[-0.1457, -0.3733, -0.0524,  0.0607, -0.3449,  0.0056,  0.1054]],
       dtype=torch.float64)
	q_value: tensor([[0.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13857966026102897, distance: 1.2210639340524274 entropy -4.34348483728064
epoch: 5, step: 39
	action: tensor([[-0.1493, -0.2694, -0.0850,  0.0657, -0.3068, -0.0550, -0.1144]],
       dtype=torch.float64)
	q_value: tensor([[0.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.060728607599946915, distance: 1.178579368110154 entropy -4.325435627108306
epoch: 5, step: 40
	action: tensor([[-0.1494, -0.3751, -0.0965,  0.1089, -0.2381,  0.0619,  0.0669]],
       dtype=torch.float64)
	q_value: tensor([[0.7461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11188903841340103, distance: 1.2066669495986284 entropy -4.331694202142154
epoch: 5, step: 41
	action: tensor([[-0.1476, -0.2757, -0.0862,  0.0094, -0.2344,  0.3162, -0.0180]],
       dtype=torch.float64)
	q_value: tensor([[0.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02627945121249131, distance: 1.1592831137094282 entropy -4.324067014908067
epoch: 5, step: 42
	action: tensor([[-0.1463, -0.2221, -0.0853,  0.1136, -0.2614, -0.0570,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[0.7083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003449993667026807, distance: 1.1423685582787013 entropy -4.334227655818037
epoch: 5, step: 43
	action: tensor([[-0.1479, -0.3548, -0.0742,  0.1519, -0.2369,  0.0171, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[0.7117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07608951194706881, distance: 1.1870824724220828 entropy -4.338331773291855
epoch: 5, step: 44
	action: tensor([[-0.1482, -0.2678, -0.0607,  0.0525, -0.2358, -0.1452, -0.0277]],
       dtype=torch.float64)
	q_value: tensor([[0.7382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07872689909798924, distance: 1.1885362920419944 entropy -4.323654153716302
epoch: 5, step: 45
	action: tensor([[-0.1479, -0.4244, -0.0936,  0.1718, -0.2374, -0.0418,  0.0050]],
       dtype=torch.float64)
	q_value: tensor([[0.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13672845818052282, distance: 1.220070874135194 entropy -4.337344000325518
epoch: 5, step: 46
	action: tensor([[-0.1494, -0.2715, -0.0809, -0.0573, -0.2388, -0.0465, -0.0602]],
       dtype=torch.float64)
	q_value: tensor([[0.7517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12223597078901172, distance: 1.212268403103582 entropy -4.313101822063347
epoch: 5, step: 47
	action: tensor([[-0.1465, -0.2765, -0.0786,  0.1291, -0.3040,  0.1574,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[0.7257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0011791317422753522, distance: 1.14501872155404 entropy -4.345420162324683
epoch: 5, step: 48
	action: tensor([[-0.1478, -0.3440, -0.0984,  0.0096, -0.2834,  0.0917, -0.0394]],
       dtype=torch.float64)
	q_value: tensor([[0.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1303443057678566, distance: 1.2166399379301633 entropy -4.327225589627347
epoch: 5, step: 49
	action: tensor([[-0.1473, -0.2551, -0.0882,  0.1129, -0.2504,  0.2834, -0.1269]],
       dtype=torch.float64)
	q_value: tensor([[0.7381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03267485514704482, distance: 1.1254933460985552 entropy -4.330707611258759
epoch: 5, step: 50
	action: tensor([[-0.1481, -0.1524, -0.1260,  0.0937, -0.1846,  0.0239, -0.1270]],
       dtype=torch.float64)
	q_value: tensor([[0.7215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05613505398326046, distance: 1.1117614780375156 entropy -4.325985194011683
epoch: 5, step: 51
	action: tensor([[-0.1461, -0.3408, -0.0894,  0.0506, -0.2010,  0.1887,  0.0898]],
       dtype=torch.float64)
	q_value: tensor([[0.7116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08199801211781121, distance: 1.1903369765575569 entropy -4.345292549403815
epoch: 5, step: 52
	action: tensor([[-0.1454, -0.2815, -0.0857,  0.1883, -0.2404, -0.2577, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[0.7050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.032326944814021985, distance: 1.1626937147684295 entropy -4.332706885445397
epoch: 5, step: 53
	action: tensor([[-0.1505, -0.3145, -0.0422,  0.0147, -0.2701,  0.1677,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[0.7547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0883867962889553, distance: 1.1938460463997533 entropy -4.319388271010745
epoch: 5, step: 54
	action: tensor([[-0.1459, -0.3037, -0.0855,  0.0828, -0.2138,  0.2838,  0.0616]],
       dtype=torch.float64)
	q_value: tensor([[0.7126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014675947765520503, distance: 1.1527108371718586 entropy -4.336689175750175
epoch: 5, step: 55
	action: tensor([[-0.1462, -0.2891, -0.0865,  0.0010, -0.2399,  0.1999,  0.0662]],
       dtype=torch.float64)
	q_value: tensor([[0.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06432911973125677, distance: 1.180577944551329 entropy -4.328213249744979
epoch: 5, step: 56
	action: tensor([[-0.1452, -0.3639, -0.0915,  0.0436, -0.2616,  0.1866,  0.1114]],
       dtype=torch.float64)
	q_value: tensor([[0.7036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10891116996925243, distance: 1.2050500146206222 entropy -4.339859013527264
epoch: 5, step: 57
	action: tensor([[-0.1466, -0.1541, -0.0895,  0.0270, -0.2281,  0.2631,  0.1315]],
       dtype=torch.float64)
	q_value: tensor([[0.7150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06814245756535664, distance: 1.1046671913392998 entropy -4.326620354080764
epoch: 5, step: 58
	action: tensor([[-0.1451, -0.2730, -0.0719,  0.0007, -0.2208,  0.2030,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[0.6671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04679903397600249, distance: 1.1708151942750178 entropy -4.344642561739869
epoch: 5, step: 59
	action: tensor([[-0.1449, -0.2651, -0.0738, -0.0926, -0.2461,  0.0191,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[0.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11963924747456489, distance: 1.2108650669065524 entropy -4.342046348440102
epoch: 5, step: 60
	action: tensor([[-0.1456, -0.1302, -0.0860,  0.0461, -0.2738,  0.2568,  0.1194]],
       dtype=torch.float64)
	q_value: tensor([[0.7135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09102399050119014, distance: 1.0910204820762521 entropy -4.351927718817527
epoch: 5, step: 61
	action: tensor([[-0.1459, -0.2082, -0.0888,  0.1105, -0.2425, -0.0043, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[0.6706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02044244492696179, distance: 1.1325872611326917 entropy -4.3426150421975365
epoch: 5, step: 62
	action: tensor([[-0.1474, -0.2657, -0.0993,  0.0402, -0.2560, -0.0005,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[0.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06189357263614714, distance: 1.1792263889274994 entropy -4.339486607094474
epoch: 5, step: 63
	action: tensor([[-0.1469, -0.3110, -0.0759,  0.0875, -0.2793,  0.0703, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[0.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06393857189536134, distance: 1.1803613224288418 entropy -4.340778809796177
epoch: 5, step: 64
	action: tensor([[-0.1476, -0.1685, -0.0629,  0.0730, -0.2551, -0.0692, -0.0772]],
       dtype=torch.float64)
	q_value: tensor([[0.7270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024828569426634473, distance: 1.1300487466398332 entropy -4.330556549884234
epoch: 5, step: 65
	action: tensor([[-0.1474, -0.3512, -0.0935,  0.2183, -0.1706,  0.1110,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[0.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020555758662879287, distance: 1.1560458582065076 entropy -4.346826673909541
epoch: 5, step: 66
	action: tensor([[-0.1479, -0.3378, -0.0783,  0.0361, -0.2507, -0.0322,  0.0796]],
       dtype=torch.float64)
	q_value: tensor([[0.7187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12981561650856088, distance: 1.2163553787499117 entropy -4.318884339402427
epoch: 5, step: 67
	action: tensor([[-0.1475, -0.2317, -0.0744,  0.1026, -0.2280, -0.2546,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[0.7213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03651538605986682, distance: 1.1650500151236671 entropy -4.3371719907579624
epoch: 5, step: 68
	action: tensor([[-0.1491, -0.4425, -0.0804,  0.0433, -0.2143,  0.2367,  0.0863]],
       dtype=torch.float64)
	q_value: tensor([[0.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16443903995139308, distance: 1.2348524622271995 entropy -4.331845184649715
epoch: 5, step: 69
	action: tensor([[-0.1471, -0.3039, -0.0985,  0.0599, -0.2072,  0.0362, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[0.7232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07801472890658956, distance: 1.18814389436037 entropy -4.317664384551305
epoch: 5, step: 70
	action: tensor([[-0.1464, -0.2236, -0.0660,  0.0845, -0.2115,  0.1848, -0.0526]],
       dtype=torch.float64)
	q_value: tensor([[0.7263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03125296041402226, distance: 1.1263202373507355 entropy -4.335855729747926
epoch: 5, step: 71
	action: tensor([[-0.1454, -0.3109, -0.0792,  0.1477, -0.2594, -0.0006, -0.0835]],
       dtype=torch.float64)
	q_value: tensor([[0.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04254400732377106, distance: 1.1684332075883035 entropy -4.3415034245202815
epoch: 5, step: 72
	action: tensor([[-0.1484, -0.3122, -0.0796,  0.0245, -0.1914,  0.0572, -0.0687]],
       dtype=torch.float64)
	q_value: tensor([[0.7400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09899476810080166, distance: 1.1996498546497074 entropy -4.3266411041105926
epoch: 5, step: 73
	action: tensor([[-0.1456, -0.3431, -0.0978,  0.0977, -0.3099,  0.0276, -0.0160]],
       dtype=torch.float64)
	q_value: tensor([[0.7222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09297540641607682, distance: 1.1963600112611088 entropy -4.338903397704416
epoch: 5, step: 74
	action: tensor([[-0.1491, -0.4014, -0.0833,  0.0921, -0.2679,  0.4082,  0.1136]],
       dtype=torch.float64)
	q_value: tensor([[0.7405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07028100263792925, distance: 1.1838743241212226 entropy -4.323156402919789
epoch: 5, step: 75
	action: tensor([[-0.1499, -0.2164, -0.1054, -0.1026, -0.2553,  0.1076,  0.0825]],
       dtype=torch.float64)
	q_value: tensor([[0.7149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07423122637587642, distance: 1.1860570505678512 entropy -4.303031377109638
epoch: 5, step: 76
	action: tensor([[-0.1450, -0.1587, -0.0892,  0.0749, -0.2511, -0.0576, -0.1359]],
       dtype=torch.float64)
	q_value: tensor([[0.6960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03740663583661741, distance: 1.1227372323107627 entropy -4.356861632082138
epoch: 5, step: 77
	action: tensor([[-0.1478, -0.1924, -0.1055, -0.0249, -0.1962,  0.2038, -0.1409]],
       dtype=torch.float64)
	q_value: tensor([[0.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0032029804622812907, distance: 1.1425101280108143 entropy -4.344126253774239
epoch: 5, step: 78
	action: tensor([[-0.1458, -0.1371, -0.0935,  0.1233, -0.2408,  0.0151, -0.1536]],
       dtype=torch.float64)
	q_value: tensor([[0.7074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08455405268914051, distance: 1.0948964470103812 entropy -4.3441500179682215
epoch: 5, step: 79
	action: tensor([[-0.1473, -0.2199, -0.0841,  0.0514, -0.2620, -0.0869,  0.2322]],
       dtype=torch.float64)
	q_value: tensor([[0.7171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029582222276011816, distance: 1.1611470169413807 entropy -4.34306109919153
epoch: 5, step: 80
	action: tensor([[-0.1480, -0.3027, -0.0911,  0.0223, -0.2300,  0.1635, -0.1553]],
       dtype=torch.float64)
	q_value: tensor([[0.6925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07441145195090426, distance: 1.1861565397857363 entropy -4.342360076237553
epoch: 5, step: 81
	action: tensor([[-0.1468, -0.1678, -0.0845,  0.1170, -0.2119,  0.0985,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.7363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07157357397974062, distance: 1.102631613510822 entropy -4.332569279399295
epoch: 5, step: 82
	action: tensor([[-0.1457, -0.2264, -0.0924, -0.0182, -0.2216, -0.1272,  0.0934]],
       dtype=torch.float64)
	q_value: tensor([[0.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0720094538770999, distance: 1.1848298879161243 entropy -4.343779049149545
epoch: 5, step: 83
	action: tensor([[-0.1468, -0.2179, -0.1161,  0.0388, -0.2845,  0.0365, -0.0261]],
       dtype=torch.float64)
	q_value: tensor([[0.7066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018288144747394375, distance: 1.1547608115749077 entropy -4.346421467060714
epoch: 5, step: 84
	action: tensor([[-0.1472, -0.2900, -0.1039,  0.1022, -0.2279, -0.3436, -0.0349]],
       dtype=torch.float64)
	q_value: tensor([[0.7171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09105540702609383, distance: 1.1953087432550302 entropy -4.34144849605159
epoch: 5, step: 85
	action: tensor([[-0.1508, -0.3383, -0.0639,  0.1602, -0.3041, -0.1354, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[0.7635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08114314454775728, distance: 1.1898666515015774 entropy -4.318179453702426
epoch: 5, step: 86
	action: tensor([[-0.1506, -0.2861, -0.0737,  0.0164, -0.2257,  0.2055,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[0.7610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05578224727334047, distance: 1.1758281976989788 entropy -4.317577780125597
epoch: 5, step: 87
	action: tensor([[-0.1449, -0.2855, -0.0597,  0.1992, -0.2561, -0.0307,  0.0624]],
       dtype=torch.float64)
	q_value: tensor([[0.6981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0029582408151005435, distance: 1.1426503774090915 entropy -4.339814020933648
epoch: 5, step: 88
	action: tensor([[-0.1488, -0.2285, -0.0823,  0.1134, -0.2144,  0.1434,  0.0852]],
       dtype=torch.float64)
	q_value: tensor([[0.7208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028723816832497917, distance: 1.1277895418506823 entropy -4.325222144330049
epoch: 5, step: 89
	action: tensor([[-0.1458, -0.2498, -0.0669,  0.1660, -0.2510,  0.0255,  0.1575]],
       dtype=torch.float64)
	q_value: tensor([[0.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02161739535385021, distance: 1.131907804799276 entropy -4.337981543187964
epoch: 5, step: 90
	action: tensor([[-0.1479, -0.2606, -0.0945,  0.0414, -0.2103,  0.1240,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[0.6988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.036079865170911996, distance: 1.1648052252554995 entropy -4.330505395813744
epoch: 5, step: 91
	action: tensor([[-0.1451, -0.2756, -0.0756,  0.1656, -0.2698,  0.0820, -0.0383]],
       dtype=torch.float64)
	q_value: tensor([[0.7016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009300272622031036, distance: 1.1390104668444982 entropy -4.3430633559362315
epoch: 5, step: 92
	action: tensor([[-0.1479, -0.2200, -0.0846,  0.1517, -0.2204,  0.0911,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[0.7236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04509057239261982, distance: 1.118247108306924 entropy -4.328873764629061
epoch: 5, step: 93
	action: tensor([[-0.1465, -0.3288, -0.0959, -0.0376, -0.2155,  0.1090, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[0.7028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13338101191696294, distance: 1.2182731128349344 entropy -4.336997879376606
epoch: 5, step: 94
	action: tensor([[-0.1453, -0.3292, -0.0821,  0.1185, -0.2616, -0.0183, -0.0600]],
       dtype=torch.float64)
	q_value: tensor([[0.7227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07565461377548344, distance: 1.1868425703680254 entropy -4.3407811221836194
epoch: 5, step: 95
	action: tensor([[-0.1485, -0.2504, -0.0803,  0.0829, -0.2196,  0.1742,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[0.7406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0023671093842285718, distance: 1.1429890575390662 entropy -4.326626744364577
epoch: 5, step: 96
	action: tensor([[-0.1455, -0.2437, -0.0654,  0.1369, -0.1857, -0.2858,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[0.7003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035847606326447456, distance: 1.1646746602826514 entropy -4.338720545801295
epoch: 5, step: 97
	action: tensor([[-0.1490, -0.3658, -0.0722,  0.0764, -0.2015,  0.1744, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[0.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09375526149433555, distance: 1.1967867459610808 entropy -4.329455839054299
epoch: 5, step: 98
	action: tensor([[-0.1460, -0.2529, -0.0994,  0.0250, -0.2862, -0.0572, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[0.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06382024380578644, distance: 1.1802956824625812 entropy -4.328148624203581
epoch: 5, step: 99
	action: tensor([[-0.1480, -0.2094, -0.0605, -0.0662, -0.2527, -0.0878,  0.1292]],
       dtype=torch.float64)
	q_value: tensor([[0.7275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07850004098780627, distance: 1.1884113098640783 entropy -4.339132719328336
epoch: 5, step: 100
	action: tensor([[-0.1464, -0.2305, -0.1003,  0.0912, -0.2559,  0.1137, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[0.6936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009604539165263937, distance: 1.1388355453312988 entropy -4.355205994546672
epoch: 5, step: 101
	action: tensor([[-0.1464, -0.1718, -0.0897, -0.0080, -0.2340,  0.0568, -0.1089]],
       dtype=torch.float64)
	q_value: tensor([[0.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002624245859451757, distance: 1.1428417472805132 entropy -4.3381989031498
epoch: 5, step: 102
	action: tensor([[-0.1459, -0.1661, -0.0745,  0.1672, -0.1983,  0.2635,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[0.7087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13109079500676302, distance: 1.066703923005481 entropy -4.349307951106207
epoch: 5, step: 103
	action: tensor([[-0.1463, -0.2192, -0.0509,  0.0060, -0.1765,  0.2119, -0.0439]],
       dtype=torch.float64)
	q_value: tensor([[0.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005311654453887038, distance: 1.1413010268498809 entropy -4.335307255361962
epoch: 5, step: 104
	action: tensor([[-0.1445, -0.1510, -0.0932, -0.0199, -0.2464,  0.0161, -0.0870]],
       dtype=torch.float64)
	q_value: tensor([[0.6913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010072164470378064, distance: 1.1385666571864639 entropy -4.347800061228606
epoch: 5, step: 105
	action: tensor([[-0.1461, -0.3328, -0.0780,  0.0310, -0.1995, -0.2471,  0.0545]],
       dtype=torch.float64)
	q_value: tensor([[0.7069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1572312366164228, distance: 1.2310247006701218 entropy -4.3520002265996975
epoch: 5, step: 106
	action: tensor([[-0.1488, -0.2406, -0.1206, -0.0489, -0.2262, -0.3245,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[0.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11280358687415837, distance: 1.207163100083563 entropy -4.329527535364136
epoch: 5, step: 107
	action: tensor([[-0.1492, -0.2444, -0.0809,  0.1317, -0.2329,  0.3970,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[0.7440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07537326286082702, distance: 1.1003729786753096 entropy -4.3323827681281974
epoch: 5, step: 108
	action: tensor([[-0.1482, -0.2791, -0.0578,  0.0789, -0.2794,  0.1838,  0.0657]],
       dtype=torch.float64)
	q_value: tensor([[0.6933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021962330440823852, distance: 1.1568422387932544 entropy -4.319808986802635
epoch: 5, step: 109
	action: tensor([[-0.1464, -0.1695, -0.0774,  0.0479, -0.2550,  0.0322,  0.1040]],
       dtype=torch.float64)
	q_value: tensor([[0.7052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026970849749527237, distance: 1.1288068048934965 entropy -4.332314230888519
epoch: 5, step: 110
	action: tensor([[-0.1460, -0.2545, -0.0985,  0.1597, -0.2516,  0.0138,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[0.6870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008693545889029153, distance: 1.139359191239133 entropy -4.348310981556163
epoch: 5, step: 111
	action: tensor([[-0.1479, -0.1940, -0.0832,  0.1502, -0.2264, -0.4225,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[0.7146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003071137277987468, distance: 1.1425856836103154 entropy -4.331438073414921
epoch: 5, step: 112
	action: tensor([[-1.5132e-01, -2.5507e-01, -7.0337e-02,  8.6342e-02, -2.5627e-01,
         -2.0596e-02, -1.4310e-05]], dtype=torch.float64)
	q_value: tensor([[0.7548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03672883893605072, distance: 1.1651699701567793 entropy -4.321111558435064
epoch: 5, step: 113
	action: tensor([[-0.1474, -0.1988, -0.0847, -0.0065, -0.2699, -0.0364, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[0.7174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03294094374854306, distance: 1.163039432075517 entropy -4.339785870864188
epoch: 5, step: 114
	action: tensor([[-0.1470, -0.1894, -0.0699,  0.0854, -0.2849,  0.0996, -0.0841]],
       dtype=torch.float64)
	q_value: tensor([[0.7160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.038430609155902906, distance: 1.122139909019227 entropy -4.347871887251488
epoch: 5, step: 115
	action: tensor([[-0.1471, -0.3012, -0.0810,  0.0591, -0.1764,  0.0466,  0.0722]],
       dtype=torch.float64)
	q_value: tensor([[0.7125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07280049752632212, distance: 1.1852669546403525 entropy -4.341032201997271
epoch: 5, step: 116
	action: tensor([[-0.1454, -0.3850, -0.0743, -0.0922, -0.2511,  0.0771,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[0.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21488709220576996, distance: 1.261318170458894 entropy -4.341002587971701
epoch: 5, step: 117
	action: tensor([[-0.1462, -0.3399, -0.0950,  0.0878, -0.2274, -0.1546,  0.0855]],
       dtype=torch.float64)
	q_value: tensor([[0.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1220534117774481, distance: 1.2121697966035678 entropy -4.340060678226222
epoch: 5, step: 118
	action: tensor([[-0.1489, -0.3577, -0.0906,  0.1666, -0.2157,  0.0213, -0.0874]],
       dtype=torch.float64)
	q_value: tensor([[0.7342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07301932054436078, distance: 1.1853878300819205 entropy -4.326684293920082
epoch: 5, step: 119
	action: tensor([[-0.1482, -0.2550, -0.0668,  0.0341, -0.2202, -0.1754,  0.1213]],
       dtype=torch.float64)
	q_value: tensor([[0.7445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0817146361781429, distance: 1.1901810913599968 entropy -4.321522990860933
epoch: 5, step: 120
	action: tensor([[-0.1478, -0.3477, -0.0882,  0.1539, -0.2099, -0.0724,  0.0489]],
       dtype=torch.float64)
	q_value: tensor([[0.7121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08593018533490215, distance: 1.1924979631270916 entropy -4.339616153443688
epoch: 5, step: 121
	action: tensor([[-0.1484, -0.3475, -0.0695,  0.0109, -0.2895, -0.1339, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[0.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1631440682362988, distance: 1.2341656320057006 entropy -4.324463492810355
epoch: 5, step: 122
	action: tensor([[-0.1493, -0.1666, -0.0670,  0.0523, -0.2224,  0.2097, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[0.7446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060456572099209516, distance: 1.1092134390394188 entropy -4.330527529066496
epoch: 5, step: 123
	action: tensor([[-0.1452, -0.2749, -0.0883,  0.1736, -0.2118, -0.2951,  0.1092]],
       dtype=torch.float64)
	q_value: tensor([[0.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.041769237633488165, distance: 1.167998964583489 entropy -4.346507992599381
epoch: 5, step: 124
	action: tensor([[-0.1505, -0.3055, -0.0906,  0.0426, -0.2539,  0.2698,  0.1216]],
       dtype=torch.float64)
	q_value: tensor([[0.7386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049765286572687195, distance: 1.172472855734137 entropy -4.320581475135578
epoch: 5, step: 125
	action: tensor([[-0.1463, -0.1862, -0.0902, -0.0107, -0.2321, -0.1614, -0.0369]],
       dtype=torch.float64)
	q_value: tensor([[0.6995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03810577221971956, distance: 1.1659434747114061 entropy -4.329359106612013
epoch: 5, step: 126
	action: tensor([[-0.1474, -0.3317, -0.1121,  0.1126, -0.1788,  0.0562, -0.0412]],
       dtype=torch.float64)
	q_value: tensor([[0.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07255791168452119, distance: 1.1851329384655158 entropy -4.3448565248448014
epoch: 5, step: 127
	action: tensor([[-0.1466, -0.2795, -0.0693,  0.0913, -0.1820,  0.2825, -0.0310]],
       dtype=torch.float64)
	q_value: tensor([[0.7275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013836684823864132, distance: 1.1363997113537825 entropy -4.32982731324304
LOSS epoch 5 actor 0.3679337461730067 critic 0.5152846814160189
epoch: 6, step: 0
	action: tensor([[-0.3435, -0.2738, -0.1201, -0.0060, -0.3699, -0.0204,  0.1861]],
       dtype=torch.float64)
	q_value: tensor([[0.5445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32163964952994806, distance: 1.315567762842941 entropy -3.984106208813841
epoch: 6, step: 1
	action: tensor([[-0.3537, -0.3000, -0.1477,  0.2527, -0.3025, -0.0221, -0.2744]],
       dtype=torch.float64)
	q_value: tensor([[0.5469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24586781395851243, distance: 1.27729930663422 entropy -3.974606246966833
epoch: 6, step: 2
	action: tensor([[-0.3722, -0.3144, -0.1032, -0.0879, -0.3186, -0.3468,  0.1885]],
       dtype=torch.float64)
	q_value: tensor([[0.6087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42299212782352114, distance: 1.3650794915586248 entropy -3.936466638197989
epoch: 6, step: 3
	action: tensor([[-0.3635, -0.4363, -0.1090,  0.0329, -0.3733,  0.3558,  0.1136]],
       dtype=torch.float64)
	q_value: tensor([[0.5717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41494110846970145, distance: 1.3612123335629933 entropy -3.952232309810792
epoch: 6, step: 4
	action: tensor([[-0.3663, -0.5058, -0.1382,  0.0423, -0.3296, -0.1608,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.5623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5154195773384336, distance: 1.408715003316106 entropy -1.604225242079853
epoch: 6, step: 5
	action: tensor([[-0.3721, -0.3299, -0.1326,  0.0693, -0.3039,  0.2674, -0.0538]],
       dtype=torch.float64)
	q_value: tensor([[0.6011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3364305641546501, distance: 1.3229087621364002 entropy -1.6085372369691713
epoch: 6, step: 6
	action: tensor([[-0.3588, -0.4633, -0.0960,  0.1833, -0.3011,  0.6611, -0.0342]],
       dtype=torch.float64)
	q_value: tensor([[0.5644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26473230378463364, distance: 1.2869331826461918 entropy -3.958030290718001
epoch: 6, step: 7
	action: tensor([[-0.3870, -0.3882, -0.0985,  0.0875, -0.3111,  0.1687,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[0.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40220877826901535, distance: 1.3550740673400665 entropy -1.4206813139563859
epoch: 6, step: 8
	action: tensor([[-0.3596, -0.4325, -0.1116,  0.1792, -0.3646, -0.4189,  0.1372]],
       dtype=torch.float64)
	q_value: tensor([[0.5630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3888860851160634, distance: 1.3486212754480744 entropy -3.9560688045325825
epoch: 6, step: 9
	action: tensor([[-0.3861, -0.3744, -0.0988,  0.2056, -0.3394,  0.1165, -0.2883]],
       dtype=torch.float64)
	q_value: tensor([[0.6001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34264422135147843, distance: 1.3259805898826247 entropy -3.907935905921511
epoch: 6, step: 10
	action: tensor([[-0.3767, -0.4808, -0.0957,  0.0330, -0.3758, -0.2932, -0.0836]],
       dtype=torch.float64)
	q_value: tensor([[0.6075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.508283913809995, distance: 1.405394478269649 entropy -3.9259513167434705
epoch: 6, step: 11
	action: tensor([[-0.3803, -0.3131, -0.0947,  0.1558, -0.2906,  0.3136, -0.0890]],
       dtype=torch.float64)
	q_value: tensor([[0.6237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2753760537961887, distance: 1.2923371310364893 entropy -1.5969998911214713
epoch: 6, step: 12
	action: tensor([[-0.3623, -0.2393, -0.0985,  0.0048, -0.2884,  0.1225,  0.2362]],
       dtype=torch.float64)
	q_value: tensor([[0.5624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29876264793897156, distance: 1.3041321103415209 entropy -3.95107412011357
epoch: 6, step: 13
	action: tensor([[-0.3462, -0.4276, -0.1050,  0.2627, -0.2702, -0.1180, -0.1169]],
       dtype=torch.float64)
	q_value: tensor([[0.5252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.333578868923593, distance: 1.3214965872989064 entropy -3.987616525047438
epoch: 6, step: 14
	action: tensor([[-0.3725, -0.2290, -0.1129,  0.1439, -0.3722, -0.1278,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[0.6026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.263480567270985, distance: 1.286296170357113 entropy -3.9327795209658407
epoch: 6, step: 15
	action: tensor([[-0.3622, -0.4049, -0.0920,  0.1833, -0.3239, -0.0145,  0.0629]],
       dtype=torch.float64)
	q_value: tensor([[0.5705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36067348654311093, distance: 1.3348536534390312 entropy -3.9615445388684143
epoch: 6, step: 16
	action: tensor([[-0.3659, -0.4192, -0.0925,  0.1881, -0.4266,  0.0356,  0.1272]],
       dtype=torch.float64)
	q_value: tensor([[0.5715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36999831510388725, distance: 1.3394197851183722 entropy -3.9442310134605134
epoch: 6, step: 17
	action: tensor([[-0.3725, -0.2752, -0.1491,  0.2057, -0.4258, -0.1888,  0.0712]],
       dtype=torch.float64)
	q_value: tensor([[0.5711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27159310194641484, distance: 1.2904190772308388 entropy -3.9276174736694593
epoch: 6, step: 18
	action: tensor([[-0.3739, -0.2882, -0.1118,  0.0560, -0.4120,  0.0232, -0.1596]],
       dtype=torch.float64)
	q_value: tensor([[0.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34239836782016053, distance: 1.3258591832098405 entropy -3.9374315777023194
epoch: 6, step: 19
	action: tensor([[-0.3676, -0.2670, -0.0853,  0.0257, -0.3262,  0.2456,  0.0808]],
       dtype=torch.float64)
	q_value: tensor([[0.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30138181456944024, distance: 1.3054464455458803 entropy -3.947247623738765
epoch: 6, step: 20
	action: tensor([[-0.3508, -0.2930, -0.1516, -0.1346, -0.3048, -0.2606, -0.0829]],
       dtype=torch.float64)
	q_value: tensor([[0.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3909982864111654, distance: 1.3496463693863159 entropy -3.975318617293832
epoch: 6, step: 21
	action: tensor([[-0.3598, -0.4233, -0.1036,  0.1163, -0.3060,  0.0365, -0.1361]],
       dtype=torch.float64)
	q_value: tensor([[0.6005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4001095049643253, distance: 1.3540593338852778 entropy -3.9670957046144326
epoch: 6, step: 22
	action: tensor([[-0.3659, -0.2721, -0.1564,  0.1786, -0.3077,  0.0054,  0.1009]],
       dtype=torch.float64)
	q_value: tensor([[0.5945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2715646501350206, distance: 1.2904046406294118 entropy -3.9450921458110355
epoch: 6, step: 23
	action: tensor([[-0.3592, -0.2899, -0.0810,  0.1271, -0.3571,  0.1108,  0.1224]],
       dtype=torch.float64)
	q_value: tensor([[0.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28021634051503397, distance: 1.2947871372451982 entropy -3.9615609441561768
epoch: 6, step: 24
	action: tensor([[-3.5670e-01, -2.6811e-01, -1.0019e-01,  6.6197e-02, -2.2600e-01,
          5.5430e-01,  4.3462e-04]], dtype=torch.float64)
	q_value: tensor([[0.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19109439937484818, distance: 1.2489060934577139 entropy -3.9628423743749304
epoch: 6, step: 25
	action: tensor([[-0.3616, -0.4196, -0.1274,  0.0850, -0.2805,  0.0291, -0.1004]],
       dtype=torch.float64)
	q_value: tensor([[0.5399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41614062728331613, distance: 1.36178919640486 entropy -1.6190446584974016
epoch: 6, step: 26
	action: tensor([[-0.3628, -0.4031, -0.1131,  0.2434, -0.3133, -0.1632, -0.0421]],
       dtype=torch.float64)
	q_value: tensor([[0.5906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34670790200003676, distance: 1.3279856968940618 entropy -3.9522536535582042
epoch: 6, step: 27
	action: tensor([[-0.3733, -0.5338, -0.1442, -0.0473, -0.2999, -0.0807, -0.2680]],
       dtype=torch.float64)
	q_value: tensor([[0.5952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5763880923701927, distance: 1.436773357327093 entropy -3.9333654025609968
epoch: 6, step: 28
	action: tensor([[-0.3743, -0.3567, -0.1390, -0.0416, -0.2955,  0.4581, -0.0857]],
       dtype=torch.float64)
	q_value: tensor([[0.6384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3740878394678562, distance: 1.341417418306456 entropy -1.5186194917355635
epoch: 6, step: 29
	action: tensor([[-0.3662, -0.4134, -0.1495, -0.0989, -0.3640, -0.0591, -0.1100]],
       dtype=torch.float64)
	q_value: tensor([[0.5688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49197787032958673, distance: 1.3977769807774976 entropy -1.5495186450594889
epoch: 6, step: 30
	action: tensor([[-0.3648, -0.3396, -0.1118,  0.0826, -0.2550, -0.3811,  0.0708]],
       dtype=torch.float64)
	q_value: tensor([[0.6049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3816338672640436, distance: 1.3450956816958017 entropy -1.621961290591523
epoch: 6, step: 31
	action: tensor([[-0.3677, -0.4326, -0.0833,  0.0093, -0.2524,  0.1933,  0.1186]],
       dtype=torch.float64)
	q_value: tensor([[0.5892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43956658808812876, distance: 1.3730064340682961 entropy -3.945515902723467
epoch: 6, step: 32
	action: tensor([[-0.3539, -0.2011, -0.1222,  0.0429, -0.4026, -0.0654,  0.1798]],
       dtype=torch.float64)
	q_value: tensor([[0.5531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2585158581410443, distance: 1.283766502620444 entropy -1.819946529670375
epoch: 6, step: 33
	action: tensor([[-0.3560, -0.3155, -0.0922,  0.1345, -0.3610,  0.1978, -0.0382]],
       dtype=torch.float64)
	q_value: tensor([[0.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28321814662391986, distance: 1.2963042340566215 entropy -3.9715966843407844
epoch: 6, step: 34
	action: tensor([[-0.3618, -0.4510, -0.0900, -0.0990, -0.3386, -0.2404,  0.2464]],
       dtype=torch.float64)
	q_value: tensor([[0.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5232280506689915, distance: 1.4123396694898354 entropy -3.9517708243047345
epoch: 6, step: 35
	action: tensor([[-0.3677, -0.4904, -0.1089,  0.0751, -0.1930,  0.3928, -0.2783]],
       dtype=torch.float64)
	q_value: tensor([[0.5695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40719673492796127, distance: 1.3574820719974405 entropy -1.7546917635241701
epoch: 6, step: 36
	action: tensor([[-0.3713, -0.3255, -0.1238, -0.0012, -0.2672,  0.3626, -0.0628]],
       dtype=torch.float64)
	q_value: tensor([[0.6012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34049698605397816, distance: 1.3249198727017584 entropy -1.4833395111275365
epoch: 6, step: 37
	action: tensor([[-0.3579, -0.3549, -0.0941,  0.2016, -0.2441,  0.6469,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[0.5606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1651433387990202, distance: 1.2352258496128956 entropy -1.6812046150595976
epoch: 6, step: 38
	action: tensor([[-0.3748, -0.5060, -0.1194, -0.1130, -0.3716, -0.1547,  0.0876]],
       dtype=torch.float64)
	q_value: tensor([[0.5487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5770184797812603, distance: 1.4370606068030383 entropy -1.499071220396872
epoch: 6, step: 39
	action: tensor([[-0.3705, -0.5070, -0.1113,  0.0991, -0.3518,  0.3390, -0.1891]],
       dtype=torch.float64)
	q_value: tensor([[0.5960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4462219445044371, distance: 1.3761765931118661 entropy -1.5578275936286812
epoch: 6, step: 40
	action: tensor([[-0.3772, -0.5087, -0.0883,  0.2769, -0.3761,  0.1668, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[0.6024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3888244565141201, distance: 1.348591354143994 entropy -1.5058276395953853
epoch: 6, step: 41
	action: tensor([[-0.3816, -0.2395, -0.1153,  0.0210, -0.3678, -0.1114,  0.2728]],
       dtype=torch.float64)
	q_value: tensor([[0.5891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3342500432309763, distance: 1.3218290921874523 entropy -1.7295425065424863
epoch: 6, step: 42
	action: tensor([[-0.3571, -0.3187, -0.1125,  0.0931, -0.3627, -0.1041, -0.1767]],
       dtype=torch.float64)
	q_value: tensor([[0.5432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3352183787394276, distance: 1.3223086656754413 entropy -3.9680994538995895
epoch: 6, step: 43
	action: tensor([[-0.3679, -0.3824, -0.1620,  0.0025, -0.3738, -0.0627, -0.2453]],
       dtype=torch.float64)
	q_value: tensor([[0.6040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43339225626698186, distance: 1.3700588431520058 entropy -3.9462532749119856
epoch: 6, step: 44
	action: tensor([[-0.3726, -0.3940, -0.1088,  0.2324, -0.3336,  0.3234,  0.0327]],
       dtype=torch.float64)
	q_value: tensor([[0.6213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2953957310064632, distance: 1.302440594984468 entropy -1.6351525039223682
epoch: 6, step: 45
	action: tensor([[-0.3700, -0.3327, -0.0990, -0.0284, -0.3690,  0.1103,  0.1678]],
       dtype=torch.float64)
	q_value: tensor([[0.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40087829896632465, distance: 1.3544310368944488 entropy -3.9295640629868944
epoch: 6, step: 46
	action: tensor([[-0.3542, -0.2944, -0.1047, -0.0239, -0.2788,  0.2218, -0.1192]],
       dtype=torch.float64)
	q_value: tensor([[0.5474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.329575144815218, distance: 1.3195113710565587 entropy -3.969980085973286
epoch: 6, step: 47
	action: tensor([[-0.3529, -0.5653, -0.1233,  0.0103, -0.2940,  0.4055, -0.0355]],
       dtype=torch.float64)
	q_value: tensor([[0.5670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49252503330806907, distance: 1.3980332653119356 entropy -1.982677266850859
epoch: 6, step: 48
	action: tensor([[-0.3729, -0.1464, -0.0676,  0.2507, -0.2846, -0.1277,  0.0709]],
       dtype=torch.float64)
	q_value: tensor([[0.5890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15291132791801876, distance: 1.2287248636403472 entropy -1.4614117348688258
epoch: 6, step: 49
	action: tensor([[-0.3579, -0.2902, -0.1151,  0.0213, -0.2832,  0.1134,  0.1024]],
       dtype=torch.float64)
	q_value: tensor([[0.5514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32744011065841194, distance: 1.3184515085379174 entropy -3.9698172438480888
epoch: 6, step: 50
	action: tensor([[-0.3485, -0.5463, -0.1133,  0.1414, -0.2025,  0.1219,  0.0618]],
       dtype=torch.float64)
	q_value: tensor([[0.5443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45567458264042404, distance: 1.3806666758392427 entropy -3.9818647155978364
epoch: 6, step: 51
	action: tensor([[-0.3644, -0.4406, -0.1520,  0.0386, -0.3164,  0.0064, -0.4075]],
       dtype=torch.float64)
	q_value: tensor([[0.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.459188667560392, distance: 1.3823321769789358 entropy -1.675478534758103
epoch: 6, step: 52
	action: tensor([[-0.3821, -0.5653, -0.1226,  0.0699, -0.3550,  0.3507, -0.0532]],
       dtype=torch.float64)
	q_value: tensor([[0.6412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5194208934272968, distance: 1.4105735638754187 entropy -1.561965382408451
epoch: 6, step: 53
	action: tensor([[-0.3769, -0.3664, -0.1332,  0.1865, -0.3764, -0.2072,  0.2718]],
       dtype=torch.float64)
	q_value: tensor([[0.5952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3593273148697298, distance: 1.3341931765274837 entropy -1.4674411803480685
epoch: 6, step: 54
	action: tensor([[-0.3747, -0.5467, -0.0622,  0.2197, -0.3432,  0.1193,  0.1778]],
       dtype=torch.float64)
	q_value: tensor([[0.5685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4451011148057138, distance: 1.3756432176602038 entropy -3.9310646663852498
epoch: 6, step: 55
	action: tensor([[-0.3762, -0.2620, -0.1519,  0.1481, -0.4627,  0.3425,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[0.5684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27036228196654166, distance: 1.289794404879711 entropy -1.6559923811170751
epoch: 6, step: 56
	action: tensor([[-0.3708, -0.4854, -0.1007,  0.1511, -0.2766, -0.2964,  0.0367]],
       dtype=torch.float64)
	q_value: tensor([[0.5641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47019160655141956, distance: 1.3875340924381752 entropy -3.933601344072387
epoch: 6, step: 57
	action: tensor([[-0.3761, -0.4309, -0.0975,  0.1768, -0.3379,  0.2294,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[0.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37010591465908194, distance: 1.3394723830456132 entropy -1.8300315697183858
epoch: 6, step: 58
	action: tensor([[-0.3682, -0.5362, -0.1431,  0.0193, -0.2333,  0.2217, -0.0251]],
       dtype=torch.float64)
	q_value: tensor([[0.5718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5149777456231206, distance: 1.408509627739058 entropy -1.9230474967974318
epoch: 6, step: 59
	action: tensor([[-0.3652, -0.3435, -0.0815,  0.1425, -0.2539,  0.3388,  0.2010]],
       dtype=torch.float64)
	q_value: tensor([[0.5851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27230638439311594, distance: 1.2907809477991044 entropy -1.5544432022170227
epoch: 6, step: 60
	action: tensor([[-0.3560, -0.4288, -0.1363,  0.2271, -0.2598,  0.0764,  0.1165]],
       dtype=torch.float64)
	q_value: tensor([[0.5340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34587473039323213, distance: 1.3275748389941042 entropy -3.9601699867419318
epoch: 6, step: 61
	action: tensor([[-0.3655, -0.3207, -0.0991,  0.1097, -0.3290, -0.2713, -0.0619]],
       dtype=torch.float64)
	q_value: tensor([[0.5674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34613693673325785, distance: 1.327704153253113 entropy -3.9428484143355256
epoch: 6, step: 62
	action: tensor([[-0.3686, -0.2087, -0.1074, -0.0904, -0.2954,  0.5472, -0.1028]],
       dtype=torch.float64)
	q_value: tensor([[0.5985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25311009139005214, distance: 1.2810064219924262 entropy -3.9473553549024105
epoch: 6, step: 63
	action: tensor([[-0.3598, -0.4165, -0.0948,  0.0787, -0.3728, -0.1349, -0.1299]],
       dtype=torch.float64)
	q_value: tensor([[0.5502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4218746820326207, distance: 1.3645434021852239 entropy -1.5730560802476208
epoch: 6, step: 64
	action: tensor([[-0.3724, -0.4777, -0.1239,  0.1610, -0.3081,  0.1207,  0.0801]],
       dtype=torch.float64)
	q_value: tensor([[0.6098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4289475621494958, distance: 1.3679330396968978 entropy -1.7202890590124127
epoch: 6, step: 65
	action: tensor([[-0.3677, -0.4032, -0.1369,  0.2498, -0.2391,  0.0480, -0.1900]],
       dtype=torch.float64)
	q_value: tensor([[0.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33353533776413036, distance: 1.3214750187402453 entropy -1.8659311512430392
epoch: 6, step: 66
	action: tensor([[-0.3699, -0.3349, -0.0648,  0.1042, -0.3068, -0.1047, -0.0670]],
       dtype=torch.float64)
	q_value: tensor([[0.6006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36014011511470945, distance: 1.334592002649091 entropy -3.937780545943247
epoch: 6, step: 67
	action: tensor([[-0.3607, -0.5510, -0.0914,  0.0878, -0.3370,  0.0735,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[0.5828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5092800160171767, distance: 1.4058584776107903 entropy -3.961490262641577
epoch: 6, step: 68
	action: tensor([[-0.3740, -0.2164, -0.1424,  0.0380, -0.3212,  0.3851,  0.0539]],
       dtype=torch.float64)
	q_value: tensor([[0.5917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.251894087313697, distance: 1.2803847339535925 entropy -1.5831166206127947
epoch: 6, step: 69
	action: tensor([[-0.3544, -0.3361, -0.1510,  0.1198, -0.2907, -0.0200,  0.1442]],
       dtype=torch.float64)
	q_value: tensor([[0.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33359471118109907, distance: 1.3215044366385824 entropy -3.967599389378028
epoch: 6, step: 70
	action: tensor([[-0.3582, -0.3743, -0.1180,  0.1716, -0.3261,  0.2900, -0.1037]],
       dtype=torch.float64)
	q_value: tensor([[0.5593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30031676685628783, distance: 1.304912149108148 entropy -3.9632123835619173
epoch: 6, step: 71
	action: tensor([[-0.3681, -0.2324, -0.1175,  0.1123, -0.3260,  0.1894,  0.1219]],
       dtype=torch.float64)
	q_value: tensor([[0.5756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24843131308270006, distance: 1.2786127176631787 entropy -3.936711262938205
epoch: 6, step: 72
	action: tensor([[-0.3527, -0.3030, -0.1093, -0.1448, -0.3278,  0.0710,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[0.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4055354321934057, distance: 1.3566805299596643 entropy -3.9725097387411132
epoch: 6, step: 73
	action: tensor([[-0.3480, -0.3110, -0.0718, -0.0336, -0.2982,  0.0468,  0.0662]],
       dtype=torch.float64)
	q_value: tensor([[0.5516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36303887595500584, distance: 1.336013401790433 entropy -3.984570694611758
epoch: 6, step: 74
	action: tensor([[-0.3486, -0.2286, -0.1118,  0.4256, -0.3301,  0.0168, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[0.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09093215242433961, distance: 1.1952412253896514 entropy -3.98395111352102
epoch: 6, step: 75
	action: tensor([[-0.3733, -0.3788, -0.0814,  0.0274, -0.3607,  0.0643, -0.0207]],
       dtype=torch.float64)
	q_value: tensor([[0.5741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41940445684193794, distance: 1.3633575751939275 entropy -3.9332916195953236
epoch: 6, step: 76
	action: tensor([[-0.3604, -0.2521, -0.1267,  0.0242, -0.2889,  0.1098,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[0.5741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3011353677827313, distance: 1.3053228314339438 entropy -2.3421194277424497
epoch: 6, step: 77
	action: tensor([[-0.3494, -0.4725, -0.1204,  0.2853, -0.1036, -0.0692,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[0.5520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3642062603746732, distance: 1.3365853984732394 entropy -3.9810896943914584
epoch: 6, step: 78
	action: tensor([[-0.3665, -0.3366, -0.0949,  0.1017, -0.3228,  0.0566,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[0.5607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3448266656795349, distance: 1.327057831246775 entropy -3.9414014226755434
epoch: 6, step: 79
	action: tensor([[-0.3581, -0.2273, -0.1023, -0.0118, -0.2748,  0.2468,  0.1488]],
       dtype=torch.float64)
	q_value: tensor([[0.5606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27251496466657277, distance: 1.2908867479479158 entropy -3.9615005709667477
epoch: 6, step: 80
	action: tensor([[-0.3443, -0.2045, -0.0852,  0.2007, -0.3321,  0.1966, -0.2576]],
       dtype=torch.float64)
	q_value: tensor([[0.5281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14943579653009276, distance: 1.2268714274757322 entropy -3.9892614392595354
epoch: 6, step: 81
	action: tensor([[-0.3672, -0.4478, -0.1168,  0.1645, -0.2340, -0.0287,  0.1135]],
       dtype=torch.float64)
	q_value: tensor([[0.5777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41476577489869015, distance: 1.3611279930869102 entropy -3.947552654949056
epoch: 6, step: 82
	action: tensor([[-0.3631, -0.4268, -0.1094,  0.0159, -0.2874,  0.2784, -0.1586]],
       dtype=torch.float64)
	q_value: tensor([[0.5688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41749879046434324, distance: 1.3624420583703172 entropy -3.950773228662718
epoch: 6, step: 83
	action: tensor([[-0.3649, -0.4695, -0.1018, -0.0030, -0.2943, -0.0499, -0.1069]],
       dtype=torch.float64)
	q_value: tensor([[0.5874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5026845575813512, distance: 1.4027833579934579 entropy -1.5915127830121911
epoch: 6, step: 84
	action: tensor([[-0.3646, -0.2027, -0.1209, -0.0071, -0.4306,  0.1463,  0.1625]],
       dtype=torch.float64)
	q_value: tensor([[0.6014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28556363583301136, distance: 1.29748839729248 entropy -1.6708132304620398
epoch: 6, step: 85
	action: tensor([[-0.3533, -0.3832, -0.0885,  0.0593, -0.3311, -0.1121, -0.0737]],
       dtype=torch.float64)
	q_value: tensor([[0.5420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3975822239520874, distance: 1.352836703049797 entropy -3.9731854667405635
epoch: 6, step: 86
	action: tensor([[-0.3641, -0.2225, -0.1266,  0.1753, -0.3850,  0.3494,  0.1049]],
       dtype=torch.float64)
	q_value: tensor([[0.5939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19414337102499934, distance: 1.250503551026868 entropy -3.952701824545074
epoch: 6, step: 87
	action: tensor([[-0.3624, -0.5346, -0.1411,  0.0778, -0.2581, -0.0844, -0.1040]],
       dtype=torch.float64)
	q_value: tensor([[0.5450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5204225651822159, distance: 1.4110384445785558 entropy -3.949834123729326
epoch: 6, step: 88
	action: tensor([[-0.3728, -0.3955, -0.1143,  0.1371, -0.3032,  0.3121, -0.1835]],
       dtype=torch.float64)
	q_value: tensor([[0.6121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3440002604976842, distance: 1.3266500255315021 entropy -1.6035484297331808
epoch: 6, step: 89
	action: tensor([[-0.3700, -0.3774, -0.0869,  0.1183, -0.3212, -0.0865,  0.0499]],
       dtype=torch.float64)
	q_value: tensor([[0.5852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38582431380362747, distance: 1.347133951125188 entropy -1.6731380192336207
epoch: 6, step: 90
	action: tensor([[-0.3630, -0.5122, -0.1143,  0.2566, -0.4369,  0.2585,  0.1740]],
       dtype=torch.float64)
	q_value: tensor([[0.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38555695938971457, distance: 1.347003999739304 entropy -3.954851560471116
epoch: 6, step: 91
	action: tensor([[-0.3835, -0.2649, -0.1232,  0.1664, -0.3127, -0.2107, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[0.5761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3020208766483621, distance: 1.3057669352423844 entropy -1.588390283685017
epoch: 6, step: 92
	action: tensor([[-0.3653, -0.3739, -0.1178,  0.1946, -0.2742,  0.4039,  0.2106]],
       dtype=torch.float64)
	q_value: tensor([[0.5830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26757968924046205, distance: 1.288381052149305 entropy -3.9558238685455636
epoch: 6, step: 93
	action: tensor([[-0.3642, -0.3464, -0.1249,  0.0874, -0.3775, -0.3453, -0.0287]],
       dtype=torch.float64)
	q_value: tensor([[0.5420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3637981687556733, distance: 1.336385468993049 entropy -1.8495952881905404
epoch: 6, step: 94
	action: tensor([[-0.3758, -0.5337, -0.1404, -0.0838, -0.3003,  0.1186,  0.1659]],
       dtype=torch.float64)
	q_value: tensor([[0.6083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.581310559700636, distance: 1.439014860350447 entropy -3.93302303587271
epoch: 6, step: 95
	action: tensor([[-0.3621, -0.3095, -0.1261, -0.0057, -0.2922,  0.3860,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[0.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31848691043087696, distance: 1.3139976985655248 entropy -1.5707034372215065
epoch: 6, step: 96
	action: tensor([[-0.3554, -0.3597, -0.0898,  0.1178, -0.3069, -0.0235,  0.2436]],
       dtype=torch.float64)
	q_value: tensor([[0.5476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3494131370844338, distance: 1.3293188409129544 entropy -1.7437097148752057
epoch: 6, step: 97
	action: tensor([[-0.3592, -0.4020, -0.1286, -0.0177, -0.2914, -0.1408,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[0.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4520258071464347, distance: 1.3789352093286125 entropy -3.9599157318122358
epoch: 6, step: 98
	action: tensor([[-0.3607, -0.4212, -0.0768,  0.2393, -0.2832,  0.2152, -0.0561]],
       dtype=torch.float64)
	q_value: tensor([[0.5858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3029028321015912, distance: 1.306209106844628 entropy -1.9204819351029578
epoch: 6, step: 99
	action: tensor([[-0.3675, -0.4636, -0.1524, -0.0278, -0.2777,  0.1798,  0.1111]],
       dtype=torch.float64)
	q_value: tensor([[0.5744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4884910981876702, distance: 1.3961427136884128 entropy -3.9367307467056882
epoch: 6, step: 100
	action: tensor([[-0.3583, -0.4842, -0.0870, -0.0166, -0.3689, -0.2599,  0.0657]],
       dtype=torch.float64)
	q_value: tensor([[0.5646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5110409559434654, distance: 1.4066783753808383 entropy -1.6647194801095477
epoch: 6, step: 101
	action: tensor([[-0.3733, -0.3791, -0.1546, -0.0588, -0.3496,  0.5214,  0.0548]],
       dtype=torch.float64)
	q_value: tensor([[0.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3979660672304659, distance: 1.353022467299148 entropy -1.6507692405504641
epoch: 6, step: 102
	action: tensor([[-0.3694, -0.4756, -0.1561,  0.0553, -0.2555,  0.2224,  0.1385]],
       dtype=torch.float64)
	q_value: tensor([[0.5598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45772052073713665, distance: 1.3816365927211034 entropy -1.5187850114143526
epoch: 6, step: 103
	action: tensor([[-0.3600, -0.3766,  0.0016, -0.1076, -0.3448,  0.2190,  0.1097]],
       dtype=torch.float64)
	q_value: tensor([[0.5602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44247958761965034, distance: 1.3743948887151425 entropy -1.699552612678975
epoch: 6, step: 104
	action: tensor([[-0.3523, -0.4034, -0.1110, -0.0943, -0.4285, -0.3904, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[0.5513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44619918590680063, distance: 1.3761657649078356 entropy -1.7880037627249816
epoch: 6, step: 105
	action: tensor([[-0.3805, -0.2622, -0.0940, -0.0228, -0.1832,  0.2569, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[0.6179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3204802902810313, distance: 1.3149906197898245 entropy -1.7117927183220942
epoch: 6, step: 106
	action: tensor([[-0.3445, -0.1152, -0.1080,  0.0913, -0.2017, -0.0952,  0.1642]],
       dtype=torch.float64)
	q_value: tensor([[0.5427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1691216313406274, distance: 1.2373328445514422 entropy -3.9903644209727025
epoch: 6, step: 107
	action: tensor([[-0.3437, -0.1513, -0.1163, -0.0730, -0.2065, -0.1104, -0.1585]],
       dtype=torch.float64)
	q_value: tensor([[0.5323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2617658851321738, distance: 1.2854230513156968 entropy -3.998476129763803
epoch: 6, step: 108
	action: tensor([[-0.3459, -0.3426, -0.0972,  0.2579, -0.3464,  0.1091,  0.0618]],
       dtype=torch.float64)
	q_value: tensor([[0.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24131916805036835, distance: 1.2749654735649385 entropy -3.996871602165472
epoch: 6, step: 109
	action: tensor([[-0.3665, -0.3651, -0.1282,  0.1750, -0.3045,  0.3407, -0.0178]],
       dtype=torch.float64)
	q_value: tensor([[0.5641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29315152751048146, distance: 1.301311901750329 entropy -3.9404504738799586
epoch: 6, step: 110
	action: tensor([[-0.3656, -0.4604, -0.1080,  0.1824, -0.3146,  0.1300, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[0.5630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3930331941570784, distance: 1.3506332153712486 entropy -3.9402360936870573
epoch: 6, step: 111
	action: tensor([[-0.3689, -0.3112, -0.1181, -0.3136, -0.4419,  0.3263,  0.1929]],
       dtype=torch.float64)
	q_value: tensor([[0.5823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4816762713612033, distance: 1.3929430351799106 entropy -2.347081823371917
epoch: 6, step: 112
	action: tensor([[-0.3596, -0.3992, -0.1122,  0.3775, -0.2962,  0.1832, -0.0323]],
       dtype=torch.float64)
	q_value: tensor([[0.5503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22873275069360033, distance: 1.2684852171207635 entropy -1.6285043853212602
epoch: 6, step: 113
	action: tensor([[-0.3760, -0.2107, -0.0609,  0.4033, -0.3827,  0.4680,  0.1664]],
       dtype=torch.float64)
	q_value: tensor([[0.5796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.044666201544993456, distance: 1.1696218296423284 entropy -3.9200793736760198
epoch: 6, step: 114
	action: tensor([[-0.3801, -0.3746, -0.1045,  0.1611, -0.3174,  0.3295, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[0.5437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32179951943202734, distance: 1.3156473281501873 entropy -1.8037696026126646
epoch: 6, step: 115
	action: tensor([[-0.3660, -0.3952, -0.0931,  0.0749, -0.3124,  0.0124, -0.2009]],
       dtype=torch.float64)
	q_value: tensor([[0.5651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40657935197897177, distance: 1.3571842535730154 entropy -1.923496128105175
epoch: 6, step: 116
	action: tensor([[-0.3656, -0.3111, -0.1117,  0.1016, -0.3731, -0.3248,  0.1758]],
       dtype=torch.float64)
	q_value: tensor([[0.6018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33671602999838046, distance: 1.3230500433640542 entropy -1.9482046796721144
epoch: 6, step: 117
	action: tensor([[-0.3712, -0.4401, -0.1392,  0.0970, -0.4188,  0.0643,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[0.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.439891495003776, distance: 1.3731613675466392 entropy -3.9394929656631485
epoch: 6, step: 118
	action: tensor([[-0.3709, -0.0967, -0.1314,  0.2599, -0.3424,  0.4912,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[0.5830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05129306106065079, distance: 1.1733257239330819 entropy -1.7378968434205906
epoch: 6, step: 119
	action: tensor([[-0.3688, -0.4241, -0.1309,  0.0725, -0.2591, -0.2234,  0.1202]],
       dtype=torch.float64)
	q_value: tensor([[0.5407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4519894826481696, distance: 1.3789179612044267 entropy -3.941159196780558
epoch: 6, step: 120
	action: tensor([[-0.3653, -0.2312, -0.1001, -0.0519, -0.3243,  0.0998,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[0.5803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32502884940476373, distance: 1.3172534974083645 entropy -3.9510793774532753
epoch: 6, step: 121
	action: tensor([[-0.3477, -0.2822, -0.1096,  0.0946, -0.2685,  0.5518, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[0.5498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18932230426447472, distance: 1.247976694334925 entropy -3.9856833214407286
epoch: 6, step: 122
	action: tensor([[-0.3645, -0.4351, -0.1191,  0.1531, -0.2710, -0.0233,  0.2358]],
       dtype=torch.float64)
	q_value: tensor([[0.5481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4063084727989039, distance: 1.3570535639469048 entropy -1.6131805294933053
epoch: 6, step: 123
	action: tensor([[-0.3644, -0.4339, -0.0762,  0.1137, -0.2309, -0.2490,  0.0895]],
       dtype=torch.float64)
	q_value: tensor([[0.5583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4417414705477092, distance: 1.374043204660731 entropy -3.9480436821885663
epoch: 6, step: 124
	action: tensor([[-0.3664, -0.5100, -0.0902,  0.1068, -0.3178, -0.6823, -0.0406]],
       dtype=torch.float64)
	q_value: tensor([[0.5817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46852266277387966, distance: 1.3867463128229234 entropy -3.9487243148813875
epoch: 6, step: 125
	action: tensor([[-0.4021, -0.4200, -0.1250, -0.1830, -0.3467, -0.2386, -0.0231]],
       dtype=torch.float64)
	q_value: tensor([[0.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5589522872750361, distance: 1.4288054727900434 entropy -1.8219251904137794
epoch: 6, step: 126
	action: tensor([[-0.3674, -0.6210, -0.1172,  0.0182, -0.3121,  0.0795, -0.0651]],
       dtype=torch.float64)
	q_value: tensor([[0.6071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6020038755915211, distance: 1.4483998610132003 entropy -1.6349426047174231
epoch: 6, step: 127
	action: tensor([[-0.3790, -0.3404, -0.0963,  0.0638, -0.2895, -0.1035,  0.0825]],
       dtype=torch.float64)
	q_value: tensor([[0.6105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39512396655120985, distance: 1.351646402952832 entropy -1.4888053456209656
LOSS epoch 6 actor 0.5102308448821788 critic 31.0558224323609
epoch: 7, step: 0
	action: tensor([[-0.8465, -0.8909, -0.1522, -0.1477, -0.4732,  0.4935, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[0.2563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2721833750192517, distance: 1.7249574322761587 entropy -0.520134501521102
epoch: 7, step: 1
	action: tensor([[-1.1465, -1.0641, -0.2198,  0.5288, -0.6310,  0.9232,  0.1254]],
       dtype=torch.float64)
	q_value: tensor([[0.2827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3305067484364215, distance: 1.7469556339810435 entropy -0.24433467247458612
epoch: 7, step: 2
	action: tensor([[-1.2693, -1.2489, -0.2549,  0.7628, -0.5556,  0.4828, -0.1132]],
       dtype=torch.float64)
	q_value: tensor([[0.3034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7469556339810435 entropy -0.015557707171063069
epoch: 7, step: 3
	action: tensor([[-1.4498, -1.2956, -0.3364,  0.8419, -0.6726,  0.7255,  0.1132]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7469556339810435 entropy -0.016095208514745836
epoch: 7, step: 4
	action: tensor([[-1.3848, -1.0571, -0.4062,  0.5752, -0.6913,  0.3378,  0.0864]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.756611352300049, distance: 1.8999600245985433 entropy -0.016095208514745836
epoch: 7, step: 5
	action: tensor([[-1.3837, -0.5159, -0.0585,  0.5054, -0.5228,  0.0990,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.3098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5528317485908438, distance: 1.8283855883908253 entropy 0.021893975367438694
epoch: 7, step: 6
	action: tensor([[-1.3227, -0.9330, -0.1425,  0.2700, -0.6259, -0.4297,  0.2072]],
       dtype=torch.float64)
	q_value: tensor([[0.2760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5670232104930557, distance: 1.833460639487758 entropy -0.13939522905617358
epoch: 7, step: 7
	action: tensor([[-1.2259, -0.9131, -0.2740,  0.3170, -0.9601,  1.7286,  0.5806]],
       dtype=torch.float64)
	q_value: tensor([[0.2919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0984329579248104, distance: 1.6576935306754557 entropy -0.05428657869639058
epoch: 7, step: 8
	action: tensor([[-1.2684, -0.9768, -0.2622,  0.5445, -0.7208,  0.4943,  0.6233]],
       dtype=torch.float64)
	q_value: tensor([[0.3154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6130781527435873, distance: 1.849834575909556 entropy 0.08914661826675187
epoch: 7, step: 9
	action: tensor([[-1.4046, -1.3451, -0.1715,  0.4936, -0.9128,  0.1738,  0.1264]],
       dtype=torch.float64)
	q_value: tensor([[0.2796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.849834575909556 entropy -0.018783218308002718
epoch: 7, step: 10
	action: tensor([[-1.3990, -1.4694, -0.2981,  0.3754, -0.6428,  0.1493, -0.3909]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.849834575909556 entropy -0.016095208514745836
epoch: 7, step: 11
	action: tensor([[-1.4675, -1.1379, -0.2670,  0.6747, -0.6674, -0.1335,  1.2322]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.743690035718243, distance: 1.8955018659572564 entropy -0.016095208514745836
epoch: 7, step: 12
	action: tensor([[-1.4028, -1.3993, -0.3641,  0.1245, -0.9452,  0.6590,  0.3572]],
       dtype=torch.float64)
	q_value: tensor([[0.2883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8955018659572564 entropy 0.09982333653093653
epoch: 7, step: 13
	action: tensor([[-1.3526, -1.0650, -0.3743,  0.3667, -0.5890, -0.2992,  0.4065]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6548166170338976, distance: 1.8645496693334154 entropy -0.016095208514745836
epoch: 7, step: 14
	action: tensor([[-1.4920, -0.6054, -0.2518,  0.8170, -0.7160,  0.4834,  0.2939]],
       dtype=torch.float64)
	q_value: tensor([[0.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4504282022360195, distance: 1.7913386004831826 entropy -0.006749362005845892
epoch: 7, step: 15
	action: tensor([[-1.3819, -0.6601, -0.2461,  0.1347, -0.9300, -0.5734,  0.5313]],
       dtype=torch.float64)
	q_value: tensor([[0.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5160900492301295, distance: 1.8151803577361452 entropy -0.00683628940220639
epoch: 7, step: 16
	action: tensor([[-1.2690, -1.2242, -0.2290,  0.2601, -0.8123,  0.4138,  0.1298]],
       dtype=torch.float64)
	q_value: tensor([[0.3015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6541688879535736, distance: 1.864322196612771 entropy -0.02287821291860375
epoch: 7, step: 17
	action: tensor([[-1.3828, -1.4896, -0.0234, -0.6089, -1.1545,  0.7579,  0.2400]],
       dtype=torch.float64)
	q_value: tensor([[0.3081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.864322196612771 entropy 0.01332532317647964
epoch: 7, step: 18
	action: tensor([[-1.3364, -1.3209, -0.1295, -0.0597, -0.8134, -0.0315, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.864322196612771 entropy -0.016095208514745836
epoch: 7, step: 19
	action: tensor([[-1.3159, -0.9270, -0.2919,  0.1832, -0.5694,  0.1158, -0.6016]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.726016400426026, distance: 1.8893870112553923 entropy -0.016095208514745836
epoch: 7, step: 20
	action: tensor([[-1.5818, -0.7992, -0.1219, -0.3141, -0.9154,  0.7606,  0.2529]],
       dtype=torch.float64)
	q_value: tensor([[0.3319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8130157676242793, distance: 1.919299616465944 entropy -0.049260091302998306
epoch: 7, step: 21
	action: tensor([[-1.3476, -1.5653, -0.2575, -0.4003, -0.5966, -0.6999,  0.0407]],
       dtype=torch.float64)
	q_value: tensor([[0.3163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.919299616465944 entropy -0.02507705157038823
epoch: 7, step: 22
	action: tensor([[-1.1974, -0.3866, -0.2637, -0.4393, -0.9658,  0.0248, -0.1118]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4561763238679255, distance: 1.7934383969070242 entropy -0.016095208514745836
epoch: 7, step: 23
	action: tensor([[-1.2953, -0.5672, -0.1248,  0.7976, -0.6286, -1.3217, -0.1528]],
       dtype=torch.float64)
	q_value: tensor([[0.3182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0629258750007997, distance: 1.6436089779786014 entropy -0.17711875694843124
epoch: 7, step: 24
	action: tensor([[-1.5169, -0.7489, -0.2270,  0.1168, -0.9015,  1.4894,  0.3367]],
       dtype=torch.float64)
	q_value: tensor([[0.3230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4212938588580446, distance: 1.7806577061175461 entropy 0.036990061748601075
epoch: 7, step: 25
	action: tensor([[-1.5055, -1.0435, -0.3733,  0.6233, -0.5199, -0.1992, -0.8716]],
       dtype=torch.float64)
	q_value: tensor([[0.3158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8187372191025553, distance: 1.921250476769111 entropy 0.04899851381924062
epoch: 7, step: 26
	action: tensor([[-1.8161, -1.1714, -0.2762,  0.6870, -0.7487,  0.2026, -0.2274]],
       dtype=torch.float64)
	q_value: tensor([[0.3609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.921250476769111 entropy 0.0838297892603403
epoch: 7, step: 27
	action: tensor([[-1.2397, -0.7892, -0.0492,  0.3905, -0.7014,  0.2883,  0.2823]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5787038925194206, distance: 1.837627287382732 entropy -0.016095208514745836
epoch: 7, step: 28
	action: tensor([[-1.2670, -0.9381, -0.3668,  0.1393, -0.6912, -0.1996,  0.8308]],
       dtype=torch.float64)
	q_value: tensor([[0.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5997125048773602, distance: 1.8450976463661724 entropy -0.10721195281964858
epoch: 7, step: 29
	action: tensor([[-1.4219, -0.7861, -0.2926, -0.0896, -0.9534,  0.2121, -0.6106]],
       dtype=torch.float64)
	q_value: tensor([[0.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7936191739419924, distance: 1.9126710948958474 entropy -0.04092718336855973
epoch: 7, step: 30
	action: tensor([[-1.4245, -0.8641, -0.4139,  0.7474, -0.9110,  1.1055,  0.5913]],
       dtype=torch.float64)
	q_value: tensor([[0.3484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3934298455923018, distance: 1.77038224081987 entropy -0.00639497740038264
epoch: 7, step: 31
	action: tensor([[-1.5429, -1.5855, -0.3319, -0.0927, -0.9190,  1.3130, -0.2427]],
       dtype=torch.float64)
	q_value: tensor([[0.3102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.77038224081987 entropy 0.09272947343466677
epoch: 7, step: 32
	action: tensor([[-1.3335, -0.8800, -0.2692,  0.5350, -0.4993,  0.0578,  0.4777]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6905832420595286, distance: 1.8770675840561954 entropy -0.016095208514745836
epoch: 7, step: 33
	action: tensor([[-1.4190, -0.6496, -0.2900,  0.4225, -0.8748,  1.4710,  0.2029]],
       dtype=torch.float64)
	q_value: tensor([[0.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2529811515557423, distance: 1.7176531591303617 entropy -0.06977747156148388
epoch: 7, step: 34
	action: tensor([[-1.4753, -1.1922, -0.1898, -0.3312, -0.3595,  0.5428,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[0.3122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5558750176839524, distance: 1.8294750867547382 entropy 0.04904621914551076
epoch: 7, step: 35
	action: tensor([[-1.3624, -1.1399, -0.1874,  0.0779, -0.8259, -0.0128,  0.3559]],
       dtype=torch.float64)
	q_value: tensor([[0.3129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6318781804866727, distance: 1.8564770511557578 entropy -0.05236902784201304
epoch: 7, step: 36
	action: tensor([[-1.4165, -0.9285, -0.3437, -0.1086, -0.7843,  0.9485,  0.8449]],
       dtype=torch.float64)
	q_value: tensor([[0.2985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.687008650958587, distance: 1.8758202747315387 entropy -0.0030509476747465464
epoch: 7, step: 37
	action: tensor([[-1.2566, -1.0171, -0.1908,  0.1307, -0.6262,  0.6938, -0.3455]],
       dtype=torch.float64)
	q_value: tensor([[0.3002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6340558881477083, distance: 1.8572449492239402 entropy -0.015740894996703503
epoch: 7, step: 38
	action: tensor([[-1.2337, -0.2773, -0.2003, -0.1712, -0.7636,  0.2628, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[0.3208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5597942828644173, distance: 1.8308772389151398 entropy -0.04823989206908319
epoch: 7, step: 39
	action: tensor([[-1.2453, -1.0171, -0.1573, -0.0082, -0.1841, -0.5700,  0.6424]],
       dtype=torch.float64)
	q_value: tensor([[0.2934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3725644294630919, distance: 1.7626484391913058 entropy -0.22832994822508507
epoch: 7, step: 40
	action: tensor([[-1.1553, -1.1211, -0.2453,  0.3440, -0.8275, -0.1149,  0.5496]],
       dtype=torch.float64)
	q_value: tensor([[0.2707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5331777059212195, distance: 1.8213336936844153 entropy -0.08845929306262006
epoch: 7, step: 41
	action: tensor([[-1.5341, -1.1541, -0.3261,  0.2801, -0.5643,  0.6626,  0.2758]],
       dtype=torch.float64)
	q_value: tensor([[0.2890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7734054039414704, distance: 1.905738781921455 entropy -0.01778569891410488
epoch: 7, step: 42
	action: tensor([[-1.1624, -1.3664, -0.2958,  0.2492, -0.4060,  0.2355,  0.3881]],
       dtype=torch.float64)
	q_value: tensor([[0.3099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.905738781921455 entropy 0.02261943290322364
epoch: 7, step: 43
	action: tensor([[-1.1819, -1.2623, -0.2990, -0.3419, -0.6262,  0.4371,  0.0787]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.905738781921455 entropy -0.016095208514745836
epoch: 7, step: 44
	action: tensor([[-1.3921, -1.3246, -0.3351,  0.1560, -0.8030,  1.5200, -0.5684]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.905738781921455 entropy -0.016095208514745836
epoch: 7, step: 45
	action: tensor([[-1.4246, -0.9819, -0.2623, -0.0600, -0.7338,  0.4768, -0.1728]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7680363186960562, distance: 1.9038932113326412 entropy -0.016095208514745836
epoch: 7, step: 46
	action: tensor([[-1.3215, -0.9239, -0.3194,  0.5307, -0.2792,  0.4608, -0.2506]],
       dtype=torch.float64)
	q_value: tensor([[0.3204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.619632132609805, distance: 1.8521529501270404 entropy -0.03309211944993488
epoch: 7, step: 47
	action: tensor([[-1.4977, -0.9295, -0.2435, -0.0145, -0.5507, -0.1628,  0.5917]],
       dtype=torch.float64)
	q_value: tensor([[0.3140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.721103670020086, distance: 1.887683750274456 entropy -0.06922637373657939
epoch: 7, step: 48
	action: tensor([[-1.0905, -0.9704, -0.2321,  0.0055, -0.5591, -0.0376,  0.4692]],
       dtype=torch.float64)
	q_value: tensor([[0.2888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.473742358482904, distance: 1.7998401105105428 entropy -0.044839002067006625
epoch: 7, step: 49
	action: tensor([[-1.2016, -0.9360, -0.1915,  0.1227, -0.5427,  0.0230, -0.3813]],
       dtype=torch.float64)
	q_value: tensor([[0.2730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6017760911800707, distance: 1.8458297971222226 entropy -0.14254845311516465
epoch: 7, step: 50
	action: tensor([[-1.2496, -1.0710, -0.2276,  0.2958, -0.7074,  0.2559,  0.2882]],
       dtype=torch.float64)
	q_value: tensor([[0.3142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6705834004757247, distance: 1.8700781927852859 entropy -0.10469433849018908
epoch: 7, step: 51
	action: tensor([[-1.4028, -0.8804, -0.4008,  0.1584, -0.6232,  0.3140,  0.2750]],
       dtype=torch.float64)
	q_value: tensor([[0.2876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.819963468048619, distance: 1.921668336792378 entropy -0.04433714726087745
epoch: 7, step: 52
	action: tensor([[-1.2892, -0.6455, -0.2071,  0.1887, -0.6990, -0.9847,  0.0469]],
       dtype=torch.float64)
	q_value: tensor([[0.2980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2319529671396712, distance: 1.7096185152550134 entropy -0.06932073379831302
epoch: 7, step: 53
	action: tensor([[-1.2802, -1.2519, -0.2144,  1.0818, -0.7571,  0.0563, -0.2111]],
       dtype=torch.float64)
	q_value: tensor([[0.3114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7096185152550134 entropy -0.040069301311212176
epoch: 7, step: 54
	action: tensor([[-1.4284, -0.9028, -0.2712,  0.3910, -0.6436,  1.2738, -0.1581]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3985953977104506, distance: 1.7722916497864742 entropy -0.016095208514745836
epoch: 7, step: 55
	action: tensor([[-1.5685, -0.8854, -0.0856,  0.9974, -0.7048,  0.3391,  0.1297]],
       dtype=torch.float64)
	q_value: tensor([[0.3223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.479876647841408, distance: 1.802070315259597 entropy 0.0406442875685769
epoch: 7, step: 56
	action: tensor([[-1.4501, -1.1000, -0.1927,  0.5132, -0.8822, -0.2370,  0.8644]],
       dtype=torch.float64)
	q_value: tensor([[0.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7101457311007602, distance: 1.8838790458909374 entropy 0.06770894828553922
epoch: 7, step: 57
	action: tensor([[-1.3656, -0.8966, -0.1485,  0.3506, -0.8063,  0.9679,  0.3113]],
       dtype=torch.float64)
	q_value: tensor([[0.2940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.566057197795533, distance: 1.8331156264566957 entropy 0.07236197094841741
epoch: 7, step: 58
	action: tensor([[-1.4880, -0.5414, -0.2930, -0.0686, -0.6786,  0.6298,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[0.2947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8293183269219417, distance: 1.9248531389967265 entropy -0.004086702238961681
epoch: 7, step: 59
	action: tensor([[-1.2836, -0.6518, -0.4432,  0.2283, -0.7456,  0.3824,  0.2538]],
       dtype=torch.float64)
	q_value: tensor([[0.3031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7143674347709354, distance: 1.8853457722285214 entropy -0.11404969308296896
epoch: 7, step: 60
	action: tensor([[-1.2281, -0.8827, -0.2503,  0.5471, -0.5043,  0.1222,  0.2845]],
       dtype=torch.float64)
	q_value: tensor([[0.2933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5857258952350155, distance: 1.8401275843849443 entropy -0.11025261569292587
epoch: 7, step: 61
	action: tensor([[-1.1623, -0.7123, -0.3108,  0.5065, -0.4801,  0.0147,  0.2600]],
       dtype=torch.float64)
	q_value: tensor([[0.2802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5052485766863293, distance: 1.811265459587629 entropy -0.0955161002708326
epoch: 7, step: 62
	action: tensor([[-1.2561, -0.7696, -0.4301,  0.3405, -0.6597,  0.3084,  0.6352]],
       dtype=torch.float64)
	q_value: tensor([[0.2771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.694073436523726, distance: 1.878284645024059 entropy -0.14903156533463996
epoch: 7, step: 63
	action: tensor([[-1.2773, -1.2498, -0.3264,  0.1499, -0.4885,  0.6393, -0.1238]],
       dtype=torch.float64)
	q_value: tensor([[0.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.878284645024059 entropy -0.0914495082399924
epoch: 7, step: 64
	action: tensor([[-1.1140, -1.4016, -0.4630,  0.5550, -0.6218, -0.4377,  0.2791]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.878284645024059 entropy -0.016095208514745836
epoch: 7, step: 65
	action: tensor([[-1.5125, -1.1314, -0.2503, -0.3891, -0.8097, -0.0701,  1.0932]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3690654756059382, distance: 1.7613482213723817 entropy -0.016095208514745836
epoch: 7, step: 66
	action: tensor([[-1.2986, -1.0959, -0.2287,  0.6080, -0.7608, -0.4003,  0.1117]],
       dtype=torch.float64)
	q_value: tensor([[0.3062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5510292832558847, distance: 1.8277399948109516 entropy 0.04838937472236874
epoch: 7, step: 67
	action: tensor([[-1.3236, -0.8016, -0.3618,  0.2922, -0.7110,  0.4927,  0.6482]],
       dtype=torch.float64)
	q_value: tensor([[0.3106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7308534402262366, distance: 1.8910625311986482 entropy 0.029251621329804602
epoch: 7, step: 68
	action: tensor([[-1.3715, -0.7867, -0.0816,  0.0173, -0.6668,  0.0052,  0.4900]],
       dtype=torch.float64)
	q_value: tensor([[0.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7341526473519888, distance: 1.8922045045560618 entropy -0.06800358221866451
epoch: 7, step: 69
	action: tensor([[-1.0406, -1.1410, -0.2982, -0.1629, -0.6860,  0.0791,  0.2128]],
       dtype=torch.float64)
	q_value: tensor([[0.2726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.375273156937646, distance: 1.7636543491292254 entropy -0.1119239853037425
epoch: 7, step: 70
	action: tensor([[-1.3044, -1.1606, -0.1023,  0.8463, -0.8537, -0.1547,  0.6047]],
       dtype=torch.float64)
	q_value: tensor([[0.3010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5001910703342944, distance: 1.8094362769387207 entropy -0.0982875890937137
epoch: 7, step: 71
	action: tensor([[-1.3831, -1.6302, -0.3769,  0.6781, -0.9488, -0.6168,  0.1911]],
       dtype=torch.float64)
	q_value: tensor([[0.2910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8094362769387207 entropy 0.06800977822734088
epoch: 7, step: 72
	action: tensor([[-1.4927, -1.0056, -0.2564,  0.0694, -0.4946, -0.0711,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.750394069466429, distance: 1.897816222769402 entropy -0.016095208514745836
epoch: 7, step: 73
	action: tensor([[-1.4734, -1.0776, -0.2676, -0.1568, -0.6043,  0.7379,  0.4284]],
       dtype=torch.float64)
	q_value: tensor([[0.2977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7009658726215835, distance: 1.8806857836034083 entropy -0.04411296357063473
epoch: 7, step: 74
	action: tensor([[-1.4756, -1.1650, -0.1444,  0.3251, -0.9301,  0.5046, -0.4052]],
       dtype=torch.float64)
	q_value: tensor([[0.3066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7728456385698959, distance: 1.9055464515090785 entropy -0.030798055967766143
epoch: 7, step: 75
	action: tensor([[-1.6981, -0.6826, -0.2210,  0.6868, -0.5802,  0.4369, -0.3145]],
       dtype=torch.float64)
	q_value: tensor([[0.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9055464515090785 entropy 0.08437355099394446
epoch: 7, step: 76
	action: tensor([[-1.5676, -1.0274, -0.1362,  0.3738, -0.8134,  0.9915,  0.1496]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6592416701636634, distance: 1.866102939731795 entropy -0.016095208514745836
epoch: 7, step: 77
	action: tensor([[-1.5030, -1.2213, -0.2528, -0.0836, -0.9152,  0.2213,  0.5008]],
       dtype=torch.float64)
	q_value: tensor([[0.3159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6250030180134054, distance: 1.854050660789826 entropy 0.06660605013075226
epoch: 7, step: 78
	action: tensor([[-1.3954, -1.2085, -0.3015,  0.7242, -0.9602,  0.8862,  0.3985]],
       dtype=torch.float64)
	q_value: tensor([[0.3189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.538996676631431, distance: 1.823424389499534 entropy 0.046820067782954035
epoch: 7, step: 79
	action: tensor([[-1.4869, -0.8706, -0.4976,  0.8577, -0.8822,  0.0141,  0.3088]],
       dtype=torch.float64)
	q_value: tensor([[0.3192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7425442872972177, distance: 1.8951060495533656 entropy 0.12609373629522505
epoch: 7, step: 80
	action: tensor([[-1.6145, -1.2455, -0.2448,  0.6357, -0.7187,  0.9125, -0.1737]],
       dtype=torch.float64)
	q_value: tensor([[0.3213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8951060495533656 entropy 0.07795420433186262
epoch: 7, step: 81
	action: tensor([[-1.3303, -0.6684, -0.3275,  0.2575, -0.7280, -0.8884,  0.8089]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3394689114408385, distance: 1.7503114444572172 entropy -0.016095208514745836
epoch: 7, step: 82
	action: tensor([[-1.3283, -0.8623, -0.2658,  0.3655, -0.8301,  0.8391,  0.2230]],
       dtype=torch.float64)
	q_value: tensor([[0.3060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6190948705255392, distance: 1.8519630107519265 entropy 0.005549288253164729
epoch: 7, step: 83
	action: tensor([[-1.4990, -0.7390, -0.1524,  0.1742, -0.8797,  0.8867, -0.2769]],
       dtype=torch.float64)
	q_value: tensor([[0.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7362706276522362, distance: 1.8929372500464294 entropy -0.015875772209780363
epoch: 7, step: 84
	action: tensor([[-1.4261, -1.1236, -0.2421,  0.2604, -0.7736,  0.7148,  0.0676]],
       dtype=torch.float64)
	q_value: tensor([[0.3232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.723802123243522, distance: 1.888619503683324 entropy 0.0016790768677080895
epoch: 7, step: 85
	action: tensor([[-1.4549, -1.3932, -0.4131,  0.4053, -0.9344,  0.1421,  0.1507]],
       dtype=torch.float64)
	q_value: tensor([[0.3160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.888619503683324 entropy 0.028585519814150282
epoch: 7, step: 86
	action: tensor([[-1.3571, -1.4252, -0.2036, -0.1585, -0.4549, -0.3961, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.888619503683324 entropy -0.016095208514745836
epoch: 7, step: 87
	action: tensor([[-1.4587, -1.0452, -0.3532,  0.8033, -0.5272, -0.7678,  0.2116]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5941487794892253, distance: 1.843122213520097 entropy -0.016095208514745836
epoch: 7, step: 88
	action: tensor([[-1.6225, -1.0284, -0.2657,  0.0357, -0.4914, -1.3793, -0.2286]],
       dtype=torch.float64)
	q_value: tensor([[0.3189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9577879927250685, distance: 1.6011776654542451 entropy 0.0777752768651314
epoch: 7, step: 89
	action: tensor([[-1.6644, -1.3163, -0.3299,  0.2446, -1.0238, -0.0218,  1.0611]],
       dtype=torch.float64)
	q_value: tensor([[0.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6011776654542451 entropy 0.1218861662553468
epoch: 7, step: 90
	action: tensor([[-1.3496, -0.7610, -0.1912,  0.1209, -0.9362,  0.3666, -0.4443]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7797459728004785, distance: 1.9079159914855286 entropy -0.016095208514745836
epoch: 7, step: 91
	action: tensor([[-1.4825, -1.1077, -0.2978,  0.0987, -0.5633,  0.2514,  0.4059]],
       dtype=torch.float64)
	q_value: tensor([[0.3277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.779660012871323, distance: 1.907886491381384 entropy -0.03184327639579822
epoch: 7, step: 92
	action: tensor([[-1.4631, -0.8261, -0.3111,  0.0910, -0.7416,  0.6000,  0.2206]],
       dtype=torch.float64)
	q_value: tensor([[0.2975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.830639691944746, distance: 1.9253025646931334 entropy -0.021406960727949804
epoch: 7, step: 93
	action: tensor([[-1.4172, -0.8883, -0.1217,  0.8796, -0.8440, -0.8117,  0.8783]],
       dtype=torch.float64)
	q_value: tensor([[0.3018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3739580538748646, distance: 1.763166045634821 entropy -0.05070272803620484
epoch: 7, step: 94
	action: tensor([[-1.2755, -0.9924, -0.1604,  0.4516, -0.7049,  1.9210,  0.0754]],
       dtype=torch.float64)
	q_value: tensor([[0.2962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8692420231062117, distance: 1.5645500516917572 entropy 0.10280420868204658
epoch: 7, step: 95
	action: tensor([[-1.4783, -0.4946, -0.2687,  0.2959, -0.8915,  0.6563,  0.2150]],
       dtype=torch.float64)
	q_value: tensor([[0.3338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7208417601196087, distance: 1.8875929020341027 entropy 0.12273321245529101
epoch: 7, step: 96
	action: tensor([[-1.3705, -1.0235, -0.1771,  0.4085, -0.7980, -0.2591,  0.7333]],
       dtype=torch.float64)
	q_value: tensor([[0.2984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6601003889868053, distance: 1.8664042151655484 entropy -0.05841665114839819
epoch: 7, step: 97
	action: tensor([[-1.5557, -0.7921, -0.3191,  0.2196, -0.5970, -0.0244, -0.3498]],
       dtype=torch.float64)
	q_value: tensor([[0.2879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9050302617787915, distance: 1.9504374343206148 entropy 0.015688361679225382
epoch: 7, step: 98
	action: tensor([[-1.3569, -1.2302, -0.4054,  0.8523, -0.8604,  0.3450,  0.3778]],
       dtype=torch.float64)
	q_value: tensor([[0.3220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6512312502534283, distance: 1.8632901938190394 entropy -0.0236159201680805
epoch: 7, step: 99
	action: tensor([[-1.6338, -1.0249, -0.2009, -0.6969, -0.9180,  0.3998,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[0.3175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8632901938190394 entropy 0.1064177807101689
epoch: 7, step: 100
	action: tensor([[-1.4386, -0.8015, -0.3452,  0.2705, -0.6312, -0.1869,  0.2904]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.786776373494459, distance: 1.910327172868378 entropy -0.016095208514745836
epoch: 7, step: 101
	action: tensor([[-1.3932, -0.6825, -0.2629,  0.3464, -0.9809, -0.2839,  0.2283]],
       dtype=torch.float64)
	q_value: tensor([[0.2947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6781162268791792, distance: 1.8727137694731177 entropy -0.05169219901170942
epoch: 7, step: 102
	action: tensor([[-1.5487e+00, -6.3681e-01, -3.8450e-01, -2.1067e-01, -9.3697e-01,
          6.7261e-02, -1.6568e-04]], dtype=torch.float64)
	q_value: tensor([[0.3027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8324903088890059, distance: 1.9259318244669792 entropy -0.02023984417215677
epoch: 7, step: 103
	action: tensor([[-1.4456, -1.4991, -0.3260, -0.2424, -0.8281, -0.8919,  0.3347]],
       dtype=torch.float64)
	q_value: tensor([[0.3259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9259318244669792 entropy -0.03976996750964247
epoch: 7, step: 104
	action: tensor([[-1.2741, -0.7395, -0.2896,  0.3889, -0.5543, -0.6765,  0.3845]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4614950127050856, distance: 1.7953791334887392 entropy -0.016095208514745836
epoch: 7, step: 105
	action: tensor([[-1.3348, -0.9851, -0.1644,  0.3796, -0.7823,  0.3753,  0.1031]],
       dtype=torch.float64)
	q_value: tensor([[0.2935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7165103725068516, distance: 1.8860898464193945 entropy -0.06153303467566616
epoch: 7, step: 106
	action: tensor([[-1.3044, -0.9028, -0.2786, -0.0098, -0.7290,  0.1419,  1.0289]],
       dtype=torch.float64)
	q_value: tensor([[0.2950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6929182884435807, distance: 1.8778819222364365 entropy -0.022936126571916984
epoch: 7, step: 107
	action: tensor([[-1.5002, -0.5934, -0.3066,  0.3739, -0.7008, -0.7714, -0.1450]],
       dtype=torch.float64)
	q_value: tensor([[0.2773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5847454431260997, distance: 1.8397786827416953 entropy -0.05109087564171201
epoch: 7, step: 108
	action: tensor([[-1.3619, -1.0010, -0.2663,  0.5391, -0.5415, -0.1259, -0.4890]],
       dtype=torch.float64)
	q_value: tensor([[0.3209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7135094313777373, distance: 1.885047772609596 entropy -0.004528507992079378
epoch: 7, step: 109
	action: tensor([[-1.4944, -1.3866, -0.1807, -0.0180, -0.3886,  0.0632, -0.5016]],
       dtype=torch.float64)
	q_value: tensor([[0.3307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.885047772609596 entropy -0.001728786243528743
epoch: 7, step: 110
	action: tensor([[-1.2979, -0.8945, -0.4117,  0.5813, -0.5512, -0.1544,  0.7987]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6807504371087196, distance: 1.8736345490221282 entropy -0.016095208514745836
epoch: 7, step: 111
	action: tensor([[-1.3345, -0.6782, -0.2945, -0.0479, -0.8392,  0.4809,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[0.2856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7592061417386158, distance: 1.900854027411037 entropy -0.02474140101474509
epoch: 7, step: 112
	action: tensor([[-1.4798, -1.0692, -0.1964,  0.4436, -0.7413, -0.0880,  0.2636]],
       dtype=torch.float64)
	q_value: tensor([[0.3030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7823449626246548, distance: 1.9088077088914106 entropy -0.10266884961299863
epoch: 7, step: 113
	action: tensor([[-1.3599, -0.7165, -0.1321, -0.3596, -0.6677,  1.1299,  0.4022]],
       dtype=torch.float64)
	q_value: tensor([[0.2995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5950602625162547, distance: 1.8434459858272454 entropy 0.025928121408905817
epoch: 7, step: 114
	action: tensor([[-1.3293, -1.1757, -0.3162,  0.5778, -0.5715,  0.7650,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[0.2956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5631325563014569, distance: 1.8320706897932841 entropy -0.09858117355568329
epoch: 7, step: 115
	action: tensor([[-1.5215, -0.1774, -0.3769,  0.3931, -0.9248,  0.1642,  0.3373]],
       dtype=torch.float64)
	q_value: tensor([[0.3158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6248020401741154, distance: 1.853979683685089 entropy 0.033258896750374745
epoch: 7, step: 116
	action: tensor([[-1.5553, -1.0183, -0.2185,  0.4140, -0.6291, -0.8561,  0.8054]],
       dtype=torch.float64)
	q_value: tensor([[0.3010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5248546646041539, distance: 1.8183391331907497 entropy -0.07788866256809397
epoch: 7, step: 117
	action: tensor([[-1.5590, -1.3839, -0.3336, -0.1304, -0.7922,  0.2403,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[0.3094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8183391331907497 entropy 0.0916518880656935
epoch: 7, step: 118
	action: tensor([[-1.2766, -1.3272, -0.3428,  0.0718, -0.7970, -0.0465, -0.0683]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8183391331907497 entropy -0.016095208514745836
epoch: 7, step: 119
	action: tensor([[-1.5258, -0.9616, -0.3819,  0.2144, -0.8951, -0.0758,  0.1737]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8398426623258621, distance: 1.9284297949621958 entropy -0.016095208514745836
epoch: 7, step: 120
	action: tensor([[-1.3098, -0.7126, -0.2169,  0.4192, -0.7249,  0.3223,  0.2950]],
       dtype=torch.float64)
	q_value: tensor([[0.3177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6490914747275158, distance: 1.8625381229561422 entropy 0.03422734131089977
epoch: 7, step: 121
	action: tensor([[-1.3121, -1.0060, -0.2597, -0.1513, -0.6093,  0.1884,  0.2468]],
       dtype=torch.float64)
	q_value: tensor([[0.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.621602245485144, distance: 1.8528492815950828 entropy -0.09101154873790128
epoch: 7, step: 122
	action: tensor([[-1.1875, -1.0008, -0.3528, -0.1115, -0.7017,  0.0541,  0.7784]],
       dtype=torch.float64)
	q_value: tensor([[0.2942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5191648145930379, distance: 1.8162891315663743 entropy -0.0940193094191876
epoch: 7, step: 123
	action: tensor([[-1.2728, -0.8025, -0.0547,  0.9481, -0.5487,  1.6063, -0.0913]],
       dtype=torch.float64)
	q_value: tensor([[0.2862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6174466989349328, distance: 1.4553641817953489 entropy -0.07598068686107907
epoch: 7, step: 124
	action: tensor([[-1.6223, -1.3023, -0.2898, -0.2093, -0.4348, -0.6361,  0.6050]],
       dtype=torch.float64)
	q_value: tensor([[0.3328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4553641817953489 entropy 0.10018985589090394
epoch: 7, step: 125
	action: tensor([[-1.3928, -1.0336, -0.3068,  0.3118, -0.6568,  0.4343,  0.2321]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7761989239627503, distance: 1.9066983206039578 entropy -0.016095208514745836
epoch: 7, step: 126
	action: tensor([[-1.4534, -1.2546, -0.3832,  0.4073, -0.9389,  0.2069, -0.2664]],
       dtype=torch.float64)
	q_value: tensor([[0.2988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy -0.023504869495896457
epoch: 7, step: 127
	action: tensor([[-1.4849, -1.3639, -0.2733,  0.0337, -0.6614,  0.1023,  0.6919]],
       dtype=torch.float64)
	q_value: tensor([[0.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy -0.016095208514745836
LOSS epoch 7 actor 298.6239368917278 critic 2210.9101874732487
epoch: 8, step: 0
	action: tensor([[-2.1933, -1.2423, -0.2362,  0.2518, -1.1787,  0.8402, -0.8685]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 1
	action: tensor([[-2.1207, -1.5963, -0.2924,  0.2320, -0.8916,  1.0243, -0.4894]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 2
	action: tensor([[-2.2307, -1.0761,  0.0509, -0.5131, -0.7949,  1.9071,  0.8576]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 3
	action: tensor([[-2.6376, -1.8375, -0.3196,  0.3490, -0.6800,  0.1761, -0.2065]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 4
	action: tensor([[-2.3277, -1.6113, -0.4434,  0.5548, -0.9034,  1.2319,  0.1778]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 5
	action: tensor([[-2.7516, -1.7733, -0.3343, -0.0630, -1.1192, -0.3921,  0.8822]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 6
	action: tensor([[-2.2611e+00, -1.3717e+00, -1.0205e-03,  1.3220e+00, -7.7613e-01,
         -6.1493e-01,  6.2931e-02]], dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 7
	action: tensor([[-2.2957e+00, -7.8027e-01, -4.2819e-01,  8.0875e-04, -4.9101e-01,
          4.9679e-01,  9.6998e-01]], dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 8
	action: tensor([[-2.6441, -1.6372, -0.4671, -0.9576, -0.7678,  0.2702,  1.2494]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 9
	action: tensor([[-2.3957, -1.9213, -0.4970,  1.0058, -0.7367,  1.5026,  0.0886]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 10
	action: tensor([[-2.4122, -1.3650, -0.2094,  0.0843, -1.0056,  0.9109, -0.2694]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 11
	action: tensor([[-2.0144, -1.8463, -0.4843,  0.2173, -0.4114, -0.9263,  0.6444]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 12
	action: tensor([[-2.1433, -1.0873, -0.6392,  1.4905, -1.1089,  0.3917, -0.6441]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 13
	action: tensor([[-2.2593, -1.0344, -0.4935,  1.1904, -0.6501,  0.2296,  1.1892]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 14
	action: tensor([[-2.2374, -0.9470, -0.3571,  1.1822, -1.1922, -0.7682,  0.4480]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 15
	action: tensor([[-2.2446, -0.9376, -0.5068,  0.3943, -1.0274, -0.6279,  0.4831]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 16
	action: tensor([[-2.1556, -1.3200, -0.4157,  0.4125, -0.9780,  0.4722,  0.5595]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 17
	action: tensor([[-1.9843, -0.7039, -0.4048,  0.1594, -0.7926,  0.5257, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 18
	action: tensor([[-2.1681, -1.5909, -0.0735, -0.0497, -1.0274,  0.6834, -0.0673]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 19
	action: tensor([[-2.2015, -1.8797, -0.4290,  1.1573, -1.2791,  0.0769, -0.3394]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 20
	action: tensor([[-2.3364, -1.6074, -0.2152,  1.3864, -1.0444,  0.1628, -0.6451]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 21
	action: tensor([[-2.1260, -1.0077, -0.4233, -0.1516, -1.0850, -0.3282, -0.7071]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 22
	action: tensor([[-1.9263, -2.3102,  0.0828,  0.3998, -0.7142, -0.4846,  0.8679]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 23
	action: tensor([[-2.4442, -0.8068, -0.2705, -0.2801, -0.8880,  2.3886,  0.3781]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 24
	action: tensor([[-2.5812, -0.8855, -0.5024, -0.7926, -1.5611,  2.9759, -0.2754]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 25
	action: tensor([[-2.4198, -2.1465, -0.3201,  0.7497, -0.9296, -0.6772,  2.0942]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 26
	action: tensor([[-2.0497, -1.3216, -0.1095,  1.4906, -0.7572,  1.1348, -0.4731]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 27
	action: tensor([[-2.2610, -1.2193, -0.0329,  0.0948, -0.6970,  0.3519,  0.0653]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 28
	action: tensor([[-2.0986, -1.1828, -0.4388,  1.1762, -1.1036, -0.8311,  0.6238]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 29
	action: tensor([[-2.6891, -1.0750, -0.0163, -0.0499, -1.1329,  1.6205,  1.1572]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 30
	action: tensor([[-1.9756, -1.4879, -0.0024, -0.1536, -0.9599, -0.3169,  1.1708]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 31
	action: tensor([[-2.2593, -1.6192, -0.4967,  1.0346, -1.3402,  0.3753,  0.5976]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 32
	action: tensor([[-2.0788, -0.8233, -0.3980,  0.6414, -1.1680,  0.2806,  0.7833]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 33
	action: tensor([[-2.9015, -1.8158, -0.3222,  0.1187, -0.9322,  0.6106,  0.8155]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 34
	action: tensor([[-2.4698, -0.8145, -0.0513,  1.1962, -1.1464,  0.1678,  0.2036]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 35
	action: tensor([[-2.2580, -1.4645, -0.4776, -0.0371, -1.4098,  0.3679, -0.1069]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 36
	action: tensor([[-2.8114, -2.0996, -0.0986,  0.9051, -1.5932,  0.4487, -0.4847]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 37
	action: tensor([[-2.1203, -1.0938, -0.2413,  1.0622, -0.6999,  1.0504, -0.2708]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 38
	action: tensor([[-2.4051, -1.5508, -0.3676,  1.2815, -1.0966,  1.2476,  0.4861]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 39
	action: tensor([[-2.1382, -0.9556,  0.1046, -0.3402, -1.3154, -0.5713,  1.1667]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 40
	action: tensor([[-2.7162, -1.3227, -0.2282,  1.6764, -0.7045,  0.1852,  0.4202]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 41
	action: tensor([[-2.1824, -1.7027, -0.2838,  0.3263, -0.8088,  0.4089,  1.7610]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 42
	action: tensor([[-2.6372, -1.8658, -0.0925,  0.5630, -1.0483,  0.6811,  0.0571]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 43
	action: tensor([[-2.2412, -1.3898, -0.1583,  0.1457, -0.8714,  0.3589,  1.1497]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 44
	action: tensor([[-2.0663, -2.2666, -0.4340,  1.4787, -1.2722,  0.1744,  0.0671]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 45
	action: tensor([[-2.6499, -1.5053, -0.7252,  0.3049, -1.0994, -0.4312,  0.2969]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 46
	action: tensor([[-1.9263, -1.2112,  0.2225,  0.3199, -0.9700, -0.8209,  0.5631]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 47
	action: tensor([[-2.3880, -1.5216, -0.3590,  0.3521, -0.9055, -0.9085, -0.3434]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 48
	action: tensor([[-2.5441, -0.9559, -0.2213,  0.9289, -0.9409,  0.2723, -0.2480]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 49
	action: tensor([[-2.0879, -1.6051, -0.5085,  0.3126, -0.5008, -0.0917,  0.3681]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 50
	action: tensor([[-2.4164, -1.5948, -0.4502, -0.1546, -0.6315,  0.0897,  0.5257]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 51
	action: tensor([[-1.9679, -1.7462, -0.4215, -1.3184, -1.1814,  1.1775,  0.5991]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 52
	action: tensor([[-2.5787, -1.0501, -0.6543,  1.3278, -0.7297, -0.2772, -0.1599]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 53
	action: tensor([[-2.1963, -0.4691, -0.3931,  0.9463, -1.1783,  2.0434,  0.1424]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 54
	action: tensor([[-2.2346, -2.1297, -0.8021, -1.0123, -0.9669,  0.2460, -0.0532]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 55
	action: tensor([[-2.2899e+00, -1.3567e+00, -1.5354e-03,  3.2210e-01, -1.7037e+00,
         -3.7067e-01,  9.4540e-01]], dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 56
	action: tensor([[-2.8221, -1.8594, -0.2948, -0.2670, -1.4698, -0.0658,  1.1733]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 57
	action: tensor([[-1.7136, -1.0805, -0.3176,  1.7020, -1.0891,  0.3573,  0.1816]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 58
	action: tensor([[-1.6337, -1.2555, -0.4114,  0.8117, -1.2754,  0.6633, -0.7431]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 59
	action: tensor([[-2.0316, -1.4634, -0.4922,  1.6956, -1.7376, -0.3939,  0.3184]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 60
	action: tensor([[-2.4066, -1.5025, -0.3532, -0.9515, -1.0527,  0.3337,  0.8488]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 61
	action: tensor([[-2.0646, -1.7615, -0.3295, -0.0639, -0.5898, -0.9691, -0.3675]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 62
	action: tensor([[-2.2464, -1.3958, -0.3393, -0.2094, -0.8570, -0.9367,  1.1625]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 63
	action: tensor([[-2.2238, -0.5968, -0.5094,  0.3359, -1.0590,  0.2163,  0.4174]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 64
	action: tensor([[-2.2387, -1.9301, -0.1852,  1.0160, -0.4665,  0.6817, -1.0116]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 65
	action: tensor([[-2.1687, -1.2145, -0.3771,  0.9364, -1.1183,  0.9673,  2.0590]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 66
	action: tensor([[-2.3991, -1.4597,  0.0080,  1.0025, -1.0687, -0.3846, -0.5353]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 67
	action: tensor([[-2.1183, -0.6005, -0.5955,  0.4925, -0.8261,  0.9233,  0.6463]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 68
	action: tensor([[-2.5020, -1.3431, -0.0883,  2.2459, -0.7571,  1.6017,  0.7165]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 69
	action: tensor([[-2.1406, -0.6314, -0.1158,  1.1640, -0.6247, -0.6657,  0.5011]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 70
	action: tensor([[-2.1798, -0.7524, -0.1686,  2.1376, -1.0865, -0.8485,  0.7265]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 71
	action: tensor([[-2.7638, -1.2590, -0.6216,  0.2009, -1.3653,  1.2824,  0.9871]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 72
	action: tensor([[-2.0680, -1.3484,  0.0045, -0.0447, -0.4465,  0.1681,  0.4040]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 73
	action: tensor([[-2.6729, -2.0415, -0.2405,  0.4620, -0.7142,  2.5614, -0.0528]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 74
	action: tensor([[-2.6299, -1.5266, -0.5231,  0.6038, -1.1632,  0.7360,  0.8763]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 75
	action: tensor([[-2.3884, -0.9634, -0.2047,  0.1854, -0.9376,  0.4056, -0.3368]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 76
	action: tensor([[-2.3040, -1.5057, -0.0806,  0.3677, -1.0184,  1.0143,  0.1067]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 77
	action: tensor([[-2.4043, -2.1291, -0.6468,  1.4357, -1.0814, -0.5182,  0.9872]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 78
	action: tensor([[-2.3993, -1.0044, -0.2112,  0.3400, -0.5147,  0.5588,  0.1070]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 79
	action: tensor([[-2.1439, -1.5905, -0.5362,  0.3689, -1.1095,  0.5183, -0.7584]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 80
	action: tensor([[-2.0917, -1.7232, -0.4788, -0.5589, -1.5861,  0.0296, -0.3287]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 81
	action: tensor([[-2.0897, -0.8480, -0.1956,  0.8171, -0.8859,  0.9099,  1.5885]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 82
	action: tensor([[-2.1252, -1.6638, -0.4998,  0.5407, -1.3575,  0.9290,  0.5837]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 83
	action: tensor([[-2.3500, -1.6972, -0.0647, -0.2029, -1.0934,  0.4289,  0.8761]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 84
	action: tensor([[-1.8258, -1.0045,  0.0878,  1.3055, -0.9380, -0.1156,  1.4513]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 85
	action: tensor([[-2.6876, -1.6512, -0.1565, -0.2433, -1.1804,  0.7395,  0.6303]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 86
	action: tensor([[-2.7794e+00, -1.9184e+00, -1.7544e-03, -1.1184e-01, -1.0216e+00,
          8.8157e-01, -9.6316e-01]], dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 87
	action: tensor([[-2.2065, -2.0864, -0.3467, -0.3975, -0.6019,  0.2310, -0.9773]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 88
	action: tensor([[-3.1423, -1.5717, -0.4604,  0.9344, -1.2012,  1.5483, -0.0869]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 89
	action: tensor([[-2.6866, -1.8800, -0.4281,  1.0669, -0.8642, -0.4876,  0.2376]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 90
	action: tensor([[-2.1111, -1.3124, -0.6286, -1.2724, -0.8580,  0.7146,  0.6678]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 91
	action: tensor([[-1.8388, -1.1121, -0.3543,  0.0156, -1.1662,  1.5249, -0.3500]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 92
	action: tensor([[-2.1077, -2.2433, -0.3212,  1.0546, -0.9348,  0.8203,  1.0627]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 93
	action: tensor([[-1.7488, -1.6314, -0.5101,  0.8020, -1.0602, -0.3363, -0.2182]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 94
	action: tensor([[-1.8247, -1.1346, -0.0151,  0.3480, -0.6056,  0.2665,  1.0817]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 95
	action: tensor([[-2.5016, -1.8225, -0.2010,  0.9380, -1.0617,  0.8392, -0.2796]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 96
	action: tensor([[-2.3325, -0.4624, -0.1928,  0.4677, -1.4014,  1.4244, -1.2204]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 97
	action: tensor([[-2.2429, -0.9327, -0.2593,  0.1524, -0.7832, -0.7680, -0.8240]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 98
	action: tensor([[-2.4774, -1.4791, -0.6010,  1.1659, -1.1949,  0.1733,  0.3968]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 99
	action: tensor([[-2.8085, -1.9512, -0.5060,  0.0099, -1.2081,  1.0848, -0.0845]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 100
	action: tensor([[-2.0856, -1.7954,  0.1804, -0.3883, -1.1211,  1.6389, -1.0233]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 101
	action: tensor([[-2.4095, -1.7521, -0.7617, -0.3593, -1.2332, -1.8320,  0.6292]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 102
	action: tensor([[-2.2696, -1.1748, -0.1818, -0.4868, -1.4480,  1.5757,  0.2183]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 103
	action: tensor([[-1.8682, -1.4978, -0.5631,  0.5078, -0.7113, -0.2498, -0.2819]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 104
	action: tensor([[-2.8175, -1.8000, -0.1380,  0.9397, -1.3713, -0.6500,  1.2508]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 105
	action: tensor([[-2.4852, -2.0000, -0.1729,  0.1459, -0.9038,  0.9368, -0.4998]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 106
	action: tensor([[-1.9991, -1.2667,  0.1982,  0.3252, -1.1737, -0.9160,  1.2906]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 107
	action: tensor([[-2.4508, -1.3324, -0.2632,  0.8338, -0.8819, -1.1760,  0.3297]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 108
	action: tensor([[-2.2243, -1.8937, -0.2252,  0.5981, -0.9655,  0.8213, -0.0116]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 109
	action: tensor([[-2.1721, -1.4770,  0.1601,  0.3201, -0.8452,  0.2001,  0.3607]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 110
	action: tensor([[-1.6002, -1.9236, -0.7190,  0.2193, -1.1353,  0.7178, -1.2051]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 111
	action: tensor([[-1.9770, -1.4492, -0.1079,  0.4011, -0.5176,  2.5130,  0.5607]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 112
	action: tensor([[-2.2416, -1.5377, -0.1962,  0.5669, -0.9177,  1.4041, -0.0863]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 113
	action: tensor([[-2.5835, -1.6477, -0.2449,  1.0411, -0.9995, -0.1778, -0.0802]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 114
	action: tensor([[-2.5338, -1.6849, -0.5416,  2.1339, -1.3470,  1.0134, -0.1702]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 115
	action: tensor([[-2.0294, -0.7558, -0.3383, -0.2069, -1.1847,  1.7392,  0.2306]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 116
	action: tensor([[-2.2648, -1.4337, -0.2214,  1.6493, -1.0430,  0.5472,  0.2766]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 117
	action: tensor([[-1.5274, -1.4939, -0.0816,  1.3020, -1.2087, -0.3340,  0.3489]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 118
	action: tensor([[-2.5246, -1.4821, -0.5330,  0.6437, -1.1168,  0.4373, -0.2060]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 119
	action: tensor([[-2.4239, -1.5212, -0.5348,  1.2426, -0.5654,  1.9001,  0.1149]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 120
	action: tensor([[-2.1759, -1.2831, -0.6660,  0.1825, -0.8008, -0.5948,  1.3123]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 121
	action: tensor([[-2.2411, -1.6168, -0.2469,  0.1895, -0.6361,  0.6521, -0.2542]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 122
	action: tensor([[-2.2850, -1.4061, -0.2003, -0.5159, -1.1569, -0.1342,  0.4118]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 123
	action: tensor([[-2.0729, -1.5841, -0.5710,  0.1130, -1.3719,  0.0312, -0.6148]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 124
	action: tensor([[-2.3206, -0.9058, -0.3787,  1.5284, -1.2359,  1.8337, -0.9513]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 125
	action: tensor([[-2.2430, -1.6450,  0.0115,  1.0498, -1.2454,  1.2134, -0.3637]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 126
	action: tensor([[-2.2705, -1.2951, -0.3371,  1.0163, -0.7052,  2.0866,  1.0840]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
epoch: 8, step: 127
	action: tensor([[-2.4823, -1.7541, -0.2364,  0.4114, -1.1594,  1.2567,  0.9932]],
       dtype=torch.float64)
	q_value: tensor([[0.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.5989689034997807
LOSS epoch 8 actor 1253.0446249977429 critic 2506.0905484168784
epoch: 9, step: 0
	action: tensor([[-2.9676, -1.2692, -0.8381,  0.3701, -0.8485, -0.6722,  0.4301]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 1
	action: tensor([[-2.7206, -2.4325, -0.5428, -0.0650, -0.7138,  0.7971,  1.0376]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 2
	action: tensor([[-2.7882, -1.4893, -0.5397,  0.7605, -1.8946, -0.5223,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 3
	action: tensor([[-3.1934, -2.0128, -0.4825,  0.4434, -0.8531,  1.0321,  0.6118]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 4
	action: tensor([[-3.3817, -2.3133, -0.3717, -0.6541, -0.8866, -0.8281,  1.5753]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 5
	action: tensor([[-2.5958, -2.3048, -0.6005,  0.4437, -1.3326,  0.8199,  0.1827]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 6
	action: tensor([[-2.7363, -1.3099,  0.3093,  0.0525, -1.0421,  1.4255,  0.2907]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 7
	action: tensor([[-1.9095, -0.8192, -1.1251,  0.1322, -1.3279,  1.0481, -0.3317]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 8
	action: tensor([[-3.2311, -1.0885, -1.1016,  0.3610, -1.6942, -0.8720, -0.9260]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 9
	action: tensor([[-2.7523, -0.8214,  0.0255,  0.7683, -0.1771,  0.5684,  0.3723]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 10
	action: tensor([[-2.9507, -2.7293, -0.6701, -0.6991, -1.6388,  3.3569, -0.4666]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 11
	action: tensor([[-3.0041, -1.0712, -0.3713,  0.1727, -1.8754, -1.1384,  2.0233]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 12
	action: tensor([[-3.5185, -2.0168, -0.0613,  1.5351, -1.5121,  1.2602,  1.3435]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 13
	action: tensor([[-3.2132, -0.8226,  0.0932,  1.3598, -1.0764,  0.6556,  2.1328]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 14
	action: tensor([[-3.4581, -1.1411, -0.0674,  0.5643, -1.1764, -1.4155,  0.0669]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 15
	action: tensor([[-3.2090, -1.7745,  0.2431, -0.5709, -0.9391,  1.1261,  0.5117]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 16
	action: tensor([[-2.4079, -1.4969, -0.5402, -0.1307, -0.9765,  1.8129,  0.2469]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 17
	action: tensor([[-2.8060, -1.8016, -0.1480,  1.8477, -1.6412,  2.2475, -0.2335]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 18
	action: tensor([[-2.8938, -2.2824, -0.1739,  0.6655, -0.7767,  1.4681,  1.3050]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 19
	action: tensor([[-3.3509, -2.2434, -0.1671,  0.6440, -1.5071,  0.7331, -0.7791]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 20
	action: tensor([[-2.8846, -1.2276, -0.9613,  0.4404, -1.8696, -0.1794,  1.2058]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 21
	action: tensor([[-3.0607, -0.1997, -0.6914,  0.6751, -1.0858,  0.3277,  0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 22
	action: tensor([[-2.7158, -1.5581, -0.7012,  0.6506, -1.0898,  1.7472,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 23
	action: tensor([[-3.2907, -2.6750, -0.2266,  0.4226, -0.9081,  3.3267, -0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 24
	action: tensor([[-2.5249, -1.5059, -1.0409, -0.7055, -1.6279,  0.9602,  0.4166]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 25
	action: tensor([[-3.3165, -1.6544, -0.6470,  0.5487, -1.8344, -0.4599,  0.6554]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 26
	action: tensor([[-2.2737, -1.8580,  0.2048,  2.2244, -1.1739,  0.9985, -0.3415]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 27
	action: tensor([[-3.1912, -1.6407, -0.3535,  0.6124, -1.0695,  2.0596,  0.6127]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 28
	action: tensor([[-3.7662, -1.5230, -0.5983,  0.2567, -0.4409,  0.2464,  1.5722]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 29
	action: tensor([[-2.6066, -1.0591, -0.3217,  0.7240, -1.1818,  0.3268,  0.4568]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 30
	action: tensor([[-2.7814, -1.7446, -0.4248,  1.5269, -0.6997, -0.9530,  1.0143]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 31
	action: tensor([[-2.8028, -1.5855, -0.0267,  1.4323, -0.9628,  0.3562,  0.5671]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 32
	action: tensor([[-3.0511, -1.6454, -0.4572,  3.0576, -0.7149,  0.2676,  1.9236]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 33
	action: tensor([[-2.9530, -2.4247, -0.4713,  1.5819, -0.3816,  1.0414,  0.7223]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 34
	action: tensor([[-2.5506, -1.4026, -0.5205, -0.0950, -0.8483,  1.1760,  0.4102]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 35
	action: tensor([[-2.7842, -1.1408, -0.1070, -0.0038, -1.1748,  0.3079,  1.4185]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 36
	action: tensor([[-2.5209, -1.2519, -0.2079,  0.0553, -1.3947, -0.6174,  1.3701]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 37
	action: tensor([[-2.7598, -2.0466, -0.5224,  0.2800, -1.0044,  1.6762,  0.7716]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 38
	action: tensor([[-3.0580, -1.7106, -0.5219, -0.8347, -1.1332,  0.2296,  0.9190]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 39
	action: tensor([[-2.2772, -0.3840, -0.2316,  1.0196, -1.1421,  3.2240, -0.5473]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 40
	action: tensor([[-3.4386, -1.3616, -0.1964,  1.6330, -1.0604,  2.8213,  0.6630]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 41
	action: tensor([[-3.3725, -2.5971, -0.4739, -0.0880, -1.1951,  2.5473,  2.1162]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 42
	action: tensor([[-2.0952, -0.7496, -0.2225,  1.5898, -1.5987,  1.2214,  0.1964]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 43
	action: tensor([[-2.5391, -2.0829, -0.5865, -0.0467, -0.5214, -0.0722,  1.0920]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 44
	action: tensor([[-3.2431, -1.7619, -0.3125,  0.5109, -1.8357,  0.2862,  2.0159]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 45
	action: tensor([[-2.3779, -1.5753,  0.4842, -0.4625, -1.3122,  0.1420,  1.2870]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 46
	action: tensor([[-2.9242, -1.2886, -0.1395,  0.7373, -2.3242, -0.4969, -1.7639]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 47
	action: tensor([[-3.1374, -1.5374,  0.1568,  0.6548, -1.1412, -0.0918,  0.3264]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 48
	action: tensor([[-2.8810, -2.1296, -0.2564,  1.2154, -1.0829, -0.1619,  0.3326]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 49
	action: tensor([[-3.2684, -2.9378, -0.0796, -0.0494, -1.3080,  0.6845,  0.4659]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 50
	action: tensor([[-2.9164, -1.7843,  0.0794,  0.7396, -1.5021,  2.8079, -0.5191]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 51
	action: tensor([[-2.9286, -1.0864, -0.6813,  0.4436, -0.5422, -1.0974,  0.5195]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 52
	action: tensor([[-2.9450, -2.1097, -0.4326,  0.6285, -1.4837, -0.3649,  0.9034]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 53
	action: tensor([[-2.4480, -0.8424,  0.7963, -0.0924, -0.3348, -1.9114, -0.5023]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 54
	action: tensor([[-2.8511, -1.3302, -0.1042,  0.6245, -1.2401,  2.5724,  0.8133]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 55
	action: tensor([[-3.1300, -0.7772, -0.2224,  1.6651, -1.0304,  0.6445, -0.7167]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 56
	action: tensor([[-3.0220, -1.9967, -0.0821,  2.2598, -1.1843, -0.0253, -0.3288]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 57
	action: tensor([[-3.4295, -2.8204,  0.3588, -0.1987, -0.7458,  0.5354,  0.8079]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 58
	action: tensor([[-2.9039, -2.7473, -0.5588,  0.3590, -1.2474,  0.0234,  0.3286]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 59
	action: tensor([[-2.9674, -1.2616, -0.4229, -0.2987, -1.3666,  1.1776,  1.1374]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 60
	action: tensor([[-3.0485, -1.7561, -0.4126,  1.0350, -1.0066, -0.6835,  0.1345]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 61
	action: tensor([[-2.4151, -1.6530, -0.6130, -0.0249, -1.2663, -1.2751, -0.1822]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 62
	action: tensor([[-2.6458, -1.9921, -0.2873,  1.3670, -1.0213,  2.6280, -0.5246]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 63
	action: tensor([[-2.6710, -1.5893, -0.2510,  1.6552, -0.5300, -0.7047,  0.8172]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 64
	action: tensor([[-2.7112, -1.9314, -0.8561,  0.4905, -1.3241,  2.2033, -1.0558]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 65
	action: tensor([[-3.5351, -2.3252, -0.0067, -0.2683, -0.7500,  2.3271,  0.9582]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 66
	action: tensor([[-3.7477, -2.0745, -0.7632, -0.0220, -1.1812,  0.9648,  1.5917]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 67
	action: tensor([[-3.3145, -1.6727, -0.2026,  0.6287, -1.1624,  0.7901, -1.0408]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 68
	action: tensor([[-3.1466, -1.4284,  0.0096,  1.2351, -1.7706, -1.2183, -0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 69
	action: tensor([[-2.9393, -2.2074,  0.0411,  2.2722, -1.6169,  1.1146,  2.2807]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 70
	action: tensor([[-2.7607, -1.7764, -0.6047, -0.1193, -1.0059,  1.1801,  0.9599]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 71
	action: tensor([[-2.7412, -1.4569, -0.3755,  0.5271, -1.8237, -0.1471,  0.3205]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 72
	action: tensor([[-2.9453, -0.4045,  0.2497,  1.1101, -0.9752, -1.3570, -1.4093]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 73
	action: tensor([[-2.8797, -1.3298, -0.5986,  0.2365, -0.3806, -0.1596,  0.7518]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 74
	action: tensor([[-2.3321, -2.3251, -0.5575,  2.5926, -0.9375, -0.9085,  0.9650]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 75
	action: tensor([[-3.3587, -1.7407, -0.2027, -1.1084, -0.7532,  0.1888, -0.7697]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 76
	action: tensor([[-2.7426, -1.5046, -0.1108, -0.2790, -0.9461, -0.8433,  1.1008]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 77
	action: tensor([[-2.8398, -2.7574, -0.0234, -0.4462, -1.4179,  2.8992, -0.3520]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 78
	action: tensor([[-2.2527, -1.0332,  0.1240, -0.1059, -1.3583, -1.7863,  0.2447]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 79
	action: tensor([[-2.7628, -0.6616, -0.2690,  1.1311, -0.7429,  0.3298, -0.3680]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 80
	action: tensor([[-2.9848, -0.1803, -0.7347, -0.7286, -1.7564,  2.4052,  0.3615]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 81
	action: tensor([[-2.9982, -0.6787, -0.1237,  0.8189, -1.5259,  2.1523, -0.8849]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 82
	action: tensor([[-3.3926, -1.1191, -0.4959,  0.6043, -1.0643, -0.7718,  0.2244]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 83
	action: tensor([[-1.8181, -1.7086, -0.0500,  0.0304, -1.3903,  0.5984,  0.8270]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 84
	action: tensor([[-2.6766, -2.2513, -0.1928,  2.6149, -0.7401,  0.8330,  0.7637]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 85
	action: tensor([[-2.9794, -0.8273, -0.5369,  1.1515, -2.0487,  1.4957,  0.9297]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 86
	action: tensor([[-3.0924, -2.1105,  0.1156,  0.3478, -1.2873,  2.2418, -0.3357]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 87
	action: tensor([[-3.2281, -1.5521, -0.1341,  1.0100, -1.0548,  1.0420, -0.5127]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 88
	action: tensor([[-2.8935, -2.0260, -0.8870,  2.5635, -1.1127,  1.6460,  1.3374]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 89
	action: tensor([[-2.8069, -1.8183, -0.5855, -0.5940, -1.2918,  1.3386, -0.0437]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 90
	action: tensor([[-2.2864, -1.4724, -0.5230, -0.6667, -1.5245,  1.3398,  2.0211]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 91
	action: tensor([[-2.9006, -1.8701, -0.8755,  0.3301, -0.9896,  0.4133,  1.4379]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 92
	action: tensor([[-3.1275, -1.8840, -0.4238,  2.3463, -1.1730, -1.3754,  0.0959]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 93
	action: tensor([[-1.8101, -1.7764, -0.1237, -0.4533, -0.6341,  0.2929,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 94
	action: tensor([[-3.8235, -1.5816, -0.0143, -0.0367, -1.5608,  0.7563, -0.6462]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 95
	action: tensor([[-2.2881, -2.2061, -0.1498,  1.2872, -1.4983, -3.3266,  0.6659]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 96
	action: tensor([[-2.1853, -0.8910,  0.6828,  0.6722, -0.6894, -1.2977,  1.7099]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 97
	action: tensor([[-2.1756, -1.2435, -0.3677,  0.1613, -0.3807,  1.8597, -1.0155]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 98
	action: tensor([[-2.9808, -2.0941, -0.3619,  0.0709, -1.0396, -0.1373,  0.7800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 99
	action: tensor([[-2.4846, -0.4369,  0.2267, -1.1270, -1.0996,  1.8608, -0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 100
	action: tensor([[-2.5173, -1.7995, -1.1614, -0.8615, -0.8282, -0.2841,  0.6743]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 101
	action: tensor([[-2.1342, -1.5928, -0.0774, -0.0400, -1.2561, -0.3849,  0.7117]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 102
	action: tensor([[-3.0016, -1.0240,  0.1633,  1.8486, -1.3693, -2.2687,  0.9576]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 103
	action: tensor([[-2.8093, -0.7013, -0.1676,  0.4357, -1.1530,  1.3417, -0.2299]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 104
	action: tensor([[-3.3714, -1.5974, -0.2671,  1.5555, -0.7205,  2.0397,  1.6566]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 105
	action: tensor([[-3.1382, -2.0413, -0.3037, -0.3763, -0.8801, -0.1678,  1.7277]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 106
	action: tensor([[-2.4733, -1.6841,  0.0817,  0.3180, -1.1860, -0.8340, -0.4115]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 107
	action: tensor([[-3.2570, -2.8362,  0.0419,  0.1110, -1.0792,  1.0447,  0.8694]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 108
	action: tensor([[-2.4666, -2.1870, -0.3793,  0.1897, -0.9752, -0.4294,  0.3492]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 109
	action: tensor([[-3.0170, -2.1957, -0.6340,  0.5770, -1.3597, -0.4279,  0.6855]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 110
	action: tensor([[-3.3952, -2.3562,  0.2354,  2.6552, -0.7448,  1.3418,  1.5544]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 111
	action: tensor([[-3.6833, -2.1095,  0.1178,  0.6168, -0.8837, -0.2234,  1.6178]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 112
	action: tensor([[-3.9448, -1.7946, -0.5097,  2.5047, -1.3946, -0.4551, -1.3400]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 113
	action: tensor([[-3.3926, -2.4537, -0.7928, -0.9332, -0.5584,  0.0704,  2.4023]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 114
	action: tensor([[-3.2837, -1.5358, -0.1364,  1.1117, -1.2158,  1.1507, -1.1742]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 115
	action: tensor([[-3.1639, -2.2212,  0.2186,  0.5567, -1.0892,  1.7803,  1.9331]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 116
	action: tensor([[-2.1070, -1.7850, -0.1367,  1.4232, -1.5051, -2.3743, -0.7663]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 117
	action: tensor([[-2.7792, -0.6293, -0.2424,  0.9702, -1.2173,  0.4719,  0.3732]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 118
	action: tensor([[-3.5831, -1.6213, -0.3053,  1.1628, -1.0152,  1.1118,  0.4206]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 119
	action: tensor([[-3.7050, -1.6970, -0.6211,  1.3385, -1.3042,  1.3653,  0.9373]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 120
	action: tensor([[-2.9807, -1.7944, -1.0660, -0.1472, -1.4748,  0.6882,  0.7974]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 121
	action: tensor([[-2.8482, -0.7039,  0.2602,  0.7291, -1.5441,  1.2293,  0.3277]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 122
	action: tensor([[-2.4550, -2.7570, -0.1007,  0.3991, -2.3672, -0.3326,  0.8094]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 123
	action: tensor([[-2.5058, -1.6541, -0.9668,  0.9334, -0.6132, -0.0032,  1.1599]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 124
	action: tensor([[-2.3169, -2.2637, -0.8409,  1.5957, -1.1033, -0.7750,  1.4988]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 125
	action: tensor([[-3.1677, -1.7140, -0.0790,  0.8214, -1.4094, -0.9207,  0.3478]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 126
	action: tensor([[-2.9991, -1.6885,  0.1813, -0.1518, -1.4831,  0.1907,  1.4445]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
epoch: 9, step: 127
	action: tensor([[-3.0743, -1.9830,  0.1053,  0.7053, -0.8952,  1.0013,  2.0676]],
       dtype=torch.float64)
	q_value: tensor([[-0.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 0.9389947825775243
LOSS epoch 9 actor 1240.3227604996225 critic 2480.6474994958485
epoch: 10, step: 0
	action: tensor([[-3.0537, -0.8807,  0.1634, -0.2747, -1.7484, -0.5319, -0.3153]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 1
	action: tensor([[-3.1752, -0.6883,  0.4299,  2.1243, -1.0859,  2.5965, -0.7967]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 2
	action: tensor([[-3.1400, -1.6035, -0.0597,  0.7141, -1.0182, -1.5004, -0.7559]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 3
	action: tensor([[-4.4387, -1.2399,  0.5718,  2.3498, -1.0280, -0.3876,  1.9655]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 4
	action: tensor([[-2.2586, -1.3286, -0.7165,  1.9759, -1.3987, -0.0771,  2.1205]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 5
	action: tensor([[-2.8107, -1.8281, -0.1230,  0.4873, -2.3109,  1.3171, -0.7017]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 6
	action: tensor([[-3.7994, -3.0745, -0.8221, -0.0156, -1.9456,  1.2450,  2.1678]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 7
	action: tensor([[-3.2464, -1.3484, -0.6054, -1.0069, -1.6750, -0.3280,  1.3202]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 8
	action: tensor([[-3.4659, -1.2939, -0.1117,  2.1732, -1.5923,  1.4035, -0.5582]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 9
	action: tensor([[-3.6229, -1.6177, -0.2264,  1.6345, -0.6749, -1.5298,  0.6206]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 10
	action: tensor([[-2.9000, -0.4020,  0.0560,  0.2200, -1.3135,  1.3667,  0.4976]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 11
	action: tensor([[-3.2117, -0.7501, -0.7971,  0.6937, -1.7451,  1.6750,  1.1812]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 12
	action: tensor([[-4.1395, -1.0996, -0.1294, -0.4991, -0.4911, -0.7555, -0.2263]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 13
	action: tensor([[-2.3125, -2.1252, -0.6034,  2.3251, -1.2999,  1.9514, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 14
	action: tensor([[-2.2795, -3.5342, -0.1412, -0.0889, -1.4508, -0.0918, -0.1613]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 15
	action: tensor([[-3.2995, -1.3419, -0.6095, -0.1034, -0.9460,  0.4708,  1.4478]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 16
	action: tensor([[-2.0389, -0.5706, -0.2614,  0.9967, -0.4429,  2.0995,  2.0480]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 17
	action: tensor([[-3.4851, -1.7668,  0.1540,  0.5393, -0.9952, -0.9652, -0.5236]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 18
	action: tensor([[-2.0715, -1.4309, -0.3083,  1.0721, -1.4884,  0.3946,  0.8240]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 19
	action: tensor([[-3.3198, -0.9080, -0.2180,  0.3874, -0.3669,  0.7711,  0.8417]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 20
	action: tensor([[-3.1978, -1.7934, -0.5593,  1.7676, -1.5684, -0.3350,  0.8102]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 21
	action: tensor([[-2.1979, -1.6493, -0.5516,  0.0620, -0.8373, -0.3462,  0.7718]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 22
	action: tensor([[-3.2236, -2.7369, -0.6471,  0.8604, -1.0744, -1.9694,  0.8346]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 23
	action: tensor([[-3.3425, -2.1855, -0.6402,  0.8598, -0.2613, -0.2784,  1.5061]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 24
	action: tensor([[-2.9331, -2.7207,  0.1814,  1.2279, -1.3029, -0.4255, -0.4869]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 25
	action: tensor([[-2.9586, -1.2348,  0.2792, -0.2842, -1.0668,  1.2915,  1.0887]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 26
	action: tensor([[-3.7984, -2.9062, -0.2207, -2.1133, -1.6396,  0.2550,  2.3098]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 27
	action: tensor([[-2.2769, -1.6430, -0.3113, -0.5917, -1.5260,  1.3004,  1.0215]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 28
	action: tensor([[-2.9855, -1.3152, -0.1381, -0.0039, -0.8312, -0.1939,  0.9785]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 29
	action: tensor([[-3.7006, -1.6355, -0.2975,  0.5697, -1.4053, -1.8842,  0.2110]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 30
	action: tensor([[-3.7956, -2.2628, -0.4244,  0.6132, -1.0438,  1.0763, -0.3911]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 31
	action: tensor([[-2.5214, -2.7347, -0.9134,  0.6015, -0.8803,  0.2576, -1.3377]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 32
	action: tensor([[-2.5720, -2.7400, -0.4128,  0.3883, -1.8999, -0.5502, -0.5050]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 33
	action: tensor([[-3.0568, -2.8530, -0.7496,  0.0397, -1.1084, -0.3406, -0.5085]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 34
	action: tensor([[-3.5236, -2.3051,  0.5795,  0.7236, -0.8277,  0.4418,  0.4121]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 35
	action: tensor([[-3.0399, -2.4378, -0.2079,  1.0068, -1.8063, -0.0191,  0.1713]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 36
	action: tensor([[-2.9801, -3.0966, -0.1907,  2.1563, -1.3192, -2.5372, -0.4818]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 37
	action: tensor([[-2.7053, -1.7627, -1.5730,  0.4616, -1.8191,  1.3962,  2.3014]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 38
	action: tensor([[-2.8378, -1.9775, -0.4102, -0.1437, -1.4133,  0.7271,  2.6062]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 39
	action: tensor([[-3.3770, -1.6536, -0.0750,  1.9922, -0.3484,  0.8467,  2.0535]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 40
	action: tensor([[-3.9985, -0.3597, -0.4765,  1.6941, -1.6972,  2.0325, -0.6481]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 41
	action: tensor([[-3.4744, -1.9584, -0.5555,  1.6602, -1.0988,  1.7707, -0.6337]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 42
	action: tensor([[-3.9890, -2.7623, -0.8915, -0.2550, -1.4903, -0.8010,  3.0422]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 43
	action: tensor([[-3.9854, -1.3092, -0.0909,  0.8702, -0.8090,  0.8445,  0.2312]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 44
	action: tensor([[-3.6783, -1.1144, -0.4747,  0.6289, -0.8209,  2.4150,  0.0932]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 45
	action: tensor([[-4.0126, -1.1593,  0.3210, -0.6257, -0.8831,  2.0675,  1.6760]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 46
	action: tensor([[-3.4907, -1.2562, -0.1073,  1.7223, -1.7535,  1.8612,  2.0397]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 47
	action: tensor([[-4.1707, -1.4858, -0.6862, -0.0205, -1.4143, -0.0514,  1.3196]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 48
	action: tensor([[-3.5986, -1.8755,  0.1290,  2.0350, -1.3893,  1.0964,  0.3809]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 49
	action: tensor([[-2.7734, -0.9200, -0.4046, -0.3988, -0.9316, -1.6822,  0.2631]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 50
	action: tensor([[-3.1101, -1.4597, -0.7292,  1.6000, -0.9794, -1.2361,  0.9283]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 51
	action: tensor([[-3.4895, -1.6856,  0.0441,  1.6037, -1.3151,  3.0478, -0.0795]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 52
	action: tensor([[-3.4564, -1.7613, -0.1822,  1.0040, -0.9197,  2.3095,  1.0183]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 53
	action: tensor([[-3.2178, -0.6502, -0.5652,  1.6577, -0.9792,  2.2999, -1.3589]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 54
	action: tensor([[-2.7693,  0.5366, -1.1184,  2.1586, -1.3455,  0.3245,  0.1876]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 55
	action: tensor([[-3.3479, -1.9569, -0.3213,  1.2591, -1.0788, -0.3573,  1.5932]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 56
	action: tensor([[-2.9721, -2.7038, -0.2772,  0.7062, -1.4735, -0.3449, -0.2231]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 57
	action: tensor([[-4.1099, -1.7176, -0.4049, -0.1094, -1.4383, -1.0023,  0.0688]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 58
	action: tensor([[-3.6932, -0.6857,  0.1517,  1.9861, -0.8023,  1.2672,  0.4223]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 59
	action: tensor([[-3.0851, -1.6742,  0.2562,  2.2392, -0.3680,  0.1979,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 60
	action: tensor([[-3.0794, -1.3858, -0.3334,  1.7640, -0.8296,  0.4248,  0.5787]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 61
	action: tensor([[-3.6913, -1.4999,  0.0201,  0.0732, -1.5136,  1.6133, -1.2075]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 62
	action: tensor([[-2.8750, -1.8608, -0.9523, -0.7886, -1.2483,  1.0951,  0.7902]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 63
	action: tensor([[-3.1784e+00, -1.5838e+00,  6.6234e-01,  3.0894e-03, -1.3305e+00,
          1.4996e+00, -5.0122e-01]], dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 64
	action: tensor([[-2.6388, -0.8613, -0.9810,  0.0684, -1.3900,  1.5042, -0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 65
	action: tensor([[-2.8435, -1.4994, -0.0339,  0.5685, -1.1201, -0.6458, -0.0370]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 66
	action: tensor([[-3.8697, -1.9954,  0.1886,  1.1899, -1.8650, -1.1835,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 67
	action: tensor([[-3.5011, -2.8933, -0.3851,  0.7575, -1.7888,  0.4970,  1.3194]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 68
	action: tensor([[-3.3801, -1.3792, -0.5033,  0.4362, -1.4921,  0.9186, -0.2245]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 69
	action: tensor([[-3.4860, -1.6398, -0.0094,  1.0532, -1.5136,  2.4968,  1.0866]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 70
	action: tensor([[-3.1430, -1.8043,  0.6510, -0.1213, -0.4913, -1.6615,  1.6723]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 71
	action: tensor([[-3.8204, -2.0603,  0.0961,  2.6921, -1.8310,  1.4004,  0.5975]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 72
	action: tensor([[-3.4887, -2.9724,  0.3125,  1.8332, -1.3223,  1.6283,  0.2748]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 73
	action: tensor([[-3.5169, -1.5947, -0.0946,  1.6721, -1.2441,  1.4325,  0.2993]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 74
	action: tensor([[-2.5471, -2.1580,  0.7213,  1.6075, -1.2105,  3.1623,  0.9791]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 75
	action: tensor([[-3.3679, -1.4803, -0.2398,  0.6260, -0.9266, -1.0242,  1.6795]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 76
	action: tensor([[-2.8013, -0.6521,  0.0303,  1.0896, -1.3016,  2.3003,  0.6349]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 77
	action: tensor([[-2.9404, -1.6672, -0.8997,  0.9103, -1.5418,  1.6847,  1.2216]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 78
	action: tensor([[-3.7882, -1.7620, -0.3505, -0.1657, -1.0017, -1.1234, -0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 79
	action: tensor([[-2.5788, -1.7981, -0.6359, -0.2569, -1.5061,  1.2440,  0.9811]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 80
	action: tensor([[-3.0591, -0.8766,  0.1242,  0.5166, -1.2903, -0.4521,  1.0092]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 81
	action: tensor([[-3.2855, -2.4532,  1.0808,  1.0058, -0.4593, -1.0108, -0.7200]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 82
	action: tensor([[-3.3140, -2.0153, -0.2287,  0.3855, -1.0440,  1.7658,  0.6971]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 83
	action: tensor([[-4.2399, -1.7231, -0.1073, -0.4634, -0.7915,  1.4374,  1.8037]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 84
	action: tensor([[-3.9031, -1.3700, -1.0251, -0.6136, -0.8859, -0.3634, -2.1630]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 85
	action: tensor([[-3.5799, -2.1381, -0.9901,  0.1669, -0.2465,  0.9775,  0.8717]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 86
	action: tensor([[-3.9358, -3.6863, -0.3111, -0.3629, -1.4269, -0.5672,  0.1017]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 87
	action: tensor([[-3.7648, -2.2862, -0.0232,  1.2565, -0.7590,  1.2608,  0.4724]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 88
	action: tensor([[-3.7811, -1.8184,  0.2801,  1.3080, -2.0432,  2.3316, -1.2266]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 89
	action: tensor([[-2.6841, -2.8318,  0.1350, -0.4373, -1.1524, -0.2701,  1.0099]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 90
	action: tensor([[-3.3331, -1.7012, -0.4773, -0.3218, -0.7413,  2.7532,  0.8155]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 91
	action: tensor([[-3.2551, -1.8079,  0.2363,  1.0392, -1.3720, -0.6059,  1.0258]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 92
	action: tensor([[-3.2182, -1.6099,  0.8292,  1.4664, -0.9765,  0.6756,  0.8218]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 93
	action: tensor([[-3.8928, -1.9411,  0.1211, -1.2811, -0.7927,  2.8528, -0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 94
	action: tensor([[-3.3435, -1.1123, -0.0234, -2.1589, -1.7026,  1.7597, -0.4655]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 95
	action: tensor([[-3.2010, -1.5572,  0.1042,  0.0048, -0.3041,  1.9069,  1.1145]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 96
	action: tensor([[-2.7803, -1.3497,  0.4657,  1.4576, -1.1001,  1.4434,  2.2059]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 97
	action: tensor([[-3.1865, -1.5413, -0.1572,  1.9252, -0.5768,  0.3077, -0.1285]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 98
	action: tensor([[-2.9913, -1.2229, -0.5565,  1.2763, -1.5577, -2.2093,  1.6321]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 99
	action: tensor([[-3.2736, -1.6519, -0.3629,  0.6233, -1.1604, -0.7396,  0.6152]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 100
	action: tensor([[-3.4298, -0.9507, -0.0243, -0.1035, -1.1810,  0.3812, -1.2746]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 101
	action: tensor([[-4.1044, -3.0599,  0.4338,  0.8619, -1.2274, -0.9481,  0.5942]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 102
	action: tensor([[-3.4210, -1.1379, -0.4809, -0.0120, -1.6472, -0.5496,  1.6181]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 103
	action: tensor([[-3.2300, -1.8148,  0.3504,  0.2727, -0.6090,  1.2214, -2.3175]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 104
	action: tensor([[-2.8487, -1.9692, -0.4298,  1.1541, -1.4033, -0.7987,  0.2958]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 105
	action: tensor([[-2.6630, -1.4841, -0.6839, -0.2666, -0.9182,  1.1209,  0.9622]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 106
	action: tensor([[-2.8185, -0.7207, -0.5990,  2.7750, -1.7066,  1.7144, -0.6320]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 107
	action: tensor([[-3.0642, -1.8768, -0.3325,  2.5814, -0.9810,  2.9951,  0.1404]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 108
	action: tensor([[-3.2495, -1.3275, -0.4634,  0.4327, -1.3027,  1.5856,  0.4888]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 109
	action: tensor([[-4.0031, -2.7560,  0.0423,  1.2264, -1.8751,  1.2731,  0.8391]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 110
	action: tensor([[-3.1568, -1.8063,  0.5268,  0.5225, -1.2151,  2.3295,  1.1664]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 111
	action: tensor([[-3.1725, -1.1081,  0.1869, -0.3382, -1.5797, -0.6572,  1.2393]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 112
	action: tensor([[-3.1178, -1.7226, -0.2209,  0.4462, -1.1076,  1.9098, -0.3262]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 113
	action: tensor([[-3.0074, -2.4994, -0.3456,  0.9464, -1.7783,  0.0506,  1.1308]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 114
	action: tensor([[-3.6597, -1.6977,  0.7876, -0.4196, -1.7782,  3.0278,  1.8668]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 115
	action: tensor([[-2.7621, -0.6635,  0.1767, -1.5648, -1.3136,  0.7022,  0.7050]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 116
	action: tensor([[-3.2452, -1.9830, -0.0707,  1.2976, -1.0743,  0.4472, -0.7797]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 117
	action: tensor([[-3.6995, -3.3205, -0.5818,  2.5530, -1.8077,  2.9183,  0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 118
	action: tensor([[-3.7656, -2.2868, -0.9368,  1.1137, -2.4773, -0.3999,  0.8633]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 119
	action: tensor([[-3.9118, -2.1171,  0.2202, -0.2398, -0.8483,  2.0084,  2.4441]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 120
	action: tensor([[-3.4903, -1.1023,  0.6595, -2.1168,  0.0123,  2.2331,  1.2115]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 121
	action: tensor([[-2.3287, -0.4283, -1.1734,  0.0461, -0.9125,  1.6572,  1.1770]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 122
	action: tensor([[-4.2777, -2.5447, -0.3220,  1.7063, -0.9442,  1.3389,  1.1584]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 123
	action: tensor([[-3.6847, -0.2367,  0.9851,  0.7317, -1.2150, -0.5729,  0.7341]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 124
	action: tensor([[-2.3459, -3.3616, -1.3308,  1.4112, -1.1250, -0.5724, -0.5935]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 125
	action: tensor([[-3.1598, -0.0558,  0.2872, -0.9589, -0.9335,  2.5256, -1.2188]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 126
	action: tensor([[-2.7978, -1.6819, -0.3127,  1.8816, -1.3089,  0.2424,  1.5149]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
epoch: 10, step: 127
	action: tensor([[-3.4202, -1.9787, -0.4091,  0.8771, -0.7915,  2.2367, -0.4398]],
       dtype=torch.float64)
	q_value: tensor([[-0.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.1223846079642752
LOSS epoch 10 actor 1226.1373370024917 critic 2452.276918774199
epoch: 11, step: 0
	action: tensor([[-4.4235, -1.6189, -0.0624,  0.6372, -1.1654,  1.7162, -0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 1
	action: tensor([[-2.6839, -0.9030, -0.0060,  1.0618, -1.7901,  1.0501,  1.2344]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 2
	action: tensor([[-3.6550, -3.8354,  0.1525, -0.2205, -0.4237,  2.4182, -1.0382]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 3
	action: tensor([[-3.9594, -1.0699, -0.0551,  2.9133, -1.3906,  3.2627,  2.0989]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 4
	action: tensor([[-3.2077, -2.2287, -0.4551,  0.2876, -0.9812,  1.5247, -0.8839]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 5
	action: tensor([[-3.2955, -1.3325, -0.4993, -0.1661, -0.6079, -1.5344,  1.5385]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 6
	action: tensor([[-3.9393, -2.3131, -0.4253, -0.9225, -1.4811, -1.2633,  0.9953]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 7
	action: tensor([[-3.2277, -2.1471,  0.2579,  2.8157, -2.2101,  1.6429,  0.3980]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 8
	action: tensor([[-3.4407, -1.3662, -1.0825, -0.4247, -1.5105,  0.3323, -1.2600]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 9
	action: tensor([[-4.4599, -1.3050, -0.5766,  0.6749, -2.0137,  1.1773, -1.0315]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 10
	action: tensor([[-3.6440, -1.4100, -1.3540,  1.5990, -1.3620, -0.6191,  0.6757]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 11
	action: tensor([[-4.4089, -0.4990, -0.6946,  0.9883, -1.4699,  0.0573, -0.4857]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 12
	action: tensor([[-4.8394, -2.2080, -0.1716, -0.8585, -0.4614,  1.0714,  0.1803]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 13
	action: tensor([[-3.0350, -1.6042, -0.2193, -0.1193, -2.0082,  0.8737,  2.3131]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 14
	action: tensor([[-3.2304, -2.3899, -0.9018,  0.4082, -0.9566,  1.7735,  0.9094]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 15
	action: tensor([[-2.2911, -2.3284, -0.1407,  0.8937, -0.4993,  0.0565, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 16
	action: tensor([[-3.1381, -2.8053,  0.3049,  0.1491, -1.7994,  4.2456, -0.7831]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 17
	action: tensor([[-4.3009, -1.3940, -0.8357, -0.0924, -1.0341,  1.3665,  1.0979]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 18
	action: tensor([[-3.2101, -0.8809, -0.7305, -0.2269,  0.4297,  0.0764, -0.9936]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 19
	action: tensor([[-3.9362, -2.6580, -0.0998, -0.9949, -2.1302,  1.6493, -2.3412]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 20
	action: tensor([[-4.3135, -1.4391,  0.2342,  1.5171, -0.6628, -1.3616,  1.1465]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 21
	action: tensor([[-3.3609, -1.1556, -0.6960,  0.5662, -1.9894, -1.4975, -0.4226]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 22
	action: tensor([[-3.2114, -2.7557, -0.8740,  0.9322, -1.6054,  1.0986,  1.9556]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 23
	action: tensor([[-4.9166, -2.7313,  0.2056,  1.6677, -1.1810,  1.9243,  0.5178]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 24
	action: tensor([[-2.6296, -2.9415, -0.2080,  0.7361, -0.7704,  0.9492,  1.4045]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 25
	action: tensor([[-3.0119, -2.1219, -0.1357, -0.1250, -0.1625, -2.9049, -0.1587]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 26
	action: tensor([[-3.2279, -2.1305, -0.3931,  3.0548, -2.0020,  0.4513,  0.1065]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 27
	action: tensor([[-2.3972,  0.1071, -0.4623,  0.1489, -1.8702,  1.8599,  0.1153]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 28
	action: tensor([[-2.9621, -2.5653, -0.6188, -0.4198, -1.4415,  2.5134,  0.3345]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 29
	action: tensor([[-2.7029, -0.8788, -0.3154,  0.8100, -0.6428, -0.3495,  1.1797]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 30
	action: tensor([[-2.8730, -1.7823,  0.2848,  0.7623, -0.9737, -0.0060,  0.5842]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 31
	action: tensor([[-4.1284, -3.4141, -0.6838, -1.7276, -0.5412,  1.2737,  0.1069]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 32
	action: tensor([[-4.5965, -2.3627, -0.4134,  1.2990, -2.0066,  1.1793,  2.8114]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 33
	action: tensor([[-4.1228, -2.1059,  0.1038,  0.7219, -1.6328,  0.9975,  1.8410]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 34
	action: tensor([[-3.4165, -1.6839,  0.4733, -1.1867, -0.9552,  0.3346,  0.9195]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 35
	action: tensor([[-2.7851, -2.1355, -0.1213,  1.0114, -1.0293, -1.0278, -0.6904]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 36
	action: tensor([[-3.8941, -2.3940, -0.6146,  2.5418, -0.8306,  1.6674,  0.7098]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 37
	action: tensor([[-3.4281, -1.4830, -0.2589,  1.5551, -1.7891,  0.2384, -0.1472]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 38
	action: tensor([[-3.5845, -3.0899, -0.4979,  2.0980, -1.6152,  1.4111,  1.0982]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 39
	action: tensor([[-3.1476, -2.6058, -0.4697, -0.3757, -0.4565,  0.6979,  0.8603]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 40
	action: tensor([[-2.7887, -1.6712, -0.0167,  0.6957, -1.3428, -0.0471,  0.6024]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 41
	action: tensor([[-4.8198, -1.7241, -1.1196,  2.0848, -1.7920,  0.2302, -0.3121]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 42
	action: tensor([[-3.6783, -1.7334, -0.4184, -1.4149, -0.3566, -3.7227, -1.4214]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 43
	action: tensor([[-3.8991, -0.1328, -0.4409,  1.2241, -1.4905,  1.6386,  1.7298]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 44
	action: tensor([[-4.0659, -2.2311, -0.6834,  2.5288, -1.1611,  1.3346,  1.2658]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 45
	action: tensor([[-2.8299, -2.0621, -0.4894, -0.0210, -0.9584,  1.8947,  0.5732]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 46
	action: tensor([[-4.1019, -0.2344, -1.2048,  0.4857, -1.7302, -0.2097, -1.3717]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 47
	action: tensor([[-3.4766, -2.3413, -0.4779,  1.5261, -0.8997,  0.0265,  1.4203]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 48
	action: tensor([[-3.6538, -0.7948,  0.0375,  1.3762, -1.8209,  1.1313,  2.3116]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 49
	action: tensor([[-2.6423, -0.6084, -0.3717, -0.1399, -1.7322,  0.1317,  2.4850]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 50
	action: tensor([[-3.1884, -2.8734,  0.1098, -2.3473, -1.6984,  0.4277,  1.1174]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 51
	action: tensor([[-3.5555, -1.2458,  0.7720,  3.1181, -1.0479,  1.1873,  0.6055]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 52
	action: tensor([[-3.5938, -0.0134,  0.4029, -1.3333, -1.3023,  1.7809,  0.1482]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 53
	action: tensor([[-2.7181, -1.7088,  0.2547, -0.3484, -2.2959,  0.6653,  1.6676]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 54
	action: tensor([[-2.8595, -1.2851, -0.6125,  1.8212, -2.0554, -1.0034, -0.1322]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 55
	action: tensor([[-4.1272, -1.4826,  0.0851, -0.7072, -1.8472,  0.9881, -0.2486]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 56
	action: tensor([[-2.5504, -1.8971,  0.9399,  1.6867, -0.8308, -0.1280, -0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 57
	action: tensor([[-4.1579, -2.4583, -0.5914,  1.3339,  0.1655,  0.0607,  0.9398]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 58
	action: tensor([[-3.7345, -2.8332, -0.1703,  0.5893, -1.9661,  1.4359, -3.7024]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 59
	action: tensor([[-4.4964, -2.4015, -0.3859,  2.0847, -1.6786, -0.2924, -0.6140]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 60
	action: tensor([[-4.6575, -2.3339, -0.5517,  1.8378, -1.1195,  3.9138, -0.6950]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 61
	action: tensor([[-4.1668, -2.7670, -0.5782,  1.5824, -1.5967, -1.2848,  0.8572]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 62
	action: tensor([[-3.4743, -2.3103, -0.8440,  0.5787, -2.5165, -0.0732,  0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 63
	action: tensor([[-3.5561, -1.8306, -0.4070,  0.5280, -1.7901,  0.7356,  0.7189]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 64
	action: tensor([[-3.8488, -0.9523, -0.4783,  1.8614, -1.0509, -0.6643, -1.9963]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 65
	action: tensor([[-3.1617, -2.2130, -0.2636, -1.1405, -0.8803,  1.1973, -0.2999]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 66
	action: tensor([[-3.1970, -1.2061, -0.3722, -1.1201, -1.7252,  1.7025, -0.4755]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 67
	action: tensor([[-3.4073, -2.2020, -0.7624,  1.1906, -2.3825,  2.2508,  2.6085]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 68
	action: tensor([[-3.1178, -1.6579, -0.7109, -1.0326, -1.9946,  0.4585,  1.8606]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 69
	action: tensor([[-4.5711, -1.7504, -0.8768,  0.3239, -1.0826,  2.5470,  0.7919]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 70
	action: tensor([[-3.0916, -2.2701, -0.2009, -0.8709, -1.7569, -0.1005,  0.1626]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 71
	action: tensor([[-3.5075, -0.8528,  0.1580,  0.2565, -0.4644,  0.3588,  0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 72
	action: tensor([[-3.8790, -0.4662,  0.3809, -0.0976, -1.6094,  0.3511, -1.0469]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 73
	action: tensor([[-3.4975, -2.7246, -0.4872, -0.4866, -1.4800,  0.6498,  0.9864]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 74
	action: tensor([[-4.7744, -2.9668, -0.1673,  0.8006, -1.4664, -5.1286,  0.5346]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 75
	action: tensor([[-2.8329, -0.5785,  0.0714, -0.1112, -2.0581,  2.0480,  1.1027]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 76
	action: tensor([[-2.9298, -1.7769,  0.4709,  0.5437, -1.5700, -1.4884,  2.3820]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 77
	action: tensor([[-2.2797, -1.6978, -0.0844,  0.6851, -1.3076,  1.8737, -2.3777]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 78
	action: tensor([[-4.6614, -1.6543, -0.1736,  2.2479, -1.5173, -0.5419, -0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 79
	action: tensor([[-3.5000, -1.9977, -0.3652, -0.3668, -0.7213,  0.0082,  1.6454]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 80
	action: tensor([[-4.2603e+00, -1.0108e+00, -1.5250e-03,  1.8825e+00, -8.0059e-01,
          1.9116e+00,  4.0329e-01]], dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 81
	action: tensor([[-2.9690, -2.4184,  0.1757, -0.9815, -1.5000, -0.8291,  1.1603]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 82
	action: tensor([[-2.1752, -0.2918, -0.0707,  1.0887, -0.8848,  0.9975,  2.1400]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 83
	action: tensor([[-3.8358, -0.5032, -0.4223,  0.2851, -2.0276, -0.7623, -0.1717]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 84
	action: tensor([[-3.2847, -1.1972, -0.0164,  1.4516, -1.2973,  3.2559, -0.0958]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 85
	action: tensor([[-3.3554, -1.2107, -0.0584,  1.5897, -1.1211,  1.4802,  1.7257]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 86
	action: tensor([[-4.2560, -1.1913, -0.0570, -0.2258, -1.2172,  0.2720, -2.5444]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 87
	action: tensor([[-4.2593, -0.9258, -1.1457,  2.0298, -0.6819,  3.9104, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 88
	action: tensor([[-3.1451, -1.8391, -1.3649,  0.5938, -2.5288, -1.2621, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 89
	action: tensor([[-3.3524, -2.2582, -0.1596,  0.9896, -1.4687,  2.5632,  0.6895]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 90
	action: tensor([[-3.2370, -1.6457,  0.2125, -1.5028, -0.5182, -5.1139, -0.0970]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 91
	action: tensor([[-3.8021, -1.8194,  0.7732, -0.0381, -1.9800,  2.7538,  0.4484]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 92
	action: tensor([[-3.7379, -0.3225,  0.3966, -2.6582, -1.4827,  0.0464,  0.8224]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 93
	action: tensor([[-2.7672, -3.0925,  0.5467,  0.8155, -0.3833,  0.1364, -1.1858]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 94
	action: tensor([[-2.8009, -1.2022, -0.2402, -0.1127, -0.9908,  2.2471,  0.7950]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 95
	action: tensor([[-3.7184, -1.6864, -0.4755, -1.6894, -1.5893,  1.5929, -1.2648]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 96
	action: tensor([[-4.4055, -1.6597,  0.7804,  0.9568, -1.6649,  0.0725, -1.7761]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 97
	action: tensor([[-3.4997, -2.6042, -0.1221,  0.4678, -0.9310,  2.0405,  0.3182]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 98
	action: tensor([[-4.0194, -2.9216, -0.0349,  0.5883, -1.0804,  0.0278, -0.4693]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 99
	action: tensor([[-2.9010e+00, -1.4787e+00, -5.4263e-05,  3.3028e-01, -1.0742e+00,
         -1.2581e-01,  1.6589e+00]], dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 100
	action: tensor([[-3.7137, -1.8501,  0.3739,  1.7721, -0.8735, -0.0822,  1.8822]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 101
	action: tensor([[-2.9404, -0.4221, -0.5233,  1.0673, -0.3432,  0.5650,  0.3710]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 102
	action: tensor([[-3.0275, -1.9216, -1.2288,  0.6833, -1.4307, -0.2394, -2.9327]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 103
	action: tensor([[-2.5658, -0.9718, -0.1243,  0.6518, -0.9215,  3.4242,  0.5513]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 104
	action: tensor([[-4.6211, -1.0407,  0.0462,  0.4017, -1.7210, -0.1732,  0.4975]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 105
	action: tensor([[-3.5482, -0.9712, -0.8261,  1.4779, -0.3420,  1.3263, -0.4155]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 106
	action: tensor([[-2.1796, -2.3488, -0.1531,  0.8961, -2.1715,  0.6722,  0.6694]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 107
	action: tensor([[-3.3856, -3.2504, -0.2905,  0.3855, -1.2184,  3.1860,  0.2359]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 108
	action: tensor([[-3.3197, -1.0335, -0.3602, -0.5220, -0.1513, -0.1441,  0.2761]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 109
	action: tensor([[-4.2634, -2.7988,  0.1144, -1.1021, -1.4462,  1.0836, -0.4516]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 110
	action: tensor([[-2.8450, -1.4781, -0.0569, -0.7137, -1.7409, -1.9760,  0.2053]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 111
	action: tensor([[-2.9042, -2.6615,  0.0531,  1.2968, -1.0839, -0.3063, -0.5924]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 112
	action: tensor([[-2.7401, -1.4036, -0.3212,  2.3896, -1.9502,  1.1350,  2.0413]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 113
	action: tensor([[-4.0433, -1.7145, -0.2976,  2.0593, -0.1483,  1.5613,  1.0628]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 114
	action: tensor([[-3.1563e+00, -2.3108e+00,  7.1101e-05,  1.6288e+00, -1.1130e+00,
          9.0737e-02,  1.4847e+00]], dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 115
	action: tensor([[-3.9165, -0.9673, -0.6017,  0.6268, -0.9135,  0.8453,  0.8409]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 116
	action: tensor([[-4.1084, -2.1199, -0.5257,  2.3454, -0.7708, -1.0428, -0.8147]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 117
	action: tensor([[-3.3175, -2.7399,  0.3391,  1.0993, -1.0442,  1.7007,  1.5532]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 118
	action: tensor([[-1.9892, -2.7275, -0.6447,  1.5358, -1.5621, -0.6355,  1.6277]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 119
	action: tensor([[-3.1509, -1.8653, -0.3556,  0.9860, -1.8676,  0.5347,  1.9959]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 120
	action: tensor([[-3.6432,  0.7543, -0.6018,  2.5821, -1.9482,  3.3795,  0.6989]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 121
	action: tensor([[-3.4539, -3.4266,  0.3811,  0.2890, -1.7318,  1.0456,  1.9077]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 122
	action: tensor([[-3.6862, -1.9584, -0.4106,  2.5393, -1.5184, -1.3637,  0.9039]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 123
	action: tensor([[-4.0868, -2.4668,  0.0334, -0.5687, -1.8464,  1.4000,  2.0906]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 124
	action: tensor([[-3.1348, -2.0122, -0.4705, -0.3600, -2.0340,  4.3710,  0.5561]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 125
	action: tensor([[-4.1776, -1.7944,  0.4964,  0.7542, -1.0916,  0.0828,  1.8180]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 126
	action: tensor([[-3.2562, -2.2325, -0.2447,  1.8275, -1.5844,  1.5925,  1.7699]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
epoch: 11, step: 127
	action: tensor([[-3.4041, -0.9433, -0.8293,  1.2264, -1.5773,  0.7111, -0.0050]],
       dtype=torch.float64)
	q_value: tensor([[-1.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.2339317792039306
LOSS epoch 11 actor 1196.387459701601 critic 2392.7773872667603
epoch: 12, step: 0
	action: tensor([[-4.0086, -1.5447, -0.2008, -0.1177, -2.0774,  0.9527,  0.7809]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 1
	action: tensor([[-4.3864, -0.4959,  0.2442,  0.5948, -1.6099,  0.2969, -0.7415]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 2
	action: tensor([[-3.7751, -1.8727, -0.8100,  0.6162, -1.1743, -1.9892,  0.6304]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 3
	action: tensor([[-4.0706, -2.4004, -0.2381,  0.7621, -1.4336,  2.4291,  1.2821]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 4
	action: tensor([[-2.9542, -2.2463, -1.5259,  0.8286, -1.8685, -0.9160, -0.1705]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 5
	action: tensor([[-3.6092, -2.1766, -0.4351,  2.1223, -2.4978,  2.6832,  0.0518]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 6
	action: tensor([[-2.2090, -1.6047, -0.6191,  0.4976, -1.2289, -2.3068,  1.3018]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 7
	action: tensor([[-3.2934, -2.7328, -0.5690, -0.5348, -1.2224,  0.8838,  0.2473]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 8
	action: tensor([[-3.7223, -1.0992, -0.2975,  0.3071, -0.8228, -0.1035, -0.8845]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 9
	action: tensor([[-3.6769, -1.9037, -0.9242, -0.3425, -1.3393, -0.6676, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 10
	action: tensor([[-4.1700, -1.3244, -0.8744, -0.2942, -1.5692,  0.6103,  1.2932]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 11
	action: tensor([[-3.5315, -1.5185, -0.4188,  3.1609, -1.2047, -0.3477,  1.6883]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 12
	action: tensor([[-2.7498, -2.2979, -0.1107,  1.2176, -1.1838,  2.7132, -0.2596]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 13
	action: tensor([[-1.7288, -0.8293, -0.0262,  1.8521, -0.9934, -0.8015,  0.5403]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 14
	action: tensor([[-2.8072, -3.4777,  0.2052,  1.4408, -0.9179,  2.0605,  0.4559]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 15
	action: tensor([[-2.6360,  0.3515,  0.0728, -0.3829, -2.4687,  2.2192,  0.2439]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 16
	action: tensor([[-3.7495, -0.6776, -1.3271,  2.2039, -1.5261,  1.9011,  1.9513]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 17
	action: tensor([[-3.2398, -1.4812, -0.5646,  0.8969, -1.7134, -0.2333, -1.4192]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 18
	action: tensor([[-4.1106, -0.7522, -0.1134,  3.1006, -2.3019,  0.6046,  1.1191]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 19
	action: tensor([[-3.4486, -0.3970,  0.7627, -0.3348, -2.0094, -0.0951, -0.3127]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 20
	action: tensor([[-3.8055, -2.2351, -1.3669,  0.1158, -0.7285,  0.6877,  2.4178]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 21
	action: tensor([[-4.5568, -2.6207, -0.0123, -0.4736, -1.9682, -0.6724, -0.2103]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 22
	action: tensor([[-2.5520, -1.9148,  0.3935,  1.9234, -1.7630,  1.3409,  1.0326]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 23
	action: tensor([[-3.5437, -2.0212, -0.1766,  0.3969, -0.9517,  1.0061,  1.1767]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 24
	action: tensor([[-3.6643, -1.3701,  0.8191, -0.3460, -1.6928,  0.3224,  0.3074]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 25
	action: tensor([[-2.7048, -2.8475,  0.2166, -0.2642, -1.3060,  1.2443,  0.0607]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 26
	action: tensor([[-3.8542, -0.9909,  0.6754,  0.7928, -1.8074,  1.0020, -0.0769]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 27
	action: tensor([[-3.8987, -1.7443, -0.5883,  0.9515, -1.9441, -0.2341, -0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 28
	action: tensor([[-4.3423, -1.7389,  0.4672,  0.7741, -2.1383, -1.0567,  1.1519]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 29
	action: tensor([[-3.4057, -2.7071,  0.1444,  0.7394, -1.8695, -1.3325, -2.0243]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 30
	action: tensor([[-3.3143, -0.0785,  0.0402, -0.4710, -1.3425,  2.2509,  1.1977]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 31
	action: tensor([[-4.2375, -2.8467, -1.2986, -0.4789, -1.8047, -2.6998,  1.8095]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 32
	action: tensor([[-3.2492, -3.3101, -0.1144, -0.6472, -2.0073,  0.1478, -0.2614]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 33
	action: tensor([[-2.2147, -1.9372, -1.0575,  0.1709, -0.5615, -0.8540,  1.2868]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 34
	action: tensor([[-2.8385, -0.4923, -0.3703,  3.9964, -1.3858, -0.9157,  1.5672]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 35
	action: tensor([[-3.4104, -1.3307, -1.3337,  1.3034, -1.2386,  0.5859,  0.6385]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 36
	action: tensor([[-4.3157, -2.3696, -0.5405,  0.5701, -1.4082,  1.1207,  2.2133]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 37
	action: tensor([[-3.5735, -2.5541, -0.3275, -1.7077, -1.1801, -0.4224,  1.5715]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 38
	action: tensor([[-5.8778, -1.3907,  0.3255, -0.1758, -1.3898,  0.7516, -0.4853]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 39
	action: tensor([[-4.3907, -1.4241, -0.7435,  1.0067, -2.8084,  0.6259,  2.1901]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 40
	action: tensor([[-2.9326, -3.8672, -1.1631,  0.7011, -0.7184,  1.7099,  0.3316]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 41
	action: tensor([[-2.7224, -1.8587, -0.2478,  0.2324, -1.3911, -0.4199,  1.5381]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 42
	action: tensor([[-4.4190, -1.5298, -0.2189, -0.0785, -0.6139,  0.7717,  0.3374]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 43
	action: tensor([[-4.9371, -2.5538, -0.4994,  0.9363, -0.5824, -2.3043,  0.1938]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 44
	action: tensor([[-2.9984, -2.0759,  0.3934, -1.7087, -0.2405,  0.6487,  1.1569]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 45
	action: tensor([[-2.7097, -0.9686,  0.4359, -1.5023, -0.8904,  2.4890,  0.7643]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 46
	action: tensor([[-4.0496, -1.5466, -0.4719, -0.2918, -0.2943,  1.3486,  1.6867]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 47
	action: tensor([[-3.8047, -2.6500, -0.4789,  0.9501, -1.8178,  2.5198,  1.2420]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 48
	action: tensor([[-3.8725, -2.0057,  0.4306,  2.4992, -2.1218,  2.0897,  1.3324]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 49
	action: tensor([[-3.2954, -1.8030, -0.9522, -0.0222, -0.8749, -1.1074,  1.6807]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 50
	action: tensor([[-3.2543, -2.9646,  0.2870,  2.6704, -1.0387,  0.8823,  2.8209]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 51
	action: tensor([[-2.9100, -2.6868,  0.4277, -0.3647, -1.6219,  0.9557, -0.6231]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 52
	action: tensor([[-2.3813, -2.8987, -0.7144,  0.2035, -1.5334,  0.8351, -0.3137]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
epoch: 12, step: 53
	action: tensor([[-2.5447, -0.7915, -0.0945, -0.5231, -0.8321, -0.0787,  0.3762]],
       dtype=torch.float64)
	q_value: tensor([[-2.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9066983206039578 entropy 1.321263454541559
