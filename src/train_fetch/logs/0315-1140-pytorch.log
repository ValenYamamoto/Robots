epoch: 0, step: 0
	action: tensor([[ 0.5318,  1.5461,  1.2700, -0.3901,  0.9229,  0.5649,  0.6342]],
       dtype=torch.float64)
	q_value: tensor([[-41.3475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 1
	action: tensor([[ 1.0439,  0.8640,  0.0425, -0.8776,  0.5247, -0.3295, -0.7586]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 2
	action: tensor([[-0.9359,  3.1407, -0.0105,  1.0095,  0.3088,  0.2091, -1.8805]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 3
	action: tensor([[-0.9267, -0.6199, -2.1679, -0.4606,  1.1111, -1.6505, -0.0974]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13913660160721708, distance: 1.2213625419898244 entropy 1.448572039604187
epoch: 0, step: 4
	action: tensor([[ 0.3349, -0.3177,  1.8613, -0.8963,  1.8035,  0.8904,  1.6077]],
       dtype=torch.float64)
	q_value: tensor([[-60.9141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07371701893734883, distance: 1.1857731487672474 entropy 1.448572039604187
epoch: 0, step: 5
	action: tensor([[ 5.8307, -6.2800,  2.0462, -3.8047,  4.7847,  4.8643,  6.1073]],
       dtype=torch.float64)
	q_value: tensor([[-67.6370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 6
	action: tensor([[ 1.6956, -0.4761, -1.0623, -0.0415,  0.3901,  1.6429, -0.9532]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5841172535517996, distance: 0.7379758415411295 entropy 1.448572039604187
epoch: 0, step: 7
	action: tensor([[ 5.4884, -4.1009,  0.5270, -3.0411,  1.1714,  4.3226,  3.4573]],
       dtype=torch.float64)
	q_value: tensor([[-55.9892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 8
	action: tensor([[ 0.1001,  0.1662,  1.3079,  0.3481, -0.6717,  1.7856,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 9
	action: tensor([[ 0.0451, -2.1635,  0.6637, -2.6688, -0.5527,  2.0775,  0.2450]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 10
	action: tensor([[ 0.9663, -0.9404, -0.3648, -1.5387, -1.1165,  1.0841,  2.2322]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6291896019992544, distance: 1.460637707429158 entropy 1.448572039604187
epoch: 0, step: 11
	action: tensor([[ 5.0130, -6.2800,  3.0034, -4.7894,  5.5497,  5.8673,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-64.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 12
	action: tensor([[ 1.0083, -2.2817, -1.6975, -1.6736, -1.3282,  1.1122,  1.1903]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 13
	action: tensor([[ 0.1677,  1.1020,  0.4224,  0.2171,  1.6041, -0.6194,  0.8466]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 14
	action: tensor([[-7.5149e-01,  1.0280e+00,  5.3869e-01, -2.8156e-01,  3.9508e-01,
         -6.8014e-01,  6.5926e-04]], dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28406194728388967, distance: 1.296730366828779 entropy 1.448572039604187
epoch: 0, step: 15
	action: tensor([[ 3.5610, -1.3319,  0.6954, -1.9446,  3.5960,  0.9392,  0.9253]],
       dtype=torch.float64)
	q_value: tensor([[-36.5400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 16
	action: tensor([[ 1.9470, -0.9625,  0.5805, -0.4939,  0.9811, -0.7523,  0.3185]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2678539144322247, distance: 1.28852040726748 entropy 1.448572039604187
epoch: 0, step: 17
	action: tensor([[ 6.1147, -5.1707,  2.1693, -2.2554,  2.7930,  4.1477,  2.8727]],
       dtype=torch.float64)
	q_value: tensor([[-56.1416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 18
	action: tensor([[-0.3155,  0.0661,  0.5356, -0.5247, -0.3574, -0.3999, -0.1963]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4121320108320894, distance: 1.359860447586676 entropy 1.448572039604187
epoch: 0, step: 19
	action: tensor([[ 1.9595, -1.2658,  0.5823, -0.3899,  1.8969,  0.2605,  1.6164]],
       dtype=torch.float64)
	q_value: tensor([[-32.2049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8787172020041931, distance: 1.5685103876788142 entropy 1.448572039604187
epoch: 0, step: 20
	action: tensor([[ 4.4839, -5.3606,  3.1484, -3.8566,  3.9459,  5.2563,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-72.3113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 21
	action: tensor([[-1.0532, -1.0538,  0.0305,  0.8840, -1.0836,  0.3249,  0.7388]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1508454399729815, distance: 1.678267926410199 entropy 1.448572039604187
epoch: 0, step: 22
	action: tensor([[ 5.1750, -4.1380,  0.7530, -0.8783,  2.8242,  3.1721,  3.3351]],
       dtype=torch.float64)
	q_value: tensor([[-46.3753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 23
	action: tensor([[-2.0991,  1.3189,  0.4470, -0.1724,  0.8948, -0.8669, -0.3835]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 24
	action: tensor([[ 0.2226, -1.0598,  1.4248,  1.2157, -0.6086,  0.3545,  1.6874]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4430005681792575, distance: 0.8540515188043075 entropy 1.448572039604187
epoch: 0, step: 25
	action: tensor([[ 5.4272, -6.2800,  0.6351, -4.6315,  5.7818,  6.1800,  6.1202]],
       dtype=torch.float64)
	q_value: tensor([[-59.2854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 26
	action: tensor([[ 0.2006,  1.6863, -1.4434, -0.3750,  1.6661, -0.4368,  0.8731]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 27
	action: tensor([[-0.9816,  0.1163, -1.5393,  0.3754, -0.7606,  1.1863, -1.3684]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1067419905958928, distance: 1.6609722205520365 entropy 1.448572039604187
epoch: 0, step: 28
	action: tensor([[ 1.4563, -1.3132,  0.7264, -0.7191,  0.7587,  2.5186,  1.0738]],
       dtype=torch.float64)
	q_value: tensor([[-43.5150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 29
	action: tensor([[-0.3775, -1.1962, -0.7948, -0.2557,  1.1694,  0.0874,  2.3346]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8656044822549906, distance: 1.5630270048958244 entropy 1.448572039604187
epoch: 0, step: 30
	action: tensor([[ 6.1800, -6.0593,  1.3986, -5.4075,  3.7461,  3.8893,  3.8118]],
       dtype=torch.float64)
	q_value: tensor([[-60.2626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 31
	action: tensor([[-2.3564,  0.8137,  1.9737,  0.6655,  0.1344,  1.2232, -1.1751]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 32
	action: tensor([[ 1.3133,  0.5001,  0.8250, -0.7240,  1.8441, -1.4632,  1.3704]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 33
	action: tensor([[ 0.5808,  0.6198,  0.7405, -0.0831, -1.6657,  0.4349, -0.9158]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 34
	action: tensor([[ 0.8057, -0.6075,  0.4413,  0.1507,  1.7642,  0.4395, -1.4710]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17823558748552581, distance: 1.0373620548748796 entropy 1.448572039604187
epoch: 0, step: 35
	action: tensor([[ 3.0273, -3.7941,  0.4819, -2.6565,  1.6620,  2.7383,  2.4179]],
       dtype=torch.float64)
	q_value: tensor([[-65.3114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 36
	action: tensor([[-1.0977,  1.2400, -1.6857,  0.2859, -1.8586, -0.6706,  1.3053]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 37
	action: tensor([[-1.6210,  0.0981,  1.0925,  0.8584, -2.4492,  0.3352,  1.9093]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8209818566710292, distance: 1.5442211723316723 entropy 1.448572039604187
epoch: 0, step: 38
	action: tensor([[ 6.1800, -6.2800,  1.1695, -3.5136,  2.9019,  5.7847,  3.7368]],
       dtype=torch.float64)
	q_value: tensor([[-64.9218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 39
	action: tensor([[-0.6157, -0.5972, -0.5135,  0.1222, -0.2295,  0.3542, -0.6748]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8788817057716898, distance: 1.5685790569335067 entropy 1.448572039604187
epoch: 0, step: 40
	action: tensor([[ 1.2998, -0.9025,  0.9705, -2.0262,  1.8265,  0.6987, -0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-34.9280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17077223260647867, distance: 1.2382059884368724 entropy 1.448572039604187
epoch: 0, step: 41
	action: tensor([[ 5.2657, -6.2800,  1.0203, -5.2013,  2.9934,  4.7301,  5.7065]],
       dtype=torch.float64)
	q_value: tensor([[-71.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 42
	action: tensor([[-0.0552, -0.2869,  1.6025, -1.6872,  0.1586, -0.8253, -2.6501]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 43
	action: tensor([[ 0.3552,  0.0898,  0.6531, -1.2935, -0.4797,  1.0626, -1.0129]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28613857928727426, distance: 0.9668601243697587 entropy 1.448572039604187
epoch: 0, step: 44
	action: tensor([[ 3.3948, -3.1622, -0.3691, -1.3110,  2.6660,  3.9069,  1.3963]],
       dtype=torch.float64)
	q_value: tensor([[-46.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 45
	action: tensor([[-0.7598, -1.4499, -0.0771,  0.2021, -0.5560,  0.6229, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1893626116276836, distance: 1.6932283911853825 entropy 1.448572039604187
epoch: 0, step: 46
	action: tensor([[ 2.1041, -4.1173,  1.3860, -0.2631,  1.6168,  3.8614,  1.6208]],
       dtype=torch.float64)
	q_value: tensor([[-43.4293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 47
	action: tensor([[-0.1757, -0.5531, -0.7548,  0.9656, -1.3731, -0.8492,  2.2422]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12339140342223232, distance: 1.0714195236519477 entropy 1.448572039604187
epoch: 0, step: 48
	action: tensor([[ 6.1800, -5.9589,  1.1802, -1.5969,  2.2231,  3.2415,  0.6432]],
       dtype=torch.float64)
	q_value: tensor([[-60.0074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 49
	action: tensor([[ 1.2902,  0.6553,  0.5782,  0.4730,  1.3460, -0.3040, -0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 50
	action: tensor([[-1.9919,  0.3950, -1.2086, -3.6321, -0.0710, -0.8567, -0.9380]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 51
	action: tensor([[-1.9682, -1.6599,  0.2484, -0.5914, -1.5515,  2.1213, -1.0863]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 52
	action: tensor([[ 1.5872,  0.2026, -1.9701, -0.8307, -0.4550, -0.1193, -1.0574]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 53
	action: tensor([[ 1.0535, -0.1898,  1.0140, -0.7060, -0.5320,  1.0250, -0.6358]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6742148981116278, distance: 0.6531640952903153 entropy 1.448572039604187
epoch: 0, step: 54
	action: tensor([[ 5.7936, -5.0890,  0.6925, -3.9933,  3.0108,  3.6594,  3.5699]],
       dtype=torch.float64)
	q_value: tensor([[-48.1802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 55
	action: tensor([[ 0.1269, -1.2316, -0.0838, -0.5586, -0.2527,  0.3080, -0.5281]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9272560216420207, distance: 1.588643312521893 entropy 1.448572039604187
epoch: 0, step: 56
	action: tensor([[ 2.9667, -4.7406, -1.1534, -3.4369,  2.4562,  1.4659,  3.0480]],
       dtype=torch.float64)
	q_value: tensor([[-43.5803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 57
	action: tensor([[-1.7657, -1.5339,  1.0841,  0.1694,  0.4668,  0.3183, -0.6110]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 58
	action: tensor([[-0.1695, -0.6251, -0.0350, -1.2077, -0.3078,  0.3502,  1.8104]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.709590541047048, distance: 1.4962451180575345 entropy 1.448572039604187
epoch: 0, step: 59
	action: tensor([[ 6.1800, -4.7292,  0.2890, -2.4247,  4.3000,  3.4444,  2.6426]],
       dtype=torch.float64)
	q_value: tensor([[-49.0157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 60
	action: tensor([[-0.9206, -0.7935,  1.7596, -0.1664, -0.2269, -0.6112,  0.3707]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.436027188110816, distance: 1.786067051162425 entropy 1.448572039604187
epoch: 0, step: 61
	action: tensor([[ 5.8531, -4.9746, -0.4894, -3.4228,  3.4638,  4.8327,  2.4768]],
       dtype=torch.float64)
	q_value: tensor([[-48.9000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 62
	action: tensor([[-0.3762,  0.1972, -0.3882, -2.3036,  0.1578,  2.0623,  0.7083]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29530560033210984, distance: 0.9606321190355648 entropy 1.448572039604187
epoch: 0, step: 63
	action: tensor([[ 6.1800, -5.4913,  1.0991, -3.7523,  5.0950,  4.7410,  2.5176]],
       dtype=torch.float64)
	q_value: tensor([[-65.8199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 64
	action: tensor([[ 0.4552,  0.3321,  0.9304, -0.9922,  0.4718, -0.5323, -1.4007]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26711351265977856, distance: 0.9796592658177025 entropy 1.448572039604187
epoch: 0, step: 65
	action: tensor([[ 2.6028, -3.7500, -0.3700, -2.4762,  1.0530,  2.1445,  2.2849]],
       dtype=torch.float64)
	q_value: tensor([[-48.8199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 66
	action: tensor([[ 1.1656, -1.3493,  0.0769, -1.1123, -0.7220, -1.1518, -0.7052]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6928626650428771, distance: 1.4889069487493554 entropy 1.448572039604187
epoch: 0, step: 67
	action: tensor([[ 5.0700, -4.5051,  1.9753, -4.6154,  1.5277,  2.8684,  3.6205]],
       dtype=torch.float64)
	q_value: tensor([[-56.6633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 68
	action: tensor([[-1.3499, -0.6453,  1.2951, -0.8748,  1.0891, -0.0346, -1.0694]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7561877160352233, distance: 1.8998140259807457 entropy 1.448572039604187
epoch: 0, step: 69
	action: tensor([[ 2.9895, -4.2119,  0.7999, -2.7168,  0.9971,  4.1029,  2.5441]],
       dtype=torch.float64)
	q_value: tensor([[-57.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 70
	action: tensor([[ 0.9387, -1.0371, -1.3288, -1.1228,  0.2279,  0.5007, -0.2660]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38735698998555557, distance: 1.3478786882240763 entropy 1.448572039604187
epoch: 0, step: 71
	action: tensor([[ 5.5457, -2.6146, -0.0645, -0.7577,  2.6136,  3.5784,  1.2317]],
       dtype=torch.float64)
	q_value: tensor([[-51.7268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 72
	action: tensor([[-0.6665,  0.3970, -0.8544, -2.1583,  0.3082, -0.7659, -0.4265]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2755658385223332, distance: 0.9739937142813726 entropy 1.448572039604187
epoch: 0, step: 73
	action: tensor([[ 0.7117, -0.3159,  0.7291,  0.5508, -0.5983,  0.5299,  1.2371]],
       dtype=torch.float64)
	q_value: tensor([[-47.9575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8328351892467756, distance: 0.4678742282567649 entropy 1.448572039604187
epoch: 0, step: 74
	action: tensor([[ 6.1800, -4.9235,  1.1293, -3.0308,  3.7798,  3.7707,  3.4125]],
       dtype=torch.float64)
	q_value: tensor([[-45.6073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 75
	action: tensor([[ 0.5196, -1.7218,  1.0250, -0.0734, -0.7835, -0.4974,  1.0394]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8390204945851716, distance: 1.5518508479342954 entropy 1.448572039604187
epoch: 0, step: 76
	action: tensor([[ 4.0850, -4.7863,  4.3963, -3.8281,  4.4780,  5.4219,  4.2005]],
       dtype=torch.float64)
	q_value: tensor([[-49.8336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 77
	action: tensor([[-0.2565,  0.4155,  1.3538,  0.4235, -0.2433, -0.5709, -1.4070]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 78
	action: tensor([[ 0.4808, -2.0654, -0.4663, -1.4962,  0.8661,  0.2677,  0.8691]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 79
	action: tensor([[-1.3118,  1.0304, -1.4514,  0.7615, -0.2699, -1.2885,  0.8527]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 80
	action: tensor([[-0.8508, -0.7451, -0.0866, -0.1560,  0.1354, -1.5093,  0.5369]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7063700275694844, distance: 1.4948351462135132 entropy 1.448572039604187
epoch: 0, step: 81
	action: tensor([[ 2.8079, -2.0016, -0.4372, -2.9260,  1.4267,  1.3795,  1.9461]],
       dtype=torch.float64)
	q_value: tensor([[-45.1493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 82
	action: tensor([[ 0.9264, -1.4570,  0.4347,  0.4831,  1.1180, -0.8647, -0.4859]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20463030729707388, distance: 1.2559824932979766 entropy 1.448572039604187
epoch: 0, step: 83
	action: tensor([[ 3.6296, -4.5191,  1.3767, -2.2436,  3.2486,  2.9296,  2.3507]],
       dtype=torch.float64)
	q_value: tensor([[-58.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 84
	action: tensor([[-0.1493,  0.3660, -0.1377,  0.1879, -0.2042, -0.1020, -0.7550]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 85
	action: tensor([[-1.1756, -0.7402, -0.1487, -1.8371, -0.8523, -0.7393,  0.7856]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1744415990164221, distance: 1.0397539857985343 entropy 1.448572039604187
epoch: 0, step: 86
	action: tensor([[ 2.6971, -3.4073, -0.1075, -1.1679,  0.9262,  3.1044,  1.5885]],
       dtype=torch.float64)
	q_value: tensor([[-50.3627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 87
	action: tensor([[ 1.8304, -0.6380,  0.8272, -2.0728, -0.5890,  0.9105,  0.1718]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2620653234840563, distance: 0.9830274669773673 entropy 1.448572039604187
epoch: 0, step: 88
	action: tensor([[ 6.1800, -4.4024,  1.4679, -3.5826,  4.6873,  5.8694,  4.9226]],
       dtype=torch.float64)
	q_value: tensor([[-59.3777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 89
	action: tensor([[-1.0769,  0.3954,  1.4062, -2.6115, -0.7651,  0.1588, -0.4866]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 90
	action: tensor([[ 1.3613, -0.1089, -1.3069, -0.4696,  0.7536,  0.6901,  0.2243]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 91
	action: tensor([[-0.7633,  0.5773, -2.1356, -1.4259, -0.6245,  1.4942, -0.4275]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 92
	action: tensor([[ 1.3463, -1.7068, -0.5982, -0.3243, -0.7428, -0.2253, -0.9892]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9448247900082127, distance: 1.5958678811965918 entropy 1.448572039604187
epoch: 0, step: 93
	action: tensor([[ 2.9656, -4.3243,  0.6129, -1.2617,  2.2035,  2.8663,  2.1759]],
       dtype=torch.float64)
	q_value: tensor([[-52.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 94
	action: tensor([[ 0.8638, -0.9169,  2.7389, -1.2029,  1.9805,  1.8259,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 95
	action: tensor([[-0.0892, -1.2619,  0.8225,  0.1482,  1.1652, -0.4436,  0.6258]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8061329408095379, distance: 1.537912227915937 entropy 1.448572039604187
epoch: 0, step: 96
	action: tensor([[ 4.1889, -4.6924,  1.7230, -2.0080,  3.4005,  5.4286,  3.3261]],
       dtype=torch.float64)
	q_value: tensor([[-50.2797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 97
	action: tensor([[-0.1333, -0.8874, -0.5065, -0.3198, -0.4592, -1.7909, -1.0864]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014603711105381167, distance: 1.1359576861907417 entropy 1.448572039604187
epoch: 0, step: 98
	action: tensor([[ 3.5762, -1.7321,  0.8931, -2.2627,  0.8630,  0.9483, -1.3605]],
       dtype=torch.float64)
	q_value: tensor([[-55.0600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 99
	action: tensor([[ 1.1698,  2.3749,  0.5973,  0.6183,  1.4209, -0.3722, -0.8555]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 100
	action: tensor([[-0.2929, -1.1472, -1.0359,  0.0641,  0.3801, -1.3385,  1.1455]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.503845953032616, distance: 1.4033253451766394 entropy 1.448572039604187
epoch: 0, step: 101
	action: tensor([[ 2.6841, -3.5470, -0.5495,  0.2438,  1.6782,  1.1692, -0.2194]],
       dtype=torch.float64)
	q_value: tensor([[-51.8208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 102
	action: tensor([[ 1.8121,  0.7774,  0.3927,  1.1933,  1.4257, -0.4696, -0.0529]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 103
	action: tensor([[-1.4847, -2.5965,  0.0465,  2.4103,  0.3467,  0.6823,  0.6057]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 104
	action: tensor([[ 1.6819,  0.6220,  0.2429, -0.3825,  0.6275, -0.4767,  0.9326]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 105
	action: tensor([[-0.4379, -0.6480, -0.0967, -1.2335, -0.3953, -1.6544,  0.7019]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06970359780366864, distance: 1.1037414794092357 entropy 1.448572039604187
epoch: 0, step: 106
	action: tensor([[ 3.8104, -2.3132,  1.8410, -1.4625,  0.9870,  2.6813,  1.7392]],
       dtype=torch.float64)
	q_value: tensor([[-46.6487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 107
	action: tensor([[-1.0871,  0.6320, -0.8195,  0.2742,  1.6726,  0.0782, -0.5977]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4808874882404133, distance: 1.3925722132115306 entropy 1.448572039604187
epoch: 0, step: 108
	action: tensor([[-0.3593,  0.9519,  1.0789, -0.4807, -0.2828, -0.1280,  0.6198]],
       dtype=torch.float64)
	q_value: tensor([[-47.1386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 109
	action: tensor([[ 2.1481,  2.1494, -0.2088, -0.5711,  0.2449, -1.2421,  1.0520]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 110
	action: tensor([[ 1.5291,  0.3338,  0.8400,  0.8348,  0.1322, -1.7800, -0.5693]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 111
	action: tensor([[ 0.6723, -0.2616,  1.1587,  1.5414,  2.0509, -0.6317, -0.3292]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 112
	action: tensor([[ 2.4434, -0.6315,  0.1443, -1.8768,  0.6151, -1.1944, -0.9403]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 113
	action: tensor([[ 0.7857, -1.1712, -0.1159,  1.0254, -2.4858,  0.4594,  0.6657]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20755532467691307, distance: 1.0186879502886077 entropy 1.448572039604187
epoch: 0, step: 114
	action: tensor([[ 6.1800, -5.8740,  1.6873, -3.8854,  4.1430,  5.4060,  4.1122]],
       dtype=torch.float64)
	q_value: tensor([[-62.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 115
	action: tensor([[ 0.6281,  0.9994,  0.5425, -1.6225,  0.4226, -0.5217, -0.4520]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 116
	action: tensor([[-0.0501,  0.6290, -1.0826, -0.5490,  1.0878,  0.1184,  0.4145]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 117
	action: tensor([[ 0.9278, -1.4035,  0.1192,  1.6231,  1.4322, -0.0936,  0.2000]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7356847826459296, distance: 0.5883256414073582 entropy 1.448572039604187
epoch: 0, step: 118
	action: tensor([[ 5.9739, -3.1582,  0.1237,  0.0179,  1.6402,  3.2461,  1.9971]],
       dtype=torch.float64)
	q_value: tensor([[-63.8700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 119
	action: tensor([[-0.4578,  0.1774, -2.4965,  0.0225, -0.4826,  0.0690,  0.6669]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15753286616952655, distance: 1.2311851220389192 entropy 1.448572039604187
epoch: 0, step: 120
	action: tensor([[-0.9761, -0.6819, -0.6192, -2.7892, -0.5295, -0.0098,  1.1655]],
       dtype=torch.float64)
	q_value: tensor([[-41.7669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3756257670660469, distance: 0.9042305571261945 entropy 1.448572039604187
epoch: 0, step: 121
	action: tensor([[ 5.9311, -5.6042, -0.1295, -1.5176,  3.6834,  4.9578,  2.4035]],
       dtype=torch.float64)
	q_value: tensor([[-53.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 122
	action: tensor([[-0.7219,  1.1023, -1.4407, -1.5446, -0.6184, -0.3528, -0.2962]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 123
	action: tensor([[-0.5904,  2.5939,  0.3110, -0.6873, -0.1006, -0.8291, -0.3174]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 124
	action: tensor([[ 1.0070, -0.6344, -0.8402, -0.7196, -1.8437,  1.2497, -0.4876]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2851725097120117, distance: 1.2972910051932993 entropy 1.448572039604187
epoch: 0, step: 125
	action: tensor([[ 4.5272, -1.8036,  0.9247, -3.7288,  1.9939,  3.3987,  2.9592]],
       dtype=torch.float64)
	q_value: tensor([[-52.3747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 126
	action: tensor([[ 0.8162,  1.2349, -1.7584,  0.6294,  1.5654, -0.9342,  0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 0, step: 127
	action: tensor([[ 0.0545, -0.2446, -1.4342,  0.3187, -0.3236,  0.4627, -0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09120049612116032, distance: 1.1953882169909984 entropy 1.448572039604187
LOSS epoch 0 actor 268.7236661288976 critic 81.79804502474715 
epoch: 1, step: 0
	action: tensor([[ 1.6183, -1.7161,  0.6220,  0.4045, -1.5918,  3.8598,  2.4797]],
       dtype=torch.float64)
	q_value: tensor([[-25.5463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 1
	action: tensor([[-1.6781,  0.8288, -1.0854, -0.2709, -0.4661,  1.6085,  1.8415]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 2
	action: tensor([[ 0.8532, -0.4034, -1.4002, -2.1177,  0.1066,  0.1854,  0.5626]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 3
	action: tensor([[-0.7951, -0.5272,  0.2572, -1.6763,  0.3009,  0.7935, -1.8511]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.84440042934615, distance: 1.554119109558173 entropy 1.448572039604187
epoch: 1, step: 4
	action: tensor([[ 1.5463, -2.6101, -2.2626, -1.0679,  0.1160,  0.8009,  3.2755]],
       dtype=torch.float64)
	q_value: tensor([[-46.5291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 5
	action: tensor([[-0.9723, -0.6600,  1.9486,  2.0047,  0.0124, -1.0853,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10002661806286606, distance: 1.0856042269217894 entropy 1.448572039604187
epoch: 1, step: 6
	action: tensor([[ 6.1800, -5.6539,  0.4865, -3.1228,  1.8263,  4.8732,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-57.3944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 7
	action: tensor([[ 0.5131, -0.0102, -0.8026,  0.6081,  0.4229,  0.3422, -0.3816]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 8
	action: tensor([[ 0.1625,  0.6995,  2.3569, -0.8802, -0.2139, -0.7950, -0.8837]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 9
	action: tensor([[ 0.3401,  0.0186, -1.0110,  0.1658, -1.0443,  0.4124,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 10
	action: tensor([[ 0.7400, -1.0224, -0.2102, -1.5475, -0.4004, -1.5173,  1.5986]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10049656154200326, distance: 1.200469244909025 entropy 1.448572039604187
epoch: 1, step: 11
	action: tensor([[ 5.7569, -4.2832,  1.5089, -6.0369,  4.4497,  5.0153,  5.8785]],
       dtype=torch.float64)
	q_value: tensor([[-44.2870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 12
	action: tensor([[ 0.2604, -1.3240,  0.8415, -0.7270,  0.4482,  1.3856, -1.0066]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5787067459791453, distance: 1.4378296186767547 entropy 1.448572039604187
epoch: 1, step: 13
	action: tensor([[ 4.5296, -4.6790,  2.0676, -4.5931,  3.4248,  3.1023,  4.2082]],
       dtype=torch.float64)
	q_value: tensor([[-47.0748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 14
	action: tensor([[ 1.4716, -0.4541, -1.5194, -1.5301,  0.8825, -0.1339,  0.2113]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 15
	action: tensor([[ 0.5296,  0.9864, -1.0717,  0.4098, -1.2543, -0.7092, -1.1029]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 16
	action: tensor([[-0.8178, -1.5877,  0.0812,  0.6640, -0.3579,  0.1269,  0.3736]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0156918831814514, distance: 1.6246834883674726 entropy 1.448572039604187
epoch: 1, step: 17
	action: tensor([[ 2.4279, -2.1344,  1.2659, -1.7276,  0.8576,  4.1026,  2.7916]],
       dtype=torch.float64)
	q_value: tensor([[-34.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 18
	action: tensor([[-1.0826, -0.2883,  1.5756, -1.6311,  0.8028,  0.6466, -3.1019]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2697997301114294, distance: 1.7240524074095782 entropy 1.448572039604187
epoch: 1, step: 19
	action: tensor([[ 3.1069, -6.2800,  2.3095, -0.2185,  1.9543,  1.2973,  4.1499]],
       dtype=torch.float64)
	q_value: tensor([[-64.9308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 20
	action: tensor([[-0.7450,  1.1537, -0.4373,  0.6387,  0.1266, -0.8147,  0.6670]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 21
	action: tensor([[-0.1373, -0.0854,  0.2851,  0.4435, -0.5910,  1.2987,  1.4043]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 22
	action: tensor([[ 0.1182,  1.2179,  0.3001,  1.7220, -0.9700,  0.1467, -0.4766]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 23
	action: tensor([[ 0.4375, -0.7335,  2.3287,  0.0498,  0.0301, -0.2262,  0.6588]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09217114145560301, distance: 1.1959197600425475 entropy 1.448572039604187
epoch: 1, step: 24
	action: tensor([[ 5.5401, -5.3386,  1.4392, -0.7851,  3.5277,  5.1773,  5.6235]],
       dtype=torch.float64)
	q_value: tensor([[-46.0385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 25
	action: tensor([[ 0.2046,  0.1460, -0.0630,  0.9581,  1.2489,  1.0049,  0.6413]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 26
	action: tensor([[-1.9316, -0.5821, -0.4276, -0.4922,  0.2170,  0.6895,  0.3994]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 27
	action: tensor([[-1.1513,  1.6205,  1.0048, -1.5281,  1.6561, -0.4879, -1.2200]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6831522644319639, distance: 1.484630560645225 entropy 1.448572039604187
epoch: 1, step: 28
	action: tensor([[ 0.9043, -3.7278, -0.4852,  0.8109,  1.3410,  0.6904,  2.7210]],
       dtype=torch.float64)
	q_value: tensor([[-48.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 29
	action: tensor([[ 0.8732, -0.4409,  1.8422, -1.0567, -0.4018, -0.3173,  0.3958]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 30
	action: tensor([[ 1.7925,  1.0960, -0.1355,  1.8563,  0.2083,  0.8517,  0.7199]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 31
	action: tensor([[ 0.1186, -0.2636,  0.9775, -0.4758,  0.8005, -0.5609,  1.3145]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19385546552142507, distance: 1.2503527950262214 entropy 1.448572039604187
epoch: 1, step: 32
	action: tensor([[ 5.9626, -5.6028,  2.0029, -3.0744,  2.3370,  2.6995,  2.6420]],
       dtype=torch.float64)
	q_value: tensor([[-35.9411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 33
	action: tensor([[-1.0783,  0.1866, -0.4399, -0.5089, -1.1833, -0.3615, -1.0571]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7863596095148944, distance: 1.5294706186776066 entropy 1.448572039604187
epoch: 1, step: 34
	action: tensor([[ 1.6557, -0.0993,  2.0005, -2.1955, -0.9230, -1.3986,  0.8254]],
       dtype=torch.float64)
	q_value: tensor([[-33.7966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4437506885058896, distance: 0.8534762424183887 entropy 1.448572039604187
epoch: 1, step: 35
	action: tensor([[ 6.1800, -6.2800,  4.3788, -4.4997,  6.1800,  4.7573,  6.1391]],
       dtype=torch.float64)
	q_value: tensor([[-52.6976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 36
	action: tensor([[ 0.4604,  0.9648, -0.8819,  0.7481,  1.7443, -1.3413,  1.1847]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 37
	action: tensor([[-0.9312,  0.4746,  1.5815, -1.1672,  0.8190,  0.0456, -2.0697]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 38
	action: tensor([[ 0.3090, -1.0052, -1.1649,  0.7614,  1.3471,  1.6462,  0.3280]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3779549824868236, distance: 0.9025423743392426 entropy 1.448572039604187
epoch: 1, step: 39
	action: tensor([[ 5.8639, -4.4137,  1.7266, -3.8947,  2.0136,  1.4470,  2.5987]],
       dtype=torch.float64)
	q_value: tensor([[-46.3001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 40
	action: tensor([[ 0.3363, -0.1618,  1.3747,  1.4060,  1.3775, -0.4906,  0.9498]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 41
	action: tensor([[ 2.7072,  0.7562, -1.6287, -0.7548,  1.0440,  0.4140,  1.2408]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 42
	action: tensor([[ 0.1747,  1.7814,  0.9529,  1.5083, -0.6538, -0.0732,  0.4053]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 43
	action: tensor([[-0.9074,  0.9915, -1.3784, -1.2393,  1.1291, -0.1694,  1.2062]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 44
	action: tensor([[-1.5811, -0.1245,  2.4519, -0.3956,  0.3448,  0.2825,  1.8009]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6819099007456284, distance: 1.874039692344926 entropy 1.448572039604187
epoch: 1, step: 45
	action: tensor([[ 4.5877, -6.2800,  2.8175, -2.5775,  4.4627,  4.2759,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.3236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 46
	action: tensor([[ 2.6394,  0.6781,  0.6085, -2.3349, -0.5791,  0.7027,  0.6600]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 47
	action: tensor([[ 0.7386, -1.6213, -0.7125,  0.6782,  1.1101,  0.2379, -0.4626]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07393113705398258, distance: 1.1858913749163826 entropy 1.448572039604187
epoch: 1, step: 48
	action: tensor([[ 2.8043, -3.8249,  1.1958, -1.9404, -0.5098,  1.8368,  4.5063]],
       dtype=torch.float64)
	q_value: tensor([[-42.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 49
	action: tensor([[-0.4007, -0.2718, -0.8143,  0.6731,  0.9759,  0.9373, -0.9049]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03515600327148127, distance: 1.164285786936986 entropy 1.448572039604187
epoch: 1, step: 50
	action: tensor([[ 1.2658, -0.3722, -0.2182, -1.7235,  0.1984,  0.5777,  1.7529]],
       dtype=torch.float64)
	q_value: tensor([[-35.2484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3969633881954302, distance: 1.3525371584487234 entropy 1.448572039604187
epoch: 1, step: 51
	action: tensor([[ 5.3609, -5.6266,  2.6634, -4.5951,  3.3325,  5.5567,  5.9908]],
       dtype=torch.float64)
	q_value: tensor([[-45.1922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 52
	action: tensor([[ 0.4679,  1.4779, -1.2080, -1.3364, -0.2640, -0.9475,  1.1183]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 53
	action: tensor([[-2.3701, -0.8622, -0.1540,  0.2904,  0.7555,  0.8701,  1.0567]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 54
	action: tensor([[-0.5831, -2.3979,  1.0105,  0.5299, -0.9929, -0.0443,  0.7849]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 55
	action: tensor([[ 0.0410, -0.2722,  0.7843, -0.1636, -1.9263,  0.2647, -0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0188599119731776, distance: 1.1550849642875458 entropy 1.448572039604187
epoch: 1, step: 56
	action: tensor([[ 6.1800, -4.2406,  2.5666, -2.8192,  3.9146,  2.4838,  4.2259]],
       dtype=torch.float64)
	q_value: tensor([[-39.7549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 57
	action: tensor([[ 1.4487,  0.0204, -0.3941,  0.0152, -1.3891, -0.0909,  1.6118]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 58
	action: tensor([[ 2.0522, -1.2284,  0.2146,  2.0265,  0.9189,  0.2735, -0.1896]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5010772682045197, distance: 0.8083014150112154 entropy 1.448572039604187
epoch: 1, step: 59
	action: tensor([[ 6.1800, -5.8879,  2.0295, -4.3995,  2.0505,  2.1420,  3.6293]],
       dtype=torch.float64)
	q_value: tensor([[-56.8830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 60
	action: tensor([[-0.7088, -0.4080, -1.2676, -0.8331,  1.7094,  0.0225, -2.3810]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2120564664350264, distance: 1.2598479096281034 entropy 1.448572039604187
epoch: 1, step: 61
	action: tensor([[ 2.0097, -0.6133,  0.4329, -0.3020, -0.2115,  1.8081, -1.1638]],
       dtype=torch.float64)
	q_value: tensor([[-54.4071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2911716534040204, distance: 0.9634456756329779 entropy 1.448572039604187
epoch: 1, step: 62
	action: tensor([[ 4.9603, -5.2983,  1.9728, -5.6975,  3.2744,  3.1222,  4.4562]],
       dtype=torch.float64)
	q_value: tensor([[-48.7851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 63
	action: tensor([[ 1.6641, -0.6609,  0.8255, -0.9898, -1.7561,  0.2067, -1.2954]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05855123758312497, distance: 1.177369104885208 entropy 1.448572039604187
epoch: 1, step: 64
	action: tensor([[ 6.1800, -6.2800,  1.6743, -3.4452,  3.2748,  3.6483,  4.8481]],
       dtype=torch.float64)
	q_value: tensor([[-47.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 65
	action: tensor([[-0.8044, -0.3891, -0.5782, -2.8805,  1.6819,  0.1400,  0.5991]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 66
	action: tensor([[ 0.0188, -0.8056, -0.1210, -0.3904,  0.2659,  1.3406, -0.9967]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03533679459628658, distance: 1.1643874545017305 entropy 1.448572039604187
epoch: 1, step: 67
	action: tensor([[ 4.0003, -2.9496, -0.0417, -1.7489,  1.6549,  1.7901,  1.4657]],
       dtype=torch.float64)
	q_value: tensor([[-38.5969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 68
	action: tensor([[ 0.4935, -0.2513,  2.4428, -0.9068,  0.7876, -0.6148, -1.6514]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 69
	action: tensor([[-0.8220, -0.1756, -0.8433, -0.3050,  0.8709, -0.2179, -1.4338]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8028374716654201, distance: 1.5365085504449225 entropy 1.448572039604187
epoch: 1, step: 70
	action: tensor([[ 0.1423, -1.9864,  0.1105,  1.9266, -1.5551,  0.7147,  1.2199]],
       dtype=torch.float64)
	q_value: tensor([[-36.9355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 71
	action: tensor([[ 2.2356,  0.4904, -1.3629,  1.1188, -1.0910,  0.6288, -1.1228]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 72
	action: tensor([[ 0.3163, -2.4597, -1.6972, -1.3604,  0.0995,  1.6104,  0.2526]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 73
	action: tensor([[-0.5566,  0.2001,  0.6009, -0.1923,  1.7342,  0.8194, -0.7405]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17584874725675204, distance: 1.2408875397926518 entropy 1.448572039604187
epoch: 1, step: 74
	action: tensor([[ 2.0797, -2.5854,  0.3469, -2.1633,  1.1024,  1.7512,  3.0749]],
       dtype=torch.float64)
	q_value: tensor([[-43.7262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 75
	action: tensor([[-0.8122,  1.2218, -0.6800,  0.1429,  0.4550, -0.1175, -0.5250]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 76
	action: tensor([[ 1.5313, -0.3579,  0.1355, -1.2260,  0.1711, -0.2951,  0.7713]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.334670883144774, distance: 1.3220375368439001 entropy 1.448572039604187
epoch: 1, step: 77
	action: tensor([[ 6.1800, -5.9957,  2.2875, -2.5830,  3.1832,  4.4545,  4.3059]],
       dtype=torch.float64)
	q_value: tensor([[-37.0977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 78
	action: tensor([[-0.6296, -0.6038,  0.8578, -1.4498,  0.1485, -0.0261,  0.4305]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.156888037182437, distance: 1.6806237406110727 entropy 1.448572039604187
epoch: 1, step: 79
	action: tensor([[ 6.0439, -4.1013,  1.2461, -1.7061,  1.6601,  1.2389,  3.7018]],
       dtype=torch.float64)
	q_value: tensor([[-34.0855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 80
	action: tensor([[-0.9564, -0.6932,  0.9931,  0.0767,  2.3771,  0.6850,  0.5486]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2407213693349042, distance: 1.7129734086718669 entropy 1.448572039604187
epoch: 1, step: 81
	action: tensor([[ 5.5640, -2.9580,  1.2013, -3.9136,  2.8524,  3.8264,  3.7230]],
       dtype=torch.float64)
	q_value: tensor([[-50.8463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 82
	action: tensor([[ 1.5637, -0.8575, -1.3255, -1.8501, -0.6490,  0.9777,  0.9364]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6585329817029051, distance: 1.473732799942637 entropy 1.448572039604187
epoch: 1, step: 83
	action: tensor([[ 6.1800, -6.2800,  1.8394, -4.9890,  2.5486,  5.3339,  5.8950]],
       dtype=torch.float64)
	q_value: tensor([[-47.4448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 84
	action: tensor([[ 1.3809,  1.3831,  0.5031, -0.2348,  2.5981, -0.3709,  1.6260]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 85
	action: tensor([[ 0.3143,  0.3688,  0.4135, -0.2480, -0.2958, -0.5204, -0.3580]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 86
	action: tensor([[ 0.2192,  0.7756, -0.0708, -1.2467,  0.5250, -0.8950, -1.1879]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 87
	action: tensor([[ 1.1103,  0.0291, -1.4543, -0.7787,  0.4122,  0.7886,  1.1855]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6874245294921688, distance: 0.6397851272643765 entropy 1.448572039604187
epoch: 1, step: 88
	action: tensor([[ 5.3841, -5.1865,  4.0713, -2.2200,  1.8756,  2.8035,  3.1946]],
       dtype=torch.float64)
	q_value: tensor([[-39.2974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 89
	action: tensor([[ 1.5509,  0.1177, -0.7273,  0.6333, -0.2841,  1.8282, -0.4468]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 90
	action: tensor([[ 1.1813,  1.8167,  0.3577, -0.5538, -0.5874,  0.7971, -1.5021]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 91
	action: tensor([[ 0.2354,  0.1395, -0.3052,  1.5224, -1.0633, -0.3640, -1.8917]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 92
	action: tensor([[-0.7239,  1.6503, -1.0264, -0.1553, -0.7564,  0.7691, -0.5514]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 93
	action: tensor([[-0.0806,  0.7552,  1.2852, -0.3405, -0.0779,  1.0558,  0.7001]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 94
	action: tensor([[-0.7980,  0.5702,  0.6502, -0.4944,  1.6395, -1.4079,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41983501620772, distance: 1.363564338622593 entropy 1.448572039604187
epoch: 1, step: 95
	action: tensor([[ 2.9983, -3.8643,  1.5438, -1.7683,  1.3156,  1.4747,  1.4890]],
       dtype=torch.float64)
	q_value: tensor([[-40.7173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 96
	action: tensor([[-0.7659, -0.1230,  0.7828, -0.6538, -0.1341,  1.0909, -1.0373]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7135022089131575, distance: 1.4979558990065514 entropy 1.448572039604187
epoch: 1, step: 97
	action: tensor([[ 3.3640, -3.2737,  1.0751, -2.7893,  0.3214,  2.1253,  2.0087]],
       dtype=torch.float64)
	q_value: tensor([[-36.6792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 98
	action: tensor([[ 2.0030, -0.7018, -1.7658, -0.5728,  0.2796,  1.3821,  1.1042]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02894538952708825, distance: 1.127660895835736 entropy 1.448572039604187
epoch: 1, step: 99
	action: tensor([[ 6.1800, -4.8151,  3.8784, -3.5774,  2.7860,  5.2143,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 100
	action: tensor([[-0.4186,  0.1754,  0.5593, -0.0674, -0.3819,  0.2830, -0.2745]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03850941083990311, distance: 1.1661701250634058 entropy 1.448572039604187
epoch: 1, step: 101
	action: tensor([[ 3.1916, -4.3984,  0.6220, -0.9107,  1.8991,  1.1751,  0.4971]],
       dtype=torch.float64)
	q_value: tensor([[-26.1581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 102
	action: tensor([[ 1.3707,  0.1676,  1.4764, -2.3163, -0.8975,  1.4788, -0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8635820123059612, distance: 0.42266132750436175 entropy 1.448572039604187
epoch: 1, step: 103
	action: tensor([[ 6.1800, -5.6076,  4.0169, -6.0922,  1.7032,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 104
	action: tensor([[-0.6546, -1.7368,  0.5935,  0.0683, -0.0503,  2.5962,  2.7172]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11605744434078202, distance: 1.20892669500905 entropy 1.448572039604187
epoch: 1, step: 105
	action: tensor([[ 6.1800, -6.2800,  1.7993, -5.3853,  5.2826,  6.1800,  5.3558]],
       dtype=torch.float64)
	q_value: tensor([[-53.9536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 106
	action: tensor([[-0.4359,  0.3424, -1.1081, -0.1976, -1.2432,  1.3653, -2.3415]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16761262988003023, distance: 1.2365340654109118 entropy 1.448572039604187
epoch: 1, step: 107
	action: tensor([[ 0.4205, -2.2618,  1.0229, -0.9329,  1.5374,  2.3593,  2.4493]],
       dtype=torch.float64)
	q_value: tensor([[-42.7203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 108
	action: tensor([[ 0.9958, -2.1870,  0.7089, -0.5512, -0.3358, -2.0779,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 109
	action: tensor([[-1.0198,  0.3723, -0.2026, -2.1582, -1.2924, -0.4354,  1.2670]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03843346839190054, distance: 1.1661274853760466 entropy 1.448572039604187
epoch: 1, step: 110
	action: tensor([[ 6.1800, -2.9557,  0.8214, -1.4233,  1.5603,  3.4063,  3.9814]],
       dtype=torch.float64)
	q_value: tensor([[-38.6790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 111
	action: tensor([[ 1.2545, -1.9981,  0.3125,  1.0938, -1.4007, -0.5159,  0.1378]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 112
	action: tensor([[-0.1506, -0.6767, -0.3153,  0.1297, -1.7813,  0.5088, -0.6692]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5690422743346801, distance: 1.4334218349757177 entropy 1.448572039604187
epoch: 1, step: 113
	action: tensor([[ 4.1091, -4.0787,  2.7483, -0.9950, -0.0154,  3.4298,  3.4475]],
       dtype=torch.float64)
	q_value: tensor([[-37.9922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 114
	action: tensor([[ 1.6115, -3.0641,  0.2103,  0.0459,  0.5344, -0.8378,  1.4874]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 115
	action: tensor([[ 0.2634, -2.0459,  0.0566, -1.6906, -0.4524, -1.4891, -2.1516]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 116
	action: tensor([[-0.1265,  0.2273,  1.2143, -0.6533, -0.2577, -1.0902, -0.5326]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25853139644461987, distance: 1.2837744276266878 entropy 1.448572039604187
epoch: 1, step: 117
	action: tensor([[ 5.1773, -5.2752, -1.5350, -1.4041,  1.6994,  0.6400,  0.7489]],
       dtype=torch.float64)
	q_value: tensor([[-35.2752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 118
	action: tensor([[-1.0561, -0.9579, -0.5013, -1.3792,  0.7175,  0.2969, -0.3522]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3737305907732793, distance: 1.3412430296102247 entropy 1.448572039604187
epoch: 1, step: 119
	action: tensor([[ 3.0708, -3.0491,  1.2379, -2.9318,  0.9762, -0.0043,  0.7257]],
       dtype=torch.float64)
	q_value: tensor([[-39.6489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 120
	action: tensor([[-1.2906,  0.0702,  1.4469,  0.5289, -0.2873,  1.4822, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027006042433340594, distance: 1.1287863912706524 entropy 1.448572039604187
epoch: 1, step: 121
	action: tensor([[ 6.1800, -4.7312,  0.3508, -2.4138,  1.6825,  4.0269,  5.9810]],
       dtype=torch.float64)
	q_value: tensor([[-41.0393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 122
	action: tensor([[ 1.6812,  1.8489, -1.6758,  2.2151, -0.0219,  0.7843, -1.4500]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 123
	action: tensor([[ 0.6824,  0.7803, -0.2116, -1.3348, -0.7416, -0.6286, -0.4085]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 124
	action: tensor([[-0.4131, -0.9152, -1.9574, -0.2618, -0.0845, -0.1532,  1.4673]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5056491307802087, distance: 1.4041664176799487 entropy 1.448572039604187
epoch: 1, step: 125
	action: tensor([[ 4.5234, -5.8891,  0.6738, -1.3253, -0.9493,  3.9583,  2.9126]],
       dtype=torch.float64)
	q_value: tensor([[-39.6462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 126
	action: tensor([[-0.2075,  0.5088,  1.6863,  0.8523,  1.3024, -0.4603,  0.3000]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 1, step: 127
	action: tensor([[ 0.0078,  1.9786,  0.3295,  0.0831, -1.6455, -0.5948,  1.1522]],
       dtype=torch.float64)
	q_value: tensor([[-32.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
LOSS epoch 1 actor 177.24772054852744 critic 239.2374128828893 
epoch: 2, step: 0
	action: tensor([[ 0.4733,  3.4256, -0.0640,  0.2060, -0.8241,  1.2613, -1.0739]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 1
	action: tensor([[ 1.5112,  0.3013, -0.7264, -0.0558,  0.6623,  1.4659,  0.0292]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 2
	action: tensor([[-0.4262,  0.3204,  0.4946,  1.2559, -0.0918,  0.1534,  0.7478]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 3
	action: tensor([[ 1.7983,  0.1623,  0.0647,  0.2164,  0.8433,  0.2404, -1.2678]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 4
	action: tensor([[ 1.2037, -1.0083,  0.4974, -1.8219, -0.4896,  0.4275, -0.3056]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.464523203605425, distance: 1.3848566529692115 entropy 1.448572039604187
epoch: 2, step: 5
	action: tensor([[ 5.7868, -6.2800,  2.8290, -2.9041,  1.6516,  3.6449,  4.5680]],
       dtype=torch.float64)
	q_value: tensor([[-44.1138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 6
	action: tensor([[-2.0194, -1.2446,  1.3435, -0.0821, -1.3644, -0.9790, -0.9607]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 7
	action: tensor([[-0.7157, -0.3085,  0.8112, -0.2233,  0.6197,  1.1007,  1.7360]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4428037261043116, distance: 1.3745492996532658 entropy 1.448572039604187
epoch: 2, step: 8
	action: tensor([[ 5.8235, -4.4718,  1.6813, -4.0439,  1.4158,  3.2964,  4.0688]],
       dtype=torch.float64)
	q_value: tensor([[-42.0944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 9
	action: tensor([[ 0.1848, -0.5704, -1.4154, -0.1654,  1.0542,  1.7544,  1.5792]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44966408964245164, distance: 0.8489275334284466 entropy 1.448572039604187
epoch: 2, step: 10
	action: tensor([[ 4.0496, -6.2800,  3.1900, -2.0102,  2.5650,  3.0721,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.3515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 11
	action: tensor([[ 1.0000,  0.4225,  0.6314, -0.3428, -0.0093, -0.5351, -1.5277]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8359657223677026, distance: 0.46347252985158865 entropy 1.448572039604187
epoch: 2, step: 12
	action: tensor([[ 2.1474, -2.5340, -0.4995,  0.1845,  2.1177,  1.9209,  0.5959]],
       dtype=torch.float64)
	q_value: tensor([[-40.0981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 13
	action: tensor([[-0.9377,  0.5330,  0.3941, -0.3029, -0.1180, -0.0164, -0.4817]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.646369231928619, distance: 1.4683186459841675 entropy 1.448572039604187
epoch: 2, step: 14
	action: tensor([[ 0.7725, -0.6393,  1.3521, -1.0647,  2.4763, -0.1134,  0.6466]],
       dtype=torch.float64)
	q_value: tensor([[-30.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02902807788408901, distance: 1.1608344971173585 entropy 1.448572039604187
epoch: 2, step: 15
	action: tensor([[ 5.0621, -4.7009,  1.9511, -1.4456,  0.0655,  3.8700,  3.7513]],
       dtype=torch.float64)
	q_value: tensor([[-55.4599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 16
	action: tensor([[-1.0314,  0.1785, -0.7124, -0.6900, -0.2410, -0.6870,  0.7940]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3579875850881924, distance: 1.33353553544552 entropy 1.448572039604187
epoch: 2, step: 17
	action: tensor([[ 3.1996, -0.5745,  1.9832, -0.2435,  1.4158,  1.6344,  0.4125]],
       dtype=torch.float64)
	q_value: tensor([[-31.2495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 18
	action: tensor([[ 0.2150,  0.8107,  0.9180, -0.1401, -1.5186, -0.8594, -0.7610]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 19
	action: tensor([[-0.2082,  0.8468,  0.0382,  0.7177, -1.2984, -0.5305, -0.5136]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 20
	action: tensor([[ 0.9888, -1.5907, -1.5919,  1.3536, -0.5210, -1.3673,  1.2373]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012011164145997233, distance: 1.1511961938077035 entropy 1.448572039604187
epoch: 2, step: 21
	action: tensor([[ 3.9634, -3.5083,  1.6700, -1.5791,  2.1710,  1.8615,  1.9770]],
       dtype=torch.float64)
	q_value: tensor([[-54.1572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 22
	action: tensor([[ 0.7284, -1.1599, -2.0695, -1.0250,  0.4660, -0.7155,  0.9947]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 23
	action: tensor([[-0.3651, -1.0519,  0.1004, -1.2182,  0.6604,  0.2874, -0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8303660681587899, distance: 1.5481950384557297 entropy 1.448572039604187
epoch: 2, step: 24
	action: tensor([[ 1.9158, -1.2763,  0.0712, -1.0369,  0.3664,  1.4659,  2.8603]],
       dtype=torch.float64)
	q_value: tensor([[-39.3252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 25
	action: tensor([[ 0.2293, -0.4012, -0.6371, -0.9024, -0.8675,  1.7823,  1.7257]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017552848969146884, distance: 1.1342565400637157 entropy 1.448572039604187
epoch: 2, step: 26
	action: tensor([[ 5.7581, -5.6736,  3.2113, -0.8245,  2.9460,  3.2736,  5.0388]],
       dtype=torch.float64)
	q_value: tensor([[-45.8244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 27
	action: tensor([[ 1.8053, -0.8874,  1.6461,  0.1933, -0.4856, -0.7635, -1.4093]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4182618906745501, distance: 1.362808739435499 entropy 1.448572039604187
epoch: 2, step: 28
	action: tensor([[ 6.0557, -4.6376,  1.6923, -0.9067,  0.6851,  0.9903,  2.6028]],
       dtype=torch.float64)
	q_value: tensor([[-56.2135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 29
	action: tensor([[ 0.6520, -1.4808,  1.4524, -1.8627, -0.4280, -0.0173,  1.0279]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12467006243189194, distance: 1.213582375462031 entropy 1.448572039604187
epoch: 2, step: 30
	action: tensor([[ 5.0273, -6.2800,  3.2476, -3.3055,  2.3191,  3.6939,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.4431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 31
	action: tensor([[-0.7894, -0.1239,  0.9677, -0.5184,  0.3370,  1.1292,  0.4918]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5601098672380229, distance: 1.4293358449132632 entropy 1.448572039604187
epoch: 2, step: 32
	action: tensor([[ 3.0946, -4.3344, -0.1220, -1.6604,  1.0514,  0.7386,  3.3094]],
       dtype=torch.float64)
	q_value: tensor([[-37.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 33
	action: tensor([[-1.5616, -2.7002, -0.2285, -0.5955,  0.1588, -0.2455,  2.0133]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 34
	action: tensor([[ 0.7573, -1.1850, -1.3849, -1.1318,  0.0661,  0.8320,  1.3982]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40348960266022504, distance: 1.3556928110619917 entropy 1.448572039604187
epoch: 2, step: 35
	action: tensor([[ 5.0598, -5.7707,  3.2729, -2.5109,  0.8298,  1.1451,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.6609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 36
	action: tensor([[-0.5038, -1.0240, -0.7714, -0.4988, -0.9205,  0.3979, -0.5734]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9236085080359648, distance: 1.5871392719751911 entropy 1.448572039604187
epoch: 2, step: 37
	action: tensor([[ 2.9520, -3.4103,  0.3529, -0.9351,  2.9670,  0.3196,  2.4293]],
       dtype=torch.float64)
	q_value: tensor([[-37.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 38
	action: tensor([[-2.4277, -2.7363, -0.9612, -0.0677,  0.9131,  0.2368,  0.3528]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 39
	action: tensor([[ 0.6493, -2.6764, -1.1444,  1.8309, -0.7298,  0.1308, -0.4329]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 40
	action: tensor([[-0.1161, -1.3077,  1.7504,  0.5457, -0.3161,  2.7583,  2.1035]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 41
	action: tensor([[-0.4060, -1.1780,  0.1874,  0.3988,  0.2982,  1.1514, -0.4407]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21669537068790978, distance: 1.2622565154468088 entropy 1.448572039604187
epoch: 2, step: 42
	action: tensor([[ 2.5977, -0.9969,  2.3066, -1.1629,  0.5977,  3.5311,  2.0421]],
       dtype=torch.float64)
	q_value: tensor([[-38.7370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 43
	action: tensor([[-0.2388, -1.5935,  1.3223,  0.8500,  0.2257, -1.1100,  0.5066]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5000734340198758, distance: 1.4015640624277226 entropy 1.448572039604187
epoch: 2, step: 44
	action: tensor([[ 4.2599, -2.6476,  1.4229, -2.0037,  2.5050,  2.4587,  3.4105]],
       dtype=torch.float64)
	q_value: tensor([[-45.6706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 45
	action: tensor([[ 1.6631,  0.6574,  2.3522, -1.1302, -0.9419,  0.4313,  0.9693]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 46
	action: tensor([[ 0.0050, -2.3378, -0.0712, -0.6943, -0.6447,  1.1513, -0.5113]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 47
	action: tensor([[ 0.3859, -1.0688, -0.1392,  0.1771, -1.7585, -0.2749, -0.7893]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3106111058653991, distance: 1.310067325955438 entropy 1.448572039604187
epoch: 2, step: 48
	action: tensor([[ 4.7169, -3.4630,  3.3813, -2.7832,  3.7676,  2.6889,  3.5558]],
       dtype=torch.float64)
	q_value: tensor([[-43.0067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 49
	action: tensor([[-0.5155,  0.0799,  0.4897, -1.2937,  1.8223,  1.9628, -1.7913]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07014416258301426, distance: 1.1034800964752276 entropy 1.448572039604187
epoch: 2, step: 50
	action: tensor([[ 3.9834, -4.3228, -0.2263, -3.1265,  1.1602,  1.4293,  1.9751]],
       dtype=torch.float64)
	q_value: tensor([[-59.9571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 51
	action: tensor([[-0.2054, -1.1571,  0.8702,  1.4163,  0.7691,  1.3671,  2.0231]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6425920906524918, distance: 0.6841302167503427 entropy 1.448572039604187
epoch: 2, step: 52
	action: tensor([[ 5.6721, -6.2800,  3.5165, -4.3666,  2.4023,  3.4228,  5.5198]],
       dtype=torch.float64)
	q_value: tensor([[-51.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 53
	action: tensor([[ 1.0017, -0.2028,  0.5654,  2.3181,  0.2547, -0.2656, -1.0164]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 54
	action: tensor([[-0.7166,  1.2061,  0.1024, -0.8617,  0.4181, -0.0431,  1.1868]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09743363350524548, distance: 1.1987974936983385 entropy 1.448572039604187
epoch: 2, step: 55
	action: tensor([[ 3.6617, -1.4955,  0.6089, -0.2558,  0.6127,  1.3808,  1.8101]],
       dtype=torch.float64)
	q_value: tensor([[-36.2529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 56
	action: tensor([[ 0.6123,  1.0054,  1.1765,  0.0940,  0.8034,  0.9128, -0.2214]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 57
	action: tensor([[ 0.3806, -0.5907,  0.9602, -0.6964,  1.0498, -0.4076, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35527854974405737, distance: 1.332204743316338 entropy 1.448572039604187
epoch: 2, step: 58
	action: tensor([[ 2.3939, -4.7396,  2.2770, -2.1427,  1.1408,  3.6627,  2.3265]],
       dtype=torch.float64)
	q_value: tensor([[-38.1894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 59
	action: tensor([[-0.8967,  0.7752,  0.7138,  0.5996,  0.5467,  1.1933, -0.4040]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 60
	action: tensor([[-0.0081, -0.7917,  0.5434, -1.3700,  0.7498, -1.0018, -1.5074]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6443714233484237, distance: 1.467427500191332 entropy 1.448572039604187
epoch: 2, step: 61
	action: tensor([[ 1.2509, -0.6938,  1.8489, -0.8672,  0.0611,  0.0682,  2.6394]],
       dtype=torch.float64)
	q_value: tensor([[-49.2847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34278280413360207, distance: 0.9277076808632262 entropy 1.448572039604187
epoch: 2, step: 62
	action: tensor([[ 5.0834, -3.9428,  4.1340, -5.5048,  3.6279,  5.3466,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-54.7684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 63
	action: tensor([[-0.4758,  0.7418,  0.0591,  1.4666, -1.8953, -0.3716, -0.3215]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 64
	action: tensor([[ 0.4968, -0.4412,  1.1040, -1.7166,  1.5460,  0.2490,  0.5450]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3589510015265651, distance: 1.3340084861139487 entropy 1.448572039604187
epoch: 2, step: 65
	action: tensor([[ 6.1800, -5.6112,  2.0221, -1.0131,  1.0463,  1.9041,  4.2517]],
       dtype=torch.float64)
	q_value: tensor([[-45.7029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 66
	action: tensor([[-0.1861,  1.5743, -0.6132,  1.3267, -0.7456,  0.9912,  0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 67
	action: tensor([[-1.1200,  0.2009, -1.3123,  0.9810,  0.6617, -0.1579,  0.0635]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1621395968313855, distance: 1.6826684758751966 entropy 1.448572039604187
epoch: 2, step: 68
	action: tensor([[ 0.6388, -0.8458, -0.0629, -2.0347,  0.0667,  0.7520,  0.8445]],
       dtype=torch.float64)
	q_value: tensor([[-33.8608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5813870978139843, distance: 1.4390496853091224 entropy 1.448572039604187
epoch: 2, step: 69
	action: tensor([[ 4.7778, -6.2800,  3.2319, -3.2951,  1.8559,  3.3512,  5.2072]],
       dtype=torch.float64)
	q_value: tensor([[-44.4811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 70
	action: tensor([[-0.1425,  0.4499, -0.3486,  0.9113,  0.6329, -0.7599,  0.3784]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 71
	action: tensor([[-0.3225, -0.1114,  0.2893, -0.7305,  0.7466,  0.3262, -1.7636]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4315529498413162, distance: 1.3691795419926605 entropy 1.448572039604187
epoch: 2, step: 72
	action: tensor([[ 0.7443, -0.3034,  0.9086, -3.0754, -0.6482, -0.0849,  1.8889]],
       dtype=torch.float64)
	q_value: tensor([[-42.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 73
	action: tensor([[ 1.3662, -1.1295, -1.5254, -0.9597, -1.1885, -0.5500, -1.1768]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 74
	action: tensor([[-1.1339, -0.7062, -1.1032, -0.5973,  0.9190, -0.4670, -0.1612]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8261869330452163, distance: 1.5464265906672463 entropy 1.448572039604187
epoch: 2, step: 75
	action: tensor([[ 0.6331, -0.3369,  0.6808, -0.1633,  1.6124, -0.0509,  2.0522]],
       dtype=torch.float64)
	q_value: tensor([[-37.8975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34199304892271987, distance: 0.9282649106559346 entropy 1.448572039604187
epoch: 2, step: 76
	action: tensor([[ 4.0981, -6.0307,  2.8740, -2.4770,  1.7649,  1.3618,  3.2100]],
       dtype=torch.float64)
	q_value: tensor([[-47.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 77
	action: tensor([[-1.1858,  0.5726,  0.6638, -1.9543,  0.3152,  1.2697, -0.6741]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 78
	action: tensor([[-1.5947, -1.8445, -2.2439, -0.6850,  0.4451,  0.0477,  0.1354]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 79
	action: tensor([[-0.8145, -0.3970,  0.3398,  0.8681, -0.3586,  0.8736, -0.2834]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03793440940733861, distance: 1.1658472380829814 entropy 1.448572039604187
epoch: 2, step: 80
	action: tensor([[ 3.0364, -2.3254, -0.6917, -1.7130,  1.0287,  2.2309,  2.8925]],
       dtype=torch.float64)
	q_value: tensor([[-34.8594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 81
	action: tensor([[ 1.2854, -1.0844,  0.3502,  0.2800, -0.9675,  0.8791, -0.1852]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0709225182201666, distance: 1.1030181540243655 entropy 1.448572039604187
epoch: 2, step: 82
	action: tensor([[ 6.1800, -4.6951,  1.5582, -1.7288,  1.6134,  1.4123,  3.7039]],
       dtype=torch.float64)
	q_value: tensor([[-41.3302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 83
	action: tensor([[ 1.1282, -1.0404,  1.4385, -0.1356,  0.4158,  1.0655,  1.0178]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4053039644909655, distance: 1.3565688142932217 entropy 1.448572039604187
epoch: 2, step: 84
	action: tensor([[ 5.6266, -6.1158,  2.7729, -4.5855,  2.7566,  3.0963,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.4410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 85
	action: tensor([[-0.8407,  0.3372, -0.2302,  0.1080,  1.7225,  0.1438,  0.5602]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 86
	action: tensor([[ 1.7920, -0.1486, -0.1005,  0.1201, -0.0361,  0.9057, -0.6478]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 87
	action: tensor([[ 0.6968, -0.5201,  1.2471, -0.9043,  0.6521, -1.2956,  0.5667]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26615875105313114, distance: 0.9802971794933695 entropy 1.448572039604187
epoch: 2, step: 88
	action: tensor([[ 4.7778, -5.8637,  0.5666, -1.7225,  1.8278,  2.9762,  1.9730]],
       dtype=torch.float64)
	q_value: tensor([[-41.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 89
	action: tensor([[-0.0932, -0.7225, -0.3451,  1.8712, -0.0811, -0.6837,  1.9445]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3137785640662707, distance: 0.9479574267760188 entropy 1.448572039604187
epoch: 2, step: 90
	action: tensor([[ 5.0162, -3.0662,  2.7264, -1.0976,  1.4212,  1.4106,  3.5007]],
       dtype=torch.float64)
	q_value: tensor([[-48.3006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 91
	action: tensor([[-0.9292, -0.3241,  0.1014, -0.6807,  0.5270,  0.2091,  1.6982]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1826252694830264, distance: 1.6906210914986195 entropy 1.448572039604187
epoch: 2, step: 92
	action: tensor([[ 4.6645, -3.4489,  2.1747, -2.0502,  1.4678,  1.9477,  3.8711]],
       dtype=torch.float64)
	q_value: tensor([[-37.0871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 93
	action: tensor([[0.0683, 0.3406, 1.8844, 1.1404, 0.1549, 0.4724, 0.3105]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 94
	action: tensor([[-0.8211, -1.1166,  0.3088,  1.7241,  1.0866,  0.6500, -0.1888]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30374189814208474, distance: 0.9548646687336224 entropy 1.448572039604187
epoch: 2, step: 95
	action: tensor([[ 2.4589, -2.4569,  0.0420, -1.6531,  0.1134,  2.3408,  1.8692]],
       dtype=torch.float64)
	q_value: tensor([[-44.4907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 96
	action: tensor([[ 1.0157, -1.0844, -1.2113, -1.4129,  0.0091, -1.1633, -1.5900]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 97
	action: tensor([[ 0.5901,  1.3635,  0.3105, -0.7555,  2.9377,  0.6030,  1.2417]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 98
	action: tensor([[ 1.3222, -0.9336,  0.8156,  1.0886,  0.0807, -0.2693,  0.0661]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049510819793893046, distance: 1.1156559398794055 entropy 1.448572039604187
epoch: 2, step: 99
	action: tensor([[ 5.4679, -4.9753, -0.1777, -2.5647, -1.4893,  4.4306,  2.1662]],
       dtype=torch.float64)
	q_value: tensor([[-45.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 100
	action: tensor([[-0.1688, -1.5107,  0.7698, -0.9483,  0.9080, -0.9375,  0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9892176228040761, distance: 1.6139788610984145 entropy 1.448572039604187
epoch: 2, step: 101
	action: tensor([[ 2.2664, -1.9329,  0.3890, -2.4069,  2.1442,  0.2219,  0.9274]],
       dtype=torch.float64)
	q_value: tensor([[-41.6992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 102
	action: tensor([[-1.2459, -1.3459, -0.2018,  1.6817, -0.1162,  0.5318, -0.4927]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5159316555104114, distance: 1.4089529939309726 entropy 1.448572039604187
epoch: 2, step: 103
	action: tensor([[ 3.2812, -1.8180,  1.5667, -0.7795,  1.6337,  1.7557,  2.8173]],
       dtype=torch.float64)
	q_value: tensor([[-45.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 104
	action: tensor([[-0.7497, -0.5901, -0.0020,  1.2191, -1.4748, -0.2388, -0.3282]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1524271302984821, distance: 1.2284668177183398 entropy 1.448572039604187
epoch: 2, step: 105
	action: tensor([[ 3.6446, -2.8526, -0.1129, -1.6669,  0.6264, -0.3125,  2.6455]],
       dtype=torch.float64)
	q_value: tensor([[-41.5931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 106
	action: tensor([[-0.7033,  0.1943, -1.2261,  0.0145,  1.4804, -1.0929,  0.3355]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5272366477605697, distance: 1.414196837500741 entropy 1.448572039604187
epoch: 2, step: 107
	action: tensor([[2.6101, 0.2829, 1.3693, 0.3571, 2.0749, 0.2339, 1.8603]],
       dtype=torch.float64)
	q_value: tensor([[-38.7586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 108
	action: tensor([[ 1.0177, -1.8086,  0.0895, -0.1141, -0.4046, -0.2680, -0.6771]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 109
	action: tensor([[ 0.5133, -1.9128, -0.7534,  0.9654,  0.8095,  0.3210, -0.7847]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 110
	action: tensor([[-0.8211,  0.1580, -3.1958,  0.3141, -1.4635, -1.6969,  1.0393]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.477204346223245 entropy 1.448572039604187
epoch: 2, step: 111
	action: tensor([[-0.3965,  0.7705, -0.1681,  0.2773, -1.3967, -0.4559,  1.1483]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 112
	action: tensor([[ 1.3123, -0.7325, -0.4417, -1.2775, -1.6868,  0.5954,  1.1393]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6840605454777895, distance: 1.4850310829146354 entropy 1.448572039604187
epoch: 2, step: 113
	action: tensor([[ 4.7914, -6.2800,  2.4458, -3.8189,  3.1369,  5.1877,  4.9305]],
       dtype=torch.float64)
	q_value: tensor([[-45.3126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 114
	action: tensor([[ 0.2684, -0.8565,  0.8480,  0.5670,  1.0118,  0.3106,  0.4976]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2282411916234599, distance: 1.0053041855872518 entropy 1.448572039604187
epoch: 2, step: 115
	action: tensor([[ 3.1566, -2.3370,  0.9974, -3.1022,  1.9962,  3.8248,  3.0483]],
       dtype=torch.float64)
	q_value: tensor([[-38.5517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 116
	action: tensor([[ 0.3670, -0.9434, -1.2120,  0.3783, -0.0689, -0.6781,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2738882824980464, distance: 1.2915831325833846 entropy 1.448572039604187
epoch: 2, step: 117
	action: tensor([[ 1.2020, -1.7437,  1.9733, -1.4270,  2.7021, -0.0905,  1.0209]],
       dtype=torch.float64)
	q_value: tensor([[-35.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 118
	action: tensor([[ 0.9008,  0.1922, -1.2290, -2.3389,  0.9756, -1.1460,  0.8276]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 119
	action: tensor([[ 0.0664, -0.4883,  1.3310, -0.9121,  0.7494, -0.1303,  0.9833]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5324719490250367, distance: 1.416618666633591 entropy 1.448572039604187
epoch: 2, step: 120
	action: tensor([[ 5.3108, -2.9016,  1.3062, -3.4003,  3.1681,  4.5538,  4.9124]],
       dtype=torch.float64)
	q_value: tensor([[-37.9855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 121
	action: tensor([[ 0.4584, -0.4304, -0.9273, -1.2203, -0.2429, -0.2825, -0.2542]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11548550680434233, distance: 1.0762401011019809 entropy 1.448572039604187
epoch: 2, step: 122
	action: tensor([[ 3.9396, -1.3563,  0.2132, -1.5051,  2.1102, -0.5984,  1.2598]],
       dtype=torch.float64)
	q_value: tensor([[-33.2307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 123
	action: tensor([[-1.3462,  0.3764, -1.8867,  1.0894,  1.6378,  1.5639,  0.6077]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5880454164013196, distance: 1.4420760116088556 entropy 1.448572039604187
epoch: 2, step: 124
	action: tensor([[ 1.2554,  0.5908,  1.3181, -1.9579,  0.2238,  0.3504,  1.6668]],
       dtype=torch.float64)
	q_value: tensor([[-48.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 125
	action: tensor([[-0.5308, -1.7859, -0.7501,  0.6270,  0.8511,  0.9559, -0.7814]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 2, step: 126
	action: tensor([[ 0.1152, -1.4719, -0.7939, -1.1943, -0.8724,  1.0746,  0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-37.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.698563681289853, distance: 1.4914119210775292 entropy 1.448572039604187
epoch: 2, step: 127
	action: tensor([[ 4.6916, -2.6947,  1.3185, -1.3041,  3.2945,  0.0210,  3.7099]],
       dtype=torch.float64)
	q_value: tensor([[-41.8086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
LOSS epoch 2 actor 226.03933592847824 critic 130.40467181547956 
epoch: 3, step: 0
	action: tensor([[-2.0231,  1.2466, -0.4893,  0.0140, -2.9281,  1.9675, -0.9068]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 1
	action: tensor([[ 1.3444, -0.2237,  1.0723,  0.8188,  0.7249, -0.5606,  0.6513]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 2
	action: tensor([[-0.4020, -0.8009,  0.1106,  0.6452, -2.1722,  1.0798, -0.2734]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6779471153590984, distance: 1.4823331727907156 entropy 1.448572039604187
epoch: 3, step: 3
	action: tensor([[ 5.2362, -1.5549,  3.9795, -1.9723,  2.5866,  3.0029,  3.9976]],
       dtype=torch.float64)
	q_value: tensor([[-54.9488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 4
	action: tensor([[-0.7324, -0.5012,  0.2201,  1.6919, -0.0870, -2.9447,  0.3855]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 5
	action: tensor([[-0.3986, -0.5203,  0.7885, -1.1691,  0.8759, -0.1468,  0.1430]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.078561568923595, distance: 1.6498259863907487 entropy 1.448572039604187
epoch: 3, step: 6
	action: tensor([[ 4.2520, -0.6925,  0.9331, -1.5814, -1.1690,  2.1466,  1.4420]],
       dtype=torch.float64)
	q_value: tensor([[-41.1692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 7
	action: tensor([[ 0.9536,  1.0097, -1.6722, -1.4176, -0.1297, -2.3269, -0.5261]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 8
	action: tensor([[ 1.7230, -1.0858, -0.8601, -0.5491,  0.8814,  2.5627, -0.6313]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04477345563547497, distance: 1.1184327727533163 entropy 1.448572039604187
epoch: 3, step: 9
	action: tensor([[ 3.7724, -4.3195,  1.4605, -1.9566,  0.5276,  2.6192,  3.4977]],
       dtype=torch.float64)
	q_value: tensor([[-63.0151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 10
	action: tensor([[-1.0250,  0.8358, -0.0896, -0.4388, -0.6944,  0.6804,  1.3871]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4640440881467953, distance: 1.3846301080760306 entropy 1.448572039604187
epoch: 3, step: 11
	action: tensor([[ 3.5949, -1.9876,  2.4118, -2.1170,  2.3538,  0.9624,  2.7327]],
       dtype=torch.float64)
	q_value: tensor([[-42.6981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 12
	action: tensor([[-7.0230e-01,  6.0230e-01, -8.8852e-04, -2.0603e+00,  2.6181e-02,
         -9.3027e-02, -1.4273e+00]], dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32912291288105, distance: 1.3192869475134898 entropy 1.448572039604187
epoch: 3, step: 13
	action: tensor([[ 2.1585, -1.8078,  0.0530, -0.0648,  0.7715,  1.0194,  0.7064]],
       dtype=torch.float64)
	q_value: tensor([[-50.7082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 14
	action: tensor([[ 0.8653,  0.8566, -0.6548, -2.6504, -0.2875,  1.7855,  0.1981]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 15
	action: tensor([[-0.3128, -0.9954,  0.2499, -1.1730,  0.3323,  0.0803, -0.9475]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9840531825099295, distance: 1.6118823799334792 entropy 1.448572039604187
epoch: 3, step: 16
	action: tensor([[ 2.6227, -1.1141, -0.5822, -0.6067,  0.6444, -1.7791,  0.2521]],
       dtype=torch.float64)
	q_value: tensor([[-46.4327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 17
	action: tensor([[ 1.1524, -0.3754,  1.2664, -0.4989, -1.5539,  0.3022, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4506944743604453, distance: 0.8481324446869932 entropy 1.448572039604187
epoch: 3, step: 18
	action: tensor([[ 3.1531, -5.6836,  1.3220, -1.5004,  1.3206,  2.3425,  2.8509]],
       dtype=torch.float64)
	q_value: tensor([[-48.5939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 19
	action: tensor([[-0.1555,  0.8470, -0.6212, -0.9975,  1.2961,  0.5502,  0.4137]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 20
	action: tensor([[-0.3447, -0.9562,  1.3030, -1.1785,  0.8261, -0.5985,  0.2658]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.987910401678334, distance: 1.6134484580963708 entropy 1.448572039604187
epoch: 3, step: 21
	action: tensor([[ 3.2797, -3.4166,  2.2652, -0.1290,  1.5279,  0.8631,  2.6311]],
       dtype=torch.float64)
	q_value: tensor([[-45.9609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 22
	action: tensor([[ 0.6197,  0.9732, -1.2427,  0.7792, -0.6180, -0.2578, -0.1238]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 23
	action: tensor([[ 1.2865, -1.2079, -0.0404, -2.8219, -1.0520, -1.1834, -0.8866]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 24
	action: tensor([[ 1.2299,  0.2405,  1.5825, -0.2473, -1.0094,  1.0305,  0.8023]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 25
	action: tensor([[-0.0164, -0.9653, -1.4314,  2.0356,  0.6308, -0.9140,  0.4552]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5786674363002873, distance: 1.4378117176408116 entropy 1.448572039604187
epoch: 3, step: 26
	action: tensor([[ 1.0699, -1.6676,  0.5296, -0.7607, -0.0071,  1.5388,  1.9922]],
       dtype=torch.float64)
	q_value: tensor([[-54.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 27
	action: tensor([[ 0.2056, -0.5013,  1.4333, -1.7022,  0.3999,  0.6695, -0.2931]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4198582133623494, distance: 1.3635754774814233 entropy 1.448572039604187
epoch: 3, step: 28
	action: tensor([[ 2.9106, -3.4383,  3.1487, -1.6298,  2.8092,  1.5068,  2.6780]],
       dtype=torch.float64)
	q_value: tensor([[-47.9296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 29
	action: tensor([[ 0.1396, -0.2244,  0.0020,  0.8895,  0.0958, -0.5646, -0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5553194523258258, distance: 0.7630987799592996 entropy 1.448572039604187
epoch: 3, step: 30
	action: tensor([[ 1.2712, -2.9439,  2.1035, -0.7936,  0.0077, -0.4700,  0.9773]],
       dtype=torch.float64)
	q_value: tensor([[-34.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 31
	action: tensor([[ 1.4604, -2.1379,  1.0261,  0.0575, -1.8656, -0.5198, -0.4205]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 32
	action: tensor([[-0.4862, -0.0227,  0.3100, -1.0649,  0.7400, -0.2067, -0.8456]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7713204597302916, distance: 1.5230187947788187 entropy 1.448572039604187
epoch: 3, step: 33
	action: tensor([[ 0.5201, -0.5719,  1.7012,  0.1724, -1.9979,  1.1502,  0.4403]],
       dtype=torch.float64)
	q_value: tensor([[-41.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5200510369482143, distance: 0.7927828052507396 entropy 1.448572039604187
epoch: 3, step: 34
	action: tensor([[ 5.9740, -3.7925,  1.3168, -3.0664,  1.8448,  3.1345,  4.1378]],
       dtype=torch.float64)
	q_value: tensor([[-57.0956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 35
	action: tensor([[ 1.0347,  0.1538, -0.8557, -1.1965,  1.5705,  1.0993, -0.7680]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 36
	action: tensor([[-0.6829,  1.0816,  2.2182,  0.6535, -2.9848, -0.3330, -0.8480]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 37
	action: tensor([[ 0.3544,  0.0250, -0.7921, -1.7434,  0.1351,  1.6905, -0.3691]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 38
	action: tensor([[ 0.1012, -0.8772,  2.4736,  0.2405,  0.3033, -1.1100,  0.7777]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12877014227362404, distance: 1.2157924716210946 entropy 1.448572039604187
epoch: 3, step: 39
	action: tensor([[ 4.4216, -3.7206,  0.6887, -2.4256,  0.9410,  0.7350,  4.7141]],
       dtype=torch.float64)
	q_value: tensor([[-57.7598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 40
	action: tensor([[ 0.6661, -0.5326,  1.2599,  2.1823, -0.5408,  1.3738, -0.3021]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 41
	action: tensor([[ 1.7879, -0.0402,  0.6738,  0.2990,  0.1763, -0.3020, -0.3659]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 42
	action: tensor([[-0.0160,  0.2373, -0.8796, -0.3950, -0.4687,  0.4222,  1.1119]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 43
	action: tensor([[ 0.6923,  0.3662,  0.3560, -0.3950, -0.6884,  0.9415,  0.6791]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 44
	action: tensor([[-0.6447,  1.5693, -0.0826,  1.5632, -0.3002,  0.9776,  2.4646]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 45
	action: tensor([[ 0.2707,  0.6029,  1.2111,  1.2755,  0.9212,  0.1103, -0.4229]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 46
	action: tensor([[-0.6579, -0.3294,  0.6917,  1.0042,  0.0734,  0.4310, -1.1723]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34809365384845214, distance: 0.9239517612858912 entropy 1.448572039604187
epoch: 3, step: 47
	action: tensor([[ 2.8318, -1.1594,  1.4397,  0.4164, -0.7140,  0.6058,  0.5382]],
       dtype=torch.float64)
	q_value: tensor([[-47.4108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 48
	action: tensor([[-0.5842, -0.1885,  0.7113, -1.6495,  0.2702,  0.2192, -0.8864]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0042447069181741, distance: 1.6200636061398093 entropy 1.448572039604187
epoch: 3, step: 49
	action: tensor([[ 3.9518, -0.1518, -0.6903,  1.0756,  0.9471,  2.0734,  1.9002]],
       dtype=torch.float64)
	q_value: tensor([[-45.7536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 50
	action: tensor([[ 0.1643,  0.6253,  1.3098, -1.5015,  0.2373,  0.0383, -0.4565]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06196014354371604, distance: 1.1792633516432844 entropy 1.448572039604187
epoch: 3, step: 51
	action: tensor([[ 1.6792, -1.8019,  3.1752, -0.9433,  0.9688,  0.1832,  2.4564]],
       dtype=torch.float64)
	q_value: tensor([[-43.0585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 52
	action: tensor([[ 0.8455, -0.0170, -0.6684, -0.9749,  1.1907,  1.7490,  0.8127]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7668227908246176, distance: 0.5525858290122506 entropy 1.448572039604187
epoch: 3, step: 53
	action: tensor([[ 4.1700, -4.6936,  3.9073, -1.4051,  1.5239,  2.0575,  4.7639]],
       dtype=torch.float64)
	q_value: tensor([[-54.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 54
	action: tensor([[ 0.8912, -0.4595,  0.9781,  0.0451, -1.1019, -2.3556,  0.9464]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23856436321237395, distance: 0.9985579937913523 entropy 1.448572039604187
epoch: 3, step: 55
	action: tensor([[ 5.1946, -2.1600,  2.3633, -0.7291,  1.0574,  2.1574,  2.8717]],
       dtype=torch.float64)
	q_value: tensor([[-56.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 56
	action: tensor([[ 0.3184,  0.0260,  1.9036, -2.1262,  0.6780,  0.4216, -0.8513]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0774838068855177, distance: 1.0991164108625164 entropy 1.448572039604187
epoch: 3, step: 57
	action: tensor([[ 3.4618, -3.4685,  2.4973, -1.4473,  0.2927,  2.9401,  2.8661]],
       dtype=torch.float64)
	q_value: tensor([[-55.0887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 58
	action: tensor([[ 0.1452, -0.9042, -0.2093,  0.0685,  0.1510, -0.3488,  0.6194]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49372565963113035, distance: 1.3985954596057024 entropy 1.448572039604187
epoch: 3, step: 59
	action: tensor([[ 2.4328, -4.0981,  1.0759, -1.7123,  0.0468,  0.2332,  0.3062]],
       dtype=torch.float64)
	q_value: tensor([[-34.9461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 60
	action: tensor([[-0.7593, -0.7709, -0.3705,  1.1259,  0.7965, -0.4873, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9136123444120314, distance: 1.583010061482544 entropy 1.448572039604187
epoch: 3, step: 61
	action: tensor([[ 1.7180, -0.3202, -1.3301, -0.4347,  0.4682, -0.2863,  0.3611]],
       dtype=torch.float64)
	q_value: tensor([[-42.4476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27334482731209575, distance: 1.2913076011892262 entropy 1.448572039604187
epoch: 3, step: 62
	action: tensor([[ 3.1878, -2.9998,  0.7006, -0.9599,  1.6828, -0.1064,  3.2428]],
       dtype=torch.float64)
	q_value: tensor([[-43.4954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 63
	action: tensor([[ 0.6060,  0.4284, -0.0167,  0.0371,  0.8587,  0.5376, -0.4527]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 64
	action: tensor([[-0.4425, -0.6493,  0.8062,  0.9894,  0.8724,  0.1739,  1.6330]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27306832941668846, distance: 0.9756712064027564 entropy 1.448572039604187
epoch: 3, step: 65
	action: tensor([[ 4.2129, -2.2308,  2.1855, -0.6675,  1.8438,  1.4007,  2.5126]],
       dtype=torch.float64)
	q_value: tensor([[-46.9565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 66
	action: tensor([[-1.9783, -1.5648, -0.8291, -1.3994,  0.6523, -0.7351, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 67
	action: tensor([[ 0.4426, -0.6675,  0.0458, -0.3355, -2.2469, -0.1655,  0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21127874904233068, distance: 1.2594436533568822 entropy 1.448572039604187
epoch: 3, step: 68
	action: tensor([[ 4.1511, -2.2837,  1.9108, -0.7630,  1.9158,  1.0706,  3.5450]],
       dtype=torch.float64)
	q_value: tensor([[-50.8925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 69
	action: tensor([[ 0.1854,  0.0873, -0.2176,  0.1004,  1.3806, -0.6735, -0.9928]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39235135255546105, distance: 0.8920371901705566 entropy 1.448572039604187
epoch: 3, step: 70
	action: tensor([[ 1.3525, -1.0588, -2.0345,  0.5373, -1.2059,  0.8833,  0.3891]],
       dtype=torch.float64)
	q_value: tensor([[-44.8148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5625824372616886, distance: 1.4304680516449861 entropy 1.448572039604187
epoch: 3, step: 71
	action: tensor([[ 5.0723, -5.2526,  2.7182, -2.3497,  1.0147,  0.2506,  5.7661]],
       dtype=torch.float64)
	q_value: tensor([[-51.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 72
	action: tensor([[-1.1388, -0.2820,  2.2453, -0.6548, -1.0957,  1.4782, -0.6783]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5881364251470105, distance: 1.4421173327346202 entropy 1.448572039604187
epoch: 3, step: 73
	action: tensor([[ 4.7805, -2.5741,  2.7640, -2.2685,  1.0793,  2.1511,  4.6972]],
       dtype=torch.float64)
	q_value: tensor([[-62.6821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 74
	action: tensor([[ 0.8475,  0.6067,  0.7059, -0.1147,  0.5501,  1.1143, -0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 75
	action: tensor([[ 0.8460,  0.7637, -1.3290, -1.0780, -0.7313, -1.2365, -1.0483]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 76
	action: tensor([[-1.1852, -0.3004, -2.1205, -1.3447,  0.0156, -0.1458, -0.6252]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10363231219558555, distance: 1.0834273373560013 entropy 1.448572039604187
epoch: 3, step: 77
	action: tensor([[ 0.8860, -0.3520,  0.5558, -1.5176,  1.6850,  0.4141,  1.3776]],
       dtype=torch.float64)
	q_value: tensor([[-51.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2907895822147386, distance: 1.300122933466261 entropy 1.448572039604187
epoch: 3, step: 78
	action: tensor([[ 4.7860, -6.0352,  4.0043, -1.0331,  0.4242,  1.7740,  4.9295]],
       dtype=torch.float64)
	q_value: tensor([[-54.6648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 79
	action: tensor([[ 0.4086, -0.3620,  0.6297, -0.4679, -1.6366,  0.7101, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09559634105318493, distance: 1.088272985316499 entropy 1.448572039604187
epoch: 3, step: 80
	action: tensor([[ 3.9827, -3.7199,  2.1602, -2.5162, -0.3031,  2.1440,  2.6686]],
       dtype=torch.float64)
	q_value: tensor([[-45.4195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 81
	action: tensor([[-0.0415, -0.9861, -0.5211, -1.1951, -1.5741,  0.0889, -0.3525]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5284477997850612, distance: 1.414757480123101 entropy 1.448572039604187
epoch: 3, step: 82
	action: tensor([[ 2.3408, -4.0994,  1.6693, -0.9634,  0.8382,  1.0228,  2.7589]],
       dtype=torch.float64)
	q_value: tensor([[-47.9084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 83
	action: tensor([[ 1.8455,  0.5593,  0.8530,  1.2722, -0.0771,  0.9026,  1.0675]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 84
	action: tensor([[-0.4828,  1.2319,  2.5225, -1.1928,  1.6610,  1.2084,  1.8713]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 85
	action: tensor([[-0.4420, -1.4745,  1.2877, -0.1080, -0.2008,  0.5805,  1.5656]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8357742937746788, distance: 1.5504805955269032 entropy 1.448572039604187
epoch: 3, step: 86
	action: tensor([[ 5.6076, -4.6359,  2.2734, -1.6414,  0.7352,  3.5682,  4.8314]],
       dtype=torch.float64)
	q_value: tensor([[-46.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 87
	action: tensor([[-0.7421, -0.6828, -1.0508, -0.3248, -2.2098,  0.9436,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7524661070545307, distance: 1.5148914239051265 entropy 1.448572039604187
epoch: 3, step: 88
	action: tensor([[ 3.9975, -3.0439,  2.7900, -0.3187,  1.2899,  1.7054,  2.5992]],
       dtype=torch.float64)
	q_value: tensor([[-53.9288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 89
	action: tensor([[ 0.4913, -1.1481,  0.4127,  0.6135,  0.5943,  2.3758, -0.4756]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5109395695009563, distance: 0.800272615749251 entropy 1.448572039604187
epoch: 3, step: 90
	action: tensor([[ 3.7115, -1.9151,  2.6945, -2.6213,  1.0226,  1.0499,  5.1673]],
       dtype=torch.float64)
	q_value: tensor([[-56.3744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 91
	action: tensor([[ 0.8486,  0.8533, -0.3249, -1.3147,  2.2550, -1.5582, -1.2686]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 92
	action: tensor([[ 0.5999,  0.3205, -0.6251,  1.6709, -1.4423, -0.1191,  0.9187]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 93
	action: tensor([[-0.6435, -0.6913, -0.2601,  0.7618,  1.0752, -0.3567, -0.5389]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8191676490169124, distance: 1.5434517423708478 entropy 1.448572039604187
epoch: 3, step: 94
	action: tensor([[ 1.1559, -0.9171, -0.3259, -2.0043, -0.5087,  1.5241, -0.7545]],
       dtype=torch.float64)
	q_value: tensor([[-42.7868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41269719120239956, distance: 1.3601325501732664 entropy 1.448572039604187
epoch: 3, step: 95
	action: tensor([[ 3.0830, -3.3320,  2.1975, -2.5957,  2.4869,  0.5010,  5.4621]],
       dtype=torch.float64)
	q_value: tensor([[-60.0782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 96
	action: tensor([[ 0.3510,  0.5285,  0.2495,  0.2847, -0.4898, -1.1866, -0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 97
	action: tensor([[ 0.2986,  1.5620,  0.3305,  0.0373,  0.2967, -1.0059, -0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 98
	action: tensor([[ 0.4072, -0.9880, -1.6023,  1.6514, -0.1826,  0.9584,  1.4593]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10058081283827325, distance: 1.2005151965084424 entropy 1.448572039604187
epoch: 3, step: 99
	action: tensor([[ 3.7401, -5.2124,  2.5254, -1.3349,  1.0658,  2.2269,  5.4700]],
       dtype=torch.float64)
	q_value: tensor([[-49.6279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 100
	action: tensor([[-1.2657, -0.6233,  0.2248, -1.8221, -1.5019, -1.2713,  0.5884]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13287826680815007, distance: 1.2180028820924207 entropy 1.448572039604187
epoch: 3, step: 101
	action: tensor([[ 2.9842, -2.6684,  1.1503, -2.7997, -0.4872,  0.0101,  1.9195]],
       dtype=torch.float64)
	q_value: tensor([[-56.1084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 102
	action: tensor([[ 0.8292,  1.0081,  0.4483, -0.9070,  0.0225, -0.4386,  2.4759]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 103
	action: tensor([[ 0.8065,  0.9754,  1.2379, -1.1044,  1.5237,  0.4188, -0.1889]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 104
	action: tensor([[-0.6110, -1.2919, -0.9110, -0.8696,  0.4765, -0.3102,  0.3165]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.316806544836032, distance: 1.3131601094854968 entropy 1.448572039604187
epoch: 3, step: 105
	action: tensor([[ 1.4399, -1.3397,  1.2249, -2.4755,  0.9649,  0.0138,  2.1555]],
       dtype=torch.float64)
	q_value: tensor([[-44.5773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 106
	action: tensor([[ 1.4269, -0.2560, -2.7236, -1.9140, -1.7425,  0.2796,  0.9503]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 107
	action: tensor([[ 1.2781, -1.1826, -0.1226, -1.2628,  0.2874,  0.9718,  0.6354]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8491642401011354, distance: 1.55612484400232 entropy 1.448572039604187
epoch: 3, step: 108
	action: tensor([[ 5.5234, -6.1712,  2.5542, -3.2828,  0.2722,  2.0714,  4.8046]],
       dtype=torch.float64)
	q_value: tensor([[-51.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 109
	action: tensor([[ 1.6473,  0.1364, -1.0481,  1.3395, -0.0490, -0.3140, -0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 110
	action: tensor([[-0.4737,  0.1716, -0.8090, -0.3007, -0.7756, -1.2637,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4465099625773149, distance: 0.8513567767884085 entropy 1.448572039604187
epoch: 3, step: 111
	action: tensor([[ 1.1048, -0.3471,  0.1399,  0.2292, -0.0521,  1.0118,  1.3884]],
       dtype=torch.float64)
	q_value: tensor([[-38.3189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7785349732557805, distance: 0.5385292165829973 entropy 1.448572039604187
epoch: 3, step: 112
	action: tensor([[ 5.2954, -3.1766,  3.7893, -3.9348,  1.0795,  2.0988,  4.8353]],
       dtype=torch.float64)
	q_value: tensor([[-43.7396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 113
	action: tensor([[-0.7351, -1.7016, -1.1660,  0.1755,  0.6562,  2.2040,  0.7639]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45801192210390007, distance: 1.381774681835954 entropy 1.448572039604187
epoch: 3, step: 114
	action: tensor([[ 4.3813, -2.5520,  2.7172, -0.8837, -0.0149, -0.6440,  3.5591]],
       dtype=torch.float64)
	q_value: tensor([[-55.9802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 115
	action: tensor([[ 1.0570, -0.0996,  1.3103,  0.5542,  0.0925, -0.7416, -0.3387]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6871174486225158, distance: 0.6400993194554484 entropy 1.448572039604187
epoch: 3, step: 116
	action: tensor([[ 3.2075, -1.7119,  1.3271,  0.4211, -0.2293,  0.9678,  1.5448]],
       dtype=torch.float64)
	q_value: tensor([[-46.7035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 117
	action: tensor([[ 0.9491,  0.4179,  0.1578,  0.4533, -1.1782, -0.0514, -0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 118
	action: tensor([[ 0.7592,  0.7596,  1.4135, -2.5448, -0.5432,  0.1461, -0.9282]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20555167354155335, distance: 1.0199749844261268 entropy 1.448572039604187
epoch: 3, step: 119
	action: tensor([[ 6.1800, -1.7500,  2.3808, -0.1901,  1.2572,  0.7008,  3.0215]],
       dtype=torch.float64)
	q_value: tensor([[-52.9279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 120
	action: tensor([[ 1.3787,  0.1538, -0.6986, -1.3597,  0.9847,  0.7759, -0.6107]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 121
	action: tensor([[-0.3771, -1.5679,  0.7064,  0.0657,  1.7389,  0.2867, -1.0004]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9051732213654164, distance: 1.5795156290817252 entropy 1.448572039604187
epoch: 3, step: 122
	action: tensor([[ 0.8257, -2.5653,  3.1278, -0.7013,  0.3614,  0.3148,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[-56.1265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 123
	action: tensor([[ 0.5047,  0.7367, -1.4827,  0.8552, -2.0792, -0.1090,  1.4062]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 124
	action: tensor([[ 1.1987, -0.2984,  0.8311, -1.0532,  0.2705, -0.0664,  0.5210]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11810613378068502, distance: 1.0746445839894325 entropy 1.448572039604187
epoch: 3, step: 125
	action: tensor([[ 3.7132, -4.6892,  1.9359, -2.8149,  1.3728,  2.1172,  1.0956]],
       dtype=torch.float64)
	q_value: tensor([[-40.2228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 3, step: 126
	action: tensor([[ 0.0297, -0.6937,  0.0048, -1.4292,  1.7231,  0.8184, -0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-45.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3157362893985405, distance: 1.3126263551395025 entropy 1.448572039604187
epoch: 3, step: 127
	action: tensor([[ 4.1652, -3.7064,  2.7213, -1.1813,  1.3278,  1.6501,  4.3624]],
       dtype=torch.float64)
	q_value: tensor([[-54.4911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
LOSS epoch 3 actor 329.8294008007447 critic 27.26938095422461 
epoch: 4, step: 0
	action: tensor([[ 0.7023, -0.9933,  0.5289,  0.2101,  1.2837,  0.3805,  0.0431]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15385649169215276, distance: 1.2292284185448745 entropy 1.448572039604187
epoch: 4, step: 1
	action: tensor([[-0.8625, -0.7399,  1.5467, -1.4939,  0.8892, -0.9682,  2.2434]],
       dtype=torch.float64)
	q_value: tensor([[-47.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7638343663271716, distance: 1.5197970364431073 entropy 1.448572039604187
epoch: 4, step: 2
	action: tensor([[ 4.2310, -3.7184,  1.3847, -0.0525,  0.9587, -0.1162,  4.0586]],
       dtype=torch.float64)
	q_value: tensor([[-61.3286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 3
	action: tensor([[ 0.6788,  1.7793, -0.7858, -0.9820, -0.5863, -0.6186, -0.2163]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 4
	action: tensor([[-0.4096, -0.2996,  0.7626, -3.4802, -1.1866, -1.5019, -0.4998]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 5
	action: tensor([[ 0.4227,  1.3175,  1.6728, -0.3792,  1.4739, -0.9018,  1.0744]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 6
	action: tensor([[ 0.4828, -1.6972,  0.7089, -0.9362, -0.6293, -0.8464, -0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8665610386181022, distance: 1.5634276610480067 entropy 1.448572039604187
epoch: 4, step: 7
	action: tensor([[ 2.4697, -4.1133, -0.4157, -1.1136,  0.8758,  1.8242,  2.4522]],
       dtype=torch.float64)
	q_value: tensor([[-47.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 8
	action: tensor([[-1.6061, -0.0788,  1.1561, -0.9831,  0.1635,  1.6359, -1.2818]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3056067558755609, distance: 1.737598013041404 entropy 1.448572039604187
epoch: 4, step: 9
	action: tensor([[ 1.8271, -2.2135,  1.1674, -1.0404, -0.2677,  0.7163,  2.6950]],
       dtype=torch.float64)
	q_value: tensor([[-63.7046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 10
	action: tensor([[ 0.1671,  0.6987, -0.3538, -0.1054, -1.2337,  0.1908,  1.7167]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 11
	action: tensor([[ 1.0187,  0.7851, -1.6187, -0.8740,  0.8087,  1.2765,  0.4485]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 12
	action: tensor([[-0.1845, -0.5809, -1.0147, -2.7334,  0.9795,  0.6552,  1.2611]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 13
	action: tensor([[-0.5275, -1.1783, -0.9083,  1.0385,  0.9240, -0.1945, -0.5352]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1421135106973623, distance: 1.6748577742179442 entropy 1.448572039604187
epoch: 4, step: 14
	action: tensor([[ 3.0687, -0.7101,  0.7406, -0.3582, -2.3997,  2.6274,  0.6339]],
       dtype=torch.float64)
	q_value: tensor([[-48.7822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 15
	action: tensor([[ 0.8361, -0.7438,  2.3463, -0.0994,  2.3662, -0.0405, -2.4128]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 16
	action: tensor([[ 0.2729,  0.0239,  1.4975,  0.0372, -2.0208,  1.4634,  1.3526]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 17
	action: tensor([[ 0.6234, -0.5451, -1.1005,  0.5028, -0.7030,  0.0282,  1.7656]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27502055536817394, distance: 0.9743602089126397 entropy 1.448572039604187
epoch: 4, step: 18
	action: tensor([[ 3.7043, -4.3725,  2.9865, -2.5889,  2.0405, -0.8783,  2.0803]],
       dtype=torch.float64)
	q_value: tensor([[-48.4070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 19
	action: tensor([[ 0.4130,  0.8749, -2.3799, -0.2827,  0.7716,  1.0949,  1.1630]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 20
	action: tensor([[-0.6876, -0.4210,  0.7255,  1.1583,  0.3533, -0.0936,  1.3547]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18614232714535128, distance: 1.032359418687766 entropy 1.448572039604187
epoch: 4, step: 21
	action: tensor([[ 2.0544, -1.7816,  2.0620, -0.5713,  0.5866, -0.6815,  1.5752]],
       dtype=torch.float64)
	q_value: tensor([[-44.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 22
	action: tensor([[ 0.1234,  0.1858, -1.2821, -0.2006,  1.8512,  0.6439,  0.3821]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 23
	action: tensor([[ 0.7219,  1.2378, -0.0619, -0.3228, -0.3910, -0.1724,  1.1161]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 24
	action: tensor([[ 2.1412,  1.0698, -0.9911, -0.3676,  1.0562, -1.0670, -1.2874]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 25
	action: tensor([[-0.1165, -0.2720,  0.1946, -1.4148, -2.3902, -2.0927,  0.5663]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27182252407767427, distance: 0.9765068943482753 entropy 1.448572039604187
epoch: 4, step: 26
	action: tensor([[ 2.6625, -2.2631,  1.7936, -0.1264, -0.3499,  0.1202,  2.4878]],
       dtype=torch.float64)
	q_value: tensor([[-65.1090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 27
	action: tensor([[-0.1438, -0.0218, -0.7378, -0.5761,  2.1582, -0.2599,  0.9935]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12180680914394826, distance: 1.072387457749493 entropy 1.448572039604187
epoch: 4, step: 28
	action: tensor([[ 2.9915, -1.7425,  0.6222,  0.3452, -1.4624,  1.8083,  0.3758]],
       dtype=torch.float64)
	q_value: tensor([[-55.2690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 29
	action: tensor([[ 0.5767, -0.2879, -0.8026, -2.5399,  0.2730,  0.7985,  1.6635]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 30
	action: tensor([[ 1.9488,  0.2714,  0.3905,  0.1786, -2.4978, -1.2172, -0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4032546418860885, distance: 0.8839978688207384 entropy 1.448572039604187
epoch: 4, step: 31
	action: tensor([[ 2.6161, -3.4468,  1.0327, -2.2084,  2.1432,  1.5612,  3.2500]],
       dtype=torch.float64)
	q_value: tensor([[-63.8732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 32
	action: tensor([[ 0.1973, -1.1622, -2.7195, -0.6234, -0.1105,  1.6262,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43355228909094756, distance: 1.3701353219613899 entropy 1.448572039604187
epoch: 4, step: 33
	action: tensor([[ 3.1588, -2.4196,  3.5376, -1.9946,  3.1282,  0.8342,  2.5155]],
       dtype=torch.float64)
	q_value: tensor([[-63.6214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 34
	action: tensor([[ 0.7279, -1.0764, -0.9174, -0.5772, -0.1157, -0.1874,  0.6584]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5879085174071665, distance: 1.4420138524891304 entropy 1.448572039604187
epoch: 4, step: 35
	action: tensor([[ 2.1639, -2.7784,  1.8918, -1.9301,  2.0947,  1.6210,  3.5951]],
       dtype=torch.float64)
	q_value: tensor([[-43.2727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 36
	action: tensor([[-0.8498, -1.4770, -0.7161,  0.2124,  0.3482, -1.5476,  1.0936]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6432565093601021, distance: 1.466929944457715 entropy 1.448572039604187
epoch: 4, step: 37
	action: tensor([[ 2.7352, -0.5803,  1.1781, -1.7963, -2.2583, -0.3037,  2.0858]],
       dtype=torch.float64)
	q_value: tensor([[-52.9791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 38
	action: tensor([[ 0.0511,  0.3918,  1.6993,  0.8387,  0.5941, -0.0187, -0.1121]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 39
	action: tensor([[ 1.1843,  0.1318, -1.1443,  0.9588, -0.4922,  0.5731, -0.4370]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 40
	action: tensor([[-1.4820,  0.0969, -1.4749, -1.6198,  1.2859,  1.1329, -0.2436]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09116152203155858, distance: 1.0909379411779236 entropy 1.448572039604187
epoch: 4, step: 41
	action: tensor([[ 1.6101, -2.8723,  1.7325, -0.3302, -0.9688,  1.3104,  0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-63.9692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 42
	action: tensor([[ 0.7257, -0.4419, -0.0580, -2.9809,  1.3401,  1.4249,  1.2173]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 43
	action: tensor([[ 0.6309,  1.0660,  0.7454, -0.0948,  2.6693, -1.6509,  0.7755]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 44
	action: tensor([[-0.1879,  0.5795,  0.5605, -0.7643, -0.7562, -1.3930,  1.2396]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19124551985178595, distance: 1.0291176886988629 entropy 1.448572039604187
epoch: 4, step: 45
	action: tensor([[ 2.6131, -2.7543,  3.0304, -0.2617,  2.1696,  1.2571,  1.5585]],
       dtype=torch.float64)
	q_value: tensor([[-46.0811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 46
	action: tensor([[-1.4325,  0.5300,  0.3516, -0.5605,  0.1939,  0.1500,  2.5468]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3284869555211594, distance: 1.746198448132897 entropy 1.448572039604187
epoch: 4, step: 47
	action: tensor([[ 2.5434, -3.8696,  2.6322, -0.8126,  2.8503,  1.5465,  2.7121]],
       dtype=torch.float64)
	q_value: tensor([[-58.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 48
	action: tensor([[ 1.5594, -0.0169,  0.8760, -0.4072,  1.3519, -0.6095,  1.0070]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 49
	action: tensor([[-0.6251, -0.3699,  0.6573,  0.7389,  0.8706,  2.0111,  1.2406]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4561279034326916, distance: 0.8439273897670256 entropy 1.448572039604187
epoch: 4, step: 50
	action: tensor([[ 3.8285, -4.3497,  2.4295, -0.9419,  0.6387,  0.4684,  3.2627]],
       dtype=torch.float64)
	q_value: tensor([[-56.4289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 51
	action: tensor([[-1.0103,  1.0540, -1.4442,  0.1443, -0.4284,  0.7589, -1.1655]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 52
	action: tensor([[-1.8398, -1.6938, -1.0205, -2.0425,  1.2706,  0.1163,  0.2819]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 53
	action: tensor([[ 0.7071, -0.1058,  1.4988, -0.0034,  1.1478,  1.7718,  0.1739]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 54
	action: tensor([[ 0.1825, -0.9851, -0.7341, -0.2642, -1.2791,  0.8337,  1.7854]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6944330534553027, distance: 1.4895973828491142 entropy 1.448572039604187
epoch: 4, step: 55
	action: tensor([[ 3.8608, -4.4510,  4.8706, -2.0544,  1.3762,  2.5015,  0.8519]],
       dtype=torch.float64)
	q_value: tensor([[-51.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 56
	action: tensor([[ 0.5573,  0.6351, -2.1567,  1.2948, -0.8264,  0.3693,  2.2425]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 57
	action: tensor([[-0.2075, -0.9990, -1.5940, -0.9263, -0.7927, -1.2942,  2.0209]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4965811019047168, distance: 0.8119353509732621 entropy 1.448572039604187
epoch: 4, step: 58
	action: tensor([[ 4.5498, -3.3085,  0.8477, -1.1044,  1.5633,  2.5148,  4.1186]],
       dtype=torch.float64)
	q_value: tensor([[-59.5042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 59
	action: tensor([[ 0.2526,  1.4724, -0.5646,  0.3013,  0.0734, -1.2850, -0.9797]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 60
	action: tensor([[ 0.7190, -0.8227,  0.2454, -1.3033, -0.1032,  0.6917, -1.5578]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.587311175379472, distance: 1.4417425974096292 entropy 1.448572039604187
epoch: 4, step: 61
	action: tensor([[ 2.9116, -1.1624,  2.3751,  0.3481,  1.8695, -0.1641, -0.6102]],
       dtype=torch.float64)
	q_value: tensor([[-53.5730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 62
	action: tensor([[-1.7675, -0.6692,  0.9781, -0.9963,  0.9408, -0.1610, -1.0769]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 63
	action: tensor([[ 0.9249,  0.0353,  1.0207,  0.8938,  0.1884, -1.4582,  0.9349]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 64
	action: tensor([[-1.1613,  0.6868,  0.1461, -1.0059, -0.0847,  1.2230, -0.0495]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6050318360739717, distance: 1.4497680309671899 entropy 1.448572039604187
epoch: 4, step: 65
	action: tensor([[ 0.0605, -1.3417, -0.5496, -0.3285,  0.8550, -2.4221,  2.9901]],
       dtype=torch.float64)
	q_value: tensor([[-49.2726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.312736877351651, distance: 1.3111293402370348 entropy 1.448572039604187
epoch: 4, step: 66
	action: tensor([[ 2.7362, -2.4698,  2.6345, -1.2880, -0.8252,  0.1556,  4.4777]],
       dtype=torch.float64)
	q_value: tensor([[-69.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 67
	action: tensor([[-1.2385, -0.0022, -0.7483, -0.1208, -0.3881, -0.6950,  2.0638]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9552772179163111, distance: 1.600150616989584 entropy 1.448572039604187
epoch: 4, step: 68
	action: tensor([[ 1.8197, -3.6317,  1.0589, -0.9885, -0.7971,  1.8130,  2.0088]],
       dtype=torch.float64)
	q_value: tensor([[-52.3247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 69
	action: tensor([[ 0.8237,  1.9539,  0.4247, -1.4603, -0.7665,  1.4684, -1.3954]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 70
	action: tensor([[ 0.6925,  0.8589, -0.0744,  0.0604,  0.6458,  0.3017,  0.4652]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 71
	action: tensor([[-0.1755,  0.2279, -0.3413,  1.0402, -2.0792, -0.7604,  1.6331]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 72
	action: tensor([[-0.3831,  0.2012,  1.5061,  0.6469,  2.0332, -2.1594, -1.3127]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 73
	action: tensor([[ 0.1315, -0.2240,  1.2473, -0.2656,  0.2670, -0.2061, -0.8827]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012885615155703634, distance: 1.136947559170573 entropy 1.448572039604187
epoch: 4, step: 74
	action: tensor([[ 1.3586, -0.7930,  0.3886,  0.4060, -0.7741,  0.8575,  4.4650]],
       dtype=torch.float64)
	q_value: tensor([[-44.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9169987571878802 entropy 1.448572039604187
epoch: 4, step: 75
	action: tensor([[ 1.9989,  0.4323,  1.0866,  0.3236, -0.2688,  0.6354, -3.0482]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 76
	action: tensor([[ 1.0434,  0.5060, -0.9464,  1.0339, -0.7456, -0.2560,  1.6014]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 77
	action: tensor([[-1.1691, -1.9636,  0.5629, -1.9563, -0.1450,  1.0359, -0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 78
	action: tensor([[ 1.0664, -0.5341, -1.3077, -0.4561, -1.6738,  0.1500, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 79
	action: tensor([[ 0.9766,  1.1047, -0.2117, -0.3840, -0.2753,  0.2662,  1.2460]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 80
	action: tensor([[-0.9591,  0.3623, -1.6312,  0.0467, -0.7908,  0.6921, -0.8657]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8537117376462457, distance: 1.5580370939929977 entropy 1.448572039604187
epoch: 4, step: 81
	action: tensor([[ 1.7436, -0.2580,  1.6481,  0.3086, -1.5330,  0.7705, -0.5753]],
       dtype=torch.float64)
	q_value: tensor([[-43.0091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2051626475771855, distance: 1.0202246848627077 entropy 1.448572039604187
epoch: 4, step: 82
	action: tensor([[ 2.6536, -1.9421,  2.4363, -0.9858, -0.0947,  0.7945,  2.8040]],
       dtype=torch.float64)
	q_value: tensor([[-58.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 83
	action: tensor([[-0.1075, -0.4484,  0.2640,  1.1991,  1.0082,  0.6498,  0.7912]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 84
	action: tensor([[ 0.1477,  0.1309, -1.1406, -1.1603,  0.6744,  0.6970, -0.5215]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 85
	action: tensor([[ 0.0835, -0.2438,  0.3168,  0.6549,  1.6130, -1.7280, -0.7747]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39135433708841294, distance: 0.8927687069460918 entropy 1.448572039604187
epoch: 4, step: 86
	action: tensor([[ 1.4090, -0.7849, -0.6416, -1.3238, -0.7705, -0.7720,  1.7165]],
       dtype=torch.float64)
	q_value: tensor([[-57.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6507884092911207, distance: 1.470287952733127 entropy 1.448572039604187
epoch: 4, step: 87
	action: tensor([[ 5.0564, -4.0294,  2.7140, -2.6302,  1.1490,  2.1592,  2.7285]],
       dtype=torch.float64)
	q_value: tensor([[-55.5917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 88
	action: tensor([[-0.0694, -1.6080,  2.0172, -0.5773, -1.5371,  1.0474, -0.4251]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36555600825415846, distance: 1.3372464448806007 entropy 1.448572039604187
epoch: 4, step: 89
	action: tensor([[ 3.2325, -2.8179,  0.7883, -1.8099,  2.8236,  1.7629,  4.1965]],
       dtype=torch.float64)
	q_value: tensor([[-64.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 90
	action: tensor([[ 0.7168, -0.7794, -1.1403,  0.7178,  0.2438, -0.0854, -0.0568]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12540309584728582, distance: 1.0701894395903395 entropy 1.448572039604187
epoch: 4, step: 91
	action: tensor([[ 1.2391, -0.3036,  0.7388,  0.2508, -1.1628, -0.3067,  0.6103]],
       dtype=torch.float64)
	q_value: tensor([[-39.1770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36337686398915425, distance: 0.9130570246138698 entropy 1.448572039604187
epoch: 4, step: 92
	action: tensor([[ 3.2470, -2.4368,  0.8882, -2.2992,  2.0578,  3.1127,  3.1093]],
       dtype=torch.float64)
	q_value: tensor([[-45.5209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 93
	action: tensor([[-0.7769,  0.2654,  1.0045, -1.3145, -0.3334, -0.0561,  0.8226]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0893900948444286, distance: 1.654117891045135 entropy 1.448572039604187
epoch: 4, step: 94
	action: tensor([[ 3.4546, -2.8774,  3.9670, -2.3296,  2.0448, -0.6313,  3.3153]],
       dtype=torch.float64)
	q_value: tensor([[-44.5951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 95
	action: tensor([[ 0.5515,  0.9700, -0.6243,  0.2361,  0.5761,  0.3358,  1.2416]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 96
	action: tensor([[ 0.4475, -2.1998, -0.6428,  0.7510, -1.7036, -0.6212,  1.0014]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 97
	action: tensor([[-0.2070, -0.3794, -0.9233, -0.2128,  1.1384, -0.6454,  1.4102]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32809976611713476, distance: 1.3187790621898023 entropy 1.448572039604187
epoch: 4, step: 98
	action: tensor([[ 0.9927, -1.0258,  1.1338, -1.7879, -0.1434,  0.3020,  1.7621]],
       dtype=torch.float64)
	q_value: tensor([[-46.2756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18597474412911685, distance: 1.2462191300204009 entropy 1.448572039604187
epoch: 4, step: 99
	action: tensor([[ 6.1800, -4.5462,  5.4331, -3.4080,  1.1487,  0.1169,  4.3311]],
       dtype=torch.float64)
	q_value: tensor([[-58.3675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 100
	action: tensor([[ 0.8645,  1.8197,  0.3830, -0.2673, -1.1354,  1.0787, -0.4484]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 101
	action: tensor([[-0.1346, -0.3637,  2.3165,  0.6763, -1.6480, -0.4272,  1.4825]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2054853681088067, distance: 1.256428169825153 entropy 1.448572039604187
epoch: 4, step: 102
	action: tensor([[ 4.2402, -3.3004,  2.2782, -2.6344,  1.3139,  1.4236,  2.9405]],
       dtype=torch.float64)
	q_value: tensor([[-61.4578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 103
	action: tensor([[-2.9281,  0.9190,  0.9766, -0.2075, -0.3760,  1.5410, -0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 104
	action: tensor([[-0.3044,  0.3122,  0.4017,  0.3665, -0.8137, -0.6755,  0.6683]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 105
	action: tensor([[-0.0113,  0.0319, -1.5270,  1.4897, -0.1864,  0.5148, -1.0439]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 106
	action: tensor([[ 0.3472, -0.8543,  0.4730,  0.1073, -1.2937,  0.9850, -1.0482]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05787099725905642, distance: 1.1107386394027392 entropy 1.448572039604187
epoch: 4, step: 107
	action: tensor([[ 1.7392, -2.1285,  3.7737, -1.5582,  0.6895,  1.6200,  2.1995]],
       dtype=torch.float64)
	q_value: tensor([[-51.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 108
	action: tensor([[ 1.2465,  1.3239,  0.5869, -0.2420, -1.2633,  0.0183, -0.7882]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 109
	action: tensor([[-1.1527,  1.5121,  1.0134,  0.5207,  0.4148, -1.7877,  0.9149]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19369840239643932, distance: 1.0275558910305411 entropy 1.448572039604187
epoch: 4, step: 110
	action: tensor([[ 0.6115, -0.8041, -1.1257, -0.5314, -0.4572,  0.0916, -0.2416]],
       dtype=torch.float64)
	q_value: tensor([[-58.9960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 111
	action: tensor([[ 1.5295,  1.0078, -1.3192,  0.5317, -1.4408, -0.9265,  0.5091]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 112
	action: tensor([[ 0.4671, -0.0874,  1.3459, -2.1482, -0.9097,  1.4216, -0.4857]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40656337630586137, distance: 0.8815437400961996 entropy 1.448572039604187
epoch: 4, step: 113
	action: tensor([[ 4.5442, -3.9856,  3.6958, -3.3523,  1.7384,  0.2199,  0.9518]],
       dtype=torch.float64)
	q_value: tensor([[-60.3134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 114
	action: tensor([[ 0.3736, -1.1106,  0.3260,  0.3543, -0.7383,  1.1079,  0.8177]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13421539004490468, distance: 1.064784263536993 entropy 1.448572039604187
epoch: 4, step: 115
	action: tensor([[ 2.8808, -3.0811,  2.0027, -2.0411,  1.5642,  1.1881,  1.7696]],
       dtype=torch.float64)
	q_value: tensor([[-44.2886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 116
	action: tensor([[-1.0651, -0.5980, -1.7175, -0.8995,  0.4798,  0.2178,  0.2974]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5122438523906352, distance: 1.4072381721856795 entropy 1.448572039604187
epoch: 4, step: 117
	action: tensor([[ 2.6188, -1.5377,  3.3464, -1.4826, -0.2163,  1.4148,  1.6465]],
       dtype=torch.float64)
	q_value: tensor([[-49.3758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 118
	action: tensor([[ 0.3690,  0.2017, -1.5278, -0.8480,  1.6190, -0.2301,  0.6749]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 119
	action: tensor([[-0.9797, -0.6191,  0.8113, -1.6238,  0.2493, -0.6571,  1.4934]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7977243046849234, distance: 1.534328097859423 entropy 1.448572039604187
epoch: 4, step: 120
	action: tensor([[ 4.7645, -3.2207,  1.5780, -1.9519,  3.4401,  0.9914,  1.6101]],
       dtype=torch.float64)
	q_value: tensor([[-51.6960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 121
	action: tensor([[ 2.1837, -1.4108, -0.6371,  0.0545,  0.0424,  0.1235,  0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7035391908747779, distance: 1.4935946794260355 entropy 1.448572039604187
epoch: 4, step: 122
	action: tensor([[ 1.6294, -4.2213,  4.1537, -1.0618,  1.7172,  0.3858, -0.0549]],
       dtype=torch.float64)
	q_value: tensor([[-46.5708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 123
	action: tensor([[-0.3492,  0.0197, -0.6081, -0.9144, -0.9913,  0.6329,  1.1967]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18738881629241644, distance: 1.2469618603395582 entropy 1.448572039604187
epoch: 4, step: 124
	action: tensor([[ 3.8990, -3.5469,  0.3440, -0.9484,  0.6964,  2.4327,  1.5340]],
       dtype=torch.float64)
	q_value: tensor([[-44.6884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 125
	action: tensor([[-1.2634,  1.5732,  0.5673,  1.2408,  2.3424, -0.2449,  0.6399]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 4, step: 126
	action: tensor([[-0.0369, -0.4612, -0.9660, -0.1447, -2.9751,  1.0024,  1.7273]],
       dtype=torch.float64)
	q_value: tensor([[-48.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0012954755686189756, distance: 1.1450852491039833 entropy 1.448572039604187
epoch: 4, step: 127
	action: tensor([[ 6.1800, -5.3078,  2.8844, -3.1763,  2.3349,  0.5842,  3.9412]],
       dtype=torch.float64)
	q_value: tensor([[-67.0252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
LOSS epoch 4 actor 341.9354799887146 critic 26.054200681154764 
epoch: 5, step: 0
	action: tensor([[ 1.8749, -0.2795,  0.5042,  0.1369, -0.3245, -0.5071,  0.3668]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06269324198217163, distance: 1.1078923598276835 entropy 1.3432114124298096
epoch: 5, step: 1
	action: tensor([[-0.1127, -1.3095,  1.5095, -1.1905,  0.0579,  0.7410,  0.6596]],
       dtype=torch.float64)
	q_value: tensor([[-38.9073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9405285034520658, distance: 1.5941042013807472 entropy 1.3432114124298096
epoch: 5, step: 2
	action: tensor([[ 1.2642, -2.7966,  1.9391, -1.1394,  1.5561,  0.8843,  1.8310]],
       dtype=torch.float64)
	q_value: tensor([[-47.1556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 3
	action: tensor([[ 0.0343, -1.0780, -0.0272, -1.3081,  0.0546,  0.9752,  0.3632]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7442448171715188, distance: 1.5113338650628347 entropy 1.3432114124298096
epoch: 5, step: 4
	action: tensor([[ 1.2495, -1.5830,  1.4737, -2.0886, -0.6246,  0.6949,  2.5103]],
       dtype=torch.float64)
	q_value: tensor([[-44.5375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 5
	action: tensor([[-0.5361, -1.2431,  0.1679,  0.8190, -0.5575,  0.0482, -0.4319]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5703398480749564, distance: 1.4340144213513095 entropy 1.3432114124298096
epoch: 5, step: 6
	action: tensor([[ 1.4766, -2.2217,  1.1755, -0.5471,  1.3476,  0.1201,  0.7130]],
       dtype=torch.float64)
	q_value: tensor([[-43.3688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 7
	action: tensor([[-2.5815, -0.4782,  0.3105, -0.2816, -0.6655,  0.9254, -0.6691]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 8
	action: tensor([[ 0.5141,  1.2161, -0.8216,  1.5091, -0.0894,  0.7948,  0.7379]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 9
	action: tensor([[-0.9027, -0.5434,  0.6301, -1.5286, -1.6406, -1.0233, -0.4760]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7285288843067088, distance: 1.5045097753106962 entropy 1.3432114124298096
epoch: 5, step: 10
	action: tensor([[ 0.3505, -0.1634,  0.7825,  1.4067,  1.2287, -0.5663,  1.6180]],
       dtype=torch.float64)
	q_value: tensor([[-52.1020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9843809672383674, distance: 0.14301571484189474 entropy 1.3432114124298096
epoch: 5, step: 11
	action: tensor([[ 0.7650,  1.0865,  1.1625, -0.9630,  0.4347,  0.7526,  1.2793]],
       dtype=torch.float64)
	q_value: tensor([[-50.9290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 12
	action: tensor([[-0.1867, -0.4847,  0.8307, -0.5765,  0.7398,  0.8549,  0.5635]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36180703452640417, distance: 1.335409556701603 entropy 1.3432114124298096
epoch: 5, step: 13
	action: tensor([[ 0.9305, -0.0056,  1.1624, -1.2265, -1.1362, -0.0338,  3.2155]],
       dtype=torch.float64)
	q_value: tensor([[-39.3632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8889474115534649 entropy 1.3432114124298096
epoch: 5, step: 14
	action: tensor([[-0.1407, -0.1069, -0.0827,  0.7096,  2.7500, -0.2348,  1.3120]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 15
	action: tensor([[ 0.2080,  2.1008,  0.5399, -0.4097,  0.1403, -0.2124,  0.3998]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 16
	action: tensor([[ 1.8912, -0.4960, -1.0121, -0.7166,  0.1420, -0.6354,  1.4106]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 17
	action: tensor([[-2.1911, -1.0080,  1.0806, -0.6945, -0.3481, -0.2702, -0.9383]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 18
	action: tensor([[ 0.2830,  0.7661,  0.2453,  1.2319,  1.3207, -0.2417,  0.0873]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 19
	action: tensor([[-1.1943,  0.3087,  0.3306, -0.5034, -0.8639,  0.1695,  0.1545]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.233635778149322, distance: 1.7102628887756146 entropy 1.3432114124298096
epoch: 5, step: 20
	action: tensor([[ 2.2703, -0.5088, -0.7769, -0.3851,  0.2530, -0.5411,  0.6503]],
       dtype=torch.float64)
	q_value: tensor([[-37.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.548410961388154, distance: 1.423966629609368 entropy 1.3432114124298096
epoch: 5, step: 21
	action: tensor([[ 0.1399, -1.6015,  2.2573, -0.0961,  0.4846,  1.7422,  1.9924]],
       dtype=torch.float64)
	q_value: tensor([[-41.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 22
	action: tensor([[-0.5992,  1.1088,  2.1362, -0.3006, -1.0413,  2.2194,  0.3631]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 23
	action: tensor([[ 0.1881, -0.8494,  0.6578, -0.6318, -2.2753,  0.6718, -0.4465]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7493988642422067, distance: 1.5135651287093999 entropy 1.3432114124298096
epoch: 5, step: 24
	action: tensor([[ 0.6174, -1.6085,  0.7226, -0.4939, -0.4298,  0.2850,  0.4999]],
       dtype=torch.float64)
	q_value: tensor([[-54.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 25
	action: tensor([[ 0.7805, -0.9168,  0.8941, -0.7892,  0.6254, -0.8326,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35065686231313964, distance: 1.3299313021951802 entropy 1.3432114124298096
epoch: 5, step: 26
	action: tensor([[-0.0654, -1.2325,  0.3738, -1.2495,  2.4474,  0.7826,  2.5686]],
       dtype=torch.float64)
	q_value: tensor([[-42.5076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7242197562505372, distance: 1.5026332747482818 entropy 1.3432114124298096
epoch: 5, step: 27
	action: tensor([[ 3.6464, -4.3748,  4.1669, -1.3712,  1.0829,  0.5051,  3.0516]],
       dtype=torch.float64)
	q_value: tensor([[-67.5963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 28
	action: tensor([[-0.0862,  0.0400, -1.4677, -0.4075,  1.4450,  0.1018,  0.8260]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 29
	action: tensor([[-2.0699e+00, -9.9067e-01, -3.9543e-01,  6.0059e-03,  1.4504e-03,
         -9.5805e-01, -3.8869e-01]], dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 30
	action: tensor([[ 0.6125,  0.9176, -0.5599, -1.0076,  1.1586,  2.2315, -0.2969]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 31
	action: tensor([[ 0.6441, -1.1471,  0.4964,  0.9712, -0.2133,  0.0506, -2.5457]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3362500742633556, distance: 0.932306980662252 entropy 1.3432114124298096
epoch: 5, step: 32
	action: tensor([[ 2.2662,  1.8182,  0.1307,  0.2624,  2.2364, -1.7991,  0.3078]],
       dtype=torch.float64)
	q_value: tensor([[-62.8806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 33
	action: tensor([[-2.4822,  1.3748,  0.0888,  0.8741, -1.7423,  1.5025,  1.1792]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 34
	action: tensor([[ 1.4476, -1.0684,  0.2712, -0.3092, -0.7872, -0.9702,  0.8458]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7864183090899755, distance: 1.5294957475870978 entropy 1.3432114124298096
epoch: 5, step: 35
	action: tensor([[ 2.7101, -1.8171,  0.9139, -2.3049, -1.0966, -0.5379,  2.8960]],
       dtype=torch.float64)
	q_value: tensor([[-46.6844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 36
	action: tensor([[-0.7169, -0.0697, -2.0365,  0.9052,  0.5058,  0.2475,  0.4561]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1633652855479548, distance: 1.683145349699533 entropy 1.3432114124298096
epoch: 5, step: 37
	action: tensor([[ 1.2055,  0.7639,  0.7695, -0.6467, -0.0167, -1.3363,  1.4167]],
       dtype=torch.float64)
	q_value: tensor([[-39.8701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 38
	action: tensor([[-0.0636,  0.3328,  1.0179,  0.5043, -0.0175,  0.0430,  1.4926]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 39
	action: tensor([[-0.2408,  0.1290, -0.1208, -0.4728, -0.4602,  1.2396,  1.4318]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07793407488636583, distance: 1.0988481460040647 entropy 1.3432114124298096
epoch: 5, step: 40
	action: tensor([[ 4.7569, -1.9182,  2.1535, -0.7192,  1.2235, -0.4414,  3.7245]],
       dtype=torch.float64)
	q_value: tensor([[-41.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 41
	action: tensor([[ 1.0888e+00, -6.5242e-01, -9.9200e-01, -2.4995e-01, -4.3524e-01,
          1.0807e-03,  5.6369e-01]], dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07064030838501956, distance: 1.1840730276172122 entropy 1.3432114124298096
epoch: 5, step: 42
	action: tensor([[ 1.6209, -2.4531,  1.6698, -1.0725,  1.0849,  2.3571, -0.6603]],
       dtype=torch.float64)
	q_value: tensor([[-36.9709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 43
	action: tensor([[-0.9415,  0.2808,  1.3901,  0.8011,  0.7326, -0.0346, -0.6535]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08477729232522768, distance: 1.094762938752369 entropy 1.3432114124298096
epoch: 5, step: 44
	action: tensor([[-0.6938,  1.3996,  1.4223,  0.6255,  0.1171,  0.6744,  1.0457]],
       dtype=torch.float64)
	q_value: tensor([[-47.1076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 45
	action: tensor([[-1.1547, -0.9403,  1.2386,  0.3064,  0.5664, -0.5566,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4259511251301022, distance: 1.7823693969685477 entropy 1.3432114124298096
epoch: 5, step: 46
	action: tensor([[ 0.9002, -1.9135,  0.6923,  0.8971,  0.6105, -0.0607, -0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-43.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 47
	action: tensor([[-1.2576,  0.4993,  1.5882,  0.3351, -0.7010, -0.7494, -0.4193]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 48
	action: tensor([[ 0.3124, -0.6833,  1.0921,  0.3551,  0.1244, -1.2036,  2.2516]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0825199183652936, distance: 1.190624023946201 entropy 1.3432114124298096
epoch: 5, step: 49
	action: tensor([[ 1.6765, -2.3971,  2.2709, -1.2469,  1.2569,  0.7234,  2.7813]],
       dtype=torch.float64)
	q_value: tensor([[-49.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 50
	action: tensor([[ 0.1596,  0.4412,  0.8480, -0.6814,  0.3687, -0.4476,  0.9436]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17095247786864354, distance: 1.0419488653811981 entropy 1.3432114124298096
epoch: 5, step: 51
	action: tensor([[-0.7885, -1.5073,  1.9185, -2.0738,  0.1351, -0.9855,  1.7288]],
       dtype=torch.float64)
	q_value: tensor([[-36.2289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0915830665333861, distance: 1.1955977477270785 entropy 1.3432114124298096
epoch: 5, step: 52
	action: tensor([[ 1.0321, -1.7838, -0.2149,  0.1281,  0.1962,  2.9988,  1.3988]],
       dtype=torch.float64)
	q_value: tensor([[-57.0960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 53
	action: tensor([[-0.1019, -0.4479, -1.3633,  0.4881, -0.6961, -0.0495, -0.5487]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38004439464405704, distance: 1.344321740037114 entropy 1.3432114124298096
epoch: 5, step: 54
	action: tensor([[ 0.8661, -1.6281,  1.1109, -1.7718, -0.8477,  0.8057, -0.3544]],
       dtype=torch.float64)
	q_value: tensor([[-34.5431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 55
	action: tensor([[-0.2110, -1.0890, -1.8902, -0.6145,  0.7922, -0.6202,  0.8572]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04005475223794308, distance: 1.1670374552200402 entropy 1.3432114124298096
epoch: 5, step: 56
	action: tensor([[ 2.0649, -1.8007, -0.8943, -0.7077,  1.5465,  0.2449,  0.6552]],
       dtype=torch.float64)
	q_value: tensor([[-48.9457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 57
	action: tensor([[-0.3828, -1.4913, -0.3273, -1.7378, -0.1647, -0.3248,  0.3755]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035099756320039255, distance: 1.1642541547890133 entropy 1.3432114124298096
epoch: 5, step: 58
	action: tensor([[ 3.5502,  0.1112,  2.1531, -0.5935,  0.2671,  0.4807,  2.2189]],
       dtype=torch.float64)
	q_value: tensor([[-46.0078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 59
	action: tensor([[ 0.9344, -0.3452,  1.0336, -0.2436, -0.0970,  2.9150, -0.1134]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 60
	action: tensor([[-9.8226e-04,  2.6948e-02, -1.0145e-01,  7.2005e-01,  7.4255e-01,
          1.0227e+00, -7.7265e-01]], dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 61
	action: tensor([[ 2.1520e-01, -3.7756e-01, -4.4332e-02, -3.1936e-02,  1.1788e+00,
         -1.3355e-04, -1.9946e-01]], dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1347244562350013, distance: 1.064471180257472 entropy 1.3432114124298096
epoch: 5, step: 62
	action: tensor([[ 0.0627, -1.6584,  0.5669,  0.0893,  0.0830,  0.5221,  0.8253]],
       dtype=torch.float64)
	q_value: tensor([[-36.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 63
	action: tensor([[-0.1631, -1.2531, -0.4261, -1.0298,  1.1322, -0.6632, -0.9061]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7319283405244976, distance: 1.5059884902393388 entropy 1.3432114124298096
epoch: 5, step: 64
	action: tensor([[-0.1316, -1.2604,  0.3840, -0.0731, -0.1188,  0.2073,  0.5281]],
       dtype=torch.float64)
	q_value: tensor([[-50.6438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8750336193439261, distance: 1.5669719516308642 entropy 1.3432114124298096
epoch: 5, step: 65
	action: tensor([[ 1.8907, -1.0473, -0.1551, -1.5154, -0.1788,  1.2552,  1.5219]],
       dtype=torch.float64)
	q_value: tensor([[-36.7897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6206519070680865, distance: 1.456805477027162 entropy 1.3432114124298096
epoch: 5, step: 66
	action: tensor([[ 2.8667, -3.7266,  3.8869, -1.7374,  0.5116,  0.7619,  4.4180]],
       dtype=torch.float64)
	q_value: tensor([[-56.1969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 67
	action: tensor([[-0.2237,  1.5334,  1.5333,  0.4814, -0.8326,  2.0350,  0.3280]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 68
	action: tensor([[ 0.2738, -1.0016, -0.1421, -0.1004, -0.6373,  0.2465,  0.3301]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49272841206911755, distance: 1.3981285134925894 entropy 1.3432114124298096
epoch: 5, step: 69
	action: tensor([[ 0.6981, -2.7953,  2.4562, -0.9921, -0.4429,  0.4262,  1.4140]],
       dtype=torch.float64)
	q_value: tensor([[-35.2661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 70
	action: tensor([[-0.0937, -0.5883,  1.3959, -0.8679, -0.7767, -1.0009,  0.2663]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7246966686876168, distance: 1.502841071622047 entropy 1.3432114124298096
epoch: 5, step: 71
	action: tensor([[ 0.0695, -0.4004,  0.9836, -0.1718,  0.8544,  1.8120, -0.3402]],
       dtype=torch.float64)
	q_value: tensor([[-43.0233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3869578430479491, distance: 0.8959873199773298 entropy 1.3432114124298096
epoch: 5, step: 72
	action: tensor([[-0.6779, -2.7636,  0.5662,  0.0175,  1.1145,  0.5677,  1.1078]],
       dtype=torch.float64)
	q_value: tensor([[-48.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 73
	action: tensor([[-0.3579, -1.5842,  1.5046, -0.5879, -1.2287,  0.7057,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.028345537832942, distance: 1.6297750455011997 entropy 1.3432114124298096
epoch: 5, step: 74
	action: tensor([[ 2.3682, -1.5838,  2.0462,  0.2316, -0.0644,  0.1832,  2.8047]],
       dtype=torch.float64)
	q_value: tensor([[-50.6023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 75
	action: tensor([[ 1.3228,  0.0759,  1.5043, -1.0137, -0.1365,  0.1464,  1.6398]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8025155891044919, distance: 0.508537639315415 entropy 1.3432114124298096
epoch: 5, step: 76
	action: tensor([[ 1.7862, -2.2548,  1.2675, -0.3167,  1.4050,  1.9947,  3.1133]],
       dtype=torch.float64)
	q_value: tensor([[-49.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 77
	action: tensor([[ 0.2177,  0.6094, -0.5437, -1.7761, -0.9888, -0.2844,  0.1689]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 78
	action: tensor([[ 1.2990, -0.0202,  1.3967,  1.5382, -1.3333,  1.5076, -0.4307]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 79
	action: tensor([[ 1.2190, -0.6350, -2.1104, -0.8655,  0.3522,  0.9328,  0.4097]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 80
	action: tensor([[-0.8325,  0.1894,  0.3995, -1.5114,  0.2216, -0.6999, -0.2393]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7366067348388525, distance: 1.5080211541292192 entropy 1.3432114124298096
epoch: 5, step: 81
	action: tensor([[ 0.4066, -0.0190,  1.3913, -0.8178, -0.2721,  0.5897,  1.6060]],
       dtype=torch.float64)
	q_value: tensor([[-39.1873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3809906257605786, distance: 0.9003374314381508 entropy 1.3432114124298096
epoch: 5, step: 82
	action: tensor([[ 4.2879, -2.5620,  1.4088, -0.4063,  1.8954,  1.8385,  4.6740]],
       dtype=torch.float64)
	q_value: tensor([[-46.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 83
	action: tensor([[1.9943, 0.5311, 0.7181, 1.5965, 0.1504, 1.0118, 0.1646]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 84
	action: tensor([[-1.2325,  1.1234,  0.2659, -0.4854,  0.8370, -1.2520,  0.2165]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6278043838663263, distance: 1.4600166219224693 entropy 1.3432114124298096
epoch: 5, step: 85
	action: tensor([[ 1.1452, -0.0455,  1.0728, -0.1786,  0.2419, -0.0216, -0.8865]],
       dtype=torch.float64)
	q_value: tensor([[-42.0238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 86
	action: tensor([[ 0.7979, -1.2647, -0.1183, -1.4708,  0.7198, -0.4544, -0.6112]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7980059360592704, distance: 1.5344482770235264 entropy 1.3432114124298096
epoch: 5, step: 87
	action: tensor([[ 0.1259, -0.2739,  1.6833, -0.1685,  0.6870,  0.5502,  1.6277]],
       dtype=torch.float64)
	q_value: tensor([[-49.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11260044555447557, distance: 1.077993882871979 entropy 1.3432114124298096
epoch: 5, step: 88
	action: tensor([[ 2.2886, -0.9596,  2.3993, -0.7055,  1.1273,  0.8773,  2.2147]],
       dtype=torch.float64)
	q_value: tensor([[-47.7009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 89
	action: tensor([[ 0.8887, -2.3090, -0.2557, -0.2732,  0.6177, -1.4143,  1.2711]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 90
	action: tensor([[ 1.1502,  0.3648, -1.2072,  0.3718,  1.1075, -0.7962, -1.1864]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 91
	action: tensor([[ 0.5970,  1.6417, -0.1008,  0.1451, -0.1180,  0.2581,  0.6233]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 92
	action: tensor([[ 0.5982, -0.8641,  0.5596,  1.3190, -2.5401,  0.1938, -0.5679]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7355825652313821, distance: 0.5884393906526034 entropy 1.3432114124298096
epoch: 5, step: 93
	action: tensor([[ 2.9473, -3.1119,  0.9095,  0.1240,  1.2055,  1.0793,  1.9906]],
       dtype=torch.float64)
	q_value: tensor([[-60.4139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 94
	action: tensor([[ 1.0920, -1.7307,  0.6944, -0.6528, -0.3792, -0.4861, -0.7581]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8349856665757096, distance: 1.5501475255734307 entropy 1.3432114124298096
epoch: 5, step: 95
	action: tensor([[ 1.4788, -1.0202,  1.7555, -0.8245,  0.2791, -0.4542,  1.4793]],
       dtype=torch.float64)
	q_value: tensor([[-44.7461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2691597817726017, distance: 0.978290672068388 entropy 1.3432114124298096
epoch: 5, step: 96
	action: tensor([[ 2.4372, -3.1301,  2.8870, -0.6455, -0.1774, -0.1429,  3.6252]],
       dtype=torch.float64)
	q_value: tensor([[-52.1753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 97
	action: tensor([[ 0.8115,  0.5946, -0.5429,  0.8128,  1.4858, -0.1754,  0.1402]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 98
	action: tensor([[ 2.3560, -0.7644,  0.7906,  1.0336,  1.2420, -0.6421, -0.2698]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06504646998729124, distance: 1.1065007333715178 entropy 1.3432114124298096
epoch: 5, step: 99
	action: tensor([[ 1.4824,  1.0274,  1.3327, -1.4153,  0.9938,  2.0717,  0.6172]],
       dtype=torch.float64)
	q_value: tensor([[-53.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8143848279868016, distance: 0.49301874217170094 entropy 1.3432114124298096
epoch: 5, step: 100
	action: tensor([[ 3.5330, -4.4737,  1.4887, -2.3833, -0.2968,  0.5724,  5.0604]],
       dtype=torch.float64)
	q_value: tensor([[-61.1987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 101
	action: tensor([[-0.2094, -0.9928,  0.7333, -1.0166,  0.1011, -0.6038, -0.8941]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0621826044030778, distance: 1.6433128557810799 entropy 1.3432114124298096
epoch: 5, step: 102
	action: tensor([[ 1.0783, -0.7598, -0.4378,  0.5450,  2.5838, -0.1114, -0.8654]],
       dtype=torch.float64)
	q_value: tensor([[-43.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3749068845841438, distance: 0.9047509569709403 entropy 1.3432114124298096
epoch: 5, step: 103
	action: tensor([[-0.5058, -0.7896,  1.2163, -1.4902, -0.5378,  0.2956, -0.4156]],
       dtype=torch.float64)
	q_value: tensor([[-61.8838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.174347438896405, distance: 1.687412118923236 entropy 1.3432114124298096
epoch: 5, step: 104
	action: tensor([[ 2.0772, -1.1315,  1.5489,  0.2197, -0.1361,  0.1354, -0.1923]],
       dtype=torch.float64)
	q_value: tensor([[-45.8475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8083942636475532, distance: 1.5388746787133596 entropy 1.3432114124298096
epoch: 5, step: 105
	action: tensor([[ 0.6239, -1.3231,  1.0420, -1.0479,  0.7943,  1.7484,  1.8867]],
       dtype=torch.float64)
	q_value: tensor([[-51.2175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 106
	action: tensor([[-0.1927,  1.7200, -0.0928,  0.5995, -0.3431, -0.6291, -1.2462]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 107
	action: tensor([[-1.3562, -0.1197, -0.3917, -0.3624, -0.0032, -0.0646, -0.3262]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4407993535310135, distance: 1.7878156435460837 entropy 1.3432114124298096
epoch: 5, step: 108
	action: tensor([[-1.0402, -0.0447, -0.5287, -0.4577,  1.2412,  0.5874, -0.6436]],
       dtype=torch.float64)
	q_value: tensor([[-35.2294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7643523445919382, distance: 1.520020176455618 entropy 1.3432114124298096
epoch: 5, step: 109
	action: tensor([[ 1.0664,  0.3760,  0.1502, -0.0590,  0.5113,  0.0788, -0.1457]],
       dtype=torch.float64)
	q_value: tensor([[-41.9138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9179467866712613, distance: 0.3277966342037447 entropy 1.3432114124298096
epoch: 5, step: 110
	action: tensor([[-0.3973, -0.7928,  1.6837, -1.6679, -0.9452,  0.4485,  0.2181]],
       dtype=torch.float64)
	q_value: tensor([[-32.2048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7917593833548782, distance: 1.5317805017701205 entropy 1.3432114124298096
epoch: 5, step: 111
	action: tensor([[ 2.4906, -2.5037,  2.5786, -1.2545,  0.3549,  1.6749,  0.7087]],
       dtype=torch.float64)
	q_value: tensor([[-50.7031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 112
	action: tensor([[-0.8759, -0.2652, -0.3062, -2.1760,  1.4219,  0.0753,  0.6271]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03142673071876034, distance: 1.162186655664519 entropy 1.3432114124298096
epoch: 5, step: 113
	action: tensor([[ 0.9027, -1.3563,  0.8049, -0.5136,  0.2408, -0.3152,  1.2707]],
       dtype=torch.float64)
	q_value: tensor([[-53.5574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7919983476478547, distance: 1.5318826440120514 entropy 1.3432114124298096
epoch: 5, step: 114
	action: tensor([[ 2.0300, -2.4284,  2.3988, -1.6713,  1.6540, -0.2697,  1.0101]],
       dtype=torch.float64)
	q_value: tensor([[-44.0721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 115
	action: tensor([[-1.3222, -0.1103, -0.9373, -1.1141,  0.6360, -0.6050, -0.7867]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5883127684243632, distance: 1.4421973953267568 entropy 1.3432114124298096
epoch: 5, step: 116
	action: tensor([[ 0.5644, -0.7801, -1.1933, -1.8426, -0.1649, -0.1192, -0.5548]],
       dtype=torch.float64)
	q_value: tensor([[-44.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15357445920702806, distance: 1.0528125980491265 entropy 1.3432114124298096
epoch: 5, step: 117
	action: tensor([[ 0.6397, -1.0438, -0.2894, -1.5321, -1.2139,  0.9296,  2.1433]],
       dtype=torch.float64)
	q_value: tensor([[-46.9323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7445743091161232, distance: 1.5114766055813758 entropy 1.3432114124298096
epoch: 5, step: 118
	action: tensor([[ 4.4116, -4.0608,  3.4360, -3.4520, -0.2239,  2.8652,  3.1493]],
       dtype=torch.float64)
	q_value: tensor([[-56.7204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 119
	action: tensor([[ 1.0585, -0.2722, -2.6229, -0.2947,  1.2124, -0.9115, -0.4293]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.484094367528246, distance: 0.8219432399238418 entropy 1.3432114124298096
epoch: 5, step: 120
	action: tensor([[ 0.8173, -1.7854,  1.1769, -0.0889,  0.7611, -0.4893,  0.8989]],
       dtype=torch.float64)
	q_value: tensor([[-51.7524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 121
	action: tensor([[-0.2365,  1.0448,  0.4018, -0.8013, -0.6310, -0.8370,  1.0141]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 122
	action: tensor([[ 0.8436, -0.0653, -0.6484, -0.3596,  0.0769, -1.2821, -1.6836]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 123
	action: tensor([[ 0.2398,  1.6392,  0.3755,  0.9331, -0.1066,  0.9390,  0.3044]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 124
	action: tensor([[-0.3569,  0.1067,  0.0495,  1.0903, -0.3995,  1.2244, -0.2543]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 125
	action: tensor([[-1.5042,  0.5653, -0.2443, -1.2243,  1.1670, -0.1023,  2.0880]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1392489738205467, distance: 1.6737375495349598 entropy 1.3432114124298096
epoch: 5, step: 126
	action: tensor([[ 3.1430, -2.4448,  0.6528, -1.0996, -1.6713,  0.5060,  1.7167]],
       dtype=torch.float64)
	q_value: tensor([[-54.2124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 5, step: 127
	action: tensor([[ 0.7079,  0.0817, -0.5692, -0.2945,  0.2162, -0.2821, -0.6190]],
       dtype=torch.float64)
	q_value: tensor([[-43.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
LOSS epoch 5 actor 388.07753830531374 critic 51.403346498135996 
epoch: 6, step: 0
	action: tensor([[ 1.7713e-04, -2.0731e-01,  1.2604e-01, -1.0480e+00, -9.2519e-01,
          1.4396e+00,  6.0150e-01]], dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14390776631759272, distance: 1.223917649428507 entropy 1.3432114124298096
epoch: 6, step: 1
	action: tensor([[ 0.8833, -0.8202,  0.8822, -0.6673, -0.0511,  2.3915,  2.6546]],
       dtype=torch.float64)
	q_value: tensor([[-38.1125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 2
	action: tensor([[-2.0272,  0.4395, -1.2496,  1.0998,  0.3329, -1.4431, -1.7555]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 3
	action: tensor([[ 0.1311, -0.1438,  1.8907,  0.2471, -0.6132, -1.2850, -0.0904]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13869360865371194, distance: 1.0620269272259912 entropy 1.3432114124298096
epoch: 6, step: 4
	action: tensor([[-1.3761,  1.0778, -0.6024, -0.4959, -0.3791, -0.0488,  0.7466]],
       dtype=torch.float64)
	q_value: tensor([[-43.4988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38478067830977336, distance: 1.3466266062966405 entropy 1.3432114124298096
epoch: 6, step: 5
	action: tensor([[ 0.9167,  1.0369,  1.1836, -0.4982,  0.0647,  0.8292,  1.6510]],
       dtype=torch.float64)
	q_value: tensor([[-33.3848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 6
	action: tensor([[-0.9604, -0.1998,  0.3740, -0.4371, -0.8374, -0.1136,  0.0559]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3442001788807687, distance: 1.752080437588581 entropy 1.3432114124298096
epoch: 6, step: 7
	action: tensor([[ 0.5624, -1.0555,  1.5167,  0.4174,  1.4245, -1.2629,  0.3980]],
       dtype=torch.float64)
	q_value: tensor([[-30.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18783305852411758, distance: 1.0312865345874305 entropy 1.3432114124298096
epoch: 6, step: 8
	action: tensor([[ 1.3927, -0.1658, -0.2111,  0.3414, -0.9072, -2.1709, -1.3860]],
       dtype=torch.float64)
	q_value: tensor([[-45.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4452511975532615, distance: 0.8523243186167363 entropy 1.3432114124298096
epoch: 6, step: 9
	action: tensor([[-0.1132, -1.0562,  0.4029, -0.7196,  0.0383, -1.4853,  0.3671]],
       dtype=torch.float64)
	q_value: tensor([[-48.3317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6423475513197661, distance: 1.4665241763385461 entropy 1.3432114124298096
epoch: 6, step: 10
	action: tensor([[ 0.8334, -0.7297,  1.0016, -0.2841, -0.4567, -0.4400, -1.4082]],
       dtype=torch.float64)
	q_value: tensor([[-36.2212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2786127725174883, distance: 1.293975973642228 entropy 1.3432114124298096
epoch: 6, step: 11
	action: tensor([[ 0.7425, -0.4933,  0.3207, -0.7466,  0.0867,  0.2434,  2.0149]],
       dtype=torch.float64)
	q_value: tensor([[-40.2790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17403516681578024, distance: 1.239930223727684 entropy 1.3432114124298096
epoch: 6, step: 12
	action: tensor([[ 1.8517, -2.4227,  1.9371, -0.2129,  0.3670,  2.4080,  1.1612]],
       dtype=torch.float64)
	q_value: tensor([[-40.0053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 13
	action: tensor([[-0.9657,  0.7126,  0.6091, -0.2870,  0.6794, -0.1176, -0.5953]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 14
	action: tensor([[ 0.6757,  0.8863,  1.1646,  0.0484, -0.2885,  1.6861,  0.1908]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 15
	action: tensor([[-0.2445,  0.0792,  0.0253,  1.4214,  0.6169, -0.6461, -0.2805]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 16
	action: tensor([[-0.1426, -1.0012, -0.0158, -1.5523,  0.2306, -1.5184, -0.0302]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06789192282334366, distance: 1.1825522645971644 entropy 1.3432114124298096
epoch: 6, step: 17
	action: tensor([[ 0.6468, -1.3330, -0.8050, -1.1964, -1.7202, -2.2295,  2.0076]],
       dtype=torch.float64)
	q_value: tensor([[-40.4064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 18
	action: tensor([[-0.6242, -1.2546, -0.7262, -0.6516,  1.5395, -1.1140,  0.8325]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8240519833639315, distance: 1.5455223819032116 entropy 1.3432114124298096
epoch: 6, step: 19
	action: tensor([[-0.7226, -2.3971,  0.5095,  0.7328, -0.4923,  1.7804,  2.4228]],
       dtype=torch.float64)
	q_value: tensor([[-42.2560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 20
	action: tensor([[ 0.9129, -0.5821,  0.6499,  0.0864, -0.1560, -1.7009, -0.3176]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05129157853122568, distance: 1.1146103490862025 entropy 1.3432114124298096
epoch: 6, step: 21
	action: tensor([[ 1.4600,  1.1938, -0.7386,  0.3967, -0.4641, -1.9432,  0.0967]],
       dtype=torch.float64)
	q_value: tensor([[-39.3935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6124256472783556, distance: 0.7124168921185282 entropy 1.3432114124298096
epoch: 6, step: 22
	action: tensor([[ 1.0400, -0.3826,  1.0516, -0.3782,  0.1657,  0.5899,  0.4819]],
       dtype=torch.float64)
	q_value: tensor([[-40.2972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3401334534360513, distance: 0.929575671395578 entropy 1.3432114124298096
epoch: 6, step: 23
	action: tensor([[ 3.1609, -0.2212,  0.8659, -1.7006,  0.0823,  0.1224,  0.3623]],
       dtype=torch.float64)
	q_value: tensor([[-33.7107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 24
	action: tensor([[-0.0035, -0.7950,  0.4426, -0.7575, -0.8939, -0.0070,  0.6865]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8787818837088506, distance: 1.5685373882971947 entropy 1.3432114124298096
epoch: 6, step: 25
	action: tensor([[ 1.2497, -0.8843,  1.3771, -1.5650,  1.1213, -0.2240,  1.4030]],
       dtype=torch.float64)
	q_value: tensor([[-32.8387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2834063467162331, distance: 0.9687086371029191 entropy 1.3432114124298096
epoch: 6, step: 26
	action: tensor([[ 0.0432, -2.0622,  1.9792, -1.8131, -0.8628,  0.9362,  2.1979]],
       dtype=torch.float64)
	q_value: tensor([[-45.3376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 27
	action: tensor([[ 0.7433,  0.5061,  1.2774, -2.3573, -0.9373, -0.1247,  0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11568569023462316, distance: 1.076118306813594 entropy 1.3432114124298096
epoch: 6, step: 28
	action: tensor([[ 0.0660, -0.6902,  2.0172, -0.1355, -1.1630,  0.0540,  0.1294]],
       dtype=torch.float64)
	q_value: tensor([[-40.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2917852809569996, distance: 1.3006242859862802 entropy 1.3432114124298096
epoch: 6, step: 29
	action: tensor([[ 1.1365,  0.6605,  0.1213, -0.7341, -1.7234,  1.3580,  1.1990]],
       dtype=torch.float64)
	q_value: tensor([[-41.7038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9075334491048725, distance: 0.3479757843310925 entropy 1.3432114124298096
epoch: 6, step: 30
	action: tensor([[ 3.0639, -1.5659, -0.5657, -0.9400,  1.5406,  0.2082,  1.8872]],
       dtype=torch.float64)
	q_value: tensor([[-43.2722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 31
	action: tensor([[-0.0879, -1.0207,  1.0826, -1.0064, -1.6255, -0.5081,  0.2238]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0843560343293066, distance: 1.6521240194256193 entropy 1.3432114124298096
epoch: 6, step: 32
	action: tensor([[-0.5069,  0.5289,  1.9059, -1.2595, -0.8264, -0.1593,  2.9348]],
       dtype=torch.float64)
	q_value: tensor([[-41.8884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5040044858996835, distance: 1.4033993113056413 entropy 1.3432114124298096
epoch: 6, step: 33
	action: tensor([[ 0.3628, -1.7704,  1.8639, -0.8954,  2.6918,  1.4806,  1.8002]],
       dtype=torch.float64)
	q_value: tensor([[-52.0097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 34
	action: tensor([[ 1.0529,  1.3061, -2.8132, -2.2483,  0.3046,  2.1559, -0.6849]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 35
	action: tensor([[ 1.3283,  0.7019, -1.6655, -0.6047, -0.1464,  0.1247,  0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6326953477441932, distance: 0.6935374348737949 entropy 1.3432114124298096
epoch: 6, step: 36
	action: tensor([[-0.0930, -1.1460,  0.5832, -3.0477,  1.1291,  0.1307,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-32.7485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 37
	action: tensor([[-0.0070,  0.0487,  0.8988,  2.0534, -1.6942,  1.5558, -1.1482]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 38
	action: tensor([[ 0.8749,  0.3933,  0.6570, -0.8050, -0.5521, -1.6572, -0.5730]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 39
	action: tensor([[ 2.1783, -0.5672,  0.8211, -0.7164, -0.6720, -0.4616,  1.4764]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19998721543060372, distance: 1.2535596452893036 entropy 1.3432114124298096
epoch: 6, step: 40
	action: tensor([[ 3.2502, -1.7279,  2.4637, -1.0596, -0.6144, -0.2221,  1.6384]],
       dtype=torch.float64)
	q_value: tensor([[-41.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 41
	action: tensor([[-0.4829,  1.3425,  1.4109,  0.1287,  0.3350, -0.7326, -0.3324]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 42
	action: tensor([[-1.1086, -0.4206, -1.0749, -0.0997,  0.1540,  1.7161,  0.0303]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8788995185157593, distance: 1.568586492375287 entropy 1.3432114124298096
epoch: 6, step: 43
	action: tensor([[ 0.6625, -0.4207, -0.5773, -1.4743,  0.1406,  1.9140,  0.5842]],
       dtype=torch.float64)
	q_value: tensor([[-38.7493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3730468528343329, distance: 0.9060960489552182 entropy 1.3432114124298096
epoch: 6, step: 44
	action: tensor([[3.2146, 0.7119, 4.3301, 0.9477, 0.0412, 0.8159, 2.7358]],
       dtype=torch.float64)
	q_value: tensor([[-46.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 45
	action: tensor([[ 0.6468, -0.0499,  1.3506, -0.6001,  0.9471, -1.0406, -0.9421]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5897386001478427, distance: 0.7329713880514462 entropy 1.3432114124298096
epoch: 6, step: 46
	action: tensor([[ 0.3379, -0.6326,  0.1287,  0.2514,  1.1394, -0.4354,  0.9490]],
       dtype=torch.float64)
	q_value: tensor([[-41.0066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08143141154356759, distance: 1.0967622360778368 entropy 1.3432114124298096
epoch: 6, step: 47
	action: tensor([[ 1.2483, -1.2869,  0.8762, -0.8874, -0.8788,  1.3951, -0.3191]],
       dtype=torch.float64)
	q_value: tensor([[-33.6348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11132224848931682, distance: 1.2063593587213273 entropy 1.3432114124298096
epoch: 6, step: 48
	action: tensor([[ 2.5687, -1.9887, -0.9221, -1.4992,  0.6599,  0.3025,  1.7802]],
       dtype=torch.float64)
	q_value: tensor([[-44.9612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 49
	action: tensor([[ 0.8353, -0.3108, -0.3758, -0.8340, -0.3967, -0.1115,  0.0940]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.038458767288326756, distance: 1.1661416902142416 entropy 1.3432114124298096
epoch: 6, step: 50
	action: tensor([[-0.0078, -0.2672,  0.6250,  0.4633,  0.5551, -1.0530,  0.9211]],
       dtype=torch.float64)
	q_value: tensor([[-28.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12158517004591196, distance: 1.0725227742132233 entropy 1.3432114124298096
epoch: 6, step: 51
	action: tensor([[-1.8814,  1.1855,  0.0411,  0.8536, -0.4304, -0.3020, -0.3224]],
       dtype=torch.float64)
	q_value: tensor([[-32.8145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 52
	action: tensor([[-0.0191, -0.3858,  0.4061, -1.6859, -0.9107,  1.5393,  1.0207]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3769178406428533, distance: 1.3427980652162297 entropy 1.3432114124298096
epoch: 6, step: 53
	action: tensor([[ 2.9363, -1.9309,  0.1540, -2.4045, -0.4019,  0.8083,  2.3071]],
       dtype=torch.float64)
	q_value: tensor([[-44.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 54
	action: tensor([[-0.3413, -0.0946, -1.3438,  0.3600, -0.6691, -1.0475, -0.7164]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11340843409554269, distance: 1.0775030075631564 entropy 1.3432114124298096
epoch: 6, step: 55
	action: tensor([[-0.3271, -1.0765,  1.2603, -1.4171, -0.3662,  0.3808, -0.3994]],
       dtype=torch.float64)
	q_value: tensor([[-33.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.169507412178199, distance: 1.6855330103697261 entropy 1.3432114124298096
epoch: 6, step: 56
	action: tensor([[-0.1290, -0.6222,  0.4808, -0.0763, -0.5663, -1.7301,  0.1968]],
       dtype=torch.float64)
	q_value: tensor([[-40.4507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3017835997511993, distance: 1.3056479500366815 entropy 1.3432114124298096
epoch: 6, step: 57
	action: tensor([[ 1.2391, -1.0629,  1.9818,  0.4706, -0.7152, -1.2980,  0.9377]],
       dtype=torch.float64)
	q_value: tensor([[-36.7074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4158416854007363, distance: 1.3616454545671612 entropy 1.3432114124298096
epoch: 6, step: 58
	action: tensor([[ 1.7568, -0.1117,  1.8993, -1.5870, -0.7958,  1.1822,  0.4026]],
       dtype=torch.float64)
	q_value: tensor([[-49.5651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 59
	action: tensor([[-0.3324, -0.4630,  0.8456, -0.4913, -0.3092, -0.5890,  1.2331]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8698249390492079, distance: 1.564793982137238 entropy 1.3432114124298096
epoch: 6, step: 60
	action: tensor([[ 2.0963,  0.6231,  0.4054, -0.0805,  0.5216,  2.0282,  0.3499]],
       dtype=torch.float64)
	q_value: tensor([[-32.0620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 61
	action: tensor([[ 0.3728, -0.8386,  1.4011, -1.6389, -0.5767, -0.6620,  1.0746]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35143331360473273, distance: 1.3303135156730714 entropy 1.3432114124298096
epoch: 6, step: 62
	action: tensor([[-0.8629, -0.8763,  2.0167, -0.4349,  0.3091,  0.9204,  2.0964]],
       dtype=torch.float64)
	q_value: tensor([[-41.1407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1791179793906013, distance: 1.6892622044164238 entropy 1.3432114124298096
epoch: 6, step: 63
	action: tensor([[ 1.5419, -2.6879,  1.5275, -1.0939,  0.2616,  0.5433,  2.5014]],
       dtype=torch.float64)
	q_value: tensor([[-47.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 64
	action: tensor([[ 8.3773e-02, -2.3375e-01,  2.5051e-01,  1.0961e+00, -1.9822e-01,
          6.8334e-01, -6.1795e-04]], dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 65
	action: tensor([[-0.3308, -1.3694,  0.7067,  1.2736,  0.2063, -0.4654,  2.4753]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019541506882238213, distance: 1.1554712629309394 entropy 1.3432114124298096
epoch: 6, step: 66
	action: tensor([[ 1.8328, -1.4347, -0.3761, -0.5244, -0.2202,  0.3069,  0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-47.2076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 67
	action: tensor([[ 1.9775, -0.8429, -0.1696, -0.8126,  1.0046,  0.1012,  0.2774]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 68
	action: tensor([[-0.6966,  1.6318,  0.8071,  0.1806,  0.8352, -0.4255,  0.1592]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 69
	action: tensor([[-0.2250,  1.4757, -0.6445,  0.4505,  0.2134,  1.3106, -0.2249]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 70
	action: tensor([[-0.7709,  0.0523,  1.1247, -0.2274,  0.7346,  0.0423,  1.2125]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8348041238304333, distance: 1.5500708424136358 entropy 1.3432114124298096
epoch: 6, step: 71
	action: tensor([[-0.1650, -0.9410,  0.7941, -0.3714,  1.7498, -1.6270,  1.8563]],
       dtype=torch.float64)
	q_value: tensor([[-34.2588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4084725618232785, distance: 1.3580973092685256 entropy 1.3432114124298096
epoch: 6, step: 72
	action: tensor([[-0.8210,  0.0877,  0.9785, -0.7054,  1.4412,  0.3472,  0.8141]],
       dtype=torch.float64)
	q_value: tensor([[-46.7839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1217228731721653, distance: 1.666867283049158 entropy 1.3432114124298096
epoch: 6, step: 73
	action: tensor([[ 0.6250, -0.8397, -0.3559,  0.1790,  0.2049,  1.5329,  0.3655]],
       dtype=torch.float64)
	q_value: tensor([[-38.3846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5509128422885687, distance: 0.7668704640104531 entropy 1.3432114124298096
epoch: 6, step: 74
	action: tensor([[ 2.1689, -1.9390,  1.1405, -2.0757, -0.5541,  0.0826,  2.1581]],
       dtype=torch.float64)
	q_value: tensor([[-36.2203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 75
	action: tensor([[ 0.3476,  0.4011, -0.9341,  0.9038,  0.1545, -0.5956, -0.0715]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 76
	action: tensor([[ 1.1390, -0.2938, -0.5055, -0.4819,  0.7337,  1.2222, -0.4393]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 77
	action: tensor([[ 1.2662,  0.6282, -1.8854, -1.1009, -1.6716, -0.9168,  0.5362]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 78
	action: tensor([[ 0.7327, -0.7837, -1.1041,  1.2912,  0.2603,  0.4483, -0.9116]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4409514245889238, distance: 0.8556210604087753 entropy 1.3432114124298096
epoch: 6, step: 79
	action: tensor([[ 1.3581, -1.5100, -0.0984, -2.6677, -0.1093, -1.5852,  0.9012]],
       dtype=torch.float64)
	q_value: tensor([[-37.0984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 80
	action: tensor([[-0.7183, -0.6178,  0.3082, -0.2672,  1.1563, -1.6710, -0.1954]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8769265410246856, distance: 1.567762712637563 entropy 1.3432114124298096
epoch: 6, step: 81
	action: tensor([[ 0.5210, -1.1368,  1.2800,  0.6786,  0.2109,  0.5099,  0.6419]],
       dtype=torch.float64)
	q_value: tensor([[-39.8478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06007653523152534, distance: 1.1782170524430706 entropy 1.3432114124298096
epoch: 6, step: 82
	action: tensor([[ 1.5476, -0.7693,  2.8550, -0.3005, -1.4160,  0.7142,  0.5549]],
       dtype=torch.float64)
	q_value: tensor([[-39.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13988461393077412, distance: 1.2217634791133023 entropy 1.3432114124298096
epoch: 6, step: 83
	action: tensor([[ 0.1829, -2.7750,  1.4565,  0.4808,  0.5498,  0.0073,  2.8143]],
       dtype=torch.float64)
	q_value: tensor([[-53.9308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 84
	action: tensor([[ 1.5210,  0.3413, -0.5921, -2.3975,  0.1056, -0.0883, -2.1585]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 85
	action: tensor([[ 0.8097, -0.0806, -1.7208, -0.3173,  0.5419,  1.2438,  0.9232]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 86
	action: tensor([[-0.1593,  1.6436, -0.0597, -0.7201,  0.2744,  1.0499,  0.2767]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 87
	action: tensor([[0.7139, 1.7061, 1.4474, 0.1656, 0.6157, 0.6670, 0.8301]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 88
	action: tensor([[ 0.9170,  0.8216, -0.8148, -0.5993,  0.0354,  1.3230,  0.7024]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 89
	action: tensor([[-1.6440,  0.1830,  0.9265,  0.5315,  0.0243,  0.6020, -0.9827]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.63790622731759, distance: 1.4645399074203969 entropy 1.3432114124298096
epoch: 6, step: 90
	action: tensor([[ 1.2658,  0.5832,  0.3550, -0.3550,  0.3212,  0.0230,  0.7238]],
       dtype=torch.float64)
	q_value: tensor([[-40.8483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 91
	action: tensor([[ 0.5413,  0.2661,  0.9493,  2.5038, -0.1885, -1.0941,  0.6635]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 92
	action: tensor([[ 0.4245, -0.6965,  0.2251, -0.8892, -0.3716, -0.2791, -1.2204]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6383666913266546, distance: 1.4647457557517654 entropy 1.3432114124298096
epoch: 6, step: 93
	action: tensor([[ 1.6054,  0.3044,  0.5295, -1.5893,  1.5558,  0.3566,  1.2392]],
       dtype=torch.float64)
	q_value: tensor([[-35.1997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23463789667052393, distance: 1.0011292964646483 entropy 1.3432114124298096
epoch: 6, step: 94
	action: tensor([[ 0.3971, -2.9284,  0.7540,  0.6146, -1.0696, -0.0627,  1.6598]],
       dtype=torch.float64)
	q_value: tensor([[-45.8727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 95
	action: tensor([[ 0.3572, -2.0615,  0.6153,  0.8499,  1.4972, -0.7831,  1.5705]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 96
	action: tensor([[-1.1624, -0.0206,  1.1195, -0.9318,  0.6168,  0.6713,  0.9971]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4418670381699554, distance: 1.7882066250082598 entropy 1.3432114124298096
epoch: 6, step: 97
	action: tensor([[ 1.0313, -0.1834,  0.4813, -1.7415, -1.1968, -0.9953,  1.9653]],
       dtype=torch.float64)
	q_value: tensor([[-38.7031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14476037926800323, distance: 1.224373688670739 entropy 1.3432114124298096
epoch: 6, step: 98
	action: tensor([[ 1.9640, -1.5406,  1.4521, -1.1305,  1.4432,  0.8376,  2.6401]],
       dtype=torch.float64)
	q_value: tensor([[-44.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 99
	action: tensor([[ 0.0131, -0.4622, -0.3973, -0.1568,  0.2924, -0.3323, -0.1199]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1864677483312216, distance: 1.2464781268642424 entropy 1.3432114124298096
epoch: 6, step: 100
	action: tensor([[ 0.8346,  1.5377, -0.6028,  1.7768,  0.7607,  0.8489,  2.3519]],
       dtype=torch.float64)
	q_value: tensor([[-26.1082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 101
	action: tensor([[ 0.5790, -1.0336, -0.6887,  0.3147, -0.1165,  0.3607,  0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1400173092133754, distance: 1.2218345904916894 entropy 1.3432114124298096
epoch: 6, step: 102
	action: tensor([[-0.6096, -0.2803,  2.4392,  0.3655,  0.4606,  1.0015,  0.9052]],
       dtype=torch.float64)
	q_value: tensor([[-31.0221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30107644393910094, distance: 1.3052932743597419 entropy 1.3432114124298096
epoch: 6, step: 103
	action: tensor([[ 1.5513, -1.8118,  1.3492, -2.1370,  0.0310, -0.1165,  1.1610]],
       dtype=torch.float64)
	q_value: tensor([[-44.3645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 104
	action: tensor([[-0.2007,  0.4748,  0.7090, -1.9647,  0.8024,  1.2571,  0.7762]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01755884690683529, distance: 1.154347217734034 entropy 1.3432114124298096
epoch: 6, step: 105
	action: tensor([[ 0.5399, -1.7776,  1.0542, -0.7891, -0.2259,  0.0895,  2.1115]],
       dtype=torch.float64)
	q_value: tensor([[-44.6673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 106
	action: tensor([[-0.5805,  1.5372,  0.7683,  0.7272, -0.1978,  0.1075, -0.5122]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 107
	action: tensor([[ 1.1349,  0.6537, -1.2198, -0.5164, -0.6017, -0.2112,  0.9934]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7548924839661846, distance: 0.5665457782558319 entropy 1.3432114124298096
epoch: 6, step: 108
	action: tensor([[ 1.5037,  0.2933,  2.4086,  0.9389,  0.2112, -0.5492,  1.3841]],
       dtype=torch.float64)
	q_value: tensor([[-33.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2971381620115964, distance: 0.9593822413618223 entropy 1.3432114124298096
epoch: 6, step: 109
	action: tensor([[ 2.2145, -0.1608,  3.4688, -0.3444,  0.8673,  0.5589,  0.6002]],
       dtype=torch.float64)
	q_value: tensor([[-52.0297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 110
	action: tensor([[ 0.3386,  0.2878,  1.3027, -0.9038,  0.1465,  0.3537,  1.2676]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3529249516944529, distance: 0.9205216769886405 entropy 1.3432114124298096
epoch: 6, step: 111
	action: tensor([[ 0.0303, -0.7490,  0.1626,  0.4050, -0.3865,  0.3774,  1.4481]],
       dtype=torch.float64)
	q_value: tensor([[-36.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048111777341616424, distance: 1.1164767151790589 entropy 1.3432114124298096
epoch: 6, step: 112
	action: tensor([[ 1.2907, -0.2336,  0.1362, -0.2913,  0.4997, -0.6144,  1.2299]],
       dtype=torch.float64)
	q_value: tensor([[-32.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2070528109485359, distance: 1.0190108899006436 entropy 1.3432114124298096
epoch: 6, step: 113
	action: tensor([[ 0.3164, -0.5055,  1.1310, -0.5647,  0.5100, -1.2420,  1.0690]],
       dtype=torch.float64)
	q_value: tensor([[-35.3584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10951997532337066, distance: 1.2053807625894382 entropy 1.3432114124298096
epoch: 6, step: 114
	action: tensor([[ 0.3294, -1.3955,  0.2923, -2.2176, -0.4401, -0.7293,  0.9383]],
       dtype=torch.float64)
	q_value: tensor([[-36.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03642410931405182, distance: 1.1649987161671949 entropy 1.3432114124298096
epoch: 6, step: 115
	action: tensor([[ 0.1315, -1.4156,  0.2704, -1.6754,  1.8352,  0.2064,  0.7964]],
       dtype=torch.float64)
	q_value: tensor([[-43.2299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 116
	action: tensor([[ 0.3429,  0.1175, -0.7984, -0.9654, -1.3526,  0.0952,  0.0609]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 117
	action: tensor([[-1.1317, -0.9369, -0.7112, -0.1715,  0.6448,  0.2207,  0.1297]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3561159902670226, distance: 1.7565277959797685 entropy 1.3432114124298096
epoch: 6, step: 118
	action: tensor([[ 0.6827, -0.4247,  0.1155, -0.8529, -0.0776, -0.5282,  0.3920]],
       dtype=torch.float64)
	q_value: tensor([[-33.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3039970867639681, distance: 1.3067575074574764 entropy 1.3432114124298096
epoch: 6, step: 119
	action: tensor([[ 0.6039, -0.6935, -0.7386,  0.6781, -0.2539,  0.4451,  1.0722]],
       dtype=torch.float64)
	q_value: tensor([[-28.6924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3831012941597627, distance: 0.8988011571008451 entropy 1.3432114124298096
epoch: 6, step: 120
	action: tensor([[ 1.0132,  0.1798,  1.0391, -0.6755, -0.8024,  0.2766,  1.4142]],
       dtype=torch.float64)
	q_value: tensor([[-32.6359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7995117454451125, distance: 0.5123906081094882 entropy 1.3432114124298096
epoch: 6, step: 121
	action: tensor([[ 0.7310, -1.0345,  1.9160,  0.8451,  1.5378,  0.9061,  1.6476]],
       dtype=torch.float64)
	q_value: tensor([[-38.0623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5856934573299761, distance: 1.441007729661365 entropy 1.3432114124298096
epoch: 6, step: 122
	action: tensor([[ 0.7849, -0.4750,  0.6003,  0.5408,  0.3673, -0.3286, -0.0814]],
       dtype=torch.float64)
	q_value: tensor([[-54.2685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5684950833149318, distance: 0.7517086842577249 entropy 1.3432114124298096
epoch: 6, step: 123
	action: tensor([[ 0.4097,  0.6688,  0.3111,  0.1497,  0.5231,  0.2220, -0.4309]],
       dtype=torch.float64)
	q_value: tensor([[-32.1512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 6, step: 124
	action: tensor([[-0.5672, -0.5615,  0.5472, -0.4536,  0.9982, -0.9766, -1.4549]],
       dtype=torch.float64)
	q_value: tensor([[-36.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.078362474662447, distance: 1.649746970508639 entropy 1.3432114124298096
epoch: 6, step: 125
	action: tensor([[ 0.0962, -0.6005,  0.1612, -0.9241, -0.5483, -0.6321, -0.5488]],
       dtype=torch.float64)
	q_value: tensor([[-42.6566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.561492708074141, distance: 1.4299691676750357 entropy 1.3432114124298096
epoch: 6, step: 126
	action: tensor([[-0.2425, -0.9984,  0.5470, -0.8019,  0.0787,  0.0020, -0.5292]],
       dtype=torch.float64)
	q_value: tensor([[-31.2179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1579161193435774, distance: 1.681024228122483 entropy 1.3432114124298096
epoch: 6, step: 127
	action: tensor([[ 1.4075, -0.8297, -1.5652, -0.3366, -0.8622, -0.3077,  0.5947]],
       dtype=torch.float64)
	q_value: tensor([[-33.1900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37856562191490495, distance: 1.3436012997517912 entropy 1.3432114124298096
LOSS epoch 6 actor 360.78158254747166 critic 134.47643287619852 
epoch: 7, step: 0
	action: tensor([[-0.0951,  0.6102,  1.4921, -1.3197, -2.5795, -2.3187,  2.7460]],
       dtype=torch.float64)
	q_value: tensor([[-34.5501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 1
	action: tensor([[ 1.1169, -1.0269, -1.0051, -0.0675, -0.3481,  0.8810, -1.5693]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16311272338881744, distance: 1.2341490025119066 entropy 1.3432114124298096
epoch: 7, step: 2
	action: tensor([[-1.3022, -0.2009, -0.6395,  0.8796,  0.7833,  0.0489, -0.4450]],
       dtype=torch.float64)
	q_value: tensor([[-36.5371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.165358182167013, distance: 1.6839204297449113 entropy 1.3432114124298096
epoch: 7, step: 3
	action: tensor([[ 0.5526,  1.2021, -1.1790, -0.0669,  0.2172, -1.6574,  0.9858]],
       dtype=torch.float64)
	q_value: tensor([[-29.7713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 4
	action: tensor([[-0.3394, -0.3395,  1.2697,  2.0023,  0.4691, -0.1408, -1.2897]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 5
	action: tensor([[ 1.6178, -0.8352,  1.5189, -0.6170, -1.0039, -0.6345,  1.2463]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08453260366899329, distance: 1.1917303493860993 entropy 1.3432114124298096
epoch: 7, step: 6
	action: tensor([[ 1.8057, -0.2749,  0.6057,  0.2882,  1.1999,  0.1619, -0.2837]],
       dtype=torch.float64)
	q_value: tensor([[-39.4892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 7
	action: tensor([[ 0.3821, -0.1396,  0.7547,  1.5360, -1.4990, -0.6247,  0.3022]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 8
	action: tensor([[ 0.0853, -0.4393,  0.3124, -1.3549,  1.3363,  1.0249,  1.6980]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22189837677081248, distance: 1.2649525567931739 entropy 1.3432114124298096
epoch: 7, step: 9
	action: tensor([[ 1.3118, -0.2740,  3.0266, -2.6162,  0.4966,  0.7428,  1.1667]],
       dtype=torch.float64)
	q_value: tensor([[-40.9433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 10
	action: tensor([[-0.7718, -0.0507, -1.0397,  0.2389,  1.0860,  0.2799,  1.0290]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7043260688456037, distance: 1.4939395910872555 entropy 1.3432114124298096
epoch: 7, step: 11
	action: tensor([[ 0.5520, -2.1382, -0.6729,  0.4163, -0.2753, -1.1718,  0.2932]],
       dtype=torch.float64)
	q_value: tensor([[-31.2603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 12
	action: tensor([[-0.6634, -0.9427, -1.4023,  2.1598, -0.1274,  1.1195,  1.0343]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03661071615972622, distance: 1.1232013036646882 entropy 1.3432114124298096
epoch: 7, step: 13
	action: tensor([[ 0.2364, -1.7784,  1.4706, -0.2442,  1.4285,  0.9891,  1.5125]],
       dtype=torch.float64)
	q_value: tensor([[-39.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 14
	action: tensor([[-0.7598, -1.8104, -1.3750, -0.8353,  0.1365, -1.1639, -0.4284]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 15
	action: tensor([[-0.5289, -0.7277,  0.1294,  0.6419,  0.7654, -0.7760, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7208952705752765, distance: 1.5011839545373642 entropy 1.3432114124298096
epoch: 7, step: 16
	action: tensor([[-1.8417, -1.0570,  0.3180, -0.2876,  1.2754,  1.4545,  0.2682]],
       dtype=torch.float64)
	q_value: tensor([[-29.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8915938868702016, distance: 1.5738764758355699 entropy 1.3432114124298096
epoch: 7, step: 17
	action: tensor([[ 1.8873, -1.1203, -0.0180,  0.5664, -1.0694,  1.1843,  1.3804]],
       dtype=torch.float64)
	q_value: tensor([[-41.3642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26702274287390737, distance: 0.9797199305387366 entropy 1.3432114124298096
epoch: 7, step: 18
	action: tensor([[ 2.8716, -1.8034,  1.0124, -0.2795,  0.2115, -0.8986,  1.2753]],
       dtype=torch.float64)
	q_value: tensor([[-39.8806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 19
	action: tensor([[-0.0875, -0.5234, -0.0627, -0.2031, -0.4934,  0.0396, -0.5135]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31619495062631864, distance: 1.3128551237618458 entropy 1.3432114124298096
epoch: 7, step: 20
	action: tensor([[ 0.4854, -1.0274,  0.5127,  0.5294, -0.4036,  0.5259,  0.0707]],
       dtype=torch.float64)
	q_value: tensor([[-23.6384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2536576686891985, distance: 0.9886116652405736 entropy 1.3432114124298096
epoch: 7, step: 21
	action: tensor([[-0.7583, -1.7692,  0.7543, -0.1762,  1.0716,  0.3038,  2.3289]],
       dtype=torch.float64)
	q_value: tensor([[-30.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 22
	action: tensor([[ 0.0365, -0.8942, -0.6040, -1.0804, -1.0055,  1.2856, -0.7019]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5843119602474658, distance: 1.4403798713095692 entropy 1.3432114124298096
epoch: 7, step: 23
	action: tensor([[ 0.4287, -0.8045,  1.3450, -1.1604,  1.4435,  0.8424,  1.9674]],
       dtype=torch.float64)
	q_value: tensor([[-36.4093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5700470912359454, distance: 1.4338807442055734 entropy 1.3432114124298096
epoch: 7, step: 24
	action: tensor([[ 1.7845, -0.3234,  1.0078, -1.7232,  0.3812,  0.5120,  1.6898]],
       dtype=torch.float64)
	q_value: tensor([[-43.6036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22551029221691776, distance: 1.00708126943665 entropy 1.3432114124298096
epoch: 7, step: 25
	action: tensor([[ 2.3856, -1.5148,  0.3021, -1.2358,  1.2084, -0.7439,  0.3626]],
       dtype=torch.float64)
	q_value: tensor([[-41.8920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 26
	action: tensor([[-0.1230,  1.1494, -1.1688, -1.2347, -0.7761, -1.8002, -0.8847]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 27
	action: tensor([[ 0.6288, -1.0723, -0.3134,  0.2151,  0.4586, -0.6077,  1.1018]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49676578015479955, distance: 1.400017989071623 entropy 1.3432114124298096
epoch: 7, step: 28
	action: tensor([[-0.5295, -0.3532,  0.6337,  0.5875, -1.1199,  0.8391, -0.5934]],
       dtype=torch.float64)
	q_value: tensor([[-31.6313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04258858837526336, distance: 1.1197111247030844 entropy 1.3432114124298096
epoch: 7, step: 29
	action: tensor([[ 1.1540, -1.3662, -0.9107, -0.7812,  0.0848, -0.4755,  1.7229]],
       dtype=torch.float64)
	q_value: tensor([[-33.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9248970880013356, distance: 1.587670776559889 entropy 1.3432114124298096
epoch: 7, step: 30
	action: tensor([[ 1.1612,  0.2347,  2.2707, -1.1587, -1.3227, -0.2489,  1.8957]],
       dtype=torch.float64)
	q_value: tensor([[-39.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9469086713610804, distance: 0.26367471801791725 entropy 1.3432114124298096
epoch: 7, step: 31
	action: tensor([[ 1.5376,  0.2664, -0.3051,  0.7257,  0.3422,  1.7031, -0.4808]],
       dtype=torch.float64)
	q_value: tensor([[-44.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7449007380396827, distance: 0.5779779819947085 entropy 1.3432114124298096
epoch: 7, step: 32
	action: tensor([[ 0.5945, -0.8934,  0.2947, -0.1059, -1.8176, -0.8813,  0.5078]],
       dtype=torch.float64)
	q_value: tensor([[-35.8130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12489059986386453, distance: 1.2137013557880603 entropy 1.3432114124298096
epoch: 7, step: 33
	action: tensor([[-2.0378, -1.6578,  1.0379,  0.4083, -0.4664, -0.2396,  2.4092]],
       dtype=torch.float64)
	q_value: tensor([[-35.9307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 34
	action: tensor([[ 1.5559, -0.5052,  0.1607, -0.0436, -1.1137,  0.9752,  0.2067]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34285541873354586, distance: 0.9276564291739853 entropy 1.3432114124298096
epoch: 7, step: 35
	action: tensor([[ 1.2218, -1.3022,  0.0724, -0.0985, -0.7304,  0.2940, -1.2242]],
       dtype=torch.float64)
	q_value: tensor([[-34.1226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.588455963102529, distance: 1.4422624046692458 entropy 1.3432114124298096
epoch: 7, step: 36
	action: tensor([[ 0.4937, -0.0445, -0.5188,  0.7208,  0.6553, -0.3136, -0.6643]],
       dtype=torch.float64)
	q_value: tensor([[-35.3833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7610977482232802, distance: 0.5593283273761395 entropy 1.3432114124298096
epoch: 7, step: 37
	action: tensor([[ 0.9663, -0.4347, -0.8835,  0.2147, -1.5627,  0.0541, -1.0990]],
       dtype=torch.float64)
	q_value: tensor([[-26.6602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4116436691957177, distance: 0.877762269294209 entropy 1.3432114124298096
epoch: 7, step: 38
	action: tensor([[ 0.2076,  1.0689, -0.3207,  0.5293, -0.0051, -0.8156,  0.8782]],
       dtype=torch.float64)
	q_value: tensor([[-33.1022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 39
	action: tensor([[-0.0856,  0.7505,  0.6561, -0.1250, -0.0075, -0.4720, -1.8540]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 40
	action: tensor([[ 0.2836,  0.3791, -0.3497,  0.2162,  0.4825, -0.7424, -1.0689]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 41
	action: tensor([[ 1.1248,  0.8576, -0.4491,  1.0182,  0.7164, -0.1521, -0.9271]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 42
	action: tensor([[-1.4452,  0.7278, -0.3692, -0.9384,  0.2073,  0.8140, -0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7462328479283524, distance: 1.5121949031856434 entropy 1.3432114124298096
epoch: 7, step: 43
	action: tensor([[ 0.4940, -0.2642, -0.6431,  0.6005, -0.5592, -0.0356, -0.8472]],
       dtype=torch.float64)
	q_value: tensor([[-32.7641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6223919405682247, distance: 0.7031975044021241 entropy 1.3432114124298096
epoch: 7, step: 44
	action: tensor([[-1.1530,  0.8903,  0.0026,  0.8830,  0.6689,  1.1860,  1.0564]],
       dtype=torch.float64)
	q_value: tensor([[-25.7236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 45
	action: tensor([[-1.6518, -1.4136,  2.4721, -1.0058,  0.4294, -0.6965,  0.7369]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4442874702249937, distance: 1.7890926602714565 entropy 1.3432114124298096
epoch: 7, step: 46
	action: tensor([[-0.7040, -0.9137,  0.3559,  0.4259,  0.6322,  1.3809,  0.8119]],
       dtype=torch.float64)
	q_value: tensor([[-41.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07788306045888604, distance: 1.1880713323464969 entropy 1.3432114124298096
epoch: 7, step: 47
	action: tensor([[ 0.9827, -1.0197,  0.2920, -1.1585,  0.1288,  1.1723,  2.0702]],
       dtype=torch.float64)
	q_value: tensor([[-34.3890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5136214997150674, distance: 1.4078790200783602 entropy 1.3432114124298096
epoch: 7, step: 48
	action: tensor([[ 1.9389, -2.8482,  2.2659,  1.2324,  0.2357, -1.1203,  2.0100]],
       dtype=torch.float64)
	q_value: tensor([[-42.6974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 49
	action: tensor([[-0.8582, -0.0583, -0.8307,  1.0748, -0.0201, -0.7673, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7258267621032617, distance: 1.5033333531675832 entropy 1.3432114124298096
epoch: 7, step: 50
	action: tensor([[-0.3627,  0.0606, -0.2187,  0.2292,  0.9632, -1.0406, -1.0944]],
       dtype=torch.float64)
	q_value: tensor([[-28.8739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26261692454694674, distance: 1.2858564761271079 entropy 1.3432114124298096
epoch: 7, step: 51
	action: tensor([[-0.1061, -0.3805,  0.5130, -0.4495, -0.1775,  0.4007,  1.4080]],
       dtype=torch.float64)
	q_value: tensor([[-30.6970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28855225405579565, distance: 1.2989956919542762 entropy 1.3432114124298096
epoch: 7, step: 52
	action: tensor([[ 1.3487, -1.6981,  0.7170, -1.6268,  0.0403,  1.0514, -0.0862]],
       dtype=torch.float64)
	q_value: tensor([[-29.6690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 53
	action: tensor([[ 0.0600, -1.0992,  1.3690,  0.8525,  0.8670,  0.5211,  0.8903]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.084572555418435, distance: 1.0948853820889677 entropy 1.3432114124298096
epoch: 7, step: 54
	action: tensor([[-0.3471,  1.9393,  0.5358, -0.1902,  0.7617, -0.7309,  0.2502]],
       dtype=torch.float64)
	q_value: tensor([[-37.4307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 55
	action: tensor([[-0.8443, -0.2686,  1.4818,  0.1639,  0.8560,  0.4106,  0.9227]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7070650470398621, distance: 1.4951395449500369 entropy 1.3432114124298096
epoch: 7, step: 56
	action: tensor([[-0.6581, -0.9177, -0.4367,  0.6993, -1.7452, -0.5564,  0.5484]],
       dtype=torch.float64)
	q_value: tensor([[-33.0024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7029304097381444, distance: 1.4933277781651288 entropy 1.3432114124298096
epoch: 7, step: 57
	action: tensor([[ 0.5603, -1.4102, -0.6015, -0.1762, -2.2621,  0.7061,  0.8985]],
       dtype=torch.float64)
	q_value: tensor([[-35.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 58
	action: tensor([[-0.6267, -0.3450,  0.3449, -0.0787,  1.0213,  0.4850, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.526137158896302, distance: 1.41368769125032 entropy 1.3432114124298096
epoch: 7, step: 59
	action: tensor([[-0.1986, -0.9834,  2.6846, -0.0538,  0.8898, -0.4414,  2.2731]],
       dtype=torch.float64)
	q_value: tensor([[-28.1196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5740836548461459, distance: 1.435722802147725 entropy 1.3432114124298096
epoch: 7, step: 60
	action: tensor([[ 0.8966, -0.9548,  1.2670, -0.1918, -0.2704,  0.7681,  1.1390]],
       dtype=torch.float64)
	q_value: tensor([[-45.9447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16359929611080326, distance: 1.2344071204214668 entropy 1.3432114124298096
epoch: 7, step: 61
	action: tensor([[ 1.1202, -2.0673,  2.2650, -1.3121,  0.6691,  1.1358, -0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-34.6541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 62
	action: tensor([[ 1.0348, -2.3673,  0.8405, -2.0009, -0.6413, -1.8354, -1.2703]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 63
	action: tensor([[ 0.8056,  0.7927,  0.4809,  0.4532, -0.3768,  0.6235,  0.4440]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 64
	action: tensor([[-0.2889,  2.0208,  0.0823, -0.1443,  0.7344, -0.3758,  0.3232]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 65
	action: tensor([[-0.5382,  0.3298,  1.2584,  0.2367,  0.5500, -0.7440,  0.5609]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 66
	action: tensor([[ 1.3059, -1.2774,  0.7374, -0.7024, -0.4855,  2.1038, -0.3638]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047684979569069696, distance: 1.1167269842561627 entropy 1.3432114124298096
epoch: 7, step: 67
	action: tensor([[ 1.1286, -2.3322, -0.0108, -0.6382, -0.5022,  1.1877,  1.7155]],
       dtype=torch.float64)
	q_value: tensor([[-44.4817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 68
	action: tensor([[ 0.4527, -1.4617,  0.0548,  0.8071, -0.0835, -1.3303,  0.2592]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3178207910582438, distance: 1.3136657309537567 entropy 1.3432114124298096
epoch: 7, step: 69
	action: tensor([[-0.9256, -0.0904,  0.3936,  0.3479,  0.7958,  0.8622, -0.4238]],
       dtype=torch.float64)
	q_value: tensor([[-34.0411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1705091459228978, distance: 1.2380668606883778 entropy 1.3432114124298096
epoch: 7, step: 70
	action: tensor([[-2.6886, -0.0280, -1.2195,  0.8779, -0.0632,  0.2287, -1.8759]],
       dtype=torch.float64)
	q_value: tensor([[-30.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 71
	action: tensor([[ 0.3018, -0.5220, -1.1497,  0.9714,  0.6870,  0.4385,  1.5059]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15048326303181125, distance: 1.0547333129683878 entropy 1.3432114124298096
epoch: 7, step: 72
	action: tensor([[1.0997, 1.2422, 0.0154, 0.0349, 0.0926, 0.0335, 0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-34.1950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 73
	action: tensor([[ 0.2019, -2.6175,  0.1693,  0.6817, -1.9828, -0.1531, -0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 74
	action: tensor([[-0.0289, -1.0067, -0.1174, -0.8017,  0.7021,  0.1604,  0.5296]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8199009967176718, distance: 1.543762811209884 entropy 1.3432114124298096
epoch: 7, step: 75
	action: tensor([[ 0.7583,  1.1306,  0.4985, -0.1472,  0.4484, -1.3479,  0.7194]],
       dtype=torch.float64)
	q_value: tensor([[-29.6586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 76
	action: tensor([[ 1.7481, -0.3938,  0.0602, -2.0916, -0.7037,  0.4757,  0.3241]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 77
	action: tensor([[ 0.8577, -0.4999,  0.1145, -0.1276, -0.6875,  0.2688, -0.3995]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27111900377174136, distance: 0.9769785009273133 entropy 1.3432114124298096
epoch: 7, step: 78
	action: tensor([[ 0.0192,  0.9655,  1.0912, -0.5885, -0.1917,  0.9335, -1.6495]],
       dtype=torch.float64)
	q_value: tensor([[-26.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 79
	action: tensor([[-0.3199,  0.4132,  1.1187, -0.2996,  0.8702, -0.2001, -0.6754]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 80
	action: tensor([[-0.7522, -0.4571, -0.0077,  0.1948, -0.3638,  0.3634, -0.4151]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7766965762698994, distance: 1.5253282930757468 entropy 1.3432114124298096
epoch: 7, step: 81
	action: tensor([[-0.8540, -1.4667, -0.7881,  1.1306, -0.6841,  0.8738,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-25.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 82
	action: tensor([[ 0.6555,  0.2018, -1.1166, -0.1909,  0.1106,  0.2098, -0.0751]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 83
	action: tensor([[-0.8492,  0.9715,  0.1092, -0.0428, -1.3058, -0.1480, -0.1524]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 84
	action: tensor([[ 0.4373, -0.9452,  0.2758,  0.6698, -0.0418,  0.4962, -0.3803]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4233677457999869, distance: 0.8689727523820485 entropy 1.3432114124298096
epoch: 7, step: 85
	action: tensor([[ 1.1475, -1.5658,  0.7436, -1.3447, -0.7018, -0.5384, -0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-30.2791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 86
	action: tensor([[ 0.9129,  1.3661, -0.6841,  1.1462, -0.8224, -0.6513,  1.3869]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 87
	action: tensor([[ 0.0302,  1.6253, -0.4352,  0.8128,  0.5024,  0.3143, -0.4809]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 88
	action: tensor([[ 0.8005, -0.0456, -0.9998,  0.4940,  1.5353, -0.2164, -1.7869]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 89
	action: tensor([[ 0.3266,  0.0374,  2.8552, -1.5098,  0.3260, -1.3547,  0.8418]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 90
	action: tensor([[-1.0932, -1.2340,  0.9760, -0.0501, -0.5077, -0.0977, -1.0672]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4116476190959215, distance: 1.7771071678658388 entropy 1.3432114124298096
epoch: 7, step: 91
	action: tensor([[ 0.5643,  0.9399,  1.3455, -0.4377, -0.8475,  0.9400,  1.1783]],
       dtype=torch.float64)
	q_value: tensor([[-37.7711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 92
	action: tensor([[ 0.1627, -0.5354, -0.2298, -0.1591, -0.1503, -0.8307,  1.1197]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1872279124320424, distance: 1.2468773691551949 entropy 1.3432114124298096
epoch: 7, step: 93
	action: tensor([[ 0.8607,  0.3747,  0.2875,  1.7316, -0.2849,  0.0188,  0.7977]],
       dtype=torch.float64)
	q_value: tensor([[-27.3584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6101858077794963, distance: 0.7144724985919949 entropy 1.3432114124298096
epoch: 7, step: 94
	action: tensor([[-0.0664,  0.3177,  1.9230,  1.9311,  0.7688, -0.4313, -0.4246]],
       dtype=torch.float64)
	q_value: tensor([[-33.2226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7723775789842988, distance: 0.5459642582472799 entropy 1.3432114124298096
epoch: 7, step: 95
	action: tensor([[ 0.7021, -0.5032,  0.8150,  1.5918, -0.0473, -0.2515, -0.3711]],
       dtype=torch.float64)
	q_value: tensor([[-45.1198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8145210668479661, distance: 0.4928377746330398 entropy 1.3432114124298096
epoch: 7, step: 96
	action: tensor([[-0.8778, -1.4375,  0.5491,  0.5597, -1.9998, -0.2973,  0.4788]],
       dtype=torch.float64)
	q_value: tensor([[-36.9093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 97
	action: tensor([[-1.0808,  0.3431,  0.7954,  0.8562,  1.7413,  0.7525,  0.4436]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03083916093070893, distance: 1.1265607650392933 entropy 1.3432114124298096
epoch: 7, step: 98
	action: tensor([[0.7212, 0.9793, 0.2985, 0.0456, 0.9440, 1.5809, 0.7081]],
       dtype=torch.float64)
	q_value: tensor([[-39.5074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 99
	action: tensor([[ 1.2629, -0.6150,  0.3329, -1.3292,  0.8668,  0.8142, -0.1200]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4431892967472466, distance: 1.3747329526493 entropy 1.3432114124298096
epoch: 7, step: 100
	action: tensor([[ 0.9929,  0.1005,  1.4634, -0.6939, -2.1416, -0.5587,  0.3597]],
       dtype=torch.float64)
	q_value: tensor([[-35.6849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.687929014113305, distance: 0.6392686246523275 entropy 1.3432114124298096
epoch: 7, step: 101
	action: tensor([[-0.1120, -2.3711,  0.5509,  0.0908,  0.1481,  0.1522,  0.8817]],
       dtype=torch.float64)
	q_value: tensor([[-40.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 102
	action: tensor([[-0.0235, -0.7196,  0.3457,  0.6576,  0.8155,  1.6052,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6301304116350432, distance: 0.6959547531119042 entropy 1.3432114124298096
epoch: 7, step: 103
	action: tensor([[-0.2192, -2.6121,  1.5954, -0.6930, -0.5171,  1.3077,  0.9818]],
       dtype=torch.float64)
	q_value: tensor([[-37.7886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 104
	action: tensor([[-0.0186, -0.0207,  1.6085,  1.2873,  0.6512, -1.2362, -0.4195]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7785898666524589, distance: 0.5384624712137692 entropy 1.3432114124298096
epoch: 7, step: 105
	action: tensor([[ 0.9316, -0.0191,  1.4186,  0.2525,  0.3058,  0.3105,  1.6055]],
       dtype=torch.float64)
	q_value: tensor([[-40.6274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6931534868771903, distance: 0.6338949464904778 entropy 1.3432114124298096
epoch: 7, step: 106
	action: tensor([[ 0.1278, -1.3415,  0.7649, -0.8002,  0.2330,  0.6528,  1.4722]],
       dtype=torch.float64)
	q_value: tensor([[-35.8420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9620152348406656, distance: 1.6029053591489595 entropy 1.3432114124298096
epoch: 7, step: 107
	action: tensor([[ 0.1063, -1.1549,  1.6821, -1.8032,  0.9026, -1.4566,  1.2032]],
       dtype=torch.float64)
	q_value: tensor([[-35.3157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2613078479701525, distance: 0.9835318668064845 entropy 1.3432114124298096
epoch: 7, step: 108
	action: tensor([[ 0.3168, -0.0080,  0.8831, -0.4699,  1.0510,  1.3391, -1.8475]],
       dtype=torch.float64)
	q_value: tensor([[-41.0239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48162702821460823, distance: 0.8239063837235812 entropy 1.3432114124298096
epoch: 7, step: 109
	action: tensor([[-0.6009,  0.2722,  0.6363,  0.9587, -0.1779, -0.9903, -0.5662]],
       dtype=torch.float64)
	q_value: tensor([[-40.2518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 110
	action: tensor([[-0.5106, -0.3755,  0.5987, -1.2899,  1.9553,  0.2692,  0.6447]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.953232296224809, distance: 1.5993136413743039 entropy 1.3432114124298096
epoch: 7, step: 111
	action: tensor([[-0.1261,  0.3940,  2.4233, -0.9522, -0.0893,  0.7230,  0.1662]],
       dtype=torch.float64)
	q_value: tensor([[-38.2286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 112
	action: tensor([[-1.3096,  0.6465, -0.5579,  1.5902,  1.1439,  0.2726,  0.5186]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 113
	action: tensor([[ 0.3319,  0.3033,  0.4947,  1.5744,  0.4302, -0.0509, -0.3939]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 114
	action: tensor([[ 0.6834,  0.6697, -0.0649, -0.1478, -1.7807, -0.0879, -0.4164]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 115
	action: tensor([[ 0.4580, -0.9288, -1.1201, -0.0066,  0.2237, -1.5634, -1.2854]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14994179745489178, distance: 1.2271414424571636 entropy 1.3432114124298096
epoch: 7, step: 116
	action: tensor([[ 0.0338, -1.4937, -0.1921, -0.1551, -0.1934,  1.9835, -0.8726]],
       dtype=torch.float64)
	q_value: tensor([[-37.0951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 117
	action: tensor([[ 0.3983, -0.3353, -1.0413, -0.4976, -0.0764, -0.5672,  1.5742]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30433797463716084, distance: 0.9544558445814761 entropy 1.3432114124298096
epoch: 7, step: 118
	action: tensor([[-1.2315,  0.7749,  0.0980, -0.2270,  0.0757,  1.3400, -0.2255]],
       dtype=torch.float64)
	q_value: tensor([[-31.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04089287592624413, distance: 1.167507586604854 entropy 1.3432114124298096
epoch: 7, step: 119
	action: tensor([[ 0.0478, -0.2819, -2.4471, -1.1137, -1.4330,  0.2594, -0.8289]],
       dtype=torch.float64)
	q_value: tensor([[-32.7073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8359046220287741, distance: 0.463558840141481 entropy 1.3432114124298096
epoch: 7, step: 120
	action: tensor([[-1.0157, -1.5997,  0.2453, -1.9311,  0.8081,  1.1522,  0.9345]],
       dtype=torch.float64)
	q_value: tensor([[-39.2907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 121
	action: tensor([[-0.2327, -0.7787,  2.2189,  0.7505, -0.4153, -1.4553,  1.7047]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4615266318512883, distance: 1.3834391447864816 entropy 1.3432114124298096
epoch: 7, step: 122
	action: tensor([[-0.1679,  0.5586,  0.6399,  0.0648, -0.6890, -0.1291,  0.9672]],
       dtype=torch.float64)
	q_value: tensor([[-42.1171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 123
	action: tensor([[ 0.2653, -0.3222,  0.3867, -1.6620,  0.0285,  0.4259,  0.1740]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47339156098516044, distance: 1.3890432945295303 entropy 1.3432114124298096
epoch: 7, step: 124
	action: tensor([[ 1.6581, -0.7515,  1.7435,  0.8252,  0.6254,  1.6280, -0.3032]],
       dtype=torch.float64)
	q_value: tensor([[-30.7658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40657851942046386, distance: 1.357183851912255 entropy 1.3432114124298096
epoch: 7, step: 125
	action: tensor([[ 2.5393, -0.5851,  0.9289, -2.2616, -0.4039, -1.8138,  0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-46.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 7, step: 126
	action: tensor([[-0.3197,  0.9271,  1.2556, -1.3016, -0.9387, -1.0274,  0.1204]],
       dtype=torch.float64)
	q_value: tensor([[-32.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2188818630199263, distance: 1.2633901909043201 entropy 1.3432114124298096
epoch: 7, step: 127
	action: tensor([[ 0.0854,  1.0960,  1.2897,  0.3946, -0.4936, -0.4578, -0.8665]],
       dtype=torch.float64)
	q_value: tensor([[-32.3116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
LOSS epoch 7 actor 302.48459300637114 critic 233.8260597336064 
epoch: 8, step: 0
	action: tensor([[ 0.1908, -1.9842, -0.5423, -0.9074, -0.8551,  0.0916, -0.7289]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 1
	action: tensor([[ 1.2983, -0.7216,  0.3559,  0.5715, -0.6632,  0.1454,  2.4266]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30239503805538437, distance: 0.9557877802610831 entropy 1.3432114124298096
epoch: 8, step: 2
	action: tensor([[ 1.7558, -1.6174,  2.0079, -0.4419, -1.3446,  0.8142, -0.4215]],
       dtype=torch.float64)
	q_value: tensor([[-42.5831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 3
	action: tensor([[-0.4681, -1.3076, -0.5881,  0.1885, -0.8523,  0.7396,  0.3306]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1376253321332386, distance: 1.6731022644587805 entropy 1.3432114124298096
epoch: 8, step: 4
	action: tensor([[ 0.7162,  0.0719,  1.6689, -0.3023,  1.4816,  0.3018,  1.0235]],
       dtype=torch.float64)
	q_value: tensor([[-32.9313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6802098941625528, distance: 0.6471265297735096 entropy 1.3432114124298096
epoch: 8, step: 5
	action: tensor([[-0.0518, -1.9030,  2.4254, -1.2436,  0.4500, -0.3364,  1.3434]],
       dtype=torch.float64)
	q_value: tensor([[-37.6536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 6
	action: tensor([[ 0.3345,  0.0500, -0.4913, -0.7073,  0.4988,  1.1076, -0.8680]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 7
	action: tensor([[ 0.4997, -0.3809,  0.1486,  1.5173,  0.6341,  0.5582,  0.8026]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 8
	action: tensor([[-0.0794,  1.1073, -0.9807,  0.3298,  0.0191, -0.1640,  1.5309]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 9
	action: tensor([[ 0.6302, -0.4667, -0.9172,  2.0843, -0.9986, -0.5880, -0.5382]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 10
	action: tensor([[ 1.7696, -0.6496, -1.9651,  0.5713,  0.8721, -0.3508, -1.6991]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 11
	action: tensor([[ 0.2289,  0.8545,  1.4064,  0.4754,  1.1205, -1.3459, -1.7494]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 12
	action: tensor([[ 0.8673,  1.3924, -1.3393,  0.2587,  1.9273,  0.7078,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 13
	action: tensor([[-1.0273,  0.5480,  0.3357, -0.5090, -0.9355, -0.1581,  1.6502]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8585907361575915, distance: 1.5600861356959301 entropy 1.3432114124298096
epoch: 8, step: 14
	action: tensor([[ 0.7885, -2.4438,  0.2895,  0.5893,  1.1351, -0.1239,  1.2332]],
       dtype=torch.float64)
	q_value: tensor([[-33.5772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 15
	action: tensor([[-0.1395,  1.1221,  0.5817, -0.3442,  0.1375, -0.7170, -0.1879]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 16
	action: tensor([[-0.2552, -1.6239, -0.6503,  0.8614,  0.3777, -0.4103,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8664877706861509, distance: 1.5633969762128397 entropy 1.3432114124298096
epoch: 8, step: 17
	action: tensor([[-1.1378, -0.7695,  1.4247,  1.2899,  0.3625,  0.3596,  0.2404]],
       dtype=torch.float64)
	q_value: tensor([[-31.5456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11451204272835735, distance: 1.0768321733531017 entropy 1.3432114124298096
epoch: 8, step: 18
	action: tensor([[ 0.1624,  0.6329, -0.1823,  1.5318,  2.0061, -0.1366,  0.2823]],
       dtype=torch.float64)
	q_value: tensor([[-36.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 19
	action: tensor([[ 0.0395,  1.1021,  0.0204, -0.6950, -0.4423,  1.2117, -0.3667]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 20
	action: tensor([[-0.1038,  1.6158, -1.2499, -0.4517,  0.0874,  1.2362, -2.2778]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 21
	action: tensor([[ 0.1831, -2.3261, -0.5634,  0.1212,  0.2060,  1.0495, -0.6204]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 22
	action: tensor([[-0.1745,  1.7840,  0.1444, -0.3318,  1.7937,  0.1572, -0.6746]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 23
	action: tensor([[ 0.0241,  1.4620,  0.1162, -0.3590, -0.6590,  0.6464,  0.3754]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 24
	action: tensor([[-1.4939,  0.3101,  0.6501,  0.6938,  0.0405,  0.3272,  0.5507]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33467132234181185, distance: 1.3220377543637882 entropy 1.3432114124298096
epoch: 8, step: 25
	action: tensor([[ 0.1296,  0.7032,  0.5114, -0.4502,  0.1473, -0.7644,  1.4637]],
       dtype=torch.float64)
	q_value: tensor([[-30.5266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 26
	action: tensor([[-1.1392,  0.0048, -0.9519,  0.2626,  0.3854,  1.4734, -0.6829]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6393702332665772, distance: 1.4651942843933699 entropy 1.3432114124298096
epoch: 8, step: 27
	action: tensor([[ 0.6972,  0.9765, -0.4204,  0.0558, -1.1527, -1.0363,  1.5221]],
       dtype=torch.float64)
	q_value: tensor([[-33.1154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 28
	action: tensor([[ 0.8024,  0.5195,  0.7560, -0.0796,  0.9487,  0.6615,  1.6666]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 29
	action: tensor([[-0.3205,  0.6922, -0.3302, -0.3011,  0.3902,  0.4606,  1.1404]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 30
	action: tensor([[-1.0834, -0.0114,  1.1434, -0.8362, -1.0398, -1.1219,  1.8830]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.116555087853278, distance: 1.6648360885017326 entropy 1.3432114124298096
epoch: 8, step: 31
	action: tensor([[-0.7651, -1.9258, -0.8704, -0.8103, -0.6387,  0.3108,  0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-37.0611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 32
	action: tensor([[ 0.9931, -0.0356, -0.3182,  0.3304,  0.2861, -0.7545, -1.1668]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6957825982950776, distance: 0.6311734462777184 entropy 1.3432114124298096
epoch: 8, step: 33
	action: tensor([[ 0.4551, -0.2626,  0.4092,  0.0399,  0.2311, -1.0359,  0.4930]],
       dtype=torch.float64)
	q_value: tensor([[-28.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21534147702922635, distance: 1.0136710454916142 entropy 1.3432114124298096
epoch: 8, step: 34
	action: tensor([[-1.3145, -0.4706,  0.5242,  0.1479, -1.0073, -0.4535,  0.9738]],
       dtype=torch.float64)
	q_value: tensor([[-25.9224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.353259912884003, distance: 1.75546284404506 entropy 1.3432114124298096
epoch: 8, step: 35
	action: tensor([[ 0.9047, -0.8354,  0.3926,  0.3541,  0.1493,  0.3120, -2.0427]],
       dtype=torch.float64)
	q_value: tensor([[-31.8421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1874974183559598, distance: 1.0314996098885734 entropy 1.3432114124298096
epoch: 8, step: 36
	action: tensor([[ 0.3071,  0.7276, -0.1339,  0.8260,  0.9422,  1.0880,  0.8521]],
       dtype=torch.float64)
	q_value: tensor([[-39.5379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 37
	action: tensor([[ 0.3546, -0.2036,  1.2868,  2.0657,  0.7647, -0.5863,  0.8866]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 38
	action: tensor([[ 0.7864, -1.0129, -1.0011, -0.5853, -1.8644,  0.0805, -1.3470]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5552289697318904, distance: 1.4270982118419815 entropy 1.3432114124298096
epoch: 8, step: 39
	action: tensor([[-0.1822, -0.8415, -0.7623,  0.2774, -0.1632, -0.0349,  1.1734]],
       dtype=torch.float64)
	q_value: tensor([[-39.6278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6138814777944395, distance: 1.4537593197700003 entropy 1.3432114124298096
epoch: 8, step: 40
	action: tensor([[ 1.1992, -2.0892, -0.1795, -0.4762, -0.1815,  0.6290,  1.0053]],
       dtype=torch.float64)
	q_value: tensor([[-29.8441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 41
	action: tensor([[-0.6356,  1.6751,  0.1599,  0.3288, -0.3828,  1.9782, -0.4519]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 42
	action: tensor([[ 1.5864,  0.5757, -1.0980, -0.8801,  1.5868,  0.2864,  0.4144]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19342471568203568, distance: 1.02773027028013 entropy 1.3432114124298096
epoch: 8, step: 43
	action: tensor([[ 0.3925, -0.9849,  1.8585, -0.3514,  1.8009, -0.5201,  0.2719]],
       dtype=torch.float64)
	q_value: tensor([[-37.5630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20632333381489465, distance: 1.256864782680358 entropy 1.3432114124298096
epoch: 8, step: 44
	action: tensor([[ 0.4263,  0.8433, -0.1733, -0.2315,  0.7155, -0.6872, -0.2801]],
       dtype=torch.float64)
	q_value: tensor([[-39.8260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 45
	action: tensor([[-1.1402,  0.2851,  0.0732, -0.3488,  1.4971,  0.1293,  0.2167]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0844762068403524, distance: 1.6521716449337156 entropy 1.3432114124298096
epoch: 8, step: 46
	action: tensor([[ 1.7685, -0.0550, -1.4260, -0.0284,  0.1482,  0.7907,  0.3192]],
       dtype=torch.float64)
	q_value: tensor([[-31.8440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5850508765281531, distance: 0.7371470282223436 entropy 1.3432114124298096
epoch: 8, step: 47
	action: tensor([[ 0.7089, -2.2149,  0.3308, -0.8448,  1.3243,  0.5552,  0.2600]],
       dtype=torch.float64)
	q_value: tensor([[-33.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 48
	action: tensor([[-1.5567,  0.6129, -0.9009,  0.2595, -1.5672,  1.6680,  0.2559]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9226297755497146, distance: 1.5867354521750299 entropy 1.3432114124298096
epoch: 8, step: 49
	action: tensor([[ 1.1593, -0.5140,  0.0830, -0.9841, -0.4134,  1.5069,  0.5353]],
       dtype=torch.float64)
	q_value: tensor([[-39.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2685993480634622, distance: 0.9786656938508562 entropy 1.3432114124298096
epoch: 8, step: 50
	action: tensor([[ 1.3432, -2.4826,  1.2209, -0.4501, -2.2828, -0.3603,  1.0277]],
       dtype=torch.float64)
	q_value: tensor([[-37.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 51
	action: tensor([[-0.3461, -0.7922, -0.1981,  0.4587, -0.2412, -1.7497, -2.6652]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3016542055252516, distance: 1.3055830592575082 entropy 1.3432114124298096
epoch: 8, step: 52
	action: tensor([[-0.5089, -0.3974, -0.4444, -0.6840, -1.3868, -0.4241, -0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-48.2117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33876336120440875, distance: 1.324062855807226 entropy 1.3432114124298096
epoch: 8, step: 53
	action: tensor([[ 0.2649,  0.9075,  1.6723,  0.9386, -0.7508, -1.3568,  0.7018]],
       dtype=torch.float64)
	q_value: tensor([[-30.0401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 54
	action: tensor([[ 0.5673, -0.5895,  0.6194,  0.1561,  0.9706,  0.7824,  0.7530]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36212487235868795, distance: 0.9139543987523401 entropy 1.3432114124298096
epoch: 8, step: 55
	action: tensor([[ 0.7710, -1.1935,  0.8884, -0.5273,  1.0050, -0.2811,  0.5959]],
       dtype=torch.float64)
	q_value: tensor([[-32.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7188881281114452, distance: 1.5003082565229513 entropy 1.3432114124298096
epoch: 8, step: 56
	action: tensor([[ 0.8969, -0.6745, -1.1594, -0.4750,  1.3189, -0.6075, -1.2352]],
       dtype=torch.float64)
	q_value: tensor([[-32.6266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26672038392898356, distance: 1.2879442747280851 entropy 1.3432114124298096
epoch: 8, step: 57
	action: tensor([[-0.8086,  0.2873,  1.6633,  0.5073, -0.6760, -1.0315, -0.0528]],
       dtype=torch.float64)
	q_value: tensor([[-37.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3985341858276561, distance: 1.3532973663701626 entropy 1.3432114124298096
epoch: 8, step: 58
	action: tensor([[ 1.0794,  1.3946, -1.2464, -0.8356,  0.3109, -0.8636,  2.8490]],
       dtype=torch.float64)
	q_value: tensor([[-34.9641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 59
	action: tensor([[ 1.6693, -1.3993, -0.7746, -0.1085,  1.6399, -0.5029,  0.1135]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8339133621124173, distance: 1.5496945321115025 entropy 1.3432114124298096
epoch: 8, step: 60
	action: tensor([[ 0.2745, -0.4009,  0.0416, -0.8236, -0.8069,  0.1012, -1.0432]],
       dtype=torch.float64)
	q_value: tensor([[-41.2610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3013163001411112, distance: 1.30541358560691 entropy 1.3432114124298096
epoch: 8, step: 61
	action: tensor([[-1.6574,  0.4835,  1.1425, -1.7893,  0.5895,  1.8667, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-28.3253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9186834225611615, distance: 1.585106164311783 entropy 1.3432114124298096
epoch: 8, step: 62
	action: tensor([[ 0.6472, -1.8500, -1.2065,  0.8651,  0.0885,  1.0463, -0.2369]],
       dtype=torch.float64)
	q_value: tensor([[-47.3580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 63
	action: tensor([[-0.9997, -0.1258, -0.5233, -0.1983, -0.3549, -0.7371,  0.7192]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.728825792243877, distance: 1.5046389839450387 entropy 1.3432114124298096
epoch: 8, step: 64
	action: tensor([[-2.6004, -0.5062,  0.6347, -0.0360, -0.9258,  0.9093,  0.6998]],
       dtype=torch.float64)
	q_value: tensor([[-26.7474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 65
	action: tensor([[-0.4323, -0.2299,  0.1466,  1.0817, -0.0827,  0.3874,  0.4651]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 66
	action: tensor([[ 0.8929, -0.2611,  0.5936,  1.3121, -0.1262,  1.0030,  0.4177]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 67
	action: tensor([[ 0.4347,  0.1173, -0.7853, -1.2103, -1.6022, -1.3209,  0.6234]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 68
	action: tensor([[-1.5766, -1.8636, -0.2822,  0.4727, -0.0941,  0.0232,  0.9563]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 69
	action: tensor([[ 0.7223,  0.4165,  0.3751, -0.4011,  1.7752, -0.7824,  0.8194]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 70
	action: tensor([[-1.4952,  1.0160,  2.3787,  0.5332,  0.2808, -1.5172,  0.4819]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 71
	action: tensor([[ 0.1394, -1.1142, -0.5984, -0.6946, -0.4721,  1.2436,  0.5858]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6260217121793601, distance: 1.4592169425204555 entropy 1.3432114124298096
epoch: 8, step: 72
	action: tensor([[ 0.1454, -2.1687,  1.7499, -0.2294, -0.9545,  1.1104,  0.2380]],
       dtype=torch.float64)
	q_value: tensor([[-35.1810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 73
	action: tensor([[-0.0394, -1.9821,  0.9609,  1.4053,  0.1676,  0.8803, -0.8488]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 74
	action: tensor([[ 0.7247,  0.1554, -0.2889, -0.1213, -0.2493,  0.3746, -1.5100]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 75
	action: tensor([[ 0.9082,  1.3532, -0.8180,  2.4062, -0.1382,  0.2543,  0.1973]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 76
	action: tensor([[ 0.9793, -0.3130, -0.2537,  0.6292, -0.0856, -0.6252,  0.7895]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6810659792264043, distance: 0.6462597634040247 entropy 1.3432114124298096
epoch: 8, step: 77
	action: tensor([[ 0.1109, -1.0569, -0.1014, -0.8824,  0.4204, -1.1716,  0.3090]],
       dtype=torch.float64)
	q_value: tensor([[-28.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6812599397991035, distance: 1.4837957600395417 entropy 1.3432114124298096
epoch: 8, step: 78
	action: tensor([[ 1.5989,  0.0601, -1.0758, -0.3454, -0.0399,  0.5820,  0.4259]],
       dtype=torch.float64)
	q_value: tensor([[-31.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3894506405386641, distance: 0.8941637993053204 entropy 1.3432114124298096
epoch: 8, step: 79
	action: tensor([[ 2.3021, -1.2415,  1.1318, -0.0973, -1.1567,  0.5835,  0.5177]],
       dtype=torch.float64)
	q_value: tensor([[-31.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 80
	action: tensor([[ 0.7230, -0.4696, -0.3779,  0.8533,  0.5513,  0.3326, -0.1918]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8297530563278557, distance: 0.47216778775305446 entropy 1.3432114124298096
epoch: 8, step: 81
	action: tensor([[ 0.2102, -0.8862,  0.4643,  0.3248, -0.2714, -0.6557,  0.3039]],
       dtype=torch.float64)
	q_value: tensor([[-28.1051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32334367230574346, distance: 1.3164155867217386 entropy 1.3432114124298096
epoch: 8, step: 82
	action: tensor([[-0.2044, -0.3502,  0.1365, -1.1071, -1.2590, -0.8187,  0.4107]],
       dtype=torch.float64)
	q_value: tensor([[-27.2145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32627058280456556, distance: 1.317870576285728 entropy 1.3432114124298096
epoch: 8, step: 83
	action: tensor([[-0.7694,  0.4730,  1.8300, -0.9707,  0.9337,  0.5126, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-30.2206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7653547502973919, distance: 1.5204519101018323 entropy 1.3432114124298096
epoch: 8, step: 84
	action: tensor([[ 1.6900, -1.3855,  1.0125, -0.4703, -1.4652, -0.0602,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-36.5545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6612053169617289, distance: 1.474919608715691 entropy 1.3432114124298096
epoch: 8, step: 85
	action: tensor([[-0.5890, -1.3095,  0.7169, -0.2399, -0.5979,  0.1713,  2.2434]],
       dtype=torch.float64)
	q_value: tensor([[-38.8228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 86
	action: tensor([[ 0.8888, -0.0963, -0.6322, -0.1655,  0.4915,  0.9336,  0.3481]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 87
	action: tensor([[ 0.6076, -1.3142,  0.1577, -1.0833,  1.8441, -0.0581,  0.5840]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0515186365411942, distance: 1.63905839510372 entropy 1.3432114124298096
epoch: 8, step: 88
	action: tensor([[ 0.2944, -1.8215,  0.5200, -0.9024,  0.8726,  1.4561,  2.5318]],
       dtype=torch.float64)
	q_value: tensor([[-39.6820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 89
	action: tensor([[-0.9817,  0.8222,  0.8079, -0.3385, -0.3923,  0.3462,  0.5928]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40867768780731306, distance: 1.35819620040504 entropy 1.3432114124298096
epoch: 8, step: 90
	action: tensor([[ 0.5165, -1.7171,  0.7571, -1.5179, -1.2746,  0.3457,  1.4571]],
       dtype=torch.float64)
	q_value: tensor([[-29.3232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7715679494557468, distance: 1.5231251895127502 entropy 1.3432114124298096
epoch: 8, step: 91
	action: tensor([[ 1.0515, -1.8447,  0.4772, -0.0613, -1.0248,  0.8059, -0.0293]],
       dtype=torch.float64)
	q_value: tensor([[-40.6324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 92
	action: tensor([[-0.1663,  0.7335, -0.6512,  0.8420, -1.1459,  0.4135,  0.2125]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 93
	action: tensor([[-1.9466,  2.7043,  0.4724, -1.8335,  0.1657,  0.7610, -0.4546]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 94
	action: tensor([[ 0.6928,  1.0004,  0.9647, -0.7651, -1.2744, -0.1361, -1.0705]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 95
	action: tensor([[-0.7332, -0.7943, -1.1159, -0.6295,  0.0748, -1.8414,  1.4131]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1449520853683094, distance: 1.058161410239911 entropy 1.3432114124298096
epoch: 8, step: 96
	action: tensor([[ 0.7477, -1.9824,  1.5007, -1.3366,  0.4187, -0.2965,  1.6872]],
       dtype=torch.float64)
	q_value: tensor([[-37.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 97
	action: tensor([[ 0.5827,  0.7114,  0.0134,  0.5273,  1.7385,  1.7139, -0.7667]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 98
	action: tensor([[ 2.5716,  0.1065,  0.8682, -0.2322,  1.0624,  2.5504, -0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 99
	action: tensor([[ 1.4854, -0.9102, -0.2874, -1.3051,  1.4908,  1.2217, -0.2297]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6560430842337392, distance: 1.4726261527291606 entropy 1.3432114124298096
epoch: 8, step: 100
	action: tensor([[ 0.9473, -0.3024,  0.6442, -0.0368,  1.4107,  1.4178,  0.9080]],
       dtype=torch.float64)
	q_value: tensor([[-43.9098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3448209390023602, distance: 0.9262680794846229 entropy 1.3432114124298096
epoch: 8, step: 101
	action: tensor([[ 1.0687, -1.9802,  0.2624, -1.7981,  0.1876,  0.2856, -0.2539]],
       dtype=torch.float64)
	q_value: tensor([[-38.7512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 102
	action: tensor([[ 0.5434,  0.0211, -0.2664, -0.8127, -0.1505,  0.9837,  0.9501]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5903318052592427, distance: 0.7324412874130005 entropy 1.3432114124298096
epoch: 8, step: 103
	action: tensor([[ 0.7112, -2.5114,  1.3248, -1.6869,  0.9291, -0.6689,  1.6418]],
       dtype=torch.float64)
	q_value: tensor([[-31.0709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 104
	action: tensor([[-1.8712, -0.1449,  1.4135, -0.0982,  0.7998,  0.3605,  0.7249]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 105
	action: tensor([[-0.5323, -1.0952,  0.4048,  0.9700, -0.2917, -0.2055, -1.4425]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31539796251876906, distance: 1.3124575806917198 entropy 1.3432114124298096
epoch: 8, step: 106
	action: tensor([[0.5588, 0.2809, 0.7223, 0.3350, 0.5077, 0.6566, 0.6180]],
       dtype=torch.float64)
	q_value: tensor([[-38.0606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891819519389619, distance: 0.11902306960803963 entropy 1.3432114124298096
epoch: 8, step: 107
	action: tensor([[-0.6949, -0.3711,  0.3980, -1.2047, -1.9499, -0.2085, -1.1842]],
       dtype=torch.float64)
	q_value: tensor([[-28.9461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0629470683532962, distance: 1.6436174207189331 entropy 1.3432114124298096
epoch: 8, step: 108
	action: tensor([[ 1.3917, -0.4619,  0.8261, -0.5046, -0.8217,  1.4398,  0.2505]],
       dtype=torch.float64)
	q_value: tensor([[-40.0084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5666892646795119, distance: 0.7532799668243367 entropy 1.3432114124298096
epoch: 8, step: 109
	action: tensor([[ 1.0110, -2.0845,  2.6738, -0.2882, -1.1770, -1.0395, -1.0050]],
       dtype=torch.float64)
	q_value: tensor([[-37.0753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 110
	action: tensor([[-1.3870,  0.0359,  0.5662, -0.3378,  0.1644,  0.4433, -0.4293]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.434995041163952, distance: 1.785688631952747 entropy 1.3432114124298096
epoch: 8, step: 111
	action: tensor([[-0.1484, -1.2451, -0.1112,  1.1170,  0.7114,  1.3988, -1.0884]],
       dtype=torch.float64)
	q_value: tensor([[-29.7948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42708756570915407, distance: 0.8661653720768766 entropy 1.3432114124298096
epoch: 8, step: 112
	action: tensor([[-0.3862,  0.2087,  1.3298, -0.1920,  1.0476,  0.7767, -1.7680]],
       dtype=torch.float64)
	q_value: tensor([[-39.6316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01644734618781496, distance: 1.134894524107463 entropy 1.3432114124298096
epoch: 8, step: 113
	action: tensor([[ 0.7467, -0.4359,  0.6288, -1.5633, -1.6953,  1.0344,  0.0800]],
       dtype=torch.float64)
	q_value: tensor([[-39.5855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023161386180627686, distance: 1.1310143172721652 entropy 1.3432114124298096
epoch: 8, step: 114
	action: tensor([[-0.7219, -1.0703,  1.5425, -0.4370, -0.3608, -1.2702, -0.4840]],
       dtype=torch.float64)
	q_value: tensor([[-39.2661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1490207003373776, distance: 1.677555868782697 entropy 1.3432114124298096
epoch: 8, step: 115
	action: tensor([[ 1.8828, -1.2163,  0.3000,  0.6240,  1.2178, -0.1115, -1.1871]],
       dtype=torch.float64)
	q_value: tensor([[-36.5201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39357129268333435, distance: 1.3508940503517315 entropy 1.3432114124298096
epoch: 8, step: 116
	action: tensor([[ 0.9835,  1.5245,  1.3972, -0.4508,  0.1054, -0.1337,  0.5130]],
       dtype=torch.float64)
	q_value: tensor([[-42.5434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 117
	action: tensor([[ 0.7630,  0.5661,  0.2446, -0.2522, -1.1043,  0.3977,  0.3702]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 118
	action: tensor([[-1.0352, -0.4915, -1.0473,  0.5586,  2.4425,  1.0166,  0.9810]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9378392618424305, distance: 1.5929992401266824 entropy 1.3432114124298096
epoch: 8, step: 119
	action: tensor([[-0.8180,  0.4454, -0.6552, -1.5667, -0.4818,  1.1102,  1.0950]],
       dtype=torch.float64)
	q_value: tensor([[-45.2417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24930829237993568, distance: 1.279061729162047 entropy 1.3432114124298096
epoch: 8, step: 120
	action: tensor([[ 1.3688, -0.7998,  1.6956, -0.8042,  0.2574, -1.3320,  1.4508]],
       dtype=torch.float64)
	q_value: tensor([[-38.4097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5799827667572433, distance: 0.7416350527310827 entropy 1.3432114124298096
epoch: 8, step: 121
	action: tensor([[ 0.8835, -1.5285, -0.3997, -0.6329, -0.0879,  2.2608,  0.7168]],
       dtype=torch.float64)
	q_value: tensor([[-39.1129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 122
	action: tensor([[ 0.7018, -1.6139, -0.5312,  0.0282,  1.0414,  0.2926,  1.3684]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46524666673810944, distance: 1.3851986649731 entropy 1.3432114124298096
epoch: 8, step: 123
	action: tensor([[ 0.2792, -0.2700,  0.1230, -1.7127, -0.7650,  0.1830,  2.5730]],
       dtype=torch.float64)
	q_value: tensor([[-37.2936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4187399646360839, distance: 1.3630384108670655 entropy 1.3432114124298096
epoch: 8, step: 124
	action: tensor([[ 1.0189, -0.7195,  1.4038, -2.5812,  1.3788, -0.4138,  1.9189]],
       dtype=torch.float64)
	q_value: tensor([[-43.1729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 125
	action: tensor([[ 0.5053,  0.4484, -0.7468,  0.2262,  0.6982,  1.5674,  0.4187]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 8, step: 126
	action: tensor([[ 1.2944, -0.5039,  0.0980, -0.2671,  0.0549, -1.1396,  0.0732]],
       dtype=torch.float64)
	q_value: tensor([[-33.2549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.172730110609425, distance: 1.239240879566118 entropy 1.3432114124298096
epoch: 8, step: 127
	action: tensor([[ 0.7027, -0.5659, -0.1074, -0.3365, -0.3757, -0.8256,  0.9526]],
       dtype=torch.float64)
	q_value: tensor([[-29.3249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18640380160947578, distance: 1.2464445358694243 entropy 1.3432114124298096
LOSS epoch 8 actor 284.6238877322165 critic 228.42239586855283 
epoch: 9, step: 0
	action: tensor([[-0.4512,  1.5206,  0.5114, -0.0131,  1.5660,  0.2546,  0.6324]],
       dtype=torch.float64)
	q_value: tensor([[-31.8804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 1
	action: tensor([[ 0.2295,  0.7368,  1.1942, -1.2023, -1.1178,  0.9222,  0.0995]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 2
	action: tensor([[ 0.5632, -1.4395, -0.3393, -0.5358,  1.0770, -0.1097,  0.3243]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9405838282315291, distance: 1.5941269253022543 entropy 1.3432114124298096
epoch: 9, step: 3
	action: tensor([[ 0.5819, -0.8354,  0.5940,  0.7211,  0.4492, -0.8248, -0.6204]],
       dtype=torch.float64)
	q_value: tensor([[-36.5672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18577743481482745, distance: 1.0325908214458719 entropy 1.3432114124298096
epoch: 9, step: 4
	action: tensor([[ 1.8565,  0.2322,  0.2217,  0.2917, -0.7072,  1.0529, -0.8498]],
       dtype=torch.float64)
	q_value: tensor([[-34.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8024439635697588, distance: 0.5086298516022101 entropy 1.3432114124298096
epoch: 9, step: 5
	action: tensor([[0.6219, 0.2466, 1.1541, 0.5558, 0.1008, 0.2409, 0.8531]],
       dtype=torch.float64)
	q_value: tensor([[-37.6990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9487090238964768, distance: 0.2591654924548272 entropy 1.3432114124298096
epoch: 9, step: 6
	action: tensor([[ 1.7072,  1.1294,  2.5648,  1.5701,  0.6254, -0.7795,  1.6269]],
       dtype=torch.float64)
	q_value: tensor([[-33.8109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.254186039679718, distance: 0.9882616608355288 entropy 1.3432114124298096
epoch: 9, step: 7
	action: tensor([[ 2.1598, -0.5642,  1.0231, -1.9677,  0.1309, -0.9626,  0.7609]],
       dtype=torch.float64)
	q_value: tensor([[-57.5219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 8
	action: tensor([[ 0.0871, -0.4856,  0.1369, -1.1479, -0.0293,  0.0960,  0.6096]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5812394697340075, distance: 1.4389825135519159 entropy 1.3432114124298096
epoch: 9, step: 9
	action: tensor([[-0.7437,  0.9610, -0.0219, -0.2728,  0.7361, -1.8639,  0.2431]],
       dtype=torch.float64)
	q_value: tensor([[-30.8561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11125175016690814, distance: 1.2063210945455092 entropy 1.3432114124298096
epoch: 9, step: 10
	action: tensor([[ 0.8876,  0.3066, -1.2938, -1.8594, -1.1591,  0.8587, -0.1637]],
       dtype=torch.float64)
	q_value: tensor([[-33.6101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 11
	action: tensor([[-0.0580,  0.8995,  0.9434, -0.2468,  0.5782, -0.3103,  0.4039]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 12
	action: tensor([[-0.5780,  0.5194, -0.1140, -0.5226,  0.8656, -0.4515, -0.1217]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30188522417680663, distance: 1.3056989120911846 entropy 1.3432114124298096
epoch: 9, step: 13
	action: tensor([[-1.1077, -0.4807, -0.0317,  0.6534, -0.0636,  0.9644, -0.3854]],
       dtype=torch.float64)
	q_value: tensor([[-27.6170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5586259309396338, distance: 1.428655909481194 entropy 1.3432114124298096
epoch: 9, step: 14
	action: tensor([[ 0.1103, -0.7485,  0.4281,  0.6013,  0.8354,  0.0667,  1.7475]],
       dtype=torch.float64)
	q_value: tensor([[-34.7840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2670294787250185, distance: 0.9797154288555807 entropy 1.3432114124298096
epoch: 9, step: 15
	action: tensor([[ 0.4153, -2.0033,  0.2210,  1.1675,  0.3556,  1.3498,  1.2987]],
       dtype=torch.float64)
	q_value: tensor([[-39.1913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 16
	action: tensor([[-0.5752, -0.2572,  0.6045, -0.7276,  0.1132, -1.9091, -0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43450317047124853, distance: 1.3705896549506629 entropy 1.3432114124298096
epoch: 9, step: 17
	action: tensor([[-0.7477, -0.6418, -1.2189, -1.3723,  0.0872,  0.7199, -0.3188]],
       dtype=torch.float64)
	q_value: tensor([[-33.6398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1692185453436259, distance: 1.2373841276646864 entropy 1.3432114124298096
epoch: 9, step: 18
	action: tensor([[ 0.2929, -1.3999, -0.2201,  0.6014,  0.3661,  0.3006,  0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-39.3217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 19
	action: tensor([[-0.5031, -0.7946, -0.2312,  0.2039, -1.2030, -0.9760, -0.2223]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4094839854140331, distance: 1.3585848463300785 entropy 1.3432114124298096
epoch: 9, step: 20
	action: tensor([[ 0.5169,  1.6119,  1.3886,  1.5257, -0.4298, -0.9142,  0.3928]],
       dtype=torch.float64)
	q_value: tensor([[-34.8533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 21
	action: tensor([[-0.8529,  1.9636,  0.3262,  0.0496, -0.1031, -1.2347, -0.2620]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 22
	action: tensor([[-0.0219,  0.2581, -0.5523, -0.8765,  0.0576,  1.0891, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5121695043753416, distance: 0.7992656820873418 entropy 1.3432114124298096
epoch: 9, step: 23
	action: tensor([[ 1.1831, -1.1622,  0.3851, -0.6094,  0.8073,  1.1745,  0.4726]],
       dtype=torch.float64)
	q_value: tensor([[-32.7261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6051256887627805, distance: 1.4498104172413413 entropy 1.3432114124298096
epoch: 9, step: 24
	action: tensor([[-0.9601, -0.7669,  1.6513, -0.2390, -0.3080,  0.4073,  0.9010]],
       dtype=torch.float64)
	q_value: tensor([[-42.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.30165041918983, distance: 1.736106545689742 entropy 1.3432114124298096
epoch: 9, step: 25
	action: tensor([[ 0.2516,  0.6111,  0.7451, -0.7660, -1.4041,  0.1600,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-37.8636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4351773983152797, distance: 0.8600282691159515 entropy 1.3432114124298096
epoch: 9, step: 26
	action: tensor([[ 2.6114,  0.1177, -1.0103,  0.3758,  1.4913,  0.4403, -0.0802]],
       dtype=torch.float64)
	q_value: tensor([[-34.4774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 27
	action: tensor([[-1.2486, -0.3133, -1.4166, -1.5406,  0.0279,  0.1449,  0.7707]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04347438944040938, distance: 1.1191930241169632 entropy 1.3432114124298096
epoch: 9, step: 28
	action: tensor([[ 2.0681, -0.1758,  0.5933, -0.0026,  0.8580,  1.2624, -0.5421]],
       dtype=torch.float64)
	q_value: tensor([[-41.0749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02880823858981052, distance: 1.1277405279627604 entropy 1.3432114124298096
epoch: 9, step: 29
	action: tensor([[-0.2160,  0.9649,  0.1846, -0.5999, -1.4019,  0.5868,  0.5689]],
       dtype=torch.float64)
	q_value: tensor([[-40.7369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 30
	action: tensor([[-0.9284, -2.1133, -1.0325,  0.1976,  0.3869, -0.4733, -0.3023]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 31
	action: tensor([[-1.7983, -0.7698,  1.3995,  1.5658,  0.4127,  0.0219,  1.8293]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 32
	action: tensor([[-0.1186, -0.4227, -0.6499,  0.6984, -1.3895, -0.7227, -0.3245]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20793748112705057, distance: 1.018442289532036 entropy 1.3432114124298096
epoch: 9, step: 33
	action: tensor([[ 0.1942,  0.3779,  0.3250,  0.2294, -1.0051, -0.5552, -0.4640]],
       dtype=torch.float64)
	q_value: tensor([[-35.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 34
	action: tensor([[ 0.1779, -0.2935,  0.5059, -0.5187, -0.3507,  1.6305,  0.3152]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48201674033326847, distance: 0.8235966196245456 entropy 1.3432114124298096
epoch: 9, step: 35
	action: tensor([[ 0.2099, -0.6434,  2.3751,  0.5185,  0.2008, -0.9497, -1.1275]],
       dtype=torch.float64)
	q_value: tensor([[-38.3228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027739487410798236, distance: 1.1283608702381676 entropy 1.3432114124298096
epoch: 9, step: 36
	action: tensor([[-0.2612,  1.6045, -2.0996, -0.2938,  0.5905,  1.5814,  2.0951]],
       dtype=torch.float64)
	q_value: tensor([[-47.9993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 37
	action: tensor([[-0.1235, -0.5429, -0.0110,  0.4714, -0.2625,  1.2877, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28958550678279293, distance: 0.9645230254388566 entropy 1.3432114124298096
epoch: 9, step: 38
	action: tensor([[ 0.5340,  0.1690,  2.3381,  0.4392, -0.1113,  0.6399,  0.4625]],
       dtype=torch.float64)
	q_value: tensor([[-33.6365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.691647530580555, distance: 0.63544857286463 entropy 1.3432114124298096
epoch: 9, step: 39
	action: tensor([[ 1.4817,  1.3670,  1.4424, -1.1572, -1.2684, -0.2372,  1.5070]],
       dtype=torch.float64)
	q_value: tensor([[-42.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 40
	action: tensor([[-0.0920, -1.7274,  1.5343, -0.6165,  1.0568,  1.1273,  0.4236]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8978753548202856, distance: 1.5764875174863155 entropy 1.3432114124298096
epoch: 9, step: 41
	action: tensor([[ 1.0025, -0.8474,  1.2848, -1.7758,  0.0965, -0.5978, -0.5877]],
       dtype=torch.float64)
	q_value: tensor([[-42.6352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18264801382747287, distance: 1.0345732722764212 entropy 1.3432114124298096
epoch: 9, step: 42
	action: tensor([[-1.2950, -1.1965,  1.3341, -1.2698, -1.7308, -0.8834, -0.9267]],
       dtype=torch.float64)
	q_value: tensor([[-39.4476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0214585531936304, distance: 1.627005847819295 entropy 1.3432114124298096
epoch: 9, step: 43
	action: tensor([[-1.3892, -0.8947, -0.7314,  0.5032, -0.2080, -0.6223, -0.4873]],
       dtype=torch.float64)
	q_value: tensor([[-50.2583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.653869593260556, distance: 1.8642170794378607 entropy 1.3432114124298096
epoch: 9, step: 44
	action: tensor([[ 0.9913, -0.3339,  0.1215, -1.3513,  0.3747, -0.3902,  1.1323]],
       dtype=torch.float64)
	q_value: tensor([[-35.9566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2983562629448113, distance: 1.3039280618245506 entropy 1.3432114124298096
epoch: 9, step: 45
	action: tensor([[ 0.3192, -0.2299, -0.6078,  0.9688,  1.6605, -0.3538,  0.6666]],
       dtype=torch.float64)
	q_value: tensor([[-35.3900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42765180099288913, distance: 0.8657387436566599 entropy 1.3432114124298096
epoch: 9, step: 46
	action: tensor([[-0.2510, -0.4794, -1.0873, -2.0944,  0.1482, -1.8685, -0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-36.9122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067320292113256, distance: 0.9528120947386279 entropy 1.3432114124298096
epoch: 9, step: 47
	action: tensor([[-0.1566,  0.9653, -1.4881, -0.7517,  0.4580,  0.8032,  0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-42.5952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 48
	action: tensor([[ 0.6016,  0.9255,  0.0639,  0.7793,  0.0338, -0.4318,  0.4493]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 49
	action: tensor([[-0.3952, -0.8234, -0.5603,  0.2575,  0.5577, -1.7085, -0.7264]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5565288690382764, distance: 1.427694489477725 entropy 1.3432114124298096
epoch: 9, step: 50
	action: tensor([[ 0.2312,  1.7767,  1.4701, -0.1824,  1.3960,  1.0118, -0.1466]],
       dtype=torch.float64)
	q_value: tensor([[-37.4004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 51
	action: tensor([[-0.9429, -1.6117,  0.3441, -0.3391,  1.0879, -0.8798,  1.8227]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3425711712724069, distance: 1.7514715628845432 entropy 1.3432114124298096
epoch: 9, step: 52
	action: tensor([[-0.6869, -0.6505,  0.0828,  0.1953, -0.0847,  0.2260, -0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-40.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7987016933351663, distance: 1.534745133744545 entropy 1.3432114124298096
epoch: 9, step: 53
	action: tensor([[-1.3892,  0.3750,  0.8552, -0.5960, -0.0114,  1.1228,  0.4500]],
       dtype=torch.float64)
	q_value: tensor([[-28.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9767820651130958, distance: 1.6089260721556824 entropy 1.3432114124298096
epoch: 9, step: 54
	action: tensor([[-0.6642, -0.5162,  0.0203, -2.0880,  0.1627, -0.3557,  0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-39.1321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2158457486398493, distance: 1.2618157197146722 entropy 1.3432114124298096
epoch: 9, step: 55
	action: tensor([[ 0.0421, -2.4091,  0.4959, -0.7114, -0.7478, -0.0404, -0.1476]],
       dtype=torch.float64)
	q_value: tensor([[-37.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 56
	action: tensor([[-0.7694, -1.3977,  0.3966, -0.3424, -2.7662, -0.3166,  0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2301508054355312, distance: 1.7089281711385076 entropy 1.3432114124298096
epoch: 9, step: 57
	action: tensor([[ 0.5558, -0.7582, -0.4981, -0.4624,  0.5622, -0.6442,  0.3785]],
       dtype=torch.float64)
	q_value: tensor([[-53.3660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.461688041253556, distance: 1.3835155354302282 entropy 1.3432114124298096
epoch: 9, step: 58
	action: tensor([[ 0.4371,  0.4763,  0.0858, -0.8846, -1.5141,  0.3119,  0.5899]],
       dtype=torch.float64)
	q_value: tensor([[-30.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5691697529944634, distance: 0.7511207958811373 entropy 1.3432114124298096
epoch: 9, step: 59
	action: tensor([[ 1.2917, -0.6202,  0.9537,  0.3296, -0.5556, -0.9000,  0.3069]],
       dtype=torch.float64)
	q_value: tensor([[-34.8767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06484859295555068, distance: 1.1808660151333832 entropy 1.3432114124298096
epoch: 9, step: 60
	action: tensor([[ 0.2666, -2.2135,  1.1983,  1.2194,  0.6007, -0.9896, -0.3252]],
       dtype=torch.float64)
	q_value: tensor([[-36.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 61
	action: tensor([[-0.6013, -1.2310, -0.0618,  0.9512,  0.4994,  0.6740, -1.0403]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3817727176825809, distance: 1.345163269213248 entropy 1.3432114124298096
epoch: 9, step: 62
	action: tensor([[ 1.5661, -0.0436, -0.3442, -1.5027,  0.1780, -1.6326, -0.0909]],
       dtype=torch.float64)
	q_value: tensor([[-39.4690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04682520717621197, distance: 1.117230974218192 entropy 1.3432114124298096
epoch: 9, step: 63
	action: tensor([[ 1.2339, -0.5240,  1.4029, -1.8559,  0.8309, -1.3670, -0.7077]],
       dtype=torch.float64)
	q_value: tensor([[-38.9924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6702532301278585, distance: 0.6571234539845594 entropy 1.3432114124298096
epoch: 9, step: 64
	action: tensor([[ 0.0598, -0.9381, -1.1850,  0.0614,  1.0880,  0.9867,  0.3471]],
       dtype=torch.float64)
	q_value: tensor([[-43.4085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12468831610854103, distance: 1.2135922237949444 entropy 1.3432114124298096
epoch: 9, step: 65
	action: tensor([[ 1.4095, -1.1174, -0.4742,  0.4338, -1.2508, -1.3547,  1.1130]],
       dtype=torch.float64)
	q_value: tensor([[-39.7931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25327352155310345, distance: 1.2810899534651576 entropy 1.3432114124298096
epoch: 9, step: 66
	action: tensor([[-1.3831, -2.0281, -0.6987, -1.5613, -1.6375,  0.6179,  1.1494]],
       dtype=torch.float64)
	q_value: tensor([[-46.0257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 67
	action: tensor([[-0.8682, -0.9119, -0.3854, -0.2698, -0.2404,  1.3678, -0.1977]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0162718233064623, distance: 1.6249171925835921 entropy 1.3432114124298096
epoch: 9, step: 68
	action: tensor([[ 0.7337, -0.9651, -0.0308, -2.5559, -1.4320, -1.1302,  0.6449]],
       dtype=torch.float64)
	q_value: tensor([[-38.4496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06736264551285709, distance: 1.182259175174377 entropy 1.3432114124298096
epoch: 9, step: 69
	action: tensor([[ 0.7247,  0.4995,  0.3433, -3.3087,  0.6741, -0.8679, -1.3180]],
       dtype=torch.float64)
	q_value: tensor([[-45.9582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 70
	action: tensor([[ 0.4171, -0.1000, -0.0978, -0.3181, -0.7575, -0.7630, -0.2145]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39292789977010545, distance: 0.8916138998267797 entropy 1.3432114124298096
epoch: 9, step: 71
	action: tensor([[ 1.8344, -0.5530,  1.0494, -0.1946, -0.7814,  1.4115,  0.4332]],
       dtype=torch.float64)
	q_value: tensor([[-26.3651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3443436912666843, distance: 0.92660537572743 entropy 1.3432114124298096
epoch: 9, step: 72
	action: tensor([[ 1.8957, -1.6674,  0.1551, -0.0480, -0.0331, -0.8551,  1.9189]],
       dtype=torch.float64)
	q_value: tensor([[-42.9246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 73
	action: tensor([[ 0.5030, -0.9509, -1.0578,  0.6091,  0.6660,  2.4665,  0.1862]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49798558443462815, distance: 0.8108019554122142 entropy 1.3432114124298096
epoch: 9, step: 74
	action: tensor([[ 1.7283, -2.1396,  1.3018, -0.0108, -1.2416, -1.0899,  2.6978]],
       dtype=torch.float64)
	q_value: tensor([[-46.3788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 75
	action: tensor([[ 0.5346, -1.1629, -1.4108,  0.0378, -0.4680,  0.4362, -1.1729]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6423623449513223, distance: 1.46653078126497 entropy 1.3432114124298096
epoch: 9, step: 76
	action: tensor([[-0.2121, -1.0660, -0.3984, -0.6505,  0.5267,  0.3198, -0.6746]],
       dtype=torch.float64)
	q_value: tensor([[-38.8721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7014845189315453, distance: 1.4926936807855555 entropy 1.3432114124298096
epoch: 9, step: 77
	action: tensor([[-1.8338,  0.4991,  2.0903, -2.3817, -0.1251,  0.7194, -0.1852]],
       dtype=torch.float64)
	q_value: tensor([[-34.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2051328753222803, distance: 1.256244461976604 entropy 1.3432114124298096
epoch: 9, step: 78
	action: tensor([[-0.1065, -1.6498, -0.1138,  1.8771,  0.6063, -0.0052, -0.8015]],
       dtype=torch.float64)
	q_value: tensor([[-49.5054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3924667995749266, distance: 0.891952447180355 entropy 1.3432114124298096
epoch: 9, step: 79
	action: tensor([[-0.1406, -0.9015,  0.8901, -1.3600, -1.4717, -0.3136,  1.6335]],
       dtype=torch.float64)
	q_value: tensor([[-43.6803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0266535401529406, distance: 1.6290951438620391 entropy 1.3432114124298096
epoch: 9, step: 80
	action: tensor([[ 3.2885, -1.8520,  1.5960, -1.8373,  0.2849, -0.9258,  1.7577]],
       dtype=torch.float64)
	q_value: tensor([[-43.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 81
	action: tensor([[-1.2010, -0.1688,  0.4391,  0.5883,  1.6486, -0.9275,  0.2775]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8827264853432353, distance: 1.5701831383937837 entropy 1.3432114124298096
epoch: 9, step: 82
	action: tensor([[-1.0150,  0.9262,  0.9164,  0.2395,  0.7534,  0.0644, -1.0889]],
       dtype=torch.float64)
	q_value: tensor([[-37.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 83
	action: tensor([[ 0.2602, -0.5155, -1.6818,  0.2311, -0.5893, -0.2502, -1.1627]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06571584984338308, distance: 1.1061045629224582 entropy 1.3432114124298096
epoch: 9, step: 84
	action: tensor([[-0.7826, -0.2422,  0.8106, -1.3014,  0.9879,  1.7507, -0.6613]],
       dtype=torch.float64)
	q_value: tensor([[-34.7990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5195652624474343, distance: 1.4106405756846645 entropy 1.3432114124298096
epoch: 9, step: 85
	action: tensor([[-0.3376, -0.9993,  0.0978,  0.4081,  0.7390,  0.2256,  1.9107]],
       dtype=torch.float64)
	q_value: tensor([[-45.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43588838622816817, distance: 1.3712512443824239 entropy 1.3432114124298096
epoch: 9, step: 86
	action: tensor([[ 0.7596, -0.0794,  0.1646, -1.8322,  1.6166, -1.6397,  0.1682]],
       dtype=torch.float64)
	q_value: tensor([[-40.5423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017576429240606, distance: 0.956224327745754 entropy 1.3432114124298096
epoch: 9, step: 87
	action: tensor([[ 2.6499, -1.3307,  0.5139, -0.3985, -0.0357, -0.1006,  2.3484]],
       dtype=torch.float64)
	q_value: tensor([[-42.4157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 88
	action: tensor([[ 0.9853,  0.8489, -0.1460,  0.7329, -0.0332,  0.6039, -1.5718]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 89
	action: tensor([[-0.5833,  0.5049,  1.0080,  0.4452, -0.0316,  0.4083,  2.2875]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 90
	action: tensor([[-1.1030, -0.7076, -0.0499, -0.3563, -0.4619,  0.2010, -0.1803]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.428156095956059, distance: 1.7831792195296685 entropy 1.3432114124298096
epoch: 9, step: 91
	action: tensor([[-0.9550, -0.5618,  0.2179,  0.2321,  0.1382,  0.3315, -1.1944]],
       dtype=torch.float64)
	q_value: tensor([[-31.7371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0278554307047205, distance: 1.629578133138565 entropy 1.3432114124298096
epoch: 9, step: 92
	action: tensor([[ 0.1024,  0.6037, -0.8063, -0.9146,  0.1967, -0.3460,  2.0645]],
       dtype=torch.float64)
	q_value: tensor([[-34.3490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7531007173588364, distance: 0.5686127679054527 entropy 1.3432114124298096
epoch: 9, step: 93
	action: tensor([[ 0.6428,  0.2738,  0.1188, -0.7841, -0.9752,  0.0153,  1.3929]],
       dtype=torch.float64)
	q_value: tensor([[-40.2059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5525995870981838, distance: 0.7654289491172174 entropy 1.3432114124298096
epoch: 9, step: 94
	action: tensor([[ 2.3437, -1.1915,  0.6949, -0.8873, -0.0615, -0.0907, -0.3081]],
       dtype=torch.float64)
	q_value: tensor([[-35.8767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 95
	action: tensor([[-0.1219, -1.5053, -0.7823, -0.0641, -1.0258,  0.8455, -0.4381]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9716328008057757, distance: 1.6068291824725183 entropy 1.3432114124298096
epoch: 9, step: 96
	action: tensor([[ 0.4786, -1.0325,  1.6683, -0.0903, -2.4021,  1.4360,  2.0576]],
       dtype=torch.float64)
	q_value: tensor([[-39.0679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047334355242837134, distance: 1.1169325441833937 entropy 1.3432114124298096
epoch: 9, step: 97
	action: tensor([[ 1.1450, -1.6427,  0.0439,  0.3245, -2.0116,  0.1210,  2.1357]],
       dtype=torch.float64)
	q_value: tensor([[-56.9970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 98
	action: tensor([[ 0.4015, -0.8072,  0.3104,  0.1567, -0.5028, -0.8903, -0.5047]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2438767848362331, distance: 1.276278268545916 entropy 1.3432114124298096
epoch: 9, step: 99
	action: tensor([[ 0.3678, -0.5447, -1.3776,  0.9654,  0.5242,  0.6399, -0.4259]],
       dtype=torch.float64)
	q_value: tensor([[-31.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09341858086965227, distance: 1.0895824518130794 entropy 1.3432114124298096
epoch: 9, step: 100
	action: tensor([[1.4378, 1.0719, 0.6212, 0.1116, 1.1591, 0.3951, 0.7831]],
       dtype=torch.float64)
	q_value: tensor([[-33.7280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 101
	action: tensor([[ 0.6555, -1.0685,  0.0168,  0.5642, -0.1925,  0.8077,  0.9178]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4016785429769718, distance: 0.8851644881419428 entropy 1.3432114124298096
epoch: 9, step: 102
	action: tensor([[-0.1002, -0.3757, -0.9558, -1.3445, -1.5384,  0.3790,  0.4397]],
       dtype=torch.float64)
	q_value: tensor([[-36.2099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06237967806441547, distance: 1.108077659899698 entropy 1.3432114124298096
epoch: 9, step: 103
	action: tensor([[ 0.8600, -0.7952, -0.9737, -0.3239, -0.2928, -0.0211,  1.5470]],
       dtype=torch.float64)
	q_value: tensor([[-39.4928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2430231237178686, distance: 1.2758402444017507 entropy 1.3432114124298096
epoch: 9, step: 104
	action: tensor([[ 1.8037, -1.1682,  2.6824, -1.0210, -0.1173, -0.1858,  2.5706]],
       dtype=torch.float64)
	q_value: tensor([[-39.6586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 105
	action: tensor([[ 1.9348, -0.9929,  1.1854,  1.3051,  0.6851,  0.5721, -0.8232]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47784912654382516, distance: 1.3911428978146956 entropy 1.3432114124298096
epoch: 9, step: 106
	action: tensor([[ 0.6866, -0.8858,  1.8256, -0.0405,  0.4113,  1.2771, -0.6686]],
       dtype=torch.float64)
	q_value: tensor([[-49.2965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22431293023587617, distance: 1.2662017556843141 entropy 1.3432114124298096
epoch: 9, step: 107
	action: tensor([[-0.0325, -0.1192,  0.9792,  1.0778,  1.0210, -1.0649,  0.2972]],
       dtype=torch.float64)
	q_value: tensor([[-43.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 108
	action: tensor([[ 1.7598, -0.1136, -0.7663, -0.9894,  0.1379,  0.9032, -1.2024]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 109
	action: tensor([[-0.6304, -0.1331,  1.5630, -1.2339, -0.0937, -0.0142, -0.1365]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0158576774743553, distance: 1.624750303567186 entropy 1.3432114124298096
epoch: 9, step: 110
	action: tensor([[ 0.6153, -0.0541,  1.6436, -0.0034,  0.1630,  1.3441,  1.2327]],
       dtype=torch.float64)
	q_value: tensor([[-34.0226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.695784240261942, distance: 0.6311717429443835 entropy 1.3432114124298096
epoch: 9, step: 111
	action: tensor([[ 1.0447, -2.6488,  1.6750, -0.0261, -1.6272,  0.2332,  0.7992]],
       dtype=torch.float64)
	q_value: tensor([[-43.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 112
	action: tensor([[ 1.4322, -0.1570,  1.1740,  0.2199,  0.4092,  1.1820,  1.1963]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 113
	action: tensor([[-0.4454,  0.1775, -1.1785,  0.7835,  1.1228, -1.3330,  1.1539]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3555076190381803, distance: 1.3323173232381849 entropy 1.3432114124298096
epoch: 9, step: 114
	action: tensor([[ 0.6162,  0.3792, -0.5777, -0.6312, -0.0872,  1.7894, -0.3102]],
       dtype=torch.float64)
	q_value: tensor([[-37.2260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 115
	action: tensor([[ 1.7116, -0.0898,  0.1917,  0.5326,  2.7321, -0.7866,  0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 116
	action: tensor([[-0.0084, -1.5860,  1.0782, -0.1533,  0.5477, -0.4676, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.000419907657288, distance: 1.6185170442058232 entropy 1.3432114124298096
epoch: 9, step: 117
	action: tensor([[ 0.2158, -1.9259,  0.2359, -0.4437, -0.3394,  0.9654, -0.9922]],
       dtype=torch.float64)
	q_value: tensor([[-33.6956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 118
	action: tensor([[-1.6084, -0.9846, -1.4799,  0.7211, -0.9715,  0.0057,  0.6499]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.852910135479592, distance: 1.9328615162055225 entropy 1.3432114124298096
epoch: 9, step: 119
	action: tensor([[ 0.9042, -0.8275, -0.3199, -0.5666, -2.2170, -0.3143, -0.2859]],
       dtype=torch.float64)
	q_value: tensor([[-43.1737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4286562279196866, distance: 1.367793585302607 entropy 1.3432114124298096
epoch: 9, step: 120
	action: tensor([[ 0.1078, -0.1294,  0.7720, -0.3810, -0.5802,  0.1978,  0.9769]],
       dtype=torch.float64)
	q_value: tensor([[-42.9110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06453724976654651, distance: 1.1068020188524157 entropy 1.3432114124298096
epoch: 9, step: 121
	action: tensor([[ 0.9027, -0.6597,  1.0711, -0.5612, -0.1212, -0.7700,  0.6100]],
       dtype=torch.float64)
	q_value: tensor([[-30.9932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10048907596085987, distance: 1.2004651621042541 entropy 1.3432114124298096
epoch: 9, step: 122
	action: tensor([[ 1.9780, -0.4666,  0.6133,  0.3663,  0.6267,  1.0239, -0.2181]],
       dtype=torch.float64)
	q_value: tensor([[-32.8384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 123
	action: tensor([[-1.2385,  1.3598, -0.6842, -0.0068, -0.8656, -0.0579,  0.2074]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 124
	action: tensor([[ 0.1443,  0.1771,  0.3350, -0.5228, -1.0419, -0.9991, -0.5797]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36025764499050184, distance: 0.9152911130479147 entropy 1.3432114124298096
epoch: 9, step: 125
	action: tensor([[ 0.8055,  1.2587,  2.1291,  1.0713,  0.5372,  0.2357, -1.0420]],
       dtype=torch.float64)
	q_value: tensor([[-30.3272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 9, step: 126
	action: tensor([[ 0.7145, -0.8874,  0.4644,  1.1044,  0.3802,  0.2100, -0.6831]],
       dtype=torch.float64)
	q_value: tensor([[-38.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6740577191217146, distance: 0.6533216397763917 entropy 1.3432114124298096
epoch: 9, step: 127
	action: tensor([[ 0.4932, -0.5973, -0.1386,  1.7603,  0.3674, -0.0610,  0.0788]],
       dtype=torch.float64)
	q_value: tensor([[-37.9424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
LOSS epoch 9 actor 424.8775746090006 critic 130.32023968424468 
epoch: 10, step: 0
	action: tensor([[ 0.9707, -1.0654,  0.6494, -1.1391, -0.5757,  1.1725, -0.0680]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3352477704529595, distance: 1.3223232193635284 entropy 1.2378510236740112
epoch: 10, step: 1
	action: tensor([[ 1.1453, -1.0932,  1.4945, -1.2718,  0.5270,  0.8956,  0.4673]],
       dtype=torch.float64)
	q_value: tensor([[-44.6178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2672505859605787, distance: 1.288213789310226 entropy 1.2378510236740112
epoch: 10, step: 2
	action: tensor([[-0.2559, -1.2105,  1.6543, -1.8406, -0.5904, -0.0450,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-46.0804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6364912689356734, distance: 1.4639071756057185 entropy 1.2378510236740112
epoch: 10, step: 3
	action: tensor([[ 1.3352, -0.4269, -1.0653, -1.7439,  0.3599,  0.7912, -0.9091]],
       dtype=torch.float64)
	q_value: tensor([[-46.6022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3148666887309617, distance: 1.3121925108233043 entropy 1.2378510236740112
epoch: 10, step: 4
	action: tensor([[ 2.8047, -0.8957,  1.3655, -0.2793, -0.5398, -1.0051, -0.8629]],
       dtype=torch.float64)
	q_value: tensor([[-47.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 5
	action: tensor([[-0.0244, -1.7381,  1.1394, -0.6376,  0.0998,  0.8618, -1.4503]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8544131809052244, distance: 1.5583318461997273 entropy 1.2378510236740112
epoch: 10, step: 6
	action: tensor([[-0.4664, -0.7332,  0.4190,  0.7905, -0.2901, -0.0491, -0.4940]],
       dtype=torch.float64)
	q_value: tensor([[-45.9980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1404866796404185, distance: 1.2220860928158552 entropy 1.2378510236740112
epoch: 10, step: 7
	action: tensor([[ 0.9928, -1.5145,  1.3827,  0.4563, -0.7449,  0.4782,  0.7220]],
       dtype=torch.float64)
	q_value: tensor([[-35.4933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 8
	action: tensor([[ 0.0940, -0.0940,  0.1305, -0.3922, -0.2611,  0.1229,  0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11583203213124937, distance: 1.076029261715003 entropy 1.2378510236740112
epoch: 10, step: 9
	action: tensor([[-0.2088, -0.5159,  0.2885,  0.8574, -0.2323, -1.3991, -0.1957]],
       dtype=torch.float64)
	q_value: tensor([[-27.4965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015058392264925557, distance: 1.135695579382465 entropy 1.2378510236740112
epoch: 10, step: 10
	action: tensor([[ 0.7752,  0.0456,  0.9840,  0.7156, -0.4006, -0.5665, -0.1457]],
       dtype=torch.float64)
	q_value: tensor([[-37.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8487135377584188, distance: 0.44509911102598215 entropy 1.2378510236740112
epoch: 10, step: 11
	action: tensor([[ 1.3073, -1.0349,  0.3313, -0.1219, -1.1682,  0.3521,  1.4069]],
       dtype=torch.float64)
	q_value: tensor([[-35.5879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37097580395550755, distance: 1.339897535505711 entropy 1.2378510236740112
epoch: 10, step: 12
	action: tensor([[ 1.3550, -0.8887,  1.9225, -0.5929, -1.2345,  1.9227,  0.7999]],
       dtype=torch.float64)
	q_value: tensor([[-47.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3460057591921587, distance: 0.9254301728755361 entropy 1.2378510236740112
epoch: 10, step: 13
	action: tensor([[ 1.2714, -2.3881,  1.1601, -0.6469,  1.5954, -0.1156,  1.4162]],
       dtype=torch.float64)
	q_value: tensor([[-57.2672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 14
	action: tensor([[ 0.5628,  0.5109, -1.6813,  0.6445,  1.3734,  1.0947,  0.1932]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 15
	action: tensor([[-1.1209,  0.6146, -0.3665, -0.1922, -0.3761, -1.2569,  0.4569]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28460133884585126, distance: 1.2970026948038784 entropy 1.2378510236740112
epoch: 10, step: 16
	action: tensor([[-0.0395, -0.2311, -0.3309,  0.3786,  1.1380,  0.6037,  0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-35.8554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4441457032125644, distance: 0.8531731449266703 entropy 1.2378510236740112
epoch: 10, step: 17
	action: tensor([[-0.1985,  0.2578, -0.7987,  0.1474,  0.7895, -0.0464, -0.8666]],
       dtype=torch.float64)
	q_value: tensor([[-34.0659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 18
	action: tensor([[ 0.7956, -1.1407,  1.2474, -1.0023,  0.1268, -0.6221, -0.1097]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36795361495334755, distance: 1.3384198807973122 entropy 1.2378510236740112
epoch: 10, step: 19
	action: tensor([[ 0.3570, -0.8298,  0.4162,  0.1222,  0.8741, -1.0360,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[-38.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25262212355485714, distance: 1.280756982298298 entropy 1.2378510236740112
epoch: 10, step: 20
	action: tensor([[-0.0028, -1.3578, -0.5213, -0.3588, -0.2295, -0.5027, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[-34.9461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 21
	action: tensor([[ 0.9071,  0.3199,  0.1575,  2.4955,  1.0200, -0.0670, -0.3054]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 22
	action: tensor([[ 0.2044, -0.0076, -1.0522,  0.1122, -0.6799, -0.7473, -0.9518]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6527928749084828, distance: 0.6742966587475032 entropy 1.2378510236740112
epoch: 10, step: 23
	action: tensor([[-0.8161,  0.2272, -0.4399,  0.1804, -0.3760,  0.3655,  0.6530]],
       dtype=torch.float64)
	q_value: tensor([[-32.9814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5231769984061272, distance: 1.412316001418535 entropy 1.2378510236740112
epoch: 10, step: 24
	action: tensor([[ 0.0085,  0.0419,  0.1195, -0.2410, -1.3782,  1.3476,  0.4409]],
       dtype=torch.float64)
	q_value: tensor([[-32.0865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15316177091145478, distance: 1.053069224573718 entropy 1.2378510236740112
epoch: 10, step: 25
	action: tensor([[-0.1004, -1.2659,  1.3290, -0.8207,  0.2449,  0.2903,  0.3267]],
       dtype=torch.float64)
	q_value: tensor([[-41.9306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1547951256378122, distance: 1.679808155738589 entropy 1.2378510236740112
epoch: 10, step: 26
	action: tensor([[ 1.1742,  0.2266,  1.2396, -1.5728, -0.6924,  1.4830,  1.7990]],
       dtype=torch.float64)
	q_value: tensor([[-38.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8986398751806148, distance: 0.3643260805236088 entropy 1.2378510236740112
epoch: 10, step: 27
	action: tensor([[ 0.7867, -2.1586,  1.8220,  0.0251,  0.6466, -0.4613,  2.5279]],
       dtype=torch.float64)
	q_value: tensor([[-58.5443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 28
	action: tensor([[-0.2115,  0.1962, -2.4181,  0.7507, -1.4086,  0.0042, -0.3949]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3054309305041898, distance: 1.307475749587405 entropy 1.2378510236740112
epoch: 10, step: 29
	action: tensor([[ 0.7743,  0.8008,  1.5090, -0.2231, -0.8835, -0.6735,  3.1143]],
       dtype=torch.float64)
	q_value: tensor([[-44.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 30
	action: tensor([[ 0.0219,  0.4525,  0.5745, -0.9957,  0.0591,  0.0889, -0.7523]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08005510734214472, distance: 1.0975835756584824 entropy 1.2378510236740112
epoch: 10, step: 31
	action: tensor([[-0.9785,  0.8633,  1.1652, -0.4369,  0.4906,  1.1085, -0.5492]],
       dtype=torch.float64)
	q_value: tensor([[-30.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10100161100364735, distance: 1.2007446782094573 entropy 1.2378510236740112
epoch: 10, step: 32
	action: tensor([[-0.8462, -0.5514, -0.5912, -1.0612,  0.8006,  0.0224, -0.1473]],
       dtype=torch.float64)
	q_value: tensor([[-43.3599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.609979777743431, distance: 1.4520009610366793 entropy 1.2378510236740112
epoch: 10, step: 33
	action: tensor([[ 0.1434,  0.9839, -0.9390, -0.6492, -0.7886,  0.2639, -1.4961]],
       dtype=torch.float64)
	q_value: tensor([[-36.8443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 34
	action: tensor([[ 1.1419,  0.8567, -0.7686,  0.0397,  1.1358, -1.0662,  1.0010]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 35
	action: tensor([[-0.6469, -0.9845, -1.3219, -0.2212,  1.1747, -0.0453, -0.4215]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.883179289934787, distance: 1.570371945264149 entropy 1.2378510236740112
epoch: 10, step: 36
	action: tensor([[-0.0950, -0.2056, -0.5815,  0.4110,  0.6317, -0.1826, -1.6479]],
       dtype=torch.float64)
	q_value: tensor([[-40.9148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015960988609820648, distance: 1.135175086785742 entropy 1.2378510236740112
epoch: 10, step: 37
	action: tensor([[-0.6281,  0.7193,  0.0306, -0.9435, -0.4266, -0.3524, -1.1414]],
       dtype=torch.float64)
	q_value: tensor([[-36.8756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23213325156690234, distance: 1.2702392620223402 entropy 1.2378510236740112
epoch: 10, step: 38
	action: tensor([[ 0.1005, -0.5438, -0.8912, -0.5613,  0.6877, -0.2819, -0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-33.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09668064415230959, distance: 1.1983861537250584 entropy 1.2378510236740112
epoch: 10, step: 39
	action: tensor([[ 0.1223,  0.0902, -1.1024, -0.0155, -0.4301,  1.7839,  0.4027]],
       dtype=torch.float64)
	q_value: tensor([[-33.6159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33634332698990543, distance: 0.9322414867025473 entropy 1.2378510236740112
epoch: 10, step: 40
	action: tensor([[ 0.7530, -1.7504,  0.1769,  1.0524,  1.2547,  0.2433,  2.0009]],
       dtype=torch.float64)
	q_value: tensor([[-42.8892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 41
	action: tensor([[-0.0039, -0.8705, -0.2252,  0.9196, -0.9403,  0.0954, -0.4623]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06765631828843599, distance: 1.1049552998021792 entropy 1.2378510236740112
epoch: 10, step: 42
	action: tensor([[ 1.3992, -0.4177,  0.4818, -0.4308, -0.3910,  0.8364,  0.8587]],
       dtype=torch.float64)
	q_value: tensor([[-38.4125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32351776577239466, distance: 0.941206442194603 entropy 1.2378510236740112
epoch: 10, step: 43
	action: tensor([[ 0.4891, -2.4195,  0.9272,  0.4879,  1.3336, -1.4728,  0.1552]],
       dtype=torch.float64)
	q_value: tensor([[-41.2962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 44
	action: tensor([[-0.5116, -1.6077, -0.4918, -0.4823,  1.0017, -0.6121, -0.6805]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9260860272616604, distance: 1.5881610242562747 entropy 1.2378510236740112
epoch: 10, step: 45
	action: tensor([[ 1.8132, -0.5343, -0.8374,  1.8062, -0.1904, -0.6935,  0.6163]],
       dtype=torch.float64)
	q_value: tensor([[-40.2812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8880834311668073, distance: 0.38282816945726916 entropy 1.2378510236740112
epoch: 10, step: 46
	action: tensor([[ 0.6358, -1.4780,  0.1183, -1.1257,  0.5677, -0.2576,  1.4669]],
       dtype=torch.float64)
	q_value: tensor([[-49.8525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 47
	action: tensor([[-2.1508, -1.7126, -0.5434, -0.7373, -0.3093,  0.1167,  0.6845]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 48
	action: tensor([[-0.3835,  0.2040, -0.4772,  0.2175, -1.1628,  0.1336,  0.7186]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0033742119013642213, distance: 1.1462732581536919 entropy 1.2378510236740112
epoch: 10, step: 49
	action: tensor([[ 1.0523,  0.4460, -1.3672,  0.9341,  0.2225, -0.9579,  0.9416]],
       dtype=torch.float64)
	q_value: tensor([[-35.3129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 50
	action: tensor([[ 0.9023,  0.0211,  1.3635,  1.0136,  0.1479,  0.6163, -0.1439]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 51
	action: tensor([[ 0.2455,  0.1348,  0.8708,  0.3103, -0.1704, -0.6283, -0.2123]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6251649997736979, distance: 0.7006106930725364 entropy 1.2378510236740112
epoch: 10, step: 52
	action: tensor([[ 0.4370,  0.6211, -0.9658, -1.2333, -0.2817,  0.6801, -0.7013]],
       dtype=torch.float64)
	q_value: tensor([[-31.2128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8872215246726786, distance: 0.3842994852624914 entropy 1.2378510236740112
epoch: 10, step: 53
	action: tensor([[-0.6199, -0.5098, -1.1477, -0.9153, -0.3189,  0.8541,  1.7480]],
       dtype=torch.float64)
	q_value: tensor([[-35.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4644026261000114, distance: 1.3847996426066191 entropy 1.2378510236740112
epoch: 10, step: 54
	action: tensor([[ 2.2283, -1.8018,  1.2738, -0.7780, -0.3512,  0.5184,  2.8656]],
       dtype=torch.float64)
	q_value: tensor([[-48.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 55
	action: tensor([[-2.1538,  0.3782,  0.1117, -0.3839,  1.2079,  1.5296,  1.6865]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 56
	action: tensor([[-0.5821,  0.3747,  0.3199, -0.5606, -1.1978, -0.7489,  0.1848]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20704857032352386, distance: 1.2572425368202713 entropy 1.2378510236740112
epoch: 10, step: 57
	action: tensor([[ 0.7565, -1.2843,  1.6938,  0.1676, -1.3316,  0.0829,  0.3001]],
       dtype=torch.float64)
	q_value: tensor([[-35.1643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5910442616053642, distance: 1.4434369686065052 entropy 1.2378510236740112
epoch: 10, step: 58
	action: tensor([[ 0.2322, -0.3111, -0.3838, -1.8771,  0.5205, -0.4403,  1.2709]],
       dtype=torch.float64)
	q_value: tensor([[-47.4421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1912001630159943, distance: 1.2489615407521828 entropy 1.2378510236740112
epoch: 10, step: 59
	action: tensor([[ 0.9220,  0.1028,  0.9074,  0.2278, -0.1927,  0.2792,  1.3656]],
       dtype=torch.float64)
	q_value: tensor([[-42.9631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8558491725899212, distance: 0.4344754713425858 entropy 1.2378510236740112
epoch: 10, step: 60
	action: tensor([[ 0.6880, -0.3234,  2.1020, -1.1924,  0.5954, -0.2532,  1.3724]],
       dtype=torch.float64)
	q_value: tensor([[-39.6102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6195237733600523, distance: 0.7058630627377953 entropy 1.2378510236740112
epoch: 10, step: 61
	action: tensor([[ 1.7390, -1.4246,  0.8860, -0.8150,  1.1011,  0.8578,  1.8654]],
       dtype=torch.float64)
	q_value: tensor([[-45.7095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7760311038927454, distance: 1.5250426058835547 entropy 1.2378510236740112
epoch: 10, step: 62
	action: tensor([[ 1.2550, -1.5588,  1.2543, -1.0003,  0.1567, -0.8398,  2.1597]],
       dtype=torch.float64)
	q_value: tensor([[-55.5885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 63
	action: tensor([[-0.1374,  0.5135,  0.4112,  0.3664,  0.4210,  0.0778, -0.9214]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 64
	action: tensor([[-1.1392, -1.3087,  0.9007,  0.9655, -0.8170,  1.0672,  0.3985]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4823641681252395, distance: 1.3932663479827456 entropy 1.2378510236740112
epoch: 10, step: 65
	action: tensor([[-0.1800, -1.4372, -0.0799, -0.4328, -0.7805, -1.6893,  2.0547]],
       dtype=torch.float64)
	q_value: tensor([[-49.8219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 66
	action: tensor([[ 1.2985, -1.0716, -0.4105,  0.0246,  0.9939, -0.1779,  0.7178]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5099040518028979, distance: 1.4061490848291653 entropy 1.2378510236740112
epoch: 10, step: 67
	action: tensor([[ 0.2364, -1.6878, -0.0230, -0.3665, -0.7759,  1.4870,  0.6732]],
       dtype=torch.float64)
	q_value: tensor([[-42.0004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 68
	action: tensor([[ 0.9908, -1.5056, -0.9943, -0.9751, -1.6782,  0.3652,  1.1732]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8825411357509185, distance: 1.570105846240611 entropy 1.2378510236740112
epoch: 10, step: 69
	action: tensor([[ 0.6209, -0.7165,  1.4595, -0.9562,  1.1327, -1.1339,  2.6883]],
       dtype=torch.float64)
	q_value: tensor([[-51.8578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976797256311974, distance: 0.959012562277835 entropy 1.2378510236740112
epoch: 10, step: 70
	action: tensor([[ 0.4908,  0.1734,  1.7756, -0.9003,  0.7054,  0.5203,  0.6397]],
       dtype=torch.float64)
	q_value: tensor([[-54.2067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5120692816702096, distance: 0.7993477807421338 entropy 1.2378510236740112
epoch: 10, step: 71
	action: tensor([[ 1.3754, -1.3353,  1.0309, -1.6237,  0.9143,  0.1873,  1.9408]],
       dtype=torch.float64)
	q_value: tensor([[-40.7989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1505853859767301, distance: 1.2274847918668341 entropy 1.2378510236740112
epoch: 10, step: 72
	action: tensor([[ 1.1283, -2.3694,  2.4574, -0.7394,  0.4222,  0.2134,  2.9714]],
       dtype=torch.float64)
	q_value: tensor([[-53.7178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 73
	action: tensor([[ 0.1316,  0.8351, -0.0698,  0.6315, -0.4231, -0.7646, -0.5168]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 74
	action: tensor([[-0.1833, -0.6152, -0.0884, -0.2159, -0.8038,  0.6179, -1.0732]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4866485132263616, distance: 1.3952783120858374 entropy 1.2378510236740112
epoch: 10, step: 75
	action: tensor([[ 1.6591, -0.1046,  0.5804, -2.0111,  0.3545, -0.0036,  0.9344]],
       dtype=torch.float64)
	q_value: tensor([[-36.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06655565464733593, distance: 1.105607326207433 entropy 1.2378510236740112
epoch: 10, step: 76
	action: tensor([[ 0.7205, -1.8167,  0.6245, -0.5440,  0.3676, -1.3057,  2.6889]],
       dtype=torch.float64)
	q_value: tensor([[-46.2812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 77
	action: tensor([[-1.8762,  0.8892, -1.3980, -0.2874, -0.7590, -0.3613,  0.9494]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 78
	action: tensor([[-0.3356, -1.4858, -0.0890,  0.6423,  0.0807, -0.2986, -0.4201]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7372003032210079, distance: 1.5082788512764351 entropy 1.2378510236740112
epoch: 10, step: 79
	action: tensor([[-0.1858,  2.2274, -0.3960, -1.2474, -0.2513, -0.2056,  0.8640]],
       dtype=torch.float64)
	q_value: tensor([[-36.4837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 80
	action: tensor([[-0.3650, -0.6882, -0.2910,  0.1575, -0.2658,  0.7247, -1.0157]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4497603603162834, distance: 1.3778590838865283 entropy 1.2378510236740112
epoch: 10, step: 81
	action: tensor([[ 0.3711, -0.3998,  0.2040, -0.7326,  0.1539,  0.3554, -0.6654]],
       dtype=torch.float64)
	q_value: tensor([[-35.3266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10607559338862238, distance: 1.2035083226959657 entropy 1.2378510236740112
epoch: 10, step: 82
	action: tensor([[-1.5072,  0.6405,  0.8215, -0.6282, -1.3974,  0.0825,  0.1639]],
       dtype=torch.float64)
	q_value: tensor([[-30.7117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3234113981865945, distance: 1.7442942576519844 entropy 1.2378510236740112
epoch: 10, step: 83
	action: tensor([[ 1.5796, -1.5288,  1.1703, -0.7650,  1.0890, -0.0136,  0.0463]],
       dtype=torch.float64)
	q_value: tensor([[-43.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42548184525914556, distance: 1.3662731653811502 entropy 1.2378510236740112
epoch: 10, step: 84
	action: tensor([[-0.1162, -1.3493,  1.1782, -0.1063, -0.0917,  0.2997,  1.6849]],
       dtype=torch.float64)
	q_value: tensor([[-44.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 85
	action: tensor([[-1.8787, -0.5819,  0.8893, -0.5261,  1.3773,  2.3926,  1.3453]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 86
	action: tensor([[ 0.5701,  0.6810, -0.3399,  0.1651,  0.2898, -0.1257,  1.9975]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 87
	action: tensor([[ 0.2729, -0.7339, -0.7734, -0.9026, -2.0183,  0.3437,  0.0438]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37517005532336767, distance: 1.3419455568408207 entropy 1.2378510236740112
epoch: 10, step: 88
	action: tensor([[ 1.1223, -1.7165,  0.6453,  0.3725,  1.4348,  0.4021, -0.8160]],
       dtype=torch.float64)
	q_value: tensor([[-46.9304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 89
	action: tensor([[-0.4832, -0.8758,  0.5434,  0.2052, -1.8114, -0.2023,  0.7190]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6929889379855532, distance: 1.4889624775172368 entropy 1.2378510236740112
epoch: 10, step: 90
	action: tensor([[ 1.4049,  0.2043,  1.6160, -1.5233,  1.3568,  0.8594,  1.0846]],
       dtype=torch.float64)
	q_value: tensor([[-44.8266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6864116439930175, distance: 0.6408208850433995 entropy 1.2378510236740112
epoch: 10, step: 91
	action: tensor([[ 1.2724, -2.4350, -0.1616,  0.2521, -0.9094,  1.9382,  1.9347]],
       dtype=torch.float64)
	q_value: tensor([[-51.0718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 92
	action: tensor([[ 1.3032,  0.3308, -0.5675, -0.3753,  0.1183, -0.0836, -0.7124]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5843802841802327, distance: 0.7377424332636219 entropy 1.2378510236740112
epoch: 10, step: 93
	action: tensor([[ 0.2156, -0.4671, -0.7894, -1.7369,  0.0159, -1.6489,  0.9450]],
       dtype=torch.float64)
	q_value: tensor([[-30.5338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29138514049608866, distance: 0.9633005779474435 entropy 1.2378510236740112
epoch: 10, step: 94
	action: tensor([[ 1.7173, -0.8595,  1.4110,  0.0967, -0.4167, -1.3269,  1.7985]],
       dtype=torch.float64)
	q_value: tensor([[-43.9775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13380664441831414, distance: 1.21850184789972 entropy 1.2378510236740112
epoch: 10, step: 95
	action: tensor([[ 1.7836, -1.3062,  0.3088,  0.0040,  0.7123,  0.5278,  1.0156]],
       dtype=torch.float64)
	q_value: tensor([[-50.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 96
	action: tensor([[ 0.4871, -0.9994,  2.1999, -0.1212,  0.6903, -0.5643,  1.6801]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0905047083294519, distance: 1.195007045446864 entropy 1.2378510236740112
epoch: 10, step: 97
	action: tensor([[-0.6507, -1.5894,  0.8306, -0.2077,  1.5795,  0.2280,  0.4488]],
       dtype=torch.float64)
	q_value: tensor([[-48.1737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 98
	action: tensor([[-0.0409, -0.1391,  0.0377, -0.9756, -1.1774, -2.2785,  1.3240]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45081697815584465, distance: 0.848037865957966 entropy 1.2378510236740112
epoch: 10, step: 99
	action: tensor([[-0.4239,  0.7415,  0.5938,  0.8411, -0.6667,  1.3184,  0.7006]],
       dtype=torch.float64)
	q_value: tensor([[-46.9938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 100
	action: tensor([[-0.3668,  1.7883,  0.3015, -0.4819,  1.9766, -0.8135,  0.9968]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 101
	action: tensor([[ 0.8605,  1.9025, -0.6044,  0.5410, -0.2492,  1.1768,  1.0045]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 102
	action: tensor([[-0.8144, -1.0104, -0.0071, -0.3342,  0.3766,  0.3414,  0.6985]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1635104043450153, distance: 1.6832018015465733 entropy 1.2378510236740112
epoch: 10, step: 103
	action: tensor([[ 1.0332, -0.2534,  0.0872,  1.6099, -0.7664,  0.0766,  1.7640]],
       dtype=torch.float64)
	q_value: tensor([[-35.6988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9357338712095495, distance: 0.2901000230331584 entropy 1.2378510236740112
epoch: 10, step: 104
	action: tensor([[-0.2507,  0.2336,  1.4881, -0.2219,  0.8170,  0.5437, -0.6114]],
       dtype=torch.float64)
	q_value: tensor([[-49.4196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12535013985307597, distance: 1.0702218385661708 entropy 1.2378510236740112
epoch: 10, step: 105
	action: tensor([[-0.3535,  0.9577, -0.0894,  0.8575,  0.3495, -0.8804, -0.2568]],
       dtype=torch.float64)
	q_value: tensor([[-39.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 106
	action: tensor([[-0.0275, -0.8582,  0.4886,  0.7755,  0.5389,  0.7158,  0.1240]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4915043160653806, distance: 0.8160191087838117 entropy 1.2378510236740112
epoch: 10, step: 107
	action: tensor([[-0.1352, -1.4537, -0.2458,  1.8797, -1.5214,  0.0912, -0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-36.6008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 108
	action: tensor([[ 1.6620,  0.2578, -0.2277,  0.5606,  0.2815, -0.6707,  0.5208]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 109
	action: tensor([[ 0.7011, -0.7283,  1.7912, -0.5723,  0.2113, -0.5272,  0.8212]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 110
	action: tensor([[-1.6708,  0.3997,  0.6052,  0.0625, -0.8915,  0.3691,  0.6292]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 111
	action: tensor([[-0.3790,  0.4017,  0.9726, -1.7079, -0.9352, -0.5789, -0.7879]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5663101624818923, distance: 1.4321733105902672 entropy 1.2378510236740112
epoch: 10, step: 112
	action: tensor([[-0.5947,  0.1274,  0.0192, -0.4906, -0.8846, -1.3186,  0.8448]],
       dtype=torch.float64)
	q_value: tensor([[-39.4988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09749458128708643, distance: 1.1988307818314348 entropy 1.2378510236740112
epoch: 10, step: 113
	action: tensor([[-2.1377, -0.6397, -0.1107, -1.2643,  0.0646, -0.5454, -1.0426]],
       dtype=torch.float64)
	q_value: tensor([[-36.7463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 114
	action: tensor([[-0.2873, -1.0129,  0.4175,  0.1509, -2.2645,  0.0223,  1.5505]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7010817617398286, distance: 1.4925170030992507 entropy 1.2378510236740112
epoch: 10, step: 115
	action: tensor([[ 1.6243, -0.9225,  1.0473, -0.0220,  0.3552,  1.0515,  0.8643]],
       dtype=torch.float64)
	q_value: tensor([[-52.4788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4683440709870146, distance: 1.3866619869087238 entropy 1.2378510236740112
epoch: 10, step: 116
	action: tensor([[ 0.9404, -0.4117, -0.3116, -1.0207, -0.1311, -0.6322,  2.0734]],
       dtype=torch.float64)
	q_value: tensor([[-47.2437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3131115552562782, distance: 1.3113164364306686 entropy 1.2378510236740112
epoch: 10, step: 117
	action: tensor([[ 0.9505, -2.2525, -0.5414,  0.0688, -1.3348,  1.0402,  2.1416]],
       dtype=torch.float64)
	q_value: tensor([[-47.5660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 118
	action: tensor([[-0.5066,  0.9490, -0.7792,  1.5170,  0.4749,  0.6415,  0.1887]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 119
	action: tensor([[-1.3203,  0.1445,  0.4342,  1.2448, -0.3544, -2.6436, -0.8575]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4191119225167603, distance: 1.3632170763256373 entropy 1.2378510236740112
epoch: 10, step: 120
	action: tensor([[ 1.3353,  0.1708,  0.7917, -0.3027,  0.1809, -0.8112, -0.9086]],
       dtype=torch.float64)
	q_value: tensor([[-51.1540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6958031428765301, distance: 0.6311521335369954 entropy 1.2378510236740112
epoch: 10, step: 121
	action: tensor([[-0.7202,  0.1217,  0.2400, -1.3804,  0.6653,  0.6642,  0.4702]],
       dtype=torch.float64)
	q_value: tensor([[-35.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6574161273510495, distance: 1.4732365113258377 entropy 1.2378510236740112
epoch: 10, step: 122
	action: tensor([[ 0.2731, -1.5741,  0.9695, -1.1432,  0.6839,  0.5709,  0.2128]],
       dtype=torch.float64)
	q_value: tensor([[-40.8885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 10, step: 123
	action: tensor([[-0.8545,  0.5685,  0.2683, -0.7426,  0.4417,  1.4544,  0.4489]],
       dtype=torch.float64)
	q_value: tensor([[-42.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0343340048137446, distance: 1.1638234261015716 entropy 1.2378510236740112
epoch: 10, step: 124
	action: tensor([[ 0.1111, -0.6610,  1.3386,  0.3821, -0.4046,  0.2536,  1.2684]],
       dtype=torch.float64)
	q_value: tensor([[-43.5875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19869748934214027, distance: 1.0243655041046456 entropy 1.2378510236740112
epoch: 10, step: 125
	action: tensor([[-0.5600, -0.6612,  1.1610, -0.2834,  0.4097,  0.4698, -0.2151]],
       dtype=torch.float64)
	q_value: tensor([[-40.2525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8697179036426987, distance: 1.5647491943195502 entropy 1.2378510236740112
epoch: 10, step: 126
	action: tensor([[-0.3516, -0.0370, -0.6080, -0.8460, -0.8759,  0.3906,  0.5027]],
       dtype=torch.float64)
	q_value: tensor([[-34.7068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19772281056723928, distance: 1.2523763381270823 entropy 1.2378510236740112
epoch: 10, step: 127
	action: tensor([[ 0.4239,  0.5491,  0.2665, -0.6495, -0.5317, -0.2431,  1.4896]],
       dtype=torch.float64)
	q_value: tensor([[-35.3901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6751189639665719, distance: 0.6522571882687213 entropy 1.2378510236740112
LOSS epoch 10 actor 429.6463494292566 critic 73.35412077321374 
epoch: 11, step: 0
	action: tensor([[ 0.1546, -1.9840,  0.8630, -0.4354, -0.8990,  1.7029,  1.2801]],
       dtype=torch.float64)
	q_value: tensor([[-38.3995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 1
	action: tensor([[ 0.1676,  0.0162,  0.2264,  0.6196,  0.2761, -0.7031, -1.3452]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6160457019072321, distance: 0.7090819984914232 entropy 1.2378510236740112
epoch: 11, step: 2
	action: tensor([[ 1.0123, -0.0036,  0.2776,  0.5111, -0.0213,  0.3725, -1.6186]],
       dtype=torch.float64)
	q_value: tensor([[-35.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9194056196955955, distance: 0.32486959979513497 entropy 1.2378510236740112
epoch: 11, step: 3
	action: tensor([[-0.3494,  0.7308,  0.4190,  1.2394,  0.5944,  0.0304,  0.4218]],
       dtype=torch.float64)
	q_value: tensor([[-37.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 4
	action: tensor([[ 1.2287, -0.5271, -0.4160, -1.6324, -0.5801, -0.8938, -1.3363]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 5
	action: tensor([[-1.9563,  0.3115, -0.0912, -0.0446, -0.2782, -0.0881, -0.9806]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 6
	action: tensor([[ 0.6792, -2.2275, -0.5225,  0.2385, -0.9801,  0.7942, -0.3316]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 7
	action: tensor([[ 0.2451, -0.1833, -0.0646,  0.9071,  0.1383, -0.2164, -0.6528]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 8
	action: tensor([[-0.7883,  0.1577,  2.1210,  1.1897, -0.0441,  0.0209,  0.2058]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03830451905660459, distance: 1.122213479421844 entropy 1.2378510236740112
epoch: 11, step: 9
	action: tensor([[ 1.5609, -1.1367, -1.0006, -1.0589, -0.8665, -0.3937,  1.7957]],
       dtype=torch.float64)
	q_value: tensor([[-45.5897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0067890737744745, distance: 1.6210916065481555 entropy 1.2378510236740112
epoch: 11, step: 10
	action: tensor([[-0.2224, -1.8869,  1.9324, -0.8751,  1.2996, -0.0373,  1.3367]],
       dtype=torch.float64)
	q_value: tensor([[-53.3527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 11
	action: tensor([[-0.3319,  0.3670, -0.2860, -0.0216,  0.0376,  0.9769,  0.9280]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 12
	action: tensor([[ 0.7006, -0.3906, -0.1216,  1.6791, -0.8130,  1.2057,  1.4058]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 13
	action: tensor([[ 1.2078, -1.4194, -2.0076, -0.0095, -0.9140,  0.1646,  0.6556]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6953123173250997, distance: 1.4899838186805903 entropy 1.2378510236740112
epoch: 11, step: 14
	action: tensor([[ 0.9954, -2.3603,  1.6042, -0.3550,  0.9460, -0.1511,  0.7214]],
       dtype=torch.float64)
	q_value: tensor([[-49.9860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 15
	action: tensor([[ 0.6908, -0.6980, -0.1860, -0.8569,  0.7801, -0.6706, -0.1528]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5509856810113538, distance: 1.4251500337834644 entropy 1.2378510236740112
epoch: 11, step: 16
	action: tensor([[ 0.0204, -1.1614,  0.1856, -1.3623,  0.4521, -0.9008,  0.2118]],
       dtype=torch.float64)
	q_value: tensor([[-35.3455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6818280762776889, distance: 1.484046443254246 entropy 1.2378510236740112
epoch: 11, step: 17
	action: tensor([[ 0.4834,  0.5815,  1.0046, -0.7139, -1.1404, -0.5755,  1.4917]],
       dtype=torch.float64)
	q_value: tensor([[-39.1123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 18
	action: tensor([[ 0.8323, -0.5585,  0.3144,  0.5551,  0.8245, -0.0211,  0.3575]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6006397170800652, distance: 0.7231678808835763 entropy 1.2378510236740112
epoch: 11, step: 19
	action: tensor([[-0.3051, -1.5655,  0.0467, -0.7838,  1.0532,  0.8209, -0.5463]],
       dtype=torch.float64)
	q_value: tensor([[-35.3205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 20
	action: tensor([[ 0.5457, -0.6730,  0.3899,  0.6561,  1.5206, -0.1104,  0.4347]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5086960126069979, distance: 0.8021061342514872 entropy 1.2378510236740112
epoch: 11, step: 21
	action: tensor([[ 0.3346,  0.3668, -0.3375, -0.1411,  0.0098,  0.0705,  0.8416]],
       dtype=torch.float64)
	q_value: tensor([[-39.4919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7742109245161545, distance: 0.5437611256129531 entropy 1.2378510236740112
epoch: 11, step: 22
	action: tensor([[ 0.7624, -0.4328, -0.6487,  1.0575, -1.0416, -0.1050,  0.0752]],
       dtype=torch.float64)
	q_value: tensor([[-30.2400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7801413819823388, distance: 0.5365725362884892 entropy 1.2378510236740112
epoch: 11, step: 23
	action: tensor([[ 0.0184, -1.0560,  0.9195, -1.1034,  0.3470, -0.3049,  1.3613]],
       dtype=torch.float64)
	q_value: tensor([[-38.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0609566218823359, distance: 1.6428243024501474 entropy 1.2378510236740112
epoch: 11, step: 24
	action: tensor([[ 1.1120, -2.0938, -0.4395,  0.6704,  0.6644,  0.2397,  2.1090]],
       dtype=torch.float64)
	q_value: tensor([[-40.2152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 25
	action: tensor([[ 0.3699,  0.6417,  0.7116, -0.1920,  1.2452,  1.1134, -0.6804]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 26
	action: tensor([[ 0.5721, -2.3515,  0.0214, -0.2400, -1.1622, -0.2055,  0.6597]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 27
	action: tensor([[ 0.2706, -1.0498, -0.1468, -0.1459, -0.9367, -0.1551, -0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5897458703886747, distance: 1.4428478811630712 entropy 1.2378510236740112
epoch: 11, step: 28
	action: tensor([[ 0.4806, -0.2993, -0.0488,  0.7926,  0.7890,  0.8489, -0.5704]],
       dtype=torch.float64)
	q_value: tensor([[-34.5998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9433833749303169, distance: 0.27228811285120597 entropy 1.2378510236740112
epoch: 11, step: 29
	action: tensor([[ 1.4386, -1.5920, -1.1099,  0.0285,  1.4528, -1.1805, -1.8377]],
       dtype=torch.float64)
	q_value: tensor([[-36.6819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 30
	action: tensor([[-0.4529, -0.0575, -0.5185,  1.2677, -1.1259, -0.2574, -0.3502]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03993623843394911, distance: 1.1212610392244822 entropy 1.2378510236740112
epoch: 11, step: 31
	action: tensor([[ 0.4764,  0.1249, -1.8698,  1.0772,  0.4282,  1.0565,  0.8582]],
       dtype=torch.float64)
	q_value: tensor([[-39.2587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2289599744123766, distance: 1.0048359281049315 entropy 1.2378510236740112
epoch: 11, step: 32
	action: tensor([[ 1.4165e+00, -6.7733e-01,  2.0316e+00, -6.9043e-04, -2.2776e-01,
         -1.3705e+00,  2.2900e+00]], dtype=torch.float64)
	q_value: tensor([[-45.3562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3267254261717454, distance: 0.9389723421295493 entropy 1.2378510236740112
epoch: 11, step: 33
	action: tensor([[ 1.0233, -0.4304,  0.3283,  0.2177, -0.4836,  0.4761,  0.9382]],
       dtype=torch.float64)
	q_value: tensor([[-53.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5911555911388406, distance: 0.7317044979464866 entropy 1.2378510236740112
epoch: 11, step: 34
	action: tensor([[ 0.0679,  0.0535,  2.2039, -1.0162, -0.1875, -0.8127,  0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-36.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14104728589719762, distance: 1.0605748431166537 entropy 1.2378510236740112
epoch: 11, step: 35
	action: tensor([[ 0.7990, -1.4714, -0.0394, -0.7345, -0.1817,  0.0654, -0.2927]],
       dtype=torch.float64)
	q_value: tensor([[-40.5390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0217202192045352, distance: 1.6271111476170579 entropy 1.2378510236740112
epoch: 11, step: 36
	action: tensor([[ 0.5736, -1.3270,  1.6668, -0.9736, -0.2591,  0.3582,  1.0220]],
       dtype=torch.float64)
	q_value: tensor([[-36.7079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 37
	action: tensor([[ 0.9094,  0.9141,  0.4024, -0.5695,  0.4174, -1.8232,  1.6060]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 38
	action: tensor([[ 0.6258,  0.2122, -0.9132, -1.4850, -0.4106,  0.4064,  0.5707]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 39
	action: tensor([[ 0.2314, -0.1731, -1.0423, -1.2887,  1.2105,  0.0227,  0.2135]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 40
	action: tensor([[-2.0232,  1.4568,  0.2106, -0.6596,  0.6713,  0.0759,  0.9358]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 41
	action: tensor([[ 0.6909,  0.1370,  0.7454,  1.0574,  0.4624, -1.2081,  0.5630]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 42
	action: tensor([[ 0.1648, -0.8701,  0.4842,  1.5987,  0.2779, -0.8497, -0.6229]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5892095553905358, distance: 0.7334438304246829 entropy 1.2378510236740112
epoch: 11, step: 43
	action: tensor([[ 1.0212, -0.4082,  0.4412,  0.9361,  0.1214, -0.5087,  0.2635]],
       dtype=torch.float64)
	q_value: tensor([[-43.7414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.728653457757713, distance: 0.5960996119985178 entropy 1.2378510236740112
epoch: 11, step: 44
	action: tensor([[ 1.1398, -1.4714,  0.7862, -0.2172, -2.5272, -0.0314,  2.5487]],
       dtype=torch.float64)
	q_value: tensor([[-36.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 45
	action: tensor([[ 0.0621,  0.4101, -1.8297, -1.1313,  1.0199, -0.8583, -0.5214]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 46
	action: tensor([[ 0.6732, -0.4305,  0.0766, -1.3192, -1.4465,  1.0146, -0.9466]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19150787826257654, distance: 1.249122848525464 entropy 1.2378510236740112
epoch: 11, step: 47
	action: tensor([[ 0.3587, -1.9237, -0.1313,  0.1707, -1.0766,  1.6048,  0.7950]],
       dtype=torch.float64)
	q_value: tensor([[-44.2227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 48
	action: tensor([[-0.5657,  0.0221, -0.4805,  0.7835,  0.4884, -0.4294, -0.2832]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3079248230602265, distance: 1.308724053169823 entropy 1.2378510236740112
epoch: 11, step: 49
	action: tensor([[-0.1954, -0.5943,  0.3721, -0.7216, -0.0230, -0.1034,  0.9434]],
       dtype=torch.float64)
	q_value: tensor([[-30.4985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8381300273531855, distance: 1.5514750936532045 entropy 1.2378510236740112
epoch: 11, step: 50
	action: tensor([[-0.0260, -0.5599,  1.4434, -0.5716, -1.2628,  0.2354,  0.3813]],
       dtype=torch.float64)
	q_value: tensor([[-33.5532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4552240571547095, distance: 1.3804530038866594 entropy 1.2378510236740112
epoch: 11, step: 51
	action: tensor([[ 2.2916, -0.3849,  1.0332, -0.8109,  0.3585, -0.5401,  1.1452]],
       dtype=torch.float64)
	q_value: tensor([[-39.6639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 52
	action: tensor([[ 0.2326, -0.7072, -0.5272, -0.9951,  0.2292,  1.1490,  0.9330]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0891216063942255, distance: 1.194248983170943 entropy 1.2378510236740112
epoch: 11, step: 53
	action: tensor([[ 1.0151, -1.2922,  0.7848,  0.1465,  0.5849,  1.0609,  2.3011]],
       dtype=torch.float64)
	q_value: tensor([[-43.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3377701692750066, distance: 1.323571621668813 entropy 1.2378510236740112
epoch: 11, step: 54
	action: tensor([[ 0.9730, -1.6792,  2.1957, -2.7453,  1.1849, -0.3279,  2.6668]],
       dtype=torch.float64)
	q_value: tensor([[-54.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 55
	action: tensor([[-1.7548, -0.9018,  0.2784, -0.5018, -0.3441,  1.0668,  1.0665]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 56
	action: tensor([[ 1.8657,  0.2433,  0.7298,  0.3776, -0.9439, -0.9169,  0.4988]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 57
	action: tensor([[-0.6979,  0.7375, -0.5178,  0.3242,  0.8442,  1.2508, -0.5044]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 58
	action: tensor([[-0.2605,  0.7354,  0.2550, -2.4813, -0.4910, -0.3457, -0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04045834635585699, distance: 1.1672638681883283 entropy 1.2378510236740112
epoch: 11, step: 59
	action: tensor([[ 0.2589,  0.4072,  1.1192, -0.4994, -1.4023,  0.1159,  1.5314]],
       dtype=torch.float64)
	q_value: tensor([[-39.9757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 60
	action: tensor([[-0.1643, -0.4383,  0.3922,  1.9974,  1.1218, -1.3510,  1.0320]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 61
	action: tensor([[ 0.1731, -0.5969,  0.5831, -0.0581, -0.3208, -0.9385, -0.7886]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33092265295992807, distance: 1.320179856040671 entropy 1.2378510236740112
epoch: 11, step: 62
	action: tensor([[ 0.5005,  0.6194,  0.7304,  0.4030, -0.3157,  0.0548, -0.1166]],
       dtype=torch.float64)
	q_value: tensor([[-33.5941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 63
	action: tensor([[-0.3756,  0.0788, -0.1515,  0.3052, -1.2903,  0.7678,  0.4899]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11649303885121487, distance: 1.2091625925704625 entropy 1.2378510236740112
epoch: 11, step: 64
	action: tensor([[-0.6980, -0.0530,  0.0432, -0.6468,  0.5108,  0.0119, -0.4610]],
       dtype=torch.float64)
	q_value: tensor([[-37.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7865441062780132, distance: 1.5295495991615176 entropy 1.2378510236740112
epoch: 11, step: 65
	action: tensor([[-0.1476, -0.7341, -0.0905, -1.6657,  1.1651, -0.1509,  0.6254]],
       dtype=torch.float64)
	q_value: tensor([[-30.3952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5755766223549579, distance: 1.4364035091331493 entropy 1.2378510236740112
epoch: 11, step: 66
	action: tensor([[ 1.1153, -2.7303,  0.3746, -0.4747,  0.2612, -0.3374,  1.0485]],
       dtype=torch.float64)
	q_value: tensor([[-42.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 67
	action: tensor([[ 0.0990, -1.5949,  1.0207, -0.8176, -0.1225,  1.4226,  0.8737]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.516447808020891, distance: 1.4091928374389815 entropy 1.2378510236740112
epoch: 11, step: 68
	action: tensor([[ 1.6435, -1.6964,  2.7520, -0.0166, -0.4730, -0.8824,  1.2881]],
       dtype=torch.float64)
	q_value: tensor([[-45.6133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 69
	action: tensor([[-0.0296, -0.9592, -0.0436, -1.8228,  0.9334, -0.6260, -0.6976]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4725577457951675, distance: 1.3886501983094888 entropy 1.2378510236740112
epoch: 11, step: 70
	action: tensor([[-0.8264, -1.8573,  1.1555,  1.1373,  0.9946, -1.0022, -0.1373]],
       dtype=torch.float64)
	q_value: tensor([[-43.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 71
	action: tensor([[-0.6882, -3.6494,  0.6338,  0.1255, -0.5235,  0.7082,  0.5817]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 72
	action: tensor([[-1.0433, -0.3234,  0.5748, -0.2099, -0.2304,  0.5665, -0.1283]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1632128081473607, distance: 1.683086033283636 entropy 1.2378510236740112
epoch: 11, step: 73
	action: tensor([[ 0.2344, -0.0966,  0.1683, -2.0145,  0.0539, -1.1275,  2.0583]],
       dtype=torch.float64)
	q_value: tensor([[-33.4150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03194555141884847, distance: 1.162478916215118 entropy 1.2378510236740112
epoch: 11, step: 74
	action: tensor([[ 1.4649, -1.3521,  0.5308,  0.0625, -1.0060, -1.6252,  0.8826]],
       dtype=torch.float64)
	q_value: tensor([[-48.6101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.628759534897857, distance: 1.4604449079890744 entropy 1.2378510236740112
epoch: 11, step: 75
	action: tensor([[-0.4435, -0.7345, -0.9415, -0.4928,  0.0026,  0.9870, -0.1768]],
       dtype=torch.float64)
	q_value: tensor([[-47.6858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5822219397136057, distance: 1.4394294849354095 entropy 1.2378510236740112
epoch: 11, step: 76
	action: tensor([[ 0.8279, -1.4572,  0.6506, -0.8742,  0.3087,  0.4205,  0.8772]],
       dtype=torch.float64)
	q_value: tensor([[-37.9576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 77
	action: tensor([[ 1.7593, -1.3336,  1.3289, -0.4764,  1.6787, -0.5712, -1.1589]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3344325676621498, distance: 1.3219195017226408 entropy 1.2378510236740112
epoch: 11, step: 78
	action: tensor([[-0.4675, -0.8345,  0.6623, -1.1446, -0.2819, -0.9230,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-52.4168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7969513509269825, distance: 1.5339982107422732 entropy 1.2378510236740112
epoch: 11, step: 79
	action: tensor([[-0.1301, -0.5786,  1.9749,  1.0518, -0.2559, -0.4948, -0.5971]],
       dtype=torch.float64)
	q_value: tensor([[-34.7946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020274300426406766, distance: 1.1326844632555897 entropy 1.2378510236740112
epoch: 11, step: 80
	action: tensor([[ 0.5102, -1.1739,  2.6445,  0.7281, -0.8702, -0.3551,  1.4535]],
       dtype=torch.float64)
	q_value: tensor([[-46.2489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8655202401661486, distance: 1.5629917149542725 entropy 1.2378510236740112
epoch: 11, step: 81
	action: tensor([[ 1.4578, -0.6387,  0.4345, -0.4167,  0.1505,  0.2783,  0.9517]],
       dtype=torch.float64)
	q_value: tensor([[-53.1098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2497895435604196, distance: 1.2793080617536927 entropy 1.2378510236740112
epoch: 11, step: 82
	action: tensor([[ 0.5176, -0.8848,  0.3422, -1.7837, -0.1842, -0.8689,  1.4125]],
       dtype=torch.float64)
	q_value: tensor([[-39.4088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3522617405474988, distance: 1.3307211934725793 entropy 1.2378510236740112
epoch: 11, step: 83
	action: tensor([[ 1.5463, -1.8107, -0.3061, -0.1041, -1.6788,  0.5351,  2.9994]],
       dtype=torch.float64)
	q_value: tensor([[-43.5375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 84
	action: tensor([[ 5.1770e-01, -6.3368e-01,  1.2835e-01, -1.8780e+00, -1.5315e-01,
         -1.5402e-01, -6.4174e-05]], dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5104864380138541, distance: 1.4064202420874903 entropy 1.2378510236740112
epoch: 11, step: 85
	action: tensor([[-0.8720, -0.6781,  0.3531,  0.2919,  0.7899, -0.8946,  0.4645]],
       dtype=torch.float64)
	q_value: tensor([[-38.8796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.206339471573421, distance: 1.6997805700687871 entropy 1.2378510236740112
epoch: 11, step: 86
	action: tensor([[ 0.5748, -0.7663, -0.1724, -0.8173, -0.1791,  1.6410, -0.8227]],
       dtype=torch.float64)
	q_value: tensor([[-34.1996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048124575790917445, distance: 1.1164692094560518 entropy 1.2378510236740112
epoch: 11, step: 87
	action: tensor([[0.8865, 0.3316, 1.2152, 1.0638, 1.4250, 0.3695, 1.5921]],
       dtype=torch.float64)
	q_value: tensor([[-44.3141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6922868920957039, distance: 0.6347894372568916 entropy 1.2378510236740112
epoch: 11, step: 88
	action: tensor([[ 0.4797, -1.9989,  1.2135, -1.5662, -1.2628, -0.0323,  1.7607]],
       dtype=torch.float64)
	q_value: tensor([[-51.2928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 89
	action: tensor([[ 0.0263,  1.0865,  0.7913, -0.2609,  0.0924, -0.4909,  1.6183]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 90
	action: tensor([[ 1.1570, -1.2188, -0.3975, -0.2917, -0.1222, -0.5389,  0.2195]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9291076100325202, distance: 1.5894062644375186 entropy 1.2378510236740112
epoch: 11, step: 91
	action: tensor([[ 1.8036, -1.1677, -0.2617, -0.5235, -0.0416,  0.6033,  1.9046]],
       dtype=torch.float64)
	q_value: tensor([[-38.0411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 92
	action: tensor([[-0.6636, -0.9510, -0.6086,  0.4178, -1.0043,  0.6822, -0.4029]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2060479583134667, distance: 1.6996682743522946 entropy 1.2378510236740112
epoch: 11, step: 93
	action: tensor([[-0.1675,  1.3212,  0.8228,  1.2957, -0.8406,  1.6924,  0.1463]],
       dtype=torch.float64)
	q_value: tensor([[-40.3791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 94
	action: tensor([[-0.9807, -1.2477,  0.0720,  0.0131,  1.9842, -0.4760,  1.0438]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4374407094748594, distance: 1.7865851647979194 entropy 1.2378510236740112
epoch: 11, step: 95
	action: tensor([[ 0.4771, -0.0573,  1.3377, -1.3725, -0.0228,  0.1341,  1.5321]],
       dtype=torch.float64)
	q_value: tensor([[-46.3988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04948524159960699, distance: 1.1156709512403975 entropy 1.2378510236740112
epoch: 11, step: 96
	action: tensor([[ 1.4441, -1.1475,  1.4789, -2.2263,  1.3675, -0.2665,  2.5911]],
       dtype=torch.float64)
	q_value: tensor([[-43.5384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6165977000919569, distance: 0.7085721034868498 entropy 1.2378510236740112
epoch: 11, step: 97
	action: tensor([[ 1.0941, -2.7709,  2.2877, -1.4387, -0.4265,  0.1428,  2.4681]],
       dtype=torch.float64)
	q_value: tensor([[-61.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 98
	action: tensor([[-0.2264,  0.6138, -0.3459, -0.4430, -1.8613, -1.0945, -0.3014]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 99
	action: tensor([[-0.1881, -0.9682,  1.2669, -0.4996, -1.0264, -0.2846,  1.4738]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.07677922526201, distance: 1.6491184808788266 entropy 1.2378510236740112
epoch: 11, step: 100
	action: tensor([[ 0.3054, -0.7419, -0.8317, -1.4039,  0.4338,  0.7380, -0.7461]],
       dtype=torch.float64)
	q_value: tensor([[-41.8523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08059965259086144, distance: 1.1895675402087575 entropy 1.2378510236740112
epoch: 11, step: 101
	action: tensor([[ 0.6030, -0.0689, -0.3300, -0.3255, -0.1230, -0.7046,  1.1613]],
       dtype=torch.float64)
	q_value: tensor([[-42.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3778582434319516, distance: 0.9026125523002217 entropy 1.2378510236740112
epoch: 11, step: 102
	action: tensor([[ 0.1639, -1.2908, -0.9072, -0.0174, -1.3070,  0.3301,  1.4283]],
       dtype=torch.float64)
	q_value: tensor([[-34.9288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8453928651909772, distance: 1.5545371738959928 entropy 1.2378510236740112
epoch: 11, step: 103
	action: tensor([[ 1.4103, -1.0700,  0.4166, -1.0881,  1.3605, -0.6996,  1.3966]],
       dtype=torch.float64)
	q_value: tensor([[-46.8326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44340980487141124, distance: 1.3748379729009161 entropy 1.2378510236740112
epoch: 11, step: 104
	action: tensor([[ 0.5104, -1.9783,  0.4588,  0.0888,  1.2237,  0.5202,  2.6294]],
       dtype=torch.float64)
	q_value: tensor([[-47.6983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 105
	action: tensor([[ 1.3274,  1.0544, -1.6895,  0.5597,  1.2650, -1.7603, -0.4423]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 106
	action: tensor([[ 0.4463, -0.6830, -0.4941, -0.0943, -0.5552,  0.2073, -0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.042055989580976005, distance: 1.1681597021640098 entropy 1.2378510236740112
epoch: 11, step: 107
	action: tensor([[ 2.2289, -0.0063,  0.7256, -0.9336,  0.2840,  0.5432,  0.6320]],
       dtype=torch.float64)
	q_value: tensor([[-31.6175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 108
	action: tensor([[-1.3698, -0.7794, -0.3697, -1.3595,  0.1285, -0.4424, -0.4712]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32014365686887203, distance: 1.314822992147486 entropy 1.2378510236740112
epoch: 11, step: 109
	action: tensor([[ 0.4420, -0.6132,  1.7454, -0.5966, -0.1220,  1.2802, -0.2888]],
       dtype=torch.float64)
	q_value: tensor([[-41.2091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23867326137640932, distance: 0.9984865859039448 entropy 1.2378510236740112
epoch: 11, step: 110
	action: tensor([[ 0.4950, -3.0648,  2.1386, -0.9759, -0.3152,  0.8139,  2.5359]],
       dtype=torch.float64)
	q_value: tensor([[-41.6578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 111
	action: tensor([[-0.2875, -0.5515, -1.0532,  0.5037,  0.2847, -0.1459, -0.3163]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5609143074128171, distance: 1.4297043019749522 entropy 1.2378510236740112
epoch: 11, step: 112
	action: tensor([[ 0.2232, -0.7412,  0.2843,  0.0551, -1.4035, -1.0217,  1.2413]],
       dtype=torch.float64)
	q_value: tensor([[-32.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.040290636368015775, distance: 1.1671697896031046 entropy 1.2378510236740112
epoch: 11, step: 113
	action: tensor([[ 0.4987, -0.9699, -0.3396, -1.3969,  1.3106,  0.2667,  1.5408]],
       dtype=torch.float64)
	q_value: tensor([[-43.1376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6449566561916114, distance: 1.467688606155234 entropy 1.2378510236740112
epoch: 11, step: 114
	action: tensor([[ 0.2625, -2.0441,  2.5908, -0.7195, -1.7112,  0.2344,  1.1159]],
       dtype=torch.float64)
	q_value: tensor([[-49.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 115
	action: tensor([[ 0.1294, -0.8704,  0.0810,  0.1247, -0.1052, -0.8086, -0.6536]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46625566219162806, distance: 1.3856755193677708 entropy 1.2378510236740112
epoch: 11, step: 116
	action: tensor([[ 0.4759, -0.5370, -0.4005,  0.1365, -0.1842, -0.6112,  0.2219]],
       dtype=torch.float64)
	q_value: tensor([[-32.5294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10160062419699056, distance: 1.0846544791135455 entropy 1.2378510236740112
epoch: 11, step: 117
	action: tensor([[ 1.1143, -0.6372, -0.3365, -0.3171,  1.0943, -2.0188,  1.2072]],
       dtype=torch.float64)
	q_value: tensor([[-29.8901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10276452759901122, distance: 1.083951650161672 entropy 1.2378510236740112
epoch: 11, step: 118
	action: tensor([[ 0.4924, -1.0124,  0.4104,  0.3197,  0.9252, -1.3932, -0.4438]],
       dtype=torch.float64)
	q_value: tensor([[-46.2757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1992542798578505, distance: 1.2531767583793705 entropy 1.2378510236740112
epoch: 11, step: 119
	action: tensor([[ 1.5137, -1.4301,  0.2539, -0.6456,  0.9594, -0.8516, -1.4950]],
       dtype=torch.float64)
	q_value: tensor([[-39.5934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 120
	action: tensor([[ 0.1423,  0.9361,  0.4333, -0.9015,  0.0693,  0.4866, -0.6861]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 121
	action: tensor([[ 0.2953, -0.8533,  0.5206, -0.2252, -0.3552,  0.5516, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2609865731738483, distance: 1.285026028265104 entropy 1.2378510236740112
epoch: 11, step: 122
	action: tensor([[ 0.1772, -0.4312,  1.0983,  0.6732,  0.9921, -1.6097,  0.5768]],
       dtype=torch.float64)
	q_value: tensor([[-32.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46168793784371565, distance: 0.8396025499999901 entropy 1.2378510236740112
epoch: 11, step: 123
	action: tensor([[ 0.4717,  0.0936,  0.7742,  0.7746,  0.3180,  0.1456, -0.9628]],
       dtype=torch.float64)
	q_value: tensor([[-40.9571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9835561695939737, distance: 0.14674326837672405 entropy 1.2378510236740112
epoch: 11, step: 124
	action: tensor([[ 0.5489, -0.1450,  0.0931,  0.5140, -0.3832, -0.7542, -0.7199]],
       dtype=torch.float64)
	q_value: tensor([[-35.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7223084644514929, distance: 0.6030287447463181 entropy 1.2378510236740112
epoch: 11, step: 125
	action: tensor([[ 0.1360,  0.4315, -1.6862,  0.1931,  0.0062,  0.4684, -0.1243]],
       dtype=torch.float64)
	q_value: tensor([[-31.2229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 11, step: 126
	action: tensor([[ 1.4581, -0.7259,  0.9108, -1.3907, -0.8033,  0.4515, -0.3807]],
       dtype=torch.float64)
	q_value: tensor([[-43.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02693630552375048, distance: 1.1288268420172745 entropy 1.2378510236740112
epoch: 11, step: 127
	action: tensor([[ 0.6283, -0.1932,  0.0504, -0.1301,  1.7729, -1.1172,  0.3305]],
       dtype=torch.float64)
	q_value: tensor([[-41.8045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.502292245274089, distance: 0.8073166269528375 entropy 1.2378510236740112
LOSS epoch 11 actor 396.37387641422316 critic 79.83927595412402 
epoch: 12, step: 0
	action: tensor([[-0.8415, -1.6258,  0.9622,  0.5636, -0.5808, -1.3008,  1.0620]],
       dtype=torch.float64)
	q_value: tensor([[-37.3626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 1
	action: tensor([[ 1.3254,  0.5873,  0.2305,  0.4924,  1.1334, -1.2024,  0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 2
	action: tensor([[ 1.0390, -1.3704, -0.6395,  1.4527, -0.9654,  0.4769, -2.1203]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5434594008968834, distance: 0.7732081008724075 entropy 1.2378510236740112
epoch: 12, step: 3
	action: tensor([[ 1.6956, -0.0446,  0.4710, -0.2679, -0.5501,  1.3939,  0.1473]],
       dtype=torch.float64)
	q_value: tensor([[-50.8834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6684093578384891, distance: 0.6589581385577061 entropy 1.2378510236740112
epoch: 12, step: 4
	action: tensor([[ 1.4656, -2.8219,  1.9047, -0.7197,  1.9497,  1.2370,  2.6224]],
       dtype=torch.float64)
	q_value: tensor([[-39.4185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 5
	action: tensor([[ 0.0631,  0.4749,  0.3946,  0.9883, -1.5111,  0.4977, -0.1750]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 6
	action: tensor([[ 0.3082, -1.0395, -0.0897, -0.0279, -0.7151,  0.4011, -0.1496]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42906114561784636, distance: 1.3679874054048404 entropy 1.2378510236740112
epoch: 12, step: 7
	action: tensor([[ 1.5565,  0.2442, -0.5756, -0.1576, -1.0526, -0.0704,  0.3562]],
       dtype=torch.float64)
	q_value: tensor([[-32.2178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4522721074264957, distance: 0.8469136294379618 entropy 1.2378510236740112
epoch: 12, step: 8
	action: tensor([[ 1.8979, -0.3754,  0.7544, -0.4649, -0.2563, -0.5892,  0.1132]],
       dtype=torch.float64)
	q_value: tensor([[-34.8839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 9
	action: tensor([[ 0.1442,  1.1022,  0.0588, -0.7183, -0.8398,  1.6333, -1.0443]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 10
	action: tensor([[ 0.2395,  0.0250, -0.7423,  0.6146,  0.7760, -0.2755,  0.7237]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 11
	action: tensor([[-1.1053, -0.8168,  1.5210, -0.0955,  0.5222, -0.0206, -0.3021]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5301215601210005, distance: 1.8202346904108442 entropy 1.2378510236740112
epoch: 12, step: 12
	action: tensor([[ 1.8285e+00,  5.3965e-04, -4.1123e-01, -4.0357e-01, -1.3019e+00,
         -9.4578e-01,  1.0190e-01]], dtype=torch.float64)
	q_value: tensor([[-35.1150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022858443335419754, distance: 1.1573493191745334 entropy 1.2378510236740112
epoch: 12, step: 13
	action: tensor([[-0.2264, -1.5100,  0.0914, -0.0588, -1.2488, -0.5931,  0.6412]],
       dtype=torch.float64)
	q_value: tensor([[-37.4658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6651665250121699, distance: 1.4766770629628545 entropy 1.2378510236740112
epoch: 12, step: 14
	action: tensor([[ 1.4158,  0.0434,  1.3701,  1.7204, -1.0076, -1.7273,  0.0794]],
       dtype=torch.float64)
	q_value: tensor([[-35.9298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16891910216328532, distance: 1.0432258585234813 entropy 1.2378510236740112
epoch: 12, step: 15
	action: tensor([[ 1.6838, -0.6172,  0.1212, -0.3496,  1.6852,  0.1509,  1.5578]],
       dtype=torch.float64)
	q_value: tensor([[-50.8846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 16
	action: tensor([[ 0.6194,  1.6737, -1.2747, -1.1142,  0.8348, -0.3695,  1.5566]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 17
	action: tensor([[ 0.5777, -0.7305,  1.6970, -1.0657, -0.8920, -0.4931,  0.1215]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14305336179542483, distance: 1.223460481401098 entropy 1.2378510236740112
epoch: 12, step: 18
	action: tensor([[-0.1246, -1.0330, -0.9146, -0.3638,  0.3638, -0.8047,  0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-36.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5148934203526383, distance: 1.4084704276225934 entropy 1.2378510236740112
epoch: 12, step: 19
	action: tensor([[ 0.0864,  0.5702, -0.1479, -0.8983,  0.8412,  0.2651, -0.7648]],
       dtype=torch.float64)
	q_value: tensor([[-34.4184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5757559222642072, distance: 0.7453574342435906 entropy 1.2378510236740112
epoch: 12, step: 20
	action: tensor([[-0.4998, -0.4606, -0.1703,  0.6563,  0.4371, -0.0303,  1.0598]],
       dtype=torch.float64)
	q_value: tensor([[-30.5343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3393094198860471, distance: 1.3243328595759885 entropy 1.2378510236740112
epoch: 12, step: 21
	action: tensor([[ 0.8139, -1.5839, -0.3165, -0.8595, -0.3636, -1.0404,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-32.5676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 22
	action: tensor([[-1.3369,  0.7542, -0.2954,  0.0582, -0.1091, -1.1011,  0.5669]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5719865044955412, distance: 1.43476607723003 entropy 1.2378510236740112
epoch: 12, step: 23
	action: tensor([[ 0.5394, -1.6128,  0.0895, -0.6434,  2.1327,  0.8324, -0.2545]],
       dtype=torch.float64)
	q_value: tensor([[-35.0043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8199390218914662, distance: 1.5437789388845962 entropy 1.2378510236740112
epoch: 12, step: 24
	action: tensor([[ 0.4236, -0.5123,  1.5066,  0.6437, -0.5702,  0.1238,  0.8727]],
       dtype=torch.float64)
	q_value: tensor([[-47.4352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39100421949737885, distance: 0.8930254483554951 entropy 1.2378510236740112
epoch: 12, step: 25
	action: tensor([[ 0.2767, -0.7732,  0.1675,  1.6373, -0.6494, -1.1070,  1.0354]],
       dtype=torch.float64)
	q_value: tensor([[-35.7504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7236024102907982, distance: 0.6016221523136475 entropy 1.2378510236740112
epoch: 12, step: 26
	action: tensor([[ 0.3499, -1.9540,  0.7978,  0.0530, -1.1348, -2.1047, -0.1054]],
       dtype=torch.float64)
	q_value: tensor([[-43.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 27
	action: tensor([[-0.2714, -1.3032, -0.4335, -1.8729,  0.3638,  0.8757,  0.0586]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15631698521702142, distance: 1.2305383292360994 entropy 1.2378510236740112
epoch: 12, step: 28
	action: tensor([[ 0.1903, -1.3099,  0.8920, -1.2856,  1.6571, -0.3001,  1.2585]],
       dtype=torch.float64)
	q_value: tensor([[-44.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 29
	action: tensor([[ 0.1948,  0.3852, -0.1976,  1.0129,  0.1615, -0.7105, -0.3628]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 30
	action: tensor([[ 1.1290, -0.9689, -0.0257, -0.3696,  0.5487,  0.9592, -0.6572]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2625432577028681, distance: 1.285818964207061 entropy 1.2378510236740112
epoch: 12, step: 31
	action: tensor([[ 0.9141, -1.5184, -0.3307,  0.9293,  1.1025, -1.5149,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-38.3420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 32
	action: tensor([[ 0.8574,  1.2190,  0.3686,  1.2047, -1.8072, -1.0580,  0.7805]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 33
	action: tensor([[-1.0276,  0.2720,  0.7043, -0.5572, -1.0684, -0.3911,  1.8346]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1372073174350894, distance: 1.67293866807871 entropy 1.2378510236740112
epoch: 12, step: 34
	action: tensor([[-0.2474, -1.5152,  1.2502, -0.1130, -1.1653, -0.2383,  0.3987]],
       dtype=torch.float64)
	q_value: tensor([[-40.9255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0741521610376705, distance: 1.6480751077923093 entropy 1.2378510236740112
epoch: 12, step: 35
	action: tensor([[0.7910, 0.1670, 0.7384, 1.0094, 0.8992, 0.5170, 0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-35.7146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.867762695015559, distance: 0.41613446670247267 entropy 1.2378510236740112
epoch: 12, step: 36
	action: tensor([[-0.0081,  1.1191,  1.2346, -0.5873, -0.8337, -0.2669,  1.5966]],
       dtype=torch.float64)
	q_value: tensor([[-36.5889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 37
	action: tensor([[ 1.9965,  0.1794, -0.7710, -0.7166, -1.2247, -0.4758,  0.2895]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 38
	action: tensor([[-0.2930, -0.1739, -0.5052, -0.2282,  0.7404, -2.4319, -0.5251]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03444320941824919, distance: 1.1638848625050393 entropy 1.2378510236740112
epoch: 12, step: 39
	action: tensor([[-0.4819, -0.5840,  0.8857,  0.0683,  1.6326, -0.0696,  0.5329]],
       dtype=torch.float64)
	q_value: tensor([[-37.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7048085383423863, distance: 1.494151032223013 entropy 1.2378510236740112
epoch: 12, step: 40
	action: tensor([[-1.4357,  0.3196,  0.0396,  0.5301, -0.3203,  1.2788, -0.6939]],
       dtype=torch.float64)
	q_value: tensor([[-36.7227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15781158186545374, distance: 1.2313333381174005 entropy 1.2378510236740112
epoch: 12, step: 41
	action: tensor([[ 0.3754, -1.1874,  1.0511,  0.6264, -1.0191, -0.4432,  0.4167]],
       dtype=torch.float64)
	q_value: tensor([[-38.8454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33218394107787197, distance: 1.3208052615995025 entropy 1.2378510236740112
epoch: 12, step: 42
	action: tensor([[ 0.1458, -1.1133,  1.3725, -0.6676, -0.8405, -0.5539,  1.3373]],
       dtype=torch.float64)
	q_value: tensor([[-38.1581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.979076820236509, distance: 1.60985966532694 entropy 1.2378510236740112
epoch: 12, step: 43
	action: tensor([[ 0.5189, -1.5552,  1.6964, -0.9427,  0.2075,  0.3847,  3.1929]],
       dtype=torch.float64)
	q_value: tensor([[-37.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 44
	action: tensor([[ 0.2048, -1.3279, -1.0777,  0.2204,  0.6229,  1.0036,  0.3584]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.351924517089403, distance: 1.3305552572600459 entropy 1.2378510236740112
epoch: 12, step: 45
	action: tensor([[ 1.8341, -2.4797,  0.5219, -1.1167, -0.3284,  0.7752,  0.9552]],
       dtype=torch.float64)
	q_value: tensor([[-39.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 46
	action: tensor([[ 1.2720, -1.2714,  0.9655, -0.5390, -0.5577,  0.3248, -1.4849]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6057192899533852, distance: 1.4500784740183879 entropy 1.2378510236740112
epoch: 12, step: 47
	action: tensor([[-0.1020, -0.1827,  0.7540, -1.1561, -0.2921, -0.7979,  0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-40.2234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5633120559009019, distance: 1.4308019778184893 entropy 1.2378510236740112
epoch: 12, step: 48
	action: tensor([[-0.4968,  0.2149,  0.3628, -0.4557,  0.2368, -0.1943, -1.1345]],
       dtype=torch.float64)
	q_value: tensor([[-30.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4922038312431636, distance: 1.3978828238372674 entropy 1.2378510236740112
epoch: 12, step: 49
	action: tensor([[ 0.4015, -0.7579,  0.0177,  0.2179,  0.0340,  0.0994,  0.1361]],
       dtype=torch.float64)
	q_value: tensor([[-29.7792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.057958256083463944, distance: 1.1106872005913273 entropy 1.2378510236740112
epoch: 12, step: 50
	action: tensor([[-0.2163, -0.6100, -1.0983, -0.1757, -1.8001, -0.2271,  0.8049]],
       dtype=torch.float64)
	q_value: tensor([[-28.1892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2933486924853641, distance: 1.3014111025569923 entropy 1.2378510236740112
epoch: 12, step: 51
	action: tensor([[-0.1212,  0.6688,  1.4395, -1.1234,  1.1194,  1.0209,  1.4692]],
       dtype=torch.float64)
	q_value: tensor([[-40.5013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 52
	action: tensor([[-0.0377, -0.3719,  0.7127,  0.3343,  0.1924,  0.6765,  1.0037]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5273091439479227, distance: 0.7867654749973728 entropy 1.2378510236740112
epoch: 12, step: 53
	action: tensor([[ 0.7342, -2.2263,  1.5810, -0.3752, -0.2599, -1.3014,  1.4537]],
       dtype=torch.float64)
	q_value: tensor([[-32.9307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 54
	action: tensor([[-1.2725, -0.4611,  0.5818, -0.0653,  1.8535,  0.3344, -0.3240]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4901396709833392, distance: 1.805795418621198 entropy 1.2378510236740112
epoch: 12, step: 55
	action: tensor([[-1.9206, -1.2291,  0.7327, -0.3963, -0.4845,  0.0500, -0.6829]],
       dtype=torch.float64)
	q_value: tensor([[-41.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 56
	action: tensor([[ 0.1076, -1.2683,  0.4345, -0.2487,  0.8136,  1.7227, -1.5546]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07459276235177148, distance: 1.1862566194363076 entropy 1.2378510236740112
epoch: 12, step: 57
	action: tensor([[-0.0177, -1.1316,  1.1032, -0.6948,  0.8452,  0.1675,  1.3275]],
       dtype=torch.float64)
	q_value: tensor([[-47.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1015408253161558, distance: 1.6589206333852744 entropy 1.2378510236740112
epoch: 12, step: 58
	action: tensor([[ 1.7500, -0.9174,  0.0294,  0.9957,  0.3188, -0.7634,  1.5283]],
       dtype=torch.float64)
	q_value: tensor([[-37.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048589232691716666, distance: 1.1161966746080896 entropy 1.2378510236740112
epoch: 12, step: 59
	action: tensor([[ 1.5366, -2.0741,  1.7957, -0.3051,  0.2440, -0.8255,  1.5808]],
       dtype=torch.float64)
	q_value: tensor([[-45.1833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 60
	action: tensor([[-0.5345,  0.1597,  1.0103,  0.2450,  0.8389,  0.1844,  0.3631]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0482647073718947, distance: 1.1163870251964463 entropy 1.2378510236740112
epoch: 12, step: 61
	action: tensor([[ 0.4294, -1.0929,  0.4434,  0.1009, -0.8961, -0.4728,  0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-31.8282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5644910757690689, distance: 1.4313414177874992 entropy 1.2378510236740112
epoch: 12, step: 62
	action: tensor([[ 0.0378, -0.7872, -0.6985,  1.1060,  1.7572,  0.2130,  1.0665]],
       dtype=torch.float64)
	q_value: tensor([[-32.7785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013688571207447708, distance: 1.1521498518244353 entropy 1.2378510236740112
epoch: 12, step: 63
	action: tensor([[-0.3189, -1.5913,  0.3367,  0.5294, -0.4188, -0.0665,  0.4952]],
       dtype=torch.float64)
	q_value: tensor([[-43.3068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 64
	action: tensor([[ 0.5386, -0.6577, -0.8651, -0.2588,  0.2899,  0.9517, -1.5109]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26778051482010345, distance: 0.9792133689435535 entropy 1.2378510236740112
epoch: 12, step: 65
	action: tensor([[ 0.1990, -0.9746,  0.0948,  1.0855,  1.4637, -0.0197, -0.8501]],
       dtype=torch.float64)
	q_value: tensor([[-37.3412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38614760389111347, distance: 0.8965792240492562 entropy 1.2378510236740112
epoch: 12, step: 66
	action: tensor([[ 1.6889,  0.0566, -0.7869, -1.2064, -0.0592, -0.4754,  1.0060]],
       dtype=torch.float64)
	q_value: tensor([[-40.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3737526726526943, distance: 1.3412538093977704 entropy 1.2378510236740112
epoch: 12, step: 67
	action: tensor([[ 0.3224, -2.5241,  1.1171, -1.4130, -0.2488, -0.2911,  1.3279]],
       dtype=torch.float64)
	q_value: tensor([[-39.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 68
	action: tensor([[-0.9447, -0.2134, -0.6379, -1.4222, -2.0424, -0.4655,  1.2909]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20356038974003998, distance: 1.2554246058590481 entropy 1.2378510236740112
epoch: 12, step: 69
	action: tensor([[ 0.5751, -0.8322,  0.3662, -0.1434,  0.7759,  0.3760, -0.3789]],
       dtype=torch.float64)
	q_value: tensor([[-47.8847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17463813362195535, distance: 1.2402485876144322 entropy 1.2378510236740112
epoch: 12, step: 70
	action: tensor([[ 2.0135, -2.0801, -0.0376,  0.0488, -0.2759,  0.1442,  1.1105]],
       dtype=torch.float64)
	q_value: tensor([[-32.0680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 71
	action: tensor([[ 0.9122, -0.4066,  0.2396,  0.0276,  1.1187, -0.8756,  1.6194]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3859257091825157, distance: 0.8967412566579511 entropy 1.2378510236740112
epoch: 12, step: 72
	action: tensor([[-0.3720, -1.8191, -0.4683, -0.0059, -0.5567,  0.3564, -0.0357]],
       dtype=torch.float64)
	q_value: tensor([[-38.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 73
	action: tensor([[ 0.5178,  0.2533,  1.2772, -0.3525, -0.8025,  0.5788, -1.0391]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 74
	action: tensor([[ 0.8436, -0.5178, -1.7845,  0.9330,  1.7774,  0.3973, -1.3426]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29388868340352436, distance: 0.9615973973151205 entropy 1.2378510236740112
epoch: 12, step: 75
	action: tensor([[-1.4666,  0.6671,  0.3713, -0.5604,  1.1052, -0.1426, -1.4293]],
       dtype=torch.float64)
	q_value: tensor([[-48.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2782168650834382, distance: 1.7272461150136948 entropy 1.2378510236740112
epoch: 12, step: 76
	action: tensor([[-0.7146, -1.3619,  0.0094,  0.9393, -0.3532, -0.1793, -0.9408]],
       dtype=torch.float64)
	q_value: tensor([[-39.3109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8364019821940847, distance: 1.5507456432348505 entropy 1.2378510236740112
epoch: 12, step: 77
	action: tensor([[ 0.7271, -1.1722,  0.8078,  0.1440, -0.7610,  0.7269, -0.3064]],
       dtype=torch.float64)
	q_value: tensor([[-38.7163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14225593184537844, distance: 1.2230336447025425 entropy 1.2378510236740112
epoch: 12, step: 78
	action: tensor([[ 0.1644, -1.6416,  2.2884, -0.2354, -1.1506, -0.4848,  1.4744]],
       dtype=torch.float64)
	q_value: tensor([[-36.8861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 79
	action: tensor([[ 0.4627, -0.3311, -0.1881, -0.2292, -0.2744,  0.7218,  0.7018]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41112902766342097, distance: 0.8781460793723029 entropy 1.2378510236740112
epoch: 12, step: 80
	action: tensor([[ 1.3539, -1.8546,  0.8449, -0.1479, -0.1324, -0.2308,  1.6230]],
       dtype=torch.float64)
	q_value: tensor([[-30.9101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 81
	action: tensor([[-0.8789, -1.1172,  0.3430,  0.0477,  0.6966, -0.5203,  0.1877]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3522825562845853, distance: 1.755098266514568 entropy 1.2378510236740112
epoch: 12, step: 82
	action: tensor([[ 1.7042, -1.9970,  0.2400, -0.4629, -0.5137, -1.4895,  0.6301]],
       dtype=torch.float64)
	q_value: tensor([[-32.8594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 83
	action: tensor([[ 0.8311, -1.1255, -2.1018, -1.5600,  0.9040, -0.3958,  0.6867]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 84
	action: tensor([[ 0.7788, -0.6843, -0.4145,  1.5644,  0.2394, -0.6706,  0.6753]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7618373193042395, distance: 0.5584618983368869 entropy 1.2378510236740112
epoch: 12, step: 85
	action: tensor([[-0.2165, -1.3069, -0.1025, -0.3544,  0.4414, -1.2634,  1.2087]],
       dtype=torch.float64)
	q_value: tensor([[-40.2604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8268740180194643, distance: 1.5467174772496923 entropy 1.2378510236740112
epoch: 12, step: 86
	action: tensor([[-0.4285,  0.0440,  0.6959, -0.4962,  0.3384, -0.5299,  0.3833]],
       dtype=torch.float64)
	q_value: tensor([[-36.9630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6432902312061741, distance: 1.466944996073735 entropy 1.2378510236740112
epoch: 12, step: 87
	action: tensor([[ 0.5256, -0.9671,  1.1826, -0.2378,  0.0112, -0.3026,  0.3146]],
       dtype=torch.float64)
	q_value: tensor([[-27.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4941462436666888, distance: 1.3987923449970092 entropy 1.2378510236740112
epoch: 12, step: 88
	action: tensor([[ 1.3976,  0.6089, -0.2497, -1.5861, -0.5422, -0.3840,  1.2027]],
       dtype=torch.float64)
	q_value: tensor([[-31.4887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16915715419000255, distance: 1.043076438763802 entropy 1.2378510236740112
epoch: 12, step: 89
	action: tensor([[ 0.0971, -2.0024,  0.7377, -0.8066,  1.6595, -0.9636,  2.6986]],
       dtype=torch.float64)
	q_value: tensor([[-41.0879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 90
	action: tensor([[ 0.2001,  0.3645, -0.3267,  1.4397, -0.3328, -0.2526,  0.5402]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 91
	action: tensor([[-0.2474, -1.8022,  1.5321, -0.4082, -0.1824,  0.0152,  0.3497]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 92
	action: tensor([[ 0.4425, -1.7823, -0.1931,  0.5502,  0.4271,  0.8907,  0.6452]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 93
	action: tensor([[ 1.4776, -1.1009,  0.6368,  0.1780,  0.7182, -0.0123,  0.7080]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5441638667529396, distance: 1.4220124088262198 entropy 1.2378510236740112
epoch: 12, step: 94
	action: tensor([[ 1.4973, -0.7309,  0.7997, -0.5305,  0.4031, -0.6421,  1.0529]],
       dtype=torch.float64)
	q_value: tensor([[-38.8588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11631342125342381, distance: 1.2090653256783135 entropy 1.2378510236740112
epoch: 12, step: 95
	action: tensor([[ 0.7084, -0.3526,  1.9145, -0.5149,  0.9039, -0.1409,  1.1156]],
       dtype=torch.float64)
	q_value: tensor([[-36.6648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.489656738598624, distance: 0.8175002339528916 entropy 1.2378510236740112
epoch: 12, step: 96
	action: tensor([[ 0.9079, -0.2467,  1.2766, -1.0425, -0.2318,  0.6784,  1.7791]],
       dtype=torch.float64)
	q_value: tensor([[-38.2438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4489634328181352, distance: 0.84946776500355 entropy 1.2378510236740112
epoch: 12, step: 97
	action: tensor([[ 2.1704, -3.2732,  1.0703,  1.2044, -1.0657, -0.3227,  2.6103]],
       dtype=torch.float64)
	q_value: tensor([[-42.8844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 98
	action: tensor([[ 1.2351,  0.3324, -0.0339, -1.7155, -0.1226,  0.7764, -0.7257]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 99
	action: tensor([[ 0.1489, -0.2029, -0.1033, -0.1181, -0.7469, -0.2612, -0.2171]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24168908081664742, distance: 0.9965069869208549 entropy 1.2378510236740112
epoch: 12, step: 100
	action: tensor([[-0.4855, -0.1071, -0.7091,  0.5876,  1.8945,  0.8926, -0.4530]],
       dtype=torch.float64)
	q_value: tensor([[-25.4899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04901148035334102, distance: 1.1159489562949245 entropy 1.2378510236740112
epoch: 12, step: 101
	action: tensor([[-0.7675, -1.7008,  0.2336, -0.8250,  0.7356,  2.5745,  0.2785]],
       dtype=torch.float64)
	q_value: tensor([[-41.8060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 102
	action: tensor([[ 1.5564, -0.1249,  0.7842,  0.1836,  0.7987,  1.4014,  1.5268]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 103
	action: tensor([[ 1.0149,  0.1940,  0.1220,  0.3631, -0.8443, -1.0884,  0.4846]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 104
	action: tensor([[ 0.4373,  0.5248, -0.2644, -0.2814,  0.0193,  0.5374,  0.8006]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 105
	action: tensor([[ 0.5055,  2.0050,  0.3864, -0.2990,  0.7405,  0.6608, -0.6856]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 106
	action: tensor([[-0.5568, -0.6489, -0.1961, -1.2341, -0.2846, -0.2276,  0.9656]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5234965430384655, distance: 1.4124641372987117 entropy 1.2378510236740112
epoch: 12, step: 107
	action: tensor([[ 0.5291, -0.2243,  0.9392, -0.2340,  0.5145, -0.2248,  0.6370]],
       dtype=torch.float64)
	q_value: tensor([[-34.5806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025516536639814, distance: 0.955680484803335 entropy 1.2378510236740112
epoch: 12, step: 108
	action: tensor([[ 0.0966, -1.8038, -0.1455,  0.1756,  1.1440,  0.2843,  1.1888]],
       dtype=torch.float64)
	q_value: tensor([[-29.3120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 109
	action: tensor([[ 0.3864,  0.2290,  2.2055, -1.0294, -1.2054,  1.3487,  0.9325]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 110
	action: tensor([[ 1.0869, -0.4440,  0.1993, -0.6210,  0.3425,  0.5724,  0.9801]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08332392791291676, distance: 1.0956318301840875 entropy 1.2378510236740112
epoch: 12, step: 111
	action: tensor([[ 1.5186, -1.9669,  3.4748,  1.1749,  0.7061, -1.0804,  1.6472]],
       dtype=torch.float64)
	q_value: tensor([[-35.4440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 112
	action: tensor([[ 1.3222,  1.0486,  0.5742,  1.3645, -0.1647,  0.6279,  1.6597]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 113
	action: tensor([[ 0.7653, -0.1922,  0.0146, -0.5626,  0.4013, -0.0314,  0.2537]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23065211731575175, distance: 1.0037327039872113 entropy 1.2378510236740112
epoch: 12, step: 114
	action: tensor([[ 0.1185, -1.3329,  1.3661,  0.4697, -0.5966,  2.3751, -0.3148]],
       dtype=torch.float64)
	q_value: tensor([[-27.5217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4070511176695345, distance: 0.8811813983469253 entropy 1.2378510236740112
epoch: 12, step: 115
	action: tensor([[ 1.3031, -1.0803,  1.8116, -0.7588,  1.1118, -0.3943,  0.4836]],
       dtype=torch.float64)
	q_value: tensor([[-49.4015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09954884869673841, distance: 1.0858923463459775 entropy 1.2378510236740112
epoch: 12, step: 116
	action: tensor([[ 2.0367, -0.4282,  1.9389,  0.9430, -0.3410,  0.6927,  0.9055]],
       dtype=torch.float64)
	q_value: tensor([[-40.7367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 117
	action: tensor([[ 0.8430,  0.4203, -0.1156, -0.6152,  0.8543, -0.2970,  0.8541]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 118
	action: tensor([[-0.4436, -1.2161, -0.6014,  0.6343,  0.4342,  0.3036, -0.3565]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8842377651690001, distance: 1.5708132113506128 entropy 1.2378510236740112
epoch: 12, step: 119
	action: tensor([[-1.2111,  0.2437,  0.1878, -0.0013, -0.0769, -0.0404, -0.7764]],
       dtype=torch.float64)
	q_value: tensor([[-34.8842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.011854467699247, distance: 1.6231362390687374 entropy 1.2378510236740112
epoch: 12, step: 120
	action: tensor([[ 1.5607,  1.1453,  1.2703,  0.3401,  1.2457,  0.3487, -0.3120]],
       dtype=torch.float64)
	q_value: tensor([[-30.7185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 121
	action: tensor([[ 1.2875, -1.1946, -0.8340, -0.5666,  1.1794, -0.8056, -0.1455]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9466602534262814, distance: 1.5966207681265072 entropy 1.2378510236740112
epoch: 12, step: 122
	action: tensor([[ 1.6767, -0.5149, -0.4359, -0.6803, -0.6013,  0.8372,  0.6014]],
       dtype=torch.float64)
	q_value: tensor([[-41.9213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22884352320577084, distance: 1.2685423939671252 entropy 1.2378510236740112
epoch: 12, step: 123
	action: tensor([[ 1.1400, -1.6782,  0.7646,  0.0489, -0.9131, -0.4101,  2.6152]],
       dtype=torch.float64)
	q_value: tensor([[-40.1232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 124
	action: tensor([[ 0.7056,  1.0168, -0.3348,  0.1819, -0.8219, -0.0519,  0.4132]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 125
	action: tensor([[ 0.5936, -0.7373,  0.2453, -1.1642, -1.4527,  1.4073,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21563389425909207, distance: 1.2617057827288118 entropy 1.2378510236740112
epoch: 12, step: 126
	action: tensor([[ 2.2168, -2.0690,  1.2368,  0.7634, -0.9762,  0.5941,  1.8822]],
       dtype=torch.float64)
	q_value: tensor([[-43.0081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 127
	action: tensor([[ 0.4843, -0.7557,  0.1136, -0.0751, -0.5062,  1.6266,  1.1449]],
       dtype=torch.float64)
	q_value: tensor([[-40.5907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42987651500808066, distance: 0.8640545447945791 entropy 1.2378510236740112
LOSS epoch 12 actor 401.90750151960776 critic 121.1846211396848 
epoch: 13, step: 0
	action: tensor([[ 1.0628, -3.2460,  2.9693, -2.4041, -0.6345,  0.7150,  1.6789]],
       dtype=torch.float64)
	q_value: tensor([[-38.9670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 1
	action: tensor([[ 0.5940, -0.7614,  0.1526, -0.1826, -0.5614,  0.4416, -1.2043]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03162905785554382, distance: 1.1623006387333483 entropy 1.2378510236740112
epoch: 13, step: 2
	action: tensor([[-0.2951, -0.1745,  0.9520,  0.0595, -1.3709, -1.0322, -0.4293]],
       dtype=torch.float64)
	q_value: tensor([[-30.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.039733989451936624, distance: 1.1668574786121597 entropy 1.2378510236740112
epoch: 13, step: 3
	action: tensor([[ 0.3028, -1.2370, -0.0469, -1.4526,  1.0721, -1.0292,  1.1233]],
       dtype=torch.float64)
	q_value: tensor([[-32.9882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5992119156991338, distance: 1.4471371806205684 entropy 1.2378510236740112
epoch: 13, step: 4
	action: tensor([[ 0.8330, -0.3315,  1.4798, -1.1444,  0.8295, -3.3861, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-39.3155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 5
	action: tensor([[ 0.3915, -0.6008,  0.9935,  0.3055,  1.3846,  0.8426, -0.4258]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19577539911440023, distance: 1.0262315687093893 entropy 1.2378510236740112
epoch: 13, step: 6
	action: tensor([[-0.0239, -1.4063,  1.3876, -1.9444, -0.1890, -0.5699,  0.5311]],
       dtype=torch.float64)
	q_value: tensor([[-36.5312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 7
	action: tensor([[ 1.0486, -0.5198, -0.0113,  0.5429,  1.0510, -0.5252,  0.6799]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5618012540789596, distance: 0.7575167816599424 entropy 1.2378510236740112
epoch: 13, step: 8
	action: tensor([[-0.6065, -1.5926,  1.0168, -0.7954, -0.0276,  0.1161,  0.7422]],
       dtype=torch.float64)
	q_value: tensor([[-33.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 9
	action: tensor([[ 1.1123, -0.5467,  1.0420,  0.3453, -0.7429,  1.4612, -0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6805805982184732, distance: 0.6467513430702913 entropy 1.2378510236740112
epoch: 13, step: 10
	action: tensor([[ 1.4262,  0.1587,  2.0662, -1.1417,  0.7687,  0.0058,  0.9631]],
       dtype=torch.float64)
	q_value: tensor([[-37.8943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9823428073640249, distance: 0.15206088746827376 entropy 1.2378510236740112
epoch: 13, step: 11
	action: tensor([[ 2.3104, -1.7552,  1.7541, -0.2805, -0.2247, -0.3971,  1.9737]],
       dtype=torch.float64)
	q_value: tensor([[-39.1746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 12
	action: tensor([[ 0.2681, -1.1052,  0.6524, -0.8459, -0.2230, -0.3849, -0.9705]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0305610435271033, distance: 1.6306648816698042 entropy 1.2378510236740112
epoch: 13, step: 13
	action: tensor([[ 0.8400, -1.5682,  1.1783, -0.1136,  1.0406,  0.4773,  0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-31.7487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 14
	action: tensor([[-0.0667, -1.0254,  0.1465,  0.3761,  0.7508,  0.4864, -1.0436]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17889576427596077, distance: 1.2424942766998868 entropy 1.2378510236740112
epoch: 13, step: 15
	action: tensor([[-0.3249, -1.4655,  0.2164, -1.1042, -1.1755, -0.2175,  0.4939]],
       dtype=torch.float64)
	q_value: tensor([[-34.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 16
	action: tensor([[ 0.0104, -0.8394,  1.5789, -1.5382, -0.4638, -0.3866, -0.6020]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 17
	action: tensor([[-1.2328, -0.0272,  0.9161,  0.0800, -1.8170, -0.1321, -1.5624]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.266400177765704, distance: 1.7227608391430955 entropy 1.2378510236740112
epoch: 13, step: 18
	action: tensor([[ 0.1219, -0.1247, -0.1286,  0.5749,  1.4110, -0.7649,  0.9280]],
       dtype=torch.float64)
	q_value: tensor([[-42.4176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3171924062975986, distance: 0.9455965183021671 entropy 1.2378510236740112
epoch: 13, step: 19
	action: tensor([[-0.3951, -0.0659,  1.0171, -0.2476, -0.5166, -0.2231,  1.0380]],
       dtype=torch.float64)
	q_value: tensor([[-32.7189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5383417051821127, distance: 1.4193290780774868 entropy 1.2378510236740112
epoch: 13, step: 20
	action: tensor([[ 1.2047, -0.8685,  0.5538, -1.1571,  1.0415, -0.8807,  0.3584]],
       dtype=torch.float64)
	q_value: tensor([[-28.8666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1791406608047299, distance: 1.242623324043934 entropy 1.2378510236740112
epoch: 13, step: 21
	action: tensor([[ 0.3389,  0.3984,  1.8896, -0.5472,  1.1522, -0.0775,  0.8109]],
       dtype=torch.float64)
	q_value: tensor([[-35.6612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 22
	action: tensor([[ 0.2440,  1.5294, -1.0462,  1.3331, -0.2029,  0.3687,  1.2927]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 23
	action: tensor([[-1.0336, -1.3635,  0.1399, -0.4309, -1.0773, -1.4938,  1.4991]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30002942959663303, distance: 1.304767964780728 entropy 1.2378510236740112
epoch: 13, step: 24
	action: tensor([[ 0.7334, -1.1601,  0.5902, -0.1461, -0.7209,  1.0172, -0.4070]],
       dtype=torch.float64)
	q_value: tensor([[-39.9887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18056722480833787, distance: 1.243374780559956 entropy 1.2378510236740112
epoch: 13, step: 25
	action: tensor([[ 0.2094, -1.1598,  0.8620, -0.5052, -0.1010,  0.1765,  1.4095]],
       dtype=torch.float64)
	q_value: tensor([[-35.2005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9242881407437233, distance: 1.5874196243692935 entropy 1.2378510236740112
epoch: 13, step: 26
	action: tensor([[ 2.0126, -1.4759,  1.6752,  0.0486, -0.4565,  0.3153,  1.8461]],
       dtype=torch.float64)
	q_value: tensor([[-34.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 27
	action: tensor([[-0.0684, -0.5852,  0.7972,  1.3912,  0.0850, -1.5545, -0.1981]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35762608633930726, distance: 0.9171716907072278 entropy 1.2378510236740112
epoch: 13, step: 28
	action: tensor([[ 0.8402, -1.7330, -0.5988,  0.0645, -0.1411, -0.0962,  0.2236]],
       dtype=torch.float64)
	q_value: tensor([[-37.6829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 29
	action: tensor([[ 1.1066, -0.6905,  0.3190, -0.4596, -0.6501,  0.2738,  0.8912]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17366306853682656, distance: 1.239733716639061 entropy 1.2378510236740112
epoch: 13, step: 30
	action: tensor([[ 2.3108, -1.3035,  1.2893,  0.0633, -0.2983,  0.2733,  1.9791]],
       dtype=torch.float64)
	q_value: tensor([[-32.5611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 31
	action: tensor([[-0.9753, -0.4927, -0.3727, -0.3492,  0.1163,  1.0778, -0.1239]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9659395708516834, distance: 1.6045075885401339 entropy 1.2378510236740112
epoch: 13, step: 32
	action: tensor([[-0.3559, -0.7909, -0.2300, -0.4462, -0.7958, -0.3192,  0.7722]],
       dtype=torch.float64)
	q_value: tensor([[-32.5142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6992262164990548, distance: 1.4917027599150507 entropy 1.2378510236740112
epoch: 13, step: 33
	action: tensor([[ 0.2476, -0.0444,  2.1001,  0.3083,  1.3573, -0.5571,  0.6358]],
       dtype=torch.float64)
	q_value: tensor([[-30.5095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6854096970995468, distance: 0.6418438127634708 entropy 1.2378510236740112
epoch: 13, step: 34
	action: tensor([[-0.0132, -1.2378, -0.4206, -1.4047, -1.4554,  0.0237,  1.6605]],
       dtype=torch.float64)
	q_value: tensor([[-38.7742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5818196854891651, distance: 1.4392464975230133 entropy 1.2378510236740112
epoch: 13, step: 35
	action: tensor([[ 1.3930, -2.5781,  3.2669, -0.8269,  0.1467,  1.1166,  2.0132]],
       dtype=torch.float64)
	q_value: tensor([[-43.7193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 36
	action: tensor([[-1.0321,  0.0346,  0.7255,  0.6055, -0.3861,  0.6981,  0.3391]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06129629744039611, distance: 1.1788947070353355 entropy 1.2378510236740112
epoch: 13, step: 37
	action: tensor([[-0.6194, -0.7266,  1.8588,  0.4216,  0.4386, -0.1817,  1.2385]],
       dtype=torch.float64)
	q_value: tensor([[-30.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6404423699415922, distance: 1.4656733194956517 entropy 1.2378510236740112
epoch: 13, step: 38
	action: tensor([[ 1.3603, -0.5751,  0.7202, -1.3572, -0.3838, -0.4379,  0.0410]],
       dtype=torch.float64)
	q_value: tensor([[-35.4127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1420876992187068, distance: 1.2229435765605006 entropy 1.2378510236740112
epoch: 13, step: 39
	action: tensor([[ 1.3036e+00, -7.7968e-01,  1.1658e+00, -1.6383e+00,  5.4945e-01,
          3.5484e-04,  2.0522e-01]], dtype=torch.float64)
	q_value: tensor([[-32.2598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10568074442280462, distance: 1.082188673307077 entropy 1.2378510236740112
epoch: 13, step: 40
	action: tensor([[ 0.1861, -1.0054,  0.9884, -1.5694, -0.5884, -0.3816,  0.3493]],
       dtype=torch.float64)
	q_value: tensor([[-35.5623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7404867740707972, distance: 1.5097048734472702 entropy 1.2378510236740112
epoch: 13, step: 41
	action: tensor([[ 1.7450, -0.1732,  1.2106,  0.2240,  0.6393, -1.0969,  0.2116]],
       dtype=torch.float64)
	q_value: tensor([[-33.7833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5315733727664367, distance: 0.7832086590699747 entropy 1.2378510236740112
epoch: 13, step: 42
	action: tensor([[ 0.9753, -0.6748,  1.3645, -0.3837, -0.5677, -0.6163, -0.2105]],
       dtype=torch.float64)
	q_value: tensor([[-35.0554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011311600669107857, distance: 1.15079823671857 entropy 1.2378510236740112
epoch: 13, step: 43
	action: tensor([[ 0.3166,  0.0527,  1.0742, -1.0229,  0.1442,  1.0249,  1.0158]],
       dtype=torch.float64)
	q_value: tensor([[-31.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37670522650814975, distance: 0.9034485722169074 entropy 1.2378510236740112
epoch: 13, step: 44
	action: tensor([[ 3.1432, -2.1867,  1.8756, -1.8702, -0.2509, -0.6734,  0.9311]],
       dtype=torch.float64)
	q_value: tensor([[-35.7181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 45
	action: tensor([[ 0.0871, -1.3005,  0.2039, -1.6341, -2.0041, -0.0619, -0.2019]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.806303867689875, distance: 1.5379849978417794 entropy 1.2378510236740112
epoch: 13, step: 46
	action: tensor([[ 0.5833, -1.3127,  1.4384, -1.7576,  0.9299, -1.1427,  1.7153]],
       dtype=torch.float64)
	q_value: tensor([[-42.1164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 47
	action: tensor([[ 1.2475,  1.3121,  0.8414,  0.2627,  0.9954, -1.3119, -0.3332]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 48
	action: tensor([[ 1.3205, -0.7416, -0.6331,  0.7207, -0.3486,  0.2115, -0.0346]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4979903143896295, distance: 0.8107981357352501 entropy 1.2378510236740112
epoch: 13, step: 49
	action: tensor([[ 2.1843, -0.9104,  1.7471,  0.5579,  0.7273, -0.4468,  1.2027]],
       dtype=torch.float64)
	q_value: tensor([[-33.4983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 50
	action: tensor([[ 1.3511, -1.8635,  0.6247, -0.6790,  1.1758, -0.1183, -0.9401]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 51
	action: tensor([[-0.7763, -0.2060,  0.9731, -0.0179, -0.7508, -1.2429,  0.2963]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8376690009082293, distance: 1.5512805165853092 entropy 1.2378510236740112
epoch: 13, step: 52
	action: tensor([[ 0.6956, -1.3055,  2.4969, -0.7812, -0.0841,  0.8373, -1.4182]],
       dtype=torch.float64)
	q_value: tensor([[-30.4585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3047637498347535, distance: 1.3071415940182327 entropy 1.2378510236740112
epoch: 13, step: 53
	action: tensor([[ 0.9129, -0.6456,  0.4902, -1.1918, -0.2007, -0.3512, -0.0731]],
       dtype=torch.float64)
	q_value: tensor([[-43.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47304753531821864, distance: 1.3888811195665844 entropy 1.2378510236740112
epoch: 13, step: 54
	action: tensor([[-0.3818, -0.8467, -0.1134,  0.2101, -0.2137,  0.0840, -0.9638]],
       dtype=torch.float64)
	q_value: tensor([[-29.5986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7375893963880222, distance: 1.5084477518162038 entropy 1.2378510236740112
epoch: 13, step: 55
	action: tensor([[-0.2727, -0.2384,  0.0523,  0.9944,  0.4491, -0.3172,  0.4153]],
       dtype=torch.float64)
	q_value: tensor([[-28.9489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 56
	action: tensor([[ 0.1440, -1.3564,  1.0547, -1.6656,  0.3611, -0.1469,  1.5442]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7751540531440779, distance: 1.5246660063619732 entropy 1.2378510236740112
epoch: 13, step: 57
	action: tensor([[ 1.7789, -0.5388,  3.7495, -1.4543,  0.3493, -0.6338,  1.4782]],
       dtype=torch.float64)
	q_value: tensor([[-38.6641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 58
	action: tensor([[ 0.0944,  0.2548,  0.7169, -0.1259, -0.6088, -1.2638, -1.6674]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32742997956266395, distance: 0.9384809160928302 entropy 1.2378510236740112
epoch: 13, step: 59
	action: tensor([[-0.8183,  0.0308, -0.5361,  1.9460,  0.5874, -0.6950, -0.4996]],
       dtype=torch.float64)
	q_value: tensor([[-34.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 60
	action: tensor([[ 0.5768, -0.0638, -1.3109, -0.0300,  1.6570,  0.9891,  0.8860]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 61
	action: tensor([[-0.4948,  0.3045, -0.0244, -1.2221, -0.6047, -0.0751,  0.6581]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3347609134085672, distance: 1.3220821251325094 entropy 1.2378510236740112
epoch: 13, step: 62
	action: tensor([[ 0.6368, -0.2577, -1.0098, -0.9504,  1.4099,  0.8476,  2.5294]],
       dtype=torch.float64)
	q_value: tensor([[-30.0705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5464111688353381, distance: 0.7707044550091017 entropy 1.2378510236740112
epoch: 13, step: 63
	action: tensor([[ 1.2633, -3.1870,  3.8663, -2.1068,  0.6106, -1.7854,  4.1082]],
       dtype=torch.float64)
	q_value: tensor([[-51.3802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 64
	action: tensor([[ 0.2656,  1.0965,  0.1103,  0.3819,  0.2401, -1.1121,  0.9622]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 65
	action: tensor([[-0.4045, -0.7027,  1.5965,  0.2503, -1.1383,  1.0292, -0.3575]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02159027929159163, distance: 1.156631642154059 entropy 1.2378510236740112
epoch: 13, step: 66
	action: tensor([[ 1.1851, -0.6784,  0.4964, -0.8270,  1.2217, -0.0699,  1.0481]],
       dtype=torch.float64)
	q_value: tensor([[-38.0786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35971893186970494, distance: 1.3343853506627508 entropy 1.2378510236740112
epoch: 13, step: 67
	action: tensor([[ 0.1809, -2.3479,  0.0265, -0.9277,  0.5432, -0.3294,  0.8998]],
       dtype=torch.float64)
	q_value: tensor([[-36.0620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 68
	action: tensor([[ 0.4157,  0.0381, -0.3380,  0.2812,  0.5039, -0.8579, -0.4035]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 69
	action: tensor([[ 0.7035,  0.3629,  0.3401, -2.3163,  0.6856, -0.3866,  0.4038]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0234131550086224, distance: 1.1308685549653412 entropy 1.2378510236740112
epoch: 13, step: 70
	action: tensor([[ 0.6231,  0.6353,  0.8710, -1.7416,  2.0486, -0.5126,  1.5224]],
       dtype=torch.float64)
	q_value: tensor([[-37.4759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36025620446008444, distance: 0.9152921435439703 entropy 1.2378510236740112
epoch: 13, step: 71
	action: tensor([[ 1.3089, -2.5719,  0.6349, -0.2386, -1.8824, -0.2261,  2.0378]],
       dtype=torch.float64)
	q_value: tensor([[-44.6139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 72
	action: tensor([[ 0.8109, -0.1302, -0.5502,  0.1831, -2.0522, -0.0389,  0.9561]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 73
	action: tensor([[ 0.5884,  0.5450, -0.2111, -1.4031,  0.6479, -1.2067, -0.4283]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 74
	action: tensor([[ 1.9807, -1.4055, -0.4442, -0.2944, -0.2853,  0.9099,  1.9387]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5026356454251311, distance: 1.4027605276140716 entropy 1.2378510236740112
epoch: 13, step: 75
	action: tensor([[ 2.9958, -2.7165,  1.5190, -4.4298,  1.8717, -0.6847,  3.4260]],
       dtype=torch.float64)
	q_value: tensor([[-47.6340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 76
	action: tensor([[-0.2651, -0.8131,  1.1148,  0.4698,  0.6916, -0.0996,  0.2267]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21815167421485038, distance: 1.2630117081306909 entropy 1.2378510236740112
epoch: 13, step: 77
	action: tensor([[-0.4541,  0.1301,  0.2264,  0.2366, -0.2991, -0.0557,  0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-30.5374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02451397584808601, distance: 1.130231010701697 entropy 1.2378510236740112
epoch: 13, step: 78
	action: tensor([[-9.1047e-01,  3.4111e-02, -1.0039e-03, -5.0816e-01,  1.6568e+00,
          2.4965e-01,  1.4195e+00]], dtype=torch.float64)
	q_value: tensor([[-23.5034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8515410996187145, distance: 1.557124620626563 entropy 1.2378510236740112
epoch: 13, step: 79
	action: tensor([[ 2.8935, -1.3146, -0.6301, -0.6794, -0.1119,  0.1833,  1.8037]],
       dtype=torch.float64)
	q_value: tensor([[-39.2209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 80
	action: tensor([[ 0.1422,  1.9058,  0.1017, -0.5993, -0.7726, -1.1694, -0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 81
	action: tensor([[-0.1049,  0.1669, -0.2163,  0.4513,  0.1654, -0.7263,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 82
	action: tensor([[ 0.4895, -1.0064, -0.4037, -1.5336,  0.5449,  0.0964,  0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6049767099973624, distance: 1.4497431340436568 entropy 1.2378510236740112
epoch: 13, step: 83
	action: tensor([[ 1.3635, -2.0729,  1.1468,  0.3568,  1.5187, -1.1142,  1.7076]],
       dtype=torch.float64)
	q_value: tensor([[-35.7408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 84
	action: tensor([[ 0.8248, -1.7493,  0.2990, -0.2655, -0.6177, -0.6991,  0.7962]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8781647933691974, distance: 1.56827977224 entropy 1.2378510236740112
epoch: 13, step: 85
	action: tensor([[ 0.5587, -1.1652,  0.9825, -0.1405, -0.0215, -1.0588,  1.7392]],
       dtype=torch.float64)
	q_value: tensor([[-33.3360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6521389492882206, distance: 1.4708892644189613 entropy 1.2378510236740112
epoch: 13, step: 86
	action: tensor([[ 1.2553, -1.4906,  0.4417, -0.1610, -0.0825,  1.2698, -0.3195]],
       dtype=torch.float64)
	q_value: tensor([[-36.1020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 87
	action: tensor([[ 0.5192,  1.1874, -0.5929,  0.2818,  0.5926, -0.1816, -1.0791]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 88
	action: tensor([[ 0.6360, -0.6956,  0.0543, -0.3550,  0.3227, -0.9910,  0.8807]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3816896504337668, distance: 1.3451228353958997 entropy 1.2378510236740112
epoch: 13, step: 89
	action: tensor([[ 1.1948, -1.5700, -0.0991,  0.0266,  0.0188, -0.4670,  0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-30.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 90
	action: tensor([[-0.2214,  1.4024, -0.0325,  0.2729, -1.0191, -0.6956,  0.8327]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 91
	action: tensor([[ 1.0699,  1.3542, -0.6908,  0.2285,  0.6187,  1.3377, -0.5925]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 92
	action: tensor([[ 1.3632, -0.0835,  0.6435,  1.3420, -0.3702, -1.8757,  0.7832]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 93
	action: tensor([[ 0.7809, -1.6738,  0.7602, -2.5041, -1.0571, -0.0930,  1.0028]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21133002950372393, distance: 1.259470312855427 entropy 1.2378510236740112
epoch: 13, step: 94
	action: tensor([[ 0.9973, -1.7570,  1.2471, -1.2936,  0.8934,  0.1155,  2.6295]],
       dtype=torch.float64)
	q_value: tensor([[-42.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 95
	action: tensor([[ 1.0937, -1.0993,  0.4804,  0.7665, -0.0308,  1.1021,  0.9917]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4932550281113546, distance: 0.8146131516608278 entropy 1.2378510236740112
epoch: 13, step: 96
	action: tensor([[ 0.2145, -1.9518,  2.6611, -0.3148,  0.5745, -0.8775,  2.2859]],
       dtype=torch.float64)
	q_value: tensor([[-39.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 97
	action: tensor([[ 1.1775, -1.0549,  0.1627, -0.0152,  1.0815, -0.5738, -0.0623]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45591096899390027, distance: 1.3807787742148119 entropy 1.2378510236740112
epoch: 13, step: 98
	action: tensor([[ 0.6106, -0.3989, -1.5834, -0.4989,  0.9719,  0.0989,  1.1482]],
       dtype=torch.float64)
	q_value: tensor([[-34.3983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4346365326350272, distance: 0.8604399456929849 entropy 1.2378510236740112
epoch: 13, step: 99
	action: tensor([[ 1.4152, -0.3932,  1.7427,  0.5829,  1.2772, -0.4649,  2.4878]],
       dtype=torch.float64)
	q_value: tensor([[-40.0090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05655890959941656, distance: 1.1115118240680357 entropy 1.2378510236740112
epoch: 13, step: 100
	action: tensor([[ 1.1575, -0.2358,  0.7249, -0.8543,  0.8991,  1.0663,  4.2521]],
       dtype=torch.float64)
	q_value: tensor([[-48.6744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0220906871351603 entropy 1.2378510236740112
epoch: 13, step: 101
	action: tensor([[ 0.2292, -0.6800, -0.6966,  2.4514,  0.4662,  0.2579,  0.4727]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4832395430443507, distance: 0.8226239132427178 entropy 1.2378510236740112
epoch: 13, step: 102
	action: tensor([[-0.0747, -1.2501,  1.5082, -0.5192, -0.2006,  0.0825,  1.1900]],
       dtype=torch.float64)
	q_value: tensor([[-41.0897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 103
	action: tensor([[ 1.0823,  0.1042,  0.3711, -1.4205,  1.4450, -0.6359, -0.3492]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.278214818829022, distance: 0.9722113210497748 entropy 1.2378510236740112
epoch: 13, step: 104
	action: tensor([[ 1.6564, -0.8609,  0.3116,  0.1932, -0.0265, -0.1390, -0.9715]],
       dtype=torch.float64)
	q_value: tensor([[-35.7504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3825563706958277, distance: 1.3455446603848031 entropy 1.2378510236740112
epoch: 13, step: 105
	action: tensor([[-0.4246, -1.5910,  1.3502,  1.2037, -0.0492, -0.5815,  1.1230]],
       dtype=torch.float64)
	q_value: tensor([[-34.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 106
	action: tensor([[ 0.8020, -1.7724, -0.0209, -0.2067,  0.3958, -0.6008,  1.2552]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 107
	action: tensor([[-0.2162, -0.1266,  0.9247, -0.6295, -0.2963,  0.5207, -0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31377282519487126, distance: 1.3116465778392066 entropy 1.2378510236740112
epoch: 13, step: 108
	action: tensor([[ 0.5934, -2.4122, -0.0496, -1.2423,  0.6126,  0.5438,  1.4096]],
       dtype=torch.float64)
	q_value: tensor([[-26.6755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 109
	action: tensor([[ 0.5680, -0.2487, -0.1690, -0.5381, -1.6986, -0.9013, -0.6687]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39579714216325845, distance: 0.8895043593169301 entropy 1.2378510236740112
epoch: 13, step: 110
	action: tensor([[-0.1906, -0.9650,  0.2738, -0.3357,  0.3324,  0.1225,  0.8408]],
       dtype=torch.float64)
	q_value: tensor([[-33.8104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8831417786833797, distance: 1.5703563049821796 entropy 1.2378510236740112
epoch: 13, step: 111
	action: tensor([[ 2.1489, -2.2501,  0.6926, -0.1596, -0.0150, -0.4172,  1.1951]],
       dtype=torch.float64)
	q_value: tensor([[-29.4457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 112
	action: tensor([[-0.3589,  1.2679, -0.6664,  0.5986, -0.4390,  0.3418,  0.3916]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 113
	action: tensor([[ 0.2059, -0.1748,  1.1321,  0.5851, -0.4198, -1.7436,  0.4223]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3288931027369675, distance: 0.9374595646174915 entropy 1.2378510236740112
epoch: 13, step: 114
	action: tensor([[-0.8791, -0.3185, -1.2348,  0.3313, -0.2304,  0.1817, -0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-35.0985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1948394227855932, distance: 1.6953449203983326 entropy 1.2378510236740112
epoch: 13, step: 115
	action: tensor([[ 1.2629, -0.5770, -0.2100, -1.5123,  0.7134, -0.5785, -0.5964]],
       dtype=torch.float64)
	q_value: tensor([[-29.1644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.508973471961458, distance: 1.4057157011081949 entropy 1.2378510236740112
epoch: 13, step: 116
	action: tensor([[-0.0479,  0.4062, -0.1653, -1.3425,  0.0105, -0.2421,  0.5127]],
       dtype=torch.float64)
	q_value: tensor([[-36.3996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13964645869690062, distance: 1.0614393126178696 entropy 1.2378510236740112
epoch: 13, step: 117
	action: tensor([[ 2.9503, -0.4445,  0.4456,  0.0379, -0.0066, -0.6360,  0.3989]],
       dtype=torch.float64)
	q_value: tensor([[-28.9739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 118
	action: tensor([[ 0.5504, -1.7771,  0.1411, -1.1939,  0.7792,  0.1870,  0.2817]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 119
	action: tensor([[ 0.9068,  0.5825, -0.4619, -1.0082,  0.3074, -0.5548, -0.2414]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 120
	action: tensor([[-1.1602, -1.9812,  0.7335, -0.2327,  0.4588,  0.7947, -0.6203]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 121
	action: tensor([[-0.1333, -0.4163, -0.9454,  0.7027,  1.2078, -0.7386, -0.7587]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42557270671954694, distance: 1.3663167084109566 entropy 1.2378510236740112
epoch: 13, step: 122
	action: tensor([[ 1.0505,  1.0323, -0.3837,  0.2930, -0.3295, -0.1269, -0.9307]],
       dtype=torch.float64)
	q_value: tensor([[-33.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 123
	action: tensor([[-5.8363e-01, -3.8516e-04,  6.2259e-01,  1.1859e+00, -1.0336e+00,
          1.2621e+00, -6.0092e-01]], dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 124
	action: tensor([[ 0.6413, -0.0412, -1.7947, -1.6131, -0.3328, -0.1224,  2.0687]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 125
	action: tensor([[ 1.0425, -1.2872, -1.4502,  0.9984,  0.7168, -0.2507, -0.1688]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28177896036205485, distance: 1.2955770986750892 entropy 1.2378510236740112
epoch: 13, step: 126
	action: tensor([[ 0.6913, -2.5683,  1.4590,  0.4160, -0.4520, -1.2628,  1.7585]],
       dtype=torch.float64)
	q_value: tensor([[-39.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 127
	action: tensor([[-0.1673,  0.4610,  0.1096, -0.3557, -1.4184,  0.3215, -0.1824]],
       dtype=torch.float64)
	q_value: tensor([[-37.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
LOSS epoch 13 actor 328.8772572359238 critic 166.10781720792437 
epoch: 14, step: 0
	action: tensor([[ 1.1744, -0.3606,  1.0434,  1.1634, -0.3127,  1.3478,  0.3842]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 1
	action: tensor([[-0.6320,  0.1575,  0.6480,  1.1697,  0.5906, -0.1534,  1.9393]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 2
	action: tensor([[ 1.0091, -0.0475, -0.5264, -1.4805,  1.2829, -0.4371,  0.2279]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 3
	action: tensor([[ 0.0083,  1.0653, -0.6568, -0.1816,  0.1604, -0.0536,  0.4140]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 4
	action: tensor([[-0.6401, -0.5194,  0.8163, -0.7432, -0.0452, -0.0092, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2447290848013504, distance: 1.714504621600737 entropy 1.2378510236740112
epoch: 14, step: 5
	action: tensor([[ 0.4818, -0.0718,  2.4650, -0.5383,  0.5744,  1.3675, -0.8745]],
       dtype=torch.float64)
	q_value: tensor([[-27.0412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5250053760421249, distance: 0.7886803861418628 entropy 1.2378510236740112
epoch: 14, step: 6
	action: tensor([[ 1.2199, -0.3945,  0.7594, -0.9478,  0.1124, -1.4267,  0.9785]],
       dtype=torch.float64)
	q_value: tensor([[-41.5744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18932818782247363, distance: 1.030336842524543 entropy 1.2378510236740112
epoch: 14, step: 7
	action: tensor([[ 1.4402, -0.9076,  1.1117, -1.8768,  0.3509,  1.1937,  1.0866]],
       dtype=torch.float64)
	q_value: tensor([[-35.7871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06702143713085462, distance: 1.1820701911463107 entropy 1.2378510236740112
epoch: 14, step: 8
	action: tensor([[ 3.4510, -3.4329,  2.0316, -3.6825,  0.6559, -1.8965,  3.5626]],
       dtype=torch.float64)
	q_value: tensor([[-45.9099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 9
	action: tensor([[ 0.5799, -0.1716, -0.3711,  1.1686, -0.6285,  0.1288,  0.4858]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 10
	action: tensor([[ 1.6133, -0.0723,  0.5138, -1.0036, -0.2596,  0.2705,  1.6194]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 11
	action: tensor([[ 0.1090, -0.2992,  1.1552,  0.0803, -1.0830, -0.2634,  0.9831]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1371950969051675, distance: 1.0629503896748502 entropy 1.2378510236740112
epoch: 14, step: 12
	action: tensor([[ 1.1428, -1.8788,  0.7066, -0.8527,  1.2324, -1.0060,  1.0626]],
       dtype=torch.float64)
	q_value: tensor([[-31.7738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 13
	action: tensor([[ 1.0426,  0.7955,  0.4310, -0.5902, -0.4313, -0.3781,  0.9707]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 14
	action: tensor([[-0.6188,  0.2598,  0.0212,  0.0467, -1.0041, -1.2325,  0.5050]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13036612250812651, distance: 1.0671486471991227 entropy 1.2378510236740112
epoch: 14, step: 15
	action: tensor([[-1.8874, -0.4905, -0.2390, -0.1871,  0.0288, -0.8059,  0.7465]],
       dtype=torch.float64)
	q_value: tensor([[-31.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 16
	action: tensor([[ 1.0037,  1.0102, -0.1431,  0.0249, -0.1344, -0.6810,  1.4263]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 17
	action: tensor([[ 0.9654, -1.1589, -0.6072, -0.3247,  0.3315, -0.8367, -1.2564]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.828284498869386, distance: 1.5473144517621815 entropy 1.2378510236740112
epoch: 14, step: 18
	action: tensor([[ 1.6251, -0.1948,  0.3909,  1.0551,  1.4071,  0.7103, -0.4551]],
       dtype=torch.float64)
	q_value: tensor([[-37.3375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2856768467298343, distance: 0.9671727611220363 entropy 1.2378510236740112
epoch: 14, step: 19
	action: tensor([[-0.6780, -1.7121,  1.8968,  0.3733,  0.8524,  0.4949,  1.0606]],
       dtype=torch.float64)
	q_value: tensor([[-43.1968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 20
	action: tensor([[ 0.4605, -1.3132,  1.4975,  1.3454,  0.1713,  0.9296, -0.8430]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3289856177556688, distance: 0.9373949459414593 entropy 1.2378510236740112
epoch: 14, step: 21
	action: tensor([[ 1.4527,  0.2812,  2.2650, -1.7825,  1.4782, -1.9728,  1.4980]],
       dtype=torch.float64)
	q_value: tensor([[-43.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7051521781332112, distance: 0.6213776884935309 entropy 1.2378510236740112
epoch: 14, step: 22
	action: tensor([[ 1.7524, -2.7370,  0.9982, -1.0308,  0.2908, -0.4647,  1.9997]],
       dtype=torch.float64)
	q_value: tensor([[-50.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 23
	action: tensor([[ 0.3246, -0.9070,  0.7710, -0.7867,  0.2347,  0.9669,  1.3250]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4265077070037815, distance: 1.3667647027859873 entropy 1.2378510236740112
epoch: 14, step: 24
	action: tensor([[-0.2194, -1.9701,  2.5686, -2.1367,  0.6374, -1.6167,  2.9797]],
       dtype=torch.float64)
	q_value: tensor([[-37.8948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 25
	action: tensor([[-0.2133, -0.8012, -1.7039, -0.3174,  1.0130,  0.5181, -1.1130]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31414510826555464, distance: 1.3118324049542873 entropy 1.2378510236740112
epoch: 14, step: 26
	action: tensor([[-0.0414, -0.2520,  0.1793,  0.1541,  0.6536,  0.9976,  0.8334]],
       dtype=torch.float64)
	q_value: tensor([[-39.9439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5702527843804046, distance: 0.7501761090969014 entropy 1.2378510236740112
epoch: 14, step: 27
	action: tensor([[ 0.3883, -2.5101,  3.3043, -0.1945, -0.0646,  0.0582,  2.1816]],
       dtype=torch.float64)
	q_value: tensor([[-33.0111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 28
	action: tensor([[ 2.4659, -0.7096,  0.3308,  0.1601, -0.4128,  0.6153,  1.0635]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 29
	action: tensor([[ 1.6084, -1.3499, -0.0947, -0.9706,  1.6202,  1.3518,  0.3170]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8094544268619956, distance: 1.539325691891193 entropy 1.2378510236740112
epoch: 14, step: 30
	action: tensor([[ 1.8625, -3.1006,  2.8880, -1.6858,  2.0091, -0.1060,  2.8474]],
       dtype=torch.float64)
	q_value: tensor([[-49.7367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 31
	action: tensor([[-0.7575, -1.8096,  0.3067,  0.1093, -1.5114,  0.0944, -0.5757]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 32
	action: tensor([[ 0.8588, -0.8274, -0.1831,  0.1722, -1.6311, -0.9558,  2.4173]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17213384830636747, distance: 1.0412062261616701 entropy 1.2378510236740112
epoch: 14, step: 33
	action: tensor([[ 1.9913, -2.0465,  2.0671, -2.0190, -0.1384,  0.1580,  3.0651]],
       dtype=torch.float64)
	q_value: tensor([[-49.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 34
	action: tensor([[-0.0105,  0.1334, -0.2790, -0.0904, -0.0863,  0.3556,  2.0097]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38828311799424653, distance: 0.8950183230046144 entropy 1.2378510236740112
epoch: 14, step: 35
	action: tensor([[ 2.6233, -1.4233,  2.1192, -0.7753,  0.8944,  0.3265,  2.0250]],
       dtype=torch.float64)
	q_value: tensor([[-38.5209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 36
	action: tensor([[-0.3323,  0.2172,  0.6896, -0.2996,  0.1383, -0.0560,  0.9589]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19284235225519963, distance: 1.2498221537980412 entropy 1.2378510236740112
epoch: 14, step: 37
	action: tensor([[-0.3566, -0.8493,  0.4999, -0.3068, -1.0189,  0.9320,  2.0925]],
       dtype=torch.float64)
	q_value: tensor([[-27.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7381626820062828, distance: 1.5086965735708504 entropy 1.2378510236740112
epoch: 14, step: 38
	action: tensor([[ 0.6048, -2.2684,  2.0366, -1.3773,  0.2950, -0.9790,  3.7081]],
       dtype=torch.float64)
	q_value: tensor([[-42.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 39
	action: tensor([[ 0.3674, -0.0728,  0.0112, -0.5196,  0.3062,  0.9512,  0.5563]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5945768641737676, distance: 0.7286365582006807 entropy 1.2378510236740112
epoch: 14, step: 40
	action: tensor([[ 0.0185, -2.8662,  1.6323, -1.4976, -1.2082, -1.4973,  3.3441]],
       dtype=torch.float64)
	q_value: tensor([[-31.1722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 41
	action: tensor([[ 0.7331, -0.1741,  0.8603, -1.2008,  0.6013,  1.2631, -0.6864]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28979553440585004, distance: 0.9643804386426688 entropy 1.2378510236740112
epoch: 14, step: 42
	action: tensor([[-0.9830, -1.5796,  1.9393, -0.5730,  0.5755, -0.1217,  1.6494]],
       dtype=torch.float64)
	q_value: tensor([[-34.9572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 43
	action: tensor([[-1.0002, -1.6194, -1.1654, -1.0928, -0.9790,  0.9550, -0.4262]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5257929094752674, distance: 1.4135282401114009 entropy 1.2378510236740112
epoch: 14, step: 44
	action: tensor([[ 2.1141, -0.2705,  0.1810, -0.1726, -0.4286,  0.2443,  1.9502]],
       dtype=torch.float64)
	q_value: tensor([[-42.9647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10697550634966124, distance: 1.081405013448832 entropy 1.2378510236740112
epoch: 14, step: 45
	action: tensor([[ 2.3625, -2.7059,  1.7866, -0.6099, -0.6130, -0.6562,  2.9838]],
       dtype=torch.float64)
	q_value: tensor([[-43.8204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 46
	action: tensor([[ 0.1526, -0.0890, -0.9216, -1.1223, -0.3528, -0.7771,  1.4777]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4594164367734682, distance: 0.8413721094826786 entropy 1.2378510236740112
epoch: 14, step: 47
	action: tensor([[ 1.7031, -2.1678,  4.2346, -1.3673, -1.9208, -0.3023,  2.7842]],
       dtype=torch.float64)
	q_value: tensor([[-35.8812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 48
	action: tensor([[ 0.5372, -0.6944,  0.7888,  0.3085,  0.1522, -0.1647,  0.0602]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1704674042857831, distance: 1.042253641613656 entropy 1.2378510236740112
epoch: 14, step: 49
	action: tensor([[ 2.2348,  0.2964,  0.1422, -0.7025,  1.3182,  0.7055,  1.2726]],
       dtype=torch.float64)
	q_value: tensor([[-28.8045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 50
	action: tensor([[-0.9750,  0.4228,  1.0445, -0.5802, -0.6358, -0.7172,  0.4994]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0404532370405777, distance: 1.6346320744653764 entropy 1.2378510236740112
epoch: 14, step: 51
	action: tensor([[-0.0420, -0.5356, -0.2712, -0.7289,  1.0390, -0.8842, -0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-30.1773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.56834053918651, distance: 1.433101258853405 entropy 1.2378510236740112
epoch: 14, step: 52
	action: tensor([[-0.6277, -1.6743, -0.3482, -1.3904, -1.3106,  0.2574,  0.8075]],
       dtype=torch.float64)
	q_value: tensor([[-32.2221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 53
	action: tensor([[-0.8879,  0.1192, -0.1485, -0.0930,  0.5850, -0.2059,  0.9661]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8326173574612243, distance: 1.5491468600353442 entropy 1.2378510236740112
epoch: 14, step: 54
	action: tensor([[-0.0344,  0.1610,  1.2951, -1.2418, -1.8932, -0.6347,  1.1197]],
       dtype=torch.float64)
	q_value: tensor([[-29.3451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32461694360014715, distance: 1.3170487371360777 entropy 1.2378510236740112
epoch: 14, step: 55
	action: tensor([[ 0.7684, -1.0102,  0.5770,  0.0048, -0.2256, -1.3680,  0.6939]],
       dtype=torch.float64)
	q_value: tensor([[-39.1368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4781142043511424, distance: 1.3912676549932048 entropy 1.2378510236740112
epoch: 14, step: 56
	action: tensor([[ 1.0606, -0.2044,  1.5567, -0.8606, -0.2479, -1.0710,  0.9021]],
       dtype=torch.float64)
	q_value: tensor([[-33.4439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5127496111319076, distance: 0.7987903147727307 entropy 1.2378510236740112
epoch: 14, step: 57
	action: tensor([[ 1.2341, -1.4096,  1.3263,  0.8596, -0.8416,  0.8014,  1.6292]],
       dtype=torch.float64)
	q_value: tensor([[-34.9421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17727358965417395, distance: 1.2416391389622656 entropy 1.2378510236740112
epoch: 14, step: 58
	action: tensor([[ 2.3795, -4.2606,  2.6102,  0.6379, -0.1333,  0.5380,  2.2168]],
       dtype=torch.float64)
	q_value: tensor([[-44.8823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 59
	action: tensor([[ 0.6450, -0.5610,  1.1400,  0.6541,  0.6648,  0.2026,  1.4083]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43629877403154504, distance: 0.8591741124598464 entropy 1.2378510236740112
epoch: 14, step: 60
	action: tensor([[ 1.9115, -3.2904,  0.5611, -0.2094,  1.7090,  0.6560,  1.0149]],
       dtype=torch.float64)
	q_value: tensor([[-37.5583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 61
	action: tensor([[ 2.1231, -0.1468,  0.7201, -0.7798, -1.2817, -1.3651,  0.5908]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 62
	action: tensor([[ 0.6950, -1.4230,  1.4143, -0.7082,  0.0505, -1.6456,  0.6873]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07325464644335922, distance: 1.1855178077806974 entropy 1.2378510236740112
epoch: 14, step: 63
	action: tensor([[ 1.4347, -0.8818, -0.3037,  1.2894,  1.1469, -0.5379,  0.8980]],
       dtype=torch.float64)
	q_value: tensor([[-37.1586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6223306003867146, distance: 0.7032546172071826 entropy 1.2378510236740112
epoch: 14, step: 64
	action: tensor([[ 1.2416, -1.1003,  0.5031,  0.5613, -0.8311, -0.4739,  2.0722]],
       dtype=torch.float64)
	q_value: tensor([[-44.3136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3400292414352082, distance: 1.3246886979146069 entropy 1.2378510236740112
epoch: 14, step: 65
	action: tensor([[ 0.7692, -1.9260,  3.0968, -1.6100,  0.1173, -0.2763,  1.1672]],
       dtype=torch.float64)
	q_value: tensor([[-44.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 66
	action: tensor([[ 0.5961, -1.1926,  0.0340, -1.0762, -0.7841, -1.0037,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6573599205673206, distance: 1.4732115306988687 entropy 1.2378510236740112
epoch: 14, step: 67
	action: tensor([[ 0.8235, -1.0879,  0.5433,  0.6172, -0.0130,  0.9297,  0.5218]],
       dtype=torch.float64)
	q_value: tensor([[-34.7344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36979848476677557, distance: 0.9084403475935191 entropy 1.2378510236740112
epoch: 14, step: 68
	action: tensor([[-0.3783, -1.5013,  1.5917, -0.7157,  0.2970, -0.7957,  2.8984]],
       dtype=torch.float64)
	q_value: tensor([[-36.2439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 69
	action: tensor([[ 0.8274, -1.2456,  1.3590, -1.4602,  1.7416, -1.0493,  0.7479]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08877356855055774, distance: 1.0923702089266818 entropy 1.2378510236740112
epoch: 14, step: 70
	action: tensor([[ 1.7345, -1.9435,  0.7397,  0.3876,  1.4698, -0.5990,  1.6758]],
       dtype=torch.float64)
	q_value: tensor([[-43.0506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 71
	action: tensor([[ 0.3152, -1.6463,  2.0637,  0.0619, -0.8317,  0.7866,  0.4666]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38967562885756446, distance: 1.3490045481351995 entropy 1.2378510236740112
epoch: 14, step: 72
	action: tensor([[ 1.0674, -0.3045,  1.3030, -0.0212,  0.4962, -0.1869,  1.7501]],
       dtype=torch.float64)
	q_value: tensor([[-38.6687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4565378200723733, distance: 0.8436092955953732 entropy 1.2378510236740112
epoch: 14, step: 73
	action: tensor([[ 1.3239, -2.4538,  1.2020, -1.3421, -0.1330,  0.9639,  1.4537]],
       dtype=torch.float64)
	q_value: tensor([[-38.3519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 74
	action: tensor([[ 1.1491, -0.9987,  1.0346,  0.0969,  0.9675,  1.1940,  1.7411]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4141920980289462, distance: 1.3608520015294159 entropy 1.2378510236740112
epoch: 14, step: 75
	action: tensor([[ 2.2923, -4.8412,  1.8347, -1.7228,  0.5835, -1.1110,  2.6288]],
       dtype=torch.float64)
	q_value: tensor([[-46.1498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 76
	action: tensor([[ 1.1582, -0.0710,  0.8330,  0.5321,  0.1531,  0.7218, -1.3390]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 77
	action: tensor([[ 0.8048, -1.1080,  1.1426, -0.1155, -0.7262,  1.1284,  0.7958]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03921593833564496, distance: 1.1216815805674203 entropy 1.2378510236740112
epoch: 14, step: 78
	action: tensor([[ 2.5677, -2.5783,  2.2717, -1.6518,  0.6482, -0.3337,  2.2605]],
       dtype=torch.float64)
	q_value: tensor([[-37.6111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 79
	action: tensor([[-0.2534, -1.2525,  0.8505,  0.7819, -0.1556, -0.9879,  0.8498]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.543089762945359, distance: 1.4215177545097661 entropy 1.2378510236740112
epoch: 14, step: 80
	action: tensor([[ 1.0370, -0.8418, -0.3297, -1.7225,  0.0968, -0.4443, -0.1458]],
       dtype=torch.float64)
	q_value: tensor([[-34.7073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5434084335623204, distance: 1.4216645290414787 entropy 1.2378510236740112
epoch: 14, step: 81
	action: tensor([[ 0.5951, -2.8336,  0.2248,  0.7349,  0.2466, -0.2372,  0.7207]],
       dtype=torch.float64)
	q_value: tensor([[-36.8071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 82
	action: tensor([[ 0.1162, -0.4509,  0.1394,  0.0865,  0.2460, -0.7798, -0.8634]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13567819079471133, distance: 1.219507108550292 entropy 1.2378510236740112
epoch: 14, step: 83
	action: tensor([[-0.8129, -1.3033, -1.8460, -0.0250,  0.7873, -0.6492, -0.6192]],
       dtype=torch.float64)
	q_value: tensor([[-27.6804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9082005171927912, distance: 1.5807700460102478 entropy 1.2378510236740112
epoch: 14, step: 84
	action: tensor([[-0.3474,  0.3063,  1.0122,  0.1725, -1.3174, -0.4959,  0.5853]],
       dtype=torch.float64)
	q_value: tensor([[-41.5813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 85
	action: tensor([[ 1.2130, -0.2605,  0.3487, -1.0644,  1.5375,  0.5833,  0.0756]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12193226353915554, distance: 1.212104355779648 entropy 1.2378510236740112
epoch: 14, step: 86
	action: tensor([[ 0.5180, -2.7151,  2.2281, -2.7514, -0.0981,  0.0073,  2.7621]],
       dtype=torch.float64)
	q_value: tensor([[-37.7582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 87
	action: tensor([[ 0.9374, -1.5814, -0.8789, -0.4604,  0.2033,  0.5125,  0.2632]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6710447209429768, distance: 1.4792811719164431 entropy 1.2378510236740112
epoch: 14, step: 88
	action: tensor([[ 1.7522, -2.3661,  3.6371, -1.6516, -1.0191, -0.9283,  0.8390]],
       dtype=torch.float64)
	q_value: tensor([[-37.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 89
	action: tensor([[-0.6684, -1.4502,  1.3406, -1.1562,  0.2229,  0.5024, -0.0395]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.399652574996483, distance: 1.772682174189661 entropy 1.2378510236740112
epoch: 14, step: 90
	action: tensor([[ 0.1375, -1.8623,  0.9160, -1.5389,  1.1572, -0.8689, -0.2790]],
       dtype=torch.float64)
	q_value: tensor([[-35.9226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 91
	action: tensor([[-0.1328, -0.3718,  1.3054, -0.5175,  0.2145, -0.9121,  1.5361]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5094183405657566, distance: 1.4059228991501274 entropy 1.2378510236740112
epoch: 14, step: 92
	action: tensor([[ 1.5399, -1.5382,  0.1641, -0.3096, -0.9874, -0.0530,  2.6164]],
       dtype=torch.float64)
	q_value: tensor([[-33.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 93
	action: tensor([[-0.6837, -0.4567,  0.3856,  0.2604,  0.3991, -0.2759,  1.5457]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.801388660666934, distance: 1.5358910355709459 entropy 1.2378510236740112
epoch: 14, step: 94
	action: tensor([[ 0.6803, -0.3329,  1.0288, -0.7045,  0.7802, -0.7161,  1.7352]],
       dtype=torch.float64)
	q_value: tensor([[-32.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1799330865330121, distance: 1.0362900740329295 entropy 1.2378510236740112
epoch: 14, step: 95
	action: tensor([[ 1.0693, -0.7605,  2.6667, -1.4895, -0.1519,  1.9496,  0.0547]],
       dtype=torch.float64)
	q_value: tensor([[-36.8785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46566318985114286, distance: 0.8364967154534497 entropy 1.2378510236740112
epoch: 14, step: 96
	action: tensor([[ 2.0951, -2.6830,  3.2120, -1.6682, -0.4446,  0.7579,  3.2851]],
       dtype=torch.float64)
	q_value: tensor([[-47.5705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 97
	action: tensor([[ 0.0362, -1.8055,  0.3192, -0.3052, -0.6869,  0.6243, -0.3143]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 98
	action: tensor([[-0.2936, -0.9680,  0.0269, -0.2439, -0.4282,  0.3338,  1.1718]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8661834465442553, distance: 1.5632695178801854 entropy 1.2378510236740112
epoch: 14, step: 99
	action: tensor([[ 2.3422, -2.1652,  2.0455,  0.8682,  0.3122,  0.4148,  2.5295]],
       dtype=torch.float64)
	q_value: tensor([[-32.8071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 100
	action: tensor([[ 0.0831,  0.3468,  0.2222, -0.2096,  0.5033, -1.2078,  0.9368]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25944734830979677, distance: 0.9847696692913804 entropy 1.2378510236740112
epoch: 14, step: 101
	action: tensor([[-1.2520, -0.1521, -0.2813, -0.3969,  0.7953, -1.5641,  0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-29.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.050121039966644, distance: 1.6384999959177342 entropy 1.2378510236740112
epoch: 14, step: 102
	action: tensor([[-0.6132, -0.5697,  0.8282,  0.5560, -0.4690,  1.0053, -0.9093]],
       dtype=torch.float64)
	q_value: tensor([[-33.7598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05716407216443642, distance: 1.1111552818028803 entropy 1.2378510236740112
epoch: 14, step: 103
	action: tensor([[-0.6808, -2.0949,  0.9038,  0.0745, -0.8508,  1.5008,  1.4397]],
       dtype=torch.float64)
	q_value: tensor([[-35.3873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 104
	action: tensor([[ 0.6170, -0.0240,  0.7436,  0.1900, -0.6683,  0.9918,  2.8343]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 105
	action: tensor([[ 0.8849, -1.0857,  0.3561,  0.1440, -0.9108,  1.5488,  1.2014]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33007912949832974, distance: 0.936630826449239 entropy 1.2378510236740112
epoch: 14, step: 106
	action: tensor([[ 2.8761, -2.2396,  2.7864, -1.8718,  0.3476, -2.4035,  4.2883]],
       dtype=torch.float64)
	q_value: tensor([[-41.9458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 107
	action: tensor([[ 0.5112, -0.5375,  0.2704, -0.4355, -0.8414, -0.2218,  1.2242]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2046614960473947, distance: 1.255998752340525 entropy 1.2378510236740112
epoch: 14, step: 108
	action: tensor([[ 0.7098, -2.0178,  1.8404, -0.7990,  0.2507,  0.6453,  2.0428]],
       dtype=torch.float64)
	q_value: tensor([[-32.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 109
	action: tensor([[ 1.1384, -1.7989, -0.6295, -0.9246, -1.2852, -0.4413,  1.4125]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 110
	action: tensor([[ 0.8432, -0.8733,  0.1291, -0.8959, -0.0301,  0.2907,  0.9638]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6604873855705011, distance: 1.4746008625898779 entropy 1.2378510236740112
epoch: 14, step: 111
	action: tensor([[ 0.8821, -1.6113,  2.5858, -0.4569,  0.5213, -0.5464,  1.3176]],
       dtype=torch.float64)
	q_value: tensor([[-34.4460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 112
	action: tensor([[ 0.2253, -1.1808, -0.4643,  1.4349, -0.3439,  1.0093, -0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.388549997214209, distance: 0.8948230628628704 entropy 1.2378510236740112
epoch: 14, step: 113
	action: tensor([[ 2.0148, -1.1709,  2.1812, -0.4095,  1.6257,  0.4141,  0.9530]],
       dtype=torch.float64)
	q_value: tensor([[-39.5941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 114
	action: tensor([[-0.3876,  0.5930,  0.9811, -0.2744,  0.3698,  0.8625,  0.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 115
	action: tensor([[ 0.1550,  0.0958,  0.9234, -1.1338,  0.0570,  0.8235,  1.2559]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07562648156156926, distance: 1.1002222940345836 entropy 1.2378510236740112
epoch: 14, step: 116
	action: tensor([[ 1.4049, -4.1406,  2.6475, -0.5595,  1.1066, -0.8278,  1.6869]],
       dtype=torch.float64)
	q_value: tensor([[-36.7455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 117
	action: tensor([[ 0.6883,  0.3720,  1.4950, -0.2662, -1.0530,  0.4562,  1.5785]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 118
	action: tensor([[-1.6011, -1.2810,  1.3702, -0.0395,  0.1151,  0.3296,  1.9072]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5566301316805764, distance: 1.8297453191045883 entropy 1.2378510236740112
epoch: 14, step: 119
	action: tensor([[ 0.8221, -3.6412,  1.7700, -1.1496, -0.9601,  1.0215,  2.8681]],
       dtype=torch.float64)
	q_value: tensor([[-40.5201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 120
	action: tensor([[ 1.6837,  0.9724, -0.1322, -0.5760,  1.1802, -0.8133, -0.2780]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 121
	action: tensor([[-1.0393,  0.7878,  0.7829, -0.8701,  0.3241,  0.4346, -0.1091]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8151352948930348, distance: 1.541740191292497 entropy 1.2378510236740112
epoch: 14, step: 122
	action: tensor([[ 0.0937, -1.3843,  1.5536, -0.8073,  0.1828,  0.1347,  0.5252]],
       dtype=torch.float64)
	q_value: tensor([[-33.3629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9760792827180251, distance: 1.6086400453207863 entropy 1.2378510236740112
epoch: 14, step: 123
	action: tensor([[ 1.4794, -1.1257,  1.0621,  1.0879,  0.5053, -0.1515,  1.6389]],
       dtype=torch.float64)
	q_value: tensor([[-33.5256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40274992366477425, distance: 1.3553355196176418 entropy 1.2378510236740112
epoch: 14, step: 124
	action: tensor([[ 1.3372, -0.5518,  2.4470, -1.6181,  0.3436,  0.1831,  2.7368]],
       dtype=torch.float64)
	q_value: tensor([[-45.2754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.92058121228042, distance: 0.3224915354477277 entropy 1.2378510236740112
epoch: 14, step: 125
	action: tensor([[ 2.8194, -4.3898,  2.9724, -0.4903,  1.2126, -1.0444,  5.4153]],
       dtype=torch.float64)
	q_value: tensor([[-52.4473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 126
	action: tensor([[ 2.0524, -2.3236,  1.1052, -0.6446, -0.3753,  0.5003,  0.3932]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 14, step: 127
	action: tensor([[ 0.3435,  0.0047,  0.2170,  0.9069, -1.0694, -0.9395,  1.5903]],
       dtype=torch.float64)
	q_value: tensor([[-37.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
LOSS epoch 14 actor 332.1694098012635 critic 147.4421039181936 
epoch: 15, step: 0
	action: tensor([[-1.7278, -0.6561,  1.3686, -0.5808, -0.8156, -0.1480, -0.9225]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 1
	action: tensor([[ 0.8478, -0.4963,  0.7125, -1.5230, -0.0548, -0.4512,  0.2900]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29685499385261394, distance: 1.3031739879423752 entropy 1.1324905157089233
epoch: 15, step: 2
	action: tensor([[ 1.4382, -1.2530,  0.4007, -1.3275,  0.3108, -0.7247,  0.3303]],
       dtype=torch.float64)
	q_value: tensor([[-35.1869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3868632922876094, distance: 1.3476388423079757 entropy 1.1324905157089233
epoch: 15, step: 3
	action: tensor([[ 1.3564, -2.6259,  1.1768, -1.2367,  1.0984, -0.1192,  2.8114]],
       dtype=torch.float64)
	q_value: tensor([[-41.9465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 4
	action: tensor([[ 0.6986, -0.2099,  0.7792, -0.7069,  1.3077,  0.0313, -0.2715]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17367303270943013, distance: 1.0402378606996785 entropy 1.1324905157089233
epoch: 15, step: 5
	action: tensor([[ 1.4737, -0.7945,  1.3975, -1.0504, -0.3409, -0.4432,  1.4666]],
       dtype=torch.float64)
	q_value: tensor([[-35.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19384285966365689, distance: 1.0274638382766366 entropy 1.1324905157089233
epoch: 15, step: 6
	action: tensor([[ 2.7279, -4.9179,  2.7249, -2.4402,  1.7693, -0.8762,  3.5051]],
       dtype=torch.float64)
	q_value: tensor([[-44.0202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 7
	action: tensor([[ 0.4725, -0.6246, -0.7381, -0.5008,  0.5493, -0.1273,  1.7583]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13417582367459024, distance: 1.2187002102023003 entropy 1.1324905157089233
epoch: 15, step: 8
	action: tensor([[ 1.6969, -2.8672,  2.7678, -2.3396, -0.1831,  0.1382,  4.3243]],
       dtype=torch.float64)
	q_value: tensor([[-43.5914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 9
	action: tensor([[ 1.5701, -1.0193,  0.2854, -0.7080, -0.0962, -0.3654, -0.1809]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7873762390837751, distance: 1.5299057729106995 entropy 1.1324905157089233
epoch: 15, step: 10
	action: tensor([[ 1.2840, -2.7870, -0.1491, -0.6482,  0.2871, -0.1838,  1.2020]],
       dtype=torch.float64)
	q_value: tensor([[-37.4652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 11
	action: tensor([[-0.3495,  0.0377,  0.1809, -0.4771,  0.4023, -0.5521,  1.5575]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4494755238537076, distance: 1.3777237222788332 entropy 1.1324905157089233
epoch: 15, step: 12
	action: tensor([[ 2.3073, -2.0593,  2.3034, -1.2625,  0.8359,  0.3469,  1.4041]],
       dtype=torch.float64)
	q_value: tensor([[-35.6906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 13
	action: tensor([[-0.4466, -1.4887,  0.6550,  0.2659,  1.2989,  0.2320,  1.3917]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7378419870036952, distance: 1.5085573881609815 entropy 1.1324905157089233
epoch: 15, step: 14
	action: tensor([[ 1.1759, -2.4101,  1.6812, -2.0833, -1.0693,  0.1910,  1.7524]],
       dtype=torch.float64)
	q_value: tensor([[-43.3560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 15
	action: tensor([[-0.9970, -1.3998,  1.9893, -0.8633,  0.4433, -0.0865,  0.6392]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5813902545960463, distance: 1.8385842113790307 entropy 1.1324905157089233
epoch: 15, step: 16
	action: tensor([[ 1.0934, -2.2663,  1.0567, -0.2097,  0.7012, -0.3463,  1.3545]],
       dtype=torch.float64)
	q_value: tensor([[-40.0306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 17
	action: tensor([[ 0.5265, -0.9818,  1.7953, -0.2231,  0.0769, -0.7440,  0.3978]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16804880956234536, distance: 1.2367650070180451 entropy 1.1324905157089233
epoch: 15, step: 18
	action: tensor([[ 1.4750, -1.3172, -0.2740, -1.6380, -0.2997, -0.5593,  2.0232]],
       dtype=torch.float64)
	q_value: tensor([[-38.2466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 19
	action: tensor([[ 0.2393, -0.5853,  1.1470, -0.9165, -0.3226,  0.8786,  1.1137]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22948278192668847, distance: 1.2688723063346954 entropy 1.1324905157089233
epoch: 15, step: 20
	action: tensor([[ 3.2305, -3.2335,  3.0522, -2.1518,  0.0398,  0.1961,  3.3231]],
       dtype=torch.float64)
	q_value: tensor([[-39.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 21
	action: tensor([[ 0.0681, -0.9632,  1.2957, -0.2364,  0.8070,  0.9013, -0.5110]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49484577755599823, distance: 1.3991197520867777 entropy 1.1324905157089233
epoch: 15, step: 22
	action: tensor([[ 1.1784, -1.3685,  1.9553, -0.0322,  0.7576,  0.2461,  1.5403]],
       dtype=torch.float64)
	q_value: tensor([[-38.5903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 23
	action: tensor([[-0.6473, -0.2736,  0.6408, -1.2834,  1.7451,  1.0138, -0.2265]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6976446464976895, distance: 1.4910083904609133 entropy 1.1324905157089233
epoch: 15, step: 24
	action: tensor([[ 1.3959, -1.7109,  2.8974, -0.2169,  0.3277,  0.3730,  2.4633]],
       dtype=torch.float64)
	q_value: tensor([[-44.8869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 25
	action: tensor([[-1.7023, -1.6266,  1.1567, -0.9439,  0.2093,  0.2847,  0.9334]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 26
	action: tensor([[ 0.5481, -0.0806, -0.4109,  0.0672, -0.0463,  0.2703, -0.1562]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6987930898554646, distance: 0.6280426808930862 entropy 1.1324905157089233
epoch: 15, step: 27
	action: tensor([[ 1.2683, -0.9005, -0.5209,  0.4341,  0.4303, -0.1421,  1.0291]],
       dtype=torch.float64)
	q_value: tensor([[-26.7966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03856350372392703, distance: 1.1220623631588962 entropy 1.1324905157089233
epoch: 15, step: 28
	action: tensor([[ 1.3868, -1.6056,  1.8564, -2.1647,  2.3137,  0.2154,  3.8756]],
       dtype=torch.float64)
	q_value: tensor([[-42.0451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 29
	action: tensor([[ 1.0258, -0.5301,  1.2378, -0.8466,  0.3921,  0.2175, -0.5294]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08734427597700434, distance: 1.0932265849092089 entropy 1.1324905157089233
epoch: 15, step: 30
	action: tensor([[ 1.0837, -1.4173,  2.5039, -2.8430,  0.2723,  0.5223,  0.9444]],
       dtype=torch.float64)
	q_value: tensor([[-34.8594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 31
	action: tensor([[ 1.4109, -1.8785,  0.2120, -0.7061, -0.3736, -0.1345,  1.5121]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 32
	action: tensor([[ 0.8486, -1.4690,  0.9969,  0.0826, -0.5995, -0.2823,  0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6987089695330626, distance: 1.491475704374829 entropy 1.1324905157089233
epoch: 15, step: 33
	action: tensor([[ 0.1084, -1.3230,  1.3524,  0.3941,  1.3854,  1.7675,  0.2311]],
       dtype=torch.float64)
	q_value: tensor([[-36.6982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 34
	action: tensor([[ 1.1293,  0.0056,  1.5272,  0.8500,  0.0071, -0.6251,  0.9032]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 35
	action: tensor([[ 0.6159,  0.8454,  0.7886, -0.0635,  0.2581,  0.3253,  0.1004]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 36
	action: tensor([[ 1.4198, -1.3522,  2.2145, -0.8181, -0.3660, -0.6541,  0.2368]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 37
	action: tensor([[ 0.4213, -0.1846,  1.0508, -0.8339,  0.1985, -0.2780,  0.9019]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.056228170575411074, distance: 1.1760764846167229 entropy 1.1324905157089233
epoch: 15, step: 38
	action: tensor([[-0.0139, -0.3605,  1.9438, -2.6617,  1.0199, -0.7590,  3.3729]],
       dtype=torch.float64)
	q_value: tensor([[-32.9105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 39
	action: tensor([[-0.4938,  0.1713, -0.1344, -0.4801, -0.6554,  0.4694,  0.9877]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32793953628549355, distance: 1.3186995071269083 entropy 1.1324905157089233
epoch: 15, step: 40
	action: tensor([[ 1.0670, -2.3905,  2.9682, -0.4277,  0.8661,  0.8225,  2.1765]],
       dtype=torch.float64)
	q_value: tensor([[-33.3230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 41
	action: tensor([[ 0.4900, -1.3073,  1.8189, -0.0312,  0.5543,  0.1251,  0.6640]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6667790002646441, distance: 1.477391865063205 entropy 1.1324905157089233
epoch: 15, step: 42
	action: tensor([[ 1.3210, -3.0849,  2.5151, -1.7388,  0.9149, -0.5307,  3.3095]],
       dtype=torch.float64)
	q_value: tensor([[-40.7343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 43
	action: tensor([[ 1.2532, -0.6435,  1.2986, -0.0529, -0.0951,  0.9117,  0.9659]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11932734078977136, distance: 1.0739002664536874 entropy 1.1324905157089233
epoch: 15, step: 44
	action: tensor([[ 2.6540, -2.7776,  1.9934, -0.7281,  0.0587, -1.0978,  2.4259]],
       dtype=torch.float64)
	q_value: tensor([[-41.3493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 45
	action: tensor([[ 0.3167, -0.3759, -0.8510, -0.5061,  0.5402,  0.4925,  1.5709]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3198256821931562, distance: 0.943771390773529 entropy 1.1324905157089233
epoch: 15, step: 46
	action: tensor([[ 1.1384, -1.1603,  2.8888, -2.5188,  0.8866, -1.1182,  4.1273]],
       dtype=torch.float64)
	q_value: tensor([[-42.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 47
	action: tensor([[ 0.3063,  0.2547, -0.1384, -1.0969,  1.1710, -0.7234,  2.9855]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1771454887419347, distance: 1.0380498750601654 entropy 1.1324905157089233
epoch: 15, step: 48
	action: tensor([[ 3.1785, -4.7857,  5.9512, -1.7268, -0.2030, -0.0228,  4.4013]],
       dtype=torch.float64)
	q_value: tensor([[-54.7494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 49
	action: tensor([[ 0.6874, -0.4893, -0.5863,  0.7732,  0.6175,  0.4083,  2.0450]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7259399861610084, distance: 0.5990727025865892 entropy 1.1324905157089233
epoch: 15, step: 50
	action: tensor([[ 4.1447, -3.6525,  3.8348, -1.4612,  0.0230, -0.3394,  2.4777]],
       dtype=torch.float64)
	q_value: tensor([[-48.6735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 51
	action: tensor([[ 2.0167, -0.3629, -0.0313,  0.5889,  1.6247, -0.4106,  0.2841]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4704976436023727, distance: 0.8327039827298592 entropy 1.1324905157089233
epoch: 15, step: 52
	action: tensor([[ 1.8881, -1.2262,  3.6556, -1.1393,  1.2982, -0.6221,  3.7484]],
       dtype=torch.float64)
	q_value: tensor([[-45.2809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 53
	action: tensor([[ 0.7328, -0.2954,  1.3775,  0.3043, -0.5143,  0.3729,  0.9161]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6491879214908653, distance: 0.6777881351824875 entropy 1.1324905157089233
epoch: 15, step: 54
	action: tensor([[ 2.0667, -2.3324,  1.5091, -0.9820,  0.4549, -0.2786,  3.8848]],
       dtype=torch.float64)
	q_value: tensor([[-36.3757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 55
	action: tensor([[ 0.7402, -1.5843,  0.7438, -0.4106,  0.3589, -1.0345,  0.9399]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6388665712865211, distance: 1.4649691920594212 entropy 1.1324905157089233
epoch: 15, step: 56
	action: tensor([[ 0.1572, -1.5237,  2.5144, -1.0777, -0.8473, -0.6222,  2.9479]],
       dtype=torch.float64)
	q_value: tensor([[-38.1496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 57
	action: tensor([[ 1.2504, -0.3891,  0.7877, -0.3842,  0.4107,  0.9257,  1.6956]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24023837458432062, distance: 0.9974597281489677 entropy 1.1324905157089233
epoch: 15, step: 58
	action: tensor([[ 4.1945, -4.3424,  4.6546, -2.3223,  0.9605, -0.6298,  4.8060]],
       dtype=torch.float64)
	q_value: tensor([[-46.3319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 59
	action: tensor([[-0.6432, -2.1672,  1.2946, -1.6386,  0.2901, -0.7897,  0.2836]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 60
	action: tensor([[ 0.4423, -0.8227,  1.2617, -0.3661, -0.0822, -0.9886,  1.2534]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3899056975078421, distance: 1.349116211175341 entropy 1.1324905157089233
epoch: 15, step: 61
	action: tensor([[-0.2901, -1.9159,  1.0657, -0.7646,  1.2821,  0.2434,  1.8328]],
       dtype=torch.float64)
	q_value: tensor([[-36.7976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 62
	action: tensor([[ 0.3161,  0.1089,  0.5414, -0.1610, -1.0684, -0.4719,  0.7653]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5049360738164902, distance: 0.805169534837249 entropy 1.1324905157089233
epoch: 15, step: 63
	action: tensor([[ 0.9520, -1.9207,  1.6455, -1.1765, -1.0938, -0.8625,  1.7823]],
       dtype=torch.float64)
	q_value: tensor([[-32.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 64
	action: tensor([[ 0.5066, -0.4502,  1.3157, -0.1633, -1.8820,  0.9018,  0.6870]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4153837001109765, distance: 0.8749679662743284 entropy 1.1324905157089233
epoch: 15, step: 65
	action: tensor([[ 0.7804, -2.3578,  1.2187, -2.0390,  1.1356, -0.7869,  2.5462]],
       dtype=torch.float64)
	q_value: tensor([[-44.0095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 66
	action: tensor([[ 0.0232, -0.4262,  0.7292,  0.6910,  1.0235,  1.0373, -0.5567]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6761061796329315, distance: 0.6512654276601533 entropy 1.1324905157089233
epoch: 15, step: 67
	action: tensor([[ 1.1042, -1.1995,  1.4690, -1.5352, -0.7859,  0.1509,  0.9441]],
       dtype=torch.float64)
	q_value: tensor([[-39.7923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058946522362441356, distance: 1.1101044542620058 entropy 1.1324905157089233
epoch: 15, step: 68
	action: tensor([[ 3.3467, -2.9163,  2.6413, -1.8183, -0.2872, -1.7242,  2.2146]],
       dtype=torch.float64)
	q_value: tensor([[-44.7359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 69
	action: tensor([[-0.2956, -1.1195, -0.3903,  0.7412,  0.6825, -1.0590,  1.4915]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8582553118312966, distance: 1.5599453531011214 entropy 1.1324905157089233
epoch: 15, step: 70
	action: tensor([[ 1.2986, -1.6033,  1.2924,  0.7691,  0.5779, -1.1952,  2.4643]],
       dtype=torch.float64)
	q_value: tensor([[-43.2554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 71
	action: tensor([[ 1.0339, -0.5691,  0.0506,  0.0984, -0.6536,  0.5092,  1.2442]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44073027122898545, distance: 0.8557902807329956 entropy 1.1324905157089233
epoch: 15, step: 72
	action: tensor([[ 2.2925, -1.9036,  2.5790, -0.2042, -0.1622, -0.3824,  2.7101]],
       dtype=torch.float64)
	q_value: tensor([[-39.5020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 73
	action: tensor([[-0.0078, -1.2307,  0.2792, -0.1542, -0.2290, -0.5958,  1.2229]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9596808998595281, distance: 1.60195153585857 entropy 1.1324905157089233
epoch: 15, step: 74
	action: tensor([[ 0.4230, -2.6290,  0.7680, -1.5195,  0.5919, -1.2362,  2.2437]],
       dtype=torch.float64)
	q_value: tensor([[-36.4942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 75
	action: tensor([[ 2.3006,  0.6877,  0.8199, -0.9562, -0.6482, -0.3320,  0.5122]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 76
	action: tensor([[-0.1390,  0.0838,  0.4346, -0.2806,  0.6790,  0.3161,  1.6141]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12816903810483415, distance: 1.0684958443875103 entropy 1.1324905157089233
epoch: 15, step: 77
	action: tensor([[ 1.8645, -1.3150,  3.2398, -2.0507,  1.0376, -0.6514,  2.7609]],
       dtype=torch.float64)
	q_value: tensor([[-38.6967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 78
	action: tensor([[ 1.6291, -1.1330,  1.7965, -1.3992,  1.5558, -0.5519,  0.7153]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 79
	action: tensor([[ 0.0059, -0.9891, -0.7790,  1.4180,  0.5194, -0.2660,  0.3542]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2462385331147352, distance: 1.277489328439163 entropy 1.1324905157089233
epoch: 15, step: 80
	action: tensor([[ 1.6752, -1.5767, -0.5792, -1.0784,  0.7868, -0.2728,  1.4177]],
       dtype=torch.float64)
	q_value: tensor([[-40.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 81
	action: tensor([[ 1.1047, -0.6409,  0.4814, -0.7399, -1.5229,  0.0273,  1.0105]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31528388800890417, distance: 1.3124006697014268 entropy 1.1324905157089233
epoch: 15, step: 82
	action: tensor([[ 0.9942, -3.5947,  2.3024, -1.0820,  1.8678,  0.5109,  2.6406]],
       dtype=torch.float64)
	q_value: tensor([[-41.7270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 83
	action: tensor([[ 0.8059, -1.0763, -0.1859, -0.4434, -0.1954,  0.7700,  1.2047]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4078143409492172, distance: 1.3577799326698052 entropy 1.1324905157089233
epoch: 15, step: 84
	action: tensor([[ 2.6707, -4.0295,  2.5361, -1.8921,  0.3598, -1.7301,  2.9675]],
       dtype=torch.float64)
	q_value: tensor([[-42.7346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 85
	action: tensor([[ 0.1900, -2.3971,  2.7547, -1.3943, -0.7959, -0.3500,  0.7471]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 86
	action: tensor([[ 1.6183, -1.2560,  0.6415, -0.2067,  0.7350, -1.0921,  1.0381]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30956512780804757, distance: 1.3095444495721758 entropy 1.1324905157089233
epoch: 15, step: 87
	action: tensor([[ 2.1127, -1.8264,  1.6243, -1.1635,  1.8221,  0.7407,  0.3308]],
       dtype=torch.float64)
	q_value: tensor([[-44.3889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 88
	action: tensor([[-0.0159, -0.2400,  0.5556, -0.9686, -0.5575,  0.6945,  0.2876]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3481601887136503, distance: 1.328701552317131 entropy 1.1324905157089233
epoch: 15, step: 89
	action: tensor([[ 0.7411, -2.1409,  1.1901, -1.0173,  1.5196, -0.3034,  3.4933]],
       dtype=torch.float64)
	q_value: tensor([[-32.8480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 90
	action: tensor([[ 0.9096, -0.4767,  1.5892, -0.3771,  1.2392,  0.2699, -0.3728]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23327312807263445, distance: 1.0020214893711397 entropy 1.1324905157089233
epoch: 15, step: 91
	action: tensor([[ 0.0892, -1.6312,  0.3242, -1.0512, -0.0109, -0.6666,  1.2372]],
       dtype=torch.float64)
	q_value: tensor([[-40.1515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 92
	action: tensor([[ 0.7986, -0.0100,  1.2760,  0.1845,  1.4892, -0.9684, -0.1322]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9265589895999605, distance: 0.3101173147919796 entropy 1.1324905157089233
epoch: 15, step: 93
	action: tensor([[-0.1554, -1.7405,  1.9329, -0.1293,  0.2379, -0.6149,  1.7914]],
       dtype=torch.float64)
	q_value: tensor([[-39.9053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 94
	action: tensor([[-0.4418, -2.0540,  2.3798, -1.4047, -1.1961, -0.0664,  0.4553]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 95
	action: tensor([[-0.4709, -0.9759,  0.0402, -0.2735, -0.6868,  0.6473,  0.5042]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9945451332089783, distance: 1.6161386900855803 entropy 1.1324905157089233
epoch: 15, step: 96
	action: tensor([[ 1.5495, -2.3372,  2.0448, -0.7389,  0.6275, -0.7011,  1.6800]],
       dtype=torch.float64)
	q_value: tensor([[-34.9896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 97
	action: tensor([[ 1.0556, -1.0434,  1.9624, -0.9152,  1.2131, -0.1530,  0.7638]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 98
	action: tensor([[ 0.5275, -1.2165,  1.2188,  0.3729,  0.5477, -0.4686, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47751031150911993, distance: 1.3909834203893754 entropy 1.1324905157089233
epoch: 15, step: 99
	action: tensor([[-0.7090, -1.0326,  2.2095, -3.0690,  0.9882,  0.3979,  1.6155]],
       dtype=torch.float64)
	q_value: tensor([[-37.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 100
	action: tensor([[-0.2476, -0.2972,  2.0002, -1.4347, -1.4713, -0.4083,  1.4035]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 101
	action: tensor([[-0.9963, -2.6732,  0.6462, -0.8406, -0.2449, -1.2695,  0.9365]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 102
	action: tensor([[ 0.8537, -0.8587,  1.2423, -0.1723,  1.3413, -0.8632,  0.1234]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13265324194333183, distance: 1.0657444336940656 entropy 1.1324905157089233
epoch: 15, step: 103
	action: tensor([[ 1.1975, -0.0358,  1.4115, -0.8829,  0.0709, -0.8115,  0.0887]],
       dtype=torch.float64)
	q_value: tensor([[-39.8952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6832397841916613, distance: 0.6440535944454184 entropy 1.1324905157089233
epoch: 15, step: 104
	action: tensor([[ 0.9402, -2.1031,  0.0507, -1.9063,  1.6864, -0.3793,  1.0559]],
       dtype=torch.float64)
	q_value: tensor([[-35.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 105
	action: tensor([[ 1.1924, -0.6550,  0.5170,  0.7631, -0.8106,  0.5725,  0.2840]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6320356215167469, distance: 0.6941599965058154 entropy 1.1324905157089233
epoch: 15, step: 106
	action: tensor([[ 1.1417, -1.5088,  2.3763, -0.3719,  1.0059, -0.4003,  3.3279]],
       dtype=torch.float64)
	q_value: tensor([[-38.6432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 107
	action: tensor([[-0.0483, -1.3138,  0.1203, -0.8886, -0.3328, -0.2063,  1.0628]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9876288406354756, distance: 1.6133341923032436 entropy 1.1324905157089233
epoch: 15, step: 108
	action: tensor([[ 1.8225, -1.2484,  0.8741, -0.9652,  0.4457, -0.7410,  2.1162]],
       dtype=torch.float64)
	q_value: tensor([[-37.4152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11263313366928096, distance: 1.2070706432018035 entropy 1.1324905157089233
epoch: 15, step: 109
	action: tensor([[ 2.6897, -4.1106,  3.5236, -1.9356,  0.0560,  0.4455,  3.8705]],
       dtype=torch.float64)
	q_value: tensor([[-51.1113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 110
	action: tensor([[ 1.5902, -0.6651,  1.3330, -0.8663, -0.9515,  0.0029, -0.2981]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12768618029191048, distance: 1.0687916931085006 entropy 1.1324905157089233
epoch: 15, step: 111
	action: tensor([[ 1.1887, -1.4564,  1.0317, -1.6626,  1.4743, -0.8984,  0.7859]],
       dtype=torch.float64)
	q_value: tensor([[-38.4143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 112
	action: tensor([[ 1.5120, -0.9922,  1.9299,  0.9071, -1.3444, -0.3489,  0.2210]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 113
	action: tensor([[ 1.3353, -0.4800,  0.0464, -2.3580, -0.1771, -0.8313,  1.7273]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028719181893207102, distance: 1.1606602530692087 entropy 1.1324905157089233
epoch: 15, step: 114
	action: tensor([[ 3.8282, -4.6635,  2.2845, -3.0214,  0.8267, -0.6795,  3.5413]],
       dtype=torch.float64)
	q_value: tensor([[-49.6884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 115
	action: tensor([[ 1.4196, -0.7004,  1.0003, -1.0229,  0.2590, -0.6011,  1.2159]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09064435027149242, distance: 1.0912482944845165 entropy 1.1324905157089233
epoch: 15, step: 116
	action: tensor([[ 2.6922, -2.6347,  3.2391, -0.6875,  0.8223, -0.9191,  3.8128]],
       dtype=torch.float64)
	q_value: tensor([[-40.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 117
	action: tensor([[ 0.1258, -2.3214,  1.1118, -0.4770,  0.6500, -0.5838,  0.6724]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 118
	action: tensor([[ 1.6047, -1.9270,  1.6725,  0.8252, -0.8304, -0.5391,  1.1408]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 119
	action: tensor([[-0.4273, -0.8494,  1.2921, -0.4465, -0.8432, -0.6494,  0.8073]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1808153239979995, distance: 1.689919970999197 entropy 1.1324905157089233
epoch: 15, step: 120
	action: tensor([[-0.6097, -0.9699,  1.1451, -0.5564,  0.9529,  0.6307,  0.8009]],
       dtype=torch.float64)
	q_value: tensor([[-34.2793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.244041965520006, distance: 1.7142421936405012 entropy 1.1324905157089233
epoch: 15, step: 121
	action: tensor([[ 0.9695, -1.4430,  1.9382,  0.3717,  0.3140,  0.4651,  2.6616]],
       dtype=torch.float64)
	q_value: tensor([[-39.2689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 122
	action: tensor([[ 0.7276, -2.4842,  1.2284,  0.3346, -0.2637, -0.0550,  1.2495]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 123
	action: tensor([[ 1.2852, -0.6718, -0.0043, -0.1756,  0.6393,  1.6423,  0.1032]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3549877911084902, distance: 0.9190532196538861 entropy 1.1324905157089233
epoch: 15, step: 124
	action: tensor([[ 1.4463, -2.7510,  2.5915, -2.1411,  0.3427, -0.5472,  3.0855]],
       dtype=torch.float64)
	q_value: tensor([[-45.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 125
	action: tensor([[ 0.5374, -1.3780,  1.1863,  0.1530, -0.4456, -0.3873,  1.0904]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6950408251397451, distance: 1.489864508758918 entropy 1.1324905157089233
epoch: 15, step: 126
	action: tensor([[ 2.2168, -2.9712,  1.7880, -2.0271,  0.8206,  0.3045,  1.9835]],
       dtype=torch.float64)
	q_value: tensor([[-38.2551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 127
	action: tensor([[ 0.8428, -0.1116,  0.9711, -0.9096,  0.0865,  0.1894,  1.7260]],
       dtype=torch.float64)
	q_value: tensor([[-39.8642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34575325425014003, distance: 0.9256088083485811 entropy 1.1324905157089233
LOSS epoch 15 actor 328.21993895036024 critic 103.76833986216758 
epoch: 16, step: 0
	action: tensor([[ 2.5410, -2.6927,  1.5391, -2.4670,  0.7997, -0.1608,  3.1046]],
       dtype=torch.float64)
	q_value: tensor([[-48.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 1
	action: tensor([[-0.0268, -0.5444,  0.9256,  0.1864, -0.9795,  0.1806,  0.8266]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003657277928442171, distance: 1.1422497447038207 entropy 1.1324905157089233
epoch: 16, step: 2
	action: tensor([[ 2.2391, -1.7155,  1.4393,  0.1809,  1.3617, -0.1064,  1.1940]],
       dtype=torch.float64)
	q_value: tensor([[-38.6378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 3
	action: tensor([[ 1.2856, -1.0351,  0.6632, -1.0475, -0.3385,  0.6762, -1.1742]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4768376768455169, distance: 1.3906667626417526 entropy 1.1324905157089233
epoch: 16, step: 4
	action: tensor([[-0.3324, -2.3064,  0.1640,  0.4348,  0.1110,  0.0221,  0.6827]],
       dtype=torch.float64)
	q_value: tensor([[-46.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 5
	action: tensor([[ 0.3972, -1.4604,  0.4695, -1.2634,  0.7286,  1.0121,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8620926193008793, distance: 1.5615551703389734 entropy 1.1324905157089233
epoch: 16, step: 6
	action: tensor([[ 3.0748, -1.4944,  1.1078, -2.6575, -0.5088, -0.6128,  2.1177]],
       dtype=torch.float64)
	q_value: tensor([[-48.6604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 7
	action: tensor([[ 1.1611, -1.1419,  0.1974,  0.4185,  0.4804, -0.2041,  0.7352]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2612645321168372, distance: 1.2851676494380715 entropy 1.1324905157089233
epoch: 16, step: 8
	action: tensor([[ 0.8458, -1.9208,  1.7211, -0.7079,  0.1283,  0.8679,  1.0972]],
       dtype=torch.float64)
	q_value: tensor([[-46.8346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 9
	action: tensor([[ 1.3381, -2.3173,  1.8169,  0.2911, -0.4715, -0.1611,  0.5829]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 10
	action: tensor([[-0.2382, -0.9254,  1.4948, -1.6977, -0.5723,  1.4190,  0.6558]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5974667579058217, distance: 1.4463473628139276 entropy 1.1324905157089233
epoch: 16, step: 11
	action: tensor([[ 1.5130, -3.0656,  2.5461, -1.9092,  0.9571, -1.0727,  2.1448]],
       dtype=torch.float64)
	q_value: tensor([[-53.7753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 12
	action: tensor([[ 0.8853, -0.6350, -0.3297, -0.5217, -0.1097, -0.3323,  0.7608]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32551945980301644, distance: 1.3174973405717771 entropy 1.1324905157089233
epoch: 16, step: 13
	action: tensor([[ 1.2742, -3.1043,  1.6585,  0.2815, -0.2993, -0.0278,  2.8700]],
       dtype=torch.float64)
	q_value: tensor([[-39.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 14
	action: tensor([[ 1.7572, -0.2722, -0.5145, -0.5369,  1.2020,  0.2221,  1.6749]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22513612057000287, distance: 1.2666273617008204 entropy 1.1324905157089233
epoch: 16, step: 15
	action: tensor([[ 3.0081, -2.8180,  2.8703, -1.7501,  0.4303, -0.3222,  1.8291]],
       dtype=torch.float64)
	q_value: tensor([[-57.1518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 16
	action: tensor([[ 1.2243, -0.8589,  1.0181, -0.8393,  0.9396, -0.1361,  1.7237]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2507265784853958, distance: 1.279787555173563 entropy 1.1324905157089233
epoch: 16, step: 17
	action: tensor([[ 4.0252, -3.3574,  2.8673, -0.9947,  0.4529, -1.2641,  4.1978]],
       dtype=torch.float64)
	q_value: tensor([[-53.2721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 18
	action: tensor([[ 1.2227,  0.5207,  0.9269, -1.9809,  0.9442,  0.3377, -0.1269]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4811479735311819, distance: 0.8242870025885051 entropy 1.1324905157089233
epoch: 16, step: 19
	action: tensor([[ 2.2664, -2.8150,  3.0948, -2.2289, -0.3044,  0.3931,  2.2021]],
       dtype=torch.float64)
	q_value: tensor([[-48.0524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 20
	action: tensor([[ 0.3334, -0.4451, -0.0059,  0.0578, -0.3969, -0.2005,  0.1539]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17129578318462724, distance: 1.0417331095808309 entropy 1.1324905157089233
epoch: 16, step: 21
	action: tensor([[-1.0657, -0.8243,  0.5304, -0.8738,  0.9354,  0.8152,  2.4128]],
       dtype=torch.float64)
	q_value: tensor([[-30.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1571135714870668, distance: 1.6807116052549862 entropy 1.1324905157089233
epoch: 16, step: 22
	action: tensor([[ 2.2945, -3.2350,  2.8496, -2.2794, -0.6029, -0.8433,  4.2243]],
       dtype=torch.float64)
	q_value: tensor([[-58.8513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 23
	action: tensor([[ 0.9540, -2.0889,  0.8859, -0.6099,  0.2176,  0.6508,  0.2625]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 24
	action: tensor([[-0.5244, -0.5267,  1.2298, -0.9579, -0.5179,  0.2184, -0.6102]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1585096920232223, distance: 1.6812554098259413 entropy 1.1324905157089233
epoch: 16, step: 25
	action: tensor([[ 0.7989,  0.4202, -1.3834,  0.0444,  0.6261, -0.3099,  0.9719]],
       dtype=torch.float64)
	q_value: tensor([[-37.2478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9445567073339729, distance: 0.269451869033082 entropy 1.1324905157089233
epoch: 16, step: 26
	action: tensor([[ 1.4880, -2.8415,  2.6499, -1.3987,  0.4120,  0.5677,  2.0787]],
       dtype=torch.float64)
	q_value: tensor([[-43.0892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 27
	action: tensor([[ 1.2659, -1.0879,  1.5218,  0.7978, -1.3389, -0.3543,  0.6275]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.687580448633035, distance: 1.4865822258046675 entropy 1.1324905157089233
epoch: 16, step: 28
	action: tensor([[ 1.2222, -0.3759,  1.5747, -0.7324, -0.0937,  0.9909,  2.3343]],
       dtype=torch.float64)
	q_value: tensor([[-53.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4740648502292911, distance: 0.8298943191622575 entropy 1.1324905157089233
epoch: 16, step: 29
	action: tensor([[ 4.1583, -3.9500,  4.9706, -2.5146, -0.4953, -0.4966,  5.7055]],
       dtype=torch.float64)
	q_value: tensor([[-62.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 30
	action: tensor([[ 0.4664, -1.2705, -0.2680, -0.4255, -0.1356,  0.6950,  1.2237]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6709170052811331, distance: 1.4792246411182068 entropy 1.1324905157089233
epoch: 16, step: 31
	action: tensor([[ 2.6961, -1.5669,  2.4499, -0.9438, -1.5436,  1.4026,  3.9648]],
       dtype=torch.float64)
	q_value: tensor([[-48.8678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 32
	action: tensor([[ 0.4937, -0.0396,  1.8268,  0.3906, -0.0800,  0.4559, -0.4969]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 33
	action: tensor([[ 0.9384,  0.2022,  0.8830, -0.2901,  1.1197, -0.5247,  0.3837]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8334019756590934, distance: 0.46708037122799095 entropy 1.1324905157089233
epoch: 16, step: 34
	action: tensor([[ 1.3650, -1.7106,  1.8525, -2.1398,  0.1447, -0.0299,  0.1859]],
       dtype=torch.float64)
	q_value: tensor([[-40.3554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 35
	action: tensor([[ 0.3681, -1.7169,  0.1754, -1.4254,  0.1367,  0.8378,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9119234359901769, distance: 1.5823113438980294 entropy 1.1324905157089233
epoch: 16, step: 36
	action: tensor([[ 2.2913, -3.0664,  1.5017, -0.4951,  0.9950,  0.7725,  2.0019]],
       dtype=torch.float64)
	q_value: tensor([[-46.8806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 37
	action: tensor([[ 1.0827, -1.0478,  0.8834,  0.3225,  0.7205, -0.7328, -0.1786]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22689853571300067, distance: 1.267538085398019 entropy 1.1324905157089233
epoch: 16, step: 38
	action: tensor([[-0.0896, -0.5341,  1.1915,  0.1840,  0.8204, -1.7247,  0.8855]],
       dtype=torch.float64)
	q_value: tensor([[-44.4143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01441107594943336, distance: 1.1525603750753526 entropy 1.1324905157089233
epoch: 16, step: 39
	action: tensor([[ 1.6445, -0.9757,  0.4036, -1.0434,  0.2301, -0.1300,  1.2155]],
       dtype=torch.float64)
	q_value: tensor([[-43.8725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6075743902726176, distance: 1.4509158758447542 entropy 1.1324905157089233
epoch: 16, step: 40
	action: tensor([[ 2.5632, -2.0757,  3.6855, -1.8692, -0.7013,  0.1594,  2.0389]],
       dtype=torch.float64)
	q_value: tensor([[-51.1760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 41
	action: tensor([[ 0.7872, -0.5826, -0.8799,  0.4534,  0.5581,  1.2472,  1.4448]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7333120077072419, distance: 0.5909604602038927 entropy 1.1324905157089233
epoch: 16, step: 42
	action: tensor([[ 3.2414, -2.9621,  1.7367, -1.5262,  0.3234, -0.2772,  2.8542]],
       dtype=torch.float64)
	q_value: tensor([[-55.0317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 43
	action: tensor([[-0.7987, -0.2814,  1.1178, -0.9613,  0.9910,  0.6887, -0.5595]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2076712871044504, distance: 1.7002935130384766 entropy 1.1324905157089233
epoch: 16, step: 44
	action: tensor([[ 1.1948, -0.5501, -1.1040, -0.9575, -0.4536,  0.3308,  1.1813]],
       dtype=torch.float64)
	q_value: tensor([[-42.4570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28853531942351474, distance: 1.2989871559843205 entropy 1.1324905157089233
epoch: 16, step: 45
	action: tensor([[ 2.0824, -3.5919,  2.6240, -1.6170, -0.2842, -0.5661,  2.0413]],
       dtype=torch.float64)
	q_value: tensor([[-50.7510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 46
	action: tensor([[ 0.0572, -0.3091, -0.1069, -0.4883,  0.2710, -0.3571,  0.7547]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22800345592996973, distance: 1.2681087166423812 entropy 1.1324905157089233
epoch: 16, step: 47
	action: tensor([[ 1.4096, -1.0228,  1.4345, -0.9294, -0.5343, -0.3978,  2.1596]],
       dtype=torch.float64)
	q_value: tensor([[-33.6506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058717935723253456, distance: 1.1774618058839046 entropy 1.1324905157089233
epoch: 16, step: 48
	action: tensor([[ 2.3042, -2.7872,  2.2189, -1.3521,  0.8949, -0.7931,  3.6513]],
       dtype=torch.float64)
	q_value: tensor([[-58.0361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 49
	action: tensor([[ 0.4353, -1.7125,  1.7703,  0.4011, -0.3809,  0.6136, -0.2675]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3347223826324499, distance: 1.3220630426071316 entropy 1.1324905157089233
epoch: 16, step: 50
	action: tensor([[ 1.6267, -1.7474,  1.2806, -0.9427,  1.0917, -0.4935,  1.7760]],
       dtype=torch.float64)
	q_value: tensor([[-47.5300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 51
	action: tensor([[-0.1907,  0.4838,  1.3705,  0.1571, -0.3357,  1.0030,  0.3785]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 52
	action: tensor([[ 0.1713, -0.4798,  1.1027, -1.3802,  0.0332, -1.2086,  1.0523]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24989991503641384, distance: 1.2793645496649961 entropy 1.1324905157089233
epoch: 16, step: 53
	action: tensor([[ 1.2597, -1.3680,  0.9246,  0.3576,  0.6711,  1.1242,  0.3250]],
       dtype=torch.float64)
	q_value: tensor([[-43.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.466508631682933, distance: 1.3857950478110068 entropy 1.1324905157089233
epoch: 16, step: 54
	action: tensor([[ 0.4000, -2.1630,  1.1847, -2.0734, -0.9179,  1.3647,  2.2883]],
       dtype=torch.float64)
	q_value: tensor([[-54.0327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 55
	action: tensor([[ 0.4620, -0.6872,  0.4697, -1.1648,  0.4707,  0.1656,  0.9045]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6202765452948651, distance: 1.456636760737326 entropy 1.1324905157089233
epoch: 16, step: 56
	action: tensor([[ 2.4099, -2.5026,  1.7914, -1.8189,  0.3025, -1.7894,  3.5433]],
       dtype=torch.float64)
	q_value: tensor([[-41.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 57
	action: tensor([[ 0.0158, -0.5685,  0.0563, -0.9801, -0.0850,  0.1960,  1.1068]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6045574668932696, distance: 1.449553774751323 entropy 1.1324905157089233
epoch: 16, step: 58
	action: tensor([[-0.1796, -1.9535,  0.8019, -0.2402, -0.3675, -1.1067,  1.8877]],
       dtype=torch.float64)
	q_value: tensor([[-40.9653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 59
	action: tensor([[ 1.4273, -0.3438,  0.8851, -0.5057, -0.3518,  0.4082,  0.5725]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28631225593734677, distance: 0.9667425026316224 entropy 1.1324905157089233
epoch: 16, step: 60
	action: tensor([[-2.4487e-03, -2.0323e+00,  2.5381e+00, -1.1540e+00, -1.8022e-01,
         -9.9881e-02,  2.5379e+00]], dtype=torch.float64)
	q_value: tensor([[-42.4567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 61
	action: tensor([[ 0.8585, -0.0792,  1.1206, -1.4383,  0.8721, -1.1931,  0.3787]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4257647983550129, distance: 0.867164717386323 entropy 1.1324905157089233
epoch: 16, step: 62
	action: tensor([[ 1.1261, -1.6554,  2.3676, -0.2782,  0.3904, -1.4809,  0.9826]],
       dtype=torch.float64)
	q_value: tensor([[-43.8273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 63
	action: tensor([[ 0.7016, -0.3340,  0.9834, -1.5740,  0.6851, -1.5076, -0.7417]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26711090887732536, distance: 0.9796610060717423 entropy 1.1324905157089233
epoch: 16, step: 64
	action: tensor([[ 0.2811, -0.1226,  0.4909, -1.0458,  1.0571,  0.7629,  1.1630]],
       dtype=torch.float64)
	q_value: tensor([[-44.9637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028571774210769174, distance: 1.1605770931697736 entropy 1.1324905157089233
epoch: 16, step: 65
	action: tensor([[ 2.2924, -3.6914,  2.7258, -1.4114,  1.2395, -1.7773,  2.7308]],
       dtype=torch.float64)
	q_value: tensor([[-46.7697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 66
	action: tensor([[ 1.0507,  0.2338,  0.9939,  0.2598, -0.0933,  1.3055,  0.1737]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 67
	action: tensor([[ 0.2331, -1.0938,  0.5966,  0.6421,  0.8557, -0.6325,  2.5613]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15161255215252578, distance: 1.2280325780650192 entropy 1.1324905157089233
epoch: 16, step: 68
	action: tensor([[ 1.8845, -2.5590,  1.7709, -0.8547,  1.1398,  0.2294,  3.4060]],
       dtype=torch.float64)
	q_value: tensor([[-58.5299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 69
	action: tensor([[-0.7969, -0.0230,  1.4827, -0.2739,  0.0312, -1.5273,  1.1855]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7240826872729436, distance: 1.50257354671784 entropy 1.1324905157089233
epoch: 16, step: 70
	action: tensor([[ 0.7551, -0.9183, -0.0759, -0.9727, -0.0460, -0.0398,  2.0862]],
       dtype=torch.float64)
	q_value: tensor([[-42.0416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.800877140863899, distance: 1.53567295535392 entropy 1.1324905157089233
epoch: 16, step: 71
	action: tensor([[ 1.2277, -2.7707,  3.5949, -2.8467,  0.9678,  0.2608,  3.8675]],
       dtype=torch.float64)
	q_value: tensor([[-54.8856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 72
	action: tensor([[ 1.0447, -1.4326, -0.3277, -1.1637, -0.6666,  0.2048,  0.5686]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0404961394581966, distance: 1.6346492592011026 entropy 1.1324905157089233
epoch: 16, step: 73
	action: tensor([[ 2.7257, -2.7385,  2.1933, -1.9247,  1.3245, -0.0370,  2.1324]],
       dtype=torch.float64)
	q_value: tensor([[-47.7143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 74
	action: tensor([[-0.2478, -1.7937,  0.6180, -1.7519,  1.2380,  0.5407, -0.7924]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 75
	action: tensor([[ 0.5982, -1.0495,  0.4909, -0.9284,  1.0062, -0.3571,  1.7714]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.821984391396497, distance: 1.5446461964256804 entropy 1.1324905157089233
epoch: 16, step: 76
	action: tensor([[ 1.6443, -2.5492,  2.7124, -1.7478,  0.5956, -0.5075,  3.7089]],
       dtype=torch.float64)
	q_value: tensor([[-51.3862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 77
	action: tensor([[ 1.0033, -0.0120,  0.9643, -0.9157,  0.5014, -0.4604,  1.1728]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45076489160948663, distance: 0.8480780805263044 entropy 1.1324905157089233
epoch: 16, step: 78
	action: tensor([[ 1.7716, -1.9441,  1.1100, -0.9241,  0.5991,  0.3051,  2.3137]],
       dtype=torch.float64)
	q_value: tensor([[-43.0070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 79
	action: tensor([[ 0.8314, -1.0952,  1.6331, -0.0497,  0.5901,  0.1665,  0.8853]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5203488839251205, distance: 1.4110042539698373 entropy 1.1324905157089233
epoch: 16, step: 80
	action: tensor([[ 2.4401, -1.9117,  0.7706, -0.7387,  0.7438, -0.5731,  1.4315]],
       dtype=torch.float64)
	q_value: tensor([[-48.3136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 81
	action: tensor([[ 0.2688, -0.2903,  2.0112,  0.8988,  2.4835,  0.5683, -0.4528]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23563404796289433, distance: 1.0004775781806756 entropy 1.1324905157089233
epoch: 16, step: 82
	action: tensor([[ 1.1629, -1.7457,  0.5748, -1.2527,  1.0535,  1.0696,  1.9459]],
       dtype=torch.float64)
	q_value: tensor([[-62.9564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 83
	action: tensor([[ 1.1253, -1.6657,  0.8801, -0.0574, -0.4851, -0.7511,  0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7562196244220734, distance: 1.5165128902984795 entropy 1.1324905157089233
epoch: 16, step: 84
	action: tensor([[ 0.9189, -1.1722,  2.4963,  0.0564, -0.7983,  0.6546,  0.3257]],
       dtype=torch.float64)
	q_value: tensor([[-43.9475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5574803872699785, distance: 1.4281308031929871 entropy 1.1324905157089233
epoch: 16, step: 85
	action: tensor([[ 2.2680, -2.6458,  0.5136, -0.8412, -0.1402,  0.0518,  3.2099]],
       dtype=torch.float64)
	q_value: tensor([[-53.3445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 86
	action: tensor([[ 0.4995, -0.0921, -0.0428, -0.4932, -1.4400,  0.8285,  1.5895]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974955971072085, distance: 0.9591382670323936 entropy 1.1324905157089233
epoch: 16, step: 87
	action: tensor([[ 2.8381, -1.5958,  2.3519, -1.2127,  1.3758, -1.1837,  3.3435]],
       dtype=torch.float64)
	q_value: tensor([[-50.1452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 88
	action: tensor([[ 0.7800, -0.4989,  0.9482,  0.0722,  1.7019,  0.2849,  0.6606]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.211204143556315, distance: 1.0163399652685787 entropy 1.1324905157089233
epoch: 16, step: 89
	action: tensor([[ 2.8274, -0.9784,  0.9086, -1.6314,  0.2253, -0.2593,  2.5473]],
       dtype=torch.float64)
	q_value: tensor([[-49.5277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 90
	action: tensor([[ 0.9965,  0.4403,  0.9376, -1.9641, -1.3625, -0.9924,  1.2356]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3275491178200918, distance: 0.9383977917121404 entropy 1.1324905157089233
epoch: 16, step: 91
	action: tensor([[ 1.4479, -2.4228,  1.1122, -1.0011, -0.0119, -1.0122,  2.4203]],
       dtype=torch.float64)
	q_value: tensor([[-51.2121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 92
	action: tensor([[ 0.3171, -1.4098,  1.2007, -0.8557,  1.0869,  1.3240,  0.8976]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7292380297222625, distance: 1.5048183633863172 entropy 1.1324905157089233
epoch: 16, step: 93
	action: tensor([[ 4.0796, -4.2032,  1.7303, -2.9708,  0.8191, -0.2750,  2.6835]],
       dtype=torch.float64)
	q_value: tensor([[-54.6790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 94
	action: tensor([[ 2.7342, -1.1968,  0.6409, -0.2257, -0.4024, -0.0817,  0.2653]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 95
	action: tensor([[ 1.3932,  0.3965,  0.5340, -0.5980, -0.0115, -0.4900,  0.0774]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6796078332756815, distance: 0.6477354078480282 entropy 1.1324905157089233
epoch: 16, step: 96
	action: tensor([[ 1.2851, -0.1454,  0.9372, -0.9863,  1.5269,  0.6716,  0.2258]],
       dtype=torch.float64)
	q_value: tensor([[-36.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20320997080488945, distance: 1.021477109021493 entropy 1.1324905157089233
epoch: 16, step: 97
	action: tensor([[ 2.6485, -2.1277,  4.1064, -0.6581, -0.3672, -0.5000,  2.9606]],
       dtype=torch.float64)
	q_value: tensor([[-48.7359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 98
	action: tensor([[ 1.0671,  0.8308,  0.7350, -0.6634,  1.2946,  0.9152, -0.7823]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 99
	action: tensor([[ 0.3932,  0.0513,  1.2089,  0.0271, -0.2590, -0.4246,  1.8579]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5564781422920617, distance: 0.7621039403654848 entropy 1.1324905157089233
epoch: 16, step: 100
	action: tensor([[ 1.6244, -1.9316,  2.1794, -0.8582, -0.7135,  0.6608,  2.1194]],
       dtype=torch.float64)
	q_value: tensor([[-45.5548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 101
	action: tensor([[-1.3539, -0.7408,  1.2109, -1.0747,  0.6533, -0.0209,  0.5241]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5890963808438316, distance: 1.8413264940909209 entropy 1.1324905157089233
epoch: 16, step: 102
	action: tensor([[ 1.3964, -1.4322,  0.5868, -0.8053,  0.8241,  0.3102,  0.2152]],
       dtype=torch.float64)
	q_value: tensor([[-42.3562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 103
	action: tensor([[-0.5245, -1.7565,  0.9777, -0.7696,  0.9540, -0.3704,  1.1594]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 104
	action: tensor([[ 1.3813, -1.6838,  0.1297,  0.6166,  0.6151, -1.2852,  1.6116]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27893138519671523, distance: 1.294137184088593 entropy 1.1324905157089233
epoch: 16, step: 105
	action: tensor([[ 2.5561, -0.5345,  2.1623, -2.0054, -0.2470,  1.1120,  1.4584]],
       dtype=torch.float64)
	q_value: tensor([[-56.9802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 106
	action: tensor([[ 0.5539, -0.3016,  1.0233, -1.0838, -0.2099,  0.1851,  0.9325]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0832525323538762, distance: 1.1910268434554045 entropy 1.1324905157089233
epoch: 16, step: 107
	action: tensor([[ 1.7223, -2.9847,  0.7358, -1.1129,  0.5176,  0.1047,  3.1992]],
       dtype=torch.float64)
	q_value: tensor([[-41.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 108
	action: tensor([[-0.6346, -0.2770, -0.0064, -0.3036, -0.8436,  0.7213,  1.2792]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.809812184991807, distance: 1.539477859045655 entropy 1.1324905157089233
epoch: 16, step: 109
	action: tensor([[ 2.5652, -0.8273,  1.5863, -0.8965,  0.4355, -0.4834,  2.2620]],
       dtype=torch.float64)
	q_value: tensor([[-41.8740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 110
	action: tensor([[-0.4464, -1.1167,  0.9017, -1.3573,  0.3084,  0.4563,  0.1541]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2042893291602468, distance: 1.6989906639408074 entropy 1.1324905157089233
epoch: 16, step: 111
	action: tensor([[ 2.2315, -2.8676,  0.6694, -0.8985, -0.5364,  0.9135,  1.3017]],
       dtype=torch.float64)
	q_value: tensor([[-43.4489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 112
	action: tensor([[ 0.7643, -0.7981,  0.5014, -0.9730, -0.9493,  1.4960,  2.1666]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01265933399489838, distance: 1.1370778657871579 entropy 1.1324905157089233
epoch: 16, step: 113
	action: tensor([[ 5.1936, -4.8397,  4.4337, -1.1066,  1.2990,  0.1474,  5.8977]],
       dtype=torch.float64)
	q_value: tensor([[-62.6470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 114
	action: tensor([[ 1.1485, -0.6746, -0.3778, -0.6219, -0.8354,  0.1831,  0.5785]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39642306718798537, distance: 1.3522755642918973 entropy 1.1324905157089233
epoch: 16, step: 115
	action: tensor([[ 1.2941, -1.5118,  0.3390, -2.0096, -1.4140, -0.3414,  1.6882]],
       dtype=torch.float64)
	q_value: tensor([[-43.2732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 116
	action: tensor([[ 0.4020,  0.6310, -0.2337, -1.8255, -0.3883, -2.9546, -0.1980]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 117
	action: tensor([[ 0.9725, -1.6953,  0.1682, -0.4637, -0.3822,  0.0288,  1.1616]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9347703560843972, distance: 1.5917373445393088 entropy 1.1324905157089233
epoch: 16, step: 118
	action: tensor([[ 2.2940, -2.6624,  0.8411, -1.8758, -0.9934, -0.0421,  2.6639]],
       dtype=torch.float64)
	q_value: tensor([[-47.4989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 119
	action: tensor([[ 0.5093, -0.1985,  0.8001,  0.4443, -2.1968, -1.0275,  2.0874]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7587456498774349, distance: 0.562075000780646 entropy 1.1324905157089233
epoch: 16, step: 120
	action: tensor([[ 4.1299, -0.5884,  2.8383, -1.3998,  0.6266,  0.6607,  2.2534]],
       dtype=torch.float64)
	q_value: tensor([[-58.4604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 121
	action: tensor([[ 0.7032, -1.5580,  0.0082, -0.7633, -0.4288, -0.4453,  1.2253]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.97674505579354, distance: 1.6089110109258935 entropy 1.1324905157089233
epoch: 16, step: 122
	action: tensor([[ 1.0730, -2.2857,  2.6696, -1.7870,  0.4009,  1.4232,  2.3494]],
       dtype=torch.float64)
	q_value: tensor([[-46.2214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 123
	action: tensor([[ 0.2815, -0.3083,  1.7129,  0.2210, -1.2991,  0.4181,  1.6081]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4669863202275194, distance: 0.8354604025826331 entropy 1.1324905157089233
epoch: 16, step: 124
	action: tensor([[ 3.2909, -3.7890,  2.2460, -0.5592,  0.0449,  0.2082,  3.2910]],
       dtype=torch.float64)
	q_value: tensor([[-50.4554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 16, step: 125
	action: tensor([[ 0.0929, -0.6650, -0.0283, -1.0361,  0.4892, -1.4615,  0.5093]],
       dtype=torch.float64)
	q_value: tensor([[-41.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4058414411414357, distance: 1.3568282081248981 entropy 1.1324905157089233
epoch: 16, step: 126
	action: tensor([[ 0.2229,  0.0545,  0.0856, -0.6771,  0.6679,  0.2251,  0.7561]],
       dtype=torch.float64)
	q_value: tensor([[-40.9609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23763046760356232, distance: 0.9991701684220765 entropy 1.1324905157089233
epoch: 16, step: 127
	action: tensor([[ 1.0066, -1.6618,  2.6247, -0.8784, -0.7183,  0.8603, -0.2776]],
       dtype=torch.float64)
	q_value: tensor([[-36.0625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
LOSS epoch 16 actor 393.44360730498937 critic 57.642846273906684 
epoch: 17, step: 0
	action: tensor([[ 0.1932, -0.0253,  0.0209, -0.3447,  0.4816,  0.1627, -0.3435]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32551824655871875, distance: 0.939813752657502 entropy 1.1324905157089233
epoch: 17, step: 1
	action: tensor([[ 0.5068, -1.7421, -0.0209, -1.6787, -0.6824,  0.0551,  0.6615]],
       dtype=torch.float64)
	q_value: tensor([[-30.8606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 2
	action: tensor([[ 1.5027, -0.0862,  0.4587,  0.4684, -0.6435, -1.6413,  0.7691]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 3
	action: tensor([[-0.9991, -0.4046,  0.1741, -0.8011,  0.2293, -0.8594, -0.1834]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.026273548496527, distance: 1.6289424113969686 entropy 1.1324905157089233
epoch: 17, step: 4
	action: tensor([[ 0.0396,  0.9421, -0.4788,  1.2309,  0.4254, -0.3009,  1.6309]],
       dtype=torch.float64)
	q_value: tensor([[-34.4182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 5
	action: tensor([[-0.5071, -0.2041,  1.8651, -0.3702,  0.6401, -0.6259,  0.8282]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5610207508061711, distance: 1.4297530490401547 entropy 1.1324905157089233
epoch: 17, step: 6
	action: tensor([[ 2.0243, -0.5276, -0.1178,  0.7036, -0.2109, -0.3130,  0.3024]],
       dtype=torch.float64)
	q_value: tensor([[-44.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.250446090044502, distance: 0.9907364247011082 entropy 1.1324905157089233
epoch: 17, step: 7
	action: tensor([[ 1.1660, -1.8991,  2.3240,  0.3091,  1.5279,  0.2937,  0.6678]],
       dtype=torch.float64)
	q_value: tensor([[-46.8898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 8
	action: tensor([[-0.5623, -0.5807,  0.1984,  0.0027, -0.3749,  0.9171,  0.3701]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5594415050851633, distance: 1.4290296432987883 entropy 1.1324905157089233
epoch: 17, step: 9
	action: tensor([[ 1.7551, -1.0266,  1.3934, -1.0463,  0.7816, -0.3663,  0.6460]],
       dtype=torch.float64)
	q_value: tensor([[-38.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17158173224441353, distance: 1.041553366134291 entropy 1.1324905157089233
epoch: 17, step: 10
	action: tensor([[ 1.3984, -2.1905,  2.3614,  0.3406, -0.4757,  0.4584,  2.8531]],
       dtype=torch.float64)
	q_value: tensor([[-54.1726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 11
	action: tensor([[-0.3570, -0.0716,  0.2613, -0.4689, -0.6857,  0.2606,  1.5272]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43227774397725316, distance: 1.3695261053962433 entropy 1.1324905157089233
epoch: 17, step: 12
	action: tensor([[ 2.1754, -2.5793,  0.8401,  0.1344,  0.0489, -0.4614,  2.1798]],
       dtype=torch.float64)
	q_value: tensor([[-43.4778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 13
	action: tensor([[-0.2724,  0.3509, -1.0063, -0.1827,  0.1758,  0.1515, -0.7328]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 14
	action: tensor([[ 0.3281, -0.5350, -0.1201, -0.3610,  0.8116, -0.4579,  0.3021]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28772307344451153, distance: 1.2985776742244919 entropy 1.1324905157089233
epoch: 17, step: 15
	action: tensor([[-0.5455,  0.7141,  0.9251, -0.9520,  0.0258, -0.6804,  1.5819]],
       dtype=torch.float64)
	q_value: tensor([[-37.2169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5869861212665124, distance: 1.4415949677691953 entropy 1.1324905157089233
epoch: 17, step: 16
	action: tensor([[ 0.4381, -0.7630,  1.0045, -0.4734,  1.5725, -0.2016,  0.5108]],
       dtype=torch.float64)
	q_value: tensor([[-44.4450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36716711335759156, distance: 1.3380350648534582 entropy 1.1324905157089233
epoch: 17, step: 17
	action: tensor([[ 1.9652, -1.3636,  1.6214, -1.9200,  0.0923,  0.8858,  1.3639]],
       dtype=torch.float64)
	q_value: tensor([[-47.8262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 18
	action: tensor([[ 1.8449, -1.7549, -0.9719, -0.0370,  0.9337,  1.5433,  0.3085]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 19
	action: tensor([[-0.2616, -2.0902,  0.4844, -0.6288,  0.5492,  2.0521,  0.2897]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 20
	action: tensor([[ 0.1187, -1.2507,  0.4188, -0.3712,  1.0798, -0.0661,  0.1002]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.028430572636641, distance: 1.6298092078630861 entropy 1.1324905157089233
epoch: 17, step: 21
	action: tensor([[-0.7594,  0.1253,  1.0814, -0.9307,  0.4235, -0.6425,  0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-44.4741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0452692020194734, distance: 1.6365600018008322 entropy 1.1324905157089233
epoch: 17, step: 22
	action: tensor([[ 0.0996, -0.8269,  0.1480, -0.4561, -0.6698,  0.3701,  0.1815]],
       dtype=torch.float64)
	q_value: tensor([[-35.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.645995033116211, distance: 1.4681517714135017 entropy 1.1324905157089233
epoch: 17, step: 23
	action: tensor([[ 1.0606, -1.1207,  1.6203, -0.4410, -0.3680, -1.3491,  1.1446]],
       dtype=torch.float64)
	q_value: tensor([[-38.2415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10043887150786346, distance: 1.2004377791045027 entropy 1.1324905157089233
epoch: 17, step: 24
	action: tensor([[ 1.3129, -1.6498,  1.6081, -0.7411,  1.3666,  0.5648,  1.0939]],
       dtype=torch.float64)
	q_value: tensor([[-53.4005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 25
	action: tensor([[ 0.4445, -1.6380,  1.6734, -0.9470, -0.6372, -0.0366, -0.1737]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6147632816510409, distance: 1.4541564231300212 entropy 1.1324905157089233
epoch: 17, step: 26
	action: tensor([[ 1.0804, -1.6335,  1.4508, -0.1722, -0.2683, -1.0435,  1.3663]],
       dtype=torch.float64)
	q_value: tensor([[-46.3677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 27
	action: tensor([[ 0.6079, -0.7833,  0.8378,  0.6841,  0.1074, -0.3589, -0.3295]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2693905667000238, distance: 0.9781361974221268 entropy 1.1324905157089233
epoch: 17, step: 28
	action: tensor([[ 0.5203, -0.7349,  1.8830, -0.9958, -0.1268,  0.5802, -0.7826]],
       dtype=torch.float64)
	q_value: tensor([[-42.2034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0197805602085811, distance: 1.1329698397807737 entropy 1.1324905157089233
epoch: 17, step: 29
	action: tensor([[ 2.1222,  0.7532,  0.5857, -1.1638, -0.4551, -0.3987,  1.4893]],
       dtype=torch.float64)
	q_value: tensor([[-46.2679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 30
	action: tensor([[-0.0225,  0.1207,  0.5064, -1.0086, -0.3244, -0.0379,  0.2756]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24522250489930886, distance: 1.2769684691389849 entropy 1.1324905157089233
epoch: 17, step: 31
	action: tensor([[ 1.5460,  0.7798, -1.1922, -1.7121, -0.6617,  0.6075, -0.2117]],
       dtype=torch.float64)
	q_value: tensor([[-33.6348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033535755967662895, distance: 1.1249924008732142 entropy 1.1324905157089233
epoch: 17, step: 32
	action: tensor([[ 0.2721, -1.8706,  1.7186, -0.9765,  0.0171,  0.0888,  2.3195]],
       dtype=torch.float64)
	q_value: tensor([[-49.8463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 33
	action: tensor([[-1.1546, -0.8519,  1.0669, -1.6306,  0.5746, -0.4690,  0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8462762851719052, distance: 1.5549092206588546 entropy 1.1324905157089233
epoch: 17, step: 34
	action: tensor([[ 0.2613, -1.6336,  0.9635, -0.4463,  0.1144,  0.1891, -0.3292]],
       dtype=torch.float64)
	q_value: tensor([[-43.8994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 35
	action: tensor([[-0.4078,  0.9795,  0.6941, -1.0703, -0.1216,  0.0804,  1.6992]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24046899447984038, distance: 1.2745287898992403 entropy 1.1324905157089233
epoch: 17, step: 36
	action: tensor([[ 1.4815, -2.0189,  2.1985, -0.3255,  1.4968,  0.3651,  1.7795]],
       dtype=torch.float64)
	q_value: tensor([[-47.4340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 37
	action: tensor([[ 0.0726,  0.9311,  1.0378,  1.4957,  0.3967, -0.7847,  1.4799]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 38
	action: tensor([[ 0.2298,  0.2716, -1.7697, -1.4569, -0.5998, -0.2889, -0.2160]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 39
	action: tensor([[ 1.0977, -0.9622, -0.1411,  0.4655,  0.5241,  0.6819,  0.7758]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3557656121823103, distance: 0.9184989087472595 entropy 1.1324905157089233
epoch: 17, step: 40
	action: tensor([[ 0.1909, -1.9878,  0.9550, -1.7432, -0.9368, -0.8634,  3.3331]],
       dtype=torch.float64)
	q_value: tensor([[-51.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 41
	action: tensor([[-0.9079, -0.6889,  0.8067, -0.0103,  0.4365,  0.0948,  0.4451]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2019449825664288, distance: 1.698086952592359 entropy 1.1324905157089233
epoch: 17, step: 42
	action: tensor([[ 0.0835,  0.2306,  0.2968, -0.3214, -0.6954, -0.5173,  1.3921]],
       dtype=torch.float64)
	q_value: tensor([[-36.9134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33301427440758713, distance: 0.934576729339564 entropy 1.1324905157089233
epoch: 17, step: 43
	action: tensor([[-0.6404, -0.2832,  2.0494, -0.0073,  0.6977,  0.2767,  3.0734]],
       dtype=torch.float64)
	q_value: tensor([[-40.2302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6639994459079306, distance: 1.476159486793412 entropy 1.1324905157089233
epoch: 17, step: 44
	action: tensor([[ 2.4819, -2.2758,  2.8596, -2.1408, -0.0409, -1.0292,  2.8963]],
       dtype=torch.float64)
	q_value: tensor([[-66.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 45
	action: tensor([[ 0.0014, -0.0863, -0.0705,  0.1560, -0.3063,  0.4208, -0.3571]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39115194374493867, distance: 0.8929171310915155 entropy 1.1324905157089233
epoch: 17, step: 46
	action: tensor([[-0.4341,  0.0398, -0.2941, -0.6927, -0.1484, -0.4695,  1.1616]],
       dtype=torch.float64)
	q_value: tensor([[-29.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2536129784262866, distance: 1.2812634372814389 entropy 1.1324905157089233
epoch: 17, step: 47
	action: tensor([[-0.9119, -0.6663,  2.0943, -0.5872, -0.0651, -0.2174,  2.5754]],
       dtype=torch.float64)
	q_value: tensor([[-37.4097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4113779516652922, distance: 1.7770078081340244 entropy 1.1324905157089233
epoch: 17, step: 48
	action: tensor([[ 1.2530, -2.5736,  0.5117, -1.0873,  1.0970, -0.6307,  3.9258]],
       dtype=torch.float64)
	q_value: tensor([[-58.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 49
	action: tensor([[-1.4465, -0.8066, -0.9309, -0.3965,  0.5026,  0.9668,  0.9183]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5188786543706607, distance: 1.8161859695065445 entropy 1.1324905157089233
epoch: 17, step: 50
	action: tensor([[ 2.6791, -0.9391,  0.8615, -0.8494,  0.0529,  0.8833,  2.0648]],
       dtype=torch.float64)
	q_value: tensor([[-49.7705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 51
	action: tensor([[-0.1770, -1.6254,  0.7082, -0.4410, -0.3978, -0.1771,  0.1656]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.169610375734122, distance: 1.6855730071017772 entropy 1.1324905157089233
epoch: 17, step: 52
	action: tensor([[ 0.5466,  0.0567, -0.7930, -0.1672,  0.1553,  0.8551, -0.5168]],
       dtype=torch.float64)
	q_value: tensor([[-39.7727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7814708644604456, distance: 0.5349477523926726 entropy 1.1324905157089233
epoch: 17, step: 53
	action: tensor([[ 0.6042, -0.9530,  0.1221, -1.6531,  0.8573,  0.4395, -0.2048]],
       dtype=torch.float64)
	q_value: tensor([[-35.8566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7427950190836086, distance: 1.510705632022843 entropy 1.1324905157089233
epoch: 17, step: 54
	action: tensor([[ 1.4581, -2.5229,  2.0562, -2.5673, -0.6114,  0.4322,  1.4744]],
       dtype=torch.float64)
	q_value: tensor([[-49.6713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 55
	action: tensor([[-0.0054, -0.8046,  0.2144, -0.6321,  1.9091,  1.1075,  0.8983]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.337305968315621, distance: 1.3233419646848152 entropy 1.1324905157089233
epoch: 17, step: 56
	action: tensor([[ 2.7705, -3.3330,  0.9618, -0.6941, -0.2537,  0.3451,  3.5244]],
       dtype=torch.float64)
	q_value: tensor([[-58.0815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 57
	action: tensor([[-0.4471, -0.6405,  0.5118,  0.5075,  0.8625, -0.0396, -0.9422]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3454994502341584, distance: 1.3273897373570736 entropy 1.1324905157089233
epoch: 17, step: 58
	action: tensor([[ 0.6592, -0.2854, -0.1221, -0.9722, -0.2897, -0.9765, -0.1103]],
       dtype=torch.float64)
	q_value: tensor([[-40.7252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11742911983966864, distance: 1.209669374489701 entropy 1.1324905157089233
epoch: 17, step: 59
	action: tensor([[ 1.2559,  0.0891,  0.8897,  0.7235, -0.3668,  0.4517,  1.2305]],
       dtype=torch.float64)
	q_value: tensor([[-37.0155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6745859591750607, distance: 0.6527920205495104 entropy 1.1324905157089233
epoch: 17, step: 60
	action: tensor([[ 1.7866, -1.9908,  2.6227, -0.3281, -1.9815, -0.4082,  2.2567]],
       dtype=torch.float64)
	q_value: tensor([[-50.6281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 61
	action: tensor([[ 0.5731, -0.7648, -0.5761, -1.1118, -0.4536, -0.5144,  1.1458]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34383736390777675, distance: 1.3265696262488178 entropy 1.1324905157089233
epoch: 17, step: 62
	action: tensor([[ 0.3894, -2.8922,  1.1012,  0.7022, -0.5261, -0.4463,  1.5262]],
       dtype=torch.float64)
	q_value: tensor([[-46.5805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 63
	action: tensor([[ 0.4704, -1.1333,  0.9577, -0.6028,  0.4100,  0.4835,  0.4198]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7586866104040901, distance: 1.517577649818703 entropy 1.1324905157089233
epoch: 17, step: 64
	action: tensor([[ 0.5131, -1.4555,  1.1068, -0.5380,  0.0082, -0.3086,  0.8871]],
       dtype=torch.float64)
	q_value: tensor([[-45.6345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 65
	action: tensor([[ 0.3144,  0.9717,  0.3892, -0.2874,  1.0868, -0.6605,  0.2364]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 66
	action: tensor([[ 0.3179, -0.7182,  0.3994,  1.1835, -0.6374,  0.9041,  0.2237]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8233436018964533, distance: 0.4809737547236071 entropy 1.1324905157089233
epoch: 17, step: 67
	action: tensor([[-1.4129, -2.7540,  1.0422, -0.7646, -0.1903, -0.9902,  0.3092]],
       dtype=torch.float64)
	q_value: tensor([[-46.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 68
	action: tensor([[-0.9790,  1.0031, -0.3055, -0.7920, -0.5268, -0.3500,  0.6054]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14656436616299673, distance: 1.2253380322310876 entropy 1.1324905157089233
epoch: 17, step: 69
	action: tensor([[ 2.2440, -0.4934,  0.2922,  0.5164,  1.6349, -0.2438, -1.2341]],
       dtype=torch.float64)
	q_value: tensor([[-34.9903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 70
	action: tensor([[ 1.1733,  0.1146, -0.7206, -0.1524, -0.9044, -1.0946,  2.4057]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 71
	action: tensor([[ 1.5768, -0.8287,  0.2401, -0.2179, -0.1000,  1.1116, -0.0460]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05054818006670714, distance: 1.1729099773957343 entropy 1.1324905157089233
epoch: 17, step: 72
	action: tensor([[ 0.3974, -2.2951,  1.0521, -0.0544, -0.7002,  0.0452,  2.7212]],
       dtype=torch.float64)
	q_value: tensor([[-52.0746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 73
	action: tensor([[-0.2728, -0.5808,  1.2689,  0.2623, -0.0265,  0.7463,  0.3128]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13853539497546463, distance: 1.0621244647995582 entropy 1.1324905157089233
epoch: 17, step: 74
	action: tensor([[ 2.0555, -1.2551,  0.6972, -1.1017, -0.3218,  0.6035,  1.8479]],
       dtype=torch.float64)
	q_value: tensor([[-41.9902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5488596422172043, distance: 1.4241729250525 entropy 1.1324905157089233
epoch: 17, step: 75
	action: tensor([[ 3.0638, -2.2243,  2.8882, -2.1723,  1.8587, -0.3583,  3.9900]],
       dtype=torch.float64)
	q_value: tensor([[-65.2159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 76
	action: tensor([[ 0.9419,  0.0780, -0.1978, -1.6705,  1.5412, -0.5327,  0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 77
	action: tensor([[-0.0272, -0.3863, -0.1559,  0.4229, -0.1215, -0.2188,  0.8293]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11530337826434744, distance: 1.076350898579373 entropy 1.1324905157089233
epoch: 17, step: 78
	action: tensor([[ 2.6269, -0.3140,  2.8234, -0.7221, -0.6453, -1.0103,  1.0526]],
       dtype=torch.float64)
	q_value: tensor([[-35.8485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 79
	action: tensor([[ 1.1285, -0.6469, -0.9448, -0.5574,  0.8529,  0.5607, -0.5462]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027227349029456338, distance: 1.159818361837857 entropy 1.1324905157089233
epoch: 17, step: 80
	action: tensor([[ 0.5886, -0.7071,  0.1525, -0.0148,  0.9066,  0.8469,  0.1286]],
       dtype=torch.float64)
	q_value: tensor([[-47.5631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22732249109404956, distance: 1.0059023639208478 entropy 1.1324905157089233
epoch: 17, step: 81
	action: tensor([[-0.1957, -2.8655,  2.4059, -0.3261,  0.5015, -0.4911,  1.2529]],
       dtype=torch.float64)
	q_value: tensor([[-45.0737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 82
	action: tensor([[ 0.6786, -0.4975, -0.5997,  0.0396,  0.1249, -0.3587,  1.2104]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18579272978975392, distance: 1.032581122915293 entropy 1.1324905157089233
epoch: 17, step: 83
	action: tensor([[ 1.4896, -1.1787,  0.6916, -0.1723,  0.5916, -0.1084,  1.3095]],
       dtype=torch.float64)
	q_value: tensor([[-44.9055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7255491831009195, distance: 1.5032124515330518 entropy 1.1324905157089233
epoch: 17, step: 84
	action: tensor([[ 2.3373, -2.1457,  1.7858, -2.4372, -0.0418,  0.0311,  1.4975]],
       dtype=torch.float64)
	q_value: tensor([[-57.1393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 85
	action: tensor([[ 1.1393, -0.6963,  0.3711, -1.1526, -0.0319,  1.1299,  0.2709]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15789855896147365, distance: 1.2313795873467162 entropy 1.1324905157089233
epoch: 17, step: 86
	action: tensor([[ 2.5436, -1.7097,  0.2149, -0.0522,  0.2915,  0.0547,  1.5712]],
       dtype=torch.float64)
	q_value: tensor([[-50.7418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 87
	action: tensor([[ 0.3524,  0.0505,  0.1196, -1.2181, -0.0388,  0.6393,  0.7651]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1844121148231631, distance: 1.0334562030000456 entropy 1.1324905157089233
epoch: 17, step: 88
	action: tensor([[ 0.6764, -2.5357,  1.0573, -0.3205,  0.0965,  0.0104,  0.6469]],
       dtype=torch.float64)
	q_value: tensor([[-42.9230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 89
	action: tensor([[ 0.6267, -0.3265,  0.6501,  1.0581,  1.8239,  0.8185,  1.4011]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7497361870634804, distance: 0.5724739402168002 entropy 1.1324905157089233
epoch: 17, step: 90
	action: tensor([[ 2.0549, -2.1402,  2.6554,  1.3269,  0.0435,  1.1593,  1.7263]],
       dtype=torch.float64)
	q_value: tensor([[-62.2545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 91
	action: tensor([[ 0.1428,  0.5794,  0.4649, -0.9027, -0.2073, -0.4800,  0.9351]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2499220687132908, distance: 0.9910826815254186 entropy 1.1324905157089233
epoch: 17, step: 92
	action: tensor([[ 1.1755, -0.8663,  0.0096, -1.2272, -0.4238, -0.5868,  1.0708]],
       dtype=torch.float64)
	q_value: tensor([[-37.4099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7236069519622745, distance: 1.502366225857693 entropy 1.1324905157089233
epoch: 17, step: 93
	action: tensor([[ 2.0745, -2.2408,  0.7414, -0.6767, -0.0783,  0.0174,  2.1090]],
       dtype=torch.float64)
	q_value: tensor([[-49.9707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 94
	action: tensor([[ 0.2935,  0.1343,  1.0051, -0.5981, -1.2993,  0.7305, -0.5979]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3992529363754518, distance: 0.8869569102120916 entropy 1.1324905157089233
epoch: 17, step: 95
	action: tensor([[ 0.4089, -0.7634, -0.3241,  0.6535, -0.6424,  0.2758,  0.2444]],
       dtype=torch.float64)
	q_value: tensor([[-41.6018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3198926782111754, distance: 0.9437249096873508 entropy 1.1324905157089233
epoch: 17, step: 96
	action: tensor([[ 2.4179, -1.8325, -0.1273,  0.3846,  0.1544,  0.4471,  2.8956]],
       dtype=torch.float64)
	q_value: tensor([[-40.3727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 97
	action: tensor([[ 0.1415, -0.9806, -0.3794,  0.1724,  0.5601,  0.5392,  0.5993]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18222886924409853, distance: 1.2442494958090995 entropy 1.1324905157089233
epoch: 17, step: 98
	action: tensor([[ 1.1793, -1.6826, -0.0188, -0.8686,  0.4558,  0.3603,  1.2769]],
       dtype=torch.float64)
	q_value: tensor([[-43.6047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 99
	action: tensor([[ 0.9557, -0.5098,  0.1494,  0.7883, -0.1371, -0.0911,  0.3333]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7367853878587622, distance: 0.5870994733841609 entropy 1.1324905157089233
epoch: 17, step: 100
	action: tensor([[-0.2949, -0.9484,  1.0125,  1.4415, -0.8909,  0.6752,  0.5476]],
       dtype=torch.float64)
	q_value: tensor([[-41.7601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6741616717483687, distance: 0.6532174496784229 entropy 1.1324905157089233
epoch: 17, step: 101
	action: tensor([[ 2.3873, -0.7469,  1.2106,  0.1168,  0.9993,  1.2712,  1.4684]],
       dtype=torch.float64)
	q_value: tensor([[-52.6035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 102
	action: tensor([[ 1.0496,  0.7608, -0.3336,  0.3636,  0.7884,  0.7749, -0.4583]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 103
	action: tensor([[ 1.9130, -0.1269,  0.3794, -1.3861,  0.3840, -1.0051,  1.5382]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 104
	action: tensor([[ 0.3645,  0.0467,  0.3093, -1.0565,  0.5580,  0.0485, -0.9797]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056708962996875445, distance: 1.1114234281101245 entropy 1.1324905157089233
epoch: 17, step: 105
	action: tensor([[-1.7477,  0.1757, -0.3957, -0.5538, -0.6412, -1.0864, -1.8786]],
       dtype=torch.float64)
	q_value: tensor([[-35.7786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0197860167285016, distance: 1.6263326235976834 entropy 1.1324905157089233
epoch: 17, step: 106
	action: tensor([[ 0.0465, -1.0326,  0.4008, -0.0255, -0.3950, -0.5842,  0.8030]],
       dtype=torch.float64)
	q_value: tensor([[-43.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7338296320895925, distance: 1.5068148921598552 entropy 1.1324905157089233
epoch: 17, step: 107
	action: tensor([[ 1.3863,  0.0882,  1.5551, -0.1876,  0.7445, -0.8187,  1.1358]],
       dtype=torch.float64)
	q_value: tensor([[-40.5368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8866788397781105, distance: 0.3852229914516722 entropy 1.1324905157089233
epoch: 17, step: 108
	action: tensor([[ 1.0633, -0.2488,  0.7565, -0.5150,  0.0637,  0.9369,  1.9859]],
       dtype=torch.float64)
	q_value: tensor([[-51.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5014270346134213, distance: 0.8080180382158231 entropy 1.1324905157089233
epoch: 17, step: 109
	action: tensor([[ 3.6950, -3.9195,  2.3145, -2.3942, -0.7147,  1.0404,  5.1916]],
       dtype=torch.float64)
	q_value: tensor([[-59.2519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 110
	action: tensor([[-1.7524,  0.3786,  0.3542, -0.8624,  1.3758, -0.3468,  1.2239]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 111
	action: tensor([[ 0.7570,  0.2941,  0.4091,  0.8684, -0.2750,  0.4994,  0.3428]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 112
	action: tensor([[-0.2004,  0.1667,  0.8865, -0.3938,  0.2717,  0.1177, -0.9088]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05957314375432854, distance: 1.1779372731863933 entropy 1.1324905157089233
epoch: 17, step: 113
	action: tensor([[ 0.5021,  0.2734, -1.1335,  0.9903, -0.1823,  0.0930,  0.7219]],
       dtype=torch.float64)
	q_value: tensor([[-34.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 114
	action: tensor([[-0.1678, -1.3281,  0.8034, -0.7737,  0.0677,  0.3015,  2.1086]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.206373367822298, distance: 1.6997936269815364 entropy 1.1324905157089233
epoch: 17, step: 115
	action: tensor([[ 1.2350, -1.3171,  1.5007, -0.9981, -1.3967,  1.3031,  3.6115]],
       dtype=torch.float64)
	q_value: tensor([[-56.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 116
	action: tensor([[ 1.0125, -1.1979, -1.9002, -0.4016, -0.0319, -0.0449, -0.0251]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48354149324450724, distance: 1.3938195190381837 entropy 1.1324905157089233
epoch: 17, step: 117
	action: tensor([[ 1.9970, -2.1988,  0.6771,  0.5206,  1.2979,  0.6130,  1.5821]],
       dtype=torch.float64)
	q_value: tensor([[-52.7243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 118
	action: tensor([[ 1.0379,  0.9763, -0.4114, -1.2493,  0.2148,  1.8336,  0.5498]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 119
	action: tensor([[-0.3166, -1.1946, -0.1642, -0.4443,  1.2491,  0.0219, -0.3633]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9711709906645785, distance: 1.6066409898566043 entropy 1.1324905157089233
epoch: 17, step: 120
	action: tensor([[-0.7069, -0.0759,  0.4367,  0.1640, -0.7310, -0.3283, -0.4462]],
       dtype=torch.float64)
	q_value: tensor([[-45.3455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.543102787766214, distance: 1.421523753828623 entropy 1.1324905157089233
epoch: 17, step: 121
	action: tensor([[ 0.2715, -0.8486,  0.3846, -0.8016, -0.2401, -0.5498, -0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-32.7488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7976671615365847, distance: 1.5343037122974637 entropy 1.1324905157089233
epoch: 17, step: 122
	action: tensor([[ 1.1241, -0.3547, -0.1883,  0.8484,  0.4755,  0.5367,  0.9022]],
       dtype=torch.float64)
	q_value: tensor([[-36.8714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9149618722048273, distance: 0.3337056335237377 entropy 1.1324905157089233
epoch: 17, step: 123
	action: tensor([[ 0.5624, -0.5222,  1.8611, -1.9878,  0.7420,  1.5715,  1.0957]],
       dtype=torch.float64)
	q_value: tensor([[-49.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12648758824253858, distance: 1.0695257210740254 entropy 1.1324905157089233
epoch: 17, step: 124
	action: tensor([[ 2.7945, -4.2741,  1.6177, -1.1631, -0.0659,  0.2213,  1.9590]],
       dtype=torch.float64)
	q_value: tensor([[-64.7079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 125
	action: tensor([[ 1.0653, -0.3971,  0.4977,  0.5821, -0.1066,  1.2526,  0.9355]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 17, step: 126
	action: tensor([[-0.1455, -0.8150,  0.9951, -1.0367, -0.1628, -0.6727,  0.5355]],
       dtype=torch.float64)
	q_value: tensor([[-40.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.924014304760733, distance: 1.5873066714102497 entropy 1.1324905157089233
epoch: 17, step: 127
	action: tensor([[ 1.0362,  0.1472,  0.6483, -0.2690,  0.4321, -0.8592,  0.0971]],
       dtype=torch.float64)
	q_value: tensor([[-40.2004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6840524611182344, distance: 0.6432268750324679 entropy 1.1324905157089233
LOSS epoch 17 actor 418.1767366936884 critic 86.5357346665849 
epoch: 18, step: 0
	action: tensor([[ 1.3695, -0.5264,  0.9002, -0.8230,  0.6316,  0.6066,  0.7861]],
       dtype=torch.float64)
	q_value: tensor([[-36.0722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06526510441007782, distance: 1.1810969381671723 entropy 1.1324905157089233
epoch: 18, step: 1
	action: tensor([[ 2.8785, -2.7327,  2.9776, -1.7699,  0.4755,  1.5736,  2.3273]],
       dtype=torch.float64)
	q_value: tensor([[-49.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 2
	action: tensor([[ 0.4411, -0.7455,  0.0898,  0.5555, -0.2129,  0.5194,  1.6124]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5183549916921874, distance: 0.7941823391903243 entropy 1.1324905157089233
epoch: 18, step: 3
	action: tensor([[ 2.3001, -2.6400,  1.5394, -0.1970, -0.5007,  0.8703,  4.2550]],
       dtype=torch.float64)
	q_value: tensor([[-48.1583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 4
	action: tensor([[ 2.1553,  0.0369,  0.2081, -1.1219, -1.6372, -0.7843,  0.6881]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 5
	action: tensor([[ 1.4397,  0.8452, -0.3008,  0.1900, -0.0030,  1.1451,  0.5148]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 6
	action: tensor([[ 1.1751, -0.2326, -1.3250, -0.7348, -0.4273, -0.1353,  1.4413]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 7
	action: tensor([[ 0.7891,  0.9331,  0.5510,  0.0170,  2.1624, -1.1773,  0.6296]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 8
	action: tensor([[ 0.8353, -1.0015,  0.0225, -0.6243, -0.7314,  1.8849,  0.6845]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09826439883606986, distance: 1.0866665567131562 entropy 1.1324905157089233
epoch: 18, step: 9
	action: tensor([[ 2.5384, -4.2321,  1.0503, -3.1506, -0.2327,  0.4795,  3.1636]],
       dtype=torch.float64)
	q_value: tensor([[-56.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 10
	action: tensor([[ 0.3173,  0.3153,  1.2621,  0.7131, -0.2796,  0.2767,  1.3551]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 11
	action: tensor([[-0.1798, -0.9026, -0.7578, -1.1032, -0.9230, -1.6087,  0.4054]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4111285973245, distance: 0.8781464002408674 entropy 1.1324905157089233
epoch: 18, step: 12
	action: tensor([[ 0.1531,  0.6538,  0.2377, -0.2079,  1.2448,  0.7311,  1.0906]],
       dtype=torch.float64)
	q_value: tensor([[-43.5794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 13
	action: tensor([[ 0.0457,  0.4839,  0.4717,  0.1740, -0.0960, -0.7860,  1.1761]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 14
	action: tensor([[ 0.1416, -1.3108,  0.2426,  0.0739, -0.1472,  1.2087,  1.0262]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09016580193595636, distance: 1.1948213392340263 entropy 1.1324905157089233
epoch: 18, step: 15
	action: tensor([[ 2.2906, -2.2884,  1.6893, -0.7884,  0.5517, -0.5345,  2.8857]],
       dtype=torch.float64)
	q_value: tensor([[-48.3691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 16
	action: tensor([[ 0.0876, -1.0140,  0.0411, -1.4119,  0.7924,  0.5102, -1.7233]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7161635445887591, distance: 1.4991187270193445 entropy 1.1324905157089233
epoch: 18, step: 17
	action: tensor([[ 0.5209, -1.2083, -1.2185, -1.1487,  0.8944, -1.4337,  0.8159]],
       dtype=torch.float64)
	q_value: tensor([[-48.5352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2346190769344556, distance: 1.2715199684606109 entropy 1.1324905157089233
epoch: 18, step: 18
	action: tensor([[-0.0505,  0.4618,  1.8003,  0.1559, -0.0101,  0.2977,  0.7190]],
       dtype=torch.float64)
	q_value: tensor([[-52.6699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 19
	action: tensor([[ 1.3694,  0.2645, -0.8163, -0.8491,  0.7629, -0.2648, -0.2513]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 20
	action: tensor([[ 0.9147, -0.5420, -0.3240,  1.0123, -0.7616,  0.5740,  0.5984]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8438477842195978, distance: 0.45220021955131584 entropy 1.1324905157089233
epoch: 18, step: 21
	action: tensor([[ 0.5512, -1.8441,  0.4069, -0.0860,  0.8477,  2.3923,  3.1910]],
       dtype=torch.float64)
	q_value: tensor([[-44.6048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 22
	action: tensor([[ 1.9727, -1.2022, -1.1773, -0.1243,  1.1991, -0.2527,  0.5885]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 23
	action: tensor([[ 1.7066, -0.2069, -0.1827, -0.2415,  0.1296,  0.3892,  1.1326]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 24
	action: tensor([[ 0.8704, -0.0723,  0.6947, -1.0871,  0.7435, -1.5079,  0.7839]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3819535445039577, distance: 0.8996368853398644 entropy 1.1324905157089233
epoch: 18, step: 25
	action: tensor([[ 0.2372, -1.2836,  0.7332, -0.1509, -0.0860, -0.5808,  0.4080]],
       dtype=torch.float64)
	q_value: tensor([[-43.6209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9514925955518247, distance: 1.5986012461208343 entropy 1.1324905157089233
epoch: 18, step: 26
	action: tensor([[-0.1648, -1.9022,  1.0351,  0.6444,  1.0281, -0.9449,  1.0992]],
       dtype=torch.float64)
	q_value: tensor([[-39.7185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 27
	action: tensor([[ 1.6449, -0.9695,  2.3244,  1.1175, -0.0948, -0.5625,  0.3349]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 28
	action: tensor([[-0.8186,  0.1077,  2.1109, -0.3640,  0.4293, -0.0832,  0.1605]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7275605886112091, distance: 1.5040883144862824 entropy 1.1324905157089233
epoch: 18, step: 29
	action: tensor([[ 0.3080, -2.1747, -0.1528, -1.0152, -0.6519, -0.1495,  1.9513]],
       dtype=torch.float64)
	q_value: tensor([[-42.0168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 30
	action: tensor([[ 0.1940,  0.6398, -0.0376, -0.6229, -1.4266,  0.1077,  0.7577]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 31
	action: tensor([[ 0.7848, -1.4126,  0.5918, -0.0343, -1.4460, -0.1671,  0.3630]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.677286332505074, distance: 1.4820412694323115 entropy 1.1324905157089233
epoch: 18, step: 32
	action: tensor([[ 0.1536, -0.6350,  2.3241, -0.0754,  0.9608,  0.8591,  1.1491]],
       dtype=torch.float64)
	q_value: tensor([[-46.4955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33679279274121177, distance: 1.323088031796124 entropy 1.1324905157089233
epoch: 18, step: 33
	action: tensor([[ 1.3142, -3.0844,  1.8515,  0.0435,  0.2663,  0.3745,  2.8634]],
       dtype=torch.float64)
	q_value: tensor([[-55.6194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 34
	action: tensor([[ 0.8482, -0.5303,  0.2639, -0.2465,  0.8015, -1.1342, -0.1737]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009388420421472365, distance: 1.1389597938217721 entropy 1.1324905157089233
epoch: 18, step: 35
	action: tensor([[ 0.5476, -0.8477,  0.9406, -0.0458, -0.5394, -0.4681,  0.5365]],
       dtype=torch.float64)
	q_value: tensor([[-39.1247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3654537004162519, distance: 1.33719635050356 entropy 1.1324905157089233
epoch: 18, step: 36
	action: tensor([[ 1.4651, -0.0768,  1.8391,  1.0229,  0.4226,  1.0576,  0.2600]],
       dtype=torch.float64)
	q_value: tensor([[-39.4734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1790399407000176, distance: 1.2425702515699426 entropy 1.1324905157089233
epoch: 18, step: 37
	action: tensor([[ 2.0070, -1.4899,  0.1369, -1.8109, -0.5943, -0.5793,  2.2972]],
       dtype=torch.float64)
	q_value: tensor([[-57.0502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 38
	action: tensor([[ 0.7356, -0.4160,  0.6806, -0.0042,  0.9622,  0.1596,  0.7381]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37178152260258046, distance: 0.9070099392783635 entropy 1.1324905157089233
epoch: 18, step: 39
	action: tensor([[ 2.3928, -2.3357,  0.6841, -0.9266, -0.5953,  1.9200,  2.2965]],
       dtype=torch.float64)
	q_value: tensor([[-42.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 40
	action: tensor([[ 1.4003, -0.4012,  0.4952,  0.7354,  0.4874,  0.0115,  2.6288]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4208730164551141, distance: 0.8708504761228582 entropy 1.1324905157089233
epoch: 18, step: 41
	action: tensor([[ 2.8262, -3.1977,  1.7669, -2.1932,  1.3876, -0.7980,  2.9677]],
       dtype=torch.float64)
	q_value: tensor([[-65.1399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 42
	action: tensor([[ 1.7621, -1.6023, -1.0786, -0.9901, -0.5231, -0.0683, -0.0526]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0976587749428166, distance: 1.657387712825433 entropy 1.1324905157089233
epoch: 18, step: 43
	action: tensor([[ 0.7803, -0.8865, -0.0780, -1.2035,  1.0627,  0.4587,  1.6390]],
       dtype=torch.float64)
	q_value: tensor([[-51.9445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7177696256959492, distance: 1.4998200422347954 entropy 1.1324905157089233
epoch: 18, step: 44
	action: tensor([[ 2.2213, -3.6991,  1.5406, -1.7578, -0.3332, -1.6566,  3.4941]],
       dtype=torch.float64)
	q_value: tensor([[-55.9773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 45
	action: tensor([[ 1.6678, -1.5239, -0.2188, -0.2986,  0.7721,  0.1728,  0.3714]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9487394011658863, distance: 1.5974731830621414 entropy 1.1324905157089233
epoch: 18, step: 46
	action: tensor([[ 1.3815, -2.0807,  0.4828, -1.8166, -1.6500,  0.3081,  2.0813]],
       dtype=torch.float64)
	q_value: tensor([[-51.7414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 47
	action: tensor([[ 1.3541, -0.8593, -0.4183,  0.5821, -0.4087, -0.2224,  0.4846]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1332606817605676, distance: 1.0653711752574122 entropy 1.1324905157089233
epoch: 18, step: 48
	action: tensor([[ 0.3206, -1.0678,  1.0555, -1.2612,  0.8493, -0.2319,  1.1001]],
       dtype=torch.float64)
	q_value: tensor([[-46.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8294761697150206, distance: 1.547818637286829 entropy 1.1324905157089233
epoch: 18, step: 49
	action: tensor([[ 2.2576, -0.2171,  0.6966, -1.6794, -1.1227, -0.7602,  1.8875]],
       dtype=torch.float64)
	q_value: tensor([[-46.9646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 50
	action: tensor([[ 0.4962, -1.1122, -0.1468, -0.7169, -0.4248, -0.6775, -0.9674]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7987448212260564, distance: 1.534763533105639 entropy 1.1324905157089233
epoch: 18, step: 51
	action: tensor([[ 0.8234,  0.5556, -0.4361, -0.3448,  0.5649,  0.9852, -0.6222]],
       dtype=torch.float64)
	q_value: tensor([[-40.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0911575017731434 entropy 1.1324905157089233
epoch: 18, step: 52
	action: tensor([[-0.0725,  0.5681, -0.5415,  1.2256,  0.8189, -1.6402, -0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 53
	action: tensor([[ 1.3087, -0.5904, -0.6481, -1.3167, -0.2810, -1.1322,  0.1630]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3165211851417655, distance: 1.3130178170309974 entropy 1.1324905157089233
epoch: 18, step: 54
	action: tensor([[ 2.0503, -0.6787,  0.5480, -0.4660,  0.8849,  0.6823,  2.1924]],
       dtype=torch.float64)
	q_value: tensor([[-45.5811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 55
	action: tensor([[ 0.8257, -0.1356,  0.5034, -0.6364, -0.5689, -0.6272,  0.7486]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14614327936715465, distance: 1.0574240744671777 entropy 1.1324905157089233
epoch: 18, step: 56
	action: tensor([[ 0.3208, -1.8825,  0.0076, -1.1888,  1.2102,  0.2385,  1.1067]],
       dtype=torch.float64)
	q_value: tensor([[-37.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 57
	action: tensor([[-0.3959,  1.0278, -0.3178,  0.7757, -0.1080, -0.6992, -0.4546]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 58
	action: tensor([[ 1.2193, -0.1553,  0.4732, -0.4585, -1.0332,  1.1313,  0.6968]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7093605257423579, distance: 0.6169273057046232 entropy 1.1324905157089233
epoch: 18, step: 59
	action: tensor([[ 1.8717, -2.8580,  1.6380,  0.1613,  0.2616,  0.6832,  1.4051]],
       dtype=torch.float64)
	q_value: tensor([[-48.6046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 60
	action: tensor([[-0.1345, -0.5148,  0.3331,  0.4998, -0.1429, -0.0485,  0.5118]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09244556052333053, distance: 1.0901670113211193 entropy 1.1324905157089233
epoch: 18, step: 61
	action: tensor([[ 0.4596, -0.6695,  0.2113, -1.8973, -0.5385,  1.3480,  1.9706]],
       dtype=torch.float64)
	q_value: tensor([[-33.4557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.347520537911594, distance: 1.3283863057104652 entropy 1.1324905157089233
epoch: 18, step: 62
	action: tensor([[ 5.3339, -3.0079,  3.2645, -0.2209,  0.0387,  0.3273,  4.8865]],
       dtype=torch.float64)
	q_value: tensor([[-61.7979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 63
	action: tensor([[ 1.1544, -0.1957,  0.1711, -0.7593,  1.2339,  0.4253,  0.4995]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10774002034945518, distance: 1.0809420215052097 entropy 1.1324905157089233
epoch: 18, step: 64
	action: tensor([[ 2.3813, -0.6528,  1.9713, -0.7454,  1.1215,  0.7081,  1.4672]],
       dtype=torch.float64)
	q_value: tensor([[-45.5181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 65
	action: tensor([[ 0.9334, -1.2477,  1.2766, -0.6123,  0.0574, -0.7233,  0.3230]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4648609866763105, distance: 1.3850163480024618 entropy 1.1324905157089233
epoch: 18, step: 66
	action: tensor([[ 1.2555, -1.0936,  1.6982, -1.3222, -1.1557,  0.8000,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[-45.0625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34369270738883406, distance: 0.9270652626379577 entropy 1.1324905157089233
epoch: 18, step: 67
	action: tensor([[ 2.0612, -1.4395,  1.5803, -0.0795, -0.4120,  0.2679,  2.0770]],
       dtype=torch.float64)
	q_value: tensor([[-54.2364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 68
	action: tensor([[-0.5102, -0.4827, -0.3633,  0.6775, -0.2894, -0.3389, -0.6153]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43978339657800625, distance: 1.3731098222153721 entropy 1.1324905157089233
epoch: 18, step: 69
	action: tensor([[ 0.2066,  0.6794, -0.7993, -1.1212, -0.4947,  0.0233,  0.4099]],
       dtype=torch.float64)
	q_value: tensor([[-32.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8300638582958553, distance: 0.4717365972936999 entropy 1.1324905157089233
epoch: 18, step: 70
	action: tensor([[ 1.2192, -1.2531,  0.7925, -0.3412, -0.6515, -0.5249,  0.2718]],
       dtype=torch.float64)
	q_value: tensor([[-33.7426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8316994876707926, distance: 1.5487588649132908 entropy 1.1324905157089233
epoch: 18, step: 71
	action: tensor([[ 0.1755, -3.0050,  1.7387, -0.6679,  1.2567, -0.4808,  0.5637]],
       dtype=torch.float64)
	q_value: tensor([[-46.3475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 72
	action: tensor([[-0.2426,  0.0452,  1.4766,  0.0100, -1.0070, -0.4549,  0.9025]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06962252332362251, distance: 1.1835100848739097 entropy 1.1324905157089233
epoch: 18, step: 73
	action: tensor([[ 1.0304e+00, -8.6595e-01,  5.3250e-01, -1.5563e-01,  3.0928e-04,
          9.6300e-01,  1.0065e+00]], dtype=torch.float64)
	q_value: tensor([[-39.4964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1119614301295585, distance: 1.078381944117572 entropy 1.1324905157089233
epoch: 18, step: 74
	action: tensor([[ 2.7881, -2.9825,  0.3431, -0.7526,  0.7544, -1.4152,  1.7332]],
       dtype=torch.float64)
	q_value: tensor([[-49.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 75
	action: tensor([[ 0.1646, -0.0889, -0.4745,  0.9718,  1.7440, -0.5353,  0.3636]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 76
	action: tensor([[ 0.3435, -0.1870, -0.5415,  0.0525,  1.2699, -0.2560,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35605536643579716, distance: 0.9182923310618603 entropy 1.1324905157089233
epoch: 18, step: 77
	action: tensor([[ 0.0588, -1.1999,  0.6382, -0.3122, -0.9744,  1.3305,  0.3964]],
       dtype=torch.float64)
	q_value: tensor([[-37.2150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46429521597619394, distance: 1.3847488559493248 entropy 1.1324905157089233
epoch: 18, step: 78
	action: tensor([[ 1.1903, -2.1397,  0.3357, -0.7064, -0.7886, -0.0434,  1.1923]],
       dtype=torch.float64)
	q_value: tensor([[-48.7814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 79
	action: tensor([[ 0.0136,  0.1032, -0.6516, -0.0438,  0.6713, -0.5416,  1.1606]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28020933544327686, distance: 0.9708671308979439 entropy 1.1324905157089233
epoch: 18, step: 80
	action: tensor([[-0.0221, -0.8089,  0.4015, -0.3677,  0.1797,  0.4038,  2.1898]],
       dtype=torch.float64)
	q_value: tensor([[-36.4629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.522416629723663, distance: 1.411963443931154 entropy 1.1324905157089233
epoch: 18, step: 81
	action: tensor([[ 2.7223, -3.2911,  2.1284, -0.2578, -0.1276,  1.6005,  2.9053]],
       dtype=torch.float64)
	q_value: tensor([[-52.7036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 82
	action: tensor([[ 0.9810, -0.5953,  0.3099, -0.3820, -0.7059, -0.1430,  0.2950]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16483584621405178, distance: 1.23506284483919 entropy 1.1324905157089233
epoch: 18, step: 83
	action: tensor([[ 0.5420, -2.0292,  0.3243,  0.1713,  0.8766,  1.1258,  3.2907]],
       dtype=torch.float64)
	q_value: tensor([[-38.7734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 84
	action: tensor([[ 0.5451, -1.2934, -0.0015, -0.3276, -0.7386, -0.0286,  0.1027]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.859682554164982, distance: 1.560544300052282 entropy 1.1324905157089233
epoch: 18, step: 85
	action: tensor([[ 0.1595, -1.2481,  0.6423, -0.9029,  0.6210,  0.6297,  1.3306]],
       dtype=torch.float64)
	q_value: tensor([[-41.3549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0427903712106792, distance: 1.6355679599528397 entropy 1.1324905157089233
epoch: 18, step: 86
	action: tensor([[ 3.0501, -1.4828,  2.5485, -0.5408, -0.3284,  0.7248,  3.6003]],
       dtype=torch.float64)
	q_value: tensor([[-50.0878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 87
	action: tensor([[ 1.0049, -0.7775, -0.2106, -2.0378,  0.4304,  0.8145,  0.1069]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5315423732384326, distance: 1.4161889510459447 entropy 1.1324905157089233
epoch: 18, step: 88
	action: tensor([[ 1.9755, -2.7184,  0.9502, -1.6087,  0.3196, -0.8554,  1.6380]],
       dtype=torch.float64)
	q_value: tensor([[-52.4275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 89
	action: tensor([[-0.1956, -1.0813, -0.0988,  1.2074, -0.5112,  1.0664, -0.4245]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09362796784191751, distance: 1.0894566178060252 entropy 1.1324905157089233
epoch: 18, step: 90
	action: tensor([[ 0.0956, -0.5933,  0.4010, -0.8146,  0.6667,  0.8258,  0.1877]],
       dtype=torch.float64)
	q_value: tensor([[-46.1016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33521173497618717, distance: 1.3223053759077248 entropy 1.1324905157089233
epoch: 18, step: 91
	action: tensor([[ 0.2344, -1.4475,  1.0677, -0.7231,  0.7758, -0.7241,  1.7119]],
       dtype=torch.float64)
	q_value: tensor([[-39.6451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 92
	action: tensor([[-0.7997, -1.6054,  0.4521,  0.3990, -0.2538,  0.8471,  1.1631]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7014733586695281, distance: 1.4926887853898816 entropy 1.1324905157089233
epoch: 18, step: 93
	action: tensor([[ 1.1018, -0.6038,  1.5698, -0.8190,  0.3867,  1.0675,  2.6794]],
       dtype=torch.float64)
	q_value: tensor([[-45.2193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1424892108986604, distance: 1.059684275048726 entropy 1.1324905157089233
epoch: 18, step: 94
	action: tensor([[ 4.4391, -4.8552,  3.0588, -1.6012, -0.0883,  0.2999,  4.1303]],
       dtype=torch.float64)
	q_value: tensor([[-68.4093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 95
	action: tensor([[-0.5531, -0.2718, -0.1626,  1.1809, -0.2021, -0.1021, -0.4001]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021344620649980994, distance: 1.1564925679835092 entropy 1.1324905157089233
epoch: 18, step: 96
	action: tensor([[ 8.7286e-01, -8.8582e-01,  5.1904e-01, -1.4590e-05, -2.4519e-01,
          4.0579e-02, -1.6755e-01]], dtype=torch.float64)
	q_value: tensor([[-34.7831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19519657530521983, distance: 1.2510548857552863 entropy 1.1324905157089233
epoch: 18, step: 97
	action: tensor([[ 0.5397, -1.5636,  0.7202, -0.4985, -1.8062,  0.6818,  2.2113]],
       dtype=torch.float64)
	q_value: tensor([[-38.7791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 98
	action: tensor([[ 0.9376,  0.4225, -1.2189, -1.7936, -0.6010, -0.4460, -1.0280]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 99
	action: tensor([[ 0.5520,  0.1349,  1.2034,  0.8000,  0.1965, -1.1749,  0.1921]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 100
	action: tensor([[-0.2259, -0.9949, -0.1815, -0.5699, -0.0748, -0.6493,  1.0192]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7069148601963797, distance: 1.495073772638552 entropy 1.1324905157089233
epoch: 18, step: 101
	action: tensor([[-0.3216, -1.5290,  1.6399, -0.3255,  0.0416, -0.5385,  0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-39.2686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 102
	action: tensor([[ 1.6007,  0.5192,  0.1915,  0.3095,  1.1528, -0.8535, -0.0630]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 103
	action: tensor([[ 0.0439, -0.7004,  0.2457, -1.0938, -1.1006, -0.4044, -0.5151]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7089314050323032, distance: 1.4959566501175554 entropy 1.1324905157089233
epoch: 18, step: 104
	action: tensor([[-0.1490, -0.9764,  1.0087,  0.6740, -0.1278,  2.5362, -0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-37.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5157028515770058, distance: 0.7963658885263897 entropy 1.1324905157089233
epoch: 18, step: 105
	action: tensor([[ 2.1696e+00, -2.9778e+00,  1.6562e+00, -9.6103e-01, -2.2587e-03,
          5.1210e-01,  1.5787e+00]], dtype=torch.float64)
	q_value: tensor([[-54.8146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 106
	action: tensor([[ 0.2832,  1.4433,  1.2482, -0.0029,  1.0501, -0.1153, -0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 107
	action: tensor([[ 0.9060, -1.0184,  0.6021,  0.5102, -0.7802,  0.2298,  1.0429]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1143862291934381, distance: 1.0769086708644973 entropy 1.1324905157089233
epoch: 18, step: 108
	action: tensor([[ 0.9254, -1.7789,  1.2010, -0.0377, -0.9329, -0.8838,  1.3882]],
       dtype=torch.float64)
	q_value: tensor([[-48.1576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 109
	action: tensor([[-0.0357, -0.5940, -0.2391, -0.2872, -0.2738, -0.6006,  1.0315]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34717944106272713, distance: 1.3282181683620606 entropy 1.1324905157089233
epoch: 18, step: 110
	action: tensor([[-0.3482, -0.5996,  0.9064, -1.1896, -0.0423, -0.2035,  1.5988]],
       dtype=torch.float64)
	q_value: tensor([[-36.8393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0657935261809808, distance: 1.6447509628758838 entropy 1.1324905157089233
epoch: 18, step: 111
	action: tensor([[ 0.7732, -1.5560,  0.9193, -0.7768,  0.3007, -1.0151,  3.1736]],
       dtype=torch.float64)
	q_value: tensor([[-45.0134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 112
	action: tensor([[ 0.6829, -0.4097,  0.5316, -1.2033,  0.9389,  1.1231, -1.0435]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09032864448128541, distance: 1.1949105735883079 entropy 1.1324905157089233
epoch: 18, step: 113
	action: tensor([[ 2.0317, -2.1019,  0.7123,  0.5912,  0.9677,  0.9504,  1.7630]],
       dtype=torch.float64)
	q_value: tensor([[-45.1376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 114
	action: tensor([[ 0.4836,  0.6755,  0.4984, -0.9075, -1.3615, -0.9892,  0.0966]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 115
	action: tensor([[0.8345, 0.0576, 0.2834, 1.7469, 0.0650, 0.0514, 0.7772]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 116
	action: tensor([[-9.3730e-04, -9.5352e-01,  3.7171e-01, -8.6367e-01, -7.1340e-01,
         -5.8370e-02,  4.1677e-01]], dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9818775810696743, distance: 1.610998387626859 entropy 1.1324905157089233
epoch: 18, step: 117
	action: tensor([[1.1741, 0.4055, 0.7864, 0.4809, 1.3219, 0.8977, 0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-38.6458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6787869426475233, distance: 0.6485646724664857 entropy 1.1324905157089233
epoch: 18, step: 118
	action: tensor([[ 1.5247, -1.5505,  0.4385, -1.1961,  0.5415,  0.2486,  1.9815]],
       dtype=torch.float64)
	q_value: tensor([[-49.1498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7993640706837932, distance: 1.5350276950063146 entropy 1.1324905157089233
epoch: 18, step: 119
	action: tensor([[ 4.7904, -3.2497,  3.0361, -1.4881,  0.2064,  0.1694,  2.6905]],
       dtype=torch.float64)
	q_value: tensor([[-63.0473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 120
	action: tensor([[ 1.3686,  1.0451,  1.1213, -0.5098,  1.6237, -1.6122,  0.3935]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 121
	action: tensor([[ 0.4652, -0.7468, -0.3545, -0.0217, -0.3364,  0.0688, -0.2420]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07794405967109497, distance: 1.188104949346656 entropy 1.1324905157089233
epoch: 18, step: 122
	action: tensor([[-0.4365, -1.2452, -0.9641, -0.9240,  1.9083,  0.1599,  0.7513]],
       dtype=torch.float64)
	q_value: tensor([[-34.3426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30052318938609757, distance: 1.3050157210163553 entropy 1.1324905157089233
epoch: 18, step: 123
	action: tensor([[ 1.7694, -1.5302,  0.3635, -1.7223, -0.8020, -0.2225,  0.9445]],
       dtype=torch.float64)
	q_value: tensor([[-55.1163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 124
	action: tensor([[ 1.2512, -0.7645,  1.0993,  0.2824, -0.3745, -0.0029,  0.0212]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09214424521726161, distance: 1.195905034355466 entropy 1.1324905157089233
epoch: 18, step: 125
	action: tensor([[ 0.9595, -0.5595,  0.6318, -0.8349, -0.2853,  0.7875,  0.8088]],
       dtype=torch.float64)
	q_value: tensor([[-44.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056782276908636575, distance: 1.1113802365715417 entropy 1.1324905157089233
epoch: 18, step: 126
	action: tensor([[ 3.7088, -3.6436,  1.1016,  0.1354,  0.3579,  0.9033,  1.1992]],
       dtype=torch.float64)
	q_value: tensor([[-45.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 18, step: 127
	action: tensor([[ 0.3357, -0.8178,  0.3745, -0.5927, -0.4525, -1.3357, -0.2795]],
       dtype=torch.float64)
	q_value: tensor([[-35.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48424055207327354, distance: 1.394147870853696 entropy 1.1324905157089233
LOSS epoch 18 actor 409.1218347880632 critic 409.8818670569182 
epoch: 19, step: 0
	action: tensor([[-0.0182,  0.3348, -0.1261,  0.4250, -0.1871,  1.0148, -0.2664]],
       dtype=torch.float64)
	q_value: tensor([[-33.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 1
	action: tensor([[ 0.1068,  0.1956,  1.2963,  0.6217, -0.4631,  0.0724,  0.7866]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 2
	action: tensor([[-0.3320, -0.8469, -0.8604,  0.2673,  0.3933,  0.8441,  1.3743]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5631814308927365, distance: 1.4307422001101158 entropy 1.1324905157089233
epoch: 19, step: 3
	action: tensor([[ 2.5660, -1.4366,  2.5607, -0.6196, -0.6294, -0.3409,  2.3948]],
       dtype=torch.float64)
	q_value: tensor([[-42.6954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 4
	action: tensor([[-1.0877,  1.0292, -0.1481, -0.3488,  1.5062,  0.1152,  0.6902]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2824139745445431, distance: 1.2958979839395819 entropy 1.1324905157089233
epoch: 19, step: 5
	action: tensor([[-0.2791, -0.5567,  0.6091, -0.8085,  0.2046, -0.1326,  0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-37.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9657105902850307, distance: 1.6044141442244875 entropy 1.1324905157089233
epoch: 19, step: 6
	action: tensor([[ 2.0437,  0.1877,  0.7182,  0.8073, -1.6152,  0.4231, -0.8690]],
       dtype=torch.float64)
	q_value: tensor([[-30.1037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48435575659960073, distance: 0.821734990403786 entropy 1.1324905157089233
epoch: 19, step: 7
	action: tensor([[ 0.7411, -2.0113,  0.8852, -0.1697,  0.9775, -0.6541,  1.0203]],
       dtype=torch.float64)
	q_value: tensor([[-44.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 8
	action: tensor([[ 0.4901, -0.5844, -0.2558, -0.4061, -1.0264,  0.4412,  0.6676]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12483879663188269, distance: 1.213673408902925 entropy 1.1324905157089233
epoch: 19, step: 9
	action: tensor([[ 2.0294, -2.0780,  1.0437, -1.5175, -0.1721,  0.7584,  2.8753]],
       dtype=torch.float64)
	q_value: tensor([[-36.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 10
	action: tensor([[ 0.6669,  0.3269,  1.4798,  0.4467,  0.1303,  0.8075, -0.1983]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 11
	action: tensor([[ 0.4009, -0.6724, -0.0721, -0.9353, -0.2932,  0.7724,  0.1857]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35419188476974806, distance: 1.3316705540406226 entropy 1.1324905157089233
epoch: 19, step: 12
	action: tensor([[ 0.4934, -1.6342,  0.4033, -0.5110,  0.4617, -0.2834,  1.9289]],
       dtype=torch.float64)
	q_value: tensor([[-35.5603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 13
	action: tensor([[-0.9112, -0.6615,  0.7825, -0.2259, -0.4428,  0.1186,  1.1457]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3467210430906547, distance: 1.7530222449782469 entropy 1.1324905157089233
epoch: 19, step: 14
	action: tensor([[ 0.5594, -0.4930,  1.4759, -1.2367,  0.5904,  0.9382,  1.6974]],
       dtype=torch.float64)
	q_value: tensor([[-34.3129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058825057839255956, distance: 1.177521372745634 entropy 1.1324905157089233
epoch: 19, step: 15
	action: tensor([[ 4.0371, -3.7448,  2.8948, -0.2388,  0.5392, -0.0191,  3.8649]],
       dtype=torch.float64)
	q_value: tensor([[-50.2975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 16
	action: tensor([[ 0.5137, -1.3388,  1.0333, -0.3704, -2.0550,  0.5202, -0.1129]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7166226195064831, distance: 1.4993192212527102 entropy 1.1324905157089233
epoch: 19, step: 17
	action: tensor([[-0.0281, -2.1967,  0.4498, -2.7163, -0.7149,  0.6953,  0.2488]],
       dtype=torch.float64)
	q_value: tensor([[-47.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 18
	action: tensor([[ 0.4461, -0.5137,  0.1662, -1.3934,  0.4611, -0.3934,  0.6026]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5334680840194084, distance: 1.4170790059519083 entropy 1.1324905157089233
epoch: 19, step: 19
	action: tensor([[ 1.8244, -2.0215,  1.1368,  0.4763,  0.9751,  0.7355,  1.7545]],
       dtype=torch.float64)
	q_value: tensor([[-35.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 20
	action: tensor([[-0.2147,  1.1305,  0.5331,  0.7751,  0.1588, -0.8121, -0.5234]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 21
	action: tensor([[ 0.3290,  0.7861,  0.4785,  0.0212, -0.5063,  0.5351, -0.9955]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 22
	action: tensor([[-0.4021,  0.1925, -0.5035, -0.1833, -0.8375,  0.6209, -0.0781]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15297125338688855, distance: 1.2287567962595378 entropy 1.1324905157089233
epoch: 19, step: 23
	action: tensor([[ 0.4079, -1.2725, -0.5703,  0.5959, -0.0409, -0.5861,  0.3081]],
       dtype=torch.float64)
	q_value: tensor([[-27.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45659428164550175, distance: 1.3811027613769034 entropy 1.1324905157089233
epoch: 19, step: 24
	action: tensor([[ 1.0607, -0.3840, -0.4580,  0.3915,  0.5919,  0.3351,  0.9096]],
       dtype=torch.float64)
	q_value: tensor([[-38.0018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7111939870709898, distance: 0.6149783242763094 entropy 1.1324905157089233
epoch: 19, step: 25
	action: tensor([[ 2.1646, -1.9852,  1.0486, -2.3796, -0.7575,  0.8903,  1.9674]],
       dtype=torch.float64)
	q_value: tensor([[-40.9513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 26
	action: tensor([[ 1.2455, -0.5162, -0.0025, -1.0997,  0.2371, -1.0545,  0.1232]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3215454593788394, distance: 1.3155208832966685 entropy 1.1324905157089233
epoch: 19, step: 27
	action: tensor([[ 1.1322, -0.2728, -0.0123, -0.6992, -0.1121,  0.2964,  0.5651]],
       dtype=torch.float64)
	q_value: tensor([[-37.6029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0848631166549102, distance: 1.0947116072581933 entropy 1.1324905157089233
epoch: 19, step: 28
	action: tensor([[ 2.1385, -1.3173,  1.4145, -1.5475,  0.5206,  0.2223,  2.7075]],
       dtype=torch.float64)
	q_value: tensor([[-36.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 29
	action: tensor([[-0.4668, -0.8491,  0.8322, -0.7734, -1.2743, -0.1580, -0.4383]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.263949753352164, distance: 1.7218292655965715 entropy 1.1324905157089233
epoch: 19, step: 30
	action: tensor([[-0.1517, -0.8561,  0.4857, -0.6911, -0.4140,  0.0847,  1.0232]],
       dtype=torch.float64)
	q_value: tensor([[-36.1047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0104166833222914, distance: 1.6225561431759634 entropy 1.1324905157089233
epoch: 19, step: 31
	action: tensor([[ 2.6528, -1.6790,  0.9807, -0.1854,  0.1094, -1.2178,  1.9772]],
       dtype=torch.float64)
	q_value: tensor([[-36.3735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 32
	action: tensor([[-0.1278, -1.1859,  0.0939, -0.0107,  0.7873, -0.2650,  0.7160]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9469079570949384, distance: 1.5967223462629039 entropy 1.1324905157089233
epoch: 19, step: 33
	action: tensor([[ 0.6206, -2.3134,  0.1101, -0.0574, -0.5842, -0.1389,  2.5460]],
       dtype=torch.float64)
	q_value: tensor([[-36.8658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 34
	action: tensor([[-0.8013, -0.3092,  0.2855,  0.6659, -0.1138,  0.1196, -0.5051]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3816371230093736, distance: 1.345097266517406 entropy 1.1324905157089233
epoch: 19, step: 35
	action: tensor([[ 1.4081, -0.8069,  0.2963, -0.0822,  0.1953, -1.5957, -0.2477]],
       dtype=torch.float64)
	q_value: tensor([[-29.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2187269661292197, distance: 1.2633099118239821 entropy 1.1324905157089233
epoch: 19, step: 36
	action: tensor([[ 0.1966, -0.9066,  0.7731,  0.2680, -0.4652, -0.8991,  1.1205]],
       dtype=torch.float64)
	q_value: tensor([[-40.4269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4298246787650848, distance: 1.3683528076315072 entropy 1.1324905157089233
epoch: 19, step: 37
	action: tensor([[ 1.0432, -0.1741,  0.8657, -0.5617, -0.9203,  0.3833,  1.7956]],
       dtype=torch.float64)
	q_value: tensor([[-36.7533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5022067676767842, distance: 0.8073859492833165 entropy 1.1324905157089233
epoch: 19, step: 38
	action: tensor([[ 2.8282, -3.6146,  1.9694, -1.1907, -0.7790,  0.4704,  3.2441]],
       dtype=torch.float64)
	q_value: tensor([[-46.8351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 39
	action: tensor([[ 0.5225,  0.6664,  1.0258, -0.7252,  1.2072,  0.6247, -0.0697]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 40
	action: tensor([[ 0.5106, -0.4415,  1.3504, -0.7846, -0.2441,  1.4795, -1.0531]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4251774185847452, distance: 0.8676081113336025 entropy 1.1324905157089233
epoch: 19, step: 41
	action: tensor([[ 0.5233, -0.4132, -0.3915,  0.7556, -1.2158, -0.1013,  1.1397]],
       dtype=torch.float64)
	q_value: tensor([[-41.9803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6830656094071654, distance: 0.6442306407947272 entropy 1.1324905157089233
epoch: 19, step: 42
	action: tensor([[ 2.4192, -1.5213,  1.8075, -0.4380,  0.6670,  0.0663,  1.9825]],
       dtype=torch.float64)
	q_value: tensor([[-40.4740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 43
	action: tensor([[-0.1411, -0.5832,  0.9178, -0.1703, -1.1885,  0.0499,  0.5127]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5011671727987497, distance: 1.4020749259650234 entropy 1.1324905157089233
epoch: 19, step: 44
	action: tensor([[ 1.0135, -0.7403,  0.7307, -0.9340, -0.3312, -0.4231,  1.5210]],
       dtype=torch.float64)
	q_value: tensor([[-34.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4213018260232899, distance: 1.3642684955261801 entropy 1.1324905157089233
epoch: 19, step: 45
	action: tensor([[ 2.7985, -1.9719,  2.1881, -0.6243, -0.7710,  0.3936,  0.8617]],
       dtype=torch.float64)
	q_value: tensor([[-43.4511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 46
	action: tensor([[ 0.7063, -0.6302, -0.1280,  0.0221,  0.0060, -0.3070, -0.1241]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018866225265418146, distance: 1.1334981258018055 entropy 1.1324905157089233
epoch: 19, step: 47
	action: tensor([[ 0.6883, -1.6967,  0.4734,  0.4742,  0.4230,  1.6593,  1.1236]],
       dtype=torch.float64)
	q_value: tensor([[-30.3594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 48
	action: tensor([[ 0.8016, -1.2471, -0.3662,  0.6223, -0.2631,  0.4492,  0.4494]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07012783296841807, distance: 1.103489785786653 entropy 1.1324905157089233
epoch: 19, step: 49
	action: tensor([[ 1.2306, -2.0803,  0.0320, -0.2661, -0.5508, -0.4830,  2.6033]],
       dtype=torch.float64)
	q_value: tensor([[-41.5627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 50
	action: tensor([[-0.2688,  0.0840,  0.4445, -0.3598, -0.2010,  1.1360,  1.7546]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19046725079924964, distance: 1.029612732544589 entropy 1.1324905157089233
epoch: 19, step: 51
	action: tensor([[ 2.2912, -2.6333,  1.4985, -0.9744, -0.3315,  0.6619,  2.2316]],
       dtype=torch.float64)
	q_value: tensor([[-43.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 52
	action: tensor([[ 0.2768,  0.1927,  0.7863, -0.3562, -0.3423, -0.3396,  0.6581]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3669063525996359, distance: 0.9105224769106439 entropy 1.1324905157089233
epoch: 19, step: 53
	action: tensor([[ 0.7931, -0.9047,  1.3837,  0.0426,  1.0295, -0.4317, -0.0938]],
       dtype=torch.float64)
	q_value: tensor([[-29.6503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08833935707834817, distance: 1.1938200281995286 entropy 1.1324905157089233
epoch: 19, step: 54
	action: tensor([[ 0.3792,  0.2106,  1.0565, -0.3158,  0.1742,  0.4438,  0.9402]],
       dtype=torch.float64)
	q_value: tensor([[-40.6416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6670848682771873, distance: 0.6602728821762113 entropy 1.1324905157089233
epoch: 19, step: 55
	action: tensor([[ 2.6184, -2.4699,  1.6723, -0.9203, -0.7553,  0.0514,  2.7496]],
       dtype=torch.float64)
	q_value: tensor([[-35.7758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 56
	action: tensor([[-0.6040, -0.0586,  0.0911, -1.0267, -0.0134,  0.8527,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6630033097773071, distance: 1.4757175768104567 entropy 1.1324905157089233
epoch: 19, step: 57
	action: tensor([[ 0.8973, -1.0692,  1.0867, -0.3842, -0.4661, -1.4495,  0.6963]],
       dtype=torch.float64)
	q_value: tensor([[-32.4946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43193868575320393, distance: 1.3693639941803026 entropy 1.1324905157089233
epoch: 19, step: 58
	action: tensor([[ 0.0819, -0.2991, -0.0162,  0.8943, -0.8166,  1.4952,  0.5852]],
       dtype=torch.float64)
	q_value: tensor([[-41.2648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 59
	action: tensor([[-0.3805, -1.3636,  0.3042, -1.0266,  0.7185,  0.9039,  0.6334]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8324803770937679, distance: 1.5490889628778364 entropy 1.1324905157089233
epoch: 19, step: 60
	action: tensor([[ 2.0570, -3.1229,  0.9249,  0.0370, -0.0721,  0.6541,  1.7421]],
       dtype=torch.float64)
	q_value: tensor([[-41.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 61
	action: tensor([[-0.7956, -1.6497,  0.4192, -1.7330, -0.9072, -0.2098, -0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4706322657686788, distance: 1.3877420190361152 entropy 1.1324905157089233
epoch: 19, step: 62
	action: tensor([[ 0.4883, -1.3156,  0.7018, -0.6205, -0.0425,  1.3581,  2.7052]],
       dtype=torch.float64)
	q_value: tensor([[-40.2941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 63
	action: tensor([[ 0.3974, -0.2774, -0.8948,  1.6438, -0.5523,  2.1978,  0.3531]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 64
	action: tensor([[-0.3461, -1.6923, -0.4268,  0.8968,  0.4167, -0.9319,  0.2147]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8786721026938287, distance: 1.5684915612260515 entropy 1.1324905157089233
epoch: 19, step: 65
	action: tensor([[ 1.4204, -1.2362, -0.0187,  0.3655, -0.3953,  0.7186,  0.5169]],
       dtype=torch.float64)
	q_value: tensor([[-38.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04207195811953568, distance: 1.1681686526100377 entropy 1.1324905157089233
epoch: 19, step: 66
	action: tensor([[ 1.4391, -4.1018,  1.9603, -0.4642, -0.4636, -0.8995,  2.6226]],
       dtype=torch.float64)
	q_value: tensor([[-46.5552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 67
	action: tensor([[ 0.0650,  0.0855,  0.4834, -0.1509, -0.7374,  0.9221,  0.5057]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4802074804803804, distance: 0.8250337330336562 entropy 1.1324905157089233
epoch: 19, step: 68
	action: tensor([[ 0.9460, -1.9017,  1.0624, -1.4974, -1.5706,  1.3880,  2.6405]],
       dtype=torch.float64)
	q_value: tensor([[-33.7048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 69
	action: tensor([[ 0.4070, -0.0477, -0.3463,  0.7056,  0.1577,  0.2634,  0.1600]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 70
	action: tensor([[-0.3508, -0.3761,  0.0838,  0.3647, -0.1563, -0.2044,  0.6462]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22593258440441555, distance: 1.2670390135010106 entropy 1.1324905157089233
epoch: 19, step: 71
	action: tensor([[ 0.4917, -0.1057,  1.2253, -0.3752,  0.4345,  1.1308,  0.5790]],
       dtype=torch.float64)
	q_value: tensor([[-28.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6204158184947574, distance: 0.705035111911661 entropy 1.1324905157089233
epoch: 19, step: 72
	action: tensor([[ 2.6579, -0.1476,  1.4050, -0.7545,  0.4262,  1.4270,  3.4182]],
       dtype=torch.float64)
	q_value: tensor([[-40.2153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 73
	action: tensor([[ 8.1352e-01,  5.8135e-01,  1.2804e-01, -1.0307e+00,  4.7513e-01,
          9.9132e-01,  1.0289e-04]], dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 74
	action: tensor([[ 0.0079, -0.5589, -0.0880,  0.1972, -0.1107,  0.0437, -0.2076]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.050716457131095405, distance: 1.173003912137766 entropy 1.1324905157089233
epoch: 19, step: 75
	action: tensor([[ 0.6642, -0.0568, -0.0149,  0.1277,  0.1586, -0.8470,  0.8380]],
       dtype=torch.float64)
	q_value: tensor([[-27.4048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5673656930405662, distance: 0.7526917758287561 entropy 1.1324905157089233
epoch: 19, step: 76
	action: tensor([[ 0.2240, -0.6871,  0.5196, -0.7536,  1.2451, -0.2244,  1.3247]],
       dtype=torch.float64)
	q_value: tensor([[-32.3113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6719684648430857, distance: 1.4796899844841171 entropy 1.1324905157089233
epoch: 19, step: 77
	action: tensor([[ 0.9249, -1.1243,  1.9643, -0.8622,  0.4559,  0.6446,  1.0152]],
       dtype=torch.float64)
	q_value: tensor([[-40.5803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2816856038662394, distance: 1.295529917084454 entropy 1.1324905157089233
epoch: 19, step: 78
	action: tensor([[ 3.8843, -3.8199,  2.2280, -1.5232, -0.4091, -0.9457,  3.8084]],
       dtype=torch.float64)
	q_value: tensor([[-48.6452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 79
	action: tensor([[ 0.7098,  1.5190,  0.5051, -0.7332,  0.2370, -0.2978,  0.3749]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 80
	action: tensor([[-0.0251, -1.0137, -0.1984, -0.9285,  0.3499,  1.7590, -1.5243]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1486375685399588, distance: 1.2264453518310767 entropy 1.1324905157089233
epoch: 19, step: 81
	action: tensor([[ 0.4115, -1.9257,  0.2686, -0.9907,  0.5877,  0.6745,  1.0165]],
       dtype=torch.float64)
	q_value: tensor([[-46.2386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 82
	action: tensor([[-0.0117, -0.3106, -0.1808, -0.2812, -1.0063, -1.1641,  1.0083]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17274236529293008, distance: 1.040823490289522 entropy 1.1324905157089233
epoch: 19, step: 83
	action: tensor([[ 0.8331, -1.8303,  0.3047,  1.3506, -0.1098, -1.1614,  0.3716]],
       dtype=torch.float64)
	q_value: tensor([[-35.1810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 84
	action: tensor([[-0.6170, -0.9152, -0.3202,  0.6306,  0.6161, -0.1160,  1.1182]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.984913046843555, distance: 1.6122316271321888 entropy 1.1324905157089233
epoch: 19, step: 85
	action: tensor([[ 1.1079, -0.7017, -0.6683, -0.0572,  0.3531,  0.5501,  2.2726]],
       dtype=torch.float64)
	q_value: tensor([[-38.0259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17904704097996949, distance: 1.0368497553906544 entropy 1.1324905157089233
epoch: 19, step: 86
	action: tensor([[ 3.5860, -3.5036,  2.9424, -2.0118,  0.9138,  0.0643,  3.4997]],
       dtype=torch.float64)
	q_value: tensor([[-55.4163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 87
	action: tensor([[ 0.5949,  0.5417, -0.2954,  0.3488, -1.0106,  0.8832,  0.9773]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 88
	action: tensor([[ 0.1764,  0.3967,  0.1313,  1.6676, -0.4154,  0.8777, -0.4194]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 89
	action: tensor([[-0.1330, -1.5769,  0.4139,  0.1168,  1.6205, -0.2104,  0.4323]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8506742061142729, distance: 1.5567600542504458 entropy 1.1324905157089233
epoch: 19, step: 90
	action: tensor([[ 0.6884,  0.3962,  0.3639, -0.9717, -0.0628,  0.2418,  0.5686]],
       dtype=torch.float64)
	q_value: tensor([[-40.8285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6152098818973999, distance: 0.7098533698985287 entropy 1.1324905157089233
epoch: 19, step: 91
	action: tensor([[ 1.5917, -0.8619, -0.1095, -1.5107, -0.3672,  0.0360,  1.5469]],
       dtype=torch.float64)
	q_value: tensor([[-33.1667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7674575973973303, distance: 1.5213572030255917 entropy 1.1324905157089233
epoch: 19, step: 92
	action: tensor([[ 3.7549, -3.6395,  1.4159, -0.0664, -1.0672, -0.8876,  2.8007]],
       dtype=torch.float64)
	q_value: tensor([[-51.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 93
	action: tensor([[ 0.3449, -0.7604,  0.2195, -0.9007, -0.1045, -0.5886,  0.0706]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6871613986022369, distance: 1.4863976446770608 entropy 1.1324905157089233
epoch: 19, step: 94
	action: tensor([[ 0.6007, -1.2555,  0.6284,  0.8668, -0.8833, -0.6761,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-31.7428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14496104162499868, distance: 1.2244809927621396 entropy 1.1324905157089233
epoch: 19, step: 95
	action: tensor([[ 1.8297, -1.0951,  1.0717, -0.7763, -0.3485,  0.5023,  1.7216]],
       dtype=torch.float64)
	q_value: tensor([[-41.4162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40624249871241314, distance: 1.3570217318769888 entropy 1.1324905157089233
epoch: 19, step: 96
	action: tensor([[ 5.7774, -3.0901,  2.0880, -1.4768, -0.0192,  1.0936,  4.8706]],
       dtype=torch.float64)
	q_value: tensor([[-53.4049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 97
	action: tensor([[ 0.3685,  0.0403,  1.0375,  0.4207, -0.0444,  0.0039,  0.1621]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 98
	action: tensor([[ 0.0281,  0.4611, -0.1413, -0.6839,  0.4630, -0.9956,  0.2163]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27916073407208797, distance: 0.9715740601110012 entropy 1.1324905157089233
epoch: 19, step: 99
	action: tensor([[ 0.4300, -0.4177,  0.5308, -1.8320, -0.1061,  0.1016,  0.5316]],
       dtype=torch.float64)
	q_value: tensor([[-26.5046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4960805847953602, distance: 1.3996974995022076 entropy 1.1324905157089233
epoch: 19, step: 100
	action: tensor([[ 2.0636, -1.6768,  1.3032,  0.3993, -0.4214,  1.4689,  2.6105]],
       dtype=torch.float64)
	q_value: tensor([[-38.3753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 101
	action: tensor([[ 0.5083,  0.5061,  0.4288,  0.2116,  0.2354,  0.1454, -1.9195]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 102
	action: tensor([[ 1.4157,  0.1324, -0.1675, -0.0250,  0.1244,  0.8813, -0.7588]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 103
	action: tensor([[-0.4847,  0.1935, -0.3963,  0.0383,  0.5234, -0.7976,  0.3776]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2981826521417117, distance: 1.3038408809896445 entropy 1.1324905157089233
epoch: 19, step: 104
	action: tensor([[ 0.1555, -0.1344,  1.1498, -0.1652, -0.0369, -0.3102, -0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-26.1008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08870656139581246, distance: 1.0924103719813965 entropy 1.1324905157089233
epoch: 19, step: 105
	action: tensor([[ 0.2617, -1.0888,  1.5598, -0.8213, -0.5371,  0.6983,  0.3400]],
       dtype=torch.float64)
	q_value: tensor([[-30.0334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5379449444505453, distance: 1.4191460334534423 entropy 1.1324905157089233
epoch: 19, step: 106
	action: tensor([[ 1.3456, -2.0252,  0.9832, -1.9916, -0.1682,  0.0039,  1.4508]],
       dtype=torch.float64)
	q_value: tensor([[-40.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 107
	action: tensor([[ 0.3682,  0.1507,  0.0919,  0.2045,  0.4673, -0.2242,  1.9303]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 108
	action: tensor([[ 0.7838,  0.2696,  0.4667, -1.1286,  0.2729, -0.0732,  0.5886]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3984193986076541, distance: 0.8875720241729627 entropy 1.1324905157089233
epoch: 19, step: 109
	action: tensor([[ 1.1267, -2.1248,  0.8939, -0.7473, -0.1571, -0.2305,  1.0588]],
       dtype=torch.float64)
	q_value: tensor([[-33.6573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 110
	action: tensor([[ 0.3395,  0.3742,  0.2851, -1.0728, -0.2435, -0.2548,  0.8166]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26710161928804466, distance: 0.9796672148007661 entropy 1.1324905157089233
epoch: 19, step: 111
	action: tensor([[ 1.1853, -3.0784,  1.0766, -0.1847, -0.2275,  0.1845,  1.9044]],
       dtype=torch.float64)
	q_value: tensor([[-31.6879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 112
	action: tensor([[ 0.7605, -0.8002, -0.7601,  0.6539,  1.6477,  0.8020,  0.8792]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5149008125949857, distance: 0.7970250419661742 entropy 1.1324905157089233
epoch: 19, step: 113
	action: tensor([[ 1.2925, -1.3664,  1.8079, -0.2685, -0.2618,  0.9269,  2.8885]],
       dtype=torch.float64)
	q_value: tensor([[-49.5225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 114
	action: tensor([[ 1.1380, -0.6857,  0.3267, -0.7173,  1.3040, -0.2362, -0.4853]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38018151656928256, distance: 1.344388524630106 entropy 1.1324905157089233
epoch: 19, step: 115
	action: tensor([[ 1.4670, -1.3559,  0.7505, -0.2448,  0.2717,  0.2130,  0.6134]],
       dtype=torch.float64)
	q_value: tensor([[-40.2115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 116
	action: tensor([[ 0.5898,  2.1248, -1.3566, -0.0127, -0.4695, -1.3257, -1.2209]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 117
	action: tensor([[-0.5106,  0.6741, -0.2413, -0.1059, -0.0624, -0.3410,  0.9898]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 118
	action: tensor([[ 0.8701, -0.5336,  0.2562, -0.4349,  0.8323,  0.0062,  1.0116]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058560692191911734, distance: 1.177374362797642 entropy 1.1324905157089233
epoch: 19, step: 119
	action: tensor([[ 1.7391, -3.3095, -0.7038, -1.7428,  1.2360,  1.4733,  1.3840]],
       dtype=torch.float64)
	q_value: tensor([[-39.1309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 120
	action: tensor([[-1.6526, -0.2204,  1.2061,  1.9316, -0.8174,  1.4683,  0.7244]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3277155329962649, distance: 0.9382816692555905 entropy 1.1324905157089233
epoch: 19, step: 121
	action: tensor([[ 1.5925, -0.7345,  1.9734, -1.9195, -0.8947,  0.0544,  2.8425]],
       dtype=torch.float64)
	q_value: tensor([[-51.1242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8288245196947404, distance: 0.4734536519930306 entropy 1.1324905157089233
epoch: 19, step: 122
	action: tensor([[ 5.6074, -5.5644,  1.8213, -0.0765, -0.1868,  1.0413,  5.2133]],
       dtype=torch.float64)
	q_value: tensor([[-65.1851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 123
	action: tensor([[-0.0270, -0.2806, -0.3389, -1.0063, -1.0299, -0.3109,  0.6962]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07166961074850575, distance: 1.184642068577596 entropy 1.1324905157089233
epoch: 19, step: 124
	action: tensor([[ 0.4923, -1.4801,  1.3274, -0.0860,  0.8403,  1.5962,  2.0319]],
       dtype=torch.float64)
	q_value: tensor([[-33.6021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 125
	action: tensor([[-0.2109,  0.9452,  0.1265,  0.7542, -0.1715,  0.4769,  1.2031]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 126
	action: tensor([[ 0.4088,  0.5363,  0.5656, -0.2776,  1.5150,  0.8879, -1.6358]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 19, step: 127
	action: tensor([[-0.1934, -0.1966, -0.0526, -1.5218,  0.6377,  0.9845,  0.3935]],
       dtype=torch.float64)
	q_value: tensor([[-31.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12591187294187556, distance: 1.2142521823800814 entropy 1.1324905157089233
LOSS epoch 19 actor 283.44209207120525 critic 225.82539023301845 
epoch: 20, step: 0
	action: tensor([[ 2.3116, -2.7148,  2.5022, -1.6542,  1.4239,  0.3242,  2.7150]],
       dtype=torch.float64)
	q_value: tensor([[-38.7066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 1
	action: tensor([[-0.3085,  0.4351,  0.1855, -0.2556, -0.6096,  0.6018, -0.7325]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 2
	action: tensor([[ 0.8147, -1.2557,  0.0781,  0.2801,  0.2566, -0.4651, -0.6074]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5550371299835828, distance: 1.4270101918107743 entropy 1.0271300077438354
epoch: 20, step: 3
	action: tensor([[ 1.0652,  0.7472,  0.5339, -0.9859, -0.2960,  1.4506,  1.3113]],
       dtype=torch.float64)
	q_value: tensor([[-35.3673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.037023123395064805 entropy 1.0271300077438354
epoch: 20, step: 4
	action: tensor([[ 0.2726, -0.5437, -1.2568, -0.2425,  0.0426,  0.0076,  0.3309]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10302382199693016, distance: 1.0837950118496937 entropy 1.0271300077438354
epoch: 20, step: 5
	action: tensor([[ 1.4441, -1.1386,  1.6889, -0.0853,  0.8750,  0.4518,  1.6799]],
       dtype=torch.float64)
	q_value: tensor([[-32.2308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7485893440763489, distance: 1.51321489318083 entropy 1.0271300077438354
epoch: 20, step: 6
	action: tensor([[ 4.6079, -3.9165,  2.0503, -1.8556,  0.7294,  2.2706,  4.2580]],
       dtype=torch.float64)
	q_value: tensor([[-52.9832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 7
	action: tensor([[ 0.4163, -1.2385, -0.6266, -0.2630,  0.9101, -0.4201, -0.6462]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8360523906935071, distance: 1.5505980303197513 entropy 1.0271300077438354
epoch: 20, step: 8
	action: tensor([[ 0.7987, -0.7249, -0.1494, -0.9432, -1.4270, -0.7351,  1.1576]],
       dtype=torch.float64)
	q_value: tensor([[-37.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.439543484694217, distance: 1.3729954164342302 entropy 1.0271300077438354
epoch: 20, step: 9
	action: tensor([[ 2.5066, -2.5723,  2.2556, -0.6517, -0.4962,  0.5853,  2.1514]],
       dtype=torch.float64)
	q_value: tensor([[-42.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 10
	action: tensor([[-0.3894, -1.3222,  1.4602,  0.9358, -0.6919,  1.2104,  0.6065]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3664077668802992, distance: 0.9108809421714656 entropy 1.0271300077438354
epoch: 20, step: 11
	action: tensor([[ 1.9812, -3.8055,  1.3886, -1.5648,  1.2596, -0.3441,  4.0071]],
       dtype=torch.float64)
	q_value: tensor([[-44.9876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 12
	action: tensor([[-0.5466,  0.6307, -0.1209, -0.0851, -1.0986, -0.5707, -0.6072]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 13
	action: tensor([[ 0.9504, -0.2395, -0.6451,  1.3225,  0.2380,  0.3608,  0.4829]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 14
	action: tensor([[-0.4009, -0.6994, -0.3019,  0.9135,  0.2158,  0.5348, -0.1055]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13306699655444287, distance: 1.2181043333288957 entropy 1.0271300077438354
epoch: 20, step: 15
	action: tensor([[ 1.0270, -2.2170,  0.8155, -0.4443, -0.9485, -0.8871,  0.4449]],
       dtype=torch.float64)
	q_value: tensor([[-31.6226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 16
	action: tensor([[-0.3692, -0.0421,  1.2766, -0.9643,  0.4204, -0.7452,  0.7939]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6713829827119298, distance: 1.4794308863255403 entropy 1.0271300077438354
epoch: 20, step: 17
	action: tensor([[ 0.2006, -0.9950, -0.3902,  0.9951,  0.1802,  0.9066,  0.1215]],
       dtype=torch.float64)
	q_value: tensor([[-33.0161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42401674213907403, distance: 0.8684836027517597 entropy 1.0271300077438354
epoch: 20, step: 18
	action: tensor([[ 1.2043, -2.2165, -0.4657, -1.4898,  0.4725,  0.2382,  2.1389]],
       dtype=torch.float64)
	q_value: tensor([[-37.3013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 19
	action: tensor([[0.9465, 0.9356, 0.0252, 0.3992, 0.0209, 0.6929, 0.3534]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 20
	action: tensor([[ 1.8714, -0.1704, -1.3309, -0.0240,  0.3158, -0.8806,  0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 21
	action: tensor([[ 0.1643, -0.0408,  0.1451, -0.4703, -0.6702, -1.0004,  1.2512]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15734485404708443, distance: 1.0504651088633754 entropy 1.0271300077438354
epoch: 20, step: 22
	action: tensor([[ 0.1874, -0.4086, -0.1867,  0.0581,  0.2435,  0.6749,  1.6368]],
       dtype=torch.float64)
	q_value: tensor([[-33.8434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34868607155486797, distance: 0.9235318468325795 entropy 1.0271300077438354
epoch: 20, step: 23
	action: tensor([[ 3.4492, -2.9903,  1.4068, -0.8670, -0.3520,  0.2121,  3.5255]],
       dtype=torch.float64)
	q_value: tensor([[-40.7242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 24
	action: tensor([[-0.6760, -0.5908,  0.3118,  0.1296, -0.5371,  0.0841, -0.5935]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8052246425295806, distance: 1.5375254738000264 entropy 1.0271300077438354
epoch: 20, step: 25
	action: tensor([[-0.7986,  0.1910, -0.6838, -0.3816, -1.0882,  0.2391, -0.1765]],
       dtype=torch.float64)
	q_value: tensor([[-28.2279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5815262376485779, distance: 1.4391129919882142 entropy 1.0271300077438354
epoch: 20, step: 26
	action: tensor([[ 0.6477, -0.1114, -1.0579, -0.1939,  0.7857, -0.1304,  0.4219]],
       dtype=torch.float64)
	q_value: tensor([[-28.6901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5661483922083311, distance: 0.7537499544128167 entropy 1.0271300077438354
epoch: 20, step: 27
	action: tensor([[ 2.4500, -1.2615,  2.0133, -0.9789, -0.3288, -0.6845,  1.4701]],
       dtype=torch.float64)
	q_value: tensor([[-33.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 28
	action: tensor([[ 1.1624,  0.0926, -0.5669,  0.4399, -0.0921, -1.2714, -0.7457]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 29
	action: tensor([[ 1.6287, -0.6710,  0.6786,  0.1796,  0.0342, -0.5636, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18345416533563252, distance: 1.2448941168550793 entropy 1.0271300077438354
epoch: 20, step: 30
	action: tensor([[ 1.2814, -2.3837,  1.0308,  0.4471, -0.6736,  0.3769,  0.5767]],
       dtype=torch.float64)
	q_value: tensor([[-37.4099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 31
	action: tensor([[-0.4800,  0.4434, -0.0139,  0.0221, -0.2910,  0.1010,  0.0518]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 32
	action: tensor([[ 0.4605, -0.9112,  0.1295, -0.1577,  0.1344,  0.2479, -0.9182]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3029299945693362, distance: 1.3062227224715737 entropy 1.0271300077438354
epoch: 20, step: 33
	action: tensor([[-0.2110, -1.5556, -0.6690, -0.2436,  0.1695, -0.4304, -0.8219]],
       dtype=torch.float64)
	q_value: tensor([[-32.1699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 34
	action: tensor([[ 0.1402, -1.1593,  0.1144, -0.5995,  0.2593, -0.2034,  1.1077]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0171073761165363, distance: 1.6252538444873816 entropy 1.0271300077438354
epoch: 20, step: 35
	action: tensor([[ 2.4331, -2.0121,  0.8646, -1.0570, -0.3474, -0.3250,  2.5888]],
       dtype=torch.float64)
	q_value: tensor([[-37.3816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 36
	action: tensor([[ 1.2683,  0.1941, -0.6939,  1.0975, -0.1011,  0.5707,  0.2718]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 37
	action: tensor([[ 1.6158, -0.2948,  0.3251, -0.0221, -1.2762, -0.8633, -0.0360]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 38
	action: tensor([[-0.0292, -1.0009, -0.7441,  0.2498,  0.1818,  0.5452,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46902531044307416, distance: 1.3869836210999589 entropy 1.0271300077438354
epoch: 20, step: 39
	action: tensor([[ 0.5650, -0.3526,  1.4042, -0.8122, -0.6367,  0.9555,  1.3036]],
       dtype=torch.float64)
	q_value: tensor([[-33.5374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45920515993378086, distance: 0.8415365105919677 entropy 1.0271300077438354
epoch: 20, step: 40
	action: tensor([[ 4.9929, -3.8348,  2.6344, -1.5103, -0.4698,  0.6590,  3.9680]],
       dtype=torch.float64)
	q_value: tensor([[-44.2732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 41
	action: tensor([[ 1.1289, -1.0680,  0.6133,  0.8152, -0.0473,  0.0693, -0.2903]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0855850749601953, distance: 1.0942797089880159 entropy 1.0271300077438354
epoch: 20, step: 42
	action: tensor([[ 0.5977, -3.0194,  0.9562, -0.6933,  0.0137,  0.3572,  1.3645]],
       dtype=torch.float64)
	q_value: tensor([[-40.2036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 43
	action: tensor([[ 0.7393, -0.4344, -0.2117,  0.9160,  0.3462, -0.1222,  1.1509]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8380482538759232, distance: 0.46052107596914604 entropy 1.0271300077438354
epoch: 20, step: 44
	action: tensor([[ 1.6869, -2.2029,  0.7633, -0.1636,  0.5222, -0.5476,  1.9636]],
       dtype=torch.float64)
	q_value: tensor([[-38.5757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 45
	action: tensor([[-0.2849,  0.2338,  0.9101, -0.3812,  0.1602,  0.2946,  1.3651]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0751270998653486, distance: 1.1865515137711369 entropy 1.0271300077438354
epoch: 20, step: 46
	action: tensor([[ 2.7663, -1.7520,  1.6023, -0.6050,  0.4969,  1.4488,  1.8927]],
       dtype=torch.float64)
	q_value: tensor([[-35.6093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 47
	action: tensor([[ 1.6898, -0.6225,  0.4362, -0.0697, -0.7120, -0.0287,  1.2739]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19648984485897536, distance: 1.2517315584405402 entropy 1.0271300077438354
epoch: 20, step: 48
	action: tensor([[ 3.6700, -2.4213,  1.9345, -0.8040, -1.1513,  0.2821,  3.4126]],
       dtype=torch.float64)
	q_value: tensor([[-44.5303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 49
	action: tensor([[ 0.3425, -0.1615,  0.2194, -0.8669,  0.1516, -0.5621,  1.7755]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17028610684822132, distance: 1.2379488990117793 entropy 1.0271300077438354
epoch: 20, step: 50
	action: tensor([[ 1.9201, -2.0563,  2.2038, -1.5006, -1.3247,  0.4229,  3.7330]],
       dtype=torch.float64)
	q_value: tensor([[-38.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 51
	action: tensor([[-0.2453,  0.1523, -0.4261, -0.4445, -0.3568,  0.0666,  0.7699]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09877777463374404, distance: 1.086357182361297 entropy 1.0271300077438354
epoch: 20, step: 52
	action: tensor([[ 1.6412, -1.5531,  1.3703, -0.2638, -0.8753,  0.4106,  1.9017]],
       dtype=torch.float64)
	q_value: tensor([[-28.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.600141136271189, distance: 1.447557548411348 entropy 1.0271300077438354
epoch: 20, step: 53
	action: tensor([[ 3.3756, -3.5091,  3.7944, -1.4881, -0.5728,  0.6086,  5.1291]],
       dtype=torch.float64)
	q_value: tensor([[-53.9006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 54
	action: tensor([[ 0.5188, -0.5491, -0.7389, -0.0011, -0.3160, -0.2273,  0.8451]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14218259912704356, distance: 1.0598737086035996 entropy 1.0271300077438354
epoch: 20, step: 55
	action: tensor([[ 0.9276, -1.2330,  0.7765, -1.4016, -1.2611,  1.1707,  1.6605]],
       dtype=torch.float64)
	q_value: tensor([[-34.3626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2546248069291903, distance: 1.281780405994586 entropy 1.0271300077438354
epoch: 20, step: 56
	action: tensor([[ 5.0682, -5.9334,  3.7639, -1.2630, -1.1984,  0.6119,  3.7151]],
       dtype=torch.float64)
	q_value: tensor([[-54.2550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 57
	action: tensor([[-0.3808, -1.4297,  0.6278,  0.4743,  0.4079, -0.0635,  0.6014]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6426615112946079, distance: 1.4666643439573546 entropy 1.0271300077438354
epoch: 20, step: 58
	action: tensor([[ 2.1384, -1.7543,  1.1702,  0.0722, -1.3036,  0.4680,  1.6517]],
       dtype=torch.float64)
	q_value: tensor([[-35.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 59
	action: tensor([[ 0.6940,  0.6285,  0.2960,  0.5664,  0.5928, -0.1945,  0.7467]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 60
	action: tensor([[ 1.1022,  0.1293,  0.1392,  0.5808, -0.2695,  0.4682,  0.4767]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 61
	action: tensor([[ 0.7300, -0.0245, -0.0782,  0.3903, -0.4632, -1.2968, -0.0392]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 62
	action: tensor([[ 0.1895, -0.0638, -0.0206,  0.4421, -0.3365,  0.5012,  0.3618]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 63
	action: tensor([[-0.1095, -0.0805,  0.8384,  0.5567, -0.5911, -0.0293, -0.6828]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 64
	action: tensor([[-0.5412, -0.2169, -0.5131,  0.7190,  0.3142,  0.5369,  0.5812]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1958782675763986, distance: 1.2514116107045836 entropy 1.0271300077438354
epoch: 20, step: 65
	action: tensor([[ 1.1107, -0.9230,  1.1254, -1.2429,  0.3597,  0.1840,  1.5112]],
       dtype=torch.float64)
	q_value: tensor([[-30.9834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2675150256881238, distance: 1.2883481893798754 entropy 1.0271300077438354
epoch: 20, step: 66
	action: tensor([[ 4.5284, -3.2540,  2.1764, -1.5963, -1.1014,  1.7719,  4.6405]],
       dtype=torch.float64)
	q_value: tensor([[-46.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 67
	action: tensor([[ 0.8625, -1.7886,  0.8251, -1.0108,  0.9426, -0.3636, -1.3654]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 68
	action: tensor([[ 0.3647, -1.3947,  0.6292, -0.3578, -0.6605,  0.7016, -0.6092]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6335872929868873, distance: 1.462607737314636 entropy 1.0271300077438354
epoch: 20, step: 69
	action: tensor([[ 1.0419, -1.7769,  0.5158,  0.0082, -0.5744,  1.2369,  0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-36.8938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 70
	action: tensor([[-0.2966, -0.4895, -0.0526,  0.0266, -1.5269, -0.0060,  0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3946956869893581, distance: 1.3514389206871937 entropy 1.0271300077438354
epoch: 20, step: 71
	action: tensor([[ 0.9719,  0.3730,  0.9914,  0.4779, -0.4857,  0.3489,  0.4248]],
       dtype=torch.float64)
	q_value: tensor([[-32.8957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8968227626307553, distance: 0.36757726402692936 entropy 1.0271300077438354
epoch: 20, step: 72
	action: tensor([[ 2.0933, -2.4602,  0.0636, -0.8914,  1.2363, -0.2059,  2.9911]],
       dtype=torch.float64)
	q_value: tensor([[-34.6580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 73
	action: tensor([[ 0.0276, -0.3576,  1.2012, -0.9284, -0.7519, -0.3074,  0.8622]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5847016368416373, distance: 1.4405569979827593 entropy 1.0271300077438354
epoch: 20, step: 74
	action: tensor([[ 1.7589, -2.6493,  1.0972,  0.0773,  0.5061,  0.4037,  2.2264]],
       dtype=torch.float64)
	q_value: tensor([[-34.6444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 75
	action: tensor([[ 1.3643,  0.9191,  0.8967,  0.0229, -0.1149, -1.1126, -0.4821]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 76
	action: tensor([[ 0.5165, -0.5968, -0.4804, -0.8785, -1.1000, -0.4695,  0.6106]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14693652501104326, distance: 1.2255368799388722 entropy 1.0271300077438354
epoch: 20, step: 77
	action: tensor([[ 1.8308, -1.2865,  0.4244, -1.1298,  0.1897,  1.1403,  1.4058]],
       dtype=torch.float64)
	q_value: tensor([[-35.5251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7425213397970694, distance: 1.5105870107656831 entropy 1.0271300077438354
epoch: 20, step: 78
	action: tensor([[ 4.0828, -6.0681,  4.5838, -1.8940, -1.4714,  0.6354,  5.5734]],
       dtype=torch.float64)
	q_value: tensor([[-54.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 79
	action: tensor([[-0.2635, -0.1244, -0.5818, -0.7176, -1.2124,  0.2336,  0.4517]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14941856449696855, distance: 1.2268622309775985 entropy 1.0271300077438354
epoch: 20, step: 80
	action: tensor([[ 1.6941, -0.3666,  1.6870, -1.0330, -1.2570,  2.5181,  1.1223]],
       dtype=torch.float64)
	q_value: tensor([[-31.8897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6526670054228634, distance: 0.6744188706446407 entropy 1.0271300077438354
epoch: 20, step: 81
	action: tensor([[ 6.1147, -4.9420,  3.2109, -2.9336,  0.1446,  2.8001,  5.5963]],
       dtype=torch.float64)
	q_value: tensor([[-59.6993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 82
	action: tensor([[ 1.2146,  0.2148, -0.2612,  0.1591, -0.2216, -0.4730,  1.6929]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 83
	action: tensor([[-0.8195,  0.0379,  0.7555, -0.3154,  0.0936,  0.4736,  0.8258]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7296836693916053, distance: 1.5050122532807748 entropy 1.0271300077438354
epoch: 20, step: 84
	action: tensor([[ 1.7366, -1.6360,  0.8237, -1.0002, -0.6996, -0.0394,  0.6887]],
       dtype=torch.float64)
	q_value: tensor([[-31.6793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 85
	action: tensor([[-0.3814,  0.0035,  0.7914,  0.0275,  0.7446,  0.2139,  0.2709]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02471223124327704, distance: 1.158397611326403 entropy 1.0271300077438354
epoch: 20, step: 86
	action: tensor([[ 3.2753e-01, -1.8466e+00,  1.3177e+00,  6.8228e-04, -2.1045e-01,
          5.3849e-01,  1.9718e+00]], dtype=torch.float64)
	q_value: tensor([[-29.5872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 87
	action: tensor([[-0.1560, -0.4220, -0.3177, -1.4567,  1.3668,  1.3561, -0.0570]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2583714049170345, distance: 0.9854847918694842 entropy 1.0271300077438354
epoch: 20, step: 88
	action: tensor([[ 1.7483, -2.0426,  1.9158, -0.3630,  1.1062,  1.1671,  2.1552]],
       dtype=torch.float64)
	q_value: tensor([[-43.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 89
	action: tensor([[-0.1150,  0.8338,  0.3347,  0.3710,  0.6446, -0.0120, -0.2476]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 90
	action: tensor([[ 0.0909, -0.9078,  0.4326, -1.1908,  0.7772, -0.3113,  0.2865]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9359554245530175, distance: 1.5922247483730922 entropy 1.0271300077438354
epoch: 20, step: 91
	action: tensor([[ 1.6920, -0.9087,  0.8354, -1.1212, -0.5353,  0.2000,  1.1484]],
       dtype=torch.float64)
	q_value: tensor([[-34.8451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2798620593205181, distance: 1.2946079681237654 entropy 1.0271300077438354
epoch: 20, step: 92
	action: tensor([[ 3.1217, -2.4962,  1.5255, -1.2753, -0.0879,  2.0198,  3.8458]],
       dtype=torch.float64)
	q_value: tensor([[-46.2759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 93
	action: tensor([[ 0.5390,  0.2136,  0.0645, -0.6839,  0.1759, -0.5345,  0.3250]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39815159744598916, distance: 0.8877695591096652 entropy 1.0271300077438354
epoch: 20, step: 94
	action: tensor([[-0.6254,  0.0090, -0.9190, -0.5645, -0.6175,  0.2203,  1.0007]],
       dtype=torch.float64)
	q_value: tensor([[-27.0676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28293287096094555, distance: 1.2961601336142778 entropy 1.0271300077438354
epoch: 20, step: 95
	action: tensor([[ 2.2913, -1.8847,  1.6639, -0.1706,  0.5576, -0.3810,  0.7141]],
       dtype=torch.float64)
	q_value: tensor([[-32.7534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 96
	action: tensor([[ 0.6738,  1.2273, -0.8849, -0.9792, -0.3112,  0.7831,  0.1525]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 97
	action: tensor([[-0.4044, -0.6430,  0.6361, -0.8224, -0.3547, -0.0087, -0.5841]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0865516697360404, distance: 1.652993954109357 entropy 1.0271300077438354
epoch: 20, step: 98
	action: tensor([[ 0.1320,  0.0128, -0.1953, -0.2440,  1.3793, -0.0526,  1.3415]],
       dtype=torch.float64)
	q_value: tensor([[-29.1387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29087993925848943, distance: 0.963643905446399 entropy 1.0271300077438354
epoch: 20, step: 99
	action: tensor([[ 1.5144, -2.4660,  0.2406,  0.4144, -0.3971,  1.9858,  2.1787]],
       dtype=torch.float64)
	q_value: tensor([[-37.5434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 100
	action: tensor([[ 0.2207, -0.7369,  1.1166, -0.5925,  0.5113,  0.1123, -0.2359]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5746876612851843, distance: 1.4359982330758887 entropy 1.0271300077438354
epoch: 20, step: 101
	action: tensor([[ 1.8232, -1.1089,  0.7550, -0.8286,  0.5463,  0.2883,  0.4294]],
       dtype=torch.float64)
	q_value: tensor([[-32.3028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6481735946257119, distance: 1.4691230384107805 entropy 1.0271300077438354
epoch: 20, step: 102
	action: tensor([[ 2.1426, -2.8032,  0.6158, -0.6896, -1.2640, -0.7100,  2.9857]],
       dtype=torch.float64)
	q_value: tensor([[-43.8544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 103
	action: tensor([[ 0.2854,  0.0900,  0.1696,  0.5780,  1.1852, -0.4406, -0.2329]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 104
	action: tensor([[ 1.1069, -0.2970, -0.1846,  0.6039,  0.7760,  0.7678, -0.2367]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 105
	action: tensor([[ 0.9460,  0.3207, -0.5915, -0.3652,  0.4526,  0.6969, -1.3919]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 106
	action: tensor([[-0.1669, -0.1613,  0.6879, -0.2113, -0.4885, -0.5318,  0.2339]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2799211420896637, distance: 1.294637849524042 entropy 1.0271300077438354
epoch: 20, step: 107
	action: tensor([[ 1.2259, -1.0583,  0.2491, -0.3367,  0.0962,  0.7438,  1.7707]],
       dtype=torch.float64)
	q_value: tensor([[-26.4801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40674235894006294, distance: 1.357262892602991 entropy 1.0271300077438354
epoch: 20, step: 108
	action: tensor([[ 4.7171, -4.7134,  3.0653, -1.9243, -1.3414,  0.2682,  4.0737]],
       dtype=torch.float64)
	q_value: tensor([[-50.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 109
	action: tensor([[ 0.1404,  0.4085,  0.4360, -0.3923, -0.7631,  0.6364,  0.8557]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 110
	action: tensor([[ 0.2286,  0.3620,  0.3028, -0.3346,  0.1079,  0.2511,  1.6014]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6322236193586459, distance: 0.6939826461129742 entropy 1.0271300077438354
epoch: 20, step: 111
	action: tensor([[ 3.0932, -0.8193,  2.1132, -1.8622,  0.3228,  0.0212,  3.4828]],
       dtype=torch.float64)
	q_value: tensor([[-37.1910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 112
	action: tensor([[ 0.4594, -0.7211,  0.8681, -0.9060, -0.7416,  0.1542,  0.5668]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5405538560844445, distance: 1.4203492162385267 entropy 1.0271300077438354
epoch: 20, step: 113
	action: tensor([[ 1.7530, -1.7979,  1.8745, -0.2877, -0.7520,  0.7494,  0.3374]],
       dtype=torch.float64)
	q_value: tensor([[-35.9023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 114
	action: tensor([[-0.7752, -0.1217,  1.1070, -0.0231, -0.9090,  0.0139,  0.7810]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7706072380495339, distance: 1.5227121423644419 entropy 1.0271300077438354
epoch: 20, step: 115
	action: tensor([[ 1.4173, -0.2994,  0.3948, -0.8937, -0.7083,  0.8317,  2.0370]],
       dtype=torch.float64)
	q_value: tensor([[-32.7293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2814432546988429, distance: 0.9700346072489007 entropy 1.0271300077438354
epoch: 20, step: 116
	action: tensor([[ 5.0630, -5.3466,  2.9226, -2.9999, -0.8278,  0.3219,  5.0947]],
       dtype=torch.float64)
	q_value: tensor([[-52.9120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 117
	action: tensor([[-0.0729,  0.5876,  0.0546,  0.1955, -0.4114,  0.9764, -0.2666]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 118
	action: tensor([[-0.0289, -1.2263, -0.3020, -0.7703,  0.5649, -0.6868,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8044913359536079, distance: 1.5372131602971797 entropy 1.0271300077438354
epoch: 20, step: 119
	action: tensor([[ 0.5715, -0.3028, -0.0709, -1.0480, -0.3825, -0.8645,  1.1163]],
       dtype=torch.float64)
	q_value: tensor([[-35.4049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13672249883089505, distance: 1.2200676759930225 entropy 1.0271300077438354
epoch: 20, step: 120
	action: tensor([[ 1.2832, -2.0389,  2.1689, -0.9154, -0.4847,  0.2514,  0.3633]],
       dtype=torch.float64)
	q_value: tensor([[-35.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 121
	action: tensor([[ 0.6816, -1.3210,  0.8501,  0.2805,  0.1497,  0.2254,  1.0914]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3342644420459002, distance: 1.3218362245541355 entropy 1.0271300077438354
epoch: 20, step: 122
	action: tensor([[ 2.1737, -2.8812,  2.7758, -1.1777,  0.3154,  0.3650,  2.3930]],
       dtype=torch.float64)
	q_value: tensor([[-41.5833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 123
	action: tensor([[-0.0133, -1.4695,  1.2929, -0.8131, -0.2640,  0.3657,  1.3629]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1102041442021027, distance: 1.6623364550818356 entropy 1.0271300077438354
epoch: 20, step: 124
	action: tensor([[ 3.1876, -2.3626,  0.7314, -2.0506, -1.0833,  0.4470,  2.7283]],
       dtype=torch.float64)
	q_value: tensor([[-42.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 125
	action: tensor([[-0.6487, -0.2222, -0.1404,  0.2067, -0.6672,  1.1657,  1.0767]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44694956945226005, distance: 1.3765227414013224 entropy 1.0271300077438354
epoch: 20, step: 126
	action: tensor([[ 1.8855, -2.4312,  2.1327, -0.3516, -0.3408, -1.3753,  3.1532]],
       dtype=torch.float64)
	q_value: tensor([[-36.6015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 20, step: 127
	action: tensor([[ 0.1852,  0.2211,  0.0647, -0.9604,  1.0976, -0.5442,  0.8195]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07043368209017009, distance: 1.1033082935607785 entropy 1.0271300077438354
LOSS epoch 20 actor 315.7887427374809 critic 505.6595830062694 
epoch: 21, step: 0
	action: tensor([[ 1.3128, -2.0256,  0.0809,  1.2938,  0.0312,  0.9890,  1.0647]],
       dtype=torch.float64)
	q_value: tensor([[-35.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 1
	action: tensor([[ 1.5964, -0.4643, -0.6356, -0.9786,  0.5557,  0.2568,  0.3486]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 2
	action: tensor([[ 0.6999, -1.4281, -0.0065,  0.6077, -0.2906,  1.0077,  0.8203]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33907700994523193, distance: 0.9303194971165476 entropy 1.0271300077438354
epoch: 21, step: 3
	action: tensor([[ 3.5089, -3.5104,  1.9216, -1.5532, -0.5827,  0.2354,  3.3058]],
       dtype=torch.float64)
	q_value: tensor([[-45.7467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 4
	action: tensor([[ 0.2815, -0.6688, -0.6258, -0.1753,  0.4148,  0.3348,  0.9711]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04624701246333829, distance: 1.1705064433192773 entropy 1.0271300077438354
epoch: 21, step: 5
	action: tensor([[ 1.7803, -3.0924,  0.5950, -0.7957,  0.5587,  2.0531,  2.7771]],
       dtype=torch.float64)
	q_value: tensor([[-39.4991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 6
	action: tensor([[-0.6718, -0.4734, -0.1178, -0.4961, -0.7136,  0.6902,  0.7236]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9784780031370137, distance: 1.6096160960986627 entropy 1.0271300077438354
epoch: 21, step: 7
	action: tensor([[ 1.9223, -1.9199,  0.4381, -0.0639, -0.8692, -0.1153,  0.9184]],
       dtype=torch.float64)
	q_value: tensor([[-37.3226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 8
	action: tensor([[ 0.4315, -0.8938, -0.3972, -1.0634, -0.7312,  0.7699,  0.1648]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6261420937451407, distance: 1.4592709576568725 entropy 1.0271300077438354
epoch: 21, step: 9
	action: tensor([[ 2.6424, -2.6625,  1.3462, -1.0193, -0.3308,  0.1557,  2.2606]],
       dtype=torch.float64)
	q_value: tensor([[-41.3247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 10
	action: tensor([[ 0.1190, -0.9208,  0.5556, -0.7268, -0.0564,  0.0290, -0.8613]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9070977418293276, distance: 1.580313205605136 entropy 1.0271300077438354
epoch: 21, step: 11
	action: tensor([[-0.1876, -0.5491,  0.2800,  0.0177,  0.2981, -1.3549, -0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-32.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.510083886368796, distance: 1.406232820839035 entropy 1.0271300077438354
epoch: 21, step: 12
	action: tensor([[ 0.4009, -0.1223,  0.3669,  0.1820, -0.2701,  1.0305, -0.1059]],
       dtype=torch.float64)
	q_value: tensor([[-31.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8402643677577992, distance: 0.457359385606933 entropy 1.0271300077438354
epoch: 21, step: 13
	action: tensor([[ 2.2361, -1.8503,  0.7675, -0.3146,  0.4200,  1.1823,  2.5866]],
       dtype=torch.float64)
	q_value: tensor([[-34.5411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 14
	action: tensor([[-0.0353, -0.4595,  0.5850, -0.0511,  1.2690, -1.1863, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21568550662313002, distance: 1.2617325666673358 entropy 1.0271300077438354
epoch: 21, step: 15
	action: tensor([[ 0.0573, -2.1497,  0.3791, -0.9325, -0.1157, -0.3411,  0.9415]],
       dtype=torch.float64)
	q_value: tensor([[-35.2887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 16
	action: tensor([[-0.2251,  0.4256,  1.0309,  0.8820,  0.1595, -0.3556,  1.0515]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 17
	action: tensor([[-1.0165, -0.8207,  0.7986, -0.0783, -0.5471, -0.4099, -0.0579]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3843365002916497, distance: 1.7670159384610091 entropy 1.0271300077438354
epoch: 21, step: 18
	action: tensor([[ 2.1099, -0.1390,  1.2169,  0.1578, -0.7822, -0.5140,  1.4357]],
       dtype=torch.float64)
	q_value: tensor([[-32.8390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0781931506260094, distance: 1.0986937617629609 entropy 1.0271300077438354
epoch: 21, step: 19
	action: tensor([[ 3.8167, -3.3365,  1.9957, -0.7344, -0.0828,  1.4479,  4.1464]],
       dtype=torch.float64)
	q_value: tensor([[-47.0513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 20
	action: tensor([[ 1.3300,  0.1986, -0.1000, -0.3188, -0.1249, -0.0388,  1.4552]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5764731555620104, distance: 0.7447271115844294 entropy 1.0271300077438354
epoch: 21, step: 21
	action: tensor([[ 2.6566, -3.9133,  2.1199, -1.4504, -0.2245,  1.3365,  3.2647]],
       dtype=torch.float64)
	q_value: tensor([[-44.6526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 22
	action: tensor([[ 0.3475, -0.7794,  0.0454, -0.0470,  1.7274, -2.0028, -0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05594307126233233, distance: 1.1118745387789781 entropy 1.0271300077438354
epoch: 21, step: 23
	action: tensor([[ 1.6499,  0.1276, -0.2269, -0.7885, -0.6878,  1.0411,  0.8633]],
       dtype=torch.float64)
	q_value: tensor([[-44.1899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42910529699499056, distance: 0.8646387596571434 entropy 1.0271300077438354
epoch: 21, step: 24
	action: tensor([[ 4.8954, -3.2557,  3.0835, -1.7860, -0.5793,  1.1908,  4.2366]],
       dtype=torch.float64)
	q_value: tensor([[-49.1846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 25
	action: tensor([[ 0.2924, -0.4992, -0.6896, -0.5486, -0.4641, -0.4283,  1.3101]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011530848490732382, distance: 1.1509229739078426 entropy 1.0271300077438354
epoch: 21, step: 26
	action: tensor([[ 2.0282, -2.4059,  2.3219, -1.0051, -0.6925,  0.7661,  1.8290]],
       dtype=torch.float64)
	q_value: tensor([[-39.9896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 27
	action: tensor([[ 0.6895,  0.0142,  0.7949, -0.0943, -0.8893,  1.1125, -0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9074675212442698, distance: 0.3480998141072663 entropy 1.0271300077438354
epoch: 21, step: 28
	action: tensor([[ 4.1234, -3.1458,  1.1854, -1.3565, -0.0624,  0.6230,  2.7144]],
       dtype=torch.float64)
	q_value: tensor([[-39.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 29
	action: tensor([[ 0.0330, -1.6659, -0.7397, -0.3428, -0.6268, -0.5394,  1.1050]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5254443433217106, distance: 1.4133667711985995 entropy 1.0271300077438354
epoch: 21, step: 30
	action: tensor([[ 2.1014, -2.1452,  0.6150, -0.6367, -0.0208, -0.4438,  1.9159]],
       dtype=torch.float64)
	q_value: tensor([[-42.6497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 31
	action: tensor([[ 1.5668,  0.0488, -1.2729,  0.2314, -0.5850, -0.2773, -1.4107]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 32
	action: tensor([[ 0.0208, -1.9627, -0.3798,  0.4217,  0.5174,  0.8662, -0.3197]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 33
	action: tensor([[ 1.3968, -0.4247,  0.7934, -0.9853, -0.2702, -0.2003,  0.2480]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019275428249241555, distance: 1.1553204763892049 entropy 1.0271300077438354
epoch: 21, step: 34
	action: tensor([[ 3.1619, -2.4357,  1.6247, -1.9915,  0.3080,  0.5566,  2.6713]],
       dtype=torch.float64)
	q_value: tensor([[-38.5951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 35
	action: tensor([[ 1.0017,  0.2097, -0.8027, -0.1335,  0.8826, -0.7470, -0.4026]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 36
	action: tensor([[-0.2316,  0.0534, -0.3397,  0.3888, -0.0043, -0.0368,  0.2399]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1492111018203468, distance: 1.0555227552417197 entropy 1.0271300077438354
epoch: 21, step: 37
	action: tensor([[ 0.9899, -0.7217,  0.4788, -1.4895,  0.3990, -0.6805,  0.6495]],
       dtype=torch.float64)
	q_value: tensor([[-26.2602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3351621031128982, distance: 1.3222807996222514 entropy 1.0271300077438354
epoch: 21, step: 38
	action: tensor([[ 2.9568, -1.2408,  0.7981, -1.4896,  0.3208,  0.4055,  1.7719]],
       dtype=torch.float64)
	q_value: tensor([[-41.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 39
	action: tensor([[ 0.4336,  0.7391,  0.0539,  0.0329, -0.1648, -0.1003, -0.3033]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 40
	action: tensor([[ 0.2974, -0.9065,  0.1199,  0.6198,  0.4086, -0.5568,  0.3845]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07188299148435306, distance: 1.184760000080106 entropy 1.0271300077438354
epoch: 21, step: 41
	action: tensor([[ 1.0434, -0.9133,  0.0058, -0.8356, -0.7519,  0.9762,  0.2978]],
       dtype=torch.float64)
	q_value: tensor([[-34.6535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38572273280368896, distance: 1.3470845777233111 entropy 1.0271300077438354
epoch: 21, step: 42
	action: tensor([[ 2.5497, -3.2778,  2.4697, -2.0652,  1.0264,  0.0925,  3.9628]],
       dtype=torch.float64)
	q_value: tensor([[-44.8820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 43
	action: tensor([[ 0.1926, -1.6870,  0.0759,  0.7435,  0.0667, -0.3659,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2852758032055265, distance: 1.2973431378940026 entropy 1.0271300077438354
epoch: 21, step: 44
	action: tensor([[ 1.4048, -0.1531,  1.1085, -0.6877,  0.8895,  0.2157,  0.9896]],
       dtype=torch.float64)
	q_value: tensor([[-35.5285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4094984048820347, distance: 0.8793610612324082 entropy 1.0271300077438354
epoch: 21, step: 45
	action: tensor([[ 4.6283, -3.2576,  3.9653, -1.8654, -0.4518,  0.7079,  4.1967]],
       dtype=torch.float64)
	q_value: tensor([[-44.7169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 46
	action: tensor([[-0.9766, -0.3300,  0.1529,  1.0836, -0.1142,  1.7365,  0.5152]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2323317136900962, distance: 1.0026364593782116 entropy 1.0271300077438354
epoch: 21, step: 47
	action: tensor([[ 2.2569, -3.1504,  1.1681, -1.7516, -0.4249,  0.8319,  2.6422]],
       dtype=torch.float64)
	q_value: tensor([[-45.3042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 48
	action: tensor([[ 0.2827,  0.1375, -0.1576,  0.2268,  0.3149, -0.2223,  0.2408]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 49
	action: tensor([[-1.1231,  0.2447,  0.1251, -0.8705, -0.8538, -0.8248, -0.4693]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8384883403855223, distance: 1.5516263034828202 entropy 1.0271300077438354
epoch: 21, step: 50
	action: tensor([[-1.0585, -0.3228,  0.0765,  0.5620,  0.2455,  0.2792, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-31.1934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7807211158254099, distance: 1.5270548882981612 entropy 1.0271300077438354
epoch: 21, step: 51
	action: tensor([[ 0.0254,  0.4239,  0.7284,  0.7840, -0.8340, -0.7759,  0.9620]],
       dtype=torch.float64)
	q_value: tensor([[-30.1408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 52
	action: tensor([[ 0.6243, -1.4203, -0.2789,  0.6051,  0.3925,  0.3785, -0.7912]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01553126807102434, distance: 1.1354229198495784 entropy 1.0271300077438354
epoch: 21, step: 53
	action: tensor([[ 0.8904, -0.3205, -0.6632, -1.3717,  0.5487, -0.7838,  0.1952]],
       dtype=torch.float64)
	q_value: tensor([[-39.4197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2093351145708029, distance: 1.258432787413393 entropy 1.0271300077438354
epoch: 21, step: 54
	action: tensor([[ 0.2582, -1.8469, -0.2473, -1.1948,  0.3462,  0.2712,  1.8925]],
       dtype=torch.float64)
	q_value: tensor([[-39.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 55
	action: tensor([[ 1.4429, -0.2297,  0.1188,  0.2725,  0.1754,  0.4668, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 56
	action: tensor([[ 1.2821, -0.4023,  0.6064,  0.6922, -0.8274, -0.3374,  0.6498]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38983252667601276, distance: 0.8938841149797878 entropy 1.0271300077438354
epoch: 21, step: 57
	action: tensor([[ 1.8923, -2.1925,  1.4622,  0.2701, -1.3820,  1.1392,  1.0086]],
       dtype=torch.float64)
	q_value: tensor([[-40.5588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 58
	action: tensor([[ 1.3894, -0.2180, -0.6923, -0.0046, -0.4865,  0.4667,  0.7435]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4917057410151717, distance: 0.8158574723199933 entropy 1.0271300077438354
epoch: 21, step: 59
	action: tensor([[ 2.0015, -2.7763,  3.0370, -0.7947, -1.4703,  1.5750,  2.2024]],
       dtype=torch.float64)
	q_value: tensor([[-42.8530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 60
	action: tensor([[0.3497, 0.7806, 0.0454, 1.1258, 0.1051, 0.4321, 0.7715]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 61
	action: tensor([[ 0.3940,  0.1019, -1.6891, -0.3515, -0.6456,  1.1902,  0.0202]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5757959667243796, distance: 0.7453222562151685 entropy 1.0271300077438354
epoch: 21, step: 62
	action: tensor([[ 3.0984, -1.1869,  1.1738, -0.2454, -0.1691, -0.0734,  0.7657]],
       dtype=torch.float64)
	q_value: tensor([[-39.6605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 63
	action: tensor([[ 0.8676, -0.7804,  0.7932, -0.0300, -0.6183, -0.5805,  1.0280]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25823347198583013, distance: 1.2836224685892985 entropy 1.0271300077438354
epoch: 21, step: 64
	action: tensor([[ 3.1190, -3.1011,  0.8347, -0.2233, -0.8797,  0.7042,  1.2456]],
       dtype=torch.float64)
	q_value: tensor([[-39.9543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 65
	action: tensor([[ 0.2261,  0.5800, -0.1059,  0.0254,  1.3164,  0.5377, -0.6044]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 66
	action: tensor([[-0.7054,  0.1856,  0.6620, -0.6666, -0.1176, -0.7559, -0.6141]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9230050991526804, distance: 1.5868903208379734 entropy 1.0271300077438354
epoch: 21, step: 67
	action: tensor([[ 0.6716, -0.7928,  0.3090, -0.6598, -0.7052,  0.2966,  0.0051]],
       dtype=torch.float64)
	q_value: tensor([[-28.3346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4901871442817136, distance: 1.3969378975685465 entropy 1.0271300077438354
epoch: 21, step: 68
	action: tensor([[ 0.4228, -1.3797,  1.2979,  0.8918, -0.9801, -0.4418,  1.7552]],
       dtype=torch.float64)
	q_value: tensor([[-36.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 69
	action: tensor([[-1.4604,  0.1970,  0.3627,  0.1285,  1.5586, -0.1779, -1.4753]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2619990534706669, distance: 1.7210873109748466 entropy 1.0271300077438354
epoch: 21, step: 70
	action: tensor([[ 0.1326,  0.4050,  0.4416, -0.4548, -0.6677,  0.0922, -0.5981]],
       dtype=torch.float64)
	q_value: tensor([[-38.6617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41736351709662944, distance: 0.8734851599197205 entropy 1.0271300077438354
epoch: 21, step: 71
	action: tensor([[ 0.1713,  0.9781, -0.7889,  0.3720, -0.2917, -0.3279,  0.7991]],
       dtype=torch.float64)
	q_value: tensor([[-28.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 72
	action: tensor([[ 0.2705, -1.7010,  0.0712,  0.2394, -0.1786, -0.8309,  1.7337]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6966471535878078, distance: 1.4905702867517814 entropy 1.0271300077438354
epoch: 21, step: 73
	action: tensor([[ 1.7545, -2.0761,  1.7683, -1.6361,  0.2256,  0.0979,  3.1898]],
       dtype=torch.float64)
	q_value: tensor([[-45.4782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 74
	action: tensor([[-0.8208, -0.1853,  0.5481, -0.5550,  0.1539, -0.6399,  0.9852]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1555318215603267, distance: 1.680095283282737 entropy 1.0271300077438354
epoch: 21, step: 75
	action: tensor([[ 1.2075,  0.9251,  0.8868, -1.2647, -0.3026, -0.6021,  1.2163]],
       dtype=torch.float64)
	q_value: tensor([[-32.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8118124589925206, distance: 0.49642326545002424 entropy 1.0271300077438354
epoch: 21, step: 76
	action: tensor([[ 3.1801, -3.6250,  2.1512, -2.3354, -0.3597,  2.0885,  3.9557]],
       dtype=torch.float64)
	q_value: tensor([[-44.0507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 77
	action: tensor([[-1.1445,  0.5215,  0.6093, -1.4033, -0.0946, -0.6961, -0.2586]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.969949621410495, distance: 1.6061431624283087 entropy 1.0271300077438354
epoch: 21, step: 78
	action: tensor([[ 1.7252, -0.0340, -1.0794,  0.5662, -0.6053,  0.4897,  0.6229]],
       dtype=torch.float64)
	q_value: tensor([[-31.8464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7765300564238679, distance: 0.5409613695681914 entropy 1.0271300077438354
epoch: 21, step: 79
	action: tensor([[ 3.1149, -2.3801,  1.7107, -1.0439, -0.5608, -0.7839,  4.3578]],
       dtype=torch.float64)
	q_value: tensor([[-45.2996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 80
	action: tensor([[ 0.0182, -1.2855,  1.2562, -0.7002,  0.0837,  0.0151,  0.7277]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1376490831735415, distance: 1.6731115593076804 entropy 1.0271300077438354
epoch: 21, step: 81
	action: tensor([[ 3.3098, -2.1305,  1.3945, -1.4762,  0.0858, -1.0927,  1.6889]],
       dtype=torch.float64)
	q_value: tensor([[-39.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 82
	action: tensor([[-0.1679,  0.7062, -0.0736, -0.1721,  0.1443,  0.1351,  0.4184]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 83
	action: tensor([[-0.5369,  0.4563, -0.8889, -0.1491,  0.9264,  0.4360,  0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13262063769611698, distance: 1.065764464591811 entropy 1.0271300077438354
epoch: 21, step: 84
	action: tensor([[ 1.2799, -1.4301, -0.1458, -0.2281,  1.2318,  0.1153,  0.6494]],
       dtype=torch.float64)
	q_value: tensor([[-32.7972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 85
	action: tensor([[-0.4602, -0.3421,  0.7912,  0.1250, -0.1383,  1.8749, -0.2225]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38449315917166027, distance: 0.8977866337197231 entropy 1.0271300077438354
epoch: 21, step: 86
	action: tensor([[ 2.5390, -1.9006,  1.3858, -0.3691,  0.7188,  1.7077,  1.1071]],
       dtype=torch.float64)
	q_value: tensor([[-43.6270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 87
	action: tensor([[-0.6982, -0.3893, -0.7482,  0.0604, -0.6013, -0.7003,  1.0601]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5126179723285724, distance: 1.407412232506409 entropy 1.0271300077438354
epoch: 21, step: 88
	action: tensor([[ 0.2252, -1.8233,  0.1369, -0.9431, -0.2968,  0.6927,  1.8437]],
       dtype=torch.float64)
	q_value: tensor([[-37.0719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 89
	action: tensor([[ 0.3009,  0.3352,  0.5801,  0.5359, -0.4231, -0.4351,  0.5882]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 90
	action: tensor([[ 0.1859, -0.4546, -0.8437,  1.3492,  1.0170,  1.8121, -0.5289]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 91
	action: tensor([[ 1.6211, -0.2698,  2.0030, -1.7071,  0.6676,  1.0880, -0.6598]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 92
	action: tensor([[ 1.3963, -0.1892,  0.3376,  0.1272, -0.1041,  0.3434, -0.8439]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 93
	action: tensor([[ 0.3873, -1.2620,  0.3955, -0.5411, -1.0032, -0.7761,  0.4180]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9015094980589802, distance: 1.5779961627807795 entropy 1.0271300077438354
epoch: 21, step: 94
	action: tensor([[ 0.6967, -1.6182,  0.8634, -2.1990, -0.2027,  1.1251,  2.1519]],
       dtype=torch.float64)
	q_value: tensor([[-38.1112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 95
	action: tensor([[ 0.8570, -1.2257, -0.3413, -0.7739,  0.2194,  0.7755, -0.5760]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7362538917339285, distance: 1.5078679467604772 entropy 1.0271300077438354
epoch: 21, step: 96
	action: tensor([[ 1.7620, -2.4797,  1.3018, -0.4867,  0.0646,  0.4672,  0.8074]],
       dtype=torch.float64)
	q_value: tensor([[-42.6300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 97
	action: tensor([[ 0.5383,  0.0207,  0.4392,  0.2278, -0.1382,  0.2482,  0.8047]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8697273681507389, distance: 0.41303160678534745 entropy 1.0271300077438354
epoch: 21, step: 98
	action: tensor([[ 2.3253, -2.7951,  1.5488, -1.8309, -1.0375,  0.1280,  2.4197]],
       dtype=torch.float64)
	q_value: tensor([[-33.9405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 99
	action: tensor([[ 0.3153, -0.9344,  0.6388, -0.5161, -0.4602, -0.1220, -0.7003]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7488767591326297, distance: 1.5133392513884005 entropy 1.0271300077438354
epoch: 21, step: 100
	action: tensor([[ 0.0521, -1.3411,  1.0506, -0.5564, -1.4251,  0.2490,  1.3415]],
       dtype=torch.float64)
	q_value: tensor([[-32.3919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 101
	action: tensor([[ 1.7347, -0.3186, -1.0942, -1.0461, -0.3913, -0.0785,  0.2047]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 102
	action: tensor([[-0.2654,  0.2663, -0.4995, -1.0662, -0.8691,  0.1657,  0.0731]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14084869570319292, distance: 1.0606974386844643 entropy 1.0271300077438354
epoch: 21, step: 103
	action: tensor([[ 1.0749, -2.0850, -0.6339, -0.1012,  0.3949, -0.0276,  1.8769]],
       dtype=torch.float64)
	q_value: tensor([[-31.8253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 104
	action: tensor([[-0.1027,  0.3788, -0.3443,  0.1914, -0.8756,  1.1633,  1.4506]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 105
	action: tensor([[ 1.0985, -1.3700, -0.2132,  0.1160,  0.0554,  0.9291, -0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08708824405674509, distance: 1.1931336461132496 entropy 1.0271300077438354
epoch: 21, step: 106
	action: tensor([[ 1.9065, -1.6542,  1.1315, -0.9450,  0.5443,  1.0131,  1.2680]],
       dtype=torch.float64)
	q_value: tensor([[-43.8769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 107
	action: tensor([[ 0.9152,  0.3658,  0.7517, -1.1576, -0.1737,  0.2648,  0.5186]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6291848266907312, distance: 0.6968438019683857 entropy 1.0271300077438354
epoch: 21, step: 108
	action: tensor([[ 3.2753, -2.5467,  0.7727, -0.5665, -1.0379,  0.3737,  1.9945]],
       dtype=torch.float64)
	q_value: tensor([[-38.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 109
	action: tensor([[ 0.8502,  1.1296, -0.8742, -0.9580,  0.2821,  0.8294,  1.0034]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 110
	action: tensor([[ 0.6952,  0.5249,  0.0127,  0.5180, -0.1930, -0.0638,  0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 111
	action: tensor([[-0.5325, -0.6573, -0.6528, -0.8787,  0.1523,  1.5311,  1.0382]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4191734556772544, distance: 1.3632466307760422 entropy 1.0271300077438354
epoch: 21, step: 112
	action: tensor([[ 4.1291, -3.7463,  2.4994, -0.5827,  0.7837,  0.8499,  4.6144]],
       dtype=torch.float64)
	q_value: tensor([[-48.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 113
	action: tensor([[0.2421, 0.6260, 0.8114, 0.9958, 0.4253, 0.2201, 1.2458]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 114
	action: tensor([[-0.2647,  0.1164, -0.6336, -0.5439,  0.9884,  1.5295,  0.6666]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 115
	action: tensor([[ 0.5336,  0.2659, -1.4579, -0.1866,  1.0194,  1.1269,  0.6245]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 116
	action: tensor([[-0.0618,  0.0735,  0.7928,  1.1846,  0.0943,  1.3204, -0.9286]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 117
	action: tensor([[ 0.6891, -0.2160,  0.9193, -0.1826, -0.3984,  0.1337, -0.7705]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48771244848526263, distance: 0.8190559972526227 entropy 1.0271300077438354
epoch: 21, step: 118
	action: tensor([[ 0.7534, -1.3864, -0.0276, -0.5398, -0.6457,  0.3991, -0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-31.6455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8570802613635502, distance: 1.5594520666379474 entropy 1.0271300077438354
epoch: 21, step: 119
	action: tensor([[ 2.7252, -2.2667,  1.3941,  0.4247, -0.2950,  0.0640,  1.3960]],
       dtype=torch.float64)
	q_value: tensor([[-39.8287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 120
	action: tensor([[-1.0748, -1.1081, -0.6604, -0.6241,  0.1223, -0.4113,  0.1621]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6811789404888278, distance: 1.4837600166431995 entropy 1.0271300077438354
epoch: 21, step: 121
	action: tensor([[-0.2481, -1.5959, -1.0531,  0.4244,  0.0493,  0.2622,  0.9834]],
       dtype=torch.float64)
	q_value: tensor([[-36.6829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 122
	action: tensor([[ 0.6685,  0.4163, -0.0535, -0.7581,  0.6738,  0.2315, -0.3958]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 123
	action: tensor([[-2.5586e-01,  9.4609e-04,  4.6376e-01, -9.8276e-01, -9.7766e-03,
          9.4827e-01,  3.5074e-01]], dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2315719539478489, distance: 1.2699499006684993 entropy 1.0271300077438354
epoch: 21, step: 124
	action: tensor([[ 3.2492, -2.4546,  1.3129, -0.6655,  0.3440,  1.6044,  2.7423]],
       dtype=torch.float64)
	q_value: tensor([[-36.8979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 125
	action: tensor([[ 1.5330,  0.5304,  0.5054, -0.7549, -0.7793, -0.4783,  0.9379]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 21, step: 126
	action: tensor([[-0.9926, -1.3005,  1.9734, -0.3510, -0.6438,  1.1203,  0.5483]],
       dtype=torch.float64)
	q_value: tensor([[-36.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9886725810936328, distance: 1.6137577324357464 entropy 1.0271300077438354
epoch: 21, step: 127
	action: tensor([[ 2.9464, -3.5635,  2.5678, -1.9858, -1.0168,  1.3506,  2.2784]],
       dtype=torch.float64)
	q_value: tensor([[-48.2946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
LOSS epoch 21 actor 272.6204582988483 critic 165.9433166328764 
epoch: 22, step: 0
	action: tensor([[ 0.0807, -0.6317,  0.6716, -0.4870, -0.2504,  0.7912, -0.6821]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2509731639632935, distance: 1.279913706435781 entropy 1.0271300077438354
epoch: 22, step: 1
	action: tensor([[ 1.5276, -1.6714,  1.1738, -0.9247, -0.0967,  0.8638,  0.8757]],
       dtype=torch.float64)
	q_value: tensor([[-39.3441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 2
	action: tensor([[ 0.2496, -1.2008,  1.2386, -0.5727,  0.5928, -0.8506, -0.4241]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7698987560432675, distance: 1.5224074666379912 entropy 1.0271300077438354
epoch: 22, step: 3
	action: tensor([[ 0.4795, -1.9425,  0.1227, -0.4499, -1.0834, -0.5796,  0.1288]],
       dtype=torch.float64)
	q_value: tensor([[-42.4914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 4
	action: tensor([[-0.3632,  0.3934,  0.4210,  0.1333,  0.2565,  0.1302, -0.8818]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 5
	action: tensor([[ 0.9271,  0.6486,  0.1418, -0.2263, -0.6636, -0.0670, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 6
	action: tensor([[ 0.6925,  1.0341,  0.5333, -0.8454, -0.4977,  0.3412,  0.8079]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 7
	action: tensor([[-0.9033,  1.5254,  0.0509, -0.1810, -0.6031,  1.0994, -0.7395]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 8
	action: tensor([[-0.4877, -0.2096,  0.0195,  0.2871,  0.4480, -0.2901,  1.1886]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4518000233581132, distance: 1.378827995903797 entropy 1.0271300077438354
epoch: 22, step: 9
	action: tensor([[ 0.8216, -1.1297,  2.5184, -0.8352, -0.7053,  0.6495,  1.1607]],
       dtype=torch.float64)
	q_value: tensor([[-39.6788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025414355468305505, distance: 1.1297092851860524 entropy 1.0271300077438354
epoch: 22, step: 10
	action: tensor([[ 6.1800, -5.8548,  3.2147, -2.6738, -0.0945,  1.2921,  5.4991]],
       dtype=torch.float64)
	q_value: tensor([[-60.9540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 11
	action: tensor([[-0.5444, -0.7991,  0.2302,  0.4860, -0.6593, -0.3761,  0.5417]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5983254494526611, distance: 1.446736040510471 entropy 1.0271300077438354
epoch: 22, step: 12
	action: tensor([[ 1.1039, -0.3858,  1.6925,  1.3745, -0.8392,  0.6183,  2.3487]],
       dtype=torch.float64)
	q_value: tensor([[-38.8498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.167693317907593, distance: 1.043994916421442 entropy 1.0271300077438354
epoch: 22, step: 13
	action: tensor([[ 6.1800, -6.2800,  3.8105, -4.8419,  0.0106,  0.8230,  6.1184]],
       dtype=torch.float64)
	q_value: tensor([[-69.3191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 14
	action: tensor([[ 0.6034, -0.3483,  0.9072, -0.0077,  0.1648,  0.2125,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4602058935943588, distance: 0.8407575239875494 entropy 1.0271300077438354
epoch: 22, step: 15
	action: tensor([[ 2.1922, -3.0823,  1.7230, -1.1981, -0.5039, -0.5370,  3.2360]],
       dtype=torch.float64)
	q_value: tensor([[-37.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 16
	action: tensor([[-0.3248,  0.8278,  0.3030,  1.2890, -1.1004, -1.2681,  0.4950]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 17
	action: tensor([[ 0.0614, -0.3480,  0.4377,  0.0870, -1.5749,  1.7685, -0.1955]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20979650305869912, distance: 1.0172464124762097 entropy 1.0271300077438354
epoch: 22, step: 18
	action: tensor([[ 3.3817, -2.8040,  1.7205, -2.4594, -0.1859,  1.0347,  2.4466]],
       dtype=torch.float64)
	q_value: tensor([[-53.9907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 19
	action: tensor([[ 0.2203, -0.0719,  0.9648, -0.5213, -0.5463, -0.5840, -0.5434]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030672122010037217, distance: 1.1617614404697303 entropy 1.0271300077438354
epoch: 22, step: 20
	action: tensor([[-0.3005, -1.3804,  0.7767, -1.0525, -0.5636, -1.1014,  1.0921]],
       dtype=torch.float64)
	q_value: tensor([[-34.8116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7936327645270067, distance: 1.5325810721725002 entropy 1.0271300077438354
epoch: 22, step: 21
	action: tensor([[ 1.2752, -1.5563,  1.1313, -1.8605, -0.2960,  0.2155,  1.6936]],
       dtype=torch.float64)
	q_value: tensor([[-48.1138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 22
	action: tensor([[-0.9591, -0.6320, -0.9859, -1.0365, -0.6228,  0.2707,  1.1134]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5285118103372846, distance: 1.414787104445061 entropy 1.0271300077438354
epoch: 22, step: 23
	action: tensor([[ 2.6860, -1.8541,  1.9103, -1.0269, -0.9933,  0.6931,  2.9185]],
       dtype=torch.float64)
	q_value: tensor([[-51.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 24
	action: tensor([[-0.4615, -0.2776, -0.1115,  0.2595,  0.0373,  1.3085, -0.7188]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0629580359829306, distance: 1.1077358561164146 entropy 1.0271300077438354
epoch: 22, step: 25
	action: tensor([[ 1.9510, -1.6066,  0.9873, -1.0355, -1.0365,  1.4771,  0.5804]],
       dtype=torch.float64)
	q_value: tensor([[-40.9202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 26
	action: tensor([[-0.1553, -0.6218,  1.1328,  0.4894,  0.0488, -0.3045, -0.0485]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07800852075172626, distance: 1.1881404731678211 entropy 1.0271300077438354
epoch: 22, step: 27
	action: tensor([[ 2.4019, -1.6455,  0.4856, -1.1230, -1.0562, -0.1085,  0.3204]],
       dtype=torch.float64)
	q_value: tensor([[-37.3033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 28
	action: tensor([[ 1.6921,  0.5561, -0.4407, -0.8639, -0.1873, -0.0825, -0.5106]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 29
	action: tensor([[ 1.1852, -0.7350, -0.1555, -1.2616, -0.2632, -0.6337,  0.1433]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6302933880975994, distance: 1.4611324192209685 entropy 1.0271300077438354
epoch: 22, step: 30
	action: tensor([[ 2.6334, -1.9446,  1.6684, -1.8262, -0.9686, -0.3664,  1.8723]],
       dtype=torch.float64)
	q_value: tensor([[-44.9704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 31
	action: tensor([[ 0.3318,  0.0333, -0.1739,  0.1784, -0.4697,  0.2441, -1.2939]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6758985288497228, distance: 0.6514741598115604 entropy 1.0271300077438354
epoch: 22, step: 32
	action: tensor([[ 0.5377, -0.7643, -0.2479,  0.0055,  0.1529, -0.1863,  0.4244]],
       dtype=torch.float64)
	q_value: tensor([[-32.3236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16983356266517768, distance: 1.2377095213332785 entropy 1.0271300077438354
epoch: 22, step: 33
	action: tensor([[ 1.5074, -1.7478,  0.9974, -0.7173,  0.7679,  1.2974,  1.4984]],
       dtype=torch.float64)
	q_value: tensor([[-38.4330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 34
	action: tensor([[-0.0527, -1.2321,  0.2576, -0.2561, -0.0549,  1.3124,  0.5791]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37240382451297616, distance: 1.340595178463919 entropy 1.0271300077438354
epoch: 22, step: 35
	action: tensor([[ 5.1147, -3.7314,  1.9007, -1.4371, -0.9687,  0.6827,  3.2698]],
       dtype=torch.float64)
	q_value: tensor([[-50.9084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 36
	action: tensor([[-0.1600, -0.5094, -0.3395, -0.3143,  0.8498, -0.9863,  0.9381]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5326816685391269, distance: 1.416715595786189 entropy 1.0271300077438354
epoch: 22, step: 37
	action: tensor([[ 0.9862, -1.8482,  0.3540,  0.3006, -1.0621,  1.1994,  1.2513]],
       dtype=torch.float64)
	q_value: tensor([[-40.4059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 38
	action: tensor([[-0.1666, -1.2532,  0.0090, -1.8770,  0.7485,  0.0552,  0.4407]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42881784842241544, distance: 1.3678709507425295 entropy 1.0271300077438354
epoch: 22, step: 39
	action: tensor([[ 2.5060, -0.6256,  3.3860, -0.2732, -1.1967,  0.3644,  3.3500]],
       dtype=torch.float64)
	q_value: tensor([[-52.9903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 40
	action: tensor([[ 1.6908,  0.7019, -0.4670,  0.1648, -0.2165,  0.1044,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 41
	action: tensor([[ 1.1371, -1.0420,  1.0597, -0.3430,  0.6387,  1.0331,  0.8094]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4551729071180104, distance: 1.380428742728404 entropy 1.0271300077438354
epoch: 22, step: 42
	action: tensor([[ 6.1800, -5.1719,  2.6890, -1.7738, -0.3593,  0.5729,  4.7845]],
       dtype=torch.float64)
	q_value: tensor([[-56.6131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 43
	action: tensor([[ 0.0109,  0.2506, -0.6168, -0.7953, -1.6738,  0.4098,  0.4150]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3278194279946426, distance: 0.9382091653202526 entropy 1.0271300077438354
epoch: 22, step: 44
	action: tensor([[ 1.7188, -2.3027,  1.3612, -1.2333, -0.6335, -0.6145,  2.5669]],
       dtype=torch.float64)
	q_value: tensor([[-46.2387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 45
	action: tensor([[ 0.4605, -0.8520, -0.4718, -0.3487, -0.5055,  0.9726,  1.6821]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22717455393823904, distance: 1.267680657879514 entropy 1.0271300077438354
epoch: 22, step: 46
	action: tensor([[ 6.1800, -5.0539,  3.8003, -1.9550, -1.2484,  1.6130,  4.5574]],
       dtype=torch.float64)
	q_value: tensor([[-58.7876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3719479016773248 entropy 1.0271300077438354
epoch: 22, step: 47
	action: tensor([[-0.8317,  0.1031,  1.3185, -0.0403,  0.5707, -1.7076,  0.8889]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5084235554370629, distance: 1.405459534665078 entropy 1.0271300077438354
epoch: 22, step: 48
	action: tensor([[ 1.2963, -0.8105,  0.8834, -0.0372, -0.6598,  0.0699,  1.6517]],
       dtype=torch.float64)
	q_value: tensor([[-44.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22934847139688808, distance: 1.2688029976917499 entropy 1.0271300077438354
epoch: 22, step: 49
	action: tensor([[ 5.6694, -4.6387,  3.8976, -2.5363, -0.5743,  1.2425,  5.6512]],
       dtype=torch.float64)
	q_value: tensor([[-57.1009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 50
	action: tensor([[-0.5394, -0.9689, -0.8814,  0.8193, -0.0544, -1.1255,  1.5742]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8944525038264304, distance: 1.575065264694404 entropy 1.0271300077438354
epoch: 22, step: 51
	action: tensor([[ 0.4942, -2.0382,  1.1688, -1.3037, -0.3811,  0.2527,  0.6052]],
       dtype=torch.float64)
	q_value: tensor([[-53.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 52
	action: tensor([[-0.6607, -1.2669, -0.2072, -0.4689, -0.8174,  0.2779,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1337910464753986, distance: 1.6716010586024441 entropy 1.0271300077438354
epoch: 22, step: 53
	action: tensor([[ 0.6845, -0.9937,  1.0493, -0.4937,  0.2003,  1.4710,  2.2890]],
       dtype=torch.float64)
	q_value: tensor([[-43.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11245364047430995, distance: 1.2069732752111577 entropy 1.0271300077438354
epoch: 22, step: 54
	action: tensor([[ 6.0684, -5.4991,  5.1720, -2.6513, -0.0715,  1.6736,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-70.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 55
	action: tensor([[ 0.2830, -0.1812, -0.1167, -0.7876, -0.6314,  0.5386,  0.9003]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0537941346708527, distance: 1.1131392875715527 entropy 1.0271300077438354
epoch: 22, step: 56
	action: tensor([[ 4.4734, -2.8439,  2.3508,  0.2384, -0.9558,  0.7423,  4.8441]],
       dtype=torch.float64)
	q_value: tensor([[-45.1827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 57
	action: tensor([[-1.3394, -0.2028,  0.2526, -0.3603, -0.0900, -0.1968,  0.2138]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6059088991371007, distance: 1.8472952255142419 entropy 1.0271300077438354
epoch: 22, step: 58
	action: tensor([[ 0.3422, -1.7643,  0.8551, -0.2307, -0.3240,  0.4614,  1.2750]],
       dtype=torch.float64)
	q_value: tensor([[-34.9411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 59
	action: tensor([[-0.1785, -0.8853,  0.0032, -0.7953, -0.3111,  0.0961,  1.9524]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8820985363765339, distance: 1.5699212636297608 entropy 1.0271300077438354
epoch: 22, step: 60
	action: tensor([[ 5.0479, -6.0963,  1.9688, -2.7961, -1.3748,  1.5035,  5.1207]],
       dtype=torch.float64)
	q_value: tensor([[-55.7350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 61
	action: tensor([[-0.2895, -1.1980, -0.3056, -0.2475,  1.3284, -0.7089,  1.1451]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.094947250959199, distance: 1.65631616109689 entropy 1.0271300077438354
epoch: 22, step: 62
	action: tensor([[ 2.7905, -1.2080, -0.4422, -1.2720,  0.3081, -1.2802,  1.6064]],
       dtype=torch.float64)
	q_value: tensor([[-48.7437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 63
	action: tensor([[-0.8472, -0.1005,  1.2572,  0.2143,  0.0110,  0.8307,  0.3220]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15059717920920557, distance: 1.2274910825674605 entropy 1.0271300077438354
epoch: 22, step: 64
	action: tensor([[ 2.8973, -1.9775,  2.0453, -1.6622,  0.1560,  1.0855,  1.7597]],
       dtype=torch.float64)
	q_value: tensor([[-42.5615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 65
	action: tensor([[-0.1694,  0.0381, -0.2280,  0.1605,  1.2744, -0.1674,  0.2732]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12760703631220338, distance: 1.0688401770987774 entropy 1.0271300077438354
epoch: 22, step: 66
	action: tensor([[ 1.2449, -0.7420,  2.5196, -0.8571, -0.1760, -0.5546,  1.6209]],
       dtype=torch.float64)
	q_value: tensor([[-37.2308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7038358378358375, distance: 0.6227632059367124 entropy 1.0271300077438354
epoch: 22, step: 67
	action: tensor([[ 5.3083, -6.1600,  2.3736, -2.7247,  0.3345,  1.9343,  5.5709]],
       dtype=torch.float64)
	q_value: tensor([[-60.8349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 68
	action: tensor([[ 0.5105, -0.0183, -0.2463, -0.4749, -0.3303,  0.5378,  1.2115]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5530472457479738, distance: 0.7650459179050609 entropy 1.0271300077438354
epoch: 22, step: 69
	action: tensor([[ 4.5874, -3.7688,  2.6795, -1.4077, -0.0879,  1.9836,  4.9485]],
       dtype=torch.float64)
	q_value: tensor([[-46.7873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 70
	action: tensor([[ 1.0377, -0.3935, -0.2748, -1.2279,  1.4569,  0.2342,  0.1736]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31074097065121675, distance: 1.3101322297997873 entropy 1.0271300077438354
epoch: 22, step: 71
	action: tensor([[ 2.2679, -1.6721,  3.4241, -1.0441,  0.2855, -0.7925,  2.6417]],
       dtype=torch.float64)
	q_value: tensor([[-51.0678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 72
	action: tensor([[ 0.0599, -0.1061,  0.2458, -0.6634,  1.1443,  1.1844, -0.0362]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3437749717589311, distance: 0.9270071596496883 entropy 1.0271300077438354
epoch: 22, step: 73
	action: tensor([[ 2.3305, -1.3732,  2.1692, -1.3898, -0.3039, -0.0802,  2.5003]],
       dtype=torch.float64)
	q_value: tensor([[-45.8575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 74
	action: tensor([[ 0.9427, -0.0731,  1.1455, -0.2319, -0.5134,  0.3535,  0.5155]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7290989558564672, distance: 0.5956100713337831 entropy 1.0271300077438354
epoch: 22, step: 75
	action: tensor([[ 4.3259, -2.7126,  1.2419, -0.4461, -0.8749,  0.2924,  3.4273]],
       dtype=torch.float64)
	q_value: tensor([[-43.2434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 76
	action: tensor([[ 0.4064, -1.0199, -1.0978, -0.6057, -1.6621,  0.2929,  0.9311]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4934170516266454, distance: 1.3984509752264067 entropy 1.0271300077438354
epoch: 22, step: 77
	action: tensor([[ 4.3700, -3.2305,  1.6900, -1.5535, -0.9415,  0.4585,  3.6123]],
       dtype=torch.float64)
	q_value: tensor([[-56.5992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 78
	action: tensor([[ 0.3339, -1.2102, -1.6341,  0.1223, -0.0156, -0.7644, -0.1068]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4735845300064916, distance: 1.3891342525465562 entropy 1.0271300077438354
epoch: 22, step: 79
	action: tensor([[ 0.5889, -1.4779,  0.5857,  1.5876,  0.7129,  0.0222,  0.8457]],
       dtype=torch.float64)
	q_value: tensor([[-46.5156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 80
	action: tensor([[-0.4283,  0.0630,  1.0095, -0.1833, -0.5982,  0.0471,  1.6612]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3271932369496402, distance: 1.3183289018206628 entropy 1.0271300077438354
epoch: 22, step: 81
	action: tensor([[ 2.1298, -3.1407,  2.4260, -0.9500, -1.0101,  0.7329,  3.8967]],
       dtype=torch.float64)
	q_value: tensor([[-47.7267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 82
	action: tensor([[-0.7056, -0.5124, -1.4346, -0.3336, -0.4172, -0.8872,  0.0688]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14259961432083768, distance: 1.2232176243195252 entropy 1.0271300077438354
epoch: 22, step: 83
	action: tensor([[ 0.9010, -0.5198,  0.5704,  0.2570, -0.3815,  0.2786,  1.9343]],
       dtype=torch.float64)
	q_value: tensor([[-40.8150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46060315014790665, distance: 0.840448093127324 entropy 1.0271300077438354
epoch: 22, step: 84
	action: tensor([[ 5.5821, -6.2800,  2.7410, -4.0108,  0.2370,  1.5764,  3.9836]],
       dtype=torch.float64)
	q_value: tensor([[-56.4850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 85
	action: tensor([[ 0.0577, -0.1011, -0.1838,  0.2526,  0.3433,  0.7517,  1.2800]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 86
	action: tensor([[-0.2733, -1.2230,  0.9610,  0.1176,  0.1283, -0.1249,  0.4428]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9013349224768938, distance: 1.5779237240361685 entropy 1.0271300077438354
epoch: 22, step: 87
	action: tensor([[ 1.7510, -2.0768,  0.9986, -0.7880, -0.1640,  0.2253,  1.2796]],
       dtype=torch.float64)
	q_value: tensor([[-41.4958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 88
	action: tensor([[ 1.1463, -0.5593,  0.1815, -0.4177,  0.0580, -0.2143,  1.0914]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16750747333656713, distance: 1.236478382313345 entropy 1.0271300077438354
epoch: 22, step: 89
	action: tensor([[ 4.3043, -3.3795,  2.8287, -1.6109,  0.7406, -0.2549,  4.1726]],
       dtype=torch.float64)
	q_value: tensor([[-46.9926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 90
	action: tensor([[ 1.1742, -1.5131,  0.4135, -0.4205,  0.3708,  0.2345, -0.0527]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8564252456360486, distance: 1.5591770231360482 entropy 1.0271300077438354
epoch: 22, step: 91
	action: tensor([[ 1.9783, -2.4476,  1.4566, -0.6077,  0.5817,  0.5126,  2.1631]],
       dtype=torch.float64)
	q_value: tensor([[-46.0964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 92
	action: tensor([[ 0.7473,  0.3931, -0.5954, -1.7359, -0.2966,  0.5903,  0.1895]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 93
	action: tensor([[ 0.3597, -0.2460,  0.6373, -0.0400, -0.2100,  0.7103,  0.1654]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6044635455657624, distance: 0.7196974292304097 entropy 1.0271300077438354
epoch: 22, step: 94
	action: tensor([[ 2.1204, -1.4299,  2.5716, -1.3622,  0.9593, -0.0335,  2.5830]],
       dtype=torch.float64)
	q_value: tensor([[-38.4080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 95
	action: tensor([[ 0.4973, -0.4361, -0.9203, -0.3325,  0.0674,  0.7243, -0.4950]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3463398138235212, distance: 0.9251937918012859 entropy 1.0271300077438354
epoch: 22, step: 96
	action: tensor([[ 1.5913, -2.1322,  0.6191, -0.3613,  0.9411,  1.0854,  1.5994]],
       dtype=torch.float64)
	q_value: tensor([[-39.1462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 97
	action: tensor([[ 1.0903,  0.1219, -0.8633, -0.6559,  0.0298, -0.7170, -0.2416]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 98
	action: tensor([[-0.2275, -1.1977,  0.4111, -0.6434, -1.0476, -0.0290, -0.2829]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1559786588411334, distance: 1.6802694143946624 entropy 1.0271300077438354
epoch: 22, step: 99
	action: tensor([[ 0.8992, -1.4585,  1.8687, -0.4677,  0.3855,  0.6936,  1.2009]],
       dtype=torch.float64)
	q_value: tensor([[-42.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 100
	action: tensor([[-0.3532, -0.8664,  0.5612, -0.9600, -0.8701, -1.2505,  0.4094]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6010076651380409, distance: 1.4479494452889736 entropy 1.0271300077438354
epoch: 22, step: 101
	action: tensor([[ 1.0432, -0.4846,  0.8996, -1.3039, -0.7839, -0.0718,  1.9265]],
       dtype=torch.float64)
	q_value: tensor([[-42.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11438169732136827, distance: 1.208018759518365 entropy 1.0271300077438354
epoch: 22, step: 102
	action: tensor([[ 5.8602, -6.2800,  4.7714, -2.8500,  0.4216,  1.1887,  5.1891]],
       dtype=torch.float64)
	q_value: tensor([[-61.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 103
	action: tensor([[ 0.8431,  0.3713, -0.1869,  1.2470,  0.3214,  1.7130,  0.4091]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 104
	action: tensor([[-0.5958,  0.8006, -0.6963, -0.1031,  0.5412,  0.1963,  0.6107]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 105
	action: tensor([[ 0.4657,  0.0700,  0.7384,  0.1159,  0.2357, -0.0626,  0.9331]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7528142543652531, distance: 0.568942536575443 entropy 1.0271300077438354
epoch: 22, step: 106
	action: tensor([[ 3.5662, -2.5530,  0.9739, -1.2031, -0.9392,  0.6844,  2.1416]],
       dtype=torch.float64)
	q_value: tensor([[-39.3445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 107
	action: tensor([[ 0.4672, -0.0643,  0.6888, -0.2283, -0.7000,  0.0177,  0.6833]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4747966080742586, distance: 0.8293167830718857 entropy 1.0271300077438354
epoch: 22, step: 108
	action: tensor([[ 2.3834, -2.2547,  1.9940, -1.5080,  0.6986,  0.8063,  3.2791]],
       dtype=torch.float64)
	q_value: tensor([[-39.6182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 109
	action: tensor([[-0.3613, -1.5706,  0.4658, -0.4907,  0.9331, -0.2448, -0.6823]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2349769587046846, distance: 1.710776273082617 entropy 1.0271300077438354
epoch: 22, step: 110
	action: tensor([[ 0.3605, -0.7769,  0.1400,  0.3518, -0.1207,  1.3449,  0.8018]],
       dtype=torch.float64)
	q_value: tensor([[-41.9203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5754711712365959, distance: 0.745607532836043 entropy 1.0271300077438354
epoch: 22, step: 111
	action: tensor([[ 2.8951, -4.1134,  2.5508, -3.5358, -1.7322,  1.2070,  4.3224]],
       dtype=torch.float64)
	q_value: tensor([[-50.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 112
	action: tensor([[ 1.1943, -0.4032, -1.1799,  0.3977, -0.6236,  0.6694,  1.4002]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 113
	action: tensor([[ 1.2470,  0.5677,  0.7379,  0.6330,  1.1893, -0.0417,  0.7088]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 114
	action: tensor([[-0.1073,  0.3608, -0.4138, -0.5001, -0.7481,  1.0146,  0.3671]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2748874938910659, distance: 0.9744496210064676 entropy 1.0271300077438354
epoch: 22, step: 115
	action: tensor([[ 3.7736, -0.7714,  1.8772, -0.3560, -1.0275,  1.2487,  1.4409]],
       dtype=torch.float64)
	q_value: tensor([[-41.9803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 116
	action: tensor([[-0.2996, -0.3372,  0.9965,  0.1377, -0.6031,  0.6790,  0.4936]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10622577956891022, distance: 1.0818588577785524 entropy 1.0271300077438354
epoch: 22, step: 117
	action: tensor([[ 2.1390, -3.8790,  2.7184, -0.4277, -0.6273, -0.2390,  1.6415]],
       dtype=torch.float64)
	q_value: tensor([[-41.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 118
	action: tensor([[ 0.3870, -0.9002,  1.1792, -1.2058,  0.7778,  1.4316,  0.9873]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43885729425333864, distance: 1.3726681430175274 entropy 1.0271300077438354
epoch: 22, step: 119
	action: tensor([[ 5.8672, -5.9976,  3.8104, -3.1711,  0.7089,  1.5245,  5.4785]],
       dtype=torch.float64)
	q_value: tensor([[-60.1197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 120
	action: tensor([[-0.4032, -1.3528, -0.6337, -0.0404,  0.1636, -0.1318,  0.5189]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.998972829202946, distance: 1.6179315309212974 entropy 1.0271300077438354
epoch: 22, step: 121
	action: tensor([[ 2.1099, -1.4687,  1.5303,  0.1461,  0.0057,  0.6976,  0.5382]],
       dtype=torch.float64)
	q_value: tensor([[-43.1115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 122
	action: tensor([[ 0.0235, -0.2992,  0.1663, -0.6368,  0.3745, -0.1916, -0.6104]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34691784247062296, distance: 1.3280892037781473 entropy 1.0271300077438354
epoch: 22, step: 123
	action: tensor([[ 0.9621, -0.6957,  0.1784, -0.5260, -0.1473, -0.5714,  0.8460]],
       dtype=torch.float64)
	q_value: tensor([[-32.3602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44290396925439435, distance: 1.3745970493076374 entropy 1.0271300077438354
epoch: 22, step: 124
	action: tensor([[ 3.3838, -2.2333,  1.8059, -1.1522, -1.3074,  0.5213,  3.0304]],
       dtype=torch.float64)
	q_value: tensor([[-43.7147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 125
	action: tensor([[ 1.0414, -0.3732,  0.3866, -0.7484,  0.3510, -0.5001,  0.3300]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06907837207868739, distance: 1.183209001828344 entropy 1.0271300077438354
epoch: 22, step: 126
	action: tensor([[ 2.1481, -2.1934,  0.7932, -0.9849,  0.4052,  0.1981,  2.6848]],
       dtype=torch.float64)
	q_value: tensor([[-39.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 22, step: 127
	action: tensor([[ 0.3420, -0.2526,  0.1032, -0.4161,  0.7096, -0.3712,  0.4804]],
       dtype=torch.float64)
	q_value: tensor([[-43.3453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008751161981404909, distance: 1.1393260801976293 entropy 1.0271300077438354
LOSS epoch 22 actor 408.78471007163864 critic 61.141622786435285 
epoch: 23, step: 0
	action: tensor([[ 0.3793, -0.6108,  0.8722, -1.0570, -0.7835,  0.3448,  0.9093]],
       dtype=torch.float64)
	q_value: tensor([[-38.4502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4401037271296937, distance: 1.3732625620530676 entropy 1.0271300077438354
epoch: 23, step: 1
	action: tensor([[ 3.3744, -2.7662,  1.3024, -2.1501, -1.2429,  0.9971,  1.4515]],
       dtype=torch.float64)
	q_value: tensor([[-53.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 2
	action: tensor([[-0.5468,  0.2014, -1.9694, -1.2146,  0.6369, -0.1348,  0.4557]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 3
	action: tensor([[ 0.2603, -0.1024,  0.6813, -0.6257, -0.4095,  0.9888,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3927892225563657, distance: 0.891715732435118 entropy 1.0271300077438354
epoch: 23, step: 4
	action: tensor([[ 2.5226, -1.6031,  1.1124, -0.2698, -0.4342,  0.8704,  1.3537]],
       dtype=torch.float64)
	q_value: tensor([[-45.7293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 5
	action: tensor([[-1.2493, -0.6860,  0.1165, -0.7021,  0.7804, -0.5773,  0.4385]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3105535468724194, distance: 1.7394610646200934 entropy 1.0271300077438354
epoch: 23, step: 6
	action: tensor([[-0.0442, -0.4043,  0.0831, -0.8348, -0.7240,  0.1515,  0.7004]],
       dtype=torch.float64)
	q_value: tensor([[-42.7042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4936748659728949, distance: 1.39857168001028 entropy 1.0271300077438354
epoch: 23, step: 7
	action: tensor([[ 2.8606, -1.8872,  0.6018, -0.3932, -0.2866, -0.4844,  1.0743]],
       dtype=torch.float64)
	q_value: tensor([[-46.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 8
	action: tensor([[-0.0269,  0.0284,  0.5000, -0.4353,  0.4420,  1.6975, -0.0885]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6348409510546872, distance: 0.6915088249846657 entropy 1.0271300077438354
epoch: 23, step: 9
	action: tensor([[ 1.2824, -0.2399,  1.9055, -1.4769, -0.2005,  0.5353,  1.5509]],
       dtype=torch.float64)
	q_value: tensor([[-52.2755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8395708530267518, distance: 0.4583511553700463 entropy 1.0271300077438354
epoch: 23, step: 10
	action: tensor([[ 5.7216, -2.4731,  2.6158, -2.7974,  0.1564,  0.0387,  4.8424]],
       dtype=torch.float64)
	q_value: tensor([[-68.0510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 11
	action: tensor([[-0.3109, -0.1727,  0.6032, -0.5477, -0.9963,  1.1259,  0.5252]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29314959926230877, distance: 1.3013109315418196 entropy 1.0271300077438354
epoch: 23, step: 12
	action: tensor([[ 1.6083, -2.0840,  1.3423, -1.4903,  0.8406,  0.6008,  1.2483]],
       dtype=torch.float64)
	q_value: tensor([[-51.6812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 13
	action: tensor([[-0.4024,  0.3142, -0.0805, -0.2872, -1.3713,  0.2851, -0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11551743224698763, distance: 1.2086341858609226 entropy 1.0271300077438354
epoch: 23, step: 14
	action: tensor([[ 0.6526, -0.0964,  0.7368,  0.6242, -1.5292, -0.5586,  0.6238]],
       dtype=torch.float64)
	q_value: tensor([[-40.7229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8021956732185475, distance: 0.508949376714494 entropy 1.0271300077438354
epoch: 23, step: 15
	action: tensor([[ 1.9659, -1.3208,  1.5066, -0.5695, -0.8479,  0.0893,  1.6285]],
       dtype=torch.float64)
	q_value: tensor([[-50.1823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 16
	action: tensor([[ 0.5045, -1.1174,  0.4712, -0.7078,  0.5399,  0.1270, -0.3275]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9379544104658717, distance: 1.593046568341258 entropy 1.0271300077438354
epoch: 23, step: 17
	action: tensor([[ 0.5298, -0.3200,  1.3151,  0.3646,  0.8109,  0.0855, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[-44.7198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49752630936746134, distance: 0.8111727575051181 entropy 1.0271300077438354
epoch: 23, step: 18
	action: tensor([[ 2.4723, -2.7565,  0.6022, -0.7820,  0.5802, -0.0590,  1.1001]],
       dtype=torch.float64)
	q_value: tensor([[-45.0585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 19
	action: tensor([[ 0.1165, -0.3494,  0.7521, -0.5402, -0.2464, -0.5272,  1.0679]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4051905304981436, distance: 1.35651406310543 entropy 1.0271300077438354
epoch: 23, step: 20
	action: tensor([[ 1.8011, -1.8000,  1.8049, -0.6133,  0.6059,  1.2374,  1.5931]],
       dtype=torch.float64)
	q_value: tensor([[-44.1765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 21
	action: tensor([[ 0.8188,  1.0418, -0.4193, -0.4130,  0.1464,  0.2968,  0.3742]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 22
	action: tensor([[ 0.6952, -0.6582,  0.2298, -0.3282,  0.2910, -0.8243,  0.7061]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3251711916153919, distance: 1.3173242489771062 entropy 1.0271300077438354
epoch: 23, step: 23
	action: tensor([[ 1.5110,  0.2980,  1.6605, -1.6986,  0.1723,  1.2065,  2.7198]],
       dtype=torch.float64)
	q_value: tensor([[-42.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9222158983323089, distance: 0.31915533865388934 entropy 1.0271300077438354
epoch: 23, step: 24
	action: tensor([[ 6.1171, -5.6512,  3.5578, -2.7516,  1.1592,  1.9735,  6.0534]],
       dtype=torch.float64)
	q_value: tensor([[-88.5405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 25
	action: tensor([[ 1.0183, -0.6070, -0.5527,  0.2820, -0.2345, -0.7848,  1.6811]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1428494581904387, distance: 1.0594616606528446 entropy 1.0271300077438354
epoch: 23, step: 26
	action: tensor([[ 2.4756, -0.2437,  1.6610, -0.4278, -1.9226,  0.4586,  1.6054]],
       dtype=torch.float64)
	q_value: tensor([[-58.5323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 27
	action: tensor([[ 0.4617,  0.7060,  1.6661,  0.5924, -0.5625,  0.3997,  0.0232]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 28
	action: tensor([[ 0.1083, -0.5686,  0.9291,  0.8460, -0.7259, -0.3183, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43178419996589223, distance: 0.8626077309988993 entropy 1.0271300077438354
epoch: 23, step: 29
	action: tensor([[ 1.5595, -0.9372,  0.3824, -0.4401,  0.3623, -0.1744,  0.4914]],
       dtype=torch.float64)
	q_value: tensor([[-42.5534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5922480452622505, distance: 1.4439829173693743 entropy 1.0271300077438354
epoch: 23, step: 30
	action: tensor([[ 2.6936, -2.0102,  0.8875, -2.3031,  1.7006, -0.0646,  1.9826]],
       dtype=torch.float64)
	q_value: tensor([[-50.8758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 31
	action: tensor([[-0.5358, -0.2451,  1.1609, -2.2032, -0.4991,  0.2969,  0.8199]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6683247422582492, distance: 1.4780767601313602 entropy 1.0271300077438354
epoch: 23, step: 32
	action: tensor([[ 3.4176, -2.8577,  1.3224, -2.2714, -0.9251,  1.6580,  2.8811]],
       dtype=torch.float64)
	q_value: tensor([[-59.2238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 33
	action: tensor([[ 1.0716,  0.2584, -0.6803, -0.2047,  0.1534,  1.8168, -0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 34
	action: tensor([[ 0.6906, -0.3648, -0.2896,  0.1346, -0.1982,  0.4743,  0.3169]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5902860825536571, distance: 0.7324821598353661 entropy 1.0271300077438354
epoch: 23, step: 35
	action: tensor([[ 1.8415, -1.1784,  2.0080, -0.1125, -0.1032, -0.2168,  1.6686]],
       dtype=torch.float64)
	q_value: tensor([[-41.8709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 36
	action: tensor([[-0.1863, -0.2733,  0.1626, -0.2586,  0.2942, -1.0829,  0.1726]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4404776807176036, distance: 1.3734408489081074 entropy 1.0271300077438354
epoch: 23, step: 37
	action: tensor([[-0.0672, -0.6930,  0.0564,  0.7399, -0.2735,  0.6267, -0.4957]],
       dtype=torch.float64)
	q_value: tensor([[-34.7681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2423786187799435, distance: 0.9960538181381973 entropy 1.0271300077438354
epoch: 23, step: 38
	action: tensor([[ 0.2526,  0.1658,  0.7471, -1.2000, -0.1905, -0.5915,  1.8561]],
       dtype=torch.float64)
	q_value: tensor([[-40.6688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15584674907947305, distance: 1.230288094032758 entropy 1.0271300077438354
epoch: 23, step: 39
	action: tensor([[ 1.7356, -2.6397,  2.5263, -0.7817, -0.7948,  1.3299,  2.6535]],
       dtype=torch.float64)
	q_value: tensor([[-56.2297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 40
	action: tensor([[ 0.9716, -0.1357,  0.3767,  1.0401, -0.9929, -0.2795,  0.3489]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 41
	action: tensor([[ 1.2681, -1.6468, -0.0413, -1.1172,  0.2317,  0.0478, -0.2849]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0393880919641583, distance: 1.634205368389524 entropy 1.0271300077438354
epoch: 23, step: 42
	action: tensor([[ 1.1010, -1.4152,  1.3490, -0.4644,  0.3454,  1.0424,  1.3162]],
       dtype=torch.float64)
	q_value: tensor([[-52.4355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 43
	action: tensor([[ 0.6665, -0.9082,  0.4728,  0.4033, -0.2746, -1.1794, -0.2485]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23211249913224608, distance: 1.2702285648561669 entropy 1.0271300077438354
epoch: 23, step: 44
	action: tensor([[ 0.7207, -2.0219,  1.4135,  0.1753, -0.0298,  0.1857,  1.0267]],
       dtype=torch.float64)
	q_value: tensor([[-41.3703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 45
	action: tensor([[ 0.5692, -1.1991, -0.6093, -0.3079, -0.7260,  0.6899, -0.2461]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6735973961641191, distance: 1.480410610090966 entropy 1.0271300077438354
epoch: 23, step: 46
	action: tensor([[ 1.7748,  0.0637,  1.7681, -1.3996, -0.5522, -0.2127,  0.8387]],
       dtype=torch.float64)
	q_value: tensor([[-50.3498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8763858715248328, distance: 0.4023377185296114 entropy 1.0271300077438354
epoch: 23, step: 47
	action: tensor([[ 2.9259, -3.0864,  3.3878, -1.0609, -1.2890,  2.3286,  3.3187]],
       dtype=torch.float64)
	q_value: tensor([[-59.3876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 48
	action: tensor([[ 0.7539,  0.5608, -0.2278, -0.9198,  0.0652, -0.1434, -0.2226]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 49
	action: tensor([[-0.2939, -0.6607,  0.1671,  0.7144,  0.1633, -0.8148,  0.0796]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30137176184433345, distance: 1.3054414034746546 entropy 1.0271300077438354
epoch: 23, step: 50
	action: tensor([[ 0.4266, -1.2877,  0.6068, -0.5533,  0.3286, -0.5526,  0.9141]],
       dtype=torch.float64)
	q_value: tensor([[-36.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9942019356081966, distance: 1.615999641143195 entropy 1.0271300077438354
epoch: 23, step: 51
	action: tensor([[ 1.4831, -1.8835,  1.9287, -0.9162,  0.7075,  1.9762,  1.8666]],
       dtype=torch.float64)
	q_value: tensor([[-48.5721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 52
	action: tensor([[ 0.1575, -1.3422, -0.2644, -1.2346, -0.5431,  1.1668,  0.1959]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8201021090911575, distance: 1.5438481073954458 entropy 1.0271300077438354
epoch: 23, step: 53
	action: tensor([[ 3.4157, -2.4800,  1.5757, -1.7285,  0.1038,  0.6700,  0.8538]],
       dtype=torch.float64)
	q_value: tensor([[-58.9168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 54
	action: tensor([[ 0.3511,  0.4184,  0.2537, -0.6463,  0.0203,  1.0263,  0.4876]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 55
	action: tensor([[-0.5536, -1.2159, -0.4574,  0.3650,  0.7454,  0.4014, -0.5176]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9328778470953685, distance: 1.590958669636591 entropy 1.0271300077438354
epoch: 23, step: 56
	action: tensor([[ 0.2627, -0.2186,  0.4652, -0.5853,  0.6821,  0.9192,  1.8995]],
       dtype=torch.float64)
	q_value: tensor([[-44.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2970083085538583, distance: 0.959470860021384 entropy 1.0271300077438354
epoch: 23, step: 57
	action: tensor([[ 3.7711, -3.5486,  3.0260, -3.0758,  0.3251, -0.0362,  3.9397]],
       dtype=torch.float64)
	q_value: tensor([[-62.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 58
	action: tensor([[ 0.2598, -1.1693, -0.2830, -0.4377, -0.2801,  0.8383,  0.5285]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6516389342709143, distance: 1.4706666674102606 entropy 1.0271300077438354
epoch: 23, step: 59
	action: tensor([[ 2.9024, -1.7459,  0.6126, -1.1512, -1.6561,  0.1572,  1.0402]],
       dtype=torch.float64)
	q_value: tensor([[-52.3348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 60
	action: tensor([[ 0.0592,  0.3921,  0.2764, -0.1493, -0.8755, -0.4177,  1.0908]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 61
	action: tensor([[ 0.4365, -1.1092, -0.5664, -0.5363,  0.5359, -0.0813,  0.2939]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7286388723325818, distance: 1.504557641281955 entropy 1.0271300077438354
epoch: 23, step: 62
	action: tensor([[ 0.3915,  1.0574, -0.2329, -0.6084, -0.1723, -0.0451,  0.2588]],
       dtype=torch.float64)
	q_value: tensor([[-47.3161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 63
	action: tensor([[ 1.6535e+00,  1.9199e-01,  4.2510e-01, -4.9671e-01, -4.1043e-04,
         -8.8536e-01,  7.8512e-02]], dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 64
	action: tensor([[ 0.0863, -0.5187, -0.5235, -0.4847,  1.3725, -0.6207,  1.1637]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3601655835627122, distance: 1.334604497622048 entropy 1.0271300077438354
epoch: 23, step: 65
	action: tensor([[ 0.2288, -1.2019,  0.9901, -0.6717, -0.9092,  0.3295,  0.9143]],
       dtype=torch.float64)
	q_value: tensor([[-49.5657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9632864211423364, distance: 1.6034245348842286 entropy 1.0271300077438354
epoch: 23, step: 66
	action: tensor([[ 3.1654, -1.5113,  1.5935, -1.6043, -0.2455,  0.6932,  2.4679]],
       dtype=torch.float64)
	q_value: tensor([[-54.8954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 67
	action: tensor([[ 0.3565, -0.1498, -0.1004,  0.6009, -0.2558, -0.7357, -0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6520384873188145, distance: 0.6750287938153193 entropy 1.0271300077438354
epoch: 23, step: 68
	action: tensor([[ 1.4938, -0.7150,  0.4349, -0.3054, -0.0162, -0.2251,  0.6334]],
       dtype=torch.float64)
	q_value: tensor([[-34.3018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3356612289876506, distance: 1.3225279317457537 entropy 1.0271300077438354
epoch: 23, step: 69
	action: tensor([[ 1.3433, -2.5902,  1.2093, -1.0953, -1.5772,  0.5374,  1.7038]],
       dtype=torch.float64)
	q_value: tensor([[-49.6628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 70
	action: tensor([[ 0.8628, -1.1016, -0.2921, -0.1448,  0.6407,  0.1459, -0.4708]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5402466859818118, distance: 1.4202076078933163 entropy 1.0271300077438354
epoch: 23, step: 71
	action: tensor([[ 1.9795, -1.3587,  0.4795, -0.3163,  1.5702,  0.5340,  2.0069]],
       dtype=torch.float64)
	q_value: tensor([[-45.6389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 72
	action: tensor([[-0.7783, -0.5756,  0.4046, -0.1433,  0.2409,  0.0783,  0.7562]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.097709339049477, distance: 1.657407688387014 entropy 1.0271300077438354
epoch: 23, step: 73
	action: tensor([[ 0.5509, -1.3275,  1.0052, -1.3851, -0.2581,  1.5900,  0.3949]],
       dtype=torch.float64)
	q_value: tensor([[-40.6540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39484494471924747, distance: 1.3515112329883026 entropy 1.0271300077438354
epoch: 23, step: 74
	action: tensor([[ 2.8385, -3.5608,  1.3238, -1.5128, -0.3945,  0.5188,  2.2796]],
       dtype=torch.float64)
	q_value: tensor([[-65.2439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 75
	action: tensor([[ 1.4687, -0.9087,  0.7314, -0.1537, -0.8028, -0.9424,  0.2572]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5573435144767815, distance: 1.4280680490971924 entropy 1.0271300077438354
epoch: 23, step: 76
	action: tensor([[ 1.8548, -0.8333,  2.1718, -0.5456,  0.5994,  0.8963,  0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-50.0551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 77
	action: tensor([[ 0.6355, -0.3473,  1.1302,  0.4187,  0.3369, -0.3950,  0.3753]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5602305572368979, distance: 0.7588732037364296 entropy 1.0271300077438354
epoch: 23, step: 78
	action: tensor([[ 0.8098, -0.8884,  0.8186, -0.2621, -1.0683,  0.9466, -0.4966]],
       dtype=torch.float64)
	q_value: tensor([[-42.2276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09475336098668674, distance: 1.0887800480380792 entropy 1.0271300077438354
epoch: 23, step: 79
	action: tensor([[ 1.7706, -0.5987,  1.4593, -0.4335,  0.0965,  0.4479,  1.3330]],
       dtype=torch.float64)
	q_value: tensor([[-51.1947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07014736048517745, distance: 1.1838004087419285 entropy 1.0271300077438354
epoch: 23, step: 80
	action: tensor([[ 4.8499, -3.8493,  2.6368, -1.8632,  0.4627,  0.2238,  3.4267]],
       dtype=torch.float64)
	q_value: tensor([[-61.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 81
	action: tensor([[ 0.5064, -0.9630, -1.4568, -0.6527, -0.7778,  0.2722,  1.4288]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25449129476283194, distance: 1.2817122032016368 entropy 1.0271300077438354
epoch: 23, step: 82
	action: tensor([[ 3.0614, -4.1382,  1.3259, -0.8028,  0.9326, -0.0137,  3.9210]],
       dtype=torch.float64)
	q_value: tensor([[-63.0466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 83
	action: tensor([[ 0.8929, -0.4630, -0.1548,  0.5449, -0.2435,  0.8085,  0.5754]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8511300798601094, distance: 0.4415299529009689 entropy 1.0271300077438354
epoch: 23, step: 84
	action: tensor([[ 2.4654, -0.4620,  0.9229,  0.1158,  0.8309, -1.1144,  2.0966]],
       dtype=torch.float64)
	q_value: tensor([[-48.7181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 85
	action: tensor([[ 0.2277, -0.6922,  0.5134,  0.1404, -0.0163, -0.3869,  0.6806]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1796934489625981, distance: 1.2429145645181803 entropy 1.0271300077438354
epoch: 23, step: 86
	action: tensor([[ 1.8712, -1.7645, -0.1824, -0.6280,  0.3257,  0.3793,  1.5581]],
       dtype=torch.float64)
	q_value: tensor([[-40.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 87
	action: tensor([[-0.0492, -0.9567,  0.3646, -0.1455, -0.9723, -0.0107, -0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6905746230099097, distance: 1.4879004189129665 entropy 1.0271300077438354
epoch: 23, step: 88
	action: tensor([[ 0.9072, -1.7928,  0.4184, -0.1892, -0.0216, -0.9334,  0.1696]],
       dtype=torch.float64)
	q_value: tensor([[-41.5816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 89
	action: tensor([[ 0.7882, -0.4176, -0.1663, -0.7589, -0.1639, -0.0468, -1.0043]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1321577516578949, distance: 1.2176154931128056 entropy 1.0271300077438354
epoch: 23, step: 90
	action: tensor([[-0.7351, -1.6944,  1.2658, -1.3817,  0.0494,  1.1926,  0.1345]],
       dtype=torch.float64)
	q_value: tensor([[-38.2322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 91
	action: tensor([[ 0.4340,  0.4426,  0.0616,  0.1706,  0.7476, -0.7699, -0.6733]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 92
	action: tensor([[0.8197, 0.0611, 1.0353, 0.2478, 0.7340, 0.4697, 0.2289]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 93
	action: tensor([[ 0.9617, -0.3608, -1.2532, -0.2238, -0.2095, -0.0532,  1.0548]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 94
	action: tensor([[ 0.5546,  0.8209,  0.2984, -0.7783,  0.1029, -0.0996,  0.5681]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 95
	action: tensor([[-0.4702, -0.4773,  0.0876,  0.4888, -0.8315,  0.1308,  0.9502]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3223509495442256, distance: 1.3159217313005591 entropy 1.0271300077438354
epoch: 23, step: 96
	action: tensor([[ 1.7477, -1.6800,  0.3425, -0.5384, -0.5663,  0.9876,  0.1496]],
       dtype=torch.float64)
	q_value: tensor([[-44.6906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 97
	action: tensor([[ 0.4627, -0.7801, -0.2151, -0.4424,  0.2418,  0.4580, -1.0628]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21126271564930765, distance: 1.2594353178594075 entropy 1.0271300077438354
epoch: 23, step: 98
	action: tensor([[ 1.5602, -0.2641,  1.7834, -0.3038, -0.4542,  0.5978,  0.8074]],
       dtype=torch.float64)
	q_value: tensor([[-41.6142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967405860162903, distance: 0.9596535417833509 entropy 1.0271300077438354
epoch: 23, step: 99
	action: tensor([[ 3.8014, -1.3250,  1.9667, -0.9425,  0.1803,  0.0943,  1.7341]],
       dtype=torch.float64)
	q_value: tensor([[-58.3670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 100
	action: tensor([[-0.4720, -1.0450,  0.4215,  0.6586, -0.6112,  0.3253,  0.4243]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4209440838360379, distance: 1.3640967912823978 entropy 1.0271300077438354
epoch: 23, step: 101
	action: tensor([[ 1.3554, -2.3236,  0.6647, -0.1421, -0.3072, -0.2520,  1.0260]],
       dtype=torch.float64)
	q_value: tensor([[-46.1234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 102
	action: tensor([[ 1.0132, -1.2739,  0.0535,  0.6459, -0.5575,  1.8773,  0.4891]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5241170325338054, distance: 0.789417543779225 entropy 1.0271300077438354
epoch: 23, step: 103
	action: tensor([[ 3.4766, -2.8410,  2.4218, -0.9813, -0.0705,  0.8802,  2.4298]],
       dtype=torch.float64)
	q_value: tensor([[-65.7128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 104
	action: tensor([[-0.3977, -0.6384, -1.1721, -0.7157, -0.2645,  0.0167,  0.1897]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18830004020656288, distance: 1.2474402392573258 entropy 1.0271300077438354
epoch: 23, step: 105
	action: tensor([[ 0.6176,  0.4781,  0.5214,  0.4596, -0.1542,  0.2647, -0.1388]],
       dtype=torch.float64)
	q_value: tensor([[-43.6020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 106
	action: tensor([[-0.7027, -1.1046,  0.3802, -0.2384,  0.6561, -0.8747, -0.1716]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2347301617318842, distance: 1.7106818143768854 entropy 1.0271300077438354
epoch: 23, step: 107
	action: tensor([[-0.0994, -0.5944, -0.4315,  0.3229,  1.1545, -0.6240, -0.5965]],
       dtype=torch.float64)
	q_value: tensor([[-41.1675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.418980945363024, distance: 1.363154165708874 entropy 1.0271300077438354
epoch: 23, step: 108
	action: tensor([[-0.6118, -0.8388,  0.5331,  1.4426, -0.0093,  0.3449, -0.2452]],
       dtype=torch.float64)
	q_value: tensor([[-39.6747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36538376627215396, distance: 0.9116167198717564 entropy 1.0271300077438354
epoch: 23, step: 109
	action: tensor([[ 1.4972, -0.6082,  1.1738, -1.3587,  0.7571,  0.7354,  1.4718]],
       dtype=torch.float64)
	q_value: tensor([[-46.5062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006748715188992671, distance: 1.1481991876982776 entropy 1.0271300077438354
epoch: 23, step: 110
	action: tensor([[ 4.7905, -3.4411,  2.3613, -2.2478,  1.8371,  1.7482,  4.3741]],
       dtype=torch.float64)
	q_value: tensor([[-67.6360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 111
	action: tensor([[ 1.0014,  0.9795, -0.1215, -0.0969, -0.5245,  0.3548, -0.2015]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 112
	action: tensor([[ 0.1973, -1.4854, -0.2643, -0.6057,  0.3418,  0.4871, -0.5924]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7688243296459423, distance: 1.5219453037369337 entropy 1.0271300077438354
epoch: 23, step: 113
	action: tensor([[ 0.9423, -0.7992,  1.6390, -0.3032,  0.5510,  0.1739,  0.4941]],
       dtype=torch.float64)
	q_value: tensor([[-46.4112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11783643082788853, distance: 1.2098898210167064 entropy 1.0271300077438354
epoch: 23, step: 114
	action: tensor([[ 0.5687, -2.2046, -0.3381, -0.1062, -0.1733,  1.2617,  2.6885]],
       dtype=torch.float64)
	q_value: tensor([[-51.4651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 115
	action: tensor([[ 0.2699, -0.9304, -0.8525, -0.1336,  0.4564,  0.3492, -0.8024]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29709591604460495, distance: 1.3032950303700106 entropy 1.0271300077438354
epoch: 23, step: 116
	action: tensor([[ 0.1825, -1.0855,  1.4192, -0.1747,  0.5081,  0.0644, -0.1042]],
       dtype=torch.float64)
	q_value: tensor([[-43.0695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7179060987769179, distance: 1.4998796197805024 entropy 1.0271300077438354
epoch: 23, step: 117
	action: tensor([[ 1.2329, -1.3807, -0.2306, -0.3558, -0.6659, -0.1357,  1.8592]],
       dtype=torch.float64)
	q_value: tensor([[-45.8089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 118
	action: tensor([[-1.0961, -0.5756,  0.5734,  0.0662,  0.3368, -1.0202, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3903325900946868, distance: 1.7692363745156674 entropy 1.0271300077438354
epoch: 23, step: 119
	action: tensor([[ 0.8457, -0.4156, -0.4367, -0.3531,  0.3285,  1.5104, -0.2468]],
       dtype=torch.float64)
	q_value: tensor([[-38.4704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6655734277743577, distance: 0.6617700100527301 entropy 1.0271300077438354
epoch: 23, step: 120
	action: tensor([[ 2.7270, -2.0839,  0.2680, -0.5806, -0.1090,  0.7498,  0.9329]],
       dtype=torch.float64)
	q_value: tensor([[-53.2943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 121
	action: tensor([[ 0.2953, -0.6711, -0.1866, -1.8037, -0.4408, -0.7780, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14942523588582057, distance: 1.2268657914138865 entropy 1.0271300077438354
epoch: 23, step: 122
	action: tensor([[ 0.8970, -1.0748,  0.5022, -0.4886,  0.3120,  0.5931,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-48.5429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5930658314278179, distance: 1.4443536880136099 entropy 1.0271300077438354
epoch: 23, step: 123
	action: tensor([[ 0.9500, -1.3028, -0.2655, -0.2015,  0.3463, -0.9047,  0.8993]],
       dtype=torch.float64)
	q_value: tensor([[-49.1897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 124
	action: tensor([[ 0.1873, -0.2755,  0.0711, -0.5338, -0.3656,  1.3516,  0.4348]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3119163696707289, distance: 0.9492427881254776 entropy 1.0271300077438354
epoch: 23, step: 125
	action: tensor([[ 1.6080, -1.4056,  2.5849, -1.1822,  0.6119,  0.7642,  2.3602]],
       dtype=torch.float64)
	q_value: tensor([[-51.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08240962491021298, distance: 1.0961780917483601 entropy 1.0271300077438354
epoch: 23, step: 126
	action: tensor([[ 6.1800, -6.2800,  3.7752, -3.1828,  0.0437,  2.5335,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-83.4696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 23, step: 127
	action: tensor([[-0.2095,  0.6055,  0.8330,  0.7028, -0.6047,  0.8045,  0.7272]],
       dtype=torch.float64)
	q_value: tensor([[-45.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
LOSS epoch 23 actor 461.001507698874 critic 61.402772731498544 
epoch: 24, step: 0
	action: tensor([[ 0.8162,  0.7009, -1.1487, -1.8956, -0.4052,  0.6266, -0.2351]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 1
	action: tensor([[ 0.3945, -0.2718,  0.5060, -0.9017,  0.3381,  1.1144, -0.3430]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2100882205490242, distance: 1.0170586279598284 entropy 1.0271300077438354
epoch: 24, step: 2
	action: tensor([[ 0.0013, -0.7211,  0.0664, -0.3028,  0.4038, -0.0330,  1.1149]],
       dtype=torch.float64)
	q_value: tensor([[-41.5154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5319966910621932, distance: 1.4163989851220304 entropy 1.0271300077438354
epoch: 24, step: 3
	action: tensor([[ 0.7980, -2.7554,  1.5967, -1.9220, -0.1124, -0.3102,  1.8265]],
       dtype=torch.float64)
	q_value: tensor([[-41.5333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 4
	action: tensor([[-0.8522, -0.1196, -0.4405, -0.0888,  0.4903,  0.8319, -0.4677]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5550555547582834, distance: 1.4270186457129006 entropy 1.0271300077438354
epoch: 24, step: 5
	action: tensor([[ 0.9057, -0.8185, -0.3905, -0.3830,  0.3028,  0.3859,  1.1426]],
       dtype=torch.float64)
	q_value: tensor([[-34.7155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21651417509982718, distance: 1.262162521570818 entropy 1.0271300077438354
epoch: 24, step: 6
	action: tensor([[ 1.7417, -1.8954,  0.7646, -2.0609, -0.2033,  0.8096,  1.8242]],
       dtype=torch.float64)
	q_value: tensor([[-50.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 7
	action: tensor([[ 1.6292, -0.5297,  0.6515, -1.3749, -0.7182,  0.2282, -1.0692]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07362790126432595, distance: 1.185723938624035 entropy 1.0271300077438354
epoch: 24, step: 8
	action: tensor([[ 0.5378, -1.0406,  1.4462,  0.3349, -1.3658,  0.5772,  1.1515]],
       dtype=torch.float64)
	q_value: tensor([[-47.4572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01453149433301748, distance: 1.1526287819283736 entropy 1.0271300077438354
epoch: 24, step: 9
	action: tensor([[ 1.7785, -2.4287,  1.3004, -1.5448,  0.4412, -0.3592,  1.2342]],
       dtype=torch.float64)
	q_value: tensor([[-54.9302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 10
	action: tensor([[ 0.5075, -0.3114, -0.4424,  0.2031,  0.0558, -1.0902,  1.8989]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3249625013725489, distance: 0.9402008568473297 entropy 1.0271300077438354
epoch: 24, step: 11
	action: tensor([[ 1.2975, -0.9106,  0.1191, -0.7112, -0.1887, -0.1706,  0.8211]],
       dtype=torch.float64)
	q_value: tensor([[-50.6864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7239578385352463, distance: 1.5025191416106327 entropy 1.0271300077438354
epoch: 24, step: 12
	action: tensor([[ 2.0109, -0.8458,  1.2507, -1.3215,  0.2066,  1.1701,  1.4281]],
       dtype=torch.float64)
	q_value: tensor([[-48.0614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 13
	action: tensor([[-0.0956,  0.1205,  0.7922,  0.9230, -0.1113,  0.7704,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 14
	action: tensor([[-0.5162, -0.0842,  0.2540, -0.0627,  0.3420, -0.3592,  0.1317]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.531597548969011, distance: 1.4162144608057865 entropy 1.0271300077438354
epoch: 24, step: 15
	action: tensor([[ 0.2348, -0.1697,  1.1974,  0.0497,  0.7112, -1.5677,  0.2907]],
       dtype=torch.float64)
	q_value: tensor([[-29.6869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35349304582055485, distance: 0.920117506106316 entropy 1.0271300077438354
epoch: 24, step: 16
	action: tensor([[ 8.7454e-02, -1.0172e+00, -3.9035e-04,  1.3703e-01, -9.1953e-01,
         -2.9544e-01,  1.0184e+00]], dtype=torch.float64)
	q_value: tensor([[-39.3187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5056740197591927, distance: 1.4041780233466736 entropy 1.0271300077438354
epoch: 24, step: 17
	action: tensor([[ 0.5172, -1.7487,  0.6736, -1.1587,  0.2229,  0.4069,  1.4955]],
       dtype=torch.float64)
	q_value: tensor([[-44.6726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 18
	action: tensor([[ 0.1462,  0.2293, -0.0038,  0.4316,  0.7411,  0.4537,  0.6403]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 19
	action: tensor([[-0.0291, -1.1255,  0.3781, -0.5304, -0.1255,  0.2178,  0.3965]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9973444488230983, distance: 1.617272406223705 entropy 1.0271300077438354
epoch: 24, step: 20
	action: tensor([[ 0.9813,  0.1052,  1.0138, -0.1050, -0.6950, -0.7044,  1.7520]],
       dtype=torch.float64)
	q_value: tensor([[-40.7544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6549733548643866, distance: 0.6721760138997498 entropy 1.0271300077438354
epoch: 24, step: 21
	action: tensor([[ 1.3412, -1.9093,  1.7794, -0.1054, -0.4876,  0.6521,  1.0800]],
       dtype=torch.float64)
	q_value: tensor([[-50.7263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 22
	action: tensor([[ 0.7406, -0.4955,  1.0748, -0.3799,  0.2567,  0.2941,  0.0574]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17504109467371798, distance: 1.0393763982165947 entropy 1.0271300077438354
epoch: 24, step: 23
	action: tensor([[ 0.7024, -1.0695,  0.0193, -0.4368,  0.5954,  0.6462,  1.3996]],
       dtype=torch.float64)
	q_value: tensor([[-39.0072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4643152963873167, distance: 1.3847583506982992 entropy 1.0271300077438354
epoch: 24, step: 24
	action: tensor([[ 1.6429, -1.1182,  1.4025, -0.6035,  0.2050,  0.1920,  2.6369]],
       dtype=torch.float64)
	q_value: tensor([[-54.1005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44683825026515445, distance: 1.376469789887615 entropy 1.0271300077438354
epoch: 24, step: 25
	action: tensor([[ 5.3150, -3.2563,  1.2792, -1.0766,  0.1914,  0.7702,  3.6838]],
       dtype=torch.float64)
	q_value: tensor([[-69.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 26
	action: tensor([[0.5757, 0.1435, 0.5802, 0.1203, 0.1779, 0.1671, 0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8811328594871226, distance: 0.3945368755530935 entropy 1.0271300077438354
epoch: 24, step: 27
	action: tensor([[ 1.4226,  0.4235,  0.5663, -0.8943,  0.2671, -0.1198, -1.1829]],
       dtype=torch.float64)
	q_value: tensor([[-33.5895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6480380998397697, distance: 0.6788979856940484 entropy 1.0271300077438354
epoch: 24, step: 28
	action: tensor([[ 0.7709, -0.0151, -0.3940,  0.3383, -0.1928,  0.5904, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-39.3833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9197969783851867, distance: 0.32407987188929926 entropy 1.0271300077438354
epoch: 24, step: 29
	action: tensor([[ 0.9407, -0.6442,  0.0068, -0.4707,  0.1833,  0.8610, -0.3002]],
       dtype=torch.float64)
	q_value: tensor([[-36.3781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09270297195569421, distance: 1.0900123972611704 entropy 1.0271300077438354
epoch: 24, step: 30
	action: tensor([[ 0.2992, -0.6018, -0.2817, -1.0268,  0.9503,  0.0414,  1.1526]],
       dtype=torch.float64)
	q_value: tensor([[-42.4140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3974855342290964, distance: 1.3527899052062415 entropy 1.0271300077438354
epoch: 24, step: 31
	action: tensor([[ 1.9119, -0.6471,  0.1708, -0.5658,  0.0048,  0.3499,  1.3216]],
       dtype=torch.float64)
	q_value: tensor([[-47.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41347845241999903, distance: 1.3605085939372665 entropy 1.0271300077438354
epoch: 24, step: 32
	action: tensor([[ 2.9253, -2.6166,  1.5181, -1.3979,  0.1935, -0.2122,  2.2152]],
       dtype=torch.float64)
	q_value: tensor([[-54.7752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 33
	action: tensor([[ 0.3968, -1.9417,  0.3635, -0.6291,  0.0272, -0.4692, -0.5646]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 34
	action: tensor([[-0.4574, -0.7813, -0.4757,  0.9466,  1.1290, -0.5367,  0.2698]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7596780025337988, distance: 1.518005327655391 entropy 1.0271300077438354
epoch: 24, step: 35
	action: tensor([[ 1.5766, -1.0255,  0.4712,  0.2933,  0.1582,  0.1269, -0.6124]],
       dtype=torch.float64)
	q_value: tensor([[-39.7950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43236563088628355, distance: 1.369568122934524 entropy 1.0271300077438354
epoch: 24, step: 36
	action: tensor([[ 0.1764,  0.0078, -0.9907, -0.0733,  0.0601,  0.9289,  0.6145]],
       dtype=torch.float64)
	q_value: tensor([[-45.4042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48482758476806853, distance: 0.8213589496993148 entropy 1.0271300077438354
epoch: 24, step: 37
	action: tensor([[ 1.5863, -1.6554,  1.2124, -1.2145,  0.3944,  1.4806,  0.5592]],
       dtype=torch.float64)
	q_value: tensor([[-41.5945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 38
	action: tensor([[ 1.2595, -0.4125,  0.7792,  0.1812, -0.6386, -0.6307,  0.0919]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16017913553004548, distance: 1.048696994351477 entropy 1.0271300077438354
epoch: 24, step: 39
	action: tensor([[ 1.9435, -0.2511,  0.4187, -0.3130, -0.9002,  0.5365, -0.2455]],
       dtype=torch.float64)
	q_value: tensor([[-40.6109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 40
	action: tensor([[-0.7072, -1.4738,  0.3008, -0.1713,  0.1468, -0.0569, -0.1619]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2052603097405794, distance: 1.699364822017272 entropy 1.0271300077438354
epoch: 24, step: 41
	action: tensor([[ 0.9794, -0.7793,  0.3688, -0.6683,  0.8669,  0.3750,  1.1564]],
       dtype=torch.float64)
	q_value: tensor([[-37.5068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4668290685840859, distance: 1.3859464399019497 entropy 1.0271300077438354
epoch: 24, step: 42
	action: tensor([[ 1.9252, -1.9521,  0.6467,  0.3379,  1.6003,  0.5637,  2.1464]],
       dtype=torch.float64)
	q_value: tensor([[-50.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 43
	action: tensor([[ 0.0513, -0.1030,  0.2625, -1.5287,  0.0956, -0.8868, -0.3193]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29181002772815834, distance: 1.3006367439767437 entropy 1.0271300077438354
epoch: 24, step: 44
	action: tensor([[-0.0733, -0.1107,  1.9321, -0.1815, -0.0256,  0.1911,  0.2676]],
       dtype=torch.float64)
	q_value: tensor([[-36.3704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06738390982768272, distance: 1.1051167087307485 entropy 1.0271300077438354
epoch: 24, step: 45
	action: tensor([[ 0.0749, -0.4621,  0.2023, -0.2745,  0.5579, -0.0019,  0.7295]],
       dtype=torch.float64)
	q_value: tensor([[-42.1156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21526336682296687, distance: 1.261513482958333 entropy 1.0271300077438354
epoch: 24, step: 46
	action: tensor([[0.0659, 0.2718, 0.9867, 0.1418, 0.7509, 0.8147, 0.8822]],
       dtype=torch.float64)
	q_value: tensor([[-36.6255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 47
	action: tensor([[-0.8779, -1.1481, -0.2298, -0.3359,  1.6018, -0.1892, -0.1100]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2237304709957897, distance: 1.706466499389876 entropy 1.0271300077438354
epoch: 24, step: 48
	action: tensor([[-0.7309,  0.9813, -1.4688, -0.2680,  0.5885,  1.1755,  0.1124]],
       dtype=torch.float64)
	q_value: tensor([[-43.9148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 49
	action: tensor([[-0.5378, -0.6109, -0.1936, -0.5287, -0.8950, -0.7632, -0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4844512340820448, distance: 1.3942468141992082 entropy 1.0271300077438354
epoch: 24, step: 50
	action: tensor([[-0.4935, -0.5723,  0.6192,  0.2698,  0.1405,  1.8499, -0.4664]],
       dtype=torch.float64)
	q_value: tensor([[-35.0659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32946935503084374, distance: 0.9370569989149595 entropy 1.0271300077438354
epoch: 24, step: 51
	action: tensor([[ 0.3547, -1.2044,  0.4558, -0.2586, -0.1289, -0.4801,  2.0162]],
       dtype=torch.float64)
	q_value: tensor([[-48.9521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9454287214035726, distance: 1.5961156464221435 entropy 1.0271300077438354
epoch: 24, step: 52
	action: tensor([[ 2.9885, -2.3900,  1.5025, -0.5545, -0.3260,  0.4122,  1.1206]],
       dtype=torch.float64)
	q_value: tensor([[-53.5222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 53
	action: tensor([[-0.0979, -0.7833, -0.0451,  0.6739,  0.7615,  1.4564,  0.1147]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5401917506092367, distance: 0.7759702526410849 entropy 1.0271300077438354
epoch: 24, step: 54
	action: tensor([[ 0.7564, -1.6419,  0.7262,  1.0232,  1.0780,  1.4007,  0.3006]],
       dtype=torch.float64)
	q_value: tensor([[-46.7696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 55
	action: tensor([[ 0.0962, -0.0069, -0.4552,  0.0425,  0.7206,  0.2687,  0.9337]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4798418125554209, distance: 0.8253238827779791 entropy 1.0271300077438354
epoch: 24, step: 56
	action: tensor([[-0.1409, -1.2044,  0.9773, -0.9103, -0.2581,  0.2845,  1.1314]],
       dtype=torch.float64)
	q_value: tensor([[-39.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2106437653885496, distance: 1.701437792413964 entropy 1.0271300077438354
epoch: 24, step: 57
	action: tensor([[ 2.6952, -1.1946,  0.8255, -0.3417,  0.9423,  0.2261,  2.6132]],
       dtype=torch.float64)
	q_value: tensor([[-48.9975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 58
	action: tensor([[-0.3752, -0.7460,  1.0946,  0.2980, -0.1687,  1.1117,  0.4733]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1613239392200082, distance: 1.0479819837744961 entropy 1.0271300077438354
epoch: 24, step: 59
	action: tensor([[ 0.9680, -0.7409, -0.3233, -0.1648,  0.5145,  1.3603,  1.3898]],
       dtype=torch.float64)
	q_value: tensor([[-45.8504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44666725061114554, distance: 0.8512358010353234 entropy 1.0271300077438354
epoch: 24, step: 60
	action: tensor([[ 3.4101, -2.3822,  0.9160, -0.4047,  0.7578,  0.0526,  1.7158]],
       dtype=torch.float64)
	q_value: tensor([[-58.7363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 61
	action: tensor([[-0.3634, -0.4172,  0.1723,  0.2889, -0.3751,  0.1371,  0.3124]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23324135621048625, distance: 1.2708103210768014 entropy 1.0271300077438354
epoch: 24, step: 62
	action: tensor([[ 0.7319,  0.7660,  0.2372, -0.0732,  0.4845,  0.1954,  0.5583]],
       dtype=torch.float64)
	q_value: tensor([[-33.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 63
	action: tensor([[-0.5182,  0.8654,  0.0608,  0.6505, -1.0978,  0.4077, -0.1147]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 64
	action: tensor([[ 0.3612, -0.3142,  0.2769,  1.1270,  0.1511, -0.6378,  0.7367]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.816739913537156, distance: 0.4898810463980795 entropy 1.0271300077438354
epoch: 24, step: 65
	action: tensor([[ 0.3920, -0.3086, -0.6510, -0.9503,  0.3803, -0.8724,  0.3911]],
       dtype=torch.float64)
	q_value: tensor([[-39.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04066667211771402, distance: 1.1673807200375466 entropy 1.0271300077438354
epoch: 24, step: 66
	action: tensor([[1.0619, 0.4646, 1.3688, 0.4532, 0.1632, 0.3311, 0.4999]],
       dtype=torch.float64)
	q_value: tensor([[-38.4884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7800295043079165, distance: 0.5367090395859161 entropy 1.0271300077438354
epoch: 24, step: 67
	action: tensor([[ 0.9005, -1.8767,  0.1742, -1.0325,  0.7340,  0.1434,  1.4601]],
       dtype=torch.float64)
	q_value: tensor([[-44.4790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 68
	action: tensor([[ 0.6049, -1.4870, -0.2527,  0.1568,  0.8532, -0.8407,  0.7566]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7103031217785887, distance: 1.4965569132906995 entropy 1.0271300077438354
epoch: 24, step: 69
	action: tensor([[ 1.1719, -0.3659,  0.9137, -1.1533, -0.7713, -1.2283,  0.8200]],
       dtype=torch.float64)
	q_value: tensor([[-44.9239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008162372934803708, distance: 1.1490050448340594 entropy 1.0271300077438354
epoch: 24, step: 70
	action: tensor([[ 3.0590, -0.7844, -0.4061,  0.1990, -0.1061, -0.0414,  1.0722]],
       dtype=torch.float64)
	q_value: tensor([[-48.3113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 71
	action: tensor([[ 1.3212, -0.6147, -0.2922,  0.0984,  0.2421,  0.2908,  0.3948]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.207014829204881, distance: 1.0190352946450416 entropy 1.0271300077438354
epoch: 24, step: 72
	action: tensor([[ 1.7380, -1.5935,  0.9278, -0.5403,  1.2812,  0.4926,  2.8326]],
       dtype=torch.float64)
	q_value: tensor([[-43.9087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 73
	action: tensor([[ 0.5536, -0.2653, -1.4764,  0.5796,  0.2184,  0.6358,  0.7139]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3735091411205035, distance: 0.9057619292243164 entropy 1.0271300077438354
epoch: 24, step: 74
	action: tensor([[ 2.2278, -0.6806, -0.0452, -0.7413,  0.3256,  0.3929,  0.1862]],
       dtype=torch.float64)
	q_value: tensor([[-45.5377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 75
	action: tensor([[ 0.8622,  0.3064,  0.5099, -0.2182, -1.1390,  0.3106,  0.7951]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 76
	action: tensor([[ 0.4897, -0.4707, -0.2857, -0.4798, -0.1588,  0.1137,  0.6984]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03610520007294293, distance: 1.1648194664577756 entropy 1.0271300077438354
epoch: 24, step: 77
	action: tensor([[ 1.4289, -0.4861,  0.1988, -0.2019,  0.1585,  0.1792,  1.0964]],
       dtype=torch.float64)
	q_value: tensor([[-39.5688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03399287802875672, distance: 1.1247263177399758 entropy 1.0271300077438354
epoch: 24, step: 78
	action: tensor([[ 2.0716, -1.6905,  1.2163, -1.4358, -0.2849,  0.3952,  1.4163]],
       dtype=torch.float64)
	q_value: tensor([[-48.8481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 79
	action: tensor([[ 0.9102, -1.2564,  0.5502,  0.1805,  0.8693, -0.2838, -0.9957]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5237486661328348, distance: 1.4125810066472138 entropy 1.0271300077438354
epoch: 24, step: 80
	action: tensor([[ 0.8774, -0.2667,  0.7570,  0.7877, -0.7291, -0.8576,  0.6873]],
       dtype=torch.float64)
	q_value: tensor([[-43.4405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6022757959788292, distance: 0.7216850419202236 entropy 1.0271300077438354
epoch: 24, step: 81
	action: tensor([[ 0.8353, -0.7810,  0.6046, -0.5290, -0.0319,  0.5319,  2.0528]],
       dtype=torch.float64)
	q_value: tensor([[-43.0713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1934267539578194, distance: 1.2501282750366682 entropy 1.0271300077438354
epoch: 24, step: 82
	action: tensor([[ 2.9793, -3.8873,  1.3806, -1.2279,  0.2568,  0.6842,  2.1643]],
       dtype=torch.float64)
	q_value: tensor([[-58.4024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 83
	action: tensor([[ 0.5524, -1.4873,  0.8545,  0.4988,  1.0126,  0.9987, -0.1811]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07827219873413571, distance: 1.188285772264182 entropy 1.0271300077438354
epoch: 24, step: 84
	action: tensor([[ 2.0224, -1.1084,  0.4652, -1.2559, -0.0459, -0.2018,  0.9864]],
       dtype=torch.float64)
	q_value: tensor([[-49.7437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 85
	action: tensor([[ 0.9204, -0.7404,  0.5137, -0.1780,  0.6478, -0.0987,  1.1997]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14340605200874612, distance: 1.2236492167972746 entropy 1.0271300077438354
epoch: 24, step: 86
	action: tensor([[ 3.2837, -2.4850,  0.8168, -1.9059, -0.0804,  0.4492,  1.2107]],
       dtype=torch.float64)
	q_value: tensor([[-46.4839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 87
	action: tensor([[-0.9311,  0.3615,  0.2471,  0.0739,  0.5994,  0.4518,  0.6363]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31061626818743915, distance: 1.3100699060429792 entropy 1.0271300077438354
epoch: 24, step: 88
	action: tensor([[ 1.7220, -0.0285, -0.0706, -1.0511, -0.6066,  0.1504,  0.6606]],
       dtype=torch.float64)
	q_value: tensor([[-37.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09801231636327312, distance: 1.1991135183360493 entropy 1.0271300077438354
epoch: 24, step: 89
	action: tensor([[ 2.0029, -1.6085,  0.1418, -1.1991,  0.1309,  0.9352,  2.3137]],
       dtype=torch.float64)
	q_value: tensor([[-49.3268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 90
	action: tensor([[ 1.0006, -0.0048,  0.1711, -1.0296,  0.5049,  0.3767, -0.3886]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2690614847329327, distance: 0.9783564592550862 entropy 1.0271300077438354
epoch: 24, step: 91
	action: tensor([[ 0.5104, -0.9986,  1.2882, -0.4343,  0.0646,  0.8760,  0.9960]],
       dtype=torch.float64)
	q_value: tensor([[-39.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28655010152646954, distance: 1.2979861098579297 entropy 1.0271300077438354
epoch: 24, step: 92
	action: tensor([[ 3.5604, -2.0202,  1.0235, -0.0763, -0.5217,  0.3608,  1.5392]],
       dtype=torch.float64)
	q_value: tensor([[-50.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 93
	action: tensor([[-0.5311, -0.2752,  0.7494,  0.4947, -1.2069,  0.0716, -0.1959]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10671977550291456, distance: 1.2038587353134098 entropy 1.0271300077438354
epoch: 24, step: 94
	action: tensor([[ 1.2382,  0.0366, -0.1727,  0.4789,  0.2681,  1.0742,  1.2728]],
       dtype=torch.float64)
	q_value: tensor([[-38.6340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 95
	action: tensor([[-0.4107, -1.2706,  1.4305, -0.9276, -0.3990,  0.1801,  0.4298]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3419530573745582, distance: 1.7512404748492982 entropy 1.0271300077438354
epoch: 24, step: 96
	action: tensor([[ 0.5445, -0.6290, -0.1629, -1.2530,  0.8976,  0.2788,  0.7315]],
       dtype=torch.float64)
	q_value: tensor([[-46.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4810546042428465, distance: 1.3926507858708415 entropy 1.0271300077438354
epoch: 24, step: 97
	action: tensor([[ 1.7743, -1.5615,  1.2632, -1.7968, -0.4195, -0.8388,  0.5484]],
       dtype=torch.float64)
	q_value: tensor([[-47.3162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 98
	action: tensor([[-0.4222, -0.8576,  0.0814, -1.1345,  0.5323,  1.0261,  1.2071]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6247833512627652, distance: 1.4586611745983256 entropy 1.0271300077438354
epoch: 24, step: 99
	action: tensor([[ 2.4761, -0.1155,  0.2755, -0.6500, -0.3119,  1.1118,  1.7985]],
       dtype=torch.float64)
	q_value: tensor([[-52.8545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 100
	action: tensor([[ 0.4167,  0.3339,  0.2338, -1.2462, -0.7477,  0.5072, -0.8830]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34792395081966365, distance: 0.9240720141491259 entropy 1.0271300077438354
epoch: 24, step: 101
	action: tensor([[ 0.8603, -0.2200, -0.3828,  0.0902, -0.0820,  0.0748,  1.0991]],
       dtype=torch.float64)
	q_value: tensor([[-38.1898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6420877612245327, distance: 0.6846127261784815 entropy 1.0271300077438354
epoch: 24, step: 102
	action: tensor([[ 1.0356, -0.7683,  0.3861, -0.5583,  0.4510, -0.3151,  2.1286]],
       dtype=torch.float64)
	q_value: tensor([[-44.1244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4431477275432403, distance: 1.3747131538030204 entropy 1.0271300077438354
epoch: 24, step: 103
	action: tensor([[ 2.2726, -3.1892,  1.7711, -0.8806,  0.8678,  1.2222,  2.0939]],
       dtype=torch.float64)
	q_value: tensor([[-56.9482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 104
	action: tensor([[ 0.3978, -0.8348,  0.2597,  0.5664, -0.6618, -0.4069,  0.2929]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12790847356333312, distance: 1.0686555033941139 entropy 1.0271300077438354
epoch: 24, step: 105
	action: tensor([[ 0.3790,  0.0240,  1.1453, -0.5256,  0.8433, -0.1119,  0.3873]],
       dtype=torch.float64)
	q_value: tensor([[-38.7417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30526780232493866, distance: 0.9538177644576786 entropy 1.0271300077438354
epoch: 24, step: 106
	action: tensor([[ 1.4304, -0.1156,  0.2716, -0.1852,  0.0985,  1.4733,  1.2375]],
       dtype=torch.float64)
	q_value: tensor([[-38.8592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6145890978947731, distance: 0.7104257443004196 entropy 1.0271300077438354
epoch: 24, step: 107
	action: tensor([[ 2.8424, -3.5395,  0.8166, -0.0745,  0.3115,  0.5822,  1.3727]],
       dtype=torch.float64)
	q_value: tensor([[-57.7335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 108
	action: tensor([[-0.2138, -1.5617,  0.7111,  0.2833,  0.7486,  0.1058, -0.1019]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6340283198466001, distance: 1.4628051573635412 entropy 1.0271300077438354
epoch: 24, step: 109
	action: tensor([[ 0.5988, -0.2262,  0.4706, -0.4386, -0.6529, -0.6513,  0.7763]],
       dtype=torch.float64)
	q_value: tensor([[-39.5847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08230240445836601, distance: 1.0962421340924542 entropy 1.0271300077438354
epoch: 24, step: 110
	action: tensor([[ 1.0226, -0.5148,  1.0098,  0.4154, -1.3523, -0.4526,  0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-39.4564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.210734577117915, distance: 1.0166424314455873 entropy 1.0271300077438354
epoch: 24, step: 111
	action: tensor([[ 2.3617,  0.2412,  0.3596, -1.0108,  0.8523, -0.3905,  1.1614]],
       dtype=torch.float64)
	q_value: tensor([[-46.8973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 112
	action: tensor([[-0.3561, -0.2801, -0.1040,  0.6089, -0.7404,  0.5555,  0.9564]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035591059764003674, distance: 1.1645304248848336 entropy 1.0271300077438354
epoch: 24, step: 113
	action: tensor([[ 1.5191, -0.9954, -0.1490,  0.1469, -0.5492, -0.0353,  1.1090]],
       dtype=torch.float64)
	q_value: tensor([[-41.4138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.388230337014704, distance: 1.3483028691151844 entropy 1.0271300077438354
epoch: 24, step: 114
	action: tensor([[ 3.1165, -1.9432,  1.1649, -0.6152,  0.3764,  1.0876, -0.0876]],
       dtype=torch.float64)
	q_value: tensor([[-53.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 115
	action: tensor([[ 0.6561,  1.6587,  0.3770, -0.4116, -0.4256,  1.0474,  0.1077]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 116
	action: tensor([[-0.5958,  0.6272, -0.0726, -0.1151, -0.1239, -0.3739, -0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017625853892046406, distance: 1.1543852244074795 entropy 1.0271300077438354
epoch: 24, step: 117
	action: tensor([[ 0.8704,  0.6996, -0.3159,  0.5322,  0.4757,  0.0346, -0.7082]],
       dtype=torch.float64)
	q_value: tensor([[-27.6034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 118
	action: tensor([[-0.3351, -0.1484, -0.1866, -0.0473, -1.0944,  0.0435, -0.2850]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21256453098110217, distance: 1.2601119307406494 entropy 1.0271300077438354
epoch: 24, step: 119
	action: tensor([[-0.5766, -0.0638,  0.1879, -0.2736, -0.2932, -0.1077,  0.4420]],
       dtype=torch.float64)
	q_value: tensor([[-33.8978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5939192293244477, distance: 1.4447405029661349 entropy 1.0271300077438354
epoch: 24, step: 120
	action: tensor([[ 0.1078, -0.9887, -1.0898, -0.5708, -0.2790, -0.4559,  0.9212]],
       dtype=torch.float64)
	q_value: tensor([[-32.1005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27237992184295057, distance: 1.2908182498864123 entropy 1.0271300077438354
epoch: 24, step: 121
	action: tensor([[ 0.1391,  0.2403,  0.4447,  0.4302, -0.6159,  0.1143, -0.2127]],
       dtype=torch.float64)
	q_value: tensor([[-46.1399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 122
	action: tensor([[ 0.1124, -0.1923,  0.6056,  0.7806,  0.2407,  0.9542,  0.4121]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 123
	action: tensor([[-0.3752, -0.2673,  0.2479, -0.8389, -0.1428, -0.2211, -0.1878]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7164705215000451, distance: 1.4992527976504764 entropy 1.0271300077438354
epoch: 24, step: 124
	action: tensor([[ 0.9253, -0.3777, -1.7343,  0.4945, -0.1009, -0.6653,  0.8489]],
       dtype=torch.float64)
	q_value: tensor([[-32.1641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40858682904788934, distance: 0.8800395481092165 entropy 1.0271300077438354
epoch: 24, step: 125
	action: tensor([[-0.1159, -0.4335, -0.1007, -1.4562,  1.8625,  0.4402,  1.2181]],
       dtype=torch.float64)
	q_value: tensor([[-48.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2660487320779492, distance: 1.2876027767676033 entropy 1.0271300077438354
epoch: 24, step: 126
	action: tensor([[ 2.1540, -0.9826,  1.5407, -0.0852,  1.3623,  0.8359,  2.5424]],
       dtype=torch.float64)
	q_value: tensor([[-55.2055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 127
	action: tensor([[ 0.3812, -0.7557, -0.2004, -0.3948,  0.0773, -0.1625, -1.1469]],
       dtype=torch.float64)
	q_value: tensor([[-39.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41050565898955793, distance: 1.359077146637288 entropy 1.0271300077438354
LOSS epoch 24 actor 428.82470280487723 critic 80.63569628413589 
epoch: 25, step: 0
	action: tensor([[-0.2285,  0.1023,  0.8312,  0.6679,  0.6583, -0.3148,  0.9464]],
       dtype=torch.float64)
	q_value: tensor([[-29.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 1
	action: tensor([[ 3.0687e-01,  2.0771e-01, -6.1850e-04,  6.0800e-01, -3.9572e-01,
          7.6973e-01,  4.1679e-01]], dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 2
	action: tensor([[ 0.6095, -0.3517, -0.0433,  0.0190,  0.2262,  0.5843, -0.4938]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5861692120013156, distance: 0.7361530101346 entropy 0.9217694997787476
epoch: 25, step: 3
	action: tensor([[-0.0856,  0.0995, -0.0464, -1.3708,  0.4064,  0.0218,  0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-29.3494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17765540727889495, distance: 1.2418404690856908 entropy 0.9217694997787476
epoch: 25, step: 4
	action: tensor([[ 0.5705, -0.8416, -0.2409,  0.4681, -0.5856,  0.4630,  0.5222]],
       dtype=torch.float64)
	q_value: tensor([[-31.3897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2757534113856104, distance: 0.9738676112984986 entropy 0.9217694997787476
epoch: 25, step: 5
	action: tensor([[1.4211, 0.0523, 0.3064, 0.0477, 0.8222, 0.2130, 0.2344]],
       dtype=torch.float64)
	q_value: tensor([[-36.1329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5855274743724672, distance: 0.7367235743549815 entropy 0.9217694997787476
epoch: 25, step: 6
	action: tensor([[ 0.1340, -0.5660,  0.8836,  1.0914, -0.1202, -0.0535,  0.7403]],
       dtype=torch.float64)
	q_value: tensor([[-34.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7243735992348924, distance: 0.6007822599742527 entropy 0.9217694997787476
epoch: 25, step: 7
	action: tensor([[ 1.1201, -1.2274, -0.2055, -0.1297,  0.2239, -0.3178,  1.3647]],
       dtype=torch.float64)
	q_value: tensor([[-34.9341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.820128703552129, distance: 1.543859386339098 entropy 0.9217694997787476
epoch: 25, step: 8
	action: tensor([[ 2.0431, -0.6844,  1.2888, -0.0087,  0.3835,  0.2050,  0.5414]],
       dtype=torch.float64)
	q_value: tensor([[-43.7111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 9
	action: tensor([[ 0.2823, -0.0352,  1.2206,  0.0772,  1.1535, -0.3131,  0.5816]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.554620234640469, distance: 0.7636984942736977 entropy 0.9217694997787476
epoch: 25, step: 10
	action: tensor([[ 0.2358, -0.3792, -0.0175, -0.1622,  0.1841, -0.0607,  1.1650]],
       dtype=torch.float64)
	q_value: tensor([[-34.0365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06991313456828108, distance: 1.1036171709402636 entropy 0.9217694997787476
epoch: 25, step: 11
	action: tensor([[ 0.9958, -0.5505, -0.5396, -0.2767,  0.4114,  0.1308,  0.3619]],
       dtype=torch.float64)
	q_value: tensor([[-33.3757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.054158640761427956, distance: 1.1129248600623403 entropy 0.9217694997787476
epoch: 25, step: 12
	action: tensor([[-0.4072, -0.9873,  0.8394, -0.0746,  0.6354,  1.2237,  0.3318]],
       dtype=torch.float64)
	q_value: tensor([[-34.0126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2879147877078265, distance: 1.2986743357668498 entropy 0.9217694997787476
epoch: 25, step: 13
	action: tensor([[ 0.7989, -2.0039, -0.1383, -1.0136, -0.0090, -0.3843,  1.3034]],
       dtype=torch.float64)
	q_value: tensor([[-38.0889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 14
	action: tensor([[-0.9662,  0.8011,  0.6578,  0.2055,  0.9773,  0.0898, -0.8382]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 15
	action: tensor([[ 0.7504, -0.6262,  0.9048, -0.6212, -0.1086, -1.0006, -1.1121]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23272659113022653, distance: 1.2705450700569032 entropy 0.9217694997787476
epoch: 25, step: 16
	action: tensor([[ 1.3022, -0.4802,  0.5488,  0.0240,  0.3091,  0.3720,  0.3741]],
       dtype=torch.float64)
	q_value: tensor([[-32.2448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24299838908648475, distance: 0.9956463250965647 entropy 0.9217694997787476
epoch: 25, step: 17
	action: tensor([[ 0.0072, -0.7087,  0.1705, -0.1409,  0.2985,  0.7662,  1.6567]],
       dtype=torch.float64)
	q_value: tensor([[-35.7200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.032612434538819235, distance: 1.162854474963741 entropy 0.9217694997787476
epoch: 25, step: 18
	action: tensor([[ 1.8454, -1.4958,  0.2043, -0.0194,  0.3656,  1.3487, -0.1885]],
       dtype=torch.float64)
	q_value: tensor([[-41.7575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 19
	action: tensor([[-0.1710,  0.0851, -0.2595,  0.1234,  0.9536,  0.8521,  0.4955]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 20
	action: tensor([[ 0.9990, -0.0134,  0.4544, -0.9822,  0.6151,  0.5858,  1.5670]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36725173737996675, distance: 0.9102740748693661 entropy 0.9217694997787476
epoch: 25, step: 21
	action: tensor([[ 3.4589, -3.0127,  0.4604, -1.3787,  0.5885,  0.8711,  1.5744]],
       dtype=torch.float64)
	q_value: tensor([[-43.0050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 22
	action: tensor([[ 0.8264, -0.1327, -0.1456, -1.1372,  0.1590, -0.2796, -0.0578]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0657190238467531, distance: 1.1813485495994038 entropy 0.9217694997787476
epoch: 25, step: 23
	action: tensor([[-0.0602, -0.8527, -0.1280, -0.4279,  0.1612, -0.9729,  0.1971]],
       dtype=torch.float64)
	q_value: tensor([[-30.7216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6529618032036344, distance: 1.4712555097418645 entropy 0.9217694997787476
epoch: 25, step: 24
	action: tensor([[ 0.9427, -0.1856,  1.3324, -0.0888, -1.1280,  0.2090, -0.5505]],
       dtype=torch.float64)
	q_value: tensor([[-30.2293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6287220460208719, distance: 0.6972785000485827 entropy 0.9217694997787476
epoch: 25, step: 25
	action: tensor([[ 2.4340, -0.0085,  0.4371, -0.3035, -0.5480, -0.2889,  0.6860]],
       dtype=torch.float64)
	q_value: tensor([[-36.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 26
	action: tensor([[-0.3191, -0.5558, -0.0244, -0.4423,  0.5222,  0.2226,  0.5128]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6051047370139859, distance: 1.4498009550032749 entropy 0.9217694997787476
epoch: 25, step: 27
	action: tensor([[ 0.5158,  0.4400,  0.4809,  0.3173, -0.9059,  1.4938,  0.5728]],
       dtype=torch.float64)
	q_value: tensor([[-29.8658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 28
	action: tensor([[-0.6139,  0.3967,  0.5615, -1.4750, -0.8151, -0.5636, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6070450717291971, distance: 1.4506769880173047 entropy 0.9217694997787476
epoch: 25, step: 29
	action: tensor([[-0.0189,  0.7604,  0.0751,  0.0834,  0.3970, -1.3112,  0.7045]],
       dtype=torch.float64)
	q_value: tensor([[-31.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 30
	action: tensor([[-0.4357, -0.7190, -0.6675,  0.8711, -0.7641,  0.4653, -0.6335]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6516204691615146, distance: 1.4706584464558372 entropy 0.9217694997787476
epoch: 25, step: 31
	action: tensor([[-0.5275,  1.5964, -0.8498, -0.8683,  0.0899, -0.9373,  0.8619]],
       dtype=torch.float64)
	q_value: tensor([[-33.3248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 32
	action: tensor([[ 0.2350, -0.3508, -0.0917, -0.2538,  0.1621,  0.6445, -0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2656066116368446, distance: 0.980665896120363 entropy 0.9217694997787476
epoch: 25, step: 33
	action: tensor([[ 0.5146,  0.2814,  0.5662,  0.2440, -0.1988,  0.7638,  1.0835]],
       dtype=torch.float64)
	q_value: tensor([[-28.2641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9866526668411186, distance: 0.13220684573749766 entropy 0.9217694997787476
epoch: 25, step: 34
	action: tensor([[ 1.5428, -1.1170,  0.5834, -0.9844,  1.2138, -0.1449,  0.9297]],
       dtype=torch.float64)
	q_value: tensor([[-36.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6029814290353814, distance: 1.4488417052445253 entropy 0.9217694997787476
epoch: 25, step: 35
	action: tensor([[ 0.3762, -1.0447,  0.9456, -0.5189, -0.4536,  1.7913,  1.4441]],
       dtype=torch.float64)
	q_value: tensor([[-44.5127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05447674385797907, distance: 1.1127376962150632 entropy 0.9217694997787476
epoch: 25, step: 36
	action: tensor([[ 2.3432, -1.8580,  0.5598, -1.8522, -0.5878,  0.0532,  2.2028]],
       dtype=torch.float64)
	q_value: tensor([[-50.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 37
	action: tensor([[ 0.8038,  0.3088, -0.5191, -0.8609,  0.4644, -0.2175,  0.5186]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 38
	action: tensor([[ 0.8959,  0.4212, -0.5006, -0.0864, -0.2105,  0.3713,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 39
	action: tensor([[-0.1369, -0.4095, -1.3719, -0.7940,  0.5159, -1.4766,  0.6423]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3210767708325326, distance: 0.9429030213807147 entropy 0.9217694997787476
epoch: 25, step: 40
	action: tensor([[ 0.2757, -0.0501,  0.2194, -0.1513, -0.0996, -0.1252, -0.6837]],
       dtype=torch.float64)
	q_value: tensor([[-36.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34447331218679045, distance: 0.9265137779066147 entropy 0.9217694997787476
epoch: 25, step: 41
	action: tensor([[ 0.4204, -0.5135,  1.0997, -0.5752,  0.1575, -1.0190, -1.2291]],
       dtype=torch.float64)
	q_value: tensor([[-24.3372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24616004843248263, distance: 1.2774491014200224 entropy 0.9217694997787476
epoch: 25, step: 42
	action: tensor([[ 0.1136, -0.6511,  0.2773, -1.1161, -0.8302, -0.1230,  0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-31.9062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7701118775366356, distance: 1.5224991238542387 entropy 0.9217694997787476
epoch: 25, step: 43
	action: tensor([[ 0.6427, -1.1916,  0.1014,  0.4680,  0.2783, -0.1955,  0.5197]],
       dtype=torch.float64)
	q_value: tensor([[-32.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2760824128484829, distance: 1.2926949579177343 entropy 0.9217694997787476
epoch: 25, step: 44
	action: tensor([[ 0.1713, -0.5836, -0.4737, -0.6666,  0.2311, -0.0197,  0.4619]],
       dtype=torch.float64)
	q_value: tensor([[-35.7749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3191802948052276, distance: 1.314343164914174 entropy 0.9217694997787476
epoch: 25, step: 45
	action: tensor([[-0.1035, -1.8021,  0.2848,  0.3474, -0.0579,  0.2895,  0.6356]],
       dtype=torch.float64)
	q_value: tensor([[-30.9563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 46
	action: tensor([[ 0.4876, -1.3518,  0.0357,  1.4006, -0.1185, -0.2873, -0.1531]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4344538425330885, distance: 0.8605789546436068 entropy 0.9217694997787476
epoch: 25, step: 47
	action: tensor([[ 0.2772, -0.9292,  0.2984, -1.0944,  0.5979,  1.3453,  0.6919]],
       dtype=torch.float64)
	q_value: tensor([[-38.2963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3698561164120471, distance: 1.3393502709147007 entropy 0.9217694997787476
epoch: 25, step: 48
	action: tensor([[ 1.8063, -1.3239,  0.2257, -0.5706,  0.5631,  0.6422,  2.0038]],
       dtype=torch.float64)
	q_value: tensor([[-43.1844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 49
	action: tensor([[ 0.0585, -1.4558,  0.0679, -0.6994,  0.4652, -0.1711, -0.2309]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0442402748916169, distance: 1.63614829247263 entropy 0.9217694997787476
epoch: 25, step: 50
	action: tensor([[ 0.7616, -0.0698,  0.2528, -0.4709, -0.6806,  0.6301,  1.3274]],
       dtype=torch.float64)
	q_value: tensor([[-32.7222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5753238284690552, distance: 0.7457369119726662 entropy 0.9217694997787476
epoch: 25, step: 51
	action: tensor([[ 1.5572, -1.7174,  0.1543, -0.3319,  0.0053,  1.5803,  2.0254]],
       dtype=torch.float64)
	q_value: tensor([[-40.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 52
	action: tensor([[ 0.1325,  0.3935, -0.9523, -0.1876, -0.3569, -0.3744, -0.5188]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 53
	action: tensor([[ 0.8581, -0.0490,  0.1785,  0.4871, -0.3849,  0.1857, -0.1697]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 54
	action: tensor([[-0.9502, -0.0955, -0.1753,  0.3724,  0.7668, -0.3369,  0.9083]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9588159568099508, distance: 1.6015979707014727 entropy 0.9217694997787476
epoch: 25, step: 55
	action: tensor([[-0.6385,  0.1423,  0.6243, -0.4775, -0.0595,  0.7538,  0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-30.4132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4084301724281221, distance: 1.3580768724640684 entropy 0.9217694997787476
epoch: 25, step: 56
	action: tensor([[ 0.1587, -0.6162, -0.0165,  0.6781, -0.8032, -1.1748, -0.2250]],
       dtype=torch.float64)
	q_value: tensor([[-30.1161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2799130824722432, distance: 0.9710669061223328 entropy 0.9217694997787476
epoch: 25, step: 57
	action: tensor([[ 1.0002, -0.8657,  1.6111,  0.1905, -0.2627,  0.0423,  0.5148]],
       dtype=torch.float64)
	q_value: tensor([[-32.6963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24350036638320538, distance: 1.2760851420850527 entropy 0.9217694997787476
epoch: 25, step: 58
	action: tensor([[ 0.9165, -0.8345, -0.2812, -0.6333,  0.1558,  0.3340,  0.8666]],
       dtype=torch.float64)
	q_value: tensor([[-38.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44509430085869495, distance: 1.3756399744369596 entropy 0.9217694997787476
epoch: 25, step: 59
	action: tensor([[ 1.7892, -1.3967,  0.6241, -0.6932,  0.5484,  0.2669,  0.1729]],
       dtype=torch.float64)
	q_value: tensor([[-39.0459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 60
	action: tensor([[-1.1635, -1.1045, -0.1321, -0.6977,  0.3289, -0.9875, -0.2558]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6345219149179873, distance: 1.4630260773099473 entropy 0.9217694997787476
epoch: 25, step: 61
	action: tensor([[-0.7094, -0.3867,  0.4065,  0.3757,  0.3914,  0.0313, -0.4475]],
       dtype=torch.float64)
	q_value: tensor([[-32.4766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5446509037471037, distance: 1.4222366460534215 entropy 0.9217694997787476
epoch: 25, step: 62
	action: tensor([[ 0.8334,  0.5871,  0.3739, -0.8683,  1.2229,  0.7947,  0.8527]],
       dtype=torch.float64)
	q_value: tensor([[-27.1420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8503236684825362, distance: 0.4427241965122342 entropy 0.9217694997787476
epoch: 25, step: 63
	action: tensor([[ 1.8519, -0.8845,  0.7274, -1.1359,  0.1417, -0.3347,  1.0034]],
       dtype=torch.float64)
	q_value: tensor([[-39.5182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 64
	action: tensor([[ 0.4585,  0.5574, -0.5690, -0.1005, -0.1651, -0.4613, -0.1834]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 65
	action: tensor([[ 0.6563, -0.6698, -0.3044,  0.4778,  0.3282,  0.2255, -0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4547612763737182, distance: 0.8449870234623184 entropy 0.9217694997787476
epoch: 25, step: 66
	action: tensor([[ 0.1532, -0.6367,  0.2939, -0.7040, -0.0741, -0.1372, -0.4749]],
       dtype=torch.float64)
	q_value: tensor([[-30.6309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6057575234956709, distance: 1.4500957377166497 entropy 0.9217694997787476
epoch: 25, step: 67
	action: tensor([[ 0.5303, -0.0498,  0.4021, -0.6941, -1.0231, -0.1700, -0.7014]],
       dtype=torch.float64)
	q_value: tensor([[-27.6461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18752992218462117, distance: 1.0314789773252662 entropy 0.9217694997787476
epoch: 25, step: 68
	action: tensor([[ 0.1763,  0.1851,  0.5466, -0.4141,  0.0168, -0.6518, -0.2027]],
       dtype=torch.float64)
	q_value: tensor([[-30.0411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20279442710697937, distance: 1.0217434358024937 entropy 0.9217694997787476
epoch: 25, step: 69
	action: tensor([[-0.0627, -1.3227,  0.1977,  0.4263, -0.5280,  0.1392,  1.2680]],
       dtype=torch.float64)
	q_value: tensor([[-25.0897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47746826425419187, distance: 1.390963627820032 entropy 0.9217694997787476
epoch: 25, step: 70
	action: tensor([[ 1.8992e-01, -1.8294e+00,  2.6171e-01, -7.3870e-04, -1.5852e-01,
          2.2033e-01,  9.8926e-01]], dtype=torch.float64)
	q_value: tensor([[-39.6512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 71
	action: tensor([[-0.1591, -0.9910,  0.5877, -0.1798, -0.2913, -0.1596, -0.4624]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8688880575009104, distance: 1.5644019105952878 entropy 0.9217694997787476
epoch: 25, step: 72
	action: tensor([[0.2384, 1.0587, 0.6273, 0.9479, 0.0257, 0.9982, 0.9523]],
       dtype=torch.float64)
	q_value: tensor([[-29.5677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 73
	action: tensor([[ 1.0195, -0.6916,  0.6009,  0.6605, -0.7343,  0.0022,  0.0433]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3948200746242182, distance: 0.8902232857057355 entropy 0.9217694997787476
epoch: 25, step: 74
	action: tensor([[ 0.8338, -1.0173,  0.4633, -0.4983,  0.0913,  0.2768,  2.4153]],
       dtype=torch.float64)
	q_value: tensor([[-35.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6047590615125027, distance: 1.44964483196469 entropy 0.9217694997787476
epoch: 25, step: 75
	action: tensor([[ 2.2592, -1.8195,  1.8036, -0.3297, -0.1435,  0.3388,  1.5900]],
       dtype=torch.float64)
	q_value: tensor([[-51.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 76
	action: tensor([[ 0.3075,  0.2149, -0.6784,  0.0473, -0.2264, -0.2410, -0.4883]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 77
	action: tensor([[-0.1470, -0.5994,  0.2921, -0.2147, -0.4913, -0.0591,  0.2321]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.530056410482922, distance: 1.4155017631612001 entropy 0.9217694997787476
epoch: 25, step: 78
	action: tensor([[ 1.3764, -0.3666,  0.8708,  0.9176,  0.4927,  0.2456,  1.8130]],
       dtype=torch.float64)
	q_value: tensor([[-28.6326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32222724185944873, distance: 0.9421037833330915 entropy 0.9217694997787476
epoch: 25, step: 79
	action: tensor([[ 0.1541, -1.0865,  0.2688, -0.3528, -1.6513,  0.6689,  1.4344]],
       dtype=torch.float64)
	q_value: tensor([[-47.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8561456587543339, distance: 1.5590596088051087 entropy 0.9217694997787476
epoch: 25, step: 80
	action: tensor([[ 1.7570, -1.6785,  0.4497,  0.0959,  0.9945,  0.4243,  1.1029]],
       dtype=torch.float64)
	q_value: tensor([[-46.7801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 81
	action: tensor([[-0.6080, -0.7758,  0.0459, -0.1791, -0.2164, -0.9488,  0.7750]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7872924287925738, distance: 1.5298699037554948 entropy 0.9217694997787476
epoch: 25, step: 82
	action: tensor([[ 0.7706, -0.0481,  0.2218,  0.0942, -0.3935, -0.1299,  0.4216]],
       dtype=torch.float64)
	q_value: tensor([[-31.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7223361381380167, distance: 0.6029986962174692 entropy 0.9217694997787476
epoch: 25, step: 83
	action: tensor([[ 0.2084, -0.0427, -0.2016, -0.0136,  0.8764,  0.8660,  0.1836]],
       dtype=torch.float64)
	q_value: tensor([[-29.6607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7231178589789509, distance: 0.6021492718719835 entropy 0.9217694997787476
epoch: 25, step: 84
	action: tensor([[ 0.2513, -0.7086,  1.0845, -0.8083,  0.4971, -0.3266,  0.1431]],
       dtype=torch.float64)
	q_value: tensor([[-31.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5771387765820428, distance: 1.4371154160836896 entropy 0.9217694997787476
epoch: 25, step: 85
	action: tensor([[1.7631, 0.7532, 0.6323, 0.4554, 1.3929, 0.7944, 0.2634]],
       dtype=torch.float64)
	q_value: tensor([[-31.7529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3832981445851713, distance: 0.8986577433563255 entropy 0.9217694997787476
epoch: 25, step: 86
	action: tensor([[ 0.3381, -0.8567, -0.0140,  0.0836,  0.3948, -0.0370,  1.3033]],
       dtype=torch.float64)
	q_value: tensor([[-43.0123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20814658812692954, distance: 1.2578142457475745 entropy 0.9217694997787476
epoch: 25, step: 87
	action: tensor([[ 1.1364, -0.7895, -0.1935,  0.6318,  1.5146,  0.6225,  0.5429]],
       dtype=torch.float64)
	q_value: tensor([[-37.3434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.353821143100325, distance: 0.9198840002174349 entropy 0.9217694997787476
epoch: 25, step: 88
	action: tensor([[ 1.0585, -0.2391,  1.7072, -0.4550, -0.8266,  0.1339, -0.4701]],
       dtype=torch.float64)
	q_value: tensor([[-43.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6069852387686392, distance: 0.7173995904981079 entropy 0.9217694997787476
epoch: 25, step: 89
	action: tensor([[ 0.1518, -0.2998, -0.6153, -0.7117,  0.5079, -0.1092, -0.2340]],
       dtype=torch.float64)
	q_value: tensor([[-37.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021338853577847372, distance: 1.1320689182388999 entropy 0.9217694997787476
epoch: 25, step: 90
	action: tensor([[ 0.5918, -0.7781,  0.8471,  0.1545, -0.2020,  0.0966, -0.3269]],
       dtype=torch.float64)
	q_value: tensor([[-28.3410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03526858944452549, distance: 1.1239834141980005 entropy 0.9217694997787476
epoch: 25, step: 91
	action: tensor([[ 0.3843, -0.7501, -0.5143,  0.1847,  0.3869,  0.0373,  0.3199]],
       dtype=torch.float64)
	q_value: tensor([[-31.7056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028954207794233078, distance: 1.160792830378888 entropy 0.9217694997787476
epoch: 25, step: 92
	action: tensor([[ 0.5207,  0.0635,  0.3054, -1.1175,  0.0210,  0.3845,  0.5952]],
       dtype=torch.float64)
	q_value: tensor([[-31.1162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.222757417554712, distance: 1.0088694851469644 entropy 0.9217694997787476
epoch: 25, step: 93
	action: tensor([[ 1.2800, -1.7614,  0.1340, -0.6114,  0.3502,  1.0684,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-33.8107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 94
	action: tensor([[ 0.3886, -0.8016, -0.6120, -0.4667, -0.7154, -0.0795,  0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32684224730097555, distance: 1.3181545676009094 entropy 0.9217694997787476
epoch: 25, step: 95
	action: tensor([[ 0.1804,  0.4697, -0.9664,  1.1306,  0.9695, -0.0281,  0.3717]],
       dtype=torch.float64)
	q_value: tensor([[-32.8325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 96
	action: tensor([[ 0.4250,  1.1320,  0.7168, -0.3038,  1.1547, -1.1742, -0.4568]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 97
	action: tensor([[ 0.2211,  0.1751,  0.1800, -0.7278, -1.0352, -0.9424,  0.1557]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3416809066382712, distance: 0.9284850575516532 entropy 0.9217694997787476
epoch: 25, step: 98
	action: tensor([[ 0.5594,  0.1316,  0.5216,  0.3272,  0.8939,  0.1547, -0.1399]],
       dtype=torch.float64)
	q_value: tensor([[-30.9693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9113510421089905, distance: 0.34071677048747245 entropy 0.9217694997787476
epoch: 25, step: 99
	action: tensor([[ 0.2307,  0.2637,  0.3168, -0.6130,  0.0125, -0.4211, -0.9346]],
       dtype=torch.float64)
	q_value: tensor([[-30.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28609029099386785, distance: 0.9668928248593087 entropy 0.9217694997787476
epoch: 25, step: 100
	action: tensor([[ 0.0116, -0.0793, -0.2137,  0.5726,  0.8825, -0.2392, -0.2356]],
       dtype=torch.float64)
	q_value: tensor([[-25.3720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38298001421089256, distance: 0.8988895032104276 entropy 0.9217694997787476
epoch: 25, step: 101
	action: tensor([[ 0.9813, -0.9376, -0.3352,  0.4211, -0.3742, -0.6968,  0.9901]],
       dtype=torch.float64)
	q_value: tensor([[-26.7799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13091285182741075, distance: 1.2169458751920657 entropy 0.9217694997787476
epoch: 25, step: 102
	action: tensor([[ 0.6276, -0.0646,  0.6229, -0.6013,  0.7715, -0.3459,  0.3206]],
       dtype=torch.float64)
	q_value: tensor([[-39.8678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2666264767626533, distance: 0.9799847254867896 entropy 0.9217694997787476
epoch: 25, step: 103
	action: tensor([[ 1.6639, -0.7387,  0.0590, -0.2021,  0.1643, -0.1303,  0.3433]],
       dtype=torch.float64)
	q_value: tensor([[-30.2410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4324791038928204, distance: 1.3696223709321842 entropy 0.9217694997787476
epoch: 25, step: 104
	action: tensor([[ 0.9701, -0.7651,  0.3219, -0.9682,  0.0855, -0.1391,  1.3551]],
       dtype=torch.float64)
	q_value: tensor([[-37.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5797896420250948, distance: 1.4383226656277626 entropy 0.9217694997787476
epoch: 25, step: 105
	action: tensor([[ 1.7573, -0.9219,  0.9091,  0.4420,  0.3383,  1.7746,  1.2537]],
       dtype=torch.float64)
	q_value: tensor([[-40.6611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10902267153751177, distance: 1.2051105972861476 entropy 0.9217694997787476
epoch: 25, step: 106
	action: tensor([[ 2.9580, -3.2918, -0.2958, -0.5185,  1.2789,  0.8785,  2.5582]],
       dtype=torch.float64)
	q_value: tensor([[-53.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 107
	action: tensor([[-0.2881, -0.5161,  0.4234, -0.3166, -0.7555,  0.2098, -0.1159]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6119557707204895, distance: 1.4528917374076906 entropy 0.9217694997787476
epoch: 25, step: 108
	action: tensor([[ 0.1456, -0.7623, -0.3645, -0.7909, -0.8299, -0.0308,  0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-29.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49305726268442096, distance: 1.3982825100597744 entropy 0.9217694997787476
epoch: 25, step: 109
	action: tensor([[ 0.9567, -0.1042,  1.2816, -1.2828,  0.1215, -0.5429,  0.5048]],
       dtype=torch.float64)
	q_value: tensor([[-34.0011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3446016877257183, distance: 0.9264230512195072 entropy 0.9217694997787476
epoch: 25, step: 110
	action: tensor([[ 0.5965, -0.7685,  0.6321,  0.3023,  0.4365,  0.8152,  0.3779]],
       dtype=torch.float64)
	q_value: tensor([[-35.9788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35232741871705386, distance: 0.920946600756111 entropy 0.9217694997787476
epoch: 25, step: 111
	action: tensor([[ 1.5181, -2.2792,  0.1686,  0.8396,  1.8586,  1.4725,  1.4594]],
       dtype=torch.float64)
	q_value: tensor([[-36.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 112
	action: tensor([[-0.1983, -0.6849, -0.3788, -0.3790, -0.1844, -0.9324, -1.2932]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4015301057111951, distance: 1.3547460980308856 entropy 0.9217694997787476
epoch: 25, step: 113
	action: tensor([[ 0.6148, -0.2844,  0.2064,  1.3835, -0.5256, -0.2288, -0.2643]],
       dtype=torch.float64)
	q_value: tensor([[-29.5340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9899498052597615, distance: 0.11472126621591525 entropy 0.9217694997787476
epoch: 25, step: 114
	action: tensor([[ 0.5137, -1.1861,  0.1877, -1.1437,  0.3287,  0.6348,  0.6552]],
       dtype=torch.float64)
	q_value: tensor([[-34.4518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9331973210061064, distance: 1.591090144263476 entropy 0.9217694997787476
epoch: 25, step: 115
	action: tensor([[ 1.6849, -0.6722,  0.9510, -0.2070,  0.7841,  0.4820,  0.7799]],
       dtype=torch.float64)
	q_value: tensor([[-40.7739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34784555717630816, distance: 1.3285464981145707 entropy 0.9217694997787476
epoch: 25, step: 116
	action: tensor([[ 1.6854, -1.5221, -0.0706,  0.2832,  0.6612,  0.3189,  1.0516]],
       dtype=torch.float64)
	q_value: tensor([[-42.3377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 117
	action: tensor([[-0.0041,  0.7307,  0.6108, -0.6449, -0.5188, -0.8870,  1.2807]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 118
	action: tensor([[-0.4918,  0.0817,  0.4743,  0.5816, -0.3527,  0.4864, -0.5863]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 119
	action: tensor([[-0.1854, -0.3547, -0.0615, -0.9107,  0.1509, -1.2785,  0.5215]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2832722919120907, distance: 1.296331582495431 entropy 0.9217694997787476
epoch: 25, step: 120
	action: tensor([[ 0.2349, -0.0223,  0.4753, -0.0111,  0.8331, -1.2353, -0.3348]],
       dtype=torch.float64)
	q_value: tensor([[-30.6235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26051051883381404, distance: 0.9840625256972867 entropy 0.9217694997787476
epoch: 25, step: 121
	action: tensor([[ 0.0516, -0.9405, -0.2730,  0.1320,  0.1048,  0.7696,  0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-28.4211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17335655104287429, distance: 1.2395718197131373 entropy 0.9217694997787476
epoch: 25, step: 122
	action: tensor([[ 0.6399, -0.6039,  0.5774, -0.2609,  0.7601, -0.5215,  0.2948]],
       dtype=torch.float64)
	q_value: tensor([[-33.5864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07948841551980856, distance: 1.1889557356595817 entropy 0.9217694997787476
epoch: 25, step: 123
	action: tensor([[ 0.8592, -0.1990,  0.5412, -0.6833,  0.5192,  0.7719,  0.3108]],
       dtype=torch.float64)
	q_value: tensor([[-30.6604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37951087159673635, distance: 0.9014129263009353 entropy 0.9217694997787476
epoch: 25, step: 124
	action: tensor([[ 1.5711, -2.0141, -0.2237,  0.8437, -0.1527,  1.2069,  0.6121]],
       dtype=torch.float64)
	q_value: tensor([[-35.0318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 25, step: 125
	action: tensor([[-0.1348, -0.8462,  0.0528, -0.8256, -0.9162, -0.1610,  0.4476]],
       dtype=torch.float64)
	q_value: tensor([[-31.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7935962922417825, distance: 1.5325654901037353 entropy 0.9217694997787476
epoch: 25, step: 126
	action: tensor([[ 1.6138,  0.1780,  0.4339, -0.4448,  1.1675,  0.6159, -0.7921]],
       dtype=torch.float64)
	q_value: tensor([[-34.4267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3053093484930095, distance: 0.9537892440682759 entropy 0.9217694997787476
epoch: 25, step: 127
	action: tensor([[ 0.2866, -1.2975,  0.0152,  0.2165, -0.0618, -0.6167,  1.1439]],
       dtype=torch.float64)
	q_value: tensor([[-37.6732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7127176307484431, distance: 1.4976129178165845 entropy 0.9217694997787476
LOSS epoch 25 actor 374.0474784217682 critic 209.08039491049038 
epoch: 26, step: 0
	action: tensor([[ 0.7733,  0.8560, -0.1117,  0.3743, -0.1203,  0.4733,  0.2524]],
       dtype=torch.float64)
	q_value: tensor([[-33.1619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 1
	action: tensor([[ 0.5878, -0.0800,  0.1465, -0.0767,  0.1809,  0.5287,  1.0325]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7642796565484267, distance: 0.5555910305380737 entropy 0.9217694997787476
epoch: 26, step: 2
	action: tensor([[ 1.3280, -0.7726, -0.3751,  0.1438,  0.4266,  0.0263,  0.4168]],
       dtype=torch.float64)
	q_value: tensor([[-30.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031926322586635525, distance: 1.162468085597102 entropy 0.9217694997787476
epoch: 26, step: 3
	action: tensor([[ 0.7530, -1.2595, -0.1336,  0.2922,  1.1730, -0.0710, -0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-32.1132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3901164873681884, distance: 1.349218509207851 entropy 0.9217694997787476
epoch: 26, step: 4
	action: tensor([[ 0.7907, -0.8190,  0.1744,  0.1478, -0.2185,  0.8012,  0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-31.9669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3432879588146387, distance: 0.9273510818829303 entropy 0.9217694997787476
epoch: 26, step: 5
	action: tensor([[ 2.4884, -0.6308, -0.1401,  0.4197, -0.6718, -0.1559,  1.1414]],
       dtype=torch.float64)
	q_value: tensor([[-31.7018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 6
	action: tensor([[-0.5223,  1.0523, -0.7349,  0.4070, -0.2476,  0.2001,  0.5006]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 7
	action: tensor([[ 1.0751,  0.4803, -0.4185,  0.1448,  0.5756,  0.0476, -0.1777]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 8
	action: tensor([[ 0.2916, -0.8562, -0.2422,  0.1380,  0.5255, -0.1609, -0.5408]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2709413692551157, distance: 1.2900883440405402 entropy 0.9217694997787476
epoch: 26, step: 9
	action: tensor([[ 0.2183,  0.0919, -0.3801, -0.7366,  0.2280,  0.3871, -0.7116]],
       dtype=torch.float64)
	q_value: tensor([[-25.9184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4265396120351007, distance: 0.8665794886489161 entropy 0.9217694997787476
epoch: 26, step: 10
	action: tensor([[-0.2109,  0.3500,  0.3145, -0.8656, -0.5859,  1.1124, -0.4912]],
       dtype=torch.float64)
	q_value: tensor([[-23.5115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09055629302019896, distance: 1.0913011285979697 entropy 0.9217694997787476
epoch: 26, step: 11
	action: tensor([[ 0.8520, -0.2349, -0.5115, -0.2405, -0.7818, -0.8921,  1.0587]],
       dtype=torch.float64)
	q_value: tensor([[-29.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3624400116966845, distance: 0.9137286033304741 entropy 0.9217694997787476
epoch: 26, step: 12
	action: tensor([[ 0.2705,  0.1767,  0.1145, -0.5055, -0.3736,  0.2745, -0.5745]],
       dtype=torch.float64)
	q_value: tensor([[-33.4865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48694402125971714, distance: 0.8196700557871642 entropy 0.9217694997787476
epoch: 26, step: 13
	action: tensor([[ 0.3660, -1.1911,  0.0419, -0.7135,  0.2285,  0.2084,  0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-23.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9643378642485743, distance: 1.6038538364860042 entropy 0.9217694997787476
epoch: 26, step: 14
	action: tensor([[ 1.3822, -1.1012, -0.6870, -0.5922,  0.8748,  0.0359, -0.3206]],
       dtype=torch.float64)
	q_value: tensor([[-30.6060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9425616108037425, distance: 1.594939060675082 entropy 0.9217694997787476
epoch: 26, step: 15
	action: tensor([[ 0.6823, -0.3764,  0.2440,  0.6612, -0.2164,  1.5832,  0.4964]],
       dtype=torch.float64)
	q_value: tensor([[-35.7279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9372192417433418, distance: 0.2867279116865159 entropy 0.9217694997787476
epoch: 26, step: 16
	action: tensor([[ 0.3909, -0.2137,  0.6370,  0.0373, -0.0236,  0.5064,  0.9360]],
       dtype=torch.float64)
	q_value: tensor([[-36.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6484401469808041, distance: 0.678510121288491 entropy 0.9217694997787476
epoch: 26, step: 17
	action: tensor([[ 0.7760, -1.0766,  0.1865, -0.0187,  0.3069, -0.3420,  1.5252]],
       dtype=torch.float64)
	q_value: tensor([[-29.4440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5542828250240401, distance: 1.4266640484908206 entropy 0.9217694997787476
epoch: 26, step: 18
	action: tensor([[ 1.0011, -0.2955,  0.5666,  0.8250,  1.4821, -0.0084,  0.4905]],
       dtype=torch.float64)
	q_value: tensor([[-36.0311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7717479943297656, distance: 0.5467187826980959 entropy 0.9217694997787476
epoch: 26, step: 19
	action: tensor([[-0.2990, -0.7184, -0.4977,  0.3288,  0.9319,  0.8745,  0.8943]],
       dtype=torch.float64)
	q_value: tensor([[-34.1117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08671034449631176, distance: 1.1929262462657986 entropy 0.9217694997787476
epoch: 26, step: 20
	action: tensor([[-0.0260,  1.9905,  0.2922, -0.1492, -0.1914,  0.6162, -0.6094]],
       dtype=torch.float64)
	q_value: tensor([[-33.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 21
	action: tensor([[-0.1096, -0.0694, -0.1147, -1.0133, -0.0565, -0.5717,  0.2723]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18049068452314043, distance: 1.2433344737477372 entropy 0.9217694997787476
epoch: 26, step: 22
	action: tensor([[ 0.2758,  0.8298, -0.3291,  0.3153, -0.5201, -0.1529,  0.9661]],
       dtype=torch.float64)
	q_value: tensor([[-24.6392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 23
	action: tensor([[-0.7136, -1.0871,  1.1329, -1.5277, -0.4602,  0.3975, -0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2852320372395822, distance: 1.7299033720213044 entropy 0.9217694997787476
epoch: 26, step: 24
	action: tensor([[ 0.1518, -0.2780,  0.2568, -0.8428,  0.4834, -0.1136,  0.3084]],
       dtype=torch.float64)
	q_value: tensor([[-34.7268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3532702232833427, distance: 1.3312173101612617 entropy 0.9217694997787476
epoch: 26, step: 25
	action: tensor([[ 1.3016, -0.8220, -0.1215, -0.3824,  0.0678,  0.6733,  0.4573]],
       dtype=torch.float64)
	q_value: tensor([[-25.3203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20476138649454345, distance: 1.2560508249255118 entropy 0.9217694997787476
epoch: 26, step: 26
	action: tensor([[ 1.5628, -0.3282, -0.5566, -0.5448, -0.0211, -0.2335,  0.6785]],
       dtype=torch.float64)
	q_value: tensor([[-34.7842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32435547127382747, distance: 1.3169187415193047 entropy 0.9217694997787476
epoch: 26, step: 27
	action: tensor([[ 0.2448, -1.1064, -0.8472,  0.8519, -0.3532, -0.5421, -0.6418]],
       dtype=torch.float64)
	q_value: tensor([[-33.8845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32161041954026404, distance: 1.315553214911979 entropy 0.9217694997787476
epoch: 26, step: 28
	action: tensor([[ 1.0242, -0.2767, -0.3510,  0.1065,  0.6701,  0.4862,  0.1706]],
       dtype=torch.float64)
	q_value: tensor([[-30.5166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6960997851620501, distance: 0.6308443195779228 entropy 0.9217694997787476
epoch: 26, step: 29
	action: tensor([[ 1.1138, -0.3984, -0.5994, -0.1288, -0.1504,  1.6111,  0.7619]],
       dtype=torch.float64)
	q_value: tensor([[-29.3869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7253159037598389, distance: 0.5997544111752807 entropy 0.9217694997787476
epoch: 26, step: 30
	action: tensor([[ 0.5941, -0.3700,  0.5476, -0.7702, -0.0624, -0.2731,  0.9647]],
       dtype=torch.float64)
	q_value: tensor([[-40.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17972680570984512, distance: 1.2429321365806272 entropy 0.9217694997787476
epoch: 26, step: 31
	action: tensor([[-0.3939, -1.4453,  0.4121, -1.1220, -0.1163,  0.1168,  0.4181]],
       dtype=torch.float64)
	q_value: tensor([[-29.6607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 32
	action: tensor([[ 1.0880, -1.9859,  0.4474,  0.3548,  0.8358, -0.2879,  1.3776]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 33
	action: tensor([[ 0.1907, -0.1789,  0.5048, -0.1135,  0.0364,  0.3238,  0.9305]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38660861276340885, distance: 0.8962424911370516 entropy 0.9217694997787476
epoch: 26, step: 34
	action: tensor([[ 0.0700, -2.1733, -0.2746, -0.0491,  0.9224, -0.3486,  1.7361]],
       dtype=torch.float64)
	q_value: tensor([[-27.7988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 35
	action: tensor([[ 0.2348,  1.2488, -0.2391, -0.7568, -0.1705,  0.2504,  0.8906]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 36
	action: tensor([[ 1.8633,  0.5620, -0.5445,  1.0995,  1.0094,  0.2111,  0.7072]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 37
	action: tensor([[ 0.6061, -0.6568,  0.0513, -1.1550,  0.5390, -0.5870, -0.8750]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5899936717217211, distance: 1.4429603285998676 entropy 0.9217694997787476
epoch: 26, step: 38
	action: tensor([[ 0.2043, -0.5963, -0.2223, -1.1861,  0.5066, -0.5013,  0.4769]],
       dtype=torch.float64)
	q_value: tensor([[-29.3025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5296206123392702, distance: 1.415300163740437 entropy 0.9217694997787476
epoch: 26, step: 39
	action: tensor([[-0.5182, -0.5280, -0.3113, -0.3410,  1.0524,  0.9293,  0.8023]],
       dtype=torch.float64)
	q_value: tensor([[-28.7933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.302777566126631, distance: 1.3061463134089726 entropy 0.9217694997787476
epoch: 26, step: 40
	action: tensor([[-0.0506, -0.0929,  0.7382,  0.0518, -0.2637, -0.7287,  0.3281]],
       dtype=torch.float64)
	q_value: tensor([[-32.7780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002698901784221386, distance: 1.1427989742815643 entropy 0.9217694997787476
epoch: 26, step: 41
	action: tensor([[ 0.1828,  0.4192,  0.0438, -0.0010, -0.2358,  0.8672,  0.7783]],
       dtype=torch.float64)
	q_value: tensor([[-24.6781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 42
	action: tensor([[ 0.1119,  0.0514, -0.9092, -1.3854, -0.6539, -0.5518,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 43
	action: tensor([[ 0.1553,  0.0733, -0.1233,  0.7383,  0.7189,  1.8105,  0.4932]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 44
	action: tensor([[-0.9961,  0.6608, -0.2136, -0.2051,  0.7872, -0.7052,  0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6505508312197992, distance: 1.4701821485177093 entropy 0.9217694997787476
epoch: 26, step: 45
	action: tensor([[ 0.8501, -0.4900,  1.7061, -1.1323, -1.0171, -0.4457, -0.5249]],
       dtype=torch.float64)
	q_value: tensor([[-23.3822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24354315606860644, distance: 0.995288008249466 entropy 0.9217694997787476
epoch: 26, step: 46
	action: tensor([[-0.0725, -0.2494,  0.2697, -0.0423,  0.9037, -0.6517,  0.8763]],
       dtype=torch.float64)
	q_value: tensor([[-34.3551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18657173898039803, distance: 1.2465327508646205 entropy 0.9217694997787476
epoch: 26, step: 47
	action: tensor([[ 0.7621,  0.3013,  1.0052,  0.5971, -1.2046,  0.8707,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[-26.1839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9692013392762534, distance: 0.20082723508808287 entropy 0.9217694997787476
epoch: 26, step: 48
	action: tensor([[ 0.2632,  0.4431,  0.3269, -0.7020,  0.4109,  0.7565,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[-34.0424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 49
	action: tensor([[ 1.0211,  0.1409, -1.0599,  0.2800,  0.3259, -0.2254, -0.1829]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 50
	action: tensor([[ 0.2552,  0.1614, -0.4952, -0.6900,  0.4112,  0.6271,  0.9389]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 51
	action: tensor([[-0.2737,  0.2290,  0.0611, -1.0081,  0.4745,  0.6078, -0.2726]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026588888535572286, distance: 1.1594578704128242 entropy 0.9217694997787476
epoch: 26, step: 52
	action: tensor([[-0.1189, -0.1449, -0.6332, -0.8776,  0.7949, -0.0792,  0.6529]],
       dtype=torch.float64)
	q_value: tensor([[-25.2377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020788485300639925, distance: 1.132387193492034 entropy 0.9217694997787476
epoch: 26, step: 53
	action: tensor([[-0.9289,  2.0032,  0.7070,  0.7329,  0.9080,  0.0502,  1.2426]],
       dtype=torch.float64)
	q_value: tensor([[-27.8179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 54
	action: tensor([[-0.0865, -1.1192,  0.9044, -0.7392, -0.7321, -0.6649, -0.0961]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0806631917587026, distance: 1.65065984092176 entropy 0.9217694997787476
epoch: 26, step: 55
	action: tensor([[-0.2901,  0.1386,  0.2817, -0.2137, -0.1619,  0.5106, -0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-30.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0603262828631429, distance: 1.109290345309773 entropy 0.9217694997787476
epoch: 26, step: 56
	action: tensor([[ 0.1345, -0.3530,  0.6567,  1.3062,  0.9776, -0.1007,  0.5351]],
       dtype=torch.float64)
	q_value: tensor([[-23.6265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 57
	action: tensor([[0.7252, 0.0231, 1.0865, 0.7260, 0.4552, 0.3223, 0.3593]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 58
	action: tensor([[ 0.9302, -0.7270,  0.3126,  0.5635, -0.3175,  0.9646, -0.0590]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7214828376352846, distance: 0.6039245357928978 entropy 0.9217694997787476
epoch: 26, step: 59
	action: tensor([[ 0.7250,  0.2406, -0.5012, -0.9693,  0.4634,  0.1864,  0.4746]],
       dtype=torch.float64)
	q_value: tensor([[-33.0858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5764850577011297, distance: 0.7447166471852074 entropy 0.9217694997787476
epoch: 26, step: 60
	action: tensor([[ 1.3391, -0.0646, -0.3151,  0.3745, -0.0065,  0.9395,  0.3247]],
       dtype=torch.float64)
	q_value: tensor([[-28.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9002068610395569, distance: 0.36149894566589125 entropy 0.9217694997787476
epoch: 26, step: 61
	action: tensor([[-0.2363, -0.6068,  0.5674, -0.8933,  0.6543,  0.4115,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-33.3849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8456800520343737, distance: 1.554658130594272 entropy 0.9217694997787476
epoch: 26, step: 62
	action: tensor([[ 2.6227e-01, -1.5803e+00, -6.7268e-01,  1.2254e-01,  3.2592e-04,
          8.2192e-01,  1.1619e+00]], dtype=torch.float64)
	q_value: tensor([[-27.4522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 63
	action: tensor([[-0.1690, -0.0435,  0.0328,  0.2131,  0.5095,  0.1633, -0.1827]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23881675869508878, distance: 0.9983924824837851 entropy 0.9217694997787476
epoch: 26, step: 64
	action: tensor([[ 0.6940,  0.1806, -0.3331, -0.2629, -0.2950, -0.7995, -0.6710]],
       dtype=torch.float64)
	q_value: tensor([[-22.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6399314922404474, distance: 0.6866718796763789 entropy 0.9217694997787476
epoch: 26, step: 65
	action: tensor([[-0.6409, -0.7654, -0.3422,  0.9890,  0.1257,  0.5575,  0.7789]],
       dtype=torch.float64)
	q_value: tensor([[-23.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4152645647820685, distance: 1.3613679116146924 entropy 0.9217694997787476
epoch: 26, step: 66
	action: tensor([[ 0.8240, -0.1722,  0.2758, -1.2096, -0.2179, -0.2922,  0.6982]],
       dtype=torch.float64)
	q_value: tensor([[-31.2796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12006453600426514, distance: 1.2110950151388071 entropy 0.9217694997787476
epoch: 26, step: 67
	action: tensor([[ 1.0428, -0.4741, -0.0865, -1.3157,  0.6568, -0.1411,  0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-30.2610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4556860351580765, distance: 1.3806721070250878 entropy 0.9217694997787476
epoch: 26, step: 68
	action: tensor([[-0.3622, -0.5323,  0.8115, -0.0309,  0.7846,  0.0660,  1.5431]],
       dtype=torch.float64)
	q_value: tensor([[-31.5014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5703255520935393, distance: 1.4340078938820475 entropy 0.9217694997787476
epoch: 26, step: 69
	action: tensor([[ 0.0471, -0.2444,  0.7241,  0.7579,  0.6622,  0.3416, -0.2648]],
       dtype=torch.float64)
	q_value: tensor([[-32.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7618648328949889, distance: 0.5584296394290952 entropy 0.9217694997787476
epoch: 26, step: 70
	action: tensor([[ 1.1896,  0.3248, -0.2378,  0.0658, -0.7525, -0.6200,  0.1187]],
       dtype=torch.float64)
	q_value: tensor([[-27.4779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7638417976993351, distance: 0.556106806095625 entropy 0.9217694997787476
epoch: 26, step: 71
	action: tensor([[ 0.8100, -0.6658, -0.1681,  0.7564,  0.2719,  0.0861,  0.4616]],
       dtype=torch.float64)
	q_value: tensor([[-28.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6371581247272531, distance: 0.6893112947169637 entropy 0.9217694997787476
epoch: 26, step: 72
	action: tensor([[-0.0678,  0.2995,  1.2048,  0.3889,  0.8078,  0.9277,  0.6073]],
       dtype=torch.float64)
	q_value: tensor([[-30.2044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 73
	action: tensor([[ 1.0504,  0.2043,  0.5235, -0.3215, -0.4352, -0.1803,  0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7208616904791462, distance: 0.6045975950161654 entropy 0.9217694997787476
epoch: 26, step: 74
	action: tensor([[ 1.5634,  0.5870, -0.8720, -0.0625, -0.2800,  0.1163, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[-26.4464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6867479272698309, distance: 0.6404771937881988 entropy 0.9217694997787476
epoch: 26, step: 75
	action: tensor([[ 0.3307,  0.8048,  0.5643,  0.0121,  1.1391,  0.0026, -0.1844]],
       dtype=torch.float64)
	q_value: tensor([[-30.3755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 76
	action: tensor([[ 0.3116, -0.8223,  0.3103,  0.9999,  0.9041,  0.8079,  0.6696]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7462706361294658, distance: 0.5764240050148407 entropy 0.9217694997787476
epoch: 26, step: 77
	action: tensor([[ 0.4960, -0.7358, -0.5634,  1.0519, -0.3643,  0.6215,  0.8381]],
       dtype=torch.float64)
	q_value: tensor([[-34.0421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5223102914442941, distance: 0.7909146786047022 entropy 0.9217694997787476
epoch: 26, step: 78
	action: tensor([[ 0.8580, -1.1865,  1.1429, -0.0251, -0.4440, -0.2439,  0.9626]],
       dtype=torch.float64)
	q_value: tensor([[-34.4680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6264070011551446, distance: 1.4593898144126864 entropy 0.9217694997787476
epoch: 26, step: 79
	action: tensor([[ 0.5405, -0.2411, -0.4059,  0.3491,  0.6789,  0.2029, -1.1492]],
       dtype=torch.float64)
	q_value: tensor([[-35.1185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7120565168849669, distance: 0.6140593098671551 entropy 0.9217694997787476
epoch: 26, step: 80
	action: tensor([[ 0.0917,  0.5879, -0.9664, -0.0469, -0.5476,  0.1115,  1.7378]],
       dtype=torch.float64)
	q_value: tensor([[-26.6667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 81
	action: tensor([[ 0.9037,  0.6232,  0.2021, -1.3394,  0.3378,  1.0097,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 82
	action: tensor([[-0.2170,  0.0135,  0.6545, -1.2631, -0.6702,  0.2319,  0.2616]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6205182727237077, distance: 1.456745413771968 entropy 0.9217694997787476
epoch: 26, step: 83
	action: tensor([[-0.0888,  0.4387,  0.2950, -0.3531, -1.2430,  1.1140, -0.5843]],
       dtype=torch.float64)
	q_value: tensor([[-28.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 84
	action: tensor([[-0.2608, -0.7947,  0.8100, -0.1292,  0.0029, -0.5369,  1.7362]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8761792126229446, distance: 1.5674505666302117 entropy 0.9217694997787476
epoch: 26, step: 85
	action: tensor([[ 0.7990, -0.7514,  1.0740, -0.6948,  0.6975,  0.4178,  0.5884]],
       dtype=torch.float64)
	q_value: tensor([[-32.7717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3158208852329141, distance: 1.3126685524011426 entropy 0.9217694997787476
epoch: 26, step: 86
	action: tensor([[ 0.5060, -0.0342,  0.6720, -0.6213,  1.0256,  0.7322,  0.7230]],
       dtype=torch.float64)
	q_value: tensor([[-32.1235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3699707701844567, distance: 0.9083161637259736 entropy 0.9217694997787476
epoch: 26, step: 87
	action: tensor([[-0.0974, -0.4080,  0.5961, -0.0969, -0.7564, -0.1651,  0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-31.2678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26839663103188705, distance: 1.2887961593026596 entropy 0.9217694997787476
epoch: 26, step: 88
	action: tensor([[-0.0812, -1.3324, -0.3101,  0.3298,  0.4684,  0.5750, -0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-26.7757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4792166374899396, distance: 1.3917863881473387 entropy 0.9217694997787476
epoch: 26, step: 89
	action: tensor([[ 0.8311,  0.1525, -0.7037, -0.7088,  0.3864, -0.3001,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[-30.1746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4961026998759682, distance: 0.8123210529028115 entropy 0.9217694997787476
epoch: 26, step: 90
	action: tensor([[-0.2687, -1.4992,  0.7261,  0.3898,  0.6922,  0.6583,  0.1823]],
       dtype=torch.float64)
	q_value: tensor([[-26.3400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26886288584683826, distance: 1.2890330143162032 entropy 0.9217694997787476
epoch: 26, step: 91
	action: tensor([[ 0.3868, -0.4414,  0.2169, -0.9208,  0.4209, -0.1685, -0.9308]],
       dtype=torch.float64)
	q_value: tensor([[-31.5458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41212653014013223, distance: 1.359857808674948 entropy 0.9217694997787476
epoch: 26, step: 92
	action: tensor([[ 0.0771, -1.0753,  0.5600, -0.7691,  0.2667, -0.0892, -0.3966]],
       dtype=torch.float64)
	q_value: tensor([[-25.9146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0607000036831429, distance: 1.642722021855411 entropy 0.9217694997787476
epoch: 26, step: 93
	action: tensor([[ 0.2961, -1.0061, -0.6099, -0.3538, -0.0274, -0.2378,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[-27.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6106611173107537, distance: 1.4523081701937177 entropy 0.9217694997787476
epoch: 26, step: 94
	action: tensor([[ 0.4650, -1.3572,  0.1814, -0.9300, -0.0904,  0.6286,  0.2708]],
       dtype=torch.float64)
	q_value: tensor([[-28.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 95
	action: tensor([[ 1.1352, -1.1769,  0.9267, -0.4224,  0.0319, -0.2156, -0.3553]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7016505388046341, distance: 1.4927665027148447 entropy 0.9217694997787476
epoch: 26, step: 96
	action: tensor([[ 0.2924,  1.2401,  0.5796, -0.7364,  0.5144,  0.2127,  1.0854]],
       dtype=torch.float64)
	q_value: tensor([[-31.0857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 97
	action: tensor([[ 0.0842, -0.2814,  0.3964, -0.8001, -0.1054,  0.3831,  0.3465]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26707880489291314, distance: 1.2881264749928019 entropy 0.9217694997787476
epoch: 26, step: 98
	action: tensor([[ 1.1446, -0.3840, -0.0753, -0.2990,  0.2128,  0.8163, -0.6852]],
       dtype=torch.float64)
	q_value: tensor([[-26.7683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.406909534079429, distance: 0.88128659576188 entropy 0.9217694997787476
epoch: 26, step: 99
	action: tensor([[ 0.0460,  0.4017,  0.6392, -0.2637, -0.0632,  0.4848, -0.0274]],
       dtype=torch.float64)
	q_value: tensor([[-30.0929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 100
	action: tensor([[-0.0064,  0.3876, -0.2934, -0.4218, -0.5746,  0.0458,  0.5488]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47773980258427784, distance: 0.8269898085884756 entropy 0.9217694997787476
epoch: 26, step: 101
	action: tensor([[ 0.6473,  0.0521, -0.6271, -0.5510,  0.4012,  0.4394,  0.1367]],
       dtype=torch.float64)
	q_value: tensor([[-25.0092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6778416461120977, distance: 0.6495183004781083 entropy 0.9217694997787476
epoch: 26, step: 102
	action: tensor([[ 0.5352, -0.7432, -0.9941,  0.0150,  0.0175,  0.7153,  0.1120]],
       dtype=torch.float64)
	q_value: tensor([[-26.5529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09424081437592091, distance: 1.0890882356249585 entropy 0.9217694997787476
epoch: 26, step: 103
	action: tensor([[ 0.1189,  0.3255,  0.9108, -1.4234,  0.6990,  0.5992,  0.4979]],
       dtype=torch.float64)
	q_value: tensor([[-30.6800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016601538312192465, distance: 1.153804091138874 entropy 0.9217694997787476
epoch: 26, step: 104
	action: tensor([[ 0.7192, -0.9073,  0.3521,  0.1588, -0.2294, -0.2497,  1.2879]],
       dtype=torch.float64)
	q_value: tensor([[-31.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18731877419658693, distance: 1.2469250816917536 entropy 0.9217694997787476
epoch: 26, step: 105
	action: tensor([[ 1.1274, -0.3944,  0.1240, -1.3999, -0.4172, -0.0298,  0.7140]],
       dtype=torch.float64)
	q_value: tensor([[-33.7803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35992182314540355, distance: 1.3344849025043117 entropy 0.9217694997787476
epoch: 26, step: 106
	action: tensor([[ 1.6075, -0.9595,  0.4649, -0.0773,  0.6095, -0.4588,  1.4417]],
       dtype=torch.float64)
	q_value: tensor([[-34.0121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46834482181491843, distance: 1.386662341438837 entropy 0.9217694997787476
epoch: 26, step: 107
	action: tensor([[ 1.1843, -1.2389, -0.3088, -0.3179,  1.1902,  1.0850,  0.7387]],
       dtype=torch.float64)
	q_value: tensor([[-38.6144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.568805472520971, distance: 1.4333136639901711 entropy 0.9217694997787476
epoch: 26, step: 108
	action: tensor([[ 1.4291,  0.3952, -0.5929,  0.2377, -0.6316,  1.1063,  0.4324]],
       dtype=torch.float64)
	q_value: tensor([[-41.4385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.978734532124581, distance: 0.16687610883045292 entropy 0.9217694997787476
epoch: 26, step: 109
	action: tensor([[ 0.8908, -0.1047, -0.5686, -0.1028,  0.0681, -0.7498,  1.5313]],
       dtype=torch.float64)
	q_value: tensor([[-35.7722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4588392575528567, distance: 0.8418211547147251 entropy 0.9217694997787476
epoch: 26, step: 110
	action: tensor([[ 1.2074, -1.4926, -0.8296,  0.0410,  0.7929,  0.2257,  0.2953]],
       dtype=torch.float64)
	q_value: tensor([[-34.6728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5339219287158181, distance: 1.4172886895506647 entropy 0.9217694997787476
epoch: 26, step: 111
	action: tensor([[ 1.1792, -0.3853, -0.4092, -0.7271, -0.2292, -0.1318,  0.1952]],
       dtype=torch.float64)
	q_value: tensor([[-36.2066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2204391188778927, distance: 1.2641969933668655 entropy 0.9217694997787476
epoch: 26, step: 112
	action: tensor([[ 1.7252, -1.4649,  0.9505,  0.0279,  0.2484,  1.0383,  0.3419]],
       dtype=torch.float64)
	q_value: tensor([[-29.5297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 113
	action: tensor([[ 0.2363, -0.7134,  1.4896, -0.1213, -0.0516,  0.1265,  1.1181]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1756262507312618, distance: 1.2407701325883929 entropy 0.9217694997787476
epoch: 26, step: 114
	action: tensor([[ 0.5167, -1.1537,  0.7474, -1.0592,  0.1886, -0.7915,  1.1415]],
       dtype=torch.float64)
	q_value: tensor([[-32.0179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7336720984014524, distance: 1.506746436904783 entropy 0.9217694997787476
epoch: 26, step: 115
	action: tensor([[ 0.7914, -1.8301, -0.2825, -1.2612,  0.1364, -0.6223,  0.3138]],
       dtype=torch.float64)
	q_value: tensor([[-34.0643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 26, step: 116
	action: tensor([[ 0.1390, -0.4010,  0.9052,  0.3010,  0.6637,  1.1435, -0.3026]],
       dtype=torch.float64)
	q_value: tensor([[-27.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5934625232842203, distance: 0.7296372316645665 entropy 0.9217694997787476
epoch: 26, step: 117
	action: tensor([[ 0.1734, -0.3766, -1.0379,  0.7076, -0.4168,  0.1871,  0.5160]],
       dtype=torch.float64)
	q_value: tensor([[-31.2145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07875756732137362, distance: 1.0983573485242146 entropy 0.9217694997787476
epoch: 26, step: 118
	action: tensor([[ 0.6885, -0.9357, -0.3906, -0.9171,  1.0291, -0.0818, -0.3884]],
       dtype=torch.float64)
	q_value: tensor([[-29.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.741086051963654, distance: 1.5099647590447158 entropy 0.9217694997787476
epoch: 26, step: 119
	action: tensor([[ 0.8677, -0.8363,  0.6864, -0.9523, -0.8041,  0.1486, -0.6358]],
       dtype=torch.float64)
	q_value: tensor([[-31.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5027888048486524, distance: 1.4028320155093637 entropy 0.9217694997787476
epoch: 26, step: 120
	action: tensor([[ 0.6312, -0.8686,  0.1247,  0.1444, -0.6805, -0.0677,  1.1913]],
       dtype=torch.float64)
	q_value: tensor([[-31.0220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10016672500282509, distance: 1.200289331446226 entropy 0.9217694997787476
epoch: 26, step: 121
	action: tensor([[ 0.2709, -0.7685, -0.4036,  0.6850,  0.8779,  0.9039, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-34.1540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5273410106531531, distance: 0.7867389544441141 entropy 0.9217694997787476
epoch: 26, step: 122
	action: tensor([[-0.0951, -0.2583,  0.1528, -0.8152, -0.5541,  0.1479, -0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-31.0065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4116163241075408, distance: 1.3596121258969809 entropy 0.9217694997787476
epoch: 26, step: 123
	action: tensor([[-0.0523, -0.5370, -0.0710, -0.0192,  0.5008, -0.2293,  0.5193]],
       dtype=torch.float64)
	q_value: tensor([[-24.4012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3179441472476314, distance: 1.3137272131400235 entropy 0.9217694997787476
epoch: 26, step: 124
	action: tensor([[-0.1648, -0.2719, -0.4248, -0.0281,  0.0502, -0.3204,  0.0801]],
       dtype=torch.float64)
	q_value: tensor([[-25.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14597055892214916, distance: 1.2250206890229562 entropy 0.9217694997787476
epoch: 26, step: 125
	action: tensor([[-0.0063, -0.2652, -1.1773, -0.6621,  0.1847, -0.2354, -0.3786]],
       dtype=torch.float64)
	q_value: tensor([[-22.1167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2786229432605162, distance: 0.971936419799418 entropy 0.9217694997787476
epoch: 26, step: 126
	action: tensor([[ 0.5448,  0.4328,  0.5268, -0.1878, -0.2048,  0.3270, -0.2751]],
       dtype=torch.float64)
	q_value: tensor([[-24.8239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8865898290144157, distance: 0.38537425297231154 entropy 0.9217694997787476
epoch: 26, step: 127
	action: tensor([[ 0.2902, -0.5390,  0.2302, -0.5400,  0.7480, -0.1647, -0.3319]],
       dtype=torch.float64)
	q_value: tensor([[-24.6622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3786282688709812, distance: 1.3436318284310278 entropy 0.9217694997787476
LOSS epoch 26 actor 353.43328153247506 critic 270.27252977508164 
epoch: 27, step: 0
	action: tensor([[ 0.1595,  0.1024,  0.0087, -0.5957, -0.1873,  0.7183, -0.0527]],
       dtype=torch.float64)
	q_value: tensor([[-24.7453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4138831927755653, distance: 0.8760901163872982 entropy 0.9217694997787476
epoch: 27, step: 1
	action: tensor([[-0.1316, -0.3975, -0.1879, -0.6387,  0.8055, -1.3649, -0.4584]],
       dtype=torch.float64)
	q_value: tensor([[-24.9559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42152596087270156, distance: 1.3643760617214848 entropy 0.9217694997787476
epoch: 27, step: 2
	action: tensor([[-1.0468, -0.3501, -0.2086,  0.4186,  0.6149,  0.2084,  0.2143]],
       dtype=torch.float64)
	q_value: tensor([[-26.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0008930317887899, distance: 1.6187084325726804 entropy 0.9217694997787476
epoch: 27, step: 3
	action: tensor([[ 0.6254,  0.0988, -0.6853, -0.1013, -0.4129, -0.3941, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-24.2733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7295697011816679, distance: 0.5950923498756179 entropy 0.9217694997787476
epoch: 27, step: 4
	action: tensor([[ 0.2588,  0.1494,  0.4616, -0.7357, -0.6062, -0.0851, -0.6401]],
       dtype=torch.float64)
	q_value: tensor([[-23.5082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2095832630222597, distance: 1.0173836575218351 entropy 0.9217694997787476
epoch: 27, step: 5
	action: tensor([[ 0.6892,  0.7887,  1.3972,  0.4401,  0.7228, -0.4059,  0.0312]],
       dtype=torch.float64)
	q_value: tensor([[-24.1133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 6
	action: tensor([[ 0.9559,  0.0382, -0.0211,  0.1465, -0.3897, -0.3002,  0.1234]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 7
	action: tensor([[ 0.5061, -0.2622,  0.1667,  0.3655, -0.4932, -0.4397,  0.4862]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5723299171114289, distance: 0.7483609690327195 entropy 0.9217694997787476
epoch: 27, step: 8
	action: tensor([[-0.3737, -0.7710,  0.0695,  0.3870, -0.4167,  0.0517,  0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-26.1161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4519113587570738, distance: 1.378880864556343 entropy 0.9217694997787476
epoch: 27, step: 9
	action: tensor([[-0.8943, -0.1325, -0.6370, -0.1250, -0.1635,  0.0630, -0.1706]],
       dtype=torch.float64)
	q_value: tensor([[-25.9235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8862799050944588, distance: 1.5716642057803172 entropy 0.9217694997787476
epoch: 27, step: 10
	action: tensor([[ 0.0873, -0.9837, -0.0207,  0.3701,  0.3733, -0.1208, -0.0570]],
       dtype=torch.float64)
	q_value: tensor([[-22.2028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3553609743310471, distance: 1.3322452533488653 entropy 0.9217694997787476
epoch: 27, step: 11
	action: tensor([[-0.2049, -0.0149, -0.3351, -0.0896, -1.0353,  0.4082, -0.5160]],
       dtype=torch.float64)
	q_value: tensor([[-25.7796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05762510876429894, distance: 1.1768539507663307 entropy 0.9217694997787476
epoch: 27, step: 12
	action: tensor([[-0.8829, -0.5918, -0.5330, -0.0263, -1.8833,  0.2143,  1.2985]],
       dtype=torch.float64)
	q_value: tensor([[-25.2855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2154095036817565, distance: 1.7032707973305308 entropy 0.9217694997787476
epoch: 27, step: 13
	action: tensor([[ 0.4391, -0.5120, -0.1314, -0.8461, -0.7626, -0.4474,  0.3096]],
       dtype=torch.float64)
	q_value: tensor([[-39.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2721565860191013, distance: 1.290704958799425 entropy 0.9217694997787476
epoch: 27, step: 14
	action: tensor([[1.0721, 0.4730, 0.6710, 0.1167, 1.2247, 0.1936, 0.3885]],
       dtype=torch.float64)
	q_value: tensor([[-28.0183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9163516608057776, distance: 0.3309675038956979 entropy 0.9217694997787476
epoch: 27, step: 15
	action: tensor([[ 0.1453, -0.8791,  0.5363,  0.0663,  0.8316, -1.3873,  0.7348]],
       dtype=torch.float64)
	q_value: tensor([[-29.7610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40508474875765454, distance: 1.3564630034391003 entropy 0.9217694997787476
epoch: 27, step: 16
	action: tensor([[ 0.8781, -0.2354,  0.6349, -0.4011,  0.2068,  0.1742,  0.3493]],
       dtype=torch.float64)
	q_value: tensor([[-29.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3538299848120202, distance: 0.9198777067764946 entropy 0.9217694997787476
epoch: 27, step: 17
	action: tensor([[ 0.6302, -0.7005,  0.3154,  0.4131,  0.5157,  0.3423, -0.8131]],
       dtype=torch.float64)
	q_value: tensor([[-26.5491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4369322969019971, distance: 0.8586911797977289 entropy 0.9217694997787476
epoch: 27, step: 18
	action: tensor([[ 0.6666, -0.6743, -0.1257,  0.6371, -0.6221, -0.0276,  0.2151]],
       dtype=torch.float64)
	q_value: tensor([[-28.0463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.525121783986127, distance: 0.788583738429215 entropy 0.9217694997787476
epoch: 27, step: 19
	action: tensor([[ 0.3394,  0.1460,  0.5859, -0.1963,  0.0858,  0.1629,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-28.7621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.617826711593709, distance: 0.7074355137131493 entropy 0.9217694997787476
epoch: 27, step: 20
	action: tensor([[ 0.2021, -1.1402,  0.1206, -1.3568,  0.7454,  0.0216, -0.4339]],
       dtype=torch.float64)
	q_value: tensor([[-23.1220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9453319164936229, distance: 1.5960759344162745 entropy 0.9217694997787476
epoch: 27, step: 21
	action: tensor([[ 0.5835,  0.1373, -0.6070,  0.0625, -0.4781, -0.4617, -0.3017]],
       dtype=torch.float64)
	q_value: tensor([[-31.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7915567857857045, distance: 0.5224570260369982 entropy 0.9217694997787476
epoch: 27, step: 22
	action: tensor([[ 0.7443,  0.0233, -0.2344, -1.1326,  0.4806,  0.2096, -0.1780]],
       dtype=torch.float64)
	q_value: tensor([[-22.8291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23077873192443044, distance: 1.0036501064681727 entropy 0.9217694997787476
epoch: 27, step: 23
	action: tensor([[ 0.0318, -0.4406,  0.3287,  0.9353, -0.6733,  0.6724,  0.5835]],
       dtype=torch.float64)
	q_value: tensor([[-26.7868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.693700409108936, distance: 0.6333297683848785 entropy 0.9217694997787476
epoch: 27, step: 24
	action: tensor([[ 0.9418,  0.2455,  0.1612, -0.2234,  0.3315, -0.1261,  0.5657]],
       dtype=torch.float64)
	q_value: tensor([[-30.5304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8113414824522163, distance: 0.49704407596861006 entropy 0.9217694997787476
epoch: 27, step: 25
	action: tensor([[ 0.3828,  0.1024, -0.4725, -0.7972, -0.0677,  0.9870, -0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-25.7171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6485430370077452, distance: 0.6784108252045927 entropy 0.9217694997787476
epoch: 27, step: 26
	action: tensor([[-0.9576,  0.5827, -0.6036,  0.0040,  0.2299,  0.7218, -0.1241]],
       dtype=torch.float64)
	q_value: tensor([[-27.5288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 27
	action: tensor([[-0.7289, -0.5022,  0.3896, -0.0943,  0.5091,  0.7981,  0.2593]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5763244114459294, distance: 1.4367443365612556 entropy 0.9217694997787476
epoch: 27, step: 28
	action: tensor([[ 0.4415,  0.0434,  0.0295, -0.2298, -0.5770, -0.2777,  1.0855]],
       dtype=torch.float64)
	q_value: tensor([[-26.9142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5233013882456059, distance: 0.7900937691255658 entropy 0.9217694997787476
epoch: 27, step: 29
	action: tensor([[ 0.6005, -0.2941,  0.5964, -0.7552, -0.3620, -0.7424, -0.3098]],
       dtype=torch.float64)
	q_value: tensor([[-28.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14015079374527328, distance: 1.2219061206531638 entropy 0.9217694997787476
epoch: 27, step: 30
	action: tensor([[-0.3195, -0.9976,  0.2658, -0.6231,  0.2979, -0.6345,  0.5170]],
       dtype=torch.float64)
	q_value: tensor([[-25.3995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0654793296568656, distance: 1.6446258790569495 entropy 0.9217694997787476
epoch: 27, step: 31
	action: tensor([[-0.7851, -0.4672,  0.8625, -1.0626, -0.1531,  0.2199,  1.2175]],
       dtype=torch.float64)
	q_value: tensor([[-27.7064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3672846275646546, distance: 1.760686086185687 entropy 0.9217694997787476
epoch: 27, step: 32
	action: tensor([[ 1.1690, -0.5609,  0.0012, -1.1376, -0.2376, -0.6794,  0.3889]],
       dtype=torch.float64)
	q_value: tensor([[-31.9345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4883328928853976, distance: 1.3960685167215527 entropy 0.9217694997787476
epoch: 27, step: 33
	action: tensor([[ 0.3590,  0.2170, -0.9111, -0.1239,  0.9482, -0.2664,  0.3392]],
       dtype=torch.float64)
	q_value: tensor([[-30.7208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6958733402172468, distance: 0.6310793060954584 entropy 0.9217694997787476
epoch: 27, step: 34
	action: tensor([[ 0.5227, -0.3659,  0.1045, -0.2571,  0.8338,  0.1068,  0.2574]],
       dtype=torch.float64)
	q_value: tensor([[-25.1054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22054743426660728, distance: 1.0103027584399895 entropy 0.9217694997787476
epoch: 27, step: 35
	action: tensor([[-0.6588,  0.0384, -0.4455, -0.2685,  1.3986, -0.1878,  0.9160]],
       dtype=torch.float64)
	q_value: tensor([[-25.4656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5513313451092754, distance: 1.4253088346552296 entropy 0.9217694997787476
epoch: 27, step: 36
	action: tensor([[ 0.8218,  0.8130, -0.5225, -0.1738,  0.0433, -0.2227, -0.6258]],
       dtype=torch.float64)
	q_value: tensor([[-28.3437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 37
	action: tensor([[1.0407, 0.3153, 0.1579, 0.7272, 0.0502, 0.2022, 1.5989]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 38
	action: tensor([[-0.4532, -0.2364,  0.8646, -0.1380, -0.4239, -0.6472, -0.2678]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6510619437962188, distance: 1.4704097605392061 entropy 0.9217694997787476
epoch: 27, step: 39
	action: tensor([[ 0.1280, -0.1470, -0.6037,  1.0827,  0.2750,  0.4451,  0.4068]],
       dtype=torch.float64)
	q_value: tensor([[-24.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 40
	action: tensor([[ 0.0145, -1.6836, -0.2673, -0.0343, -0.8177, -1.3777,  0.8299]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2763763413564544, distance: 1.292843826838451 entropy 0.9217694997787476
epoch: 27, step: 41
	action: tensor([[ 0.9063, -0.8056, -0.0055, -1.2513, -0.5907, -0.6040,  1.3413]],
       dtype=torch.float64)
	q_value: tensor([[-33.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6243518101343388, distance: 1.4584674521272123 entropy 0.9217694997787476
epoch: 27, step: 42
	action: tensor([[-0.1638, -1.4654,  0.1445, -0.0486,  0.6048, -0.5908, -0.6439]],
       dtype=torch.float64)
	q_value: tensor([[-36.4353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 43
	action: tensor([[ 0.7000, -0.5619, -0.5247,  0.0630,  0.2689,  0.1334, -0.1145]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2647766673163501, distance: 0.9812198691579777 entropy 0.9217694997787476
epoch: 27, step: 44
	action: tensor([[ 0.3999, -0.2272,  0.1382,  0.1848, -0.4073,  0.2687, -1.0916]],
       dtype=torch.float64)
	q_value: tensor([[-25.3458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6089351817361599, distance: 0.7156176882788028 entropy 0.9217694997787476
epoch: 27, step: 45
	action: tensor([[ 0.3185, -0.6521, -0.3896,  0.7854,  0.2209, -0.7871,  0.2424]],
       dtype=torch.float64)
	q_value: tensor([[-24.5647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16456433849260277, distance: 1.0459554763176044 entropy 0.9217694997787476
epoch: 27, step: 46
	action: tensor([[-0.0650, -0.3687,  0.1533,  0.5151, -0.3096, -1.0634, -0.6591]],
       dtype=torch.float64)
	q_value: tensor([[-26.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12418359544614754, distance: 1.070935292805092 entropy 0.9217694997787476
epoch: 27, step: 47
	action: tensor([[ 0.0407,  0.3564,  0.3794, -0.3396, -0.0142, -0.1397, -0.6177]],
       dtype=torch.float64)
	q_value: tensor([[-25.4451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3663812672152936, distance: 0.9108999905326834 entropy 0.9217694997787476
epoch: 27, step: 48
	action: tensor([[-0.8536, -0.9408,  0.0951,  0.2812,  1.3740, -0.1152, -0.7799]],
       dtype=torch.float64)
	q_value: tensor([[-21.4519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1937417468727176, distance: 1.6949209322027199 entropy 0.9217694997787476
epoch: 27, step: 49
	action: tensor([[ 0.2223, -0.7588,  0.1820, -1.1976,  0.0443, -1.0591,  0.9484]],
       dtype=torch.float64)
	q_value: tensor([[-28.6903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5492521520373101, distance: 1.424353369565514 entropy 0.9217694997787476
epoch: 27, step: 50
	action: tensor([[ 0.9409, -0.4365,  0.3661, -0.1171,  0.4972,  1.3436,  0.2330]],
       dtype=torch.float64)
	q_value: tensor([[-30.4780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5422722788548449, distance: 0.7742127174030593 entropy 0.9217694997787476
epoch: 27, step: 51
	action: tensor([[ 0.1515, -0.4093,  0.3786, -0.6560,  0.0161,  0.3412,  0.1837]],
       dtype=torch.float64)
	q_value: tensor([[-32.8326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25845557253433027, distance: 1.2837357546688668 entropy 0.9217694997787476
epoch: 27, step: 52
	action: tensor([[-0.3600, -0.9939,  0.0954,  0.0709,  0.5394,  0.9716, -0.1092]],
       dtype=torch.float64)
	q_value: tensor([[-25.1045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30357168528599887, distance: 1.3065443390672111 entropy 0.9217694997787476
epoch: 27, step: 53
	action: tensor([[ 1.0550, -0.4053,  0.5976, -0.2521, -0.4859, -0.3568,  0.1190]],
       dtype=torch.float64)
	q_value: tensor([[-28.8399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12388617906747601, distance: 1.0711171155356822 entropy 0.9217694997787476
epoch: 27, step: 54
	action: tensor([[ 0.6981, -0.4110,  0.1848, -0.1619, -0.2344, -0.3837,  0.5078]],
       dtype=torch.float64)
	q_value: tensor([[-27.2942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12275036699792374, distance: 1.071811199838246 entropy 0.9217694997787476
epoch: 27, step: 55
	action: tensor([[-0.2353,  0.3230,  0.3305,  0.2285,  0.1273,  0.1739,  0.7085]],
       dtype=torch.float64)
	q_value: tensor([[-26.0819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 56
	action: tensor([[-0.2869, -0.1100, -0.3081,  1.0135,  0.2670,  0.9103, -0.4820]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 57
	action: tensor([[-0.5667, -0.6754,  0.4550,  0.5439,  0.2320,  0.1362, -0.3365]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37437047299328596, distance: 1.3415553680173535 entropy 0.9217694997787476
epoch: 27, step: 58
	action: tensor([[ 0.0743,  0.6571, -0.6016, -0.5269,  0.3178,  1.2857,  1.1649]],
       dtype=torch.float64)
	q_value: tensor([[-24.9907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 59
	action: tensor([[ 0.0730, -0.8009,  0.2064,  0.6637,  0.0027, -0.6058,  0.1111]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05470753410551832, distance: 1.175229589511363 entropy 0.9217694997787476
epoch: 27, step: 60
	action: tensor([[ 0.6450,  0.3369,  0.2688, -0.5776, -0.5126, -0.4668,  0.4391]],
       dtype=torch.float64)
	q_value: tensor([[-25.8243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6126562597222435, distance: 0.7122049113131067 entropy 0.9217694997787476
epoch: 27, step: 61
	action: tensor([[-0.4269, -0.7552, -0.6756, -0.5310,  0.1975, -0.3443,  0.8908]],
       dtype=torch.float64)
	q_value: tensor([[-25.6837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.602676565009991, distance: 1.4487039242604025 entropy 0.9217694997787476
epoch: 27, step: 62
	action: tensor([[ 1.4047, -0.2414,  0.3222,  0.8184, -0.3765, -0.9862,  0.5177]],
       dtype=torch.float64)
	q_value: tensor([[-28.3986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45366417599375763, distance: 0.8458367152395119 entropy 0.9217694997787476
epoch: 27, step: 63
	action: tensor([[ 0.6857, -0.1896, -1.2696, -0.5412, -0.5629,  1.7312, -0.4259]],
       dtype=torch.float64)
	q_value: tensor([[-31.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5319963033376103, distance: 0.7828550095678466 entropy 0.9217694997787476
epoch: 27, step: 64
	action: tensor([[-0.0930, -1.1560,  0.7856, -0.2942, -0.2253, -0.1169, -0.8084]],
       dtype=torch.float64)
	q_value: tensor([[-35.9358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.030342187628607, distance: 1.6305770019565304 entropy 0.9217694997787476
epoch: 27, step: 65
	action: tensor([[ 0.1951, -0.1003,  0.3706,  0.2045, -0.6928, -0.4017,  0.2415]],
       dtype=torch.float64)
	q_value: tensor([[-28.0704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4474137446143165, distance: 0.8506614116801658 entropy 0.9217694997787476
epoch: 27, step: 66
	action: tensor([[-0.5899,  0.1351, -0.7891, -0.4270, -0.3855,  0.0369, -0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-24.7062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19071227841246952, distance: 1.2487057434767899 entropy 0.9217694997787476
epoch: 27, step: 67
	action: tensor([[ 0.5469, -0.4625, -0.7569, -0.0846, -0.6954, -0.7586,  1.5053]],
       dtype=torch.float64)
	q_value: tensor([[-22.2301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962041696549419, distance: 0.9600194634662488 entropy 0.9217694997787476
epoch: 27, step: 68
	action: tensor([[ 1.0030, -0.7912,  0.3224,  1.3730,  0.2241,  0.1872,  0.2441]],
       dtype=torch.float64)
	q_value: tensor([[-35.4154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7866221574457, distance: 0.5286051051314029 entropy 0.9217694997787476
epoch: 27, step: 69
	action: tensor([[ 0.5383,  1.1230,  0.9947,  0.0226, -0.3981,  0.4891,  0.7064]],
       dtype=torch.float64)
	q_value: tensor([[-32.7334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 70
	action: tensor([[ 0.7278, -0.0725,  0.3869,  0.4092, -0.2041, -0.2627,  0.4942]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8221608115391805, distance: 0.4825812313203516 entropy 0.9217694997787476
epoch: 27, step: 71
	action: tensor([[-0.0180,  0.1115,  1.0163, -0.4532,  0.6911,  0.0016,  0.8489]],
       dtype=torch.float64)
	q_value: tensor([[-25.8005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056027470896900744, distance: 1.1118248363196328 entropy 0.9217694997787476
epoch: 27, step: 72
	action: tensor([[ 1.7575, -0.6114, -1.0627,  0.0456, -0.6021, -0.6211, -0.3171]],
       dtype=torch.float64)
	q_value: tensor([[-27.1001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2636142346713355, distance: 1.2863642091233844 entropy 0.9217694997787476
epoch: 27, step: 73
	action: tensor([[ 0.2965, -0.2626, -1.0590, -0.9760, -0.1509, -0.0145, -0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-32.8473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3356607804584789, distance: 0.9327207515851379 entropy 0.9217694997787476
epoch: 27, step: 74
	action: tensor([[ 0.1290,  0.0093, -0.1466, -0.2920, -0.6575, -0.7430,  0.5687]],
       dtype=torch.float64)
	q_value: tensor([[-26.4971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39175858460534807, distance: 0.8924721801653732 entropy 0.9217694997787476
epoch: 27, step: 75
	action: tensor([[-0.8705, -0.4300,  0.1635, -0.1579, -0.3473,  0.4491,  0.2971]],
       dtype=torch.float64)
	q_value: tensor([[-25.5996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9788144342553791, distance: 1.609752944201975 entropy 0.9217694997787476
epoch: 27, step: 76
	action: tensor([[ 0.0646, -0.4320,  0.4211, -0.3413,  0.6020, -0.4258,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[-25.6215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33545734158581153, distance: 1.3224269865849874 entropy 0.9217694997787476
epoch: 27, step: 77
	action: tensor([[-0.0788, -0.9845,  0.2039, -0.0680,  0.0513, -1.0505,  0.0534]],
       dtype=torch.float64)
	q_value: tensor([[-23.5495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.757005031085241, distance: 1.5168519555768416 entropy 0.9217694997787476
epoch: 27, step: 78
	action: tensor([[ 0.8605,  0.1398,  1.1033,  0.1839,  0.4608, -0.1630, -0.2991]],
       dtype=torch.float64)
	q_value: tensor([[-26.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8438406177207902, distance: 0.452210596141672 entropy 0.9217694997787476
epoch: 27, step: 79
	action: tensor([[ 1.0122, -1.9715,  0.1877, -0.3015, -0.2606, -1.2156, -0.8866]],
       dtype=torch.float64)
	q_value: tensor([[-26.8591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 80
	action: tensor([[ 1.9894,  1.0689,  0.2660,  0.4745, -0.4246,  0.6652,  0.5461]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 81
	action: tensor([[-0.3668, -0.5436, -0.6424,  0.2138, -0.1362, -1.1949, -0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25116247648227263, distance: 1.2800105488495224 entropy 0.9217694997787476
epoch: 27, step: 82
	action: tensor([[ 0.0055,  0.3629, -0.0223,  1.0411, -0.7018, -0.0523, -0.6789]],
       dtype=torch.float64)
	q_value: tensor([[-25.6794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 83
	action: tensor([[ 0.0210, -0.1359,  0.4933, -0.8187, -0.5848,  0.2362,  1.5528]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2999491262007987, distance: 1.304727666109772 entropy 0.9217694997787476
epoch: 27, step: 84
	action: tensor([[ 0.1823, -1.1079,  1.2099,  0.1973, -0.7246, -0.1495,  0.4727]],
       dtype=torch.float64)
	q_value: tensor([[-33.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5492890918900613, distance: 1.4243703503681664 entropy 0.9217694997787476
epoch: 27, step: 85
	action: tensor([[-0.3171,  1.4415, -0.0326, -0.5618, -0.5332, -0.3037,  0.2077]],
       dtype=torch.float64)
	q_value: tensor([[-31.1982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 86
	action: tensor([[ 1.0595, -0.7155,  1.2856,  0.8270, -0.5065, -0.1777,  1.6668]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03765054446865235, distance: 1.1225949797968402 entropy 0.9217694997787476
epoch: 27, step: 87
	action: tensor([[ 1.3950, -0.1001, -0.2207,  0.0193, -0.7417,  0.4282,  1.1487]],
       dtype=torch.float64)
	q_value: tensor([[-37.4098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5905317859213111, distance: 0.732262493970818 entropy 0.9217694997787476
epoch: 27, step: 88
	action: tensor([[-0.2147, -0.2826, -0.0165, -0.5833,  1.4487,  0.1327, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-35.7899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36796703954444254, distance: 1.3384264481601729 entropy 0.9217694997787476
epoch: 27, step: 89
	action: tensor([[ 0.2993, -0.1606, -0.1535, -0.6039,  0.1013, -0.7156,  0.5192]],
       dtype=torch.float64)
	q_value: tensor([[-27.3440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00666313613094327, distance: 1.1481503851064598 entropy 0.9217694997787476
epoch: 27, step: 90
	action: tensor([[ 0.4950,  0.2967,  1.2982, -0.5368,  0.2919,  0.3523,  0.4099]],
       dtype=torch.float64)
	q_value: tensor([[-24.4969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.675596494240708, distance: 0.6517776479770905 entropy 0.9217694997787476
epoch: 27, step: 91
	action: tensor([[ 1.7260, -0.1785, -0.9709,  0.9387, -0.0604,  0.5331,  0.7487]],
       dtype=torch.float64)
	q_value: tensor([[-28.0426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9067623533362996, distance: 0.34942368940873764 entropy 0.9217694997787476
epoch: 27, step: 92
	action: tensor([[ 0.8923,  0.8016, -0.7837, -1.0264,  0.5977,  1.0646, -0.2725]],
       dtype=torch.float64)
	q_value: tensor([[-36.6669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9110602211915458, distance: 0.3412751889547547 entropy 0.9217694997787476
epoch: 27, step: 93
	action: tensor([[ 0.1073, -0.6126, -0.4095, -0.4768, -0.4824,  0.4019, -0.8905]],
       dtype=torch.float64)
	q_value: tensor([[-30.6552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2911034462367834, distance: 1.3002809906027437 entropy 0.9217694997787476
epoch: 27, step: 94
	action: tensor([[-0.0314,  0.1379,  0.0923,  1.1076, -0.1133, -0.2870,  1.5860]],
       dtype=torch.float64)
	q_value: tensor([[-25.8365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 95
	action: tensor([[ 0.7637, -0.2326,  0.2360,  0.0154, -0.2008,  1.5488, -0.4499]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8970236491640842, distance: 0.36721925240749814 entropy 0.9217694997787476
epoch: 27, step: 96
	action: tensor([[ 0.8182, -0.8390, -0.5202,  0.7429, -0.2300,  0.9572, -0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-32.6355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6377260152136033, distance: 0.688771656598034 entropy 0.9217694997787476
epoch: 27, step: 97
	action: tensor([[ 0.5625, -0.0813,  0.7533,  0.6287,  0.6056, -0.6136,  0.0523]],
       dtype=torch.float64)
	q_value: tensor([[-31.8283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.852450781021594, distance: 0.4395670708122993 entropy 0.9217694997787476
epoch: 27, step: 98
	action: tensor([[-0.0349,  1.0501, -0.4061, -0.4427, -1.0265, -0.0688,  0.5980]],
       dtype=torch.float64)
	q_value: tensor([[-25.5827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 99
	action: tensor([[ 0.0318, -0.9879,  0.3117, -0.0145,  0.5376,  0.0383,  0.5336]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5757236255298854, distance: 1.4364705166490899 entropy 0.9217694997787476
epoch: 27, step: 100
	action: tensor([[ 0.5335,  0.3506,  0.2137, -1.3739, -0.0033,  0.6606,  0.5944]],
       dtype=torch.float64)
	q_value: tensor([[-27.4867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.463363952658565, distance: 0.8382944952251455 entropy 0.9217694997787476
epoch: 27, step: 101
	action: tensor([[ 1.2212, -0.9295,  0.2849, -0.2128, -0.2115, -0.3706,  0.1394]],
       dtype=torch.float64)
	q_value: tensor([[-31.1366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5390190862573099, distance: 1.4196415316891564 entropy 0.9217694997787476
epoch: 27, step: 102
	action: tensor([[ 0.8346, -0.3817,  0.8243,  0.5206,  1.4836,  0.7408,  0.6663]],
       dtype=torch.float64)
	q_value: tensor([[-29.7143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45380552604737023, distance: 0.8457272891374829 entropy 0.9217694997787476
epoch: 27, step: 103
	action: tensor([[ 0.3185, -0.1533,  0.4723,  0.2413,  0.4252, -0.2799,  2.0557]],
       dtype=torch.float64)
	q_value: tensor([[-34.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5349311410224991, distance: 0.7803965186308428 entropy 0.9217694997787476
epoch: 27, step: 104
	action: tensor([[-0.3854, -0.1480, -1.0020,  0.7105, -0.2771, -0.3610,  0.3314]],
       dtype=torch.float64)
	q_value: tensor([[-34.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3223748016908521, distance: 1.315933599336256 entropy 0.9217694997787476
epoch: 27, step: 105
	action: tensor([[ 0.5392, -0.8975,  0.3857, -0.3692,  0.6665, -0.0374, -0.2017]],
       dtype=torch.float64)
	q_value: tensor([[-26.0779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5378073498000018, distance: 1.4190825489734198 entropy 0.9217694997787476
epoch: 27, step: 106
	action: tensor([[ 0.3009, -0.0899,  0.0363,  0.5108,  0.6042, -0.9100, -0.3962]],
       dtype=torch.float64)
	q_value: tensor([[-26.5723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5104348309793143, distance: 0.8006854729609577 entropy 0.9217694997787476
epoch: 27, step: 107
	action: tensor([[ 0.0513, -0.2058,  0.6299, -1.1032,  0.4761,  0.3392, -0.3106]],
       dtype=torch.float64)
	q_value: tensor([[-23.3202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4250223075278774, distance: 1.3660529224219522 entropy 0.9217694997787476
epoch: 27, step: 108
	action: tensor([[ 1.1285,  0.0375, -0.8951,  0.1410,  1.2378,  0.0839,  0.5462]],
       dtype=torch.float64)
	q_value: tensor([[-25.6388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8006053150106136, distance: 0.5109912718792958 entropy 0.9217694997787476
epoch: 27, step: 109
	action: tensor([[ 0.9662, -0.1383, -0.2339, -0.0908,  0.0678, -0.7118,  0.0916]],
       dtype=torch.float64)
	q_value: tensor([[-31.0096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39456272072093734, distance: 0.8904125501512162 entropy 0.9217694997787476
epoch: 27, step: 110
	action: tensor([[ 0.0854, -0.0060, -0.7920,  0.3490,  0.4290,  0.2988, -0.4501]],
       dtype=torch.float64)
	q_value: tensor([[-24.5592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42119415118718095, distance: 0.8706089927159383 entropy 0.9217694997787476
epoch: 27, step: 111
	action: tensor([[ 0.8189, -1.2248,  0.2831, -0.0137,  0.9194,  1.1982, -0.1836]],
       dtype=torch.float64)
	q_value: tensor([[-22.2384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22129188519114096, distance: 1.2646385870266232 entropy 0.9217694997787476
epoch: 27, step: 112
	action: tensor([[ 0.7473,  0.3674, -0.4536,  0.2984,  0.3969, -0.5928,  0.3668]],
       dtype=torch.float64)
	q_value: tensor([[-35.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9340598308767749, distance: 0.29385407768879057 entropy 0.9217694997787476
epoch: 27, step: 113
	action: tensor([[ 0.5594,  0.4077, -0.0072, -0.3386, -0.9167, -0.0951, -0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-24.3245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8362572732824687, distance: 0.46306046373394416 entropy 0.9217694997787476
epoch: 27, step: 114
	action: tensor([[-0.7134, -0.3759, -0.2008, -1.0364, -0.1584, -0.1777, -0.8180]],
       dtype=torch.float64)
	q_value: tensor([[-25.2637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7136796092285553, distance: 1.4980334393125672 entropy 0.9217694997787476
epoch: 27, step: 115
	action: tensor([[-0.2971,  0.0066, -0.4870, -0.0565,  0.2855, -1.6378, -0.2706]],
       dtype=torch.float64)
	q_value: tensor([[-24.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05673432772330078, distance: 1.1114084851393953 entropy 0.9217694997787476
epoch: 27, step: 116
	action: tensor([[ 0.5281, -1.4196, -0.6124, -0.1556, -0.5668,  0.2688,  0.0464]],
       dtype=torch.float64)
	q_value: tensor([[-24.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6892261262145156, distance: 1.4873068842682742 entropy 0.9217694997787476
epoch: 27, step: 117
	action: tensor([[-0.0530, -0.3415,  0.0058, -0.4963,  0.5601, -0.0734,  0.3562]],
       dtype=torch.float64)
	q_value: tensor([[-31.7498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3255529722580741, distance: 1.3175139952843955 entropy 0.9217694997787476
epoch: 27, step: 118
	action: tensor([[ 0.1155, -0.7905,  0.2019,  0.2898,  0.0743, -0.2172,  0.3390]],
       dtype=torch.float64)
	q_value: tensor([[-23.6682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19591102626568202, distance: 1.2514287505441872 entropy 0.9217694997787476
epoch: 27, step: 119
	action: tensor([[-0.8967,  0.2706, -0.3379, -0.6877, -1.2641, -0.1151, -0.0949]],
       dtype=torch.float64)
	q_value: tensor([[-25.4292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6189763361904277, distance: 1.4560521961801152 entropy 0.9217694997787476
epoch: 27, step: 120
	action: tensor([[ 0.5127,  0.3020, -0.8567, -0.1960, -0.2699,  0.3338,  0.5804]],
       dtype=torch.float64)
	q_value: tensor([[-27.9327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8261363742476435, distance: 0.47715673625151545 entropy 0.9217694997787476
epoch: 27, step: 121
	action: tensor([[ 0.0645, -0.0246,  0.7393, -0.4027, -0.3777,  0.1521, -0.4399]],
       dtype=torch.float64)
	q_value: tensor([[-26.7219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12085885306839816, distance: 1.0729660901230036 entropy 0.9217694997787476
epoch: 27, step: 122
	action: tensor([[ 0.6881, -1.7496,  0.1433, -0.4465, -0.1791,  1.1687,  0.7970]],
       dtype=torch.float64)
	q_value: tensor([[-23.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 123
	action: tensor([[ 0.4910,  0.1012, -0.2159,  0.4658, -0.3177, -0.3243, -0.6224]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 124
	action: tensor([[-0.0167, -0.1786,  0.3271,  0.1061, -0.0909, -0.3340,  0.9460]],
       dtype=torch.float64)
	q_value: tensor([[-27.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1145879137815663, distance: 1.0767860393878657 entropy 0.9217694997787476
epoch: 27, step: 125
	action: tensor([[ 0.6576, -0.4299,  1.1626, -0.5660,  0.4348,  0.1644,  0.2490]],
       dtype=torch.float64)
	q_value: tensor([[-25.2231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12997797201503536, distance: 1.067386775033588 entropy 0.9217694997787476
epoch: 27, step: 126
	action: tensor([[ 0.2384, -1.0291,  0.5080, -0.0401, -0.4074, -0.0492, -0.4417]],
       dtype=torch.float64)
	q_value: tensor([[-27.3706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6065224975324972, distance: 1.4504411053986037 entropy 0.9217694997787476
epoch: 27, step: 127
	action: tensor([[-0.1321, -0.1965, -1.6316, -0.6178, -0.2208, -1.0260, -0.0993]],
       dtype=torch.float64)
	q_value: tensor([[-26.9717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6928204631289023, distance: 0.634238839634626 entropy 0.9217694997787476
LOSS epoch 27 actor 329.005860160654 critic 252.7926208747965 
epoch: 28, step: 0
	action: tensor([[ 0.2706, -0.0089,  0.0770, -0.4299,  0.6537,  1.0061,  0.8903]],
       dtype=torch.float64)
	q_value: tensor([[-29.6905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6262753441148337, distance: 0.6995722414005084 entropy 0.9217694997787476
epoch: 28, step: 1
	action: tensor([[ 0.5489, -0.9622,  0.2482,  0.8675, -0.7879, -0.1789, -0.6928]],
       dtype=torch.float64)
	q_value: tensor([[-31.1463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36736843026459354, distance: 0.9101901335835703 entropy 0.9217694997787476
epoch: 28, step: 2
	action: tensor([[ 0.2022, -1.0541,  0.1024, -0.4811, -0.1414,  0.5464, -0.2697]],
       dtype=torch.float64)
	q_value: tensor([[-32.0628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6620714460056463, distance: 1.475304059760694 entropy 0.9217694997787476
epoch: 28, step: 3
	action: tensor([[-0.0794, -0.2613,  0.2572,  0.0341, -0.2430,  0.4853, -0.6239]],
       dtype=torch.float64)
	q_value: tensor([[-29.0967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1801949262810767, distance: 1.036124621927651 entropy 0.9217694997787476
epoch: 28, step: 4
	action: tensor([[ 0.8629, -0.7952,  0.8922, -1.1531, -0.3547,  0.6483, -0.2338]],
       dtype=torch.float64)
	q_value: tensor([[-24.5578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28073380306961515, distance: 1.295048786838467 entropy 0.9217694997787476
epoch: 28, step: 5
	action: tensor([[ 1.1123,  0.3169,  0.6726, -0.6963,  0.6013, -0.0509,  0.4961]],
       dtype=torch.float64)
	q_value: tensor([[-32.4948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7261734158854292, distance: 0.5988175191251991 entropy 0.9217694997787476
epoch: 28, step: 6
	action: tensor([[ 0.1357, -0.6626, -0.0581,  0.6166, -0.2381,  0.5378,  0.6219]],
       dtype=torch.float64)
	q_value: tensor([[-28.1537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3738332644523308, distance: 0.9055275949545531 entropy 0.9217694997787476
epoch: 28, step: 7
	action: tensor([[-0.8261,  0.5413, -0.3257,  1.4756,  0.5734, -0.8774,  1.2806]],
       dtype=torch.float64)
	q_value: tensor([[-28.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 8
	action: tensor([[ 0.4554, -0.3604,  0.0100, -0.0219,  0.1673,  0.0727,  0.7483]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34914832302923393, distance: 0.9232040634766653 entropy 0.9217694997787476
epoch: 28, step: 9
	action: tensor([[-0.6311,  0.5990,  0.3377,  0.2236, -0.5456, -0.9214,  0.5498]],
       dtype=torch.float64)
	q_value: tensor([[-26.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 10
	action: tensor([[ 0.1024, -0.3599, -0.8545,  0.1741, -0.7368,  0.4126, -0.1062]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023277589789036157, distance: 1.1309470431841881 entropy 0.9217694997787476
epoch: 28, step: 11
	action: tensor([[ 0.4376, -0.4536,  0.1448, -1.3296,  1.1704,  0.6957, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-26.7384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29763920637755237, distance: 1.3035679452153908 entropy 0.9217694997787476
epoch: 28, step: 12
	action: tensor([[ 0.4797, -1.0761, -0.6882, -0.0433, -0.1929,  0.5240, -0.6529]],
       dtype=torch.float64)
	q_value: tensor([[-32.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3666257071900285, distance: 1.3377701037662824 entropy 0.9217694997787476
epoch: 28, step: 13
	action: tensor([[ 0.4965,  0.2839, -0.0311,  0.3716, -0.4577,  0.1540,  0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-30.4705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 14
	action: tensor([[-0.0084,  0.3138,  0.1328, -0.1039, -0.5212,  0.1992,  0.5877]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 15
	action: tensor([[ 0.9604, -0.8626,  0.2409, -0.3227, -0.2471,  0.3884,  0.7379]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23536473210938702, distance: 1.271903881341318 entropy 0.9217694997787476
epoch: 28, step: 16
	action: tensor([[-0.2744,  0.1883,  0.0436, -0.9101, -0.2775,  0.7050,  0.8624]],
       dtype=torch.float64)
	q_value: tensor([[-32.3569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14378498937379103, distance: 1.2238519654174198 entropy 0.9217694997787476
epoch: 28, step: 17
	action: tensor([[-0.1980, -0.6248,  0.2085,  0.1375, -0.4394,  0.0733,  0.0968]],
       dtype=torch.float64)
	q_value: tensor([[-30.5482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37411646223346984, distance: 1.3414313893479382 entropy 0.9217694997787476
epoch: 28, step: 18
	action: tensor([[ 1.5349,  0.6504, -0.3312,  0.5461,  0.1461,  0.8997, -0.3578]],
       dtype=torch.float64)
	q_value: tensor([[-25.5894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 19
	action: tensor([[ 0.9557, -0.0788,  1.2193,  0.1555, -0.8017,  0.3632,  1.3452]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7782982613799624, distance: 0.5388169419588767 entropy 0.9217694997787476
epoch: 28, step: 20
	action: tensor([[ 0.0738, -0.8843, -0.8790,  0.3638,  1.7255,  0.2750,  0.9512]],
       dtype=torch.float64)
	q_value: tensor([[-35.2488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2814121585770899, distance: 1.2953917102384367 entropy 0.9217694997787476
epoch: 28, step: 21
	action: tensor([[-1.1829,  0.0175, -1.2075,  0.6433,  0.4760, -0.2317, -0.2871]],
       dtype=torch.float64)
	q_value: tensor([[-35.7290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.324876604180266, distance: 1.7448441705082958 entropy 0.9217694997787476
epoch: 28, step: 22
	action: tensor([[ 0.3419, -0.5144,  0.1838, -0.3967, -0.0516,  0.3257,  0.4621]],
       dtype=torch.float64)
	q_value: tensor([[-26.1394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0632728721163276, distance: 1.1799919922611886 entropy 0.9217694997787476
epoch: 28, step: 23
	action: tensor([[ 0.6099, -0.7910, -0.4624,  0.7221,  1.0880, -0.1778, -0.3521]],
       dtype=torch.float64)
	q_value: tensor([[-26.8184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29100979958079565, distance: 0.9635556659191686 entropy 0.9217694997787476
epoch: 28, step: 24
	action: tensor([[ 1.0254, -0.3148,  0.2730, -0.2539, -0.1890, -0.8866,  0.3489]],
       dtype=torch.float64)
	q_value: tensor([[-29.2236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1077136673307948, distance: 1.0809579842714332 entropy 0.9217694997787476
epoch: 28, step: 25
	action: tensor([[ 0.5322, -0.3641, -0.4098,  0.0264,  1.0949, -0.0190,  0.4248]],
       dtype=torch.float64)
	q_value: tensor([[-27.7341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34053659400183234, distance: 0.9292916693354825 entropy 0.9217694997787476
epoch: 28, step: 26
	action: tensor([[ 0.0219, -0.4154,  0.8101, -0.3825,  0.1155,  0.1868, -0.4877]],
       dtype=torch.float64)
	q_value: tensor([[-27.7493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26111913859604763, distance: 1.2850935728150141 entropy 0.9217694997787476
epoch: 28, step: 27
	action: tensor([[ 1.0122, -0.7683,  0.2369,  0.0043, -0.2111, -0.5541,  0.3616]],
       dtype=torch.float64)
	q_value: tensor([[-25.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23262908064939958, distance: 1.2704948180741515 entropy 0.9217694997787476
epoch: 28, step: 28
	action: tensor([[ 0.4070, -0.9638,  0.2231, -0.7967, -0.3528, -0.8026,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[-29.0787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7682808208793661, distance: 1.5217114607956952 entropy 0.9217694997787476
epoch: 28, step: 29
	action: tensor([[ 2.1689,  0.2353,  0.5919, -0.9226, -0.2427,  1.2172, -0.6542]],
       dtype=torch.float64)
	q_value: tensor([[-29.2555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 30
	action: tensor([[ 0.1512, -0.6717,  0.0160, -1.3958, -0.0914, -0.8112, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4240793212083731, distance: 1.3656008655005356 entropy 0.9217694997787476
epoch: 28, step: 31
	action: tensor([[ 0.0637, -0.6839, -0.2598, -0.8728, -0.0119,  0.6756, -0.1561]],
       dtype=torch.float64)
	q_value: tensor([[-29.6402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43381847029351195, distance: 1.3702625190446427 entropy 0.9217694997787476
epoch: 28, step: 32
	action: tensor([[-0.4786,  0.6785, -1.0857,  0.1439,  0.0278, -0.7489, -0.5445]],
       dtype=torch.float64)
	q_value: tensor([[-28.6812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 33
	action: tensor([[-0.3905, -0.3404,  0.5608, -0.3866, -0.2021, -0.3961, -0.4353]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7460995613856325, distance: 1.5121371906426027 entropy 0.9217694997787476
epoch: 28, step: 34
	action: tensor([[ 0.1273, -0.5642, -0.3493,  0.6102, -0.1132, -1.1995,  0.6547]],
       dtype=torch.float64)
	q_value: tensor([[-24.4035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05261990350762724, distance: 1.1138297702777453 entropy 0.9217694997787476
epoch: 28, step: 35
	action: tensor([[-1.0740,  0.4039,  0.6158, -0.2062,  0.2133, -0.0548,  0.3023]],
       dtype=torch.float64)
	q_value: tensor([[-29.4920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9253256955237608, distance: 1.5878475262069751 entropy 0.9217694997787476
epoch: 28, step: 36
	action: tensor([[-0.9173, -0.7959,  0.4554, -0.2242,  0.5165,  0.2509, -0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-25.5524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.309615976469007, distance: 1.7391081118200769 entropy 0.9217694997787476
epoch: 28, step: 37
	action: tensor([[ 0.2861, -0.7763,  0.2284, -0.9830,  0.4800,  0.0967,  0.2433]],
       dtype=torch.float64)
	q_value: tensor([[-27.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7503682793943149, distance: 1.5139844355314218 entropy 0.9217694997787476
epoch: 28, step: 38
	action: tensor([[-0.0131, -0.1352, -0.1510,  0.1702, -0.4070, -0.2257,  0.2441]],
       dtype=torch.float64)
	q_value: tensor([[-28.4363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24370894074807248, distance: 0.9951789388780546 entropy 0.9217694997787476
epoch: 28, step: 39
	action: tensor([[-0.2413, -0.4610,  0.2691, -0.0958,  0.8582,  0.4811,  0.4880]],
       dtype=torch.float64)
	q_value: tensor([[-23.6223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17557003375346025, distance: 1.2407404661953343 entropy 0.9217694997787476
epoch: 28, step: 40
	action: tensor([[ 0.2535,  0.0132, -1.5911,  0.3785,  0.6002, -1.0233,  1.2632]],
       dtype=torch.float64)
	q_value: tensor([[-26.8412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45583962847978987, distance: 0.8441510185402757 entropy 0.9217694997787476
epoch: 28, step: 41
	action: tensor([[-0.1865,  0.4663, -0.7984, -0.5900, -0.5749, -0.1961, -0.3617]],
       dtype=torch.float64)
	q_value: tensor([[-33.9208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5528852892533991, distance: 0.7651845152529015 entropy 0.9217694997787476
epoch: 28, step: 42
	action: tensor([[ 0.7157, -0.6489, -0.0745, -0.3848,  0.5292,  1.3565,  0.8380]],
       dtype=torch.float64)
	q_value: tensor([[-23.2201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4014066493056473, distance: 0.8853655868021705 entropy 0.9217694997787476
epoch: 28, step: 43
	action: tensor([[ 1.0831, -0.8302, -0.2957,  0.7769,  0.2147,  0.0580,  0.8186]],
       dtype=torch.float64)
	q_value: tensor([[-35.2926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.520104793017431, distance: 0.7927384066958796 entropy 0.9217694997787476
epoch: 28, step: 44
	action: tensor([[-0.3717, -0.5829, -0.2647, -0.1355, -0.2031,  0.9586,  1.1443]],
       dtype=torch.float64)
	q_value: tensor([[-33.0836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44396733982031544, distance: 1.375103471379308 entropy 0.9217694997787476
epoch: 28, step: 45
	action: tensor([[ 0.7580, -0.6346,  0.0625,  0.3687,  0.5401,  1.6488,  0.5197]],
       dtype=torch.float64)
	q_value: tensor([[-32.3895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7748248033987429, distance: 0.5430214293544908 entropy 0.9217694997787476
epoch: 28, step: 46
	action: tensor([[-0.2593, -0.0064, -0.2194,  0.1781, -0.8675,  0.1756, -0.1960]],
       dtype=torch.float64)
	q_value: tensor([[-35.8350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05819814177382732, distance: 1.110545776428593 entropy 0.9217694997787476
epoch: 28, step: 47
	action: tensor([[ 0.1211, -0.4920,  0.0596, -0.8721, -0.1464, -0.3101,  0.5574]],
       dtype=torch.float64)
	q_value: tensor([[-25.2417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4952376644947396, distance: 1.3993031358292072 entropy 0.9217694997787476
epoch: 28, step: 48
	action: tensor([[ 0.5340, -0.3942, -1.2072,  0.5397, -1.0320, -0.1300,  0.0794]],
       dtype=torch.float64)
	q_value: tensor([[-27.4005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3506535104634241, distance: 0.9221359249557228 entropy 0.9217694997787476
epoch: 28, step: 49
	action: tensor([[-0.0900,  0.3506,  0.1013, -0.1775,  0.0570, -0.2628, -0.6844]],
       dtype=torch.float64)
	q_value: tensor([[-31.6049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3276426761776382, distance: 0.9383325096131844 entropy 0.9217694997787476
epoch: 28, step: 50
	action: tensor([[ 0.5858, -0.1354,  0.4845,  0.6608,  0.0564, -0.6064, -0.4008]],
       dtype=torch.float64)
	q_value: tensor([[-21.6926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7937906426139848, distance: 0.5196499354722371 entropy 0.9217694997787476
epoch: 28, step: 51
	action: tensor([[-0.6446, -0.6685, -0.3528,  0.3083,  0.3062,  0.6764, -0.5905]],
       dtype=torch.float64)
	q_value: tensor([[-25.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5424303958142864, distance: 1.4212140125449422 entropy 0.9217694997787476
epoch: 28, step: 52
	action: tensor([[ 1.3313,  0.1227, -0.3252,  0.1889,  0.6192, -0.8138,  0.6698]],
       dtype=torch.float64)
	q_value: tensor([[-26.3029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6656673310209539, distance: 0.6616770947157959 entropy 0.9217694997787476
epoch: 28, step: 53
	action: tensor([[ 0.8574, -0.9550,  0.4519,  0.2103,  0.3603,  0.3229,  0.9484]],
       dtype=torch.float64)
	q_value: tensor([[-29.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005930119936090161, distance: 1.1477322878998997 entropy 0.9217694997787476
epoch: 28, step: 54
	action: tensor([[1.0300, 0.1947, 1.3789, 0.1133, 0.0882, 0.5991, 0.2439]],
       dtype=torch.float64)
	q_value: tensor([[-32.1596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7604245710320837, distance: 0.5601158088643837 entropy 0.9217694997787476
epoch: 28, step: 55
	action: tensor([[ 0.1808, -0.4529,  0.7194, -0.7730, -0.1687, -1.2836,  0.6644]],
       dtype=torch.float64)
	q_value: tensor([[-30.4301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38148250408407036, distance: 1.3450219995354589 entropy 0.9217694997787476
epoch: 28, step: 56
	action: tensor([[ 1.1493, -1.1153,  0.1023,  0.0226,  0.8338,  0.1154, -0.1386]],
       dtype=torch.float64)
	q_value: tensor([[-29.3681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5290190716967687, distance: 1.4150218449595509 entropy 0.9217694997787476
epoch: 28, step: 57
	action: tensor([[ 0.3227, -0.0992,  0.2641, -0.5766,  0.0299, -0.1367, -1.1212]],
       dtype=torch.float64)
	q_value: tensor([[-31.1285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1355764651768816, distance: 1.0639469759511861 entropy 0.9217694997787476
epoch: 28, step: 58
	action: tensor([[ 1.0993,  1.4682, -0.0733, -0.4166,  1.0396, -0.1190,  0.2503]],
       dtype=torch.float64)
	q_value: tensor([[-24.4377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 59
	action: tensor([[ 0.3320, -0.4470,  0.5098,  0.3784, -0.2089,  0.9969,  0.1598]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7548870014809058, distance: 0.5665521143763446 entropy 0.9217694997787476
epoch: 28, step: 60
	action: tensor([[ 1.1868, -0.9868,  0.1131, -0.4504,  0.3926,  1.0237, -0.3221]],
       dtype=torch.float64)
	q_value: tensor([[-29.3862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30443359421062444, distance: 1.306976204899837 entropy 0.9217694997787476
epoch: 28, step: 61
	action: tensor([[ 1.1448, -0.4487,  0.1919, -0.2488, -0.0243,  0.4493, -0.6982]],
       dtype=torch.float64)
	q_value: tensor([[-34.1322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2635777475488781, distance: 0.9820195749404489 entropy 0.9217694997787476
epoch: 28, step: 62
	action: tensor([[-0.5773, -0.7003, -0.4496, -0.6202, -0.4880, -0.0642,  0.0894]],
       dtype=torch.float64)
	q_value: tensor([[-28.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7194359424742447, distance: 1.5005473136564147 entropy 0.9217694997787476
epoch: 28, step: 63
	action: tensor([[-0.5349, -0.2336, -0.0204,  0.3230, -0.3212, -0.2425,  0.6534]],
       dtype=torch.float64)
	q_value: tensor([[-27.6320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.356586807664492, distance: 1.3328475805435613 entropy 0.9217694997787476
epoch: 28, step: 64
	action: tensor([[ 0.0625, -0.2043,  0.0473,  0.2624, -0.1072, -0.4363,  0.2495]],
       dtype=torch.float64)
	q_value: tensor([[-25.3841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22730186017216336, distance: 1.005915792909223 entropy 0.9217694997787476
epoch: 28, step: 65
	action: tensor([[-1.2098, -0.1721, -0.1931, -0.0464, -0.2099,  1.5243,  1.0217]],
       dtype=torch.float64)
	q_value: tensor([[-23.4078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8002141866097299, distance: 1.5353902668214627 entropy 0.9217694997787476
epoch: 28, step: 66
	action: tensor([[ 0.2596, -0.0009, -0.3484,  0.6637, -0.9128, -0.0131,  0.2443]],
       dtype=torch.float64)
	q_value: tensor([[-35.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 67
	action: tensor([[-0.0083,  0.6369,  0.0413,  0.0984,  1.1987,  0.0343,  0.2897]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 68
	action: tensor([[ 0.1388, -0.9163, -0.6203,  0.7971,  0.2511,  0.0566,  0.6935]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13449788722486655, distance: 1.2188732305790475 entropy 0.9217694997787476
epoch: 28, step: 69
	action: tensor([[ 0.7638,  0.5341,  0.3366, -0.3461, -0.7434, -0.0630,  1.2556]],
       dtype=torch.float64)
	q_value: tensor([[-30.3707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9183876002394403, distance: 0.3269149393174035 entropy 0.9217694997787476
epoch: 28, step: 70
	action: tensor([[ 0.7672,  0.2705, -0.7864,  0.1775,  0.2066,  0.5528, -0.1146]],
       dtype=torch.float64)
	q_value: tensor([[-32.3193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9517479329395995, distance: 0.2513707000576297 entropy 0.9217694997787476
epoch: 28, step: 71
	action: tensor([[ 0.5843,  0.5247, -0.0630,  0.2696, -0.2441, -0.3896, -0.3149]],
       dtype=torch.float64)
	q_value: tensor([[-26.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 72
	action: tensor([[-0.0257, -0.0756, -0.3033, -0.1500, -0.4503, -0.0559,  0.1347]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1964173050580713, distance: 1.0258219346082071 entropy 0.9217694997787476
epoch: 28, step: 73
	action: tensor([[ 0.4365, -0.5821,  0.2069, -0.5270, -0.4748,  0.9569,  0.5870]],
       dtype=torch.float64)
	q_value: tensor([[-23.3630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04506878155773353, distance: 1.1182598673175594 entropy 0.9217694997787476
epoch: 28, step: 74
	action: tensor([[ 1.2531,  0.2630,  0.5387,  0.7021, -0.3765,  0.2882,  0.6468]],
       dtype=torch.float64)
	q_value: tensor([[-31.6573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7898009280061353, distance: 0.5246529153234828 entropy 0.9217694997787476
epoch: 28, step: 75
	action: tensor([[-0.4673, -0.0993,  1.5062, -1.2624,  0.4630,  0.4924,  0.7253]],
       dtype=torch.float64)
	q_value: tensor([[-30.8609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8355880419094692, distance: 1.5504019400982612 entropy 0.9217694997787476
epoch: 28, step: 76
	action: tensor([[ 0.2974, -0.2907, -0.0580,  1.1732, -0.0328, -0.3675,  0.0392]],
       dtype=torch.float64)
	q_value: tensor([[-32.6403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 77
	action: tensor([[ 1.2906, -0.7861, -0.3120,  0.5258,  0.2052,  0.3645,  0.3081]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41149988627088196, distance: 0.8778695168158824 entropy 0.9217694997787476
epoch: 28, step: 78
	action: tensor([[ 0.7587, -1.1396, -0.4851, -0.0621, -0.1021, -0.4625,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[-31.8242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6774928181339945, distance: 1.4821324914258225 entropy 0.9217694997787476
epoch: 28, step: 79
	action: tensor([[ 0.3685, -0.6440,  0.2086,  0.1598, -0.9579,  0.5261,  1.1252]],
       dtype=torch.float64)
	q_value: tensor([[-30.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20462564074122402, distance: 1.0205692680168497 entropy 0.9217694997787476
epoch: 28, step: 80
	action: tensor([[ 0.2530, -0.8220,  0.8601, -0.3441,  0.0873,  0.2414,  1.5780]],
       dtype=torch.float64)
	q_value: tensor([[-33.4506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41277962106114274, distance: 1.3601722309694284 entropy 0.9217694997787476
epoch: 28, step: 81
	action: tensor([[-0.2968,  0.6431,  0.6015,  0.1636,  1.2185,  0.6629,  0.8706]],
       dtype=torch.float64)
	q_value: tensor([[-33.7224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 82
	action: tensor([[ 0.6268, -0.6499,  0.3231,  0.3849,  0.0776, -0.9528, -0.3812]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07941092642499858, distance: 1.0979677935984797 entropy 0.9217694997787476
epoch: 28, step: 83
	action: tensor([[ 0.4774, -0.9938, -0.5084,  0.3239,  0.6941,  0.1749, -0.6414]],
       dtype=torch.float64)
	q_value: tensor([[-26.5132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12472330291024236, distance: 1.2136110998602812 entropy 0.9217694997787476
epoch: 28, step: 84
	action: tensor([[-0.5362,  0.4844,  0.3721,  0.8001, -0.0959, -0.0598, -0.4193]],
       dtype=torch.float64)
	q_value: tensor([[-28.8735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 85
	action: tensor([[ 0.3149,  0.4134,  0.3750,  0.1375,  0.4888, -0.9342,  0.3733]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 86
	action: tensor([[ 0.6578, -0.7714,  1.7539,  0.3601, -0.1071,  0.0214, -0.6088]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.050535146115169516, distance: 1.1729027013376192 entropy 0.9217694997787476
epoch: 28, step: 87
	action: tensor([[ 0.7282, -1.1616, -0.4237, -1.5215, -1.0873,  0.6096, -0.3128]],
       dtype=torch.float64)
	q_value: tensor([[-33.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8469453680507488, distance: 1.5551909414682767 entropy 0.9217694997787476
epoch: 28, step: 88
	action: tensor([[-0.2440, -0.7128, -0.1324, -0.3539, -0.2137,  0.4031,  1.2586]],
       dtype=torch.float64)
	q_value: tensor([[-38.7234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6488383721376185, distance: 1.4694192879991852 entropy 0.9217694997787476
epoch: 28, step: 89
	action: tensor([[-0.3305, -1.3221, -0.2633, -0.2020,  0.5357, -0.3875,  0.6833]],
       dtype=torch.float64)
	q_value: tensor([[-31.8812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0579721004070946, distance: 1.6416343645592788 entropy 0.9217694997787476
epoch: 28, step: 90
	action: tensor([[-0.0449, -1.1377, -0.4910,  0.2843,  0.6685,  0.3291,  0.5830]],
       dtype=torch.float64)
	q_value: tensor([[-30.1922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5767568022346368, distance: 1.436941374815724 entropy 0.9217694997787476
epoch: 28, step: 91
	action: tensor([[ 0.9492, -0.6678, -0.7230, -0.0248,  0.4603,  0.3201, -0.1206]],
       dtype=torch.float64)
	q_value: tensor([[-30.6780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15969047061000907, distance: 1.0490020516142888 entropy 0.9217694997787476
epoch: 28, step: 92
	action: tensor([[ 0.3556,  0.7767, -0.0016,  0.5545,  0.8070, -0.1746,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[-28.9461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 93
	action: tensor([[ 0.3845, -0.5068, -0.2732,  0.9504, -0.5515, -0.3163,  1.3208]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6158371904760652, distance: 0.7092745105108228 entropy 0.9217694997787476
epoch: 28, step: 94
	action: tensor([[ 1.1506, -0.1652,  0.9984,  0.6617, -0.3922,  0.0880,  1.6015]],
       dtype=torch.float64)
	q_value: tensor([[-34.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5863362018054132, distance: 0.7360044682012069 entropy 0.9217694997787476
epoch: 28, step: 95
	action: tensor([[ 0.7518, -0.5674, -0.4465,  0.5744,  0.4838,  0.0171,  0.4338]],
       dtype=torch.float64)
	q_value: tensor([[-35.9300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.552641965770768, distance: 0.7653926967691339 entropy 0.9217694997787476
epoch: 28, step: 96
	action: tensor([[ 1.5671,  0.4363, -1.6325, -0.9574, -0.0616, -0.1785,  0.4810]],
       dtype=torch.float64)
	q_value: tensor([[-28.2753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07555619074180342, distance: 1.100264124558342 entropy 0.9217694997787476
epoch: 28, step: 97
	action: tensor([[ 0.5122,  0.0134,  0.1829,  0.2590,  0.3383,  0.2875, -1.9831]],
       dtype=torch.float64)
	q_value: tensor([[-35.7459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8688307068266772, distance: 0.4144506092680712 entropy 0.9217694997787476
epoch: 28, step: 98
	action: tensor([[ 0.1415, -0.4654, -0.6354, -0.0230, -0.5437,  0.2785,  0.5511]],
       dtype=torch.float64)
	q_value: tensor([[-29.9691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016213973126357306, distance: 1.135029157520083 entropy 0.9217694997787476
epoch: 28, step: 99
	action: tensor([[ 0.5436, -0.2061,  0.0772, -0.1955,  1.0697,  0.3034,  0.9729]],
       dtype=torch.float64)
	q_value: tensor([[-28.0037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4907953881093744, distance: 0.8165877441592622 entropy 0.9217694997787476
epoch: 28, step: 100
	action: tensor([[-0.8635, -1.1720, -0.0561, -0.7470, -0.4868, -0.8416, -0.1178]],
       dtype=torch.float64)
	q_value: tensor([[-29.5795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5401697334592763, distance: 1.4201721298351502 entropy 0.9217694997787476
epoch: 28, step: 101
	action: tensor([[0.2282, 0.3222, 0.5818, 0.0745, 0.1090, 0.5497, 0.0727]],
       dtype=torch.float64)
	q_value: tensor([[-31.1073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 102
	action: tensor([[-0.0045, -0.4355, -0.2002,  0.2168,  0.4208,  0.9792, -0.6170]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37378417173785716, distance: 0.9055630918388822 entropy 0.9217694997787476
epoch: 28, step: 103
	action: tensor([[-0.0604, -0.5855, -0.3653,  0.6957,  0.9018, -0.9841, -0.5730]],
       dtype=torch.float64)
	q_value: tensor([[-26.9739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27248202137919875, distance: 1.290870038388688 entropy 0.9217694997787476
epoch: 28, step: 104
	action: tensor([[ 0.5253, -1.2747, -0.6469,  0.8670, -1.0423, -0.2719,  0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-27.0577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10056740602269154, distance: 1.200507884398931 entropy 0.9217694997787476
epoch: 28, step: 105
	action: tensor([[-0.6248, -1.1378,  0.4343, -0.5287,  0.1130, -0.3083,  1.3765]],
       dtype=torch.float64)
	q_value: tensor([[-35.1844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2384701161952663, distance: 1.7121126800869526 entropy 0.9217694997787476
epoch: 28, step: 106
	action: tensor([[ 0.3381,  1.2440,  0.3778,  0.2523, -0.1595, -0.0343, -0.5623]],
       dtype=torch.float64)
	q_value: tensor([[-32.1155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 107
	action: tensor([[ 0.7488,  0.0324,  0.8114, -0.3071,  1.1204, -1.2180,  1.0753]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6861319913070782, distance: 0.6411065578730673 entropy 0.9217694997787476
epoch: 28, step: 108
	action: tensor([[ 1.7554, -0.7792, -0.5077, -0.6940,  0.4713, -0.6298, -0.1524]],
       dtype=torch.float64)
	q_value: tensor([[-30.9170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7795609548990852, distance: 1.5265573600632676 entropy 0.9217694997787476
epoch: 28, step: 109
	action: tensor([[ 0.2755,  0.0424, -0.1886, -0.2932,  0.3692,  0.5663, -0.2070]],
       dtype=torch.float64)
	q_value: tensor([[-33.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.647527473797423, distance: 0.6793902795303444 entropy 0.9217694997787476
epoch: 28, step: 110
	action: tensor([[-0.4645,  0.1959, -0.6476, -0.0610, -0.4440, -0.0108,  0.0455]],
       dtype=torch.float64)
	q_value: tensor([[-23.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03162970215433436, distance: 1.1623010016878295 entropy 0.9217694997787476
epoch: 28, step: 111
	action: tensor([[ 0.4369, -0.4041, -0.4484,  0.7960, -0.0455, -0.1121,  0.3753]],
       dtype=torch.float64)
	q_value: tensor([[-22.8097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5943809010244423, distance: 0.728812631853308 entropy 0.9217694997787476
epoch: 28, step: 112
	action: tensor([[ 1.4880, -1.2928, -0.2967,  0.4352, -0.5343,  0.1517,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-26.8858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3195378719778039, distance: 1.3145212858543982 entropy 0.9217694997787476
epoch: 28, step: 113
	action: tensor([[-0.3820,  0.1969,  0.7463, -0.6360,  0.9840,  0.6248,  0.8050]],
       dtype=torch.float64)
	q_value: tensor([[-35.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22532086262295858, distance: 1.2667228574234983 entropy 0.9217694997787476
epoch: 28, step: 114
	action: tensor([[ 0.2217, -0.1764, -0.2290, -0.3466,  0.9617,  0.4943, -0.5834]],
       dtype=torch.float64)
	q_value: tensor([[-30.1798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3480384732566719, distance: 0.9239908643920518 entropy 0.9217694997787476
epoch: 28, step: 115
	action: tensor([[-0.6002, -0.5252, -0.4774, -0.3412,  0.2945,  0.0307,  0.8729]],
       dtype=torch.float64)
	q_value: tensor([[-25.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7690198650583455, distance: 1.5220294234467515 entropy 0.9217694997787476
epoch: 28, step: 116
	action: tensor([[ 0.9622, -0.6371, -1.2253,  0.1734,  0.4348, -0.1511, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-27.6196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08701532502821152, distance: 1.093423584453622 entropy 0.9217694997787476
epoch: 28, step: 117
	action: tensor([[ 0.8129, -0.5621, -0.1224, -0.0875, -0.6942,  0.0778, -1.1741]],
       dtype=torch.float64)
	q_value: tensor([[-29.9864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1315567288545595, distance: 1.0664178861523146 entropy 0.9217694997787476
epoch: 28, step: 118
	action: tensor([[ 1.5663, -0.3988,  0.1755, -0.2629,  0.2614, -0.2014,  0.2786]],
       dtype=torch.float64)
	q_value: tensor([[-27.8857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09643309583288318, distance: 1.1982508931901596 entropy 0.9217694997787476
epoch: 28, step: 119
	action: tensor([[-0.0522, -0.2036,  0.9463,  0.4349,  0.4123, -0.8384,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-29.5649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1523185495449273, distance: 1.0535933798997286 entropy 0.9217694997787476
epoch: 28, step: 120
	action: tensor([[ 0.3367, -0.1625,  0.3377,  0.3109, -0.6478, -0.0242, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-26.5282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5969843289793846, distance: 0.7264699591546351 entropy 0.9217694997787476
epoch: 28, step: 121
	action: tensor([[-0.3330, -0.4568,  0.1031,  0.7824, -0.3987,  1.2948,  0.6569]],
       dtype=torch.float64)
	q_value: tensor([[-25.6345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3195720364158712, distance: 0.9439473466340895 entropy 0.9217694997787476
epoch: 28, step: 122
	action: tensor([[-0.2087,  1.0320, -1.2119,  0.5719,  1.4421,  0.8805,  0.3098]],
       dtype=torch.float64)
	q_value: tensor([[-32.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 123
	action: tensor([[-0.1677, -0.7573,  0.2724, -0.8693,  0.5683,  0.5420, -0.9874]],
       dtype=torch.float64)
	q_value: tensor([[-29.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7077906852510372, distance: 1.4954572877085337 entropy 0.9217694997787476
epoch: 28, step: 124
	action: tensor([[ 0.3251,  0.3587,  0.2577, -0.7469,  0.6656, -0.0636, -0.0437]],
       dtype=torch.float64)
	q_value: tensor([[-28.8149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43160368661082193, distance: 0.862744738673402 entropy 0.9217694997787476
epoch: 28, step: 125
	action: tensor([[ 0.0692, -0.2980, -0.1559,  0.4842, -0.4569, -1.1261,  0.7835]],
       dtype=torch.float64)
	q_value: tensor([[-23.9908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29936212368284953, distance: 0.9578632232029063 entropy 0.9217694997787476
epoch: 28, step: 126
	action: tensor([[-0.3601, -0.5608,  0.5230,  0.4640, -0.5892, -0.4621,  0.2154]],
       dtype=torch.float64)
	q_value: tensor([[-29.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22601422326813791, distance: 1.2670812009363686 entropy 0.9217694997787476
epoch: 28, step: 127
	action: tensor([[-0.1748,  0.4009, -0.5243, -0.3712,  0.1969, -0.2848,  0.0585]],
       dtype=torch.float64)
	q_value: tensor([[-27.2121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3618094825206766, distance: 0.9141803178197221 entropy 0.9217694997787476
LOSS epoch 28 actor 356.1577693942618 critic 235.4471501313758 
epoch: 29, step: 0
	action: tensor([[ 0.9960, -0.9608, -0.4155, -2.0404,  0.3917,  0.4422, -0.4575]],
       dtype=torch.float64)
	q_value: tensor([[-22.4227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5831315931421488, distance: 1.4398432049671486 entropy 0.9217694997787476
epoch: 29, step: 1
	action: tensor([[ 0.5829, -0.0562,  0.1072,  0.0935, -0.8940, -0.6805, -0.1290]],
       dtype=torch.float64)
	q_value: tensor([[-40.2112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6239125981799596, distance: 0.7017801599548454 entropy 0.9217694997787476
epoch: 29, step: 2
	action: tensor([[ 0.0545, -0.4569,  0.8348, -0.4066, -0.0791, -0.2653,  1.4286]],
       dtype=torch.float64)
	q_value: tensor([[-28.6388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44657597649937575, distance: 1.3763450253269651 entropy 0.9217694997787476
epoch: 29, step: 3
	action: tensor([[-0.6389, -0.2450, -0.0532,  0.8536, -0.9494,  0.9666,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-32.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18110204245068817, distance: 1.2436563839536552 entropy 0.9217694997787476
epoch: 29, step: 4
	action: tensor([[-0.0806,  0.0916,  0.1036,  0.2695,  0.4025,  0.5284,  1.2271]],
       dtype=torch.float64)
	q_value: tensor([[-33.6331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 5
	action: tensor([[ 0.6745, -0.2948,  0.0781, -0.3921,  0.4362, -0.4138,  1.2077]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11532244504407341, distance: 1.0763392998823946 entropy 0.9217694997787476
epoch: 29, step: 6
	action: tensor([[-0.1797, -1.5081, -0.3270, -0.0964,  0.3819,  0.6802,  1.5457]],
       dtype=torch.float64)
	q_value: tensor([[-30.4683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 7
	action: tensor([[ 0.5616, -0.7873,  0.3576, -0.2298,  0.2556, -0.0329,  0.2542]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3136168083876274, distance: 1.3115686933742499 entropy 0.9217694997787476
epoch: 29, step: 8
	action: tensor([[ 0.3751, -1.2012,  0.6141, -0.1065, -0.1523, -0.9319,  0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-27.7212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8531752156604349, distance: 1.557811605414373 entropy 0.9217694997787476
epoch: 29, step: 9
	action: tensor([[-0.2111,  0.6106,  1.1398,  0.0946, -0.1203,  0.0913, -0.8389]],
       dtype=torch.float64)
	q_value: tensor([[-30.9962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 10
	action: tensor([[ 1.1003, -1.2466,  1.1907,  0.1049,  0.8793, -0.0055,  1.7033]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6365874304103005, distance: 1.4639501851247718 entropy 0.9217694997787476
epoch: 29, step: 11
	action: tensor([[ 0.2392, -0.1167,  0.8609, -0.0391,  0.4612,  1.0617,  0.1951]],
       dtype=torch.float64)
	q_value: tensor([[-39.2736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.66318512721735, distance: 0.6641288147328555 entropy 0.9217694997787476
epoch: 29, step: 12
	action: tensor([[ 0.7399, -0.5823,  0.2444,  0.4237,  1.0065,  0.2165,  0.4097]],
       dtype=torch.float64)
	q_value: tensor([[-30.6277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5326498688677423, distance: 0.7823081914485018 entropy 0.9217694997787476
epoch: 29, step: 13
	action: tensor([[ 0.6118,  0.2685,  0.7408, -1.0359, -0.0393, -1.6008,  0.9699]],
       dtype=torch.float64)
	q_value: tensor([[-30.0466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117869225170715, distance: 0.9493320730561665 entropy 0.9217694997787476
epoch: 29, step: 14
	action: tensor([[-0.5615, -0.6915, -0.0074,  0.1024, -0.9620,  0.6502,  0.0719]],
       dtype=torch.float64)
	q_value: tensor([[-33.8288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8115764619823651, distance: 1.5402280484235857 entropy 0.9217694997787476
epoch: 29, step: 15
	action: tensor([[ 0.9296, -0.2541,  0.0066, -0.0892, -0.8244, -0.1570, -0.6716]],
       dtype=torch.float64)
	q_value: tensor([[-32.3049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41918948443515414, distance: 0.8721153461348458 entropy 0.9217694997787476
epoch: 29, step: 16
	action: tensor([[ 0.6893, -0.4965, -0.5044, -0.2753,  0.6330, -0.3392, -0.6976]],
       dtype=torch.float64)
	q_value: tensor([[-28.6896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0002632688682593365, distance: 1.144494879195246 entropy 0.9217694997787476
epoch: 29, step: 17
	action: tensor([[-0.4381,  0.8349,  1.0879,  0.9648,  0.0826,  0.1257, -0.0213]],
       dtype=torch.float64)
	q_value: tensor([[-27.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 18
	action: tensor([[ 1.8966, -0.7548,  0.2586, -0.0508,  0.1933, -0.3930,  0.5399]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3666645341458996, distance: 1.3377891072035772 entropy 0.9217694997787476
epoch: 29, step: 19
	action: tensor([[ 0.5679, -0.7877, -0.1909, -0.8244,  0.1674,  0.3730,  1.1640]],
       dtype=torch.float64)
	q_value: tensor([[-32.9645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.466851312799752, distance: 1.385956948684224 entropy 0.9217694997787476
epoch: 29, step: 20
	action: tensor([[ 1.5299,  0.6259, -0.7534, -0.0935, -0.0021, -0.2498,  0.1569]],
       dtype=torch.float64)
	q_value: tensor([[-35.0999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.648844140296388, distance: 0.6781201559020148 entropy 0.9217694997787476
epoch: 29, step: 21
	action: tensor([[ 0.9164, -1.0504,  0.1180,  0.8839,  0.1653,  0.2782,  0.9721]],
       dtype=torch.float64)
	q_value: tensor([[-30.7693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4384127240862761, distance: 0.8575615940982819 entropy 0.9217694997787476
epoch: 29, step: 22
	action: tensor([[-0.1737,  0.3677, -0.2080, -1.3530, -0.5808,  0.1309,  0.8053]],
       dtype=torch.float64)
	q_value: tensor([[-35.3095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04471904029181961, distance: 1.1184646285666486 entropy 0.9217694997787476
epoch: 29, step: 23
	action: tensor([[ 0.8007,  0.6952,  0.1503,  1.3765, -0.3318,  0.5038,  0.2641]],
       dtype=torch.float64)
	q_value: tensor([[-32.0836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 24
	action: tensor([[ 0.2686, -0.9464,  0.4265, -0.7472,  0.1658, -0.6097,  0.0935]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8704141478211846, distance: 1.5650405072796967 entropy 0.9217694997787476
epoch: 29, step: 25
	action: tensor([[-0.2932,  0.1502,  0.0821, -0.5844,  0.1792,  0.3760,  0.5222]],
       dtype=torch.float64)
	q_value: tensor([[-29.6482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07634278127563365, distance: 1.1872221605656263 entropy 0.9217694997787476
epoch: 29, step: 26
	action: tensor([[-0.3367, -0.0046,  0.0871,  0.8097,  0.4328,  0.4366,  0.0719]],
       dtype=torch.float64)
	q_value: tensor([[-26.8981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 27
	action: tensor([[ 0.5788, -0.5533,  0.0807,  0.6196,  0.3763, -0.7409,  0.9512]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3871862847086719, distance: 0.8958203657960693 entropy 0.9217694997787476
epoch: 29, step: 28
	action: tensor([[ 0.1080, -0.6079, -0.5234,  0.2127,  0.7350,  0.6120,  0.8678]],
       dtype=torch.float64)
	q_value: tensor([[-30.1258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1486357128989172, distance: 1.0558796201762939 entropy 0.9217694997787476
epoch: 29, step: 29
	action: tensor([[-0.4507, -0.2007,  1.1109, -0.7207, -0.5255,  0.0502,  1.5528]],
       dtype=torch.float64)
	q_value: tensor([[-31.5373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8757300558342525, distance: 1.5672629317818294 entropy 0.9217694997787476
epoch: 29, step: 30
	action: tensor([[ 2.6280e-01, -7.8688e-02,  9.2511e-04, -1.0526e+00,  3.2740e-01,
         -7.8911e-02,  1.8551e-01]], dtype=torch.float64)
	q_value: tensor([[-35.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10295409739809114, distance: 1.201808890629332 entropy 0.9217694997787476
epoch: 29, step: 31
	action: tensor([[ 0.1060, -0.3443, -0.5553, -0.3793, -0.1042,  0.5600,  0.7531]],
       dtype=torch.float64)
	q_value: tensor([[-27.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08042182518384, distance: 1.0973647888574094 entropy 0.9217694997787476
epoch: 29, step: 32
	action: tensor([[ 0.7277, -0.2322,  0.5470,  0.0611, -0.1937, -0.6364,  0.5812]],
       dtype=torch.float64)
	q_value: tensor([[-29.8633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41763255830417534, distance: 0.8732834641415934 entropy 0.9217694997787476
epoch: 29, step: 33
	action: tensor([[-0.8845, -0.4681,  0.7536,  0.8172,  0.1694,  0.4849, -0.4749]],
       dtype=torch.float64)
	q_value: tensor([[-27.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04671925715455294, distance: 1.17077057936264 entropy 0.9217694997787476
epoch: 29, step: 34
	action: tensor([[ 0.3853, -0.7957, -0.0346, -0.4750, -0.5464,  0.8241,  0.1976]],
       dtype=torch.float64)
	q_value: tensor([[-30.9136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2681381064547155, distance: 1.2886648114051695 entropy 0.9217694997787476
epoch: 29, step: 35
	action: tensor([[-0.3175,  0.6759,  0.5102,  0.6490, -0.2501,  1.0772, -0.4375]],
       dtype=torch.float64)
	q_value: tensor([[-32.3525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 36
	action: tensor([[ 1.7643, -0.2052,  0.1512, -0.1010,  0.2330, -0.3747, -0.5302]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 37
	action: tensor([[ 0.4175, -0.4840,  0.1224, -0.6497, -1.2652,  0.3193,  0.6057]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23928092197910766, distance: 1.273918296869879 entropy 0.9217694997787476
epoch: 29, step: 38
	action: tensor([[ 0.0945, -1.2041,  0.2832, -0.7334, -0.3705,  1.1670, -0.0868]],
       dtype=torch.float64)
	q_value: tensor([[-34.8507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7265324143223011, distance: 1.503640661561614 entropy 0.9217694997787476
epoch: 29, step: 39
	action: tensor([[-0.1990,  0.1175,  0.4583, -0.3245, -0.5558,  0.0784,  0.0712]],
       dtype=torch.float64)
	q_value: tensor([[-36.2682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026766419730582802, distance: 1.1595581203985934 entropy 0.9217694997787476
epoch: 29, step: 40
	action: tensor([[ 0.1663, -1.1684, -0.4348,  0.4451,  0.4923, -0.1033,  0.1851]],
       dtype=torch.float64)
	q_value: tensor([[-26.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5348966049021779, distance: 1.4177389009136878 entropy 0.9217694997787476
epoch: 29, step: 41
	action: tensor([[-0.2992, -0.4434,  0.3415, -0.4510,  0.4631,  0.8356,  0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-30.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28854150343780316, distance: 1.298990273068258 entropy 0.9217694997787476
epoch: 29, step: 42
	action: tensor([[ 0.2548,  0.8833, -0.2172, -0.3752, -0.5657,  0.3516,  1.0552]],
       dtype=torch.float64)
	q_value: tensor([[-28.3649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 43
	action: tensor([[ 0.1097, -1.5104, -0.3232, -0.6156, -0.8532, -0.0158,  0.4462]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8709006331618321, distance: 1.5652440236411809 entropy 0.9217694997787476
epoch: 29, step: 44
	action: tensor([[ 1.1982,  0.1484, -0.4766,  0.7448, -0.6084,  0.3811, -0.2045]],
       dtype=torch.float64)
	q_value: tensor([[-35.0226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.05086774246755154 entropy 0.9217694997787476
epoch: 29, step: 45
	action: tensor([[ 0.1987, -2.2128,  0.7428, -0.1701,  0.7159,  1.1513, -0.5774]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 46
	action: tensor([[ 0.0760, -1.2573, -0.0402,  0.8036,  0.5849, -0.3899,  0.4800]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3671477557240548, distance: 1.3380255922425845 entropy 0.9217694997787476
epoch: 29, step: 47
	action: tensor([[ 0.3575,  0.1978, -0.7097, -0.8247,  0.1241,  0.6193,  0.2166]],
       dtype=torch.float64)
	q_value: tensor([[-31.3808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.674135806735479, distance: 0.6532433753308307 entropy 0.9217694997787476
epoch: 29, step: 48
	action: tensor([[ 1.4938,  0.0826,  1.2559, -0.8393,  0.6417, -0.3144,  0.4997]],
       dtype=torch.float64)
	q_value: tensor([[-28.9394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.748982149862321, distance: 0.5733357148015332 entropy 0.9217694997787476
epoch: 29, step: 49
	action: tensor([[ 1.1463,  0.8665, -0.7924, -1.2265, -0.2597,  0.4220,  0.6119]],
       dtype=torch.float64)
	q_value: tensor([[-33.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7024722855161404, distance: 0.6241951736797725 entropy 0.9217694997787476
epoch: 29, step: 50
	action: tensor([[-0.9549,  0.2058, -0.1228, -0.8381, -0.5387,  0.7901,  0.2748]],
       dtype=torch.float64)
	q_value: tensor([[-35.6163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9199859119568812, distance: 1.5856440940404146 entropy 0.9217694997787476
epoch: 29, step: 51
	action: tensor([[ 0.1896, -0.4300,  0.3598, -0.4863,  0.0024, -0.4336,  0.3425]],
       dtype=torch.float64)
	q_value: tensor([[-31.5041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37531314791352033, distance: 1.3420153727410034 entropy 0.9217694997787476
epoch: 29, step: 52
	action: tensor([[-0.3594, -0.5491,  0.8878,  0.7370,  0.7062,  0.6072,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-26.6219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3481722768818033, distance: 0.9238960430979921 entropy 0.9217694997787476
epoch: 29, step: 53
	action: tensor([[-0.3599, -0.4653, -1.1793, -0.6995, -0.0794, -1.2476, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-30.7283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30440843721012, distance: 0.9544075056501999 entropy 0.9217694997787476
epoch: 29, step: 54
	action: tensor([[-0.1104,  0.0687, -0.2700,  0.3073,  0.2518,  0.1827,  0.6948]],
       dtype=torch.float64)
	q_value: tensor([[-31.7530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 55
	action: tensor([[ 0.2932, -0.6593,  0.7394,  0.2089,  0.4947,  0.1541,  1.0146]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16016307812309383, distance: 1.048707019866815 entropy 0.9217694997787476
epoch: 29, step: 56
	action: tensor([[ 0.3249, -1.4088, -0.3663,  0.0489, -0.2309, -0.3099,  0.3069]],
       dtype=torch.float64)
	q_value: tensor([[-30.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 57
	action: tensor([[-0.1907, -1.1122,  0.5876, -0.3977,  0.2235, -0.9222, -0.4972]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0523998266402108, distance: 1.6394103702069227 entropy 0.9217694997787476
epoch: 29, step: 58
	action: tensor([[ 1.1352,  1.1561,  0.1488, -0.5253, -1.1547,  0.0428,  0.8651]],
       dtype=torch.float64)
	q_value: tensor([[-30.0902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9545041479354586, distance: 0.2440858451253817 entropy 0.9217694997787476
epoch: 29, step: 59
	action: tensor([[-0.3803, -0.1849,  0.5270, -1.4295, -0.1186, -0.1146, -0.0371]],
       dtype=torch.float64)
	q_value: tensor([[-37.2044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8148022068144607, distance: 1.5415987255734467 entropy 0.9217694997787476
epoch: 29, step: 60
	action: tensor([[-0.1709, -0.1662, -0.2036, -0.5874,  0.9505, -0.3010, -0.4600]],
       dtype=torch.float64)
	q_value: tensor([[-29.3156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3224781844921636, distance: 1.31598503794609 entropy 0.9217694997787476
epoch: 29, step: 61
	action: tensor([[ 0.3525,  0.6672, -0.6590,  0.6250,  0.3380, -0.5459, -0.6420]],
       dtype=torch.float64)
	q_value: tensor([[-26.2153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 62
	action: tensor([[ 0.4677,  0.4694, -0.0846, -0.2890, -0.2911,  0.2799,  0.1422]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 63
	action: tensor([[ 0.5966, -0.1655, -0.7438,  0.2161, -1.0319, -1.1413,  1.2289]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 64
	action: tensor([[ 0.5445, -0.7892,  0.5870,  0.7195, -0.6180,  0.9968,  0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.731649449344256, distance: 0.5927996501538302 entropy 0.9217694997787476
epoch: 29, step: 65
	action: tensor([[ 0.4001, -0.7892,  0.3853,  0.4593, -0.6253,  0.3062,  0.7633]],
       dtype=torch.float64)
	q_value: tensor([[-34.7272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32611289369512697, distance: 0.9393993746662272 entropy 0.9217694997787476
epoch: 29, step: 66
	action: tensor([[ 0.8424, -0.6246,  0.2488,  0.1967,  0.7812,  0.5914, -0.3032]],
       dtype=torch.float64)
	q_value: tensor([[-32.3352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39945548513898665, distance: 0.8868073737583662 entropy 0.9217694997787476
epoch: 29, step: 67
	action: tensor([[ 0.3525, -0.0273,  0.2933,  0.6025, -0.0076, -0.2134,  0.8678]],
       dtype=torch.float64)
	q_value: tensor([[-30.3376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8242189941277598, distance: 0.47978058088724235 entropy 0.9217694997787476
epoch: 29, step: 68
	action: tensor([[-0.1047,  0.2169,  0.5499,  0.7325,  0.7606,  0.0347, -0.7657]],
       dtype=torch.float64)
	q_value: tensor([[-27.7347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 69
	action: tensor([[-0.3816, -0.7798,  0.3160, -0.3599,  0.5632, -0.4302, -0.6434]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0087766686945927, distance: 1.6218942011170798 entropy 0.9217694997787476
epoch: 29, step: 70
	action: tensor([[ 0.6164,  0.0885,  0.4669,  0.2520,  1.1583, -0.7157,  0.1627]],
       dtype=torch.float64)
	q_value: tensor([[-27.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.811214311406925, distance: 0.4972115716295885 entropy 0.9217694997787476
epoch: 29, step: 71
	action: tensor([[ 0.4095, -0.7926, -0.2524, -1.7727, -0.4648,  0.9152, -0.2739]],
       dtype=torch.float64)
	q_value: tensor([[-27.7944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5590088498384931, distance: 1.4288313928158958 entropy 0.9217694997787476
epoch: 29, step: 72
	action: tensor([[-0.1107, -1.0134, -0.1990,  0.7873, -0.4917,  0.4671, -0.1161]],
       dtype=torch.float64)
	q_value: tensor([[-38.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.212239762465271, distance: 1.2599431677274806 entropy 0.9217694997787476
epoch: 29, step: 73
	action: tensor([[ 1.6810,  0.8222,  0.7784, -0.7335,  0.0277,  0.7061, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-32.2226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8340823882655217, distance: 0.4661255804006263 entropy 0.9217694997787476
epoch: 29, step: 74
	action: tensor([[ 0.9261, -0.3904,  0.1099,  0.5062,  0.4684,  0.9684,  0.4887]],
       dtype=torch.float64)
	q_value: tensor([[-33.7335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8621345536664732, distance: 0.4248977280059963 entropy 0.9217694997787476
epoch: 29, step: 75
	action: tensor([[-0.1068, -0.6874, -1.4159, -0.2221, -0.1721,  0.0138,  0.6893]],
       dtype=torch.float64)
	q_value: tensor([[-31.9611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2590303319376541, distance: 1.2840288738641685 entropy 0.9217694997787476
epoch: 29, step: 76
	action: tensor([[ 0.1752, -1.0607, -0.7638,  0.9242, -0.6469, -0.0200,  0.1935]],
       dtype=torch.float64)
	q_value: tensor([[-33.0402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2408876184393165, distance: 1.2747438308571426 entropy 0.9217694997787476
epoch: 29, step: 77
	action: tensor([[ 0.1831, -0.6895, -0.4251, -0.3488,  1.4224,  0.6891, -0.3474]],
       dtype=torch.float64)
	q_value: tensor([[-33.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016943284977631645, distance: 1.1539980095784752 entropy 0.9217694997787476
epoch: 29, step: 78
	action: tensor([[ 0.5635,  0.7101,  0.0962,  0.3433,  0.0040, -0.2171,  0.1435]],
       dtype=torch.float64)
	q_value: tensor([[-32.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 79
	action: tensor([[-0.4527, -0.2728,  0.3465,  0.5629, -0.6901,  0.1605,  0.6814]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004908446307743786, distance: 1.1471492922827164 entropy 0.9217694997787476
epoch: 29, step: 80
	action: tensor([[ 0.5906,  0.1746,  0.0447,  0.0566,  0.0218, -0.7635, -0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-29.7416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7012862136805447, distance: 0.6254380897488846 entropy 0.9217694997787476
epoch: 29, step: 81
	action: tensor([[ 0.6531,  1.1247,  0.2099, -1.6766, -1.3060, -0.6193,  0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-24.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7580451510143724, distance: 0.5628904213217402 entropy 0.9217694997787476
epoch: 29, step: 82
	action: tensor([[ 0.9039, -1.4713,  0.0970, -0.3060, -0.4381,  0.9986, -0.2351]],
       dtype=torch.float64)
	q_value: tensor([[-36.6922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3708363826915073, distance: 1.339829403392385 entropy 0.9217694997787476
epoch: 29, step: 83
	action: tensor([[-0.7502,  0.4591, -0.1293,  0.3362,  0.5992, -0.8426,  0.9243]],
       dtype=torch.float64)
	q_value: tensor([[-36.2289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39898611172194687, distance: 1.3535160026856063 entropy 0.9217694997787476
epoch: 29, step: 84
	action: tensor([[ 0.0921, -0.9429,  1.5581, -0.9351, -0.1448, -0.4323,  0.3660]],
       dtype=torch.float64)
	q_value: tensor([[-27.7858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.73643953984287, distance: 1.5079485586317196 entropy 0.9217694997787476
epoch: 29, step: 85
	action: tensor([[-0.1793, -0.3782,  0.0771,  0.3757,  0.4574, -0.2817,  0.3588]],
       dtype=torch.float64)
	q_value: tensor([[-32.8731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09962913661338502, distance: 1.1999960393302498 entropy 0.9217694997787476
epoch: 29, step: 86
	action: tensor([[-0.1598,  0.0472,  0.7116, -0.2007, -0.5732,  0.3932, -0.1985]],
       dtype=torch.float64)
	q_value: tensor([[-24.8562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10855272050023312, distance: 1.0804496303340991 entropy 0.9217694997787476
epoch: 29, step: 87
	action: tensor([[ 0.7370, -0.7057, -0.0842, -0.0454,  0.3598,  0.5871, -0.1504]],
       dtype=torch.float64)
	q_value: tensor([[-27.4869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21929731963218912, distance: 1.0111126141743687 entropy 0.9217694997787476
epoch: 29, step: 88
	action: tensor([[-0.7229, -0.4657,  1.4874, -1.3016,  0.1333, -0.3497,  0.3199]],
       dtype=torch.float64)
	q_value: tensor([[-29.2207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1178889379003052, distance: 1.6653605945626802 entropy 0.9217694997787476
epoch: 29, step: 89
	action: tensor([[-0.3982,  0.0573,  0.8079,  0.3714, -0.0628, -0.6111,  0.5862]],
       dtype=torch.float64)
	q_value: tensor([[-32.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04991948211360597, distance: 1.1725589623382187 entropy 0.9217694997787476
epoch: 29, step: 90
	action: tensor([[-0.2016,  0.1161,  0.8919, -0.2234, -0.2299,  0.0996,  0.0699]],
       dtype=torch.float64)
	q_value: tensor([[-26.8666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009053492692920884, distance: 1.1495127383203598 entropy 0.9217694997787476
epoch: 29, step: 91
	action: tensor([[ 1.2290, -0.3981,  0.4763, -0.4363,  0.2838, -0.1914, -0.2494]],
       dtype=torch.float64)
	q_value: tensor([[-26.4603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08730233133964604, distance: 1.0932517063567595 entropy 0.9217694997787476
epoch: 29, step: 92
	action: tensor([[ 0.1225, -0.2121,  0.2811,  0.3646,  0.2539, -0.4236,  0.4504]],
       dtype=torch.float64)
	q_value: tensor([[-28.3495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32369525766758545, distance: 0.941082959665402 entropy 0.9217694997787476
epoch: 29, step: 93
	action: tensor([[ 1.1039,  0.5685, -0.7225,  0.2645,  1.0220,  1.7009,  1.1772]],
       dtype=torch.float64)
	q_value: tensor([[-25.0740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8525965988466273, distance: 0.43934981260300066 entropy 0.9217694997787476
epoch: 29, step: 94
	action: tensor([[-0.0988, -0.0501,  1.0513, -0.2606, -0.9815, -0.4354,  0.3976]],
       dtype=torch.float64)
	q_value: tensor([[-42.0317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17048367085535787, distance: 1.2380533879151765 entropy 0.9217694997787476
epoch: 29, step: 95
	action: tensor([[ 0.3511, -1.2471, -0.0278, -0.5861,  0.0640,  0.4660,  0.0631]],
       dtype=torch.float64)
	q_value: tensor([[-30.8757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8833437926118355, distance: 1.5704405326655333 entropy 0.9217694997787476
epoch: 29, step: 96
	action: tensor([[ 0.4721, -0.3751, -0.0041, -1.1837,  0.2671, -0.8414,  0.4188]],
       dtype=torch.float64)
	q_value: tensor([[-32.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34463194803326536, distance: 1.3269617553218487 entropy 0.9217694997787476
epoch: 29, step: 97
	action: tensor([[ 0.4880,  0.4467,  0.6058, -0.5712, -0.8221,  0.6719, -0.2452]],
       dtype=torch.float64)
	q_value: tensor([[-29.8111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 98
	action: tensor([[ 1.0622, -0.6097,  0.0990, -0.3242, -0.3962,  0.8018, -0.2420]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20394530290423574, distance: 1.0210056559053657 entropy 0.9217694997787476
epoch: 29, step: 99
	action: tensor([[ 0.2165,  0.7709,  0.1643,  0.6612, -0.0056,  0.2048,  0.6705]],
       dtype=torch.float64)
	q_value: tensor([[-32.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 100
	action: tensor([[ 0.0394, -1.3665,  0.6203, -0.6058, -0.4938, -0.3112,  1.4324]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1460724002967528, distance: 1.6764047315305686 entropy 0.9217694997787476
epoch: 29, step: 101
	action: tensor([[ 1.5986,  0.0552,  0.3679, -0.5967,  0.2760, -0.2855,  0.2257]],
       dtype=torch.float64)
	q_value: tensor([[-36.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23766513995468852, distance: 0.9991474471751312 entropy 0.9217694997787476
epoch: 29, step: 102
	action: tensor([[ 0.1873,  0.5868, -0.3842,  0.1634,  0.5933, -0.3753, -0.1968]],
       dtype=torch.float64)
	q_value: tensor([[-30.3716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 103
	action: tensor([[ 1.1913, -0.6271,  1.2262,  0.1688, -1.5857,  0.2393,  0.4460]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15301120915772515, distance: 1.0531628344847268 entropy 0.9217694997787476
epoch: 29, step: 104
	action: tensor([[-0.8643, -0.1262, -0.0887, -0.6616, -0.9287,  0.4037,  0.5418]],
       dtype=torch.float64)
	q_value: tensor([[-40.0933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.05497533376204, distance: 1.6404386760077687 entropy 0.9217694997787476
epoch: 29, step: 105
	action: tensor([[-0.8369,  0.5261, -0.1908, -0.1370, -0.7340, -0.2723,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-32.0443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3544593996223595, distance: 1.331802080475033 entropy 0.9217694997787476
epoch: 29, step: 106
	action: tensor([[-0.1864,  0.3682,  0.4781, -0.2734, -0.3260, -0.7702,  0.1292]],
       dtype=torch.float64)
	q_value: tensor([[-26.6718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0726516738623848, distance: 1.1019912330363293 entropy 0.9217694997787476
epoch: 29, step: 107
	action: tensor([[ 0.7247, -1.2046,  0.1039, -0.0822,  0.0812,  0.4370, -0.1058]],
       dtype=torch.float64)
	q_value: tensor([[-25.3519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4924166276546522, distance: 1.397982493144539 entropy 0.9217694997787476
epoch: 29, step: 108
	action: tensor([[ 0.6339, -0.3662, -0.7722,  0.5080, -0.6349, -0.4479, -0.2769]],
       dtype=torch.float64)
	q_value: tensor([[-32.0394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5598958060437094, distance: 0.7591619748079229 entropy 0.9217694997787476
epoch: 29, step: 109
	action: tensor([[-0.1291, -0.1720,  0.6016,  0.4588, -0.0536, -0.1559,  1.0024]],
       dtype=torch.float64)
	q_value: tensor([[-28.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3159795953084207, distance: 0.946435935626941 entropy 0.9217694997787476
epoch: 29, step: 110
	action: tensor([[ 0.0079,  0.1380, -0.0748, -0.7949, -0.9396,  1.8040,  0.3568]],
       dtype=torch.float64)
	q_value: tensor([[-27.9090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26903263705178004, distance: 0.97837576528161 entropy 0.9217694997787476
epoch: 29, step: 111
	action: tensor([[ 0.5937, -0.7188, -0.4926,  1.0642,  0.4358, -1.1164, -0.6154]],
       dtype=torch.float64)
	q_value: tensor([[-38.9636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28694908591732904, distance: 0.9663110898655557 entropy 0.9217694997787476
epoch: 29, step: 112
	action: tensor([[ 0.2442, -0.2104,  0.3055, -0.1215,  0.0624, -0.1901,  0.1241]],
       dtype=torch.float64)
	q_value: tensor([[-31.3015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1982588184406795, distance: 1.0246458588082832 entropy 0.9217694997787476
epoch: 29, step: 113
	action: tensor([[-0.6457, -0.4347, -1.1568,  0.3585,  0.3680,  0.2306,  0.4060]],
       dtype=torch.float64)
	q_value: tensor([[-24.3826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9174344443190285, distance: 1.5845901632412247 entropy 0.9217694997787476
epoch: 29, step: 114
	action: tensor([[ 0.3691,  0.3989,  0.4408, -0.3145,  0.8595, -0.0236,  0.1324]],
       dtype=torch.float64)
	q_value: tensor([[-28.6372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6990222731762494, distance: 0.6278037018182823 entropy 0.9217694997787476
epoch: 29, step: 115
	action: tensor([[ 0.8637, -0.0476, -0.8962, -0.3771,  0.2299, -0.0655,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-25.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.537838890771123, distance: 0.777953056985619 entropy 0.9217694997787476
epoch: 29, step: 116
	action: tensor([[ 0.4731, -0.2455, -0.3034, -1.1909,  0.2221,  0.2237, -0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-27.6087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04126242876606456, distance: 1.1677148209252641 entropy 0.9217694997787476
epoch: 29, step: 117
	action: tensor([[ 0.4128, -0.7350,  0.7536, -0.3975, -0.5208, -0.1662,  0.4846]],
       dtype=torch.float64)
	q_value: tensor([[-29.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45358991072845845, distance: 1.3796776950969918 entropy 0.9217694997787476
epoch: 29, step: 118
	action: tensor([[ 0.9182, -0.2982, -0.4411,  0.2003, -0.5081, -0.5057,  0.2863]],
       dtype=torch.float64)
	q_value: tensor([[-30.3327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4907515673228482, distance: 0.8166228800815966 entropy 0.9217694997787476
epoch: 29, step: 119
	action: tensor([[ 0.8468, -0.9107, -0.4547, -0.9440, -1.0708,  0.0400,  0.3931]],
       dtype=torch.float64)
	q_value: tensor([[-29.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7059392105108224, distance: 1.4946464294861241 entropy 0.9217694997787476
epoch: 29, step: 120
	action: tensor([[-0.8057, -0.6665,  0.3236, -0.3106,  0.0837, -0.0626, -0.6524]],
       dtype=torch.float64)
	q_value: tensor([[-37.0886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2147788700585513, distance: 1.7030283554063874 entropy 0.9217694997787476
epoch: 29, step: 121
	action: tensor([[-0.3743,  0.4659, -0.2637, -0.2807, -0.3739,  0.6909, -0.3073]],
       dtype=torch.float64)
	q_value: tensor([[-27.4988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 122
	action: tensor([[ 1.2187, -1.0020,  0.4146,  0.6044,  0.4386,  0.2988,  0.2514]],
       dtype=torch.float64)
	q_value: tensor([[-31.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10369006828052552, distance: 1.0833924322996389 entropy 0.9217694997787476
epoch: 29, step: 123
	action: tensor([[-0.9799,  0.0275, -1.3545, -0.5788,  0.1538,  0.1501,  0.3266]],
       dtype=torch.float64)
	q_value: tensor([[-33.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5209533152066423, distance: 1.4112847061640208 entropy 0.9217694997787476
epoch: 29, step: 124
	action: tensor([[ 0.4699, -0.8931,  0.9438, -0.7311, -1.1452, -0.1872,  1.3859]],
       dtype=torch.float64)
	q_value: tensor([[-28.6032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7227274165124407, distance: 1.5019828574315723 entropy 0.9217694997787476
epoch: 29, step: 125
	action: tensor([[ 0.1975, -1.3012, -0.1133, -0.4191,  1.4237,  0.1403,  0.7058]],
       dtype=torch.float64)
	q_value: tensor([[-39.1568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9095324035491714, distance: 1.581321622949637 entropy 0.9217694997787476
epoch: 29, step: 126
	action: tensor([[-0.6105, -0.5918, -0.6314,  0.5542,  0.8582, -0.2320, -0.0353]],
       dtype=torch.float64)
	q_value: tensor([[-35.1250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.886590211522323, distance: 1.5717934754206302 entropy 0.9217694997787476
epoch: 29, step: 127
	action: tensor([[-0.1567, -0.6333,  0.3852, -0.7818, -0.1114,  0.4506,  0.5963]],
       dtype=torch.float64)
	q_value: tensor([[-27.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7307091246837245, distance: 1.5054583157985753 entropy 0.9217694997787476
LOSS epoch 29 actor 418.33237184428344 critic 470.6010734530753 
epoch: 30, step: 0
	action: tensor([[ 0.3327, -0.4306,  0.3437, -0.9599, -0.1590, -0.0770,  0.2177]],
       dtype=torch.float64)
	q_value: tensor([[-30.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4425064167713888, distance: 1.3744076700667396 entropy 0.8164090514183044
epoch: 30, step: 1
	action: tensor([[ 0.6281,  0.0441,  0.0751, -0.0980, -0.3673,  0.3946,  1.3076]],
       dtype=torch.float64)
	q_value: tensor([[-28.9006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7679023982148847, distance: 0.5513051123321265 entropy 0.8164090514183044
epoch: 30, step: 2
	action: tensor([[ 0.8243, -0.7238, -0.7305, -0.8072,  1.0396,  0.2738,  0.4412]],
       dtype=torch.float64)
	q_value: tensor([[-32.8639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27675494098381215, distance: 1.293035554733998 entropy 0.8164090514183044
epoch: 30, step: 3
	action: tensor([[ 0.3142, -0.4357,  0.1424,  0.0210,  0.3167, -0.1711, -1.3743]],
       dtype=torch.float64)
	q_value: tensor([[-35.6782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1495872570810013, distance: 1.055289392810052 entropy 0.8164090514183044
epoch: 30, step: 4
	action: tensor([[-0.3856,  0.3288, -0.5110, -0.2554, -0.1670, -0.4650,  0.7425]],
       dtype=torch.float64)
	q_value: tensor([[-29.5459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17047834105015014, distance: 1.0422467709251988 entropy 0.8164090514183044
epoch: 30, step: 5
	action: tensor([[-0.3751,  0.4164, -0.5669,  0.6725, -0.8061,  1.1297,  0.8711]],
       dtype=torch.float64)
	q_value: tensor([[-26.7975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 6
	action: tensor([[ 0.4137, -0.8977,  0.4796, -0.6837,  0.3619,  1.0797,  0.6618]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2791191121937946, distance: 1.2942321600879847 entropy 0.8164090514183044
epoch: 30, step: 7
	action: tensor([[ 0.0564, -0.1591, -0.4528, -0.7995,  0.6377,  0.8869,  0.3467]],
       dtype=torch.float64)
	q_value: tensor([[-34.9989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35174109279766663, distance: 0.9213633643539422 entropy 0.8164090514183044
epoch: 30, step: 8
	action: tensor([[ 0.1609,  0.0596, -0.0103, -0.3995, -0.0541,  0.0591,  0.1882]],
       dtype=torch.float64)
	q_value: tensor([[-31.7964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33146807679137824, distance: 0.9356593640621306 entropy 0.8164090514183044
epoch: 30, step: 9
	action: tensor([[ 0.7201, -0.2785,  1.0379,  0.2970,  0.1912,  0.1618,  0.2525]],
       dtype=torch.float64)
	q_value: tensor([[-24.6632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6449841748618937, distance: 0.6818369763622679 entropy 0.8164090514183044
epoch: 30, step: 10
	action: tensor([[-0.0551,  0.0377, -0.0007,  0.2386, -0.4636,  0.4951, -0.6399]],
       dtype=torch.float64)
	q_value: tensor([[-29.3994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 11
	action: tensor([[ 0.9845, -0.2787, -0.7159, -1.1277,  0.8383,  0.3212,  0.3477]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 12
	action: tensor([[ 1.1257, -0.5267, -0.2121, -0.5486,  0.4935, -0.5452,  0.3457]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2999223598971896, distance: 1.304714233693109 entropy 0.8164090514183044
epoch: 30, step: 13
	action: tensor([[-0.1496,  0.3918, -0.0077,  0.0438,  0.2830, -0.1917, -0.3887]],
       dtype=torch.float64)
	q_value: tensor([[-30.6961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 14
	action: tensor([[ 0.4279, -0.4149,  1.4873,  0.3294,  0.2905, -0.5333,  1.2311]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3328457159256082, distance: 0.934694813488394 entropy 0.8164090514183044
epoch: 30, step: 15
	action: tensor([[-4.3266e-01, -7.7663e-01, -4.4845e-01, -6.6961e-04, -2.7036e-01,
         -6.0251e-01,  7.0635e-02]], dtype=torch.float64)
	q_value: tensor([[-32.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7026520963112046, distance: 1.4932057443604514 entropy 0.8164090514183044
epoch: 30, step: 16
	action: tensor([[0.2575, 0.4233, 0.2075, 0.0101, 0.4234, 1.0674, 0.1906]],
       dtype=torch.float64)
	q_value: tensor([[-28.7581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 17
	action: tensor([[ 0.0880,  0.0671, -0.1344, -1.1040, -0.2630, -0.5724,  0.5111]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003950600634060164, distance: 1.1466024494590374 entropy 0.8164090514183044
epoch: 30, step: 18
	action: tensor([[-0.6685, -0.7900, -0.0641, -0.2504,  0.3498,  0.7791,  0.6189]],
       dtype=torch.float64)
	q_value: tensor([[-28.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8068455384102107, distance: 1.5382155844585217 entropy 0.8164090514183044
epoch: 30, step: 19
	action: tensor([[ 0.6206, -1.0668, -0.8069, -0.6961,  0.3210, -0.0197,  0.7873]],
       dtype=torch.float64)
	q_value: tensor([[-31.5983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6440221434671023, distance: 1.4672716442466363 entropy 0.8164090514183044
epoch: 30, step: 20
	action: tensor([[ 0.4453, -0.1126, -0.0694,  0.2335,  0.9171, -0.1573, -0.3336]],
       dtype=torch.float64)
	q_value: tensor([[-35.6771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6211657710276013, distance: 0.7043382911349564 entropy 0.8164090514183044
epoch: 30, step: 21
	action: tensor([[ 0.6630, -0.3407,  0.1163, -0.0994,  0.9049, -0.2791,  0.2669]],
       dtype=torch.float64)
	q_value: tensor([[-26.6177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31040523432956013, distance: 0.9502845565722357 entropy 0.8164090514183044
epoch: 30, step: 22
	action: tensor([[ 0.3949, -0.2196, -0.1258, -0.4021,  0.0499,  0.0880, -0.0481]],
       dtype=torch.float64)
	q_value: tensor([[-27.5431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2102792034573403, distance: 1.0169356695726173 entropy 0.8164090514183044
epoch: 30, step: 23
	action: tensor([[ 0.0885, -0.2891,  0.6171, -0.9404,  0.6573,  0.5190,  0.3708]],
       dtype=torch.float64)
	q_value: tensor([[-25.3966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30828835213781014, distance: 1.308905916142958 entropy 0.8164090514183044
epoch: 30, step: 24
	action: tensor([[ 0.8997,  0.0804, -0.2206, -0.4253,  0.2379,  0.1517, -0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-29.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6318584126240724, distance: 0.694327127517326 entropy 0.8164090514183044
epoch: 30, step: 25
	action: tensor([[-0.3984,  0.5974, -0.5814, -0.9870, -0.3335,  0.8239,  1.1073]],
       dtype=torch.float64)
	q_value: tensor([[-26.6218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24250349229564494, distance: 0.9959717284092048 entropy 0.8164090514183044
epoch: 30, step: 26
	action: tensor([[ 0.3955, -0.9545, -0.2039, -0.1203,  0.6042,  0.6136,  0.0613]],
       dtype=torch.float64)
	q_value: tensor([[-35.6363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17439098172097744, distance: 1.2401181023555992 entropy 0.8164090514183044
epoch: 30, step: 27
	action: tensor([[-0.3995, -1.5303, -0.3518,  0.1343, -0.3454, -1.1351,  0.6285]],
       dtype=torch.float64)
	q_value: tensor([[-31.1558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 28
	action: tensor([[ 0.0344, -1.1973,  0.0365,  0.8298, -0.5985,  0.4871,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07767010882701508, distance: 1.1879539660775709 entropy 0.8164090514183044
epoch: 30, step: 29
	action: tensor([[ 0.4780,  0.7724, -0.0387,  0.2198, -0.7390,  0.2130,  0.2144]],
       dtype=torch.float64)
	q_value: tensor([[-34.8043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 30
	action: tensor([[ 0.5437, -0.4647, -0.0249,  0.0155, -0.1675,  0.3194,  0.6965]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37294852674917744, distance: 0.9061670984306055 entropy 0.8164090514183044
epoch: 30, step: 31
	action: tensor([[ 0.5383,  0.0979, -0.7508,  0.0933,  1.2921, -0.1045, -0.6678]],
       dtype=torch.float64)
	q_value: tensor([[-29.4780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7351415963232374, distance: 0.5889298563830729 entropy 0.8164090514183044
epoch: 30, step: 32
	action: tensor([[ 0.0238, -0.0892,  0.9245, -0.5137,  0.2351, -0.9727, -0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-29.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2607325569288652, distance: 1.2848965923402529 entropy 0.8164090514183044
epoch: 30, step: 33
	action: tensor([[-0.5792, -0.1575,  0.6712, -0.1750,  0.2001, -0.4006, -0.7793]],
       dtype=torch.float64)
	q_value: tensor([[-28.2049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8001721716485302, distance: 1.5353723495815685 entropy 0.8164090514183044
epoch: 30, step: 34
	action: tensor([[ 0.0387, -0.5798,  0.2680,  0.7596, -0.6693,  0.5043,  0.8077]],
       dtype=torch.float64)
	q_value: tensor([[-28.3259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47327581639362315, distance: 0.8305166100339236 entropy 0.8164090514183044
epoch: 30, step: 35
	action: tensor([[ 0.8037, -0.4933, -0.5612,  0.5072,  0.6498, -0.2874,  0.6109]],
       dtype=torch.float64)
	q_value: tensor([[-32.6797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48212735322057365, distance: 0.823508677338375 entropy 0.8164090514183044
epoch: 30, step: 36
	action: tensor([[1.0199, 0.4097, 0.4187, 0.9938, 0.5385, 0.0712, 0.1611]],
       dtype=torch.float64)
	q_value: tensor([[-30.1458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.812417852769874, distance: 0.49562413272297823 entropy 0.8164090514183044
epoch: 30, step: 37
	action: tensor([[ 0.1268, -0.1123,  0.8403,  0.1154, -0.7498, -0.1733, -0.2824]],
       dtype=torch.float64)
	q_value: tensor([[-30.3270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3876217119321991, distance: 0.8955020521891424 entropy 0.8164090514183044
epoch: 30, step: 38
	action: tensor([[ 0.5602, -0.0030,  0.0264,  0.7484,  0.7053, -0.3693,  1.2721]],
       dtype=torch.float64)
	q_value: tensor([[-28.9461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8817164027268957, distance: 0.3935672527369639 entropy 0.8164090514183044
epoch: 30, step: 39
	action: tensor([[ 0.0080, -0.3190, -0.6413, -0.4874, -0.5905,  0.4239, -0.3076]],
       dtype=torch.float64)
	q_value: tensor([[-31.3554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.045762081366135465, distance: 1.1702351494689172 entropy 0.8164090514183044
epoch: 30, step: 40
	action: tensor([[ 0.6174, -0.9722,  0.1183,  0.0061, -1.0493, -0.3795, -0.5989]],
       dtype=torch.float64)
	q_value: tensor([[-28.0339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3520899926100558, distance: 1.330636684730385 entropy 0.8164090514183044
epoch: 30, step: 41
	action: tensor([[ 0.9121, -0.3110, -0.0685, -0.4397,  0.6870,  0.4016,  0.4889]],
       dtype=torch.float64)
	q_value: tensor([[-32.9655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2788880948504646, distance: 0.9717577793989502 entropy 0.8164090514183044
epoch: 30, step: 42
	action: tensor([[-0.4312, -0.0689, -0.1841, -0.0643,  0.1175,  0.3690, -0.4947]],
       dtype=torch.float64)
	q_value: tensor([[-30.4015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14773430015740607, distance: 1.225963029462516 entropy 0.8164090514183044
epoch: 30, step: 43
	action: tensor([[ 0.2837,  0.1184, -0.3105,  0.0505, -0.1557,  0.0362,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-24.7855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6311149927295581, distance: 0.6950278311194844 entropy 0.8164090514183044
epoch: 30, step: 44
	action: tensor([[ 0.1454, -0.6998,  0.0308,  0.1414,  0.5209, -0.3237,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[-23.8842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.248577415897544, distance: 1.2786875329333451 entropy 0.8164090514183044
epoch: 30, step: 45
	action: tensor([[-0.0029, -0.9504,  0.6200,  0.0186, -0.5050, -0.9854,  0.3845]],
       dtype=torch.float64)
	q_value: tensor([[-26.7477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6944005119442298, distance: 1.4895830789548514 entropy 0.8164090514183044
epoch: 30, step: 46
	action: tensor([[ 0.3834,  0.2716,  0.6052, -0.0818,  0.0969,  0.9066,  1.3077]],
       dtype=torch.float64)
	q_value: tensor([[-31.4073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9093633653707653, distance: 0.34451535180670834 entropy 0.8164090514183044
epoch: 30, step: 47
	action: tensor([[ 0.8004, -0.7505,  0.3671, -0.4610, -0.2118,  0.6020,  0.9061]],
       dtype=torch.float64)
	q_value: tensor([[-34.6060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08352392928193608, distance: 1.1911760334057244 entropy 0.8164090514183044
epoch: 30, step: 48
	action: tensor([[ 0.4325,  0.4754, -0.1691,  1.0766,  0.9368,  0.2099,  0.8090]],
       dtype=torch.float64)
	q_value: tensor([[-34.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 49
	action: tensor([[ 0.7767, -0.3602,  0.2499, -1.5598, -0.3979, -0.1447,  0.2822]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3816171576762031, distance: 1.3450875478251227 entropy 0.8164090514183044
epoch: 30, step: 50
	action: tensor([[ 0.7344,  1.2267,  0.2117, -1.0134,  0.8964,  0.5591, -0.7352]],
       dtype=torch.float64)
	q_value: tensor([[-33.6114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 51
	action: tensor([[ 0.6953, -0.9496,  0.3147, -0.2851, -0.1141, -0.2310,  0.9949]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5823150896448273, distance: 1.4394718559772823 entropy 0.8164090514183044
epoch: 30, step: 52
	action: tensor([[ 0.8172, -1.7988,  0.6505, -1.2270, -0.5152, -0.3457,  1.1131]],
       dtype=torch.float64)
	q_value: tensor([[-33.2152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 53
	action: tensor([[ 0.1416, -0.2574, -1.1591, -0.3387,  0.0014, -0.4740,  0.1309]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32337962506796003, distance: 0.9413025365762666 entropy 0.8164090514183044
epoch: 30, step: 54
	action: tensor([[ 0.3421, -0.8015,  0.3949,  0.2291, -1.0330, -0.6154,  0.6928]],
       dtype=torch.float64)
	q_value: tensor([[-28.1678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08913874465412674, distance: 1.194258379398519 entropy 0.8164090514183044
epoch: 30, step: 55
	action: tensor([[ 0.6035, -1.1207,  0.0017, -0.2446, -0.0578,  0.2468,  0.5561]],
       dtype=torch.float64)
	q_value: tensor([[-33.4757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6256655404007119, distance: 1.4590571167462263 entropy 0.8164090514183044
epoch: 30, step: 56
	action: tensor([[ 0.6324, -0.2495, -0.5093,  0.0777,  0.8222, -0.1119, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[-32.9485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5006112805980075, distance: 0.8086787986321646 entropy 0.8164090514183044
epoch: 30, step: 57
	action: tensor([[ 0.4461, -0.0350, -1.1725,  0.8036, -0.8477, -0.3601,  0.4396]],
       dtype=torch.float64)
	q_value: tensor([[-27.3767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5944315518013462, distance: 0.7287671260097677 entropy 0.8164090514183044
epoch: 30, step: 58
	action: tensor([[-0.2740, -0.5107,  0.5412, -1.0818,  0.6762,  0.3715,  0.2891]],
       dtype=torch.float64)
	q_value: tensor([[-32.8262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8606434967469445, distance: 1.5609474333173268 entropy 0.8164090514183044
epoch: 30, step: 59
	action: tensor([[-0.0300,  0.0290, -0.8504,  0.4103, -0.9806,  0.2818,  0.1846]],
       dtype=torch.float64)
	q_value: tensor([[-30.6999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14802038990302424, distance: 1.0562611194179248 entropy 0.8164090514183044
epoch: 30, step: 60
	action: tensor([[ 0.0064,  0.1108, -0.2764,  0.7618,  0.9975,  0.1466, -0.5465]],
       dtype=torch.float64)
	q_value: tensor([[-29.9081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 61
	action: tensor([[ 0.3203, -1.0657,  1.5751, -0.2563,  0.8128,  1.1473,  0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5576975182241646, distance: 1.4282303487794665 entropy 0.8164090514183044
epoch: 30, step: 62
	action: tensor([[ 0.2324, -0.5751,  0.3129,  0.9297,  1.1112,  0.9048,  0.2922]],
       dtype=torch.float64)
	q_value: tensor([[-37.1399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.773972926873013, distance: 0.5440476314617675 entropy 0.8164090514183044
epoch: 30, step: 63
	action: tensor([[-0.3496, -0.6313, -0.3225, -0.7688,  0.0029, -0.7122,  0.1797]],
       dtype=torch.float64)
	q_value: tensor([[-32.9279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.522907345919432, distance: 1.4121909826653845 entropy 0.8164090514183044
epoch: 30, step: 64
	action: tensor([[-0.1276,  0.3305, -0.3407, -0.3793, -0.0899, -0.6540, -1.0028]],
       dtype=torch.float64)
	q_value: tensor([[-29.4008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060705836946852, distance: 0.9532665243823012 entropy 0.8164090514183044
epoch: 30, step: 65
	action: tensor([[ 0.6781,  0.5795,  0.3140, -1.0841, -0.4728,  0.8118, -0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-25.0622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.783715144560968, distance: 0.532193723796202 entropy 0.8164090514183044
epoch: 30, step: 66
	action: tensor([[ 0.4809, -1.1373,  0.1805,  0.1966, -0.2107,  0.9172,  0.4237]],
       dtype=torch.float64)
	q_value: tensor([[-32.3977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04187147550511383, distance: 1.1201303848812747 entropy 0.8164090514183044
epoch: 30, step: 67
	action: tensor([[ 1.0565, -0.2199, -0.0914,  0.2926,  0.8721, -0.0268,  0.0789]],
       dtype=torch.float64)
	q_value: tensor([[-34.1777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7149631001604069, distance: 0.6109522044653087 entropy 0.8164090514183044
epoch: 30, step: 68
	action: tensor([[ 0.1956, -0.3903,  0.8036,  0.2475, -0.1961, -0.5787,  0.6078]],
       dtype=torch.float64)
	q_value: tensor([[-29.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1379822211407934, distance: 1.0624654220312828 entropy 0.8164090514183044
epoch: 30, step: 69
	action: tensor([[-0.3789,  0.3124, -0.4406,  0.2935,  0.0906,  0.8678,  0.6548]],
       dtype=torch.float64)
	q_value: tensor([[-28.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 70
	action: tensor([[ 0.5359, -0.8318, -0.2006, -1.1587,  0.2887,  0.5553, -0.3520]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5534494524079159, distance: 1.4262815241055151 entropy 0.8164090514183044
epoch: 30, step: 71
	action: tensor([[ 0.8569,  0.1634,  0.4892, -0.6996,  0.0728,  0.3570,  0.6052]],
       dtype=torch.float64)
	q_value: tensor([[-33.3207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.628332301372132, distance: 0.6976443839897263 entropy 0.8164090514183044
epoch: 30, step: 72
	action: tensor([[ 0.9917, -1.8613,  0.0790,  0.8392, -1.1207,  1.1510,  0.0599]],
       dtype=torch.float64)
	q_value: tensor([[-30.1247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 73
	action: tensor([[ 1.0797, -0.8049, -0.2712, -0.1822,  0.5504, -0.0412,  0.5033]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2700701016334237, distance: 1.2896460715123268 entropy 0.8164090514183044
epoch: 30, step: 74
	action: tensor([[-0.1159,  0.1929,  0.5336,  0.1070, -0.3601,  0.0072,  0.1169]],
       dtype=torch.float64)
	q_value: tensor([[-31.7880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39119456325290247, distance: 0.8928858783393806 entropy 0.8164090514183044
epoch: 30, step: 75
	action: tensor([[-0.6149, -0.2537, -0.2029,  0.5544, -0.3590,  0.6856,  0.1498]],
       dtype=torch.float64)
	q_value: tensor([[-25.9287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27070345465929546, distance: 1.2899675889767197 entropy 0.8164090514183044
epoch: 30, step: 76
	action: tensor([[ 0.5896,  0.4668, -0.2429,  0.0444, -0.2724,  1.0507, -0.4441]],
       dtype=torch.float64)
	q_value: tensor([[-28.9994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 77
	action: tensor([[ 0.3954, -1.2576,  0.4468,  0.1338,  0.4944, -0.4337,  0.4692]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6700637883886518, distance: 1.4788469261854653 entropy 0.8164090514183044
epoch: 30, step: 78
	action: tensor([[ 0.6018, -0.3028,  0.2619,  0.1423, -0.0295, -0.2460, -0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-31.3688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4530141536411819, distance: 0.8463397477195416 entropy 0.8164090514183044
epoch: 30, step: 79
	action: tensor([[ 0.4634,  0.5899, -0.9033, -0.2908, -0.5117,  0.9017, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-25.8566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 80
	action: tensor([[ 0.4379, -0.2119,  0.3592, -0.7829,  0.4770,  0.7414,  0.0409]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22223327012395888, distance: 1.0092096024023594 entropy 0.8164090514183044
epoch: 30, step: 81
	action: tensor([[-0.4997,  0.4282,  0.0262, -0.3828,  1.0359,  1.1429, -0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-29.4146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 82
	action: tensor([[-0.0767, -1.6156, -0.4772,  0.1185, -0.1654,  0.0978,  0.7782]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8116218058289604, distance: 1.5402473242942976 entropy 0.8164090514183044
epoch: 30, step: 83
	action: tensor([[-0.4471, -0.1376,  0.5847,  0.6849,  0.0048,  0.6597,  0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-33.7500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 84
	action: tensor([[ 0.4824, -0.0787,  0.7046, -0.4225, -0.4391, -0.6379, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1945905789191842, distance: 1.0269872359608543 entropy 0.8164090514183044
epoch: 30, step: 85
	action: tensor([[ 0.4352, -0.0112, -0.0443, -0.9161,  0.1789,  0.6031, -0.1209]],
       dtype=torch.float64)
	q_value: tensor([[-27.9688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33558572807470755, distance: 0.9327734362202931 entropy 0.8164090514183044
epoch: 30, step: 86
	action: tensor([[-0.1376, -1.1663, -0.0049,  0.3800, -0.1132,  0.1638, -0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-28.5336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5394525888825046, distance: 1.419841456110331 entropy 0.8164090514183044
epoch: 30, step: 87
	action: tensor([[-0.0803,  0.1655, -0.6410,  0.3303, -0.3149,  0.3311,  0.1736]],
       dtype=torch.float64)
	q_value: tensor([[-30.6962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 88
	action: tensor([[ 0.7287, -0.6488, -0.9533,  0.2118,  0.4278,  0.0552, -0.9568]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20273460282699518, distance: 1.0217817721618676 entropy 0.8164090514183044
epoch: 30, step: 89
	action: tensor([[ 0.0921,  0.3818, -0.0642, -0.7166,  0.7867, -0.2025,  0.3212]],
       dtype=torch.float64)
	q_value: tensor([[-30.3864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30998550457831453, distance: 0.9505737133682063 entropy 0.8164090514183044
epoch: 30, step: 90
	action: tensor([[-0.1594,  0.0589,  0.2898,  0.0992, -0.4745, -0.6405,  0.3439]],
       dtype=torch.float64)
	q_value: tensor([[-26.3261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1278810195408191, distance: 1.0686723242554925 entropy 0.8164090514183044
epoch: 30, step: 91
	action: tensor([[ 0.8047, -0.0060, -0.7747, -1.8575, -0.1482,  0.2635, -0.4244]],
       dtype=torch.float64)
	q_value: tensor([[-26.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03825959595198214, distance: 1.1222396897586011 entropy 0.8164090514183044
epoch: 30, step: 92
	action: tensor([[ 1.1421,  0.7703,  0.2777,  0.9436, -0.0041,  0.3224, -0.3718]],
       dtype=torch.float64)
	q_value: tensor([[-35.3702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 93
	action: tensor([[ 0.4743,  0.8551,  0.2413, -1.2344, -0.5483,  0.1649,  0.4624]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 94
	action: tensor([[ 0.4731, -0.5638, -0.4360,  0.3325,  0.4160,  0.0495,  0.1726]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30503747485602384, distance: 0.9539758629498205 entropy 0.8164090514183044
epoch: 30, step: 95
	action: tensor([[ 0.0700, -0.2352,  1.0508, -0.2146, -0.0939, -0.3816,  0.4445]],
       dtype=torch.float64)
	q_value: tensor([[-27.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09491906268302519, distance: 1.1974232921695616 entropy 0.8164090514183044
epoch: 30, step: 96
	action: tensor([[-0.2367, -0.8171,  0.1329,  0.1886, -0.2656,  0.5639,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-28.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3338276549880146, distance: 1.3216198475764787 entropy 0.8164090514183044
epoch: 30, step: 97
	action: tensor([[ 0.6649,  0.1860, -0.2735,  0.3116, -0.1414,  0.1608, -0.3853]],
       dtype=torch.float64)
	q_value: tensor([[-29.5291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9185909043804071, distance: 0.3265074975301916 entropy 0.8164090514183044
epoch: 30, step: 98
	action: tensor([[ 0.3771, -0.5654,  0.5980,  0.5102,  0.3072,  0.5341,  0.7967]],
       dtype=torch.float64)
	q_value: tensor([[-25.2642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6503435650812426, distance: 0.6766708309370114 entropy 0.8164090514183044
epoch: 30, step: 99
	action: tensor([[ 0.1689,  0.0936,  0.2501,  0.0886,  0.0816,  1.0507, -0.4655]],
       dtype=torch.float64)
	q_value: tensor([[-30.6381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 100
	action: tensor([[ 0.0819, -1.1407,  0.1629, -1.0767,  0.8227, -0.6705,  0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9233590571569132, distance: 1.58703635963073 entropy 0.8164090514183044
epoch: 30, step: 101
	action: tensor([[-0.1438, -0.5387,  0.0785,  0.2218, -0.3780,  0.0185, -0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-34.0402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17733054620612054, distance: 1.2416691738791306 entropy 0.8164090514183044
epoch: 30, step: 102
	action: tensor([[ 1.1144, -1.1704,  0.3911, -0.4147,  0.4961,  0.3342,  0.1757]],
       dtype=torch.float64)
	q_value: tensor([[-26.9341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7394939993706362, distance: 1.5092742437478592 entropy 0.8164090514183044
epoch: 30, step: 103
	action: tensor([[ 0.1621, -0.1398,  0.9840, -0.1912,  0.1699,  0.0200,  0.6503]],
       dtype=torch.float64)
	q_value: tensor([[-33.8927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18330924969421225, distance: 1.0341547039116394 entropy 0.8164090514183044
epoch: 30, step: 104
	action: tensor([[ 0.2029, -0.1722,  0.3489, -0.7300,  0.3749, -0.4942, -0.5806]],
       dtype=torch.float64)
	q_value: tensor([[-27.8224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24253251096206907, distance: 1.275588436827772 entropy 0.8164090514183044
epoch: 30, step: 105
	action: tensor([[ 1.0578, -0.0710, -0.7384, -0.3116, -0.2955, -0.3414,  0.0909]],
       dtype=torch.float64)
	q_value: tensor([[-26.5977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4064801079746624, distance: 0.8816055850333117 entropy 0.8164090514183044
epoch: 30, step: 106
	action: tensor([[-0.0894, -1.0061,  1.6324, -0.2853, -0.0958,  0.1199,  1.4390]],
       dtype=torch.float64)
	q_value: tensor([[-29.0502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.81486871322283, distance: 1.5416269725236396 entropy 0.8164090514183044
epoch: 30, step: 107
	action: tensor([[ 0.7194,  1.0126, -1.2403,  1.0366, -0.1051,  0.8290,  0.0434]],
       dtype=torch.float64)
	q_value: tensor([[-35.9756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 108
	action: tensor([[ 0.4821,  0.0987,  0.0119,  0.2244,  0.9453,  0.3430, -0.2243]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 109
	action: tensor([[-0.2970, -0.0769,  0.7245, -0.3200,  0.0500, -0.3562, -0.0434]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4528452938803367, distance: 1.379324272564197 entropy 0.8164090514183044
epoch: 30, step: 110
	action: tensor([[ 1.0633, -0.3500,  0.6080,  0.7938,  0.7717,  0.6858,  0.2100]],
       dtype=torch.float64)
	q_value: tensor([[-25.8767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6333957581935361, distance: 0.6928758687080881 entropy 0.8164090514183044
epoch: 30, step: 111
	action: tensor([[ 0.1070,  0.0553, -0.0517, -0.1715, -0.1002, -0.7416,  0.9800]],
       dtype=torch.float64)
	q_value: tensor([[-33.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2571330056428073, distance: 0.9863072484243697 entropy 0.8164090514183044
epoch: 30, step: 112
	action: tensor([[ 0.6889, -0.0109, -0.4599,  0.4959,  0.8137, -0.4576,  0.3771]],
       dtype=torch.float64)
	q_value: tensor([[-28.5189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7819569125425242, distance: 0.5343525112435789 entropy 0.8164090514183044
epoch: 30, step: 113
	action: tensor([[ 0.4056, -0.1701, -0.0511, -0.0856,  0.5862, -0.8439,  0.5318]],
       dtype=torch.float64)
	q_value: tensor([[-27.5242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22668997613455832, distance: 1.0063139962839311 entropy 0.8164090514183044
epoch: 30, step: 114
	action: tensor([[ 0.8908, -0.2804, -0.4289, -0.3875,  0.2261,  0.1541, -0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-26.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005147258732963, distance: 0.9570750202554658 entropy 0.8164090514183044
epoch: 30, step: 115
	action: tensor([[-0.6377, -0.6125,  0.2661, -0.2681, -0.6290, -0.1967, -0.2954]],
       dtype=torch.float64)
	q_value: tensor([[-27.6325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9850518565801163, distance: 1.6122879997615407 entropy 0.8164090514183044
epoch: 30, step: 116
	action: tensor([[-0.5575, -0.3742, -0.8473, -0.4029, -0.2713, -1.0071, -0.9937]],
       dtype=torch.float64)
	q_value: tensor([[-28.7271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10374199579079568, distance: 1.2022380717938552 entropy 0.8164090514183044
epoch: 30, step: 117
	action: tensor([[ 0.3479, -0.5239,  0.5932, -0.8820, -0.1753, -0.1283, -0.1647]],
       dtype=torch.float64)
	q_value: tensor([[-30.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.508736260985515, distance: 1.405605207350274 entropy 0.8164090514183044
epoch: 30, step: 118
	action: tensor([[ 0.7726,  0.7448,  0.6499, -0.2977, -0.4984,  0.4393,  0.1025]],
       dtype=torch.float64)
	q_value: tensor([[-28.5044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9687899907167256, distance: 0.20216391634083744 entropy 0.8164090514183044
epoch: 30, step: 119
	action: tensor([[ 0.8831, -0.6002, -0.2852, -0.0322,  0.5313,  0.8731, -0.5890]],
       dtype=torch.float64)
	q_value: tensor([[-30.7333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45146608921311915, distance: 0.8475365453705174 entropy 0.8164090514183044
epoch: 30, step: 120
	action: tensor([[ 0.8023, -0.2518,  0.1937,  0.9060,  0.1461,  0.9558,  0.9340]],
       dtype=torch.float64)
	q_value: tensor([[-31.8250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9726554328029963, distance: 0.18923097201649544 entropy 0.8164090514183044
epoch: 30, step: 121
	action: tensor([[ 0.0060, -0.7499,  0.3102,  0.0656,  0.5114, -0.7030, -0.9027]],
       dtype=torch.float64)
	q_value: tensor([[-33.6789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.525554620968909, distance: 1.4134178579347454 entropy 0.8164090514183044
epoch: 30, step: 122
	action: tensor([[-0.6454, -0.0204, -0.1864,  0.0219, -0.3041, -0.0844,  1.1637]],
       dtype=torch.float64)
	q_value: tensor([[-29.2765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48992755185591474, distance: 1.3968162181227364 entropy 0.8164090514183044
epoch: 30, step: 123
	action: tensor([[ 1.0153, -0.7994,  0.3381, -0.5019, -1.1328,  0.3649,  0.4511]],
       dtype=torch.float64)
	q_value: tensor([[-29.9089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3034378685035517, distance: 1.3064772763775545 entropy 0.8164090514183044
epoch: 30, step: 124
	action: tensor([[ 0.3559,  0.3866,  0.2455,  0.1790, -0.5210, -0.8123,  0.3284]],
       dtype=torch.float64)
	q_value: tensor([[-36.5606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 125
	action: tensor([[ 0.9015, -0.8436, -0.1760, -0.4545, -0.0098,  0.3210,  0.7388]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36306979718527765, distance: 1.336028555776311 entropy 0.8164090514183044
epoch: 30, step: 126
	action: tensor([[ 0.1350,  0.5739,  1.0759,  0.1889,  0.2038,  0.2471, -0.0740]],
       dtype=torch.float64)
	q_value: tensor([[-33.5219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 127
	action: tensor([[ 0.3732, -2.1031,  0.8619, -0.0389,  0.6457, -0.2627, -0.3188]],
       dtype=torch.float64)
	q_value: tensor([[-32.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
LOSS epoch 30 actor 365.49311051037006 critic 233.15135348543134 
epoch: 31, step: 0
	action: tensor([[ 0.1627, -0.2218, -0.0229, -1.5283, -0.1625,  0.1638, -0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3134081118587062, distance: 1.3114645036651777 entropy 0.8164090514183044
epoch: 31, step: 1
	action: tensor([[ 0.0192,  0.4288, -0.4613, -0.6035,  0.5311, -1.0223,  0.3028]],
       dtype=torch.float64)
	q_value: tensor([[-32.3073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3978982791673614, distance: 0.8879563707690987 entropy 0.8164090514183044
epoch: 31, step: 2
	action: tensor([[ 0.4233, -0.4729,  0.1281, -0.6749, -1.1714, -0.2376, -0.4386]],
       dtype=torch.float64)
	q_value: tensor([[-27.4516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25779945884989774, distance: 1.28340106410606 entropy 0.8164090514183044
epoch: 31, step: 3
	action: tensor([[ 0.0912, -0.2285,  1.0588,  0.2346, -1.2667, -0.2774,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[-32.6326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28658279770588335, distance: 0.9665592509087718 entropy 0.8164090514183044
epoch: 31, step: 4
	action: tensor([[ 0.6700, -0.2453, -0.4950,  0.2256, -0.6895, -0.4675,  0.5737]],
       dtype=torch.float64)
	q_value: tensor([[-34.4954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5852974055552957, distance: 0.7369280192770421 entropy 0.8164090514183044
epoch: 31, step: 5
	action: tensor([[-0.1812, -0.3444,  0.1429,  0.1112,  0.2284,  0.0513,  1.0257]],
       dtype=torch.float64)
	q_value: tensor([[-31.3847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10515435573746013, distance: 1.2030070241669035 entropy 0.8164090514183044
epoch: 31, step: 6
	action: tensor([[ 0.0808, -0.1326, -0.2323,  0.1484, -0.5693, -1.0497,  0.0623]],
       dtype=torch.float64)
	q_value: tensor([[-29.6201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3577886531374105, distance: 0.9170556281644836 entropy 0.8164090514183044
epoch: 31, step: 7
	action: tensor([[-0.0827,  1.1078,  0.4377,  0.0572, -0.2301, -0.5868,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[-29.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 8
	action: tensor([[-0.4117, -0.8829,  0.1468, -0.0939,  1.0609, -0.4442,  0.8011]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0100465641200636, distance: 1.6224067794065675 entropy 0.8164090514183044
epoch: 31, step: 9
	action: tensor([[-0.1335,  0.5762, -0.0153,  0.2910,  0.5341,  1.1157,  0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-32.2003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 10
	action: tensor([[ 1.4919,  0.4491, -0.1818,  0.2328,  0.5129,  0.0229, -0.5219]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 11
	action: tensor([[ 0.3612, -0.1149,  0.4054, -0.7853, -0.1845,  0.3361,  0.0378]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10442337549889646, distance: 1.0829491581462811 entropy 0.8164090514183044
epoch: 31, step: 12
	action: tensor([[-0.2140, -0.6817, -0.6407,  0.3482,  0.4211, -0.4944, -0.1069]],
       dtype=torch.float64)
	q_value: tensor([[-28.7316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5193546319719755, distance: 1.410542806210897 entropy 0.8164090514183044
epoch: 31, step: 13
	action: tensor([[ 0.6175,  1.4243,  0.2891,  0.0625,  0.6223, -0.1767,  0.2047]],
       dtype=torch.float64)
	q_value: tensor([[-29.1510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 14
	action: tensor([[ 0.4476, -0.8141, -0.0640, -1.4130, -0.0563, -0.3328,  0.4665]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6322404291723538, distance: 1.4620046658852752 entropy 0.8164090514183044
epoch: 31, step: 15
	action: tensor([[ 0.1196, -0.0672, -0.0640,  0.8172,  0.7810, -0.3257, -0.6837]],
       dtype=torch.float64)
	q_value: tensor([[-34.9003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 16
	action: tensor([[ 0.7010, -0.4492, -0.6258, -0.5465, -0.0039,  0.1489, -0.1030]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08903215866432035, distance: 1.0922152001549115 entropy 0.8164090514183044
epoch: 31, step: 17
	action: tensor([[ 0.1998, -0.3716,  0.0453,  0.1457, -0.4643, -1.2056, -0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-29.7608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.173705384895808, distance: 1.04021749690914 entropy 0.8164090514183044
epoch: 31, step: 18
	action: tensor([[ 1.3308, -0.8183,  0.0403, -0.5532,  0.4026,  0.1955, -0.1904]],
       dtype=torch.float64)
	q_value: tensor([[-30.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5505524279636864, distance: 1.4249509688504878 entropy 0.8164090514183044
epoch: 31, step: 19
	action: tensor([[ 0.6182,  0.1842,  0.3408, -0.6220, -0.1847, -0.3235, -0.7080]],
       dtype=torch.float64)
	q_value: tensor([[-33.7750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4717090256343569, distance: 0.8317509178153765 entropy 0.8164090514183044
epoch: 31, step: 20
	action: tensor([[ 0.4138,  0.1522,  0.1492, -0.5762,  1.0677, -0.5178, -0.1647]],
       dtype=torch.float64)
	q_value: tensor([[-27.8155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3294320548956148, distance: 0.9370830617556157 entropy 0.8164090514183044
epoch: 31, step: 21
	action: tensor([[ 0.7757, -0.3644,  0.6616, -0.6251,  0.3171, -0.0897, -0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-29.2412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.055358215905604524, distance: 1.1122188957114443 entropy 0.8164090514183044
epoch: 31, step: 22
	action: tensor([[ 0.2325,  0.4281,  0.2153,  0.1840, -0.1972,  0.0927,  0.1202]],
       dtype=torch.float64)
	q_value: tensor([[-29.2573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 23
	action: tensor([[ 0.3273, -0.8421,  0.1925, -0.4382,  0.8911,  1.0073,  0.4466]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12203170354906212, distance: 1.2121580706973916 entropy 0.8164090514183044
epoch: 31, step: 24
	action: tensor([[ 0.5901,  0.2988, -0.8394, -0.1510, -0.0439, -0.6810, -0.2787]],
       dtype=torch.float64)
	q_value: tensor([[-35.2473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8202430625336996, distance: 0.48517623883845207 entropy 0.8164090514183044
epoch: 31, step: 25
	action: tensor([[ 0.9376, -0.7604, -0.0233, -0.0991, -0.2690,  0.0846,  0.9460]],
       dtype=torch.float64)
	q_value: tensor([[-26.7571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12610943810961794, distance: 1.2143587108834037 entropy 0.8164090514183044
epoch: 31, step: 26
	action: tensor([[ 0.7192, -0.8200, -0.2845, -0.3402,  0.2740,  0.0349,  0.1349]],
       dtype=torch.float64)
	q_value: tensor([[-34.4656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3624466200396055, distance: 1.3357231137341004 entropy 0.8164090514183044
epoch: 31, step: 27
	action: tensor([[-0.2227, -0.2838, -0.0739,  0.1133,  0.0303, -0.1602,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-30.7656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1303886317389047, distance: 1.2166637927050763 entropy 0.8164090514183044
epoch: 31, step: 28
	action: tensor([[ 0.2018, -0.2822,  0.5103, -0.4429,  0.6739,  0.0429, -0.6337]],
       dtype=torch.float64)
	q_value: tensor([[-25.6470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07528435374987508, distance: 1.1866382863082374 entropy 0.8164090514183044
epoch: 31, step: 29
	action: tensor([[-0.4398, -0.5123, -0.2340,  0.5321,  0.5054,  1.3711, -0.0492]],
       dtype=torch.float64)
	q_value: tensor([[-28.7352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23751706013885177, distance: 0.9992444822141994 entropy 0.8164090514183044
epoch: 31, step: 30
	action: tensor([[ 0.6438,  0.0580,  0.5675, -0.0203,  0.1832,  0.4415,  0.2883]],
       dtype=torch.float64)
	q_value: tensor([[-33.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8489964518476686, distance: 0.4446827362411417 entropy 0.8164090514183044
epoch: 31, step: 31
	action: tensor([[-0.1781, -1.4458, -0.0883,  0.1185, -0.2580, -0.6841,  0.2999]],
       dtype=torch.float64)
	q_value: tensor([[-28.5340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8597212815923512, distance: 1.5605605489414445 entropy 0.8164090514183044
epoch: 31, step: 32
	action: tensor([[ 0.5350, -0.3778, -0.7677,  0.6656,  0.5593, -0.4837, -0.3625]],
       dtype=torch.float64)
	q_value: tensor([[-32.8841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3749724447131729, distance: 0.9047035103239045 entropy 0.8164090514183044
epoch: 31, step: 33
	action: tensor([[-0.9649, -0.7211,  0.5724,  0.2150, -0.6874, -0.6852,  0.9619]],
       dtype=torch.float64)
	q_value: tensor([[-28.9645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1138464119134057, distance: 1.6637704547336916 entropy 0.8164090514183044
epoch: 31, step: 34
	action: tensor([[ 1.1197, -0.6965,  0.1823,  0.5952, -0.4060,  0.0624,  0.8494]],
       dtype=torch.float64)
	q_value: tensor([[-34.5559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682319446495574, distance: 0.834483618468519 entropy 0.8164090514183044
epoch: 31, step: 35
	action: tensor([[ 1.1487, -0.8069,  0.1376, -0.3262, -0.0487,  0.3373, -0.1700]],
       dtype=torch.float64)
	q_value: tensor([[-34.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2506208886874759, distance: 1.279733481266442 entropy 0.8164090514183044
epoch: 31, step: 36
	action: tensor([[ 0.3506, -0.7769,  0.6997, -0.5773,  0.4962, -0.8303,  0.5490]],
       dtype=torch.float64)
	q_value: tensor([[-32.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5735395339226526, distance: 1.4354746341621443 entropy 0.8164090514183044
epoch: 31, step: 37
	action: tensor([[-0.9125,  0.1842, -0.6961, -0.0732, -0.6265, -0.2150, -0.1296]],
       dtype=torch.float64)
	q_value: tensor([[-31.8513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.66625408876186, distance: 1.4771592124290531 entropy 0.8164090514183044
epoch: 31, step: 38
	action: tensor([[ 0.1379, -0.5266, -0.3750, -0.1114,  0.6671, -0.7697,  0.5088]],
       dtype=torch.float64)
	q_value: tensor([[-28.3834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28802702458086826, distance: 1.298730921797419 entropy 0.8164090514183044
epoch: 31, step: 39
	action: tensor([[-0.2677, -0.3331,  0.0137, -0.4538, -1.1716,  1.0783,  0.8189]],
       dtype=torch.float64)
	q_value: tensor([[-29.4991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49931538790776764, distance: 1.401209884951244 entropy 0.8164090514183044
epoch: 31, step: 40
	action: tensor([[-0.0633, -0.7303,  0.1049, -0.3383,  0.4343,  0.5130, -0.1825]],
       dtype=torch.float64)
	q_value: tensor([[-37.5789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42766985994074425, distance: 1.3673213300753149 entropy 0.8164090514183044
epoch: 31, step: 41
	action: tensor([[ 0.4974, -0.0706,  0.2853, -0.7070, -0.5674, -0.5521,  0.1559]],
       dtype=torch.float64)
	q_value: tensor([[-29.6541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07855040132483859, distance: 1.0984808390929202 entropy 0.8164090514183044
epoch: 31, step: 42
	action: tensor([[ 0.7329, -0.9074,  0.2686, -0.2101, -0.8994, -1.0347, -0.6718]],
       dtype=torch.float64)
	q_value: tensor([[-29.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4527607376668299, distance: 1.3792841333507475 entropy 0.8164090514183044
epoch: 31, step: 43
	action: tensor([[ 0.0586,  0.0896, -0.4835, -0.1238,  0.2733, -0.9739,  0.5330]],
       dtype=torch.float64)
	q_value: tensor([[-35.1217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3025204901116395, distance: 0.9557018355358216 entropy 0.8164090514183044
epoch: 31, step: 44
	action: tensor([[-0.5822, -0.7008, -1.2742, -0.4877,  0.5092, -0.1381,  0.3019]],
       dtype=torch.float64)
	q_value: tensor([[-27.5924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48533838399033513, distance: 1.3946633725736746 entropy 0.8164090514183044
epoch: 31, step: 45
	action: tensor([[ 1.0630, -0.4222, -0.2958, -0.8552, -0.7929,  0.6844,  0.3073]],
       dtype=torch.float64)
	q_value: tensor([[-32.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04617870177674077, distance: 1.170468230830716 entropy 0.8164090514183044
epoch: 31, step: 46
	action: tensor([[ 0.2115, -0.1291, -0.3373, -0.9743,  0.1012,  0.0291,  0.4068]],
       dtype=torch.float64)
	q_value: tensor([[-36.7044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05281569988937762, distance: 1.1137146659448582 entropy 0.8164090514183044
epoch: 31, step: 47
	action: tensor([[ 1.3430,  0.1253, -0.1207,  0.0279,  0.4280,  0.7020, -0.1540]],
       dtype=torch.float64)
	q_value: tensor([[-29.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.785466232761821, distance: 0.5300349686792116 entropy 0.8164090514183044
epoch: 31, step: 48
	action: tensor([[-0.0084, -0.0352, -1.3276, -0.5753, -0.4511,  0.8253,  0.4377]],
       dtype=torch.float64)
	q_value: tensor([[-31.5884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060306868145246, distance: 0.9532939276114676 entropy 0.8164090514183044
epoch: 31, step: 49
	action: tensor([[-0.1508, -0.4408, -1.7149, -0.2863,  0.2112, -0.0390,  0.5339]],
       dtype=torch.float64)
	q_value: tensor([[-33.0802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02042973739630094, distance: 1.132594607480334 entropy 0.8164090514183044
epoch: 31, step: 50
	action: tensor([[-0.4587, -0.3745, -0.1697,  0.8457, -0.6967,  0.1021,  0.1638]],
       dtype=torch.float64)
	q_value: tensor([[-33.8511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13721795278338678, distance: 1.2203335374935689 entropy 0.8164090514183044
epoch: 31, step: 51
	action: tensor([[ 0.5648, -1.4346, -0.5454, -0.5513, -0.2495, -0.2244,  0.2031]],
       dtype=torch.float64)
	q_value: tensor([[-31.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 52
	action: tensor([[ 0.5089,  0.5987,  0.6044, -0.7457, -0.0108,  0.6097, -0.5708]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 53
	action: tensor([[ 1.4184, -0.3966,  0.5858, -0.4307,  0.4847,  0.2085,  0.3151]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06176162577558009, distance: 1.108442806162389 entropy 0.8164090514183044
epoch: 31, step: 54
	action: tensor([[ 0.5111, -0.7896,  0.0567,  0.4063, -0.1175,  0.0328,  0.9172]],
       dtype=torch.float64)
	q_value: tensor([[-32.7408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19939908467793332, distance: 1.0239169547523157 entropy 0.8164090514183044
epoch: 31, step: 55
	action: tensor([[ 0.6414, -0.2762, -0.3890, -0.6401,  0.5866,  0.0627,  0.2017]],
       dtype=torch.float64)
	q_value: tensor([[-32.3876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15239498199707657, distance: 1.0535458794260066 entropy 0.8164090514183044
epoch: 31, step: 56
	action: tensor([[ 0.1616, -0.8124, -0.3792,  0.1198,  0.5495,  0.2472,  0.7126]],
       dtype=torch.float64)
	q_value: tensor([[-29.7588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18095955967164423, distance: 1.2435813671660052 entropy 0.8164090514183044
epoch: 31, step: 57
	action: tensor([[-0.0164,  0.3137,  0.1455, -0.0985,  0.3290, -0.6620,  0.1114]],
       dtype=torch.float64)
	q_value: tensor([[-31.6049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30579332593610675, distance: 0.953456942988853 entropy 0.8164090514183044
epoch: 31, step: 58
	action: tensor([[ 0.3104, -1.0140,  0.7783, -0.0414, -0.0972,  0.4143, -0.5212]],
       dtype=torch.float64)
	q_value: tensor([[-25.1534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34048227606871184, distance: 1.32491260315715 entropy 0.8164090514183044
epoch: 31, step: 59
	action: tensor([[ 0.8081, -0.0958, -0.9888, -0.4238,  0.1557, -0.2247,  0.4212]],
       dtype=torch.float64)
	q_value: tensor([[-33.0723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47314516932090145, distance: 0.8306196030612736 entropy 0.8164090514183044
epoch: 31, step: 60
	action: tensor([[ 0.6175, -0.3253,  0.3388,  0.7327, -1.2855,  0.0371, -0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-30.7085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8398816265455942, distance: 0.45790699527145134 entropy 0.8164090514183044
epoch: 31, step: 61
	action: tensor([[-0.6422,  0.0630,  0.8090, -0.3707,  0.7707, -0.6376, -0.0546]],
       dtype=torch.float64)
	q_value: tensor([[-34.8276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8270430709122738, distance: 1.5467890396597255 entropy 0.8164090514183044
epoch: 31, step: 62
	action: tensor([[ 1.0079,  0.2367, -0.8435, -0.6360,  0.2239,  0.4571, -0.1489]],
       dtype=torch.float64)
	q_value: tensor([[-29.9416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7189455363181891, distance: 0.606669186067065 entropy 0.8164090514183044
epoch: 31, step: 63
	action: tensor([[-0.5915, -0.8069,  0.1207,  0.7289,  0.0269, -0.1481,  0.3303]],
       dtype=torch.float64)
	q_value: tensor([[-30.7010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5268900387101001, distance: 1.4140363511614664 entropy 0.8164090514183044
epoch: 31, step: 64
	action: tensor([[ 0.4997, -0.9458,  0.4195, -0.2399, -0.1767, -0.3443, -0.2329]],
       dtype=torch.float64)
	q_value: tensor([[-31.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6167669190246439, distance: 1.4550583196905968 entropy 0.8164090514183044
epoch: 31, step: 65
	action: tensor([[ 0.2969, -0.0126, -0.5756, -0.7473, -0.4378, -0.0143, -1.2052]],
       dtype=torch.float64)
	q_value: tensor([[-30.2356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.351655565869728, distance: 0.9214241415975968 entropy 0.8164090514183044
epoch: 31, step: 66
	action: tensor([[ 0.2437, -1.1011, -0.1842, -0.0519, -0.2774,  0.9270, -0.6650]],
       dtype=torch.float64)
	q_value: tensor([[-29.5715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3475269596482453, distance: 1.3283894709820228 entropy 0.8164090514183044
epoch: 31, step: 67
	action: tensor([[ 0.6470, -0.6188, -0.0798, -0.1155, -0.9582, -0.0784, -1.0024]],
       dtype=torch.float64)
	q_value: tensor([[-34.8656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027967474820215332, distance: 1.1282285666266225 entropy 0.8164090514183044
epoch: 31, step: 68
	action: tensor([[-0.3685, -0.6263, -0.1764, -0.8434,  0.3472, -0.8351,  0.7251]],
       dtype=torch.float64)
	q_value: tensor([[-32.1793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5899176539561088, distance: 1.4429258341446036 entropy 0.8164090514183044
epoch: 31, step: 69
	action: tensor([[-0.1187, -0.1113, -0.0494, -0.1399, -0.7945, -0.2413, -0.6320]],
       dtype=torch.float64)
	q_value: tensor([[-32.0821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03438776274523425, distance: 1.1244964112408857 entropy 0.8164090514183044
epoch: 31, step: 70
	action: tensor([[ 0.9887, -0.6075,  0.1770,  0.1763,  0.8240, -0.7364, -0.1326]],
       dtype=torch.float64)
	q_value: tensor([[-28.1142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15119076505690598, distance: 1.0542940153320184 entropy 0.8164090514183044
epoch: 31, step: 71
	action: tensor([[ 1.5368, -0.4822,  0.8326, -0.4389,  0.9503, -0.4865, -0.4190]],
       dtype=torch.float64)
	q_value: tensor([[-31.2416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1327248939519423, distance: 1.0657004119082067 entropy 0.8164090514183044
epoch: 31, step: 72
	action: tensor([[ 0.2067, -0.6323,  0.3635,  0.9500, -0.2435,  0.0767, -0.3352]],
       dtype=torch.float64)
	q_value: tensor([[-35.0836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6663266326531326, distance: 0.6610243615113608 entropy 0.8164090514183044
epoch: 31, step: 73
	action: tensor([[-0.8633,  0.6878,  1.0282,  0.8724, -0.0453, -0.6869, -0.2423]],
       dtype=torch.float64)
	q_value: tensor([[-31.9183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 74
	action: tensor([[-0.0549,  0.4581,  0.3179, -0.2096,  0.1832, -0.4878, -0.6268]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 75
	action: tensor([[ 0.2353, -0.5192, -0.3492,  0.0144,  0.4238,  0.1982,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10167106339233278, distance: 1.0846119569967498 entropy 0.8164090514183044
epoch: 31, step: 76
	action: tensor([[-0.3840, -0.1833,  0.2324, -0.3283,  0.4611,  0.9184, -0.8558]],
       dtype=torch.float64)
	q_value: tensor([[-27.4773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04508558641289473, distance: 1.1698565804532879 entropy 0.8164090514183044
epoch: 31, step: 77
	action: tensor([[ 0.2143,  0.4117,  0.6005, -0.3367, -0.5366,  0.7476, -0.1528]],
       dtype=torch.float64)
	q_value: tensor([[-30.5986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 78
	action: tensor([[ 0.0583, -0.2878, -0.5431, -0.6825,  0.8041, -0.0031, -0.4875]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010031563315433423, distance: 1.1385900056799707 entropy 0.8164090514183044
epoch: 31, step: 79
	action: tensor([[-0.4478, -0.3644,  0.2791,  0.1943, -1.0040,  0.1586, -0.4717]],
       dtype=torch.float64)
	q_value: tensor([[-29.3689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35284476078751914, distance: 1.3310080291345927 entropy 0.8164090514183044
epoch: 31, step: 80
	action: tensor([[ 0.3898, -0.0096,  0.1786, -0.2238,  0.2379, -0.7536, -1.1292]],
       dtype=torch.float64)
	q_value: tensor([[-31.8895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3068086560599056, distance: 0.9527594361644754 entropy 0.8164090514183044
epoch: 31, step: 81
	action: tensor([[-0.3186,  0.3911,  0.1079, -0.4710, -0.5223, -0.3793,  0.2192]],
       dtype=torch.float64)
	q_value: tensor([[-29.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03164048775633066, distance: 1.1260949341857884 entropy 0.8164090514183044
epoch: 31, step: 82
	action: tensor([[-1.3300, -0.1287, -0.4676,  0.0957, -0.1545,  0.2214,  0.2572]],
       dtype=torch.float64)
	q_value: tensor([[-27.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4106730225675905, distance: 1.776748048756073 entropy 0.8164090514183044
epoch: 31, step: 83
	action: tensor([[ 0.3463,  0.0205,  0.1169, -0.6528, -0.6389, -0.1408,  0.2692]],
       dtype=torch.float64)
	q_value: tensor([[-29.5163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20233315706296018, distance: 1.0220389878538312 entropy 0.8164090514183044
epoch: 31, step: 84
	action: tensor([[ 0.7021, -0.7445,  0.8058, -0.6063,  1.7648, -0.0302, -0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-28.7405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35685589071166546, distance: 1.3329797611395495 entropy 0.8164090514183044
epoch: 31, step: 85
	action: tensor([[ 0.6136,  0.8257, -0.2616,  0.3391, -0.0558, -0.4669,  0.1957]],
       dtype=torch.float64)
	q_value: tensor([[-36.8741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 86
	action: tensor([[-0.6121, -0.3665, -0.1114, -0.6117,  0.0348, -1.3034, -0.4092]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4763869628141044, distance: 1.390454538623301 entropy 0.8164090514183044
epoch: 31, step: 87
	action: tensor([[-0.4269, -1.2529, -0.3839, -1.1876, -0.0401, -0.6970,  0.2419]],
       dtype=torch.float64)
	q_value: tensor([[-30.4546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23139779831161778, distance: 1.269860106177967 entropy 0.8164090514183044
epoch: 31, step: 88
	action: tensor([[-1.2833, -0.1471, -0.0912,  0.7699,  0.0923,  0.2343,  0.5362]],
       dtype=torch.float64)
	q_value: tensor([[-36.3464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8043413104043071, distance: 1.5371492569583725 entropy 0.8164090514183044
epoch: 31, step: 89
	action: tensor([[-0.6251, -0.7537,  0.3893, -0.0158, -0.2156, -1.1390,  0.3483]],
       dtype=torch.float64)
	q_value: tensor([[-31.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9215589875361021, distance: 1.5862935329978685 entropy 0.8164090514183044
epoch: 31, step: 90
	action: tensor([[ 0.3639,  0.2064,  0.6920,  0.1717,  0.8316, -0.1548, -1.2235]],
       dtype=torch.float64)
	q_value: tensor([[-31.9129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.773754357977841, distance: 0.5443106157913266 entropy 0.8164090514183044
epoch: 31, step: 91
	action: tensor([[-0.0436, -0.6032,  0.2473, -0.7599, -0.3028,  0.6862,  1.0305]],
       dtype=torch.float64)
	q_value: tensor([[-32.1170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5126171769965326, distance: 1.4074118624988428 entropy 0.8164090514183044
epoch: 31, step: 92
	action: tensor([[ 0.0666, -1.2387, -0.4023, -0.2620, -0.1386, -0.3060, -0.2370]],
       dtype=torch.float64)
	q_value: tensor([[-35.4070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8668998643987085, distance: 1.5635695544951613 entropy 0.8164090514183044
epoch: 31, step: 93
	action: tensor([[ 1.1637, -0.0444,  0.9478, -0.5162,  0.2633, -0.2693, -0.3967]],
       dtype=torch.float64)
	q_value: tensor([[-32.8767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5335682028915476, distance: 0.7815392032220304 entropy 0.8164090514183044
epoch: 31, step: 94
	action: tensor([[ 0.2773, -1.0119, -0.4921, -0.4334, -0.4566,  0.4376,  0.3832]],
       dtype=torch.float64)
	q_value: tensor([[-31.2752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6157933142116412, distance: 1.4546201411550683 entropy 0.8164090514183044
epoch: 31, step: 95
	action: tensor([[ 0.2324, -0.0740,  1.0263, -0.3181,  0.0344, -0.7353, -0.3138]],
       dtype=torch.float64)
	q_value: tensor([[-34.4195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06904466214692617, distance: 1.1041323042439202 entropy 0.8164090514183044
epoch: 31, step: 96
	action: tensor([[ 0.2155, -0.0710,  0.1846, -0.8993, -0.2026,  0.0307,  0.2559]],
       dtype=torch.float64)
	q_value: tensor([[-29.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08339254894936277, distance: 1.1911038144895523 entropy 0.8164090514183044
epoch: 31, step: 97
	action: tensor([[ 0.3607, -0.5114,  0.5269, -0.5526, -0.0256, -0.3334,  0.1493]],
       dtype=torch.float64)
	q_value: tensor([[-28.6146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36268635526772885, distance: 1.3358406250536226 entropy 0.8164090514183044
epoch: 31, step: 98
	action: tensor([[ 0.5141, -0.3648, -0.1257,  0.5438,  0.3722,  0.8136,  0.0779]],
       dtype=torch.float64)
	q_value: tensor([[-28.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8584779985739873, distance: 0.43049555756205266 entropy 0.8164090514183044
epoch: 31, step: 99
	action: tensor([[ 0.0295,  0.8492,  0.3387, -0.5651, -0.1640, -0.0376, -0.7246]],
       dtype=torch.float64)
	q_value: tensor([[-30.1536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 100
	action: tensor([[-0.1459,  0.1129,  0.0416, -0.6301, -0.2054, -0.0411, -0.5401]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.036355404595783014, distance: 1.164960101552587 entropy 0.8164090514183044
epoch: 31, step: 101
	action: tensor([[ 0.3088,  0.2058,  0.3827, -0.2814, -0.1610,  0.1751,  0.6891]],
       dtype=torch.float64)
	q_value: tensor([[-26.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5946348554488308, distance: 0.7285844446550379 entropy 0.8164090514183044
epoch: 31, step: 102
	action: tensor([[ 1.8630, -0.2291,  1.2305,  0.2370, -0.2611, -0.1219,  0.5671]],
       dtype=torch.float64)
	q_value: tensor([[-28.4862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1159388379041103, distance: 1.075964268629968 entropy 0.8164090514183044
epoch: 31, step: 103
	action: tensor([[ 0.2363, -0.6642, -0.6279, -1.0070,  0.7848,  0.6523,  0.8159]],
       dtype=torch.float64)
	q_value: tensor([[-35.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04984975716590134, distance: 1.1725200269859715 entropy 0.8164090514183044
epoch: 31, step: 104
	action: tensor([[-0.2916,  0.0618, -0.1060,  0.8638,  0.7350, -0.0810,  0.3415]],
       dtype=torch.float64)
	q_value: tensor([[-37.3880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 105
	action: tensor([[ 0.2920, -0.4104, -1.0739, -0.5500, -0.5638,  0.3795, -0.4993]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14783790761051452, distance: 1.056374231649919 entropy 0.8164090514183044
epoch: 31, step: 106
	action: tensor([[ 0.6977, -0.4191,  0.9862,  0.0143, -0.4616, -0.4477,  0.1596]],
       dtype=torch.float64)
	q_value: tensor([[-31.2849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20455984366529745, distance: 1.0206114802683144 entropy 0.8164090514183044
epoch: 31, step: 107
	action: tensor([[ 0.2326, -0.2524, -0.2558, -0.8142,  0.0472, -0.6532,  0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-31.4245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13782492396037083, distance: 1.2206591604197061 entropy 0.8164090514183044
epoch: 31, step: 108
	action: tensor([[ 0.2550,  0.0029, -0.4837, -0.3113, -0.1797, -0.0598, -0.2286]],
       dtype=torch.float64)
	q_value: tensor([[-28.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.416487846901322, distance: 0.8741413132536819 entropy 0.8164090514183044
epoch: 31, step: 109
	action: tensor([[ 0.7309,  0.3339,  0.6207,  0.5035,  0.0179, -0.7394,  0.2729]],
       dtype=torch.float64)
	q_value: tensor([[-25.3175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9670072937932181, distance: 0.20785748259148196 entropy 0.8164090514183044
epoch: 31, step: 110
	action: tensor([[ 1.2275, -0.3617, -0.9512,  0.6502, -0.3927, -0.4965,  0.1392]],
       dtype=torch.float64)
	q_value: tensor([[-29.2759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6023510048409211, distance: 0.7216168040855522 entropy 0.8164090514183044
epoch: 31, step: 111
	action: tensor([[-0.2193, -0.9909,  1.1438, -0.0313, -0.5836,  0.1134, -0.3817]],
       dtype=torch.float64)
	q_value: tensor([[-33.5269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7184330114697952, distance: 1.5001096221709327 entropy 0.8164090514183044
epoch: 31, step: 112
	action: tensor([[ 0.8682, -0.1186,  0.2862, -1.6986, -0.3432, -0.1239, -0.4677]],
       dtype=torch.float64)
	q_value: tensor([[-34.6132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1836998712295692, distance: 1.2450233411022202 entropy 0.8164090514183044
epoch: 31, step: 113
	action: tensor([[ 0.4049,  0.6868, -1.1984,  0.5316, -0.1008,  1.0179,  0.4894]],
       dtype=torch.float64)
	q_value: tensor([[-35.0336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 114
	action: tensor([[ 0.9855, -0.2063,  0.2605,  0.2744, -0.1231, -0.5179, -0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5824791754314762, distance: 0.7394277820969012 entropy 0.8164090514183044
epoch: 31, step: 115
	action: tensor([[-0.1460,  0.2824, -0.0841,  0.1761, -0.2174,  0.1311,  0.5493]],
       dtype=torch.float64)
	q_value: tensor([[-28.6825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 116
	action: tensor([[-0.0919,  0.0464,  0.1991,  0.4812,  0.2614,  0.5311,  0.6309]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 117
	action: tensor([[ 0.0204, -0.9289,  0.0789, -0.5300, -0.0718, -0.5343,  0.2855]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7963392934485132, distance: 1.5337369418328606 entropy 0.8164090514183044
epoch: 31, step: 118
	action: tensor([[ 3.0756e-05, -1.0921e+00, -8.9148e-01, -4.8890e-01, -2.6361e-01,
          6.5623e-01, -5.5386e-01]], dtype=torch.float64)
	q_value: tensor([[-31.1893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6951896399325348, distance: 1.4899299080722503 entropy 0.8164090514183044
epoch: 31, step: 119
	action: tensor([[-0.2003,  1.1947, -0.3515, -0.1376,  0.3754, -0.0247, -0.6715]],
       dtype=torch.float64)
	q_value: tensor([[-35.4619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 120
	action: tensor([[ 0.7496,  0.1544, -0.3637,  0.2045, -0.9377,  0.7316, -0.5384]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 121
	action: tensor([[-0.2966,  0.7860,  1.0205, -0.3613, -0.4546, -0.3680,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 122
	action: tensor([[-0.3583, -0.8082, -0.3531, -0.2135, -0.6489, -0.5786,  0.2141]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5349352030426862, distance: 1.4177567267858366 entropy 0.8164090514183044
epoch: 31, step: 123
	action: tensor([[ 0.3836,  0.1077, -0.0797, -0.5833,  0.9878,  0.2050, -0.2415]],
       dtype=torch.float64)
	q_value: tensor([[-31.3904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48197884742728736, distance: 0.8236267440522665 entropy 0.8164090514183044
epoch: 31, step: 124
	action: tensor([[-0.0618,  0.8561, -0.2441, -0.5069,  0.8027, -0.5014,  0.7232]],
       dtype=torch.float64)
	q_value: tensor([[-28.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 125
	action: tensor([[ 0.3353, -0.0381, -0.2124, -0.1222,  0.2095, -1.0396,  0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32246904028501655, distance: 0.9419357184990347 entropy 0.8164090514183044
epoch: 31, step: 126
	action: tensor([[ 0.3334,  0.6734, -0.1631,  0.5096, -0.0166,  0.3263, -0.3396]],
       dtype=torch.float64)
	q_value: tensor([[-26.9683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 127
	action: tensor([[-0.1620, -0.6837, -0.5779, -1.7269,  1.1080,  0.8078,  0.4869]],
       dtype=torch.float64)
	q_value: tensor([[-33.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1341895145798273, distance: 1.0648001748744402 entropy 0.8164090514183044
LOSS epoch 31 actor 399.17976616469383 critic 177.56793876564373 
epoch: 32, step: 0
	action: tensor([[0.1604, 0.0254, 0.4454, 0.3114, 0.9418, 0.9810, 0.8025]],
       dtype=torch.float64)
	q_value: tensor([[-43.3396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8334704357597766, distance: 0.46698439273408543 entropy 0.8164090514183044
epoch: 32, step: 1
	action: tensor([[-0.2649, -0.1922,  0.2120, -0.0905,  0.1668,  0.0572,  0.0260]],
       dtype=torch.float64)
	q_value: tensor([[-34.7723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20488169956006796, distance: 1.2561135408932604 entropy 0.8164090514183044
epoch: 32, step: 2
	action: tensor([[-1.0694, -0.4653, -0.7327,  0.7319,  0.6441,  0.0900,  0.6627]],
       dtype=torch.float64)
	q_value: tensor([[-27.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2801529675848053, distance: 1.7279798940033468 entropy 0.8164090514183044
epoch: 32, step: 3
	action: tensor([[ 0.4831, -0.0254, -0.6125, -0.3408, -0.3573,  0.2273, -0.6104]],
       dtype=torch.float64)
	q_value: tensor([[-34.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5461378132648018, distance: 0.7709366527728528 entropy 0.8164090514183044
epoch: 32, step: 4
	action: tensor([[-0.2699, -0.0798, -0.1280,  0.0309,  0.2883,  0.7420,  0.8167]],
       dtype=torch.float64)
	q_value: tensor([[-28.6248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19876084434205787, distance: 1.0243250075640635 entropy 0.8164090514183044
epoch: 32, step: 5
	action: tensor([[ 0.0091, -0.5806,  1.1109, -0.7461,  0.8641, -1.3349,  0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-31.4640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30428427890170706, distance: 1.3069013995814223 entropy 0.8164090514183044
epoch: 32, step: 6
	action: tensor([[ 0.7725, -0.6894,  0.2285,  0.0168,  1.5699, -0.5533,  1.5798]],
       dtype=torch.float64)
	q_value: tensor([[-36.3277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12230055122160233, distance: 1.0720859539004615 entropy 0.8164090514183044
epoch: 32, step: 7
	action: tensor([[ 0.4064, -0.6757, -0.8679,  0.4502, -0.5301, -0.0381,  0.5596]],
       dtype=torch.float64)
	q_value: tensor([[-39.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10226072720644053, distance: 1.0842559284958768 entropy 0.8164090514183044
epoch: 32, step: 8
	action: tensor([[-0.1968,  0.5743,  0.1781,  0.4810,  0.0999, -0.6978,  0.5458]],
       dtype=torch.float64)
	q_value: tensor([[-34.4984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 9
	action: tensor([[-0.2324, -0.8769, -0.5527, -0.1382,  0.2051,  0.1996,  0.3921]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6400494626861088, distance: 1.4654977850854256 entropy 0.8164090514183044
epoch: 32, step: 10
	action: tensor([[ 1.2616,  0.7262, -0.6403, -0.1179, -0.2572,  0.0847, -0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-32.3189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8859967641927714, distance: 0.38638057318665414 entropy 0.8164090514183044
epoch: 32, step: 11
	action: tensor([[-0.2114, -0.1634,  0.5722,  0.7490,  1.0318,  0.8404, -0.0778]],
       dtype=torch.float64)
	q_value: tensor([[-31.3931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 12
	action: tensor([[ 0.3575, -0.7251,  0.9055, -0.1996,  1.0937,  0.2380, -0.1145]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23521735379596875, distance: 1.2718280103717878 entropy 0.8164090514183044
epoch: 32, step: 13
	action: tensor([[ 0.6926, -0.9432, -0.4709,  0.1236,  0.4703, -0.9756,  0.4372]],
       dtype=torch.float64)
	q_value: tensor([[-34.1417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4357902491359511, distance: 1.3712043838776207 entropy 0.8164090514183044
epoch: 32, step: 14
	action: tensor([[ 0.0032,  0.5799,  1.3583, -0.2519,  0.4791,  0.6662,  0.6105]],
       dtype=torch.float64)
	q_value: tensor([[-35.1681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 15
	action: tensor([[-0.6282, -0.4923, -0.2256,  0.3017,  0.5807,  0.8468, -0.7562]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3505937242375634, distance: 1.3299002172146992 entropy 0.8164090514183044
epoch: 32, step: 16
	action: tensor([[-0.6634, -0.9317, -0.4364,  0.5447,  0.4577, -0.5048, -1.3295]],
       dtype=torch.float64)
	q_value: tensor([[-32.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1073450525962927, distance: 1.6612099329918761 entropy 0.8164090514183044
epoch: 32, step: 17
	action: tensor([[ 0.6278, -0.4883,  0.2553, -0.5611,  0.4068,  0.6169, -1.0560]],
       dtype=torch.float64)
	q_value: tensor([[-36.5276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04804131552133295, distance: 1.1165180370086625 entropy 0.8164090514183044
epoch: 32, step: 18
	action: tensor([[-1.0418, -0.6072,  0.3179, -0.5785, -0.0029,  0.0125, -0.4342]],
       dtype=torch.float64)
	q_value: tensor([[-33.7148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4164281665342413, distance: 1.7788676529163667 entropy 0.8164090514183044
epoch: 32, step: 19
	action: tensor([[ 0.2625,  0.0254,  0.5913, -0.3907, -0.7970,  0.9129,  0.5519]],
       dtype=torch.float64)
	q_value: tensor([[-32.3585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.501894118639322, distance: 0.8076394569650905 entropy 0.8164090514183044
epoch: 32, step: 20
	action: tensor([[ 1.2909, -0.1015, -0.6163,  0.4639, -0.2178,  0.4611,  0.6269]],
       dtype=torch.float64)
	q_value: tensor([[-35.4903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8871341058388668, distance: 0.3844483988963713 entropy 0.8164090514183044
epoch: 32, step: 21
	action: tensor([[ 0.4267, -0.3179,  0.0652,  0.3129, -0.3040,  0.4074,  0.9894]],
       dtype=torch.float64)
	q_value: tensor([[-35.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6508851350054279, distance: 0.6761465926492914 entropy 0.8164090514183044
epoch: 32, step: 22
	action: tensor([[ 0.6398, -0.9754,  0.2618, -0.7266,  0.3596, -0.2133, -0.3848]],
       dtype=torch.float64)
	q_value: tensor([[-32.5691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8114364018021292, distance: 1.540168506695045 entropy 0.8164090514183044
epoch: 32, step: 23
	action: tensor([[ 0.1842, -0.0603,  0.4252, -0.3259,  0.0695,  0.2258, -0.6046]],
       dtype=torch.float64)
	q_value: tensor([[-33.8875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25713742519119387, distance: 0.9863043144946588 entropy 0.8164090514183044
epoch: 32, step: 24
	action: tensor([[-0.2191,  0.1474, -0.2057, -0.6263, -0.8027, -0.7353,  0.5904]],
       dtype=torch.float64)
	q_value: tensor([[-28.3066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14235214458878243, distance: 1.059768962735097 entropy 0.8164090514183044
epoch: 32, step: 25
	action: tensor([[-0.3292, -0.0092,  0.1845,  0.1338,  0.3680,  0.0662, -0.6410]],
       dtype=torch.float64)
	q_value: tensor([[-31.9842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04520526724804552, distance: 1.1181799495596092 entropy 0.8164090514183044
epoch: 32, step: 26
	action: tensor([[ 0.8024, -0.1285,  0.3449, -0.8057,  0.4954, -0.2784,  0.5468]],
       dtype=torch.float64)
	q_value: tensor([[-28.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14711087598626904, distance: 1.0568247643240845 entropy 0.8164090514183044
epoch: 32, step: 27
	action: tensor([[ 0.8679,  0.5839, -0.0874,  0.6706, -0.5835,  0.5910, -0.4346]],
       dtype=torch.float64)
	q_value: tensor([[-31.5082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 28
	action: tensor([[ 0.5165, -0.5399,  0.7476, -0.9847,  0.0896, -0.1555,  1.0848]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4433550222769058, distance: 1.3748118826299554 entropy 0.8164090514183044
epoch: 32, step: 29
	action: tensor([[-0.0759, -0.3153, -1.1910, -0.0670, -0.7195,  0.0345,  0.6181]],
       dtype=torch.float64)
	q_value: tensor([[-35.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00378017506074646, distance: 1.1465051246138898 entropy 0.8164090514183044
epoch: 32, step: 30
	action: tensor([[ 0.5407,  0.5752, -0.2615, -0.0093, -0.6049,  1.0328,  0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-33.5986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 31
	action: tensor([[ 0.1856, -0.1051, -0.3128, -0.4058,  0.1830, -0.8948,  0.0512]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09164997640033945, distance: 1.0906447400020942 entropy 0.8164090514183044
epoch: 32, step: 32
	action: tensor([[ 0.1138,  0.3850, -0.4229, -0.8054,  0.5852, -0.6237,  0.8503]],
       dtype=torch.float64)
	q_value: tensor([[-28.5344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3860394870436006, distance: 0.8966581771102641 entropy 0.8164090514183044
epoch: 32, step: 33
	action: tensor([[ 0.7782, -0.2528, -0.0302, -0.7670,  0.0897, -0.5540,  0.3978]],
       dtype=torch.float64)
	q_value: tensor([[-31.1661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0677655479437389, distance: 1.1824822906051906 entropy 0.8164090514183044
epoch: 32, step: 34
	action: tensor([[ 0.2197,  0.8560,  0.0867, -0.1034, -0.5933, -0.2698, -0.2543]],
       dtype=torch.float64)
	q_value: tensor([[-31.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 35
	action: tensor([[-0.2179, -0.6479,  1.0747,  0.0659, -0.4218,  0.8186, -0.4680]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04418115484910223, distance: 1.1693502658302308 entropy 0.8164090514183044
epoch: 32, step: 36
	action: tensor([[ 0.1576, -0.6699, -0.6437, -0.2183,  0.1972,  1.1545,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-35.9165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11120604369707632, distance: 1.0788404951098753 entropy 0.8164090514183044
epoch: 32, step: 37
	action: tensor([[-0.4369, -0.2416,  0.9395,  1.2404,  0.1479, -0.7762,  0.5411]],
       dtype=torch.float64)
	q_value: tensor([[-34.7117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4090536562028303, distance: 0.8796921535285063 entropy 0.8164090514183044
epoch: 32, step: 38
	action: tensor([[ 0.4888,  0.5232,  0.8250,  0.1923, -0.5715, -0.2398, -0.8832]],
       dtype=torch.float64)
	q_value: tensor([[-35.6646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 39
	action: tensor([[ 0.3761, -0.5008,  1.4467,  0.6825,  0.1561, -0.3329,  0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40266714937740267, distance: 0.8844329072831955 entropy 0.8164090514183044
epoch: 32, step: 40
	action: tensor([[ 0.6653, -0.5808, -0.8042,  0.0355,  1.0631,  0.9020,  0.3441]],
       dtype=torch.float64)
	q_value: tensor([[-35.4334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5177208905533192, distance: 0.7947049505299446 entropy 0.8164090514183044
epoch: 32, step: 41
	action: tensor([[ 0.2278, -0.4380, -0.3201, -1.1566, -0.3636, -0.0279, -0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-36.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2706612984619483, distance: 1.2899461911521202 entropy 0.8164090514183044
epoch: 32, step: 42
	action: tensor([[-0.2648,  0.0479, -0.3685,  0.3499, -1.0573,  0.2086,  0.7109]],
       dtype=torch.float64)
	q_value: tensor([[-32.4490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04054312857214748, distance: 1.1209065888913345 entropy 0.8164090514183044
epoch: 32, step: 43
	action: tensor([[ 0.0697, -0.7615, -0.0084, -0.0877, -0.2338, -0.3339, -0.7945]],
       dtype=torch.float64)
	q_value: tensor([[-33.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43176142367934767, distance: 1.3692792336327542 entropy 0.8164090514183044
epoch: 32, step: 44
	action: tensor([[-0.0623, -0.0977, -0.0958, -0.6792,  0.9912, -0.6633,  0.1762]],
       dtype=torch.float64)
	q_value: tensor([[-30.7616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2822185664353931, distance: 1.2957992488092303 entropy 0.8164090514183044
epoch: 32, step: 45
	action: tensor([[ 0.6708, -0.5150,  0.1390,  0.2798,  0.5700, -0.4020, -0.1938]],
       dtype=torch.float64)
	q_value: tensor([[-30.8344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.314864362956071, distance: 0.9472071597714896 entropy 0.8164090514183044
epoch: 32, step: 46
	action: tensor([[ 0.4390,  0.2468,  0.1749, -0.1487, -0.1917,  0.4818,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-29.9151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.819970063053159, distance: 0.4855445211747799 entropy 0.8164090514183044
epoch: 32, step: 47
	action: tensor([[ 0.8386,  0.4363,  0.5194,  0.2427,  0.2503, -0.4984,  1.0532]],
       dtype=torch.float64)
	q_value: tensor([[-28.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9759574513774351, distance: 0.177438127110855 entropy 0.8164090514183044
epoch: 32, step: 48
	action: tensor([[ 0.8534, -1.0142, -0.6433, -1.1443,  0.2450,  1.2616,  0.1570]],
       dtype=torch.float64)
	q_value: tensor([[-32.8985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37307191490912417, distance: 1.340921441687052 entropy 0.8164090514183044
epoch: 32, step: 49
	action: tensor([[ 1.0017, -0.4630, -0.6175, -0.1371,  0.3433, -1.0278,  0.7184]],
       dtype=torch.float64)
	q_value: tensor([[-43.2099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019456965615291644, distance: 1.1554233555976212 entropy 0.8164090514183044
epoch: 32, step: 50
	action: tensor([[-0.2826, -0.6064,  0.8989, -0.6647,  0.7092, -0.8614, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-35.4459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8869736861152269, distance: 1.5719532112852783 entropy 0.8164090514183044
epoch: 32, step: 51
	action: tensor([[ 0.8747,  0.6371, -0.1733, -0.0495,  0.1398, -0.2881, -0.5362]],
       dtype=torch.float64)
	q_value: tensor([[-33.5383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9793395584322209, distance: 0.16448507338374926 entropy 0.8164090514183044
epoch: 32, step: 52
	action: tensor([[ 0.4085, -1.1824,  0.1978,  0.0723,  0.6258, -0.0394, -0.2586]],
       dtype=torch.float64)
	q_value: tensor([[-28.0891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6029103500815061, distance: 1.4488095828217382 entropy 0.8164090514183044
epoch: 32, step: 53
	action: tensor([[ 0.9576, -0.4919,  0.1375, -0.0367,  0.7516, -0.3382,  0.5161]],
       dtype=torch.float64)
	q_value: tensor([[-33.7775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15506958189118758, distance: 1.0518823450911667 entropy 0.8164090514183044
epoch: 32, step: 54
	action: tensor([[ 0.0264, -0.2104,  0.2771, -0.2297,  0.0415,  0.3743,  0.5115]],
       dtype=torch.float64)
	q_value: tensor([[-32.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12514986428428065, distance: 1.0703443601767497 entropy 0.8164090514183044
epoch: 32, step: 55
	action: tensor([[ 0.1428, -0.8646,  0.1742, -0.6402, -0.3773, -0.8473,  0.8527]],
       dtype=torch.float64)
	q_value: tensor([[-28.9348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7042357147410365, distance: 1.4938999902838737 entropy 0.8164090514183044
epoch: 32, step: 56
	action: tensor([[ 0.5605,  0.6142, -0.8626, -0.7856,  0.9842,  0.1195, -0.7401]],
       dtype=torch.float64)
	q_value: tensor([[-35.4979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.921641957670004, distance: 0.3203306403266646 entropy 0.8164090514183044
epoch: 32, step: 57
	action: tensor([[-0.5127, -0.9468,  0.4010,  0.1253, -0.1728, -0.6083, -0.2118]],
       dtype=torch.float64)
	q_value: tensor([[-31.7241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9303287688838915, distance: 1.5899092458307407 entropy 0.8164090514183044
epoch: 32, step: 58
	action: tensor([[ 0.4318,  0.0114,  0.1849,  0.6695, -0.3771, -0.6670, -0.3323]],
       dtype=torch.float64)
	q_value: tensor([[-32.6698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8209010334886129, distance: 0.4842874705450308 entropy 0.8164090514183044
epoch: 32, step: 59
	action: tensor([[ 0.4366, -0.0489, -0.3508, -1.2582,  0.1128, -0.4208,  0.9898]],
       dtype=torch.float64)
	q_value: tensor([[-30.2907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02266349981579685, distance: 1.1313025147611733 entropy 0.8164090514183044
epoch: 32, step: 60
	action: tensor([[ 2.0966, -0.3114, -0.1500, -0.6569,  0.8615,  1.2279,  0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-34.6683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 61
	action: tensor([[-8.8343e-01,  3.2617e-01, -5.0229e-01, -4.0293e-04,  3.6626e-01,
         -4.4621e-01,  5.9504e-01]], dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.587720849361062, distance: 1.4419286372782074 entropy 0.8164090514183044
epoch: 32, step: 62
	action: tensor([[-0.1559, -0.3724,  0.5292, -0.1070, -0.3408,  0.5073,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[-28.5503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09204537936686652, distance: 1.1958509037565888 entropy 0.8164090514183044
epoch: 32, step: 63
	action: tensor([[-0.9288, -0.5039,  0.0994,  0.6610,  0.5975,  0.0659, -0.5001]],
       dtype=torch.float64)
	q_value: tensor([[-30.4907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7319491449120887, distance: 1.5059975353809407 entropy 0.8164090514183044
epoch: 32, step: 64
	action: tensor([[-0.5762,  0.2159, -1.7490,  0.4041, -0.8421, -0.3018, -0.1346]],
       dtype=torch.float64)
	q_value: tensor([[-31.6099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4465572882434392, distance: 1.376336134825677 entropy 0.8164090514183044
epoch: 32, step: 65
	action: tensor([[-0.1078,  0.4856,  0.2818,  0.3266, -0.5297, -0.9565,  0.2391]],
       dtype=torch.float64)
	q_value: tensor([[-33.9637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 66
	action: tensor([[ 0.1979, -0.6735, -0.2312, -0.0698, -0.2975,  0.3053, -0.0520]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1202537735511382, distance: 1.211197319498433 entropy 0.8164090514183044
epoch: 32, step: 67
	action: tensor([[-0.5712,  0.0484, -0.5633, -0.1636,  1.1195, -0.0149,  0.6081]],
       dtype=torch.float64)
	q_value: tensor([[-30.1609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37334747160665915, distance: 1.3410559872063301 entropy 0.8164090514183044
epoch: 32, step: 68
	action: tensor([[ 0.8235, -0.4547,  0.7144, -0.0037, -0.0069,  0.4578,  0.4542]],
       dtype=torch.float64)
	q_value: tensor([[-31.7651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4705149212361871, distance: 0.8326903970756309 entropy 0.8164090514183044
epoch: 32, step: 69
	action: tensor([[ 1.1178, -0.7838, -0.8674,  0.1349, -0.1313, -0.5322, -0.1766]],
       dtype=torch.float64)
	q_value: tensor([[-32.2031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17576019099020468, distance: 1.240840811658162 entropy 0.8164090514183044
epoch: 32, step: 70
	action: tensor([[ 0.5451,  0.7315, -0.3556,  0.6520,  1.6472,  0.3636,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[-34.3494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 71
	action: tensor([[-0.5701,  0.5458,  0.5053, -0.3995,  0.2053, -0.3992,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3805009934698873, distance: 1.3445441114939116 entropy 0.8164090514183044
epoch: 32, step: 72
	action: tensor([[-0.2026,  0.0840, -1.2111, -0.7620,  0.3675,  0.1460, -0.3877]],
       dtype=torch.float64)
	q_value: tensor([[-28.5040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43288152696572757, distance: 0.861774403070489 entropy 0.8164090514183044
epoch: 32, step: 73
	action: tensor([[-0.4673, -0.2841,  0.6575,  0.8332,  1.4337,  0.6263,  0.6046]],
       dtype=torch.float64)
	q_value: tensor([[-30.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36232558725703456, distance: 0.9138105941977471 entropy 0.8164090514183044
epoch: 32, step: 74
	action: tensor([[ 0.8591,  0.1245,  0.5233,  0.0576, -0.2844, -0.2302,  0.4703]],
       dtype=torch.float64)
	q_value: tensor([[-37.3430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8190711741441107, distance: 0.4867551729949349 entropy 0.8164090514183044
epoch: 32, step: 75
	action: tensor([[ 0.3560, -0.0833,  0.3028, -0.7443,  0.0630,  0.5017, -0.3821]],
       dtype=torch.float64)
	q_value: tensor([[-30.2021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23221963710588134, distance: 1.0027096472188508 entropy 0.8164090514183044
epoch: 32, step: 76
	action: tensor([[-0.6827, -0.7704,  0.1008,  0.1311, -0.5218,  0.6004,  1.0028]],
       dtype=torch.float64)
	q_value: tensor([[-29.5802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8863557053953455, distance: 1.5716957841856627 entropy 0.8164090514183044
epoch: 32, step: 77
	action: tensor([[-0.5069, -0.8750, -0.3117, -0.4898, -0.0829,  0.2678, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-35.0789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9404274408326541, distance: 1.5940626904100113 entropy 0.8164090514183044
epoch: 32, step: 78
	action: tensor([[-0.1760, -0.6478, -0.0791,  0.3448,  0.8438,  0.3971,  0.5698]],
       dtype=torch.float64)
	q_value: tensor([[-32.2548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03299349433022747, distance: 1.1630690163521988 entropy 0.8164090514183044
epoch: 32, step: 79
	action: tensor([[-0.0817, -0.5117, -0.3575, -0.7317,  0.1500, -0.7705, -1.1947]],
       dtype=torch.float64)
	q_value: tensor([[-31.8051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35438875124895475, distance: 1.331767346736367 entropy 0.8164090514183044
epoch: 32, step: 80
	action: tensor([[ 1.1307, -0.2886,  0.1047, -0.0566, -0.2617, -0.5051,  0.3792]],
       dtype=torch.float64)
	q_value: tensor([[-33.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26602617305450016, distance: 0.9803857272336548 entropy 0.8164090514183044
epoch: 32, step: 81
	action: tensor([[ 0.5878, -0.6971, -0.0319,  0.3222, -0.1325,  0.5009,  0.6071]],
       dtype=torch.float64)
	q_value: tensor([[-31.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42213145688911524, distance: 0.8699037843921951 entropy 0.8164090514183044
epoch: 32, step: 82
	action: tensor([[-0.2449,  0.4691,  0.6763,  0.0710, -0.1314,  0.4635,  0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-32.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 83
	action: tensor([[ 0.5492, -0.5098, -0.4866,  0.0832, -0.2960,  0.4189,  0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3390274387450688, distance: 0.9303543848341821 entropy 0.8164090514183044
epoch: 32, step: 84
	action: tensor([[-0.4037, -0.2139,  0.4429, -0.8372,  0.2538,  0.0333,  0.1680]],
       dtype=torch.float64)
	q_value: tensor([[-30.6870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7871767650881494, distance: 1.5298204005851095 entropy 0.8164090514183044
epoch: 32, step: 85
	action: tensor([[-0.2783,  0.3256,  0.1073, -0.3928,  0.6953,  0.6272, -0.7584]],
       dtype=torch.float64)
	q_value: tensor([[-29.6971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2710101008597675, distance: 0.977051483917922 entropy 0.8164090514183044
epoch: 32, step: 86
	action: tensor([[-0.6587, -0.3968, -0.5900,  0.0545, -0.2870,  0.0896,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-30.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7748964012591109, distance: 1.5245553547469473 entropy 0.8164090514183044
epoch: 32, step: 87
	action: tensor([[ 0.6682, -0.1793,  0.5840, -0.4522, -0.3522,  0.1886,  0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-29.1659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37513028546833016, distance: 0.9045892688791857 entropy 0.8164090514183044
epoch: 32, step: 88
	action: tensor([[ 0.3610,  0.5960, -0.3141,  0.6066,  0.6354,  0.4182, -0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-30.3838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 89
	action: tensor([[-0.2465, -0.3109, -0.6483, -0.0020, -0.1731, -0.3891,  0.0715]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14376335117523542, distance: 1.2238403889072695 entropy 0.8164090514183044
epoch: 32, step: 90
	action: tensor([[ 1.0751, -0.2049, -0.0582, -0.4416,  1.4740, -0.0497, -0.0631]],
       dtype=torch.float64)
	q_value: tensor([[-28.0543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2765518991808671, distance: 0.9733306147383696 entropy 0.8164090514183044
epoch: 32, step: 91
	action: tensor([[-0.1244, -0.8449,  0.9289, -0.2955, -0.2748, -0.1497,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[-35.1811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8442632853284042, distance: 1.554061328694112 entropy 0.8164090514183044
epoch: 32, step: 92
	action: tensor([[ 0.5011,  0.0545,  1.1794,  0.3529, -0.0407,  0.5698, -0.1311]],
       dtype=torch.float64)
	q_value: tensor([[-32.4639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9076044673639203, distance: 0.3478421285330371 entropy 0.8164090514183044
epoch: 32, step: 93
	action: tensor([[-0.2183, -1.0333,  0.3864, -0.5934, -0.0377, -0.1929, -0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-32.7865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1049958600503822, distance: 1.6602837462276712 entropy 0.8164090514183044
epoch: 32, step: 94
	action: tensor([[ 0.0209, -0.0183,  0.1726, -0.6796, -0.5963, -0.8505,  0.0832]],
       dtype=torch.float64)
	q_value: tensor([[-32.7380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.053981344006800436, distance: 1.174824933709712 entropy 0.8164090514183044
epoch: 32, step: 95
	action: tensor([[ 0.7902, -0.6342, -0.9000, -0.1257, -0.9368,  1.0378, -0.8420]],
       dtype=torch.float64)
	q_value: tensor([[-30.5660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022315051914302053, distance: 1.1315041673432529 entropy 0.8164090514183044
epoch: 32, step: 96
	action: tensor([[ 0.4587,  0.2051,  0.3740, -0.7821,  0.2121,  0.5547,  1.0173]],
       dtype=torch.float64)
	q_value: tensor([[-38.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5719456523590838, distance: 0.7486970974963099 entropy 0.8164090514183044
epoch: 32, step: 97
	action: tensor([[ 0.5491, -1.0377,  0.4354,  0.7957, -0.3077,  0.4573, -0.1768]],
       dtype=torch.float64)
	q_value: tensor([[-34.5428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4426687599132506, distance: 0.8543058630162353 entropy 0.8164090514183044
epoch: 32, step: 98
	action: tensor([[ 0.0713,  0.0541,  0.1913, -0.7547, -0.1998, -0.5496,  0.7624]],
       dtype=torch.float64)
	q_value: tensor([[-36.4282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06524992832850307, distance: 1.1810885250091485 entropy 0.8164090514183044
epoch: 32, step: 99
	action: tensor([[ 0.6649,  0.2674,  0.0457, -1.4387,  0.9884,  0.3452, -1.5026]],
       dtype=torch.float64)
	q_value: tensor([[-30.9289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28443908059540624, distance: 0.9680103481023414 entropy 0.8164090514183044
epoch: 32, step: 100
	action: tensor([[ 0.2480, -0.4019, -0.1198,  0.4356,  0.4761, -0.3453,  0.4342]],
       dtype=torch.float64)
	q_value: tensor([[-36.8920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28982935446519176, distance: 0.9643574763877548 entropy 0.8164090514183044
epoch: 32, step: 101
	action: tensor([[ 0.8025, -0.4644,  0.1117, -0.0559, -0.6084, -1.0296,  0.4420]],
       dtype=torch.float64)
	q_value: tensor([[-28.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05928678477222882, distance: 1.1099037425191995 entropy 0.8164090514183044
epoch: 32, step: 102
	action: tensor([[ 1.1301,  0.4501,  0.5154,  0.7557, -0.1398, -0.1192,  0.8051]],
       dtype=torch.float64)
	q_value: tensor([[-33.9287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8275131011669304, distance: 0.47526381549861746 entropy 0.8164090514183044
epoch: 32, step: 103
	action: tensor([[-0.2471,  0.0824, -1.0598, -1.1779,  0.1946, -0.0759,  0.0631]],
       dtype=torch.float64)
	q_value: tensor([[-33.9084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4391730074866136, distance: 0.8569809090559974 entropy 0.8164090514183044
epoch: 32, step: 104
	action: tensor([[-0.0522,  0.3004, -0.4347,  0.1965,  0.0902, -0.7007, -0.1140]],
       dtype=torch.float64)
	q_value: tensor([[-31.5941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 105
	action: tensor([[ 0.1827, -0.2198,  0.0677, -0.1422,  0.2151,  0.4524,  0.7201]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36162620973933646, distance: 0.9143115735638655 entropy 0.8164090514183044
epoch: 32, step: 106
	action: tensor([[ 0.8578, -0.3974,  0.9378, -0.0416,  0.0875, -0.8074,  0.3798]],
       dtype=torch.float64)
	q_value: tensor([[-30.0649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29129811429460606, distance: 0.9633597284262393 entropy 0.8164090514183044
epoch: 32, step: 107
	action: tensor([[ 0.4575,  0.6876,  0.0814, -1.0314, -0.2529,  0.3172, -0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-32.9014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7129274221970956, distance: 0.613129973843578 entropy 0.8164090514183044
epoch: 32, step: 108
	action: tensor([[ 0.3151, -0.5586, -0.4653, -0.3119, -0.3771, -0.8372, -0.4255]],
       dtype=torch.float64)
	q_value: tensor([[-31.1161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0388264741116211, distance: 1.1663481309119654 entropy 0.8164090514183044
epoch: 32, step: 109
	action: tensor([[ 0.5106,  0.3470, -0.4979, -0.3507,  0.0963, -0.4416,  1.3051]],
       dtype=torch.float64)
	q_value: tensor([[-30.7940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7491537908193747, distance: 0.5731396635668589 entropy 0.8164090514183044
epoch: 32, step: 110
	action: tensor([[-0.1209,  0.5524,  0.1512, -1.2134, -0.2984, -0.7152,  1.0252]],
       dtype=torch.float64)
	q_value: tensor([[-34.1586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045600954013233386, distance: 1.1179482270534644 entropy 0.8164090514183044
epoch: 32, step: 111
	action: tensor([[ 1.0729,  0.1893,  0.8808, -0.1413,  0.2580,  0.8006,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[-34.4125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8275396154818206, distance: 0.47522728582928175 entropy 0.8164090514183044
epoch: 32, step: 112
	action: tensor([[-0.6637, -0.0168,  1.1996, -0.6059,  0.0705,  0.2511, -0.0814]],
       dtype=torch.float64)
	q_value: tensor([[-33.2055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.904124895114375, distance: 1.5790810031050468 entropy 0.8164090514183044
epoch: 32, step: 113
	action: tensor([[0.2971, 1.5120, 0.0101, 0.7987, 0.2604, 0.9235, 0.3114]],
       dtype=torch.float64)
	q_value: tensor([[-32.3522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 114
	action: tensor([[ 0.3037,  0.2355, -0.3155,  0.8454, -0.6432, -0.7048,  0.0603]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 115
	action: tensor([[ 0.9936, -0.3523, -0.7297,  0.6487,  0.0507, -0.1232,  1.4193]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 116
	action: tensor([[ 0.7455, -0.4648,  0.0932, -0.6567, -0.6300, -0.1817,  0.5439]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18970491993329075, distance: 1.2481774208488359 entropy 0.8164090514183044
epoch: 32, step: 117
	action: tensor([[ 0.1146,  0.1157, -0.0344, -0.3434, -0.1605, -0.3086, -0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-33.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3181867616481602, distance: 0.9449077433570645 entropy 0.8164090514183044
epoch: 32, step: 118
	action: tensor([[ 0.4633, -0.4741, -0.1515, -0.1533,  1.2002, -0.0110,  0.8059]],
       dtype=torch.float64)
	q_value: tensor([[-26.1101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09691637457981128, distance: 1.0874784941722644 entropy 0.8164090514183044
epoch: 32, step: 119
	action: tensor([[ 1.1882,  0.1315, -1.1376,  0.8490, -0.6625, -1.0071, -0.6466]],
       dtype=torch.float64)
	q_value: tensor([[-33.7269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9435293984851154, distance: 0.2719367483144557 entropy 0.8164090514183044
epoch: 32, step: 120
	action: tensor([[-1.2627, -0.7343,  0.1355, -0.1062, -0.5345,  0.0095,  0.0713]],
       dtype=torch.float64)
	q_value: tensor([[-37.1366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5813383636263785, distance: 1.8385657317251232 entropy 0.8164090514183044
epoch: 32, step: 121
	action: tensor([[-0.1298, -1.0280,  0.6708,  0.4705,  0.7287, -0.0216,  0.9028]],
       dtype=torch.float64)
	q_value: tensor([[-34.1423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29015313361212214, distance: 1.299802368650845 entropy 0.8164090514183044
epoch: 32, step: 122
	action: tensor([[ 0.3713,  0.7157,  0.3695,  0.2298, -0.3117, -0.7986,  2.4971]],
       dtype=torch.float64)
	q_value: tensor([[-34.6156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 123
	action: tensor([[ 0.2535, -0.1672, -0.5641, -0.9237, -0.0945, -0.6442, -0.5154]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15739305013794602, distance: 1.0504350674899703 entropy 0.8164090514183044
epoch: 32, step: 124
	action: tensor([[ 0.9409,  0.5848, -0.2321,  0.5649,  0.5169,  0.5544, -1.3977]],
       dtype=torch.float64)
	q_value: tensor([[-30.8669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 125
	action: tensor([[ 0.8243,  0.1845, -0.3028, -0.6287, -0.9434, -0.2436, -0.4866]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 126
	action: tensor([[ 0.0944, -0.6157, -1.4792, -0.6718,  0.3976,  0.0032,  0.5379]],
       dtype=torch.float64)
	q_value: tensor([[-34.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1384907716481638, distance: 1.0621519731345506 entropy 0.8164090514183044
epoch: 32, step: 127
	action: tensor([[-1.1590,  0.4373, -0.6619, -1.1071,  0.4390, -0.0746, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-36.1732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5037007012545973, distance: 1.4032575721366845 entropy 0.8164090514183044
LOSS epoch 32 actor 447.93750109456374 critic 158.27612568944699 
epoch: 33, step: 0
	action: tensor([[-0.6489,  0.7024,  0.2085,  0.3845, -0.2549,  0.1547,  0.5517]],
       dtype=torch.float64)
	q_value: tensor([[-31.9802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 1
	action: tensor([[-0.0205,  0.0533, -0.2988, -0.0292, -0.4524,  0.1396, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3251688865139982, distance: 0.940057117928271 entropy 0.8164090514183044
epoch: 33, step: 2
	action: tensor([[ 0.1445, -0.4711, -1.0676,  0.0913,  0.4127,  0.0016,  0.2626]],
       dtype=torch.float64)
	q_value: tensor([[-27.0585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02686390857932719, distance: 1.1288688341780062 entropy 0.8164090514183044
epoch: 33, step: 3
	action: tensor([[-0.0560, -0.0485,  0.9265,  0.1126, -0.3499, -1.1514, -0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-30.8377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027978883607826854, distance: 1.128221945573556 entropy 0.8164090514183044
epoch: 33, step: 4
	action: tensor([[ 0.3708,  0.3873, -0.3942, -0.0054, -0.1964,  0.2054, -0.1702]],
       dtype=torch.float64)
	q_value: tensor([[-33.7165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 5
	action: tensor([[-0.6289,  0.3282, -0.1946, -0.8082,  0.0231,  0.1928, -0.1939]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3349844225160685, distance: 1.3221928135050307 entropy 0.8164090514183044
epoch: 33, step: 6
	action: tensor([[-0.6136,  1.2878, -0.3254, -0.2105, -0.1146, -0.2415,  0.3860]],
       dtype=torch.float64)
	q_value: tensor([[-28.8646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 7
	action: tensor([[-0.1052,  0.0562, -0.5789, -0.3032, -0.4704, -0.1437,  0.2375]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26636129864222446, distance: 0.980161884200689 entropy 0.8164090514183044
epoch: 33, step: 8
	action: tensor([[-0.3857,  0.2887,  0.4969,  0.6553, -0.3257, -0.5356,  1.1673]],
       dtype=torch.float64)
	q_value: tensor([[-28.1864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 9
	action: tensor([[ 0.1208, -0.0492, -0.4610,  0.1733,  0.3393,  0.1936,  0.3454]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44606459872349447, distance: 0.8516992285643831 entropy 0.8164090514183044
epoch: 33, step: 10
	action: tensor([[ 0.1902,  0.2319,  0.1645, -1.0008, -0.3064, -0.3218,  0.3695]],
       dtype=torch.float64)
	q_value: tensor([[-27.5475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09081095397712169, distance: 1.0911483257112196 entropy 0.8164090514183044
epoch: 33, step: 11
	action: tensor([[ 0.6407,  0.6342, -0.2141, -0.5676, -0.4052,  0.2500,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-30.5961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.916611618018758, distance: 0.3304528233877503 entropy 0.8164090514183044
epoch: 33, step: 12
	action: tensor([[-0.2075,  0.3498, -0.4350, -0.5263,  0.3752, -0.6419,  0.9097]],
       dtype=torch.float64)
	q_value: tensor([[-30.1891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19248935385578025, distance: 1.0283260120198539 entropy 0.8164090514183044
epoch: 33, step: 13
	action: tensor([[ 0.5666,  0.1668,  0.4910, -0.5992, -0.3515,  0.2834, -1.0800]],
       dtype=torch.float64)
	q_value: tensor([[-30.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5564734393180151, distance: 0.7621079809166873 entropy 0.8164090514183044
epoch: 33, step: 14
	action: tensor([[ 1.4592,  0.1227,  0.3186, -0.8774,  0.8522,  0.4577, -0.4599]],
       dtype=torch.float64)
	q_value: tensor([[-31.6273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.299022180280095, distance: 0.9580955684750806 entropy 0.8164090514183044
epoch: 33, step: 15
	action: tensor([[ 0.5301, -0.1423,  0.6300, -0.2067, -0.1705,  0.2546,  0.3744]],
       dtype=torch.float64)
	q_value: tensor([[-35.8229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5108468782717328, distance: 0.8003484496681686 entropy 0.8164090514183044
epoch: 33, step: 16
	action: tensor([[ 0.8816, -0.2805, -0.4535,  0.2279,  0.3578,  0.0998,  0.2234]],
       dtype=torch.float64)
	q_value: tensor([[-30.0111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6573617797121223, distance: 0.6698454258612329 entropy 0.8164090514183044
epoch: 33, step: 17
	action: tensor([[-0.1991, -0.3118,  0.1754, -0.3928, -0.0590,  0.4589,  0.3079]],
       dtype=torch.float64)
	q_value: tensor([[-30.4213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2499818046131439, distance: 1.2794064589827963 entropy 0.8164090514183044
epoch: 33, step: 18
	action: tensor([[-0.7233, -0.4276, -0.0512, -0.1169, -0.0237,  0.6976, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-29.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.707346396014314, distance: 1.4952627501692073 entropy 0.8164090514183044
epoch: 33, step: 19
	action: tensor([[ 0.6558, -1.0391, -0.5177, -0.6188,  1.2403,  0.8542, -0.6523]],
       dtype=torch.float64)
	q_value: tensor([[-30.5658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35245698711926066, distance: 1.3308172582229851 entropy 0.8164090514183044
epoch: 33, step: 20
	action: tensor([[ 0.3342, -0.1196,  0.0320,  0.0410,  0.4195, -1.0725,  0.3299]],
       dtype=torch.float64)
	q_value: tensor([[-40.8090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2575597406461413, distance: 0.9860239189103805 entropy 0.8164090514183044
epoch: 33, step: 21
	action: tensor([[-0.9325, -0.3743,  0.5372,  0.4999, -0.0989, -0.5835, -0.2488]],
       dtype=torch.float64)
	q_value: tensor([[-30.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8710217794144348, distance: 1.565294699870304 entropy 0.8164090514183044
epoch: 33, step: 22
	action: tensor([[ 1.1120, -0.3645, -1.2330,  0.6825, -0.0789, -0.9198,  1.0789]],
       dtype=torch.float64)
	q_value: tensor([[-32.8903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5116856440377542, distance: 0.7996619643179105 entropy 0.8164090514183044
epoch: 33, step: 23
	action: tensor([[ 0.2308, -0.8713,  0.4332, -0.3414,  0.1836, -0.1370, -0.1253]],
       dtype=torch.float64)
	q_value: tensor([[-40.3035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6578318454566925, distance: 1.4734212605515014 entropy 0.8164090514183044
epoch: 33, step: 24
	action: tensor([[-0.4981,  0.0730,  0.8544, -1.1138,  0.5718,  0.1006,  0.6281]],
       dtype=torch.float64)
	q_value: tensor([[-30.9439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8926047725757744, distance: 1.5742969669190356 entropy 0.8164090514183044
epoch: 33, step: 25
	action: tensor([[ 0.3100, -1.0766, -0.1823,  0.1412,  0.5189,  0.4955,  0.4147]],
       dtype=torch.float64)
	q_value: tensor([[-34.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22563139482063876, distance: 1.2668833595930926 entropy 0.8164090514183044
epoch: 33, step: 26
	action: tensor([[-0.1176, -0.0279,  0.7324, -0.7181, -0.2817,  0.0545, -0.4652]],
       dtype=torch.float64)
	q_value: tensor([[-34.2045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32726423231487267, distance: 1.3183641619424142 entropy 0.8164090514183044
epoch: 33, step: 27
	action: tensor([[-0.1231, -0.0982, -0.8051,  0.8327, -0.6232, -0.7781,  0.2709]],
       dtype=torch.float64)
	q_value: tensor([[-30.0168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.228561879267128, distance: 1.005095297703947 entropy 0.8164090514183044
epoch: 33, step: 28
	action: tensor([[ 0.8820, -0.6809, -0.2130, -0.5687, -0.3193,  0.1101, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-34.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3151898497275911, distance: 1.3123537527922597 entropy 0.8164090514183044
epoch: 33, step: 29
	action: tensor([[ 0.7257,  0.5448, -1.5006,  0.1931,  0.4702,  0.3417,  0.9471]],
       dtype=torch.float64)
	q_value: tensor([[-33.3761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9367991913682553, distance: 0.2876855249948843 entropy 0.8164090514183044
epoch: 33, step: 30
	action: tensor([[ 0.1775,  0.5926, -0.2583, -0.2205, -0.1839,  0.6839, -0.1075]],
       dtype=torch.float64)
	q_value: tensor([[-35.9865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 31
	action: tensor([[ 0.5762, -0.5809, -0.2346, -0.0583, -0.0743, -0.3358, -0.9396]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01697893881107193, distance: 1.1345877875563923 entropy 0.8164090514183044
epoch: 33, step: 32
	action: tensor([[ 0.4119, -0.0484,  0.3616,  0.5695, -0.0328,  0.1519, -0.6482]],
       dtype=torch.float64)
	q_value: tensor([[-30.9176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8800598043860338, distance: 0.39631368552496016 entropy 0.8164090514183044
epoch: 33, step: 33
	action: tensor([[-0.7484,  0.5934, -0.4240, -0.1835, -0.0223,  0.2525, -0.5944]],
       dtype=torch.float64)
	q_value: tensor([[-30.0081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 34
	action: tensor([[ 0.1287, -0.6784,  0.7049, -0.1652,  0.9218, -0.2515,  0.6700]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4161381693902888, distance: 1.3617880146247605 entropy 0.8164090514183044
epoch: 33, step: 35
	action: tensor([[ 0.3403,  0.0401, -0.3643,  0.3386, -0.0939,  0.1177,  0.3639]],
       dtype=torch.float64)
	q_value: tensor([[-32.5553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7190804595742815, distance: 0.6065235495115354 entropy 0.8164090514183044
epoch: 33, step: 36
	action: tensor([[ 0.6256,  0.3240,  0.1915,  0.7798,  0.6307,  0.8446, -0.1693]],
       dtype=torch.float64)
	q_value: tensor([[-27.6754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 37
	action: tensor([[ 0.3147, -0.9383,  0.7311, -0.5370,  0.3909, -0.2734, -0.6188]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7575484006204387, distance: 1.5170864874688021 entropy 0.8164090514183044
epoch: 33, step: 38
	action: tensor([[ 0.2179, -0.5991, -0.9625,  0.4140, -0.0592, -0.6259, -0.7524]],
       dtype=torch.float64)
	q_value: tensor([[-33.6510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007388225800020187, distance: 1.1485638114525267 entropy 0.8164090514183044
epoch: 33, step: 39
	action: tensor([[ 0.3906, -0.0642,  0.2229, -0.0993,  0.6050,  0.8996, -0.5049]],
       dtype=torch.float64)
	q_value: tensor([[-32.1579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7685146895298541, distance: 0.55057743946824 entropy 0.8164090514183044
epoch: 33, step: 40
	action: tensor([[ 0.0603, -0.3688, -0.3044, -0.8051,  0.2330,  0.4195,  0.9877]],
       dtype=torch.float64)
	q_value: tensor([[-31.1206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09648655040073839, distance: 1.19828010208729 entropy 0.8164090514183044
epoch: 33, step: 41
	action: tensor([[ 0.4782, -0.3827, -0.4166,  0.3286,  0.8866,  0.3249,  0.3096]],
       dtype=torch.float64)
	q_value: tensor([[-34.1557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5927369347138868, distance: 0.7302880700877091 entropy 0.8164090514183044
epoch: 33, step: 42
	action: tensor([[-0.4652, -0.2922,  0.3176, -0.9517, -0.2217,  0.1757, -0.2410]],
       dtype=torch.float64)
	q_value: tensor([[-31.3771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7995655401359685, distance: 1.5351136288474827 entropy 0.8164090514183044
epoch: 33, step: 43
	action: tensor([[-1.1989, -0.0652, -0.0041,  0.5884,  0.2903, -0.2473, -0.4928]],
       dtype=torch.float64)
	q_value: tensor([[-30.9471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8867676356961549, distance: 1.5718673832573038 entropy 0.8164090514183044
epoch: 33, step: 44
	action: tensor([[-0.5836,  0.2597, -0.4490, -1.5070,  0.0752, -0.2963,  0.8580]],
       dtype=torch.float64)
	q_value: tensor([[-31.6994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07167620560400279, distance: 1.1846457136054853 entropy 0.8164090514183044
epoch: 33, step: 45
	action: tensor([[ 0.5694, -0.4422,  0.8845, -0.2972, -0.2482, -0.7401,  0.2902]],
       dtype=torch.float64)
	q_value: tensor([[-35.4735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.051606331716357756, distance: 1.173500528254548 entropy 0.8164090514183044
epoch: 33, step: 46
	action: tensor([[ 0.3130,  0.6164, -0.2594,  0.2320,  1.0465,  1.0752,  0.7360]],
       dtype=torch.float64)
	q_value: tensor([[-32.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 47
	action: tensor([[-0.3174,  0.5788, -0.1343,  0.2589,  0.7584, -0.2014,  0.4885]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 48
	action: tensor([[-0.1264, -0.1645,  0.9423, -0.2846, -0.3504, -0.6565,  0.7651]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3533297509667699, distance: 1.3312465886489642 entropy 0.8164090514183044
epoch: 33, step: 49
	action: tensor([[ 0.2641,  0.2979, -0.5205, -0.9487,  0.4620, -0.1438, -0.5688]],
       dtype=torch.float64)
	q_value: tensor([[-32.0710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48253732669811333, distance: 0.82318264779878 entropy 0.8164090514183044
epoch: 33, step: 50
	action: tensor([[-0.0729, -1.4653,  0.4216,  0.1448, -0.0890,  0.5731,  0.8748]],
       dtype=torch.float64)
	q_value: tensor([[-29.7791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49001525132043233, distance: 1.396857326910752 entropy 0.8164090514183044
epoch: 33, step: 51
	action: tensor([[ 0.0507, -0.1719, -0.8336,  0.9938, -0.9716,  0.1485,  0.8638]],
       dtype=torch.float64)
	q_value: tensor([[-36.2481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15737659985178132, distance: 1.0504453213058935 entropy 0.8164090514183044
epoch: 33, step: 52
	action: tensor([[ 0.6204,  0.4649,  1.4375, -1.4162,  0.6914, -0.5711, -1.0862]],
       dtype=torch.float64)
	q_value: tensor([[-36.4426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 53
	action: tensor([[ 1.0491, -0.7021,  0.4700, -0.4188,  0.4359, -0.7928,  0.4073]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2255503777294956, distance: 1.2668414869300415 entropy 0.8164090514183044
epoch: 33, step: 54
	action: tensor([[ 0.5400, -0.3089,  1.0671, -0.9916, -0.6944, -0.4386,  0.3132]],
       dtype=torch.float64)
	q_value: tensor([[-33.8978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1657180028002867, distance: 1.2355304268702882 entropy 0.8164090514183044
epoch: 33, step: 55
	action: tensor([[ 0.3551, -0.1530, -0.5676, -0.0162, -0.0062, -0.0526, -0.1090]],
       dtype=torch.float64)
	q_value: tensor([[-35.3675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41368141049541074, distance: 0.8762409090678616 entropy 0.8164090514183044
epoch: 33, step: 56
	action: tensor([[-0.9293,  0.4677, -0.3410, -0.0379, -0.6401,  0.4763,  0.9235]],
       dtype=torch.float64)
	q_value: tensor([[-27.0355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5329345176213565, distance: 1.4168324499487013 entropy 0.8164090514183044
epoch: 33, step: 57
	action: tensor([[-0.0756, -1.5830, -0.1210, -0.5697,  0.0743,  0.3782,  0.5470]],
       dtype=torch.float64)
	q_value: tensor([[-33.7996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9450326050270683, distance: 1.5959531424644646 entropy 0.8164090514183044
epoch: 33, step: 58
	action: tensor([[ 1.3211, -1.2006,  0.1904,  0.1473,  0.3478, -0.1085,  0.7290]],
       dtype=torch.float64)
	q_value: tensor([[-35.9680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5647104158154728, distance: 1.4314417505713501 entropy 0.8164090514183044
epoch: 33, step: 59
	action: tensor([[ 0.7256, -0.1900,  0.2993,  0.1993,  0.3640, -0.9788,  0.1890]],
       dtype=torch.float64)
	q_value: tensor([[-37.8232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4814611040977871, distance: 0.8240382337705808 entropy 0.8164090514183044
epoch: 33, step: 60
	action: tensor([[ 0.6689, -0.7353, -0.9394,  0.0835, -0.6014, -0.2086,  0.2582]],
       dtype=torch.float64)
	q_value: tensor([[-30.8749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01842215635314637, distance: 1.1548367951068024 entropy 0.8164090514183044
epoch: 33, step: 61
	action: tensor([[ 0.0830, -0.1656, -0.6604, -0.4269,  0.1453,  0.0544, -0.3908]],
       dtype=torch.float64)
	q_value: tensor([[-34.6301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19201326505736005, distance: 1.0286291054581385 entropy 0.8164090514183044
epoch: 33, step: 62
	action: tensor([[-0.1289, -0.5535,  0.2716, -0.5547,  0.5154, -0.3362,  1.5484]],
       dtype=torch.float64)
	q_value: tensor([[-27.9401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7364548791553991, distance: 1.5079552190545575 entropy 0.8164090514183044
epoch: 33, step: 63
	action: tensor([[ 0.0813, -0.5903,  0.0162, -0.4938,  1.2973, -0.4259,  0.6478]],
       dtype=torch.float64)
	q_value: tensor([[-35.7752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5587462415400022, distance: 1.4287110475133533 entropy 0.8164090514183044
epoch: 33, step: 64
	action: tensor([[ 0.6706, -0.9032, -0.4357,  0.0051, -0.6155, -0.2602, -0.5689]],
       dtype=torch.float64)
	q_value: tensor([[-34.1164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2994120611663367, distance: 1.3044581186388067 entropy 0.8164090514183044
epoch: 33, step: 65
	action: tensor([[ 0.8806, -0.7356,  0.7627,  0.3655,  0.5283, -0.4035, -0.3849]],
       dtype=torch.float64)
	q_value: tensor([[-33.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15062431355533212, distance: 1.0546457473823687 entropy 0.8164090514183044
epoch: 33, step: 66
	action: tensor([[ 0.6644, -1.1712,  0.1449, -0.5777, -0.6267,  0.1850,  0.4658]],
       dtype=torch.float64)
	q_value: tensor([[-34.0728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8966075837700687, distance: 1.5759608867173327 entropy 0.8164090514183044
epoch: 33, step: 67
	action: tensor([[ 0.1600, -0.6706,  0.5894, -0.7126, -0.0765,  0.5383, -0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-37.7610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5010206635876371, distance: 1.4020065052363722 entropy 0.8164090514183044
epoch: 33, step: 68
	action: tensor([[ 1.7043, -0.0495, -0.2578,  1.5734,  0.9413,  0.2195, -0.3794]],
       dtype=torch.float64)
	q_value: tensor([[-32.0982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8279460483751193, distance: 0.4746669776393819 entropy 0.8164090514183044
epoch: 33, step: 69
	action: tensor([[ 1.2839, -0.1198, -0.1292, -0.7524,  0.6164,  0.6965,  1.4169]],
       dtype=torch.float64)
	q_value: tensor([[-39.9278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2703510406011187, distance: 0.9774930471885067 entropy 0.8164090514183044
epoch: 33, step: 70
	action: tensor([[ 0.6430, -0.8503,  0.4010,  0.1658,  0.4266,  0.2028,  1.1166]],
       dtype=torch.float64)
	q_value: tensor([[-40.9299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05231442092610317, distance: 1.1140093329318699 entropy 0.8164090514183044
epoch: 33, step: 71
	action: tensor([[ 0.1974, -0.0865, -0.7270,  0.8822,  0.5835, -0.1100,  0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-35.3168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40831513126737407, distance: 0.8802416718938991 entropy 0.8164090514183044
epoch: 33, step: 72
	action: tensor([[ 1.2042, -0.7599,  0.8894,  0.1445, -0.8531,  0.1569, -0.2016]],
       dtype=torch.float64)
	q_value: tensor([[-29.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004233307295204103, distance: 1.1467638763924506 entropy 0.8164090514183044
epoch: 33, step: 73
	action: tensor([[ 1.0912, -0.8054, -0.1134, -0.0562,  0.2687,  0.6590,  0.2627]],
       dtype=torch.float64)
	q_value: tensor([[-37.0968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10866575923628896, distance: 1.0803811257038451 entropy 0.8164090514183044
epoch: 33, step: 74
	action: tensor([[-0.7212, -0.4715,  0.1407, -0.6460, -0.6139,  0.2312,  0.8043]],
       dtype=torch.float64)
	q_value: tensor([[-35.2022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1017209278935716, distance: 1.6589917168201485 entropy 0.8164090514183044
epoch: 33, step: 75
	action: tensor([[-0.4901, -0.4785, -0.6043, -0.6155,  0.6714, -0.1985, -0.0654]],
       dtype=torch.float64)
	q_value: tensor([[-34.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5939235112701178, distance: 1.4447424435588803 entropy 0.8164090514183044
epoch: 33, step: 76
	action: tensor([[-0.2757, -0.2198,  0.2718,  0.1542,  0.0186, -0.1486, -0.1974]],
       dtype=torch.float64)
	q_value: tensor([[-30.9338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15415612457554562, distance: 1.2293880108988813 entropy 0.8164090514183044
epoch: 33, step: 77
	action: tensor([[ 0.3036,  0.2355, -0.5103, -0.5394, -0.4165, -0.0801,  0.1683]],
       dtype=torch.float64)
	q_value: tensor([[-27.7065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5717281313407803, distance: 0.748887303099541 entropy 0.8164090514183044
epoch: 33, step: 78
	action: tensor([[-0.4926, -0.2499,  0.0755, -0.5681,  0.3007, -0.1515, -0.1841]],
       dtype=torch.float64)
	q_value: tensor([[-28.3841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.724648437337265, distance: 1.5028200579092221 entropy 0.8164090514183044
epoch: 33, step: 79
	action: tensor([[ 0.4109, -0.1543,  0.7576, -0.6002,  0.6216, -0.7896,  0.3759]],
       dtype=torch.float64)
	q_value: tensor([[-28.7603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00826914374120491, distance: 1.139603058684212 entropy 0.8164090514183044
epoch: 33, step: 80
	action: tensor([[ 0.0448, -0.8225,  0.4459, -0.0427, -0.1435, -0.5399, -0.0308]],
       dtype=torch.float64)
	q_value: tensor([[-31.8223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6214365959847694, distance: 1.4571581131387539 entropy 0.8164090514183044
epoch: 33, step: 81
	action: tensor([[ 0.1589, -0.3829, -0.2525, -0.5286,  0.9671,  0.0616,  0.4296]],
       dtype=torch.float64)
	q_value: tensor([[-31.2859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1079488408908229, distance: 1.2045270212547314 entropy 0.8164090514183044
epoch: 33, step: 82
	action: tensor([[ 0.4056,  0.4069,  0.3458, -0.5367,  0.6039,  0.1098, -0.2425]],
       dtype=torch.float64)
	q_value: tensor([[-32.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6406960353566051, distance: 0.6859424778749466 entropy 0.8164090514183044
epoch: 33, step: 83
	action: tensor([[ 0.4210, -0.0291,  0.8521, -0.7877,  0.7123, -0.1402, -0.4138]],
       dtype=torch.float64)
	q_value: tensor([[-28.9088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10586141368420654, distance: 1.082079356582278 entropy 0.8164090514183044
epoch: 33, step: 84
	action: tensor([[ 1.0825, -0.4075,  0.3269, -0.1508, -0.8348,  0.3605, -0.2700]],
       dtype=torch.float64)
	q_value: tensor([[-31.7229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3545356723482056, distance: 0.9193752665916034 entropy 0.8164090514183044
epoch: 33, step: 85
	action: tensor([[ 1.0261,  0.1743,  0.0966,  0.4303,  0.9060, -0.4242, -0.2738]],
       dtype=torch.float64)
	q_value: tensor([[-34.0424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9427824260451175, distance: 0.2737293795901234 entropy 0.8164090514183044
epoch: 33, step: 86
	action: tensor([[ 0.2588, -0.4709,  0.4849,  0.7078,  0.3728,  0.0718,  0.3946]],
       dtype=torch.float64)
	q_value: tensor([[-32.1624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6452575562395348, distance: 0.6815744001369834 entropy 0.8164090514183044
epoch: 33, step: 87
	action: tensor([[-0.2841, -0.3509, -0.5666,  0.2759,  0.5103, -0.2041,  1.2016]],
       dtype=torch.float64)
	q_value: tensor([[-30.8582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3395998288180584, distance: 1.3244764325395477 entropy 0.8164090514183044
epoch: 33, step: 88
	action: tensor([[-0.1131,  0.4503, -0.0516, -0.8711, -0.3480, -0.1343,  0.5909]],
       dtype=torch.float64)
	q_value: tensor([[-33.0517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1468560600423181, distance: 1.0569826252236405 entropy 0.8164090514183044
epoch: 33, step: 89
	action: tensor([[ 0.5688, -0.6841,  0.3160, -1.0359,  0.1394,  0.7707,  0.9208]],
       dtype=torch.float64)
	q_value: tensor([[-30.4583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34663024253761465, distance: 1.3279474064295884 entropy 0.8164090514183044
epoch: 33, step: 90
	action: tensor([[ 0.7061, -0.7578, -0.5482, -0.0230,  0.4871, -0.2765,  0.4047]],
       dtype=torch.float64)
	q_value: tensor([[-38.1133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17035266084574263, distance: 1.2379840994965685 entropy 0.8164090514183044
epoch: 33, step: 91
	action: tensor([[ 0.0967, -0.5147,  0.1068,  0.0216,  0.4824,  0.4278,  0.3981]],
       dtype=torch.float64)
	q_value: tensor([[-32.8442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09885479013501186, distance: 1.0863107631052358 entropy 0.8164090514183044
epoch: 33, step: 92
	action: tensor([[ 0.1221,  0.0269,  0.4305, -0.1300, -0.2663,  0.3684, -0.3810]],
       dtype=torch.float64)
	q_value: tensor([[-29.8900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45554009311176047, distance: 0.8443833198183591 entropy 0.8164090514183044
epoch: 33, step: 93
	action: tensor([[ 0.7476,  0.0354,  0.7273,  0.6187, -0.3257, -0.9506,  0.2105]],
       dtype=torch.float64)
	q_value: tensor([[-28.6978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8183255150919102, distance: 0.4877571698004461 entropy 0.8164090514183044
epoch: 33, step: 94
	action: tensor([[-0.3715, -0.1197,  0.6210, -0.2876, -0.1707,  0.6815,  0.9238]],
       dtype=torch.float64)
	q_value: tensor([[-33.8495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20499294752915498, distance: 1.256171528683672 entropy 0.8164090514183044
epoch: 33, step: 95
	action: tensor([[ 0.9204, -0.2147,  0.1518, -0.2917,  0.2053,  0.8504,  0.4981]],
       dtype=torch.float64)
	q_value: tensor([[-33.7536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6595144562320487, distance: 0.6677379077689634 entropy 0.8164090514183044
epoch: 33, step: 96
	action: tensor([[-0.5169,  0.4257, -0.4174, -0.3453,  0.5500,  0.4143,  0.7962]],
       dtype=torch.float64)
	q_value: tensor([[-33.7805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 97
	action: tensor([[ 0.9360, -0.6717,  0.0953, -0.8038,  0.7776,  0.3415, -0.1085]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39566158492423353, distance: 1.3519068099003297 entropy 0.8164090514183044
epoch: 33, step: 98
	action: tensor([[ 1.0105, -0.9180,  0.6922, -1.6783, -0.1361,  1.4621,  0.7800]],
       dtype=torch.float64)
	q_value: tensor([[-34.7712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2488896591926002, distance: 1.2788474095436808 entropy 0.8164090514183044
epoch: 33, step: 99
	action: tensor([[ 0.2344, -1.0396, -0.4356, -0.4386,  0.6068,  0.5796,  1.0345]],
       dtype=torch.float64)
	q_value: tensor([[-46.1981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4679184893763302, distance: 1.3864610188179385 entropy 0.8164090514183044
epoch: 33, step: 100
	action: tensor([[ 0.8006,  0.3253, -0.4805, -0.0269,  0.6433,  0.4329,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-38.4294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.969669428439344, distance: 0.19929526955449584 entropy 0.8164090514183044
epoch: 33, step: 101
	action: tensor([[ 0.7051, -0.3485, -0.1345,  0.6044,  0.4083, -0.4704, -0.5991]],
       dtype=torch.float64)
	q_value: tensor([[-30.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6567832803262048, distance: 0.6704106601516476 entropy 0.8164090514183044
epoch: 33, step: 102
	action: tensor([[-0.5585, -0.4071,  0.1088,  0.0435, -0.4148,  0.2007,  0.4207]],
       dtype=torch.float64)
	q_value: tensor([[-30.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5761630245614215, distance: 1.4366707864640198 entropy 0.8164090514183044
epoch: 33, step: 103
	action: tensor([[ 0.3054, -0.1006, -0.7115, -0.8356, -0.0092,  0.5716, -0.1359]],
       dtype=torch.float64)
	q_value: tensor([[-29.8118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3714450122178943, distance: 0.9072528304160306 entropy 0.8164090514183044
epoch: 33, step: 104
	action: tensor([[-0.0918,  0.2469,  0.5033, -1.9480,  0.0556, -0.6646, -0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-31.4028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24058389334137553, distance: 1.2745878153632804 entropy 0.8164090514183044
epoch: 33, step: 105
	action: tensor([[-0.0201, -0.7214, -0.5175,  0.9341, -0.8942, -0.9292,  0.6854]],
       dtype=torch.float64)
	q_value: tensor([[-36.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15465411450702393, distance: 1.0521409279799 entropy 0.8164090514183044
epoch: 33, step: 106
	action: tensor([[-0.3582, -0.7050, -0.0010,  0.0395, -0.4177,  0.2540, -0.1938]],
       dtype=torch.float64)
	q_value: tensor([[-38.8284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.558858902907005, distance: 1.4287626779927243 entropy 0.8164090514183044
epoch: 33, step: 107
	action: tensor([[-0.1713, -0.1299,  0.5061, -0.4345,  0.3502, -0.1622, -0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-30.5693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3346621656985369, distance: 1.3220332193712876 entropy 0.8164090514183044
epoch: 33, step: 108
	action: tensor([[-0.0476,  0.7219, -0.5243, -0.4954, -0.0123,  0.7599,  0.2571]],
       dtype=torch.float64)
	q_value: tensor([[-28.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 109
	action: tensor([[-0.2511,  0.7310,  0.6545, -0.3143, -0.1518, -0.0251,  0.9139]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 110
	action: tensor([[-0.4935, -0.5301, -0.1493,  0.2791, -0.4572, -0.1466, -0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.538505146897954, distance: 1.41940447466128 entropy 0.8164090514183044
epoch: 33, step: 111
	action: tensor([[ 0.4913, -0.6698, -0.5858, -1.1001, -0.9079, -0.4168,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-30.5077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2172546326866398, distance: 1.2625465843563337 entropy 0.8164090514183044
epoch: 33, step: 112
	action: tensor([[ 0.6339, -0.1174,  0.6807,  0.3246, -0.5625, -0.1857, -0.4718]],
       dtype=torch.float64)
	q_value: tensor([[-36.6390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7247970823880957, distance: 0.6003205499095897 entropy 0.8164090514183044
epoch: 33, step: 113
	action: tensor([[-0.3129, -1.1346, -0.0079, -0.7389,  0.4147, -0.0413,  0.6643]],
       dtype=torch.float64)
	q_value: tensor([[-31.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.022258015387874, distance: 1.6273275464964299 entropy 0.8164090514183044
epoch: 33, step: 114
	action: tensor([[ 0.2890, -0.7963,  0.7837, -0.5841, -0.6370,  0.2226, -0.1457]],
       dtype=torch.float64)
	q_value: tensor([[-35.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5621291824681263, distance: 1.4302605702634092 entropy 0.8164090514183044
epoch: 33, step: 115
	action: tensor([[ 0.6350, -0.3660, -0.1599,  0.6229, -0.0887,  1.6601,  0.2136]],
       dtype=torch.float64)
	q_value: tensor([[-33.9033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.880358786658954, distance: 0.39581941961036016 entropy 0.8164090514183044
epoch: 33, step: 116
	action: tensor([[ 0.7633, -0.0336,  0.8950, -0.4728,  0.2666,  0.2272,  0.5940]],
       dtype=torch.float64)
	q_value: tensor([[-37.7474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5840436681429857, distance: 0.7380411265914844 entropy 0.8164090514183044
epoch: 33, step: 117
	action: tensor([[ 0.3401, -0.1077, -0.5132, -0.0836, -0.4877, -0.2705,  0.0788]],
       dtype=torch.float64)
	q_value: tensor([[-31.8679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45097642217442635, distance: 0.8479147518270684 entropy 0.8164090514183044
epoch: 33, step: 118
	action: tensor([[ 0.5161,  0.8181, -0.0213, -0.2513, -1.0152, -0.9808,  0.6715]],
       dtype=torch.float64)
	q_value: tensor([[-28.4617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 119
	action: tensor([[-0.2082, -1.1115, -0.4802,  0.2804,  0.1941,  0.2494,  0.8215]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7484291331988089, distance: 1.5131455690024185 entropy 0.8164090514183044
epoch: 33, step: 120
	action: tensor([[-0.1393, -0.5988, -1.1080, -0.0496, -0.1538,  0.3857, -1.0978]],
       dtype=torch.float64)
	q_value: tensor([[-35.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4365775704555763, distance: 1.3715802850541383 entropy 0.8164090514183044
epoch: 33, step: 121
	action: tensor([[-0.0135, -0.3272,  0.0800, -1.3040,  0.0463, -0.8642,  0.3231]],
       dtype=torch.float64)
	q_value: tensor([[-32.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.352769274200456, distance: 1.3309708945504974 entropy 0.8164090514183044
epoch: 33, step: 122
	action: tensor([[ 0.3812, -0.3643, -0.6543, -0.3830, -0.8903, -0.6077,  1.0058]],
       dtype=torch.float64)
	q_value: tensor([[-33.7116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2637173848406824, distance: 0.9819264673147331 entropy 0.8164090514183044
epoch: 33, step: 123
	action: tensor([[ 0.7589, -0.9715,  0.5512, -0.0702,  0.3425, -0.7289,  0.6158]],
       dtype=torch.float64)
	q_value: tensor([[-36.9707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48310843555429805, distance: 1.3936160706277765 entropy 0.8164090514183044
epoch: 33, step: 124
	action: tensor([[-1.2028, -0.4717,  0.0947, -0.1552,  0.3429,  0.5015,  0.8446]],
       dtype=torch.float64)
	q_value: tensor([[-34.5557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.395415346263487, distance: 1.7711164103293586 entropy 0.8164090514183044
epoch: 33, step: 125
	action: tensor([[ 0.6685, -1.0127, -0.0232, -0.3490, -0.0286,  0.6694,  0.6231]],
       dtype=torch.float64)
	q_value: tensor([[-33.6298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33716869907317015, distance: 1.3232740449958804 entropy 0.8164090514183044
epoch: 33, step: 126
	action: tensor([[-0.2128,  0.4502,  0.5757,  0.5166, -0.2786,  0.7689,  0.6802]],
       dtype=torch.float64)
	q_value: tensor([[-36.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 33, step: 127
	action: tensor([[ 0.8181,  0.4109,  0.1128,  0.0725,  0.6398, -0.4450, -0.3862]],
       dtype=torch.float64)
	q_value: tensor([[-34.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
LOSS epoch 33 actor 425.9404974056428 critic 127.883346170224 
epoch: 34, step: 0
	action: tensor([[ 0.9589, -0.1129, -1.0291,  0.1342,  0.6135,  0.2332, -0.9809]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 1
	action: tensor([[-0.3176, -1.2451, -0.0839, -0.8047, -0.7459, -0.2074,  0.6793]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8465527882739436, distance: 1.5550256499090247 entropy 0.8164090514183044
epoch: 34, step: 2
	action: tensor([[-0.1254, -0.5943,  0.7832, -0.0322,  0.6283, -1.1814,  0.4411]],
       dtype=torch.float64)
	q_value: tensor([[-35.9152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5098440917399383, distance: 1.4061211646362852 entropy 0.8164090514183044
epoch: 34, step: 3
	action: tensor([[ 0.6263,  0.0572,  0.1354, -1.2815, -0.6554,  0.1210,  1.2818]],
       dtype=torch.float64)
	q_value: tensor([[-31.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09633332376696013, distance: 1.087829487643487 entropy 0.8164090514183044
epoch: 34, step: 4
	action: tensor([[-0.1841, -0.8501, -1.2793, -0.9244, -0.2829, -0.1589,  0.0293]],
       dtype=torch.float64)
	q_value: tensor([[-37.1252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009038200482988312, distance: 1.1391611094473375 entropy 0.8164090514183044
epoch: 34, step: 5
	action: tensor([[ 0.3984, -2.0197, -0.0187, -0.4748,  0.3495,  0.1248, -0.0887]],
       dtype=torch.float64)
	q_value: tensor([[-33.9479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 6
	action: tensor([[ 0.0901, -1.3747, -0.3342, -0.6620,  0.6310,  0.0215,  1.4028]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8818296634530185, distance: 1.569809121680711 entropy 0.8164090514183044
epoch: 34, step: 7
	action: tensor([[ 0.2778, -0.5592, -0.1765, -0.5826,  0.9677,  0.5485,  0.8383]],
       dtype=torch.float64)
	q_value: tensor([[-37.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10510534399065885, distance: 1.20298034819667 entropy 0.8164090514183044
epoch: 34, step: 8
	action: tensor([[-1.2303,  0.4320, -0.2703,  0.4896, -0.8190,  0.2964,  1.2079]],
       dtype=torch.float64)
	q_value: tensor([[-33.1338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6067962454613713, distance: 1.4505646761348587 entropy 0.8164090514183044
epoch: 34, step: 9
	action: tensor([[ 0.0030, -1.1841, -0.3944,  0.8410,  1.0964, -0.1705,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-36.0743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43515285435982043, distance: 1.370899988603811 entropy 0.8164090514183044
epoch: 34, step: 10
	action: tensor([[-0.0344, -0.6970,  0.5109,  0.3660,  0.6405, -0.6415,  0.4739]],
       dtype=torch.float64)
	q_value: tensor([[-33.4955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24151922131278036, distance: 1.275068207307121 entropy 0.8164090514183044
epoch: 34, step: 11
	action: tensor([[ 0.7490, -0.7550,  0.1289, -0.1507, -0.0690,  0.1197,  0.4289]],
       dtype=torch.float64)
	q_value: tensor([[-29.7104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11287088939391254, distance: 1.2071996042299176 entropy 0.8164090514183044
epoch: 34, step: 12
	action: tensor([[ 0.4441, -0.4244,  0.3077, -1.3202,  0.2334, -0.7895,  0.1845]],
       dtype=torch.float64)
	q_value: tensor([[-30.3334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41752815893223794, distance: 1.3624561721843358 entropy 0.8164090514183044
epoch: 34, step: 13
	action: tensor([[ 0.8096, -0.0984,  0.0925,  1.2644, -0.6925,  0.1688,  0.2423]],
       dtype=torch.float64)
	q_value: tensor([[-32.5488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9857256537965761, distance: 0.1367208677500567 entropy 0.8164090514183044
epoch: 34, step: 14
	action: tensor([[-0.2148, -0.1116, -0.0768, -0.0902,  0.0548, -0.3966,  0.6514]],
       dtype=torch.float64)
	q_value: tensor([[-32.5185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13725988881983042, distance: 1.220356037789046 entropy 0.8164090514183044
epoch: 34, step: 15
	action: tensor([[-0.7258,  0.6577, -0.4807, -0.1375,  0.1179,  0.6629,  0.7547]],
       dtype=torch.float64)
	q_value: tensor([[-26.9278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021620423016227797, distance: 1.1566487062007949 entropy 0.8164090514183044
epoch: 34, step: 16
	action: tensor([[ 0.6910,  0.1689,  0.7343, -0.2419, -0.5827,  0.4229,  0.6068]],
       dtype=torch.float64)
	q_value: tensor([[-30.5800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 17
	action: tensor([[ 0.0668, -0.4258,  0.2168, -0.0525,  0.3321,  0.0594,  0.6344]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010443673990704783, distance: 1.1503043123526666 entropy 0.8164090514183044
epoch: 34, step: 18
	action: tensor([[-0.4579, -0.5040, -0.5100, -0.4732,  0.6435,  0.1775, -0.6586]],
       dtype=torch.float64)
	q_value: tensor([[-27.5092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5831090299748538, distance: 1.4398329444370042 entropy 0.8164090514183044
epoch: 34, step: 19
	action: tensor([[ 0.3218, -0.6739,  0.2915, -0.3530,  0.8256,  0.3308,  0.4287]],
       dtype=torch.float64)
	q_value: tensor([[-29.0318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2173692841734356, distance: 1.2626060416920475 entropy 0.8164090514183044
epoch: 34, step: 20
	action: tensor([[ 1.0301, -0.8193,  0.6527, -0.2371, -0.3299,  0.0358, -0.6625]],
       dtype=torch.float64)
	q_value: tensor([[-30.0972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2621523738728755, distance: 1.2856199038001803 entropy 0.8164090514183044
epoch: 34, step: 21
	action: tensor([[-0.1329, -1.4125, -0.3318, -0.4432, -0.1319,  0.6273, -0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-32.0444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 22
	action: tensor([[-0.1415, -0.5949,  0.6210,  0.5320, -0.8916,  0.5530, -0.2657]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.189058093159715, distance: 1.0305084688855917 entropy 0.8164090514183044
epoch: 34, step: 23
	action: tensor([[ 1.2249, -0.7173, -0.0512, -1.0238,  0.7723, -0.1576,  1.0814]],
       dtype=torch.float64)
	q_value: tensor([[-33.5026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6396269673339294, distance: 1.4653090084972498 entropy 0.8164090514183044
epoch: 34, step: 24
	action: tensor([[-0.1836, -0.9106, -0.5697, -0.0594,  0.5589,  0.1644,  1.2013]],
       dtype=torch.float64)
	q_value: tensor([[-36.8030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6523797328503351, distance: 1.4709964445848833 entropy 0.8164090514183044
epoch: 34, step: 25
	action: tensor([[ 0.2836,  0.4151, -0.5535, -1.2573, -0.3944, -0.5863,  0.1535]],
       dtype=torch.float64)
	q_value: tensor([[-33.9377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5112754468684789, distance: 0.799997762572505 entropy 0.8164090514183044
epoch: 34, step: 26
	action: tensor([[ 0.2976, -1.0330,  0.6496,  0.6895,  0.3520,  0.5912, -0.1540]],
       dtype=torch.float64)
	q_value: tensor([[-30.9827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3600478481613941, distance: 0.9154411808300009 entropy 0.8164090514183044
epoch: 34, step: 27
	action: tensor([[-0.0368, -0.6982, -0.1846, -0.2168, -0.3052, -0.4135,  0.4303]],
       dtype=torch.float64)
	q_value: tensor([[-33.1470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47458360618689666, distance: 1.3896050826065736 entropy 0.8164090514183044
epoch: 34, step: 28
	action: tensor([[ 0.1942, -0.9539,  0.5827, -0.4241, -0.1414, -0.6062,  0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-29.5834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.845454932351803, distance: 1.5545633159981231 entropy 0.8164090514183044
epoch: 34, step: 29
	action: tensor([[ 0.4367,  0.0681,  0.0732,  0.4914,  0.8526,  0.7667, -0.8926]],
       dtype=torch.float64)
	q_value: tensor([[-31.4285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9666269620123396, distance: 0.2090521145012592 entropy 0.8164090514183044
epoch: 34, step: 30
	action: tensor([[ 0.4934, -0.2209,  0.5568, -0.4500, -0.1845,  0.1308, -0.0615]],
       dtype=torch.float64)
	q_value: tensor([[-31.6293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2197321941951279, distance: 1.0108309650800504 entropy 0.8164090514183044
epoch: 34, step: 31
	action: tensor([[ 0.9910,  0.3022,  0.4211, -0.1100, -0.1201, -0.4249,  0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-27.5297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.836101293849254, distance: 0.46328096426378296 entropy 0.8164090514183044
epoch: 34, step: 32
	action: tensor([[ 0.2864, -0.2908,  0.9772, -0.5858,  0.4039,  0.4677,  0.2145]],
       dtype=torch.float64)
	q_value: tensor([[-28.4298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.053899360269452745, distance: 1.1130773908909253 entropy 0.8164090514183044
epoch: 34, step: 33
	action: tensor([[ 1.6467, -0.4637,  0.3585,  0.0975,  0.6279,  0.1816,  0.7607]],
       dtype=torch.float64)
	q_value: tensor([[-29.3159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025066216251960483, distance: 1.129911043230941 entropy 0.8164090514183044
epoch: 34, step: 34
	action: tensor([[ 0.2013, -0.2205, -0.5260, -0.3832, -0.2219,  0.4843,  0.8789]],
       dtype=torch.float64)
	q_value: tensor([[-33.9677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27451186408718, distance: 0.9747019852432753 entropy 0.8164090514183044
epoch: 34, step: 35
	action: tensor([[ 0.4322,  0.3514, -0.0579, -0.1902,  0.7553,  0.0398, -0.4232]],
       dtype=torch.float64)
	q_value: tensor([[-30.4911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8079244643097329, distance: 0.5015251538470175 entropy 0.8164090514183044
epoch: 34, step: 36
	action: tensor([[ 0.7490, -0.2301,  0.0884, -1.0089, -1.3577, -0.2983, -0.4196]],
       dtype=torch.float64)
	q_value: tensor([[-26.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0538881250420713, distance: 1.1747729790967048 entropy 0.8164090514183044
epoch: 34, step: 37
	action: tensor([[ 0.3339, -0.5852,  0.7017, -0.7063,  0.5804,  0.1624,  0.9250]],
       dtype=torch.float64)
	q_value: tensor([[-34.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4236719464931662, distance: 1.3654055283988982 entropy 0.8164090514183044
epoch: 34, step: 38
	action: tensor([[ 0.3863, -0.1801,  0.0884, -0.1651,  0.6662, -0.0661,  0.9072]],
       dtype=torch.float64)
	q_value: tensor([[-31.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33679019607962635, distance: 0.9319275743498336 entropy 0.8164090514183044
epoch: 34, step: 39
	action: tensor([[-0.3369,  0.1725,  1.0445,  0.1301, -0.3074, -0.2268,  0.5140]],
       dtype=torch.float64)
	q_value: tensor([[-28.7869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11099844861764852, distance: 1.0789664798223864 entropy 0.8164090514183044
epoch: 34, step: 40
	action: tensor([[ 0.9877, -0.4897, -0.4826,  0.6594,  1.2317, -0.7553,  0.0041]],
       dtype=torch.float64)
	q_value: tensor([[-30.1290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5544879603227109, distance: 0.7638118921039382 entropy 0.8164090514183044
epoch: 34, step: 41
	action: tensor([[ 0.9964, -1.0644,  0.1605, -0.0824, -0.5911,  0.6158, -0.1153]],
       dtype=torch.float64)
	q_value: tensor([[-33.3967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.231451129077765, distance: 1.2698876041478429 entropy 0.8164090514183044
epoch: 34, step: 42
	action: tensor([[ 0.3552, -0.0861,  0.0207, -1.3745,  0.5105,  0.3785,  0.4671]],
       dtype=torch.float64)
	q_value: tensor([[-34.4408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03344703947280303, distance: 1.163324316321204 entropy 0.8164090514183044
epoch: 34, step: 43
	action: tensor([[ 1.0736, -0.7961,  0.0625, -0.2118,  0.4487,  0.8348, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-32.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02783359167323829, distance: 1.1283062623755815 entropy 0.8164090514183044
epoch: 34, step: 44
	action: tensor([[ 1.1668,  0.1681,  0.6838, -0.2351, -0.0946,  0.2462,  0.2917]],
       dtype=torch.float64)
	q_value: tensor([[-33.1693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7544421577821937, distance: 0.567065985316034 entropy 0.8164090514183044
epoch: 34, step: 45
	action: tensor([[ 1.1920, -0.6153, -0.1396, -0.3453, -0.3834, -0.1390,  0.8573]],
       dtype=torch.float64)
	q_value: tensor([[-30.0649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24490921707841373, distance: 1.2768078216107261 entropy 0.8164090514183044
epoch: 34, step: 46
	action: tensor([[ 0.6319, -0.3163, -0.4220, -0.0977,  0.2516, -0.4083, -0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-34.3188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2781729245854686, distance: 0.9722395354464749 entropy 0.8164090514183044
epoch: 34, step: 47
	action: tensor([[ 0.4539, -0.8459,  0.6413, -0.1129,  0.0677, -0.6703, -0.0785]],
       dtype=torch.float64)
	q_value: tensor([[-27.1402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4540360646316328, distance: 1.3798894127755943 entropy 0.8164090514183044
epoch: 34, step: 48
	action: tensor([[ 0.1778, -0.1685, -0.4691, -0.3844,  0.0799,  0.4461,  0.8181]],
       dtype=torch.float64)
	q_value: tensor([[-30.3216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33939645415990005, distance: 0.9300946440671282 entropy 0.8164090514183044
epoch: 34, step: 49
	action: tensor([[-0.2661, -0.1902,  0.5610,  0.5288,  0.2426,  0.2803, -0.0756]],
       dtype=torch.float64)
	q_value: tensor([[-29.5927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45478829434174206, distance: 0.8449660875729388 entropy 0.8164090514183044
epoch: 34, step: 50
	action: tensor([[ 0.4902,  0.1260,  0.4142,  0.4457, -0.4244, -0.5377,  0.0788]],
       dtype=torch.float64)
	q_value: tensor([[-28.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8574426023483415, distance: 0.4320674727993718 entropy 0.8164090514183044
epoch: 34, step: 51
	action: tensor([[ 1.0587,  0.1272, -0.1625,  0.2532,  0.6195,  0.2827,  0.6904]],
       dtype=torch.float64)
	q_value: tensor([[-28.6692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.908989799452913, distance: 0.34522459512643455 entropy 0.8164090514183044
epoch: 34, step: 52
	action: tensor([[ 1.0818, -0.5098,  0.9089, -0.2162, -0.0798,  0.3547,  1.2601]],
       dtype=torch.float64)
	q_value: tensor([[-30.4086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22678052052861963, distance: 1.0062550815178632 entropy 0.8164090514183044
epoch: 34, step: 53
	action: tensor([[ 0.6721, -0.0913, -0.3779,  0.0474,  0.6173, -0.0252,  0.6637]],
       dtype=torch.float64)
	q_value: tensor([[-34.3136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6824594245235394, distance: 0.6448464406770933 entropy 0.8164090514183044
epoch: 34, step: 54
	action: tensor([[ 0.6043,  0.2992, -1.6448, -0.8690, -0.3930,  0.1900, -0.2999]],
       dtype=torch.float64)
	q_value: tensor([[-28.5457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8656231477676706, distance: 0.419487404291782 entropy 0.8164090514183044
epoch: 34, step: 55
	action: tensor([[ 0.2186, -0.0822, -0.8085, -0.5544,  0.1281, -0.2973,  0.5086]],
       dtype=torch.float64)
	q_value: tensor([[-32.2664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40331528412361817, distance: 0.8839529510262384 entropy 0.8164090514183044
epoch: 34, step: 56
	action: tensor([[-0.9743, -1.1472,  0.4024, -0.7435,  0.6189, -0.0229,  0.7833]],
       dtype=torch.float64)
	q_value: tensor([[-28.7655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2412274953921036, distance: 1.7131668578747008 entropy 0.8164090514183044
epoch: 34, step: 57
	action: tensor([[ 0.6312,  0.1363, -0.6036, -0.0074, -0.1936,  0.7501,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-34.0672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.879980007883987, distance: 0.3964454978312944 entropy 0.8164090514183044
epoch: 34, step: 58
	action: tensor([[ 0.2813, -0.0880, -0.4300,  0.0938,  0.3532, -0.1476, -0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-28.6203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.520737670938071, distance: 0.7922155090435197 entropy 0.8164090514183044
epoch: 34, step: 59
	action: tensor([[ 0.0124,  0.0605,  0.2866,  0.1835, -0.2247, -0.3420,  0.8436]],
       dtype=torch.float64)
	q_value: tensor([[-25.2183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43759013028758675, distance: 0.8581894276942549 entropy 0.8164090514183044
epoch: 34, step: 60
	action: tensor([[ 0.0950, -0.1793, -0.4746, -0.5670, -0.1224,  0.1436,  0.7670]],
       dtype=torch.float64)
	q_value: tensor([[-27.9257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13414985432230053, distance: 1.064824562293085 entropy 0.8164090514183044
epoch: 34, step: 61
	action: tensor([[ 0.5726, -0.5492, -0.1468, -0.0475,  0.5688, -0.3699,  0.4561]],
       dtype=torch.float64)
	q_value: tensor([[-29.2873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03775430756726561, distance: 1.1225344575650285 entropy 0.8164090514183044
epoch: 34, step: 62
	action: tensor([[ 0.6880, -0.9085, -0.3684, -0.8419,  0.7249, -0.0632,  0.0225]],
       dtype=torch.float64)
	q_value: tensor([[-28.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6973312538169092, distance: 1.4908707607500713 entropy 0.8164090514183044
epoch: 34, step: 63
	action: tensor([[ 1.1131,  0.2260,  0.5019, -0.1562,  0.1191, -0.4623,  0.8139]],
       dtype=torch.float64)
	q_value: tensor([[-33.4042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7267401848250366, distance: 0.5981974789413628 entropy 0.8164090514183044
epoch: 34, step: 64
	action: tensor([[ 0.2672, -0.7545,  1.0695, -0.2097,  0.4404, -0.3311,  0.0593]],
       dtype=torch.float64)
	q_value: tensor([[-30.9072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32882508850785896, distance: 1.3191391290470416 entropy 0.8164090514183044
epoch: 34, step: 65
	action: tensor([[-0.8963, -0.6460, -0.3511,  0.0240,  0.7805,  0.0725,  1.1525]],
       dtype=torch.float64)
	q_value: tensor([[-30.5228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1039274217767856, distance: 1.65986233545736 entropy 0.8164090514183044
epoch: 34, step: 66
	action: tensor([[ 0.0313, -0.4293, -0.1271, -0.2709,  0.1290, -0.6255,  0.0660]],
       dtype=torch.float64)
	q_value: tensor([[-33.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26870935232350335, distance: 1.2889550248936394 entropy 0.8164090514183044
epoch: 34, step: 67
	action: tensor([[-0.0651, -0.5354,  0.4681, -0.1728,  0.1321, -0.3165,  0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-27.4553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41793714761806267, distance: 1.3626527076000037 entropy 0.8164090514183044
epoch: 34, step: 68
	action: tensor([[ 0.2492, -1.1137, -0.6899, -0.5135, -0.3326, -0.0881,  0.8733]],
       dtype=torch.float64)
	q_value: tensor([[-27.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6846287431422433, distance: 1.4852815846065084 entropy 0.8164090514183044
epoch: 34, step: 69
	action: tensor([[ 0.1295, -0.6204, -0.8433,  1.3397,  0.6336, -0.3242, -0.5345]],
       dtype=torch.float64)
	q_value: tensor([[-35.2100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08034112181922437, distance: 1.0974129408743474 entropy 0.8164090514183044
epoch: 34, step: 70
	action: tensor([[ 0.9476, -0.4591, -0.4079, -0.5197,  0.6583,  0.7211,  1.3748]],
       dtype=torch.float64)
	q_value: tensor([[-32.8418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23908729993871558, distance: 0.9982150414066052 entropy 0.8164090514183044
epoch: 34, step: 71
	action: tensor([[ 0.5202, -1.7777, -1.4005,  1.0471, -0.5498, -0.2268,  0.7425]],
       dtype=torch.float64)
	q_value: tensor([[-37.4250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 72
	action: tensor([[ 0.1045,  0.2754, -0.1817,  0.6721,  0.3386, -0.5296, -0.4520]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 73
	action: tensor([[ 0.7032,  0.0062, -0.2206,  0.4789, -0.2917,  0.1187,  0.2654]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 74
	action: tensor([[ 0.2787, -0.1745, -0.6504, -1.4907, -0.6078, -0.2754,  0.2825]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16135292918288868, distance: 1.0479638711665573 entropy 0.8164090514183044
epoch: 34, step: 75
	action: tensor([[ 1.1597,  0.5696,  0.1149,  0.0750,  0.0181, -0.4238,  0.2066]],
       dtype=torch.float64)
	q_value: tensor([[-33.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9121918020743904, distance: 0.33909721693116174 entropy 0.8164090514183044
epoch: 34, step: 76
	action: tensor([[-0.3826, -0.3452,  0.9229, -0.1829,  0.1314, -0.6189,  0.5142]],
       dtype=torch.float64)
	q_value: tensor([[-29.3724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7206664295264034, distance: 1.5010841390698282 entropy 0.8164090514183044
epoch: 34, step: 77
	action: tensor([[ 0.1384, -1.2477,  0.1030, -0.3628,  0.2168, -0.0540,  1.0569]],
       dtype=torch.float64)
	q_value: tensor([[-29.5037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9706946550849436, distance: 1.606446854870272 entropy 0.8164090514183044
epoch: 34, step: 78
	action: tensor([[-0.0581, -0.1398,  0.1565, -0.3311, -0.5658, -0.2366,  1.1778]],
       dtype=torch.float64)
	q_value: tensor([[-34.0971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07269481197424188, distance: 1.1852085706826647 entropy 0.8164090514183044
epoch: 34, step: 79
	action: tensor([[ 0.2416, -0.1379, -0.5686,  0.7882, -0.4926,  0.1856,  0.1586]],
       dtype=torch.float64)
	q_value: tensor([[-31.7171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 80
	action: tensor([[ 0.0029, -0.3174, -0.0778, -0.4433, -0.3017,  0.8636,  0.2607]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007637214692423844, distance: 1.13996607733467 entropy 0.8164090514183044
epoch: 34, step: 81
	action: tensor([[ 0.9579, -0.2755, -0.0652, -0.1811,  0.8965, -0.4888,  0.7753]],
       dtype=torch.float64)
	q_value: tensor([[-29.4604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3259078152623952, distance: 0.9395423035744672 entropy 0.8164090514183044
epoch: 34, step: 82
	action: tensor([[ 1.2625, -0.0444, -0.0746, -0.0432, -0.4662, -0.1453,  0.5665]],
       dtype=torch.float64)
	q_value: tensor([[-30.8384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5403036004562533, distance: 0.7758758682613761 entropy 0.8164090514183044
epoch: 34, step: 83
	action: tensor([[ 0.8798, -0.0894, -0.7375,  0.5092,  0.0656,  0.4566, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-31.5936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9014254817688958, distance: 0.3592849494535509 entropy 0.8164090514183044
epoch: 34, step: 84
	action: tensor([[ 0.0918,  0.5513,  1.0630, -0.4078,  0.3742,  0.2311,  1.2951]],
       dtype=torch.float64)
	q_value: tensor([[-29.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 85
	action: tensor([[ 1.3088, -0.1484, -0.2935, -0.0329,  0.1231,  0.5031,  0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 86
	action: tensor([[-0.1819, -0.9532,  0.2493, -0.7453, -0.1608, -0.0554,  0.5168]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9887957741518574, distance: 1.6138077156942865 entropy 0.8164090514183044
epoch: 34, step: 87
	action: tensor([[ 0.7233,  0.3951,  0.3953, -0.2441, -1.1514,  0.1768,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[-31.7571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8935630335396407, distance: 0.373338636847994 entropy 0.8164090514183044
epoch: 34, step: 88
	action: tensor([[ 0.4739, -0.1209, -0.5210, -0.1361, -0.2489,  0.9213,  1.1671]],
       dtype=torch.float64)
	q_value: tensor([[-34.9139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6277252271117545, distance: 0.6982139104082169 entropy 0.8164090514183044
epoch: 34, step: 89
	action: tensor([[ 0.7042, -0.9237, -0.1066,  0.3783,  0.7323,  0.2396,  0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-33.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16682230168328405, distance: 1.0445410484825621 entropy 0.8164090514183044
epoch: 34, step: 90
	action: tensor([[ 0.8744, -0.3826,  0.7259,  0.1692,  0.5248, -1.4663, -0.0874]],
       dtype=torch.float64)
	q_value: tensor([[-31.4317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45860062112166133, distance: 0.8420067437999936 entropy 0.8164090514183044
epoch: 34, step: 91
	action: tensor([[ 0.7055, -0.8479,  0.1406, -0.8621,  0.8847,  0.0794,  1.2068]],
       dtype=torch.float64)
	q_value: tensor([[-33.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.68153246332797, distance: 1.48391601297065 entropy 0.8164090514183044
epoch: 34, step: 92
	action: tensor([[ 0.3712,  0.0232, -0.5079, -0.9012,  0.3845,  0.4331,  0.0570]],
       dtype=torch.float64)
	q_value: tensor([[-35.7873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4697479278362846, distance: 0.8332932817873023 entropy 0.8164090514183044
epoch: 34, step: 93
	action: tensor([[-0.0644, -0.9738, -0.6126, -1.0053,  0.3557, -0.7948, -0.2314]],
       dtype=torch.float64)
	q_value: tensor([[-29.3276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3506759267643509, distance: 1.3299406881175653 entropy 0.8164090514183044
epoch: 34, step: 94
	action: tensor([[ 1.1129, -1.0037,  0.3255, -0.2290,  0.2307, -0.0574,  0.0066]],
       dtype=torch.float64)
	q_value: tensor([[-34.8121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5748006209107401, distance: 1.4360497375481038 entropy 0.8164090514183044
epoch: 34, step: 95
	action: tensor([[ 1.5879, -0.2209,  0.6568,  0.1197,  0.4927,  0.5292, -0.4753]],
       dtype=torch.float64)
	q_value: tensor([[-31.5432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17662090361780047, distance: 1.038380710434636 entropy 0.8164090514183044
epoch: 34, step: 96
	action: tensor([[-0.3426, -1.3550, -0.0926, -0.6472,  0.0688, -1.1439, -0.2205]],
       dtype=torch.float64)
	q_value: tensor([[-33.3405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6479059368624778, distance: 1.4690037432851213 entropy 0.8164090514183044
epoch: 34, step: 97
	action: tensor([[-0.8204, -0.7078, -0.2008, -0.9365, -0.4983, -0.6000,  0.1244]],
       dtype=torch.float64)
	q_value: tensor([[-34.8477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5997236080237314, distance: 1.4473686789471734 entropy 0.8164090514183044
epoch: 34, step: 98
	action: tensor([[-0.5249, -0.7188, -0.1050,  0.5675, -1.1448, -0.3915,  0.8177]],
       dtype=torch.float64)
	q_value: tensor([[-32.6917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4965447348815717, distance: 1.3999146065686454 entropy 0.8164090514183044
epoch: 34, step: 99
	action: tensor([[-0.0050,  0.4330,  0.1244, -0.1041,  0.5843,  0.9585,  0.4344]],
       dtype=torch.float64)
	q_value: tensor([[-35.6637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 100
	action: tensor([[-0.2572, -0.0655,  0.7360, -0.5312,  0.5155, -0.2392,  0.5641]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5214840288429015, distance: 1.41153090790641 entropy 0.8164090514183044
epoch: 34, step: 101
	action: tensor([[ 0.3214,  0.1853, -0.5510, -0.7317,  0.7809, -0.2088,  0.7550]],
       dtype=torch.float64)
	q_value: tensor([[-28.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49047979322039625, distance: 0.8168407573820278 entropy 0.8164090514183044
epoch: 34, step: 102
	action: tensor([[ 1.0238, -0.2704,  0.2041, -0.3718, -0.1333, -0.1341,  0.0992]],
       dtype=torch.float64)
	q_value: tensor([[-30.2062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22860973564676612, distance: 1.005064121536444 entropy 0.8164090514183044
epoch: 34, step: 103
	action: tensor([[-0.1403, -0.8269,  0.0926, -0.0674, -0.9845, -0.6243,  0.6577]],
       dtype=torch.float64)
	q_value: tensor([[-28.7930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5220269611561412, distance: 1.4117827335485036 entropy 0.8164090514183044
epoch: 34, step: 104
	action: tensor([[ 1.3261, -0.0646, -0.3347,  0.2772, -0.6448, -0.0671, -0.2258]],
       dtype=torch.float64)
	q_value: tensor([[-33.5361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6675290033103527, distance: 0.6598323071459314 entropy 0.8164090514183044
epoch: 34, step: 105
	action: tensor([[ 0.9015, -0.6159, -0.0150, -0.0789,  0.0689, -0.5698, -1.1823]],
       dtype=torch.float64)
	q_value: tensor([[-31.0695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0908063619095989, distance: 1.1951723144390742 entropy 0.8164090514183044
epoch: 34, step: 106
	action: tensor([[-0.1110, -1.4463,  0.2522, -0.5405, -0.5540,  0.3431,  1.0618]],
       dtype=torch.float64)
	q_value: tensor([[-32.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 107
	action: tensor([[ 0.4512, -0.8425,  0.0762,  0.3391,  0.5142, -1.2072, -0.0784]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24553303580236552, distance: 1.27712768303508 entropy 0.8164090514183044
epoch: 34, step: 108
	action: tensor([[-0.5967,  0.6300, -0.2816, -0.2785, -0.4324,  0.4158,  0.9168]],
       dtype=torch.float64)
	q_value: tensor([[-32.1952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03405737478843207, distance: 1.1636677848719101 entropy 0.8164090514183044
epoch: 34, step: 109
	action: tensor([[ 0.0290, -0.2313, -0.1703, -0.0469,  0.7051,  0.1827, -0.0846]],
       dtype=torch.float64)
	q_value: tensor([[-30.7183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 110
	action: tensor([[ 0.1525,  0.5864, -0.3279, -1.2195, -0.1547,  0.7640, -0.1313]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 111
	action: tensor([[-0.0122, -1.1909,  0.1804, -0.4165,  0.0426,  0.1257,  0.6914]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9878049666456052, distance: 1.6134056703916198 entropy 0.8164090514183044
epoch: 34, step: 112
	action: tensor([[-0.3536, -0.6514, -0.5498, -0.3946,  0.1658,  1.1795, -0.7695]],
       dtype=torch.float64)
	q_value: tensor([[-32.7624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3776530855289253, distance: 1.343156530194733 entropy 0.8164090514183044
epoch: 34, step: 113
	action: tensor([[ 0.9817,  0.7049, -0.8913, -0.7834,  0.9026,  0.8738,  0.8427]],
       dtype=torch.float64)
	q_value: tensor([[-32.8573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8772175968030607, distance: 0.4009818895265478 entropy 0.8164090514183044
epoch: 34, step: 114
	action: tensor([[ 0.5637, -1.2711,  0.1573, -0.1610,  0.2083,  0.6673,  0.5615]],
       dtype=torch.float64)
	q_value: tensor([[-36.3435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4429192408833438, distance: 1.3746043236242218 entropy 0.8164090514183044
epoch: 34, step: 115
	action: tensor([[ 0.3679, -1.3568, -0.1829, -0.0241, -0.1261, -0.9277,  0.7652]],
       dtype=torch.float64)
	q_value: tensor([[-33.6132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 116
	action: tensor([[ 0.0839, -0.6257, -0.0664, -0.2397, -0.1041,  0.2116, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2600535206375296, distance: 1.2845505201595881 entropy 0.8164090514183044
epoch: 34, step: 117
	action: tensor([[ 0.7228, -1.3920, -0.5424, -0.2283,  0.0515, -0.1318, -0.2991]],
       dtype=torch.float64)
	q_value: tensor([[-27.3715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 118
	action: tensor([[ 1.3242, -0.0204, -0.0456, -0.2585, -0.1054,  0.7836,  0.6543]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 119
	action: tensor([[-0.2632, -1.1854, -0.2079, -0.5575,  0.7255, -0.1150,  0.2729]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.948633960636708, distance: 1.597429965201365 entropy 0.8164090514183044
epoch: 34, step: 120
	action: tensor([[-0.1564, -0.0215,  0.1868,  0.4845, -0.2607,  1.0324, -0.4540]],
       dtype=torch.float64)
	q_value: tensor([[-32.9054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 121
	action: tensor([[ 0.1980,  0.0351,  1.6533, -0.1934,  0.7703,  0.7924, -0.1378]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 122
	action: tensor([[-0.2235, -0.5131, -0.3938, -0.6379, -0.0172,  0.2199,  0.3109]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45897426484933446, distance: 1.3822306182632786 entropy 0.8164090514183044
epoch: 34, step: 123
	action: tensor([[ 0.0834, -0.6004,  0.7231,  0.2147,  0.2004,  0.3236, -0.1027]],
       dtype=torch.float64)
	q_value: tensor([[-29.0718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1243521985927657, distance: 1.070832205131623 entropy 0.8164090514183044
epoch: 34, step: 124
	action: tensor([[ 1.0958, -0.5783, -0.0759,  0.5310, -0.7891, -0.3188,  0.1756]],
       dtype=torch.float64)
	q_value: tensor([[-29.1734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44020593342150804, distance: 0.8561913556094412 entropy 0.8164090514183044
epoch: 34, step: 125
	action: tensor([[ 0.1653, -1.3678,  0.6182, -0.3949,  0.0145, -0.2203,  0.6250]],
       dtype=torch.float64)
	q_value: tensor([[-32.7737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0777166284061832, distance: 1.6494906230946011 entropy 0.8164090514183044
epoch: 34, step: 126
	action: tensor([[-0.5782, -1.4986, -0.0494, -0.5762,  0.8847, -0.8871,  0.6651]],
       dtype=torch.float64)
	q_value: tensor([[-32.8581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 34, step: 127
	action: tensor([[ 0.2641,  0.5991, -0.9053, -0.2907,  0.3830, -0.5410, -0.2309]],
       dtype=torch.float64)
	q_value: tensor([[-33.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
LOSS epoch 34 actor 407.12498427629936 critic 163.70107604686265 
epoch: 35, step: 0
	action: tensor([[ 0.9656, -1.2424, -0.0661, -0.1348,  0.9087, -0.8440,  0.2414]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7473794635409285, distance: 1.5126912922411717 entropy 0.7110485434532166
epoch: 35, step: 1
	action: tensor([[ 0.3718, -1.1899,  0.2044,  0.6829, -0.3754,  0.1245,  0.4217]],
       dtype=torch.float64)
	q_value: tensor([[-31.2151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006542647949519864, distance: 1.140594589958807 entropy 0.7110485434532166
epoch: 35, step: 2
	action: tensor([[ 0.6881, -1.1510,  0.0102, -0.1906, -0.0344, -0.2037,  0.5438]],
       dtype=torch.float64)
	q_value: tensor([[-29.8013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7537970394823312, distance: 1.515466566399267 entropy 0.7110485434532166
epoch: 35, step: 3
	action: tensor([[ 1.0100, -0.0292,  0.4226, -0.5076,  0.3455,  0.2130, -0.0673]],
       dtype=torch.float64)
	q_value: tensor([[-29.2752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49608749531799345, distance: 0.8123333082665255 entropy 0.7110485434532166
epoch: 35, step: 4
	action: tensor([[ 0.9155, -0.9126, -0.0620,  0.7557,  0.3469,  0.3425,  0.1799]],
       dtype=torch.float64)
	q_value: tensor([[-25.6860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5038211871912114, distance: 0.8060756480433728 entropy 0.7110485434532166
epoch: 35, step: 5
	action: tensor([[ 0.3558, -0.2619,  0.9490,  0.5026,  0.7788,  0.4605, -1.1313]],
       dtype=torch.float64)
	q_value: tensor([[-28.2081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7055643036291954, distance: 0.6209432692474119 entropy 0.7110485434532166
epoch: 35, step: 6
	action: tensor([[ 0.2154, -0.9551,  0.5096, -0.1136,  0.7433, -0.0400, -0.0681]],
       dtype=torch.float64)
	q_value: tensor([[-31.1694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5474707034394279, distance: 1.4235342188039286 entropy 0.7110485434532166
epoch: 35, step: 7
	action: tensor([[ 0.2693, -0.0357, -0.5357, -0.4293, -0.2118,  0.6722,  0.9284]],
       dtype=torch.float64)
	q_value: tensor([[-26.8613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4715842507080228, distance: 0.8318491359729017 entropy 0.7110485434532166
epoch: 35, step: 8
	action: tensor([[ 0.9207, -0.9367,  0.1018, -0.8945,  0.0165,  0.3512,  0.1584]],
       dtype=torch.float64)
	q_value: tensor([[-28.4946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7029626285640209, distance: 1.493341904708356 entropy 0.7110485434532166
epoch: 35, step: 9
	action: tensor([[ 0.7783, -0.9102, -0.3666, -0.1664, -0.5291, -0.1825,  1.7395]],
       dtype=torch.float64)
	q_value: tensor([[-29.7229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3838170394952167, distance: 1.3461579806438204 entropy 0.7110485434532166
epoch: 35, step: 10
	action: tensor([[ 0.0091, -1.4136,  0.7236,  0.1201,  0.0224,  0.2381,  0.4629]],
       dtype=torch.float64)
	q_value: tensor([[-35.4574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 11
	action: tensor([[ 0.3482, -0.6577, -0.4951, -0.4915,  0.1269, -0.1545,  0.4692]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26896515068539206, distance: 1.2890849585012651 entropy 0.7110485434532166
epoch: 35, step: 12
	action: tensor([[ 0.4565, -0.0160, -0.4032, -0.9340,  0.8466, -0.5548,  0.5829]],
       dtype=torch.float64)
	q_value: tensor([[-27.0535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12447085445082473, distance: 1.0707596503792431 entropy 0.7110485434532166
epoch: 35, step: 13
	action: tensor([[-0.2135, -1.0006,  0.5224, -0.4241,  0.8477, -0.1998,  1.2716]],
       dtype=torch.float64)
	q_value: tensor([[-28.3774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0681351314035874, distance: 1.6456828727260913 entropy 0.7110485434532166
epoch: 35, step: 14
	action: tensor([[ 0.6445, -0.8381, -0.1652,  0.6680,  1.1856,  0.7074,  1.1068]],
       dtype=torch.float64)
	q_value: tensor([[-29.9906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5106409098509066, distance: 0.800516933880555 entropy 0.7110485434532166
epoch: 35, step: 15
	action: tensor([[ 0.9922, -0.3250,  0.2595, -0.5373,  0.2346, -0.0538,  0.2611]],
       dtype=torch.float64)
	q_value: tensor([[-31.7411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11406748869786787, distance: 1.077102448070321 entropy 0.7110485434532166
epoch: 35, step: 16
	action: tensor([[ 1.0432, -0.0159,  0.0588,  0.1413, -0.4505,  0.2635,  0.4151]],
       dtype=torch.float64)
	q_value: tensor([[-25.9909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.829583296853387, distance: 0.4724031370121235 entropy 0.7110485434532166
epoch: 35, step: 17
	action: tensor([[ 0.8736, -0.5909, -0.4274, -0.7555,  0.5286, -0.3641,  0.7802]],
       dtype=torch.float64)
	q_value: tensor([[-26.6033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38502872900795637, distance: 1.3467472090455046 entropy 0.7110485434532166
epoch: 35, step: 18
	action: tensor([[ 0.1335, -1.5237,  1.1396, -0.2295, -1.3116, -0.2520,  0.4835]],
       dtype=torch.float64)
	q_value: tensor([[-29.6602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 19
	action: tensor([[ 0.8028, -0.8810, -0.1116, -0.3666,  0.3356,  0.7910,  0.9462]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09168892077876878, distance: 1.1956557167599389 entropy 0.7110485434532166
epoch: 35, step: 20
	action: tensor([[ 0.4288, -0.5840,  0.7538,  0.0196, -0.0279, -0.2909,  0.7326]],
       dtype=torch.float64)
	q_value: tensor([[-31.1908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006763476921253453, distance: 1.1404678153526921 entropy 0.7110485434532166
epoch: 35, step: 21
	action: tensor([[ 0.8089, -0.9123, -0.1724,  0.1164,  1.2002,  0.0797,  0.7784]],
       dtype=torch.float64)
	q_value: tensor([[-26.6662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.101238448292295, distance: 1.20087381781261 entropy 0.7110485434532166
epoch: 35, step: 22
	action: tensor([[ 0.6424, -0.9699,  0.8516,  0.4038,  0.5245,  0.4083,  1.2964]],
       dtype=torch.float64)
	q_value: tensor([[-30.1166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03634579595789755, distance: 1.1233557263200038 entropy 0.7110485434532166
epoch: 35, step: 23
	action: tensor([[ 1.6304, -0.8104,  0.0372, -0.9434, -0.6892,  0.2657,  1.0933]],
       dtype=torch.float64)
	q_value: tensor([[-30.9096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6498463860476784, distance: 1.4698683825844485 entropy 0.7110485434532166
epoch: 35, step: 24
	action: tensor([[ 1.8898, -1.2287,  0.1563, -0.4049,  0.5015, -0.4682,  0.8765]],
       dtype=torch.float64)
	q_value: tensor([[-35.9449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 25
	action: tensor([[ 0.2851, -1.0607,  0.6445, -0.5447,  0.2130, -0.3362, -0.5861]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9408367066745429, distance: 1.594230787654688 entropy 0.7110485434532166
epoch: 35, step: 26
	action: tensor([[ 0.6429, -0.6305, -0.8007, -0.5129,  0.3277, -0.3440, -0.2347]],
       dtype=torch.float64)
	q_value: tensor([[-28.5536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20390709663304296, distance: 1.2556054164882107 entropy 0.7110485434532166
epoch: 35, step: 27
	action: tensor([[ 0.4459,  0.5724,  0.2578, -0.1106,  0.3358, -0.4115,  0.6246]],
       dtype=torch.float64)
	q_value: tensor([[-27.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 28
	action: tensor([[-0.0966, -0.5578,  0.5199,  0.1752, -0.0648, -0.9024,  0.3338]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3782397378551825, distance: 1.3434424810060497 entropy 0.7110485434532166
epoch: 35, step: 29
	action: tensor([[ 0.1191, -0.5533, -0.6815, -0.8682,  0.9190, -0.4435,  0.6766]],
       dtype=torch.float64)
	q_value: tensor([[-27.1028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28244481327880067, distance: 1.2959135653407876 entropy 0.7110485434532166
epoch: 35, step: 30
	action: tensor([[-0.6634, -0.4629,  0.0445, -0.1719, -0.8907,  0.3864,  0.1774]],
       dtype=torch.float64)
	q_value: tensor([[-29.8406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.916521793884902, distance: 1.5842130058808832 entropy 0.7110485434532166
epoch: 35, step: 31
	action: tensor([[ 0.2862, -0.8730,  0.4827, -0.5929,  0.0536, -0.3805, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-28.9036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7945008365963044, distance: 1.532951892197427 entropy 0.7110485434532166
epoch: 35, step: 32
	action: tensor([[ 0.7552, -1.0600,  0.4196, -0.4038, -0.3016,  0.1919,  0.2365]],
       dtype=torch.float64)
	q_value: tensor([[-27.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6424025175589057, distance: 1.4665487170163298 entropy 0.7110485434532166
epoch: 35, step: 33
	action: tensor([[ 0.7955, -1.0215, -0.5038,  0.3252,  1.1608,  0.9975,  0.7728]],
       dtype=torch.float64)
	q_value: tensor([[-29.1406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21524206353739828, distance: 1.013735257746733 entropy 0.7110485434532166
epoch: 35, step: 34
	action: tensor([[ 0.0448, -0.9442, -0.4119,  0.1288,  0.0142,  0.8280,  0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-33.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20836205130921948, distance: 1.257926401251149 entropy 0.7110485434532166
epoch: 35, step: 35
	action: tensor([[ 1.3800, -0.3563, -1.0326,  0.6380,  0.5451,  0.0644,  0.5018]],
       dtype=torch.float64)
	q_value: tensor([[-27.8368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6657729146435771, distance: 0.6615726063009357 entropy 0.7110485434532166
epoch: 35, step: 36
	action: tensor([[ 1.5429, -0.5300, -0.6128, -0.5595, -0.0317,  0.1416,  0.0727]],
       dtype=torch.float64)
	q_value: tensor([[-30.1183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.418425619565981, distance: 1.3628874007625174 entropy 0.7110485434532166
epoch: 35, step: 37
	action: tensor([[-0.1944, -0.7375,  0.6646,  0.1079,  0.5970, -0.5526,  0.5867]],
       dtype=torch.float64)
	q_value: tensor([[-30.3784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6267489463848508, distance: 1.4595432216342217 entropy 0.7110485434532166
epoch: 35, step: 38
	action: tensor([[0.4248, 0.2116, 0.0820, 0.9795, 0.5027, 0.4690, 0.2093]],
       dtype=torch.float64)
	q_value: tensor([[-26.8264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 39
	action: tensor([[-0.0059, -0.4327,  0.0845, -1.2112, -0.3221,  0.6045,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5037035540821568, distance: 1.4032589032692606 entropy 0.7110485434532166
epoch: 35, step: 40
	action: tensor([[ 0.6474, -0.6849,  1.3478,  0.4238,  0.3389, -0.5278,  0.5547]],
       dtype=torch.float64)
	q_value: tensor([[-28.8514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13530711952190844, distance: 1.0641127206145293 entropy 0.7110485434532166
epoch: 35, step: 41
	action: tensor([[ 0.7278, -0.7579, -0.4924, -0.5092,  0.4233, -0.1762,  1.1587]],
       dtype=torch.float64)
	q_value: tensor([[-29.5124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3933850448407823, distance: 1.3508037752754705 entropy 0.7110485434532166
epoch: 35, step: 42
	action: tensor([[-0.1671, -0.3250, -0.3568,  0.3910, -0.1873, -0.1457,  0.3088]],
       dtype=torch.float64)
	q_value: tensor([[-30.8483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0034942072231542998, distance: 1.1463417985420636 entropy 0.7110485434532166
epoch: 35, step: 43
	action: tensor([[ 0.3650, -0.3959, -0.6846,  0.2755,  0.9047,  0.0588,  0.5796]],
       dtype=torch.float64)
	q_value: tensor([[-24.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34309868361924434, distance: 0.9274847111608882 entropy 0.7110485434532166
epoch: 35, step: 44
	action: tensor([[ 1.1837, -0.7627, -0.0382, -0.3698,  0.3806,  0.0231, -0.1449]],
       dtype=torch.float64)
	q_value: tensor([[-26.7534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.364869872784779, distance: 1.336910447127283 entropy 0.7110485434532166
epoch: 35, step: 45
	action: tensor([[ 0.5724, -0.2348, -0.3293, -0.1969, -0.0629,  0.0170,  0.7414]],
       dtype=torch.float64)
	q_value: tensor([[-27.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39560021136582224, distance: 0.8896493077637039 entropy 0.7110485434532166
epoch: 35, step: 46
	action: tensor([[ 0.2938,  0.3890, -0.2147, -0.1419,  0.7155,  0.2364,  0.1531]],
       dtype=torch.float64)
	q_value: tensor([[-26.3547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 47
	action: tensor([[ 0.4659, -0.6081,  0.9830, -0.6111,  1.2408, -0.4230,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.207207369556071, distance: 1.2573252354741846 entropy 0.7110485434532166
epoch: 35, step: 48
	action: tensor([[ 0.8190,  0.5206, -0.3287,  0.3386,  0.7781,  0.4253,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-29.1502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9708171627191784, distance: 0.19548815601924277 entropy 0.7110485434532166
epoch: 35, step: 49
	action: tensor([[-0.0629, -0.4436, -0.1106, -0.0779,  0.4598,  0.1982, -0.7220]],
       dtype=torch.float64)
	q_value: tensor([[-26.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05706972917465447, distance: 1.1765449156999426 entropy 0.7110485434532166
epoch: 35, step: 50
	action: tensor([[ 0.5118, -0.5319,  0.0172, -0.0079,  0.6810, -0.7059,  0.3940]],
       dtype=torch.float64)
	q_value: tensor([[-24.9439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025206323430986055, distance: 1.1586768537396068 entropy 0.7110485434532166
epoch: 35, step: 51
	action: tensor([[ 0.3498, -0.0463,  0.1134, -0.0270, -0.3061,  0.4359,  0.5045]],
       dtype=torch.float64)
	q_value: tensor([[-25.8763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.64067907952188, distance: 0.6859586627646469 entropy 0.7110485434532166
epoch: 35, step: 52
	action: tensor([[-0.1761,  0.6133,  0.2133,  0.1194,  0.3256, -0.3864,  1.1337]],
       dtype=torch.float64)
	q_value: tensor([[-25.0703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 53
	action: tensor([[ 0.3718, -0.4083,  0.6338, -0.6951,  0.4385, -0.2340,  0.3479]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31081188206216814, distance: 1.3101676685594426 entropy 0.7110485434532166
epoch: 35, step: 54
	action: tensor([[ 0.4551, -1.3062, -0.3796,  0.7259, -0.0083, -0.0137,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[-25.9412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14370628513143235, distance: 1.2238098578536545 entropy 0.7110485434532166
epoch: 35, step: 55
	action: tensor([[ 0.5733, -0.7508,  0.3259, -0.8979, -0.4859,  0.3815, -0.3662]],
       dtype=torch.float64)
	q_value: tensor([[-28.8698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5323662562667528, distance: 1.41656981454331 entropy 0.7110485434532166
epoch: 35, step: 56
	action: tensor([[ 0.1913,  0.8602,  0.0199, -0.4578,  0.7923,  0.9477,  1.3737]],
       dtype=torch.float64)
	q_value: tensor([[-28.8488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 57
	action: tensor([[ 0.1697, -0.5558,  1.3474, -0.1879,  0.2069,  0.0907,  0.5061]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11857643979346633, distance: 1.2102902289663853 entropy 0.7110485434532166
epoch: 35, step: 58
	action: tensor([[ 0.5316, -0.8675, -0.2794,  0.4295,  0.5981, -0.4161,  0.1740]],
       dtype=torch.float64)
	q_value: tensor([[-27.5299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07285759810326553, distance: 1.1852984975630814 entropy 0.7110485434532166
epoch: 35, step: 59
	action: tensor([[ 0.3386, -0.4768,  0.5077,  0.9906, -0.3412, -0.1170, -0.3154]],
       dtype=torch.float64)
	q_value: tensor([[-27.1364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7740215584830665, distance: 0.5439891001203424 entropy 0.7110485434532166
epoch: 35, step: 60
	action: tensor([[ 1.2742, -0.6386,  1.0011,  0.6505, -0.2023, -0.7051,  0.9849]],
       dtype=torch.float64)
	q_value: tensor([[-29.1129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04685906733780387, distance: 1.1172111300313892 entropy 0.7110485434532166
epoch: 35, step: 61
	action: tensor([[ 1.3610, -0.4982,  0.1919, -0.2802, -0.1692, -0.2695,  0.2630]],
       dtype=torch.float64)
	q_value: tensor([[-31.5700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11424138479849044, distance: 1.2079427059112195 entropy 0.7110485434532166
epoch: 35, step: 62
	action: tensor([[ 1.0710, -0.3068,  0.2376, -0.1459, -0.1989,  0.1536,  1.6954]],
       dtype=torch.float64)
	q_value: tensor([[-28.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.384417496127919, distance: 0.8978418135953615 entropy 0.7110485434532166
epoch: 35, step: 63
	action: tensor([[ 0.6971, -0.1125, -0.0342,  0.3740,  0.7021, -0.1028,  0.6122]],
       dtype=torch.float64)
	q_value: tensor([[-33.1602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7906018552608025, distance: 0.5236524116988962 entropy 0.7110485434532166
epoch: 35, step: 64
	action: tensor([[ 0.2691, -0.5043, -1.0780, -0.5846,  0.0329, -0.1759,  0.3389]],
       dtype=torch.float64)
	q_value: tensor([[-25.2994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08402213694041583, distance: 1.095214493144349 entropy 0.7110485434532166
epoch: 35, step: 65
	action: tensor([[ 0.4007, -0.7007,  1.2997, -0.3686, -0.1008,  1.1672,  0.2552]],
       dtype=torch.float64)
	q_value: tensor([[-28.0707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1701925432582525, distance: 1.0424262997960012 entropy 0.7110485434532166
epoch: 35, step: 66
	action: tensor([[ 0.2725,  0.1454,  0.6799, -0.5211,  1.1160, -1.0381, -0.2357]],
       dtype=torch.float64)
	q_value: tensor([[-30.9755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2607128444321566, distance: 0.9839278958852004 entropy 0.7110485434532166
epoch: 35, step: 67
	action: tensor([[ 0.0056, -0.2146,  0.4356,  0.3723,  0.3995, -0.4568,  0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-28.5161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24279175612890902, distance: 0.9957822028175884 entropy 0.7110485434532166
epoch: 35, step: 68
	action: tensor([[ 0.4953,  0.0310, -0.3605,  0.0091, -0.1214, -0.2363,  0.8850]],
       dtype=torch.float64)
	q_value: tensor([[-24.5429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6554290919619898, distance: 0.6717319369355729 entropy 0.7110485434532166
epoch: 35, step: 69
	action: tensor([[ 0.1991, -0.6852, -0.2316,  0.2237,  1.0581, -0.7713, -0.3140]],
       dtype=torch.float64)
	q_value: tensor([[-26.5672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26881207224994763, distance: 1.2890072033871902 entropy 0.7110485434532166
epoch: 35, step: 70
	action: tensor([[ 0.5405, -1.5391, -0.0708, -0.5449,  0.1105,  0.9461,  0.3595]],
       dtype=torch.float64)
	q_value: tensor([[-28.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 71
	action: tensor([[-0.2979, -1.1984, -0.1416, -0.0297,  0.0796,  0.8210,  1.0990]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6777269124085383, distance: 1.482235903679789 entropy 0.7110485434532166
epoch: 35, step: 72
	action: tensor([[ 1.0394, -0.6673,  0.1106, -0.1600,  0.3083, -0.1331,  0.3097]],
       dtype=torch.float64)
	q_value: tensor([[-31.4291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09390810427720186, distance: 1.1968703633082418 entropy 0.7110485434532166
epoch: 35, step: 73
	action: tensor([[ 1.1882, -0.7452,  0.3654, -0.5864,  0.2407, -0.6616,  0.7040]],
       dtype=torch.float64)
	q_value: tensor([[-26.3622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3956090858794481, distance: 1.3518813830761005 entropy 0.7110485434532166
epoch: 35, step: 74
	action: tensor([[ 1.0713, -0.8494,  0.7811, -0.3760,  0.2192, -0.2178, -0.2159]],
       dtype=torch.float64)
	q_value: tensor([[-29.3382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34227341220666063, distance: 1.3257974737325398 entropy 0.7110485434532166
epoch: 35, step: 75
	action: tensor([[ 0.0998, -0.1021,  0.2591, -0.5238,  0.6040,  0.5435,  0.3758]],
       dtype=torch.float64)
	q_value: tensor([[-27.7496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17956869811293474, distance: 1.0365202809580643 entropy 0.7110485434532166
epoch: 35, step: 76
	action: tensor([[ 0.0725, -0.5446,  0.3117, -1.4310, -0.3770,  0.5898,  0.4481]],
       dtype=torch.float64)
	q_value: tensor([[-25.5632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6843946628831703, distance: 1.485178390600768 entropy 0.7110485434532166
epoch: 35, step: 77
	action: tensor([[ 0.7146,  0.1995, -0.3633, -0.2194,  0.3688,  0.3046, -0.3767]],
       dtype=torch.float64)
	q_value: tensor([[-31.1201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8436626201793892, distance: 0.4524682478081402 entropy 0.7110485434532166
epoch: 35, step: 78
	action: tensor([[ 0.4605, -0.9287, -0.2234, -0.0690, -0.3241,  0.6115,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[-24.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1607138607199159, distance: 1.2328756598746093 entropy 0.7110485434532166
epoch: 35, step: 79
	action: tensor([[ 0.5653, -0.9192, -0.4420,  0.2615,  1.0482, -0.5262,  0.6231]],
       dtype=torch.float64)
	q_value: tensor([[-28.3196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2759546370206487, distance: 1.292630236665513 entropy 0.7110485434532166
epoch: 35, step: 80
	action: tensor([[ 0.4507, -0.8669,  0.4736, -0.0396, -0.1059,  0.3902, -0.5449]],
       dtype=torch.float64)
	q_value: tensor([[-29.2588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13954393014062672, distance: 1.2215808877733896 entropy 0.7110485434532166
epoch: 35, step: 81
	action: tensor([[ 0.2253, -0.4226, -0.3370, -0.4712,  0.0316, -0.5049,  0.8332]],
       dtype=torch.float64)
	q_value: tensor([[-27.4684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16455365455641102, distance: 1.234913233396755 entropy 0.7110485434532166
epoch: 35, step: 82
	action: tensor([[ 0.8352, -0.1165,  0.5661, -0.0744,  0.1956,  0.0030, -0.0967]],
       dtype=torch.float64)
	q_value: tensor([[-27.4869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6077478900277434, distance: 0.7167031899563217 entropy 0.7110485434532166
epoch: 35, step: 83
	action: tensor([[-0.6471, -0.9552, -0.0896, -0.6812,  0.3764,  0.5176,  0.7494]],
       dtype=torch.float64)
	q_value: tensor([[-24.8727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9651823537914508, distance: 1.604198556271072 entropy 0.7110485434532166
epoch: 35, step: 84
	action: tensor([[ 0.8672, -0.7499,  0.5060, -0.2574, -0.3776,  1.2492,  0.7618]],
       dtype=torch.float64)
	q_value: tensor([[-30.0384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3388757391140803, distance: 0.9304611413987804 entropy 0.7110485434532166
epoch: 35, step: 85
	action: tensor([[ 1.7574, -0.8844,  0.3019, -0.2502, -0.3748, -0.2925,  0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-31.7606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6077517201071407, distance: 1.4509958981375803 entropy 0.7110485434532166
epoch: 35, step: 86
	action: tensor([[ 0.5108, -0.1091, -0.7152,  0.1608,  0.1838, -0.3605,  0.2741]],
       dtype=torch.float64)
	q_value: tensor([[-30.9717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5974496398796652, distance: 0.7260504568341004 entropy 0.7110485434532166
epoch: 35, step: 87
	action: tensor([[ 0.0958, -0.2766,  0.6812, -0.7055, -0.2626,  0.3856,  0.1326]],
       dtype=torch.float64)
	q_value: tensor([[-24.7730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18702516342049025, distance: 1.24677089678139 entropy 0.7110485434532166
epoch: 35, step: 88
	action: tensor([[ 0.5718, -0.5713, -0.1506,  0.9752,  0.9047,  0.6206,  0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-26.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8818275474371504, distance: 0.3933823023303769 entropy 0.7110485434532166
epoch: 35, step: 89
	action: tensor([[ 0.2880, -0.8906,  0.8823, -0.7599,  0.3118, -0.3491,  1.3251]],
       dtype=torch.float64)
	q_value: tensor([[-27.9662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7726309974884993, distance: 1.5235821046460083 entropy 0.7110485434532166
epoch: 35, step: 90
	action: tensor([[ 0.6920,  0.3418,  0.5060, -0.7584, -0.1748, -0.1537,  0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-30.5980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.600141973003593, distance: 0.723618401950909 entropy 0.7110485434532166
epoch: 35, step: 91
	action: tensor([[ 0.9808, -0.9387, -0.4938,  0.0173, -0.4471,  0.0536, -0.3270]],
       dtype=torch.float64)
	q_value: tensor([[-25.9788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2750003445406233, distance: 1.2921467641310052 entropy 0.7110485434532166
epoch: 35, step: 92
	action: tensor([[-0.0437, -0.7948,  0.4074,  0.2218, -0.2159, -0.6410, -0.2817]],
       dtype=torch.float64)
	q_value: tensor([[-29.0977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4109552549508244, distance: 1.359293730990721 entropy 0.7110485434532166
epoch: 35, step: 93
	action: tensor([[ 1.4026, -0.9233,  0.5208, -0.3397,  0.4331, -0.5570,  1.4088]],
       dtype=torch.float64)
	q_value: tensor([[-27.3302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42431921494382774, distance: 1.3657158820212434 entropy 0.7110485434532166
epoch: 35, step: 94
	action: tensor([[ 0.8159, -1.1047, -0.1858,  0.1442,  0.5621,  0.5530,  0.4044]],
       dtype=torch.float64)
	q_value: tensor([[-32.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14911447673074685, distance: 1.2266999321923806 entropy 0.7110485434532166
epoch: 35, step: 95
	action: tensor([[ 0.7902, -0.3252,  0.7492,  1.0464, -0.2685,  0.8059,  0.0776]],
       dtype=torch.float64)
	q_value: tensor([[-29.2625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8663807459912788, distance: 0.4183032267103285 entropy 0.7110485434532166
epoch: 35, step: 96
	action: tensor([[ 0.6813, -0.9327, -0.1867, -0.0321,  0.1438,  0.9756, -0.3804]],
       dtype=torch.float64)
	q_value: tensor([[-29.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17812613901519703, distance: 1.0374311342291114 entropy 0.7110485434532166
epoch: 35, step: 97
	action: tensor([[ 1.0726, -0.8088,  0.1548,  0.2765,  0.1480,  0.5088,  0.4946]],
       dtype=torch.float64)
	q_value: tensor([[-29.3061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28917190446260577, distance: 0.964803756559864 entropy 0.7110485434532166
epoch: 35, step: 98
	action: tensor([[ 0.3428, -0.7808, -0.2501,  0.7081,  0.6609,  0.2010,  0.1307]],
       dtype=torch.float64)
	q_value: tensor([[-28.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3662436010675929, distance: 0.9109989406390164 entropy 0.7110485434532166
epoch: 35, step: 99
	action: tensor([[ 0.7963, -0.7179, -0.0225, -0.2011,  0.2438,  0.2933,  0.8017]],
       dtype=torch.float64)
	q_value: tensor([[-26.2243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0132470650640657, distance: 1.1518989184251256 entropy 0.7110485434532166
epoch: 35, step: 100
	action: tensor([[ 0.4069, -0.5843,  0.4870,  0.0227, -0.3096,  1.2562,  1.2717]],
       dtype=torch.float64)
	q_value: tensor([[-28.2352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5538560715011882, distance: 0.7643533737429501 entropy 0.7110485434532166
epoch: 35, step: 101
	action: tensor([[ 1.6212, -1.2187, -0.3071, -0.8240,  0.4681,  0.8129,  1.5169]],
       dtype=torch.float64)
	q_value: tensor([[-32.4606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9413901637684219, distance: 1.5944580802037778 entropy 0.7110485434532166
epoch: 35, step: 102
	action: tensor([[ 0.6858, -0.6213,  0.3385, -0.7429,  0.3254, -1.0083,  0.5318]],
       dtype=torch.float64)
	q_value: tensor([[-39.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3575893097306606, distance: 1.3333399691180094 entropy 0.7110485434532166
epoch: 35, step: 103
	action: tensor([[-0.2704, -0.8768,  0.4837,  0.6236, -0.1186, -0.3613,  0.8332]],
       dtype=torch.float64)
	q_value: tensor([[-28.8657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2940250989982418, distance: 1.3017513696042449 entropy 0.7110485434532166
epoch: 35, step: 104
	action: tensor([[ 0.5252, -0.2495,  0.1224,  0.0625,  0.8759, -0.2179,  0.7996]],
       dtype=torch.float64)
	q_value: tensor([[-28.6565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4392034678429295, distance: 0.8569576360157805 entropy 0.7110485434532166
epoch: 35, step: 105
	action: tensor([[-0.6311, -0.7769,  1.8382, -0.9646,  0.0441, -0.8044,  0.0785]],
       dtype=torch.float64)
	q_value: tensor([[-25.7285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9168814872822364, distance: 1.5843616616973737 entropy 0.7110485434532166
epoch: 35, step: 106
	action: tensor([[ 0.4554, -0.7257,  0.2462,  0.0760,  0.6364, -0.2193,  0.9695]],
       dtype=torch.float64)
	q_value: tensor([[-33.0781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11390305444018312, distance: 1.207759300985606 entropy 0.7110485434532166
epoch: 35, step: 107
	action: tensor([[ 0.3707, -0.7584, -0.7176,  0.5865,  0.5030,  0.0810,  0.4209]],
       dtype=torch.float64)
	q_value: tensor([[-27.3988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11517555479803643, distance: 1.076428652879667 entropy 0.7110485434532166
epoch: 35, step: 108
	action: tensor([[ 0.1044, -0.8976,  1.0534, -0.6626, -0.6927,  0.2018,  0.3156]],
       dtype=torch.float64)
	q_value: tensor([[-26.9557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7697554233956108, distance: 1.5223458204250915 entropy 0.7110485434532166
epoch: 35, step: 109
	action: tensor([[ 0.3284, -0.7420,  0.2893, -0.1686,  0.1837,  0.7643,  0.3362]],
       dtype=torch.float64)
	q_value: tensor([[-29.9208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08104064658059884, distance: 1.0969954961140895 entropy 0.7110485434532166
epoch: 35, step: 110
	action: tensor([[ 0.2949, -0.1132,  0.1601, -0.4861,  0.3820,  0.7303,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-26.7718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4509437132470361, distance: 0.8479400093661829 entropy 0.7110485434532166
epoch: 35, step: 111
	action: tensor([[ 0.8336, -0.6120,  0.4098,  0.3068, -0.0609, -0.1449, -0.5664]],
       dtype=torch.float64)
	q_value: tensor([[-25.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2875335688198296, distance: 0.9659149694161506 entropy 0.7110485434532166
epoch: 35, step: 112
	action: tensor([[ 0.8941, -0.3608,  0.3805, -0.2929, -0.0262, -0.1756,  0.1736]],
       dtype=torch.float64)
	q_value: tensor([[-27.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1870461227661092, distance: 1.031786037656662 entropy 0.7110485434532166
epoch: 35, step: 113
	action: tensor([[ 0.5321,  0.0415,  0.3541,  0.3754,  0.5971,  0.6320, -0.1062]],
       dtype=torch.float64)
	q_value: tensor([[-25.3436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.955220856319779, distance: 0.24215563822458086 entropy 0.7110485434532166
epoch: 35, step: 114
	action: tensor([[ 0.5783, -0.8923,  0.6639, -0.1637, -0.6618,  0.2374,  0.4763]],
       dtype=torch.float64)
	q_value: tensor([[-25.5555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2559984131728028, distance: 1.2824818826090922 entropy 0.7110485434532166
epoch: 35, step: 115
	action: tensor([[ 0.7312, -0.1981, -0.2923, -0.1358,  0.2995, -0.4104,  1.3151]],
       dtype=torch.float64)
	q_value: tensor([[-29.4147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39892923925056323, distance: 0.8871958350064775 entropy 0.7110485434532166
epoch: 35, step: 116
	action: tensor([[-0.4248, -1.0740, -0.0670, -0.7275,  0.1140, -0.4339,  0.5371]],
       dtype=torch.float64)
	q_value: tensor([[-29.4883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9236601568821974, distance: 1.5871605791594814 entropy 0.7110485434532166
epoch: 35, step: 117
	action: tensor([[ 0.3100, -0.5631, -1.1347,  0.0726, -0.6893,  0.3406,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[-29.6696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05592187782237601, distance: 1.1759059486316248 entropy 0.7110485434532166
epoch: 35, step: 118
	action: tensor([[ 0.4092, -0.6122, -0.3441, -0.2892,  0.7989, -0.0403, -0.2560]],
       dtype=torch.float64)
	q_value: tensor([[-28.2525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14567567857607666, distance: 1.2248630681473338 entropy 0.7110485434532166
epoch: 35, step: 119
	action: tensor([[ 0.8911, -0.5736,  0.0257,  0.3786,  0.7417,  0.0268,  0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-26.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4725912313793582, distance: 0.8310561472560951 entropy 0.7110485434532166
epoch: 35, step: 120
	action: tensor([[ 1.2254, -0.4583, -0.0073, -0.4212,  0.1867,  0.2875, -0.1182]],
       dtype=torch.float64)
	q_value: tensor([[-26.6322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023155705168067642, distance: 1.1310176060944481 entropy 0.7110485434532166
epoch: 35, step: 121
	action: tensor([[ 0.1045,  0.3973, -0.0591, -0.2886,  0.2711,  0.4413,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-27.0637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 122
	action: tensor([[-0.5766, -0.9878,  0.5033, -0.3711,  0.4646,  0.8587,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[-31.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8231491553995058, distance: 1.5451398507198755 entropy 0.7110485434532166
epoch: 35, step: 123
	action: tensor([[ 0.5569,  0.0480, -0.0811,  0.0285, -0.5320,  0.8400,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-29.6358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8235061716000657, distance: 0.48075239348493304 entropy 0.7110485434532166
epoch: 35, step: 124
	action: tensor([[ 0.2454, -0.4611,  0.7115, -0.0986,  0.0382,  0.7365,  1.2903]],
       dtype=torch.float64)
	q_value: tensor([[-26.9000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.362614269071294, distance: 0.9136037250196718 entropy 0.7110485434532166
epoch: 35, step: 125
	action: tensor([[ 0.8257, -0.6845, -0.7454, -0.2859,  0.8203,  0.2880,  0.9642]],
       dtype=torch.float64)
	q_value: tensor([[-29.9794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026461387299736794, distance: 1.1291022787872826 entropy 0.7110485434532166
epoch: 35, step: 126
	action: tensor([[ 0.7688, -0.0437, -0.0931, -0.4161,  1.1683,  0.5258,  0.5040]],
       dtype=torch.float64)
	q_value: tensor([[-31.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5969775558637231, distance: 0.7264760636869627 entropy 0.7110485434532166
epoch: 35, step: 127
	action: tensor([[ 0.2606, -1.0264,  0.5728,  0.7372, -0.0802, -0.1283,  0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-27.7920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10003423869995387, distance: 1.085599630667326 entropy 0.7110485434532166
LOSS epoch 35 actor 366.0268042923505 critic 175.83786730349078 
epoch: 36, step: 0
	action: tensor([[ 0.3675, -0.1231, -0.3849, -0.2649,  0.4616, -0.1182, -0.0736]],
       dtype=torch.float64)
	q_value: tensor([[-25.8110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34996486968994556, distance: 0.9226247642226784 entropy 0.7110485434532166
epoch: 36, step: 1
	action: tensor([[ 0.6340, -0.1558, -0.0243,  0.3023,  0.5150, -0.5064, -0.2619]],
       dtype=torch.float64)
	q_value: tensor([[-21.2668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6249661601213743, distance: 0.7007964957810838 entropy 0.7110485434532166
epoch: 36, step: 2
	action: tensor([[ 0.9288, -0.7645,  0.5331, -0.5959,  0.8624, -0.7205,  0.7268]],
       dtype=torch.float64)
	q_value: tensor([[-21.9957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27322754101559754, distance: 1.2912481294064742 entropy 0.7110485434532166
epoch: 36, step: 3
	action: tensor([[ 0.6966, -1.1580, -0.1313,  1.1634, -0.4500,  0.4568,  0.1655]],
       dtype=torch.float64)
	q_value: tensor([[-25.8621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.589301748479974, distance: 0.733361522951558 entropy 0.7110485434532166
epoch: 36, step: 4
	action: tensor([[-0.0217, -1.4702,  0.1469,  0.1427,  0.1986,  0.1918,  0.8331]],
       dtype=torch.float64)
	q_value: tensor([[-28.3624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 36, step: 5
	action: tensor([[ 0.7360, -0.9942,  0.5711,  0.6969, -0.1637, -0.6951, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05045253164539587, distance: 1.1728565816812526 entropy 0.7110485434532166
epoch: 36, step: 6
	action: tensor([[ 0.1990, -0.5594, -0.0073, -0.0809,  0.5174, -0.1337,  0.6481]],
       dtype=torch.float64)
	q_value: tensor([[-26.6879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1586605983268332, distance: 1.2317847201512326 entropy 0.7110485434532166
epoch: 36, step: 7
	action: tensor([[ 0.1098, -0.8399, -0.0819,  0.9005, -0.4705, -0.2899,  0.5758]],
       dtype=torch.float64)
	q_value: tensor([[-22.5756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20361073765870707, distance: 1.0212201870962687 entropy 0.7110485434532166
epoch: 36, step: 8
	action: tensor([[-0.1778, -0.1197, -0.4805,  0.4473,  0.4181, -0.8397,  0.8252]],
       dtype=torch.float64)
	q_value: tensor([[-26.6189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027439328786405204, distance: 1.1599380263561867 entropy 0.7110485434532166
epoch: 36, step: 9
	action: tensor([[ 1.0864, -0.7227, -0.3767,  0.5628,  0.1554,  0.0437,  0.3942]],
       dtype=torch.float64)
	q_value: tensor([[-24.5850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4407686553839216, distance: 0.85576091265827 entropy 0.7110485434532166
epoch: 36, step: 10
	action: tensor([[ 0.1573, -0.3808, -0.2779, -0.1747,  0.3751,  0.0053,  0.1728]],
       dtype=torch.float64)
	q_value: tensor([[-24.8120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036040418850384004, distance: 1.1235337050741085 entropy 0.7110485434532166
epoch: 36, step: 11
	action: tensor([[ 0.1917, -1.5090,  0.3431, -0.4214, -0.0333,  0.1597,  0.7537]],
       dtype=torch.float64)
	q_value: tensor([[-21.1976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 36, step: 12
	action: tensor([[-0.3952, -0.8820,  0.7332, -0.2373, -0.8043, -0.2760,  1.1625]],
       dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9992563928561216, distance: 1.6180462824325965 entropy 0.7110485434532166
epoch: 36, step: 13
	action: tensor([[ 1.4339, -1.0642,  0.6238, -0.0527,  1.0093, -0.4545,  0.3926]],
       dtype=torch.float64)
	q_value: tensor([[-28.3130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4488263533477097, distance: 1.377415170063252 entropy 0.7110485434532166
epoch: 36, step: 14
	action: tensor([[ 0.5202, -0.0746, -0.8185, -0.1741,  0.5013,  0.5982, -0.1266]],
       dtype=torch.float64)
	q_value: tensor([[-26.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7346393209835949, distance: 0.5894880128014215 entropy 0.7110485434532166
epoch: 36, step: 15
	action: tensor([[ 0.7695, -0.6669,  0.3066,  0.4906,  0.7469, -0.2432,  0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-22.7414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39821387433768884, distance: 0.8877236264804848 entropy 0.7110485434532166
epoch: 36, step: 16
	action: tensor([[ 0.0548,  0.4622,  0.0612, -0.6640, -0.2646,  0.2850,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[-23.5166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45051847400280165, distance: 0.8482683068362811 entropy 0.7110485434532166
epoch: 36, step: 17
	action: tensor([[ 1.1011, -0.7275,  0.4356,  0.8016,  0.5518,  0.0150,  0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-23.0184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4837921741761493, distance: 0.82218393260554 entropy 0.7110485434532166
epoch: 36, step: 18
	action: tensor([[ 0.4030, -0.3726, -0.0352, -0.5051,  0.5323, -0.6520,  1.1297]],
       dtype=torch.float64)
	q_value: tensor([[-25.2005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17326358599541414, distance: 1.2395227130986168 entropy 0.7110485434532166
epoch: 36, step: 19
	action: tensor([[ 1.2100,  0.7114,  0.8231, -0.2922, -0.2121,  0.2862,  0.5191]],
       dtype=torch.float64)
	q_value: tensor([[-25.0446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9433645732450123, distance: 0.27233332086603834 entropy 0.7110485434532166
epoch: 36, step: 20
	action: tensor([[ 1.4353,  0.1676,  0.6924, -0.6039, -0.5343,  1.1881,  0.9770]],
       dtype=torch.float64)
	q_value: tensor([[-26.5789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7951472601064639, distance: 0.5179377690233117 entropy 0.7110485434532166
epoch: 36, step: 21
	action: tensor([[ 1.0335, -0.1454, -0.0685, -0.2857, -0.1658,  0.2626,  1.1636]],
       dtype=torch.float64)
	q_value: tensor([[-31.3194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5372842770139556, distance: 0.7784197060534707 entropy 0.7110485434532166
epoch: 36, step: 22
	action: tensor([[ 1.5340,  0.8315, -0.3913, -0.8949,  0.0625,  0.4311,  1.0646]],
       dtype=torch.float64)
	q_value: tensor([[-26.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.587943091255489, distance: 0.7345735615163624 entropy 0.7110485434532166
epoch: 36, step: 23
	action: tensor([[-0.5743, -1.0740,  0.2442,  0.8215,  0.3224,  0.3800,  0.9699]],
       dtype=torch.float64)
	q_value: tensor([[-31.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3228280309935827, distance: 1.3161590908427496 entropy 0.7110485434532166
epoch: 36, step: 24
	action: tensor([[ 0.3247, -0.4220, -0.1052, -0.7659, -0.2663, -0.2465,  0.1440]],
       dtype=torch.float64)
	q_value: tensor([[-27.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2866113319168897, distance: 1.298016996820891 entropy 0.7110485434532166
epoch: 36, step: 25
	action: tensor([[-0.0328,  0.3221,  0.9094, -0.8388,  1.0468, -0.2583,  0.0719]],
       dtype=torch.float64)
	q_value: tensor([[-23.5174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14578799207986504, distance: 1.2249231048985851 entropy 0.7110485434532166
epoch: 36, step: 26
	action: tensor([[ 0.7841, -0.8109, -0.6358,  0.3666,  0.7299,  0.8114,  0.5101]],
       dtype=torch.float64)
	q_value: tensor([[-25.1945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44372886158107994, distance: 0.8534929872292524 entropy 0.7110485434532166
epoch: 36, step: 27
	action: tensor([[ 0.7700,  0.6696, -0.2357, -0.4758, -0.0947, -0.1208,  0.9639]],
       dtype=torch.float64)
	q_value: tensor([[-26.2575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9462903000644364, distance: 0.26520582353051253 entropy 0.7110485434532166
epoch: 36, step: 28
	action: tensor([[ 0.4090, -0.0726, -0.9055, -0.1954,  0.9930, -0.4588, -0.5013]],
       dtype=torch.float64)
	q_value: tensor([[-25.9603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4330264492110867, distance: 0.8616642865189187 entropy 0.7110485434532166
epoch: 36, step: 29
	action: tensor([[ 0.0582, -0.4821,  0.3986,  0.7527,  0.9136,  0.5342,  2.1465]],
       dtype=torch.float64)
	q_value: tensor([[-24.6648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6558935287806633, distance: 0.6712790805951934 entropy 0.7110485434532166
epoch: 36, step: 30
	action: tensor([[ 0.6851,  0.0056, -0.0273,  0.2647,  1.0255,  0.8638,  1.3647]],
       dtype=torch.float64)
	q_value: tensor([[-30.8356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9323976539638325, distance: 0.29753466882791496 entropy 0.7110485434532166
epoch: 36, step: 31
	action: tensor([[ 0.9967, -0.9101,  0.2692, -0.1870, -0.5510,  0.5433, -0.7690]],
       dtype=torch.float64)
	q_value: tensor([[-27.6310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10807105863805022, distance: 1.2045934550619597 entropy 0.7110485434532166
epoch: 36, step: 32
	action: tensor([[ 0.6907, -1.0492,  0.6707, -0.9497,  0.3137, -0.1865,  0.2615]],
       dtype=torch.float64)
	q_value: tensor([[-26.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8209151227010985, distance: 1.544192876345893 entropy 0.7110485434532166
epoch: 36, step: 33
	action: tensor([[ 0.5860, -1.5401,  0.5325, -0.2406,  0.0130,  0.6303, -0.1896]],
       dtype=torch.float64)
	q_value: tensor([[-25.8973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 36, step: 34
	action: tensor([[ 0.5451, -0.8820,  0.8428, -0.8883,  0.0978, -0.8865,  1.0744]],
       dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5352284857105927, distance: 1.4178921669054843 entropy 0.7110485434532166
epoch: 36, step: 35
	action: tensor([[ 1.1472, -0.4462, -0.2290, -0.2259,  0.3714,  0.2936,  0.3314]],
       dtype=torch.float64)
	q_value: tensor([[-27.6029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1766466688870003, distance: 1.0383644637458656 entropy 0.7110485434532166
epoch: 36, step: 36
	action: tensor([[ 1.1381,  0.1890, -0.6144, -0.2054,  0.2525, -0.0611, -0.2797]],
       dtype=torch.float64)
	q_value: tensor([[-23.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7021390172810902, distance: 0.6245446641524206 entropy 0.7110485434532166
epoch: 36, step: 37
	action: tensor([[ 0.3152, -0.3256,  0.1248, -0.2350, -0.1837,  0.4203,  0.9063]],
       dtype=torch.float64)
	q_value: tensor([[-23.1713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2459769741078266, distance: 0.9936856025861139 entropy 0.7110485434532166
epoch: 36, step: 38
	action: tensor([[ 0.5130, -0.9930, -0.1924, -0.3684,  0.5498,  0.5310,  0.5846]],
       dtype=torch.float64)
	q_value: tensor([[-24.0835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38166592151781775, distance: 1.345111284884635 entropy 0.7110485434532166
epoch: 36, step: 39
	action: tensor([[-0.1195, -0.7004, -0.2949, -0.2818,  1.0512,  0.6566,  0.3884]],
       dtype=torch.float64)
	q_value: tensor([[-25.8501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21052254619248623, distance: 1.2590504558487163 entropy 0.7110485434532166
epoch: 36, step: 40
	action: tensor([[ 0.8484, -0.0525,  0.0445, -0.0371,  0.1406,  0.1507,  0.0709]],
       dtype=torch.float64)
	q_value: tensor([[-25.5540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7289797569869222, distance: 0.5957410937877785 entropy 0.7110485434532166
epoch: 36, step: 41
	action: tensor([[-0.0403, -0.5259, -0.3239,  0.1452,  0.1000, -0.5468,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-21.5782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20377604511068825, distance: 1.2555370750522252 entropy 0.7110485434532166
epoch: 36, step: 42
	action: tensor([[ 0.2187, -1.0201, -0.1667,  0.0087,  1.7988, -0.0897, -0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-22.4372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5513958432523915, distance: 1.4253384636632407 entropy 0.7110485434532166
epoch: 36, step: 43
	action: tensor([[0.0348, 0.0194, 0.2293, 0.1063, 0.5628, 0.7549, 0.4858]],
       dtype=torch.float64)
	q_value: tensor([[-28.4896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6559511939817128, distance: 0.6712228319218263 entropy 0.7110485434532166
epoch: 36, step: 44
	action: tensor([[ 0.5374,  0.0153, -0.1867, -0.8467, -0.0086,  0.4496, -0.4883]],
       dtype=torch.float64)
	q_value: tensor([[-23.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39958963152792826, distance: 0.8867083231068764 entropy 0.7110485434532166
epoch: 36, step: 45
	action: tensor([[ 0.3993, -0.2960, -0.2285,  0.2783, -0.0405,  0.2150,  0.2235]],
       dtype=torch.float64)
	q_value: tensor([[-23.4334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5641769542185966, distance: 0.7554605474075 entropy 0.7110485434532166
epoch: 36, step: 46
	action: tensor([[ 0.9324, -0.6005,  0.2356, -0.8338, -0.5083,  0.7355,  0.5793]],
       dtype=torch.float64)
	q_value: tensor([[-21.3907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08861415023562702, distance: 1.193970731593564 entropy 0.7110485434532166
epoch: 36, step: 47
	action: tensor([[ 0.3105, -0.8280,  0.6681, -0.1967,  0.0360,  0.4572,  0.5278]],
       dtype=torch.float64)
	q_value: tensor([[-27.3911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2038083572421261, distance: 1.2555539256977089 entropy 0.7110485434532166
epoch: 36, step: 48
	action: tensor([[ 0.0118, -0.7188,  0.8244, -0.0600,  0.7068,  0.0060,  0.1411]],
       dtype=torch.float64)
	q_value: tensor([[-24.0961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36713344920645485, distance: 1.3380185913399083 entropy 0.7110485434532166
epoch: 36, step: 49
	action: tensor([[ 0.9078, -0.1491,  0.1648,  0.1607, -0.0188, -0.1336,  0.0940]],
       dtype=torch.float64)
	q_value: tensor([[-23.3370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6798326394574297, distance: 0.6475081231467567 entropy 0.7110485434532166
epoch: 36, step: 50
	action: tensor([[-0.1474,  0.5258, -0.2217,  0.5941,  0.6963,  0.3056,  1.1159]],
       dtype=torch.float64)
	q_value: tensor([[-21.9115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 36, step: 51
	action: tensor([[ 0.1104, -0.3947,  0.4945, -1.0549,  0.4420, -0.3218,  1.5122]],
       dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6141508120638413, distance: 1.4538806207685826 entropy 0.7110485434532166
epoch: 36, step: 52
	action: tensor([[ 0.4491,  0.3008,  0.0949, -0.8089,  0.2075, -0.0772,  0.1147]],
       dtype=torch.float64)
	q_value: tensor([[-27.8630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4679427851847756, distance: 0.8347104711726372 entropy 0.7110485434532166
epoch: 36, step: 53
	action: tensor([[-0.4133, -0.5818, -0.4981, -0.2986,  0.3835, -0.3702,  0.5389]],
       dtype=torch.float64)
	q_value: tensor([[-22.3390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6348264778056376, distance: 1.4631623748683111 entropy 0.7110485434532166
epoch: 36, step: 54
	action: tensor([[ 0.3138, -0.5630,  0.8794, -0.3682, -0.0216, -0.1205,  0.6773]],
       dtype=torch.float64)
	q_value: tensor([[-24.0333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29047673906169136, distance: 1.2999653713000197 entropy 0.7110485434532166
epoch: 36, step: 55
	action: tensor([[ 0.1973, -0.0717, -0.6443, -0.2352,  0.7105,  0.2584,  0.3805]],
       dtype=torch.float64)
	q_value: tensor([[-23.6183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46590044878518777, distance: 0.8363109820815691 entropy 0.7110485434532166
epoch: 36, step: 56
	action: tensor([[ 0.7856, -0.5483,  0.5019, -0.3835, -0.2681,  0.0658,  0.1673]],
       dtype=torch.float64)
	q_value: tensor([[-22.7478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018034215944385812, distance: 1.1546168222164834 entropy 0.7110485434532166
epoch: 36, step: 57
	action: tensor([[-0.0507, -0.5913,  0.1806, -0.4048, -0.9572,  0.0668,  1.3293]],
       dtype=torch.float64)
	q_value: tensor([[-23.4811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5315969885844389, distance: 1.4162142017217967 entropy 0.7110485434532166
epoch: 36, step: 58
	action: tensor([[ 1.2441, -0.0022,  0.1805, -0.1950,  0.4601,  0.3525,  0.2142]],
       dtype=torch.float64)
	q_value: tensor([[-29.0466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.616627687564869, distance: 0.7085443927737686 entropy 0.7110485434532166
epoch: 36, step: 59
	action: tensor([[ 0.9619,  0.3677,  0.0524,  0.5922, -0.6553, -0.4118,  0.6407]],
       dtype=torch.float64)
	q_value: tensor([[-23.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9696971490456344, distance: 0.19920417584452652 entropy 0.7110485434532166
epoch: 36, step: 60
	action: tensor([[ 0.7880, -0.8588,  0.1470, -0.2862,  0.7810, -0.4096,  1.0418]],
       dtype=torch.float64)
	q_value: tensor([[-26.2020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4393523256127494, distance: 1.3729042523745252 entropy 0.7110485434532166
epoch: 36, step: 61
	action: tensor([[-0.1110, -0.5367,  0.1554, -0.0899,  0.3283, -0.1727, -0.4365]],
       dtype=torch.float64)
	q_value: tensor([[-25.7235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40041712062820056, distance: 1.354208074745614 entropy 0.7110485434532166
epoch: 36, step: 62
	action: tensor([[ 0.3596, -0.5699, -0.6613,  0.0669,  0.5507, -0.2024, -0.9790]],
       dtype=torch.float64)
	q_value: tensor([[-22.3806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017525964716412767, distance: 1.154328566345078 entropy 0.7110485434532166
epoch: 36, step: 63
	action: tensor([[ 0.0569, -0.8390, -0.0137, -0.5272,  0.4044,  0.3255, -0.3093]],
       dtype=torch.float64)
	q_value: tensor([[-24.6166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5774329386696633, distance: 1.43724943255924 entropy 0.7110485434532166
epoch: 36, step: 64
	action: tensor([[ 0.7245, -1.7266,  0.8949,  0.8518,  0.4030, -0.2320, -0.3031]],
       dtype=torch.float64)
	q_value: tensor([[-23.7406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 36, step: 65
	action: tensor([[ 0.3261, -0.9210,  0.1261, -0.3526,  0.4220, -0.7362,  0.3259]],
       dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7179206763036297, distance: 1.4998859834836331 entropy 0.7110485434532166
epoch: 36, step: 66
	action: tensor([[ 0.6053, -0.5372,  0.2839, -1.0021,  0.5830,  1.0304,  0.9451]],
       dtype=torch.float64)
	q_value: tensor([[-24.8496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12630884064925407, distance: 1.2144662206352985 entropy 0.7110485434532166
epoch: 36, step: 67
	action: tensor([[ 0.3274, -1.7732,  0.2890, -0.1166,  0.0857,  1.1655, -0.4585]],
       dtype=torch.float64)
	q_value: tensor([[-28.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 36, step: 68
	action: tensor([[ 1.0074, -0.3129,  0.4122, -1.4308,  0.5869,  0.4633, -0.3173]],
       dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18805863100374842, distance: 1.2473135209034618 entropy 0.7110485434532166
epoch: 36, step: 69
	action: tensor([[ 1.0215, -0.0113, -0.2617, -0.9420,  0.1423, -0.1448,  0.2879]],
       dtype=torch.float64)
	q_value: tensor([[-27.0853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14596931162320803, distance: 1.0575317906138653 entropy 0.7110485434532166
epoch: 36, step: 70
	action: tensor([[ 0.3147,  0.3228, -0.6205, -0.6020, -0.0266, -0.0754,  1.2324]],
       dtype=torch.float64)
	q_value: tensor([[-24.7489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6876514010580661, distance: 0.6395529027062873 entropy 0.7110485434532166
epoch: 36, step: 71
	action: tensor([[-0.8919, -0.4883, -1.0042,  0.8227,  0.1532, -0.0478,  0.4124]],
       dtype=torch.float64)
	q_value: tensor([[-26.6686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.230588096061771, distance: 1.7090957072618813 entropy 0.7110485434532166
epoch: 36, step: 72
	action: tensor([[ 0.2407,  0.3274,  0.4506, -0.5714,  0.5885, -0.2233,  0.0794]],
       dtype=torch.float64)
	q_value: tensor([[-26.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3564152246215284, distance: 0.9180357086945925 entropy 0.7110485434532166
epoch: 36, step: 73
	action: tensor([[ 0.2471,  0.1088, -0.2927, -1.5703, -0.1708,  0.2723,  0.3521]],
       dtype=torch.float64)
	q_value: tensor([[-22.1368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08387546752036568, distance: 1.0953021743270508 entropy 0.7110485434532166
epoch: 36, step: 74
	action: tensor([[ 0.5530, -0.4391, -0.1208, -0.6518,  0.4196,  0.2477,  0.4853]],
       dtype=torch.float64)
	q_value: tensor([[-27.0857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.039844512966321854, distance: 1.1669194953219961 entropy 0.7110485434532166
epoch: 36, step: 75
	action: tensor([[ 0.8087, -0.1223, -0.4384,  0.5355, -0.9203,  0.2037,  0.0694]],
       dtype=torch.float64)
	q_value: tensor([[-23.6983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8512557166586, distance: 0.4413436018959749 entropy 0.7110485434532166
epoch: 36, step: 76
	action: tensor([[ 1.2287, -0.1831,  0.5850, -0.2472, -0.4548,  0.4422,  0.7069]],
       dtype=torch.float64)
	q_value: tensor([[-25.3896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5637692187816113, distance: 0.7558138513791086 entropy 0.7110485434532166
epoch: 36, step: 77
	action: tensor([[ 0.7332, -0.8293,  0.1274, -0.2307,  0.5516,  1.2583,  0.9875]],
       dtype=torch.float64)
	q_value: tensor([[-25.9125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2431246723195425, distance: 0.9955632746252961 entropy 0.7110485434532166
epoch: 36, step: 78
	action: tensor([[ 1.3778, -1.0483, -0.4768,  0.4575,  0.5244,  0.6686,  0.3985]],
       dtype=torch.float64)
	q_value: tensor([[-27.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19849486622825196, distance: 1.024495010131279 entropy 0.7110485434532166
epoch: 36, step: 79
	action: tensor([[ 0.3760, -0.2872, -0.1071, -0.7667,  0.0667, -0.4702, -0.7110]],
       dtype=torch.float64)
	q_value: tensor([[-27.6112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1378975112186669, distance: 1.220698095622665 entropy 0.7110485434532166
epoch: 36, step: 80
	action: tensor([[ 0.6443, -0.2872,  0.0876, -0.0246, -0.5439,  0.1753,  0.5522]],
       dtype=torch.float64)
	q_value: tensor([[-23.8787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49477728376818086, distance: 0.8133886874610592 entropy 0.7110485434532166
epoch: 36, step: 81
	action: tensor([[ 0.4820, -0.6635,  0.0023,  0.0452,  0.5250,  0.1678, -0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-23.8918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10156630024203417, distance: 1.0846751988941343 entropy 0.7110485434532166
epoch: 36, step: 82
	action: tensor([[-0.1128, -0.0104,  0.0428,  0.5968,  0.5827, -0.1841,  0.2332]],
       dtype=torch.float64)
	q_value: tensor([[-22.1666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 36, step: 83
	action: tensor([[ 0.6614,  0.0613,  0.0785, -0.0300,  0.3675, -0.8950, -0.3538]],
       dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5762273586936195, distance: 0.7449431841228197 entropy 0.7110485434532166
epoch: 36, step: 84
	action: tensor([[ 0.7563, -0.1534, -0.4226, -0.3685, -0.1060, -0.1474,  1.4788]],
       dtype=torch.float64)
	q_value: tensor([[-22.6867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4007690856825933, distance: 0.8858369641270838 entropy 0.7110485434532166
epoch: 36, step: 85
	action: tensor([[ 0.7772,  0.2703, -0.8083, -0.0257,  0.2411,  0.5616, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-28.3324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9590857829086141, distance: 0.23146952257293593 entropy 0.7110485434532166
epoch: 36, step: 86
	action: tensor([[ 0.7829, -0.2863,  0.2799, -1.1765,  1.3297, -0.6917,  0.7766]],
       dtype=torch.float64)
	q_value: tensor([[-22.9949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09437645349365176, distance: 1.1971265517889007 entropy 0.7110485434532166
epoch: 36, step: 87
	action: tensor([[ 0.0272, -0.3912, -0.4221, -0.2222, -0.6733,  0.0298,  0.0307]],
       dtype=torch.float64)
	q_value: tensor([[-28.3663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11046984947578475, distance: 1.2058966230997454 entropy 0.7110485434532166
epoch: 36, step: 88
	action: tensor([[ 0.8554, -1.0475,  0.0574, -0.7786,  0.0262, -0.2626, -0.1583]],
       dtype=torch.float64)
	q_value: tensor([[-23.4949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8897655056619795, distance: 1.573115651353315 entropy 0.7110485434532166
epoch: 36, step: 89
	action: tensor([[ 0.5924, -1.0076,  0.3143, -0.1738, -0.4539, -0.6045, -1.0954]],
       dtype=torch.float64)
	q_value: tensor([[-26.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6507003396228579, distance: 1.4702487322313924 entropy 0.7110485434532166
epoch: 36, step: 90
	action: tensor([[-0.0992, -0.9225,  0.4917, -0.1529,  0.6556,  0.8553,  0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-27.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2854685213135235, distance: 1.2974403980084437 entropy 0.7110485434532166
epoch: 36, step: 91
	action: tensor([[ 1.2263, -0.0024, -0.7421, -0.7479,  0.6416,  0.1620,  0.4446]],
       dtype=torch.float64)
	q_value: tensor([[-25.1622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2614481056971031, distance: 0.9834384892794502 entropy 0.7110485434532166
epoch: 36, step: 92
	action: tensor([[ 0.8754, -0.9213, -0.0068,  0.7298, -0.0273,  0.0423,  0.0688]],
       dtype=torch.float64)
	q_value: tensor([[-26.6644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38047969483671396, distance: 0.90070892448067 entropy 0.7110485434532166
epoch: 36, step: 93
	action: tensor([[ 1.4390,  0.0540, -0.3319,  0.3524,  0.8820,  0.1673,  0.5032]],
       dtype=torch.float64)
	q_value: tensor([[-24.8198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7340631782801642, distance: 0.5901276046776506 entropy 0.7110485434532166
epoch: 36, step: 94
	action: tensor([[ 0.4610,  0.0427,  0.6966,  0.4905,  0.2473,  0.3232, -0.2657]],
       dtype=torch.float64)
	q_value: tensor([[-25.5449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9409474357265881, distance: 0.2780840460970014 entropy 0.7110485434532166
epoch: 36, step: 95
	action: tensor([[ 0.7386,  0.5106,  0.5041, -0.9156,  0.0944, -0.4498,  0.0815]],
       dtype=torch.float64)
	q_value: tensor([[-23.0765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6047294595743193, distance: 0.7194554669508302 entropy 0.7110485434532166
epoch: 36, step: 96
	action: tensor([[-1.0704, -0.3681,  0.5187,  0.2815,  0.3540, -0.6114,  0.4938]],
       dtype=torch.float64)
	q_value: tensor([[-24.0096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2657575613077214, distance: 1.722516585510059 entropy 0.7110485434532166
epoch: 36, step: 97
	action: tensor([[-0.2625, -1.4266, -0.2113, -0.4511,  0.4753,  0.2425, -0.1376]],
       dtype=torch.float64)
	q_value: tensor([[-25.1031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9607194023187349, distance: 1.6023759443079806 entropy 0.7110485434532166
epoch: 36, step: 98
	action: tensor([[ 1.2536, -0.8282, -0.5489, -0.2264,  0.2537, -0.4031,  0.6016]],
       dtype=torch.float64)
	q_value: tensor([[-26.0668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48530868440866515, distance: 1.3946494292439127 entropy 0.7110485434532166
epoch: 36, step: 99
	action: tensor([[ 0.8734, -0.4847,  0.3690, -0.5396, -0.0927,  0.6503,  0.0913]],
       dtype=torch.float64)
	q_value: tensor([[-26.6785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.157047423491956, distance: 1.0506504829493502 entropy 0.7110485434532166
epoch: 36, step: 100
	action: tensor([[ 0.2510, -0.6211, -0.0312, -0.0753,  0.4155,  0.1466,  0.1980]],
       dtype=torch.float64)
	q_value: tensor([[-24.4149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08844594429726271, distance: 1.1938784855366342 entropy 0.7110485434532166
epoch: 36, step: 101
	action: tensor([[-0.1012,  0.0455, -0.5195, -0.1558,  0.1046,  0.4072, -0.7419]],
       dtype=torch.float64)
	q_value: tensor([[-21.7971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24341583494489538, distance: 0.9953717644193668 entropy 0.7110485434532166
epoch: 36, step: 102
	action: tensor([[-0.1808, -0.7031,  0.3241, -0.6661, -0.2408,  0.3150,  0.7127]],
       dtype=torch.float64)
	q_value: tensor([[-21.7335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8124947279947536, distance: 1.5406183603743857 entropy 0.7110485434532166
epoch: 36, step: 103
	action: tensor([[ 0.3913, -0.5513, -0.4855, -0.5909,  1.0022,  0.8840,  0.7599]],
       dtype=torch.float64)
	q_value: tensor([[-25.4538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23027342311858745, distance: 1.0039797060234117 entropy 0.7110485434532166
epoch: 36, step: 104
	action: tensor([[ 7.6407e-01,  6.4353e-05, -1.1810e-01, -2.7211e-01, -4.5517e-01,
         -4.8901e-01,  6.1415e-01]], dtype=torch.float64)
	q_value: tensor([[-27.7654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5063357399805257, distance: 0.8040305241194734 entropy 0.7110485434532166
epoch: 36, step: 105
	action: tensor([[-0.4309,  0.1065,  0.4641, -0.3846, -0.8126,  0.3332,  1.1035]],
       dtype=torch.float64)
	q_value: tensor([[-24.4830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40464357633900927, distance: 1.3562500337066932 entropy 0.7110485434532166
epoch: 36, step: 106
	action: tensor([[ 0.2914, -0.6263,  0.1218, -0.1482,  0.7447, -0.6002,  0.4286]],
       dtype=torch.float64)
	q_value: tensor([[-27.6302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3281236151811817, distance: 1.3187909029814076 entropy 0.7110485434532166
epoch: 36, step: 107
	action: tensor([[ 0.5979,  0.4946, -0.3921,  0.0720,  0.2427,  0.6138,  0.0860]],
       dtype=torch.float64)
	q_value: tensor([[-23.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9445169020757821, distance: 0.2695485775463474 entropy 0.7110485434532166
epoch: 36, step: 108
	action: tensor([[ 0.0782, -0.1043,  0.3282, -0.7250,  0.3045,  0.5143, -0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-22.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003370018649762052, distance: 1.1462708629270948 entropy 0.7110485434532166
epoch: 36, step: 109
	action: tensor([[ 0.1999, -0.7965,  0.4787,  0.7820, -0.1015,  0.2267,  0.6344]],
       dtype=torch.float64)
	q_value: tensor([[-22.3624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4557445746903752, distance: 0.8442247433589056 entropy 0.7110485434532166
epoch: 36, step: 110
	action: tensor([[ 0.9892, -0.2313, -0.7962, -0.9740, -0.1894,  0.2202,  0.2449]],
       dtype=torch.float64)
	q_value: tensor([[-25.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08147893126606698, distance: 1.096733866660559 entropy 0.7110485434532166
epoch: 36, step: 111
	action: tensor([[ 0.9025,  0.4629,  0.3932,  0.3118,  0.3594,  1.1612, -0.2882]],
       dtype=torch.float64)
	q_value: tensor([[-26.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9134977982533445, distance: 0.33656602545179476 entropy 0.7110485434532166
epoch: 36, step: 112
	action: tensor([[ 0.0393,  0.0062,  0.1636,  0.5409, -0.4279, -0.5253, -0.1723]],
       dtype=torch.float64)
	q_value: tensor([[-25.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 36, step: 113
	action: tensor([[-0.0954, -1.5925, -0.0417, -0.1423,  0.6021, -0.0047,  1.0437]],
       dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9309746362445237, distance: 1.590175206900222 entropy 0.7110485434532166
epoch: 36, step: 114
	action: tensor([[ 0.1965, -1.0787,  0.2224, -0.3957,  0.1872,  0.1159, -0.1071]],
       dtype=torch.float64)
	q_value: tensor([[-26.6923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8198614044951114, distance: 1.5437460187213532 entropy 0.7110485434532166
epoch: 36, step: 115
	action: tensor([[-0.4820, -0.3551, -0.1928,  0.3434, -0.2692,  0.2187,  0.2498]],
       dtype=torch.float64)
	q_value: tensor([[-24.2344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33753517972027214, distance: 1.3234553688277493 entropy 0.7110485434532166
epoch: 36, step: 116
	action: tensor([[ 0.1325,  0.0423,  0.0149, -0.5661,  0.3802,  0.0468, -0.3065]],
       dtype=torch.float64)
	q_value: tensor([[-23.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17801672172224814, distance: 1.0375001893079576 entropy 0.7110485434532166
epoch: 36, step: 117
	action: tensor([[ 1.4512, -0.6128,  0.0291,  0.4180,  0.6004, -0.5953,  0.2864]],
       dtype=torch.float64)
	q_value: tensor([[-21.2395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1737671801569134, distance: 1.040178599334039 entropy 0.7110485434532166
epoch: 36, step: 118
	action: tensor([[ 1.2917, -0.7152,  0.4643, -0.4890,  0.6500,  0.2407,  0.2243]],
       dtype=torch.float64)
	q_value: tensor([[-25.7369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29727609285729684, distance: 1.3033855461859662 entropy 0.7110485434532166
epoch: 36, step: 119
	action: tensor([[ 0.5240, -0.1596, -0.4113, -0.7524,  0.3417,  0.0582,  1.2561]],
       dtype=torch.float64)
	q_value: tensor([[-24.9108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22784785608724945, distance: 1.005560335233125 entropy 0.7110485434532166
epoch: 36, step: 120
	action: tensor([[ 0.5778, -0.0250,  0.1768,  0.4444, -0.0560,  0.5287, -0.0485]],
       dtype=torch.float64)
	q_value: tensor([[-26.5994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9550331537595175, distance: 0.2426626343065999 entropy 0.7110485434532166
epoch: 36, step: 121
	action: tensor([[ 0.7318,  0.2172,  0.0944,  0.5135,  0.2506,  0.4732, -0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-22.2164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.06661188978688314 entropy 0.7110485434532166
epoch: 36, step: 122
	action: tensor([[ 6.6305e-01, -1.1009e+00,  3.0867e-01, -3.9912e-01,  3.7552e-01,
          1.0523e-03,  6.4340e-01]], dtype=torch.float64)
	q_value: tensor([[-29.7372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7665048378529764, distance: 1.520947098989916 entropy 0.7110485434532166
epoch: 36, step: 123
	action: tensor([[ 0.2395, -0.7299, -0.8231, -0.2447,  1.1800,  0.0739, -0.3565]],
       dtype=torch.float64)
	q_value: tensor([[-25.3879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.195194877807628, distance: 1.251053997339333 entropy 0.7110485434532166
epoch: 36, step: 124
	action: tensor([[ 0.9353, -0.7409,  0.0539, -0.5231, -0.7681,  0.0153, -0.2558]],
       dtype=torch.float64)
	q_value: tensor([[-26.4531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38783440799094127, distance: 1.3481105846315908 entropy 0.7110485434532166
epoch: 36, step: 125
	action: tensor([[ 0.3997, -0.0848,  0.1356,  0.0974,  0.2016,  0.5686,  0.5659]],
       dtype=torch.float64)
	q_value: tensor([[-26.0444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7538306996843703, distance: 0.5677715655234241 entropy 0.7110485434532166
epoch: 36, step: 126
	action: tensor([[ 0.1077, -0.3224, -0.0255,  0.0667,  0.0373, -0.3905,  1.0383]],
       dtype=torch.float64)
	q_value: tensor([[-22.3254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07044224979514724, distance: 1.1033032090167512 entropy 0.7110485434532166
epoch: 36, step: 127
	action: tensor([[ 0.6626,  0.0750, -0.7104,  1.3015, -0.4930,  0.2603, -0.0794]],
       dtype=torch.float64)
	q_value: tensor([[-24.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
LOSS epoch 36 actor 349.343946909746 critic 945.0798134219506 
epoch: 37, step: 0
	action: tensor([[ 0.1871, -1.0484,  0.1509, -0.2806,  0.3908, -0.1923,  0.1832]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8137587037473009, distance: 1.5411554556946636 entropy 0.7110485434532166
epoch: 37, step: 1
	action: tensor([[-0.4239, -0.2230,  0.2436, -0.2233, -0.1588,  0.4739, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-20.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3298497148752324, distance: 1.3196476099202148 entropy 0.7110485434532166
epoch: 37, step: 2
	action: tensor([[ 0.0085, -0.0725,  0.7167,  0.4102, -0.9300, -0.2722,  0.2865]],
       dtype=torch.float64)
	q_value: tensor([[-19.9832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4849969272445174, distance: 0.8212239440327465 entropy 0.7110485434532166
epoch: 37, step: 3
	action: tensor([[ 1.0346, -0.0555,  0.4698, -0.7405,  0.6650, -0.1150,  0.5419]],
       dtype=torch.float64)
	q_value: tensor([[-22.9048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3540042728108327, distance: 0.9197536415512702 entropy 0.7110485434532166
epoch: 37, step: 4
	action: tensor([[ 0.4869, -0.5468,  0.4002, -0.0069, -0.5136, -0.3437, -0.1139]],
       dtype=torch.float64)
	q_value: tensor([[-20.3714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008341840424325064, distance: 1.1395612898513818 entropy 0.7110485434532166
epoch: 37, step: 5
	action: tensor([[ 0.0372, -0.0948,  0.6724, -0.5715,  0.4098, -0.4898,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-20.6858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2702661897907961, distance: 1.2897456229313586 entropy 0.7110485434532166
epoch: 37, step: 6
	action: tensor([[ 0.3919, -0.4864,  0.6957, -0.1450, -0.0590,  0.7837, -0.0570]],
       dtype=torch.float64)
	q_value: tensor([[-19.7697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37359804385999906, distance: 0.9056976604703251 entropy 0.7110485434532166
epoch: 37, step: 7
	action: tensor([[ 0.8743, -0.5567,  1.1149,  0.4976, -0.4150, -0.3158,  0.8450]],
       dtype=torch.float64)
	q_value: tensor([[-20.5167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2939749902228469, distance: 0.9615386282888766 entropy 0.7110485434532166
epoch: 37, step: 8
	action: tensor([[ 1.1772, -0.4448, -1.0196, -0.2904, -0.0383,  0.0144,  0.2686]],
       dtype=torch.float64)
	q_value: tensor([[-22.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07599100236324996, distance: 1.100005339821428 entropy 0.7110485434532166
epoch: 37, step: 9
	action: tensor([[-0.0742, -0.1462,  0.0448,  0.0328, -0.9319, -1.0616,  0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-22.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011698974492757, distance: 0.956626693984512 entropy 0.7110485434532166
epoch: 37, step: 10
	action: tensor([[ 0.9589, -0.9779, -0.0108, -1.1871,  0.0275, -0.3902,  0.4730]],
       dtype=torch.float64)
	q_value: tensor([[-24.1287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8079353400087459, distance: 1.5386794030501445 entropy 0.7110485434532166
epoch: 37, step: 11
	action: tensor([[ 0.6049, -0.5038,  0.3259, -0.9546,  0.3210, -0.1622, -0.0975]],
       dtype=torch.float64)
	q_value: tensor([[-24.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4178707778571702, distance: 1.362620816203104 entropy 0.7110485434532166
epoch: 37, step: 12
	action: tensor([[-0.4382, -0.3982, -0.1299,  0.0426, -0.5974,  0.0286,  0.9100]],
       dtype=torch.float64)
	q_value: tensor([[-20.9553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48002169967303243, distance: 1.3921650758076938 entropy 0.7110485434532166
epoch: 37, step: 13
	action: tensor([[ 0.8016,  0.2214, -0.3147, -0.5303, -0.2170,  0.6362,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-22.2667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8024906384910877, distance: 0.508569763180889 entropy 0.7110485434532166
epoch: 37, step: 14
	action: tensor([[ 0.8384, -0.1609,  0.0189, -0.0898,  0.5163, -0.7136,  0.1261]],
       dtype=torch.float64)
	q_value: tensor([[-21.0455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4196909214978555, distance: 0.8717387987294714 entropy 0.7110485434532166
epoch: 37, step: 15
	action: tensor([[ 0.0124, -1.2947,  0.4456, -0.7960,  0.4851,  1.1275,  0.6850]],
       dtype=torch.float64)
	q_value: tensor([[-19.4449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6947134016362597, distance: 1.4897206065562214 entropy 0.7110485434532166
epoch: 37, step: 16
	action: tensor([[ 0.1513, -0.4750, -0.7656,  0.1181, -0.4441,  0.2490,  0.3946]],
       dtype=torch.float64)
	q_value: tensor([[-24.7529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030181091663029735, distance: 1.126943172779952 entropy 0.7110485434532166
epoch: 37, step: 17
	action: tensor([[ 0.1493, -0.2661, -0.7024,  0.4843,  0.6285, -0.1495, -0.0860]],
       dtype=torch.float64)
	q_value: tensor([[-20.9755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2501019079257183, distance: 0.9909638630625556 entropy 0.7110485434532166
epoch: 37, step: 18
	action: tensor([[ 0.5474, -0.7381,  0.0071, -0.2207, -0.5804, -0.3824,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-19.0786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30324701250238273, distance: 1.306381622352526 entropy 0.7110485434532166
epoch: 37, step: 19
	action: tensor([[ 1.0831, -0.7028,  0.0421,  0.2368,  0.3746,  0.3957, -0.3931]],
       dtype=torch.float64)
	q_value: tensor([[-21.5153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31289287195566384, distance: 0.9485689842352135 entropy 0.7110485434532166
epoch: 37, step: 20
	action: tensor([[ 0.3373, -0.5326, -0.6299, -0.0379,  0.2678,  0.4272, -0.0550]],
       dtype=torch.float64)
	q_value: tensor([[-20.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21956766362149416, distance: 1.0109375335113464 entropy 0.7110485434532166
epoch: 37, step: 21
	action: tensor([[-0.1612,  0.0398, -0.1148,  0.0980,  0.7201, -0.9497,  0.3730]],
       dtype=torch.float64)
	q_value: tensor([[-19.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026172610083848413, distance: 1.159222768381504 entropy 0.7110485434532166
epoch: 37, step: 22
	action: tensor([[-0.0945, -0.1196,  0.7347,  0.4790, -0.0657,  0.2104,  0.6194]],
       dtype=torch.float64)
	q_value: tensor([[-20.0717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5547854182566447, distance: 0.7635568598891911 entropy 0.7110485434532166
epoch: 37, step: 23
	action: tensor([[ 0.3024, -0.0209,  0.3604, -0.4371,  0.6078,  0.5310,  0.3494]],
       dtype=torch.float64)
	q_value: tensor([[-20.4858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4724836893256702, distance: 0.8311408717948494 entropy 0.7110485434532166
epoch: 37, step: 24
	action: tensor([[ 1.2628, -0.3697,  0.8201, -0.5105, -0.2541, -0.6947,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-19.3480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12880345166806428, distance: 1.0681070123500096 entropy 0.7110485434532166
epoch: 37, step: 25
	action: tensor([[ 0.1726, -0.3769,  0.3598,  0.4409, -0.3128, -0.1306,  0.2811]],
       dtype=torch.float64)
	q_value: tensor([[-22.1855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4297599061470846, distance: 0.8641429039549119 entropy 0.7110485434532166
epoch: 37, step: 26
	action: tensor([[ 0.5937, -0.9652, -0.5711,  0.2059,  0.9903, -1.0279,  0.8718]],
       dtype=torch.float64)
	q_value: tensor([[-20.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4254657999398246, distance: 1.3662654759282886 entropy 0.7110485434532166
epoch: 37, step: 27
	action: tensor([[-0.0665, -0.1969,  0.2901,  0.4501,  0.0690,  0.3403,  0.2959]],
       dtype=torch.float64)
	q_value: tensor([[-23.6594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5115858819617505, distance: 0.7997436451681353 entropy 0.7110485434532166
epoch: 37, step: 28
	action: tensor([[ 0.4486, -0.1088,  0.0156, -0.9238,  0.1996, -0.8223, -0.1912]],
       dtype=torch.float64)
	q_value: tensor([[-19.1213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.061913288791400234, distance: 1.179237336212257 entropy 0.7110485434532166
epoch: 37, step: 29
	action: tensor([[ 0.0161, -0.9389,  0.1203,  0.4576,  0.1760, -0.5421,  0.8487]],
       dtype=torch.float64)
	q_value: tensor([[-21.3271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3886082224251579, distance: 1.3484863650746315 entropy 0.7110485434532166
epoch: 37, step: 30
	action: tensor([[-0.2535, -0.3995,  0.1767, -0.8783,  0.4063, -0.5958,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-21.8065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7296598448111249, distance: 1.505001888261535 entropy 0.7110485434532166
epoch: 37, step: 31
	action: tensor([[ 0.0223,  0.3196, -0.3128,  0.1424, -0.6033,  0.1952,  0.2378]],
       dtype=torch.float64)
	q_value: tensor([[-21.1498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 32
	action: tensor([[ 0.0021, -0.5626,  0.9910, -0.7172, -0.5577,  0.2123,  0.6576]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6298188025070912, distance: 1.4609197326978853 entropy 0.7110485434532166
epoch: 37, step: 33
	action: tensor([[ 1.3346,  0.1376,  0.7555, -0.3459,  0.8421, -0.2566,  0.9870]],
       dtype=torch.float64)
	q_value: tensor([[-22.8575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6875528043531625, distance: 0.6396538361402335 entropy 0.7110485434532166
epoch: 37, step: 34
	action: tensor([[ 4.3018e-01, -5.8689e-02,  8.9952e-01,  1.1102e-01,  3.1276e-04,
         -3.0077e-01,  4.3827e-01]], dtype=torch.float64)
	q_value: tensor([[-22.1791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.589389987569763, distance: 0.733282736848591 entropy 0.7110485434532166
epoch: 37, step: 35
	action: tensor([[ 0.2068, -0.4090, -0.0033, -0.0775,  0.5606,  1.0858,  0.5415]],
       dtype=torch.float64)
	q_value: tensor([[-19.8340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5209013320525413, distance: 0.792080232457033 entropy 0.7110485434532166
epoch: 37, step: 36
	action: tensor([[ 1.0366,  0.0358, -0.1271, -0.6254,  0.4509,  0.6521, -0.0655]],
       dtype=torch.float64)
	q_value: tensor([[-21.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5915065055845683, distance: 0.7313904166084073 entropy 0.7110485434532166
epoch: 37, step: 37
	action: tensor([[ 0.0817, -0.1544,  0.3086,  0.2745,  0.1999,  1.4983,  0.2044]],
       dtype=torch.float64)
	q_value: tensor([[-20.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8200515102687569, distance: 0.4854346763241014 entropy 0.7110485434532166
epoch: 37, step: 38
	action: tensor([[-0.3843,  0.3135, -0.6870, -0.5183,  0.4655, -0.9474,  0.3135]],
       dtype=torch.float64)
	q_value: tensor([[-23.2064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 39
	action: tensor([[ 0.9866, -1.7213,  0.4946, -0.5403,  0.3221, -0.4537,  0.5472]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8652985956761989, distance: 1.5628988618228792 entropy 0.7110485434532166
epoch: 37, step: 40
	action: tensor([[ 0.4845, -0.0032, -0.2332,  0.0379, -0.8125,  0.2991,  0.6542]],
       dtype=torch.float64)
	q_value: tensor([[-22.5771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6933682791153593, distance: 0.6336730447512462 entropy 0.7110485434532166
epoch: 37, step: 41
	action: tensor([[-0.5816,  0.7580,  0.0750, -0.9184,  0.3474,  0.1595,  0.1874]],
       dtype=torch.float64)
	q_value: tensor([[-21.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1491763622732014, distance: 1.2267329637006408 entropy 0.7110485434532166
epoch: 37, step: 42
	action: tensor([[ 0.1897,  0.2562, -0.2946, -0.6895,  0.1320,  0.0582, -0.2355]],
       dtype=torch.float64)
	q_value: tensor([[-22.6348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4436955605018438, distance: 0.8535185339548256 entropy 0.7110485434532166
epoch: 37, step: 43
	action: tensor([[ 0.2129, -1.1809,  0.4747, -0.7001, -1.0193, -0.6435, -0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-19.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9564604351607888, distance: 1.600634701660823 entropy 0.7110485434532166
epoch: 37, step: 44
	action: tensor([[ 0.3492,  0.0555,  0.4559, -0.8114,  0.2184, -0.4160,  0.1517]],
       dtype=torch.float64)
	q_value: tensor([[-25.2813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04338368162729045, distance: 1.1192460896825274 entropy 0.7110485434532166
epoch: 37, step: 45
	action: tensor([[-0.1300, -0.7513, -0.0718, -0.1518,  0.6950, -0.1794,  0.0526]],
       dtype=torch.float64)
	q_value: tensor([[-19.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6296742841247185, distance: 1.4608549603327845 entropy 0.7110485434532166
epoch: 37, step: 46
	action: tensor([[ 0.2621, -0.7671, -0.4470, -1.1350,  0.4514,  0.3575, -0.9050]],
       dtype=torch.float64)
	q_value: tensor([[-20.1187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4492273062462242, distance: 1.3776057520467526 entropy 0.7110485434532166
epoch: 37, step: 47
	action: tensor([[ 1.1910, -0.6784, -0.3314,  0.0352,  0.9433, -1.0914,  0.7996]],
       dtype=torch.float64)
	q_value: tensor([[-23.9879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09267525680328781, distance: 1.196195729590972 entropy 0.7110485434532166
epoch: 37, step: 48
	action: tensor([[ 0.4985, -0.7385, -0.1048,  0.1064,  0.3836, -0.3765,  0.7005]],
       dtype=torch.float64)
	q_value: tensor([[-23.7616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1554820046468759, distance: 1.2300939609695223 entropy 0.7110485434532166
epoch: 37, step: 49
	action: tensor([[-0.0311, -0.2963,  0.3318,  0.9447, -0.3527,  0.7316,  0.2558]],
       dtype=torch.float64)
	q_value: tensor([[-20.3000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 50
	action: tensor([[-0.3552, -0.5339, -0.3098, -0.3194,  0.1624,  0.3328,  0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5161146168901238, distance: 1.409038016301576 entropy 0.7110485434532166
epoch: 37, step: 51
	action: tensor([[ 0.7577,  0.3036,  0.2681, -0.3363,  0.5528, -0.0979,  0.8754]],
       dtype=torch.float64)
	q_value: tensor([[-19.9463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8040277587022657, distance: 0.506586921036681 entropy 0.7110485434532166
epoch: 37, step: 52
	action: tensor([[ 0.4590, -0.5808, -0.0107,  0.0365,  0.6151,  0.2160, -0.4972]],
       dtype=torch.float64)
	q_value: tensor([[-20.3008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18419198225243494, distance: 1.0335956619266342 entropy 0.7110485434532166
epoch: 37, step: 53
	action: tensor([[ 0.1556, -0.4414,  0.4991, -0.3211, -0.0519,  0.2576,  0.5991]],
       dtype=torch.float64)
	q_value: tensor([[-19.5996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09178657840454107, distance: 1.1957091945800016 entropy 0.7110485434532166
epoch: 37, step: 54
	action: tensor([[ 1.1437, -0.4811,  0.7618, -0.0055,  0.3938, -1.0260, -0.1128]],
       dtype=torch.float64)
	q_value: tensor([[-19.7875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2866945304654367, distance: 0.9664835584802348 entropy 0.7110485434532166
epoch: 37, step: 55
	action: tensor([[-0.2481,  0.2344,  0.1219,  0.2019,  0.5621, -0.0456,  0.2803]],
       dtype=torch.float64)
	q_value: tensor([[-22.0999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 56
	action: tensor([[ 0.6376, -0.7352,  1.0594, -0.5463,  1.5275, -0.5585, -0.5847]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1168827704288189, distance: 1.2093736139163789 entropy 0.7110485434532166
epoch: 37, step: 57
	action: tensor([[ 0.1308, -0.6006, -0.5408, -0.3474, -0.5842,  0.4083,  0.3608]],
       dtype=torch.float64)
	q_value: tensor([[-25.0392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2539756823168029, distance: 1.2814487758318525 entropy 0.7110485434532166
epoch: 37, step: 58
	action: tensor([[-0.1002, -0.9549, -0.4159, -0.2556,  0.5519,  0.6864, -0.3932]],
       dtype=torch.float64)
	q_value: tensor([[-21.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45355339554883445, distance: 1.3796603657612274 entropy 0.7110485434532166
epoch: 37, step: 59
	action: tensor([[ 0.5118, -1.3439,  0.7558, -0.0064,  0.2989,  0.2251, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-21.8601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 60
	action: tensor([[ 0.0989, -1.3168,  0.4171,  0.3754,  0.7821,  0.7469, -0.0687]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09414821986883504, distance: 1.1970017141344425 entropy 0.7110485434532166
epoch: 37, step: 61
	action: tensor([[ 0.1503, -0.0917, -0.0052, -0.8058,  0.6646, -0.1953,  0.5500]],
       dtype=torch.float64)
	q_value: tensor([[-22.4107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10118052560669377, distance: 1.2008422357554387 entropy 0.7110485434532166
epoch: 37, step: 62
	action: tensor([[ 0.2588,  0.4481, -0.0347, -0.6644, -0.1631, -0.2679,  0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-20.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.514697924001571, distance: 0.7971916990017254 entropy 0.7110485434532166
epoch: 37, step: 63
	action: tensor([[-0.4805,  0.4311,  0.3265, -0.9423, -0.4913,  0.0686,  0.5216]],
       dtype=torch.float64)
	q_value: tensor([[-19.7864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43292844143296727, distance: 1.3698371644645286 entropy 0.7110485434532166
epoch: 37, step: 64
	action: tensor([[-0.1443, -0.0273,  0.9974,  0.3407,  0.1548,  0.9070,  0.6830]],
       dtype=torch.float64)
	q_value: tensor([[-22.7970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6705395545688515, distance: 0.6568380965768573 entropy 0.7110485434532166
epoch: 37, step: 65
	action: tensor([[-0.6245, -0.3215, -0.5100,  0.0555,  0.6067,  1.0193,  0.0360]],
       dtype=torch.float64)
	q_value: tensor([[-22.8812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26445186340912863, distance: 1.2867904931485927 entropy 0.7110485434532166
epoch: 37, step: 66
	action: tensor([[-0.8710, -0.7787,  0.5061, -0.5043,  0.0770,  0.2823,  1.0471]],
       dtype=torch.float64)
	q_value: tensor([[-21.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.336317668263657, distance: 1.7491322204873767 entropy 0.7110485434532166
epoch: 37, step: 67
	action: tensor([[-0.6482, -0.1527, -0.9040, -0.4215,  0.3281,  1.3016,  0.9367]],
       dtype=torch.float64)
	q_value: tensor([[-22.9637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31262624552940776, distance: 1.311074090900004 entropy 0.7110485434532166
epoch: 37, step: 68
	action: tensor([[ 0.3145, -0.1508,  0.2596, -0.0467, -0.4022,  0.8214, -0.7340]],
       dtype=torch.float64)
	q_value: tensor([[-25.6529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5865973792992052, distance: 0.7357720836477811 entropy 0.7110485434532166
epoch: 37, step: 69
	action: tensor([[-0.5985, -0.4520, -0.4561, -0.5074,  0.4094,  0.1141,  0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-21.5196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6682178662751297, distance: 1.478029415201167 entropy 0.7110485434532166
epoch: 37, step: 70
	action: tensor([[-0.3071,  0.0389, -0.3467, -0.3847, -0.7603,  0.6772,  0.5211]],
       dtype=torch.float64)
	q_value: tensor([[-20.3004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1630557358674396, distance: 1.2341187681447852 entropy 0.7110485434532166
epoch: 37, step: 71
	action: tensor([[ 0.0054, -0.4645,  0.1075,  0.0777, -0.4322, -0.8516,  0.1590]],
       dtype=torch.float64)
	q_value: tensor([[-22.4615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14153924418344865, distance: 1.2226498994169306 entropy 0.7110485434532166
epoch: 37, step: 72
	action: tensor([[-0.4173,  0.0569, -0.3456, -0.4421, -0.0933,  0.7365, -0.5321]],
       dtype=torch.float64)
	q_value: tensor([[-21.3138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15587339143627332, distance: 1.2303022730688258 entropy 0.7110485434532166
epoch: 37, step: 73
	action: tensor([[-0.8623, -0.7838,  0.3760, -0.2941,  0.8291, -0.7554,  1.3292]],
       dtype=torch.float64)
	q_value: tensor([[-20.4206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3727489154012988, distance: 1.7627169678950634 entropy 0.7110485434532166
epoch: 37, step: 74
	action: tensor([[ 0.9491,  0.6345, -0.9796, -0.3543,  0.0694, -0.4647, -0.1189]],
       dtype=torch.float64)
	q_value: tensor([[-24.0224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8774769637972887, distance: 0.40055814619572316 entropy 0.7110485434532166
epoch: 37, step: 75
	action: tensor([[ 0.2703, -1.2310,  0.1950, -1.0421,  1.2671, -0.1511, -0.1087]],
       dtype=torch.float64)
	q_value: tensor([[-21.8981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.073696962480986, distance: 1.6478942525519733 entropy 0.7110485434532166
epoch: 37, step: 76
	action: tensor([[ 0.2317, -1.5178,  1.0335, -0.4353,  0.2495,  0.5082,  0.1684]],
       dtype=torch.float64)
	q_value: tensor([[-24.9025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 77
	action: tensor([[ 0.0584, -1.3187,  0.5195, -0.6493,  0.3737, -0.1480,  0.3670]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1501990078222963, distance: 1.6780157074798494 entropy 0.7110485434532166
epoch: 37, step: 78
	action: tensor([[-0.4267, -0.3306,  0.2518, -0.2828, -0.0892,  0.1628, -0.2232]],
       dtype=torch.float64)
	q_value: tensor([[-22.0962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5567400573708088, distance: 1.4277913402968974 entropy 0.7110485434532166
epoch: 37, step: 79
	action: tensor([[ 0.4343, -0.3286, -0.4086, -0.5111,  0.0501,  0.5161,  0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-19.5014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23491635518125797, distance: 1.0009471615427927 entropy 0.7110485434532166
epoch: 37, step: 80
	action: tensor([[ 0.5445, -0.0971,  0.1510,  0.3990,  0.7589,  0.6529,  0.1329]],
       dtype=torch.float64)
	q_value: tensor([[-20.2520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9258928741635145, distance: 0.31152053372528343 entropy 0.7110485434532166
epoch: 37, step: 81
	action: tensor([[ 0.2847, -0.1219, -0.5679, -0.1721,  0.3280, -0.4852, -0.3029]],
       dtype=torch.float64)
	q_value: tensor([[-19.2169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2965339109070626, distance: 0.9597945437570867 entropy 0.7110485434532166
epoch: 37, step: 82
	action: tensor([[-0.3905, -1.0934, -0.3776, -0.7860, -0.3449,  0.1647, -0.9490]],
       dtype=torch.float64)
	q_value: tensor([[-19.1332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8371256305054691, distance: 1.5510511547585888 entropy 0.7110485434532166
epoch: 37, step: 83
	action: tensor([[ 0.2783, -0.3287,  0.4341, -0.5804,  0.3164, -0.0290,  0.0768]],
       dtype=torch.float64)
	q_value: tensor([[-24.8506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17705859451517347, distance: 1.2415257589585662 entropy 0.7110485434532166
epoch: 37, step: 84
	action: tensor([[ 0.2877, -0.1811, -0.6649,  0.3115, -0.1165,  0.3662,  0.2065]],
       dtype=torch.float64)
	q_value: tensor([[-18.9873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4915561739477714, distance: 0.8159774977109279 entropy 0.7110485434532166
epoch: 37, step: 85
	action: tensor([[ 0.2169, -0.8571, -0.3034, -0.8855,  0.3914,  0.6855,  0.3616]],
       dtype=torch.float64)
	q_value: tensor([[-18.9420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.449470528956335, distance: 1.377721348456573 entropy 0.7110485434532166
epoch: 37, step: 86
	action: tensor([[ 0.2020, -0.9461, -0.1429,  0.5252,  0.3270,  0.4621, -0.2158]],
       dtype=torch.float64)
	q_value: tensor([[-22.8281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1331739537361818, distance: 1.0654244757273799 entropy 0.7110485434532166
epoch: 37, step: 87
	action: tensor([[-0.2710, -1.7778,  0.8347, -0.3146,  0.1835,  0.5996, -0.4819]],
       dtype=torch.float64)
	q_value: tensor([[-20.6091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 88
	action: tensor([[ 0.6503, -1.0094, -0.1753, -1.2680,  0.2521,  0.5688,  0.0920]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7619654401723126, distance: 1.5189916486781556 entropy 0.7110485434532166
epoch: 37, step: 89
	action: tensor([[ 0.0168, -0.4353,  0.3818, -0.1523, -0.2549,  0.4306,  0.1185]],
       dtype=torch.float64)
	q_value: tensor([[-24.1915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009218264511409613, distance: 1.1496065884353581 entropy 0.7110485434532166
epoch: 37, step: 90
	action: tensor([[ 1.2422, -0.0500,  0.1287,  0.2081, -0.1064,  0.0666,  1.4866]],
       dtype=torch.float64)
	q_value: tensor([[-19.7199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7225703831515398, distance: 0.6027442892523874 entropy 0.7110485434532166
epoch: 37, step: 91
	action: tensor([[-0.4181, -0.1468,  0.5581, -0.2365,  0.6904,  0.7968, -0.0871]],
       dtype=torch.float64)
	q_value: tensor([[-24.2747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09568214719194668, distance: 1.1978404810050507 entropy 0.7110485434532166
epoch: 37, step: 92
	action: tensor([[ 0.1292, -0.5631,  0.3250, -0.6535,  0.2534,  0.0718,  0.5966]],
       dtype=torch.float64)
	q_value: tensor([[-20.9848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5056200039197485, distance: 1.4041528357782045 entropy 0.7110485434532166
epoch: 37, step: 93
	action: tensor([[ 0.5849, -0.3008,  0.7162, -0.2158,  0.7876, -0.3584, -0.3247]],
       dtype=torch.float64)
	q_value: tensor([[-20.1386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2581086883049669, distance: 0.9856593268745312 entropy 0.7110485434532166
epoch: 37, step: 94
	action: tensor([[0.1837, 0.4123, 0.2724, 0.1739, 0.4609, 0.5508, 1.3401]],
       dtype=torch.float64)
	q_value: tensor([[-20.1255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 95
	action: tensor([[ 0.6534, -0.6119,  0.3660,  0.4609,  0.7133, -0.7024,  0.2905]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3468625016931016, distance: 0.9248238103537699 entropy 0.7110485434532166
epoch: 37, step: 96
	action: tensor([[ 0.1844, -0.0892,  0.1015, -0.6263,  0.9346,  0.0942,  0.0691]],
       dtype=torch.float64)
	q_value: tensor([[-20.4482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09325838396478847, distance: 1.089678714554922 entropy 0.7110485434532166
epoch: 37, step: 97
	action: tensor([[ 0.5560, -1.0468,  0.5666, -0.5165, -0.5820, -0.0746,  0.9732]],
       dtype=torch.float64)
	q_value: tensor([[-19.6495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7655076398914831, distance: 1.5205177484956276 entropy 0.7110485434532166
epoch: 37, step: 98
	action: tensor([[ 1.1026, -0.7214, -0.6470, -0.2314,  0.1589,  0.2355,  0.0622]],
       dtype=torch.float64)
	q_value: tensor([[-24.0817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12843822690359397, distance: 1.2156137063159889 entropy 0.7110485434532166
epoch: 37, step: 99
	action: tensor([[ 0.5210, -0.1597, -0.2137,  0.3941, -0.3417, -0.2133,  0.0176]],
       dtype=torch.float64)
	q_value: tensor([[-21.7407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7041739709983759, distance: 0.6224075973622435 entropy 0.7110485434532166
epoch: 37, step: 100
	action: tensor([[ 0.1728, -0.1253,  0.1446, -0.0156,  0.5608,  0.1622,  0.2084]],
       dtype=torch.float64)
	q_value: tensor([[-19.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4208759720836073, distance: 0.8708482538867703 entropy 0.7110485434532166
epoch: 37, step: 101
	action: tensor([[ 0.7867, -0.2679, -0.1743, -0.7977, -0.4914, -0.8703,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-18.0545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04602015741325127, distance: 1.1703795374882175 entropy 0.7110485434532166
epoch: 37, step: 102
	action: tensor([[ 0.4971,  0.1768,  0.5858, -0.0562,  0.6982,  0.8575,  0.4287]],
       dtype=torch.float64)
	q_value: tensor([[-22.9781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8728331381624179, distance: 0.40807845880917887 entropy 0.7110485434532166
epoch: 37, step: 103
	action: tensor([[ 1.2792, -0.9270, -0.1320, -0.3011,  0.1329,  0.5080,  0.4792]],
       dtype=torch.float64)
	q_value: tensor([[-20.3616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3431265973380737, distance: 1.3262187630880509 entropy 0.7110485434532166
epoch: 37, step: 104
	action: tensor([[ 0.7658, -0.6636,  0.3881, -0.1469,  0.3351,  0.2662,  0.3155]],
       dtype=torch.float64)
	q_value: tensor([[-22.6527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049144456763712485, distance: 1.1158709321804559 entropy 0.7110485434532166
epoch: 37, step: 105
	action: tensor([[ 1.2813, -0.9746,  0.2563, -0.2373,  0.1585,  0.9606,  0.4464]],
       dtype=torch.float64)
	q_value: tensor([[-19.5892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16012376934569783, distance: 1.2325622313008222 entropy 0.7110485434532166
epoch: 37, step: 106
	action: tensor([[ 0.8177,  0.8073,  0.0051,  0.2251,  1.0459, -0.0049,  0.8035]],
       dtype=torch.float64)
	q_value: tensor([[-23.2810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 107
	action: tensor([[ 0.2186, -1.0350,  0.1756, -0.8753, -0.0659,  0.1888,  0.2857]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9285340322129709, distance: 1.589169959332113 entropy 0.7110485434532166
epoch: 37, step: 108
	action: tensor([[-0.1928, -1.0306,  0.5239,  0.5954,  0.3034, -0.0114,  0.3631]],
       dtype=torch.float64)
	q_value: tensor([[-22.2466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26709733585551576, distance: 1.2881358943502124 entropy 0.7110485434532166
epoch: 37, step: 109
	action: tensor([[ 0.2274, -0.9101,  0.0590,  0.1271,  0.3065,  0.8704,  0.7750]],
       dtype=torch.float64)
	q_value: tensor([[-21.4218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10364905807848968, distance: 1.0834172170491727 entropy 0.7110485434532166
epoch: 37, step: 110
	action: tensor([[-0.1403, -0.4122, -1.0676, -0.2502, -0.1516,  0.6342,  0.7480]],
       dtype=torch.float64)
	q_value: tensor([[-21.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1819094573900064, distance: 1.2440814002328442 entropy 0.7110485434532166
epoch: 37, step: 111
	action: tensor([[ 0.5436, -0.2530,  0.8360, -0.1051,  0.4334,  0.4786, -0.6009]],
       dtype=torch.float64)
	q_value: tensor([[-22.4753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5169004782497417, distance: 0.7953806056290919 entropy 0.7110485434532166
epoch: 37, step: 112
	action: tensor([[ 0.9733, -0.0889, -0.0630, -1.0466,  0.3007, -0.5993, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-20.7054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007506107034062515, distance: 1.1486310100535189 entropy 0.7110485434532166
epoch: 37, step: 113
	action: tensor([[ 0.9782,  0.0608, -0.1155, -1.1202, -0.5280,  0.0744,  0.3810]],
       dtype=torch.float64)
	q_value: tensor([[-21.9411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19477704036532106, distance: 1.0268683494636985 entropy 0.7110485434532166
epoch: 37, step: 114
	action: tensor([[ 0.9907,  0.4127,  0.2060,  0.5210,  0.0091, -0.0215,  0.9522]],
       dtype=torch.float64)
	q_value: tensor([[-23.3320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9595695310199085, distance: 0.23009706697154128 entropy 0.7110485434532166
epoch: 37, step: 115
	action: tensor([[ 0.5742, -0.0025, -0.1690,  0.4495,  0.1669, -0.0339,  0.2655]],
       dtype=torch.float64)
	q_value: tensor([[-21.6472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8541288538441865, distance: 0.43706033163710806 entropy 0.7110485434532166
epoch: 37, step: 116
	action: tensor([[-0.4820, -0.1752, -0.3045, -0.2328,  0.3238,  0.0763,  0.3417]],
       dtype=torch.float64)
	q_value: tensor([[-18.4799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4162498592092281, distance: 1.3618417151962268 entropy 0.7110485434532166
epoch: 37, step: 117
	action: tensor([[-0.4227, -0.5999,  1.2619, -0.3457,  0.4572,  0.4835,  0.1471]],
       dtype=torch.float64)
	q_value: tensor([[-18.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7235483143917778, distance: 1.5023406701894915 entropy 0.7110485434532166
epoch: 37, step: 118
	action: tensor([[ 0.0721, -0.0145,  1.3537, -0.7150, -0.0662,  0.5237, -0.0348]],
       dtype=torch.float64)
	q_value: tensor([[-21.8636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09059664544195134, distance: 1.091276917571188 entropy 0.7110485434532166
epoch: 37, step: 119
	action: tensor([[-0.6913,  0.0644, -0.7237, -0.4711, -0.0405,  0.3825,  0.6330]],
       dtype=torch.float64)
	q_value: tensor([[-22.0060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41982794491899433, distance: 1.3635609430981026 entropy 0.7110485434532166
epoch: 37, step: 120
	action: tensor([[ 0.0803,  0.9699,  0.1049, -0.2538,  0.2467, -0.0718, -0.3050]],
       dtype=torch.float64)
	q_value: tensor([[-21.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 37, step: 121
	action: tensor([[ 0.5863, -0.6463,  0.1285,  0.1676,  1.0018, -0.3044,  0.7065]],
       dtype=torch.float64)
	q_value: tensor([[-26.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12522362270612064, distance: 1.0702992389773132 entropy 0.7110485434532166
epoch: 37, step: 122
	action: tensor([[-0.2703, -0.1004, -0.5405, -0.1958,  0.1862, -0.5479, -0.1592]],
       dtype=torch.float64)
	q_value: tensor([[-20.2190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1508676845719381, distance: 1.2276353656462886 entropy 0.7110485434532166
epoch: 37, step: 123
	action: tensor([[ 0.2992, -0.1848,  0.8711, -0.0520,  0.7327,  0.6556,  0.5835]],
       dtype=torch.float64)
	q_value: tensor([[-19.2042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5231078351692406, distance: 0.790254153020167 entropy 0.7110485434532166
epoch: 37, step: 124
	action: tensor([[ 0.3934, -0.0800,  0.1915, -0.0526, -0.1704,  0.1454,  0.5202]],
       dtype=torch.float64)
	q_value: tensor([[-20.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5521270760744462, distance: 0.7658330370026124 entropy 0.7110485434532166
epoch: 37, step: 125
	action: tensor([[ 0.7459, -0.7109,  0.1227,  0.2640,  0.1824, -0.2068,  0.3514]],
       dtype=torch.float64)
	q_value: tensor([[-19.0889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15315895651408362, distance: 1.0530709744668458 entropy 0.7110485434532166
epoch: 37, step: 126
	action: tensor([[ 1.1774,  0.0031, -0.3232,  0.6417,  0.1236, -0.2722,  0.1681]],
       dtype=torch.float64)
	q_value: tensor([[-19.8527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9073372404341175, distance: 0.3483447809274588 entropy 0.7110485434532166
epoch: 37, step: 127
	action: tensor([[ 0.1788, -0.1647,  0.9541,  0.3712, -0.3272, -0.3086,  0.1860]],
       dtype=torch.float64)
	q_value: tensor([[-20.6049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47564987981303364, distance: 0.8286428344341652 entropy 0.7110485434532166
LOSS epoch 37 actor 217.10932057587647 critic 354.7498942174577 
epoch: 38, step: 0
	action: tensor([[-0.0294, -0.0689, -0.6568,  0.1190, -0.3611,  0.7393, -0.0562]],
       dtype=torch.float64)
	q_value: tensor([[-20.1713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19505068026014816, distance: 1.0266938536794448 entropy 0.7110485434532166
epoch: 38, step: 1
	action: tensor([[ 0.9190, -0.2782,  0.4540,  0.5447, -0.0460,  0.2409, -0.1532]],
       dtype=torch.float64)
	q_value: tensor([[-19.0426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8144878255049726, distance: 0.4928819355931994 entropy 0.7110485434532166
epoch: 38, step: 2
	action: tensor([[ 0.2897,  0.1450,  0.4468,  0.8190, -0.3677,  0.0081,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[-18.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 3
	action: tensor([[ 0.1778, -0.3540,  0.2705, -1.2067,  0.8090, -0.6135,  1.1477]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5312674402453474, distance: 1.4160618326099161 entropy 0.7110485434532166
epoch: 38, step: 4
	action: tensor([[-0.3242, -0.4000,  0.3262,  0.1427,  0.1955, -0.1250, -0.2764]],
       dtype=torch.float64)
	q_value: tensor([[-21.9630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31120248546734275, distance: 1.3103628597388028 entropy 0.7110485434532166
epoch: 38, step: 5
	action: tensor([[ 0.3685, -0.4865,  0.3854, -0.6479,  0.7254,  0.7527,  0.5713]],
       dtype=torch.float64)
	q_value: tensor([[-18.8007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01587849246980899, distance: 1.153393703385882 entropy 0.7110485434532166
epoch: 38, step: 6
	action: tensor([[ 0.3845, -0.3580,  0.7470, -1.1349, -0.0162, -0.1499, -1.0535]],
       dtype=torch.float64)
	q_value: tensor([[-20.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3995125668610373, distance: 1.3537706508287788 entropy 0.7110485434532166
epoch: 38, step: 7
	action: tensor([[-0.2551, -0.6449,  0.9017, -0.2928,  0.1968,  0.2873, -0.2707]],
       dtype=torch.float64)
	q_value: tensor([[-22.4096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6372094266826573, distance: 1.4642283508494809 entropy 0.7110485434532166
epoch: 38, step: 8
	action: tensor([[ 0.4654, -0.2858,  0.2373, -0.1533, -0.2158, -0.1302,  0.1800]],
       dtype=torch.float64)
	q_value: tensor([[-19.9555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.250940091389393, distance: 0.9904098932892212 entropy 0.7110485434532166
epoch: 38, step: 9
	action: tensor([[-0.8715, -1.0331,  0.1828, -0.1659,  0.1873, -0.7833,  1.5786]],
       dtype=torch.float64)
	q_value: tensor([[-18.1373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2137263352384453, distance: 1.702623640136232 entropy 0.7110485434532166
epoch: 38, step: 10
	action: tensor([[-0.2640,  0.0183, -0.0098, -0.0639,  0.3574,  1.0006,  0.7522]],
       dtype=torch.float64)
	q_value: tensor([[-24.0895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35848736360478617, distance: 0.9165566251910557 entropy 0.7110485434532166
epoch: 38, step: 11
	action: tensor([[ 0.3871, -0.6225, -0.7877, -0.5712, -0.3740, -0.1936, -0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-20.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12700538233288583, distance: 1.2148416929612085 entropy 0.7110485434532166
epoch: 38, step: 12
	action: tensor([[ 0.0947,  0.3093, -0.3974, -0.7955,  0.9825, -0.4818,  0.8043]],
       dtype=torch.float64)
	q_value: tensor([[-20.8370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057731878547737, distance: 0.953470772194864 entropy 0.7110485434532166
epoch: 38, step: 13
	action: tensor([[ 0.9122, -0.4663,  0.7063,  0.0754,  0.8503, -0.0679,  0.5941]],
       dtype=torch.float64)
	q_value: tensor([[-20.6710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31355405321320085, distance: 0.9481124855621837 entropy 0.7110485434532166
epoch: 38, step: 14
	action: tensor([[ 0.4156, -0.9432, -0.2506, -0.1993,  0.0226,  0.3483, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-19.0058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3613255614343083, distance: 1.335173465760332 entropy 0.7110485434532166
epoch: 38, step: 15
	action: tensor([[ 0.3137, -0.7186,  0.2477,  0.2030,  0.0550,  0.8080,  0.1638]],
       dtype=torch.float64)
	q_value: tensor([[-19.8531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3752041581657165, distance: 0.9045357965929681 entropy 0.7110485434532166
epoch: 38, step: 16
	action: tensor([[ 0.0618, -1.0846, -0.1767,  0.5254,  0.5562,  0.7465,  0.1793]],
       dtype=torch.float64)
	q_value: tensor([[-19.2231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045719457496966376, distance: 1.117878819569908 entropy 0.7110485434532166
epoch: 38, step: 17
	action: tensor([[ 0.1458, -0.3084,  0.5174, -0.6437, -0.0850, -0.1204, -0.4618]],
       dtype=torch.float64)
	q_value: tensor([[-20.2508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3116339962203962, distance: 1.3105784592200902 entropy 0.7110485434532166
epoch: 38, step: 18
	action: tensor([[ 0.4932, -0.8930,  1.2275,  0.4188,  0.2104,  0.0256, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-19.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006647449793893978, distance: 1.1481414395399445 entropy 0.7110485434532166
epoch: 38, step: 19
	action: tensor([[ 0.2158, -0.4913,  0.7251, -0.3480, -0.1184, -0.3351,  0.4476]],
       dtype=torch.float64)
	q_value: tensor([[-21.1120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3075680082612233, distance: 1.3085455245744697 entropy 0.7110485434532166
epoch: 38, step: 20
	action: tensor([[ 0.4211, -0.4480, -0.6222,  0.3914,  0.2215,  0.6643, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-19.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5287417718887102, distance: 0.7855723087682204 entropy 0.7110485434532166
epoch: 38, step: 21
	action: tensor([[ 0.1803, -0.3143,  0.4005,  0.1781,  0.6259, -0.0598, -0.2543]],
       dtype=torch.float64)
	q_value: tensor([[-18.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117973506643016, distance: 0.9493248806527861 entropy 0.7110485434532166
epoch: 38, step: 22
	action: tensor([[-0.0994, -0.0728,  0.2756, -0.0410, -0.4633, -0.7152, -0.6065]],
       dtype=torch.float64)
	q_value: tensor([[-18.1994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024920843987121244, distance: 1.1299952805440299 entropy 0.7110485434532166
epoch: 38, step: 23
	action: tensor([[ 0.6246, -1.2746,  0.2351, -0.2601,  0.4299,  0.3000, -0.2331]],
       dtype=torch.float64)
	q_value: tensor([[-20.9887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7279104951434485, distance: 1.5042406285883962 entropy 0.7110485434532166
epoch: 38, step: 24
	action: tensor([[-1.6711e-01, -7.5619e-01,  6.8850e-04,  4.3985e-01,  4.2189e-01,
          2.9995e-01,  7.0278e-01]], dtype=torch.float64)
	q_value: tensor([[-20.8367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10820170187566336, distance: 1.204664464655354 entropy 0.7110485434532166
epoch: 38, step: 25
	action: tensor([[ 0.9767, -0.1061, -0.4618,  0.1364,  0.3762,  0.0072,  0.0760]],
       dtype=torch.float64)
	q_value: tensor([[-19.3697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7200130374936518, distance: 0.6055159644119457 entropy 0.7110485434532166
epoch: 38, step: 26
	action: tensor([[ 0.6194, -0.4604, -0.6736, -0.6562, -0.0293,  0.2047,  1.0794]],
       dtype=torch.float64)
	q_value: tensor([[-18.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0390908973169386, distance: 1.121754568687222 entropy 0.7110485434532166
epoch: 38, step: 27
	action: tensor([[ 0.2711, -0.2380, -0.4697,  0.4713,  0.0144, -0.0299,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-22.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5095112468791361, distance: 0.8014403795323927 entropy 0.7110485434532166
epoch: 38, step: 28
	action: tensor([[0.3619, 0.0645, 0.2326, 0.1920, 0.7916, 0.4246, 0.3890]],
       dtype=torch.float64)
	q_value: tensor([[-17.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8440158544867886, distance: 0.4519567972185639 entropy 0.7110485434532166
epoch: 38, step: 29
	action: tensor([[ 0.1582, -0.2514,  0.6275, -0.7644,  0.3936,  0.2121, -0.4071]],
       dtype=torch.float64)
	q_value: tensor([[-17.9712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2276390794696168, distance: 1.2679205643839526 entropy 0.7110485434532166
epoch: 38, step: 30
	action: tensor([[ 0.0303, -0.6752,  0.9669,  0.0682,  0.3313,  0.5541, -0.5710]],
       dtype=torch.float64)
	q_value: tensor([[-19.0547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0019737929273059285, distance: 1.1432143468764593 entropy 0.7110485434532166
epoch: 38, step: 31
	action: tensor([[ 0.2335, -0.3774, -0.1213, -1.4907,  0.2939, -0.0513,  0.1246]],
       dtype=torch.float64)
	q_value: tensor([[-20.8504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4026893567816685, distance: 1.3553062594720133 entropy 0.7110485434532166
epoch: 38, step: 32
	action: tensor([[ 0.3494, -0.7107, -0.1149,  0.3296,  1.0541,  0.2354, -0.4035]],
       dtype=torch.float64)
	q_value: tensor([[-21.6902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20810352338061378, distance: 1.0183355345018232 entropy 0.7110485434532166
epoch: 38, step: 33
	action: tensor([[ 0.8894, -0.2590,  0.6676, -1.0083,  0.1025,  0.2905, -0.4894]],
       dtype=torch.float64)
	q_value: tensor([[-19.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10014765279551063, distance: 1.085531224631913 entropy 0.7110485434532166
epoch: 38, step: 34
	action: tensor([[-0.4131,  0.4219, -0.5544,  0.6466, -0.7840,  0.5395,  1.2984]],
       dtype=torch.float64)
	q_value: tensor([[-20.5122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 35
	action: tensor([[-0.0602, -1.4316,  0.7312, -0.8116,  0.5468,  0.2561,  0.3170]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.185762102852559, distance: 1.6918355218179844 entropy 0.7110485434532166
epoch: 38, step: 36
	action: tensor([[ 0.6028,  0.2110, -0.9180, -0.2450,  0.1658,  0.4497,  0.6409]],
       dtype=torch.float64)
	q_value: tensor([[-21.4093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8636886563341482, distance: 0.42249608859459586 entropy 0.7110485434532166
epoch: 38, step: 37
	action: tensor([[ 0.3101, -0.4972, -0.2502, -0.3308, -0.1982, -0.1870, -0.6386]],
       dtype=torch.float64)
	q_value: tensor([[-20.2059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09183099386858329, distance: 1.195733515926165 entropy 0.7110485434532166
epoch: 38, step: 38
	action: tensor([[ 0.2752, -0.7066,  0.6666,  0.0971, -0.0583,  0.3752,  0.3895]],
       dtype=torch.float64)
	q_value: tensor([[-19.1212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08101754771849345, distance: 1.0970092830070413 entropy 0.7110485434532166
epoch: 38, step: 39
	action: tensor([[ 0.4158, -1.0580,  0.2852, -0.6026,  0.2109,  0.9853,  0.3524]],
       dtype=torch.float64)
	q_value: tensor([[-19.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4471911303758245, distance: 1.376637638366293 entropy 0.7110485434532166
epoch: 38, step: 40
	action: tensor([[-0.1953, -0.9659,  0.1821,  0.9800,  0.6849, -0.8487,  0.3878]],
       dtype=torch.float64)
	q_value: tensor([[-21.3956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32310676831771334, distance: 1.3162977495840416 entropy 0.7110485434532166
epoch: 38, step: 41
	action: tensor([[ 6.4411e-01, -1.1182e+00, -1.6488e-01, -1.7327e-01, -9.6233e-01,
          3.2672e-04, -3.5256e-01]], dtype=torch.float64)
	q_value: tensor([[-22.0059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6597565814259825, distance: 1.4742763304588238 entropy 0.7110485434532166
epoch: 38, step: 42
	action: tensor([[ 0.3030,  0.1206, -0.0892, -0.6093,  0.4461,  0.2969,  0.5801]],
       dtype=torch.float64)
	q_value: tensor([[-22.9741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46828295734767567, distance: 0.8344435913538256 entropy 0.7110485434532166
epoch: 38, step: 43
	action: tensor([[ 0.3965, -0.6054,  0.0789, -0.3790, -0.9115,  0.0197,  0.8773]],
       dtype=torch.float64)
	q_value: tensor([[-18.6766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25981971992016484, distance: 1.2844313415846902 entropy 0.7110485434532166
epoch: 38, step: 44
	action: tensor([[ 0.2522, -0.3770, -0.0065,  0.3501,  1.2058,  0.4574,  0.4586]],
       dtype=torch.float64)
	q_value: tensor([[-22.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5321612769945457, distance: 0.7827170172418753 entropy 0.7110485434532166
epoch: 38, step: 45
	action: tensor([[ 0.4891, -0.5508, -0.1272,  0.2260, -0.8030,  0.5756,  0.1882]],
       dtype=torch.float64)
	q_value: tensor([[-19.2003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3818027880473235, distance: 0.8997466002312575 entropy 0.7110485434532166
epoch: 38, step: 46
	action: tensor([[-0.1079, -0.5187, -0.2993, -0.5312, -0.2934, -0.2998,  0.3031]],
       dtype=torch.float64)
	q_value: tensor([[-20.7817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36124704224455817, distance: 1.335134959816777 entropy 0.7110485434532166
epoch: 38, step: 47
	action: tensor([[-0.8025, -0.6602,  0.0751,  0.0799, -0.1575,  0.1142, -0.4931]],
       dtype=torch.float64)
	q_value: tensor([[-19.8825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0445851653250493, distance: 1.6362863066047768 entropy 0.7110485434532166
epoch: 38, step: 48
	action: tensor([[-0.9970, -0.4060,  1.2774,  0.0690, -0.4475, -0.1085,  0.2088]],
       dtype=torch.float64)
	q_value: tensor([[-20.5181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2164062600590957, distance: 1.7036539217839004 entropy 0.7110485434532166
epoch: 38, step: 49
	action: tensor([[ 0.4630,  0.0317,  0.4877, -0.3389, -0.4949, -0.3555,  0.5341]],
       dtype=torch.float64)
	q_value: tensor([[-23.2500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3840993595386899, distance: 0.8980737885421437 entropy 0.7110485434532166
epoch: 38, step: 50
	action: tensor([[ 0.0018, -1.0154,  1.1916, -0.6124,  0.1112,  1.0042, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-19.6581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6059863474544764, distance: 1.4501990549430022 entropy 0.7110485434532166
epoch: 38, step: 51
	action: tensor([[ 0.7874, -0.1727, -0.3250, -1.2168, -0.3170, -0.5322,  0.3798]],
       dtype=torch.float64)
	q_value: tensor([[-22.7183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10241307462776095, distance: 1.2015140978852257 entropy 0.7110485434532166
epoch: 38, step: 52
	action: tensor([[ 0.4488, -0.8993,  0.4455, -0.2816, -0.1389,  0.3705, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-21.9806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36638805041402245, distance: 1.3376537793215302 entropy 0.7110485434532166
epoch: 38, step: 53
	action: tensor([[ 0.6393,  0.1771, -0.1780, -0.2349, -0.8402,  0.0667,  0.5066]],
       dtype=torch.float64)
	q_value: tensor([[-19.5230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7518251335331047, distance: 0.5700797200108461 entropy 0.7110485434532166
epoch: 38, step: 54
	action: tensor([[ 0.0721, -0.5177,  0.2245,  0.3797,  0.0097,  0.4627,  0.7176]],
       dtype=torch.float64)
	q_value: tensor([[-20.7430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3517976628321837, distance: 0.9213231622881579 entropy 0.7110485434532166
epoch: 38, step: 55
	action: tensor([[ 0.6776, -0.0273, -0.0606, -0.9449,  0.6349,  0.9012, -0.2919]],
       dtype=torch.float64)
	q_value: tensor([[-18.9979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48024620622893477, distance: 0.825002998997666 entropy 0.7110485434532166
epoch: 38, step: 56
	action: tensor([[ 0.9612, -0.9312,  0.1445, -0.9730,  0.8076,  0.0331, -0.7870]],
       dtype=torch.float64)
	q_value: tensor([[-20.6318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7867574871556537, distance: 1.5296409394452177 entropy 0.7110485434532166
epoch: 38, step: 57
	action: tensor([[ 0.3596, -0.4225, -0.2986, -0.4653, -0.2414,  0.8459,  0.5847]],
       dtype=torch.float64)
	q_value: tensor([[-23.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1795315246181799, distance: 1.0365437629026772 entropy 0.7110485434532166
epoch: 38, step: 58
	action: tensor([[ 0.1648, -0.9330, -0.4584,  0.2035,  0.5037, -0.0628,  0.5209]],
       dtype=torch.float64)
	q_value: tensor([[-20.7191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4061997498570342, distance: 1.3570011054321556 entropy 0.7110485434532166
epoch: 38, step: 59
	action: tensor([[-0.2020,  0.2556,  0.4748, -0.0095,  0.5962,  0.6713,  0.4017]],
       dtype=torch.float64)
	q_value: tensor([[-19.8812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 60
	action: tensor([[-0.0201, -0.6126,  0.8055, -1.0273, -0.1976, -0.2334,  0.3683]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8622950182387512, distance: 1.5616400341413137 entropy 0.7110485434532166
epoch: 38, step: 61
	action: tensor([[ 0.5571, -0.4942,  0.1371,  0.1424, -0.7224,  0.0736, -0.1497]],
       dtype=torch.float64)
	q_value: tensor([[-21.0999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3160780253913057, distance: 0.9463678374250988 entropy 0.7110485434532166
epoch: 38, step: 62
	action: tensor([[ 0.6907,  0.1819,  0.0098,  0.5190,  0.6869,  0.0280, -0.4755]],
       dtype=torch.float64)
	q_value: tensor([[-20.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9699688179101777, distance: 0.19830922025401862 entropy 0.7110485434532166
epoch: 38, step: 63
	action: tensor([[-0.2958, -0.4897,  0.9608, -0.4974,  0.6483, -0.1983,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-19.1022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8211362292383158, distance: 1.544286626123518 entropy 0.7110485434532166
epoch: 38, step: 64
	action: tensor([[ 1.6044, -0.6518, -0.7384, -1.3928, -0.0611,  0.3022, -0.3376]],
       dtype=torch.float64)
	q_value: tensor([[-19.8483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8307579324849728, distance: 1.5483607566550315 entropy 0.7110485434532166
epoch: 38, step: 65
	action: tensor([[-1.0145, -0.5529,  0.2338, -0.6345,  1.2650, -0.1511,  0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-25.8316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3987162723854896, distance: 1.7723363056043335 entropy 0.7110485434532166
epoch: 38, step: 66
	action: tensor([[ 0.1168, -0.0688,  0.2016, -0.2589, -0.6916, -0.3771,  0.4606]],
       dtype=torch.float64)
	q_value: tensor([[-22.4030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14619375351748198, distance: 1.0573928201775626 entropy 0.7110485434532166
epoch: 38, step: 67
	action: tensor([[ 0.7559, -0.4974, -0.6670,  0.9305,  0.5876,  0.1976,  0.5373]],
       dtype=torch.float64)
	q_value: tensor([[-19.9560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7345893558010674, distance: 0.5895435080050021 entropy 0.7110485434532166
epoch: 38, step: 68
	action: tensor([[ 0.4177, -0.3764,  1.1081,  0.1197,  0.9143,  0.4582,  0.6170]],
       dtype=torch.float64)
	q_value: tensor([[-19.8428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38973845273522667, distance: 0.8939530206196252 entropy 0.7110485434532166
epoch: 38, step: 69
	action: tensor([[-0.5815, -0.3849,  0.1921,  0.6519, -0.1771,  0.0880,  0.4363]],
       dtype=torch.float64)
	q_value: tensor([[-19.8749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18489532740491943, distance: 1.2456518767974212 entropy 0.7110485434532166
epoch: 38, step: 70
	action: tensor([[-0.0548, -0.3678,  0.4343,  0.2065,  0.3496,  0.1435, -0.5192]],
       dtype=torch.float64)
	q_value: tensor([[-20.1830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1365031104517972, distance: 1.0633765577725856 entropy 0.7110485434532166
epoch: 38, step: 71
	action: tensor([[-0.0843, -0.1275,  0.2644, -0.5489,  0.0405,  0.3670,  0.3594]],
       dtype=torch.float64)
	q_value: tensor([[-19.0844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11821060696562857, distance: 1.2100922987718736 entropy 0.7110485434532166
epoch: 38, step: 72
	action: tensor([[ 0.1729, -0.2693,  0.0695, -0.1074, -0.1322, -0.3584, -0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-18.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05266747232472524, distance: 1.1138018067233484 entropy 0.7110485434532166
epoch: 38, step: 73
	action: tensor([[ 0.0690, -0.1758,  0.5258,  0.9274,  0.3869, -0.5604, -0.2415]],
       dtype=torch.float64)
	q_value: tensor([[-18.0576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.665063692421053, distance: 0.6622741552919 entropy 0.7110485434532166
epoch: 38, step: 74
	action: tensor([[-0.5414,  0.4973, -0.1970, -0.3204,  0.9990, -0.2220, -0.6558]],
       dtype=torch.float64)
	q_value: tensor([[-20.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 75
	action: tensor([[ 1.2035, -1.0768,  0.5518, -0.0379, -0.0127,  0.4806, -0.0666]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34701699521752816, distance: 1.3281380862211472 entropy 0.7110485434532166
epoch: 38, step: 76
	action: tensor([[-0.1934, -1.1418, -0.1736, -0.1266, -0.2341,  0.4261, -0.7757]],
       dtype=torch.float64)
	q_value: tensor([[-21.1316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8008962549802545, distance: 1.5356811049808434 entropy 0.7110485434532166
epoch: 38, step: 77
	action: tensor([[ 0.6147, -0.4250, -0.0151, -0.4124,  0.7266,  1.3706,  0.3656]],
       dtype=torch.float64)
	q_value: tensor([[-22.3076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5537958455215221, distance: 0.7644049628987729 entropy 0.7110485434532166
epoch: 38, step: 78
	action: tensor([[ 0.4246,  0.6890, -0.1071,  0.2928, -0.2771, -0.1358,  0.3277]],
       dtype=torch.float64)
	q_value: tensor([[-21.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 79
	action: tensor([[ 0.9905, -0.5926,  0.4450,  0.0900,  0.9702, -0.3066, -0.2751]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21943175005474058, distance: 1.0110255578908776 entropy 0.7110485434532166
epoch: 38, step: 80
	action: tensor([[ 0.0698, -0.7645,  1.0061, -0.5753,  0.2553,  0.0388,  0.7098]],
       dtype=torch.float64)
	q_value: tensor([[-19.7796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6994449850577722, distance: 1.4917987821976235 entropy 0.7110485434532166
epoch: 38, step: 81
	action: tensor([[ 0.9041, -0.1313,  0.2471, -0.1956,  0.6165, -1.1080, -0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-19.9687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42881977785338543, distance: 0.8648549466844946 entropy 0.7110485434532166
epoch: 38, step: 82
	action: tensor([[ 0.3599,  0.5514, -0.0822, -0.2995,  1.7517, -0.0255,  0.4140]],
       dtype=torch.float64)
	q_value: tensor([[-20.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 83
	action: tensor([[ 0.4046, -1.2326, -0.1040,  0.0794,  1.0547,  0.1040,  1.8669]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5740442747025125, distance: 1.435704842707127 entropy 0.7110485434532166
epoch: 38, step: 84
	action: tensor([[ 0.2574, -0.5564, -0.3550,  0.2830, -0.1359,  0.9068,  0.2627]],
       dtype=torch.float64)
	q_value: tensor([[-23.9755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38690939090366583, distance: 0.8960227267178467 entropy 0.7110485434532166
epoch: 38, step: 85
	action: tensor([[-0.4052, -1.0393, -0.2650,  0.3662,  0.0085, -0.1512, -0.9321]],
       dtype=torch.float64)
	q_value: tensor([[-19.3012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8223073541008576, distance: 1.5447830913986087 entropy 0.7110485434532166
epoch: 38, step: 86
	action: tensor([[ 0.5563, -0.5122,  0.7268, -0.7267,  0.1789,  0.4498, -0.0751]],
       dtype=torch.float64)
	q_value: tensor([[-22.5348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11779745052679846, distance: 1.209868725676027 entropy 0.7110485434532166
epoch: 38, step: 87
	action: tensor([[-0.5188, -0.6601,  0.4080,  0.3018, -0.4516, -0.2590,  1.0915]],
       dtype=torch.float64)
	q_value: tensor([[-19.1864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6240969113427663, distance: 1.4583530138095886 entropy 0.7110485434532166
epoch: 38, step: 88
	action: tensor([[ 0.1627,  0.0306,  0.5126,  0.2027, -0.1414,  0.2253,  0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-21.5901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6549407246263432, distance: 0.6722077980316424 entropy 0.7110485434532166
epoch: 38, step: 89
	action: tensor([[ 0.0742, -0.9713,  0.2583, -1.3495, -0.4379,  0.4589, -0.3100]],
       dtype=torch.float64)
	q_value: tensor([[-18.2159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9428247562648238, distance: 1.5950470847294145 entropy 0.7110485434532166
epoch: 38, step: 90
	action: tensor([[ 0.6736,  0.1742, -0.0329, -0.2432,  0.7088, -0.0568, -0.2389]],
       dtype=torch.float64)
	q_value: tensor([[-23.6693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7361766678988146, distance: 0.587777955443887 entropy 0.7110485434532166
epoch: 38, step: 91
	action: tensor([[ 0.3499, -0.3479,  0.6299, -0.1954,  0.6922,  0.1966,  0.5789]],
       dtype=torch.float64)
	q_value: tensor([[-17.9566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21683995171340775, distance: 1.0127026711961866 entropy 0.7110485434532166
epoch: 38, step: 92
	action: tensor([[-0.3466, -0.3277, -0.5964,  0.3479,  0.0577, -0.2031, -0.5404]],
       dtype=torch.float64)
	q_value: tensor([[-18.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31130661421050076, distance: 1.310414889738106 entropy 0.7110485434532166
epoch: 38, step: 93
	action: tensor([[ 0.5432,  0.3750, -0.4961, -0.2913,  0.9745,  0.3987,  0.9205]],
       dtype=torch.float64)
	q_value: tensor([[-19.2467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9265628863667151, distance: 0.31010908729846987 entropy 0.7110485434532166
epoch: 38, step: 94
	action: tensor([[ 0.1848,  0.2099,  0.5212,  0.2441, -0.5308, -0.0768,  1.3631]],
       dtype=torch.float64)
	q_value: tensor([[-20.5120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 95
	action: tensor([[ 0.8832, -0.7610, -0.1519, -0.3704,  0.1222, -0.2456,  0.2535]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3935570287729371, distance: 1.3508871367904738 entropy 0.7110485434532166
epoch: 38, step: 96
	action: tensor([[ 0.6947, -0.5044, -0.2404, -0.4967,  0.6950,  0.2504,  0.3547]],
       dtype=torch.float64)
	q_value: tensor([[-19.5062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006513767356540301, distance: 1.1406111688328961 entropy 0.7110485434532166
epoch: 38, step: 97
	action: tensor([[ 0.7036, -0.0768,  0.2231, -0.2982,  0.3162, -0.5710,  0.3701]],
       dtype=torch.float64)
	q_value: tensor([[-19.3215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3638296894239692, distance: 0.9127322414188234 entropy 0.7110485434532166
epoch: 38, step: 98
	action: tensor([[ 0.0244, -0.4232,  0.6044,  0.2181, -0.7540,  0.2956,  0.5914]],
       dtype=torch.float64)
	q_value: tensor([[-18.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20976509722971648, distance: 1.0172666269839685 entropy 0.7110485434532166
epoch: 38, step: 99
	action: tensor([[-0.0718, -0.7369,  0.1813, -0.2398, -0.3953,  0.4897, -0.4359]],
       dtype=torch.float64)
	q_value: tensor([[-20.9271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4263561787549024, distance: 1.366692109781015 entropy 0.7110485434532166
epoch: 38, step: 100
	action: tensor([[ 0.8299,  0.4406, -0.0102, -0.4870,  0.2010, -0.3750,  0.6879]],
       dtype=torch.float64)
	q_value: tensor([[-20.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7818986612490448, distance: 0.5344238839403235 entropy 0.7110485434532166
epoch: 38, step: 101
	action: tensor([[-0.3724, -0.1152, -0.5139, -0.3819,  0.6761,  0.5022, -0.4444]],
       dtype=torch.float64)
	q_value: tensor([[-19.6376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05106240086185099, distance: 1.1731969994121132 entropy 0.7110485434532166
epoch: 38, step: 102
	action: tensor([[-0.6074, -0.0044,  0.3265, -0.4378,  0.1840, -0.1710, -0.1660]],
       dtype=torch.float64)
	q_value: tensor([[-19.1333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.735666702280237, distance: 1.5076129497717836 entropy 0.7110485434532166
epoch: 38, step: 103
	action: tensor([[-0.0996, -1.2286,  0.5706,  0.4718, -0.2315,  0.0666, -0.6620]],
       dtype=torch.float64)
	q_value: tensor([[-18.9931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4666770099929516, distance: 1.3858746010866612 entropy 0.7110485434532166
epoch: 38, step: 104
	action: tensor([[-0.3688,  0.2161,  0.5183,  0.0345,  0.2293, -0.5302,  0.8662]],
       dtype=torch.float64)
	q_value: tensor([[-23.1555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14613085243096213, distance: 1.225106361380314 entropy 0.7110485434532166
epoch: 38, step: 105
	action: tensor([[ 0.2909, -0.5902,  0.4618, -0.4023, -0.4814,  0.5303, -0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-19.6211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15559711929113806, distance: 1.2301552333656174 entropy 0.7110485434532166
epoch: 38, step: 106
	action: tensor([[ 0.5004,  0.1387,  0.1436, -0.4223,  0.8862,  0.2127,  0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-20.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6139741797819276, distance: 0.7109922559713309 entropy 0.7110485434532166
epoch: 38, step: 107
	action: tensor([[ 0.5864, -0.1080, -0.0689,  0.2404,  0.0141,  0.1912,  0.2099]],
       dtype=torch.float64)
	q_value: tensor([[-18.1259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7638926708566035, distance: 0.5560469046556186 entropy 0.7110485434532166
epoch: 38, step: 108
	action: tensor([[ 0.2708, -0.0478, -0.0497, -0.3524, -0.1695,  0.3244,  0.1033]],
       dtype=torch.float64)
	q_value: tensor([[-17.4433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38946693713169467, distance: 0.8941518658550988 entropy 0.7110485434532166
epoch: 38, step: 109
	action: tensor([[ 0.0056,  0.1875, -0.2600, -1.3944, -0.4108,  0.1824, -0.8476]],
       dtype=torch.float64)
	q_value: tensor([[-17.8349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08058633046724484, distance: 1.0972666295139408 entropy 0.7110485434532166
epoch: 38, step: 110
	action: tensor([[ 0.3379, -0.4200,  0.1317, -0.9516,  0.1205, -0.0380, -0.3802]],
       dtype=torch.float64)
	q_value: tensor([[-22.0750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3711387171228493, distance: 1.339977143207909 entropy 0.7110485434532166
epoch: 38, step: 111
	action: tensor([[ 0.4808,  0.1631, -0.4197, -0.8160,  0.3340,  0.2223,  0.2570]],
       dtype=torch.float64)
	q_value: tensor([[-19.7274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5385005820408582, distance: 0.7773959470090193 entropy 0.7110485434532166
epoch: 38, step: 112
	action: tensor([[ 0.1725, -1.0613, -0.5030, -0.1257, -0.5658, -0.2776, -0.1011]],
       dtype=torch.float64)
	q_value: tensor([[-19.1454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5676581434143471, distance: 1.432789448817196 entropy 0.7110485434532166
epoch: 38, step: 113
	action: tensor([[ 1.3804, -0.0558, -0.3464, -0.3790, -0.5363,  1.2089,  0.9988]],
       dtype=torch.float64)
	q_value: tensor([[-21.8385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6963140207213874, distance: 0.6306219223798398 entropy 0.7110485434532166
epoch: 38, step: 114
	action: tensor([[ 0.0941, -0.3677, -1.0010,  0.1626,  0.3952,  0.1405,  0.9146]],
       dtype=torch.float64)
	q_value: tensor([[-25.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06163458545205713, distance: 1.1085178468823875 entropy 0.7110485434532166
epoch: 38, step: 115
	action: tensor([[-0.0130,  0.2365,  0.1392, -0.5212, -0.5035, -0.7979,  0.4999]],
       dtype=torch.float64)
	q_value: tensor([[-20.3949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15894932103479553, distance: 1.0494645574187282 entropy 0.7110485434532166
epoch: 38, step: 116
	action: tensor([[-0.6211, -0.0484,  0.1615, -0.3030,  1.1909,  0.4750,  0.4095]],
       dtype=torch.float64)
	q_value: tensor([[-20.7755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45635468886690345, distance: 1.3809891690537675 entropy 0.7110485434532166
epoch: 38, step: 117
	action: tensor([[-0.4690, -0.1492,  0.7078,  0.0552,  0.2929,  0.4016,  0.9990]],
       dtype=torch.float64)
	q_value: tensor([[-20.8411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14964964518344037, distance: 1.2269855498136224 entropy 0.7110485434532166
epoch: 38, step: 118
	action: tensor([[ 0.7275, -0.5146, -0.0162,  0.0566,  0.2833,  0.0887,  0.0595]],
       dtype=torch.float64)
	q_value: tensor([[-20.2903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30912390226756314, distance: 0.9511670059743815 entropy 0.7110485434532166
epoch: 38, step: 119
	action: tensor([[-0.0494, -0.1311,  0.6648, -0.0432,  0.2985, -0.5058, -0.1804]],
       dtype=torch.float64)
	q_value: tensor([[-17.8853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07073773463727528, distance: 1.1841269005997224 entropy 0.7110485434532166
epoch: 38, step: 120
	action: tensor([[-0.0379, -0.0453, -0.5292,  0.3973, -0.6462,  0.0508, -0.2619]],
       dtype=torch.float64)
	q_value: tensor([[-19.2599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2274128873410255, distance: 1.0058435214819776 entropy 0.7110485434532166
epoch: 38, step: 121
	action: tensor([[ 0.4107, -1.4586,  0.2896,  0.0175,  0.2860, -0.3458,  0.3399]],
       dtype=torch.float64)
	q_value: tensor([[-19.6815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7821738871902508, distance: 1.5276776723223915 entropy 0.7110485434532166
epoch: 38, step: 122
	action: tensor([[-0.0828,  0.1720,  0.0143,  0.6761, -0.1195, -0.4399,  0.7734]],
       dtype=torch.float64)
	q_value: tensor([[-20.3383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 123
	action: tensor([[ 1.1390, -1.2856,  0.1557, -0.7858, -0.8980,  1.1892,  0.2330]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4788113004999519, distance: 1.3915956854662535 entropy 0.7110485434532166
epoch: 38, step: 124
	action: tensor([[-0.1707,  0.0935, -0.6261,  0.3427,  0.0202,  0.2143, -0.2999]],
       dtype=torch.float64)
	q_value: tensor([[-25.8566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 38, step: 125
	action: tensor([[-0.0919, -0.3517,  0.6708, -0.5563, -0.3328, -0.2662,  0.4111]],
       dtype=torch.float64)
	q_value: tensor([[-25.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5534344379363401, distance: 1.4262746314077466 entropy 0.7110485434532166
epoch: 38, step: 126
	action: tensor([[-0.1508, -0.0449, -0.0345,  0.4516, -0.2358, -0.3689, -0.8641]],
       dtype=torch.float64)
	q_value: tensor([[-19.7282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24213961118277638, distance: 0.9962109188107602 entropy 0.7110485434532166
epoch: 38, step: 127
	action: tensor([[0.0879, 0.1058, 0.0640, 0.1047, 0.4941, 0.2879, 1.1150]],
       dtype=torch.float64)
	q_value: tensor([[-20.6516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6038529155778327, distance: 0.7202527501688172 entropy 0.7110485434532166
LOSS epoch 38 actor 193.8919965337431 critic 307.2842700090196 
epoch: 39, step: 0
	action: tensor([[ 0.5108, -0.2832,  0.8082, -0.8503, -0.5224,  0.5474,  0.2567]],
       dtype=torch.float64)
	q_value: tensor([[-20.5476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12174438322241943, distance: 1.072425572133857 entropy 0.7110485434532166
epoch: 39, step: 1
	action: tensor([[ 0.4811, -0.2732,  0.1372, -0.8284,  0.5224,  0.4920,  0.6536]],
       dtype=torch.float64)
	q_value: tensor([[-22.0750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08884715431953016, distance: 1.0923261010507792 entropy 0.7110485434532166
epoch: 39, step: 2
	action: tensor([[ 0.2615,  1.0931,  0.1396, -0.5829, -0.7117, -0.0123,  0.7595]],
       dtype=torch.float64)
	q_value: tensor([[-20.9673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 3
	action: tensor([[-0.3291, -0.9290,  0.4694,  0.5851,  0.0811,  0.0745,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2798797566367994, distance: 1.2946169187006282 entropy 0.7110485434532166
epoch: 39, step: 4
	action: tensor([[ 3.0229e-01,  9.3272e-02, -4.9555e-01, -6.3360e-02,  1.9659e-04,
          1.0818e+00, -3.0600e-01]], dtype=torch.float64)
	q_value: tensor([[-21.8753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 5
	action: tensor([[ 0.4629, -1.3215,  0.9999,  0.2512,  0.5918,  0.0577, -0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45999984697194574, distance: 1.3827163506062394 entropy 0.7110485434532166
epoch: 39, step: 6
	action: tensor([[ 1.0472, -0.7929, -0.3066,  0.6267,  0.2010,  0.5158, -0.5680]],
       dtype=torch.float64)
	q_value: tensor([[-22.5613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6004818062793145, distance: 0.7233108404337846 entropy 0.7110485434532166
epoch: 39, step: 7
	action: tensor([[-0.0697,  0.5486, -0.5024,  0.3069, -0.2454,  0.4417, -0.1711]],
       dtype=torch.float64)
	q_value: tensor([[-22.4923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 8
	action: tensor([[-0.0118, -0.8396,  1.0277, -0.3857,  0.1426,  0.4421,  0.1682]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5749531479603622, distance: 1.4361192800423226 entropy 0.7110485434532166
epoch: 39, step: 9
	action: tensor([[-0.1683, -0.7695, -0.0089,  0.4851, -0.6932, -0.6183, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-21.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2587906421708004, distance: 1.2839066435957738 entropy 0.7110485434532166
epoch: 39, step: 10
	action: tensor([[ 0.8229, -0.8287,  0.7166, -1.3763,  0.8982,  0.1386,  0.3866]],
       dtype=torch.float64)
	q_value: tensor([[-23.5212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5870454870710291, distance: 1.4416219310310543 entropy 0.7110485434532166
epoch: 39, step: 11
	action: tensor([[ 0.8006, -0.0342, -0.3192,  0.3268,  1.3147,  0.0118, -0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-23.8598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8426657197204048, distance: 0.45390855920944057 entropy 0.7110485434532166
epoch: 39, step: 12
	action: tensor([[ 1.2113,  0.0369,  1.2687,  0.2426,  1.3297, -0.2226,  0.6805]],
       dtype=torch.float64)
	q_value: tensor([[-21.2810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7138146032958167, distance: 0.6121818194677798 entropy 0.7110485434532166
epoch: 39, step: 13
	action: tensor([[-0.5292, -0.4349,  1.1012,  0.2996,  0.5136,  0.5694,  0.9027]],
       dtype=torch.float64)
	q_value: tensor([[-23.5398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0735083859788146, distance: 1.1856579399298264 entropy 0.7110485434532166
epoch: 39, step: 14
	action: tensor([[ 0.2278,  0.1281,  0.4816, -0.2134, -0.1813, -1.0087,  0.9469]],
       dtype=torch.float64)
	q_value: tensor([[-22.7238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2724965272969735, distance: 0.976054860892493 entropy 0.7110485434532166
epoch: 39, step: 15
	action: tensor([[-0.1493, -0.8249, -0.7670, -0.3515,  0.5726, -0.7443,  0.3542]],
       dtype=torch.float64)
	q_value: tensor([[-21.7658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5599593998255128, distance: 1.4292669159049225 entropy 0.7110485434532166
epoch: 39, step: 16
	action: tensor([[ 0.5857, -0.5968,  0.5351,  0.5460,  0.4863,  0.3308, -0.4553]],
       dtype=torch.float64)
	q_value: tensor([[-22.9257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5977069311570786, distance: 0.7258183910765774 entropy 0.7110485434532166
epoch: 39, step: 17
	action: tensor([[ 0.7007, -0.3524,  0.4014,  0.1987,  0.7517,  0.1147,  0.2096]],
       dtype=torch.float64)
	q_value: tensor([[-21.5909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5843190082082327, distance: 0.7377968149698773 entropy 0.7110485434532166
epoch: 39, step: 18
	action: tensor([[-0.8576, -0.0714,  1.1004,  0.0442, -0.0513,  0.2072, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[-19.3123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6916865959497529, distance: 1.488389670617057 entropy 0.7110485434532166
epoch: 39, step: 19
	action: tensor([[ 0.7831, -0.5675, -0.4882, -0.4526,  0.6222,  0.2696, -0.2443]],
       dtype=torch.float64)
	q_value: tensor([[-22.9614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008976162370775143, distance: 1.149468690163265 entropy 0.7110485434532166
epoch: 39, step: 20
	action: tensor([[-0.1194, -0.5281,  1.2104, -0.3116,  0.8916,  0.6756,  0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-21.0736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3321477430171982, distance: 1.3207873170378301 entropy 0.7110485434532166
epoch: 39, step: 21
	action: tensor([[ 0.1630,  0.0794,  0.2029, -0.4657,  0.2853,  0.1074,  0.5620]],
       dtype=torch.float64)
	q_value: tensor([[-22.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28395414996938706, distance: 0.9683382994674927 entropy 0.7110485434532166
epoch: 39, step: 22
	action: tensor([[-1.1511e-01, -4.8339e-01, -1.1831e-04,  3.8860e-01,  6.1033e-04,
          5.6332e-01, -7.1314e-01]], dtype=torch.float64)
	q_value: tensor([[-18.7961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20020013175273066, distance: 1.023404582773884 entropy 0.7110485434532166
epoch: 39, step: 23
	action: tensor([[-0.0058, -0.9443,  0.4454, -0.2305, -0.0588,  0.4627,  0.9075]],
       dtype=torch.float64)
	q_value: tensor([[-21.3342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5316951408963477, distance: 1.4162595800008535 entropy 0.7110485434532166
epoch: 39, step: 24
	action: tensor([[ 0.8056, -1.2701, -0.0610, -0.4721,  0.5799, -0.2870, -0.4723]],
       dtype=torch.float64)
	q_value: tensor([[-21.8386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9641164847674304, distance: 1.6037634573458013 entropy 0.7110485434532166
epoch: 39, step: 25
	action: tensor([[-0.2051, -1.7658, -0.1771, -0.2967, -0.0558, -1.2447, -0.2975]],
       dtype=torch.float64)
	q_value: tensor([[-23.7645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 26
	action: tensor([[-0.2658, -0.4298,  0.3595, -0.8480,  0.4956, -0.0374,  0.2228]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.82252007017854, distance: 1.54487324924454 entropy 0.7110485434532166
epoch: 39, step: 27
	action: tensor([[ 0.3461,  0.1548, -0.4048, -1.0524,  0.2170, -0.3897, -0.1948]],
       dtype=torch.float64)
	q_value: tensor([[-20.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2609475625915364, distance: 0.9837716886004236 entropy 0.7110485434532166
epoch: 39, step: 28
	action: tensor([[-0.4023, -0.1600,  0.2577, -0.9152,  0.1000,  0.5631,  0.4438]],
       dtype=torch.float64)
	q_value: tensor([[-20.9294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5816776129428098, distance: 1.43918186246753 entropy 0.7110485434532166
epoch: 39, step: 29
	action: tensor([[ 0.8594,  0.5034, -0.5254,  0.3303,  0.7836,  0.5295,  0.9181]],
       dtype=torch.float64)
	q_value: tensor([[-21.4844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.970956222957607, distance: 0.19502183584380592 entropy 0.7110485434532166
epoch: 39, step: 30
	action: tensor([[ 0.4121, -0.2849, -0.0342, -0.1115,  0.0338, -0.3198,  0.6132]],
       dtype=torch.float64)
	q_value: tensor([[-22.2016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21214704593906109, distance: 1.0157323328109489 entropy 0.7110485434532166
epoch: 39, step: 31
	action: tensor([[ 0.3366, -0.6954, -0.3309, -0.0555,  0.1354,  0.8233,  0.6296]],
       dtype=torch.float64)
	q_value: tensor([[-19.3776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16584913634763954, distance: 1.0451508910706793 entropy 0.7110485434532166
epoch: 39, step: 32
	action: tensor([[-0.0520, -0.8617,  0.5308, -0.5339,  0.1286,  0.1596,  0.2981]],
       dtype=torch.float64)
	q_value: tensor([[-21.1929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.846680823814185, distance: 1.5550795598484821 entropy 0.7110485434532166
epoch: 39, step: 33
	action: tensor([[ 0.5309,  0.0368,  0.0796, -0.4204,  0.6142, -0.1629, -0.1869]],
       dtype=torch.float64)
	q_value: tensor([[-20.5482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4425570022448071, distance: 0.8543915126611495 entropy 0.7110485434532166
epoch: 39, step: 34
	action: tensor([[-0.8042, -0.2703,  0.2689,  0.1290,  0.3099,  0.4083, -0.6506]],
       dtype=torch.float64)
	q_value: tensor([[-19.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5963666467321389, distance: 1.4458492563837329 entropy 0.7110485434532166
epoch: 39, step: 35
	action: tensor([[ 1.3528, -0.9461,  0.6472,  1.0105, -0.4587,  0.7463, -0.0820]],
       dtype=torch.float64)
	q_value: tensor([[-21.6830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4035202097186311, distance: 0.8838011454450057 entropy 0.7110485434532166
epoch: 39, step: 36
	action: tensor([[-0.0946, -0.5757, -0.0923,  0.3438,  0.4791, -0.5957,  0.6469]],
       dtype=torch.float64)
	q_value: tensor([[-25.2959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3112275464316061, distance: 1.3103753821372324 entropy 0.7110485434532166
epoch: 39, step: 37
	action: tensor([[-0.2106,  0.6504,  0.9598, -0.0371,  0.3551,  0.1043,  0.6260]],
       dtype=torch.float64)
	q_value: tensor([[-20.3449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 38
	action: tensor([[-0.2439, -0.0065,  1.4444,  0.7904, -0.7954, -0.3593,  0.9035]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 39
	action: tensor([[ 0.6991, -1.7330,  0.2656, -0.6198,  0.4175, -0.3867, -0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.004753145159758, distance: 1.6202690825608257 entropy 0.7110485434532166
epoch: 39, step: 40
	action: tensor([[ 0.6330, -0.8298,  0.6886,  0.4726,  0.5742,  0.2052, -0.6196]],
       dtype=torch.float64)
	q_value: tensor([[-22.7793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22805674189608593, distance: 1.0054243118514294 entropy 0.7110485434532166
epoch: 39, step: 41
	action: tensor([[ 0.7273, -0.3487, -0.2317, -0.2622, -0.0296,  0.0638,  0.6580]],
       dtype=torch.float64)
	q_value: tensor([[-22.7389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2667978024786316, distance: 0.9798702501299076 entropy 0.7110485434532166
epoch: 39, step: 42
	action: tensor([[-0.4263,  0.0211,  0.7479,  0.5223, -0.2063,  0.0332,  0.4902]],
       dtype=torch.float64)
	q_value: tensor([[-20.2133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 43
	action: tensor([[ 0.2004, -1.0118,  0.5886, -0.3999,  0.4864, -0.1975,  0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.869322612680768, distance: 1.5645837779461698 entropy 0.7110485434532166
epoch: 39, step: 44
	action: tensor([[-0.2085,  0.0120,  0.2841, -0.2804,  0.1446,  0.4532, -0.4721]],
       dtype=torch.float64)
	q_value: tensor([[-20.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09796472308839022, distance: 1.0868471088401552 entropy 0.7110485434532166
epoch: 39, step: 45
	action: tensor([[ 0.0163, -0.3013,  0.8295, -0.6416, -0.1208, -0.2521,  0.5662]],
       dtype=torch.float64)
	q_value: tensor([[-19.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4699569665122174, distance: 1.387423363998571 entropy 0.7110485434532166
epoch: 39, step: 46
	action: tensor([[0.3062, 0.0404, 0.2927, 0.2200, 0.4543, 0.4345, 0.6351]],
       dtype=torch.float64)
	q_value: tensor([[-20.7079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7998911518372573, distance: 0.5119055514342237 entropy 0.7110485434532166
epoch: 39, step: 47
	action: tensor([[-0.0958, -1.5211,  0.2221,  0.3656,  0.5472,  0.7389,  0.8170]],
       dtype=torch.float64)
	q_value: tensor([[-19.1757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16430260737864866, distance: 1.2347801187949776 entropy 0.7110485434532166
epoch: 39, step: 48
	action: tensor([[ 0.1605, -0.4535, -0.0097,  0.1373,  0.6269,  0.2376, -0.3625]],
       dtype=torch.float64)
	q_value: tensor([[-22.5552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21730011350329936, distance: 1.0124051103413187 entropy 0.7110485434532166
epoch: 39, step: 49
	action: tensor([[ 0.2563, -0.2697, -0.1130, -0.2350,  0.0722,  0.0774,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[-19.4369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16917913960393327, distance: 1.043062637947503 entropy 0.7110485434532166
epoch: 39, step: 50
	action: tensor([[ 0.5384, -0.1410,  0.0650,  0.1784, -0.5611, -0.0120,  0.4715]],
       dtype=torch.float64)
	q_value: tensor([[-18.2597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6566541651607507, distance: 0.6705367496442027 entropy 0.7110485434532166
epoch: 39, step: 51
	action: tensor([[ 0.0860,  0.4895,  0.4307,  0.1974,  0.2384,  0.2842, -0.4701]],
       dtype=torch.float64)
	q_value: tensor([[-20.2994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 52
	action: tensor([[-0.0796, -0.9051,  0.9896, -0.1233, -0.1956, -0.3953,  0.6842]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8100713160083732, distance: 1.5395880672173199 entropy 0.7110485434532166
epoch: 39, step: 53
	action: tensor([[-0.3368, -0.2608, -0.1200, -0.4770,  0.1018, -1.3399, -0.3898]],
       dtype=torch.float64)
	q_value: tensor([[-22.0137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3433077425540121, distance: 1.3263081925152969 entropy 0.7110485434532166
epoch: 39, step: 54
	action: tensor([[ 0.8504, -0.0735,  0.0897,  0.2041,  0.9928,  0.0749,  0.6478]],
       dtype=torch.float64)
	q_value: tensor([[-23.0713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7783648711121404, distance: 0.5387359927777281 entropy 0.7110485434532166
epoch: 39, step: 55
	action: tensor([[-0.4043, -0.0466, -0.1646,  0.0364,  0.4849, -1.1460,  0.3403]],
       dtype=torch.float64)
	q_value: tensor([[-19.9113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3828102268240978, distance: 1.345668184852643 entropy 0.7110485434532166
epoch: 39, step: 56
	action: tensor([[-0.0444,  0.0058, -0.6520, -0.3396,  0.7029,  0.3059,  0.6773]],
       dtype=torch.float64)
	q_value: tensor([[-21.2215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32550611878115654, distance: 0.9398222019601322 entropy 0.7110485434532166
epoch: 39, step: 57
	action: tensor([[ 0.2869, -0.0478, -0.3183, -0.4115, -0.1081,  0.4485,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[-20.5089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4168030351155504, distance: 0.8739051945593364 entropy 0.7110485434532166
epoch: 39, step: 58
	action: tensor([[-0.5817, -0.6945, -0.3988, -0.0774,  0.4284, -0.4209, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-19.1729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.887097966615862, distance: 1.5720049766906068 entropy 0.7110485434532166
epoch: 39, step: 59
	action: tensor([[ 0.6738,  1.0052,  0.6049, -0.8084,  0.7594,  0.0272, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-21.1607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 60
	action: tensor([[ 0.8144, -0.6858,  0.7358, -0.4301,  0.7113,  0.1505,  1.1989]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18364277766653103, distance: 1.244993315046843 entropy 0.7110485434532166
epoch: 39, step: 61
	action: tensor([[-0.1141,  0.3421,  0.2396, -0.1457, -0.3627, -1.1979, -0.2373]],
       dtype=torch.float64)
	q_value: tensor([[-21.6505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2503578133995179, distance: 0.9907947636088031 entropy 0.7110485434532166
epoch: 39, step: 62
	action: tensor([[ 0.6093, -0.2021, -0.0178, -1.2501,  0.8280, -0.0289,  0.3703]],
       dtype=torch.float64)
	q_value: tensor([[-22.6182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1549827434251576, distance: 1.2298281824611028 entropy 0.7110485434532166
epoch: 39, step: 63
	action: tensor([[ 0.3983,  0.1717,  0.3944,  0.1703, -0.2561, -0.4187, -0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-22.2596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7403969189931618, distance: 0.5830578065469688 entropy 0.7110485434532166
epoch: 39, step: 64
	action: tensor([[-0.3189,  0.1291,  0.9487, -0.6392,  0.4716, -0.1981,  0.0684]],
       dtype=torch.float64)
	q_value: tensor([[-19.8095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5141408912530121, distance: 1.4081205526312521 entropy 0.7110485434532166
epoch: 39, step: 65
	action: tensor([[ 1.0708, -0.0628, -0.5082,  0.3172,  0.9390,  0.5165, -1.1540]],
       dtype=torch.float64)
	q_value: tensor([[-20.9751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8836671369731959, distance: 0.39030840313679427 entropy 0.7110485434532166
epoch: 39, step: 66
	action: tensor([[ 0.4352, -0.5097,  0.8080, -0.4774,  0.4463,  0.0600, -0.6158]],
       dtype=torch.float64)
	q_value: tensor([[-23.8318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16459104251228696, distance: 1.2349330566615428 entropy 0.7110485434532166
epoch: 39, step: 67
	action: tensor([[-0.1764, -0.8441,  0.0773,  0.3278,  0.4030, -0.1586,  0.4122]],
       dtype=torch.float64)
	q_value: tensor([[-21.2495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4625158840717669, distance: 1.3839072644604618 entropy 0.7110485434532166
epoch: 39, step: 68
	action: tensor([[ 0.6604, -0.2483,  0.4778,  0.1762,  0.0513, -0.7755,  0.3820]],
       dtype=torch.float64)
	q_value: tensor([[-20.3141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4362521775230628, distance: 0.8592096221276264 entropy 0.7110485434532166
epoch: 39, step: 69
	action: tensor([[-0.6856, -0.2575,  0.1683,  1.2378,  0.9113,  1.0312, -0.5509]],
       dtype=torch.float64)
	q_value: tensor([[-20.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 70
	action: tensor([[ 0.6542, -0.4854, -0.0098, -0.9907,  0.4381, -0.8010,  0.6914]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36843335267211086, distance: 1.338654550382725 entropy 0.7110485434532166
epoch: 39, step: 71
	action: tensor([[ 0.6530,  0.1836,  0.3630,  0.5100,  0.3826, -0.2685, -0.1340]],
       dtype=torch.float64)
	q_value: tensor([[-22.4226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9559609338421301, distance: 0.24014621382724746 entropy 0.7110485434532166
epoch: 39, step: 72
	action: tensor([[ 0.6028, -0.6660, -0.6579,  0.2289, -0.6510,  0.6303,  0.5249]],
       dtype=torch.float64)
	q_value: tensor([[-19.6815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21231663905013431, distance: 1.01562300373077 entropy 0.7110485434532166
epoch: 39, step: 73
	action: tensor([[-0.1725, -0.5025, -0.5507, -0.7152,  0.1162,  0.2597,  0.3432]],
       dtype=torch.float64)
	q_value: tensor([[-22.6296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28625594812994337, distance: 1.297837717327885 entropy 0.7110485434532166
epoch: 39, step: 74
	action: tensor([[ 0.3013, -0.2999,  0.5200,  0.3079,  0.3754,  0.5695, -0.5361]],
       dtype=torch.float64)
	q_value: tensor([[-20.6817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6846751548426846, distance: 0.6425927018955329 entropy 0.7110485434532166
epoch: 39, step: 75
	action: tensor([[ 0.5729, -0.2509, -0.4928, -0.6590, -0.8706, -1.1546,  1.0022]],
       dtype=torch.float64)
	q_value: tensor([[-20.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30705413621124245, distance: 0.9525907206701113 entropy 0.7110485434532166
epoch: 39, step: 76
	action: tensor([[-0.6102, -1.4580,  0.6568, -0.3531,  0.9378,  0.7214,  0.2780]],
       dtype=torch.float64)
	q_value: tensor([[-26.1743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9524476937479922, distance: 1.598992391442573 entropy 0.7110485434532166
epoch: 39, step: 77
	action: tensor([[-0.2717, -0.3837, -0.3469,  0.4380,  0.1682,  0.9864,  0.6748]],
       dtype=torch.float64)
	q_value: tensor([[-24.3192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1389701701034599, distance: 1.0618564075995918 entropy 0.7110485434532166
epoch: 39, step: 78
	action: tensor([[-0.0560, -0.5647,  0.2498,  0.9654, -1.1166, -0.2351,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-21.3476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45152537227266176, distance: 0.8474907451782957 entropy 0.7110485434532166
epoch: 39, step: 79
	action: tensor([[-0.2448, -0.1153,  0.5407, -0.9900, -0.1577,  0.9234,  0.0999]],
       dtype=torch.float64)
	q_value: tensor([[-26.1151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34651273747992906, distance: 1.3278894677557844 entropy 0.7110485434532166
epoch: 39, step: 80
	action: tensor([[-0.0590, -0.0784, -0.7376,  0.8999, -0.3494, -0.2132,  0.3465]],
       dtype=torch.float64)
	q_value: tensor([[-22.4291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 81
	action: tensor([[-0.0455, -1.0410,  1.0485, -0.5030, -0.5584,  0.5486,  1.1455]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8266679666347099, distance: 1.5466302483926575 entropy 0.7110485434532166
epoch: 39, step: 82
	action: tensor([[ 0.3189,  0.2938,  0.2635, -0.2440,  0.3781,  0.0686,  0.8581]],
       dtype=torch.float64)
	q_value: tensor([[-24.6790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6482004328197879, distance: 0.678741405900051 entropy 0.7110485434532166
epoch: 39, step: 83
	action: tensor([[ 0.8298, -1.1708,  0.2974,  0.2894, -0.5843,  0.5530, -0.6891]],
       dtype=torch.float64)
	q_value: tensor([[-19.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09185924988061922, distance: 1.195748988303399 entropy 0.7110485434532166
epoch: 39, step: 84
	action: tensor([[-0.1181, -1.0404,  0.1085, -0.8471, -0.2709,  0.6015,  0.4566]],
       dtype=torch.float64)
	q_value: tensor([[-24.7672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9289289301440533, distance: 1.5893326548812121 entropy 0.7110485434532166
epoch: 39, step: 85
	action: tensor([[ 0.1566, -0.4199,  1.0032, -0.6410, -0.3283,  0.6438,  0.3977]],
       dtype=torch.float64)
	q_value: tensor([[-23.4628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12126881081861995, distance: 1.211745914864614 entropy 0.7110485434532166
epoch: 39, step: 86
	action: tensor([[ 0.7516, -1.2627,  0.2196,  0.3090,  0.5971, -0.6246, -0.3185]],
       dtype=torch.float64)
	q_value: tensor([[-21.8247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5000702577589597, distance: 1.4015625785885288 entropy 0.7110485434532166
epoch: 39, step: 87
	action: tensor([[ 0.6324, -0.1801,  0.1295, -0.1813,  0.0768, -0.4082, -1.2154]],
       dtype=torch.float64)
	q_value: tensor([[-23.0334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32433405117530323, distance: 0.9406384117656754 entropy 0.7110485434532166
epoch: 39, step: 88
	action: tensor([[ 0.0123, -0.0427, -0.4852, -0.4108, -0.3354,  0.1470,  0.4878]],
       dtype=torch.float64)
	q_value: tensor([[-22.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20600069355042183, distance: 1.0196867001581966 entropy 0.7110485434532166
epoch: 39, step: 89
	action: tensor([[ 0.2556, -0.5133,  0.1394, -0.8345,  0.3410,  0.2002,  0.0965]],
       dtype=torch.float64)
	q_value: tensor([[-19.7950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4036525016065353, distance: 1.3557714844352413 entropy 0.7110485434532166
epoch: 39, step: 90
	action: tensor([[-0.0130, -0.3839, -0.8086, -0.1466, -1.5260,  0.8021,  0.7990]],
       dtype=torch.float64)
	q_value: tensor([[-20.1881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2473433777382541, distance: 1.2780554778938336 entropy 0.7110485434532166
epoch: 39, step: 91
	action: tensor([[-0.0460, -0.2673,  0.6170, -1.1808,  0.7647,  0.1511, -0.6025]],
       dtype=torch.float64)
	q_value: tensor([[-27.0649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6233718293550508, distance: 1.4580274348544309 entropy 0.7110485434532166
epoch: 39, step: 92
	action: tensor([[ 0.1617, -0.3507, -0.6285,  0.9167,  0.4941,  0.0246,  0.1528]],
       dtype=torch.float64)
	q_value: tensor([[-22.4308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36582853424775363, distance: 0.9112972126111317 entropy 0.7110485434532166
epoch: 39, step: 93
	action: tensor([[-0.2086, -0.9489, -0.5790,  0.3554, -0.7782, -0.0326,  0.6603]],
       dtype=torch.float64)
	q_value: tensor([[-19.8133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6152367503963057, distance: 1.454369595909981 entropy 0.7110485434532166
epoch: 39, step: 94
	action: tensor([[-0.8013, -0.7847,  0.0176, -1.1523,  0.1249, -0.8984,  0.2583]],
       dtype=torch.float64)
	q_value: tensor([[-24.2014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5047881167252546, distance: 1.4037648699719771 entropy 0.7110485434532166
epoch: 39, step: 95
	action: tensor([[-0.4182, -0.8705, -0.2995, -0.3848,  0.9404, -0.8505,  0.7050]],
       dtype=torch.float64)
	q_value: tensor([[-24.4605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8961753360601756, distance: 1.575781291236677 entropy 0.7110485434532166
epoch: 39, step: 96
	action: tensor([[ 0.0426, -0.6450, -0.4345, -0.1611,  1.0575,  0.2308,  0.4942]],
       dtype=torch.float64)
	q_value: tensor([[-23.1320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2055045730993328, distance: 1.2564381780739597 entropy 0.7110485434532166
epoch: 39, step: 97
	action: tensor([[ 0.2152, -0.4145, -0.1682, -0.1169,  0.2818, -0.3557, -0.2921]],
       dtype=torch.float64)
	q_value: tensor([[-21.3814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.040778291601135, distance: 1.1674433236274473 entropy 0.7110485434532166
epoch: 39, step: 98
	action: tensor([[-0.0352, -0.9338,  0.3749,  0.3578,  0.2047, -0.1055,  0.1834]],
       dtype=torch.float64)
	q_value: tensor([[-19.1732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3240269640711291, distance: 1.3167554001308266 entropy 0.7110485434532166
epoch: 39, step: 99
	action: tensor([[ 0.0821, -1.0801, -0.5038, -0.2600, -0.1812, -0.0961,  0.8458]],
       dtype=torch.float64)
	q_value: tensor([[-20.7247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7325459407561001, distance: 1.5062569816993483 entropy 0.7110485434532166
epoch: 39, step: 100
	action: tensor([[ 0.2996, -0.2396, -0.2823, -0.7484,  0.6776,  0.6889,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[-22.8851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2764436774769894, distance: 0.9734034130209376 entropy 0.7110485434532166
epoch: 39, step: 101
	action: tensor([[ 0.2647,  0.0232, -0.2201,  0.3440, -0.0086, -0.3292,  0.5896]],
       dtype=torch.float64)
	q_value: tensor([[-21.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.627685407205758, distance: 0.6982512511990374 entropy 0.7110485434532166
epoch: 39, step: 102
	action: tensor([[ 1.0093,  0.3968,  0.3746, -0.0115, -0.1239, -0.4705, -0.3982]],
       dtype=torch.float64)
	q_value: tensor([[-19.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8947834908835762, distance: 0.3711920255869385 entropy 0.7110485434532166
epoch: 39, step: 103
	action: tensor([[ 0.7165, -0.6202,  1.2392, -0.1639,  0.8129, -0.5689,  0.7281]],
       dtype=torch.float64)
	q_value: tensor([[-20.5097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17058216963167427, distance: 1.0421815415311864 entropy 0.7110485434532166
epoch: 39, step: 104
	action: tensor([[ 0.0404, -0.4561,  0.7738,  0.7842,  0.6730, -0.1847,  0.2024]],
       dtype=torch.float64)
	q_value: tensor([[-21.8170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.510320599929089, distance: 0.8007788801451026 entropy 0.7110485434532166
epoch: 39, step: 105
	action: tensor([[ 0.2808, -0.1580,  0.9586,  0.0806, -0.5443,  0.3162,  0.5404]],
       dtype=torch.float64)
	q_value: tensor([[-21.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5793079179524465, distance: 0.7422306124053326 entropy 0.7110485434532166
epoch: 39, step: 106
	action: tensor([[ 0.0943,  0.1596,  0.1633, -0.1509,  0.9048, -0.4094,  0.2114]],
       dtype=torch.float64)
	q_value: tensor([[-21.5986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3206678724022891, distance: 0.9431869220591855 entropy 0.7110485434532166
epoch: 39, step: 107
	action: tensor([[-0.7543,  0.4365,  0.7952,  0.5208,  0.5500, -0.2328, -0.3488]],
       dtype=torch.float64)
	q_value: tensor([[-19.3724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 108
	action: tensor([[ 0.0580, -1.1525,  0.8643, -0.8213,  0.8295, -0.4133, -0.1031]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0905780606226365, distance: 1.6545880656230092 entropy 0.7110485434532166
epoch: 39, step: 109
	action: tensor([[ 0.0388, -0.1165, -0.1523,  0.7603, -0.0577, -0.3719,  0.7142]],
       dtype=torch.float64)
	q_value: tensor([[-23.5989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5009253294349345, distance: 0.8084244831397315 entropy 0.7110485434532166
epoch: 39, step: 110
	action: tensor([[-0.1552, -0.5461,  0.5819,  0.6887,  0.2964, -0.4766,  0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-20.6473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08545531057808264, distance: 1.0943573507067919 entropy 0.7110485434532166
epoch: 39, step: 111
	action: tensor([[ 0.5019,  0.1762, -0.3919, -0.3031,  0.2226, -0.4360,  0.2380]],
       dtype=torch.float64)
	q_value: tensor([[-21.0870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6083949304140998, distance: 0.7161118262718524 entropy 0.7110485434532166
epoch: 39, step: 112
	action: tensor([[ 0.2740, -0.5984,  0.5833, -0.7403,  0.3688, -0.3528,  0.2836]],
       dtype=torch.float64)
	q_value: tensor([[-19.1388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5828977533523398, distance: 1.4397368635019923 entropy 0.7110485434532166
epoch: 39, step: 113
	action: tensor([[ 0.9142, -0.5277, -0.6103, -0.4451, -0.8628,  0.0955,  0.4542]],
       dtype=torch.float64)
	q_value: tensor([[-20.5273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06888807472720115, distance: 1.1831036907651717 entropy 0.7110485434532166
epoch: 39, step: 114
	action: tensor([[ 0.5924,  0.9508, -0.0430, -0.3428,  0.0159,  0.3522, -0.2729]],
       dtype=torch.float64)
	q_value: tensor([[-23.8700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 39, step: 115
	action: tensor([[-0.0080, -1.2486,  1.0116,  0.1883,  0.1215, -0.3973,  0.7757]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8151591182985489, distance: 1.5417503088251574 entropy 0.7110485434532166
epoch: 39, step: 116
	action: tensor([[ 0.5073, -1.1140, -0.4552, -0.6810, -1.4091, -0.3023, -0.0361]],
       dtype=torch.float64)
	q_value: tensor([[-22.9959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7049240248815749, distance: 1.4942016394922975 entropy 0.7110485434532166
epoch: 39, step: 117
	action: tensor([[-0.6519, -0.8959, -0.2606, -0.3268, -0.4027,  0.1632,  0.8612]],
       dtype=torch.float64)
	q_value: tensor([[-26.8578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0608130043893618, distance: 1.6427670614544647 entropy 0.7110485434532166
epoch: 39, step: 118
	action: tensor([[-0.3995,  0.0783,  0.7148, -0.4023,  0.2575, -0.0707,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[-23.2779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4318731610321169, distance: 1.3693326631529452 entropy 0.7110485434532166
epoch: 39, step: 119
	action: tensor([[ 0.6307, -0.5780,  0.4949, -0.6499,  0.5378, -0.6344,  1.1106]],
       dtype=torch.float64)
	q_value: tensor([[-19.9089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31444705742086265, distance: 1.3119831051827073 entropy 0.7110485434532166
epoch: 39, step: 120
	action: tensor([[ 0.5661,  0.0785,  0.2567,  1.2078,  0.0280, -0.5831,  0.4046]],
       dtype=torch.float64)
	q_value: tensor([[-21.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.07659935202910756 entropy 0.7110485434532166
epoch: 39, step: 121
	action: tensor([[-0.1045, -0.7556,  0.3798, -0.4070, -0.0063, -0.5478,  0.4092]],
       dtype=torch.float64)
	q_value: tensor([[-26.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.800246809143297, distance: 1.5354041785254071 entropy 0.7110485434532166
epoch: 39, step: 122
	action: tensor([[ 0.3496, -0.1451, -0.1276, -0.2933,  0.0673,  0.5357, -0.0400]],
       dtype=torch.float64)
	q_value: tensor([[-21.1896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.483178810474321, distance: 0.8226722514968707 entropy 0.7110485434532166
epoch: 39, step: 123
	action: tensor([[-0.3494, -0.0826,  0.3166, -0.0446, -1.2775,  0.9651,  0.0562]],
       dtype=torch.float64)
	q_value: tensor([[-18.7988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1757047734937487, distance: 1.2408115688320733 entropy 0.7110485434532166
epoch: 39, step: 124
	action: tensor([[ 0.3905, -0.2095,  0.4436,  0.4082,  0.0399, -0.3132,  0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-25.4183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6062249178531284, distance: 0.7180931908602161 entropy 0.7110485434532166
epoch: 39, step: 125
	action: tensor([[ 0.2970, -0.6875,  0.0154,  0.4357, -0.3415,  0.5714, -0.5784]],
       dtype=torch.float64)
	q_value: tensor([[-19.6299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37151858961741036, distance: 0.9071997282540241 entropy 0.7110485434532166
epoch: 39, step: 126
	action: tensor([[-0.1277, -0.3312, -0.2599, -0.2347,  0.0696, -0.0385,  0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-21.9045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17501728267030958, distance: 1.2404487348421205 entropy 0.7110485434532166
epoch: 39, step: 127
	action: tensor([[ 0.1885,  0.0275,  0.2812,  0.6652,  0.8853,  0.0955, -0.2703]],
       dtype=torch.float64)
	q_value: tensor([[-18.8384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
LOSS epoch 39 actor 285.2881341726989 critic 918.6888631187554 
epoch: 40, step: 0
	action: tensor([[ 0.4225, -1.4846,  1.2159, -0.2618,  0.4241, -0.2587,  0.5234]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8263189790347683, distance: 1.5464824983442245 entropy 0.6056879162788391
epoch: 40, step: 1
	action: tensor([[ 0.7216, -0.3898,  0.1286, -0.0130, -0.0068,  0.6519,  0.4353]],
       dtype=torch.float64)
	q_value: tensor([[-24.8622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5814107007965298, distance: 0.7403733098043335 entropy 0.6056879162788391
epoch: 40, step: 2
	action: tensor([[ 0.5914,  0.1339, -0.4309,  0.8550, -0.0212, -0.3772,  0.3407]],
       dtype=torch.float64)
	q_value: tensor([[-22.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 3
	action: tensor([[ 0.6059, -1.6685,  0.4706, -0.9516,  0.1669,  0.1619,  0.5501]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0464302185948031, distance: 1.6370244403487166 entropy 0.6056879162788391
epoch: 40, step: 4
	action: tensor([[ 0.4381, -0.2936,  0.7797,  0.1811,  0.3938,  0.2690, -0.0861]],
       dtype=torch.float64)
	q_value: tensor([[-25.2386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.552082154477591, distance: 0.7658714425173377 entropy 0.6056879162788391
epoch: 40, step: 5
	action: tensor([[-0.0150, -0.0923,  0.2894, -0.3480,  0.6416,  0.2433,  0.4206]],
       dtype=torch.float64)
	q_value: tensor([[-22.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08609068771332384, distance: 1.0939771339399278 entropy 0.6056879162788391
epoch: 40, step: 6
	action: tensor([[-0.3677,  0.6562,  0.2833, -0.1959, -0.4446, -0.4176, -0.4339]],
       dtype=torch.float64)
	q_value: tensor([[-20.8192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 7
	action: tensor([[ 0.2300, -1.0232,  0.9562, -0.6455,  0.2121, -0.5396, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.895859428751574, distance: 1.5756500213185902 entropy 0.6056879162788391
epoch: 40, step: 8
	action: tensor([[-0.0502,  0.3034, -0.1493,  0.1375,  0.2260,  0.2526, -0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-24.7350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 9
	action: tensor([[ 0.4688, -1.1094,  0.7308, -0.5691,  0.5112, -0.6999,  1.4204]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8083740496417016, distance: 1.5388660780145758 entropy 0.6056879162788391
epoch: 40, step: 10
	action: tensor([[ 0.3850,  0.3996,  0.5396,  0.0772, -0.1119,  0.2047, -0.2039]],
       dtype=torch.float64)
	q_value: tensor([[-25.5951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 11
	action: tensor([[-0.0939, -0.8305,  0.9354, -1.0185,  1.1094,  0.2398,  0.2165]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0260831039919096, distance: 1.628865859437723 entropy 0.6056879162788391
epoch: 40, step: 12
	action: tensor([[ 0.4330, -0.5122, -0.3088,  0.1703,  0.4437, -0.1700, -0.4661]],
       dtype=torch.float64)
	q_value: tensor([[-25.5108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20393205502536194, distance: 1.0210141516174276 entropy 0.6056879162788391
epoch: 40, step: 13
	action: tensor([[-0.3503, -1.0123,  0.1808, -0.4348,  0.5181,  0.1135,  0.5766]],
       dtype=torch.float64)
	q_value: tensor([[-21.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0007473336598736, distance: 1.6186494971174583 entropy 0.6056879162788391
epoch: 40, step: 14
	action: tensor([[ 0.3242, -1.1124,  0.2703, -1.0790,  0.4840, -0.4055, -0.3118]],
       dtype=torch.float64)
	q_value: tensor([[-23.3334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9711742141974817, distance: 1.6066423035574455 entropy 0.6056879162788391
epoch: 40, step: 15
	action: tensor([[ 0.6405, -0.7661,  0.2081, -0.5735,  0.5639,  0.1852,  0.5067]],
       dtype=torch.float64)
	q_value: tensor([[-26.5709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4631154433125242, distance: 1.384190902189698 entropy 0.6056879162788391
epoch: 40, step: 16
	action: tensor([[-0.5666, -0.0611,  0.0863,  0.1139,  0.2230, -0.0460,  0.1990]],
       dtype=torch.float64)
	q_value: tensor([[-22.8972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36829842496465814, distance: 1.33858855299902 entropy 0.6056879162788391
epoch: 40, step: 17
	action: tensor([[-0.6555, -0.4862, -0.1068, -0.0825, -0.1276,  0.0564, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-20.5555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8443120739747312, distance: 1.5540818843402882 entropy 0.6056879162788391
epoch: 40, step: 18
	action: tensor([[-0.0785, -0.2856,  0.3095, -0.2289, -0.6323, -0.2922,  0.9019]],
       dtype=torch.float64)
	q_value: tensor([[-21.6449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22463788494920967, distance: 1.2663697809170367 entropy 0.6056879162788391
epoch: 40, step: 19
	action: tensor([[ 0.3546, -0.5275,  0.5616, -0.3159,  0.7211, -0.1895,  1.5427]],
       dtype=torch.float64)
	q_value: tensor([[-23.1919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1768005225157656, distance: 1.2413896482328257 entropy 0.6056879162788391
epoch: 40, step: 20
	action: tensor([[-0.1126, -0.1225, -0.0563,  0.6584, -0.5258,  0.0255,  0.9125]],
       dtype=torch.float64)
	q_value: tensor([[-24.2051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 21
	action: tensor([[ 0.2703, -0.9822,  1.5753, -0.8244,  0.3595, -0.3797,  0.6106]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5550458683692108, distance: 1.4270142012802565 entropy 0.6056879162788391
epoch: 40, step: 22
	action: tensor([[-0.9660,  0.0610,  0.2121, -0.4035,  0.4937,  0.4821, -0.6338]],
       dtype=torch.float64)
	q_value: tensor([[-25.6004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.904434465443007, distance: 1.5792093604305877 entropy 0.6056879162788391
epoch: 40, step: 23
	action: tensor([[-0.3531, -0.6634,  0.6733,  0.2967,  0.0346, -0.5242, -0.6384]],
       dtype=torch.float64)
	q_value: tensor([[-24.2045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5947058266750003, distance: 1.4450969479035363 entropy 0.6056879162788391
epoch: 40, step: 24
	action: tensor([[ 0.3034, -0.1822,  1.3635, -0.0057, -0.7304, -0.3378,  0.5409]],
       dtype=torch.float64)
	q_value: tensor([[-25.8759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2961883498129214, distance: 0.9600302530093591 entropy 0.6056879162788391
epoch: 40, step: 25
	action: tensor([[ 0.0776, -0.1954,  0.5319, -0.9087, -0.1115, -0.0510, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-25.1859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4193362455511933, distance: 1.363324815857153 entropy 0.6056879162788391
epoch: 40, step: 26
	action: tensor([[-0.0894, -0.2988, -0.0882, -0.4375,  0.1369,  0.7172, -0.2286]],
       dtype=torch.float64)
	q_value: tensor([[-21.9039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05668444176487197, distance: 1.1763304789205085 entropy 0.6056879162788391
epoch: 40, step: 27
	action: tensor([[ 0.0211, -0.6802, -0.3267,  0.2780, -0.0847,  0.3680,  0.3253]],
       dtype=torch.float64)
	q_value: tensor([[-21.5806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09852285836184449, distance: 1.1993922614140131 entropy 0.6056879162788391
epoch: 40, step: 28
	action: tensor([[ 1.1451,  0.3802,  0.2347, -0.5489, -0.7473,  0.3480,  0.4015]],
       dtype=torch.float64)
	q_value: tensor([[-22.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8322110576181251, distance: 0.4687468492387206 entropy 0.6056879162788391
epoch: 40, step: 29
	action: tensor([[ 0.6866, -0.8662,  1.2278,  0.1200,  0.3001,  0.2007,  0.2199]],
       dtype=torch.float64)
	q_value: tensor([[-25.1517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09474329658993952, distance: 1.1973271778227714 entropy 0.6056879162788391
epoch: 40, step: 30
	action: tensor([[ 0.8135, -0.6594,  0.9127,  0.0217,  0.6341, -0.1562,  0.8928]],
       dtype=torch.float64)
	q_value: tensor([[-24.1704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09751874705101726, distance: 1.0871157501465685 entropy 0.6056879162788391
epoch: 40, step: 31
	action: tensor([[-0.0341, -0.3202,  0.8523,  0.2525,  0.0533,  0.1290,  0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-23.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24533248681122521, distance: 0.9941101791161594 entropy 0.6056879162788391
epoch: 40, step: 32
	action: tensor([[ 0.1362, -0.6136,  0.3333, -0.1362,  0.8606, -0.6536,  0.4172]],
       dtype=torch.float64)
	q_value: tensor([[-21.8513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38472302634608035, distance: 1.3465985742488622 entropy 0.6056879162788391
epoch: 40, step: 33
	action: tensor([[ 0.5337, -0.2369, -0.1005,  0.1073,  0.1474,  0.1903,  0.0768]],
       dtype=torch.float64)
	q_value: tensor([[-22.6786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5845830944570882, distance: 0.7375624129245264 entropy 0.6056879162788391
epoch: 40, step: 34
	action: tensor([[ 0.3595, -0.4060, -0.1571,  0.1868,  0.6252, -1.0109,  0.2071]],
       dtype=torch.float64)
	q_value: tensor([[-20.2279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06111847161552142, distance: 1.108822654978961 entropy 0.6056879162788391
epoch: 40, step: 35
	action: tensor([[ 0.4869, -0.9589,  0.1857, -0.1733,  0.3472,  0.2841,  1.2539]],
       dtype=torch.float64)
	q_value: tensor([[-22.9670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3458358631889846, distance: 1.3275556694927415 entropy 0.6056879162788391
epoch: 40, step: 36
	action: tensor([[ 0.2045, -0.3382, -0.1226,  0.3696, -0.5018, -0.4761,  0.0669]],
       dtype=torch.float64)
	q_value: tensor([[-24.6245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3165460341059645, distance: 0.9460439816284797 entropy 0.6056879162788391
epoch: 40, step: 37
	action: tensor([[-0.1750, -0.8195,  0.0207, -0.4600,  0.1444, -0.0032, -0.0785]],
       dtype=torch.float64)
	q_value: tensor([[-22.3183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7709826249261882, distance: 1.5228735490981045 entropy 0.6056879162788391
epoch: 40, step: 38
	action: tensor([[ 0.1918, -1.5101, -0.0360, -0.0446,  0.5038,  0.0345,  0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-22.3412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 39
	action: tensor([[ 0.3000, -1.2157,  0.5931, -0.1914,  0.6950,  0.2749,  0.9370]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7361899974217916, distance: 1.5078402016616483 entropy 0.6056879162788391
epoch: 40, step: 40
	action: tensor([[-0.1190, -0.4629,  0.0583, -0.0499,  0.4575,  0.4088, -0.2691]],
       dtype=torch.float64)
	q_value: tensor([[-24.4903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05012399112921484, distance: 1.172673155477322 entropy 0.6056879162788391
epoch: 40, step: 41
	action: tensor([[-0.5216, -0.7693,  0.0734, -0.5459,  0.1284, -0.6681,  0.3935]],
       dtype=torch.float64)
	q_value: tensor([[-21.5161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9407521575420008, distance: 1.5941960623484017 entropy 0.6056879162788391
epoch: 40, step: 42
	action: tensor([[ 0.5151, -0.3767,  0.8703, -1.0808, -0.0614,  0.0809, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[-23.7506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2502483199537926, distance: 1.279542846277916 entropy 0.6056879162788391
epoch: 40, step: 43
	action: tensor([[ 0.3375,  0.1657,  1.1977, -0.9983,  0.0719, -0.5370, -0.0130]],
       dtype=torch.float64)
	q_value: tensor([[-23.2402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04836335264315217, distance: 1.1163291681664942 entropy 0.6056879162788391
epoch: 40, step: 44
	action: tensor([[ 0.2394, -0.1558,  0.5021, -0.4894,  1.2567, -0.0574,  0.7225]],
       dtype=torch.float64)
	q_value: tensor([[-24.1018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018880197573788893, distance: 1.1334900547098987 entropy 0.6056879162788391
epoch: 40, step: 45
	action: tensor([[ 0.8571, -0.7393, -0.0912, -0.4665,  0.4686, -0.0447, -0.0438]],
       dtype=torch.float64)
	q_value: tensor([[-23.2785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37547080246919795, distance: 1.342092289323482 entropy 0.6056879162788391
epoch: 40, step: 46
	action: tensor([[ 0.3763, -0.1084, -0.4047,  0.3164, -0.6564, -0.0209,  0.0807]],
       dtype=torch.float64)
	q_value: tensor([[-22.9337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6051593478680786, distance: 0.7190641278673272 entropy 0.6056879162788391
epoch: 40, step: 47
	action: tensor([[ 0.0155, -1.2625,  0.5382, -0.4533,  0.3639, -0.0400, -0.0930]],
       dtype=torch.float64)
	q_value: tensor([[-21.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.09564014414469, distance: 1.6565900475399344 entropy 0.6056879162788391
epoch: 40, step: 48
	action: tensor([[ 0.1829, -0.2666,  0.6141, -0.3445, -0.3365,  0.4701,  0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-24.4853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15892979713461663, distance: 1.049476738327004 entropy 0.6056879162788391
epoch: 40, step: 49
	action: tensor([[ 1.0256,  0.5096,  0.5829,  0.1359, -0.1250, -0.0813,  1.1964]],
       dtype=torch.float64)
	q_value: tensor([[-22.0701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9590083397646316, distance: 0.23168848378402482 entropy 0.6056879162788391
epoch: 40, step: 50
	action: tensor([[ 0.3993, -0.0427,  0.2713,  0.2967,  0.0924,  0.7256, -0.4626]],
       dtype=torch.float64)
	q_value: tensor([[-24.6923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8892150444892764, distance: 0.38088782178470876 entropy 0.6056879162788391
epoch: 40, step: 51
	action: tensor([[-0.2050,  0.0728,  1.1876, -0.6607,  0.4787,  0.2786, -0.2521]],
       dtype=torch.float64)
	q_value: tensor([[-22.6866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29431391196544143, distance: 1.3018966301948243 entropy 0.6056879162788391
epoch: 40, step: 52
	action: tensor([[-0.3591,  0.2273,  0.3810,  0.0939,  0.2156,  0.5383,  0.2347]],
       dtype=torch.float64)
	q_value: tensor([[-24.1779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 53
	action: tensor([[ 0.0584, -1.0430,  1.2003, -1.2506,  0.7865, -0.3614,  0.4761]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8457696085528446, distance: 1.5546958478791666 entropy 0.6056879162788391
epoch: 40, step: 54
	action: tensor([[ 0.5241, -0.7274,  0.3575,  0.6998,  0.0897, -1.2905,  0.9562]],
       dtype=torch.float64)
	q_value: tensor([[-26.0982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1198617111940562, distance: 1.0735744089451913 entropy 0.6056879162788391
epoch: 40, step: 55
	action: tensor([[ 0.8618,  0.3816, -0.1345, -0.9036,  0.8582,  0.2934,  0.8434]],
       dtype=torch.float64)
	q_value: tensor([[-26.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6494786874636894, distance: 0.6775071890694225 entropy 0.6056879162788391
epoch: 40, step: 56
	action: tensor([[-0.1134, -0.1832,  0.3095, -0.2933, -0.5730,  0.4734,  0.3044]],
       dtype=torch.float64)
	q_value: tensor([[-24.0134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04743982089808174, distance: 1.1711734905009936 entropy 0.6056879162788391
epoch: 40, step: 57
	action: tensor([[ 0.0289, -1.3384, -0.8945, -0.0862,  0.6351, -0.1752,  0.0815]],
       dtype=torch.float64)
	q_value: tensor([[-22.5820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8280790097634634, distance: 1.5472274945085087 entropy 0.6056879162788391
epoch: 40, step: 58
	action: tensor([[-0.0361, -0.3161,  0.3748, -0.2244,  0.3412, -0.2261, -0.4509]],
       dtype=torch.float64)
	q_value: tensor([[-26.0395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2302784794849635, distance: 1.2692828347735363 entropy 0.6056879162788391
epoch: 40, step: 59
	action: tensor([[ 0.1107, -0.8577, -0.3855, -0.7411,  0.0664,  0.2003,  0.1192]],
       dtype=torch.float64)
	q_value: tensor([[-22.0374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6078756638323175, distance: 1.4510518266645582 entropy 0.6056879162788391
epoch: 40, step: 60
	action: tensor([[-0.3167,  0.1619, -0.2560, -0.4980,  0.4462,  0.4334, -0.5097]],
       dtype=torch.float64)
	q_value: tensor([[-23.5240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04762572777548746, distance: 1.116761724361143 entropy 0.6056879162788391
epoch: 40, step: 61
	action: tensor([[-0.2342, -0.8662,  0.0629, -0.1895, -0.3667, -0.6452,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[-21.5922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6971145815575484, distance: 1.4907755994062968 entropy 0.6056879162788391
epoch: 40, step: 62
	action: tensor([[ 0.3059, -0.9257, -0.1344,  0.2135,  0.8000,  0.2743,  0.3665]],
       dtype=torch.float64)
	q_value: tensor([[-23.5293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12844553961459004, distance: 1.2156176451303182 entropy 0.6056879162788391
epoch: 40, step: 63
	action: tensor([[ 0.7585, -0.0702,  0.2925, -0.1044,  0.1875, -0.4641,  0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-23.1311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5267054149505552, distance: 0.7872677499564786 entropy 0.6056879162788391
epoch: 40, step: 64
	action: tensor([[-0.8877, -0.0970, -0.6812, -0.7686,  0.1308,  0.2365,  0.7046]],
       dtype=torch.float64)
	q_value: tensor([[-20.8807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.600596258628302, distance: 1.447763395554787 entropy 0.6056879162788391
epoch: 40, step: 65
	action: tensor([[ 0.1953, -1.0366,  0.5270,  0.1690,  0.4017,  0.0878,  0.9772]],
       dtype=torch.float64)
	q_value: tensor([[-23.9256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.343908691588233, distance: 1.3266048313567473 entropy 0.6056879162788391
epoch: 40, step: 66
	action: tensor([[ 0.0672, -0.6293,  0.6545,  0.4174,  0.2629, -0.1353, -0.4319]],
       dtype=torch.float64)
	q_value: tensor([[-23.7436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1110577709063274, distance: 1.0789304799814943 entropy 0.6056879162788391
epoch: 40, step: 67
	action: tensor([[-0.0547, -0.6607,  0.2161, -0.3017,  0.9407,  0.3915,  0.5971]],
       dtype=torch.float64)
	q_value: tensor([[-23.9292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33236746854134713, distance: 1.3208962384097789 entropy 0.6056879162788391
epoch: 40, step: 68
	action: tensor([[ 0.8498,  0.0128,  0.8482, -0.0418, -0.2851, -0.4264,  0.2039]],
       dtype=torch.float64)
	q_value: tensor([[-22.8970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6728985705964957, distance: 0.6544823126324922 entropy 0.6056879162788391
epoch: 40, step: 69
	action: tensor([[ 0.3393, -0.7941,  0.1290, -0.4136, -0.2397,  0.7023, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-22.7922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2242987033570063, distance: 1.2661943988432276 entropy 0.6056879162788391
epoch: 40, step: 70
	action: tensor([[ 0.9045, -0.5382,  0.6949, -0.2342,  0.1886,  0.3716,  0.6319]],
       dtype=torch.float64)
	q_value: tensor([[-23.2133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20614309343015125, distance: 1.019595258154884 entropy 0.6056879162788391
epoch: 40, step: 71
	action: tensor([[ 0.9623,  0.5522, -0.3622,  1.0158, -0.1139,  0.1340,  0.9050]],
       dtype=torch.float64)
	q_value: tensor([[-22.6348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 72
	action: tensor([[ 0.5118, -1.3029,  1.3854, -0.4432,  0.1627,  0.1038,  1.0590]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7801557943787323, distance: 1.5268124737511986 entropy 0.6056879162788391
epoch: 40, step: 73
	action: tensor([[ 0.1702, -0.1884,  0.3730,  0.3744, -0.0450,  0.5975, -0.4412]],
       dtype=torch.float64)
	q_value: tensor([[-25.6448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7421545052620999, distance: 0.5810807213937146 entropy 0.6056879162788391
epoch: 40, step: 74
	action: tensor([[-0.2915, -0.5292,  0.5736, -0.2535,  0.1287, -1.1147,  0.1460]],
       dtype=torch.float64)
	q_value: tensor([[-22.7458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.715944778602355, distance: 1.4990231747509684 entropy 0.6056879162788391
epoch: 40, step: 75
	action: tensor([[ 0.6589, -1.0425,  0.1953, -0.1465,  0.5915, -0.2957,  0.4729]],
       dtype=torch.float64)
	q_value: tensor([[-24.1124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6305758227797329, distance: 1.4612589782200538 entropy 0.6056879162788391
epoch: 40, step: 76
	action: tensor([[ 0.5084,  0.1428, -0.2903, -0.7586,  0.6879, -0.0530,  0.7742]],
       dtype=torch.float64)
	q_value: tensor([[-23.2150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4382712028606085, distance: 0.8576696409933099 entropy 0.6056879162788391
epoch: 40, step: 77
	action: tensor([[-0.0845, -1.1430, -0.2621,  0.4679,  0.2708,  0.9809,  0.6256]],
       dtype=torch.float64)
	q_value: tensor([[-22.5502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11593596519022764, distance: 1.2088608993883838 entropy 0.6056879162788391
epoch: 40, step: 78
	action: tensor([[ 0.4600, -0.0185,  0.4976, -0.0564,  0.0820,  0.1156,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-24.9564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.640573724984612, distance: 0.6860592185029422 entropy 0.6056879162788391
epoch: 40, step: 79
	action: tensor([[ 0.5211, -1.1750,  0.4489, -0.1691,  0.2433, -0.1575,  0.2027]],
       dtype=torch.float64)
	q_value: tensor([[-20.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7635857531391237, distance: 1.5196899246561957 entropy 0.6056879162788391
epoch: 40, step: 80
	action: tensor([[-0.1857, -0.0018,  1.0003,  0.3138, -0.0521, -0.4530, -0.2175]],
       dtype=torch.float64)
	q_value: tensor([[-23.5363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1935239184455424, distance: 1.0276670667453747 entropy 0.6056879162788391
epoch: 40, step: 81
	action: tensor([[-0.2989, -0.1965,  0.4427, -0.1702,  1.1398,  0.3192,  0.8122]],
       dtype=torch.float64)
	q_value: tensor([[-24.4824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17369314264509517, distance: 1.2397496000932562 entropy 0.6056879162788391
epoch: 40, step: 82
	action: tensor([[-0.2627, -0.0024,  0.2697, -0.3694, -0.1194, -0.1434,  0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-23.4399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2740414790341299, distance: 1.2916607924991943 entropy 0.6056879162788391
epoch: 40, step: 83
	action: tensor([[-0.8356, -0.4883, -0.2745,  0.2740, -0.9756,  0.2738,  0.2389]],
       dtype=torch.float64)
	q_value: tensor([[-20.6327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0241532894133742, distance: 1.6280899391545598 entropy 0.6056879162788391
epoch: 40, step: 84
	action: tensor([[ 0.0512,  0.0139,  0.1619,  0.1912,  0.1934, -0.6563,  0.7888]],
       dtype=torch.float64)
	q_value: tensor([[-26.1061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975225183368799, distance: 0.9591198889063274 entropy 0.6056879162788391
epoch: 40, step: 85
	action: tensor([[ 0.1090, -0.5330,  0.3664, -0.7027, -0.2625, -0.1875, -0.1772]],
       dtype=torch.float64)
	q_value: tensor([[-21.3081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5912315643365575, distance: 1.4435219290744885 entropy 0.6056879162788391
epoch: 40, step: 86
	action: tensor([[ 0.4995,  0.1078,  0.5005, -0.3534, -0.1891, -0.1102, -0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-22.1746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5215959558684591, distance: 0.7915058232570875 entropy 0.6056879162788391
epoch: 40, step: 87
	action: tensor([[-0.1208, -0.5718, -0.2392, -0.2153,  0.1701, -0.3712,  0.2647]],
       dtype=torch.float64)
	q_value: tensor([[-21.1485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4807154787037684, distance: 1.3924913351370167 entropy 0.6056879162788391
epoch: 40, step: 88
	action: tensor([[-0.3430, -0.7286,  0.4725, -0.3071, -0.1141,  0.5041,  0.4161]],
       dtype=torch.float64)
	q_value: tensor([[-21.5494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6831273488439649, distance: 1.4846195721621704 entropy 0.6056879162788391
epoch: 40, step: 89
	action: tensor([[ 1.1193, -0.1353,  0.3443, -0.0259,  0.3794,  0.4608,  0.3690]],
       dtype=torch.float64)
	q_value: tensor([[-22.6447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6595847916644305, distance: 0.6676689355893222 entropy 0.6056879162788391
epoch: 40, step: 90
	action: tensor([[ 0.8361,  0.0628, -0.2179, -0.5186, -0.0187,  0.5546,  0.0756]],
       dtype=torch.float64)
	q_value: tensor([[-21.9263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6887084845919874, distance: 0.6384697638445649 entropy 0.6056879162788391
epoch: 40, step: 91
	action: tensor([[ 0.3845, -0.2432, -0.1000, -0.3007,  0.1846,  0.1377,  0.4560]],
       dtype=torch.float64)
	q_value: tensor([[-22.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2730136186777332, distance: 0.9757079214686316 entropy 0.6056879162788391
epoch: 40, step: 92
	action: tensor([[ 0.1785, -0.0510,  0.0528, -0.2523,  0.2965, -0.2535, -0.1236]],
       dtype=torch.float64)
	q_value: tensor([[-20.5478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22741072772112858, distance: 1.0058449273028414 entropy 0.6056879162788391
epoch: 40, step: 93
	action: tensor([[ 1.0464,  0.1356, -0.1005,  0.4531, -0.1708,  0.6142,  0.6284]],
       dtype=torch.float64)
	q_value: tensor([[-20.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9885008271348267, distance: 0.12271283641658882 entropy 0.6056879162788391
epoch: 40, step: 94
	action: tensor([[ 0.5331, -0.9066,  0.1817, -0.5164,  0.5467,  0.2843,  0.4844]],
       dtype=torch.float64)
	q_value: tensor([[-23.1385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5322206473040021, distance: 1.416502510085026 entropy 0.6056879162788391
epoch: 40, step: 95
	action: tensor([[-0.6526,  0.1443, -0.0393,  0.3269,  0.1744,  0.2937, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-23.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09736559806453893, distance: 1.1987603333738137 entropy 0.6056879162788391
epoch: 40, step: 96
	action: tensor([[-0.1118, -0.3087, -0.3309, -0.7939, -0.1206, -0.3853,  0.4355]],
       dtype=torch.float64)
	q_value: tensor([[-21.4851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2677695326087721, distance: 1.288477527914649 entropy 0.6056879162788391
epoch: 40, step: 97
	action: tensor([[ 0.2568, -0.3433,  1.2306, -0.5884,  0.1969,  0.4028,  1.0120]],
       dtype=torch.float64)
	q_value: tensor([[-22.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043227563638522426, distance: 1.168816193350575 entropy 0.6056879162788391
epoch: 40, step: 98
	action: tensor([[ 0.6891,  0.0271,  0.1843, -0.4322,  0.2896,  0.3081,  0.7854]],
       dtype=torch.float64)
	q_value: tensor([[-24.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5928989055775769, distance: 0.7301428357630525 entropy 0.6056879162788391
epoch: 40, step: 99
	action: tensor([[ 0.8733, -0.0467,  0.2231,  1.1672,  0.4726, -0.0535,  0.7799]],
       dtype=torch.float64)
	q_value: tensor([[-21.6903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9801402484982579, distance: 0.16126629124168396 entropy 0.6056879162788391
epoch: 40, step: 100
	action: tensor([[ 0.2046,  0.2018, -0.2724,  0.3876, -0.0242,  0.2381, -0.2519]],
       dtype=torch.float64)
	q_value: tensor([[-24.2106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 40, step: 101
	action: tensor([[ 0.2051, -1.1992,  0.6715, -0.2674,  0.7131, -0.1424,  1.1739]],
       dtype=torch.float64)
	q_value: tensor([[-27.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9303571251725891, distance: 1.58992092357205 entropy 0.6056879162788391
epoch: 40, step: 102
	action: tensor([[ 0.1036,  0.1423, -0.1173, -0.0854, -0.6638,  0.1217,  0.4661]],
       dtype=torch.float64)
	q_value: tensor([[-24.6803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48515957922412145, distance: 0.8210942513614986 entropy 0.6056879162788391
epoch: 40, step: 103
	action: tensor([[-0.4448, -0.4628, -1.4177, -0.1319, -0.1066,  0.1357, -0.5005]],
       dtype=torch.float64)
	q_value: tensor([[-21.7374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5314623484715946, distance: 1.416151951850548 entropy 0.6056879162788391
epoch: 40, step: 104
	action: tensor([[ 0.2901, -0.8635,  0.6813, -0.8232,  0.6162, -0.7781,  0.5346]],
       dtype=torch.float64)
	q_value: tensor([[-23.8382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7090172281605143, distance: 1.4959942133770066 entropy 0.6056879162788391
epoch: 40, step: 105
	action: tensor([[ 0.4570, -0.0662, -0.3697, -0.1385, -0.5488,  0.8018,  0.4609]],
       dtype=torch.float64)
	q_value: tensor([[-24.4798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6180239457643617, distance: 0.707252941476763 entropy 0.6056879162788391
epoch: 40, step: 106
	action: tensor([[ 0.5306, -0.2410, -0.2745,  0.0647, -0.3300, -0.7915,  0.6946]],
       dtype=torch.float64)
	q_value: tensor([[-23.1266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38887124544596485, distance: 0.8945879675220317 entropy 0.6056879162788391
epoch: 40, step: 107
	action: tensor([[ 0.2201, -0.2830,  0.3345, -0.8319,  0.5969,  0.3030, -0.3090]],
       dtype=torch.float64)
	q_value: tensor([[-23.5728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18709642315530828, distance: 1.2468083194211443 entropy 0.6056879162788391
epoch: 40, step: 108
	action: tensor([[-0.8321,  0.1046, -0.9764, -0.4926,  1.0122,  0.4932,  0.3355]],
       dtype=torch.float64)
	q_value: tensor([[-22.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2826567056261293, distance: 1.2960206197820048 entropy 0.6056879162788391
epoch: 40, step: 109
	action: tensor([[-0.2538,  0.0147,  0.4097, -1.1536, -0.1230, -0.3342,  0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-25.1952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5753760065994542, distance: 1.4363120586938005 entropy 0.6056879162788391
epoch: 40, step: 110
	action: tensor([[-0.5515, -0.5612,  0.5849, -0.5709,  0.4943,  0.2609,  0.4243]],
       dtype=torch.float64)
	q_value: tensor([[-23.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0163850665207987, distance: 1.6249628234008604 entropy 0.6056879162788391
epoch: 40, step: 111
	action: tensor([[-0.3608, -0.2300, -0.1775, -0.7070,  0.3067, -0.4199,  0.3316]],
       dtype=torch.float64)
	q_value: tensor([[-22.5578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5316763324815246, distance: 1.4162508845108892 entropy 0.6056879162788391
epoch: 40, step: 112
	action: tensor([[ 0.8469, -1.1599, -0.0285, -0.7183, -0.2672,  0.4931,  0.3788]],
       dtype=torch.float64)
	q_value: tensor([[-21.7328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8280041917226484, distance: 1.5471958323884152 entropy 0.6056879162788391
epoch: 40, step: 113
	action: tensor([[ 0.4075, -0.1162,  0.2899, -0.5451, -0.6343, -0.1218,  0.2682]],
       dtype=torch.float64)
	q_value: tensor([[-25.8837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1438050489204562, distance: 1.0588709277415944 entropy 0.6056879162788391
epoch: 40, step: 114
	action: tensor([[-0.3936, -0.5172,  0.0748,  0.0876, -0.0529, -0.1275,  0.4990]],
       dtype=torch.float64)
	q_value: tensor([[-22.0960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5263125813697178, distance: 1.4137689373768552 entropy 0.6056879162788391
epoch: 40, step: 115
	action: tensor([[ 0.6461, -0.1654, -0.0296, -0.9035,  0.7837,  0.0299,  0.3920]],
       dtype=torch.float64)
	q_value: tensor([[-21.4647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06495865633248954, distance: 1.1065526950941587 entropy 0.6056879162788391
epoch: 40, step: 116
	action: tensor([[ 0.1654, -0.3757, -0.3897, -0.1043, -0.1385,  0.3793, -0.8355]],
       dtype=torch.float64)
	q_value: tensor([[-22.8984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13733000327732758, distance: 1.0628672860750883 entropy 0.6056879162788391
epoch: 40, step: 117
	action: tensor([[ 0.5393, -0.3951,  0.4747, -0.5687, -0.2494,  0.3927, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-22.3905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05845501380213458, distance: 1.1103943180312312 entropy 0.6056879162788391
epoch: 40, step: 118
	action: tensor([[ 0.3739,  0.0039,  0.3315, -0.0761,  0.3151,  0.5375, -0.3817]],
       dtype=torch.float64)
	q_value: tensor([[-22.1538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7048641202514871, distance: 0.621681148500818 entropy 0.6056879162788391
epoch: 40, step: 119
	action: tensor([[-0.4204, -0.2930,  0.7693,  0.1301, -0.2797, -0.1165,  0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-21.4709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.375640173257515, distance: 1.3421749171158994 entropy 0.6056879162788391
epoch: 40, step: 120
	action: tensor([[ 0.6585, -0.2602,  1.0228, -0.5891, -0.6875, -0.0708,  1.0349]],
       dtype=torch.float64)
	q_value: tensor([[-22.6398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2023547978722452, distance: 1.022025123731842 entropy 0.6056879162788391
epoch: 40, step: 121
	action: tensor([[ 0.2229, -0.4385,  0.3704, -0.4426,  0.4794,  0.4148, -0.3419]],
       dtype=torch.float64)
	q_value: tensor([[-25.0825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08881468545860316, distance: 1.1940806980868643 entropy 0.6056879162788391
epoch: 40, step: 122
	action: tensor([[ 0.2033,  0.0560,  0.0739, -0.6751, -0.2755, -0.4687,  0.6429]],
       dtype=torch.float64)
	q_value: tensor([[-21.8984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0830486221018083, distance: 1.0957963436773535 entropy 0.6056879162788391
epoch: 40, step: 123
	action: tensor([[ 0.0951, -0.2238,  0.6484,  0.6160,  0.4501, -0.5189,  0.5495]],
       dtype=torch.float64)
	q_value: tensor([[-22.0867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4521015876284922, distance: 0.8470454506440643 entropy 0.6056879162788391
epoch: 40, step: 124
	action: tensor([[ 0.7246, -0.5319, -0.1291,  0.2592, -0.3722, -0.6106, -0.1298]],
       dtype=torch.float64)
	q_value: tensor([[-22.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2271909594841377, distance: 1.0059879768225137 entropy 0.6056879162788391
epoch: 40, step: 125
	action: tensor([[ 0.4582,  0.1711,  0.2104, -1.2008,  0.3780,  0.1149,  0.1171]],
       dtype=torch.float64)
	q_value: tensor([[-22.9682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16404811199256664, distance: 1.0462785811207962 entropy 0.6056879162788391
epoch: 40, step: 126
	action: tensor([[-0.6086, -0.5695,  0.4803, -0.3601, -0.3428,  0.3737,  0.2186]],
       dtype=torch.float64)
	q_value: tensor([[-22.9147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.98363059225736, distance: 1.6117107106297348 entropy 0.6056879162788391
epoch: 40, step: 127
	action: tensor([[ 0.0212, -0.9121,  0.0742,  0.2807, -0.3851, -0.7328,  0.1115]],
       dtype=torch.float64)
	q_value: tensor([[-22.9695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4716315791487098, distance: 1.3882134331668619 entropy 0.6056879162788391
LOSS epoch 40 actor 249.06616368365653 critic 234.34112416702018 
epoch: 41, step: 0
	action: tensor([[ 0.1593, -0.5254, -0.1255, -0.5295,  0.8578, -0.3929,  1.2226]],
       dtype=torch.float64)
	q_value: tensor([[-26.0630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4428718362073467, distance: 1.3745817432864071 entropy 0.6056879162788391
epoch: 41, step: 1
	action: tensor([[-0.0600, -0.3144,  0.3146,  0.3866,  0.3668,  0.7842,  0.7423]],
       dtype=torch.float64)
	q_value: tensor([[-25.6810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5244584903679526, distance: 0.7891342796226688 entropy 0.6056879162788391
epoch: 41, step: 2
	action: tensor([[-0.0143,  0.6621, -0.3088, -0.3843, -0.1979,  0.5026,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[-25.3859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 3
	action: tensor([[-0.0956, -1.1525,  0.7254, -0.2101,  0.5524, -0.7503,  1.7464]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0323009031922394, distance: 1.631363339021093 entropy 0.6056879162788391
epoch: 41, step: 4
	action: tensor([[ 0.4917, -0.1889,  0.3464,  0.2684,  0.4838, -0.2060, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[-28.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6051938772251395, distance: 0.719032685607343 entropy 0.6056879162788391
epoch: 41, step: 5
	action: tensor([[ 1.2805, -0.6384, -0.1822, -0.0702, -0.0712,  0.5029,  0.3381]],
       dtype=torch.float64)
	q_value: tensor([[-23.4326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16873108974114603, distance: 1.0433438544546154 entropy 0.6056879162788391
epoch: 41, step: 6
	action: tensor([[ 0.5571, -0.5963,  0.0503, -0.0988,  0.2748, -0.4390,  0.5577]],
       dtype=torch.float64)
	q_value: tensor([[-26.6086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10051400341951622, distance: 1.2004787580485006 entropy 0.6056879162788391
epoch: 41, step: 7
	action: tensor([[-0.1432, -0.0646, -0.1446,  0.3419,  0.3240, -0.5431, -0.3080]],
       dtype=torch.float64)
	q_value: tensor([[-23.8643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11842931744775587, distance: 1.0744476558693672 entropy 0.6056879162788391
epoch: 41, step: 8
	action: tensor([[-0.4315, -0.6152,  0.5876, -0.1307,  0.0042, -0.2708,  0.9194]],
       dtype=torch.float64)
	q_value: tensor([[-23.5861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8531506556434754, distance: 1.5578012825895557 entropy 0.6056879162788391
epoch: 41, step: 9
	action: tensor([[ 0.5656, -0.8357,  0.4509, -0.4710, -0.1554, -0.0269,  0.8361]],
       dtype=torch.float64)
	q_value: tensor([[-24.4315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5127212882106491, distance: 1.4074602967090202 entropy 0.6056879162788391
epoch: 41, step: 10
	action: tensor([[ 0.2556, -0.3685,  0.9278, -0.3725,  0.2418,  0.3409, -0.4110]],
       dtype=torch.float64)
	q_value: tensor([[-25.7760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041101640329608835, distance: 1.1205802945781522 entropy 0.6056879162788391
epoch: 41, step: 11
	action: tensor([[ 9.6997e-02,  1.7806e-02,  9.8274e-01,  8.5025e-02, -9.7091e-02,
          1.7783e-01, -5.1438e-05]], dtype=torch.float64)
	q_value: tensor([[-25.3160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5180138313910683, distance: 0.7944635582618491 entropy 0.6056879162788391
epoch: 41, step: 12
	action: tensor([[ 0.8603, -1.1584, -0.2359,  0.4672,  0.2850, -0.3231,  0.6214]],
       dtype=torch.float64)
	q_value: tensor([[-24.5777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2683566539831348, distance: 1.288775849144305 entropy 0.6056879162788391
epoch: 41, step: 13
	action: tensor([[-0.1610, -0.9930,  0.1800, -0.2817,  0.6443,  0.0674,  0.2573]],
       dtype=torch.float64)
	q_value: tensor([[-27.6391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8500536787859818, distance: 1.5564990431164702 entropy 0.6056879162788391
epoch: 41, step: 14
	action: tensor([[-0.2280, -0.5758,  0.1986,  0.1944,  0.7447, -0.9136, -0.5769]],
       dtype=torch.float64)
	q_value: tensor([[-25.4617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5453540646379069, distance: 1.4225603267626634 entropy 0.6056879162788391
epoch: 41, step: 15
	action: tensor([[ 0.1805, -1.1417,  1.0116, -0.3589,  1.2384,  0.0670,  0.7890]],
       dtype=torch.float64)
	q_value: tensor([[-28.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.902527557720008, distance: 1.5784185322904543 entropy 0.6056879162788391
epoch: 41, step: 16
	action: tensor([[-0.4221, -0.5891,  0.4472, -0.3725,  0.2335, -0.4621, -0.1656]],
       dtype=torch.float64)
	q_value: tensor([[-28.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9736043019739704, distance: 1.6076323426987011 entropy 0.6056879162788391
epoch: 41, step: 17
	action: tensor([[ 0.4298, -0.6415,  0.9530, -0.2591,  0.0121, -0.5559,  0.4476]],
       dtype=torch.float64)
	q_value: tensor([[-25.0013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2809470047252989, distance: 1.2951565746766145 entropy 0.6056879162788391
epoch: 41, step: 18
	action: tensor([[-0.3231, -0.0613,  0.4030, -0.0170,  0.3323, -0.4966, -0.1896]],
       dtype=torch.float64)
	q_value: tensor([[-25.1569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30109575272030953, distance: 1.3053029600046002 entropy 0.6056879162788391
epoch: 41, step: 19
	action: tensor([[-0.0822,  0.0062,  0.5655, -0.8794,  0.3819, -0.0766,  0.2906]],
       dtype=torch.float64)
	q_value: tensor([[-24.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38024431243983825, distance: 1.3444191079579195 entropy 0.6056879162788391
epoch: 41, step: 20
	action: tensor([[-0.2868, -0.5338,  0.3465, -0.2058, -0.5204,  0.3132,  0.3272]],
       dtype=torch.float64)
	q_value: tensor([[-23.6439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5522563420861, distance: 1.425733698723614 entropy 0.6056879162788391
epoch: 41, step: 21
	action: tensor([[ 0.2724, -0.8961,  0.2519, -0.7115, -0.4529, -0.3815,  0.6817]],
       dtype=torch.float64)
	q_value: tensor([[-24.2936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8198116791975834, distance: 1.543724928171746 entropy 0.6056879162788391
epoch: 41, step: 22
	action: tensor([[ 0.0680, -0.0231, -0.4035, -0.7717, -0.3412, -0.4785,  0.9264]],
       dtype=torch.float64)
	q_value: tensor([[-26.4342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1490541292391413, distance: 1.0556201239838408 entropy 0.6056879162788391
epoch: 41, step: 23
	action: tensor([[ 0.5775, -1.1760,  0.4650, -1.0548, -0.1742,  0.8060,  0.5906]],
       dtype=torch.float64)
	q_value: tensor([[-25.1508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8615432072954925, distance: 1.56132478425563 entropy 0.6056879162788391
epoch: 41, step: 24
	action: tensor([[ 0.1124, -0.3537,  0.1384, -0.7893,  0.7816, -0.6808,  1.2928]],
       dtype=torch.float64)
	q_value: tensor([[-28.8555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4614247045120359, distance: 1.3833909031990264 entropy 0.6056879162788391
epoch: 41, step: 25
	action: tensor([[-0.1035, -0.2413,  0.4636, -1.1860,  0.2475, -0.4427, -0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-25.9884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6599751237536726, distance: 1.4743733872138223 entropy 0.6056879162788391
epoch: 41, step: 26
	action: tensor([[-0.7391, -1.2560,  0.8082, -0.1340,  0.2806, -0.3832,  0.7633]],
       dtype=torch.float64)
	q_value: tensor([[-25.2366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.344883107937346, distance: 1.7523356332662248 entropy 0.6056879162788391
epoch: 41, step: 27
	action: tensor([[-0.4360, -0.3401,  1.1347, -0.5726,  0.0089, -0.1818,  0.5584]],
       dtype=torch.float64)
	q_value: tensor([[-27.6010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9359213406844058, distance: 1.5922107321882926 entropy 0.6056879162788391
epoch: 41, step: 28
	action: tensor([[-0.1402, -0.6250,  0.6247, -0.2911,  0.2584,  0.2002,  0.5420]],
       dtype=torch.float64)
	q_value: tensor([[-25.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5293070237049555, distance: 1.4151550804461501 entropy 0.6056879162788391
epoch: 41, step: 29
	action: tensor([[ 1.5063, -0.8442,  0.4472, -0.6382, -0.0548, -0.8696, -0.1118]],
       dtype=torch.float64)
	q_value: tensor([[-23.8039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4287844187679364, distance: 1.3678549488005127 entropy 0.6056879162788391
epoch: 41, step: 30
	action: tensor([[-0.2640, -1.3123,  0.0072, -0.2294, -0.2406, -0.0225,  0.2186]],
       dtype=torch.float64)
	q_value: tensor([[-29.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0422981206960267, distance: 1.635370886946977 entropy 0.6056879162788391
epoch: 41, step: 31
	action: tensor([[ 0.4808,  0.1937,  1.2475, -0.4641, -0.2881, -0.2414, -0.3298]],
       dtype=torch.float64)
	q_value: tensor([[-26.4766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5196424213670033, distance: 0.7931202103832052 entropy 0.6056879162788391
epoch: 41, step: 32
	action: tensor([[ 0.4759, -0.4670,  0.2119, -0.3317,  1.0576,  0.2600,  1.0157]],
       dtype=torch.float64)
	q_value: tensor([[-26.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07036573963041137, distance: 1.103348613497183 entropy 0.6056879162788391
epoch: 41, step: 33
	action: tensor([[ 0.4249,  0.0979, -0.2591, -0.5182,  0.6655, -0.5532,  0.3329]],
       dtype=torch.float64)
	q_value: tensor([[-25.8504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36287529510557015, distance: 0.9134166336195727 entropy 0.6056879162788391
epoch: 41, step: 34
	action: tensor([[-0.8631, -1.1075,  0.6762, -0.1865,  0.5640, -0.1603,  0.1234]],
       dtype=torch.float64)
	q_value: tensor([[-23.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.395460176636588, distance: 1.7711329835382832 entropy 0.6056879162788391
epoch: 41, step: 35
	action: tensor([[ 0.6927, -0.5596,  1.4463,  0.0535, -0.1097,  0.3626,  0.5752]],
       dtype=torch.float64)
	q_value: tensor([[-27.5986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2187976413608711, distance: 1.0114361369443947 entropy 0.6056879162788391
epoch: 41, step: 36
	action: tensor([[ 0.0946, -0.9166,  0.5476, -0.8537, -0.3765, -0.4442,  0.3759]],
       dtype=torch.float64)
	q_value: tensor([[-26.7361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9493252829711811, distance: 1.5977133024276002 entropy 0.6056879162788391
epoch: 41, step: 37
	action: tensor([[ 0.3404, -0.7247,  0.6163, -0.1213,  0.9324, -0.2060,  0.3120]],
       dtype=torch.float64)
	q_value: tensor([[-26.3643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29523439184290057, distance: 1.3023594841646102 entropy 0.6056879162788391
epoch: 41, step: 38
	action: tensor([[ 0.5196, -0.5677, -0.3259,  0.7233, -0.4717,  0.4888,  0.4287]],
       dtype=torch.float64)
	q_value: tensor([[-25.1184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6217974602719627, distance: 0.7037508197345959 entropy 0.6056879162788391
epoch: 41, step: 39
	action: tensor([[ 0.4799, -0.5794, -0.0892, -0.3750, -0.1133,  0.2043, -0.0458]],
       dtype=torch.float64)
	q_value: tensor([[-25.8058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08440820585756259, distance: 1.1916620006347032 entropy 0.6056879162788391
epoch: 41, step: 40
	action: tensor([[-0.2070, -0.3670,  0.6072, -0.1024, -0.1700, -0.0310, -0.0674]],
       dtype=torch.float64)
	q_value: tensor([[-23.6026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3085461053877132, distance: 1.309034847252741 entropy 0.6056879162788391
epoch: 41, step: 41
	action: tensor([[ 0.3247, -0.7038, -0.1542, -0.0457,  0.6068, -0.0665,  0.5004]],
       dtype=torch.float64)
	q_value: tensor([[-23.6813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16593283505941048, distance: 1.2356442706833224 entropy 0.6056879162788391
epoch: 41, step: 42
	action: tensor([[ 0.8745, -0.4693,  0.6837, -0.3257, -0.2390,  0.4412,  1.3281]],
       dtype=torch.float64)
	q_value: tensor([[-24.1028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843862187178646, distance: 0.9680461031988464 entropy 0.6056879162788391
epoch: 41, step: 43
	action: tensor([[ 0.0830, -1.3382, -0.7984,  0.1547,  0.5760,  0.2857,  0.4839]],
       dtype=torch.float64)
	q_value: tensor([[-27.1889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6895614114139985, distance: 1.4874544806248058 entropy 0.6056879162788391
epoch: 41, step: 44
	action: tensor([[-0.1248, -0.9577,  1.0063,  0.4446, -0.1526,  0.6112,  0.0455]],
       dtype=torch.float64)
	q_value: tensor([[-27.9216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06370035815742847, distance: 1.1072969964703365 entropy 0.6056879162788391
epoch: 41, step: 45
	action: tensor([[ 0.2382, -0.7733, -0.0436, -0.8444,  0.1386,  0.7718,  0.5853]],
       dtype=torch.float64)
	q_value: tensor([[-28.4922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41131311782617197, distance: 1.359466100001371 entropy 0.6056879162788391
epoch: 41, step: 46
	action: tensor([[ 0.2403, -0.0716,  0.8798, -0.2791, -0.3944,  0.0058, -0.1784]],
       dtype=torch.float64)
	q_value: tensor([[-26.8486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28064376226966126, distance: 0.9705741051572553 entropy 0.6056879162788391
epoch: 41, step: 47
	action: tensor([[ 0.5737, -1.1908, -0.0718,  0.2421,  0.6996,  0.4408,  0.3756]],
       dtype=torch.float64)
	q_value: tensor([[-24.5084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1928962249618409, distance: 1.249850376530873 entropy 0.6056879162788391
epoch: 41, step: 48
	action: tensor([[-0.3528,  0.6129,  0.6993, -0.3354,  0.1024,  0.0309, -0.2387]],
       dtype=torch.float64)
	q_value: tensor([[-27.2760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 49
	action: tensor([[-0.1456, -2.4879,  0.8561, -0.7879,  0.5136, -0.2987,  1.1569]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 50
	action: tensor([[ 0.8535, -1.7314,  1.4417, -1.1563,  0.4643, -0.3485,  1.5911]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24974376349862615, distance: 1.2792846308733659 entropy 0.6056879162788391
epoch: 41, step: 51
	action: tensor([[ 0.5690, -0.7958, -0.3038, -0.6716, -0.1987, -0.5086,  1.0432]],
       dtype=torch.float64)
	q_value: tensor([[-30.5669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5181050319809926, distance: 1.409962633285028 entropy 0.6056879162788391
epoch: 41, step: 52
	action: tensor([[-0.1367,  0.1102,  0.3402, -0.0203, -0.3638, -0.1340,  0.5342]],
       dtype=torch.float64)
	q_value: tensor([[-27.3052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.204935978783633, distance: 1.0203701464550314 entropy 0.6056879162788391
epoch: 41, step: 53
	action: tensor([[-0.4353, -0.4837,  0.4973, -0.2371,  0.3698, -1.2201,  0.0721]],
       dtype=torch.float64)
	q_value: tensor([[-22.8054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7968048404491781, distance: 1.5339356738817351 entropy 0.6056879162788391
epoch: 41, step: 54
	action: tensor([[-0.0594, -1.4817,  1.1403, -0.3930,  0.7891,  0.0136, -0.2510]],
       dtype=torch.float64)
	q_value: tensor([[-26.6800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 55
	action: tensor([[ 0.1460, -2.0942,  1.2674, -0.7000,  0.7737, -0.4176,  1.3916]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 56
	action: tensor([[ 0.5185, -2.2817,  0.6255, -1.2293,  0.4619, -1.2331,  1.5181]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 57
	action: tensor([[ 0.2160, -2.2989,  1.5800, -0.9701,  0.9379, -0.6554,  0.8855]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 58
	action: tensor([[ 0.6365, -2.2252,  1.5159, -1.2161,  0.5755, -0.5506,  1.7047]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 59
	action: tensor([[ 0.7471, -1.3335,  1.2593, -1.1433,  1.4051, -0.5261,  0.7658]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3583352488531204, distance: 1.3337062263709683 entropy 0.6056879162788391
epoch: 41, step: 60
	action: tensor([[ 0.0202,  0.0595,  0.4932, -0.4529, -0.9545,  0.4191, -0.3340]],
       dtype=torch.float64)
	q_value: tensor([[-30.9715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13537510207736914, distance: 1.0640708892744042 entropy 0.6056879162788391
epoch: 41, step: 61
	action: tensor([[ 0.0365, -0.3451, -0.5177, -0.3827,  0.7528, -0.1654,  1.1339]],
       dtype=torch.float64)
	q_value: tensor([[-26.2057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08136182963483352, distance: 1.1899869838261303 entropy 0.6056879162788391
epoch: 41, step: 62
	action: tensor([[-0.0174, -0.2402,  0.5874,  0.4421,  1.1868, -0.1069, -0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-25.2947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.384088212440011, distance: 0.8980819155606821 entropy 0.6056879162788391
epoch: 41, step: 63
	action: tensor([[ 0.2246, -0.5901,  0.6125, -0.3703,  0.0975, -0.2355,  0.2309]],
       dtype=torch.float64)
	q_value: tensor([[-27.1790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4026366969555919, distance: 1.3552808187493695 entropy 0.6056879162788391
epoch: 41, step: 64
	action: tensor([[-0.8284, -0.6535,  0.1952,  0.4004,  0.3267,  0.2104,  0.2678]],
       dtype=torch.float64)
	q_value: tensor([[-23.7417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8114657373086178, distance: 1.5401809778588833 entropy 0.6056879162788391
epoch: 41, step: 65
	action: tensor([[ 0.4401, -1.2680,  0.4638, -0.7282,  0.3813, -0.7126,  0.6491]],
       dtype=torch.float64)
	q_value: tensor([[-24.9279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9712750612002543, distance: 1.6066834016469473 entropy 0.6056879162788391
epoch: 41, step: 66
	action: tensor([[ 0.1303, -0.7051,  0.1661, -0.1925,  0.4157,  0.4746, -0.2888]],
       dtype=torch.float64)
	q_value: tensor([[-27.7122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15529556222036844, distance: 1.229994716258623 entropy 0.6056879162788391
epoch: 41, step: 67
	action: tensor([[ 0.7054, -0.4404,  0.8551, -0.1901,  0.3411,  0.2839,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-24.7696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2848003802917698, distance: 0.9677659342486351 entropy 0.6056879162788391
epoch: 41, step: 68
	action: tensor([[-0.3600, -0.7776, -0.4235,  0.5630, -0.0281, -0.4339, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[-24.5713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5798455599424055, distance: 1.4383481206928082 entropy 0.6056879162788391
epoch: 41, step: 69
	action: tensor([[ 0.7743, -0.5372,  0.3175, -0.0143, -0.0231, -0.8110,  1.1700]],
       dtype=torch.float64)
	q_value: tensor([[-26.0718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007914989233833714, distance: 1.1398065209098673 entropy 0.6056879162788391
epoch: 41, step: 70
	action: tensor([[ 0.7021, -0.3421,  0.5121,  0.2201,  0.2845, -0.9053, -0.8656]],
       dtype=torch.float64)
	q_value: tensor([[-26.3241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3754236774113606, distance: 0.9043768803145993 entropy 0.6056879162788391
epoch: 41, step: 71
	action: tensor([[ 0.2271, -1.0162,  0.3921, -1.1734, -0.0925, -0.4318,  0.4476]],
       dtype=torch.float64)
	q_value: tensor([[-28.1909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9220430631446537, distance: 1.5864933284913427 entropy 0.6056879162788391
epoch: 41, step: 72
	action: tensor([[ 0.3180, -1.0939,  0.7740,  0.0815,  0.4388,  0.8198,  0.5948]],
       dtype=torch.float64)
	q_value: tensor([[-27.3363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19962784250476706, distance: 1.2533719228180316 entropy 0.6056879162788391
epoch: 41, step: 73
	action: tensor([[ 1.3033, -0.7610,  0.3425, -0.3148,  0.3245,  0.2380,  0.3830]],
       dtype=torch.float64)
	q_value: tensor([[-27.6203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26089270296475764, distance: 1.284978197499293 entropy 0.6056879162788391
epoch: 41, step: 74
	action: tensor([[ 0.3632, -0.0904,  0.6657, -0.4434,  0.6988,  0.2485,  0.4234]],
       dtype=torch.float64)
	q_value: tensor([[-26.1282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2911359878363585, distance: 0.9634699138035231 entropy 0.6056879162788391
epoch: 41, step: 75
	action: tensor([[ 0.0440, -0.6527,  0.0752,  0.2414,  0.7816, -0.2629,  0.9467]],
       dtype=torch.float64)
	q_value: tensor([[-23.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1589488477047667, distance: 1.2319379311518177 entropy 0.6056879162788391
epoch: 41, step: 76
	action: tensor([[ 0.4583,  0.1793,  0.1129,  0.2263, -0.6267,  0.2903,  0.4506]],
       dtype=torch.float64)
	q_value: tensor([[-24.8625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8681417357885319, distance: 0.41553764146598543 entropy 0.6056879162788391
epoch: 41, step: 77
	action: tensor([[ 0.9534, -0.2497,  0.5995, -1.2369, -0.1920, -0.4147,  0.8476]],
       dtype=torch.float64)
	q_value: tensor([[-23.9564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08369449494849901, distance: 1.1912697857140462 entropy 0.6056879162788391
epoch: 41, step: 78
	action: tensor([[ 0.6937, -0.1972,  0.2984,  0.2767, -0.0051,  0.0487,  0.2388]],
       dtype=torch.float64)
	q_value: tensor([[-27.2593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7277071394784904, distance: 0.5971381529486994 entropy 0.6056879162788391
epoch: 41, step: 79
	action: tensor([[ 0.6143,  0.4552,  0.3165, -0.3614, -0.1042, -0.0537,  0.2621]],
       dtype=torch.float64)
	q_value: tensor([[-23.1537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8345559295069069, distance: 0.465459926515016 entropy 0.6056879162788391
epoch: 41, step: 80
	action: tensor([[ 0.0478, -0.4442,  0.8085,  0.3911,  0.1148,  0.3932, -0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-23.0448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44017599267499674, distance: 0.856214252123632 entropy 0.6056879162788391
epoch: 41, step: 81
	action: tensor([[-0.0690, -0.7515,  0.7453, -0.5060, -0.0175, -0.6461,  0.6197]],
       dtype=torch.float64)
	q_value: tensor([[-26.0216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8609797806731712, distance: 1.5610884860812253 entropy 0.6056879162788391
epoch: 41, step: 82
	action: tensor([[ 0.1659, -1.0871,  0.2514, -1.0802,  0.6748,  0.3901,  0.5982]],
       dtype=torch.float64)
	q_value: tensor([[-25.5914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9464389371564608, distance: 1.5965300054482197 entropy 0.6056879162788391
epoch: 41, step: 83
	action: tensor([[-0.3587, -0.6859, -0.3780, -0.0111,  0.4166,  0.2090, -0.9994]],
       dtype=torch.float64)
	q_value: tensor([[-27.8485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5450657357542337, distance: 1.4224276114276848 entropy 0.6056879162788391
epoch: 41, step: 84
	action: tensor([[-0.0154, -1.7213,  0.6797, -1.1946, -0.3725,  0.2828,  0.9097]],
       dtype=torch.float64)
	q_value: tensor([[-26.6991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 85
	action: tensor([[ 0.2447, -2.4617,  1.2443, -1.6619,  1.0392, -0.5589,  1.4316]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 86
	action: tensor([[ 0.2802, -1.9275,  1.0405, -0.8061,  0.6664, -0.1846,  1.6799]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 87
	action: tensor([[ 0.4502, -2.7601,  1.3980, -0.3585,  0.1974,  0.2768,  1.9810]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 88
	action: tensor([[ 0.4981, -1.7036,  1.2215, -1.5711,  1.3445, -0.5843,  0.5698]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31423701509835, distance: 1.3118782766973744 entropy 0.6056879162788391
epoch: 41, step: 89
	action: tensor([[ 0.2635,  0.8804,  0.2642,  0.0748,  0.7275, -0.8607,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[-32.3957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 90
	action: tensor([[ 0.3594, -1.6411,  1.1046, -0.4916,  0.7394, -0.2779,  1.8107]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8815079498112819, distance: 1.5696749303121469 entropy 0.6056879162788391
epoch: 41, step: 91
	action: tensor([[ 0.9031, -0.7684, -0.1132,  0.1191,  0.4552, -0.3400, -0.3934]],
       dtype=torch.float64)
	q_value: tensor([[-29.5087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08332468879122423, distance: 1.1910665104860143 entropy 0.6056879162788391
epoch: 41, step: 92
	action: tensor([[ 0.2549, -0.2074,  0.1732, -0.2558, -0.5745, -0.3272,  1.2335]],
       dtype=torch.float64)
	q_value: tensor([[-26.0718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12938727327939392, distance: 1.0677490630072297 entropy 0.6056879162788391
epoch: 41, step: 93
	action: tensor([[ 0.4278, -0.7815,  0.6100,  0.0646, -0.4965,  0.1218,  0.7955]],
       dtype=torch.float64)
	q_value: tensor([[-25.9069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07023502650670066, distance: 1.183848895964645 entropy 0.6056879162788391
epoch: 41, step: 94
	action: tensor([[-0.2687, -0.4404, -0.1129, -0.7614, -0.4088,  0.4655,  1.0218]],
       dtype=torch.float64)
	q_value: tensor([[-25.8591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5670828687438869, distance: 1.4325265333618367 entropy 0.6056879162788391
epoch: 41, step: 95
	action: tensor([[ 0.0925, -0.8652,  0.7413, -0.8422,  0.2813, -0.1846, -0.2535]],
       dtype=torch.float64)
	q_value: tensor([[-26.6252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9193376691510187, distance: 1.5853763917761488 entropy 0.6056879162788391
epoch: 41, step: 96
	action: tensor([[ 0.0279, -1.4425,  0.0216,  0.3186,  0.5994, -0.7286,  0.3109]],
       dtype=torch.float64)
	q_value: tensor([[-26.1972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 97
	action: tensor([[ 0.2141, -1.8306,  1.5606, -0.9824,  0.2444, -0.2990,  1.7324]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 98
	action: tensor([[ 0.9732, -1.2989,  2.5359, -0.7197,  0.2608, -0.8334,  1.1890]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 99
	action: tensor([[ 0.7014, -2.7892,  1.2657, -0.5500,  0.6989, -0.3161,  0.9103]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 100
	action: tensor([[ 0.0922, -2.8242,  0.6881, -1.4331,  0.4879, -0.7447,  1.5513]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 101
	action: tensor([[ 0.2959, -1.8507,  0.7128, -0.4872,  0.2148, -0.6799,  1.5531]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 102
	action: tensor([[ 0.5030, -2.3518,  1.4688, -1.1317,  1.0282,  0.0141,  0.4918]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 103
	action: tensor([[ 0.7528, -1.3848,  1.8865, -1.2974,  0.7214, -1.1100,  1.4725]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 104
	action: tensor([[ 0.3570, -2.3888,  1.8482, -0.5132, -0.0258, -0.5827,  1.5467]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 105
	action: tensor([[ 0.5859, -0.8968,  1.6263, -0.4936,  0.3561, -1.1943,  1.7202]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 106
	action: tensor([[ 1.1096, -1.7265,  1.6014, -0.8614,  1.2791, -0.0101,  1.3257]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 107
	action: tensor([[ 0.3877, -2.0645,  1.2988, -1.1483,  0.6294, -0.4208,  1.9371]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 108
	action: tensor([[ 0.1501, -2.2072,  0.8681, -0.3237,  0.7366, -0.4494,  0.9255]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 109
	action: tensor([[-0.0753, -2.5911,  1.0896, -0.3741,  0.3554, -0.5239,  1.0557]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 110
	action: tensor([[ 1.4871e-03, -2.2821e+00,  9.1661e-01, -9.2047e-01,  6.9789e-01,
         -8.3252e-01,  1.6629e+00]], dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 111
	action: tensor([[ 0.2019, -1.3437,  1.5199, -0.0537,  1.0562, -0.0584,  1.4171]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7651893299991606, distance: 1.5203806724368276 entropy 0.6056879162788391
epoch: 41, step: 112
	action: tensor([[ 0.0185,  0.1856, -0.1624, -0.2141,  0.5462,  0.3784, -0.5597]],
       dtype=torch.float64)
	q_value: tensor([[-30.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5170377201618803, distance: 0.7952676192732149 entropy 0.6056879162788391
epoch: 41, step: 113
	action: tensor([[ 0.2909, -0.7651,  0.4216,  0.1262,  0.4772,  0.3903,  0.9505]],
       dtype=torch.float64)
	q_value: tensor([[-23.4470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10375316567225046, distance: 1.0833542979236912 entropy 0.6056879162788391
epoch: 41, step: 114
	action: tensor([[ 0.1784,  0.0623, -0.4777,  0.0998,  0.0632, -0.0399,  0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-25.2001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5370009701861052, distance: 0.7786579709976109 entropy 0.6056879162788391
epoch: 41, step: 115
	action: tensor([[ 0.1530, -0.3475,  0.2130, -0.4355,  0.4966, -0.1579,  0.1494]],
       dtype=torch.float64)
	q_value: tensor([[-21.3192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19356403265799216, distance: 1.2502001734791812 entropy 0.6056879162788391
epoch: 41, step: 116
	action: tensor([[-0.1570, -0.8074,  0.5745, -0.7279,  0.5357, -0.5198,  0.5708]],
       dtype=torch.float64)
	q_value: tensor([[-22.8167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0040651791898338, distance: 1.6199910469230343 entropy 0.6056879162788391
epoch: 41, step: 117
	action: tensor([[-0.1512, -0.1366,  0.8224, -0.1604,  0.3682,  0.1485,  0.3784]],
       dtype=torch.float64)
	q_value: tensor([[-25.7251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04746594312429542, distance: 1.1711880944273967 entropy 0.6056879162788391
epoch: 41, step: 118
	action: tensor([[ 0.3607,  0.2756,  0.2848,  0.3348, -0.2420,  0.3477, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[-23.6089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 119
	action: tensor([[ 0.7094, -2.5711,  1.6446, -0.5371,  1.0069, -1.1093,  0.9200]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 120
	action: tensor([[-0.7677, -1.7117,  1.2512, -1.5281,  1.3345, -0.7332,  0.7976]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.032804584899763, distance: 1.631565483537637 entropy 0.6056879162788391
epoch: 41, step: 121
	action: tensor([[-0.1635, -1.5396,  0.3217, -0.8431, -0.8009, -0.5452,  0.3266]],
       dtype=torch.float64)
	q_value: tensor([[-32.5645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 122
	action: tensor([[-0.5949, -1.7797,  1.6191, -0.7545,  0.5534,  0.5147,  1.3848]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 123
	action: tensor([[-0.4102, -1.7993,  1.2846, -0.5617,  0.2987, -0.4596,  0.9676]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 41, step: 124
	action: tensor([[ 0.2561, -1.5737,  1.6437, -1.2861,  1.4456,  0.1077,  1.4385]],
       dtype=torch.float64)
	q_value: tensor([[-28.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6678417620110411, distance: 1.477862792929525 entropy 0.6056879162788391
epoch: 41, step: 125
	action: tensor([[ 0.2937,  0.0016,  0.3704,  0.2690,  0.1974,  0.3402, -0.7577]],
       dtype=torch.float64)
	q_value: tensor([[-32.7814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8063627735778575, distance: 0.503559878344821 entropy 0.6056879162788391
epoch: 41, step: 126
	action: tensor([[ 0.5522, -0.2381,  0.5334, -0.7949, -0.4192, -0.2793,  0.3146]],
       dtype=torch.float64)
	q_value: tensor([[-25.5163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07982047169801176, distance: 1.18913858604315 entropy 0.6056879162788391
epoch: 41, step: 127
	action: tensor([[ 1.3639, -0.4611,  0.5027, -0.0172,  0.5992,  0.1955,  0.3918]],
       dtype=torch.float64)
	q_value: tensor([[-24.5456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11632952204736935, distance: 1.0757264973940228 entropy 0.6056879162788391
LOSS epoch 41 actor 281.86339531171694 critic 256.4450587481686 
epoch: 42, step: 0
	action: tensor([[ 0.5833,  0.1590,  0.7419, -0.1350, -0.0074,  0.5424, -0.1136]],
       dtype=torch.float64)
	q_value: tensor([[-28.9926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8633075969488861, distance: 0.423086221946719 entropy 0.6056879162788391
epoch: 42, step: 1
	action: tensor([[ 1.0238, -0.1503,  0.6684, -0.1059,  0.7774,  0.3233,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-27.5237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5755257141163657, distance: 0.7455596339825656 entropy 0.6056879162788391
epoch: 42, step: 2
	action: tensor([[ 0.2311,  0.8616, -0.0445,  0.1920,  0.5219,  0.2278, -0.5980]],
       dtype=torch.float64)
	q_value: tensor([[-28.5967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 3
	action: tensor([[-0.0635, -1.6328,  1.5977, -0.2891,  0.6834, -1.0510,  1.2546]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6748536782485586, distance: 1.4809661392899547 entropy 0.6056879162788391
epoch: 42, step: 4
	action: tensor([[ 0.1799,  0.0418,  0.3817, -0.0067,  0.5020, -0.1171,  0.0864]],
       dtype=torch.float64)
	q_value: tensor([[-33.7051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.437941057183168, distance: 0.8579216436798369 entropy 0.6056879162788391
epoch: 42, step: 5
	action: tensor([[-0.2809, -0.5356,  0.0106,  0.1922, -0.0605, -0.2164, -0.1916]],
       dtype=torch.float64)
	q_value: tensor([[-25.5021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3968280117584684, distance: 1.352471621263168 entropy 0.6056879162788391
epoch: 42, step: 6
	action: tensor([[ 0.4154, -0.4227,  0.5526,  0.1028,  0.3879,  0.9907,  0.3269]],
       dtype=torch.float64)
	q_value: tensor([[-26.5189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6478654608869856, distance: 0.6790644665950517 entropy 0.6056879162788391
epoch: 42, step: 7
	action: tensor([[ 0.8884,  0.0212,  0.8754,  0.3005,  0.5610, -0.0451,  0.4816]],
       dtype=torch.float64)
	q_value: tensor([[-29.0166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.842032363080813, distance: 0.4548212568680978 entropy 0.6056879162788391
epoch: 42, step: 8
	action: tensor([[-0.0181, -0.6142,  0.1868, -0.4339, -0.1880,  0.2274,  0.7959]],
       dtype=torch.float64)
	q_value: tensor([[-28.3813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49125707613429603, distance: 1.3974392977364873 entropy 0.6056879162788391
epoch: 42, step: 9
	action: tensor([[-0.0395, -0.7075,  0.1239, -0.2346, -0.1194, -0.4220,  0.1662]],
       dtype=torch.float64)
	q_value: tensor([[-27.4028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6219747146361667, distance: 1.4573998922231584 entropy 0.6056879162788391
epoch: 42, step: 10
	action: tensor([[ 0.8890, -0.6403,  1.0121, -0.2142, -0.0449,  0.2581, -0.1342]],
       dtype=torch.float64)
	q_value: tensor([[-26.5580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09185437071762048, distance: 1.090522026222746 entropy 0.6056879162788391
epoch: 42, step: 11
	action: tensor([[-0.5223, -0.2079,  0.1146, -0.0357,  0.1825,  0.7255,  0.0871]],
       dtype=torch.float64)
	q_value: tensor([[-28.9392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19671941253117486, distance: 1.251851636065526 entropy 0.6056879162788391
epoch: 42, step: 12
	action: tensor([[-0.5118, -0.6660, -0.0597, -0.2382, -0.1605, -0.5063, -0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-27.2462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8109236623689282, distance: 1.5399505136655527 entropy 0.6056879162788391
epoch: 42, step: 13
	action: tensor([[-0.0821, -0.6665,  0.2997, -0.2647,  0.3598, -0.3717, -0.3708]],
       dtype=torch.float64)
	q_value: tensor([[-27.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6585384877868279, distance: 1.473735246227948 entropy 0.6056879162788391
epoch: 42, step: 14
	action: tensor([[-0.1735, -0.0123,  0.7263, -0.4456,  0.2901,  0.5828,  0.1163]],
       dtype=torch.float64)
	q_value: tensor([[-28.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010657629379047462, distance: 1.1504260909264232 entropy 0.6056879162788391
epoch: 42, step: 15
	action: tensor([[ 0.5599, -0.9234, -0.4673,  0.3579,  0.7833,  0.2360,  0.0827]],
       dtype=torch.float64)
	q_value: tensor([[-27.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047000681343823736, distance: 1.1171281314865946 entropy 0.6056879162788391
epoch: 42, step: 16
	action: tensor([[-0.9358, -0.6139, -0.0623, -0.1408,  0.2372,  0.0183, -0.2752]],
       dtype=torch.float64)
	q_value: tensor([[-29.7703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2389995214531133, distance: 1.712315128148006 entropy 0.6056879162788391
epoch: 42, step: 17
	action: tensor([[ 0.4495, -1.1085,  0.2174, -1.3806, -0.3220, -0.0736,  0.4097]],
       dtype=torch.float64)
	q_value: tensor([[-27.9773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9161680867747737, distance: 1.5840668105017617 entropy 0.6056879162788391
epoch: 42, step: 18
	action: tensor([[ 0.5548, -0.2707,  0.8716, -0.1472,  0.1329,  0.1058,  0.6109]],
       dtype=torch.float64)
	q_value: tensor([[-31.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36896640136744197, distance: 0.9090398770831054 entropy 0.6056879162788391
epoch: 42, step: 19
	action: tensor([[-0.4805, -0.0423,  0.2340, -0.3233,  0.0209, -0.0597,  0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-26.4582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5045720958993656, distance: 1.4036641071712157 entropy 0.6056879162788391
epoch: 42, step: 20
	action: tensor([[-0.3499, -0.1731,  0.3533, -0.9896,  0.1477,  0.3539,  0.8718]],
       dtype=torch.float64)
	q_value: tensor([[-24.9223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6618133807361279, distance: 1.4751895221093831 entropy 0.6056879162788391
epoch: 42, step: 21
	action: tensor([[-0.3018,  0.0272, -0.3527,  0.7298,  0.3625, -0.4622,  0.2819]],
       dtype=torch.float64)
	q_value: tensor([[-28.9809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05002206228110284, distance: 1.1153558589069743 entropy 0.6056879162788391
epoch: 42, step: 22
	action: tensor([[ 0.4599, -0.1282, -0.0830, -0.4236,  0.2673,  0.1102,  0.2021]],
       dtype=torch.float64)
	q_value: tensor([[-26.9558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3397534426941674, distance: 0.9298432996750171 entropy 0.6056879162788391
epoch: 42, step: 23
	action: tensor([[ 0.3825, -0.0149, -0.4982, -0.4284, -0.1266,  0.7591,  0.2253]],
       dtype=torch.float64)
	q_value: tensor([[-25.1463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5818313130975767, distance: 0.7400012405825741 entropy 0.6056879162788391
epoch: 42, step: 24
	action: tensor([[ 0.5499,  0.1565, -0.3486,  0.4699,  0.1739,  0.4800, -0.0200]],
       dtype=torch.float64)
	q_value: tensor([[-27.0880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8958695406765504, distance: 0.36927132550229946 entropy 0.6056879162788391
epoch: 42, step: 25
	action: tensor([[ 0.7108, -0.0858,  0.1592,  0.7440,  0.5243,  0.3718,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[-26.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9901190480659177, distance: 0.11375122612235199 entropy 0.6056879162788391
epoch: 42, step: 26
	action: tensor([[-0.9646,  0.0395,  0.3461, -0.8395,  0.5138, -0.4419, -0.2280]],
       dtype=torch.float64)
	q_value: tensor([[-28.2575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2360442184805525, distance: 1.7111846944703215 entropy 0.6056879162788391
epoch: 42, step: 27
	action: tensor([[ 0.3179, -0.4483,  0.6267, -0.0314,  0.6515, -0.6178, -0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-28.6663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003928777692259078, distance: 1.1420941046521624 entropy 0.6056879162788391
epoch: 42, step: 28
	action: tensor([[ 0.5284, -0.1657,  0.7559, -0.4337,  0.2925, -0.5444,  0.5729]],
       dtype=torch.float64)
	q_value: tensor([[-28.2038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18594110338110081, distance: 1.0324870346188622 entropy 0.6056879162788391
epoch: 42, step: 29
	action: tensor([[ 0.4431, -0.1630, -0.5195, -0.3353,  0.3962,  0.2386, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-26.6687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4317010982442925, distance: 0.8626708070051811 entropy 0.6056879162788391
epoch: 42, step: 30
	action: tensor([[ 0.2988, -0.5072, -0.1340, -0.1179,  0.2577, -0.1588,  0.6695]],
       dtype=torch.float64)
	q_value: tensor([[-25.7759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04776478328423028, distance: 1.171355151447177 entropy 0.6056879162788391
epoch: 42, step: 31
	action: tensor([[-0.1321, -0.4796, -0.4032, -0.2317, -0.0626, -0.4625,  0.8421]],
       dtype=torch.float64)
	q_value: tensor([[-26.0299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34140556198832805, distance: 1.3253688056025899 entropy 0.6056879162788391
epoch: 42, step: 32
	action: tensor([[ 0.9244,  0.0985, -0.1966, -0.6803,  1.0161, -0.3932,  0.5008]],
       dtype=torch.float64)
	q_value: tensor([[-27.2618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43114636817437646, distance: 0.8630917410521212 entropy 0.6056879162788391
epoch: 42, step: 33
	action: tensor([[ 0.1637, -0.6762,  0.1587, -0.3168, -0.3331,  0.0304, -0.1293]],
       dtype=torch.float64)
	q_value: tensor([[-28.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4198852666711492, distance: 1.363588467881592 entropy 0.6056879162788391
epoch: 42, step: 34
	action: tensor([[ 0.0721, -1.4132,  0.4827,  0.6613,  0.7063, -0.3608,  0.2235]],
       dtype=torch.float64)
	q_value: tensor([[-26.3612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 35
	action: tensor([[ 0.4227, -1.2121,  0.5561,  0.4407,  0.0033,  0.6881,  1.0860]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1020952443189489, distance: 1.0843558559132835 entropy 0.6056879162788391
epoch: 42, step: 36
	action: tensor([[ 0.1448, -0.1701,  0.2931, -0.3718,  0.3795, -0.1232, -0.1361]],
       dtype=torch.float64)
	q_value: tensor([[-31.6398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0004437262683230081, distance: 1.144598113644827 entropy 0.6056879162788391
epoch: 42, step: 37
	action: tensor([[ 0.9358, -0.5895,  0.2531,  0.3860,  0.3301, -0.2112,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-25.4101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3956919435663714, distance: 0.8895817923649244 entropy 0.6056879162788391
epoch: 42, step: 38
	action: tensor([[ 0.2322, -0.0123,  1.0594, -0.0264,  0.1962, -0.4067, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-27.8165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37347056831173997, distance: 0.9057898125073892 entropy 0.6056879162788391
epoch: 42, step: 39
	action: tensor([[-0.2208, -0.4657, -0.2287, -0.3513, -0.1313,  0.1341,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[-28.2412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4537292636822159, distance: 1.379743827075789 entropy 0.6056879162788391
epoch: 42, step: 40
	action: tensor([[-0.0396,  0.3140,  0.8552, -0.4642,  0.9431, -0.2384,  0.6232]],
       dtype=torch.float64)
	q_value: tensor([[-25.5788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06086400952196269, distance: 1.108972905149217 entropy 0.6056879162788391
epoch: 42, step: 41
	action: tensor([[ 0.1267, -0.3028, -0.0809,  0.0812, -0.5856,  0.6558,  0.2989]],
       dtype=torch.float64)
	q_value: tensor([[-28.7671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.307679566458976, distance: 0.9521607350284289 entropy 0.6056879162788391
epoch: 42, step: 42
	action: tensor([[ 0.1344, -0.7073,  0.1901, -0.4960,  0.2508, -0.0617, -0.6324]],
       dtype=torch.float64)
	q_value: tensor([[-27.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5675707158376642, distance: 1.4327494952544633 entropy 0.6056879162788391
epoch: 42, step: 43
	action: tensor([[ 0.5326,  0.1774,  0.2791, -0.4106,  0.5000, -0.2919, -0.5730]],
       dtype=torch.float64)
	q_value: tensor([[-28.2797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5375559203707436, distance: 0.7781911817408648 entropy 0.6056879162788391
epoch: 42, step: 44
	action: tensor([[ 0.3039, -0.1625,  0.3843,  0.3633,  0.5178, -0.4331,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-27.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5275640650610149, distance: 0.7865532959630572 entropy 0.6056879162788391
epoch: 42, step: 45
	action: tensor([[ 0.0085, -0.0712,  0.7007, -0.2724, -0.0046, -0.3609, -0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-26.7511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07114261652778064, distance: 1.1843507585104325 entropy 0.6056879162788391
epoch: 42, step: 46
	action: tensor([[ 0.9629,  0.0338,  0.3551, -0.1614, -0.2743, -0.2562,  0.2250]],
       dtype=torch.float64)
	q_value: tensor([[-26.3154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6407913985974538, distance: 0.6858514434791164 entropy 0.6056879162788391
epoch: 42, step: 47
	action: tensor([[ 0.6606,  0.0864,  1.1335, -0.5289, -0.0816, -0.2875,  0.1337]],
       dtype=torch.float64)
	q_value: tensor([[-26.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5141888317167027, distance: 0.7976097249947888 entropy 0.6056879162788391
epoch: 42, step: 48
	action: tensor([[-0.3288,  0.3182,  0.6200, -0.5646,  0.3210, -0.0084,  0.2015]],
       dtype=torch.float64)
	q_value: tensor([[-27.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25536571105398687, distance: 1.2821588204101668 entropy 0.6056879162788391
epoch: 42, step: 49
	action: tensor([[ 1.6167, -1.0192,  0.4343, -0.7165,  0.1448, -0.2339,  0.3939]],
       dtype=torch.float64)
	q_value: tensor([[-26.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6518103876645003, distance: 1.4707429989383718 entropy 0.6056879162788391
epoch: 42, step: 50
	action: tensor([[-0.2521,  0.4247, -0.7257,  0.0021,  0.0325,  0.8982,  0.2327]],
       dtype=torch.float64)
	q_value: tensor([[-31.5625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 51
	action: tensor([[-0.2835, -1.1314,  0.9630, -0.5453,  0.0465, -0.3677,  0.9966]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2566104139797512, distance: 1.7190360614275528 entropy 0.6056879162788391
epoch: 42, step: 52
	action: tensor([[ 0.3855, -0.4549, -0.2319,  1.0700,  0.0190,  0.0606,  0.6352]],
       dtype=torch.float64)
	q_value: tensor([[-30.2467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7172529419615341, distance: 0.6084932159597979 entropy 0.6056879162788391
epoch: 42, step: 53
	action: tensor([[ 0.5774, -0.1302,  0.5139, -0.2038,  0.3650, -0.3386, -0.1123]],
       dtype=torch.float64)
	q_value: tensor([[-28.8268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38534635791298666, distance: 0.8971641743026035 entropy 0.6056879162788391
epoch: 42, step: 54
	action: tensor([[ 0.1875, -0.1682,  0.7490, -0.0866, -0.0634, -0.6153,  0.6000]],
       dtype=torch.float64)
	q_value: tensor([[-26.3745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07345988497307643, distance: 1.101510919613127 entropy 0.6056879162788391
epoch: 42, step: 55
	action: tensor([[-0.0820,  0.2896,  0.1163, -0.4459,  0.3507,  0.7252, -0.2527]],
       dtype=torch.float64)
	q_value: tensor([[-26.4490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4146721845427156, distance: 0.875500250460718 entropy 0.6056879162788391
epoch: 42, step: 56
	action: tensor([[-0.3023, -0.7512,  0.8138, -0.6564,  0.0770, -0.1589,  0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-27.0747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1043627311552395, distance: 1.6600340420151252 entropy 0.6056879162788391
epoch: 42, step: 57
	action: tensor([[ 0.0653, -0.2481, -0.1008, -0.3355,  0.5661,  0.1961,  0.5072]],
       dtype=torch.float64)
	q_value: tensor([[-28.0760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04123457476681314, distance: 1.1205026174852368 entropy 0.6056879162788391
epoch: 42, step: 58
	action: tensor([[-0.7609, -0.2618,  0.6037, -0.0705,  0.2236,  0.1259, -0.0093]],
       dtype=torch.float64)
	q_value: tensor([[-25.7053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8586526973793642, distance: 1.5601121403551388 entropy 0.6056879162788391
epoch: 42, step: 59
	action: tensor([[ 0.1074, -0.5162,  0.6513, -0.6257,  0.4533,  0.2597,  1.2564]],
       dtype=torch.float64)
	q_value: tensor([[-27.3142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4350339370392202, distance: 1.3708431907755678 entropy 0.6056879162788391
epoch: 42, step: 60
	action: tensor([[-0.0225, -0.0181, -0.0223, -0.0770, -0.6139,  0.7547, -1.0311]],
       dtype=torch.float64)
	q_value: tensor([[-29.1547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2612922668980274, distance: 0.9835422394631479 entropy 0.6056879162788391
epoch: 42, step: 61
	action: tensor([[ 0.1497, -0.7763,  0.9296, -1.2524, -0.2081,  0.0386,  0.7287]],
       dtype=torch.float64)
	q_value: tensor([[-29.9864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8705739601896749, distance: 1.5651073661323796 entropy 0.6056879162788391
epoch: 42, step: 62
	action: tensor([[-0.1754, -0.7596,  0.6188, -0.0461,  0.0443,  0.2093,  0.5903]],
       dtype=torch.float64)
	q_value: tensor([[-30.6430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.529878859883451, distance: 1.4154196320461951 entropy 0.6056879162788391
epoch: 42, step: 63
	action: tensor([[ 0.1265, -0.4375,  1.1603, -0.2609, -0.5582, -0.8359, -0.2814]],
       dtype=torch.float64)
	q_value: tensor([[-27.2619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3637856896915119, distance: 1.336379354863208 entropy 0.6056879162788391
epoch: 42, step: 64
	action: tensor([[ 0.2812, -0.0835,  1.0051, -0.2470, -0.1592,  0.2770,  0.5042]],
       dtype=torch.float64)
	q_value: tensor([[-31.3827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38493716146129786, distance: 0.8974627614184552 entropy 0.6056879162788391
epoch: 42, step: 65
	action: tensor([[ 0.5413, -0.4167,  0.9369, -0.1464,  0.8108,  0.3493,  0.5297]],
       dtype=torch.float64)
	q_value: tensor([[-27.2467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2563643846777016, distance: 0.9868173669527991 entropy 0.6056879162788391
epoch: 42, step: 66
	action: tensor([[-0.3403, -0.5059,  0.4721, -0.4904,  0.0826,  0.7922,  0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-28.4106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.50351737409566, distance: 1.4031720288283693 entropy 0.6056879162788391
epoch: 42, step: 67
	action: tensor([[ 0.7531, -0.8983,  0.6395, -0.6535, -0.1292,  0.4336,  0.7715]],
       dtype=torch.float64)
	q_value: tensor([[-27.8900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43746892067305865, distance: 1.3720057298159734 entropy 0.6056879162788391
epoch: 42, step: 68
	action: tensor([[0.4148, 0.3436, 0.6285, 0.2504, 0.8135, 0.1635, 0.5830]],
       dtype=torch.float64)
	q_value: tensor([[-29.6608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 69
	action: tensor([[-0.3363, -1.4727,  1.5790, -0.3683,  0.3279, -0.2727,  1.5448]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1996380848750308, distance: 1.6971972075155923 entropy 0.6056879162788391
epoch: 42, step: 70
	action: tensor([[-0.1241, -0.7262, -0.3770, -0.0053, -1.4392,  0.3097, -0.3810]],
       dtype=torch.float64)
	q_value: tensor([[-33.3295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5543822214517566, distance: 1.4267096653633253 entropy 0.6056879162788391
epoch: 42, step: 71
	action: tensor([[ 0.8522, -0.4132,  1.0565, -0.2204,  0.2399,  0.2782,  0.6210]],
       dtype=torch.float64)
	q_value: tensor([[-32.6443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.341661202862902, distance: 0.9284989524274835 entropy 0.6056879162788391
epoch: 42, step: 72
	action: tensor([[ 0.0697, -0.7299, -0.6841,  0.5010,  0.1211,  0.1518,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-28.0029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12159261706277125, distance: 1.21192086959719 entropy 0.6056879162788391
epoch: 42, step: 73
	action: tensor([[ 0.1029,  0.1337, -0.3509, -0.2723,  0.5180, -0.3732, -0.2381]],
       dtype=torch.float64)
	q_value: tensor([[-27.5272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3383597117285897, distance: 0.930824197032528 entropy 0.6056879162788391
epoch: 42, step: 74
	action: tensor([[ 0.8185, -1.4611,  0.9762, -0.4890,  0.1601, -0.3245,  0.6126]],
       dtype=torch.float64)
	q_value: tensor([[-24.9691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7562322491630178, distance: 1.5165183410825687 entropy 0.6056879162788391
epoch: 42, step: 75
	action: tensor([[ 0.0251, -0.0772,  0.1851,  0.2471,  1.1783, -0.4603,  0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-30.1801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28185143871937235, distance: 0.9697590487241832 entropy 0.6056879162788391
epoch: 42, step: 76
	action: tensor([[ 0.2850,  0.5645,  0.9547, -0.3986, -0.5461, -0.0676,  0.7673]],
       dtype=torch.float64)
	q_value: tensor([[-28.4958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 77
	action: tensor([[-0.1704, -1.0127,  1.5811, -0.5715,  0.1166,  0.1382,  0.5493]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9608618128489275, distance: 1.6024341349557913 entropy 0.6056879162788391
epoch: 42, step: 78
	action: tensor([[ 0.9047, -0.6494,  0.8257, -0.4729, -0.6112,  0.0357,  0.8799]],
       dtype=torch.float64)
	q_value: tensor([[-31.0616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11745261694889075, distance: 1.2096820927836354 entropy 0.6056879162788391
epoch: 42, step: 79
	action: tensor([[ 0.0982,  0.3763,  0.6663, -0.0478,  0.1449,  0.5357,  0.5113]],
       dtype=torch.float64)
	q_value: tensor([[-29.8118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 80
	action: tensor([[ 0.0872, -1.6111,  1.3398, -1.4134,  0.7761, -0.8166,  1.3803]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48956282644873794, distance: 1.3966452415054713 entropy 0.6056879162788391
epoch: 42, step: 81
	action: tensor([[ 0.5794,  0.1323,  0.2990, -0.7815,  0.2070,  0.3593, -0.1163]],
       dtype=torch.float64)
	q_value: tensor([[-34.6396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4857581645062329, distance: 0.820616785084614 entropy 0.6056879162788391
epoch: 42, step: 82
	action: tensor([[-0.2883, -0.4126,  0.5127, -0.5918,  0.8362, -0.1517,  0.7091]],
       dtype=torch.float64)
	q_value: tensor([[-26.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7873460432257995, distance: 1.5298928497746698 entropy 0.6056879162788391
epoch: 42, step: 83
	action: tensor([[ 0.1647, -0.7977,  1.0423, -0.1518,  0.4361, -0.2952,  0.5109]],
       dtype=torch.float64)
	q_value: tensor([[-27.7927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4850452444095077, distance: 1.394525743594957 entropy 0.6056879162788391
epoch: 42, step: 84
	action: tensor([[ 0.2393, -0.4828,  0.4069, -0.2437,  0.3962,  0.4953, -0.3568]],
       dtype=torch.float64)
	q_value: tensor([[-28.3909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07437554388909384, distance: 1.1009664976327376 entropy 0.6056879162788391
epoch: 42, step: 85
	action: tensor([[ 0.0315, -0.0785,  0.1023, -0.2210,  0.8369,  0.9061,  0.0715]],
       dtype=torch.float64)
	q_value: tensor([[-27.5137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4954564271984909, distance: 0.8128418065178887 entropy 0.6056879162788391
epoch: 42, step: 86
	action: tensor([[ 0.2778, -0.3397, -0.5080, -0.1222, -0.1211,  0.2217, -0.1094]],
       dtype=torch.float64)
	q_value: tensor([[-28.5485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24246326634480408, distance: 0.9959981730049923 entropy 0.6056879162788391
epoch: 42, step: 87
	action: tensor([[ 0.2105, -0.2242,  0.7350,  0.0795,  0.6281, -0.1804,  0.3200]],
       dtype=torch.float64)
	q_value: tensor([[-25.2031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2793637740832463, distance: 0.9714372180225364 entropy 0.6056879162788391
epoch: 42, step: 88
	action: tensor([[-0.3450,  0.1489, -0.3898, -0.7499,  0.9895, -0.0591,  0.5575]],
       dtype=torch.float64)
	q_value: tensor([[-26.7519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09714822884882657, distance: 1.1986416006109977 entropy 0.6056879162788391
epoch: 42, step: 89
	action: tensor([[-0.4561, -0.9153,  0.3972, -0.0684,  0.2523,  0.1826,  0.8513]],
       dtype=torch.float64)
	q_value: tensor([[-28.3483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8615970817243062, distance: 1.5613473770356505 entropy 0.6056879162788391
epoch: 42, step: 90
	action: tensor([[ 0.6351, -0.1487,  0.7299,  0.2823,  0.4279, -0.9662,  0.5119]],
       dtype=torch.float64)
	q_value: tensor([[-28.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5966276662231061, distance: 0.7267913455125552 entropy 0.6056879162788391
epoch: 42, step: 91
	action: tensor([[-0.1927, -0.8260,  0.3520,  0.1892,  0.2450, -0.1686,  0.4630]],
       dtype=torch.float64)
	q_value: tensor([[-28.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5369683431697059, distance: 1.418695380155283 entropy 0.6056879162788391
epoch: 42, step: 92
	action: tensor([[ 0.6212, -1.2531, -0.0363,  0.0421, -1.0528, -0.0876,  0.2724]],
       dtype=torch.float64)
	q_value: tensor([[-27.3506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6520044108198766, distance: 1.4708293738093667 entropy 0.6056879162788391
epoch: 42, step: 93
	action: tensor([[ 0.3874, -0.3848, -0.4561,  0.1148, -0.2127, -0.3370,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[-31.7316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2456246361241602, distance: 0.9939177389262147 entropy 0.6056879162788391
epoch: 42, step: 94
	action: tensor([[ 0.9908, -1.0969, -0.1495, -0.8572, -0.5177,  0.0804, -0.1896]],
       dtype=torch.float64)
	q_value: tensor([[-25.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9296102863469689, distance: 1.5896133303468645 entropy 0.6056879162788391
epoch: 42, step: 95
	action: tensor([[ 0.3069,  0.0789,  0.8729, -0.8519, -0.4558,  0.3493,  0.2784]],
       dtype=torch.float64)
	q_value: tensor([[-31.5810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18921702619087832, distance: 1.0304074814666844 entropy 0.6056879162788391
epoch: 42, step: 96
	action: tensor([[-0.2327,  0.7558,  0.1959,  0.4446,  0.5545, -0.3380, -0.2157]],
       dtype=torch.float64)
	q_value: tensor([[-28.1713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 97
	action: tensor([[ 0.6465, -1.7564,  1.0845, -0.9699,  1.2583,  0.4710,  1.3772]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 98
	action: tensor([[ 0.2482, -2.3288,  1.2763, -0.5043,  1.1002, -0.5842,  1.4366]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 99
	action: tensor([[ 0.4481, -1.2976,  0.9498, -0.8474,  1.2628, -0.5716,  1.0170]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7392516301540846, distance: 1.5091690941011477 entropy 0.6056879162788391
epoch: 42, step: 100
	action: tensor([[ 0.2416, -0.4773,  0.0516, -0.3922,  0.4680, -0.1239,  0.0437]],
       dtype=torch.float64)
	q_value: tensor([[-32.8339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20908109554423637, distance: 1.2583006145128033 entropy 0.6056879162788391
epoch: 42, step: 101
	action: tensor([[ 0.2258,  0.5519, -0.0259, -0.4493,  0.4311, -0.1958,  0.6070]],
       dtype=torch.float64)
	q_value: tensor([[-25.9416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 102
	action: tensor([[-0.6664, -1.4304,  1.3524, -1.4854,  0.4803, -0.1262,  0.8816]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1227789729453352, distance: 1.6672820778314315 entropy 0.6056879162788391
epoch: 42, step: 103
	action: tensor([[ 0.0350, -0.2839,  0.5490,  0.4010,  0.2078, -0.5311,  0.5076]],
       dtype=torch.float64)
	q_value: tensor([[-34.2876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2097429142422479, distance: 1.0172809049248674 entropy 0.6056879162788391
epoch: 42, step: 104
	action: tensor([[-0.1582,  0.2953,  0.7092, -0.4943, -0.0604,  0.1187,  0.1985]],
       dtype=torch.float64)
	q_value: tensor([[-26.7415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01955807491243411, distance: 1.1330984103938289 entropy 0.6056879162788391
epoch: 42, step: 105
	action: tensor([[ 0.6974, -0.2977,  0.3915, -0.6028,  0.2392, -0.1925,  0.6508]],
       dtype=torch.float64)
	q_value: tensor([[-26.5970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04575916153027815, distance: 1.1178555639560768 entropy 0.6056879162788391
epoch: 42, step: 106
	action: tensor([[ 0.1752, -0.1088,  0.1203,  0.6236,  0.1927,  0.0249,  0.5242]],
       dtype=torch.float64)
	q_value: tensor([[-26.4005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 107
	action: tensor([[ 0.5885, -0.6015,  2.2158, -0.6448,  0.6915, -1.0793,  0.8533]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 108
	action: tensor([[ 0.4622, -1.4535,  1.0542, -0.3187,  1.0382, -1.0512,  1.2001]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45813106505949364, distance: 1.3818311372578878 entropy 0.6056879162788391
epoch: 42, step: 109
	action: tensor([[ 0.3960, -0.1937,  0.3352, -0.0978,  0.3860,  0.9830,  0.4132]],
       dtype=torch.float64)
	q_value: tensor([[-33.1580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6961275036394071, distance: 0.6308155495378336 entropy 0.6056879162788391
epoch: 42, step: 110
	action: tensor([[-0.0010,  0.7127, -0.0708, -0.4856,  0.6941,  0.4765, -0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-28.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 42, step: 111
	action: tensor([[-0.4293, -1.4389,  1.4799, -1.4176,  1.2785, -0.2577,  1.2884]],
       dtype=torch.float64)
	q_value: tensor([[-32.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1023311864713148, distance: 1.6592325528967542 entropy 0.6056879162788391
epoch: 42, step: 112
	action: tensor([[ 0.2739,  0.0349,  0.4623, -0.1998, -0.2254,  0.5193, -0.2797]],
       dtype=torch.float64)
	q_value: tensor([[-36.3335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.628061091409495, distance: 0.6978988769195646 entropy 0.6056879162788391
epoch: 42, step: 113
	action: tensor([[ 0.3044, -0.0557, -0.0164,  0.0985, -0.1577, -0.0744, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-26.6656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5863444916318318, distance: 0.7359970933960157 entropy 0.6056879162788391
epoch: 42, step: 114
	action: tensor([[ 0.6402, -0.1485, -0.0048, -0.1010, -0.4023, -0.1708,  0.0714]],
       dtype=torch.float64)
	q_value: tensor([[-24.6510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5116663534536932, distance: 0.7996777592600512 entropy 0.6056879162788391
epoch: 42, step: 115
	action: tensor([[-0.5746, -0.3003,  1.4246, -0.0058,  0.8709, -0.0814, -0.0388]],
       dtype=torch.float64)
	q_value: tensor([[-25.3591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6131593666105071, distance: 1.453434050123015 entropy 0.6056879162788391
epoch: 42, step: 116
	action: tensor([[ 0.2368, -0.5985,  0.7480, -0.2852,  1.0284,  0.3226,  0.4602]],
       dtype=torch.float64)
	q_value: tensor([[-33.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19086476862501112, distance: 1.2487856995298239 entropy 0.6056879162788391
epoch: 42, step: 117
	action: tensor([[-0.0773, -0.3495, -0.2691,  0.6978,  0.3158,  0.2458,  0.7202]],
       dtype=torch.float64)
	q_value: tensor([[-28.7560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3196844530570626, distance: 0.9438693664663385 entropy 0.6056879162788391
epoch: 42, step: 118
	action: tensor([[-0.2511,  0.2233,  0.3692, -0.0034,  0.3613, -0.8296,  0.6235]],
       dtype=torch.float64)
	q_value: tensor([[-27.1371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023783742377534223, distance: 1.1578726819842076 entropy 0.6056879162788391
epoch: 42, step: 119
	action: tensor([[ 0.2017, -0.7781,  1.1891, -1.5447,  0.8227, -0.2154, -0.2008]],
       dtype=torch.float64)
	q_value: tensor([[-26.4714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5657819468767167, distance: 1.43193180028783 entropy 0.6056879162788391
epoch: 42, step: 120
	action: tensor([[ 0.9092, -0.7492, -0.0997, -0.5847,  0.0801, -0.4970,  0.7341]],
       dtype=torch.float64)
	q_value: tensor([[-33.3252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5389110854130157, distance: 1.4195917190600158 entropy 0.6056879162788391
epoch: 42, step: 121
	action: tensor([[ 0.0335, -0.4014,  0.5346, -0.1491, -0.0584,  0.2605,  0.4683]],
       dtype=torch.float64)
	q_value: tensor([[-28.9502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01119162844935917, distance: 1.1507299749119557 entropy 0.6056879162788391
epoch: 42, step: 122
	action: tensor([[-0.3531,  0.0259,  0.0470, -0.0948,  0.6461, -0.3785,  0.5658]],
       dtype=torch.float64)
	q_value: tensor([[-25.7847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23127103088779144, distance: 1.26979474101421 entropy 0.6056879162788391
epoch: 42, step: 123
	action: tensor([[ 0.4633, -0.5490, -0.5582, -0.1350, -0.1201, -0.4000,  0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-25.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01286429538443934, distance: 1.151681324097266 entropy 0.6056879162788391
epoch: 42, step: 124
	action: tensor([[ 0.9952,  0.0329, -0.3711, -0.8117,  0.6173,  1.1104,  0.2012]],
       dtype=torch.float64)
	q_value: tensor([[-26.5741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6396000521941212, distance: 0.686987844839948 entropy 0.6056879162788391
epoch: 42, step: 125
	action: tensor([[-0.1918, -0.2443,  0.2693, -0.1675,  0.0978,  1.0819, -0.1381]],
       dtype=torch.float64)
	q_value: tensor([[-31.8706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28300443640161055, distance: 0.9689802550665673 entropy 0.6056879162788391
epoch: 42, step: 126
	action: tensor([[ 1.0582, -0.0432, -0.3323, -0.1124,  0.1041,  0.7177, -0.2040]],
       dtype=torch.float64)
	q_value: tensor([[-28.8948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.799586250722592, distance: 0.5122953921780259 entropy 0.6056879162788391
epoch: 42, step: 127
	action: tensor([[-0.0353, -0.3598, -0.1292, -0.2016,  0.0805,  0.4140,  0.1340]],
       dtype=torch.float64)
	q_value: tensor([[-28.2646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03198966707848905, distance: 1.1258918874278985 entropy 0.6056879162788391
LOSS epoch 42 actor 360.8761397623176 critic 177.65072871504395 
epoch: 43, step: 0
	action: tensor([[ 0.7413, -0.9062,  0.3050,  0.1405, -0.3044, -0.0226,  0.9857]],
       dtype=torch.float64)
	q_value: tensor([[-27.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11143047178269017, distance: 1.206418096411792 entropy 0.6056879162788391
epoch: 43, step: 1
	action: tensor([[ 0.9559,  0.2358, -0.2746, -0.6042, -0.5048,  0.0435,  0.7388]],
       dtype=torch.float64)
	q_value: tensor([[-31.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6325890938315692, distance: 0.6936377408963101 entropy 0.6056879162788391
epoch: 43, step: 2
	action: tensor([[ 0.5562,  0.0225, -0.0437,  0.1198,  0.0278, -0.0411,  0.2186]],
       dtype=torch.float64)
	q_value: tensor([[-31.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7677949348425719, distance: 0.5514327272878944 entropy 0.6056879162788391
epoch: 43, step: 3
	action: tensor([[ 0.2565, -0.6061,  1.0264,  0.6766, -0.0842, -0.4041,  0.8806]],
       dtype=torch.float64)
	q_value: tensor([[-26.6512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34368416080347575, distance: 0.9270712988474398 entropy 0.6056879162788391
epoch: 43, step: 4
	action: tensor([[ 0.8045, -0.4348,  0.6514, -0.2483,  0.2895,  0.4889,  0.2784]],
       dtype=torch.float64)
	q_value: tensor([[-31.4088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33939108772831905, distance: 0.9300984218845241 entropy 0.6056879162788391
epoch: 43, step: 5
	action: tensor([[ 0.4342,  0.2024, -0.3421, -0.3052, -0.2074,  0.1033, -0.1330]],
       dtype=torch.float64)
	q_value: tensor([[-29.6942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.723946861991445, distance: 0.6012471592151758 entropy 0.6056879162788391
epoch: 43, step: 6
	action: tensor([[ 0.7830,  0.2276,  0.7839, -0.1533, -0.1995, -0.1708,  0.1807]],
       dtype=torch.float64)
	q_value: tensor([[-26.8053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8140023162824043, distance: 0.49352648186537423 entropy 0.6056879162788391
epoch: 43, step: 7
	action: tensor([[ 0.3175, -0.5219,  0.3554, -0.0459,  0.5615,  0.0933, -0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-28.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10281766968165806, distance: 1.0839195491676077 entropy 0.6056879162788391
epoch: 43, step: 8
	action: tensor([[ 0.1316, -0.0075,  0.3780, -0.7802,  0.5519, -0.0291,  0.1650]],
       dtype=torch.float64)
	q_value: tensor([[-28.6145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04532530949643898, distance: 1.1699907443678252 entropy 0.6056879162788391
epoch: 43, step: 9
	action: tensor([[-0.1660,  0.3296,  0.0570,  0.4231, -0.2892, -0.8572,  0.5346]],
       dtype=torch.float64)
	q_value: tensor([[-28.6178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 10
	action: tensor([[ 0.2800, -1.7182,  0.6287, -0.5036,  0.2856,  0.0936,  0.3754]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0253330690517939, distance: 1.6285643369109284 entropy 0.6056879162788391
epoch: 43, step: 11
	action: tensor([[ 1.1553, -0.3297, -0.3460,  0.0877, -0.7385, -0.8264, -0.3673]],
       dtype=torch.float64)
	q_value: tensor([[-31.6367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2706481737239368, distance: 0.9772939958959093 entropy 0.6056879162788391
epoch: 43, step: 12
	action: tensor([[ 0.8995, -0.5305,  0.7749, -0.6274, -0.1682, -0.1546,  0.6144]],
       dtype=torch.float64)
	q_value: tensor([[-32.5691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12940261150739207, distance: 1.2161330385887041 entropy 0.6056879162788391
epoch: 43, step: 13
	action: tensor([[ 0.1437, -0.2177, -0.1856, -0.2960,  0.0722,  0.0458,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[-29.8690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13899739548079326, distance: 1.0618396197550268 entropy 0.6056879162788391
epoch: 43, step: 14
	action: tensor([[ 0.0891, -0.3924,  0.6530,  0.7738, -0.3000,  0.4474, -0.2182]],
       dtype=torch.float64)
	q_value: tensor([[-26.4625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7431166937486553, distance: 0.579995513844267 entropy 0.6056879162788391
epoch: 43, step: 15
	action: tensor([[-0.0211, -0.0478,  0.2928,  0.2998,  0.2276,  0.7715,  0.6863]],
       dtype=torch.float64)
	q_value: tensor([[-32.5476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 16
	action: tensor([[-0.4939, -1.2526,  1.8571, -1.2368,  0.5247, -0.3729,  0.2996]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0249396895025158, distance: 1.6284061715637985 entropy 0.6056879162788391
epoch: 43, step: 17
	action: tensor([[ 0.3819, -0.2975, -0.1556, -0.3737,  0.3381, -0.3083,  0.4416]],
       dtype=torch.float64)
	q_value: tensor([[-37.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045071188373070004, distance: 1.1182584580816675 entropy 0.6056879162788391
epoch: 43, step: 18
	action: tensor([[-0.0522, -0.3046, -0.6999, -0.1810, -0.3214, -0.5689,  0.1028]],
       dtype=torch.float64)
	q_value: tensor([[-27.5668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05238962171902395, distance: 1.1139651325931759 entropy 0.6056879162788391
epoch: 43, step: 19
	action: tensor([[-0.3023, -0.7056, -0.0604, -0.0171, -0.1946, -0.5244,  0.7468]],
       dtype=torch.float64)
	q_value: tensor([[-28.3884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6091495323044578, distance: 1.4516265238389627 entropy 0.6056879162788391
epoch: 43, step: 20
	action: tensor([[ 0.3145,  0.8388,  0.6300, -0.3907, -0.5396, -0.7812,  0.4297]],
       dtype=torch.float64)
	q_value: tensor([[-29.7177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 21
	action: tensor([[ 0.0458, -0.0499,  1.2056, -0.3554,  1.5489, -0.1237,  0.7231]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060946546575502025, distance: 1.1089241723947771 entropy 0.6056879162788391
epoch: 43, step: 22
	action: tensor([[ 0.6572, -0.0579,  0.0403, -0.1135, -0.3154, -0.1604,  0.5056]],
       dtype=torch.float64)
	q_value: tensor([[-35.3738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5800343918027162, distance: 0.7415894735061817 entropy 0.6056879162788391
epoch: 43, step: 23
	action: tensor([[ 0.2503, -0.2177,  0.3637, -0.4057,  0.1900, -0.2410, -0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-27.4502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017711063651347336, distance: 1.1544335539716821 entropy 0.6056879162788391
epoch: 43, step: 24
	action: tensor([[ 0.9115, -0.5702,  0.5612, -0.0230, -0.3043,  0.7849,  0.4162]],
       dtype=torch.float64)
	q_value: tensor([[-27.2074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48077245660386037, distance: 0.82458523572723 entropy 0.6056879162788391
epoch: 43, step: 25
	action: tensor([[-0.4985, -0.4560, -0.1735, -0.4747,  0.5318, -0.3088,  1.1902]],
       dtype=torch.float64)
	q_value: tensor([[-30.9986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.784743396688427, distance: 1.5287785661375994 entropy 0.6056879162788391
epoch: 43, step: 26
	action: tensor([[-1.1685, -0.5259,  0.0910, -0.2753,  0.2579, -0.5219,  0.5563]],
       dtype=torch.float64)
	q_value: tensor([[-30.6002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.490040121929491, distance: 1.8057593228503201 entropy 0.6056879162788391
epoch: 43, step: 27
	action: tensor([[ 0.6049, -0.8801,  0.5509,  0.0321,  0.1469,  0.1105,  0.5302]],
       dtype=torch.float64)
	q_value: tensor([[-30.2671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13724913517340442, distance: 1.2203502680839435 entropy 0.6056879162788391
epoch: 43, step: 28
	action: tensor([[-0.4924,  0.5346,  0.2232,  0.4808, -1.4133, -0.0731,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[-29.8994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 29
	action: tensor([[-0.9183, -1.1081,  0.6888, -0.4398,  0.4388,  0.6369,  1.3362]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1822383655406465, distance: 1.6904712405450995 entropy 0.6056879162788391
epoch: 43, step: 30
	action: tensor([[ 0.0435, -0.2064,  0.3163, -0.2739, -0.2447,  0.2456,  0.8297]],
       dtype=torch.float64)
	q_value: tensor([[-35.5574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.053125007992137374, distance: 1.1135328063543235 entropy 0.6056879162788391
epoch: 43, step: 31
	action: tensor([[-0.2847,  0.1364,  0.6509,  0.5482,  0.4880, -0.0431, -0.2516]],
       dtype=torch.float64)
	q_value: tensor([[-28.4509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 32
	action: tensor([[-0.1430, -1.4192,  0.5956, -0.4368,  0.4933, -1.0246,  0.7391]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0238035633239333, distance: 1.6279492852508044 entropy 0.6056879162788391
epoch: 43, step: 33
	action: tensor([[-0.0676, -0.7624,  0.0694, -0.0030, -0.2044, -0.0460, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[-33.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4423696176685419, distance: 1.374342498019021 entropy 0.6056879162788391
epoch: 43, step: 34
	action: tensor([[-0.6895,  0.0235,  0.7518, -0.2868,  0.1892, -0.2333,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-28.5963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8348770693981238, distance: 1.550101654878107 entropy 0.6056879162788391
epoch: 43, step: 35
	action: tensor([[ 0.1149, -0.6027,  0.3759,  0.3363,  0.2278,  0.1358, -0.3501]],
       dtype=torch.float64)
	q_value: tensor([[-29.4470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14447883952821194, distance: 1.0584542015592198 entropy 0.6056879162788391
epoch: 43, step: 36
	action: tensor([[ 0.9778, -0.9101,  0.1222, -0.4198,  0.5630,  0.0900, -0.1598]],
       dtype=torch.float64)
	q_value: tensor([[-30.3065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5150587796251729, distance: 1.4085472968223751 entropy 0.6056879162788391
epoch: 43, step: 37
	action: tensor([[-0.5041, -0.6272, -0.2801,  0.4030,  0.4740, -0.4301, -1.2090]],
       dtype=torch.float64)
	q_value: tensor([[-31.9284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7825699693335353, distance: 1.527847423499651 entropy 0.6056879162788391
epoch: 43, step: 38
	action: tensor([[ 0.6264, -0.1078,  0.6534, -1.1091,  0.0825, -0.5415, -0.3806]],
       dtype=torch.float64)
	q_value: tensor([[-35.7649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08581944908721528, distance: 1.1924371599013224 entropy 0.6056879162788391
epoch: 43, step: 39
	action: tensor([[0.3753, 0.0777, 0.6020, 0.2336, 1.0417, 0.1952, 0.1148]],
       dtype=torch.float64)
	q_value: tensor([[-31.2297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7719366718048478, distance: 0.5464927718790894 entropy 0.6056879162788391
epoch: 43, step: 40
	action: tensor([[ 0.3368,  0.0041,  0.5219, -0.5192, -0.1657,  0.4772, -0.7763]],
       dtype=torch.float64)
	q_value: tensor([[-31.4933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3994255058311206, distance: 0.8868295082869757 entropy 0.6056879162788391
epoch: 43, step: 41
	action: tensor([[-0.3123,  0.3275, -0.8806,  0.1327,  0.0246, -0.8245,  0.0837]],
       dtype=torch.float64)
	q_value: tensor([[-30.2970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 42
	action: tensor([[ 0.5965, -2.1226,  0.1213, -0.0935,  0.4446,  0.2465,  0.9168]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 43
	action: tensor([[-0.4896, -0.4546,  0.7589, -0.2762,  0.4990, -0.6756,  0.6851]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9538577825529881, distance: 1.5995696961062797 entropy 0.6056879162788391
epoch: 43, step: 44
	action: tensor([[ 0.1229, -0.5809,  0.2420, -1.0452,  0.6611, -0.5252,  0.0586]],
       dtype=torch.float64)
	q_value: tensor([[-30.4320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7043038400256059, distance: 1.493929848637396 entropy 0.6056879162788391
epoch: 43, step: 45
	action: tensor([[-0.6102, -0.4637,  0.3676,  0.0903, -0.1643,  0.2413, -0.1120]],
       dtype=torch.float64)
	q_value: tensor([[-31.9070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6321057185755625, distance: 1.4619443342143783 entropy 0.6056879162788391
epoch: 43, step: 46
	action: tensor([[ 0.6605,  0.5091, -0.0619, -0.7745, -0.0769, -0.2762,  0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-29.3392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7076196767520967, distance: 0.618772158116045 entropy 0.6056879162788391
epoch: 43, step: 47
	action: tensor([[ 0.3473, -0.7453, -0.0849, -0.5232,  0.6694, -0.4377, -0.3118]],
       dtype=torch.float64)
	q_value: tensor([[-28.2113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5750613477897235, distance: 1.4361686101422484 entropy 0.6056879162788391
epoch: 43, step: 48
	action: tensor([[ 0.9903, -0.7168, -0.0606,  0.1106,  0.7905,  0.9597,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-32.0548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38806916470065944, distance: 0.8951748295377671 entropy 0.6056879162788391
epoch: 43, step: 49
	action: tensor([[-0.0149,  0.3267,  0.2128,  0.0298, -0.5229, -0.6796, -0.8861]],
       dtype=torch.float64)
	q_value: tensor([[-34.1835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4917777032498758, distance: 0.8157997173837226 entropy 0.6056879162788391
epoch: 43, step: 50
	action: tensor([[ 4.9353e-01, -2.2154e-04,  6.9570e-01,  1.3693e-01,  1.2245e+00,
          2.7946e-02,  5.3953e-01]], dtype=torch.float64)
	q_value: tensor([[-31.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.719541707888679, distance: 0.6060254124042399 entropy 0.6056879162788391
epoch: 43, step: 51
	action: tensor([[ 0.7858, -0.3689, -0.2118,  0.0570,  0.5445,  0.5721, -0.1699]],
       dtype=torch.float64)
	q_value: tensor([[-31.7944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6266384890467209, distance: 0.6992322747251203 entropy 0.6056879162788391
epoch: 43, step: 52
	action: tensor([[ 0.1445,  0.0871,  0.5446, -0.5812,  0.2069, -0.2315, -1.2337]],
       dtype=torch.float64)
	q_value: tensor([[-30.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07060555743628127, distance: 1.1032062888637382 entropy 0.6056879162788391
epoch: 43, step: 53
	action: tensor([[ 0.0284,  0.2273,  0.0687, -0.3943,  0.5671, -0.1186,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-33.3737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26875454725627945, distance: 0.9785618546051347 entropy 0.6056879162788391
epoch: 43, step: 54
	action: tensor([[ 0.0122, -0.5994,  0.0321, -0.2633, -0.1484,  0.1324,  0.4548]],
       dtype=torch.float64)
	q_value: tensor([[-27.4574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3660129990857597, distance: 1.3374701846034622 entropy 0.6056879162788391
epoch: 43, step: 55
	action: tensor([[-0.4326,  0.0112,  1.6020, -0.0901, -0.1629,  0.1405, -0.4775]],
       dtype=torch.float64)
	q_value: tensor([[-28.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20957340598757734, distance: 1.2585567641993314 entropy 0.6056879162788391
epoch: 43, step: 56
	action: tensor([[ 0.2861, -0.5549, -0.1752, -0.0905, -0.7028,  0.2230,  0.0987]],
       dtype=torch.float64)
	q_value: tensor([[-35.4541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03809584952264289, distance: 1.1659379023834875 entropy 0.6056879162788391
epoch: 43, step: 57
	action: tensor([[-0.8514, -0.0588,  0.3602, -0.2383,  0.7647, -0.3579,  0.1370]],
       dtype=torch.float64)
	q_value: tensor([[-29.0332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0701271743629985, distance: 1.6464752488857897 entropy 0.6056879162788391
epoch: 43, step: 58
	action: tensor([[ 0.5533, -1.1766,  0.6516, -0.3336,  0.2834,  0.3111,  0.7445]],
       dtype=torch.float64)
	q_value: tensor([[-30.6185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7026919118468575, distance: 1.4932232031352408 entropy 0.6056879162788391
epoch: 43, step: 59
	action: tensor([[ 0.3018,  0.4981, -0.6493,  0.3566,  0.3333, -0.6059, -0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-32.0452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 60
	action: tensor([[-0.5829, -0.3371,  1.0507, -0.6970, -0.1795, -0.1306,  1.1258]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1377399970519906, distance: 1.6731471375148035 entropy 0.6056879162788391
epoch: 43, step: 61
	action: tensor([[ 0.7089,  0.3971, -0.1270, -0.3420,  0.3444,  0.5152, -0.1981]],
       dtype=torch.float64)
	q_value: tensor([[-32.1863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9356580560286718, distance: 0.29027108906236493 entropy 0.6056879162788391
epoch: 43, step: 62
	action: tensor([[-0.1557, -0.7285, -0.2435,  0.2784, -0.2423, -0.0050, -0.0407]],
       dtype=torch.float64)
	q_value: tensor([[-29.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3348646466976226, distance: 1.3221334981017703 entropy 0.6056879162788391
epoch: 43, step: 63
	action: tensor([[-0.0310, -0.0972,  0.6140, -0.4302, -0.6175,  0.7448,  0.4049]],
       dtype=torch.float64)
	q_value: tensor([[-29.1944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10888788935449079, distance: 1.0802464959938172 entropy 0.6056879162788391
epoch: 43, step: 64
	action: tensor([[ 0.0376, -0.2627,  0.9591,  0.0934,  0.6827,  0.5428,  0.3587]],
       dtype=torch.float64)
	q_value: tensor([[-31.2616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37107001504904924, distance: 0.9075234244937193 entropy 0.6056879162788391
epoch: 43, step: 65
	action: tensor([[-0.7522, -0.5505, -0.0298,  0.0689, -0.1699, -0.1348, -0.4366]],
       dtype=torch.float64)
	q_value: tensor([[-31.0810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9547998385100733, distance: 1.5999552673033004 entropy 0.6056879162788391
epoch: 43, step: 66
	action: tensor([[ 0.2776, -1.1852,  0.9037,  0.1342,  0.1426,  0.1222, -0.1016]],
       dtype=torch.float64)
	q_value: tensor([[-30.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.520911408855488, distance: 1.4112652636869678 entropy 0.6056879162788391
epoch: 43, step: 67
	action: tensor([[ 0.5960, -0.5216,  0.8104, -0.3095,  0.8133,  0.2147,  0.3733]],
       dtype=torch.float64)
	q_value: tensor([[-32.9773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.044777030330600764, distance: 1.118430680024662 entropy 0.6056879162788391
epoch: 43, step: 68
	action: tensor([[-0.6467, -0.8276, -0.8572, -0.3532, -0.0522,  0.3214, -0.3947]],
       dtype=torch.float64)
	q_value: tensor([[-30.1527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9020637653866221, distance: 1.578226129549333 entropy 0.6056879162788391
epoch: 43, step: 69
	action: tensor([[-0.4486, -0.2995, -0.2271, -1.0193, -0.6065,  0.0332,  0.1640]],
       dtype=torch.float64)
	q_value: tensor([[-32.5730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5250182715113556, distance: 1.4131693737002156 entropy 0.6056879162788391
epoch: 43, step: 70
	action: tensor([[-0.0558, -1.1057,  0.1074, -0.4165,  1.1581,  0.4108,  0.8217]],
       dtype=torch.float64)
	q_value: tensor([[-30.7411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7815644997118492, distance: 1.5274164668233872 entropy 0.6056879162788391
epoch: 43, step: 71
	action: tensor([[ 0.1367, -0.0018,  0.3165,  0.8116, -0.3890, -0.2080,  0.2924]],
       dtype=torch.float64)
	q_value: tensor([[-34.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 72
	action: tensor([[ 0.1993, -1.8290,  1.9200, -0.4270,  0.6066, -0.3240,  0.6173]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 73
	action: tensor([[-0.0518, -1.9627,  1.0243, -1.0123,  0.2473,  0.2829,  0.6972]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 74
	action: tensor([[-0.7885, -1.5844,  1.3457, -0.4803,  0.4030, -0.3428,  0.1395]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4979105654131861, distance: 1.8086108660782378 entropy 0.6056879162788391
epoch: 43, step: 75
	action: tensor([[ 0.2101, -0.1049,  0.4151, -0.2380, -0.6574,  0.1425,  0.3583]],
       dtype=torch.float64)
	q_value: tensor([[-35.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2787300668964925, distance: 0.9718642514182176 entropy 0.6056879162788391
epoch: 43, step: 76
	action: tensor([[ 0.5153,  0.0997,  0.2393, -0.1197, -0.4441,  0.1003,  0.6031]],
       dtype=torch.float64)
	q_value: tensor([[-28.5163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.705497975357984, distance: 0.6210132060363519 entropy 0.6056879162788391
epoch: 43, step: 77
	action: tensor([[ 0.4123, -1.3657, -0.3877, -0.4121, -0.1455, -0.3453, -0.3607]],
       dtype=torch.float64)
	q_value: tensor([[-27.8326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8665894944865797, distance: 1.5634395782908452 entropy 0.6056879162788391
epoch: 43, step: 78
	action: tensor([[-0.1473, -0.2804,  0.4554, -0.3291,  0.1135, -0.0221,  0.1562]],
       dtype=torch.float64)
	q_value: tensor([[-33.3589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32660278359459416, distance: 1.3180356143409722 entropy 0.6056879162788391
epoch: 43, step: 79
	action: tensor([[-0.1739, -0.5370,  0.1433, -0.0970,  1.2186, -0.5627, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[-27.0848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5300037713344337, distance: 1.415477413912423 entropy 0.6056879162788391
epoch: 43, step: 80
	action: tensor([[-0.6910, -0.0876,  0.3321, -0.9679, -0.2145,  0.4111, -0.0392]],
       dtype=torch.float64)
	q_value: tensor([[-32.7856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9631095687688849, distance: 1.6033523152076075 entropy 0.6056879162788391
epoch: 43, step: 81
	action: tensor([[-0.0181, -0.4865,  0.5664, -0.4577, -0.0288, -0.0388, -0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-30.9479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5019059745797851, distance: 1.402419900213427 entropy 0.6056879162788391
epoch: 43, step: 82
	action: tensor([[ 0.5990, -0.1391, -0.1917, -0.1525, -0.3873,  0.4050,  0.5867]],
       dtype=torch.float64)
	q_value: tensor([[-28.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5927996776309522, distance: 0.7302318138584694 entropy 0.6056879162788391
epoch: 43, step: 83
	action: tensor([[-0.2417, -0.3271,  0.3040, -0.4359,  0.0424, -0.3462, -0.4741]],
       dtype=torch.float64)
	q_value: tensor([[-28.6203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5525202206782966, distance: 1.4258548786483956 entropy 0.6056879162788391
epoch: 43, step: 84
	action: tensor([[-0.2817, -0.6936,  0.8803,  0.4678,  0.8606, -0.3241,  0.4171]],
       dtype=torch.float64)
	q_value: tensor([[-29.2089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2603555938591937, distance: 1.2847044838821182 entropy 0.6056879162788391
epoch: 43, step: 85
	action: tensor([[ 0.3631, -0.1639,  0.6378, -0.3324, -0.3668,  0.0060,  0.4170]],
       dtype=torch.float64)
	q_value: tensor([[-32.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24648979552612738, distance: 0.9933476354977877 entropy 0.6056879162788391
epoch: 43, step: 86
	action: tensor([[-0.1734,  0.3283,  0.7495,  0.1579, -0.4862,  0.6722,  0.7692]],
       dtype=torch.float64)
	q_value: tensor([[-27.8895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 87
	action: tensor([[ 0.9024, -0.6908,  2.0504, -0.4489,  1.3561, -0.2878, -0.4575]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3180974610009595, distance: 0.9449696210865587 entropy 0.6056879162788391
epoch: 43, step: 88
	action: tensor([[-1.0118,  0.1448,  0.5848, -0.3211,  1.0925, -0.0821, -0.2456]],
       dtype=torch.float64)
	q_value: tensor([[-40.8301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.12097127535867, distance: 1.6665720218748963 entropy 0.6056879162788391
epoch: 43, step: 89
	action: tensor([[-0.2146, -0.7262, -0.0507,  0.4985,  0.2082,  0.2425,  0.5475]],
       dtype=torch.float64)
	q_value: tensor([[-34.6459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1632855643761284, distance: 1.2342406976691118 entropy 0.6056879162788391
epoch: 43, step: 90
	action: tensor([[ 0.1585,  0.3293,  0.0865, -0.5135,  0.8707, -0.3285, -0.5879]],
       dtype=torch.float64)
	q_value: tensor([[-29.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32497139361505434, distance: 0.9401946642124219 entropy 0.6056879162788391
epoch: 43, step: 91
	action: tensor([[ 0.9258, -0.1565,  0.7745, -0.4415,  0.2688,  1.0425,  0.3764]],
       dtype=torch.float64)
	q_value: tensor([[-31.1162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6425437539960319, distance: 0.6841764768475659 entropy 0.6056879162788391
epoch: 43, step: 92
	action: tensor([[ 0.3372, -0.3137, -0.3887, -1.0743,  0.0535, -0.1796,  0.1398]],
       dtype=torch.float64)
	q_value: tensor([[-32.5286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15374673691316532, distance: 1.2291699550778954 entropy 0.6056879162788391
epoch: 43, step: 93
	action: tensor([[-0.3488, -0.9158,  0.4199,  0.6431, -0.2797,  0.0261,  0.0505]],
       dtype=torch.float64)
	q_value: tensor([[-29.8057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24941107597480028, distance: 1.2791143438206838 entropy 0.6056879162788391
epoch: 43, step: 94
	action: tensor([[ 0.4133, -0.3976,  0.1005, -0.0847,  0.4651, -0.2273, -0.4721]],
       dtype=torch.float64)
	q_value: tensor([[-32.5717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11318444959625817, distance: 1.0776391067207725 entropy 0.6056879162788391
epoch: 43, step: 95
	action: tensor([[-0.2599, -0.4491,  0.7754, -0.3046,  0.6598,  0.6660,  0.3201]],
       dtype=torch.float64)
	q_value: tensor([[-29.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3065109931487737, distance: 1.308016514982557 entropy 0.6056879162788391
epoch: 43, step: 96
	action: tensor([[ 0.8975, -1.1071,  0.3045,  0.2488,  0.3191, -0.0940, -0.6517]],
       dtype=torch.float64)
	q_value: tensor([[-31.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3249045097393297, distance: 1.3171916909421049 entropy 0.6056879162788391
epoch: 43, step: 97
	action: tensor([[ 0.1956,  0.0007,  0.3706, -0.1929, -0.0925, -0.1671, -0.3376]],
       dtype=torch.float64)
	q_value: tensor([[-34.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31061376351753367, distance: 0.9501408656251733 entropy 0.6056879162788391
epoch: 43, step: 98
	action: tensor([[ 0.7180, -0.2237, -0.0443,  0.5331, -0.0714, -0.6460,  0.1037]],
       dtype=torch.float64)
	q_value: tensor([[-27.4955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6764440788983115, distance: 0.6509256256631165 entropy 0.6056879162788391
epoch: 43, step: 99
	action: tensor([[ 0.3451,  0.3039,  0.0563,  0.0511, -1.1726, -0.2416,  0.5864]],
       dtype=torch.float64)
	q_value: tensor([[-29.6039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 43, step: 100
	action: tensor([[ 0.0256, -0.5137,  1.0564, -0.7549,  0.9664,  0.0244,  1.0668]],
       dtype=torch.float64)
	q_value: tensor([[-36.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6180437573949273, distance: 1.4556327709622645 entropy 0.6056879162788391
epoch: 43, step: 101
	action: tensor([[ 0.9878,  0.0023,  0.2870, -1.1003, -0.0943,  0.2953, -0.5897]],
       dtype=torch.float64)
	q_value: tensor([[-32.7820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2581320778705908, distance: 0.985643789339009 entropy 0.6056879162788391
epoch: 43, step: 102
	action: tensor([[-0.1117,  0.3338, -0.2209, -0.2275,  0.1976,  0.2126, -0.3397]],
       dtype=torch.float64)
	q_value: tensor([[-32.2601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41405601969129957, distance: 0.8759609415247762 entropy 0.6056879162788391
epoch: 43, step: 103
	action: tensor([[-0.4213,  0.0107, -0.0955, -0.0895,  0.4262,  0.4773,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[-27.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02287340241292357, distance: 1.157357782131934 entropy 0.6056879162788391
epoch: 43, step: 104
	action: tensor([[-0.0730, -0.5284, -0.1484, -0.0256,  0.1985, -0.3288,  0.3792]],
       dtype=torch.float64)
	q_value: tensor([[-28.2558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34639714621560613, distance: 1.327832470256814 entropy 0.6056879162788391
epoch: 43, step: 105
	action: tensor([[-0.1808, -0.3498, -0.6909, -0.8359, -0.2242, -0.5323,  0.4321]],
       dtype=torch.float64)
	q_value: tensor([[-27.7811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03223972050174728, distance: 1.1626445940405088 entropy 0.6056879162788391
epoch: 43, step: 106
	action: tensor([[ 0.3651, -0.4621,  0.4012,  0.2596,  0.7870, -0.1058,  0.3251]],
       dtype=torch.float64)
	q_value: tensor([[-29.9621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31350183727839487, distance: 0.9481485449478844 entropy 0.6056879162788391
epoch: 43, step: 107
	action: tensor([[ 0.5593, -0.4529, -0.6649, -0.1803,  0.3087,  0.1225, -0.2930]],
       dtype=torch.float64)
	q_value: tensor([[-29.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2022646285712545, distance: 1.0220828891935372 entropy 0.6056879162788391
epoch: 43, step: 108
	action: tensor([[-0.0168, -0.3133,  0.2403, -0.6332,  0.0841,  0.6251, -0.2588]],
       dtype=torch.float64)
	q_value: tensor([[-29.2591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12406421140741508, distance: 1.2132554577307402 entropy 0.6056879162788391
epoch: 43, step: 109
	action: tensor([[-0.0604, -0.0762,  0.8473, -0.1467,  0.0807,  0.5157,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-28.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26929769868598985, distance: 0.9781983610594644 entropy 0.6056879162788391
epoch: 43, step: 110
	action: tensor([[ 0.8262, -0.6294,  0.5936,  0.4319,  0.1307,  0.4522,  0.4265]],
       dtype=torch.float64)
	q_value: tensor([[-29.4868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5397534094912728, distance: 0.7763400357433204 entropy 0.6056879162788391
epoch: 43, step: 111
	action: tensor([[ 0.7736,  0.0749,  0.3380, -0.4820, -0.2167, -0.1232,  0.4244]],
       dtype=torch.float64)
	q_value: tensor([[-30.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.535429765724647, distance: 0.7799780544288016 entropy 0.6056879162788391
epoch: 43, step: 112
	action: tensor([[ 0.2741, -0.6556,  0.4899, -0.0532, -0.3150,  0.4822, -0.0802]],
       dtype=torch.float64)
	q_value: tensor([[-27.7441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.057294140637384694, distance: 1.111078634715459 entropy 0.6056879162788391
epoch: 43, step: 113
	action: tensor([[-0.6840, -0.1854,  0.4120, -0.0307, -0.2678,  0.0098, -0.3030]],
       dtype=torch.float64)
	q_value: tensor([[-29.6112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6683302370918136, distance: 1.4780791942435265 entropy 0.6056879162788391
epoch: 43, step: 114
	action: tensor([[-0.1458, -0.3499,  0.9602,  0.7331,  1.0195,  0.3483,  0.7518]],
       dtype=torch.float64)
	q_value: tensor([[-29.6794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5262523908687775, distance: 0.7876444349614616 entropy 0.6056879162788391
epoch: 43, step: 115
	action: tensor([[ 0.8884, -0.1355,  0.3349, -0.6272,  0.1007, -0.5583,  0.1208]],
       dtype=torch.float64)
	q_value: tensor([[-34.1207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1809518183261778, distance: 1.0356462060178058 entropy 0.6056879162788391
epoch: 43, step: 116
	action: tensor([[-0.1760, -0.9433,  0.4603, -0.1923,  0.4081, -0.5806,  0.2566]],
       dtype=torch.float64)
	q_value: tensor([[-28.7403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9460078366943794, distance: 1.5963531946262461 entropy 0.6056879162788391
epoch: 43, step: 117
	action: tensor([[-0.0219,  0.0089,  1.3263,  0.0059,  0.1607, -0.8092, -0.4863]],
       dtype=torch.float64)
	q_value: tensor([[-30.9185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09186274759315105, distance: 1.0905169966393573 entropy 0.6056879162788391
epoch: 43, step: 118
	action: tensor([[-0.6947,  0.0628,  0.1551, -0.5659,  0.6503,  0.1795,  1.3303]],
       dtype=torch.float64)
	q_value: tensor([[-34.9931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6806855455802838, distance: 1.4835422726122338 entropy 0.6056879162788391
epoch: 43, step: 119
	action: tensor([[ 0.3459, -0.1539, -0.9627, -0.9563,  0.6974, -0.2196, -0.0421]],
       dtype=torch.float64)
	q_value: tensor([[-33.0808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067309739449591, distance: 0.9528128199056751 entropy 0.6056879162788391
epoch: 43, step: 120
	action: tensor([[-0.2140, -0.2513,  0.1314,  0.6503, -0.5417,  0.0457,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-32.7610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3042778715001624, distance: 0.9544970747689016 entropy 0.6056879162788391
epoch: 43, step: 121
	action: tensor([[ 0.3842, -0.6867,  0.5597, -0.9712, -0.0846,  0.2283,  0.5124]],
       dtype=torch.float64)
	q_value: tensor([[-29.8533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5990081915634373, distance: 1.4470450020424008 entropy 0.6056879162788391
epoch: 43, step: 122
	action: tensor([[ 0.7125, -0.6163,  0.2901, -0.8289, -0.0081, -0.0985, -0.0810]],
       dtype=torch.float64)
	q_value: tensor([[-30.6172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44363304742843046, distance: 1.3749442872951485 entropy 0.6056879162788391
epoch: 43, step: 123
	action: tensor([[ 0.5888, -0.7430, -1.3888, -0.3943, -0.6067, -0.1038,  0.0860]],
       dtype=torch.float64)
	q_value: tensor([[-29.6878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029608415738004767, distance: 1.161161787139505 entropy 0.6056879162788391
epoch: 43, step: 124
	action: tensor([[-0.4182, -1.1646,  0.2585, -0.5021,  0.2046, -0.5022,  0.3812]],
       dtype=torch.float64)
	q_value: tensor([[-33.3084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0919969318219964, distance: 1.6551494532115765 entropy 0.6056879162788391
epoch: 43, step: 125
	action: tensor([[ 0.2426,  0.2022,  0.2631, -0.8910, -0.3309,  0.1316,  0.2329]],
       dtype=torch.float64)
	q_value: tensor([[-32.1837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21896839519611655, distance: 1.0113255919389381 entropy 0.6056879162788391
epoch: 43, step: 126
	action: tensor([[ 0.4228,  0.3705, -0.1641, -0.2410, -0.2482,  0.3156, -0.1307]],
       dtype=torch.float64)
	q_value: tensor([[-28.2920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.797536480237738, distance: 0.5149085278214048 entropy 0.6056879162788391
epoch: 43, step: 127
	action: tensor([[-0.4549,  0.1124,  0.6306, -0.5760,  0.6426, -0.1606, -0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-27.6279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6142191307409588, distance: 1.4539113880763757 entropy 0.6056879162788391
LOSS epoch 43 actor 424.26397620645287 critic 158.97454745877906 
epoch: 44, step: 0
	action: tensor([[-0.2519,  0.5245,  0.2429, -0.1139, -0.0989, -0.7085,  0.1556]],
       dtype=torch.float64)
	q_value: tensor([[-30.6634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 1
	action: tensor([[-0.2200, -0.8399,  0.3955, -0.3878,  0.9974, -0.7458,  1.0944]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9483493629943307, distance: 1.5973133087610527 entropy 0.6056879162788391
epoch: 44, step: 2
	action: tensor([[-0.2736,  0.4033,  0.8272,  0.0831,  0.5356,  0.0602,  0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-32.1930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 3
	action: tensor([[ 0.2150, -0.9921,  0.3569, -0.6124,  0.8391,  0.7062, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6209015870733596, distance: 1.4569176914975503 entropy 0.6056879162788391
epoch: 44, step: 4
	action: tensor([[-0.1811,  0.5850,  0.2405, -0.1442,  0.4217, -0.4344,  1.2068]],
       dtype=torch.float64)
	q_value: tensor([[-33.7888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 5
	action: tensor([[-0.3463, -0.2557,  2.0727, -0.4814,  0.9439, -0.6641, -0.2185]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1807596290681548, distance: 1.243476096627723 entropy 0.6056879162788391
epoch: 44, step: 6
	action: tensor([[-0.1095, -0.7523, -0.4940,  0.0806,  0.7586, -0.7863, -0.4181]],
       dtype=torch.float64)
	q_value: tensor([[-38.7309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6331122792862536, distance: 1.4623950736787217 entropy 0.6056879162788391
epoch: 44, step: 7
	action: tensor([[-0.2003,  0.0712, -0.2575, -0.9073,  0.5306,  0.6275, -0.3181]],
       dtype=torch.float64)
	q_value: tensor([[-33.1517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08775980883473555, distance: 1.092977683119443 entropy 0.6056879162788391
epoch: 44, step: 8
	action: tensor([[-0.5909, -0.1048, -0.4890,  0.0162,  0.5290, -0.1563, -0.7870]],
       dtype=torch.float64)
	q_value: tensor([[-31.1287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4998487486384595, distance: 1.4014590933169584 entropy 0.6056879162788391
epoch: 44, step: 9
	action: tensor([[-0.0352, -0.4349,  0.5332,  0.1778,  0.0154,  0.2547, -0.1046]],
       dtype=torch.float64)
	q_value: tensor([[-30.7554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1117714796412852, distance: 1.0784972703157978 entropy 0.6056879162788391
epoch: 44, step: 10
	action: tensor([[ 0.3693,  0.6204, -0.3743, -0.2717,  0.3658, -0.1604, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-28.7131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 11
	action: tensor([[-0.5795, -0.2303,  0.6255, -0.7357,  0.7840,  0.7908,  0.4691]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6624671695204307, distance: 1.4754796772900673 entropy 0.6056879162788391
epoch: 44, step: 12
	action: tensor([[ 0.1480, -0.2614,  0.6769,  0.0265,  0.3455,  0.4886,  0.7885]],
       dtype=torch.float64)
	q_value: tensor([[-33.4290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4089114967696299, distance: 0.8797979575656919 entropy 0.6056879162788391
epoch: 44, step: 13
	action: tensor([[ 0.1843, -0.4456,  0.0024, -0.1662, -0.1927,  0.2868, -0.1506]],
       dtype=torch.float64)
	q_value: tensor([[-29.4446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03075727016909291, distance: 1.126608359291255 entropy 0.6056879162788391
epoch: 44, step: 14
	action: tensor([[ 0.6268,  0.1954,  0.5462, -0.4810, -0.3289,  0.1858,  0.5208]],
       dtype=torch.float64)
	q_value: tensor([[-27.3100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6741275078205319, distance: 0.6532516934842333 entropy 0.6056879162788391
epoch: 44, step: 15
	action: tensor([[ 0.6110, -0.7668,  1.1772, -0.2040,  0.4526, -0.0596,  0.8632]],
       dtype=torch.float64)
	q_value: tensor([[-28.9799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16003485277650964, distance: 1.2325149961230553 entropy 0.6056879162788391
epoch: 44, step: 16
	action: tensor([[ 0.1359,  0.4460,  0.4045, -0.3122,  0.8360,  0.1125,  0.4665]],
       dtype=torch.float64)
	q_value: tensor([[-30.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 17
	action: tensor([[ 0.2041, -0.6839,  0.8151, -0.8514,  0.4497,  0.2822,  0.1219]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6576233008889893, distance: 1.473328584184912 entropy 0.6056879162788391
epoch: 44, step: 18
	action: tensor([[ 0.2080, -0.4775,  0.5343, -0.0118,  0.0045,  0.3623,  0.2102]],
       dtype=torch.float64)
	q_value: tensor([[-30.3835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18787799445134534, distance: 1.0312580044569084 entropy 0.6056879162788391
epoch: 44, step: 19
	action: tensor([[ 0.4092,  0.2671,  0.3046, -0.4504, -0.1435,  0.1348, -0.6512]],
       dtype=torch.float64)
	q_value: tensor([[-28.2406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6307783847454461, distance: 0.6953448656512685 entropy 0.6056879162788391
epoch: 44, step: 20
	action: tensor([[ 0.3331,  0.1251,  0.4226,  0.1685,  0.3397, -0.3477,  0.5216]],
       dtype=torch.float64)
	q_value: tensor([[-28.8228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6553061321222633, distance: 0.6718517797344131 entropy 0.6056879162788391
epoch: 44, step: 21
	action: tensor([[ 0.6343, -0.1928,  0.6597, -0.2376,  0.3908, -0.1691,  0.6217]],
       dtype=torch.float64)
	q_value: tensor([[-27.4466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40454970142177404, distance: 0.8830381196935434 entropy 0.6056879162788391
epoch: 44, step: 22
	action: tensor([[ 0.6749,  0.6751, -0.0940, -0.4063, -0.4110,  0.7373,  0.3604]],
       dtype=torch.float64)
	q_value: tensor([[-28.0664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9682510953687565, distance: 0.20390180317709072 entropy 0.6056879162788391
epoch: 44, step: 23
	action: tensor([[ 0.2356, -0.1655,  0.6492, -0.7511,  0.1324,  0.4715, -0.5040]],
       dtype=torch.float64)
	q_value: tensor([[-31.9247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042767830350750935, distance: 1.1196063063220327 entropy 0.6056879162788391
epoch: 44, step: 24
	action: tensor([[ 0.1216, -0.4104, -0.2629, -0.5990,  0.2112, -0.4308, -0.1672]],
       dtype=torch.float64)
	q_value: tensor([[-30.0044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2618262524762489, distance: 1.2854538005412153 entropy 0.6056879162788391
epoch: 44, step: 25
	action: tensor([[-0.1251,  0.0271,  0.3478,  0.3356, -0.1987,  0.3786, -0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-28.8982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 26
	action: tensor([[ 0.6591, -1.3516,  0.5519,  0.0723,  0.2438, -0.3050,  0.2896]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6931716814191133, distance: 1.4890428356244927 entropy 0.6056879162788391
epoch: 44, step: 27
	action: tensor([[-0.0480,  0.2972, -0.3322, -0.0516,  0.0889,  0.5152,  0.5813]],
       dtype=torch.float64)
	q_value: tensor([[-31.2438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 28
	action: tensor([[ 0.1376, -0.6383,  1.3014, -0.7122,  0.3183, -0.6675,  0.8293]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4962875578151087, distance: 1.3997943156771946 entropy 0.6056879162788391
epoch: 44, step: 29
	action: tensor([[ 0.3239, -0.4995,  0.4537,  0.0701,  0.0755,  0.1004,  0.2033]],
       dtype=torch.float64)
	q_value: tensor([[-31.4270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19189010270337048, distance: 1.0287075000338695 entropy 0.6056879162788391
epoch: 44, step: 30
	action: tensor([[ 0.0526, -0.3573, -0.0563, -0.1066,  0.9897,  0.2511, -0.4142]],
       dtype=torch.float64)
	q_value: tensor([[-27.6217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09019973545799176, distance: 1.0915150360315147 entropy 0.6056879162788391
epoch: 44, step: 31
	action: tensor([[ 0.1251,  0.2199,  1.1692, -0.5514,  0.0256, -0.2186, -0.0794]],
       dtype=torch.float64)
	q_value: tensor([[-31.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13986085873476473, distance: 1.061307049078855 entropy 0.6056879162788391
epoch: 44, step: 32
	action: tensor([[ 0.0116,  0.2698, -0.7462,  0.7549,  0.5419, -0.6587,  0.3532]],
       dtype=torch.float64)
	q_value: tensor([[-30.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 33
	action: tensor([[-0.4455, -0.9370,  0.9359, -0.4225,  1.0727,  0.0661,  0.0697]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1673521219762018, distance: 1.6846955586371337 entropy 0.6056879162788391
epoch: 44, step: 34
	action: tensor([[-0.5213,  0.2587, -0.2971, -0.2310, -0.1261,  0.0006,  0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-33.8355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15430848844332656, distance: 1.2294691559566304 entropy 0.6056879162788391
epoch: 44, step: 35
	action: tensor([[ 1.1541, -0.5560,  0.4976, -0.5004,  0.2356, -0.4041,  0.0924]],
       dtype=torch.float64)
	q_value: tensor([[-26.8963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11832286552314053, distance: 1.210153038585051 entropy 0.6056879162788391
epoch: 44, step: 36
	action: tensor([[ 0.0483, -0.3075,  0.0074, -0.9711, -0.1957, -0.1924, -0.1605]],
       dtype=torch.float64)
	q_value: tensor([[-29.7685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4050978260279192, distance: 1.3564693157957184 entropy 0.6056879162788391
epoch: 44, step: 37
	action: tensor([[-0.6443,  0.0579,  0.2148, -0.3481,  0.3927,  0.4918,  0.3165]],
       dtype=torch.float64)
	q_value: tensor([[-28.8746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4326914334852061, distance: 1.369723873489308 entropy 0.6056879162788391
epoch: 44, step: 38
	action: tensor([[ 0.0510, -0.2082,  1.2808, -0.5506,  0.6072,  0.1852,  0.3906]],
       dtype=torch.float64)
	q_value: tensor([[-29.7380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12267388333634277, distance: 1.2125049022803778 entropy 0.6056879162788391
epoch: 44, step: 39
	action: tensor([[-0.2820, -0.0212, -0.4196,  0.2849, -0.8035,  0.4069,  0.6736]],
       dtype=torch.float64)
	q_value: tensor([[-30.7104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03377974210504697, distance: 1.163511558580324 entropy 0.6056879162788391
epoch: 44, step: 40
	action: tensor([[ 0.4914,  0.1912,  0.3130, -0.7079,  0.6338,  0.4750,  0.8058]],
       dtype=torch.float64)
	q_value: tensor([[-30.4398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5889176074548188, distance: 0.7337044126553554 entropy 0.6056879162788391
epoch: 44, step: 41
	action: tensor([[-0.3303,  0.3612, -0.0078,  0.0675,  0.1520, -0.2601,  0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-30.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 42
	action: tensor([[-0.2245, -1.2753,  1.4633,  0.0502,  0.0905, -0.2377,  0.7365]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9557975400789185, distance: 1.6003635122349558 entropy 0.6056879162788391
epoch: 44, step: 43
	action: tensor([[ 0.5452, -0.9949,  0.5575, -0.0568,  0.2510,  0.2466, -0.3606]],
       dtype=torch.float64)
	q_value: tensor([[-33.3342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3518388750599284, distance: 1.3305131124302745 entropy 0.6056879162788391
epoch: 44, step: 44
	action: tensor([[ 0.5915, -0.2674, -0.2639, -0.6858, -0.6322,  0.2160,  0.3591]],
       dtype=torch.float64)
	q_value: tensor([[-31.5018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09407028583233279, distance: 1.0891907528795224 entropy 0.6056879162788391
epoch: 44, step: 45
	action: tensor([[ 0.0030, -0.7345, -0.2023, -0.2655,  0.4673,  0.4385, -0.8052]],
       dtype=torch.float64)
	q_value: tensor([[-29.9414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3179114332791282, distance: 1.3137109083908955 entropy 0.6056879162788391
epoch: 44, step: 46
	action: tensor([[ 0.3206,  0.1522, -0.3300, -0.0409,  0.3747,  0.2988, -0.2535]],
       dtype=torch.float64)
	q_value: tensor([[-32.2163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6698511531935999, distance: 0.6575239638827215 entropy 0.6056879162788391
epoch: 44, step: 47
	action: tensor([[ 0.3592, -0.3567,  0.3725, -0.0553, -0.7823, -0.2342,  0.4758]],
       dtype=torch.float64)
	q_value: tensor([[-27.6890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14449544530213865, distance: 1.0584439291444123 entropy 0.6056879162788391
epoch: 44, step: 48
	action: tensor([[ 0.4477, -0.1063,  0.1625,  0.5373,  0.3623,  1.0312,  0.6268]],
       dtype=torch.float64)
	q_value: tensor([[-29.0308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9537990104862112, distance: 0.24597010802666852 entropy 0.6056879162788391
epoch: 44, step: 49
	action: tensor([[ 0.0294, -0.4137,  0.6456,  0.3752,  0.4080, -0.2771, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[-32.1724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13001232775502558, distance: 1.0673657001507932 entropy 0.6056879162788391
epoch: 44, step: 50
	action: tensor([[ 0.7345,  0.1809, -0.0472, -0.0247,  0.3403,  1.3618, -0.3214]],
       dtype=torch.float64)
	q_value: tensor([[-29.4994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9896532821123427, distance: 0.11640134463388331 entropy 0.6056879162788391
epoch: 44, step: 51
	action: tensor([[ 0.2161,  0.0451,  0.1695,  0.1833,  0.4157,  0.0346, -0.0323]],
       dtype=torch.float64)
	q_value: tensor([[-34.1077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.57421269821876, distance: 0.7467118543017134 entropy 0.6056879162788391
epoch: 44, step: 52
	action: tensor([[ 0.0318,  0.5259, -0.3123, -0.2294,  1.4514,  0.8229,  0.0555]],
       dtype=torch.float64)
	q_value: tensor([[-27.4522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 53
	action: tensor([[-0.0768, -0.4648,  0.4724, -1.1181,  1.2111, -0.7598, -0.0706]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.731549633050367, distance: 1.5058238297667765 entropy 0.6056879162788391
epoch: 44, step: 54
	action: tensor([[ 0.5606,  0.0209, -0.1372,  0.0925,  0.1068,  0.4565, -0.7820]],
       dtype=torch.float64)
	q_value: tensor([[-34.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.820422075769654, distance: 0.48493459419338425 entropy 0.6056879162788391
epoch: 44, step: 55
	action: tensor([[-0.6018,  0.4871,  0.1069,  0.3773,  0.2576, -0.4975,  0.0497]],
       dtype=torch.float64)
	q_value: tensor([[-30.0732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 56
	action: tensor([[-0.1782, -0.2219, -0.1328, -0.0524,  0.8050, -0.0541,  0.3690]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11759121273088358, distance: 1.2097571078826392 entropy 0.6056879162788391
epoch: 44, step: 57
	action: tensor([[ 0.3749,  0.3987, -0.5608,  0.1904,  0.7290,  0.4831,  0.9971]],
       dtype=torch.float64)
	q_value: tensor([[-28.4200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 58
	action: tensor([[-0.4456, -0.7245,  0.6303, -0.1329,  1.2762,  0.1555, -0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8025031941404945, distance: 1.5363660960933543 entropy 0.6056879162788391
epoch: 44, step: 59
	action: tensor([[-0.4334, -0.0406,  0.0095, -1.0140, -0.2294, -0.5853, -0.2821]],
       dtype=torch.float64)
	q_value: tensor([[-34.1662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4677991865148865, distance: 1.3864046764089437 entropy 0.6056879162788391
epoch: 44, step: 60
	action: tensor([[ 0.4826, -0.3066,  0.9679, -0.6950,  0.1114,  0.1190,  0.2104]],
       dtype=torch.float64)
	q_value: tensor([[-29.8423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0027540035970905485, distance: 1.1427674034925837 entropy 0.6056879162788391
epoch: 44, step: 61
	action: tensor([[-0.1073,  0.2444, -0.8225, -0.8657,  0.7543,  0.5373, -0.1759]],
       dtype=torch.float64)
	q_value: tensor([[-28.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5563266136224034, distance: 0.7622341151612175 entropy 0.6056879162788391
epoch: 44, step: 62
	action: tensor([[ 0.5989,  0.4445, -0.1532,  0.0692,  0.0367,  0.5755, -0.2666]],
       dtype=torch.float64)
	q_value: tensor([[-32.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9475470147882109, distance: 0.262084778324717 entropy 0.6056879162788391
epoch: 44, step: 63
	action: tensor([[ 0.1582,  0.0793,  0.8894, -0.1841,  0.4542,  0.1853,  0.4585]],
       dtype=torch.float64)
	q_value: tensor([[-29.4370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45558129588742546, distance: 0.8443513692612107 entropy 0.6056879162788391
epoch: 44, step: 64
	action: tensor([[ 0.2101,  0.5905,  0.0399, -0.0486,  0.6484, -0.4463,  0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-28.6758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 65
	action: tensor([[ 0.3473,  0.1853,  0.5979,  0.0510,  1.2323,  0.0609, -0.4084]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7520128163177227, distance: 0.5698641172241462 entropy 0.6056879162788391
epoch: 44, step: 66
	action: tensor([[ 0.5226, -0.9081,  0.9450, -0.2813,  0.4079,  0.2003,  0.7366]],
       dtype=torch.float64)
	q_value: tensor([[-33.4832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3879322294214056, distance: 1.3481580945439227 entropy 0.6056879162788391
epoch: 44, step: 67
	action: tensor([[-0.2198,  0.3971,  0.1727, -0.5125,  0.6495,  0.5626,  0.2153]],
       dtype=torch.float64)
	q_value: tensor([[-30.7039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989858660644579, distance: 0.9581203852649279 entropy 0.6056879162788391
epoch: 44, step: 68
	action: tensor([[ 0.1755, -0.3990,  1.0496,  0.2943,  0.2222,  0.3425,  0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-30.6972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4509776118407772, distance: 0.8479138331632479 entropy 0.6056879162788391
epoch: 44, step: 69
	action: tensor([[ 0.2811, -0.4974,  0.0220, -0.2584,  0.0318,  0.2497, -0.2189]],
       dtype=torch.float64)
	q_value: tensor([[-30.3858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022715088623703972, distance: 1.1572682144563482 entropy 0.6056879162788391
epoch: 44, step: 70
	action: tensor([[ 0.0409,  0.4240, -1.1202, -0.3307, -0.0553,  0.2985, -0.1467]],
       dtype=torch.float64)
	q_value: tensor([[-27.5158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 71
	action: tensor([[-0.7407, -0.4379, -0.0319, -0.5809, -0.2354,  0.8400,  0.2778]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9137368147196399, distance: 1.5830615438389144 entropy 0.6056879162788391
epoch: 44, step: 72
	action: tensor([[ 0.0388, -0.5385,  1.0610, -0.4566, -0.1462, -0.3297, -0.1295]],
       dtype=torch.float64)
	q_value: tensor([[-32.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5474149303604228, distance: 1.4235085654582256 entropy 0.6056879162788391
epoch: 44, step: 73
	action: tensor([[-0.2002,  0.7878, -0.2045, -0.7316,  0.4224, -0.7619, -0.0371]],
       dtype=torch.float64)
	q_value: tensor([[-30.0759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 74
	action: tensor([[-0.1266, -0.2373,  0.8932, -0.7911,  0.5646,  0.4020,  0.3637]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4661136170836018, distance: 1.3856083983324914 entropy 0.6056879162788391
epoch: 44, step: 75
	action: tensor([[-0.6408, -0.6707, -0.2332,  0.3485,  0.1237,  0.1529,  0.1176]],
       dtype=torch.float64)
	q_value: tensor([[-30.1778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7747291840851702, distance: 1.5244835370784706 entropy 0.6056879162788391
epoch: 44, step: 76
	action: tensor([[-0.3821, -0.7704,  0.0809, -0.8903, -0.5533,  0.2213, -0.1470]],
       dtype=torch.float64)
	q_value: tensor([[-29.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9402971755429503, distance: 1.5940091829922793 entropy 0.6056879162788391
epoch: 44, step: 77
	action: tensor([[-0.3789, -0.2139, -0.1068, -0.1814,  0.2461,  0.1509,  0.1627]],
       dtype=torch.float64)
	q_value: tensor([[-31.6709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34233652317308216, distance: 1.3258286415168312 entropy 0.6056879162788391
epoch: 44, step: 78
	action: tensor([[ 0.2771,  0.0234,  0.0330, -0.0864, -0.1237,  0.3685, -0.0924]],
       dtype=torch.float64)
	q_value: tensor([[-27.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5752056828089152, distance: 0.7458406374212928 entropy 0.6056879162788391
epoch: 44, step: 79
	action: tensor([[ 0.4830, -0.0083,  0.2808, -0.3883, -0.1004, -0.4698,  0.2221]],
       dtype=torch.float64)
	q_value: tensor([[-27.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3097420277803433, distance: 0.9507414071141598 entropy 0.6056879162788391
epoch: 44, step: 80
	action: tensor([[-0.1077, -0.3614,  0.3780,  0.0983,  0.0826, -0.6245, -0.2573]],
       dtype=torch.float64)
	q_value: tensor([[-26.9212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23070065009248975, distance: 1.2695005935788282 entropy 0.6056879162788391
epoch: 44, step: 81
	action: tensor([[ 0.0410, -0.9551,  0.7357,  0.1573,  0.9769,  0.0369,  0.3841]],
       dtype=torch.float64)
	q_value: tensor([[-29.1870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37860046395063685, distance: 1.3436182788161029 entropy 0.6056879162788391
epoch: 44, step: 82
	action: tensor([[ 0.5477, -0.1901,  0.7796, -0.6079,  0.1948,  0.1763, -0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-31.6427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22747640789248924, distance: 1.0058021714209682 entropy 0.6056879162788391
epoch: 44, step: 83
	action: tensor([[-0.0586,  0.5857,  0.0845, -0.9014, -0.1716,  0.4488, -0.3978]],
       dtype=torch.float64)
	q_value: tensor([[-28.5900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3512432892177042, distance: 0.9217170578844655 entropy 0.6056879162788391
epoch: 44, step: 84
	action: tensor([[-0.4274, -0.4985,  0.2337, -0.5254, -0.1066,  0.6291,  0.5090]],
       dtype=torch.float64)
	q_value: tensor([[-30.5881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6659961021361251, distance: 1.4770448534979732 entropy 0.6056879162788391
epoch: 44, step: 85
	action: tensor([[-0.1343,  0.0742,  0.0129, -0.7065, -0.5931,  0.0519,  1.0006]],
       dtype=torch.float64)
	q_value: tensor([[-29.9395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11312314807212775, distance: 1.2073364167408356 entropy 0.6056879162788391
epoch: 44, step: 86
	action: tensor([[ 0.4517, -0.9121,  0.6206,  0.3321, -0.4341,  0.9722,  0.9801]],
       dtype=torch.float64)
	q_value: tensor([[-30.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41998353098653984, distance: 0.8715189924086884 entropy 0.6056879162788391
epoch: 44, step: 87
	action: tensor([[ 0.2647, -0.9208, -0.0507,  0.5099,  0.0804, -0.3686, -0.3539]],
       dtype=torch.float64)
	q_value: tensor([[-33.4902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14962859974002085, distance: 1.2269743191864264 entropy 0.6056879162788391
epoch: 44, step: 88
	action: tensor([[-0.3292,  0.0047,  0.0973, -0.3768, -0.7944,  0.4053,  0.5737]],
       dtype=torch.float64)
	q_value: tensor([[-30.8589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2425586549949006, distance: 1.2756018565372185 entropy 0.6056879162788391
epoch: 44, step: 89
	action: tensor([[-0.3000,  0.1684,  0.1919, -0.0231,  0.4607, -0.2072,  0.5303]],
       dtype=torch.float64)
	q_value: tensor([[-30.2761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0038962835316154187, distance: 1.1465714315159807 entropy 0.6056879162788391
epoch: 44, step: 90
	action: tensor([[ 0.1578, -0.5183,  0.6209, -0.2289, -0.5007,  0.0879,  0.0714]],
       dtype=torch.float64)
	q_value: tensor([[-27.3190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19179854940696162, distance: 1.2492752024600815 entropy 0.6056879162788391
epoch: 44, step: 91
	action: tensor([[ 0.1365,  0.1310, -0.0522, -0.1228, -0.4035,  0.0132,  0.6546]],
       dtype=torch.float64)
	q_value: tensor([[-28.5552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.451946410693321, distance: 0.8471653931448253 entropy 0.6056879162788391
epoch: 44, step: 92
	action: tensor([[-0.0211, -0.1832, -0.4756,  0.2872, -0.7724, -0.2084,  0.4942]],
       dtype=torch.float64)
	q_value: tensor([[-27.3872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21577629236199514, distance: 1.0133901457254841 entropy 0.6056879162788391
epoch: 44, step: 93
	action: tensor([[-0.0442, -1.2624,  0.2854,  0.3910,  0.4787,  0.3230,  0.1404]],
       dtype=torch.float64)
	q_value: tensor([[-29.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3709363988689214, distance: 1.3398782794555144 entropy 0.6056879162788391
epoch: 44, step: 94
	action: tensor([[ 0.3708,  0.0745, -0.6682, -0.7792,  0.7171, -0.2563, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[-32.6004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4082906527709106, distance: 0.8802598798716095 entropy 0.6056879162788391
epoch: 44, step: 95
	action: tensor([[ 0.4894,  0.0387,  0.4388, -0.4410, -0.3374, -0.2461,  1.0842]],
       dtype=torch.float64)
	q_value: tensor([[-30.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3694569211519092, distance: 0.9086864975386377 entropy 0.6056879162788391
epoch: 44, step: 96
	action: tensor([[-0.7326, -0.3226,  0.9449, -0.1541, -0.0960, -0.2402,  0.5920]],
       dtype=torch.float64)
	q_value: tensor([[-29.7528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0432416024108675, distance: 1.6357485899756894 entropy 0.6056879162788391
epoch: 44, step: 97
	action: tensor([[ 0.7756, -0.6343,  0.5275, -0.5596,  0.4903, -0.1960,  0.1458]],
       dtype=torch.float64)
	q_value: tensor([[-29.7754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25928031627778836, distance: 1.2841563414759256 entropy 0.6056879162788391
epoch: 44, step: 98
	action: tensor([[-0.1600, -0.1542, -0.4726,  0.4958, -0.1613,  0.0961, -0.3798]],
       dtype=torch.float64)
	q_value: tensor([[-29.1763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12125922187035787, distance: 1.072721743086364 entropy 0.6056879162788391
epoch: 44, step: 99
	action: tensor([[-0.4751, -0.3176,  0.3744,  0.1434,  0.1863,  0.0177,  0.4504]],
       dtype=torch.float64)
	q_value: tensor([[-28.3745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39587442276983786, distance: 1.352009888600381 entropy 0.6056879162788391
epoch: 44, step: 100
	action: tensor([[ 0.2756, -0.3460, -0.3241, -0.4505, -0.8569,  0.0206, -0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-27.6958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006416129205327192, distance: 1.1406672161267635 entropy 0.6056879162788391
epoch: 44, step: 101
	action: tensor([[ 0.9526, -0.4726,  0.4920,  0.2785, -0.5809, -0.4583,  0.1989]],
       dtype=torch.float64)
	q_value: tensor([[-29.3179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3075084494638086, distance: 0.9522783978977919 entropy 0.6056879162788391
epoch: 44, step: 102
	action: tensor([[ 0.1025, -0.0808, -0.1859,  0.0486, -0.0167, -0.2373,  0.3621]],
       dtype=torch.float64)
	q_value: tensor([[-30.4996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3248969724680336, distance: 0.9402464904878118 entropy 0.6056879162788391
epoch: 44, step: 103
	action: tensor([[ 1.1261, -0.1686,  0.0827, -0.8000, -0.1587,  0.3345,  0.4466]],
       dtype=torch.float64)
	q_value: tensor([[-26.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21437042493268843, distance: 1.0142980845254523 entropy 0.6056879162788391
epoch: 44, step: 104
	action: tensor([[ 0.2120,  0.3448,  0.3165,  0.3142, -0.0546, -0.2717, -0.2282]],
       dtype=torch.float64)
	q_value: tensor([[-31.0637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 44, step: 105
	action: tensor([[-0.8491, -0.3275,  1.0301, -0.7789,  0.8653, -0.4309,  0.6148]],
       dtype=torch.float64)
	q_value: tensor([[-36.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.35700514510564, distance: 1.7568592053569352 entropy 0.6056879162788391
epoch: 44, step: 106
	action: tensor([[-0.3730,  0.0094,  0.4820, -0.2896, -0.2960, -0.3499,  0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-32.7626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4141147533934695, distance: 1.3608147873343563 entropy 0.6056879162788391
epoch: 44, step: 107
	action: tensor([[ 0.4976, -0.8085, -0.0089, -0.3396, -0.7243, -0.2843, -0.3512]],
       dtype=torch.float64)
	q_value: tensor([[-27.6325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4595210320258698, distance: 1.3824895973133842 entropy 0.6056879162788391
epoch: 44, step: 108
	action: tensor([[-0.0807, -0.2966,  0.6316, -0.5063, -0.2389,  0.0664, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-30.4181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37946292354734257, distance: 1.3440385003833526 entropy 0.6056879162788391
epoch: 44, step: 109
	action: tensor([[-0.9137, -0.3143,  0.4587,  0.2057, -0.1380,  0.2523,  0.0534]],
       dtype=torch.float64)
	q_value: tensor([[-27.8963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7894362538825783, distance: 1.530787154484416 entropy 0.6056879162788391
epoch: 44, step: 110
	action: tensor([[-0.3582, -0.0136, -0.0555,  0.0780,  0.4396, -0.1468, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-30.1701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15741271866139028, distance: 1.2311212242147311 entropy 0.6056879162788391
epoch: 44, step: 111
	action: tensor([[-0.1691,  0.2429,  0.6185, -0.2963, -0.1044,  0.3118, -0.1161]],
       dtype=torch.float64)
	q_value: tensor([[-27.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1546261872248399, distance: 1.0521583073712355 entropy 0.6056879162788391
epoch: 44, step: 112
	action: tensor([[-0.0515, -0.4579, -0.0528,  0.5809,  0.0267,  0.1590,  0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-28.7054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20940327128445602, distance: 1.0174994889688465 entropy 0.6056879162788391
epoch: 44, step: 113
	action: tensor([[ 0.5294, -0.0761, -0.0535,  0.2947,  0.3096, -0.4438, -0.6580]],
       dtype=torch.float64)
	q_value: tensor([[-28.7212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6339401761596279, distance: 0.6923612071525023 entropy 0.6056879162788391
epoch: 44, step: 114
	action: tensor([[-0.0654, -0.2011,  0.3130,  0.1621,  0.0019, -0.3229, -0.5814]],
       dtype=torch.float64)
	q_value: tensor([[-29.7275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08950887470149449, distance: 1.0919293807139823 entropy 0.6056879162788391
epoch: 44, step: 115
	action: tensor([[-0.0779, -0.3115,  0.5488, -0.0747,  0.4837, -0.8789, -0.2799]],
       dtype=torch.float64)
	q_value: tensor([[-29.4292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33442343461014534, distance: 1.3219149780095265 entropy 0.6056879162788391
epoch: 44, step: 116
	action: tensor([[ 0.6208,  0.4200,  0.4581, -0.6100, -0.3831,  0.5748,  0.3382]],
       dtype=torch.float64)
	q_value: tensor([[-30.8790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8156935086856887, distance: 0.49127765229842996 entropy 0.6056879162788391
epoch: 44, step: 117
	action: tensor([[-0.1485, -1.3993, -0.7509, -1.3387,  0.2362,  0.2290,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-31.2270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3345161821653426, distance: 1.3219609162960233 entropy 0.6056879162788391
epoch: 44, step: 118
	action: tensor([[ 1.3025,  0.2082,  1.1501,  0.3500, -0.1404, -0.3951, -0.2433]],
       dtype=torch.float64)
	q_value: tensor([[-37.0710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7219246588196961, distance: 0.6034453327260523 entropy 0.6056879162788391
epoch: 44, step: 119
	action: tensor([[ 0.3656,  0.4683,  0.1798, -0.7549,  0.2809,  0.2382, -0.2851]],
       dtype=torch.float64)
	q_value: tensor([[-34.0136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6366382418966576, distance: 0.68980494340757 entropy 0.6056879162788391
epoch: 44, step: 120
	action: tensor([[ 0.1424, -0.5646,  0.0213, -0.4125,  0.0813,  0.1510,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-28.8895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2894855171293358, distance: 1.2994660202928683 entropy 0.6056879162788391
epoch: 44, step: 121
	action: tensor([[ 0.4686, -0.5266,  0.8635, -0.3546,  0.5971, -0.0418,  0.5690]],
       dtype=torch.float64)
	q_value: tensor([[-27.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12745254358248204, distance: 1.2150826750649388 entropy 0.6056879162788391
epoch: 44, step: 122
	action: tensor([[-0.5060, -0.2518,  0.8804,  0.1120, -0.2341, -0.6349,  0.5223]],
       dtype=torch.float64)
	q_value: tensor([[-28.9845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6127840082929623, distance: 1.4532649439855332 entropy 0.6056879162788391
epoch: 44, step: 123
	action: tensor([[ 0.1184, -1.2589,  0.0301, -0.0644, -0.1290, -0.2783, -0.3007]],
       dtype=torch.float64)
	q_value: tensor([[-29.5837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8932536951361043, distance: 1.5745668355357911 entropy 0.6056879162788391
epoch: 44, step: 124
	action: tensor([[-0.1872,  0.1092,  0.9970, -0.3080,  0.5130, -0.1627,  0.6106]],
       dtype=torch.float64)
	q_value: tensor([[-31.5309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.164454917992362, distance: 1.234860881290145 entropy 0.6056879162788391
epoch: 44, step: 125
	action: tensor([[ 0.1121, -0.5574, -0.1808, -0.1574,  0.0857,  0.1097, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[-29.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1942628989383035, distance: 1.2505661341067318 entropy 0.6056879162788391
epoch: 44, step: 126
	action: tensor([[ 0.0209, -0.7036,  0.0690,  0.2506,  0.1062, -0.4513, -0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-27.4596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31416199217316443, distance: 1.3118408320244201 entropy 0.6056879162788391
epoch: 44, step: 127
	action: tensor([[-0.1611, -0.0284, -0.7738, -0.0470,  0.0342,  0.0946,  0.0633]],
       dtype=torch.float64)
	q_value: tensor([[-28.8524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06798530653385537, distance: 1.1047603344766455 entropy 0.6056879162788391
LOSS epoch 44 actor 422.2096150747702 critic 152.66048513330387 
epoch: 45, step: 0
	action: tensor([[ 0.4322, -0.7375,  0.0052,  0.5607,  0.3651, -0.0442,  0.4884]],
       dtype=torch.float64)
	q_value: tensor([[-25.6116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3119422988844075, distance: 0.9492249026898407 entropy 0.5003275275230408
epoch: 45, step: 1
	action: tensor([[-0.0445,  0.0895,  0.3456, -0.0101,  0.2505,  0.3722, -0.9642]],
       dtype=torch.float64)
	q_value: tensor([[-28.2159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4248572899344124, distance: 0.8678496706973288 entropy 0.5003275275230408
epoch: 45, step: 2
	action: tensor([[-0.3969,  0.0899,  0.4060, -0.2503,  0.0171,  0.5246,  0.3163]],
       dtype=torch.float64)
	q_value: tensor([[-29.8520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07894506625456943, distance: 1.1886564737461665 entropy 0.5003275275230408
epoch: 45, step: 3
	action: tensor([[ 0.1186, -0.8997,  0.8588, -0.6808, -1.1023, -1.0348,  0.5100]],
       dtype=torch.float64)
	q_value: tensor([[-27.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7660916404670075, distance: 1.52076920869885 entropy 0.5003275275230408
epoch: 45, step: 4
	action: tensor([[-0.4385,  0.4378,  0.3551, -0.5602, -0.0893,  0.5715,  0.6419]],
       dtype=torch.float64)
	q_value: tensor([[-31.7884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08660223939624268, distance: 1.1928669091023651 entropy 0.5003275275230408
epoch: 45, step: 5
	action: tensor([[ 0.0695, -0.3634,  0.2341,  0.2468, -0.3892, -0.2924,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[-30.0555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12213797458412567, distance: 1.0721852407686625 entropy 0.5003275275230408
epoch: 45, step: 6
	action: tensor([[-0.1844, -0.0496, -0.3791, -0.3117,  0.1326,  0.0109,  0.3259]],
       dtype=torch.float64)
	q_value: tensor([[-26.5584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021700421167743622, distance: 1.1566939910957745 entropy 0.5003275275230408
epoch: 45, step: 7
	action: tensor([[ 0.2641, -0.1263,  1.0016, -0.3510,  0.0716, -0.4276,  0.6901]],
       dtype=torch.float64)
	q_value: tensor([[-25.4605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0791548724528397, distance: 1.0981204781547809 entropy 0.5003275275230408
epoch: 45, step: 8
	action: tensor([[-0.0216,  0.1447,  0.2727, -0.3174,  0.0833,  0.2644, -0.0687]],
       dtype=torch.float64)
	q_value: tensor([[-27.0253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24972391245101722, distance: 0.9912135854373493 entropy 0.5003275275230408
epoch: 45, step: 9
	action: tensor([[ 0.2633,  0.1980, -0.0289,  0.2505,  0.4559,  0.3602,  0.1564]],
       dtype=torch.float64)
	q_value: tensor([[-26.1130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 10
	action: tensor([[-0.5305, -1.0073,  0.8002, -0.3401,  0.4209,  0.2494,  0.3111]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1248303487477482, distance: 1.6680874834479662 entropy 0.5003275275230408
epoch: 45, step: 11
	action: tensor([[-0.4112, -0.1865,  0.2849,  0.3861, -0.2760, -0.3855, -0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-29.5803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13095596237108786, distance: 1.2169690700378286 entropy 0.5003275275230408
epoch: 45, step: 12
	action: tensor([[-0.1122,  0.0634,  0.9351,  0.0170,  0.6047,  0.1080,  0.1290]],
       dtype=torch.float64)
	q_value: tensor([[-28.1148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2775712121806849, distance: 0.9726446786894719 entropy 0.5003275275230408
epoch: 45, step: 13
	action: tensor([[-0.0940, -0.5833,  0.2800,  0.0551, -0.1511, -0.1818, -0.8025]],
       dtype=torch.float64)
	q_value: tensor([[-28.6155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34522460659844056, distance: 1.3272541582403945 entropy 0.5003275275230408
epoch: 45, step: 14
	action: tensor([[ 0.3830, -0.5412,  0.7991, -0.2900, -0.0648, -0.3002,  0.4928]],
       dtype=torch.float64)
	q_value: tensor([[-29.1470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20400282783207957, distance: 1.2556553365455672 entropy 0.5003275275230408
epoch: 45, step: 15
	action: tensor([[ 0.6121, -0.7417,  1.0851, -0.3862, -0.1099,  0.6284, -0.6432]],
       dtype=torch.float64)
	q_value: tensor([[-26.6501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.044556070820977345, distance: 1.1695601761238188 entropy 0.5003275275230408
epoch: 45, step: 16
	action: tensor([[ 0.4077, -0.3153,  0.7269, -0.1889,  0.4009,  0.9933,  0.1707]],
       dtype=torch.float64)
	q_value: tensor([[-31.6278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5438035693425536, distance: 0.7729165999599528 entropy 0.5003275275230408
epoch: 45, step: 17
	action: tensor([[ 0.1533, -0.4562,  0.1603,  0.4844, -0.2015, -0.1065, -0.5299]],
       dtype=torch.float64)
	q_value: tensor([[-29.8485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37616814422679523, distance: 0.9038377313741339 entropy 0.5003275275230408
epoch: 45, step: 18
	action: tensor([[-0.1661,  0.5715,  0.2645, -0.9511, -0.6290,  0.7383, -0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-28.4684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13089918866430017, distance: 1.0668215279115525 entropy 0.5003275275230408
epoch: 45, step: 19
	action: tensor([[-0.1789,  0.0197,  0.0395,  0.0727, -0.5214,  0.0814, -0.1460]],
       dtype=torch.float64)
	q_value: tensor([[-31.1954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1761641342944179, distance: 1.038668691205177 entropy 0.5003275275230408
epoch: 45, step: 20
	action: tensor([[ 0.2456, -0.3948,  0.5926, -0.1228,  0.5694, -0.4485, -0.1653]],
       dtype=torch.float64)
	q_value: tensor([[-26.2591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05980098907908382, distance: 1.1780639152570733 entropy 0.5003275275230408
epoch: 45, step: 21
	action: tensor([[ 0.3922,  0.0989,  0.3729, -0.2508,  0.2118,  0.2951,  0.2053]],
       dtype=torch.float64)
	q_value: tensor([[-27.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6318184264480078, distance: 0.6943648341142828 entropy 0.5003275275230408
epoch: 45, step: 22
	action: tensor([[ 1.0717,  0.0650, -0.0791,  0.2893,  0.0145,  0.0200, -0.4479]],
       dtype=torch.float64)
	q_value: tensor([[-26.2243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8726331169950374, distance: 0.40839926667122606 entropy 0.5003275275230408
epoch: 45, step: 23
	action: tensor([[-0.0123,  0.3977, -0.0979,  0.1661, -0.5495, -0.3566,  0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-28.3139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 24
	action: tensor([[ 0.3607, -0.5354,  1.3178, -0.9543, -0.2942,  0.1763,  0.2674]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28336977858980905, distance: 1.296380820938431 entropy 0.5003275275230408
epoch: 45, step: 25
	action: tensor([[-0.4646, -0.3402, -0.7269, -0.0676,  0.3670,  0.1873,  0.1793]],
       dtype=torch.float64)
	q_value: tensor([[-29.7053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47063411139452604, distance: 1.387742889835633 entropy 0.5003275275230408
epoch: 45, step: 26
	action: tensor([[ 0.5187, -0.4572,  0.0127,  0.1820, -0.5360,  0.1518, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-27.1984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38815084099457786, distance: 0.8951150866729387 entropy 0.5003275275230408
epoch: 45, step: 27
	action: tensor([[ 0.3967,  0.0881, -0.1239, -0.4996, -0.4762,  0.3493, -0.6121]],
       dtype=torch.float64)
	q_value: tensor([[-27.4319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5169326111667142, distance: 0.795354153187291 entropy 0.5003275275230408
epoch: 45, step: 28
	action: tensor([[-0.3044, -0.2485,  0.5101, -0.5367,  0.5444, -0.3648, -0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-27.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6972525015595672, distance: 1.4908361738656715 entropy 0.5003275275230408
epoch: 45, step: 29
	action: tensor([[ 0.2760, -0.2743,  0.0866, -0.1344, -0.6626, -0.3724,  0.7098]],
       dtype=torch.float64)
	q_value: tensor([[-27.6387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15862231090693368, distance: 1.0496685595053787 entropy 0.5003275275230408
epoch: 45, step: 30
	action: tensor([[ 0.7605,  0.1630, -1.0112, -0.3078,  0.3846,  0.0645, -0.5064]],
       dtype=torch.float64)
	q_value: tensor([[-27.5954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7856650862357857, distance: 0.5297892643981552 entropy 0.5003275275230408
epoch: 45, step: 31
	action: tensor([[ 0.2868, -0.1546,  0.0861, -0.7725, -0.5129,  0.0098,  0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-29.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010677678618654873, distance: 1.1504375018401132 entropy 0.5003275275230408
epoch: 45, step: 32
	action: tensor([[-0.0938, -0.0350, -0.1735, -0.1291,  0.3624,  0.8365, -0.4772]],
       dtype=torch.float64)
	q_value: tensor([[-26.9816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4026144642016646, distance: 0.8844719102240889 entropy 0.5003275275230408
epoch: 45, step: 33
	action: tensor([[ 0.2735,  0.3076, -0.2722, -0.1857, -0.3650,  0.2558,  0.1091]],
       dtype=torch.float64)
	q_value: tensor([[-28.7668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7073146405128197, distance: 0.6190948520753016 entropy 0.5003275275230408
epoch: 45, step: 34
	action: tensor([[ 0.5957, -0.0640, -0.4068, -1.1826, -0.9014,  0.4975,  0.6009]],
       dtype=torch.float64)
	q_value: tensor([[-26.0562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15038875639201532, distance: 1.0547919795754346 entropy 0.5003275275230408
epoch: 45, step: 35
	action: tensor([[ 0.3907, -0.0242,  0.3700, -0.0184, -0.6877, -0.1481, -0.4649]],
       dtype=torch.float64)
	q_value: tensor([[-32.6331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5577037614282618, distance: 0.7610502207651867 entropy 0.5003275275230408
epoch: 45, step: 36
	action: tensor([[ 0.6883, -0.2403, -0.1141,  0.2949,  0.3538,  0.0501,  0.3599]],
       dtype=torch.float64)
	q_value: tensor([[-27.6491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7078582267308932, distance: 0.6185196818142686 entropy 0.5003275275230408
epoch: 45, step: 37
	action: tensor([[ 0.2919, -0.6912, -0.1454, -0.6204, -0.4503, -0.1384, -0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-26.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.472308771440185, distance: 1.3885327995508778 entropy 0.5003275275230408
epoch: 45, step: 38
	action: tensor([[ 0.1270, -0.1643,  0.4719,  0.0222,  0.9186,  0.3485, -0.0512]],
       dtype=torch.float64)
	q_value: tensor([[-27.9202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40105920008544715, distance: 0.8856225015811474 entropy 0.5003275275230408
epoch: 45, step: 39
	action: tensor([[-0.0980, -0.4799, -0.1133,  0.2032,  0.1457,  0.0530,  0.4664]],
       dtype=torch.float64)
	q_value: tensor([[-28.7177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08785816398707103, distance: 1.193556084119442 entropy 0.5003275275230408
epoch: 45, step: 40
	action: tensor([[ 0.2366,  0.1042, -0.5912,  0.4894, -0.3512, -0.1921, -0.2700]],
       dtype=torch.float64)
	q_value: tensor([[-26.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 41
	action: tensor([[-0.7837, -1.0154,  1.0278, -0.8142,  0.7334,  0.2517,  0.9444]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4321119878981232, distance: 1.7846311841662692 entropy 0.5003275275230408
epoch: 45, step: 42
	action: tensor([[-0.3674, -0.4150,  0.1791, -0.3795,  0.4898,  0.0866,  0.6297]],
       dtype=torch.float64)
	q_value: tensor([[-32.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.631565154858633, distance: 1.4617022115684521 entropy 0.5003275275230408
epoch: 45, step: 43
	action: tensor([[ 0.1978,  0.5359, -0.6211,  0.5987,  0.0813, -0.2340,  0.5200]],
       dtype=torch.float64)
	q_value: tensor([[-26.6397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 44
	action: tensor([[-0.3688, -0.4075,  0.9964, -0.7378,  0.9445, -0.2875,  0.4205]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9588248846697831, distance: 1.601601620566037 entropy 0.5003275275230408
epoch: 45, step: 45
	action: tensor([[ 0.0170, -0.4783, -0.3009, -0.1633,  0.0122,  0.3895,  0.1879]],
       dtype=torch.float64)
	q_value: tensor([[-29.8906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05536526508598638, distance: 1.1755959775438707 entropy 0.5003275275230408
epoch: 45, step: 46
	action: tensor([[ 0.1662, -0.2487,  0.3012, -0.6356, -0.2845,  0.5081, -0.4878]],
       dtype=torch.float64)
	q_value: tensor([[-26.6099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004948288037908144, distance: 1.141509470327838 entropy 0.5003275275230408
epoch: 45, step: 47
	action: tensor([[ 0.4469,  0.5640, -0.0361, -0.4396, -0.0591,  0.0951, -0.3423]],
       dtype=torch.float64)
	q_value: tensor([[-27.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8245547783965799, distance: 0.47932211331663943 entropy 0.5003275275230408
epoch: 45, step: 48
	action: tensor([[ 0.8428, -0.4882,  0.1807,  0.4735,  0.2621, -0.4850,  0.3929]],
       dtype=torch.float64)
	q_value: tensor([[-27.0162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48615496181824525, distance: 0.8203001233999635 entropy 0.5003275275230408
epoch: 45, step: 49
	action: tensor([[-0.4886, -0.3858,  0.2830, -0.5602,  0.3623,  0.1561,  0.1960]],
       dtype=torch.float64)
	q_value: tensor([[-28.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8193131850410924, distance: 1.5435134803139565 entropy 0.5003275275230408
epoch: 45, step: 50
	action: tensor([[-0.2266, -0.2368,  0.0413, -0.0531, -0.5004, -0.3500,  0.0549]],
       dtype=torch.float64)
	q_value: tensor([[-26.8471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21505328325579898, distance: 1.2614044388143013 entropy 0.5003275275230408
epoch: 45, step: 51
	action: tensor([[ 0.3798, -0.3674,  0.7760,  0.3399,  0.1934,  0.3961,  0.7972]],
       dtype=torch.float64)
	q_value: tensor([[-25.7771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6505059483361915, distance: 0.6765136870075208 entropy 0.5003275275230408
epoch: 45, step: 52
	action: tensor([[ 0.3905, -0.5504,  0.3033, -0.0823,  0.2124, -0.3537, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-28.3962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09591405804047559, distance: 1.1979672410811018 entropy 0.5003275275230408
epoch: 45, step: 53
	action: tensor([[ 0.4029,  0.0696, -0.2366, -0.6650, -0.4120, -0.1033,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-26.2616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35966944516537647, distance: 0.9157117905094266 entropy 0.5003275275230408
epoch: 45, step: 54
	action: tensor([[-0.1888, -0.2993, -0.1831, -0.0643, -0.1803,  1.0440,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-26.0228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08776507443691361, distance: 1.0929745286903632 entropy 0.5003275275230408
epoch: 45, step: 55
	action: tensor([[ 0.3179,  0.1903,  0.5431,  0.6557,  0.0660,  0.5949, -0.2371]],
       dtype=torch.float64)
	q_value: tensor([[-29.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 56
	action: tensor([[ 0.0725, -0.3267,  0.8190, -0.6204,  1.5153,  0.0117,  0.2383]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36685813972191195, distance: 1.3378838613484576 entropy 0.5003275275230408
epoch: 45, step: 57
	action: tensor([[ 0.0782,  0.3780,  0.5210,  0.4629, -0.2038,  0.4924, -0.1954]],
       dtype=torch.float64)
	q_value: tensor([[-32.1314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 58
	action: tensor([[ 0.0913, -0.2335,  1.3071, -0.9933,  0.5468,  0.6355,  0.3952]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22822701575395055, distance: 1.2682241419119409 entropy 0.5003275275230408
epoch: 45, step: 59
	action: tensor([[ 0.8361, -0.3347,  0.1237, -0.2161,  0.2549, -0.2142,  0.1215]],
       dtype=torch.float64)
	q_value: tensor([[-31.1220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24517478878869448, distance: 0.9942140400998222 entropy 0.5003275275230408
epoch: 45, step: 60
	action: tensor([[-0.4510, -1.0640,  0.0397, -0.2674,  0.2128,  0.1001, -0.1383]],
       dtype=torch.float64)
	q_value: tensor([[-26.4920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.02536379964481, distance: 1.6285766920535871 entropy 0.5003275275230408
epoch: 45, step: 61
	action: tensor([[ 0.1002,  0.1580,  1.1069, -0.4497, -0.3480, -0.2467, -0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-29.3048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1258161672621393, distance: 1.0699366849367957 entropy 0.5003275275230408
epoch: 45, step: 62
	action: tensor([[ 0.4794, -0.1621,  0.4738,  0.2892,  0.0723,  0.6792, -0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-28.6783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8597955547043733, distance: 0.42848693563214046 entropy 0.5003275275230408
epoch: 45, step: 63
	action: tensor([[-0.6466,  0.1449,  0.1347, -0.0666, -0.0141, -0.4196,  0.4560]],
       dtype=torch.float64)
	q_value: tensor([[-28.4692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4926957432136534, distance: 1.3981132141561978 entropy 0.5003275275230408
epoch: 45, step: 64
	action: tensor([[-0.3075,  0.2248,  0.4699, -0.2099,  0.6437,  0.4600, -0.4875]],
       dtype=torch.float64)
	q_value: tensor([[-25.7544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13923964219299167, distance: 1.061690232720104 entropy 0.5003275275230408
epoch: 45, step: 65
	action: tensor([[-0.2758,  0.1115,  0.6264, -0.1491, -0.0852,  0.1185,  0.2840]],
       dtype=torch.float64)
	q_value: tensor([[-29.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01519387788691029, distance: 1.1530049938918163 entropy 0.5003275275230408
epoch: 45, step: 66
	action: tensor([[ 0.2998, -0.2300,  0.2316, -0.3281,  0.4730,  0.1756,  0.1748]],
       dtype=torch.float64)
	q_value: tensor([[-26.5713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20625822948056238, distance: 1.0195213176078937 entropy 0.5003275275230408
epoch: 45, step: 67
	action: tensor([[ 0.1522, -0.2122,  0.3571, -0.5009,  0.4290, -0.0358,  0.3711]],
       dtype=torch.float64)
	q_value: tensor([[-26.0731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08757458253230044, distance: 1.1934005066541205 entropy 0.5003275275230408
epoch: 45, step: 68
	action: tensor([[-0.0631, -0.2979, -0.1902, -0.3865,  0.3610, -0.2639,  0.3893]],
       dtype=torch.float64)
	q_value: tensor([[-25.9004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25534928567061677, distance: 1.2821504324086188 entropy 0.5003275275230408
epoch: 45, step: 69
	action: tensor([[ 0.2170,  0.4712,  0.6172, -0.7350, -0.1138, -0.4282, -0.5408]],
       dtype=torch.float64)
	q_value: tensor([[-25.9562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28055371807233664, distance: 0.9706348482404202 entropy 0.5003275275230408
epoch: 45, step: 70
	action: tensor([[-0.1608, -0.1879,  0.2467,  0.3226,  0.2997, -0.0836, -0.1716]],
       dtype=torch.float64)
	q_value: tensor([[-28.5232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1443044391159053, distance: 1.0585620805054698 entropy 0.5003275275230408
epoch: 45, step: 71
	action: tensor([[-0.0368,  0.3388,  0.1185,  0.0726, -0.4258, -0.1079,  0.9453]],
       dtype=torch.float64)
	q_value: tensor([[-26.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 72
	action: tensor([[-0.4426,  0.3340,  0.8990, -0.7438,  0.1933, -0.2751,  1.0050]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6065120865491611, distance: 1.450436405637889 entropy 0.5003275275230408
epoch: 45, step: 73
	action: tensor([[ 0.2170, -0.6427, -0.0866, -0.4423, -0.3349, -0.2526,  0.3851]],
       dtype=torch.float64)
	q_value: tensor([[-29.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42464997089981726, distance: 1.365874446350048 entropy 0.5003275275230408
epoch: 45, step: 74
	action: tensor([[ 0.3719, -0.2885,  0.4626, -0.2088,  0.0435,  0.2878,  0.0402]],
       dtype=torch.float64)
	q_value: tensor([[-27.2573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30722189047730686, distance: 0.9524754080329806 entropy 0.5003275275230408
epoch: 45, step: 75
	action: tensor([[ 0.0150, -0.3077,  0.0912, -0.3937,  0.0683,  0.0809, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[-26.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1589683418326906, distance: 1.231948292029558 entropy 0.5003275275230408
epoch: 45, step: 76
	action: tensor([[ 0.1673, -0.3709,  0.5967, -0.8715, -0.7374,  0.5591, -0.4823]],
       dtype=torch.float64)
	q_value: tensor([[-25.4510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2812341703471455, distance: 1.2953017421189061 entropy 0.5003275275230408
epoch: 45, step: 77
	action: tensor([[-0.1231,  0.4781,  0.0846, -0.0347,  0.3714, -0.3822,  0.2971]],
       dtype=torch.float64)
	q_value: tensor([[-30.4388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 78
	action: tensor([[ 0.4745, -0.2880,  0.6816, -0.7208,  0.6705,  0.4154,  0.4442]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.057911487763172786, distance: 1.1107147706700318 entropy 0.5003275275230408
epoch: 45, step: 79
	action: tensor([[ 0.1447, -0.6771,  0.3558,  0.1732,  0.5654,  0.0466, -0.2491]],
       dtype=torch.float64)
	q_value: tensor([[-28.2547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0408318953398521, distance: 1.1674733869568081 entropy 0.5003275275230408
epoch: 45, step: 80
	action: tensor([[ 0.0683, -0.0546,  0.5817, -0.3004,  0.0989,  0.3001, -0.0045]],
       dtype=torch.float64)
	q_value: tensor([[-28.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21768915694158852, distance: 1.0121534694968786 entropy 0.5003275275230408
epoch: 45, step: 81
	action: tensor([[ 0.0647, -0.5930,  0.3749, -0.5580,  0.1104, -0.2935, -0.5323]],
       dtype=torch.float64)
	q_value: tensor([[-26.1929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6247318251683278, distance: 1.4586380454502894 entropy 0.5003275275230408
epoch: 45, step: 82
	action: tensor([[-0.6797, -0.3646,  0.5240, -0.4791,  0.0243, -0.5687, -0.0540]],
       dtype=torch.float64)
	q_value: tensor([[-27.9734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1101140345927818, distance: 1.6623009622864875 entropy 0.5003275275230408
epoch: 45, step: 83
	action: tensor([[ 0.0122, -0.6763,  0.3311, -0.2779, -0.1011, -0.4552,  0.4017]],
       dtype=torch.float64)
	q_value: tensor([[-27.6840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.611509393951877, distance: 1.4526905588038772 entropy 0.5003275275230408
epoch: 45, step: 84
	action: tensor([[ 0.3150,  0.2516,  0.1646, -0.2886,  0.6265,  0.4423, -0.0594]],
       dtype=torch.float64)
	q_value: tensor([[-26.8363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7154612595429863, distance: 0.6104180899903408 entropy 0.5003275275230408
epoch: 45, step: 85
	action: tensor([[-0.0349,  0.3005, -0.0086,  0.3450, -1.0785, -0.1236,  0.2917]],
       dtype=torch.float64)
	q_value: tensor([[-27.2505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 86
	action: tensor([[-1.2521, -0.6999, -0.0891, -0.2705,  0.6630, -0.1879,  0.2572]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5248478629517992, distance: 1.818336683996515 entropy 0.5003275275230408
epoch: 45, step: 87
	action: tensor([[ 0.0984, -0.6368,  0.2224,  0.3209,  0.7495,  0.6167,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[-29.9681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32806631307144174, distance: 0.9380368521619967 entropy 0.5003275275230408
epoch: 45, step: 88
	action: tensor([[ 0.1848, -0.0729,  0.0970, -0.4371,  0.5468,  0.3004,  0.8461]],
       dtype=torch.float64)
	q_value: tensor([[-29.3448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2922119996967727, distance: 0.9627383921371189 entropy 0.5003275275230408
epoch: 45, step: 89
	action: tensor([[-0.0579, -0.1990,  0.3312, -0.4055, -0.5424,  0.1222,  0.1693]],
       dtype=torch.float64)
	q_value: tensor([[-27.6305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1905035010211824, distance: 1.2485962657460763 entropy 0.5003275275230408
epoch: 45, step: 90
	action: tensor([[ 0.1785, -0.0329, -0.3098,  0.1594, -0.3245,  0.1117, -0.1891]],
       dtype=torch.float64)
	q_value: tensor([[-26.4877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46604584688425876, distance: 0.8361971397243299 entropy 0.5003275275230408
epoch: 45, step: 91
	action: tensor([[ 0.5664, -0.1884, -0.0599, -0.0384, -0.4431, -0.4115,  0.1980]],
       dtype=torch.float64)
	q_value: tensor([[-25.6390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43064613153897724, distance: 0.86347114904778 entropy 0.5003275275230408
epoch: 45, step: 92
	action: tensor([[ 0.1483, -0.2142,  0.4917,  0.2069,  0.2793, -0.2153, -0.4748]],
       dtype=torch.float64)
	q_value: tensor([[-26.5115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34836818858102103, distance: 0.9237571907297147 entropy 0.5003275275230408
epoch: 45, step: 93
	action: tensor([[ 0.6502, -0.4571,  0.5103,  0.1545,  0.0208,  0.0434,  0.3755]],
       dtype=torch.float64)
	q_value: tensor([[-28.1306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42388347952675987, distance: 0.8685840654809271 entropy 0.5003275275230408
epoch: 45, step: 94
	action: tensor([[-0.1283,  0.7433, -0.0393, -0.6636,  0.7985, -0.5683,  0.4585]],
       dtype=torch.float64)
	q_value: tensor([[-26.7890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 95
	action: tensor([[ 0.0382, -1.0762,  1.0437, -1.1489,  0.0595, -0.2588, -0.2468]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.035178184838252, distance: 1.6325177526187187 entropy 0.5003275275230408
epoch: 45, step: 96
	action: tensor([[ 0.2825, -0.0700,  0.4613, -0.6751,  0.1786, -0.6636, -0.3197]],
       dtype=torch.float64)
	q_value: tensor([[-31.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08682538720988098, distance: 1.1929893881330145 entropy 0.5003275275230408
epoch: 45, step: 97
	action: tensor([[ 0.0137, -0.4565,  0.2145,  0.4197, -0.7524,  0.1742,  0.2396]],
       dtype=torch.float64)
	q_value: tensor([[-27.2341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22156049919638166, distance: 1.0096459929617283 entropy 0.5003275275230408
epoch: 45, step: 98
	action: tensor([[ 0.3684, -0.3847, -0.2316, -0.2875,  0.3293, -0.0602,  0.4042]],
       dtype=torch.float64)
	q_value: tensor([[-28.4443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07077943207790449, distance: 1.1031030880257777 entropy 0.5003275275230408
epoch: 45, step: 99
	action: tensor([[ 0.2151, -0.1114, -0.4066, -0.1335, -0.1164, -0.1815,  0.1712]],
       dtype=torch.float64)
	q_value: tensor([[-26.1775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3205282514256659, distance: 0.9432838421871707 entropy 0.5003275275230408
epoch: 45, step: 100
	action: tensor([[-0.1305, -0.5591, -0.1044, -0.1795,  0.0347,  0.3211, -0.3615]],
       dtype=torch.float64)
	q_value: tensor([[-25.1899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2790398250310764, distance: 1.2941920474881787 entropy 0.5003275275230408
epoch: 45, step: 101
	action: tensor([[ 0.4426, -0.0691,  0.2733, -0.3026,  0.2905, -0.6522, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-26.9620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23927941770658723, distance: 0.9980890171254284 entropy 0.5003275275230408
epoch: 45, step: 102
	action: tensor([[ 0.5811,  0.3570,  1.1158,  0.1043,  0.4599,  0.3840, -0.5904]],
       dtype=torch.float64)
	q_value: tensor([[-26.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9507171603021841, distance: 0.2540414338633324 entropy 0.5003275275230408
epoch: 45, step: 103
	action: tensor([[-0.1440, -0.4801,  0.2617, -0.4929,  0.1796,  0.2242,  0.6057]],
       dtype=torch.float64)
	q_value: tensor([[-31.8203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4898708841833892, distance: 1.396789654724879 entropy 0.5003275275230408
epoch: 45, step: 104
	action: tensor([[ 0.4976, -0.6848,  0.3219,  0.1679,  0.1638, -0.6909, -0.0597]],
       dtype=torch.float64)
	q_value: tensor([[-26.7440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0974713889596952, distance: 1.1988181148792045 entropy 0.5003275275230408
epoch: 45, step: 105
	action: tensor([[-0.1168, -0.3433, -0.3026, -0.5703, -0.4828,  0.6612,  0.5295]],
       dtype=torch.float64)
	q_value: tensor([[-27.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23792064159535165, distance: 1.2732189550902855 entropy 0.5003275275230408
epoch: 45, step: 106
	action: tensor([[ 0.7443, -0.1493,  0.3014, -0.3190,  0.4782, -0.2892,  0.6651]],
       dtype=torch.float64)
	q_value: tensor([[-29.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3837503515747763, distance: 0.8983282049961937 entropy 0.5003275275230408
epoch: 45, step: 107
	action: tensor([[ 0.1197, -0.4565, -0.2627,  0.4636,  0.2424,  0.1041, -0.3568]],
       dtype=torch.float64)
	q_value: tensor([[-26.8388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2936629527457344, distance: 0.961751087436652 entropy 0.5003275275230408
epoch: 45, step: 108
	action: tensor([[-0.1899, -0.3948, -0.2625, -0.4304,  0.2568, -0.0998,  0.1848]],
       dtype=torch.float64)
	q_value: tensor([[-27.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3568952056478707, distance: 1.332999072562015 entropy 0.5003275275230408
epoch: 45, step: 109
	action: tensor([[-0.0725, -0.2369,  1.2037, -0.6753, -0.1176, -0.9134,  0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-26.2578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49461367216210794, distance: 1.399011126885178 entropy 0.5003275275230408
epoch: 45, step: 110
	action: tensor([[0.7823, 0.5667, 0.5717, 0.3820, 0.2016, 0.1986, 0.4090]],
       dtype=torch.float64)
	q_value: tensor([[-29.4464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9647903051996012, distance: 0.2147275770228753 entropy 0.5003275275230408
epoch: 45, step: 111
	action: tensor([[ 0.6271, -0.3428, -0.0688, -0.2505,  0.6804,  0.2247,  0.6751]],
       dtype=torch.float64)
	q_value: tensor([[-29.2186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.339375788285292, distance: 0.9301091921792942 entropy 0.5003275275230408
epoch: 45, step: 112
	action: tensor([[ 0.0426,  0.3881, -0.4640,  0.1886,  0.0372,  0.1396,  0.0688]],
       dtype=torch.float64)
	q_value: tensor([[-28.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 113
	action: tensor([[-0.2035, -0.3037,  0.6518,  0.0429,  1.3884, -0.1708,  0.1014]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19065323976625814, distance: 1.2486747860356844 entropy 0.5003275275230408
epoch: 45, step: 114
	action: tensor([[ 0.2144,  0.4481, -0.3819,  0.2129, -0.8917, -0.2074,  0.3238]],
       dtype=torch.float64)
	q_value: tensor([[-31.2489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 115
	action: tensor([[-5.2823e-01,  7.6173e-04,  1.3480e+00, -2.4441e-01,  2.7442e-01,
          6.3990e-01, -8.1595e-02]], dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25327004091544647, distance: 1.2810881745186815 entropy 0.5003275275230408
epoch: 45, step: 116
	action: tensor([[ 0.4925, -0.3293, -0.1801,  0.5870, -0.0064,  0.4120, -0.3415]],
       dtype=torch.float64)
	q_value: tensor([[-31.7660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.75959537436476, distance: 0.5610842825164922 entropy 0.5003275275230408
epoch: 45, step: 117
	action: tensor([[ 0.5757,  0.6115, -0.1418, -0.6594,  0.1396,  0.1184,  0.3260]],
       dtype=torch.float64)
	q_value: tensor([[-28.4385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8685362506303926, distance: 0.4149155394913806 entropy 0.5003275275230408
epoch: 45, step: 118
	action: tensor([[ 0.0130, -0.1443,  0.4760,  0.7649,  0.2051,  0.2596,  0.5186]],
       dtype=torch.float64)
	q_value: tensor([[-27.7631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 119
	action: tensor([[-0.3779, -0.6182,  0.7822,  0.1074,  0.4334,  0.6915,  0.3267]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18612786339380616, distance: 1.246299576083461 entropy 0.5003275275230408
epoch: 45, step: 120
	action: tensor([[-0.4066, -0.5054,  0.5814,  0.4068, -0.0521,  0.6637, -0.4074]],
       dtype=torch.float64)
	q_value: tensor([[-29.9468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06603043587493018, distance: 1.1059183271070872 entropy 0.5003275275230408
epoch: 45, step: 121
	action: tensor([[-0.1740, -0.5853,  0.3425, -0.3836, -0.3313, -0.2901,  0.6840]],
       dtype=torch.float64)
	q_value: tensor([[-30.9342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7182555591165525, distance: 1.500032166473409 entropy 0.5003275275230408
epoch: 45, step: 122
	action: tensor([[ 0.6489,  0.6708, -0.2633,  0.1702,  0.2670, -0.7019, -0.3979]],
       dtype=torch.float64)
	q_value: tensor([[-27.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 123
	action: tensor([[-0.3252, -0.6488,  1.3515, -0.7128,  0.9053,  0.1561, -0.4476]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0104961957717538, distance: 1.6225882290959623 entropy 0.5003275275230408
epoch: 45, step: 124
	action: tensor([[-0.3320,  0.4831,  0.3385, -0.1767,  0.5853,  0.0632, -0.1572]],
       dtype=torch.float64)
	q_value: tensor([[-32.7773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 45, step: 125
	action: tensor([[-0.8797,  0.1242,  0.6761, -0.8530,  0.4963,  0.0160,  0.3656]],
       dtype=torch.float64)
	q_value: tensor([[-35.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1945986691653228, distance: 1.6952519360058467 entropy 0.5003275275230408
epoch: 45, step: 126
	action: tensor([[ 0.2628, -0.0126,  0.2912, -0.4555,  1.2344,  0.2800,  0.1882]],
       dtype=torch.float64)
	q_value: tensor([[-29.7045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30995909190697335, distance: 0.9505919064286289 entropy 0.5003275275230408
epoch: 45, step: 127
	action: tensor([[ 0.2781,  0.1487,  0.5224, -0.1723,  0.1121, -0.3407, -0.2771]],
       dtype=torch.float64)
	q_value: tensor([[-29.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44720437585675255, distance: 0.8508225494700795 entropy 0.5003275275230408
LOSS epoch 45 actor 377.79404083620705 critic 221.2005848482195 
epoch: 46, step: 0
	action: tensor([[ 0.0465, -0.4009,  0.0217,  0.2370,  0.1400, -0.8720,  0.0488]],
       dtype=torch.float64)
	q_value: tensor([[-25.8692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08284045002001639, distance: 1.1908002814155165 entropy 0.5003275275230408
epoch: 46, step: 1
	action: tensor([[ 0.2699,  0.2077,  0.1667,  0.0864,  0.9230, -0.2221,  0.1508]],
       dtype=torch.float64)
	q_value: tensor([[-26.1853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6457492026471057, distance: 0.6811019308914946 entropy 0.5003275275230408
epoch: 46, step: 2
	action: tensor([[ 0.0801, -0.0496,  0.4728, -0.1738,  0.3973,  0.1222, -0.3264]],
       dtype=torch.float64)
	q_value: tensor([[-26.8560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2539099087739245, distance: 0.9884445913973252 entropy 0.5003275275230408
epoch: 46, step: 3
	action: tensor([[-0.2544, -0.1328,  0.4883, -0.0299, -0.1741, -0.1987,  0.2985]],
       dtype=torch.float64)
	q_value: tensor([[-26.1906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1816311024984707, distance: 1.2439348930140555 entropy 0.5003275275230408
epoch: 46, step: 4
	action: tensor([[-0.3606, -0.1508, -0.0475,  0.0063,  0.3456,  0.1783,  0.1767]],
       dtype=torch.float64)
	q_value: tensor([[-24.6483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18108226427956509, distance: 1.2436459710719265 entropy 0.5003275275230408
epoch: 46, step: 5
	action: tensor([[-0.3208,  0.4528,  0.4548, -0.3960, -0.4012,  0.4935, -0.7051]],
       dtype=torch.float64)
	q_value: tensor([[-24.8341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07825479022760418, distance: 1.0986570272898097 entropy 0.5003275275230408
epoch: 46, step: 6
	action: tensor([[-0.7143,  0.3508,  0.3086, -0.2226, -0.0935,  0.1144,  0.6840]],
       dtype=torch.float64)
	q_value: tensor([[-29.0097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43390343463150716, distance: 1.3703031175323648 entropy 0.5003275275230408
epoch: 46, step: 7
	action: tensor([[ 0.2115, -0.2389, -0.2211,  0.0612,  0.3956,  0.4925, -0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-27.0259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45989196379326447, distance: 0.8410019694857089 entropy 0.5003275275230408
epoch: 46, step: 8
	action: tensor([[ 0.1168,  0.4044,  0.1383, -0.3150, -0.0357,  0.2722, -0.4437]],
       dtype=torch.float64)
	q_value: tensor([[-26.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 9
	action: tensor([[-0.6260,  0.2824,  1.6960, -0.7751,  1.2207,  0.4310,  0.5818]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 10
	action: tensor([[-0.7252, -0.2037,  1.8272, -0.8564,  1.3226,  0.2261,  0.5448]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0980559274445314, distance: 1.6575446031002277 entropy 0.5003275275230408
epoch: 46, step: 11
	action: tensor([[ 0.7208, -0.3292, -0.7929, -0.5349,  0.3536, -0.1647,  0.2008]],
       dtype=torch.float64)
	q_value: tensor([[-34.8090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16279543126925633, distance: 1.047062216154143 entropy 0.5003275275230408
epoch: 46, step: 12
	action: tensor([[-0.1340, -0.2173,  1.0722, -0.4906, -0.3625,  0.2000, -0.7133]],
       dtype=torch.float64)
	q_value: tensor([[-28.2408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2951373130788344, distance: 1.302310676852586 entropy 0.5003275275230408
epoch: 46, step: 13
	action: tensor([[ 0.1355, -0.4325, -0.2189, -0.2537,  0.4370,  0.2700,  0.1372]],
       dtype=torch.float64)
	q_value: tensor([[-29.3364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004684855896858631, distance: 1.1416605631706236 entropy 0.5003275275230408
epoch: 46, step: 14
	action: tensor([[-0.0234, -0.5061,  0.6765, -0.0862,  0.7040,  0.4435,  0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-25.8884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04908009237897337, distance: 1.1720901498385219 entropy 0.5003275275230408
epoch: 46, step: 15
	action: tensor([[ 0.3753,  0.0885,  0.6245,  0.4202, -0.1060, -0.1838,  0.4400]],
       dtype=torch.float64)
	q_value: tensor([[-27.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8291182853013399, distance: 0.4730472149166494 entropy 0.5003275275230408
epoch: 46, step: 16
	action: tensor([[ 0.4818,  0.0494, -0.5620, -0.1676, -0.1491,  0.0320, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-26.3026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6413615192973174, distance: 0.6853069501421964 entropy 0.5003275275230408
epoch: 46, step: 17
	action: tensor([[-0.1342, -0.6699,  0.4660,  0.1256, -0.2066,  0.7051,  0.4780]],
       dtype=torch.float64)
	q_value: tensor([[-24.9979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008781984827968214, distance: 1.149358077170654 entropy 0.5003275275230408
epoch: 46, step: 18
	action: tensor([[-0.1131, -0.8901, -0.2061, -0.5769,  0.4082,  0.7327,  0.2538]],
       dtype=torch.float64)
	q_value: tensor([[-27.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5286098940806352, distance: 1.4148324967633348 entropy 0.5003275275230408
epoch: 46, step: 19
	action: tensor([[ 0.1046, -0.7511,  0.5783, -0.3620, -0.1652,  0.0215,  0.0254]],
       dtype=torch.float64)
	q_value: tensor([[-29.3044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5851658905314723, distance: 1.440767995091514 entropy 0.5003275275230408
epoch: 46, step: 20
	action: tensor([[-0.3768,  0.2471,  0.0180,  0.0429,  0.9881,  0.6279,  0.3184]],
       dtype=torch.float64)
	q_value: tensor([[-25.8864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 21
	action: tensor([[-0.3868, -0.0316,  1.3173, -0.3415,  0.3332,  0.3195,  0.9516]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3564321835720634, distance: 1.3327716192341876 entropy 0.5003275275230408
epoch: 46, step: 22
	action: tensor([[ 0.3270, -0.0720, -0.1869,  0.7189,  0.7395,  0.0799, -0.3597]],
       dtype=torch.float64)
	q_value: tensor([[-29.7415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 23
	action: tensor([[ 0.0118,  0.0261,  1.5442, -0.5328,  0.7664,  0.5349,  0.7771]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09513705264396777, distance: 1.0885492821455554 entropy 0.5003275275230408
epoch: 46, step: 24
	action: tensor([[-0.1187,  0.1401,  0.4560,  0.1519,  0.0566,  0.1513, -0.2224]],
       dtype=torch.float64)
	q_value: tensor([[-31.2224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43778293720610517, distance: 0.8580423116391473 entropy 0.5003275275230408
epoch: 46, step: 25
	action: tensor([[-0.2787,  0.2512, -0.2028,  0.0304,  0.5361, -0.4983,  0.0793]],
       dtype=torch.float64)
	q_value: tensor([[-25.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05783097702709317, distance: 1.1107622304108795 entropy 0.5003275275230408
epoch: 46, step: 26
	action: tensor([[-0.4085, -0.0094,  1.0801, -0.3417,  0.2635,  0.4486,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-25.1150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2776969897996031, distance: 1.2935124974752288 entropy 0.5003275275230408
epoch: 46, step: 27
	action: tensor([[ 0.4244, -0.2971,  0.1797, -0.1216,  0.0190, -0.3069, -0.0231]],
       dtype=torch.float64)
	q_value: tensor([[-28.2654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18518828639270468, distance: 1.032964330651387 entropy 0.5003275275230408
epoch: 46, step: 28
	action: tensor([[-0.1465,  0.3142,  0.4889,  0.2692, -0.3295, -0.7883, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-24.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 29
	action: tensor([[-0.8211,  0.1954,  1.4249, -1.3401,  0.7498,  0.3775,  0.8377]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0972505096425564, distance: 1.6572264170923914 entropy 0.5003275275230408
epoch: 46, step: 30
	action: tensor([[-0.4471, -0.2222, -0.0722, -0.0643,  0.0112,  0.0272, -0.1379]],
       dtype=torch.float64)
	q_value: tensor([[-34.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4399805008682982, distance: 1.373203807385475 entropy 0.5003275275230408
epoch: 46, step: 31
	action: tensor([[-0.0078, -0.8257,  0.2461, -0.0347,  0.0528,  0.1958,  0.4958]],
       dtype=torch.float64)
	q_value: tensor([[-24.8070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4012913366087205, distance: 1.354630693700051 entropy 0.5003275275230408
epoch: 46, step: 32
	action: tensor([[-0.4555,  0.0392, -0.0797, -0.3149, -0.0123, -0.3680, -0.2090]],
       dtype=torch.float64)
	q_value: tensor([[-26.5274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35683868226365956, distance: 1.3329713082943357 entropy 0.5003275275230408
epoch: 46, step: 33
	action: tensor([[-0.5228,  0.0745,  0.7380, -0.6940,  0.1860, -0.3618,  0.8733]],
       dtype=torch.float64)
	q_value: tensor([[-24.8375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.817309578603739, distance: 1.5426633118254067 entropy 0.5003275275230408
epoch: 46, step: 34
	action: tensor([[-0.9158,  0.5020,  0.4393,  0.4482, -0.2394, -0.1872, -0.5824]],
       dtype=torch.float64)
	q_value: tensor([[-27.4032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0986928919433474, distance: 1.1994850810897064 entropy 0.5003275275230408
epoch: 46, step: 35
	action: tensor([[ 0.4036,  0.1294,  0.6323, -0.6080, -0.2594,  0.8955,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[-30.6446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 36
	action: tensor([[ 0.4685, -0.2882,  1.2385, -0.6668,  0.5181, -0.3515,  1.2028]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07275697487678934, distance: 1.101928665344262 entropy 0.5003275275230408
epoch: 46, step: 37
	action: tensor([[-0.3727, -0.1194,  0.4567, -0.3243, -0.2808, -0.3949, -0.2033]],
       dtype=torch.float64)
	q_value: tensor([[-28.7106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5463853488320842, distance: 1.423034916805175 entropy 0.5003275275230408
epoch: 46, step: 38
	action: tensor([[ 0.0810, -0.5192, -0.0796, -0.4742,  0.5553, -0.6490, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-25.4372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4813602476839507, distance: 1.3927944782840183 entropy 0.5003275275230408
epoch: 46, step: 39
	action: tensor([[ 0.3668, -0.7288, -0.3488, -0.4560,  1.0561, -0.3159,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-27.3004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44913460664018734, distance: 1.3775616921701075 entropy 0.5003275275230408
epoch: 46, step: 40
	action: tensor([[ 0.5464, -0.3702,  0.2156,  0.0971, -0.1738, -0.3068,  0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-30.2716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3196442377260219, distance: 0.943897263416708 entropy 0.5003275275230408
epoch: 46, step: 41
	action: tensor([[ 0.3192,  0.1400, -0.8578, -0.4731, -0.0232,  1.0769,  0.5487]],
       dtype=torch.float64)
	q_value: tensor([[-25.6134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7109387908726589, distance: 0.6152499693765534 entropy 0.5003275275230408
epoch: 46, step: 42
	action: tensor([[-0.2007,  0.0192,  0.3686,  0.0771,  0.0772,  0.2163,  0.0975]],
       dtype=torch.float64)
	q_value: tensor([[-30.9013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22683927391301728, distance: 1.0062168504487425 entropy 0.5003275275230408
epoch: 46, step: 43
	action: tensor([[ 0.2442,  0.1243,  0.4378, -0.3146, -0.0402,  0.1431,  0.3117]],
       dtype=torch.float64)
	q_value: tensor([[-25.1675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4562473822442412, distance: 0.8438346869233636 entropy 0.5003275275230408
epoch: 46, step: 44
	action: tensor([[-0.2618,  0.4876,  0.1869,  0.0347, -0.2403,  0.6734,  0.5976]],
       dtype=torch.float64)
	q_value: tensor([[-25.1800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 45
	action: tensor([[-0.8779, -0.5169,  1.1542, -0.7109,  0.8641, -0.1185,  0.8996]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4868397268231925, distance: 1.804598497890995 entropy 0.5003275275230408
epoch: 46, step: 46
	action: tensor([[-0.0292, -0.2347,  0.2461,  0.5004,  0.5680, -0.4208, -0.2614]],
       dtype=torch.float64)
	q_value: tensor([[-30.3280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26557323826850365, distance: 0.9806881782874399 entropy 0.5003275275230408
epoch: 46, step: 47
	action: tensor([[ 0.3832,  0.1969,  0.0310, -0.0513,  0.4638, -0.4973, -0.1840]],
       dtype=torch.float64)
	q_value: tensor([[-27.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5943140080205016, distance: 0.7288727257400731 entropy 0.5003275275230408
epoch: 46, step: 48
	action: tensor([[ 0.7262, -0.2245, -0.4385, -0.6351,  0.5788, -0.8220, -0.3806]],
       dtype=torch.float64)
	q_value: tensor([[-25.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04131409300466693, distance: 1.1204561503104513 entropy 0.5003275275230408
epoch: 46, step: 49
	action: tensor([[ 0.2045, -0.4381, -0.0048,  0.0669,  0.7262,  0.9138, -0.1975]],
       dtype=torch.float64)
	q_value: tensor([[-29.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5008515534331357, distance: 0.8084842338400149 entropy 0.5003275275230408
epoch: 46, step: 50
	action: tensor([[-0.1636,  0.2076,  0.1062,  0.3767,  0.2276, -0.4535,  0.1479]],
       dtype=torch.float64)
	q_value: tensor([[-29.2280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 51
	action: tensor([[-0.4680, -0.0816,  1.7969, -0.6470,  0.9518, -0.1552,  0.4425]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5990145919897139, distance: 1.4470478981174912 entropy 0.5003275275230408
epoch: 46, step: 52
	action: tensor([[-0.5503,  0.4023,  0.0855, -0.1863,  0.0819, -0.1304,  0.2061]],
       dtype=torch.float64)
	q_value: tensor([[-32.2154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2195692698291023, distance: 1.2637463946657468 entropy 0.5003275275230408
epoch: 46, step: 53
	action: tensor([[ 0.0879, -0.0418, -0.0257, -0.2073,  0.2402,  0.2081,  0.0460]],
       dtype=torch.float64)
	q_value: tensor([[-25.0055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966016233271138, distance: 0.9597483499345963 entropy 0.5003275275230408
epoch: 46, step: 54
	action: tensor([[ 0.3182,  0.2359, -0.0496, -0.1933,  0.2244,  0.4015,  0.1652]],
       dtype=torch.float64)
	q_value: tensor([[-24.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7289152919091698, distance: 0.5958119412861894 entropy 0.5003275275230408
epoch: 46, step: 55
	action: tensor([[ 0.5313,  0.2339, -0.0103,  0.3250,  0.4275, -0.0211, -0.2687]],
       dtype=torch.float64)
	q_value: tensor([[-25.6485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.89948797393863, distance: 0.3627986872431549 entropy 0.5003275275230408
epoch: 46, step: 56
	action: tensor([[ 0.1910,  0.3443,  0.2286, -0.9293,  0.0928, -0.3165,  0.2332]],
       dtype=torch.float64)
	q_value: tensor([[-26.7356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1768537617007757, distance: 1.0382338689368882 entropy 0.5003275275230408
epoch: 46, step: 57
	action: tensor([[-0.7442, -0.0037,  0.3515,  0.5826, -0.2385,  0.1327, -0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-26.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08305811980714406, distance: 1.1909199611980184 entropy 0.5003275275230408
epoch: 46, step: 58
	action: tensor([[ 1.3607, -0.2556,  0.8313, -0.4125,  0.3475,  0.5117,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-28.3538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31195721309048186, distance: 0.9492146150261002 entropy 0.5003275275230408
epoch: 46, step: 59
	action: tensor([[-0.0438, -0.6029,  0.5716, -1.1228,  0.5941, -0.2023, -0.6617]],
       dtype=torch.float64)
	q_value: tensor([[-29.6322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8728554353296083, distance: 1.5660615292676194 entropy 0.5003275275230408
epoch: 46, step: 60
	action: tensor([[ 0.7134,  0.5495,  0.1335, -0.8456, -0.2562,  0.1756, -0.6791]],
       dtype=torch.float64)
	q_value: tensor([[-30.1066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7748770714200309, distance: 0.5429584022020655 entropy 0.5003275275230408
epoch: 46, step: 61
	action: tensor([[-0.1960, -0.1652,  0.2364, -0.8494, -0.4114, -0.0613, -0.2070]],
       dtype=torch.float64)
	q_value: tensor([[-28.6882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4747655408995288, distance: 1.3896908049768137 entropy 0.5003275275230408
epoch: 46, step: 62
	action: tensor([[ 0.2308, -0.2648,  0.2112, -0.4766,  0.2371,  0.1576,  0.0586]],
       dtype=torch.float64)
	q_value: tensor([[-25.9559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01586306039858143, distance: 1.1352315697599107 entropy 0.5003275275230408
epoch: 46, step: 63
	action: tensor([[ 0.3156,  0.4314,  0.8817,  0.5443, -0.0375, -0.0881, -0.0557]],
       dtype=torch.float64)
	q_value: tensor([[-25.0183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 64
	action: tensor([[-0.3706, -0.5411,  1.6408, -0.9383,  0.5983, -0.1155,  0.4885]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8994942686208085, distance: 1.5771597569353029 entropy 0.5003275275230408
epoch: 46, step: 65
	action: tensor([[ 0.1856, -0.0293,  0.4776, -0.9136,  0.0393, -0.8120,  0.1074]],
       dtype=torch.float64)
	q_value: tensor([[-30.0998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21742991890446728, distance: 1.2626374852422744 entropy 0.5003275275230408
epoch: 46, step: 66
	action: tensor([[ 0.0074, -0.0732,  0.5297, -0.1415, -0.2455,  0.0223, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-26.4781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1343388301601064, distance: 1.0647083544852944 entropy 0.5003275275230408
epoch: 46, step: 67
	action: tensor([[ 0.3342,  0.7236, -0.2930, -0.7948,  0.3462,  0.4038, -0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-24.9739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8630328827424876, distance: 0.4235111521375096 entropy 0.5003275275230408
epoch: 46, step: 68
	action: tensor([[ 0.1722, -0.6436, -0.3345,  0.3723,  0.1921, -0.2104, -0.2460]],
       dtype=torch.float64)
	q_value: tensor([[-28.0386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007239977500150707, distance: 1.1401942154826366 entropy 0.5003275275230408
epoch: 46, step: 69
	action: tensor([[ 0.3172,  0.1019, -0.9128, -1.1453, -0.2362,  0.1095,  0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-26.5446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5127142012238766, distance: 0.7988193394573251 entropy 0.5003275275230408
epoch: 46, step: 70
	action: tensor([[ 0.4891, -0.0645,  0.1809, -0.0101,  0.5517,  0.4955, -0.5038]],
       dtype=torch.float64)
	q_value: tensor([[-28.8834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7425295027684484, distance: 0.5806580203756444 entropy 0.5003275275230408
epoch: 46, step: 71
	action: tensor([[ 0.3457,  0.2521,  0.5825,  0.0150, -0.0942, -0.2827,  0.3743]],
       dtype=torch.float64)
	q_value: tensor([[-27.8589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6879297238350246, distance: 0.6392678977294382 entropy 0.5003275275230408
epoch: 46, step: 72
	action: tensor([[-0.0346,  0.0487,  0.3222,  0.1265, -0.3795, -0.0908, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-25.3828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3676582938265165, distance: 0.9099815910489621 entropy 0.5003275275230408
epoch: 46, step: 73
	action: tensor([[ 0.5148,  0.0337,  0.0150, -0.7696,  0.0400, -1.1125, -0.6004]],
       dtype=torch.float64)
	q_value: tensor([[-25.3214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1681624539841411, distance: 1.0437006470580676 entropy 0.5003275275230408
epoch: 46, step: 74
	action: tensor([[ 0.1067,  0.2562,  0.0926, -0.0568,  0.1438,  0.3066, -0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-28.5819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 75
	action: tensor([[-0.6330, -0.0631,  1.3497, -0.7992,  1.4164, -0.6455,  0.8725]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8070768078041115, distance: 1.5383140241966342 entropy 0.5003275275230408
epoch: 46, step: 76
	action: tensor([[ 0.2705,  0.3819, -0.0938, -0.5225, -0.2832,  0.0580,  0.4630]],
       dtype=torch.float64)
	q_value: tensor([[-32.7777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.607271477628934, distance: 0.7171382961848595 entropy 0.5003275275230408
epoch: 46, step: 77
	action: tensor([[-0.0919, -0.0613,  0.5339, -0.6906,  0.7639,  0.1945, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-25.5912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.252384617965967, distance: 1.2806355564684624 entropy 0.5003275275230408
epoch: 46, step: 78
	action: tensor([[ 0.0547,  0.1438,  0.3764, -0.0833,  0.0469, -0.0036, -0.8105]],
       dtype=torch.float64)
	q_value: tensor([[-27.0588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37355228310414634, distance: 0.9057307419862807 entropy 0.5003275275230408
epoch: 46, step: 79
	action: tensor([[ 0.0665, -0.7189, -0.2004,  0.2173, -0.1740, -0.2921, -0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-27.3785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2039157090339152, distance: 1.2556099075979794 entropy 0.5003275275230408
epoch: 46, step: 80
	action: tensor([[ 0.3144, -0.3742,  0.5745, -0.6701, -0.4367, -0.0487,  0.1010]],
       dtype=torch.float64)
	q_value: tensor([[-26.8495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24923227485553645, distance: 1.2790228145939158 entropy 0.5003275275230408
epoch: 46, step: 81
	action: tensor([[ 0.2089, -0.6938,  0.1385,  0.4765,  0.1120, -0.5409,  0.5231]],
       dtype=torch.float64)
	q_value: tensor([[-25.8211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0017668529244841968, distance: 1.143332863061835 entropy 0.5003275275230408
epoch: 46, step: 82
	action: tensor([[-0.3851, -0.6235,  0.5878,  0.3573,  0.0942,  0.0247,  0.5967]],
       dtype=torch.float64)
	q_value: tensor([[-26.7052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31734143808030124, distance: 1.3134267884288924 entropy 0.5003275275230408
epoch: 46, step: 83
	action: tensor([[ 0.5603, -0.2269, -0.2618, -0.6709,  0.8135,  0.7555,  0.4715]],
       dtype=torch.float64)
	q_value: tensor([[-26.6236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42509385424652213, distance: 0.8676711729331724 entropy 0.5003275275230408
epoch: 46, step: 84
	action: tensor([[-0.0047, -1.3214,  0.4436, -0.5459,  0.0646, -0.4981, -0.2032]],
       dtype=torch.float64)
	q_value: tensor([[-29.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.124314632864202, distance: 1.6678850410705293 entropy 0.5003275275230408
epoch: 46, step: 85
	action: tensor([[-0.2875, -0.5652,  0.1222,  0.7574, -0.2145, -0.2901,  0.2169]],
       dtype=torch.float64)
	q_value: tensor([[-28.9290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034609571085294055, distance: 1.1639784481364177 entropy 0.5003275275230408
epoch: 46, step: 86
	action: tensor([[ 0.2372, -0.4292,  0.1253, -0.4999, -0.1937,  0.3074,  0.2440]],
       dtype=torch.float64)
	q_value: tensor([[-27.7338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12169226896936514, distance: 1.2119747071229603 entropy 0.5003275275230408
epoch: 46, step: 87
	action: tensor([[ 1.2838, -0.5511, -0.4296, -0.6265, -0.3975,  0.0108,  0.8139]],
       dtype=torch.float64)
	q_value: tensor([[-25.8565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3543806725165126, distance: 1.3317633748309214 entropy 0.5003275275230408
epoch: 46, step: 88
	action: tensor([[ 0.3579,  0.1632,  0.6090,  0.0735,  0.0128, -0.2310, -0.3701]],
       dtype=torch.float64)
	q_value: tensor([[-31.3683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6997564158610926, distance: 0.6270375672311382 entropy 0.5003275275230408
epoch: 46, step: 89
	action: tensor([[-0.2163, -0.5220, -0.1368, -0.3172,  0.0313,  0.2829,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-26.5842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39470547468979933, distance: 1.351443662745349 entropy 0.5003275275230408
epoch: 46, step: 90
	action: tensor([[ 0.1327,  0.1624,  0.5979, -0.3800,  0.0296,  0.2085, -0.1079]],
       dtype=torch.float64)
	q_value: tensor([[-25.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3659729304257424, distance: 0.9111934588687546 entropy 0.5003275275230408
epoch: 46, step: 91
	action: tensor([[ 0.3751, -0.0279,  0.4308, -0.7611,  0.6699,  0.0314,  0.2523]],
       dtype=torch.float64)
	q_value: tensor([[-25.6375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1326572120270304, distance: 1.0657419945891715 entropy 0.5003275275230408
epoch: 46, step: 92
	action: tensor([[ 0.0385, -0.4235,  0.0022,  0.0243, -0.2075, -0.3901,  0.4869]],
       dtype=torch.float64)
	q_value: tensor([[-26.5560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10881473728989555, distance: 1.2049976169483403 entropy 0.5003275275230408
epoch: 46, step: 93
	action: tensor([[-0.1272, -0.3183,  0.6044, -0.3417, -0.8241, -0.2355,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[-25.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3918957615587928, distance: 1.3500816965791214 entropy 0.5003275275230408
epoch: 46, step: 94
	action: tensor([[-0.4143,  0.0828,  0.0868,  0.0648,  0.3111,  0.0733, -0.0872]],
       dtype=torch.float64)
	q_value: tensor([[-26.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04396583699331402, distance: 1.16922969528943 entropy 0.5003275275230408
epoch: 46, step: 95
	action: tensor([[ 0.5792, -0.2652,  0.2614, -0.1110, -0.2737, -0.3650,  0.3446]],
       dtype=torch.float64)
	q_value: tensor([[-25.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29358379278056257, distance: 0.9618049781747542 entropy 0.5003275275230408
epoch: 46, step: 96
	action: tensor([[ 0.3085, -0.5283, -0.2068, -0.5512, -0.6224,  0.1865, -0.5162]],
       dtype=torch.float64)
	q_value: tensor([[-25.4362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2171995286545989, distance: 1.2625180068537527 entropy 0.5003275275230408
epoch: 46, step: 97
	action: tensor([[ 0.5015,  0.0571,  0.8622, -0.4652,  0.1476,  0.1202, -0.5051]],
       dtype=torch.float64)
	q_value: tensor([[-27.1880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4993676454752587, distance: 0.8096851049103445 entropy 0.5003275275230408
epoch: 46, step: 98
	action: tensor([[ 0.7395,  0.0162,  1.7252, -0.2908, -0.2501,  0.4472,  0.0725]],
       dtype=torch.float64)
	q_value: tensor([[-27.8025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7998502895593159, distance: 0.511957814388554 entropy 0.5003275275230408
epoch: 46, step: 99
	action: tensor([[ 0.0527, -0.3971,  0.2967, -0.5059,  0.2695,  0.0488,  0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-30.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31446244458579975, distance: 1.311990784320958 entropy 0.5003275275230408
epoch: 46, step: 100
	action: tensor([[ 0.5272, -0.0129,  0.4352, -0.1013, -0.1163,  0.1823, -0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-25.0841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6643267717187571, distance: 0.6630023166966212 entropy 0.5003275275230408
epoch: 46, step: 101
	action: tensor([[-0.4207, -0.1574, -0.1219, -0.6177,  0.2441,  0.6302,  0.6692]],
       dtype=torch.float64)
	q_value: tensor([[-25.6944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3064440315286683, distance: 1.3079829951623214 entropy 0.5003275275230408
epoch: 46, step: 102
	action: tensor([[-0.0658, -0.4294, -0.2946, -0.2281,  0.2960, -0.7048, -0.7500]],
       dtype=torch.float64)
	q_value: tensor([[-28.5033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3177746207189296, distance: 1.3136427183695687 entropy 0.5003275275230408
epoch: 46, step: 103
	action: tensor([[ 0.6378, -0.6372, -0.3492,  0.0017, -1.0102,  0.0998,  0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-28.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.054976088058889094, distance: 1.1124438311558353 entropy 0.5003275275230408
epoch: 46, step: 104
	action: tensor([[ 0.2832,  0.1924, -0.0663, -1.2879,  0.0849, -0.0238,  0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-28.9513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14550080166808044, distance: 1.0578218248280133 entropy 0.5003275275230408
epoch: 46, step: 105
	action: tensor([[-0.2454, -0.3402,  0.8894, -0.0237,  0.3281, -0.3179, -1.1776]],
       dtype=torch.float64)
	q_value: tensor([[-27.7069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3504125624996417, distance: 1.3298110212151872 entropy 0.5003275275230408
epoch: 46, step: 106
	action: tensor([[ 0.7492, -0.3532, -0.6559, -0.6084,  0.5919,  0.0380, -0.4392]],
       dtype=torch.float64)
	q_value: tensor([[-32.4547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10865774719381738, distance: 1.080385981368318 entropy 0.5003275275230408
epoch: 46, step: 107
	action: tensor([[ 1.0661, -0.0869, -0.4374,  0.1918,  0.0178, -0.2729, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-29.5541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6716859177839604, distance: 0.6556943611421134 entropy 0.5003275275230408
epoch: 46, step: 108
	action: tensor([[ 0.2106,  0.2494,  0.3003, -0.3174, -0.6305, -0.5743, -0.4645]],
       dtype=torch.float64)
	q_value: tensor([[-26.8766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4748944926736903, distance: 0.8292394976572605 entropy 0.5003275275230408
epoch: 46, step: 109
	action: tensor([[-0.0188, -0.6668,  0.0488, -0.7164,  0.5819,  0.4149,  0.9252]],
       dtype=torch.float64)
	q_value: tensor([[-26.6361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5098293748105189, distance: 1.4061143116650163 entropy 0.5003275275230408
epoch: 46, step: 110
	action: tensor([[ 0.2522, -0.2670,  0.1690,  0.3526,  1.0457, -0.2426, -0.5569]],
       dtype=torch.float64)
	q_value: tensor([[-28.9451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4582097711759028, distance: 0.8423106219796781 entropy 0.5003275275230408
epoch: 46, step: 111
	action: tensor([[ 0.6031, -0.7374, -0.2177,  0.0450,  0.3377, -0.0638, -0.3813]],
       dtype=torch.float64)
	q_value: tensor([[-29.8719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0590738189269453, distance: 1.1776596884629091 entropy 0.5003275275230408
epoch: 46, step: 112
	action: tensor([[ 0.5519, -0.2654,  0.7527,  0.3649, -0.6586, -0.0215,  0.4707]],
       dtype=torch.float64)
	q_value: tensor([[-27.6326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6692358542332528, distance: 0.6581363929272567 entropy 0.5003275275230408
epoch: 46, step: 113
	action: tensor([[ 0.2125, -0.2593, -0.2683, -0.0083,  0.0466, -0.1595, -0.1018]],
       dtype=torch.float64)
	q_value: tensor([[-28.0398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2503244367462911, distance: 0.9908168201617508 entropy 0.5003275275230408
epoch: 46, step: 114
	action: tensor([[ 0.2282, -0.1409, -0.1654, -0.4833, -0.1131, -0.1315, -0.3892]],
       dtype=torch.float64)
	q_value: tensor([[-24.4809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16534426691245452, distance: 1.045467131780237 entropy 0.5003275275230408
epoch: 46, step: 115
	action: tensor([[-0.1235, -0.0883, -0.1532,  0.5755, -0.3487, -0.0647,  0.9890]],
       dtype=torch.float64)
	q_value: tensor([[-24.9195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34604693884656734, distance: 0.9254010369197171 entropy 0.5003275275230408
epoch: 46, step: 116
	action: tensor([[-0.0549,  0.0079, -0.3529, -0.4984,  0.2879,  0.1315,  0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-27.4475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14205730478337608, distance: 1.0599511093020448 entropy 0.5003275275230408
epoch: 46, step: 117
	action: tensor([[-0.1135,  0.3350,  0.3736,  0.3544, -0.3566, -0.1900, -0.4286]],
       dtype=torch.float64)
	q_value: tensor([[-25.2000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 46, step: 118
	action: tensor([[-0.7886,  0.5088,  1.2432, -0.3280,  1.2507, -0.3087,  0.9209]],
       dtype=torch.float64)
	q_value: tensor([[-33.5600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.567889806811352, distance: 1.4328953113773826 entropy 0.5003275275230408
epoch: 46, step: 119
	action: tensor([[ 0.1560, -0.1332,  0.1156, -0.2013, -0.2162,  0.2887, -0.3250]],
       dtype=torch.float64)
	q_value: tensor([[-33.5025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.263052067679418, distance: 0.9823700096458373 entropy 0.5003275275230408
epoch: 46, step: 120
	action: tensor([[ 0.4387, -0.0375, -0.0573, -0.3343, -0.1290,  0.6972, -0.1723]],
       dtype=torch.float64)
	q_value: tensor([[-25.2748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5983622844167631, distance: 0.7252269549269372 entropy 0.5003275275230408
epoch: 46, step: 121
	action: tensor([[-0.4427, -0.0875,  0.6884, -0.9007,  0.5050,  0.4564, -0.3720]],
       dtype=torch.float64)
	q_value: tensor([[-26.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6948812103561979, distance: 1.4897943600061343 entropy 0.5003275275230408
epoch: 46, step: 122
	action: tensor([[-0.5078, -0.5640,  0.2970, -0.4573, -0.1165,  0.1863,  0.2404]],
       dtype=torch.float64)
	q_value: tensor([[-28.3898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9200790708454938, distance: 1.5856825617835453 entropy 0.5003275275230408
epoch: 46, step: 123
	action: tensor([[ 0.0067, -0.1125,  0.1835,  0.3272,  0.2187,  0.4626,  0.3180]],
       dtype=torch.float64)
	q_value: tensor([[-25.6725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5597059213625581, distance: 0.7593257288529789 entropy 0.5003275275230408
epoch: 46, step: 124
	action: tensor([[ 0.4102, -0.2888, -0.1051,  0.1705,  0.9509,  0.0686, -0.1577]],
       dtype=torch.float64)
	q_value: tensor([[-26.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4970483454750295, distance: 0.8115584683789537 entropy 0.5003275275230408
epoch: 46, step: 125
	action: tensor([[ 0.0914, -0.6720,  0.0223,  0.7049, -0.0011,  0.6877, -0.0980]],
       dtype=torch.float64)
	q_value: tensor([[-27.8356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4965494389344831, distance: 0.8119608842624605 entropy 0.5003275275230408
epoch: 46, step: 126
	action: tensor([[-0.5922,  0.5549, -0.1745, -0.2749, -0.0554, -0.2065, -0.0187]],
       dtype=torch.float64)
	q_value: tensor([[-29.2762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07744964057246517, distance: 1.1878324448756232 entropy 0.5003275275230408
epoch: 46, step: 127
	action: tensor([[-0.0423,  0.0956,  0.1029, -0.0937,  0.6533, -0.8577,  0.4449]],
       dtype=torch.float64)
	q_value: tensor([[-25.1797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047495011158354306, distance: 1.1168383614093673 entropy 0.5003275275230408
LOSS epoch 46 actor 358.7777486421825 critic 191.1704804586783 
epoch: 47, step: 0
	action: tensor([[ 0.5588, -0.6276, -0.0629, -0.5145, -0.7902,  0.5162, -0.3955]],
       dtype=torch.float64)
	q_value: tensor([[-25.1677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19115489517731565, distance: 1.248937809087076 entropy 0.5003275275230408
epoch: 47, step: 1
	action: tensor([[ 0.1909, -0.3046, -0.0382, -0.0201,  0.2464, -0.4428, -0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-28.2123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07817979530126651, distance: 1.0987017207832501 entropy 0.5003275275230408
epoch: 47, step: 2
	action: tensor([[ 0.8796, -0.2519, -0.2458, -0.0529,  0.1300,  0.7831, -0.4549]],
       dtype=torch.float64)
	q_value: tensor([[-24.3953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7226812906232353, distance: 0.6026237982819177 entropy 0.5003275275230408
epoch: 47, step: 3
	action: tensor([[ 0.2964,  0.3010, -0.2473, -0.2510,  0.4175,  0.0328, -0.3298]],
       dtype=torch.float64)
	q_value: tensor([[-28.5792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7014484145621843, distance: 0.6252682609875876 entropy 0.5003275275230408
epoch: 47, step: 4
	action: tensor([[ 0.6741, -0.3979,  0.4734, -0.1513, -0.2071, -0.2828, -0.1534]],
       dtype=torch.float64)
	q_value: tensor([[-24.8838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16569278636561213, distance: 1.0452488359830872 entropy 0.5003275275230408
epoch: 47, step: 5
	action: tensor([[-0.0806, -0.6551, -0.0720, -0.3409, -0.4810, -0.3273,  0.3316]],
       dtype=torch.float64)
	q_value: tensor([[-25.4119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4899121921453007, distance: 1.3968090181927266 entropy 0.5003275275230408
epoch: 47, step: 6
	action: tensor([[ 0.2687, -0.0896,  0.3949,  0.2728,  0.0705,  0.1987,  0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-25.2407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6881865466514269, distance: 0.6390047961355463 entropy 0.5003275275230408
epoch: 47, step: 7
	action: tensor([[ 0.2082, -0.5584,  0.5046, -0.1496,  0.2843, -0.4823, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[-25.0818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.289658603745619, distance: 1.2995532305120727 entropy 0.5003275275230408
epoch: 47, step: 8
	action: tensor([[ 0.1818, -0.3349,  0.1854,  0.2953,  0.1429,  0.2146, -0.1863]],
       dtype=torch.float64)
	q_value: tensor([[-25.3846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43630053137985525, distance: 0.8591727732137334 entropy 0.5003275275230408
epoch: 47, step: 9
	action: tensor([[-0.3719,  0.1188,  0.2738, -0.0287,  0.1635,  0.2242, -0.1976]],
       dtype=torch.float64)
	q_value: tensor([[-25.4341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.038269430746714295, distance: 1.1222339517106004 entropy 0.5003275275230408
epoch: 47, step: 10
	action: tensor([[-0.5612, -0.5009, -0.1460, -0.3872,  0.3588,  0.5259,  0.5793]],
       dtype=torch.float64)
	q_value: tensor([[-24.8801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6743519758879364, distance: 1.480744310998985 entropy 0.5003275275230408
epoch: 47, step: 11
	action: tensor([[-0.0355, -0.4213,  0.3313,  0.0563,  0.3620,  0.7432,  0.0945]],
       dtype=torch.float64)
	q_value: tensor([[-26.6966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2793047007707742, distance: 0.9714770334219528 entropy 0.5003275275230408
epoch: 47, step: 12
	action: tensor([[-0.1272,  0.2031,  0.7542, -0.5381,  0.8549,  0.1660, -0.5213]],
       dtype=torch.float64)
	q_value: tensor([[-26.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03448706347892916, distance: 1.1639095330408282 entropy 0.5003275275230408
epoch: 47, step: 13
	action: tensor([[ 1.3913e-01, -7.2720e-01,  4.2736e-04, -7.4807e-01,  1.9340e-01,
          4.4537e-01, -3.2995e-01]], dtype=torch.float64)
	q_value: tensor([[-28.6234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5416639793483171, distance: 1.4208608759737578 entropy 0.5003275275230408
epoch: 47, step: 14
	action: tensor([[ 0.3313, -0.1108,  0.7725, -0.4352, -0.2823,  0.8206,  0.3587]],
       dtype=torch.float64)
	q_value: tensor([[-27.1244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5011856130894334, distance: 0.8082136458306062 entropy 0.5003275275230408
epoch: 47, step: 15
	action: tensor([[0.1682, 0.4101, 0.4737, 0.2330, 0.3506, 0.2768, 0.1888]],
       dtype=torch.float64)
	q_value: tensor([[-28.1011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 16
	action: tensor([[-0.6681, -1.1101,  2.2137, -2.1924,  1.1594, -0.8512,  0.7407]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 17
	action: tensor([[-1.0053, -0.3517,  1.8215, -1.1551,  1.6184,  0.3129,  0.8565]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3367161288855405, distance: 1.7492813719935192 entropy 0.5003275275230408
epoch: 47, step: 18
	action: tensor([[-0.5857,  0.1760,  0.9594, -0.0504,  0.4476, -0.5639, -0.4687]],
       dtype=torch.float64)
	q_value: tensor([[-36.7838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49669356015523847, distance: 1.399984212738821 entropy 0.5003275275230408
epoch: 47, step: 19
	action: tensor([[ 0.0675, -0.0750,  0.1329, -0.4851,  0.6141, -0.0838, -0.0404]],
       dtype=torch.float64)
	q_value: tensor([[-29.1650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006425473740379717, distance: 1.1480148440982978 entropy 0.5003275275230408
epoch: 47, step: 20
	action: tensor([[ 0.4458,  0.1311, -0.2309, -0.2468,  0.6160, -0.3173,  0.4462]],
       dtype=torch.float64)
	q_value: tensor([[-24.9801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5700846671568007, distance: 0.7503228292944987 entropy 0.5003275275230408
epoch: 47, step: 21
	action: tensor([[-0.2347, -0.1630, -0.4646, -0.4227,  0.4720, -0.1952, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-25.2893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1974055008061364, distance: 1.2522104326451904 entropy 0.5003275275230408
epoch: 47, step: 22
	action: tensor([[ 0.2546, -0.9738,  0.6339, -0.0775, -0.4451, -0.2429,  0.1173]],
       dtype=torch.float64)
	q_value: tensor([[-24.6752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6105243732637031, distance: 1.4522465188910518 entropy 0.5003275275230408
epoch: 47, step: 23
	action: tensor([[ 0.7625,  0.0045, -0.1630, -0.5396,  0.8313, -0.0773,  0.3908]],
       dtype=torch.float64)
	q_value: tensor([[-26.4478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46137922938965426, distance: 0.8398432609871872 entropy 0.5003275275230408
epoch: 47, step: 24
	action: tensor([[-0.5390, -0.6897,  0.5428,  0.3682,  0.0432,  0.1648,  0.1854]],
       dtype=torch.float64)
	q_value: tensor([[-26.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4506177026890579, distance: 1.3782664347890887 entropy 0.5003275275230408
epoch: 47, step: 25
	action: tensor([[ 0.1777,  0.1899,  0.3051, -0.3365,  0.5208,  0.4569,  0.2021]],
       dtype=torch.float64)
	q_value: tensor([[-26.4579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5628392097577455, distance: 0.7566190896753362 entropy 0.5003275275230408
epoch: 47, step: 26
	action: tensor([[ 0.1582, -0.8482,  0.6387, -0.9742,  0.0109,  0.1483,  0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-25.7192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8912819015338334, distance: 1.573746678780994 entropy 0.5003275275230408
epoch: 47, step: 27
	action: tensor([[0.6533, 0.2755, 0.1052, 0.1645, 0.1443, 0.3270, 0.2717]],
       dtype=torch.float64)
	q_value: tensor([[-27.0977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9648114712998647, distance: 0.21466302621240216 entropy 0.5003275275230408
epoch: 47, step: 28
	action: tensor([[ 0.1114, -0.1435,  0.2944, -0.6056,  0.2992,  0.0472, -0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-26.0140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08965096038829268, distance: 1.1945391728376626 entropy 0.5003275275230408
epoch: 47, step: 29
	action: tensor([[-0.3581, -0.3405,  0.4940, -0.1772,  0.2561,  0.5511, -0.4251]],
       dtype=torch.float64)
	q_value: tensor([[-24.6748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2570877777425784, distance: 1.283037929299321 entropy 0.5003275275230408
epoch: 47, step: 30
	action: tensor([[ 0.1524,  0.4336, -0.2883,  0.3162,  1.0933, -0.4972,  0.3070]],
       dtype=torch.float64)
	q_value: tensor([[-26.6703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 31
	action: tensor([[-1.0055, -0.2878,  2.0832, -1.7193,  1.4782, -0.2515,  1.3657]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 32
	action: tensor([[ 0.0556, -0.6010,  2.4184, -1.3631,  0.9988,  0.3882,  0.2923]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 33
	action: tensor([[-0.8849, -0.8078,  1.9959, -1.3590,  1.2846,  0.1518,  0.7575]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2672203070786647, distance: 1.7230725137718905 entropy 0.5003275275230408
epoch: 47, step: 34
	action: tensor([[ 0.0577,  0.2644,  0.5373, -0.1563,  0.4353,  0.0564, -0.5079]],
       dtype=torch.float64)
	q_value: tensor([[-35.4664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4612319467307566, distance: 0.839958078220915 entropy 0.5003275275230408
epoch: 47, step: 35
	action: tensor([[ 0.2832,  0.3314, -0.4207, -0.0299,  0.8212, -0.0914,  0.0821]],
       dtype=torch.float64)
	q_value: tensor([[-26.7549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 36
	action: tensor([[-0.8301, -0.6565,  2.6903, -0.9825,  1.5385, -0.3482,  1.0049]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6646506134907186, distance: 1.4764482889282602 entropy 0.5003275275230408
epoch: 47, step: 37
	action: tensor([[-0.0639, -0.7268,  0.4462,  0.0892, -0.2889, -0.4101,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-37.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4757533271218968, distance: 1.3901561289929332 entropy 0.5003275275230408
epoch: 47, step: 38
	action: tensor([[ 0.0273, -0.6205,  0.0259, -0.4872, -0.1535,  0.6227,  0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-25.4613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3468522889068557, distance: 1.3280568847914622 entropy 0.5003275275230408
epoch: 47, step: 39
	action: tensor([[-0.0186, -0.6815, -0.0231, -0.4100,  0.1047,  0.0307,  0.1269]],
       dtype=torch.float64)
	q_value: tensor([[-26.3062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.559888769109683, distance: 1.429234558880725 entropy 0.5003275275230408
epoch: 47, step: 40
	action: tensor([[-0.2630, -0.2576, -0.0365,  0.1930,  0.0718, -0.5219, -0.4214]],
       dtype=torch.float64)
	q_value: tensor([[-24.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22646587260069806, distance: 1.26731456840983 entropy 0.5003275275230408
epoch: 47, step: 41
	action: tensor([[ 0.0379,  0.3875,  0.0680,  0.1242, -0.3029, -0.0847,  0.2224]],
       dtype=torch.float64)
	q_value: tensor([[-25.2486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 42
	action: tensor([[-1.3231, -0.5657,  2.4088, -1.4272,  1.8538,  0.2276,  0.3549]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0518131474365293, distance: 1.6391760404478948 entropy 0.5003275275230408
epoch: 47, step: 43
	action: tensor([[0.4489, 0.2961, 0.8887, 0.2129, 0.6065, 0.8169, 0.3559]],
       dtype=torch.float64)
	q_value: tensor([[-40.9362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9533938453351416, distance: 0.2470462860263758 entropy 0.5003275275230408
epoch: 47, step: 44
	action: tensor([[ 0.2105,  0.2432, -0.2110, -0.0220, -0.3152, -0.6343, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-29.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 45
	action: tensor([[-1.6552, -0.3665,  1.9434, -2.0429,  1.1992, -0.5309,  1.0277]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27293068212938176, distance: 1.2910975903964 entropy 0.5003275275230408
epoch: 47, step: 46
	action: tensor([[-0.2274, -0.2804,  0.3822,  0.1588,  0.3632,  0.0702, -0.3142]],
       dtype=torch.float64)
	q_value: tensor([[-37.9228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04772086301906686, distance: 1.17133060072319 entropy 0.5003275275230408
epoch: 47, step: 47
	action: tensor([[-0.2090, -0.5505, -0.0725, -0.2261,  0.0594,  0.1535,  0.5386]],
       dtype=torch.float64)
	q_value: tensor([[-25.8403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4886406688786116, distance: 1.39621285746858 entropy 0.5003275275230408
epoch: 47, step: 48
	action: tensor([[-0.2215, -0.4518, -0.0637, -0.6047, -0.3848, -0.1672,  0.5380]],
       dtype=torch.float64)
	q_value: tensor([[-25.0130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.588499170801629, distance: 1.4422820200749247 entropy 0.5003275275230408
epoch: 47, step: 49
	action: tensor([[-0.1450, -0.3867, -0.1363,  0.5662, -0.2606,  0.3636,  0.3535]],
       dtype=torch.float64)
	q_value: tensor([[-25.3458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15081158900927094, distance: 1.05452947365777 entropy 0.5003275275230408
epoch: 47, step: 50
	action: tensor([[ 0.5322,  0.3495,  0.3776, -0.1957,  0.2897, -0.2712, -0.3087]],
       dtype=torch.float64)
	q_value: tensor([[-26.2258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7460216075808882, distance: 0.5767068079722435 entropy 0.5003275275230408
epoch: 47, step: 51
	action: tensor([[ 0.1755, -0.2512,  0.7007, -0.6543,  0.0428,  0.2459, -0.3246]],
       dtype=torch.float64)
	q_value: tensor([[-25.8427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16300684830723733, distance: 1.234092830570285 entropy 0.5003275275230408
epoch: 47, step: 52
	action: tensor([[ 0.1691, -0.2919,  0.3407,  0.2179,  0.2421, -0.4966, -0.3435]],
       dtype=torch.float64)
	q_value: tensor([[-25.8248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16964336246993295, distance: 1.0427711905320494 entropy 0.5003275275230408
epoch: 47, step: 53
	action: tensor([[-0.1535, -0.0868, -0.4741, -0.0593,  0.0556, -0.7280, -0.4971]],
       dtype=torch.float64)
	q_value: tensor([[-25.9101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0027170128411284633, distance: 1.145897798482128 entropy 0.5003275275230408
epoch: 47, step: 54
	action: tensor([[ 0.2365, -0.0534,  0.3579, -1.0177,  0.3984,  0.5769, -0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-24.9400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033718007315784115, distance: 1.1248863229454327 entropy 0.5003275275230408
epoch: 47, step: 55
	action: tensor([[-0.6250,  0.3334,  0.3054, -0.0694,  0.3514, -0.2272,  0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-26.8515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37010864045125214, distance: 1.3394737154687049 entropy 0.5003275275230408
epoch: 47, step: 56
	action: tensor([[-0.0545, -0.5183,  0.6585, -0.0847,  0.5673,  0.4027, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-25.2129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11617864716900095, distance: 1.2089923373993112 entropy 0.5003275275230408
epoch: 47, step: 57
	action: tensor([[-0.2740,  0.2983, -0.0206,  0.0126, -0.1233,  0.1062, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[-26.6050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 58
	action: tensor([[-1.2950, -1.0982,  2.1520, -2.2110,  1.4099,  1.0813,  0.6129]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.61654234881521, distance: 1.4549572618029583 entropy 0.5003275275230408
epoch: 47, step: 59
	action: tensor([[ 0.3725, -0.3235,  0.3280, -0.4029,  0.0362,  0.5993, -0.0361]],
       dtype=torch.float64)
	q_value: tensor([[-45.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2684396763318839, distance: 0.9787725140443728 entropy 0.5003275275230408
epoch: 47, step: 60
	action: tensor([[-0.1069,  0.1839,  0.0923, -0.7676,  0.0587, -0.1807,  0.1125]],
       dtype=torch.float64)
	q_value: tensor([[-25.7544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04945083921659954, distance: 1.172297240952835 entropy 0.5003275275230408
epoch: 47, step: 61
	action: tensor([[-0.4599,  0.3644,  1.2119, -0.4800,  0.0413,  0.0447,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-24.3048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3674306942790819, distance: 1.338164040863394 entropy 0.5003275275230408
epoch: 47, step: 62
	action: tensor([[-0.3021,  0.1118,  0.4822, -0.3305,  0.2208,  0.3536, -0.1907]],
       dtype=torch.float64)
	q_value: tensor([[-28.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07047528077974219, distance: 1.1839817680853453 entropy 0.5003275275230408
epoch: 47, step: 63
	action: tensor([[ 0.5887, -0.2206,  0.0826, -0.3586,  0.0595,  0.2497,  0.1471]],
       dtype=torch.float64)
	q_value: tensor([[-25.4959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3698533140110667, distance: 0.9084008283445433 entropy 0.5003275275230408
epoch: 47, step: 64
	action: tensor([[-0.2010, -0.6717,  0.0499, -0.2515, -0.1015, -0.2446,  0.2251]],
       dtype=torch.float64)
	q_value: tensor([[-24.9407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6515019043123857, distance: 1.4706056584426292 entropy 0.5003275275230408
epoch: 47, step: 65
	action: tensor([[-0.5807,  0.2987,  0.1122, -0.4107,  0.1342,  0.2460,  0.5811]],
       dtype=torch.float64)
	q_value: tensor([[-24.7030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3085691358718059, distance: 1.3090463667450438 entropy 0.5003275275230408
epoch: 47, step: 66
	action: tensor([[-0.2776, -0.5894,  0.3156, -0.4164, -1.0207,  0.1247,  0.1435]],
       dtype=torch.float64)
	q_value: tensor([[-25.7274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7600559899807822, distance: 1.5181683563878434 entropy 0.5003275275230408
epoch: 47, step: 67
	action: tensor([[-0.0919,  0.5504,  0.2838, -0.3450,  0.5939,  0.1742,  0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-26.9842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 68
	action: tensor([[-0.3536, -0.3486,  2.5586, -1.1249,  1.5391, -0.4270,  1.3269]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 69
	action: tensor([[-1.2307, -1.1310,  2.4688, -2.2156,  2.0943, -0.2250,  0.8910]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4252413375030226, distance: 1.3661579014948144 entropy 0.5003275275230408
epoch: 47, step: 70
	action: tensor([[-0.1202,  0.0110,  0.5210, -0.8540, -0.4462, -0.1459,  0.2502]],
       dtype=torch.float64)
	q_value: tensor([[-43.3362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39996181798790786, distance: 1.3539879172560805 entropy 0.5003275275230408
epoch: 47, step: 71
	action: tensor([[ 0.2906, -0.4590,  0.4512,  0.1720, -0.4812, -0.1531,  0.2479]],
       dtype=torch.float64)
	q_value: tensor([[-25.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20294197416803295, distance: 1.0216488791270688 entropy 0.5003275275230408
epoch: 47, step: 72
	action: tensor([[ 0.5931, -0.2268,  1.1253, -0.6431,  0.4820,  0.5738,  0.2227]],
       dtype=torch.float64)
	q_value: tensor([[-25.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.324851358648492, distance: 0.9402782541650385 entropy 0.5003275275230408
epoch: 47, step: 73
	action: tensor([[ 0.0041,  0.6185, -0.1892, -0.4338,  0.1672, -0.5291, -0.2871]],
       dtype=torch.float64)
	q_value: tensor([[-27.5065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 74
	action: tensor([[-1.1014, -0.5467,  2.5862, -1.2742,  1.3322,  0.1224,  1.0757]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.957929948548772, distance: 1.6012357137146211 entropy 0.5003275275230408
epoch: 47, step: 75
	action: tensor([[-0.3684, -0.7702,  0.3064,  0.0533,  0.2481,  0.9601, -0.6016]],
       dtype=torch.float64)
	q_value: tensor([[-37.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16135888651908137, distance: 1.233218175883163 entropy 0.5003275275230408
epoch: 47, step: 76
	action: tensor([[ 0.1943, -0.0740, -0.1697, -0.3103,  0.5802, -0.6879,  0.2507]],
       dtype=torch.float64)
	q_value: tensor([[-30.3359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10299858378303206, distance: 1.0838102591096523 entropy 0.5003275275230408
epoch: 47, step: 77
	action: tensor([[ 0.6278,  0.2009, -0.4106, -0.5172, -0.1832, -0.4099,  0.3796]],
       dtype=torch.float64)
	q_value: tensor([[-24.8279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5974947248814828, distance: 0.7260097974458611 entropy 0.5003275275230408
epoch: 47, step: 78
	action: tensor([[-0.1374, -0.4655,  0.3076,  0.2468,  0.5445, -0.2186,  0.0614]],
       dtype=torch.float64)
	q_value: tensor([[-25.4346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12203033373556793, distance: 1.2121573307757907 entropy 0.5003275275230408
epoch: 47, step: 79
	action: tensor([[-0.2631,  0.1691, -0.0456,  0.4549, -0.0517, -0.1448,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[-25.3668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 80
	action: tensor([[-1.4772, -0.9534,  3.1229, -1.2378,  1.7678,  0.5145,  0.2513]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2134923121060237, distance: 1.7025336416927943 entropy 0.5003275275230408
epoch: 47, step: 81
	action: tensor([[ 0.6570, -0.6990,  0.1045, -0.6651,  0.1947,  0.3342,  0.3547]],
       dtype=torch.float64)
	q_value: tensor([[-43.8819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35082305095292954, distance: 1.3300131189326492 entropy 0.5003275275230408
epoch: 47, step: 82
	action: tensor([[-0.2010,  0.0847,  0.2500, -0.2290,  0.0885, -0.6947,  0.2292]],
       dtype=torch.float64)
	q_value: tensor([[-27.2977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12618417209432908, distance: 1.214399005528221 entropy 0.5003275275230408
epoch: 47, step: 83
	action: tensor([[-0.0770, -0.5412,  0.2757,  0.1175,  0.4156,  0.2121, -0.1324]],
       dtype=torch.float64)
	q_value: tensor([[-24.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06852208446334718, distance: 1.1829011244188827 entropy 0.5003275275230408
epoch: 47, step: 84
	action: tensor([[-0.4096, -0.1478,  0.3088, -0.2218,  0.3753,  0.0485, -0.0546]],
       dtype=torch.float64)
	q_value: tensor([[-25.8295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41724021943052314, distance: 1.3623177887339268 entropy 0.5003275275230408
epoch: 47, step: 85
	action: tensor([[-0.0198,  0.4171,  0.5827, -0.1843,  0.4558, -0.5601,  0.2256]],
       dtype=torch.float64)
	q_value: tensor([[-24.5319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 86
	action: tensor([[-0.9546, -0.9614,  2.2884, -1.0557,  1.2454,  0.4749,  0.5819]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.425059542392642, distance: 1.782041839705487 entropy 0.5003275275230408
epoch: 47, step: 87
	action: tensor([[ 0.1631,  0.2936,  0.0361, -0.6590,  0.1313, -0.1458,  0.2962]],
       dtype=torch.float64)
	q_value: tensor([[-36.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33412948168555634, distance: 0.9337950913247535 entropy 0.5003275275230408
epoch: 47, step: 88
	action: tensor([[-0.2151, -0.8435,  0.1862, -0.2000, -0.4411,  0.6365, -0.4616]],
       dtype=torch.float64)
	q_value: tensor([[-24.2942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5626188079170282, distance: 1.4304846993291331 entropy 0.5003275275230408
epoch: 47, step: 89
	action: tensor([[ 0.0099, -0.4503,  0.5683,  0.4063,  0.2796, -0.0799, -0.6265]],
       dtype=torch.float64)
	q_value: tensor([[-28.2003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2127693211829177, distance: 1.0153311221927677 entropy 0.5003275275230408
epoch: 47, step: 90
	action: tensor([[ 0.3067, -0.4617, -0.4547, -0.8143,  0.0942, -0.1143,  0.7125]],
       dtype=torch.float64)
	q_value: tensor([[-28.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1954045437869567, distance: 1.251163725034723 entropy 0.5003275275230408
epoch: 47, step: 91
	action: tensor([[ 0.0232, -0.8049, -0.1991,  0.0310,  0.0543,  0.2557,  0.1234]],
       dtype=torch.float64)
	q_value: tensor([[-26.6882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3036833019958063, distance: 1.306600273489709 entropy 0.5003275275230408
epoch: 47, step: 92
	action: tensor([[ 0.5772, -0.5975,  0.4333,  0.1627,  0.6547,  0.2321,  0.3639]],
       dtype=torch.float64)
	q_value: tensor([[-25.9737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3243510306447518, distance: 0.9406265925808069 entropy 0.5003275275230408
epoch: 47, step: 93
	action: tensor([[-0.5751, -0.2325,  0.6106, -0.0932,  0.1901,  0.1846,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-26.8700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5503021655252076, distance: 1.424835969161223 entropy 0.5003275275230408
epoch: 47, step: 94
	action: tensor([[ 0.0357,  0.4127, -0.8513, -0.0795,  0.8215, -0.2018,  0.6443]],
       dtype=torch.float64)
	q_value: tensor([[-25.1461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 95
	action: tensor([[-1.2864, -1.0629,  2.3418, -1.6356,  1.3886, -0.2490,  0.6905]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.994946375919272, distance: 1.616301241247705 entropy 0.5003275275230408
epoch: 47, step: 96
	action: tensor([[-0.1611,  0.1450,  0.3424,  0.6201,  0.1019,  0.2342, -0.5979]],
       dtype=torch.float64)
	q_value: tensor([[-38.0401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 97
	action: tensor([[-0.9819, -0.5749,  2.4540, -1.8318,  1.0861, -0.4734,  0.9849]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2254339009100248, distance: 1.2667812849277391 entropy 0.5003275275230408
epoch: 47, step: 98
	action: tensor([[ 0.0078,  0.2649,  0.9621,  0.0805, -0.3786, -0.5170,  0.4043]],
       dtype=torch.float64)
	q_value: tensor([[-36.1800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38452443310916984, distance: 0.8977638251350908 entropy 0.5003275275230408
epoch: 47, step: 99
	action: tensor([[ 0.2247,  0.2819, -0.2159,  0.2572,  0.5378, -0.2531, -0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-26.6349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 100
	action: tensor([[-0.7363, -1.1649,  1.9167, -2.3461,  0.8802, -0.2198,  0.9271]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 101
	action: tensor([[-0.8170, -0.5198,  2.4477, -2.0948,  2.2170, -0.1982,  0.5053]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 102
	action: tensor([[-1.3546, -0.6013,  2.7807, -0.9982,  1.9388,  0.0885,  1.1926]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2944622273531543, distance: 1.733393443480553 entropy 0.5003275275230408
epoch: 47, step: 103
	action: tensor([[ 0.2229,  0.3391,  0.2092, -0.5052, -0.4701, -0.0665,  0.5747]],
       dtype=torch.float64)
	q_value: tensor([[-41.4537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46403743147391063, distance: 0.8377682998337759 entropy 0.5003275275230408
epoch: 47, step: 104
	action: tensor([[-0.3021,  0.3616,  0.3774, -0.6593,  0.2741, -0.3286, -0.1517]],
       dtype=torch.float64)
	q_value: tensor([[-25.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24550193674710097, distance: 1.2771117389729085 entropy 0.5003275275230408
epoch: 47, step: 105
	action: tensor([[-0.2825,  0.0367,  0.8028, -0.5378, -0.3497, -0.4680,  0.2460]],
       dtype=torch.float64)
	q_value: tensor([[-24.9681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4875907772933048, distance: 1.3957204180536835 entropy 0.5003275275230408
epoch: 47, step: 106
	action: tensor([[-0.3347,  0.1498,  0.1369, -0.3897,  0.3453, -0.2357,  0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-25.3307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2588821645630288, distance: 1.2839533169931172 entropy 0.5003275275230408
epoch: 47, step: 107
	action: tensor([[ 0.5138, -0.6356,  0.3661, -0.8027, -0.4000, -0.1110, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-23.9280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5211356799459741, distance: 1.4113693112767243 entropy 0.5003275275230408
epoch: 47, step: 108
	action: tensor([[ 0.4631, -0.1281, -0.3965, -0.0739,  0.4032,  0.1526, -0.2552]],
       dtype=torch.float64)
	q_value: tensor([[-26.1286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5447801898948941, distance: 0.7720888308118888 entropy 0.5003275275230408
epoch: 47, step: 109
	action: tensor([[-0.2257, -0.2117,  0.3805,  0.0164,  0.4584,  0.8013, -0.3000]],
       dtype=torch.float64)
	q_value: tensor([[-25.3511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25995601033907423, distance: 0.9844314077360462 entropy 0.5003275275230408
epoch: 47, step: 110
	action: tensor([[-0.4597,  0.6408,  0.4298, -0.2033, -0.3270, -0.5569, -0.1311]],
       dtype=torch.float64)
	q_value: tensor([[-27.5508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 111
	action: tensor([[-1.7193, -1.4335,  1.7387, -2.2581,  1.9201, -0.0648,  0.7835]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 112
	action: tensor([[-1.1558,  0.1857,  3.0376, -0.8472,  1.7244, -0.7000,  0.7290]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17562268543060044, distance: 1.240768251156294 entropy 0.5003275275230408
epoch: 47, step: 113
	action: tensor([[-0.2296,  0.4036,  0.0099, -0.3818,  0.3136,  0.2347, -0.0360]],
       dtype=torch.float64)
	q_value: tensor([[-40.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23693730541315627, distance: 0.9996242983394088 entropy 0.5003275275230408
epoch: 47, step: 114
	action: tensor([[-0.1773, -0.2465,  0.6280, -0.2943, -0.3483,  0.0340,  0.4492]],
       dtype=torch.float64)
	q_value: tensor([[-25.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3189132716172167, distance: 1.314210136144263 entropy 0.5003275275230408
epoch: 47, step: 115
	action: tensor([[ 0.1207,  0.3311, -0.1479, -0.4009,  0.1921, -0.2947,  0.1771]],
       dtype=torch.float64)
	q_value: tensor([[-24.7460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42664749861652584, distance: 0.8664979688977911 entropy 0.5003275275230408
epoch: 47, step: 116
	action: tensor([[-0.0382,  0.1095,  0.3795,  0.1119,  0.4187, -0.4228, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[-23.5329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2654065717634171, distance: 0.9807994477889395 entropy 0.5003275275230408
epoch: 47, step: 117
	action: tensor([[-0.4800,  0.6984, -0.0047, -0.1492,  0.2885,  0.1691, -0.2283]],
       dtype=torch.float64)
	q_value: tensor([[-25.1456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 47, step: 118
	action: tensor([[-0.7166, -0.5134,  2.9840, -1.6964,  1.3493,  0.4314,  1.1745]],
       dtype=torch.float64)
	q_value: tensor([[-31.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10925259436437096, distance: 1.2052355127106051 entropy 0.5003275275230408
epoch: 47, step: 119
	action: tensor([[ 1.0013,  0.4674,  0.1717, -0.4910,  0.3349, -0.1181, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-40.4974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8412687030818412, distance: 0.45591929836452166 entropy 0.5003275275230408
epoch: 47, step: 120
	action: tensor([[ 0.3059, -0.5496,  0.3011, -0.5429,  0.0450,  0.1114,  0.4449]],
       dtype=torch.float64)
	q_value: tensor([[-26.6829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3062726882929967, distance: 1.3078972198126164 entropy 0.5003275275230408
epoch: 47, step: 121
	action: tensor([[ 0.1519, -0.7828,  0.5969,  0.1195,  0.3375, -0.4695,  0.4672]],
       dtype=torch.float64)
	q_value: tensor([[-25.2635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3440030408902748, distance: 1.3266513977804897 entropy 0.5003275275230408
epoch: 47, step: 122
	action: tensor([[ 0.0971,  0.2017,  0.1691, -0.0292, -0.7237,  0.0923,  0.6210]],
       dtype=torch.float64)
	q_value: tensor([[-25.9348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.528766722214356, distance: 0.7855515127977847 entropy 0.5003275275230408
epoch: 47, step: 123
	action: tensor([[ 0.1984, -0.2430, -0.4883, -0.4050,  0.1543, -0.3781,  0.5921]],
       dtype=torch.float64)
	q_value: tensor([[-26.0562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07839402893510106, distance: 1.0985740426643436 entropy 0.5003275275230408
epoch: 47, step: 124
	action: tensor([[ 0.3494, -0.3112,  0.0612,  0.0894,  0.1738,  0.0922, -0.2567]],
       dtype=torch.float64)
	q_value: tensor([[-25.2029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4020310380430303, distance: 0.8849037068513106 entropy 0.5003275275230408
epoch: 47, step: 125
	action: tensor([[ 0.5537, -0.7218,  0.6622, -0.4897,  0.4263,  0.4337, -0.5149]],
       dtype=torch.float64)
	q_value: tensor([[-24.9901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2578486659166881, distance: 1.2834261681811943 entropy 0.5003275275230408
epoch: 47, step: 126
	action: tensor([[ 0.3149, -0.5289,  0.1962,  0.1421,  0.1819, -0.2650,  0.5406]],
       dtype=torch.float64)
	q_value: tensor([[-28.4077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08114549606804677, distance: 1.096932912988911 entropy 0.5003275275230408
epoch: 47, step: 127
	action: tensor([[ 0.0249,  0.0863,  0.3124, -0.2687, -0.4570, -0.4049,  0.2008]],
       dtype=torch.float64)
	q_value: tensor([[-24.9380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15755204025238112, distance: 1.0503359603852218 entropy 0.5003275275230408
LOSS epoch 47 actor 359.43109155402664 critic 278.525224264126 
epoch: 48, step: 0
	action: tensor([[-0.2203, -1.0959,  0.5076, -1.1433, -0.1777,  0.1363,  0.1470]],
       dtype=torch.float64)
	q_value: tensor([[-24.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1466759920529785, distance: 1.6766404628456377 entropy 0.5003275275230408
epoch: 48, step: 1
	action: tensor([[ 0.3821, -0.0902,  0.8636, -1.0461,  0.6799,  0.3935, -0.0684]],
       dtype=torch.float64)
	q_value: tensor([[-28.7436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03328725390882825, distance: 1.1632343796285436 entropy 0.5003275275230408
epoch: 48, step: 2
	action: tensor([[-0.0853,  0.2430, -0.1303,  0.1224, -0.4766, -0.2148,  0.6603]],
       dtype=torch.float64)
	q_value: tensor([[-27.6735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4315116049456177, distance: 0.862814619277024 entropy 0.5003275275230408
epoch: 48, step: 3
	action: tensor([[ 0.2633, -0.3016,  0.1057,  0.0287, -0.2319,  0.0104, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-25.1682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892637795194639, distance: 0.9647414037456082 entropy 0.5003275275230408
epoch: 48, step: 4
	action: tensor([[ 0.7630, -0.1764,  0.0135,  0.0673,  0.4123, -0.4250, -0.2856]],
       dtype=torch.float64)
	q_value: tensor([[-24.5796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5266177307460628, distance: 0.7873406725628486 entropy 0.5003275275230408
epoch: 48, step: 5
	action: tensor([[ 0.3323,  0.0267,  0.0952, -0.4345,  0.5542,  0.8402,  0.6875]],
       dtype=torch.float64)
	q_value: tensor([[-26.2718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6463035587136672, distance: 0.6805688046163991 entropy 0.5003275275230408
epoch: 48, step: 6
	action: tensor([[-0.1440,  0.0236,  0.1584, -0.7381, -0.2626, -0.3873,  0.6638]],
       dtype=torch.float64)
	q_value: tensor([[-28.4466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27042307230436347, distance: 1.2898252646197752 entropy 0.5003275275230408
epoch: 48, step: 7
	action: tensor([[ 0.4263, -0.5849,  0.0647, -0.0163,  0.6790, -0.0579,  0.2869]],
       dtype=torch.float64)
	q_value: tensor([[-25.1427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04550453151290701, distance: 1.118004698526927 entropy 0.5003275275230408
epoch: 48, step: 8
	action: tensor([[ 0.1190, -0.8710, -0.0760,  0.6454,  0.0895,  0.3351,  0.0861]],
       dtype=torch.float64)
	q_value: tensor([[-25.9868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17221448802476735, distance: 1.0411555146901168 entropy 0.5003275275230408
epoch: 48, step: 9
	action: tensor([[ 0.0983, -0.4353,  0.8151, -0.2458, -0.0294,  0.4329, -0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-28.0982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007414248557144676, distance: 1.1485786461524583 entropy 0.5003275275230408
epoch: 48, step: 10
	action: tensor([[ 0.8321, -0.4894, -0.2467, -0.7565,  0.2059, -0.2993,  0.4364]],
       dtype=torch.float64)
	q_value: tensor([[-26.1684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28216703695077716, distance: 1.295773210916756 entropy 0.5003275275230408
epoch: 48, step: 11
	action: tensor([[ 0.1828, -0.1205, -0.0209, -0.6008, -0.0902, -0.1032,  0.2426]],
       dtype=torch.float64)
	q_value: tensor([[-27.0147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04616172761116999, distance: 1.1176197439267819 entropy 0.5003275275230408
epoch: 48, step: 12
	action: tensor([[-0.3616, -0.0658,  0.8917, -0.5478, -0.2418, -0.3429,  0.3075]],
       dtype=torch.float64)
	q_value: tensor([[-24.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.659339396297306, distance: 1.4740910367775624 entropy 0.5003275275230408
epoch: 48, step: 13
	action: tensor([[-0.3533, -0.0891, -0.0086, -0.1707, -0.2416, -0.0502, -0.4098]],
       dtype=torch.float64)
	q_value: tensor([[-25.2413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29618674934407796, distance: 1.3028381943973022 entropy 0.5003275275230408
epoch: 48, step: 14
	action: tensor([[ 0.2586,  0.3499,  0.8137, -0.2844, -0.3820, -0.0452, -0.0366]],
       dtype=torch.float64)
	q_value: tensor([[-24.3220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5517936226054057, distance: 0.7661180756313591 entropy 0.5003275275230408
epoch: 48, step: 15
	action: tensor([[-0.3735, -0.0484,  0.4060, -0.4322,  0.4173,  0.3211, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-26.5159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3391919401254473, distance: 1.3242747752689095 entropy 0.5003275275230408
epoch: 48, step: 16
	action: tensor([[-0.4376, -0.4058,  0.1111,  0.4648,  0.8940, -0.1449,  0.1655]],
       dtype=torch.float64)
	q_value: tensor([[-24.9661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2885879110854681, distance: 1.2990136648410409 entropy 0.5003275275230408
epoch: 48, step: 17
	action: tensor([[-0.4950, -0.2904,  0.2756, -0.8664,  0.4731,  0.2722,  0.2848]],
       dtype=torch.float64)
	q_value: tensor([[-26.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8176121271136145, distance: 1.5427917189685374 entropy 0.5003275275230408
epoch: 48, step: 18
	action: tensor([[-0.0034, -0.1381,  0.8881, -0.5859, -0.4682, -0.0546,  0.6702]],
       dtype=torch.float64)
	q_value: tensor([[-25.9371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26370289997468754, distance: 1.2864093391436846 entropy 0.5003275275230408
epoch: 48, step: 19
	action: tensor([[ 0.1025, -0.3992,  0.0869, -0.2329, -0.6197, -0.4640,  0.3491]],
       dtype=torch.float64)
	q_value: tensor([[-26.1314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15662671304493458, distance: 1.2307031224761595 entropy 0.5003275275230408
epoch: 48, step: 20
	action: tensor([[-0.2181, -0.3068, -0.2636, -0.2301,  0.3273,  0.0626, -0.1933]],
       dtype=torch.float64)
	q_value: tensor([[-24.9469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2383708964729836, distance: 1.2734504808066802 entropy 0.5003275275230408
epoch: 48, step: 21
	action: tensor([[-0.1711,  0.2782,  0.5075, -0.2214,  0.2099, -0.4055,  0.3500]],
       dtype=torch.float64)
	q_value: tensor([[-24.6399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029242020662388346, distance: 1.1274886476031427 entropy 0.5003275275230408
epoch: 48, step: 22
	action: tensor([[ 0.2005, -0.0708,  0.3886,  0.2093,  0.2804, -0.5129, -0.4593]],
       dtype=torch.float64)
	q_value: tensor([[-24.3619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38839497730248274, distance: 0.8949364871826242 entropy 0.5003275275230408
epoch: 48, step: 23
	action: tensor([[-0.5974,  0.5113,  0.0306,  0.0122, -0.6726,  1.0145, -0.5887]],
       dtype=torch.float64)
	q_value: tensor([[-26.6897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 24
	action: tensor([[-1.6250, -0.4015,  4.2021, -2.6720,  2.5248, -0.2964,  1.5609]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 25
	action: tensor([[-1.7998, -0.8782,  3.2670, -2.6685,  1.3548, -0.2622,  1.3795]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 26
	action: tensor([[-1.4758, -1.2730,  3.7983, -2.2233,  2.6324,  0.2245,  1.4509]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 27
	action: tensor([[-1.0478, -0.7655,  3.9225, -2.8048,  2.7092,  0.2020,  1.1600]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 28
	action: tensor([[-0.8691, -0.4867,  3.9415, -2.7931,  2.0026,  0.2185,  1.3280]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 29
	action: tensor([[-1.3096, -0.8262,  3.6602, -2.6643,  2.2467, -0.0669,  1.9433]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 30
	action: tensor([[-1.2695, -1.0196,  4.3982, -3.1857,  2.8479, -0.2938,  1.2239]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 31
	action: tensor([[-1.0557, -1.1253,  3.6721, -2.5983,  1.2998,  0.4475,  1.3243]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 32
	action: tensor([[-1.2253, -0.3581,  3.5474, -2.6704,  1.7506, -0.5323,  2.2183]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 33
	action: tensor([[-1.6152, -0.6950,  4.3238, -2.4728,  2.3435,  0.5557,  0.6129]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 34
	action: tensor([[-2.6790, -0.8783,  3.9307, -2.3185,  2.1999,  0.2730,  1.8322]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 35
	action: tensor([[-1.8740, -0.4519,  4.3167, -2.9002,  1.4446,  0.1412,  1.6895]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 36
	action: tensor([[-1.3643, -0.6096,  4.6714, -2.6052,  2.1460,  0.0513,  0.7688]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 37
	action: tensor([[-1.1056, -0.0491,  3.8689, -2.1465,  2.5695, -0.6108,  2.0098]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 38
	action: tensor([[-2.5571, -0.9713,  3.3700, -2.7205,  2.2884, -0.6282,  1.5274]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 39
	action: tensor([[-1.2818, -0.2700,  4.1183, -2.5880,  2.4094,  0.6477,  1.7903]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 40
	action: tensor([[-1.3024, -0.5224,  3.9093, -2.1801,  1.8240, -0.8540,  1.6916]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8412634014719118 entropy 0.5003275275230408
epoch: 48, step: 41
	action: tensor([[-1.0932, -1.2667,  3.8819, -2.5931,  1.9544,  0.5075,  2.3822]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 42
	action: tensor([[-1.3167, -0.3250,  3.8800, -1.3104,  1.9071, -0.2397,  1.4082]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2553187934977226 entropy 0.5003275275230408
epoch: 48, step: 43
	action: tensor([[-1.1445,  0.1624,  4.5518, -2.4474,  1.5384,  0.7340,  1.6817]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 44
	action: tensor([[-1.4937, -0.5042,  3.7379, -1.5596,  2.4581,  0.1694,  1.6463]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2698559718865738 entropy 0.5003275275230408
epoch: 48, step: 45
	action: tensor([[-1.8967, -0.8708,  3.6021, -2.6264,  1.9188, -0.0341,  1.1258]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 46
	action: tensor([[-2.6744, -0.7487,  3.7901, -3.1817,  2.4114, -0.1724,  2.1966]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 47
	action: tensor([[-1.5140, -0.9719,  4.9090, -2.5980,  2.4472,  0.0950,  1.5439]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 48
	action: tensor([[-1.5173, -0.6602,  3.2770, -1.6649,  2.7122,  0.1988,  0.5014]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3570570635701789 entropy 0.5003275275230408
epoch: 48, step: 49
	action: tensor([[-1.6131, -0.4239,  3.9667, -2.2892,  2.2969,  0.0986,  1.5578]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 50
	action: tensor([[-2.2669, -0.2621,  3.9024, -2.1836,  2.4473,  0.5775,  1.2091]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 51
	action: tensor([[-0.8049, -0.2396,  3.6282, -2.7425,  2.1027,  0.2139,  2.0273]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 52
	action: tensor([[-1.9182, -0.7342,  4.4958, -1.9477,  2.4617,  0.7145,  2.1856]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 53
	action: tensor([[-1.6940, -0.9257,  3.5442, -1.8906,  2.2882,  0.2139,  1.6275]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 54
	action: tensor([[-1.3889, -0.6654,  3.6652, -2.2028,  2.5107,  0.3858,  1.5830]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9469041154431018 entropy 0.5003275275230408
epoch: 48, step: 55
	action: tensor([[-1.9103, -1.1782,  4.4550, -3.0691,  3.1055, -0.1732,  1.5913]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 56
	action: tensor([[-1.6919, -0.8096,  3.9338, -2.0796,  1.4951,  1.0808,  1.2393]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 57
	action: tensor([[-1.3486, -0.6519,  3.7494, -2.3392,  1.8188,  0.5038,  1.0278]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 58
	action: tensor([[-0.9517, -0.7764,  3.4339, -2.8097,  2.1960,  0.4074,  2.1325]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 59
	action: tensor([[-1.1445, -0.7481,  3.8122, -2.5969,  1.8216,  0.4024,  1.4445]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 60
	action: tensor([[-1.6567, -0.9987,  4.3767, -1.8198,  1.8652, -0.3486,  2.3675]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 61
	action: tensor([[-1.4027, -0.6440,  3.7976, -2.7706,  2.0559, -0.2649,  2.0606]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 62
	action: tensor([[-1.2159, -0.7572,  3.6961, -2.8418,  2.4155,  0.3326,  1.0625]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 63
	action: tensor([[-1.4716, -1.1761,  4.5851, -2.7550,  2.8885,  0.1160,  1.8260]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 64
	action: tensor([[-1.1083, -1.0173,  3.7335, -1.5867,  1.9658,  0.2873,  1.7267]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.244365717547627 entropy 0.5003275275230408
epoch: 48, step: 65
	action: tensor([[-1.6551, -0.7962,  3.5334, -2.8256,  2.5181,  0.2758,  1.7232]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 66
	action: tensor([[-1.7276,  0.3291,  3.8957, -2.6315,  2.7439,  0.0716,  1.3482]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 67
	action: tensor([[-1.2693, -0.9320,  4.0117, -2.3912,  2.2329, -0.8342,  1.9123]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 68
	action: tensor([[-1.2634, -0.5297,  4.4700, -3.3948,  1.7548,  0.2624,  1.8417]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 69
	action: tensor([[-1.5711, -0.3468,  4.2941, -3.0776,  2.1725,  0.4811,  2.2635]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 70
	action: tensor([[-2.1232, -1.3804,  3.6164, -2.5265,  2.3116,  0.9217,  2.5491]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 71
	action: tensor([[-1.8231, -0.5929,  3.7901, -1.9478,  1.4097,  0.5516,  1.6787]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 72
	action: tensor([[-0.8433, -0.3246,  3.9004, -2.5340,  2.3876,  0.0927,  1.4909]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 73
	action: tensor([[-1.2426, -0.3509,  3.9879, -2.6343,  2.7311,  0.1413,  2.0310]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 74
	action: tensor([[-0.8736, -0.3673,  3.4719, -1.5528,  1.4431, -0.3760,  1.2094]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 75
	action: tensor([[-1.9533, -0.8829,  3.8654, -2.5031,  1.9131,  0.2055,  1.9696]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 76
	action: tensor([[-1.8596, -0.7256,  4.0029, -2.9593,  2.0507,  0.0161,  1.7647]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 77
	action: tensor([[-1.8074, -0.8397,  3.6198, -2.2536,  2.6923,  0.3652,  1.5236]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 78
	action: tensor([[-1.8010, -0.7691,  2.9858, -1.9741,  2.1614,  0.8836,  1.7252]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 79
	action: tensor([[-1.9411, -0.3081,  4.4433, -2.4559,  1.9634,  0.0523,  1.4383]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 80
	action: tensor([[-1.0549, -0.6158,  3.6205, -2.7968,  3.2347, -0.0443,  0.9915]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 81
	action: tensor([[-1.4526, -1.0704,  3.4765, -3.0700,  2.8179,  0.3208,  1.6586]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 82
	action: tensor([[-1.2599, -0.2426,  4.2376, -2.1308,  2.1279, -0.4263,  1.4136]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7526988015811845 entropy 0.5003275275230408
epoch: 48, step: 83
	action: tensor([[-1.5973, -0.9856,  4.2976, -1.9214,  2.8768,  0.1112,  1.8943]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1139594722616049 entropy 0.5003275275230408
epoch: 48, step: 84
	action: tensor([[-1.5406, -0.8046,  3.7747, -1.5670,  2.3512,  0.2243,  1.1065]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3639533911713526 entropy 0.5003275275230408
epoch: 48, step: 85
	action: tensor([[-1.4697, -0.4731,  4.2757, -1.9284,  2.2856,  0.1827,  1.7900]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9366798359445019 entropy 0.5003275275230408
epoch: 48, step: 86
	action: tensor([[-1.6420, -0.3443,  3.9027, -2.7390,  2.6201,  0.2731,  1.1790]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 87
	action: tensor([[-1.9335, -0.5975,  4.0208, -2.3215,  2.3130, -0.3850,  1.5774]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 88
	action: tensor([[-1.2681, -0.4172,  3.1375, -3.0050,  1.8688,  0.8209,  1.7752]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 89
	action: tensor([[-1.7790, -0.8649,  3.9740, -2.2605,  2.4335,  0.7865,  1.3241]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 90
	action: tensor([[-2.3604, -0.1987,  3.8274, -2.7056,  1.9844,  1.0051,  1.5234]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 91
	action: tensor([[-1.2688, -0.0671,  3.8524, -2.5276,  2.0140,  0.0528,  1.1956]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 92
	action: tensor([[-1.6369, -0.4446,  3.4547, -2.9772,  2.6531, -0.0630,  1.4805]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 93
	action: tensor([[-1.7024, -0.1137,  4.4588, -2.9074,  2.7145, -0.1038,  1.4631]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 94
	action: tensor([[-1.3328, -0.1266,  4.2921, -2.7357,  2.6381,  0.4928,  1.7294]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 95
	action: tensor([[-1.5841, -1.0062,  4.1518, -2.1919,  2.1984,  0.1436,  2.2793]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0325117006474986 entropy 0.5003275275230408
epoch: 48, step: 96
	action: tensor([[-1.3816, -0.6299,  3.7526, -2.2498,  2.2176,  0.6321,  1.7155]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9428233034925908 entropy 0.5003275275230408
epoch: 48, step: 97
	action: tensor([[-1.1437, -0.3667,  3.1327, -2.1452,  2.4563,  0.0908,  0.5325]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 98
	action: tensor([[-1.4733, -0.1063,  4.3598, -1.7244,  1.4508,  0.4016,  1.6491]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9964256520684609 entropy 0.5003275275230408
epoch: 48, step: 99
	action: tensor([[-1.5748, -0.6275,  4.5268, -2.2172,  2.4791,  0.3036,  1.2161]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8284361594666401 entropy 0.5003275275230408
epoch: 48, step: 100
	action: tensor([[-1.5807, -0.3027,  3.4404, -2.3255,  2.1888,  0.5829,  1.5194]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 101
	action: tensor([[-1.2512e+00, -8.1810e-01,  3.6176e+00, -2.5677e+00,  2.4657e+00,
          2.4372e-04,  1.7791e+00]], dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 102
	action: tensor([[-2.5923, -1.3190,  4.2058, -2.9404,  1.5966, -0.1301,  1.4641]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 103
	action: tensor([[-1.8407, -0.9745,  4.1107, -2.8803,  2.0391,  0.0313,  2.1816]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 104
	action: tensor([[-1.6117,  0.0311,  4.7375, -2.7959,  3.0187, -0.2660,  1.6937]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 105
	action: tensor([[-1.2486, -0.4314,  3.2665, -2.4152,  1.7414,  0.2574,  2.1419]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 106
	action: tensor([[-2.4570, -0.3551,  3.7634, -3.0371,  2.2524, -0.3344,  2.1007]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 107
	action: tensor([[-1.6187, -0.6473,  3.8969, -2.1446,  2.0696, -0.3999,  2.0324]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0360600510356195 entropy 0.5003275275230408
epoch: 48, step: 108
	action: tensor([[-2.2127,  0.1429,  3.1896, -1.8133,  2.3170,  0.5874,  1.8319]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 109
	action: tensor([[-1.5980, -0.3172,  3.3387, -2.3766,  1.9128,  0.2011,  0.8626]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 110
	action: tensor([[-8.3978e-01, -4.0279e-03,  4.2970e+00, -1.8985e+00,  2.3819e+00,
          9.2213e-02,  1.4400e+00]], dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 111
	action: tensor([[-1.5502, -1.1591,  4.1546, -2.3109,  2.1482, -0.2005,  1.2464]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 112
	action: tensor([[-1.2814, -0.4250,  4.4293, -2.2239,  2.2756,  0.2558,  1.6893]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6915663481151088 entropy 0.5003275275230408
epoch: 48, step: 113
	action: tensor([[-1.8360, -0.4096,  5.0841, -2.1951,  2.3504, -0.4070,  1.7398]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 114
	action: tensor([[-1.8173, -0.7197,  3.7448, -3.2800,  2.9599,  0.4083,  1.1075]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 115
	action: tensor([[-2.1201, -1.1013,  4.0783, -2.6012,  1.2736, -0.3153,  1.8124]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 116
	action: tensor([[-1.3829, -0.6141,  4.6061, -2.8574,  2.4128, -0.0114,  1.9455]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 117
	action: tensor([[-1.1488, -0.8464,  4.5240, -2.4726,  1.6992,  0.0084,  2.1934]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 118
	action: tensor([[-1.0310, -0.7194,  3.4239, -2.2426,  2.6324,  0.3781,  1.8394]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 119
	action: tensor([[-1.2141, -0.6261,  3.6711, -2.7081,  2.5257, -0.1156,  1.5126]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 120
	action: tensor([[-1.2122, -0.6156,  3.7321, -1.9290,  1.4775, -0.4755,  1.7526]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9150195513156287 entropy 0.5003275275230408
epoch: 48, step: 121
	action: tensor([[-1.5182, -1.0739,  3.9506, -2.8634,  1.9700,  0.0623,  1.3755]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 122
	action: tensor([[-1.8902, -0.2702,  3.9828, -2.2808,  2.2500,  0.0622,  1.5962]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 123
	action: tensor([[-1.3901, -0.8526,  4.1547, -2.7638,  2.0115,  0.3917,  1.2313]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 124
	action: tensor([[-1.3085, -0.4934,  3.5057, -1.6866,  2.5881,  0.0124,  1.7316]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.191049180899991 entropy 0.5003275275230408
epoch: 48, step: 125
	action: tensor([[-1.5034, -1.1155,  3.1470, -2.9423,  1.8484,  0.3606,  1.3271]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 126
	action: tensor([[-2.2511, -0.0833,  3.7138, -2.5500,  2.0678, -0.1477,  1.8324]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 48, step: 127
	action: tensor([[-1.5497,  0.0217,  3.6714, -2.2437,  1.5621,  0.0755,  1.4326]],
       dtype=torch.float64)
	q_value: tensor([[-31.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
LOSS epoch 48 actor 178.28025211623273 critic 316.0142698328481 
epoch: 49, step: 0
	action: tensor([[-2.5973,  0.1994,  6.1800, -3.4669,  2.4589,  0.1872,  3.2928]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 1
	action: tensor([[-2.7587, -0.7634,  5.3655, -4.0998,  2.4237,  0.7358,  2.5037]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 2
	action: tensor([[-2.4146,  0.0318,  5.9128, -2.7565,  2.3322,  0.8516,  2.9131]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 3
	action: tensor([[-2.3920, -0.4017,  6.1614, -3.5094,  2.6147,  0.1884,  2.9649]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 4
	action: tensor([[-1.4870, -0.2388,  6.1457, -2.7240,  2.9656,  1.0741,  2.0153]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 5
	action: tensor([[-2.2912, -1.1179,  5.8511, -2.8853,  2.2453,  0.5694,  2.6384]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 6
	action: tensor([[-2.3083, -0.4355,  5.7167, -3.6466,  2.3691,  0.2630,  1.8771]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 7
	action: tensor([[-2.2657, -0.0448,  5.7296, -3.9986,  2.7351,  0.4358,  1.7742]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 8
	action: tensor([[-2.3313, -0.6564,  6.1800, -3.5894,  2.9213,  0.3616,  2.9758]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 9
	action: tensor([[-2.5309, -0.8912,  6.0227, -3.4918,  2.5386,  0.5846,  2.9936]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 10
	action: tensor([[-1.8380, -1.0253,  6.1800, -3.6493,  2.5692,  0.4651,  1.4431]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 11
	action: tensor([[-2.7457, -0.1764,  5.5571, -3.5609,  3.2811,  0.3148,  2.3543]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 12
	action: tensor([[-2.0603, -0.5169,  5.9901, -3.3989,  2.5038,  0.4240,  3.0443]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 13
	action: tensor([[-3.0592, -0.3543,  6.0630, -2.8782,  2.6985,  0.7692,  2.0086]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 14
	action: tensor([[-2.5326, -0.8449,  5.3377, -3.6729,  2.9673,  0.3783,  2.7766]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 15
	action: tensor([[-1.7510, -0.2585,  5.8105, -3.1243,  2.8581,  0.3079,  2.8215]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 16
	action: tensor([[-2.8206, -0.3459,  5.0159, -3.0901,  2.6049,  0.4918,  2.2408]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 17
	action: tensor([[-2.7798, -0.6111,  5.9445, -3.4008,  2.1750,  0.8207,  2.1768]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 18
	action: tensor([[-1.5575,  0.5974,  5.8726, -3.4647,  2.9212,  0.5639,  1.7009]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 19
	action: tensor([[-2.3058, -0.0415,  5.4197, -3.8918,  2.7812, -0.4642,  2.8335]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 20
	action: tensor([[-2.9713, -0.2016,  6.1364, -4.0942,  3.0315,  0.0795,  2.0819]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 21
	action: tensor([[-1.9050, -0.4294,  5.6735, -3.5176,  2.6173, -0.0544,  2.0548]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 22
	action: tensor([[-2.4035,  0.6227,  6.1800, -2.7684,  2.7318,  0.9344,  2.3041]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 23
	action: tensor([[-2.0449, -0.4815,  5.5504, -3.5749,  2.2479,  0.9751,  3.1326]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 24
	action: tensor([[-2.2122, -1.2515,  6.1800, -3.9676,  2.4787, -0.1487,  2.3357]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 25
	action: tensor([[-2.2884, -0.6402,  5.6237, -3.5968,  2.7072,  0.1903,  1.9121]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 26
	action: tensor([[-1.7478, -0.1762,  6.1088, -3.1876,  2.1431,  0.1906,  2.8051]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 27
	action: tensor([[-2.5564,  0.0452,  5.5889, -3.7860,  2.1074, -0.3933,  2.5296]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 28
	action: tensor([[-2.1050, -0.9981,  5.9015, -4.0455,  2.8166, -0.4364,  2.5587]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 29
	action: tensor([[-3.0141e+00, -1.7899e-03,  5.9560e+00, -3.1546e+00,  2.2795e+00,
          3.6659e-01,  2.8744e+00]], dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 30
	action: tensor([[-2.9814,  0.1497,  5.0550, -3.5310,  1.9251,  0.1181,  3.0346]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 31
	action: tensor([[-1.6439, -0.3491,  5.9448, -3.0627,  3.1157,  1.0897,  2.2134]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 32
	action: tensor([[-1.7155, -0.5244,  6.1800, -3.7797,  2.2550, -0.0587,  2.4069]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 33
	action: tensor([[-2.8232, -0.5794,  6.1306, -2.7092,  2.3237,  0.9901,  2.9682]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 34
	action: tensor([[-2.3641, -0.2846,  6.1800, -3.2845,  2.8917, -0.0174,  2.7203]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 35
	action: tensor([[-1.7811, -0.0879,  6.1800, -3.2510,  2.4955, -0.5182,  2.6973]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 36
	action: tensor([[-2.2851,  0.0397,  6.1800, -3.1552,  2.4959,  0.6735,  2.2037]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 37
	action: tensor([[-2.3479,  0.0568,  6.1424, -3.1788,  2.4110,  0.6804,  2.3113]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 38
	action: tensor([[-2.5752,  0.4807,  6.1537, -4.0128,  2.5387,  0.2884,  2.6179]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 39
	action: tensor([[-2.8765, -0.5157,  6.1800, -4.2554,  2.5274,  1.1849,  2.7407]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 40
	action: tensor([[-1.9165, -0.8306,  5.7944, -3.0820,  3.2661,  0.6326,  2.5652]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 41
	action: tensor([[-1.9743, -0.4401,  6.0141, -3.7258,  2.4790,  0.4185,  2.8148]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 42
	action: tensor([[-2.8485, -0.8766,  5.9884, -3.7240,  2.9066,  0.0818,  2.6597]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 43
	action: tensor([[-2.4603, -0.3693,  5.2978, -3.7164,  2.5444,  0.6374,  2.5053]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 44
	action: tensor([[-1.8507, -0.3310,  5.9048, -3.7803,  3.2409, -0.3503,  2.7735]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 45
	action: tensor([[-1.7495, -0.6726,  6.1800, -3.8542,  2.5770,  1.0035,  2.6463]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 46
	action: tensor([[-2.4741, -0.0965,  5.7300, -3.5226,  1.8069,  0.8304,  2.6860]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 47
	action: tensor([[-2.5141,  0.2978,  6.0723, -3.2961,  1.8897,  0.0540,  2.1471]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 48
	action: tensor([[-2.7066, -0.5596,  5.4752, -2.7096,  3.1320, -0.3712,  2.5776]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 49
	action: tensor([[-2.2216, -0.2642,  4.8156, -3.4232,  2.6490,  0.1433,  3.6164]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 50
	action: tensor([[-1.3976, -0.4344,  5.9736, -3.5123,  3.4319, -0.6706,  2.4931]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 51
	action: tensor([[-2.1738, -0.5060,  6.1800, -3.4178,  2.7371,  0.2770,  2.0040]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 52
	action: tensor([[-2.4268, -0.7420,  6.0652, -2.6904,  2.6327, -0.6922,  2.7564]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 53
	action: tensor([[-2.0432, -0.9245,  5.5540, -3.5957,  2.5551,  0.6484,  2.4076]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 54
	action: tensor([[-2.2907, -0.7225,  6.1800, -3.0290,  2.3584, -0.2830,  2.5566]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 55
	action: tensor([[-1.7638, -1.5477,  6.1800, -2.9388,  2.4978, -0.1687,  2.0206]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 56
	action: tensor([[-2.3108,  0.4456,  6.1800, -2.5122,  2.8966, -0.3168,  2.5925]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 57
	action: tensor([[-2.3766, -0.4365,  6.1800, -3.7132,  2.6529,  0.0100,  2.2958]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 58
	action: tensor([[-2.3731, -0.6694,  5.6659, -3.2225,  2.2581, -0.2868,  2.5246]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 59
	action: tensor([[-2.7390, -0.7777,  6.1800, -3.7924,  1.6060,  0.5737,  2.3720]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 60
	action: tensor([[-2.3447, -0.4627,  6.1266, -3.4133,  2.6669, -0.3622,  2.5510]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 61
	action: tensor([[-2.4191, -0.0733,  5.7072, -4.1279,  1.9788, -0.4892,  2.4405]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 62
	action: tensor([[-2.6632,  0.0142,  5.6304, -3.5211,  2.7234,  0.3947,  2.9705]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 63
	action: tensor([[-2.3935, -1.2471,  5.3353, -3.6939,  3.2091,  1.0079,  3.4589]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 64
	action: tensor([[-1.8523,  0.0165,  6.1800, -2.8618,  2.1580, -0.1879,  1.8632]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 65
	action: tensor([[-2.9592, -0.1310,  6.1800, -2.5030,  2.3762,  0.6924,  2.4535]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 66
	action: tensor([[-2.3776, -0.0787,  5.7904, -3.3607,  2.4013, -0.1393,  2.3262]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 67
	action: tensor([[-2.0101, -0.5074,  6.0233, -3.4931,  2.8565,  0.4063,  2.5064]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 68
	action: tensor([[-2.2300, -0.2331,  5.9050, -2.9427,  2.2990,  0.3618,  2.1502]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 69
	action: tensor([[-2.2780, -0.5648,  6.0784, -2.9444,  2.5331, -0.0490,  2.5228]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 70
	action: tensor([[-1.4877,  0.2783,  6.0793, -3.4140,  2.0331, -0.0603,  2.6120]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 71
	action: tensor([[-2.6639, -0.3785,  6.0870, -3.8794,  2.6768,  0.0101,  2.9916]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 72
	action: tensor([[-1.8144, -0.1201,  5.5832, -3.8110,  2.9492, -0.1097,  2.4597]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 73
	action: tensor([[-2.3162, -0.1675,  6.1800, -3.1232,  3.3473,  0.0227,  2.3463]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 74
	action: tensor([[-2.4599, -0.2074,  5.6477, -3.4507,  3.2079,  0.4767,  2.4479]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 75
	action: tensor([[-2.2392, -0.6810,  6.1800, -3.5296,  2.0749,  0.1769,  2.5264]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 76
	action: tensor([[-1.8757, -0.7054,  5.8932, -3.4466,  3.0281,  0.1595,  2.5446]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 77
	action: tensor([[-2.9036e+00, -3.5954e-04,  6.1649e+00, -3.2003e+00,  2.3908e+00,
          4.6818e-01,  3.2823e+00]], dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 78
	action: tensor([[-2.2247, -0.2405,  6.0689, -3.6839,  2.3294,  1.0208,  2.9012]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 79
	action: tensor([[-2.9336, -0.2172,  6.1800, -3.2208,  2.5088,  0.2109,  2.7532]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 80
	action: tensor([[-2.0889,  0.0262,  5.2703, -3.2330,  1.8223,  0.5577,  1.9725]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 81
	action: tensor([[-1.9870, -0.6753,  6.1800, -2.9461,  2.7932,  0.1942,  2.7802]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 82
	action: tensor([[-2.3615, -1.1078,  5.6720, -3.2719,  2.6415, -0.0659,  2.6887]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 83
	action: tensor([[-2.2360, -0.2047,  6.1707, -3.8249,  2.7532, -0.0392,  2.4783]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 84
	action: tensor([[-1.9298, -0.5583,  5.6529, -3.3251,  2.3552,  0.7052,  2.5704]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 85
	action: tensor([[-2.9138,  0.2459,  6.1543, -3.7003,  2.7616, -0.0478,  2.8423]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 86
	action: tensor([[-2.0331, -0.0779,  6.1800, -3.7422,  2.4197,  0.0789,  2.5393]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 87
	action: tensor([[-2.4089, -0.5419,  6.1618, -2.8223,  2.3369,  0.5829,  2.1901]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 88
	action: tensor([[-3.0516, -0.4055,  5.9547, -3.6174,  1.6736,  0.7770,  2.9465]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 89
	action: tensor([[-2.6485, -0.2536,  6.1800, -3.3495,  3.0365,  0.8627,  2.2482]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 90
	action: tensor([[-2.3331, -1.0072,  5.9860, -3.7346,  2.8153,  0.4916,  2.3234]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 91
	action: tensor([[-2.4621, -0.7806,  6.1800, -2.9849,  2.4647, -0.3651,  2.9262]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 92
	action: tensor([[-2.1839, -0.2107,  5.3634, -3.3266,  2.1922,  0.7914,  2.9297]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 93
	action: tensor([[-1.6527, -0.2994,  6.1800, -3.9457,  2.7171,  0.5535,  2.5546]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 94
	action: tensor([[-2.0423, -0.4233,  5.4775, -2.9290,  2.2408, -0.0231,  3.1391]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 95
	action: tensor([[-2.5112, -0.4386,  5.3792, -3.5589,  2.5540,  0.6511,  2.5490]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 96
	action: tensor([[-2.7772, -0.1922,  5.4324, -3.5842,  2.3151, -0.0954,  1.8655]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 97
	action: tensor([[-1.8929, -1.0347,  6.1800, -3.0933,  2.2451,  0.9279,  2.4277]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 98
	action: tensor([[-2.6757,  0.1076,  5.2918, -3.0985,  1.8351,  0.1310,  2.8482]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 99
	action: tensor([[-2.6183, -0.6232,  6.1800, -4.1710,  2.5076,  0.1764,  2.2204]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 100
	action: tensor([[-2.1964, -0.9771,  5.9546, -3.1014,  2.0952, -0.1135,  2.3537]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 101
	action: tensor([[-2.4959, -0.4967,  5.6860, -3.0344,  2.7430,  0.1794,  1.7877]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 102
	action: tensor([[-1.8025, -0.9940,  5.9415, -3.5362,  2.9268,  0.5695,  2.9276]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 103
	action: tensor([[-2.7414, -0.4846,  5.6257, -3.0579,  2.3585,  0.0578,  2.6516]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 104
	action: tensor([[-1.9332, -0.1341,  6.1800, -3.4400,  2.6160,  0.2645,  2.7310]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 105
	action: tensor([[-2.9468, -0.7225,  5.3230, -3.4805,  2.7229,  0.7580,  2.4679]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 106
	action: tensor([[-2.2782,  0.4389,  6.1596, -3.1389,  2.2919,  0.5791,  2.1977]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 107
	action: tensor([[-2.4243e+00, -1.2116e+00,  5.8336e+00, -3.7189e+00,  3.3304e+00,
          1.2213e-03,  2.4091e+00]], dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 108
	action: tensor([[-2.5093, -0.5058,  5.9268, -3.4946,  2.9055, -0.5044,  2.9608]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 109
	action: tensor([[-2.6572, -0.8850,  5.7044, -3.5388,  2.4229,  0.1979,  2.1826]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 110
	action: tensor([[-2.1742, -0.6814,  6.1800, -3.4702,  2.3802,  0.2289,  2.1660]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 111
	action: tensor([[-3.0718, -1.1304,  6.0422, -3.1984,  3.2330,  0.1516,  2.5654]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 112
	action: tensor([[-1.2992,  0.1192,  6.1800, -2.7003,  2.8890,  0.5212,  2.5393]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 113
	action: tensor([[-2.4472, -0.0551,  6.1800, -3.8431,  2.4687,  0.6989,  2.1985]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 114
	action: tensor([[-2.3687, -0.4727,  6.1134, -2.7196,  2.4582,  0.4962,  2.5731]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 115
	action: tensor([[-2.2022, -0.4389,  5.9565, -3.1971,  2.7595,  0.6559,  2.7188]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 116
	action: tensor([[-2.3909, -0.9795,  5.5140, -3.2492,  3.1613,  0.4029,  2.9316]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 117
	action: tensor([[-2.0056, -0.8902,  5.2350, -3.7710,  2.1620,  0.0122,  2.9485]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 118
	action: tensor([[-2.4447,  0.1295,  6.1800, -3.1833,  3.4099, -0.6968,  2.4211]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 119
	action: tensor([[-1.4854,  0.1694,  5.8778, -2.6734,  2.5134,  0.2445,  2.2812]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 120
	action: tensor([[-2.3295, -0.5708,  6.1800, -3.1574,  2.6993,  0.3443,  2.7919]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 121
	action: tensor([[-2.1347,  0.1099,  5.5355, -4.0921,  2.7682,  0.7040,  2.6948]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 122
	action: tensor([[-2.6469, -1.1083,  5.5559, -3.3783,  1.6194,  0.5375,  2.6129]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 123
	action: tensor([[-2.3191, -0.4565,  5.6910, -3.6153,  1.9061, -0.2980,  2.0580]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 124
	action: tensor([[-1.8360, -0.7040,  6.1800, -2.8469,  3.0840,  0.4597,  2.8075]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 125
	action: tensor([[-2.3288, -0.6688,  5.7955, -3.3635,  2.5725, -0.1897,  2.5528]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 126
	action: tensor([[-3.1115, -0.5883,  6.0797, -3.0140,  1.9247,  0.4110,  2.4381]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 49, step: 127
	action: tensor([[-2.6134, -0.3189,  6.1500, -3.9516,  2.8158,  0.6370,  2.3296]],
       dtype=torch.float64)
	q_value: tensor([[-42.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
LOSS epoch 49 actor 26.6741516587287 critic 53.448368824452125 
epoch: 50, step: 0
	action: tensor([[-2.9852,  0.0403,  6.0667, -4.0787,  2.5965,  0.7134,  2.9883]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 1
	action: tensor([[-2.8079,  0.3210,  6.1800, -4.0765,  2.7807,  0.8694,  3.9003]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 2
	action: tensor([[-3.3089, -0.1880,  6.1800, -3.6596,  3.6265, -0.7130,  3.3263]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 3
	action: tensor([[-3.1235,  0.0945,  6.1800, -4.2228,  3.3335,  0.7108,  3.4841]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 4
	action: tensor([[-2.7639,  0.2020,  6.1800, -3.4442,  3.5729,  0.2545,  3.4574]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 5
	action: tensor([[-3.4767, -0.2336,  6.1800, -4.6364,  2.9647,  0.9408,  3.6393]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 6
	action: tensor([[-2.7799, -0.9182,  6.1800, -4.0123,  2.4108,  0.5176,  3.3202]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 7
	action: tensor([[-3.1556, -0.5979,  6.0398, -4.6764,  2.4927,  0.7327,  3.2241]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 8
	action: tensor([[-3.5141, -0.4154,  6.1800, -3.6174,  2.8323, -0.0814,  3.0237]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 9
	action: tensor([[-2.5885,  0.9071,  5.9985, -4.5591,  3.2719,  0.1463,  3.3639]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 10
	action: tensor([[-3.0633,  0.0881,  6.1150, -5.1709,  3.1419, -0.5911,  3.1853]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 11
	action: tensor([[-2.8710, -0.5752,  6.1800, -3.3214,  3.3698, -0.0664,  3.5753]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 12
	action: tensor([[-3.2505, -0.1272,  6.1800, -4.0997,  3.2281,  0.2906,  3.8411]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 13
	action: tensor([[-2.9524, -0.2917,  5.8257, -3.2294,  3.0658,  0.1168,  3.0235]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 14
	action: tensor([[-2.9453,  0.6066,  6.1800, -3.7474,  3.2852,  0.6415,  2.8870]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 15
	action: tensor([[-2.6245,  0.0853,  6.0937, -4.0011,  2.2307, -0.4509,  3.8977]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 16
	action: tensor([[-3.0663, -0.9184,  5.6575, -3.9388,  3.2703,  0.7289,  2.9868]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 17
	action: tensor([[-3.3398, -0.2061,  6.1800, -3.9997,  3.3255,  1.2471,  3.3680]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 18
	action: tensor([[-3.3034,  0.4902,  6.1800, -4.2956,  2.4582, -0.0796,  2.9107]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 19
	action: tensor([[-3.5486,  0.0765,  6.1800, -3.8808,  2.7973,  0.4524,  3.6710]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 20
	action: tensor([[-3.5222, -0.5069,  6.1800, -3.5222,  2.7126,  0.6192,  2.8874]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 21
	action: tensor([[-2.3067, -0.5619,  6.1800, -3.2425,  3.2397,  0.2959,  2.7870]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 22
	action: tensor([[-3.4016, -0.1450,  6.1800, -4.5770,  2.5750,  0.1829,  3.2228]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 23
	action: tensor([[-2.3875, -0.3996,  6.1800, -3.9079,  2.9401,  0.9745,  3.1687]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 24
	action: tensor([[-3.0755, -0.2006,  5.9428, -4.1442,  2.6667, -0.0686,  3.2067]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 25
	action: tensor([[-2.9324, -0.2571,  6.1800, -4.1889,  3.4686,  0.9077,  3.1805]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 26
	action: tensor([[-2.9173, -0.0438,  6.1800, -3.8831,  2.9887,  0.4661,  3.4659]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 27
	action: tensor([[-3.3113, -0.1286,  6.0871, -4.2032,  2.6177,  0.2158,  2.9826]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 28
	action: tensor([[-2.8870, -0.5144,  6.1800, -4.6331,  2.3984,  0.6611,  3.4859]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 29
	action: tensor([[-2.7443, -0.0523,  6.1800, -4.4604,  2.9841, -0.0315,  3.3420]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 30
	action: tensor([[-2.7828, -0.4087,  5.7044, -4.8444,  2.8256,  0.6814,  2.8230]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 31
	action: tensor([[-2.8162, -0.4797,  6.1800, -3.8236,  2.1012,  0.1214,  3.1998]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 32
	action: tensor([[-3.2275,  0.2377,  6.1800, -4.1574,  2.9006,  0.9012,  3.6068]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 33
	action: tensor([[-3.7557, -0.2055,  5.8437, -3.8001,  2.4712, -0.0545,  3.2246]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 34
	action: tensor([[-2.8494,  0.0909,  6.1800, -3.9129,  2.9531,  0.9975,  3.2794]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 35
	action: tensor([[-2.5523, -0.0583,  5.7089, -4.0518,  2.5192,  0.6406,  3.7765]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 36
	action: tensor([[-3.1832, -0.0184,  5.4827, -3.9879,  2.7042, -0.1097,  3.4312]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 37
	action: tensor([[-3.4009, -0.3706,  6.1800, -4.1849,  2.4966,  0.2528,  2.8101]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 38
	action: tensor([[-3.1801, -0.2719,  6.0152, -3.7852,  3.1378,  0.2264,  3.1283]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 39
	action: tensor([[-2.5768, -0.3437,  6.1800, -4.5989,  2.3128, -0.2080,  3.0950]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 40
	action: tensor([[-3.1759, -0.0187,  5.7904, -3.5551,  2.0395, -0.0908,  3.1926]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 41
	action: tensor([[-2.9415, -0.6404,  6.1800, -4.3683,  2.6929, -0.4809,  2.6893]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 42
	action: tensor([[-3.7089, -0.1282,  5.9768, -3.5954,  3.2762,  0.4543,  3.8183]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 43
	action: tensor([[-3.8381, -0.3182,  6.1800, -2.9166,  3.3497,  0.2483,  3.4846]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 44
	action: tensor([[-2.8366, -0.3624,  6.1800, -3.8597,  3.0421,  0.0587,  3.0210]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 45
	action: tensor([[-3.2263,  0.1592,  5.6960, -3.9087,  2.6740, -0.4226,  3.6267]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 46
	action: tensor([[-2.6844,  0.1261,  5.8493, -5.0668,  3.1328,  0.4909,  2.9233]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 47
	action: tensor([[-3.8448, -0.2545,  6.1800, -3.7149,  2.4387, -0.3292,  3.0413]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 48
	action: tensor([[-3.0990, -0.5251,  6.1800, -3.7373,  3.0582,  0.3603,  3.6009]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 49
	action: tensor([[-2.9275, -0.2264,  6.1800, -3.8608,  2.8850,  0.3216,  3.5446]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 50
	action: tensor([[-3.0231, -0.4239,  6.1800, -4.0302,  2.6379, -0.1522,  3.1546]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 51
	action: tensor([[-2.5939, -0.2253,  5.9692, -4.3205,  2.5612,  0.3121,  3.3039]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 52
	action: tensor([[-2.6283, -0.3329,  6.1800, -4.4139,  2.7077,  0.1334,  3.4156]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 53
	action: tensor([[-3.4560, -0.7997,  5.8412, -3.8068,  2.9263,  0.1475,  3.0709]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 54
	action: tensor([[-2.7430, -0.3373,  6.1800, -4.2952,  2.7202,  0.3727,  2.9524]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 55
	action: tensor([[-2.8831, -0.4376,  6.1800, -3.0319,  2.7272,  0.4945,  3.1822]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 56
	action: tensor([[-2.7079,  0.7511,  6.0450, -3.4813,  2.8937,  0.4255,  3.4695]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 57
	action: tensor([[-3.0690, -0.4475,  5.7626, -3.9742,  2.3556,  0.4246,  3.4320]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 58
	action: tensor([[-3.5643, -0.1754,  6.1800, -3.9284,  3.4399,  0.2539,  2.8455]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 59
	action: tensor([[-2.9268, -0.2716,  6.1800, -4.2896,  3.0598,  0.4497,  3.2400]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 60
	action: tensor([[-3.1294,  0.1817,  6.1446, -3.6013,  2.9860,  0.3924,  2.9329]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 61
	action: tensor([[-3.0476, -0.2527,  6.1800, -4.1667,  3.3693,  0.1530,  3.2682]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 62
	action: tensor([[-3.0070, -0.4864,  6.0025, -4.1438,  2.5458, -0.2063,  3.2653]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 63
	action: tensor([[-2.4444, -0.2237,  6.1800, -4.1924,  2.7647,  0.2039,  3.3468]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 64
	action: tensor([[-2.7512, -0.9421,  6.1800, -3.4575,  2.9311,  0.6712,  3.3600]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 65
	action: tensor([[-3.0884, -0.3982,  6.1800, -4.6323,  2.8555, -0.1853,  2.7211]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 66
	action: tensor([[-2.8636, -0.0670,  6.1800, -3.6058,  2.9909, -0.4628,  3.2650]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 67
	action: tensor([[-2.8607, -0.2428,  6.1800, -3.8290,  3.5113,  0.4277,  2.8110]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 68
	action: tensor([[-2.9041, -0.0978,  6.1226, -3.7486,  2.8773,  0.1708,  3.2714]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 69
	action: tensor([[-2.4133,  0.5424,  6.1800, -4.2864,  3.0230, -0.0580,  3.5219]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 70
	action: tensor([[-2.6926, -0.2113,  5.8170, -3.3692,  2.8297, -0.1469,  3.2163]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 71
	action: tensor([[-3.0952, -0.8707,  6.1800, -4.0148,  3.3414,  0.0084,  3.3491]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 72
	action: tensor([[-2.6218, -0.0172,  6.1800, -3.7758,  2.7674,  0.1928,  3.3883]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 73
	action: tensor([[-2.0913, -0.4063,  5.7688, -3.8922,  3.2690,  0.0071,  3.4754]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 74
	action: tensor([[-3.1794, -0.3951,  5.9055, -3.7226,  2.7732,  0.1057,  3.4498]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 75
	action: tensor([[-2.9342, -0.7993,  6.1800, -4.0580,  2.3830,  0.4938,  3.1708]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 76
	action: tensor([[-3.5615, -0.2997,  6.1800, -3.5023,  2.8034,  0.4078,  3.3307]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 77
	action: tensor([[-3.1668, -0.3649,  6.1705, -3.3499,  2.5650, -0.2179,  3.0080]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 78
	action: tensor([[-2.9630, -0.1501,  6.1800, -4.1018,  3.4468,  0.4411,  2.6808]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 79
	action: tensor([[-2.7208, -0.3516,  6.1800, -3.5877,  2.0938,  0.0216,  3.0398]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 80
	action: tensor([[-2.3006, -0.4704,  6.1800, -3.0817,  2.6644, -0.2149,  4.2322]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 81
	action: tensor([[-3.0245,  0.6717,  6.1800, -3.7051,  3.0996, -0.1727,  2.9283]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 82
	action: tensor([[-2.9993,  0.4271,  6.1800, -3.4674,  2.4285,  0.2568,  2.8683]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 83
	action: tensor([[-2.9179, -0.6340,  5.9338, -4.2105,  3.1915, -0.1126,  2.9496]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 84
	action: tensor([[-2.5514,  0.3211,  5.8394, -3.7594,  3.2056,  0.0593,  3.4925]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 85
	action: tensor([[-2.8568, -0.2092,  5.7397, -4.4482,  2.8362,  0.4748,  3.1744]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 86
	action: tensor([[-3.0779, -0.9536,  6.1800, -4.3103,  2.2467,  0.1301,  3.0394]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 87
	action: tensor([[-3.0278, -0.3000,  6.0177, -4.6194,  2.7934, -0.1877,  3.5105]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 88
	action: tensor([[-2.9272, -0.3036,  5.9938, -4.2709,  2.8411, -0.0529,  3.5994]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 89
	action: tensor([[-3.0458, -0.5550,  6.1800, -3.5399,  2.6174, -0.1142,  3.3041]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 90
	action: tensor([[-2.3287, -0.5064,  6.1800, -3.5467,  2.9160,  0.2452,  3.0973]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 91
	action: tensor([[-3.4323,  0.2158,  6.1800, -3.8276,  3.0472, -0.3599,  2.8297]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 92
	action: tensor([[-3.0152, -0.6118,  6.1800, -3.7628,  2.9934,  0.9690,  3.2435]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 93
	action: tensor([[-3.1082, -0.3480,  6.1800, -3.5730,  3.3222,  0.2066,  3.4259]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 94
	action: tensor([[-2.6636, -0.0583,  5.6220, -3.3658,  1.7484, -0.5051,  2.8474]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 95
	action: tensor([[-3.1112, -0.2838,  6.1800, -3.4165,  3.4039,  0.2796,  3.5527]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 96
	action: tensor([[-3.3471,  0.0299,  6.1800, -4.0040,  2.8288,  0.4445,  2.8370]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 97
	action: tensor([[-3.0944e+00, -1.5356e-03,  5.7362e+00, -4.5147e+00,  3.7788e+00,
          3.3361e-01,  3.1961e+00]], dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 98
	action: tensor([[-3.0164, -0.3732,  6.1800, -3.9182,  3.0098, -0.1307,  2.9734]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 99
	action: tensor([[-3.3986,  0.0341,  6.1800, -4.0430,  2.4073,  0.4891,  3.9028]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 100
	action: tensor([[-2.8739, -0.6931,  6.1800, -4.0797,  3.0201, -0.1326,  3.0975]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 101
	action: tensor([[-2.8550, -0.2850,  6.1154, -3.9304,  2.5125,  0.5832,  3.7885]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 102
	action: tensor([[-3.1830, -0.1744,  6.1800, -4.7012,  2.7646, -0.4586,  3.6436]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 103
	action: tensor([[-2.5477, -0.0376,  6.0031, -4.1675,  2.6109, -0.3398,  3.2430]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 104
	action: tensor([[-3.0255, -0.5455,  6.1800, -3.8384,  2.8735,  0.3872,  3.3391]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 105
	action: tensor([[-1.8276, -0.3182,  5.8872, -3.8799,  3.7729, -0.4363,  2.8981]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 106
	action: tensor([[-3.0251,  0.0130,  6.1800, -3.5182,  2.4879, -0.1014,  3.0374]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 107
	action: tensor([[-3.0392,  0.0396,  6.1800, -4.3648,  2.9565, -0.1541,  2.9992]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 108
	action: tensor([[-2.7643,  0.3296,  5.9015, -4.0873,  1.8516,  0.0827,  2.8490]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 109
	action: tensor([[-2.9394, -0.0294,  6.1800, -4.3791,  3.3420,  0.1147,  3.5174]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 110
	action: tensor([[-3.3003, -0.5824,  5.9748, -4.9119,  3.3644, -0.1185,  2.7206]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 111
	action: tensor([[-3.2561,  0.1424,  6.1800, -4.8311,  2.9049, -0.1759,  3.1722]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 112
	action: tensor([[-2.5362, -0.0842,  5.9634, -4.4249,  2.5370, -0.0424,  3.6367]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 113
	action: tensor([[-3.1352, -0.1914,  5.7142, -4.4062,  2.9466,  0.6446,  3.6558]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 114
	action: tensor([[-3.4675,  0.0462,  5.9299, -4.3258,  2.9367,  0.6108,  2.8910]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 115
	action: tensor([[-3.1061, -0.3230,  6.1800, -4.1936,  2.8416,  0.1579,  3.8275]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 116
	action: tensor([[-3.2168e+00, -1.5644e-03,  5.8412e+00, -4.1363e+00,  2.5891e+00,
          1.4347e-01,  3.4074e+00]], dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 117
	action: tensor([[-3.1955,  0.5450,  6.1800, -3.8584,  2.6525,  0.7062,  3.9753]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 118
	action: tensor([[-2.9035, -0.3817,  6.1800, -4.3984,  3.2359,  0.4840,  3.5285]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 119
	action: tensor([[-3.2137, -0.0087,  6.1800, -4.1215,  2.7398,  0.1384,  2.6188]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 120
	action: tensor([[-3.8067,  0.1999,  6.1800, -3.7616,  2.9872,  0.4218,  3.4105]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 121
	action: tensor([[-3.6191, -0.4269,  6.1631, -3.4494,  3.0368,  0.4207,  3.2084]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 122
	action: tensor([[-3.0583, -0.1702,  6.1800, -3.9206,  3.1792, -0.4107,  2.6891]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 123
	action: tensor([[-2.8204, -0.2253,  6.1800, -3.8641,  2.4911, -0.0089,  3.2434]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 124
	action: tensor([[-2.6382, -0.3502,  6.1800, -4.5999,  2.4376,  0.5231,  3.3091]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 125
	action: tensor([[-2.7111, -0.2751,  6.1800, -4.4897,  2.9842,  0.7111,  3.4860]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 126
	action: tensor([[-2.7943,  0.0977,  6.0074, -4.2950,  2.3692, -0.1337,  3.0749]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 50, step: 127
	action: tensor([[-2.7721, -0.7617,  6.1800, -4.2184,  3.4032,  0.5493,  3.1371]],
       dtype=torch.float64)
	q_value: tensor([[-65.9366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
LOSS epoch 50 actor 125.01204046300494 critic 253.97375028712435 
epoch: 51, step: 0
	action: tensor([[-3.6006,  0.3235,  6.1800, -3.9053,  2.3301,  0.6405,  3.5760]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 1
	action: tensor([[-3.0115, -0.3319,  5.9769, -3.5671,  3.5046,  0.8270,  3.7473]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 2
	action: tensor([[-3.4180, -0.1025,  6.0146, -4.7960,  3.0464,  0.8225,  3.8590]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 3
	action: tensor([[-3.5961, -0.3839,  5.5819, -4.2885,  2.9181,  1.1227,  3.0924]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 4
	action: tensor([[-2.9598,  0.3819,  5.6711, -4.7737,  2.9242, -0.2762,  3.6971]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 5
	action: tensor([[-3.3947,  0.6625,  6.1800, -4.1269,  3.3230,  0.2851,  3.9959]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 6
	action: tensor([[-3.2262, -0.2719,  6.1800, -4.3786,  2.8530,  0.3775,  4.5439]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 7
	action: tensor([[-3.4415, -0.1749,  5.9457, -4.1047,  2.4325, -0.0752,  4.2562]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 8
	action: tensor([[-3.3986, -0.2665,  6.1800, -3.8792,  3.6999, -0.7104,  4.1041]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 9
	action: tensor([[-2.8909, -0.2923,  6.1800, -3.7100,  3.4151,  0.3091,  3.6352]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 10
	action: tensor([[-3.1151, -0.1196,  6.1800, -4.9143,  2.5802,  0.3469,  3.2652]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 11
	action: tensor([[-3.8937, -0.5989,  6.1800, -4.3358,  2.7223,  0.6962,  4.4004]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 12
	action: tensor([[-3.4484,  0.2469,  6.1800, -4.4151,  3.0040, -0.3957,  3.5950]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 13
	action: tensor([[-3.5075, -0.1519,  6.1800, -5.1679,  2.6342,  0.8545,  4.3041]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 14
	action: tensor([[-4.3463e+00, -2.1752e-03,  6.0143e+00, -4.3079e+00,  2.6836e+00,
          1.4178e-01,  4.0703e+00]], dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 15
	action: tensor([[-4.0279,  0.1772,  6.1800, -4.5989,  3.1265, -0.3146,  4.0646]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 16
	action: tensor([[-2.5017,  0.0410,  6.1800, -4.0942,  2.5439,  0.5146,  4.0249]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 17
	action: tensor([[-3.6760,  0.0127,  6.1100, -4.6680,  2.7285,  0.0323,  3.3350]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 18
	action: tensor([[-3.0603, -0.2397,  6.1800, -4.9337,  3.0852, -0.4250,  4.8560]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 19
	action: tensor([[-3.7393,  1.1899,  6.1800, -3.9613,  2.3453, -0.0874,  3.4028]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 20
	action: tensor([[-3.4407,  0.5100,  5.9970, -4.2990,  3.2169,  0.6297,  3.7492]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 21
	action: tensor([[-3.7137,  0.2531,  5.6081, -4.1480,  3.3062,  0.0762,  3.9509]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 22
	action: tensor([[-3.7967,  0.6014,  6.1800, -4.2756,  3.4799,  0.1358,  2.9068]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 23
	action: tensor([[-3.2638, -0.4410,  6.1800, -4.5025,  3.2306,  0.4219,  3.9381]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 24
	action: tensor([[-3.8026, -0.2101,  6.1800, -4.5435,  2.8115, -0.9307,  3.9690]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 25
	action: tensor([[-3.4704,  0.0823,  5.7963, -4.7787,  2.9860,  0.2070,  4.2821]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 26
	action: tensor([[-3.3510, -0.0127,  6.1800, -4.5693,  3.5792,  0.1699,  3.7505]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 27
	action: tensor([[-3.7567,  0.2060,  6.0447, -4.3647,  2.6269,  0.2501,  3.9492]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 28
	action: tensor([[-3.8083, -0.3014,  6.1800, -3.9286,  3.0780,  0.3167,  3.9411]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 29
	action: tensor([[-2.7091,  0.4027,  6.1800, -5.1384,  3.8243,  0.3720,  3.7934]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 30
	action: tensor([[-2.9802,  0.1936,  6.1800, -4.2831,  3.1700,  0.3239,  4.2263]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 31
	action: tensor([[-3.7975, -0.5010,  5.8267, -4.0929,  2.8216, -0.2632,  3.8679]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 32
	action: tensor([[-3.1196,  0.4236,  5.5813, -4.9038,  3.0342,  0.6578,  3.8936]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 33
	action: tensor([[-3.1296,  0.3015,  6.1800, -3.8391,  3.2568,  0.3603,  4.1607]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 34
	action: tensor([[-3.3141, -0.3020,  6.1626, -4.0256,  3.0167,  0.2992,  4.1271]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 35
	action: tensor([[-3.1557,  0.0523,  6.1800, -3.7936,  2.7764,  0.6452,  4.1315]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 36
	action: tensor([[-3.1070,  0.3455,  6.1551, -4.1170,  2.5576,  0.4957,  3.2781]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 37
	action: tensor([[-2.7722, -0.1557,  6.1800, -4.2403,  3.2536, -0.0214,  3.7762]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 38
	action: tensor([[-3.8532,  0.3331,  5.8822, -4.6769,  2.9511,  0.0305,  3.7203]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 39
	action: tensor([[-3.6545,  0.0650,  6.1800, -4.8635,  3.1307,  0.0781,  3.9004]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 40
	action: tensor([[-3.4175, -0.3035,  6.1800, -4.0730,  2.7669,  1.0350,  3.9066]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 41
	action: tensor([[-3.3441, -0.0444,  5.8205, -4.5362,  2.8456,  0.4297,  3.3608]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 42
	action: tensor([[-3.4449, -0.3733,  6.1800, -4.3049,  2.9987, -0.1066,  4.3175]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 43
	action: tensor([[-3.4988, -0.0629,  6.1800, -4.2986,  2.7062,  0.9050,  3.9701]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 44
	action: tensor([[-3.4798, -0.1951,  6.1800, -3.8080,  2.7809,  0.5739,  4.0952]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 45
	action: tensor([[-3.6512, -0.2903,  6.1800, -4.4007,  2.3130, -0.1408,  3.7770]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 46
	action: tensor([[-3.7080, -0.3441,  6.1800, -4.4818,  3.1121,  0.4822,  3.5307]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 47
	action: tensor([[-2.7925, -0.3373,  6.0367, -4.4052,  2.5789,  0.1514,  3.6497]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 48
	action: tensor([[-3.0103,  0.1191,  5.8834, -4.1549,  2.8239,  1.1587,  3.5329]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 49
	action: tensor([[-3.0951,  0.3614,  6.1800, -4.6891,  2.6499,  0.0332,  3.7073]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 50
	action: tensor([[-3.5931,  0.0856,  5.8456, -4.7167,  3.0670, -0.2600,  4.0911]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 51
	action: tensor([[-3.2638, -1.1145,  6.1314, -4.3066,  2.5747, -0.0712,  3.3501]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 52
	action: tensor([[-3.4641, -0.1609,  6.1800, -4.3868,  3.3205,  0.2889,  3.8539]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 53
	action: tensor([[-3.6179, -0.0486,  6.1800, -4.0101,  3.0851, -0.3010,  3.2864]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 54
	action: tensor([[-3.2067,  0.5744,  6.1800, -4.6784,  2.7855, -0.3926,  3.5484]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 55
	action: tensor([[-3.8392, -0.1438,  6.1800, -4.6656,  3.3332,  0.5271,  3.6937]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 56
	action: tensor([[-3.6058, -0.2316,  6.1800, -4.2128,  2.9482, -0.3672,  3.7760]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 57
	action: tensor([[-2.8822, -0.2180,  5.9008, -4.6116,  2.6258,  1.1446,  3.2772]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 58
	action: tensor([[-3.2852, -0.2573,  6.1800, -4.6013,  3.1953, -0.1262,  3.8447]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 59
	action: tensor([[-2.8888, -0.5582,  6.1032, -3.9854,  2.2494,  0.2974,  4.0438]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 60
	action: tensor([[-2.7609, -0.5367,  6.1800, -5.0281,  2.2567,  0.3387,  3.4824]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 61
	action: tensor([[-2.8645,  0.4058,  6.1800, -5.1557,  2.8754, -0.0181,  4.0970]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 62
	action: tensor([[-3.5877,  0.0183,  6.1800, -4.3015,  2.9696,  0.2501,  3.4314]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 63
	action: tensor([[-3.8867,  0.3461,  6.1800, -4.2701,  3.2219,  0.0230,  4.0724]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 64
	action: tensor([[-2.7781,  0.3431,  6.1800, -4.2402,  3.1063,  0.1407,  3.3929]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 65
	action: tensor([[-3.4920,  0.0839,  6.1738, -4.6464,  2.3533, -0.2390,  3.4850]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 66
	action: tensor([[-3.6475, -0.0727,  6.1361, -5.1245,  2.6144,  0.2455,  3.0607]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 67
	action: tensor([[-3.4921, -0.8241,  5.7831, -3.8536,  3.3763,  0.3246,  3.2986]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 68
	action: tensor([[-3.0767, -0.6028,  6.1800, -3.5497,  3.4378,  0.0192,  3.6082]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 69
	action: tensor([[-3.1038,  0.4073,  5.6738, -4.6970,  2.9982, -0.2257,  4.1551]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 70
	action: tensor([[-3.7470,  0.3643,  6.1800, -4.6754,  3.1702,  0.4156,  3.9030]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 71
	action: tensor([[-4.0024, -0.6895,  6.0703, -3.8741,  3.0060,  1.0557,  3.9031]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 72
	action: tensor([[-4.5846, -0.4345,  5.9731, -3.7149,  2.6814,  0.9829,  3.5542]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 73
	action: tensor([[-3.8200, -0.1665,  6.1800, -4.5387,  3.0288,  0.4077,  4.0890]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 74
	action: tensor([[-3.0610, -0.2162,  5.5726, -3.9130,  3.2322,  0.2572,  3.7408]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 75
	action: tensor([[-3.3136, -0.4250,  6.0594, -4.2149,  2.6025,  0.2293,  4.1209]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 76
	action: tensor([[-3.3014,  0.3385,  6.1323, -4.3110,  3.2003,  0.3466,  3.5266]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 77
	action: tensor([[-3.6543, -0.4609,  5.9808, -4.4337,  2.2765,  0.9151,  3.6945]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 78
	action: tensor([[-3.6465,  0.7347,  6.1800, -4.1987,  3.0954,  0.0422,  3.9749]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 79
	action: tensor([[-3.5437, -0.0778,  5.5700, -4.8031,  2.2784,  0.6871,  3.9557]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 80
	action: tensor([[-3.4187,  0.0557,  6.1800, -3.8731,  2.1358,  0.0700,  3.8814]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 81
	action: tensor([[-3.3216, -0.1346,  5.5231, -4.9009,  3.2788, -0.2069,  4.3199]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 82
	action: tensor([[-2.7798, -0.4780,  6.1800, -4.6518,  3.2260,  0.0131,  4.0309]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 83
	action: tensor([[-3.4429, -0.4296,  6.1800, -4.4842,  2.3496,  0.0108,  3.3097]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 84
	action: tensor([[-3.2988,  0.2637,  6.1800, -4.9514,  3.1702,  0.5501,  3.6913]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 85
	action: tensor([[-3.1708, -0.1139,  5.4351, -4.0559,  2.0865,  0.5475,  3.9833]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 86
	action: tensor([[-3.2808,  0.1359,  6.1490, -4.0614,  3.1444,  0.1960,  3.6624]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 87
	action: tensor([[-3.5759,  0.5245,  6.1800, -3.9960,  2.2604,  0.1253,  3.9796]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 88
	action: tensor([[-3.5630,  0.1588,  6.1800, -4.3951,  2.6365,  0.1662,  3.7503]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 89
	action: tensor([[-3.5789, -0.1617,  6.1800, -4.4535,  2.8853, -0.1654,  4.0132]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 90
	action: tensor([[-3.6197,  0.3433,  6.1800, -4.4690,  3.2481, -0.2571,  3.6556]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 91
	action: tensor([[-3.4812,  0.2419,  6.1800, -3.9793,  3.7213,  0.6408,  4.0899]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 92
	action: tensor([[-3.1640,  0.0206,  6.1800, -4.4279,  3.2350,  0.2340,  3.5013]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 93
	action: tensor([[-3.6504, -0.3300,  5.7518, -4.1852,  2.5839, -0.0910,  3.6881]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 94
	action: tensor([[-3.1366, -0.3452,  6.1800, -4.4399,  2.6073,  0.9007,  3.5849]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 95
	action: tensor([[-3.5615,  0.3973,  6.1800, -4.1128,  3.2536,  0.5318,  3.8983]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 96
	action: tensor([[-3.7583,  0.6820,  6.1800, -4.5097,  3.1586, -0.4706,  3.8523]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 97
	action: tensor([[-3.4632, -0.1730,  5.8228, -4.4575,  2.8565,  0.8131,  4.2834]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 98
	action: tensor([[-3.6276,  0.5131,  6.1800, -3.6785,  3.1137,  0.4382,  3.6350]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 99
	action: tensor([[-3.6554, -0.3930,  6.1800, -4.1847,  2.8054,  0.4637,  3.4721]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 100
	action: tensor([[-3.3677,  0.3232,  6.0442, -4.0210,  2.8564, -0.0427,  4.2239]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 101
	action: tensor([[-3.3250,  0.2038,  6.1800, -4.1501,  3.0517,  0.3583,  3.9312]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 102
	action: tensor([[-3.7062, -0.3735,  5.8492, -4.2705,  3.2031,  0.5724,  4.0994]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 103
	action: tensor([[-3.6614, -0.1801,  6.1800, -4.9865,  2.7744,  0.4002,  4.0081]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 104
	action: tensor([[-3.8935, -0.0638,  6.1800, -4.2936,  3.4126, -0.2734,  3.6550]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 105
	action: tensor([[-3.0802,  0.4456,  6.1800, -4.5495,  3.0480,  0.2069,  3.0630]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 106
	action: tensor([[-4.4149,  0.6019,  5.7559, -4.4934,  3.5225,  0.0974,  3.3044]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 107
	action: tensor([[-3.5694,  0.3164,  6.1800, -4.4477,  2.8049, -0.4655,  3.8660]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 108
	action: tensor([[-3.1032,  0.2522,  5.8416, -4.5451,  2.4578,  0.1447,  3.8391]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 109
	action: tensor([[-3.1717, -0.2980,  5.9061, -4.5064,  3.0425,  0.1625,  3.5520]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 110
	action: tensor([[-3.6615,  0.0731,  6.1800, -4.7991,  2.8720, -0.0526,  3.5599]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 111
	action: tensor([[-3.2712,  0.3745,  6.1800, -4.4356,  2.8454,  0.5407,  3.9687]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 112
	action: tensor([[-3.3078, -0.3869,  6.1624, -4.2572,  2.1982,  0.3469,  3.9016]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 113
	action: tensor([[-2.7471, -0.5951,  6.1800, -3.9160,  3.2529,  0.4779,  4.4941]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 114
	action: tensor([[-3.5934,  0.6562,  5.9890, -5.0483,  3.2326,  0.2560,  3.4634]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 115
	action: tensor([[-3.5050,  0.2910,  6.0065, -4.2321,  2.7116,  0.6967,  3.4579]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 116
	action: tensor([[-3.6962, -0.3510,  6.1800, -4.5918,  3.4470,  0.2787,  3.4853]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 117
	action: tensor([[-3.5654, -0.2521,  6.1800, -5.0396,  3.0701, -0.2386,  3.4996]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 118
	action: tensor([[-3.3121, -0.1394,  6.1800, -4.4968,  2.4638,  0.3005,  3.5741]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 119
	action: tensor([[-3.8831,  0.3058,  6.1800, -4.1825,  3.2456,  0.8111,  3.8132]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 120
	action: tensor([[-3.0807, -0.0891,  6.1800, -4.6747,  3.2194,  0.1104,  3.6179]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 121
	action: tensor([[-3.3101, -0.2932,  6.1800, -4.0340,  3.5221, -0.1843,  3.1152]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 122
	action: tensor([[-3.7559, -0.4015,  5.9450, -4.9554,  3.1797,  0.8466,  3.6194]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 123
	action: tensor([[-3.0526,  0.2789,  5.8800, -4.6631,  3.1086,  0.5194,  3.2334]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 124
	action: tensor([[-3.2096e+00,  1.4775e-01,  5.7234e+00, -4.5620e+00,  2.6729e+00,
         -3.1732e-03,  3.6504e+00]], dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 125
	action: tensor([[-3.5554,  0.4169,  5.8330, -3.7751,  2.9873, -0.0535,  3.2315]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 126
	action: tensor([[-3.1171,  0.2681,  6.1800, -4.1440,  2.4531,  0.1141,  3.8402]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 51, step: 127
	action: tensor([[-3.5462,  0.1896,  6.1800, -4.6645,  3.2452, -0.0790,  3.5896]],
       dtype=torch.float64)
	q_value: tensor([[-61.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
LOSS epoch 51 actor 71.01169126915794 critic 142.10237592583618 
epoch: 52, step: 0
	action: tensor([[-3.6912, -0.1434,  6.1800, -5.0985,  2.7772,  0.3421,  4.1020]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 1
	action: tensor([[-3.4602, -0.0512,  6.1800, -5.2456,  2.3624,  0.6743,  4.0605]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 2
	action: tensor([[-3.2249, -0.2926,  6.1800, -4.2301,  3.3236,  0.1764,  3.9612]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 3
	action: tensor([[-3.8543, -0.3501,  5.7990, -4.5452,  2.9197, -0.0079,  3.6955]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 4
	action: tensor([[-3.2654,  0.5372,  5.2757, -4.9216,  2.9148,  0.2727,  4.3198]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 5
	action: tensor([[-3.5511,  0.6581,  5.6457, -4.5952,  2.3839,  0.1945,  4.7333]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 6
	action: tensor([[-3.7091, -0.2146,  6.1800, -3.5873,  2.8564,  0.3915,  4.3992]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 7
	action: tensor([[-4.0338,  0.3778,  6.1800, -4.7507,  2.5055, -0.1412,  3.9421]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 8
	action: tensor([[-3.5898,  0.2028,  6.0375, -4.6564,  2.9980,  0.5730,  4.8299]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 9
	action: tensor([[-3.1402, -0.2060,  5.6848, -4.3695,  3.1212,  0.7266,  3.9075]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 10
	action: tensor([[-3.6005, -0.4742,  5.8166, -5.0648,  2.5169,  0.6729,  3.8780]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 11
	action: tensor([[-3.0004, -0.3134,  6.0919, -4.3935,  3.3972,  0.3286,  4.7453]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 12
	action: tensor([[-3.7855,  0.2499,  6.1800, -4.3943,  2.4891,  0.2667,  3.5069]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 13
	action: tensor([[-3.8521, -0.0188,  6.1800, -3.9944,  3.4419,  0.2868,  4.5173]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 14
	action: tensor([[-4.1216, -0.4501,  6.1800, -3.5757,  2.6267,  0.0838,  3.3297]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 15
	action: tensor([[-3.1752,  0.2555,  6.0168, -5.0311,  2.9368,  0.1403,  4.3050]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 16
	action: tensor([[-3.3759,  0.1465,  6.1800, -5.0165,  2.8910,  0.7438,  3.4071]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 17
	action: tensor([[-3.3875,  0.0194,  6.1800, -4.3155,  3.2877,  1.0393,  3.7060]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 18
	action: tensor([[-3.6673,  0.0290,  5.9931, -4.7040,  2.5662,  0.1971,  4.1730]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 19
	action: tensor([[-4.1317,  0.1829,  6.1800, -4.6164,  3.0065, -0.2681,  3.7300]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 20
	action: tensor([[-3.6726,  0.1780,  5.9682, -4.8072,  3.0446, -0.0078,  4.5968]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 21
	action: tensor([[-3.9091,  0.5857,  6.1800, -4.6661,  3.3844,  0.1166,  3.6774]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 22
	action: tensor([[-2.8443,  0.0304,  6.1800, -4.5549,  2.5489,  0.1435,  4.6665]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 23
	action: tensor([[-3.5391,  0.0848,  6.1800, -3.8360,  2.6937, -0.4346,  3.5149]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 24
	action: tensor([[-3.7157,  0.5764,  5.7193, -5.5807,  2.8065,  0.7098,  3.7310]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 25
	action: tensor([[-3.8321,  0.2220,  6.1800, -4.6209,  2.9911,  0.6019,  3.9167]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 26
	action: tensor([[-2.9661, -0.1216,  6.1800, -4.6085,  2.5428,  0.2611,  4.2298]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 27
	action: tensor([[-2.7436, -0.3394,  6.1800, -4.9705,  3.6310, -0.5363,  3.9360]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 28
	action: tensor([[-4.7514, -0.3777,  6.0271, -4.6675,  2.7832,  0.1685,  4.4044]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 29
	action: tensor([[-2.9811, -0.2976,  5.9686, -5.4447,  3.2729, -0.0968,  4.5858]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 30
	action: tensor([[-3.2361,  0.7535,  5.9790, -4.7767,  2.8840, -0.0360,  3.4942]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 31
	action: tensor([[-3.2798,  0.7749,  5.9066, -4.0448,  2.9929,  0.3040,  3.5972]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 32
	action: tensor([[-3.4610,  0.4261,  5.7280, -4.6645,  2.9268,  0.6909,  4.0854]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 33
	action: tensor([[-3.7024,  0.3221,  6.1800, -4.1815,  3.1577,  0.1987,  4.3037]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 34
	action: tensor([[-3.7439,  0.5905,  6.1800, -4.6160,  2.8875, -0.4078,  3.9328]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 35
	action: tensor([[-3.3120,  0.1770,  6.1329, -4.5139,  2.2649,  0.7552,  3.9090]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 36
	action: tensor([[-4.0330,  0.1953,  6.1800, -4.6265,  2.9970, -0.1438,  3.7025]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 37
	action: tensor([[-3.5869, -0.0166,  6.1800, -5.4074,  3.3693,  0.4480,  3.2229]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 38
	action: tensor([[-3.6556, -0.0076,  6.1800, -4.8766,  3.0124,  0.4253,  4.1277]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 39
	action: tensor([[-3.5305, -0.3472,  6.1800, -4.6104,  2.1541,  0.9329,  4.0550]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 40
	action: tensor([[-3.5596, -0.1685,  6.0050, -4.3913,  2.7008,  0.8267,  4.3681]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 41
	action: tensor([[-3.8201, -0.6511,  5.8415, -4.3478,  2.7372, -0.3176,  4.0276]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 42
	action: tensor([[-4.1790,  0.6165,  6.1800, -4.4662,  3.1083,  0.3553,  3.7407]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 43
	action: tensor([[-4.2491,  0.2469,  6.1800, -5.3843,  2.9051, -0.0850,  3.9158]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 44
	action: tensor([[-3.1909,  0.6735,  6.0836, -4.9379,  2.6935,  0.4859,  4.0232]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 45
	action: tensor([[-3.7941, -0.4128,  6.1800, -4.7384,  2.8798,  0.3055,  4.1025]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 46
	action: tensor([[-3.9448,  0.1007,  4.9438, -4.3736,  2.4989,  0.3998,  3.8715]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 47
	action: tensor([[-3.5083e+00,  5.2979e-03,  6.1800e+00, -4.7619e+00,  3.0910e+00,
         -3.6732e-01,  3.8265e+00]], dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 48
	action: tensor([[-3.9118,  0.6887,  5.8290, -4.2769,  2.9122,  0.1760,  3.5494]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 49
	action: tensor([[-3.3637, -0.4237,  6.1800, -4.7865,  2.7216, -0.0834,  4.2874]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 50
	action: tensor([[-3.2357,  0.1537,  6.1800, -4.7989,  4.0588,  0.5168,  4.2634]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 51
	action: tensor([[-3.8743,  0.1418,  6.1800, -4.6546,  3.1178,  0.7293,  3.8894]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 52
	action: tensor([[-3.4094, -0.1228,  6.1800, -4.6474,  3.3773,  0.2111,  4.1263]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 53
	action: tensor([[-3.7471e+00,  4.5967e-01,  6.1800e+00, -4.5553e+00,  2.8591e+00,
          3.5968e-04,  3.9709e+00]], dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 54
	action: tensor([[-3.8432,  0.0883,  5.5548, -5.2759,  2.6216,  0.2804,  3.7069]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 55
	action: tensor([[-3.7187,  0.0125,  5.9237, -4.7656,  2.6745,  0.4708,  3.4833]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 56
	action: tensor([[-3.7485,  0.8937,  5.7727, -4.5331,  2.3894,  0.3417,  4.0430]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 57
	action: tensor([[-3.9471,  0.7810,  6.1800, -4.9245,  3.1848,  0.6223,  4.2656]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 58
	action: tensor([[-4.0200,  0.4205,  6.1176, -4.2009,  2.8260,  0.6525,  3.8864]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 59
	action: tensor([[-3.7847,  0.5798,  5.8922, -4.2395,  2.7281,  0.8142,  4.0902]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 60
	action: tensor([[-3.8935,  0.4057,  6.1800, -4.2587,  2.5947, -0.3488,  3.7778]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 61
	action: tensor([[-3.3793,  0.3047,  4.8697, -4.5479,  3.1066,  0.3899,  5.1329]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 62
	action: tensor([[-3.9212, -0.3232,  5.9851, -4.5029,  2.7223, -0.2433,  4.4115]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 63
	action: tensor([[-3.4776e+00, -5.4575e-04,  6.1800e+00, -4.7431e+00,  3.7134e+00,
         -1.3452e-01,  4.0084e+00]], dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 64
	action: tensor([[-4.1187,  1.0973,  5.9734, -5.2864,  2.8982,  0.1378,  3.7362]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 65
	action: tensor([[-3.5410,  1.0003,  6.1800, -4.8635,  2.5117,  0.2686,  3.9266]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 66
	action: tensor([[-4.2077, -0.1829,  6.1800, -5.4423,  3.3196,  0.5206,  4.3274]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 67
	action: tensor([[-4.1069,  0.3196,  6.1800, -4.0394,  2.8671,  0.6807,  4.8381]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 68
	action: tensor([[-3.6759, -0.6891,  6.1800, -4.8339,  2.3873,  0.7694,  3.5635]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 69
	action: tensor([[-3.7465,  0.0903,  6.1800, -4.5178,  2.4851,  0.1901,  4.8188]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 70
	action: tensor([[-3.6243, -0.2188,  6.0583, -4.2291,  3.3561,  0.5451,  4.6579]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 71
	action: tensor([[-3.6465,  0.4262,  5.9550, -4.6860,  3.0879,  0.6317,  4.0103]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 72
	action: tensor([[-4.0708, -0.1456,  6.1800, -5.0460,  3.0254,  0.1436,  3.8298]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 73
	action: tensor([[-3.0669,  0.4777,  6.1800, -4.5181,  1.9587,  0.5737,  3.6447]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 74
	action: tensor([[-3.4977, -0.0611,  6.1800, -4.1085,  3.3971,  0.6191,  4.5497]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 75
	action: tensor([[-3.2455, -0.1664,  6.1800, -5.0877,  3.3545,  0.4557,  4.3383]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 76
	action: tensor([[-3.4021, -0.2806,  5.8585, -4.1932,  3.2651,  0.4173,  3.2842]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 77
	action: tensor([[-4.3930e+00, -2.2720e-01,  6.1800e+00, -5.1202e+00,  2.3191e+00,
          4.7778e-03,  3.1981e+00]], dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 78
	action: tensor([[-3.5324,  0.3350,  6.1800, -4.2463,  2.9308,  0.5252,  3.7467]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 79
	action: tensor([[-3.5383, -0.3157,  6.1800, -4.0702,  3.1500,  0.4396,  4.3590]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 80
	action: tensor([[-4.3217,  0.5028,  6.0149, -4.8047,  2.9585,  0.4142,  3.3635]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 81
	action: tensor([[-3.2112, -0.1248,  6.1800, -4.2623,  2.7953,  0.1870,  3.6444]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 82
	action: tensor([[-3.9390,  0.5790,  5.9021, -4.3384,  2.6947,  0.2777,  3.8902]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 83
	action: tensor([[-2.7734,  0.4958,  5.6521, -5.0734,  3.2710, -0.2567,  4.2577]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 84
	action: tensor([[-3.7761,  0.4650,  6.1800, -4.7731,  2.6391, -0.2595,  3.8372]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 85
	action: tensor([[-3.4960,  0.5348,  6.1800, -4.6059,  3.3706, -0.0469,  3.7970]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 86
	action: tensor([[-4.1258e+00, -7.1538e-04,  5.7969e+00, -4.9206e+00,  3.2905e+00,
         -1.8269e-01,  3.8690e+00]], dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 87
	action: tensor([[-3.8260, -0.4383,  6.1800, -4.3021,  2.9573,  0.3974,  3.7646]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 88
	action: tensor([[-3.2741,  0.7617,  6.1800, -4.1120,  3.0334,  0.1960,  4.0574]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 89
	action: tensor([[-3.8343,  0.8571,  6.1800, -4.0083,  2.6115, -0.2394,  3.9211]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 90
	action: tensor([[-3.6160,  0.0851,  6.1800, -4.4701,  3.2312, -0.0081,  3.3910]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 91
	action: tensor([[-3.4345,  0.6998,  6.0501, -4.8361,  2.9667,  0.1756,  4.1802]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 92
	action: tensor([[-3.6278, -0.1670,  6.1800, -4.3450,  3.2314, -0.4622,  4.5206]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 93
	action: tensor([[-3.6519e+00,  4.6622e-03,  6.1369e+00, -4.6174e+00,  2.5502e+00,
         -9.9475e-02,  4.8796e+00]], dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 94
	action: tensor([[-3.7168,  0.0249,  6.1800, -4.6972,  2.8627, -0.4799,  3.9879]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 95
	action: tensor([[-4.3364,  0.1899,  5.8393, -4.1757,  2.5760,  0.3797,  3.9203]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 96
	action: tensor([[-4.0991,  0.5270,  6.1800, -4.8028,  2.8502, -0.5121,  3.3737]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 97
	action: tensor([[-3.6836, -0.3355,  6.1800, -4.8085,  3.6656, -0.3606,  4.0395]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 98
	action: tensor([[-3.4500,  0.2023,  6.1155, -4.4171,  2.9947,  0.4164,  4.2690]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 99
	action: tensor([[-3.6367,  0.1704,  6.1684, -4.5570,  2.9323,  0.4872,  4.4956]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 100
	action: tensor([[-4.0162,  0.4474,  6.1800, -3.4935,  3.0504,  0.0208,  4.1406]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 101
	action: tensor([[-4.3822,  0.7553,  6.1800, -4.7210,  2.9479,  0.6723,  3.6243]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 102
	action: tensor([[-3.5570,  0.4436,  6.0581, -4.8068,  2.7927,  0.4329,  4.1400]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 103
	action: tensor([[-3.6763,  0.1622,  6.1800, -4.7283,  2.2228,  0.2783,  3.4010]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 104
	action: tensor([[-3.6535,  0.7066,  6.1800, -4.7245,  3.5555,  0.4182,  4.5743]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 105
	action: tensor([[-3.4608, -0.1501,  6.1800, -4.8355,  2.9246,  0.2996,  3.9095]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 106
	action: tensor([[-3.5632,  0.0603,  6.1800, -4.7068,  2.4754,  0.4838,  3.2994]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 107
	action: tensor([[-3.7646, -0.2253,  5.3509, -4.9324,  2.4466,  0.7756,  4.0433]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 108
	action: tensor([[-3.4525, -0.1686,  6.1800, -5.3501,  3.4605,  0.3390,  3.6411]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 109
	action: tensor([[-3.7246,  0.0328,  6.1800, -4.3483,  2.9703, -0.3490,  3.4184]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 110
	action: tensor([[-3.8703,  0.2030,  6.1800, -4.9441,  2.9550,  0.3461,  4.1551]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 111
	action: tensor([[-3.6745, -0.2460,  5.5590, -4.6700,  2.9154,  0.8199,  3.9238]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 112
	action: tensor([[-3.4372,  0.1670,  6.1800, -4.8120,  3.0450, -0.2010,  4.1531]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 113
	action: tensor([[-3.6483,  0.5243,  6.1800, -4.3516,  2.8268,  0.6315,  4.2958]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 114
	action: tensor([[-3.8599,  0.0501,  6.1800, -4.9451,  3.2053,  0.6740,  3.3421]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 115
	action: tensor([[-3.6264, -0.3024,  6.1800, -4.0764,  2.4842,  0.0327,  3.7735]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 116
	action: tensor([[-4.2174, -0.2308,  5.7944, -4.2429,  2.9051, -0.0100,  3.3976]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 117
	action: tensor([[-4.2207,  0.3633,  6.1800, -4.8542,  3.1540,  0.5991,  3.8037]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 118
	action: tensor([[-3.4870, -0.9296,  6.1800, -4.1840,  3.3332,  0.0704,  4.3193]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 119
	action: tensor([[-3.4295e+00, -1.3446e-03,  6.1800e+00, -4.7428e+00,  3.4565e+00,
         -2.4798e-02,  3.7042e+00]], dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 120
	action: tensor([[-3.7546,  0.2848,  5.8588, -5.0485,  3.4135,  0.2744,  3.9375]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 121
	action: tensor([[-3.5178, -0.2845,  6.0716, -3.9335,  2.4400,  0.2156,  4.2635]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 122
	action: tensor([[-3.5617,  0.2895,  6.1800, -3.4740,  2.8003,  0.1016,  3.9453]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 123
	action: tensor([[-2.8439, -0.2460,  6.1800, -4.1591,  2.9376,  0.4707,  4.1726]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 124
	action: tensor([[-3.3359,  0.1754,  5.6142, -4.5309,  3.1958,  0.6077,  4.1458]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 125
	action: tensor([[-4.5947,  0.4955,  5.9396, -5.0115,  2.8838,  0.3756,  4.3731]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 126
	action: tensor([[-3.4621,  0.4277,  6.1800, -5.0781,  2.6126,  0.0858,  3.8720]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 52, step: 127
	action: tensor([[-4.2655,  0.2209,  6.1800, -4.6962,  2.6313,  0.4067,  4.2319]],
       dtype=torch.float64)
	q_value: tensor([[-40.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
LOSS epoch 52 actor 40.99031546478189 critic 82.05962431708407 
epoch: 53, step: 0
	action: tensor([[-3.6184,  0.3013,  6.1245, -5.0248,  2.9284, -0.0073,  3.8414]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 1
	action: tensor([[-3.8325, -0.1401,  5.6288, -4.8394,  2.4244,  0.3284,  4.2186]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 2
	action: tensor([[-2.9775,  0.5001,  6.1800, -5.4494,  2.5279,  0.0219,  4.1697]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 3
	action: tensor([[-4.3593,  0.6245,  6.1800, -4.8925,  3.1198,  0.4441,  3.9256]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 4
	action: tensor([[-3.5285,  0.6487,  6.1800, -4.5733,  3.2636, -0.3581,  4.5455]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 5
	action: tensor([[-3.3231,  0.4475,  5.9822, -5.1855,  3.1500,  1.1598,  4.3100]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 6
	action: tensor([[-3.6794,  0.1210,  6.1800, -4.3956,  2.9090,  0.3680,  3.9947]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 7
	action: tensor([[-3.7866,  0.2660,  5.8154, -5.2383,  3.2813,  0.7022,  4.0666]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 8
	action: tensor([[-3.7964,  0.0781,  5.9182, -4.8745,  2.9909,  0.7770,  3.5220]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 9
	action: tensor([[-3.6339,  0.4446,  5.7840, -4.6665,  2.5184,  0.1845,  4.7120]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 10
	action: tensor([[-3.5316,  0.2907,  5.9742, -4.6822,  3.2593, -0.4489,  4.1558]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 11
	action: tensor([[-4.2184,  0.4974,  6.1800, -4.3421,  3.2211,  0.4470,  4.0170]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 12
	action: tensor([[-4.0469,  0.0565,  6.1800, -5.5615,  2.7611,  0.6294,  4.5327]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 13
	action: tensor([[-3.7612,  0.0147,  6.1800, -4.5798,  3.1717, -0.3995,  4.1463]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 14
	action: tensor([[-3.2383,  0.5767,  6.1025, -4.2417,  2.4750,  0.1630,  4.1197]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 15
	action: tensor([[-3.7255,  0.4187,  6.1800, -4.4801,  2.4486,  0.5212,  4.5183]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 16
	action: tensor([[-3.4635,  0.4242,  5.9551, -5.2537,  3.0579, -0.1599,  3.8014]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 17
	action: tensor([[-4.2900,  0.8700,  6.0119, -4.9147,  2.1050, -0.1848,  3.9162]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 18
	action: tensor([[-4.0492,  0.3406,  6.1800, -5.0128,  3.2105,  0.3620,  4.4751]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 19
	action: tensor([[-3.3855, -0.2034,  6.1800, -5.0003,  2.9827,  0.4976,  4.0664]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 20
	action: tensor([[-3.8679,  0.1677,  6.1800, -4.7957,  3.1740, -0.0092,  4.7482]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 21
	action: tensor([[-4.0468,  0.1205,  6.1800, -5.2145,  2.5667,  0.5475,  4.3764]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 22
	action: tensor([[-3.5534,  0.1365,  6.1800, -4.2489,  2.9267,  0.4822,  4.8018]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 23
	action: tensor([[-3.5525, -0.0496,  6.1800, -4.5018,  2.9966,  0.1624,  4.0618]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 24
	action: tensor([[-3.4406, -0.1867,  6.1800, -4.7018,  3.2745,  0.4865,  4.5123]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 25
	action: tensor([[-3.3408,  0.5500,  5.6865, -4.5219,  2.9702,  0.3523,  3.5571]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 26
	action: tensor([[-3.8705,  0.1352,  6.1800, -4.7139,  3.3368, -0.0135,  3.6876]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 27
	action: tensor([[-3.7071,  0.0898,  6.1800, -4.9519,  2.9262,  0.2237,  4.1335]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 28
	action: tensor([[-3.8502,  0.0415,  5.7232, -4.7310,  2.5262,  0.0131,  3.6801]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 29
	action: tensor([[-3.1707,  0.1627,  5.6154, -4.1753,  2.4445,  0.9083,  3.9522]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 30
	action: tensor([[-4.3289,  0.4023,  6.1800, -4.1307,  3.0096,  0.7884,  3.9013]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 31
	action: tensor([[-3.7392,  0.0617,  6.1800, -5.1790,  2.9778,  0.2438,  4.6218]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 32
	action: tensor([[-3.8498, -0.5293,  6.0907, -4.7514,  3.2907, -0.0771,  4.6222]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 33
	action: tensor([[-3.7599e+00,  5.2952e-03,  6.0329e+00, -5.1741e+00,  3.3626e+00,
          7.9769e-01,  3.8762e+00]], dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 34
	action: tensor([[-3.5191, -0.3669,  6.1800, -4.4886,  3.1519,  0.6847,  4.1037]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 35
	action: tensor([[-3.9733, -0.2670,  6.1800, -4.9830,  2.9330,  0.1020,  4.0808]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 36
	action: tensor([[-4.2017, -0.0689,  6.1800, -5.1181,  3.0519,  0.5853,  3.8497]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 37
	action: tensor([[-3.6521,  0.1241,  6.1800, -4.9781,  3.2291,  0.5611,  4.4222]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 38
	action: tensor([[-3.6861,  0.0235,  5.7076, -4.4557,  3.5467,  0.3568,  4.2878]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 39
	action: tensor([[-3.9310, -0.5176,  6.1800, -4.4324,  3.0174,  0.0081,  3.9432]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 40
	action: tensor([[-4.0817,  0.4014,  6.1800, -4.7696,  2.6704,  0.3494,  4.6334]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 41
	action: tensor([[-3.9841,  0.5952,  5.6880, -5.3370,  3.2122,  0.6081,  3.8611]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 42
	action: tensor([[-3.2686,  0.3789,  6.1800, -4.7356,  2.5414,  0.4404,  4.0022]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 43
	action: tensor([[-4.0382,  0.0288,  6.1800, -4.5115,  3.1229,  0.2327,  4.0307]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 44
	action: tensor([[-2.7256,  0.3170,  4.9906, -5.0219,  2.6871,  0.6172,  4.4253]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 45
	action: tensor([[-3.5775,  0.4896,  6.1800, -4.5574,  3.3200,  0.6492,  4.3075]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 46
	action: tensor([[-3.3456,  0.4474,  5.7952, -4.2951,  2.4762, -0.0138,  3.8402]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 47
	action: tensor([[-3.8685,  0.0432,  6.1800, -4.9905,  2.8298, -0.3286,  4.4162]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 48
	action: tensor([[-3.8268,  0.4729,  6.1800, -4.4058,  2.7144,  0.8437,  3.9576]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 49
	action: tensor([[-3.6352,  0.4287,  6.1800, -4.7669,  2.3281, -0.0199,  3.7996]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 50
	action: tensor([[-3.5496,  0.3122,  5.7146, -4.6894,  2.6165,  0.3265,  4.0297]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 51
	action: tensor([[-4.0357,  0.5417,  6.1800, -4.7543,  3.0303,  0.5459,  4.4731]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 52
	action: tensor([[-4.2845,  0.1337,  6.1800, -4.3443,  2.8584,  0.5644,  4.9153]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 53
	action: tensor([[-4.0793, -0.2205,  6.1800, -4.5118,  2.8966,  0.2066,  5.2519]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 54
	action: tensor([[-3.5475, -0.0723,  6.1800, -4.3673,  3.5070,  0.7751,  4.1079]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 55
	action: tensor([[-3.6280,  0.6599,  6.1800, -4.5593,  3.0702,  0.3757,  4.0500]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 56
	action: tensor([[-3.7889, -0.2645,  6.1800, -4.6565,  2.5083, -0.2324,  4.3381]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 57
	action: tensor([[-4.5667,  0.5800,  6.0647, -4.6185,  2.8501, -0.2234,  3.7789]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 58
	action: tensor([[-4.1855,  0.7856,  5.7551, -4.6183,  2.9858,  0.8211,  4.4540]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 59
	action: tensor([[-3.5144, -0.2373,  6.1439, -4.2158,  2.4415, -0.2544,  4.2856]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 60
	action: tensor([[-3.7925, -0.5524,  5.5956, -4.5090,  2.9349,  0.1926,  3.3377]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 61
	action: tensor([[-3.9720,  0.3991,  5.5385, -4.4808,  3.1879, -0.0174,  4.5974]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 62
	action: tensor([[-3.8400, -0.0738,  6.1800, -3.9645,  2.4759,  0.3586,  3.9281]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 63
	action: tensor([[-3.7137,  0.0603,  6.1800, -4.9175,  2.5801,  0.2487,  4.5790]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 64
	action: tensor([[-3.2694,  0.5752,  6.1800, -4.1652,  3.1613,  0.3763,  4.1295]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 65
	action: tensor([[-3.2661,  0.5104,  6.1381, -4.6282,  2.6169,  0.1579,  4.2198]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 66
	action: tensor([[-3.1883, -0.4126,  6.1800, -4.1991,  2.9581,  0.2920,  4.5106]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 67
	action: tensor([[-3.7220e+00, -2.1976e-04,  6.0129e+00, -4.5069e+00,  2.9704e+00,
          5.0579e-03,  3.9553e+00]], dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 68
	action: tensor([[-3.7334,  0.2443,  6.1800, -4.6232,  3.0646, -0.3655,  4.3248]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 69
	action: tensor([[-3.8503, -0.3269,  6.1800, -4.7432,  2.7699, -0.2444,  3.8743]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 70
	action: tensor([[-4.2486,  0.4344,  6.1800, -4.5390,  3.5127,  0.5715,  4.6724]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 71
	action: tensor([[-3.5833, -0.0388,  6.1800, -5.2383,  3.1943,  0.3611,  4.3947]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 72
	action: tensor([[-4.0900,  0.4702,  6.1800, -4.5275,  2.7974,  0.1563,  4.3772]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 73
	action: tensor([[-3.8189, -0.0463,  5.9990, -4.1323,  2.7715, -0.0257,  4.1248]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 74
	action: tensor([[-3.6247, -0.5960,  6.1800, -4.8798,  2.6086,  0.2260,  3.8388]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 75
	action: tensor([[-3.7607, -0.2584,  6.1737, -4.5520,  2.3074,  0.3585,  4.1627]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 76
	action: tensor([[-3.5889,  0.3250,  6.1800, -4.9946,  2.7361, -0.0548,  4.1547]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 77
	action: tensor([[-3.4799,  0.3474,  6.1800, -5.7023,  3.2927,  0.3297,  4.5475]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 78
	action: tensor([[-3.4679,  0.4422,  6.1800, -4.9123,  2.5735,  0.3756,  3.9442]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 79
	action: tensor([[-3.8924, -0.1110,  6.1800, -4.9980,  2.6576,  0.0855,  3.9189]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 80
	action: tensor([[-3.7098, -0.1030,  6.1800, -4.4112,  3.1095,  0.4638,  3.6212]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 81
	action: tensor([[-4.2916,  0.0398,  6.1800, -4.5079,  3.0908,  0.5297,  4.0107]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 82
	action: tensor([[-3.9644, -0.3208,  6.1800, -4.6749,  2.8355,  0.3686,  4.7298]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 83
	action: tensor([[-3.6231,  0.2978,  5.3186, -4.7701,  2.9928,  1.1016,  3.5385]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 84
	action: tensor([[-4.3487, -0.3547,  5.5590, -4.4533,  2.4741, -0.0172,  4.4259]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 85
	action: tensor([[-4.5310,  0.3972,  6.0235, -5.5154,  2.8070,  0.0656,  3.9207]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 86
	action: tensor([[-3.9708,  0.6653,  6.1800, -4.5638,  2.9705, -0.0858,  4.4338]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 87
	action: tensor([[-4.0621,  0.1011,  5.3835, -5.5394,  3.2589,  0.8838,  3.7488]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 88
	action: tensor([[-3.9969,  0.4996,  6.1800, -4.2449,  3.2156,  0.0531,  3.6909]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 89
	action: tensor([[-3.5150, -0.0144,  5.7218, -4.7255,  3.2873,  0.3119,  3.9485]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 90
	action: tensor([[-4.1274,  0.9732,  5.8472, -4.2233,  2.9425,  0.3443,  4.1540]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 91
	action: tensor([[-3.2110,  0.2038,  6.1800, -4.1332,  3.1125,  0.6031,  3.8327]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 92
	action: tensor([[-3.0886,  0.0946,  6.1800, -4.8315,  3.3607,  0.2225,  3.7047]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 93
	action: tensor([[-3.8757,  0.1054,  6.1094, -4.6064,  3.2527,  0.3195,  3.9397]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 94
	action: tensor([[-3.7791,  0.2529,  6.0992, -4.3732,  3.0541,  0.0251,  4.2278]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 95
	action: tensor([[-3.9086,  0.2581,  6.1800, -4.8064,  2.3667,  0.1280,  4.9920]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 96
	action: tensor([[-3.8855, -0.0872,  6.0069, -4.7101,  2.5572, -0.3315,  4.1615]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 97
	action: tensor([[-3.8149, -0.3063,  6.1800, -4.5897,  3.1970,  0.2113,  5.1460]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 98
	action: tensor([[-3.7550, -0.0312,  6.0341, -4.9393,  2.7635,  0.0730,  4.1227]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 99
	action: tensor([[-4.3325,  0.6541,  6.1800, -4.6869,  3.0742, -0.4269,  3.5602]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 100
	action: tensor([[-3.5125,  0.3426,  6.1800, -4.6541,  3.2860,  0.5019,  4.2096]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 101
	action: tensor([[-3.7558, -0.6383,  6.1218, -4.6129,  2.9491,  1.0392,  3.9585]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 102
	action: tensor([[-3.8974,  0.0804,  6.1800, -5.3067,  3.0249,  0.8192,  4.7918]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 103
	action: tensor([[-3.9172, -0.1895,  6.1800, -4.3526,  2.4115,  0.3784,  4.9857]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 104
	action: tensor([[-4.6460, -0.4039,  6.1800, -4.7620,  2.2214,  0.3203,  4.5609]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 105
	action: tensor([[-3.2067, -0.3001,  6.1800, -4.6520,  3.4070,  0.6116,  4.2976]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 106
	action: tensor([[-3.5504,  0.4192,  6.1800, -4.5811,  3.0486,  0.2212,  3.6601]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 107
	action: tensor([[-3.8625,  0.1390,  6.0237, -5.2732,  2.9499,  0.1324,  4.3338]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 108
	action: tensor([[-3.7951,  0.5783,  6.1622, -5.5818,  2.7228, -0.0439,  3.8632]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 109
	action: tensor([[-4.2285, -0.0583,  5.9018, -4.3656,  3.2010, -0.0616,  3.6485]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 110
	action: tensor([[-3.5724,  0.5672,  6.1440, -4.3194,  2.6854,  0.2420,  4.6092]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 111
	action: tensor([[-3.7499,  0.5048,  6.1800, -4.8564,  2.9842,  0.2872,  3.6230]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 112
	action: tensor([[-3.8868,  0.0713,  6.1800, -5.2437,  3.1295, -0.5908,  4.0574]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 113
	action: tensor([[-3.5215,  0.4051,  5.7665, -4.5087,  3.6613, -0.1954,  4.7899]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 114
	action: tensor([[-4.7421, -0.1468,  5.7259, -4.9080,  2.7964,  0.2483,  4.1770]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 115
	action: tensor([[-3.5854,  0.1815,  6.1800, -4.9577,  3.1969,  0.1353,  4.6163]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 116
	action: tensor([[-4.3452,  0.5228,  6.1030, -4.4256,  2.7862, -0.1588,  4.1315]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 117
	action: tensor([[-3.9781, -0.1860,  6.1800, -4.6588,  2.3959,  0.2710,  4.0704]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 118
	action: tensor([[-4.4889, -0.3980,  6.1800, -4.7389,  2.9522,  0.5363,  4.2633]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 119
	action: tensor([[-3.9575,  0.1339,  6.1518, -4.1301,  3.3730,  0.1392,  3.4629]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 120
	action: tensor([[-3.9010,  0.1436,  5.8993, -4.6849,  2.5220,  0.2500,  4.2354]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 121
	action: tensor([[-3.8937,  0.0556,  6.1800, -5.0174,  3.1066, -0.1801,  4.5514]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 122
	action: tensor([[-4.3861,  1.0054,  6.1800, -5.0159,  2.4422,  0.1612,  4.0192]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 123
	action: tensor([[-3.5672,  0.1914,  5.8112, -4.9264,  2.8026,  0.3522,  4.1648]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 124
	action: tensor([[-3.8560,  0.1689,  6.1800, -4.1700,  3.0629,  0.4417,  3.6447]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 125
	action: tensor([[-3.2462,  0.1743,  5.9827, -4.7260,  2.6104,  0.3721,  4.6210]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 126
	action: tensor([[-3.7681,  0.1078,  6.1800, -4.9703,  2.8835,  0.3625,  3.7505]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 53, step: 127
	action: tensor([[-3.8458,  0.3086,  6.1800, -4.3865,  2.9435, -0.0477,  4.2665]],
       dtype=torch.float64)
	q_value: tensor([[-31.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
LOSS epoch 53 actor 163.80145507377898 critic 331.5525795086726 
epoch: 54, step: 0
	action: tensor([[-4.4181,  0.5246,  6.1800, -4.7345,  3.2239,  0.0906,  4.2288]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 1
	action: tensor([[-3.7465,  1.1871,  6.1800, -4.9184,  2.7568,  0.1707,  3.8537]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 2
	action: tensor([[-4.5947, -0.0074,  5.9491, -4.5679,  2.9087,  0.1585,  3.5648]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 3
	action: tensor([[-4.8521,  0.3899,  5.9271, -5.3861,  2.9319,  0.5067,  4.3491]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 4
	action: tensor([[-3.2874,  0.0414,  6.1800, -5.1690,  3.1745,  0.9331,  3.7371]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 5
	action: tensor([[-3.5973, -0.2498,  6.1800, -4.2934,  2.5505,  0.3293,  4.3117]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 6
	action: tensor([[-3.8427, -0.6264,  5.9638, -4.3577,  2.4788,  0.4077,  4.9918]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 7
	action: tensor([[-4.4995,  0.8221,  6.1800, -4.9579,  2.6755,  0.2267,  4.4016]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 8
	action: tensor([[-3.7892,  0.5507,  6.1800, -4.5691,  2.2095, -0.0261,  4.1064]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 9
	action: tensor([[-3.4553,  0.2531,  6.1800, -4.5426,  3.1493, -0.0965,  4.0459]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 10
	action: tensor([[-4.0677e+00,  1.1022e-01,  6.1800e+00, -4.6000e+00,  2.6211e+00,
         -4.9027e-03,  5.0916e+00]], dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 11
	action: tensor([[-3.8928,  0.1532,  6.1800, -4.7644,  3.4057,  0.2012,  4.3291]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 12
	action: tensor([[-4.6952, -0.6629,  5.8433, -4.5490,  3.5009,  0.9366,  4.2112]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 13
	action: tensor([[-3.1792,  0.7492,  6.1800, -4.6679,  2.5436, -0.0892,  4.1673]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 14
	action: tensor([[-4.1095,  0.3883,  6.1800, -4.4490,  2.7456, -0.4906,  3.5225]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 15
	action: tensor([[-3.8021,  0.7728,  6.1800, -4.7689,  2.9209, -0.5337,  4.0302]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 16
	action: tensor([[-4.0676,  0.1877,  6.1800, -4.3661,  2.8808,  1.0563,  3.9211]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 17
	action: tensor([[-4.0377,  0.1093,  6.1800, -5.7409,  3.0132,  0.2228,  3.7396]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 18
	action: tensor([[-3.6003, -0.0715,  6.1800, -4.7414,  3.0644,  0.1411,  3.8709]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 19
	action: tensor([[-4.2219, -0.4240,  6.1800, -4.5194,  2.7211, -0.0918,  4.2890]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 20
	action: tensor([[-3.6679,  0.2115,  6.1800, -4.3597,  3.5498,  0.3297,  4.3378]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 21
	action: tensor([[-3.8504,  0.4391,  6.1800, -4.8764,  3.2478, -0.0652,  3.7210]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 22
	action: tensor([[-4.2579,  0.3629,  5.8765, -4.8184,  2.5996, -0.2714,  4.0958]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 23
	action: tensor([[-3.9860,  0.3839,  6.1427, -5.1489,  3.2285,  0.5673,  4.3055]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 24
	action: tensor([[-3.7171,  0.5336,  6.1800, -4.9736,  2.8302, -0.4209,  4.6961]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 25
	action: tensor([[-3.8569,  0.7515,  5.5588, -5.0068,  2.9482,  0.8467,  4.0917]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 26
	action: tensor([[-3.3792,  0.4765,  5.9630, -3.9843,  3.0231,  1.0241,  4.0968]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 27
	action: tensor([[-3.9096,  0.4492,  6.1800, -5.1654,  3.5890,  0.1310,  4.5018]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 28
	action: tensor([[-4.5219, -0.1513,  6.1800, -5.0940,  3.1782,  0.6397,  3.8112]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 29
	action: tensor([[-3.6068,  0.1906,  5.7941, -4.9613,  2.6014,  0.6431,  4.0055]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 30
	action: tensor([[-4.0808e+00,  7.4012e-01,  6.1800e+00, -4.8449e+00,  2.4366e+00,
         -3.3820e-03,  4.4258e+00]], dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 31
	action: tensor([[-4.0931,  0.0978,  6.1800, -5.2791,  3.1822,  0.1885,  4.2739]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 32
	action: tensor([[-3.1629,  0.0347,  6.1800, -5.1676,  3.0014, -0.0089,  3.9312]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 33
	action: tensor([[-4.1919,  0.0493,  6.1800, -4.5895,  3.0199,  0.3245,  4.2243]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 34
	action: tensor([[-3.9450, -0.0223,  6.1800, -4.5798,  3.3198,  0.6873,  4.1096]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 35
	action: tensor([[-4.0537,  0.2160,  6.1800, -5.2474,  1.7939,  0.9393,  4.0298]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 36
	action: tensor([[-4.2219,  0.5136,  6.1216, -4.2131,  2.3770,  0.2567,  3.7807]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 37
	action: tensor([[-3.7570,  0.0728,  6.1800, -4.1108,  3.1519,  0.5038,  4.2027]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 38
	action: tensor([[-3.6547, -0.1641,  6.1800, -4.6548,  2.7233,  0.0979,  4.4078]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 39
	action: tensor([[-4.2731,  1.0717,  6.1800, -4.8315,  1.6438,  0.5937,  3.8553]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 40
	action: tensor([[-3.4916,  0.4714,  6.1800, -4.4651,  2.3941,  0.8386,  4.3041]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 41
	action: tensor([[-3.6048,  0.1518,  5.9666, -4.0096,  2.5747,  0.3258,  4.6823]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 42
	action: tensor([[-4.2371,  0.4871,  6.1800, -5.1653,  2.9415,  0.0403,  3.7007]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 43
	action: tensor([[-3.9375,  0.2981,  6.1800, -4.6369,  2.6061,  1.2898,  4.0212]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 44
	action: tensor([[-4.2455,  0.2050,  5.7289, -4.6450,  2.8535,  0.3928,  4.0681]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 45
	action: tensor([[-4.1841,  0.1817,  6.1800, -4.1549,  2.8312,  0.1938,  3.6293]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 46
	action: tensor([[-3.9720, -0.0377,  6.1800, -4.6537,  3.2955,  0.2153,  4.5767]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 47
	action: tensor([[-3.5021,  0.4773,  6.1800, -4.3125,  3.5306,  0.8120,  4.2962]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 48
	action: tensor([[-3.9285,  0.1125,  6.1800, -4.8942,  3.5834,  0.1086,  4.5166]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 49
	action: tensor([[-3.6773,  0.2705,  6.1451, -4.9341,  2.7281, -0.0068,  3.7870]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 50
	action: tensor([[-4.1680e+00,  6.3342e-04,  5.9502e+00, -4.8442e+00,  3.0620e+00,
          5.7845e-02,  4.9835e+00]], dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 51
	action: tensor([[-3.8731, -0.1989,  5.6813, -4.9519,  2.7232,  0.4105,  3.8200]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 52
	action: tensor([[-3.6983,  0.9813,  6.1800, -5.0842,  3.3784,  0.4353,  4.0043]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 53
	action: tensor([[-3.4430,  0.5224,  6.1750, -5.2443,  2.5194,  0.0477,  4.1233]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 54
	action: tensor([[-4.7022,  0.1755,  5.7414, -4.3457,  3.2557, -0.1962,  4.2174]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 55
	action: tensor([[-4.4020,  0.1808,  6.1800, -4.9423,  3.1394,  0.2959,  4.1270]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 56
	action: tensor([[-4.3812,  0.1748,  6.1800, -4.7128,  3.0166,  0.1810,  4.1586]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 57
	action: tensor([[-3.8726,  0.1638,  6.1800, -4.7527,  3.1703, -0.2565,  4.8002]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 58
	action: tensor([[-3.4611,  0.0425,  6.1800, -4.5493,  3.0310,  0.3946,  4.4010]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 59
	action: tensor([[-4.4351, -0.0922,  5.6087, -4.7643,  2.9277, -0.2199,  4.2479]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 60
	action: tensor([[-3.9851,  0.5603,  5.8417, -4.5350,  2.8946,  0.2520,  4.1683]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 61
	action: tensor([[-4.2565,  0.7789,  6.1800, -4.9250,  2.9937, -0.0705,  4.5146]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 62
	action: tensor([[-4.5689,  0.2968,  6.0383, -5.2348,  3.0455, -0.2279,  3.9534]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 63
	action: tensor([[-3.1847, -0.5242,  6.1800, -4.1026,  3.6485,  0.4544,  4.2195]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 64
	action: tensor([[-4.2636, -0.0840,  6.1616, -5.1216,  3.0301,  0.4872,  4.6785]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 65
	action: tensor([[-3.5149,  0.3870,  5.9277, -4.7903,  4.1209,  0.1013,  4.5964]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 66
	action: tensor([[-3.6968,  0.6497,  6.1800, -5.5623,  3.5954,  0.1846,  4.2191]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 67
	action: tensor([[-4.8427,  0.5408,  6.1800, -4.3837,  2.9875, -0.2559,  4.4166]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 68
	action: tensor([[-3.7477,  0.2899,  6.1800, -4.4583,  3.1672, -0.3679,  4.4834]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 69
	action: tensor([[-4.0613,  0.3864,  6.1800, -5.1198,  3.1369,  0.3347,  4.1178]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 70
	action: tensor([[-3.1849,  0.8600,  6.1274, -5.0235,  3.3304,  0.5914,  4.4475]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 71
	action: tensor([[-3.8969,  0.1145,  6.1800, -4.3276,  2.6263,  0.2863,  3.9881]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 72
	action: tensor([[-3.4024,  0.3147,  6.1800, -5.2008,  2.4308,  1.2467,  4.2301]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 73
	action: tensor([[-4.6010,  0.3669,  5.9307, -4.5555,  2.5096,  0.8074,  3.9300]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 74
	action: tensor([[-3.8544,  0.1317,  6.1800, -4.3740,  2.8390,  0.1570,  4.3679]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 75
	action: tensor([[-4.1154, -0.1252,  6.1800, -4.9017,  2.5978,  0.3609,  4.2279]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 76
	action: tensor([[-4.4261, -0.4265,  6.1800, -4.9117,  2.7131, -0.3323,  4.6128]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 77
	action: tensor([[-3.7419,  0.0241,  5.8275, -4.0900,  2.3130,  0.3729,  3.9397]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 78
	action: tensor([[-4.7287, -0.0762,  6.1800, -4.6028,  3.0638,  0.4201,  4.9062]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 79
	action: tensor([[-4.3865, -0.2515,  6.1800, -5.0960,  3.1399,  0.1630,  4.0654]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 80
	action: tensor([[-3.5823,  0.6397,  5.8612, -4.4315,  3.2690,  0.1315,  4.0635]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 81
	action: tensor([[-3.7604,  0.3224,  6.1800, -4.9842,  2.5745, -0.0461,  4.2075]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 82
	action: tensor([[-4.0127,  0.6402,  6.1800, -4.7929,  2.9990,  0.2336,  3.9489]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 83
	action: tensor([[-3.7808,  0.4748,  6.1800, -4.6878,  3.2584, -0.0844,  4.0346]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 84
	action: tensor([[-4.0787,  0.1144,  6.0053, -4.6321,  2.8408,  0.2145,  4.1901]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 85
	action: tensor([[-4.4572,  0.5077,  5.6527, -5.2062,  2.9660,  0.5567,  3.8135]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 86
	action: tensor([[-4.0864,  0.3954,  6.1800, -4.6361,  3.0709,  0.1803,  4.3687]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 87
	action: tensor([[-4.0763, -0.0218,  5.4688, -4.2169,  3.0906, -0.0674,  3.6659]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 88
	action: tensor([[-4.5312,  0.5347,  5.7203, -4.6478,  2.7464,  0.4641,  3.9031]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 89
	action: tensor([[-3.8166,  0.2676,  6.1800, -4.9155,  3.7305,  0.4181,  4.0770]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 90
	action: tensor([[-4.3765,  0.0166,  6.1800, -5.1957,  2.3067,  0.4692,  4.0558]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 91
	action: tensor([[-4.0404,  0.7818,  6.1800, -4.8846,  3.2614, -0.1944,  4.0671]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 92
	action: tensor([[-3.7505, -0.6014,  6.1800, -5.1375,  3.0928, -0.1567,  4.9175]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 93
	action: tensor([[-4.1540,  0.0350,  6.1800, -5.1784,  3.1225,  0.2067,  4.5618]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 94
	action: tensor([[-3.7558,  0.0619,  6.0338, -4.4428,  2.8123, -0.0565,  4.5662]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 95
	action: tensor([[-3.8608,  0.5551,  6.1800, -4.0558,  3.6231,  0.1894,  4.1769]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 96
	action: tensor([[-4.4250,  0.4851,  6.1580, -5.2386,  2.8969,  0.3532,  4.2302]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 97
	action: tensor([[-4.2835,  0.3966,  6.1800, -4.8165,  2.4997,  0.7804,  3.7774]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 98
	action: tensor([[-4.2628, -0.0164,  6.1800, -4.3616,  3.5613, -0.0677,  4.6945]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 99
	action: tensor([[-3.6101,  0.7671,  6.1800, -5.2380,  3.3612,  0.2482,  3.7118]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 100
	action: tensor([[-4.3345,  0.3232,  6.0071, -4.7587,  2.9990,  0.2090,  4.1785]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 101
	action: tensor([[-4.7743,  0.2107,  6.1800, -4.6136,  3.7592, -0.2591,  4.9128]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 102
	action: tensor([[-3.5697,  0.3822,  6.1800, -4.7998,  3.1298,  0.2656,  4.1352]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 103
	action: tensor([[-4.2165, -0.1163,  6.1800, -4.5015,  2.6742,  0.3061,  4.4068]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 104
	action: tensor([[-4.1468, -0.1673,  6.1800, -4.4318,  2.3601,  0.3923,  3.7892]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 105
	action: tensor([[-3.6194, -0.1024,  6.1800, -5.0821,  2.9338,  0.4491,  4.0781]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 106
	action: tensor([[-3.4376,  0.3954,  6.1800, -4.5280,  4.1220,  0.6402,  4.4814]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 107
	action: tensor([[-3.7146, -0.0915,  6.1800, -5.3215,  2.3245,  0.1113,  3.6957]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 108
	action: tensor([[-3.8253, -0.2139,  6.1800, -5.0810,  3.2538,  0.6421,  4.4209]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 109
	action: tensor([[-3.7346,  0.3699,  6.0257, -4.3119,  2.9703, -0.0109,  4.6494]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 110
	action: tensor([[-4.4744,  0.3989,  5.7525, -4.3722,  3.4713, -0.3188,  4.2944]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 111
	action: tensor([[-4.4102,  0.1931,  6.1800, -4.4485,  3.5733,  0.1182,  3.7201]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 112
	action: tensor([[-4.0601,  0.7753,  6.1696, -4.6096,  3.2103, -0.1645,  4.5049]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 113
	action: tensor([[-4.6359,  0.8177,  6.1800, -4.9392,  3.1376, -0.5116,  3.6506]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 114
	action: tensor([[-4.5267, -0.0472,  5.7621, -4.7938,  2.4560,  0.6291,  4.0762]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 115
	action: tensor([[-4.0862, -0.1773,  6.1800, -4.6649,  2.6344, -0.0466,  3.9801]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 116
	action: tensor([[-3.8812,  0.2380,  5.8089, -4.3155,  3.4116,  0.6401,  3.5829]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 117
	action: tensor([[-3.9311,  0.4054,  6.1800, -5.1660,  2.7494,  0.4841,  4.6101]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 118
	action: tensor([[-3.3127,  0.0794,  6.1800, -4.5549,  3.6250,  0.6035,  4.0152]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 119
	action: tensor([[-4.2133, -0.4122,  6.0854, -4.9434,  2.8389, -0.2271,  4.2345]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 120
	action: tensor([[-4.6023,  0.0554,  6.1800, -4.7275,  2.8959,  0.6024,  4.5883]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 121
	action: tensor([[-3.9946,  0.6810,  6.1800, -5.4982,  1.9755, -0.0564,  4.1954]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 122
	action: tensor([[-3.5356,  0.8551,  5.8950, -5.2427,  3.1616,  0.0685,  4.2313]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 123
	action: tensor([[-4.1557, -0.1236,  6.1800, -4.7755,  2.9255,  0.2496,  4.4877]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 124
	action: tensor([[-3.9467,  0.6029,  5.9134, -4.7968,  2.5685, -0.0069,  4.0529]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 125
	action: tensor([[-4.1289,  0.4158,  6.1800, -4.4549,  3.2943,  0.5482,  4.3854]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 126
	action: tensor([[-3.7680, -0.0256,  5.9791, -5.0623,  2.9297, -0.2078,  4.2213]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 54, step: 127
	action: tensor([[-3.9271,  0.4202,  6.1800, -5.1360,  2.8673,  0.8972,  3.8421]],
       dtype=torch.float64)
	q_value: tensor([[-35.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
LOSS epoch 54 actor 99.92770839343437 critic 203.80508614798322 
epoch: 55, step: 0
	action: tensor([[-4.2826,  0.3816,  6.0836, -5.1172,  2.7847,  0.4627,  4.5836]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 1
	action: tensor([[-4.0703,  0.8693,  6.1800, -5.8319,  3.2526, -0.1907,  4.1207]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 2
	action: tensor([[-3.8263,  0.2856,  6.1800, -4.6186,  2.7290,  0.4033,  4.3568]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 3
	action: tensor([[-3.7703,  0.1658,  6.1800, -4.7487,  3.2291,  0.2511,  4.1389]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 4
	action: tensor([[-3.5063,  0.6670,  6.1800, -4.7379,  2.5705,  0.7500,  4.0825]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 5
	action: tensor([[-4.1847,  0.4627,  6.1800, -5.1553,  2.2297,  0.0899,  4.6617]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 6
	action: tensor([[-4.1923, -0.0492,  5.7589, -5.0272,  2.6488, -0.2269,  4.5239]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 7
	action: tensor([[-3.3941,  0.0576,  6.1800, -5.0560,  3.6584,  0.3451,  4.5848]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 8
	action: tensor([[-3.7326, -0.1579,  6.0669, -4.9134,  2.9491,  0.4805,  4.5409]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 9
	action: tensor([[-3.6010,  0.1939,  5.9011, -5.0234,  2.6243,  0.6215,  4.8001]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 10
	action: tensor([[-4.4379e+00,  2.9174e-03,  6.1800e+00, -5.0442e+00,  2.9053e+00,
          5.3041e-01,  3.8698e+00]], dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 11
	action: tensor([[-4.1340,  0.1858,  6.1800, -4.8644,  3.2891,  0.2956,  3.6522]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 12
	action: tensor([[-4.0726, -0.0192,  6.1800, -4.0226,  2.7889, -0.2504,  4.5747]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 13
	action: tensor([[-4.6155,  0.7380,  6.1800, -4.6020,  2.7644, -0.3163,  4.0652]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 14
	action: tensor([[-3.9853,  0.6179,  6.1663, -5.1155,  3.1330,  0.5026,  4.1496]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 15
	action: tensor([[-4.7189,  0.6405,  6.0229, -4.4909,  3.2010,  0.4865,  4.3480]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 16
	action: tensor([[-3.6481,  0.2123,  6.0581, -5.1573,  2.8399,  0.3767,  4.2229]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 17
	action: tensor([[-4.4202,  1.2077,  6.0226, -4.4874,  3.5938,  0.1137,  3.5576]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 18
	action: tensor([[-3.4518,  0.3367,  6.1693, -4.9338,  2.6186, -0.2894,  4.7403]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 19
	action: tensor([[-4.0235,  0.5958,  6.1800, -4.9237,  2.8830,  0.5958,  4.6225]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 20
	action: tensor([[-3.4517,  0.1452,  6.1800, -4.7199,  2.5472,  0.2697,  4.4460]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 21
	action: tensor([[-3.7234,  0.7316,  6.1195, -4.5049,  3.2426,  0.5708,  4.5797]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 22
	action: tensor([[-3.5003, -0.0924,  6.1800, -5.1124,  2.5857,  0.1906,  4.2897]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 23
	action: tensor([[-3.9752,  0.3603,  5.9428, -4.9055,  3.0580, -0.2454,  4.3231]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 24
	action: tensor([[-4.2433,  0.9248,  6.1800, -5.2866,  3.1210,  0.1060,  4.5911]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 25
	action: tensor([[-3.6714,  0.1858,  6.1800, -4.5552,  3.0845,  0.0980,  4.5578]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 26
	action: tensor([[-3.7954,  0.3148,  6.1800, -4.0166,  3.3645, -0.0683,  4.3949]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 27
	action: tensor([[-3.8117,  0.8416,  6.1800, -4.7957,  2.4569,  0.1384,  4.4819]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 28
	action: tensor([[-4.2509e+00,  4.1551e-02,  6.1800e+00, -5.2490e+00,  2.9709e+00,
         -4.2589e-03,  3.8042e+00]], dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 29
	action: tensor([[-3.6405,  0.0856,  5.9890, -4.7986,  3.3951,  0.7962,  4.3605]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 30
	action: tensor([[-4.3663, -0.0594,  6.1800, -5.2381,  2.8071,  0.2621,  4.8928]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 31
	action: tensor([[-3.4720,  0.5825,  6.1800, -5.2623,  2.6801,  0.2775,  3.9829]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 32
	action: tensor([[-3.5788,  0.6684,  5.6079, -5.1094,  2.6206,  0.2620,  4.6430]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 33
	action: tensor([[-4.0766,  0.0717,  6.1800, -4.9133,  2.8934,  0.6109,  4.2851]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 34
	action: tensor([[-3.4776,  0.4715,  5.9038, -4.7922,  2.9223,  0.3607,  4.8459]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 35
	action: tensor([[-4.1756,  0.1118,  6.1800, -5.3168,  2.5877,  0.4365,  4.4686]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 36
	action: tensor([[-3.9935, -0.0352,  5.9168, -4.7811,  2.9285,  0.5269,  4.5330]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 37
	action: tensor([[-3.5051, -0.1440,  6.1800, -4.8914,  3.3008,  0.4549,  4.4930]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 38
	action: tensor([[-3.2708,  0.6817,  6.0034, -4.8484,  2.9888, -0.2690,  4.2045]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 39
	action: tensor([[-4.1741, -0.3111,  6.1766, -4.5678,  2.4937,  0.1482,  3.9884]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 40
	action: tensor([[-3.5273, -0.0818,  6.1800, -4.4196,  2.8507,  0.2215,  4.3994]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 41
	action: tensor([[-4.4340,  0.3785,  6.1105, -4.8903,  3.0049, -0.0559,  4.1254]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 42
	action: tensor([[-4.5065,  0.2919,  6.1800, -4.8476,  2.8287, -0.0163,  3.4371]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 43
	action: tensor([[-4.0232, -0.0213,  6.1800, -5.7200,  3.2687,  0.4950,  4.1763]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 44
	action: tensor([[-3.5489, -0.1189,  5.9579, -3.5324,  2.8170,  0.1156,  4.7147]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 45
	action: tensor([[-3.3990,  0.3250,  6.1800, -4.3202,  2.8613,  0.2229,  4.3023]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 46
	action: tensor([[-4.7966,  0.2864,  6.1800, -4.7033,  2.3910, -0.4099,  4.8710]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 47
	action: tensor([[-3.3569,  0.5328,  6.1800, -5.2938,  3.5105,  0.4211,  4.7137]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 48
	action: tensor([[-4.4321,  0.6487,  6.1800, -4.8092,  3.2189, -0.0631,  3.9789]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 49
	action: tensor([[-3.7836,  0.0843,  6.1800, -4.8806,  3.0436,  0.7219,  4.4806]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 50
	action: tensor([[-4.0061, -0.0372,  6.1800, -5.3802,  2.7099,  0.8262,  4.2847]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 51
	action: tensor([[-3.9563,  0.4832,  6.0364, -5.0300,  2.5118,  0.4009,  4.4818]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 52
	action: tensor([[-3.6260,  0.6155,  6.1800, -5.1216,  3.0373,  0.2508,  4.8896]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 53
	action: tensor([[-4.2028,  0.4027,  6.1800, -4.5705,  2.8446,  0.4112,  4.4157]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 54
	action: tensor([[-3.8338,  0.1938,  6.1800, -5.0199,  3.0567,  0.2368,  4.3325]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 55
	action: tensor([[-4.1004, -0.0583,  6.1800, -4.5720,  3.2976,  0.6925,  4.1561]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 56
	action: tensor([[-4.3238,  0.2399,  6.1800, -4.0368,  2.6425,  0.0532,  4.4214]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 57
	action: tensor([[-3.9856,  0.2003,  6.0381, -5.2188,  2.8281,  0.1723,  4.4268]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 58
	action: tensor([[-3.7383, -0.4567,  6.1280, -5.1502,  3.0913,  0.4366,  4.0533]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 59
	action: tensor([[-3.9794, -0.0761,  6.1800, -4.5640,  3.3383,  0.9005,  3.5387]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 60
	action: tensor([[-4.1189e+00,  5.7133e-01,  6.1800e+00, -4.8078e+00,  2.7019e+00,
          4.2170e-04,  3.7042e+00]], dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 61
	action: tensor([[-4.3373,  0.4802,  6.1800, -4.4784,  3.0232,  0.4248,  4.4629]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 62
	action: tensor([[-4.0969,  0.3842,  6.1800, -4.3390,  3.0906,  0.4707,  4.0379]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 63
	action: tensor([[-3.5280,  0.0065,  6.1800, -4.8406,  3.2280,  0.1821,  4.6376]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 64
	action: tensor([[-4.3470,  0.3362,  6.1800, -4.4031,  3.5837,  0.1115,  3.6175]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 65
	action: tensor([[-3.8547, -0.0977,  6.0488, -4.8290,  2.7268,  0.6768,  4.4210]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 66
	action: tensor([[-3.3036,  0.1359,  5.9935, -4.9799,  2.6057,  0.2466,  4.4390]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 67
	action: tensor([[-4.3245,  0.6034,  6.1800, -5.0711,  3.4076, -0.2041,  4.3745]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 68
	action: tensor([[-4.0839,  0.5159,  6.1800, -4.5595,  3.3011,  0.1614,  4.6884]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 69
	action: tensor([[-4.1848, -0.0091,  6.1800, -4.8923,  2.4288,  0.8498,  3.9087]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 70
	action: tensor([[-4.3155, -0.2389,  6.0578, -4.6529,  2.9710,  0.1666,  4.6591]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 71
	action: tensor([[-3.8372,  0.1151,  6.1800, -4.7992,  3.4539,  0.2680,  4.3366]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 72
	action: tensor([[-4.0874,  0.1092,  6.1800, -5.0207,  2.3736,  0.2077,  4.4811]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 73
	action: tensor([[-3.9744,  0.0799,  6.0236, -4.6990,  2.4817,  0.1897,  4.7474]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 74
	action: tensor([[-3.8829,  0.7369,  6.1450, -5.4387,  2.6696,  0.5598,  4.3038]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 75
	action: tensor([[-3.9760,  0.1066,  5.7552, -4.8664,  2.5653,  0.2615,  4.6454]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 76
	action: tensor([[-4.0345,  0.1839,  6.1800, -5.3832,  2.7600,  0.3988,  4.3663]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 77
	action: tensor([[-3.8583, -0.4109,  5.6539, -4.7402,  2.8770,  1.0783,  4.6245]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 78
	action: tensor([[-4.3190,  0.6255,  6.1800, -4.6709,  3.2113,  0.5079,  4.2449]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 79
	action: tensor([[-4.0782,  0.2733,  6.1800, -4.9629,  3.3384,  0.3412,  4.1024]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 80
	action: tensor([[-3.4310,  0.5927,  5.9657, -4.7064,  3.2140,  0.9711,  4.2575]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 81
	action: tensor([[-3.8388,  0.7470,  6.1800, -4.6653,  3.0006, -0.4449,  4.2510]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 82
	action: tensor([[-4.6434,  0.2917,  6.1687, -5.0692,  2.6496,  0.2708,  4.6331]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 83
	action: tensor([[-3.9256,  0.6857,  6.1800, -4.1492,  3.0807, -0.2195,  4.1370]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 84
	action: tensor([[-3.5469, -0.1331,  6.1800, -4.8405,  2.7411, -0.0587,  4.1487]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 85
	action: tensor([[-4.0035,  1.0010,  6.1602, -4.5043,  3.1931,  0.3493,  4.2892]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 86
	action: tensor([[-3.9041,  0.4188,  6.0957, -5.0243,  3.0363,  0.2614,  3.8056]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 87
	action: tensor([[-4.4623,  1.0144,  6.1800, -4.5097,  3.1279,  0.1566,  4.4116]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 88
	action: tensor([[-3.9758,  0.5596,  6.1800, -5.1359,  3.2530,  0.2271,  4.5897]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 89
	action: tensor([[-4.1665,  0.0750,  5.9130, -5.2258,  2.3359,  0.2575,  4.4991]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 90
	action: tensor([[-3.6742, -0.2761,  5.9663, -4.1496,  3.1786,  0.3303,  4.7750]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 91
	action: tensor([[-4.2640, -0.2130,  6.0713, -5.0731,  3.5358,  0.4444,  5.0153]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 92
	action: tensor([[-4.1076, -0.6136,  6.0666, -4.6932,  3.0841,  0.1050,  4.5441]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 93
	action: tensor([[-3.8392,  0.0569,  6.1800, -5.2663,  2.9085, -0.1586,  3.9047]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 94
	action: tensor([[-4.5174,  0.0554,  6.1800, -4.9147,  2.6789,  0.2747,  4.4968]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 95
	action: tensor([[-4.0778,  0.1535,  6.1800, -4.9679,  3.4487,  0.3936,  3.9599]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 96
	action: tensor([[-3.9817,  0.3785,  6.1800, -4.8301,  2.4885,  0.2413,  3.6944]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 97
	action: tensor([[-3.4478,  0.2598,  5.9746, -5.0189,  3.5377,  0.3977,  4.3012]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 98
	action: tensor([[-4.0407,  0.1846,  6.1800, -4.9908,  3.5963,  0.0068,  4.4159]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 99
	action: tensor([[-4.2285,  0.4129,  6.1800, -5.0348,  2.9038,  0.2162,  4.5103]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 100
	action: tensor([[-3.6998,  0.1590,  5.6470, -4.6589,  2.6558,  0.7419,  4.5556]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 101
	action: tensor([[-3.5776, -0.2595,  6.1800, -4.6384,  2.6789,  0.5767,  4.5682]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 102
	action: tensor([[-3.9592, -0.3429,  6.1800, -5.1098,  2.9724,  0.9409,  4.0333]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 103
	action: tensor([[-3.3570, -0.0158,  5.6828, -4.3453,  2.3149,  0.2905,  4.1467]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 104
	action: tensor([[-3.6867, -0.1046,  6.1800, -5.2669,  2.8930,  0.3946,  4.3427]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 105
	action: tensor([[-3.2884,  0.6642,  6.1800, -4.2712,  3.0175, -0.2290,  3.8061]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 106
	action: tensor([[-3.9017,  0.9291,  6.0314, -5.3804,  2.8546,  0.4896,  4.1055]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 107
	action: tensor([[-4.3852,  0.4242,  5.8968, -4.8164,  2.7000, -0.0683,  4.2734]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 108
	action: tensor([[-4.1673,  0.1956,  5.9746, -5.0511,  2.7956,  0.6298,  4.1704]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 109
	action: tensor([[-4.3488,  0.4154,  6.1800, -5.0346,  3.2872,  0.7421,  4.1188]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 110
	action: tensor([[-3.8887,  0.2093,  6.1107, -4.5518,  2.8849, -0.0366,  4.5007]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 111
	action: tensor([[-4.0455,  0.6931,  5.9034, -4.4131,  3.2320,  0.4005,  4.3967]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 112
	action: tensor([[-4.4212, -0.0173,  6.1531, -5.2732,  2.9700,  0.2802,  4.3035]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 113
	action: tensor([[-4.0114,  1.0990,  6.0029, -4.3643,  3.2247,  0.0623,  4.5421]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 114
	action: tensor([[-4.2208,  0.3226,  6.1800, -4.7564,  3.0359,  0.9789,  4.3876]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 115
	action: tensor([[-4.2633,  0.6068,  5.9959, -5.1783,  2.8813, -0.2011,  4.2095]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 116
	action: tensor([[-3.7659,  0.7235,  6.1800, -4.8060,  2.8018,  0.9473,  4.6414]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 117
	action: tensor([[-4.0491,  0.5611,  6.1800, -4.5868,  3.1911,  0.1456,  4.5566]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 118
	action: tensor([[-3.7863,  0.1978,  6.1800, -5.0784,  2.6321,  0.1236,  4.1621]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 119
	action: tensor([[-3.7829,  0.3902,  6.1800, -4.8253,  3.8053,  0.3382,  4.2952]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 120
	action: tensor([[-3.9224,  0.4025,  6.1800, -4.9031,  3.0226,  0.0968,  4.6497]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 121
	action: tensor([[-3.6991,  0.1124,  6.1800, -4.5526,  3.5099, -0.0434,  4.3744]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 122
	action: tensor([[-3.5745,  0.0374,  6.0638, -4.6238,  2.9102,  0.6484,  3.9790]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 123
	action: tensor([[-4.2795,  0.1451,  6.1145, -4.8673,  3.5374, -0.0084,  5.1795]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 124
	action: tensor([[-4.0693,  0.1391,  6.1800, -5.2738,  2.9224,  0.4055,  4.1631]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 125
	action: tensor([[-4.2630, -0.1128,  6.1800, -4.7923,  3.2018,  0.6771,  4.0949]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 126
	action: tensor([[-3.7526,  0.4047,  6.1800, -4.7477,  2.7184, -0.1562,  4.1468]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 55, step: 127
	action: tensor([[-3.4277,  0.6960,  6.1610, -4.6348,  3.2319,  0.2178,  4.0701]],
       dtype=torch.float64)
	q_value: tensor([[-48.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
LOSS epoch 55 actor 0.9113007783045907 critic 1.8283936849970195 
epoch: 56, step: 0
	action: tensor([[-3.6658,  0.1926,  6.1800, -4.8235,  3.6763,  0.5529,  4.1579]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 1
	action: tensor([[-3.8091, -0.0073,  6.1800, -4.6238,  2.9520,  0.6486,  3.8164]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 2
	action: tensor([[-3.5033, -0.3810,  6.1800, -3.9195,  2.1734,  0.5304,  4.4946]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 3
	action: tensor([[-4.1310,  0.8438,  6.1218, -5.5205,  2.6612,  0.1774,  4.6227]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 4
	action: tensor([[-4.2466, -0.0382,  5.5316, -5.1357,  3.1492, -0.5003,  3.6201]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 5
	action: tensor([[-3.9494,  0.6003,  6.1800, -5.1454,  2.9233,  0.4229,  4.2716]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 6
	action: tensor([[-4.7471,  0.0736,  5.9995, -4.5669,  2.9524,  0.3748,  4.1536]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 7
	action: tensor([[-4.1579,  0.1446,  6.0681, -5.0033,  3.5304,  0.2236,  4.1357]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 8
	action: tensor([[-4.0958,  0.0485,  5.8135, -4.7598,  2.0122,  0.1580,  4.6371]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 9
	action: tensor([[-3.4441,  0.2018,  6.1800, -5.1952,  2.6753,  0.2011,  3.7850]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 10
	action: tensor([[-3.7328, -0.0923,  6.1800, -4.9210,  2.7766,  0.3445,  4.0204]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 11
	action: tensor([[-4.1471,  0.8087,  6.0711, -5.1445,  3.1460, -0.4363,  4.3521]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 12
	action: tensor([[-4.0090,  0.6675,  6.0933, -5.2619,  3.3065, -0.2528,  4.0683]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 13
	action: tensor([[-3.5686, -0.2393,  6.1317, -5.2539,  3.5130,  0.3503,  4.8427]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 14
	action: tensor([[-3.9524,  0.0423,  6.1490, -4.5905,  3.2030,  0.3986,  4.2707]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 15
	action: tensor([[-3.9473, -0.0559,  6.1503, -4.4844,  3.5202,  0.1660,  4.3188]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 16
	action: tensor([[-4.4978, -0.0751,  6.1800, -5.1445,  3.4670,  0.0286,  4.1613]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 17
	action: tensor([[-4.1680,  0.3399,  6.1800, -4.9658,  2.9880,  0.2390,  3.9159]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 18
	action: tensor([[-3.5132,  0.6727,  6.1800, -4.6862,  3.2056,  0.7890,  4.2330]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 19
	action: tensor([[-3.8258,  0.6531,  6.1800, -4.4551,  2.7332,  0.0464,  4.2080]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 20
	action: tensor([[-4.1895,  0.4678,  6.1800, -5.3342,  3.5086,  0.0663,  4.6469]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 21
	action: tensor([[-3.9842,  0.4322,  6.1800, -4.4675,  3.0425, -0.0648,  3.8551]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 22
	action: tensor([[-4.0574,  0.4669,  5.9071, -4.8619,  3.0156, -0.3731,  4.1186]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 23
	action: tensor([[-4.5998,  0.4900,  5.9117, -4.9624,  3.0549,  0.3068,  4.1077]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 24
	action: tensor([[-4.4581,  0.4620,  6.1800, -5.2419,  2.8254,  0.5267,  4.6384]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 25
	action: tensor([[-3.9390e+00,  1.7651e-01,  6.1800e+00, -4.4047e+00,  2.6905e+00,
         -3.1822e-03,  4.3569e+00]], dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 26
	action: tensor([[-3.8301,  1.0450,  6.1800, -4.5105,  2.7705, -0.0759,  4.2920]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 27
	action: tensor([[-3.8762,  0.2163,  6.1800, -4.1501,  3.4855, -0.0794,  4.0890]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 28
	action: tensor([[-3.5455,  0.3970,  6.1800, -4.4970,  2.4208, -0.3771,  4.4303]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 29
	action: tensor([[-3.6738,  0.2738,  5.8494, -4.8761,  2.5899,  0.2719,  4.7259]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 30
	action: tensor([[-3.9527, -0.0412,  5.6466, -4.4858,  3.0404,  0.4959,  4.4095]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 31
	action: tensor([[-3.7561,  0.4817,  6.1800, -5.3000,  2.9020,  0.0731,  4.6913]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 32
	action: tensor([[-4.4065, -0.2312,  6.0562, -5.3056,  2.8016,  0.5687,  4.7684]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 33
	action: tensor([[-4.3758, -0.0806,  6.1800, -5.2026,  3.0472, -0.0334,  4.4472]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 34
	action: tensor([[-3.6941, -0.0976,  6.1800, -4.9112,  3.5892,  0.8135,  4.7363]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 35
	action: tensor([[-4.3305,  0.0863,  6.1800, -4.9575,  2.9418,  0.3795,  3.9965]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 36
	action: tensor([[-4.1052,  0.7463,  5.8072, -4.3107,  3.1375,  0.2438,  4.6011]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 37
	action: tensor([[-3.6272, -0.2640,  6.1800, -4.5499,  2.8913,  0.6529,  4.8142]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 38
	action: tensor([[-4.0437,  0.4255,  6.1800, -5.2274,  3.1344,  0.1639,  4.6876]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 39
	action: tensor([[-3.7757,  0.1170,  6.1800, -5.0897,  2.9007,  0.1028,  4.5180]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 40
	action: tensor([[-3.8710, -0.2920,  6.1800, -5.3083,  3.2728, -0.0330,  5.0294]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 41
	action: tensor([[-3.5078,  0.2492,  6.1800, -4.9843,  2.9441, -0.1347,  5.0071]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 42
	action: tensor([[-3.9649,  0.0350,  5.5361, -4.7384,  3.3348,  0.1805,  4.0533]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 43
	action: tensor([[-4.1431,  0.5512,  6.1800, -4.0839,  3.0770, -0.3109,  4.5337]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 44
	action: tensor([[-4.0657, -0.0341,  5.9570, -4.4196,  3.4370,  0.0710,  3.7676]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 45
	action: tensor([[-3.7263,  0.0699,  6.1800, -5.0608,  3.0557,  0.4491,  4.0668]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 46
	action: tensor([[-3.7826,  0.5429,  6.0237, -4.6913,  2.9971,  0.3401,  4.4531]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 47
	action: tensor([[-3.8212,  0.0559,  6.1800, -4.9337,  3.2367, -0.5546,  4.0902]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 48
	action: tensor([[-3.8197,  0.5708,  6.1800, -4.8775,  3.4826,  0.3591,  4.4022]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 49
	action: tensor([[-4.4478, -0.1799,  6.1800, -4.6494,  3.6800,  0.3584,  4.3822]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 50
	action: tensor([[-3.9961,  0.4376,  6.1800, -4.7631,  2.5766,  0.6833,  3.7763]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 51
	action: tensor([[-3.9494, -0.1868,  5.8772, -4.4783,  2.8824, -0.1027,  4.1687]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 52
	action: tensor([[-3.5849,  0.0338,  5.8962, -4.6551,  3.4199,  0.3365,  4.4416]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 53
	action: tensor([[-3.7906e+00, -1.9498e-03,  6.1800e+00, -5.0036e+00,  2.8821e+00,
          4.2528e-01,  4.5106e+00]], dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 54
	action: tensor([[-4.2295,  0.0714,  6.1800, -4.8680,  2.7188,  0.6543,  4.0562]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 55
	action: tensor([[-4.3941,  0.3505,  6.1800, -4.4434,  3.3703, -0.1981,  3.9128]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 56
	action: tensor([[-4.3589, -0.4571,  6.1800, -5.0587,  3.6681,  0.0100,  4.8877]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 57
	action: tensor([[-3.8281e+00, -3.8114e-01,  6.1800e+00, -4.5847e+00,  2.9594e+00,
          1.0249e-03,  4.7180e+00]], dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 58
	action: tensor([[-3.7764,  0.4020,  6.1800, -4.7294,  2.6878,  0.4782,  3.9043]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 59
	action: tensor([[-4.5559,  0.0900,  6.1800, -5.4034,  3.4804,  0.3930,  4.2491]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 60
	action: tensor([[-3.4156,  0.8069,  6.1800, -4.9435,  2.9494,  0.2025,  4.3071]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 61
	action: tensor([[-3.8572, -0.0538,  6.1800, -4.4729,  3.1770,  0.2041,  4.3720]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 62
	action: tensor([[-3.9143, -0.0369,  5.9556, -4.9892,  2.9929,  0.6222,  4.8596]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 63
	action: tensor([[-4.2759,  0.6591,  6.1800, -4.0043,  3.2291, -0.0557,  4.0741]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 64
	action: tensor([[-4.1326,  0.4154,  5.9766, -4.5292,  3.1392,  0.0627,  4.7543]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 65
	action: tensor([[-4.1851,  0.6128,  6.1666, -4.5155,  3.0308,  0.9312,  4.7546]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 66
	action: tensor([[-3.5696,  0.3223,  6.0257, -5.3216,  2.6837,  0.4672,  3.8967]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 67
	action: tensor([[-4.1474,  0.7681,  5.9464, -4.5968,  3.0543, -0.2170,  4.9205]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 68
	action: tensor([[-3.6626, -0.1366,  5.9232, -4.9209,  2.4287,  0.4116,  4.1084]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 69
	action: tensor([[-3.9336,  0.8281,  6.1548, -4.7552,  2.9734, -0.0157,  4.6403]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 70
	action: tensor([[-3.9498, -0.1232,  6.1800, -5.0967,  2.8253,  0.0393,  3.9605]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 71
	action: tensor([[-4.0132,  0.4255,  6.1800, -4.9375,  3.3746,  0.4724,  3.8507]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 72
	action: tensor([[-3.9390,  0.5201,  6.1800, -4.7058,  2.7137,  0.0974,  4.0900]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 73
	action: tensor([[-3.5749, -0.2091,  5.9822, -4.1955,  3.1536, -0.3479,  4.2378]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 74
	action: tensor([[-4.2552,  0.4701,  6.1736, -4.4970,  2.9740,  0.1874,  4.3473]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 75
	action: tensor([[-4.1135,  0.1406,  5.8971, -4.9288,  3.3606,  0.3499,  4.8842]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 76
	action: tensor([[-4.0190,  0.3004,  5.9486, -4.8988,  2.7708, -0.4632,  4.9414]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 77
	action: tensor([[-4.2046,  1.0450,  6.1800, -4.6518,  3.0337,  0.1835,  4.2257]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 78
	action: tensor([[-3.9476,  0.5549,  6.1800, -4.6684,  2.9928, -0.0809,  4.1762]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 79
	action: tensor([[-4.3890,  0.3122,  6.1800, -4.4619,  3.9936,  0.0341,  4.5386]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 80
	action: tensor([[-4.3180,  0.7262,  6.1800, -4.7746,  3.5551,  0.1295,  3.8821]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 81
	action: tensor([[-4.3826,  0.8413,  6.1800, -4.2349,  3.5138, -0.4576,  4.1719]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 82
	action: tensor([[-3.9956,  0.3744,  6.0090, -4.6979,  2.8648, -0.1324,  4.0159]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 83
	action: tensor([[-3.8808,  0.2998,  6.1800, -5.0413,  3.1671,  0.3970,  4.7427]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 84
	action: tensor([[-4.2581,  0.7256,  5.9343, -4.4241,  2.8990,  0.0780,  4.1056]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 85
	action: tensor([[-4.6051,  0.2830,  6.1800, -5.3876,  2.1960,  0.1378,  4.5195]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 86
	action: tensor([[-3.9870, -0.1636,  5.9783, -4.2943,  3.1686,  0.5918,  4.3696]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 87
	action: tensor([[-4.2090,  0.1980,  6.1800, -5.2380,  3.2084,  0.0488,  4.2492]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 88
	action: tensor([[-4.0070,  0.1848,  6.0622, -5.3196,  3.0076, -0.1121,  4.7531]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 89
	action: tensor([[-4.1221,  0.0067,  6.1800, -4.8045,  3.0412,  0.2355,  4.1743]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 90
	action: tensor([[-4.6868,  0.4039,  6.1800, -4.7019,  3.4699,  0.1594,  4.0834]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 91
	action: tensor([[-4.2008,  0.5354,  6.0860, -4.5386,  3.2703,  0.0389,  4.6942]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 92
	action: tensor([[-4.0470, -0.0526,  6.0704, -4.7902,  3.3349,  0.4067,  4.3590]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 93
	action: tensor([[-4.4353,  0.5847,  6.1800, -4.1131,  2.3571,  0.3156,  4.0357]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 94
	action: tensor([[-3.9814,  0.4184,  6.1800, -4.4155,  3.2062,  0.0450,  4.4736]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 95
	action: tensor([[-3.9512,  0.3206,  6.1800, -4.4922,  3.2303,  0.2948,  4.4815]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 96
	action: tensor([[-3.8342,  0.4484,  6.1800, -4.9218,  2.6712, -0.0064,  3.7172]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 97
	action: tensor([[-4.8439,  0.0939,  6.1614, -4.1050,  3.3825,  0.1417,  4.9238]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 98
	action: tensor([[-3.9978,  0.4548,  5.7009, -4.5897,  2.9715, -0.2210,  4.0865]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 99
	action: tensor([[-3.9213,  0.5967,  6.1800, -5.1192,  3.1325, -0.1094,  4.6670]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 100
	action: tensor([[-3.5869,  0.0280,  6.1800, -4.4026,  3.1974,  0.3311,  4.3128]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 101
	action: tensor([[-4.2696,  0.3187,  6.1536, -4.8109,  3.4162,  0.1845,  4.3242]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 102
	action: tensor([[-3.7566,  0.1783,  6.1800, -5.0774,  3.2465,  0.5462,  3.9656]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 103
	action: tensor([[-3.7063,  0.4679,  6.1800, -4.6649,  2.9943,  0.2625,  4.7369]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 104
	action: tensor([[-3.6889,  0.1775,  6.1800, -4.0887,  3.2576,  0.4487,  4.6200]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 105
	action: tensor([[-3.9012,  0.3919,  6.1800, -5.2893,  2.5897,  0.2511,  4.5295]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 106
	action: tensor([[-3.6552,  0.2369,  6.1800, -3.9638,  2.4387,  0.0684,  4.4398]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 107
	action: tensor([[-3.8488,  0.5311,  6.1800, -5.0549,  3.1747, -0.4037,  4.3932]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 108
	action: tensor([[-4.4446,  0.3001,  6.1800, -5.4428,  3.2900,  0.0731,  4.1907]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 109
	action: tensor([[-3.5820,  0.1493,  6.1800, -4.6678,  2.8854, -0.0851,  4.6259]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 110
	action: tensor([[-3.5670, -0.0993,  6.1671, -4.4358,  2.3518,  0.1312,  4.6590]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 111
	action: tensor([[-3.9233,  0.5645,  6.1800, -4.5925,  2.6264,  0.9747,  4.2064]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 112
	action: tensor([[-4.7186,  0.6394,  6.1800, -4.7656,  2.7362,  0.2423,  4.1437]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 113
	action: tensor([[-3.4483,  0.3318,  6.0993, -4.5848,  3.3946,  1.1335,  3.7730]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 114
	action: tensor([[-3.9586,  0.4339,  6.1800, -5.4304,  3.0777,  0.7020,  3.8065]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 115
	action: tensor([[-4.2165,  0.4478,  6.1800, -4.2243,  3.2031,  0.2487,  4.4761]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 116
	action: tensor([[-4.1217,  0.8412,  5.9788, -4.9408,  3.0917,  0.6027,  4.3855]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 117
	action: tensor([[-4.4869,  0.8007,  6.1800, -5.0050,  3.2054,  0.5597,  3.7911]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 118
	action: tensor([[-4.0340,  0.4105,  6.0755, -4.4561,  2.9235,  0.6871,  4.1613]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 119
	action: tensor([[-3.9332,  0.0565,  6.1800, -5.0099,  3.0208,  0.4478,  4.0792]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 120
	action: tensor([[-4.3041,  0.5309,  6.1085, -4.5107,  3.2997,  0.6839,  4.4511]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 121
	action: tensor([[-4.3565, -0.4097,  5.5236, -4.5861,  3.4608,  0.5925,  4.2566]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 122
	action: tensor([[-4.3803, -0.0999,  6.1211, -4.6338,  2.7413,  0.2802,  4.2963]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 123
	action: tensor([[-4.1620, -0.2261,  6.1272, -4.7951,  3.3472,  0.4623,  4.1047]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 124
	action: tensor([[-4.0119,  0.3029,  6.1800, -4.7300,  2.9315,  0.1542,  3.8680]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 125
	action: tensor([[-3.5712,  0.5092,  6.1081, -4.7224,  2.8229,  0.5582,  4.9922]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 126
	action: tensor([[-4.0583, -0.0286,  6.0560, -4.7801,  2.9045,  0.3725,  4.4041]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 56, step: 127
	action: tensor([[-3.7260,  0.6925,  6.1800, -4.6822,  3.4105,  0.4275,  3.7438]],
       dtype=torch.float64)
	q_value: tensor([[-63.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
LOSS epoch 56 actor 84.99774563394871 critic 170.05341255456975 
epoch: 57, step: 0
	action: tensor([[-3.9502,  0.4813,  6.1800, -5.0688,  2.8433,  0.4428,  4.1641]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 1
	action: tensor([[-4.3704,  0.1225,  6.1800, -4.4270,  3.2908,  0.0142,  4.1833]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 2
	action: tensor([[-3.8266,  0.3488,  6.1800, -4.7598,  2.9071, -0.4466,  4.9232]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 3
	action: tensor([[-4.2067,  0.3632,  6.1464, -4.4046,  3.2608,  0.3482,  4.3263]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 4
	action: tensor([[-4.1409,  0.0425,  6.0912, -5.0455,  3.0815, -0.1698,  4.4137]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 5
	action: tensor([[-3.3753,  1.1102,  6.1800, -4.2364,  3.3469,  0.0225,  4.0816]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 6
	action: tensor([[-3.8157,  0.3221,  6.1100, -4.6885,  3.0948, -0.2508,  4.3806]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 7
	action: tensor([[-4.3541, -0.3076,  5.9586, -5.0750,  3.2988,  0.0390,  4.4534]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 8
	action: tensor([[-3.9734,  0.3618,  6.1800, -5.0450,  2.8414,  0.3357,  3.5148]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 9
	action: tensor([[-3.4643,  0.0659,  6.1800, -4.9470,  3.1178,  0.6060,  4.4795]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 10
	action: tensor([[-3.2569, -0.3076,  6.1800, -4.2949,  2.9196, -0.3367,  4.3686]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 11
	action: tensor([[-3.9007,  0.2855,  6.1534, -5.0579,  3.1802,  0.2216,  4.5626]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 12
	action: tensor([[-4.5131,  0.4743,  6.1800, -4.7680,  2.1928,  0.3535,  4.9213]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 13
	action: tensor([[-4.1483,  0.3436,  6.1800, -4.7324,  1.9680,  0.0729,  4.1400]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 14
	action: tensor([[-4.1632, -0.1078,  6.1800, -4.9924,  2.8431,  0.6002,  3.9732]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 15
	action: tensor([[-4.1777,  0.4122,  6.1800, -4.9351,  2.9890,  0.1912,  4.4182]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 16
	action: tensor([[-3.8551,  1.0269,  6.1800, -4.2005,  2.4983,  0.2113,  4.5261]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 17
	action: tensor([[-4.8297,  0.4196,  6.1800, -4.3419,  2.6479,  0.4306,  4.4549]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 18
	action: tensor([[-4.1116, -0.1622,  6.1276, -4.4847,  3.4086, -0.2144,  4.4747]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 19
	action: tensor([[-3.7930,  0.0637,  6.1800, -4.6157,  2.6528,  0.2468,  3.4776]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 20
	action: tensor([[-3.9177,  0.6795,  6.1800, -4.8649,  3.0832,  0.5615,  4.4584]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 21
	action: tensor([[-4.3960,  0.5777,  6.1800, -4.8562,  2.8851,  0.2989,  4.6613]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 22
	action: tensor([[-4.1977, -0.0123,  5.8348, -5.2209,  2.9919, -0.0199,  4.4530]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 23
	action: tensor([[-3.8930,  0.8568,  6.1800, -5.0687,  3.1276,  0.7392,  4.5884]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 24
	action: tensor([[-3.7864, -0.0684,  6.1800, -4.5786,  3.0983,  0.1382,  4.3836]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 25
	action: tensor([[-3.1571,  0.1888,  6.1800, -5.1552,  2.8506,  0.0360,  4.2938]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 26
	action: tensor([[-3.5815,  0.1781,  5.8938, -4.4377,  2.5559,  0.2624,  4.0238]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 27
	action: tensor([[-3.7467, -0.0907,  6.0539, -4.9009,  3.1608, -0.1254,  4.1849]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 28
	action: tensor([[-4.0137,  0.2063,  5.3216, -5.0505,  3.0781,  0.0540,  4.1697]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 29
	action: tensor([[-3.9422, -0.3317,  5.9460, -4.8333,  3.1731, -0.1111,  4.2383]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 30
	action: tensor([[-4.4518,  0.4523,  5.8931, -4.5392,  2.5426,  0.4322,  4.2993]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 31
	action: tensor([[-4.2403,  0.1941,  6.1800, -5.4229,  2.6623,  0.1588,  3.9634]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 32
	action: tensor([[-4.2657,  0.1296,  6.1800, -4.6604,  3.2562,  0.1606,  3.6901]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 33
	action: tensor([[-4.6027,  0.2966,  5.8251, -5.0840,  3.3830, -0.0981,  5.3535]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 34
	action: tensor([[-3.9944,  0.7148,  6.1319, -4.6601,  3.1030, -0.4123,  4.1334]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 35
	action: tensor([[-4.3433,  0.6502,  6.0583, -4.3189,  2.5397,  0.3171,  4.0269]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 36
	action: tensor([[-4.4512,  0.3251,  6.1800, -4.7506,  2.8254,  0.5939,  4.2564]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 37
	action: tensor([[-4.2087,  0.4030,  6.1800, -5.3046,  3.3507, -0.2952,  3.8437]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 38
	action: tensor([[-4.1822e+00, -1.4075e-01,  6.1800e+00, -4.7065e+00,  3.1191e+00,
         -4.8747e-03,  5.1547e+00]], dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 39
	action: tensor([[-3.3812, -0.1295,  5.9584, -4.8983,  2.6901,  0.0877,  4.2966]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 40
	action: tensor([[-3.8655,  0.3514,  6.1800, -5.0818,  2.5900, -0.1191,  3.8119]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 41
	action: tensor([[-3.6705,  0.5435,  5.8288, -4.7634,  2.4475,  0.0626,  4.1396]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 42
	action: tensor([[-4.0465,  0.3700,  6.1800, -5.2657,  2.9897, -0.6131,  3.7452]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 43
	action: tensor([[-3.6298,  0.3060,  6.1800, -5.3248,  2.9651,  0.2820,  4.4634]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 44
	action: tensor([[-3.3658,  0.7104,  6.1800, -4.7755,  2.5146,  0.6989,  4.9325]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 45
	action: tensor([[-3.5331, -0.0228,  6.1800, -5.0808,  2.6577,  0.0421,  4.3763]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 46
	action: tensor([[-3.7481,  0.6373,  6.1800, -4.4560,  3.3045, -0.1797,  4.9878]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 47
	action: tensor([[-4.1040,  0.4305,  6.1800, -5.0378,  2.7879,  0.1719,  4.9374]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 48
	action: tensor([[-4.3011,  0.0700,  5.8786, -4.6286,  2.1268,  0.6121,  3.9889]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 49
	action: tensor([[-3.6574, -0.0639,  6.1800, -5.2533,  3.3605,  0.9275,  4.2431]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 50
	action: tensor([[-3.9635e+00, -8.6479e-02,  5.9436e+00, -4.6003e+00,  2.8638e+00,
         -3.7211e-03,  4.0355e+00]], dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 51
	action: tensor([[-4.1631e+00,  4.6730e-01,  5.3867e+00, -4.3272e+00,  2.8751e+00,
         -3.7823e-03,  4.6902e+00]], dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 52
	action: tensor([[-3.7402,  0.3406,  5.5967, -4.8683,  3.5958, -0.1755,  4.0812]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 53
	action: tensor([[-4.0139,  0.1043,  6.1800, -4.8898,  2.6486, -0.5134,  5.1867]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 54
	action: tensor([[-3.8785,  0.1742,  6.1800, -4.3569,  3.5250,  0.7095,  4.0281]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 55
	action: tensor([[-3.8749,  0.1537,  5.9220, -4.8586,  3.0764,  0.0487,  4.2495]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 56
	action: tensor([[-3.3416, -0.0532,  6.1483, -5.1899,  2.9551,  0.2344,  4.5026]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 57
	action: tensor([[-3.7277,  0.3424,  6.1800, -4.6769,  3.1019,  0.1498,  4.7476]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 58
	action: tensor([[-4.1169, -0.1288,  5.7188, -4.6053,  2.8604, -0.2149,  4.2430]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 59
	action: tensor([[-3.6819,  1.0203,  6.0147, -4.9765,  3.2312,  0.2142,  4.6921]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 60
	action: tensor([[-3.5596,  0.0554,  6.0833, -4.5880,  2.9539,  0.0583,  4.1108]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 61
	action: tensor([[-4.4081,  0.5767,  6.1800, -5.0418,  3.2935,  0.4343,  4.5869]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 62
	action: tensor([[-3.6687,  0.1698,  6.1800, -4.6098,  2.8833,  0.3984,  3.9064]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 63
	action: tensor([[-4.2063,  0.3051,  6.1800, -5.0234,  2.4893, -0.0565,  4.7726]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 64
	action: tensor([[-4.2039,  0.1333,  6.1800, -5.3032,  2.8351,  0.2319,  3.9905]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 65
	action: tensor([[-4.0762,  0.8586,  6.0036, -4.9042,  2.9210,  0.5042,  4.4757]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 66
	action: tensor([[-4.2072,  0.0969,  6.1800, -4.3958,  3.0789,  0.3720,  4.3116]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 67
	action: tensor([[-4.4037,  0.4448,  6.1800, -4.9859,  3.5653,  0.5877,  4.3671]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 68
	action: tensor([[-4.4532,  0.4643,  6.1800, -4.7143,  3.3739,  0.3390,  4.4504]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 69
	action: tensor([[-4.2105,  0.6768,  6.0282, -4.8043,  3.0376, -0.2444,  3.7012]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 70
	action: tensor([[-4.0355,  0.3679,  6.1800, -4.0894,  3.0595,  0.0748,  4.2939]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 71
	action: tensor([[-4.3932,  0.1847,  5.9683, -4.9183,  2.4418,  0.7144,  4.6760]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 72
	action: tensor([[-3.5627,  0.3043,  6.0799, -4.7539,  2.6906,  0.1936,  4.0888]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 73
	action: tensor([[-4.0962,  0.1687,  6.1800, -5.0767,  2.9790,  0.5440,  4.0880]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 74
	action: tensor([[-3.7196,  0.7749,  6.1800, -5.0184,  3.0437, -0.1930,  4.5661]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 75
	action: tensor([[-4.1853,  0.4574,  6.1800, -4.2793,  2.9007,  0.5349,  4.3359]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 76
	action: tensor([[-3.7704,  0.3213,  6.1800, -5.2088,  2.6230,  0.0717,  4.2268]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 77
	action: tensor([[-3.4511,  0.9792,  6.1800, -4.4549,  2.8857, -0.3610,  4.5101]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 78
	action: tensor([[-4.2498,  0.0751,  6.1800, -5.0424,  2.7142,  0.3126,  4.2537]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 79
	action: tensor([[-4.4368,  0.4974,  6.1192, -4.8163,  2.3380,  0.3498,  4.3940]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 80
	action: tensor([[-3.7513,  0.0701,  6.1800, -4.1492,  2.4907,  0.1632,  4.0661]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 81
	action: tensor([[-3.9707,  0.8547,  6.1800, -4.3258,  2.8940,  0.2574,  4.3850]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 82
	action: tensor([[-4.6592,  1.0491,  6.1800, -4.4628,  2.7134,  0.0694,  4.8135]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 83
	action: tensor([[-4.3093,  0.3254,  5.9893, -4.3150,  2.9312,  0.4027,  4.0465]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 84
	action: tensor([[-4.3145, -0.0138,  6.1685, -4.9059,  3.0410,  0.5756,  4.4514]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 85
	action: tensor([[-4.1100,  0.1706,  6.1800, -5.3071,  2.6616, -0.0589,  3.7740]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 86
	action: tensor([[-4.1490,  0.6297,  6.1800, -4.6631,  2.8533,  0.6926,  3.7148]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 87
	action: tensor([[-4.1215,  0.1206,  6.1800, -4.6196,  3.0252,  1.0416,  4.4309]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 88
	action: tensor([[-3.9965,  0.0810,  6.1800, -4.7505,  3.0389,  0.0220,  4.9946]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 89
	action: tensor([[-3.6396,  0.6162,  6.1800, -4.6835,  2.7048,  0.4124,  4.0480]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 90
	action: tensor([[-4.1203,  0.3709,  6.0834, -4.7434,  2.9699,  0.3048,  4.8404]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 91
	action: tensor([[-3.9165,  0.4268,  5.9803, -5.2008,  3.2363, -0.0807,  4.1981]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 92
	action: tensor([[-4.0255,  0.5465,  6.1800, -4.7760,  3.2602,  0.7049,  4.3393]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 93
	action: tensor([[-4.2179,  0.1314,  6.0761, -5.1649,  2.9404,  0.1772,  4.3305]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 94
	action: tensor([[-4.1913,  0.3819,  6.1800, -5.0428,  2.9279,  0.4323,  4.0619]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 95
	action: tensor([[-4.3728,  0.4156,  6.1800, -4.6940,  2.7496,  0.3886,  4.6662]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 96
	action: tensor([[-3.4225,  0.5588,  5.8284, -4.9931,  3.0629,  0.2202,  3.7002]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 97
	action: tensor([[-4.4218,  0.8049,  6.1800, -4.3507,  2.8196,  0.1000,  4.1950]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 98
	action: tensor([[-3.6668, -0.3112,  5.7084, -5.3022,  3.3130,  0.6543,  4.7745]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 99
	action: tensor([[-3.4763,  0.1020,  6.1135, -4.2201,  3.0920,  0.0752,  4.4949]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 100
	action: tensor([[-4.0362,  0.7460,  5.8994, -4.0911,  3.0608, -0.4823,  4.4393]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 101
	action: tensor([[-4.0440,  0.7548,  5.8457, -5.3973,  2.9523,  0.0115,  4.1445]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 102
	action: tensor([[-3.8915,  0.3784,  6.1800, -5.2032,  3.2709, -0.2644,  4.1887]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 103
	action: tensor([[-3.9384,  0.1917,  5.8294, -4.8644,  3.1059,  0.1376,  4.7317]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 104
	action: tensor([[-3.8726,  0.5202,  5.8286, -4.8983,  3.1717,  0.1478,  4.1926]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 105
	action: tensor([[-4.2630,  0.3166,  6.1800, -4.5729,  3.2181,  0.0339,  4.3555]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 106
	action: tensor([[-4.0097,  0.3960,  6.1800, -4.6311,  3.0110,  0.7714,  4.6361]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 107
	action: tensor([[-3.8266,  0.3758,  6.1800, -5.3358,  2.8790,  0.3912,  4.4318]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 108
	action: tensor([[-4.2267,  0.2872,  6.1800, -5.2279,  3.0754,  0.5061,  4.0840]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 109
	action: tensor([[-3.5892,  0.0301,  5.7778, -5.1964,  2.5743,  0.3750,  4.5528]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 110
	action: tensor([[-4.1694,  0.7292,  5.8069, -5.4974,  3.3109,  0.0768,  3.7729]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 111
	action: tensor([[-3.9969,  0.1775,  5.9822, -5.4829,  2.7593,  0.6367,  4.2349]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 112
	action: tensor([[-4.0304,  0.2849,  6.1800, -4.6421,  3.0036,  0.2306,  4.9466]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 113
	action: tensor([[-4.2384,  1.0547,  6.1800, -4.1472,  2.9881,  0.8119,  4.3819]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 114
	action: tensor([[-4.0565,  0.4568,  6.1800, -4.7928,  3.0751,  0.2584,  4.3306]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 115
	action: tensor([[-3.6949,  0.4959,  6.1050, -5.6466,  3.1892,  0.2477,  4.3141]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 116
	action: tensor([[-3.6231,  0.2886,  6.0713, -4.8398,  2.6187, -0.0089,  4.5735]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 117
	action: tensor([[-4.1874,  0.2432,  6.1800, -4.5774,  3.0294,  0.5990,  4.6685]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 118
	action: tensor([[-3.8962,  0.7309,  5.9668, -4.9674,  3.0386,  0.4699,  4.7884]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 119
	action: tensor([[-3.5909,  0.7919,  6.1800, -4.8788,  3.0667,  0.3175,  4.3583]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 120
	action: tensor([[-3.5892,  0.2431,  6.1800, -4.8321,  3.1035,  0.8643,  4.3027]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 121
	action: tensor([[-3.7469,  0.3019,  6.1800, -4.5906,  2.6248,  0.8217,  4.1628]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 122
	action: tensor([[-4.3696,  0.5882,  6.0284, -4.2730,  2.9314,  0.5997,  4.4449]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 123
	action: tensor([[-4.3799,  0.1241,  6.1800, -4.6783,  3.2118,  1.0526,  4.2691]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 124
	action: tensor([[-3.9508,  0.0479,  6.1800, -5.2396,  3.0268,  0.4357,  4.4292]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 125
	action: tensor([[-3.4322,  0.3732,  5.8993, -4.8321,  2.8867, -0.1323,  4.4762]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 126
	action: tensor([[-3.9415,  0.3061,  6.1800, -5.1241,  3.4836,  0.1866,  4.5736]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 57, step: 127
	action: tensor([[-3.7054,  0.4485,  6.0860, -4.5982,  3.1546, -0.2528,  4.7901]],
       dtype=torch.float64)
	q_value: tensor([[-61.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
LOSS epoch 57 actor 67.01701597043231 critic 134.09195322753695 
epoch: 58, step: 0
	action: tensor([[-4.8773,  0.4473,  5.6140, -4.8539,  3.0674,  0.3512,  3.5383]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 1
	action: tensor([[-4.0203,  0.5821,  6.1800, -5.0841,  3.4227,  0.0422,  4.2146]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 2
	action: tensor([[-3.9620,  0.1322,  6.1800, -5.2278,  2.6666,  0.0290,  4.7451]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 3
	action: tensor([[-3.9011,  0.4741,  6.1800, -4.5892,  3.4083,  0.6741,  4.0713]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 4
	action: tensor([[-3.0353, -0.3629,  5.9917, -5.2743,  2.6937, -0.0374,  5.0180]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 5
	action: tensor([[-3.5655, -0.2036,  6.1800, -4.4011,  2.9980,  0.0106,  4.4908]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 6
	action: tensor([[-3.9496, -0.0457,  5.6218, -4.8298,  2.7819,  0.3017,  4.6614]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 7
	action: tensor([[-4.2242, -0.0980,  5.8271, -4.8771,  3.1308,  0.4447,  4.4015]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 8
	action: tensor([[-3.7876,  0.2595,  6.1800, -4.7730,  2.9868,  0.7012,  4.5601]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 9
	action: tensor([[-3.6186, -0.0184,  6.0114, -4.9922,  3.4863,  0.2438,  4.2574]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 10
	action: tensor([[-4.4647,  0.8133,  6.1800, -5.8200,  2.8100,  0.1569,  4.3799]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 11
	action: tensor([[-3.3439,  0.2765,  6.1800, -5.1643,  3.6299,  0.5474,  4.9552]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 12
	action: tensor([[-4.4282,  0.2014,  6.1800, -5.0198,  2.9477, -0.0386,  4.3771]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 13
	action: tensor([[-3.5998, -0.5084,  6.1069, -5.0854,  2.6086,  0.0625,  4.3240]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 14
	action: tensor([[-4.1031,  0.5819,  5.6016, -4.7424,  2.6302,  0.7029,  4.3942]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 15
	action: tensor([[-4.1292,  0.2313,  6.1800, -5.3190,  3.1719, -0.0444,  4.0151]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 16
	action: tensor([[-3.9037,  0.2402,  6.1800, -5.4149,  2.9031,  0.5278,  4.2551]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 17
	action: tensor([[-3.9415,  0.2236,  5.6455, -4.6687,  3.4416, -0.0638,  3.8934]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 18
	action: tensor([[-4.0770,  0.3423,  6.1800, -4.9005,  3.0577, -0.3518,  3.8371]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 19
	action: tensor([[-3.8374,  0.4660,  6.1281, -4.4734,  2.4321,  0.3682,  4.4567]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 20
	action: tensor([[-4.2152,  0.3891,  6.1800, -4.7980,  3.0186,  0.4995,  4.1945]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 21
	action: tensor([[-4.0720,  0.0422,  6.1800, -5.0506,  2.7810, -0.4980,  4.0830]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 22
	action: tensor([[-4.4228,  0.5171,  6.1800, -5.3725,  3.1327, -0.4970,  4.9655]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 23
	action: tensor([[-3.5857, -0.1928,  6.0477, -5.0347,  2.6169,  0.1573,  4.3641]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 24
	action: tensor([[-3.8870,  0.2528,  6.1800, -4.3711,  2.6238,  0.2047,  4.1621]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 25
	action: tensor([[-3.8317,  0.3845,  6.1800, -4.6962,  3.0801,  0.2347,  4.5968]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 26
	action: tensor([[-3.9976,  0.3873,  6.0009, -4.4731,  3.3750,  0.1479,  4.2562]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 27
	action: tensor([[-4.2735, -0.2420,  6.1800, -4.8007,  2.5277,  0.3406,  4.8250]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 28
	action: tensor([[-3.5945,  0.2675,  6.1800, -4.9784,  3.2758,  0.5245,  4.5134]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 29
	action: tensor([[-4.4597,  0.8060,  6.1687, -4.4068,  3.3095,  0.9958,  4.1738]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 30
	action: tensor([[-4.6883, -0.0638,  6.1800, -5.0616,  2.8867,  0.1662,  4.4619]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 31
	action: tensor([[-4.3808,  0.0897,  6.1800, -4.9992,  3.0950,  0.4256,  4.8250]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 32
	action: tensor([[-4.2070,  0.3724,  6.0903, -5.1072,  3.1100, -0.4699,  4.4155]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 33
	action: tensor([[-3.3435,  0.1363,  6.0309, -4.7520,  2.9105, -0.0681,  4.7484]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 34
	action: tensor([[-4.8158,  0.2135,  6.0544, -5.2674,  2.7446,  0.2549,  4.7363]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 35
	action: tensor([[-4.2622,  0.3027,  6.1800, -5.3609,  2.7773,  0.1266,  4.4872]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 36
	action: tensor([[-3.7384,  1.2178,  6.0577, -4.9741,  3.3752,  0.1144,  3.7690]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 37
	action: tensor([[-4.0531, -0.1851,  6.1800, -4.3291,  2.5030,  0.2624,  4.4625]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 58, step: 38
	action: tensor([[-3.9500,  0.2209,  6.1800, -4.6046,  2.9764,  0.3448,  4.5119]],
       dtype=torch.float64)
	q_value: tensor([[-47.4894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
